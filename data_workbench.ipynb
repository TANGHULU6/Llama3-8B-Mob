{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件总行数: 3001\n",
      "   user_id   status                                     assistant_json\n",
      "0    22514   failed                                                NaN\n",
      "1    22514  success  {\"reason\": \"The individual's trajectory shows ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = 'wandb_export_2024-09-16T15_51_59.893+08_00.csv'  # 替换为实际文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 查看总行数\n",
    "total_rows = len(df)\n",
    "print(f\"文件总行数: {total_rows}\")\n",
    "\n",
    "# 筛选出userid为222的行\n",
    "filtered_df = df[df['user_id'] == 22514]\n",
    "\n",
    "# 查看筛选后的行\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉status为failed的行\n",
    "df = df[df['status'] != 'failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996\n"
     ]
    }
   ],
   "source": [
    "fix_df = pd.read_csv('wandb_export_2024-09-17T00_51_34.044+08_00.csv')  # 替换为你的修正文件路径\n",
    "\n",
    "# 获取需要替换的user_id\n",
    "# replace_user_ids = fix_df['user_id'].unique()\n",
    "replace_user_ids = [22072, 22396, 22494, 23067]\n",
    "# 从原始df中删除需要替换的user_id数据\n",
    "df = df[~df['user_id'].isin(replace_user_ids)]\n",
    "print(len(df))\n",
    "fix_df = fix_df[fix_df['status'] == 'success']\n",
    "fix_df = fix_df[fix_df['user_id'].isin(replace_user_ids)]\n",
    "\n",
    "# 将fix_df中的数据合并到df中\n",
    "df = pd.concat([df, fix_df], ignore_index=True)\n",
    "\n",
    "# 重新排序（按user_id排序）\n",
    "df_sorted = df.sort_values(by='user_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "压缩后的文件总行数: 3000\n",
      "前几行内容:\n",
      "   user_id   status                                     assistant_json\n",
      "0    22000  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "1    22001  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "2    22002  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "3    22003  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "4    22004  success  {\"reason\": \"The individual's trajectory shows ...\n"
     ]
    }
   ],
   "source": [
    "# 查看总行数\n",
    "total_rows = len(df_sorted)\n",
    "print(f\"压缩后的文件总行数: {total_rows}\")\n",
    "\n",
    "# 查看前几行\n",
    "print(\"前几行内容:\")\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_column(json_str):\n",
    "    return json.loads(json_str)\n",
    "\n",
    "# 解析assistant_json列\n",
    "df_sorted['parsed_json'] = df_sorted['assistant_json'].apply(parse_json_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取后的前几行内容:\n",
      "   user_id   status                                         prediction\n",
      "0    22000  success  [[60, 2, 70, 107], [60, 16, 71, 107], [60, 17,...\n",
      "1    22001  success  [[60, 9, 71, 60], [60, 14, 71, 60], [60, 41, 7...\n",
      "2    22002  success  [[60, 13, 46, 139], [60, 14, 57, 153], [60, 15...\n",
      "3    22003  success  [[60, 2, 174, 8], [60, 6, 174, 9], [60, 8, 174...\n",
      "4    22004  success  [[60, 0, 51, 114], [60, 9, 51, 115], [60, 10, ...\n",
      "压缩后的文件总行数: 3000\n"
     ]
    }
   ],
   "source": [
    "df_sorted['prediction'] = df_sorted['parsed_json'].apply(lambda x: x['prediction'] if x else None)\n",
    "\n",
    "# 查看解析后的数据\n",
    "print(\"提取后的前几行内容:\")\n",
    "print(df_sorted[['user_id', 'status', 'prediction']].head())\n",
    "\n",
    "# 保存提取后的结果为新文件\n",
    "total_rows = len(df_sorted)\n",
    "print(f\"压缩后的文件总行数: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id   d   t   x    y\n",
      "0    22000  60   2  70  107\n",
      "1    22000  60  16  71  107\n",
      "2    22000  60  17  76  101\n",
      "3    22000  60  24  77  101\n",
      "4    22000  60  26  77  101\n",
      "文件总行数: 462816\n",
      "展开后的数据包含 3000 个不同的用户。\n"
     ]
    }
   ],
   "source": [
    "expanded_data = []\n",
    "\n",
    "# 遍历每一行\n",
    "for index, row in df_sorted.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    predictions = row['prediction']\n",
    "    \n",
    "    # 遍历每个预测值，将其添加到expanded_data列表中\n",
    "    for pred in predictions:  # 使用eval解析字符串形式的列表\n",
    "        expanded_data.append([user_id] + pred)\n",
    "\n",
    "# 将展开的数据转换为DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_data, columns=['user_id', 'd', 't', 'x', 'y'])\n",
    "\n",
    "# 查看转换后的数据\n",
    "print(expanded_df.head())\n",
    "total_rows = len(expanded_df)\n",
    "print(f\"文件总行数: {total_rows}\")\n",
    "# 确保 expanded_df 已经创建\n",
    "unique_users = expanded_df['user_id'].nunique()\n",
    "print(f\"展开后的数据包含 {unique_users} 个不同的用户。\")\n",
    "expanded_df.to_csv('cityB.csv.gz', compression='gzip', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bl4mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
