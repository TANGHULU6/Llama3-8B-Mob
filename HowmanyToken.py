# -*- coding: utf-8 -*-
"""Llama-3 8b Instruct Unsloth 2x faster finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).

This notebook uses the `Llama-3` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

"""* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc
* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.
* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.
* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.
* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)
"""

from geobleu.Report import report_geobleu_dtw_gpt
from unsloth import FastLanguageModel
import torch
max_seq_length = 204800 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-Instruct-bnb-4bit", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

import json
from datasets import Dataset
from unsloth.chat_templates import get_chat_template

def load_custom_dataset(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Create a dataset structure suitable for the datasets library
    formatted_data = {
        "conversations": [
            convo["messages"] for convo in data
        ]
    }
    
    # Convert the dataset into the Hugging Face datasets format
    dataset = Dataset.from_dict(formatted_data)
    return dataset

def format_conversations(conversation):
    formatted_convo = []
    for message in conversation:
        formatted_message = {
            "from": message["role"],
            "value": message["content"]
        }
        formatted_convo.append(formatted_message)
    return formatted_convo

def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = []
    for convo in convos:
        formatted_convo = format_conversations(convo)
        try:
            text = tokenizer.apply_chat_template(formatted_convo, tokenize=False, add_generation_prompt=False)
            texts.append(text)
        except Exception as e:
            print(f"Error processing conversation: {convo}")
            raise e
    return {"text": texts}

# Initialize the tokenizer with the correct chat template
tokenizer = get_chat_template(
    tokenizer,
    chat_template="llama-3",
    mapping={"role": "from", "content": "value", "user": "human", "assistant": "gpt"},
)

# Load and format the custom dataset
train_dataset = load_custom_dataset("dataset60000_not_completed.json")
# train_dataset = train_dataset.select(range(10000))
train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
test_dataset = load_custom_dataset("dataset60000-79999_not_completed.json")
# test_dataset = test_dataset.select(range(100))
test_dataset = test_dataset.map(formatting_prompts_func, batched=True)
# dataset = load_custom_dataset("dataset10000.json")
# dataset = dataset.map(formatting_prompts_func, batched=True)

# # Print the 5th conversation
# # print(dataset[5]["conversations"])

# # Print the formatted text
# # print(dataset[5]["text"])


# # 确保所有数据都已处理
# assert "text" in dataset.features

# train_size = int(0.995 * len(dataset))
# test_size = len(dataset) - train_size

# train_dataset = dataset.select(range(train_size))
# test_dataset = dataset.select(range(train_size, len(dataset)))

"""If you're looking to make your own chat template, that also is possible! You must use the Jinja templating regime. We provide our own stripped down version of the `Unsloth template` which we find to be more efficient, and leverages ChatML, Zephyr and Alpaca styles.

More info on chat templates on [our wiki page!](https://github.com/unslothai/unsloth/wiki#chat-templates)
"""

unsloth_template = \
    "{{ bos_token }}"\
    "{{ 'You are a helpful assistant to the user\n' }}"\
    "{% for message in messages %}"\
        "{% if message['role'] == 'user' %}"\
            "{{ '>>> User: ' + message['content'] + '\n' }}"\
        "{% elif message['role'] == 'assistant' %}"\
            "{{ '>>> Assistant: ' + message['content'] + eos_token + '\n' }}"\
        "{% endif %}"\
    "{% endfor %}"\
    "{% if add_generation_prompt %}"\
        "{{ '>>> Assistant: ' }}"\
    "{% endif %}"
unsloth_eos_token = "eos_token"

# my own test
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the length of each input and reference response in tokens
input_lengths = []
reference_lengths = []

for i, conversation in enumerate(test_dataset):
    try:
        # Concatenate all input messages into a single string
        input_text = ""
        for message in conversation["conversations"]:
            if message["role"] != 'assistant':
                input_text += message["content"] + " "

        # Tokenize the concatenated input string
        tokenized_input_length = len(tokenizer.encode(input_text.strip()))
        input_lengths.append(tokenized_input_length)
        
        # Extract reference responses from the conversation
        reference_responses = [
            message["content"]
            for message in conversation["conversations"]
            if message["role"] == 'assistant'
        ]
        
        # Calculate the token length of each reference response
        for reference in reference_responses:
            tokenized_length = len(tokenizer.encode(reference))
            reference_lengths.append(tokenized_length)
            
    except Exception as e:
        print(f"Exception in conversation {i + 1}: {e}")

# Plotting the histograms for both input lengths and reference response lengths
plt.figure(figsize=(12, 8))

# Plot histogram for input lengths
sns.histplot(input_lengths, bins=50, kde=False, color='blue', label='Input Tokens')

# Plot histogram for reference response lengths
sns.histplot(reference_lengths, bins=50, kde=False, color='red', label='Reference Response Tokens')

plt.title('Distribution of Input and Reference Response Lengths (in Tokens)')
plt.xlabel('Number of Tokens')
plt.ylabel('Frequency')
plt.legend()

# Save the plot to a file
plt.savefig('input_and_reference_response_lengths_histogram.png')

print("Histogram saved as 'input_and_reference_response_lengths_histogram.png'")


