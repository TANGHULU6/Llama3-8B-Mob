{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7954545454545454,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00022727272727272727,
      "grad_norm": 1.7061675786972046,
      "learning_rate": 4e-05,
      "loss": 2.1043,
      "step": 1
    },
    {
      "epoch": 0.00045454545454545455,
      "grad_norm": 1.9381427764892578,
      "learning_rate": 8e-05,
      "loss": 1.7299,
      "step": 2
    },
    {
      "epoch": 0.0006818181818181819,
      "grad_norm": 1.6054526567459106,
      "learning_rate": 0.00012,
      "loss": 1.74,
      "step": 3
    },
    {
      "epoch": 0.0009090909090909091,
      "grad_norm": 1.7346073389053345,
      "learning_rate": 0.00016,
      "loss": 1.6376,
      "step": 4
    },
    {
      "epoch": 0.0011363636363636363,
      "grad_norm": 1.3739463090896606,
      "learning_rate": 0.0002,
      "loss": 1.3528,
      "step": 5
    },
    {
      "epoch": 0.0013636363636363637,
      "grad_norm": 2.2229840755462646,
      "learning_rate": 0.00019995449374288966,
      "loss": 1.3371,
      "step": 6
    },
    {
      "epoch": 0.001590909090909091,
      "grad_norm": 2.3456242084503174,
      "learning_rate": 0.00019990898748577932,
      "loss": 1.1128,
      "step": 7
    },
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 1.4616204500198364,
      "learning_rate": 0.00019986348122866897,
      "loss": 0.8797,
      "step": 8
    },
    {
      "epoch": 0.0020454545454545456,
      "grad_norm": 1.34989595413208,
      "learning_rate": 0.00019981797497155862,
      "loss": 0.9886,
      "step": 9
    },
    {
      "epoch": 0.0022727272727272726,
      "grad_norm": 0.961378812789917,
      "learning_rate": 0.00019977246871444825,
      "loss": 0.6349,
      "step": 10
    },
    {
      "epoch": 0.0025,
      "grad_norm": 0.9619693160057068,
      "learning_rate": 0.00019972696245733787,
      "loss": 0.7878,
      "step": 11
    },
    {
      "epoch": 0.0027272727272727275,
      "grad_norm": 0.5334128141403198,
      "learning_rate": 0.00019968145620022753,
      "loss": 0.7005,
      "step": 12
    },
    {
      "epoch": 0.0029545454545454545,
      "grad_norm": 0.4629640281200409,
      "learning_rate": 0.00019963594994311718,
      "loss": 0.5702,
      "step": 13
    },
    {
      "epoch": 0.003181818181818182,
      "grad_norm": 0.3844805061817169,
      "learning_rate": 0.00019959044368600683,
      "loss": 0.5703,
      "step": 14
    },
    {
      "epoch": 0.003409090909090909,
      "grad_norm": 0.3922984302043915,
      "learning_rate": 0.00019954493742889648,
      "loss": 0.5212,
      "step": 15
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 0.22252674400806427,
      "learning_rate": 0.00019949943117178614,
      "loss": 0.4904,
      "step": 16
    },
    {
      "epoch": 0.003863636363636364,
      "grad_norm": 0.24700945615768433,
      "learning_rate": 0.0001994539249146758,
      "loss": 0.5312,
      "step": 17
    },
    {
      "epoch": 0.004090909090909091,
      "grad_norm": 0.34841388463974,
      "learning_rate": 0.00019940841865756544,
      "loss": 0.5741,
      "step": 18
    },
    {
      "epoch": 0.004318181818181818,
      "grad_norm": 0.2382451593875885,
      "learning_rate": 0.0001993629124004551,
      "loss": 0.5314,
      "step": 19
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 0.16477593779563904,
      "learning_rate": 0.00019931740614334472,
      "loss": 0.5173,
      "step": 20
    },
    {
      "epoch": 0.004772727272727273,
      "grad_norm": 0.19599975645542145,
      "learning_rate": 0.00019927189988623435,
      "loss": 0.5716,
      "step": 21
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.22323282063007355,
      "learning_rate": 0.000199226393629124,
      "loss": 0.4478,
      "step": 22
    },
    {
      "epoch": 0.005227272727272727,
      "grad_norm": 0.17100629210472107,
      "learning_rate": 0.00019918088737201365,
      "loss": 0.5398,
      "step": 23
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 0.12187627702951431,
      "learning_rate": 0.0001991353811149033,
      "loss": 0.3606,
      "step": 24
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.1740286946296692,
      "learning_rate": 0.00019908987485779296,
      "loss": 0.4842,
      "step": 25
    },
    {
      "epoch": 0.005909090909090909,
      "grad_norm": 0.15255644917488098,
      "learning_rate": 0.0001990443686006826,
      "loss": 0.4734,
      "step": 26
    },
    {
      "epoch": 0.006136363636363636,
      "grad_norm": 0.14801336824893951,
      "learning_rate": 0.00019899886234357226,
      "loss": 0.4087,
      "step": 27
    },
    {
      "epoch": 0.006363636363636364,
      "grad_norm": 0.15726926922798157,
      "learning_rate": 0.00019895335608646192,
      "loss": 0.5224,
      "step": 28
    },
    {
      "epoch": 0.006590909090909091,
      "grad_norm": 0.16785353422164917,
      "learning_rate": 0.00019890784982935157,
      "loss": 0.4455,
      "step": 29
    },
    {
      "epoch": 0.006818181818181818,
      "grad_norm": 0.16072072088718414,
      "learning_rate": 0.0001988623435722412,
      "loss": 0.4757,
      "step": 30
    },
    {
      "epoch": 0.007045454545454546,
      "grad_norm": 0.1378365010023117,
      "learning_rate": 0.00019881683731513082,
      "loss": 0.4391,
      "step": 31
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 0.1322004348039627,
      "learning_rate": 0.00019877133105802047,
      "loss": 0.4415,
      "step": 32
    },
    {
      "epoch": 0.0075,
      "grad_norm": 0.12805570662021637,
      "learning_rate": 0.00019872582480091013,
      "loss": 0.3365,
      "step": 33
    },
    {
      "epoch": 0.007727272727272728,
      "grad_norm": 0.1261025369167328,
      "learning_rate": 0.00019868031854379978,
      "loss": 0.3783,
      "step": 34
    },
    {
      "epoch": 0.007954545454545454,
      "grad_norm": 0.14965221285820007,
      "learning_rate": 0.00019863481228668943,
      "loss": 0.408,
      "step": 35
    },
    {
      "epoch": 0.008181818181818182,
      "grad_norm": 0.1611635386943817,
      "learning_rate": 0.00019858930602957908,
      "loss": 0.4201,
      "step": 36
    },
    {
      "epoch": 0.00840909090909091,
      "grad_norm": 0.12039367109537125,
      "learning_rate": 0.00019854379977246874,
      "loss": 0.3704,
      "step": 37
    },
    {
      "epoch": 0.008636363636363636,
      "grad_norm": 0.10835084319114685,
      "learning_rate": 0.0001984982935153584,
      "loss": 0.3423,
      "step": 38
    },
    {
      "epoch": 0.008863636363636363,
      "grad_norm": 0.14455156028270721,
      "learning_rate": 0.00019845278725824804,
      "loss": 0.3358,
      "step": 39
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.11763431876897812,
      "learning_rate": 0.00019840728100113767,
      "loss": 0.3726,
      "step": 40
    },
    {
      "epoch": 0.009318181818181817,
      "grad_norm": 0.10410670936107635,
      "learning_rate": 0.0001983617747440273,
      "loss": 0.4256,
      "step": 41
    },
    {
      "epoch": 0.009545454545454546,
      "grad_norm": 0.11134195327758789,
      "learning_rate": 0.00019831626848691695,
      "loss": 0.3815,
      "step": 42
    },
    {
      "epoch": 0.009772727272727273,
      "grad_norm": 0.12476523220539093,
      "learning_rate": 0.0001982707622298066,
      "loss": 0.3587,
      "step": 43
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.09392108023166656,
      "learning_rate": 0.00019822525597269625,
      "loss": 0.4377,
      "step": 44
    },
    {
      "epoch": 0.010227272727272727,
      "grad_norm": 0.11695029586553574,
      "learning_rate": 0.0001981797497155859,
      "loss": 0.3944,
      "step": 45
    },
    {
      "epoch": 0.010454545454545454,
      "grad_norm": 0.16431795060634613,
      "learning_rate": 0.00019813424345847556,
      "loss": 0.4715,
      "step": 46
    },
    {
      "epoch": 0.010681818181818181,
      "grad_norm": 0.12449543923139572,
      "learning_rate": 0.0001980887372013652,
      "loss": 0.3686,
      "step": 47
    },
    {
      "epoch": 0.01090909090909091,
      "grad_norm": 0.0931965559720993,
      "learning_rate": 0.00019804323094425486,
      "loss": 0.3433,
      "step": 48
    },
    {
      "epoch": 0.011136363636363637,
      "grad_norm": 0.09647012501955032,
      "learning_rate": 0.0001979977246871445,
      "loss": 0.349,
      "step": 49
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.09154331684112549,
      "learning_rate": 0.00019795221843003414,
      "loss": 0.3964,
      "step": 50
    },
    {
      "epoch": 0.011590909090909091,
      "grad_norm": 0.11974641680717468,
      "learning_rate": 0.00019790671217292377,
      "loss": 0.4299,
      "step": 51
    },
    {
      "epoch": 0.011818181818181818,
      "grad_norm": 0.07552523165941238,
      "learning_rate": 0.00019786120591581342,
      "loss": 0.3911,
      "step": 52
    },
    {
      "epoch": 0.012045454545454545,
      "grad_norm": 0.13606470823287964,
      "learning_rate": 0.00019781569965870307,
      "loss": 0.5186,
      "step": 53
    },
    {
      "epoch": 0.012272727272727272,
      "grad_norm": 0.07631877064704895,
      "learning_rate": 0.00019777019340159273,
      "loss": 0.421,
      "step": 54
    },
    {
      "epoch": 0.0125,
      "grad_norm": 0.08050905913114548,
      "learning_rate": 0.00019772468714448238,
      "loss": 0.3634,
      "step": 55
    },
    {
      "epoch": 0.012727272727272728,
      "grad_norm": 0.10301335155963898,
      "learning_rate": 0.00019767918088737203,
      "loss": 0.4323,
      "step": 56
    },
    {
      "epoch": 0.012954545454545455,
      "grad_norm": 0.07760609686374664,
      "learning_rate": 0.00019763367463026169,
      "loss": 0.4343,
      "step": 57
    },
    {
      "epoch": 0.013181818181818182,
      "grad_norm": 0.0749678835272789,
      "learning_rate": 0.00019758816837315134,
      "loss": 0.3425,
      "step": 58
    },
    {
      "epoch": 0.013409090909090909,
      "grad_norm": 0.08354564756155014,
      "learning_rate": 0.00019754266211604096,
      "loss": 0.4176,
      "step": 59
    },
    {
      "epoch": 0.013636363636363636,
      "grad_norm": 0.07877840101718903,
      "learning_rate": 0.00019749715585893062,
      "loss": 0.3657,
      "step": 60
    },
    {
      "epoch": 0.013863636363636364,
      "grad_norm": 0.12496910989284515,
      "learning_rate": 0.00019745164960182024,
      "loss": 0.3166,
      "step": 61
    },
    {
      "epoch": 0.014090909090909091,
      "grad_norm": 0.07103057205677032,
      "learning_rate": 0.0001974061433447099,
      "loss": 0.3782,
      "step": 62
    },
    {
      "epoch": 0.014318181818181818,
      "grad_norm": 0.07451606541872025,
      "learning_rate": 0.00019736063708759955,
      "loss": 0.357,
      "step": 63
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 0.07082439213991165,
      "learning_rate": 0.0001973151308304892,
      "loss": 0.3314,
      "step": 64
    },
    {
      "epoch": 0.014772727272727272,
      "grad_norm": 0.07347650080919266,
      "learning_rate": 0.00019726962457337885,
      "loss": 0.325,
      "step": 65
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.07538234442472458,
      "learning_rate": 0.0001972241183162685,
      "loss": 0.3866,
      "step": 66
    },
    {
      "epoch": 0.015227272727272726,
      "grad_norm": 0.09152340143918991,
      "learning_rate": 0.00019717861205915816,
      "loss": 0.3394,
      "step": 67
    },
    {
      "epoch": 0.015454545454545455,
      "grad_norm": 0.08329875767230988,
      "learning_rate": 0.0001971331058020478,
      "loss": 0.3751,
      "step": 68
    },
    {
      "epoch": 0.015681818181818182,
      "grad_norm": 0.10365977138280869,
      "learning_rate": 0.00019708759954493744,
      "loss": 0.4283,
      "step": 69
    },
    {
      "epoch": 0.015909090909090907,
      "grad_norm": 0.09541261941194534,
      "learning_rate": 0.0001970420932878271,
      "loss": 0.384,
      "step": 70
    },
    {
      "epoch": 0.016136363636363636,
      "grad_norm": 0.07184746861457825,
      "learning_rate": 0.00019699658703071672,
      "loss": 0.3632,
      "step": 71
    },
    {
      "epoch": 0.016363636363636365,
      "grad_norm": 0.10609248280525208,
      "learning_rate": 0.00019695108077360637,
      "loss": 0.3913,
      "step": 72
    },
    {
      "epoch": 0.01659090909090909,
      "grad_norm": 0.0711880549788475,
      "learning_rate": 0.00019690557451649602,
      "loss": 0.361,
      "step": 73
    },
    {
      "epoch": 0.01681818181818182,
      "grad_norm": 0.07842044532299042,
      "learning_rate": 0.00019686006825938567,
      "loss": 0.3851,
      "step": 74
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.09122690558433533,
      "learning_rate": 0.00019681456200227533,
      "loss": 0.4146,
      "step": 75
    },
    {
      "epoch": 0.017272727272727273,
      "grad_norm": 0.08035755157470703,
      "learning_rate": 0.00019676905574516498,
      "loss": 0.4611,
      "step": 76
    },
    {
      "epoch": 0.0175,
      "grad_norm": 0.09286895394325256,
      "learning_rate": 0.00019672354948805463,
      "loss": 0.3461,
      "step": 77
    },
    {
      "epoch": 0.017727272727272727,
      "grad_norm": 0.07437147945165634,
      "learning_rate": 0.00019667804323094426,
      "loss": 0.3953,
      "step": 78
    },
    {
      "epoch": 0.017954545454545456,
      "grad_norm": 0.09336010366678238,
      "learning_rate": 0.0001966325369738339,
      "loss": 0.3599,
      "step": 79
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 0.08107893913984299,
      "learning_rate": 0.00019658703071672356,
      "loss": 0.3864,
      "step": 80
    },
    {
      "epoch": 0.01840909090909091,
      "grad_norm": 0.09532348066568375,
      "learning_rate": 0.0001965415244596132,
      "loss": 0.3779,
      "step": 81
    },
    {
      "epoch": 0.018636363636363635,
      "grad_norm": 0.07942318171262741,
      "learning_rate": 0.00019649601820250284,
      "loss": 0.3405,
      "step": 82
    },
    {
      "epoch": 0.018863636363636364,
      "grad_norm": 0.07078859955072403,
      "learning_rate": 0.0001964505119453925,
      "loss": 0.3378,
      "step": 83
    },
    {
      "epoch": 0.019090909090909092,
      "grad_norm": 0.07205542176961899,
      "learning_rate": 0.00019640500568828215,
      "loss": 0.3452,
      "step": 84
    },
    {
      "epoch": 0.019318181818181818,
      "grad_norm": 0.08652660995721817,
      "learning_rate": 0.0001963594994311718,
      "loss": 0.3244,
      "step": 85
    },
    {
      "epoch": 0.019545454545454546,
      "grad_norm": 0.09490218758583069,
      "learning_rate": 0.00019631399317406145,
      "loss": 0.4079,
      "step": 86
    },
    {
      "epoch": 0.01977272727272727,
      "grad_norm": 0.10674455761909485,
      "learning_rate": 0.0001962684869169511,
      "loss": 0.4546,
      "step": 87
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.06977485120296478,
      "learning_rate": 0.00019622298065984073,
      "loss": 0.3248,
      "step": 88
    },
    {
      "epoch": 0.020227272727272726,
      "grad_norm": 0.07963218539953232,
      "learning_rate": 0.00019617747440273039,
      "loss": 0.3237,
      "step": 89
    },
    {
      "epoch": 0.020454545454545454,
      "grad_norm": 0.07348807901144028,
      "learning_rate": 0.00019613196814562004,
      "loss": 0.3804,
      "step": 90
    },
    {
      "epoch": 0.020681818181818183,
      "grad_norm": 0.07830533385276794,
      "learning_rate": 0.00019608646188850966,
      "loss": 0.3325,
      "step": 91
    },
    {
      "epoch": 0.02090909090909091,
      "grad_norm": 0.10718666762113571,
      "learning_rate": 0.00019604095563139932,
      "loss": 0.4018,
      "step": 92
    },
    {
      "epoch": 0.021136363636363637,
      "grad_norm": 0.06821158528327942,
      "learning_rate": 0.00019599544937428897,
      "loss": 0.3229,
      "step": 93
    },
    {
      "epoch": 0.021363636363636362,
      "grad_norm": 0.07508662343025208,
      "learning_rate": 0.00019594994311717862,
      "loss": 0.3441,
      "step": 94
    },
    {
      "epoch": 0.02159090909090909,
      "grad_norm": 0.07056436687707901,
      "learning_rate": 0.00019590443686006828,
      "loss": 0.3285,
      "step": 95
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 0.06439346820116043,
      "learning_rate": 0.00019585893060295793,
      "loss": 0.3078,
      "step": 96
    },
    {
      "epoch": 0.022045454545454545,
      "grad_norm": 0.07567521184682846,
      "learning_rate": 0.00019581342434584758,
      "loss": 0.3968,
      "step": 97
    },
    {
      "epoch": 0.022272727272727274,
      "grad_norm": 0.05098596587777138,
      "learning_rate": 0.0001957679180887372,
      "loss": 0.3178,
      "step": 98
    },
    {
      "epoch": 0.0225,
      "grad_norm": 0.06286492198705673,
      "learning_rate": 0.00019572241183162686,
      "loss": 0.3659,
      "step": 99
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.0849071741104126,
      "learning_rate": 0.0001956769055745165,
      "loss": 0.331,
      "step": 100
    },
    {
      "epoch": 0.022954545454545453,
      "grad_norm": 0.06649050116539001,
      "learning_rate": 0.00019563139931740614,
      "loss": 0.3642,
      "step": 101
    },
    {
      "epoch": 0.023181818181818182,
      "grad_norm": 0.08586762100458145,
      "learning_rate": 0.0001955858930602958,
      "loss": 0.4107,
      "step": 102
    },
    {
      "epoch": 0.02340909090909091,
      "grad_norm": 0.08156292885541916,
      "learning_rate": 0.00019554038680318544,
      "loss": 0.4019,
      "step": 103
    },
    {
      "epoch": 0.023636363636363636,
      "grad_norm": 0.0664215087890625,
      "learning_rate": 0.0001954948805460751,
      "loss": 0.2747,
      "step": 104
    },
    {
      "epoch": 0.023863636363636365,
      "grad_norm": 0.08002714067697525,
      "learning_rate": 0.00019544937428896475,
      "loss": 0.3559,
      "step": 105
    },
    {
      "epoch": 0.02409090909090909,
      "grad_norm": 0.06395529210567474,
      "learning_rate": 0.0001954038680318544,
      "loss": 0.3825,
      "step": 106
    },
    {
      "epoch": 0.02431818181818182,
      "grad_norm": 0.0893787369132042,
      "learning_rate": 0.00019535836177474406,
      "loss": 0.375,
      "step": 107
    },
    {
      "epoch": 0.024545454545454544,
      "grad_norm": 0.06080419942736626,
      "learning_rate": 0.00019531285551763368,
      "loss": 0.3898,
      "step": 108
    },
    {
      "epoch": 0.024772727272727273,
      "grad_norm": 0.07557515799999237,
      "learning_rate": 0.00019526734926052333,
      "loss": 0.3312,
      "step": 109
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.0850738137960434,
      "learning_rate": 0.00019522184300341299,
      "loss": 0.4087,
      "step": 110
    },
    {
      "epoch": 0.025227272727272727,
      "grad_norm": 0.07683387398719788,
      "learning_rate": 0.0001951763367463026,
      "loss": 0.3496,
      "step": 111
    },
    {
      "epoch": 0.025454545454545455,
      "grad_norm": 0.09445192664861679,
      "learning_rate": 0.00019513083048919226,
      "loss": 0.3936,
      "step": 112
    },
    {
      "epoch": 0.02568181818181818,
      "grad_norm": 0.10000496357679367,
      "learning_rate": 0.00019508532423208192,
      "loss": 0.4327,
      "step": 113
    },
    {
      "epoch": 0.02590909090909091,
      "grad_norm": 0.0700996145606041,
      "learning_rate": 0.00019503981797497157,
      "loss": 0.4043,
      "step": 114
    },
    {
      "epoch": 0.026136363636363635,
      "grad_norm": 0.08308038860559464,
      "learning_rate": 0.00019499431171786122,
      "loss": 0.3166,
      "step": 115
    },
    {
      "epoch": 0.026363636363636363,
      "grad_norm": 0.08339669555425644,
      "learning_rate": 0.00019494880546075088,
      "loss": 0.3863,
      "step": 116
    },
    {
      "epoch": 0.026590909090909092,
      "grad_norm": 0.07821792364120483,
      "learning_rate": 0.0001949032992036405,
      "loss": 0.3003,
      "step": 117
    },
    {
      "epoch": 0.026818181818181817,
      "grad_norm": 0.07369182258844376,
      "learning_rate": 0.00019485779294653015,
      "loss": 0.3016,
      "step": 118
    },
    {
      "epoch": 0.027045454545454546,
      "grad_norm": 0.06983321160078049,
      "learning_rate": 0.0001948122866894198,
      "loss": 0.3276,
      "step": 119
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 0.08950214833021164,
      "learning_rate": 0.00019476678043230946,
      "loss": 0.4006,
      "step": 120
    },
    {
      "epoch": 0.0275,
      "grad_norm": 0.07427525520324707,
      "learning_rate": 0.00019472127417519909,
      "loss": 0.3738,
      "step": 121
    },
    {
      "epoch": 0.02772727272727273,
      "grad_norm": 0.06212402880191803,
      "learning_rate": 0.00019467576791808874,
      "loss": 0.335,
      "step": 122
    },
    {
      "epoch": 0.027954545454545454,
      "grad_norm": 0.07586422562599182,
      "learning_rate": 0.0001946302616609784,
      "loss": 0.3497,
      "step": 123
    },
    {
      "epoch": 0.028181818181818183,
      "grad_norm": 0.05736668407917023,
      "learning_rate": 0.00019458475540386804,
      "loss": 0.3124,
      "step": 124
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.09005642682313919,
      "learning_rate": 0.0001945392491467577,
      "loss": 0.3513,
      "step": 125
    },
    {
      "epoch": 0.028636363636363637,
      "grad_norm": 0.06907933950424194,
      "learning_rate": 0.00019449374288964735,
      "loss": 0.3928,
      "step": 126
    },
    {
      "epoch": 0.028863636363636362,
      "grad_norm": 0.07285542786121368,
      "learning_rate": 0.00019444823663253698,
      "loss": 0.3856,
      "step": 127
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 0.06889241188764572,
      "learning_rate": 0.00019440273037542663,
      "loss": 0.3681,
      "step": 128
    },
    {
      "epoch": 0.02931818181818182,
      "grad_norm": 0.0656028687953949,
      "learning_rate": 0.00019435722411831628,
      "loss": 0.3697,
      "step": 129
    },
    {
      "epoch": 0.029545454545454545,
      "grad_norm": 0.07064276933670044,
      "learning_rate": 0.00019431171786120593,
      "loss": 0.2869,
      "step": 130
    },
    {
      "epoch": 0.029772727272727274,
      "grad_norm": 0.07014083117246628,
      "learning_rate": 0.00019426621160409556,
      "loss": 0.3573,
      "step": 131
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.08147359639406204,
      "learning_rate": 0.0001942207053469852,
      "loss": 0.3358,
      "step": 132
    },
    {
      "epoch": 0.030227272727272728,
      "grad_norm": 0.06082199513912201,
      "learning_rate": 0.00019417519908987487,
      "loss": 0.2481,
      "step": 133
    },
    {
      "epoch": 0.030454545454545453,
      "grad_norm": 0.06302250176668167,
      "learning_rate": 0.00019412969283276452,
      "loss": 0.3619,
      "step": 134
    },
    {
      "epoch": 0.03068181818181818,
      "grad_norm": 0.07382666319608688,
      "learning_rate": 0.00019408418657565417,
      "loss": 0.3532,
      "step": 135
    },
    {
      "epoch": 0.03090909090909091,
      "grad_norm": 0.06583377718925476,
      "learning_rate": 0.00019403868031854382,
      "loss": 0.307,
      "step": 136
    },
    {
      "epoch": 0.031136363636363636,
      "grad_norm": 0.07309003919363022,
      "learning_rate": 0.00019399317406143345,
      "loss": 0.3704,
      "step": 137
    },
    {
      "epoch": 0.031363636363636364,
      "grad_norm": 0.08070820569992065,
      "learning_rate": 0.0001939476678043231,
      "loss": 0.4148,
      "step": 138
    },
    {
      "epoch": 0.03159090909090909,
      "grad_norm": 0.07050585001707077,
      "learning_rate": 0.00019390216154721276,
      "loss": 0.3407,
      "step": 139
    },
    {
      "epoch": 0.031818181818181815,
      "grad_norm": 0.08015639334917068,
      "learning_rate": 0.0001938566552901024,
      "loss": 0.3717,
      "step": 140
    },
    {
      "epoch": 0.032045454545454544,
      "grad_norm": 0.08776045590639114,
      "learning_rate": 0.00019381114903299203,
      "loss": 0.3345,
      "step": 141
    },
    {
      "epoch": 0.03227272727272727,
      "grad_norm": 0.06802152842283249,
      "learning_rate": 0.0001937656427758817,
      "loss": 0.291,
      "step": 142
    },
    {
      "epoch": 0.0325,
      "grad_norm": 0.09760317951440811,
      "learning_rate": 0.00019372013651877134,
      "loss": 0.4449,
      "step": 143
    },
    {
      "epoch": 0.03272727272727273,
      "grad_norm": 0.08354555815458298,
      "learning_rate": 0.000193674630261661,
      "loss": 0.3122,
      "step": 144
    },
    {
      "epoch": 0.03295454545454545,
      "grad_norm": 0.07256977260112762,
      "learning_rate": 0.00019362912400455065,
      "loss": 0.3441,
      "step": 145
    },
    {
      "epoch": 0.03318181818181818,
      "grad_norm": 0.06674839556217194,
      "learning_rate": 0.00019358361774744027,
      "loss": 0.2611,
      "step": 146
    },
    {
      "epoch": 0.03340909090909091,
      "grad_norm": 0.08592847734689713,
      "learning_rate": 0.00019353811149032992,
      "loss": 0.4058,
      "step": 147
    },
    {
      "epoch": 0.03363636363636364,
      "grad_norm": 0.10826805233955383,
      "learning_rate": 0.00019349260523321958,
      "loss": 0.4021,
      "step": 148
    },
    {
      "epoch": 0.03386363636363637,
      "grad_norm": 0.06371083855628967,
      "learning_rate": 0.00019344709897610923,
      "loss": 0.3623,
      "step": 149
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.062301408499479294,
      "learning_rate": 0.00019340159271899888,
      "loss": 0.3304,
      "step": 150
    },
    {
      "epoch": 0.03431818181818182,
      "grad_norm": 0.0838748887181282,
      "learning_rate": 0.0001933560864618885,
      "loss": 0.3879,
      "step": 151
    },
    {
      "epoch": 0.034545454545454546,
      "grad_norm": 0.06971469521522522,
      "learning_rate": 0.00019331058020477816,
      "loss": 0.3499,
      "step": 152
    },
    {
      "epoch": 0.034772727272727275,
      "grad_norm": 0.07092081010341644,
      "learning_rate": 0.00019326507394766781,
      "loss": 0.388,
      "step": 153
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.08281882852315903,
      "learning_rate": 0.00019321956769055747,
      "loss": 0.374,
      "step": 154
    },
    {
      "epoch": 0.035227272727272725,
      "grad_norm": 0.10399414598941803,
      "learning_rate": 0.00019317406143344712,
      "loss": 0.4029,
      "step": 155
    },
    {
      "epoch": 0.035454545454545454,
      "grad_norm": 0.06992468982934952,
      "learning_rate": 0.00019312855517633675,
      "loss": 0.3314,
      "step": 156
    },
    {
      "epoch": 0.03568181818181818,
      "grad_norm": 0.07443633675575256,
      "learning_rate": 0.0001930830489192264,
      "loss": 0.3962,
      "step": 157
    },
    {
      "epoch": 0.03590909090909091,
      "grad_norm": 0.10716292262077332,
      "learning_rate": 0.00019303754266211605,
      "loss": 0.3688,
      "step": 158
    },
    {
      "epoch": 0.03613636363636363,
      "grad_norm": 0.07937020808458328,
      "learning_rate": 0.0001929920364050057,
      "loss": 0.347,
      "step": 159
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.06816514581441879,
      "learning_rate": 0.00019294653014789536,
      "loss": 0.3377,
      "step": 160
    },
    {
      "epoch": 0.03659090909090909,
      "grad_norm": 0.06781031936407089,
      "learning_rate": 0.00019290102389078498,
      "loss": 0.33,
      "step": 161
    },
    {
      "epoch": 0.03681818181818182,
      "grad_norm": 0.09346484392881393,
      "learning_rate": 0.00019285551763367463,
      "loss": 0.4007,
      "step": 162
    },
    {
      "epoch": 0.03704545454545455,
      "grad_norm": 0.07772073149681091,
      "learning_rate": 0.0001928100113765643,
      "loss": 0.3639,
      "step": 163
    },
    {
      "epoch": 0.03727272727272727,
      "grad_norm": 0.08723437041044235,
      "learning_rate": 0.00019276450511945394,
      "loss": 0.4048,
      "step": 164
    },
    {
      "epoch": 0.0375,
      "grad_norm": 0.05157804489135742,
      "learning_rate": 0.0001927189988623436,
      "loss": 0.2902,
      "step": 165
    },
    {
      "epoch": 0.03772727272727273,
      "grad_norm": 0.06270661950111389,
      "learning_rate": 0.00019267349260523322,
      "loss": 0.3204,
      "step": 166
    },
    {
      "epoch": 0.037954545454545456,
      "grad_norm": 0.07819175720214844,
      "learning_rate": 0.00019262798634812287,
      "loss": 0.3644,
      "step": 167
    },
    {
      "epoch": 0.038181818181818185,
      "grad_norm": 0.07868154346942902,
      "learning_rate": 0.00019258248009101252,
      "loss": 0.3602,
      "step": 168
    },
    {
      "epoch": 0.03840909090909091,
      "grad_norm": 0.07666253298521042,
      "learning_rate": 0.00019253697383390218,
      "loss": 0.3213,
      "step": 169
    },
    {
      "epoch": 0.038636363636363635,
      "grad_norm": 0.07753273099660873,
      "learning_rate": 0.00019249146757679183,
      "loss": 0.3332,
      "step": 170
    },
    {
      "epoch": 0.038863636363636364,
      "grad_norm": 0.06626979261636734,
      "learning_rate": 0.00019244596131968146,
      "loss": 0.3741,
      "step": 171
    },
    {
      "epoch": 0.03909090909090909,
      "grad_norm": 0.08075613528490067,
      "learning_rate": 0.0001924004550625711,
      "loss": 0.4009,
      "step": 172
    },
    {
      "epoch": 0.03931818181818182,
      "grad_norm": 0.05142530798912048,
      "learning_rate": 0.00019235494880546076,
      "loss": 0.2464,
      "step": 173
    },
    {
      "epoch": 0.03954545454545454,
      "grad_norm": 0.05338346213102341,
      "learning_rate": 0.00019230944254835041,
      "loss": 0.3113,
      "step": 174
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.05401391163468361,
      "learning_rate": 0.00019226393629124007,
      "loss": 0.2983,
      "step": 175
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.07324906438589096,
      "learning_rate": 0.0001922184300341297,
      "loss": 0.3151,
      "step": 176
    },
    {
      "epoch": 0.04022727272727273,
      "grad_norm": 0.0840369388461113,
      "learning_rate": 0.00019217292377701935,
      "loss": 0.428,
      "step": 177
    },
    {
      "epoch": 0.04045454545454545,
      "grad_norm": 0.06205488741397858,
      "learning_rate": 0.000192127417519909,
      "loss": 0.3727,
      "step": 178
    },
    {
      "epoch": 0.04068181818181818,
      "grad_norm": 0.06869065016508102,
      "learning_rate": 0.00019208191126279865,
      "loss": 0.3324,
      "step": 179
    },
    {
      "epoch": 0.04090909090909091,
      "grad_norm": 0.052917562425136566,
      "learning_rate": 0.0001920364050056883,
      "loss": 0.2576,
      "step": 180
    },
    {
      "epoch": 0.04113636363636364,
      "grad_norm": 0.06414664536714554,
      "learning_rate": 0.00019199089874857793,
      "loss": 0.3205,
      "step": 181
    },
    {
      "epoch": 0.041363636363636366,
      "grad_norm": 0.08538870513439178,
      "learning_rate": 0.00019194539249146758,
      "loss": 0.3352,
      "step": 182
    },
    {
      "epoch": 0.04159090909090909,
      "grad_norm": 0.08619566261768341,
      "learning_rate": 0.00019189988623435724,
      "loss": 0.3741,
      "step": 183
    },
    {
      "epoch": 0.04181818181818182,
      "grad_norm": 0.07959244400262833,
      "learning_rate": 0.0001918543799772469,
      "loss": 0.4124,
      "step": 184
    },
    {
      "epoch": 0.042045454545454546,
      "grad_norm": 0.061386220157146454,
      "learning_rate": 0.00019180887372013651,
      "loss": 0.3152,
      "step": 185
    },
    {
      "epoch": 0.042272727272727274,
      "grad_norm": 0.06265392154455185,
      "learning_rate": 0.00019176336746302617,
      "loss": 0.3732,
      "step": 186
    },
    {
      "epoch": 0.0425,
      "grad_norm": 0.06762311607599258,
      "learning_rate": 0.00019171786120591582,
      "loss": 0.3896,
      "step": 187
    },
    {
      "epoch": 0.042727272727272725,
      "grad_norm": 0.05434518679976463,
      "learning_rate": 0.00019167235494880547,
      "loss": 0.233,
      "step": 188
    },
    {
      "epoch": 0.042954545454545454,
      "grad_norm": 0.07365730404853821,
      "learning_rate": 0.00019162684869169513,
      "loss": 0.3209,
      "step": 189
    },
    {
      "epoch": 0.04318181818181818,
      "grad_norm": 0.05732074752449989,
      "learning_rate": 0.00019158134243458478,
      "loss": 0.3343,
      "step": 190
    },
    {
      "epoch": 0.04340909090909091,
      "grad_norm": 0.053709421306848526,
      "learning_rate": 0.0001915358361774744,
      "loss": 0.346,
      "step": 191
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.07433810830116272,
      "learning_rate": 0.00019149032992036406,
      "loss": 0.389,
      "step": 192
    },
    {
      "epoch": 0.04386363636363636,
      "grad_norm": 0.04815042391419411,
      "learning_rate": 0.0001914448236632537,
      "loss": 0.2877,
      "step": 193
    },
    {
      "epoch": 0.04409090909090909,
      "grad_norm": 0.0686417669057846,
      "learning_rate": 0.00019139931740614336,
      "loss": 0.2519,
      "step": 194
    },
    {
      "epoch": 0.04431818181818182,
      "grad_norm": 0.08955147862434387,
      "learning_rate": 0.000191353811149033,
      "loss": 0.4193,
      "step": 195
    },
    {
      "epoch": 0.04454545454545455,
      "grad_norm": 0.061337485909461975,
      "learning_rate": 0.00019130830489192264,
      "loss": 0.3392,
      "step": 196
    },
    {
      "epoch": 0.04477272727272727,
      "grad_norm": 0.06793681532144547,
      "learning_rate": 0.0001912627986348123,
      "loss": 0.3575,
      "step": 197
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.09056629240512848,
      "learning_rate": 0.00019121729237770195,
      "loss": 0.4205,
      "step": 198
    },
    {
      "epoch": 0.04522727272727273,
      "grad_norm": 0.05076136440038681,
      "learning_rate": 0.0001911717861205916,
      "loss": 0.3025,
      "step": 199
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.07090847194194794,
      "learning_rate": 0.00019112627986348125,
      "loss": 0.3474,
      "step": 200
    },
    {
      "epoch": 0.045681818181818185,
      "grad_norm": 0.07313033193349838,
      "learning_rate": 0.00019108077360637088,
      "loss": 0.3728,
      "step": 201
    },
    {
      "epoch": 0.045909090909090906,
      "grad_norm": 0.06232500821352005,
      "learning_rate": 0.00019103526734926053,
      "loss": 0.3005,
      "step": 202
    },
    {
      "epoch": 0.046136363636363635,
      "grad_norm": 0.06796764582395554,
      "learning_rate": 0.00019098976109215018,
      "loss": 0.3685,
      "step": 203
    },
    {
      "epoch": 0.046363636363636364,
      "grad_norm": 0.07944875955581665,
      "learning_rate": 0.00019094425483503984,
      "loss": 0.3986,
      "step": 204
    },
    {
      "epoch": 0.04659090909090909,
      "grad_norm": 0.10799192637205124,
      "learning_rate": 0.00019089874857792946,
      "loss": 0.3607,
      "step": 205
    },
    {
      "epoch": 0.04681818181818182,
      "grad_norm": 0.07434409111738205,
      "learning_rate": 0.00019085324232081911,
      "loss": 0.3502,
      "step": 206
    },
    {
      "epoch": 0.04704545454545454,
      "grad_norm": 0.06613147258758545,
      "learning_rate": 0.00019080773606370877,
      "loss": 0.3345,
      "step": 207
    },
    {
      "epoch": 0.04727272727272727,
      "grad_norm": 0.0661909356713295,
      "learning_rate": 0.00019076222980659842,
      "loss": 0.3591,
      "step": 208
    },
    {
      "epoch": 0.0475,
      "grad_norm": 0.05988917872309685,
      "learning_rate": 0.00019071672354948807,
      "loss": 0.3788,
      "step": 209
    },
    {
      "epoch": 0.04772727272727273,
      "grad_norm": 0.058148134499788284,
      "learning_rate": 0.00019067121729237773,
      "loss": 0.3119,
      "step": 210
    },
    {
      "epoch": 0.04795454545454545,
      "grad_norm": 0.08928387612104416,
      "learning_rate": 0.00019062571103526735,
      "loss": 0.3359,
      "step": 211
    },
    {
      "epoch": 0.04818181818181818,
      "grad_norm": 0.06751455366611481,
      "learning_rate": 0.000190580204778157,
      "loss": 0.3955,
      "step": 212
    },
    {
      "epoch": 0.04840909090909091,
      "grad_norm": 0.09613555669784546,
      "learning_rate": 0.00019053469852104666,
      "loss": 0.3414,
      "step": 213
    },
    {
      "epoch": 0.04863636363636364,
      "grad_norm": 0.06579716503620148,
      "learning_rate": 0.0001904891922639363,
      "loss": 0.3249,
      "step": 214
    },
    {
      "epoch": 0.048863636363636366,
      "grad_norm": 0.05409713089466095,
      "learning_rate": 0.00019044368600682594,
      "loss": 0.3424,
      "step": 215
    },
    {
      "epoch": 0.04909090909090909,
      "grad_norm": 0.09915835410356522,
      "learning_rate": 0.0001903981797497156,
      "loss": 0.3872,
      "step": 216
    },
    {
      "epoch": 0.04931818181818182,
      "grad_norm": 0.10592375695705414,
      "learning_rate": 0.00019035267349260524,
      "loss": 0.4094,
      "step": 217
    },
    {
      "epoch": 0.049545454545454545,
      "grad_norm": 0.0731959119439125,
      "learning_rate": 0.0001903071672354949,
      "loss": 0.286,
      "step": 218
    },
    {
      "epoch": 0.049772727272727274,
      "grad_norm": 0.07367487251758575,
      "learning_rate": 0.00019026166097838455,
      "loss": 0.3515,
      "step": 219
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.08323632180690765,
      "learning_rate": 0.0001902161547212742,
      "loss": 0.3639,
      "step": 220
    },
    {
      "epoch": 0.050227272727272725,
      "grad_norm": 0.08940815180540085,
      "learning_rate": 0.00019017064846416383,
      "loss": 0.3398,
      "step": 221
    },
    {
      "epoch": 0.05045454545454545,
      "grad_norm": 0.08002936094999313,
      "learning_rate": 0.00019012514220705348,
      "loss": 0.3739,
      "step": 222
    },
    {
      "epoch": 0.05068181818181818,
      "grad_norm": 0.07206418365240097,
      "learning_rate": 0.00019007963594994313,
      "loss": 0.3807,
      "step": 223
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.13330627977848053,
      "learning_rate": 0.00019003412969283276,
      "loss": 0.4565,
      "step": 224
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.06273927539587021,
      "learning_rate": 0.0001899886234357224,
      "loss": 0.3376,
      "step": 225
    },
    {
      "epoch": 0.05136363636363636,
      "grad_norm": 0.09348835051059723,
      "learning_rate": 0.00018994311717861206,
      "loss": 0.4026,
      "step": 226
    },
    {
      "epoch": 0.05159090909090909,
      "grad_norm": 0.06390851736068726,
      "learning_rate": 0.00018989761092150172,
      "loss": 0.3392,
      "step": 227
    },
    {
      "epoch": 0.05181818181818182,
      "grad_norm": 0.07356034219264984,
      "learning_rate": 0.00018985210466439137,
      "loss": 0.3699,
      "step": 228
    },
    {
      "epoch": 0.05204545454545455,
      "grad_norm": 0.0774492621421814,
      "learning_rate": 0.00018980659840728102,
      "loss": 0.3978,
      "step": 229
    },
    {
      "epoch": 0.05227272727272727,
      "grad_norm": 0.06514929980039597,
      "learning_rate": 0.00018976109215017067,
      "loss": 0.3735,
      "step": 230
    },
    {
      "epoch": 0.0525,
      "grad_norm": 0.05833456292748451,
      "learning_rate": 0.0001897155858930603,
      "loss": 0.2838,
      "step": 231
    },
    {
      "epoch": 0.05272727272727273,
      "grad_norm": 0.1097942441701889,
      "learning_rate": 0.00018967007963594995,
      "loss": 0.3954,
      "step": 232
    },
    {
      "epoch": 0.052954545454545456,
      "grad_norm": 0.07927477359771729,
      "learning_rate": 0.0001896245733788396,
      "loss": 0.3147,
      "step": 233
    },
    {
      "epoch": 0.053181818181818184,
      "grad_norm": 0.07589427381753922,
      "learning_rate": 0.00018957906712172923,
      "loss": 0.3345,
      "step": 234
    },
    {
      "epoch": 0.053409090909090906,
      "grad_norm": 0.07487621903419495,
      "learning_rate": 0.00018953356086461888,
      "loss": 0.4204,
      "step": 235
    },
    {
      "epoch": 0.053636363636363635,
      "grad_norm": 0.07024839520454407,
      "learning_rate": 0.00018948805460750854,
      "loss": 0.3606,
      "step": 236
    },
    {
      "epoch": 0.053863636363636364,
      "grad_norm": 0.07031041383743286,
      "learning_rate": 0.0001894425483503982,
      "loss": 0.3187,
      "step": 237
    },
    {
      "epoch": 0.05409090909090909,
      "grad_norm": 0.07347232103347778,
      "learning_rate": 0.00018939704209328784,
      "loss": 0.339,
      "step": 238
    },
    {
      "epoch": 0.05431818181818182,
      "grad_norm": 0.07726796716451645,
      "learning_rate": 0.0001893515358361775,
      "loss": 0.3158,
      "step": 239
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.05945891886949539,
      "learning_rate": 0.00018930602957906712,
      "loss": 0.314,
      "step": 240
    },
    {
      "epoch": 0.05477272727272727,
      "grad_norm": 0.06547598540782928,
      "learning_rate": 0.00018926052332195677,
      "loss": 0.3277,
      "step": 241
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.08530017733573914,
      "learning_rate": 0.00018921501706484643,
      "loss": 0.3596,
      "step": 242
    },
    {
      "epoch": 0.05522727272727273,
      "grad_norm": 0.06773647665977478,
      "learning_rate": 0.00018916951080773608,
      "loss": 0.3774,
      "step": 243
    },
    {
      "epoch": 0.05545454545454546,
      "grad_norm": 0.06789691001176834,
      "learning_rate": 0.0001891240045506257,
      "loss": 0.3368,
      "step": 244
    },
    {
      "epoch": 0.05568181818181818,
      "grad_norm": 0.06547259539365768,
      "learning_rate": 0.00018907849829351536,
      "loss": 0.3501,
      "step": 245
    },
    {
      "epoch": 0.05590909090909091,
      "grad_norm": 0.04924873262643814,
      "learning_rate": 0.000189032992036405,
      "loss": 0.3255,
      "step": 246
    },
    {
      "epoch": 0.05613636363636364,
      "grad_norm": 0.05413852632045746,
      "learning_rate": 0.00018898748577929466,
      "loss": 0.334,
      "step": 247
    },
    {
      "epoch": 0.056363636363636366,
      "grad_norm": 0.0688910186290741,
      "learning_rate": 0.00018894197952218432,
      "loss": 0.3672,
      "step": 248
    },
    {
      "epoch": 0.05659090909090909,
      "grad_norm": 0.06433402001857758,
      "learning_rate": 0.00018889647326507397,
      "loss": 0.3434,
      "step": 249
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.07170845568180084,
      "learning_rate": 0.0001888509670079636,
      "loss": 0.3281,
      "step": 250
    },
    {
      "epoch": 0.057045454545454545,
      "grad_norm": 0.07674211263656616,
      "learning_rate": 0.00018880546075085325,
      "loss": 0.3757,
      "step": 251
    },
    {
      "epoch": 0.057272727272727274,
      "grad_norm": 0.05918419733643532,
      "learning_rate": 0.0001887599544937429,
      "loss": 0.3201,
      "step": 252
    },
    {
      "epoch": 0.0575,
      "grad_norm": 0.05600019544363022,
      "learning_rate": 0.00018871444823663253,
      "loss": 0.3467,
      "step": 253
    },
    {
      "epoch": 0.057727272727272724,
      "grad_norm": 0.07337084412574768,
      "learning_rate": 0.00018866894197952218,
      "loss": 0.3655,
      "step": 254
    },
    {
      "epoch": 0.05795454545454545,
      "grad_norm": 0.07618381083011627,
      "learning_rate": 0.00018862343572241183,
      "loss": 0.3614,
      "step": 255
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.07166089117527008,
      "learning_rate": 0.00018857792946530148,
      "loss": 0.4123,
      "step": 256
    },
    {
      "epoch": 0.05840909090909091,
      "grad_norm": 0.0478210486471653,
      "learning_rate": 0.00018853242320819114,
      "loss": 0.3213,
      "step": 257
    },
    {
      "epoch": 0.05863636363636364,
      "grad_norm": 0.06371495127677917,
      "learning_rate": 0.0001884869169510808,
      "loss": 0.3523,
      "step": 258
    },
    {
      "epoch": 0.05886363636363636,
      "grad_norm": 0.05182167515158653,
      "learning_rate": 0.00018844141069397044,
      "loss": 0.3014,
      "step": 259
    },
    {
      "epoch": 0.05909090909090909,
      "grad_norm": 0.07188302278518677,
      "learning_rate": 0.00018839590443686007,
      "loss": 0.3506,
      "step": 260
    },
    {
      "epoch": 0.05931818181818182,
      "grad_norm": 0.054348792880773544,
      "learning_rate": 0.00018835039817974972,
      "loss": 0.2947,
      "step": 261
    },
    {
      "epoch": 0.05954545454545455,
      "grad_norm": 0.07728137075901031,
      "learning_rate": 0.00018830489192263937,
      "loss": 0.3889,
      "step": 262
    },
    {
      "epoch": 0.059772727272727276,
      "grad_norm": 0.06299524009227753,
      "learning_rate": 0.000188259385665529,
      "loss": 0.3264,
      "step": 263
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.07232331484556198,
      "learning_rate": 0.00018821387940841865,
      "loss": 0.3831,
      "step": 264
    },
    {
      "epoch": 0.060227272727272727,
      "grad_norm": 0.07015270739793777,
      "learning_rate": 0.0001881683731513083,
      "loss": 0.3299,
      "step": 265
    },
    {
      "epoch": 0.060454545454545455,
      "grad_norm": 0.08012431114912033,
      "learning_rate": 0.00018812286689419796,
      "loss": 0.3709,
      "step": 266
    },
    {
      "epoch": 0.060681818181818184,
      "grad_norm": 0.06760948151350021,
      "learning_rate": 0.0001880773606370876,
      "loss": 0.401,
      "step": 267
    },
    {
      "epoch": 0.060909090909090906,
      "grad_norm": 0.053751010447740555,
      "learning_rate": 0.00018803185437997726,
      "loss": 0.3406,
      "step": 268
    },
    {
      "epoch": 0.061136363636363635,
      "grad_norm": 0.08076737821102142,
      "learning_rate": 0.00018798634812286692,
      "loss": 0.4255,
      "step": 269
    },
    {
      "epoch": 0.06136363636363636,
      "grad_norm": 0.05737898871302605,
      "learning_rate": 0.00018794084186575654,
      "loss": 0.3428,
      "step": 270
    },
    {
      "epoch": 0.06159090909090909,
      "grad_norm": 0.05444619059562683,
      "learning_rate": 0.0001878953356086462,
      "loss": 0.278,
      "step": 271
    },
    {
      "epoch": 0.06181818181818182,
      "grad_norm": 0.060610074549913406,
      "learning_rate": 0.00018784982935153585,
      "loss": 0.3621,
      "step": 272
    },
    {
      "epoch": 0.06204545454545454,
      "grad_norm": 0.07014616578817368,
      "learning_rate": 0.00018780432309442547,
      "loss": 0.3702,
      "step": 273
    },
    {
      "epoch": 0.06227272727272727,
      "grad_norm": 0.07291950285434723,
      "learning_rate": 0.00018775881683731513,
      "loss": 0.3897,
      "step": 274
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.058092810213565826,
      "learning_rate": 0.00018771331058020478,
      "loss": 0.3159,
      "step": 275
    },
    {
      "epoch": 0.06272727272727273,
      "grad_norm": 0.07552125304937363,
      "learning_rate": 0.00018766780432309443,
      "loss": 0.3469,
      "step": 276
    },
    {
      "epoch": 0.06295454545454546,
      "grad_norm": 0.0734025090932846,
      "learning_rate": 0.00018762229806598409,
      "loss": 0.4011,
      "step": 277
    },
    {
      "epoch": 0.06318181818181819,
      "grad_norm": 0.04623411223292351,
      "learning_rate": 0.00018757679180887374,
      "loss": 0.3115,
      "step": 278
    },
    {
      "epoch": 0.06340909090909091,
      "grad_norm": 0.0663570910692215,
      "learning_rate": 0.0001875312855517634,
      "loss": 0.3795,
      "step": 279
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.06169087067246437,
      "learning_rate": 0.00018748577929465302,
      "loss": 0.3538,
      "step": 280
    },
    {
      "epoch": 0.06386363636363636,
      "grad_norm": 0.06088394299149513,
      "learning_rate": 0.00018744027303754267,
      "loss": 0.3573,
      "step": 281
    },
    {
      "epoch": 0.06409090909090909,
      "grad_norm": 0.092741958796978,
      "learning_rate": 0.00018739476678043232,
      "loss": 0.4223,
      "step": 282
    },
    {
      "epoch": 0.06431818181818182,
      "grad_norm": 0.060233693569898605,
      "learning_rate": 0.00018734926052332195,
      "loss": 0.3739,
      "step": 283
    },
    {
      "epoch": 0.06454545454545454,
      "grad_norm": 0.05630584806203842,
      "learning_rate": 0.0001873037542662116,
      "loss": 0.3463,
      "step": 284
    },
    {
      "epoch": 0.06477272727272727,
      "grad_norm": 0.06827583909034729,
      "learning_rate": 0.00018725824800910125,
      "loss": 0.3702,
      "step": 285
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.05584290623664856,
      "learning_rate": 0.0001872127417519909,
      "loss": 0.3192,
      "step": 286
    },
    {
      "epoch": 0.06522727272727273,
      "grad_norm": 0.05505920201539993,
      "learning_rate": 0.00018716723549488056,
      "loss": 0.3529,
      "step": 287
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.07370654493570328,
      "learning_rate": 0.0001871217292377702,
      "loss": 0.4188,
      "step": 288
    },
    {
      "epoch": 0.06568181818181819,
      "grad_norm": 0.053763944655656815,
      "learning_rate": 0.00018707622298065987,
      "loss": 0.2857,
      "step": 289
    },
    {
      "epoch": 0.0659090909090909,
      "grad_norm": 0.06459903717041016,
      "learning_rate": 0.0001870307167235495,
      "loss": 0.3605,
      "step": 290
    },
    {
      "epoch": 0.06613636363636363,
      "grad_norm": 0.05214694142341614,
      "learning_rate": 0.00018698521046643914,
      "loss": 0.3222,
      "step": 291
    },
    {
      "epoch": 0.06636363636363636,
      "grad_norm": 0.05858476832509041,
      "learning_rate": 0.00018693970420932877,
      "loss": 0.3944,
      "step": 292
    },
    {
      "epoch": 0.06659090909090909,
      "grad_norm": 0.0851559266448021,
      "learning_rate": 0.00018689419795221842,
      "loss": 0.4078,
      "step": 293
    },
    {
      "epoch": 0.06681818181818182,
      "grad_norm": 0.04250055551528931,
      "learning_rate": 0.00018684869169510808,
      "loss": 0.2752,
      "step": 294
    },
    {
      "epoch": 0.06704545454545455,
      "grad_norm": 0.046099189668893814,
      "learning_rate": 0.00018680318543799773,
      "loss": 0.3066,
      "step": 295
    },
    {
      "epoch": 0.06727272727272728,
      "grad_norm": 0.050804253667593,
      "learning_rate": 0.00018675767918088738,
      "loss": 0.3323,
      "step": 296
    },
    {
      "epoch": 0.0675,
      "grad_norm": 0.06865093111991882,
      "learning_rate": 0.00018671217292377703,
      "loss": 0.3798,
      "step": 297
    },
    {
      "epoch": 0.06772727272727273,
      "grad_norm": 0.06383540481328964,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.3311,
      "step": 298
    },
    {
      "epoch": 0.06795454545454545,
      "grad_norm": 0.0669681578874588,
      "learning_rate": 0.00018662116040955634,
      "loss": 0.3692,
      "step": 299
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.0555742010474205,
      "learning_rate": 0.00018657565415244596,
      "loss": 0.367,
      "step": 300
    },
    {
      "epoch": 0.0684090909090909,
      "grad_norm": 0.055344875901937485,
      "learning_rate": 0.00018653014789533562,
      "loss": 0.3586,
      "step": 301
    },
    {
      "epoch": 0.06863636363636363,
      "grad_norm": 0.04845462366938591,
      "learning_rate": 0.00018648464163822524,
      "loss": 0.302,
      "step": 302
    },
    {
      "epoch": 0.06886363636363636,
      "grad_norm": 0.06682348251342773,
      "learning_rate": 0.0001864391353811149,
      "loss": 0.3569,
      "step": 303
    },
    {
      "epoch": 0.06909090909090909,
      "grad_norm": 0.08912056684494019,
      "learning_rate": 0.00018639362912400455,
      "loss": 0.414,
      "step": 304
    },
    {
      "epoch": 0.06931818181818182,
      "grad_norm": 0.05532374233007431,
      "learning_rate": 0.0001863481228668942,
      "loss": 0.3409,
      "step": 305
    },
    {
      "epoch": 0.06954545454545455,
      "grad_norm": 0.06503074616193771,
      "learning_rate": 0.00018630261660978385,
      "loss": 0.3309,
      "step": 306
    },
    {
      "epoch": 0.06977272727272728,
      "grad_norm": 0.0796876847743988,
      "learning_rate": 0.0001862571103526735,
      "loss": 0.2821,
      "step": 307
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0693877637386322,
      "learning_rate": 0.00018621160409556316,
      "loss": 0.3645,
      "step": 308
    },
    {
      "epoch": 0.07022727272727272,
      "grad_norm": 0.059625763446092606,
      "learning_rate": 0.0001861660978384528,
      "loss": 0.3328,
      "step": 309
    },
    {
      "epoch": 0.07045454545454545,
      "grad_norm": 0.05884481593966484,
      "learning_rate": 0.00018612059158134244,
      "loss": 0.3907,
      "step": 310
    },
    {
      "epoch": 0.07068181818181818,
      "grad_norm": 0.08517453074455261,
      "learning_rate": 0.0001860750853242321,
      "loss": 0.3878,
      "step": 311
    },
    {
      "epoch": 0.07090909090909091,
      "grad_norm": 0.05835610255599022,
      "learning_rate": 0.00018602957906712172,
      "loss": 0.3323,
      "step": 312
    },
    {
      "epoch": 0.07113636363636364,
      "grad_norm": 0.08097538352012634,
      "learning_rate": 0.00018598407281001137,
      "loss": 0.3383,
      "step": 313
    },
    {
      "epoch": 0.07136363636363637,
      "grad_norm": 0.05530744045972824,
      "learning_rate": 0.00018593856655290102,
      "loss": 0.4062,
      "step": 314
    },
    {
      "epoch": 0.0715909090909091,
      "grad_norm": 0.06257277727127075,
      "learning_rate": 0.00018589306029579068,
      "loss": 0.325,
      "step": 315
    },
    {
      "epoch": 0.07181818181818182,
      "grad_norm": 0.05506892874836922,
      "learning_rate": 0.00018584755403868033,
      "loss": 0.2964,
      "step": 316
    },
    {
      "epoch": 0.07204545454545455,
      "grad_norm": 0.07869525253772736,
      "learning_rate": 0.00018580204778156998,
      "loss": 0.3929,
      "step": 317
    },
    {
      "epoch": 0.07227272727272727,
      "grad_norm": 0.06124209240078926,
      "learning_rate": 0.00018575654152445963,
      "loss": 0.3544,
      "step": 318
    },
    {
      "epoch": 0.0725,
      "grad_norm": 0.06974968314170837,
      "learning_rate": 0.0001857110352673493,
      "loss": 0.3686,
      "step": 319
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.06938334554433823,
      "learning_rate": 0.0001856655290102389,
      "loss": 0.3651,
      "step": 320
    },
    {
      "epoch": 0.07295454545454545,
      "grad_norm": 0.05917512997984886,
      "learning_rate": 0.00018562002275312857,
      "loss": 0.332,
      "step": 321
    },
    {
      "epoch": 0.07318181818181818,
      "grad_norm": 0.06540453433990479,
      "learning_rate": 0.0001855745164960182,
      "loss": 0.3337,
      "step": 322
    },
    {
      "epoch": 0.07340909090909091,
      "grad_norm": 0.06685882806777954,
      "learning_rate": 0.00018552901023890784,
      "loss": 0.3388,
      "step": 323
    },
    {
      "epoch": 0.07363636363636364,
      "grad_norm": 0.0832434892654419,
      "learning_rate": 0.0001854835039817975,
      "loss": 0.3856,
      "step": 324
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.051416993141174316,
      "learning_rate": 0.00018543799772468715,
      "loss": 0.315,
      "step": 325
    },
    {
      "epoch": 0.0740909090909091,
      "grad_norm": 0.07864391058683395,
      "learning_rate": 0.0001853924914675768,
      "loss": 0.3558,
      "step": 326
    },
    {
      "epoch": 0.07431818181818182,
      "grad_norm": 0.08651109784841537,
      "learning_rate": 0.00018534698521046646,
      "loss": 0.3258,
      "step": 327
    },
    {
      "epoch": 0.07454545454545454,
      "grad_norm": 0.04942311719059944,
      "learning_rate": 0.0001853014789533561,
      "loss": 0.2858,
      "step": 328
    },
    {
      "epoch": 0.07477272727272727,
      "grad_norm": 0.055968571454286575,
      "learning_rate": 0.00018525597269624576,
      "loss": 0.3201,
      "step": 329
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.08349650353193283,
      "learning_rate": 0.0001852104664391354,
      "loss": 0.4091,
      "step": 330
    },
    {
      "epoch": 0.07522727272727273,
      "grad_norm": 0.06857743114233017,
      "learning_rate": 0.00018516496018202504,
      "loss": 0.3947,
      "step": 331
    },
    {
      "epoch": 0.07545454545454545,
      "grad_norm": 0.0612059123814106,
      "learning_rate": 0.00018511945392491467,
      "loss": 0.3687,
      "step": 332
    },
    {
      "epoch": 0.07568181818181818,
      "grad_norm": 0.07810255140066147,
      "learning_rate": 0.00018507394766780432,
      "loss": 0.3853,
      "step": 333
    },
    {
      "epoch": 0.07590909090909091,
      "grad_norm": 0.06273738294839859,
      "learning_rate": 0.00018502844141069397,
      "loss": 0.3121,
      "step": 334
    },
    {
      "epoch": 0.07613636363636364,
      "grad_norm": 0.08236689120531082,
      "learning_rate": 0.00018498293515358362,
      "loss": 0.3864,
      "step": 335
    },
    {
      "epoch": 0.07636363636363637,
      "grad_norm": 0.062422532588243484,
      "learning_rate": 0.00018493742889647328,
      "loss": 0.3374,
      "step": 336
    },
    {
      "epoch": 0.07659090909090908,
      "grad_norm": 0.06648407131433487,
      "learning_rate": 0.00018489192263936293,
      "loss": 0.3589,
      "step": 337
    },
    {
      "epoch": 0.07681818181818181,
      "grad_norm": 0.06637191027402878,
      "learning_rate": 0.00018484641638225258,
      "loss": 0.338,
      "step": 338
    },
    {
      "epoch": 0.07704545454545454,
      "grad_norm": 0.06902554631233215,
      "learning_rate": 0.00018480091012514224,
      "loss": 0.3829,
      "step": 339
    },
    {
      "epoch": 0.07727272727272727,
      "grad_norm": 0.060696665197610855,
      "learning_rate": 0.00018475540386803186,
      "loss": 0.3305,
      "step": 340
    },
    {
      "epoch": 0.0775,
      "grad_norm": 0.07552158832550049,
      "learning_rate": 0.00018470989761092151,
      "loss": 0.3902,
      "step": 341
    },
    {
      "epoch": 0.07772727272727273,
      "grad_norm": 0.06903132796287537,
      "learning_rate": 0.00018466439135381114,
      "loss": 0.3211,
      "step": 342
    },
    {
      "epoch": 0.07795454545454546,
      "grad_norm": 0.0616786815226078,
      "learning_rate": 0.0001846188850967008,
      "loss": 0.3352,
      "step": 343
    },
    {
      "epoch": 0.07818181818181819,
      "grad_norm": 0.06718768924474716,
      "learning_rate": 0.00018457337883959045,
      "loss": 0.3956,
      "step": 344
    },
    {
      "epoch": 0.07840909090909091,
      "grad_norm": 0.06059100478887558,
      "learning_rate": 0.0001845278725824801,
      "loss": 0.3062,
      "step": 345
    },
    {
      "epoch": 0.07863636363636364,
      "grad_norm": 0.06861752271652222,
      "learning_rate": 0.00018448236632536975,
      "loss": 0.2952,
      "step": 346
    },
    {
      "epoch": 0.07886363636363636,
      "grad_norm": 0.06726622581481934,
      "learning_rate": 0.0001844368600682594,
      "loss": 0.363,
      "step": 347
    },
    {
      "epoch": 0.07909090909090909,
      "grad_norm": 0.06782399117946625,
      "learning_rate": 0.00018439135381114906,
      "loss": 0.3341,
      "step": 348
    },
    {
      "epoch": 0.07931818181818182,
      "grad_norm": 0.0578412227332592,
      "learning_rate": 0.0001843458475540387,
      "loss": 0.379,
      "step": 349
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.0564991794526577,
      "learning_rate": 0.00018430034129692833,
      "loss": 0.3696,
      "step": 350
    },
    {
      "epoch": 0.07977272727272727,
      "grad_norm": 0.046386800706386566,
      "learning_rate": 0.000184254835039818,
      "loss": 0.2714,
      "step": 351
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.0618738979101181,
      "learning_rate": 0.0001842093287827076,
      "loss": 0.3893,
      "step": 352
    },
    {
      "epoch": 0.08022727272727273,
      "grad_norm": 0.05505363270640373,
      "learning_rate": 0.00018416382252559727,
      "loss": 0.3103,
      "step": 353
    },
    {
      "epoch": 0.08045454545454546,
      "grad_norm": 0.05215683579444885,
      "learning_rate": 0.00018411831626848692,
      "loss": 0.2694,
      "step": 354
    },
    {
      "epoch": 0.08068181818181819,
      "grad_norm": 0.046058837324380875,
      "learning_rate": 0.00018407281001137657,
      "loss": 0.2948,
      "step": 355
    },
    {
      "epoch": 0.0809090909090909,
      "grad_norm": 0.058698009699583054,
      "learning_rate": 0.00018402730375426622,
      "loss": 0.3375,
      "step": 356
    },
    {
      "epoch": 0.08113636363636363,
      "grad_norm": 0.07408032566308975,
      "learning_rate": 0.00018398179749715588,
      "loss": 0.398,
      "step": 357
    },
    {
      "epoch": 0.08136363636363636,
      "grad_norm": 0.05237920582294464,
      "learning_rate": 0.00018393629124004553,
      "loss": 0.3738,
      "step": 358
    },
    {
      "epoch": 0.08159090909090909,
      "grad_norm": 0.07140684872865677,
      "learning_rate": 0.00018389078498293518,
      "loss": 0.3639,
      "step": 359
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.07529234886169434,
      "learning_rate": 0.0001838452787258248,
      "loss": 0.3882,
      "step": 360
    },
    {
      "epoch": 0.08204545454545455,
      "grad_norm": 0.05777087062597275,
      "learning_rate": 0.00018379977246871446,
      "loss": 0.3337,
      "step": 361
    },
    {
      "epoch": 0.08227272727272728,
      "grad_norm": 0.056848667562007904,
      "learning_rate": 0.0001837542662116041,
      "loss": 0.3146,
      "step": 362
    },
    {
      "epoch": 0.0825,
      "grad_norm": 0.05532043054699898,
      "learning_rate": 0.00018370875995449374,
      "loss": 0.2977,
      "step": 363
    },
    {
      "epoch": 0.08272727272727273,
      "grad_norm": 0.058608148247003555,
      "learning_rate": 0.0001836632536973834,
      "loss": 0.3269,
      "step": 364
    },
    {
      "epoch": 0.08295454545454546,
      "grad_norm": 0.052739087492227554,
      "learning_rate": 0.00018361774744027305,
      "loss": 0.2944,
      "step": 365
    },
    {
      "epoch": 0.08318181818181818,
      "grad_norm": 0.0788414403796196,
      "learning_rate": 0.0001835722411831627,
      "loss": 0.4392,
      "step": 366
    },
    {
      "epoch": 0.0834090909090909,
      "grad_norm": 0.05678698793053627,
      "learning_rate": 0.00018352673492605235,
      "loss": 0.3249,
      "step": 367
    },
    {
      "epoch": 0.08363636363636363,
      "grad_norm": 0.06088080257177353,
      "learning_rate": 0.000183481228668942,
      "loss": 0.293,
      "step": 368
    },
    {
      "epoch": 0.08386363636363636,
      "grad_norm": 0.08711999654769897,
      "learning_rate": 0.00018343572241183166,
      "loss": 0.3863,
      "step": 369
    },
    {
      "epoch": 0.08409090909090909,
      "grad_norm": 0.0680343434214592,
      "learning_rate": 0.00018339021615472128,
      "loss": 0.3746,
      "step": 370
    },
    {
      "epoch": 0.08431818181818182,
      "grad_norm": 0.07078652828931808,
      "learning_rate": 0.00018334470989761094,
      "loss": 0.3569,
      "step": 371
    },
    {
      "epoch": 0.08454545454545455,
      "grad_norm": 0.0856291800737381,
      "learning_rate": 0.00018329920364050056,
      "loss": 0.345,
      "step": 372
    },
    {
      "epoch": 0.08477272727272728,
      "grad_norm": 0.07380834966897964,
      "learning_rate": 0.00018325369738339021,
      "loss": 0.3451,
      "step": 373
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.06743551790714264,
      "learning_rate": 0.00018320819112627987,
      "loss": 0.3993,
      "step": 374
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.056159988045692444,
      "learning_rate": 0.00018316268486916952,
      "loss": 0.2845,
      "step": 375
    },
    {
      "epoch": 0.08545454545454545,
      "grad_norm": 0.08542246371507645,
      "learning_rate": 0.00018311717861205917,
      "loss": 0.3275,
      "step": 376
    },
    {
      "epoch": 0.08568181818181818,
      "grad_norm": 0.06735829263925552,
      "learning_rate": 0.00018307167235494883,
      "loss": 0.3531,
      "step": 377
    },
    {
      "epoch": 0.08590909090909091,
      "grad_norm": 0.05633186176419258,
      "learning_rate": 0.00018302616609783848,
      "loss": 0.339,
      "step": 378
    },
    {
      "epoch": 0.08613636363636364,
      "grad_norm": 0.05816899240016937,
      "learning_rate": 0.00018298065984072813,
      "loss": 0.3111,
      "step": 379
    },
    {
      "epoch": 0.08636363636363636,
      "grad_norm": 0.048128850758075714,
      "learning_rate": 0.00018293515358361776,
      "loss": 0.24,
      "step": 380
    },
    {
      "epoch": 0.0865909090909091,
      "grad_norm": 0.06038704141974449,
      "learning_rate": 0.0001828896473265074,
      "loss": 0.3527,
      "step": 381
    },
    {
      "epoch": 0.08681818181818182,
      "grad_norm": 0.052794571965932846,
      "learning_rate": 0.00018284414106939704,
      "loss": 0.3634,
      "step": 382
    },
    {
      "epoch": 0.08704545454545455,
      "grad_norm": 0.060711950063705444,
      "learning_rate": 0.0001827986348122867,
      "loss": 0.3292,
      "step": 383
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.05335060507059097,
      "learning_rate": 0.00018275312855517634,
      "loss": 0.3584,
      "step": 384
    },
    {
      "epoch": 0.0875,
      "grad_norm": 0.04780971258878708,
      "learning_rate": 0.000182707622298066,
      "loss": 0.3148,
      "step": 385
    },
    {
      "epoch": 0.08772727272727272,
      "grad_norm": 0.0527801550924778,
      "learning_rate": 0.00018266211604095565,
      "loss": 0.2899,
      "step": 386
    },
    {
      "epoch": 0.08795454545454545,
      "grad_norm": 0.09596309810876846,
      "learning_rate": 0.0001826166097838453,
      "loss": 0.3405,
      "step": 387
    },
    {
      "epoch": 0.08818181818181818,
      "grad_norm": 0.06126606836915016,
      "learning_rate": 0.00018257110352673495,
      "loss": 0.3282,
      "step": 388
    },
    {
      "epoch": 0.08840909090909091,
      "grad_norm": 0.06030010059475899,
      "learning_rate": 0.0001825255972696246,
      "loss": 0.3571,
      "step": 389
    },
    {
      "epoch": 0.08863636363636364,
      "grad_norm": 0.04561427980661392,
      "learning_rate": 0.00018248009101251423,
      "loss": 0.229,
      "step": 390
    },
    {
      "epoch": 0.08886363636363637,
      "grad_norm": 0.06949269771575928,
      "learning_rate": 0.00018243458475540386,
      "loss": 0.3245,
      "step": 391
    },
    {
      "epoch": 0.0890909090909091,
      "grad_norm": 0.06287781894207001,
      "learning_rate": 0.0001823890784982935,
      "loss": 0.3879,
      "step": 392
    },
    {
      "epoch": 0.08931818181818182,
      "grad_norm": 0.04313502088189125,
      "learning_rate": 0.00018234357224118316,
      "loss": 0.2746,
      "step": 393
    },
    {
      "epoch": 0.08954545454545454,
      "grad_norm": 0.0717238262295723,
      "learning_rate": 0.00018229806598407282,
      "loss": 0.3631,
      "step": 394
    },
    {
      "epoch": 0.08977272727272727,
      "grad_norm": 0.051494549959897995,
      "learning_rate": 0.00018225255972696247,
      "loss": 0.2567,
      "step": 395
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.06075765937566757,
      "learning_rate": 0.00018220705346985212,
      "loss": 0.3633,
      "step": 396
    },
    {
      "epoch": 0.09022727272727273,
      "grad_norm": 0.0486692450940609,
      "learning_rate": 0.00018216154721274177,
      "loss": 0.2568,
      "step": 397
    },
    {
      "epoch": 0.09045454545454545,
      "grad_norm": 0.0656806081533432,
      "learning_rate": 0.00018211604095563143,
      "loss": 0.4147,
      "step": 398
    },
    {
      "epoch": 0.09068181818181818,
      "grad_norm": 0.052690502256155014,
      "learning_rate": 0.00018207053469852105,
      "loss": 0.3251,
      "step": 399
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.06300299614667892,
      "learning_rate": 0.0001820250284414107,
      "loss": 0.3772,
      "step": 400
    },
    {
      "epoch": 0.09113636363636364,
      "grad_norm": 0.057599108666181564,
      "learning_rate": 0.00018197952218430033,
      "loss": 0.3206,
      "step": 401
    },
    {
      "epoch": 0.09136363636363637,
      "grad_norm": 0.06868494302034378,
      "learning_rate": 0.00018193401592718998,
      "loss": 0.3503,
      "step": 402
    },
    {
      "epoch": 0.0915909090909091,
      "grad_norm": 0.06268376857042313,
      "learning_rate": 0.00018188850967007964,
      "loss": 0.3747,
      "step": 403
    },
    {
      "epoch": 0.09181818181818181,
      "grad_norm": 0.04282078891992569,
      "learning_rate": 0.0001818430034129693,
      "loss": 0.3038,
      "step": 404
    },
    {
      "epoch": 0.09204545454545454,
      "grad_norm": 0.06801824271678925,
      "learning_rate": 0.00018179749715585894,
      "loss": 0.3395,
      "step": 405
    },
    {
      "epoch": 0.09227272727272727,
      "grad_norm": 0.078680120408535,
      "learning_rate": 0.0001817519908987486,
      "loss": 0.397,
      "step": 406
    },
    {
      "epoch": 0.0925,
      "grad_norm": 0.05033671110868454,
      "learning_rate": 0.00018170648464163825,
      "loss": 0.2905,
      "step": 407
    },
    {
      "epoch": 0.09272727272727273,
      "grad_norm": 0.06054610386490822,
      "learning_rate": 0.0001816609783845279,
      "loss": 0.357,
      "step": 408
    },
    {
      "epoch": 0.09295454545454546,
      "grad_norm": 0.0643935278058052,
      "learning_rate": 0.00018161547212741753,
      "loss": 0.3762,
      "step": 409
    },
    {
      "epoch": 0.09318181818181819,
      "grad_norm": 0.07586859911680222,
      "learning_rate": 0.00018156996587030718,
      "loss": 0.3823,
      "step": 410
    },
    {
      "epoch": 0.09340909090909091,
      "grad_norm": 0.06980318576097488,
      "learning_rate": 0.0001815244596131968,
      "loss": 0.4151,
      "step": 411
    },
    {
      "epoch": 0.09363636363636364,
      "grad_norm": 0.0405484139919281,
      "learning_rate": 0.00018147895335608646,
      "loss": 0.2804,
      "step": 412
    },
    {
      "epoch": 0.09386363636363636,
      "grad_norm": 0.07266800850629807,
      "learning_rate": 0.0001814334470989761,
      "loss": 0.3199,
      "step": 413
    },
    {
      "epoch": 0.09409090909090909,
      "grad_norm": 0.0806446447968483,
      "learning_rate": 0.00018138794084186576,
      "loss": 0.3397,
      "step": 414
    },
    {
      "epoch": 0.09431818181818181,
      "grad_norm": 0.06680378317832947,
      "learning_rate": 0.00018134243458475542,
      "loss": 0.3675,
      "step": 415
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.061083752661943436,
      "learning_rate": 0.00018129692832764507,
      "loss": 0.2861,
      "step": 416
    },
    {
      "epoch": 0.09477272727272727,
      "grad_norm": 0.05873000621795654,
      "learning_rate": 0.00018125142207053472,
      "loss": 0.2854,
      "step": 417
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06608127057552338,
      "learning_rate": 0.00018120591581342437,
      "loss": 0.3132,
      "step": 418
    },
    {
      "epoch": 0.09522727272727273,
      "grad_norm": 0.04730897769331932,
      "learning_rate": 0.000181160409556314,
      "loss": 0.265,
      "step": 419
    },
    {
      "epoch": 0.09545454545454546,
      "grad_norm": 0.049975182861089706,
      "learning_rate": 0.00018111490329920365,
      "loss": 0.3346,
      "step": 420
    },
    {
      "epoch": 0.09568181818181819,
      "grad_norm": 0.07165242731571198,
      "learning_rate": 0.00018106939704209328,
      "loss": 0.4048,
      "step": 421
    },
    {
      "epoch": 0.0959090909090909,
      "grad_norm": 0.07326973229646683,
      "learning_rate": 0.00018102389078498293,
      "loss": 0.3654,
      "step": 422
    },
    {
      "epoch": 0.09613636363636363,
      "grad_norm": 0.06667455285787582,
      "learning_rate": 0.00018097838452787258,
      "loss": 0.3521,
      "step": 423
    },
    {
      "epoch": 0.09636363636363636,
      "grad_norm": 0.05721607804298401,
      "learning_rate": 0.00018093287827076224,
      "loss": 0.2813,
      "step": 424
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.0629175677895546,
      "learning_rate": 0.0001808873720136519,
      "loss": 0.3173,
      "step": 425
    },
    {
      "epoch": 0.09681818181818182,
      "grad_norm": 0.06171676516532898,
      "learning_rate": 0.00018084186575654154,
      "loss": 0.3801,
      "step": 426
    },
    {
      "epoch": 0.09704545454545455,
      "grad_norm": 0.12320716679096222,
      "learning_rate": 0.0001807963594994312,
      "loss": 0.3564,
      "step": 427
    },
    {
      "epoch": 0.09727272727272727,
      "grad_norm": 0.053558140993118286,
      "learning_rate": 0.00018075085324232082,
      "loss": 0.2756,
      "step": 428
    },
    {
      "epoch": 0.0975,
      "grad_norm": 0.05238359421491623,
      "learning_rate": 0.00018070534698521047,
      "loss": 0.2894,
      "step": 429
    },
    {
      "epoch": 0.09772727272727273,
      "grad_norm": 0.059061694890260696,
      "learning_rate": 0.00018065984072810013,
      "loss": 0.3249,
      "step": 430
    },
    {
      "epoch": 0.09795454545454546,
      "grad_norm": 0.049198050051927567,
      "learning_rate": 0.00018061433447098975,
      "loss": 0.3327,
      "step": 431
    },
    {
      "epoch": 0.09818181818181818,
      "grad_norm": 0.05717119574546814,
      "learning_rate": 0.0001805688282138794,
      "loss": 0.3058,
      "step": 432
    },
    {
      "epoch": 0.0984090909090909,
      "grad_norm": 0.0758695900440216,
      "learning_rate": 0.00018052332195676906,
      "loss": 0.3616,
      "step": 433
    },
    {
      "epoch": 0.09863636363636363,
      "grad_norm": 0.0578886978328228,
      "learning_rate": 0.0001804778156996587,
      "loss": 0.3029,
      "step": 434
    },
    {
      "epoch": 0.09886363636363636,
      "grad_norm": 0.058128636330366135,
      "learning_rate": 0.00018043230944254836,
      "loss": 0.3465,
      "step": 435
    },
    {
      "epoch": 0.09909090909090909,
      "grad_norm": 0.05953416973352432,
      "learning_rate": 0.00018038680318543802,
      "loss": 0.3193,
      "step": 436
    },
    {
      "epoch": 0.09931818181818182,
      "grad_norm": 0.05346693843603134,
      "learning_rate": 0.00018034129692832767,
      "loss": 0.3213,
      "step": 437
    },
    {
      "epoch": 0.09954545454545455,
      "grad_norm": 0.0625639408826828,
      "learning_rate": 0.0001802957906712173,
      "loss": 0.355,
      "step": 438
    },
    {
      "epoch": 0.09977272727272728,
      "grad_norm": 0.07038534432649612,
      "learning_rate": 0.00018025028441410695,
      "loss": 0.4012,
      "step": 439
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.04572946950793266,
      "learning_rate": 0.0001802047781569966,
      "loss": 0.323,
      "step": 440
    },
    {
      "epoch": 0.10022727272727272,
      "grad_norm": 0.048158396035432816,
      "learning_rate": 0.00018015927189988623,
      "loss": 0.2737,
      "step": 441
    },
    {
      "epoch": 0.10045454545454545,
      "grad_norm": 0.08178388327360153,
      "learning_rate": 0.00018011376564277588,
      "loss": 0.394,
      "step": 442
    },
    {
      "epoch": 0.10068181818181818,
      "grad_norm": 0.0745425894856453,
      "learning_rate": 0.00018006825938566553,
      "loss": 0.3082,
      "step": 443
    },
    {
      "epoch": 0.1009090909090909,
      "grad_norm": 0.05166897550225258,
      "learning_rate": 0.00018002275312855518,
      "loss": 0.317,
      "step": 444
    },
    {
      "epoch": 0.10113636363636364,
      "grad_norm": 0.048347704112529755,
      "learning_rate": 0.00017997724687144484,
      "loss": 0.2555,
      "step": 445
    },
    {
      "epoch": 0.10136363636363636,
      "grad_norm": 0.05635770410299301,
      "learning_rate": 0.0001799317406143345,
      "loss": 0.3225,
      "step": 446
    },
    {
      "epoch": 0.10159090909090909,
      "grad_norm": 0.054129380732774734,
      "learning_rate": 0.00017988623435722414,
      "loss": 0.2825,
      "step": 447
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.06278543174266815,
      "learning_rate": 0.00017984072810011377,
      "loss": 0.3289,
      "step": 448
    },
    {
      "epoch": 0.10204545454545455,
      "grad_norm": 0.06340903043746948,
      "learning_rate": 0.00017979522184300342,
      "loss": 0.3257,
      "step": 449
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.06426140666007996,
      "learning_rate": 0.00017974971558589307,
      "loss": 0.3419,
      "step": 450
    },
    {
      "epoch": 0.1025,
      "grad_norm": 0.0820436030626297,
      "learning_rate": 0.0001797042093287827,
      "loss": 0.4159,
      "step": 451
    },
    {
      "epoch": 0.10272727272727272,
      "grad_norm": 0.08225499093532562,
      "learning_rate": 0.00017965870307167235,
      "loss": 0.3733,
      "step": 452
    },
    {
      "epoch": 0.10295454545454545,
      "grad_norm": 0.05679240822792053,
      "learning_rate": 0.000179613196814562,
      "loss": 0.3125,
      "step": 453
    },
    {
      "epoch": 0.10318181818181818,
      "grad_norm": 0.0501558780670166,
      "learning_rate": 0.00017956769055745166,
      "loss": 0.3197,
      "step": 454
    },
    {
      "epoch": 0.10340909090909091,
      "grad_norm": 0.08032650500535965,
      "learning_rate": 0.0001795221843003413,
      "loss": 0.4201,
      "step": 455
    },
    {
      "epoch": 0.10363636363636364,
      "grad_norm": 0.06919601559638977,
      "learning_rate": 0.00017947667804323096,
      "loss": 0.3847,
      "step": 456
    },
    {
      "epoch": 0.10386363636363637,
      "grad_norm": 0.052369970828294754,
      "learning_rate": 0.00017943117178612062,
      "loss": 0.326,
      "step": 457
    },
    {
      "epoch": 0.1040909090909091,
      "grad_norm": 0.06595902889966965,
      "learning_rate": 0.00017938566552901024,
      "loss": 0.3013,
      "step": 458
    },
    {
      "epoch": 0.10431818181818182,
      "grad_norm": 0.06266088038682938,
      "learning_rate": 0.0001793401592718999,
      "loss": 0.3062,
      "step": 459
    },
    {
      "epoch": 0.10454545454545454,
      "grad_norm": 0.06138155609369278,
      "learning_rate": 0.00017929465301478955,
      "loss": 0.2937,
      "step": 460
    },
    {
      "epoch": 0.10477272727272727,
      "grad_norm": 0.06843145936727524,
      "learning_rate": 0.00017924914675767917,
      "loss": 0.3235,
      "step": 461
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06136907637119293,
      "learning_rate": 0.00017920364050056883,
      "loss": 0.3364,
      "step": 462
    },
    {
      "epoch": 0.10522727272727272,
      "grad_norm": 0.06373464316129684,
      "learning_rate": 0.00017915813424345848,
      "loss": 0.3148,
      "step": 463
    },
    {
      "epoch": 0.10545454545454545,
      "grad_norm": 0.05253870040178299,
      "learning_rate": 0.00017911262798634813,
      "loss": 0.3053,
      "step": 464
    },
    {
      "epoch": 0.10568181818181818,
      "grad_norm": 0.07810074836015701,
      "learning_rate": 0.00017906712172923779,
      "loss": 0.3847,
      "step": 465
    },
    {
      "epoch": 0.10590909090909091,
      "grad_norm": 0.05436268448829651,
      "learning_rate": 0.00017902161547212744,
      "loss": 0.3241,
      "step": 466
    },
    {
      "epoch": 0.10613636363636364,
      "grad_norm": 0.07814113050699234,
      "learning_rate": 0.00017897610921501706,
      "loss": 0.3431,
      "step": 467
    },
    {
      "epoch": 0.10636363636363637,
      "grad_norm": 0.08099344372749329,
      "learning_rate": 0.00017893060295790672,
      "loss": 0.3722,
      "step": 468
    },
    {
      "epoch": 0.1065909090909091,
      "grad_norm": 0.050514526665210724,
      "learning_rate": 0.00017888509670079637,
      "loss": 0.2708,
      "step": 469
    },
    {
      "epoch": 0.10681818181818181,
      "grad_norm": 0.07289981096982956,
      "learning_rate": 0.00017883959044368602,
      "loss": 0.3147,
      "step": 470
    },
    {
      "epoch": 0.10704545454545454,
      "grad_norm": 0.047178152948617935,
      "learning_rate": 0.00017879408418657565,
      "loss": 0.2909,
      "step": 471
    },
    {
      "epoch": 0.10727272727272727,
      "grad_norm": 0.05791574344038963,
      "learning_rate": 0.0001787485779294653,
      "loss": 0.2899,
      "step": 472
    },
    {
      "epoch": 0.1075,
      "grad_norm": 0.07711401581764221,
      "learning_rate": 0.00017870307167235495,
      "loss": 0.3751,
      "step": 473
    },
    {
      "epoch": 0.10772727272727273,
      "grad_norm": 0.06262601166963577,
      "learning_rate": 0.0001786575654152446,
      "loss": 0.3046,
      "step": 474
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.0683196559548378,
      "learning_rate": 0.00017861205915813426,
      "loss": 0.3753,
      "step": 475
    },
    {
      "epoch": 0.10818181818181818,
      "grad_norm": 0.0470895953476429,
      "learning_rate": 0.0001785665529010239,
      "loss": 0.2962,
      "step": 476
    },
    {
      "epoch": 0.10840909090909091,
      "grad_norm": 0.06830032914876938,
      "learning_rate": 0.00017852104664391354,
      "loss": 0.3849,
      "step": 477
    },
    {
      "epoch": 0.10863636363636364,
      "grad_norm": 0.08401838690042496,
      "learning_rate": 0.0001784755403868032,
      "loss": 0.3989,
      "step": 478
    },
    {
      "epoch": 0.10886363636363636,
      "grad_norm": 0.06860377639532089,
      "learning_rate": 0.00017843003412969284,
      "loss": 0.3503,
      "step": 479
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.06468965858221054,
      "learning_rate": 0.0001783845278725825,
      "loss": 0.3786,
      "step": 480
    },
    {
      "epoch": 0.10931818181818181,
      "grad_norm": 0.05393576994538307,
      "learning_rate": 0.00017833902161547212,
      "loss": 0.3209,
      "step": 481
    },
    {
      "epoch": 0.10954545454545454,
      "grad_norm": 0.09752725809812546,
      "learning_rate": 0.00017829351535836178,
      "loss": 0.4024,
      "step": 482
    },
    {
      "epoch": 0.10977272727272727,
      "grad_norm": 0.07555088400840759,
      "learning_rate": 0.00017824800910125143,
      "loss": 0.3703,
      "step": 483
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.06795812398195267,
      "learning_rate": 0.00017820250284414108,
      "loss": 0.3527,
      "step": 484
    },
    {
      "epoch": 0.11022727272727273,
      "grad_norm": 0.06554467976093292,
      "learning_rate": 0.00017815699658703073,
      "loss": 0.3477,
      "step": 485
    },
    {
      "epoch": 0.11045454545454546,
      "grad_norm": 0.08717542141675949,
      "learning_rate": 0.0001781114903299204,
      "loss": 0.4012,
      "step": 486
    },
    {
      "epoch": 0.11068181818181819,
      "grad_norm": 0.051257237792015076,
      "learning_rate": 0.00017806598407281,
      "loss": 0.296,
      "step": 487
    },
    {
      "epoch": 0.11090909090909092,
      "grad_norm": 0.051480937749147415,
      "learning_rate": 0.00017802047781569967,
      "loss": 0.2939,
      "step": 488
    },
    {
      "epoch": 0.11113636363636363,
      "grad_norm": 0.05471305549144745,
      "learning_rate": 0.00017797497155858932,
      "loss": 0.3398,
      "step": 489
    },
    {
      "epoch": 0.11136363636363636,
      "grad_norm": 0.04524696245789528,
      "learning_rate": 0.00017792946530147897,
      "loss": 0.308,
      "step": 490
    },
    {
      "epoch": 0.11159090909090909,
      "grad_norm": 0.061058301478624344,
      "learning_rate": 0.0001778839590443686,
      "loss": 0.3606,
      "step": 491
    },
    {
      "epoch": 0.11181818181818182,
      "grad_norm": 0.07316833734512329,
      "learning_rate": 0.00017783845278725825,
      "loss": 0.388,
      "step": 492
    },
    {
      "epoch": 0.11204545454545455,
      "grad_norm": 0.06681711226701736,
      "learning_rate": 0.0001777929465301479,
      "loss": 0.4057,
      "step": 493
    },
    {
      "epoch": 0.11227272727272727,
      "grad_norm": 0.05885514244437218,
      "learning_rate": 0.00017774744027303755,
      "loss": 0.3327,
      "step": 494
    },
    {
      "epoch": 0.1125,
      "grad_norm": 0.05804837495088577,
      "learning_rate": 0.0001777019340159272,
      "loss": 0.3077,
      "step": 495
    },
    {
      "epoch": 0.11272727272727273,
      "grad_norm": 0.05075233057141304,
      "learning_rate": 0.00017765642775881686,
      "loss": 0.3235,
      "step": 496
    },
    {
      "epoch": 0.11295454545454546,
      "grad_norm": 0.05081646516919136,
      "learning_rate": 0.00017761092150170649,
      "loss": 0.3058,
      "step": 497
    },
    {
      "epoch": 0.11318181818181818,
      "grad_norm": 0.06027887761592865,
      "learning_rate": 0.00017756541524459614,
      "loss": 0.3815,
      "step": 498
    },
    {
      "epoch": 0.1134090909090909,
      "grad_norm": 0.05868418887257576,
      "learning_rate": 0.0001775199089874858,
      "loss": 0.3701,
      "step": 499
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.0415584035217762,
      "learning_rate": 0.00017747440273037544,
      "loss": 0.237,
      "step": 500
    },
    {
      "epoch": 0.11386363636363636,
      "grad_norm": 0.05173828452825546,
      "learning_rate": 0.00017742889647326507,
      "loss": 0.2934,
      "step": 501
    },
    {
      "epoch": 0.11409090909090909,
      "grad_norm": 0.049748942255973816,
      "learning_rate": 0.00017738339021615472,
      "loss": 0.3716,
      "step": 502
    },
    {
      "epoch": 0.11431818181818182,
      "grad_norm": 0.05823475494980812,
      "learning_rate": 0.00017733788395904438,
      "loss": 0.3553,
      "step": 503
    },
    {
      "epoch": 0.11454545454545455,
      "grad_norm": 0.059952136129140854,
      "learning_rate": 0.00017729237770193403,
      "loss": 0.365,
      "step": 504
    },
    {
      "epoch": 0.11477272727272728,
      "grad_norm": 0.05510824918746948,
      "learning_rate": 0.00017724687144482368,
      "loss": 0.4059,
      "step": 505
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.05536169558763504,
      "learning_rate": 0.0001772013651877133,
      "loss": 0.355,
      "step": 506
    },
    {
      "epoch": 0.11522727272727273,
      "grad_norm": 0.051437728106975555,
      "learning_rate": 0.00017715585893060296,
      "loss": 0.3407,
      "step": 507
    },
    {
      "epoch": 0.11545454545454545,
      "grad_norm": 0.07074083387851715,
      "learning_rate": 0.0001771103526734926,
      "loss": 0.407,
      "step": 508
    },
    {
      "epoch": 0.11568181818181818,
      "grad_norm": 0.0670168474316597,
      "learning_rate": 0.00017706484641638227,
      "loss": 0.3909,
      "step": 509
    },
    {
      "epoch": 0.1159090909090909,
      "grad_norm": 0.055498141795396805,
      "learning_rate": 0.00017701934015927192,
      "loss": 0.3935,
      "step": 510
    },
    {
      "epoch": 0.11613636363636363,
      "grad_norm": 0.0520547553896904,
      "learning_rate": 0.00017697383390216154,
      "loss": 0.3002,
      "step": 511
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.04537021368741989,
      "learning_rate": 0.0001769283276450512,
      "loss": 0.2588,
      "step": 512
    },
    {
      "epoch": 0.11659090909090909,
      "grad_norm": 0.06405523419380188,
      "learning_rate": 0.00017688282138794085,
      "loss": 0.3598,
      "step": 513
    },
    {
      "epoch": 0.11681818181818182,
      "grad_norm": 0.05769503489136696,
      "learning_rate": 0.0001768373151308305,
      "loss": 0.343,
      "step": 514
    },
    {
      "epoch": 0.11704545454545455,
      "grad_norm": 0.05084606260061264,
      "learning_rate": 0.00017679180887372016,
      "loss": 0.3411,
      "step": 515
    },
    {
      "epoch": 0.11727272727272728,
      "grad_norm": 0.06423677504062653,
      "learning_rate": 0.00017674630261660978,
      "loss": 0.3608,
      "step": 516
    },
    {
      "epoch": 0.1175,
      "grad_norm": 0.04169835150241852,
      "learning_rate": 0.00017670079635949943,
      "loss": 0.2921,
      "step": 517
    },
    {
      "epoch": 0.11772727272727272,
      "grad_norm": 0.06658942997455597,
      "learning_rate": 0.0001766552901023891,
      "loss": 0.358,
      "step": 518
    },
    {
      "epoch": 0.11795454545454545,
      "grad_norm": 0.06695352494716644,
      "learning_rate": 0.00017660978384527874,
      "loss": 0.3856,
      "step": 519
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 0.055666323751211166,
      "learning_rate": 0.0001765642775881684,
      "loss": 0.3291,
      "step": 520
    },
    {
      "epoch": 0.11840909090909091,
      "grad_norm": 0.04485953599214554,
      "learning_rate": 0.00017651877133105802,
      "loss": 0.3028,
      "step": 521
    },
    {
      "epoch": 0.11863636363636364,
      "grad_norm": 0.04591428115963936,
      "learning_rate": 0.00017647326507394767,
      "loss": 0.3136,
      "step": 522
    },
    {
      "epoch": 0.11886363636363637,
      "grad_norm": 0.04449399933218956,
      "learning_rate": 0.00017642775881683732,
      "loss": 0.296,
      "step": 523
    },
    {
      "epoch": 0.1190909090909091,
      "grad_norm": 0.05059319734573364,
      "learning_rate": 0.00017638225255972698,
      "loss": 0.3216,
      "step": 524
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.04960934445261955,
      "learning_rate": 0.00017633674630261663,
      "loss": 0.3417,
      "step": 525
    },
    {
      "epoch": 0.11954545454545455,
      "grad_norm": 0.044854290783405304,
      "learning_rate": 0.00017629124004550626,
      "loss": 0.3535,
      "step": 526
    },
    {
      "epoch": 0.11977272727272727,
      "grad_norm": 0.06094077229499817,
      "learning_rate": 0.0001762457337883959,
      "loss": 0.4003,
      "step": 527
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.06369205564260483,
      "learning_rate": 0.00017620022753128556,
      "loss": 0.3326,
      "step": 528
    },
    {
      "epoch": 0.12022727272727272,
      "grad_norm": 0.06342646479606628,
      "learning_rate": 0.00017615472127417521,
      "loss": 0.3847,
      "step": 529
    },
    {
      "epoch": 0.12045454545454545,
      "grad_norm": 0.04519784823060036,
      "learning_rate": 0.00017610921501706487,
      "loss": 0.3043,
      "step": 530
    },
    {
      "epoch": 0.12068181818181818,
      "grad_norm": 0.04998958110809326,
      "learning_rate": 0.0001760637087599545,
      "loss": 0.3148,
      "step": 531
    },
    {
      "epoch": 0.12090909090909091,
      "grad_norm": 0.0710088312625885,
      "learning_rate": 0.00017601820250284415,
      "loss": 0.3542,
      "step": 532
    },
    {
      "epoch": 0.12113636363636364,
      "grad_norm": 0.05078562721610069,
      "learning_rate": 0.0001759726962457338,
      "loss": 0.3096,
      "step": 533
    },
    {
      "epoch": 0.12136363636363637,
      "grad_norm": 0.07393792271614075,
      "learning_rate": 0.00017592718998862345,
      "loss": 0.3667,
      "step": 534
    },
    {
      "epoch": 0.1215909090909091,
      "grad_norm": 0.05009531229734421,
      "learning_rate": 0.00017588168373151308,
      "loss": 0.3206,
      "step": 535
    },
    {
      "epoch": 0.12181818181818181,
      "grad_norm": 0.06499211490154266,
      "learning_rate": 0.00017583617747440273,
      "loss": 0.4169,
      "step": 536
    },
    {
      "epoch": 0.12204545454545454,
      "grad_norm": 0.06781885772943497,
      "learning_rate": 0.00017579067121729238,
      "loss": 0.3328,
      "step": 537
    },
    {
      "epoch": 0.12227272727272727,
      "grad_norm": 0.05580177903175354,
      "learning_rate": 0.00017574516496018203,
      "loss": 0.3077,
      "step": 538
    },
    {
      "epoch": 0.1225,
      "grad_norm": 0.05455482378602028,
      "learning_rate": 0.0001756996587030717,
      "loss": 0.3007,
      "step": 539
    },
    {
      "epoch": 0.12272727272727273,
      "grad_norm": 0.04011889547109604,
      "learning_rate": 0.00017565415244596134,
      "loss": 0.2736,
      "step": 540
    },
    {
      "epoch": 0.12295454545454546,
      "grad_norm": 0.07588928937911987,
      "learning_rate": 0.00017560864618885097,
      "loss": 0.372,
      "step": 541
    },
    {
      "epoch": 0.12318181818181818,
      "grad_norm": 0.06547218561172485,
      "learning_rate": 0.00017556313993174062,
      "loss": 0.3285,
      "step": 542
    },
    {
      "epoch": 0.12340909090909091,
      "grad_norm": 0.0548650361597538,
      "learning_rate": 0.00017551763367463027,
      "loss": 0.3059,
      "step": 543
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.05406401678919792,
      "learning_rate": 0.00017547212741751992,
      "loss": 0.3534,
      "step": 544
    },
    {
      "epoch": 0.12386363636363637,
      "grad_norm": 0.05892396345734596,
      "learning_rate": 0.00017542662116040955,
      "loss": 0.3402,
      "step": 545
    },
    {
      "epoch": 0.12409090909090909,
      "grad_norm": 0.07974585145711899,
      "learning_rate": 0.0001753811149032992,
      "loss": 0.3355,
      "step": 546
    },
    {
      "epoch": 0.12431818181818181,
      "grad_norm": 0.06012347340583801,
      "learning_rate": 0.00017533560864618886,
      "loss": 0.3371,
      "step": 547
    },
    {
      "epoch": 0.12454545454545454,
      "grad_norm": 0.06930217146873474,
      "learning_rate": 0.0001752901023890785,
      "loss": 0.4151,
      "step": 548
    },
    {
      "epoch": 0.12477272727272727,
      "grad_norm": 0.07039245218038559,
      "learning_rate": 0.00017524459613196816,
      "loss": 0.4137,
      "step": 549
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.05569419264793396,
      "learning_rate": 0.00017519908987485781,
      "loss": 0.3463,
      "step": 550
    },
    {
      "epoch": 0.12522727272727271,
      "grad_norm": 0.0345703586935997,
      "learning_rate": 0.00017515358361774744,
      "loss": 0.2756,
      "step": 551
    },
    {
      "epoch": 0.12545454545454546,
      "grad_norm": 0.06320793181657791,
      "learning_rate": 0.0001751080773606371,
      "loss": 0.3931,
      "step": 552
    },
    {
      "epoch": 0.12568181818181817,
      "grad_norm": 0.06752531230449677,
      "learning_rate": 0.00017506257110352675,
      "loss": 0.4149,
      "step": 553
    },
    {
      "epoch": 0.12590909090909091,
      "grad_norm": 0.0516890287399292,
      "learning_rate": 0.0001750170648464164,
      "loss": 0.3536,
      "step": 554
    },
    {
      "epoch": 0.12613636363636363,
      "grad_norm": 0.053542815148830414,
      "learning_rate": 0.00017497155858930602,
      "loss": 0.3303,
      "step": 555
    },
    {
      "epoch": 0.12636363636363637,
      "grad_norm": 0.05805669724941254,
      "learning_rate": 0.00017492605233219568,
      "loss": 0.3174,
      "step": 556
    },
    {
      "epoch": 0.1265909090909091,
      "grad_norm": 0.04319758713245392,
      "learning_rate": 0.00017488054607508533,
      "loss": 0.2885,
      "step": 557
    },
    {
      "epoch": 0.12681818181818183,
      "grad_norm": 0.06407219916582108,
      "learning_rate": 0.00017483503981797498,
      "loss": 0.3828,
      "step": 558
    },
    {
      "epoch": 0.12704545454545454,
      "grad_norm": 0.04987844079732895,
      "learning_rate": 0.00017478953356086464,
      "loss": 0.3116,
      "step": 559
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.05620274692773819,
      "learning_rate": 0.0001747440273037543,
      "loss": 0.3119,
      "step": 560
    },
    {
      "epoch": 0.1275,
      "grad_norm": 0.05813369154930115,
      "learning_rate": 0.00017469852104664391,
      "loss": 0.3185,
      "step": 561
    },
    {
      "epoch": 0.12772727272727272,
      "grad_norm": 0.06674553453922272,
      "learning_rate": 0.00017465301478953357,
      "loss": 0.3813,
      "step": 562
    },
    {
      "epoch": 0.12795454545454546,
      "grad_norm": 0.06786223500967026,
      "learning_rate": 0.00017460750853242322,
      "loss": 0.3597,
      "step": 563
    },
    {
      "epoch": 0.12818181818181817,
      "grad_norm": 0.07078289240598679,
      "learning_rate": 0.00017456200227531287,
      "loss": 0.4028,
      "step": 564
    },
    {
      "epoch": 0.12840909090909092,
      "grad_norm": 0.058574378490448,
      "learning_rate": 0.0001745164960182025,
      "loss": 0.3358,
      "step": 565
    },
    {
      "epoch": 0.12863636363636363,
      "grad_norm": 0.042616140097379684,
      "learning_rate": 0.00017447098976109215,
      "loss": 0.3262,
      "step": 566
    },
    {
      "epoch": 0.12886363636363637,
      "grad_norm": 0.05027776211500168,
      "learning_rate": 0.0001744254835039818,
      "loss": 0.3361,
      "step": 567
    },
    {
      "epoch": 0.1290909090909091,
      "grad_norm": 0.04923521727323532,
      "learning_rate": 0.00017437997724687146,
      "loss": 0.264,
      "step": 568
    },
    {
      "epoch": 0.1293181818181818,
      "grad_norm": 0.04796861484646797,
      "learning_rate": 0.0001743344709897611,
      "loss": 0.2828,
      "step": 569
    },
    {
      "epoch": 0.12954545454545455,
      "grad_norm": 0.06070738285779953,
      "learning_rate": 0.00017428896473265076,
      "loss": 0.3473,
      "step": 570
    },
    {
      "epoch": 0.12977272727272726,
      "grad_norm": 0.044811829924583435,
      "learning_rate": 0.0001742434584755404,
      "loss": 0.3188,
      "step": 571
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.04731845110654831,
      "learning_rate": 0.00017419795221843004,
      "loss": 0.2963,
      "step": 572
    },
    {
      "epoch": 0.13022727272727272,
      "grad_norm": 0.05867376923561096,
      "learning_rate": 0.0001741524459613197,
      "loss": 0.3154,
      "step": 573
    },
    {
      "epoch": 0.13045454545454546,
      "grad_norm": 0.0547201968729496,
      "learning_rate": 0.00017410693970420932,
      "loss": 0.3427,
      "step": 574
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.06114993244409561,
      "learning_rate": 0.00017406143344709897,
      "loss": 0.3629,
      "step": 575
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.0732516348361969,
      "learning_rate": 0.00017401592718998863,
      "loss": 0.3186,
      "step": 576
    },
    {
      "epoch": 0.13113636363636363,
      "grad_norm": 0.06007545441389084,
      "learning_rate": 0.00017397042093287828,
      "loss": 0.2875,
      "step": 577
    },
    {
      "epoch": 0.13136363636363638,
      "grad_norm": 0.08262346684932709,
      "learning_rate": 0.00017392491467576793,
      "loss": 0.4333,
      "step": 578
    },
    {
      "epoch": 0.1315909090909091,
      "grad_norm": 0.04957899451255798,
      "learning_rate": 0.00017387940841865758,
      "loss": 0.3214,
      "step": 579
    },
    {
      "epoch": 0.1318181818181818,
      "grad_norm": 0.08736223727464676,
      "learning_rate": 0.00017383390216154724,
      "loss": 0.4151,
      "step": 580
    },
    {
      "epoch": 0.13204545454545455,
      "grad_norm": 0.05947919934988022,
      "learning_rate": 0.00017378839590443686,
      "loss": 0.359,
      "step": 581
    },
    {
      "epoch": 0.13227272727272726,
      "grad_norm": 0.05789751186966896,
      "learning_rate": 0.00017374288964732652,
      "loss": 0.3266,
      "step": 582
    },
    {
      "epoch": 0.1325,
      "grad_norm": 0.0539686419069767,
      "learning_rate": 0.00017369738339021617,
      "loss": 0.3252,
      "step": 583
    },
    {
      "epoch": 0.13272727272727272,
      "grad_norm": 0.05595435947179794,
      "learning_rate": 0.0001736518771331058,
      "loss": 0.3676,
      "step": 584
    },
    {
      "epoch": 0.13295454545454546,
      "grad_norm": 0.06885061413049698,
      "learning_rate": 0.00017360637087599545,
      "loss": 0.4186,
      "step": 585
    },
    {
      "epoch": 0.13318181818181818,
      "grad_norm": 0.05871770903468132,
      "learning_rate": 0.0001735608646188851,
      "loss": 0.3661,
      "step": 586
    },
    {
      "epoch": 0.13340909090909092,
      "grad_norm": 0.05443551018834114,
      "learning_rate": 0.00017351535836177475,
      "loss": 0.3525,
      "step": 587
    },
    {
      "epoch": 0.13363636363636364,
      "grad_norm": 0.08042362332344055,
      "learning_rate": 0.0001734698521046644,
      "loss": 0.3448,
      "step": 588
    },
    {
      "epoch": 0.13386363636363635,
      "grad_norm": 0.06594429165124893,
      "learning_rate": 0.00017342434584755406,
      "loss": 0.3893,
      "step": 589
    },
    {
      "epoch": 0.1340909090909091,
      "grad_norm": 0.055698212236166,
      "learning_rate": 0.0001733788395904437,
      "loss": 0.2914,
      "step": 590
    },
    {
      "epoch": 0.1343181818181818,
      "grad_norm": 0.07639595121145248,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4057,
      "step": 591
    },
    {
      "epoch": 0.13454545454545455,
      "grad_norm": 0.055934395641088486,
      "learning_rate": 0.000173287827076223,
      "loss": 0.371,
      "step": 592
    },
    {
      "epoch": 0.13477272727272727,
      "grad_norm": 0.06331167370080948,
      "learning_rate": 0.00017324232081911264,
      "loss": 0.3696,
      "step": 593
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.042304303497076035,
      "learning_rate": 0.00017319681456200227,
      "loss": 0.3161,
      "step": 594
    },
    {
      "epoch": 0.13522727272727272,
      "grad_norm": 0.07049621641635895,
      "learning_rate": 0.00017315130830489192,
      "loss": 0.295,
      "step": 595
    },
    {
      "epoch": 0.13545454545454547,
      "grad_norm": 0.08519428223371506,
      "learning_rate": 0.00017310580204778157,
      "loss": 0.4534,
      "step": 596
    },
    {
      "epoch": 0.13568181818181818,
      "grad_norm": 0.06865787506103516,
      "learning_rate": 0.00017306029579067123,
      "loss": 0.4013,
      "step": 597
    },
    {
      "epoch": 0.1359090909090909,
      "grad_norm": 0.06528319418430328,
      "learning_rate": 0.00017301478953356088,
      "loss": 0.4047,
      "step": 598
    },
    {
      "epoch": 0.13613636363636364,
      "grad_norm": 0.06319870799779892,
      "learning_rate": 0.00017296928327645053,
      "loss": 0.3471,
      "step": 599
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.07740062475204468,
      "learning_rate": 0.00017292377701934018,
      "loss": 0.3875,
      "step": 600
    },
    {
      "epoch": 0.1365909090909091,
      "grad_norm": 0.052465490996837616,
      "learning_rate": 0.0001728782707622298,
      "loss": 0.3168,
      "step": 601
    },
    {
      "epoch": 0.1368181818181818,
      "grad_norm": 0.04799538850784302,
      "learning_rate": 0.00017283276450511946,
      "loss": 0.316,
      "step": 602
    },
    {
      "epoch": 0.13704545454545455,
      "grad_norm": 0.05179618299007416,
      "learning_rate": 0.0001727872582480091,
      "loss": 0.3292,
      "step": 603
    },
    {
      "epoch": 0.13727272727272727,
      "grad_norm": 0.054760292172431946,
      "learning_rate": 0.00017274175199089874,
      "loss": 0.3205,
      "step": 604
    },
    {
      "epoch": 0.1375,
      "grad_norm": 0.048465240746736526,
      "learning_rate": 0.0001726962457337884,
      "loss": 0.2937,
      "step": 605
    },
    {
      "epoch": 0.13772727272727273,
      "grad_norm": 0.05656435713171959,
      "learning_rate": 0.00017265073947667805,
      "loss": 0.3481,
      "step": 606
    },
    {
      "epoch": 0.13795454545454544,
      "grad_norm": 0.04781380295753479,
      "learning_rate": 0.0001726052332195677,
      "loss": 0.3189,
      "step": 607
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.05396062135696411,
      "learning_rate": 0.00017255972696245735,
      "loss": 0.372,
      "step": 608
    },
    {
      "epoch": 0.1384090909090909,
      "grad_norm": 0.04672205448150635,
      "learning_rate": 0.000172514220705347,
      "loss": 0.3262,
      "step": 609
    },
    {
      "epoch": 0.13863636363636364,
      "grad_norm": 0.05377371236681938,
      "learning_rate": 0.00017246871444823666,
      "loss": 0.3364,
      "step": 610
    },
    {
      "epoch": 0.13886363636363636,
      "grad_norm": 0.04824598878622055,
      "learning_rate": 0.00017242320819112628,
      "loss": 0.3313,
      "step": 611
    },
    {
      "epoch": 0.1390909090909091,
      "grad_norm": 0.06221775710582733,
      "learning_rate": 0.00017237770193401594,
      "loss": 0.3207,
      "step": 612
    },
    {
      "epoch": 0.1393181818181818,
      "grad_norm": 0.05232756957411766,
      "learning_rate": 0.00017233219567690556,
      "loss": 0.3155,
      "step": 613
    },
    {
      "epoch": 0.13954545454545456,
      "grad_norm": 0.04345572367310524,
      "learning_rate": 0.00017228668941979522,
      "loss": 0.3046,
      "step": 614
    },
    {
      "epoch": 0.13977272727272727,
      "grad_norm": 0.05474264919757843,
      "learning_rate": 0.00017224118316268487,
      "loss": 0.3279,
      "step": 615
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.05007455497980118,
      "learning_rate": 0.00017219567690557452,
      "loss": 0.2956,
      "step": 616
    },
    {
      "epoch": 0.14022727272727273,
      "grad_norm": 0.04481038078665733,
      "learning_rate": 0.00017215017064846417,
      "loss": 0.3054,
      "step": 617
    },
    {
      "epoch": 0.14045454545454544,
      "grad_norm": 0.0627032145857811,
      "learning_rate": 0.00017210466439135383,
      "loss": 0.3517,
      "step": 618
    },
    {
      "epoch": 0.14068181818181819,
      "grad_norm": 0.05472302809357643,
      "learning_rate": 0.00017205915813424348,
      "loss": 0.3415,
      "step": 619
    },
    {
      "epoch": 0.1409090909090909,
      "grad_norm": 0.05333380028605461,
      "learning_rate": 0.00017201365187713313,
      "loss": 0.3299,
      "step": 620
    },
    {
      "epoch": 0.14113636363636364,
      "grad_norm": 0.04590165987610817,
      "learning_rate": 0.00017196814562002276,
      "loss": 0.3099,
      "step": 621
    },
    {
      "epoch": 0.14136363636363636,
      "grad_norm": 0.05590417981147766,
      "learning_rate": 0.0001719226393629124,
      "loss": 0.3812,
      "step": 622
    },
    {
      "epoch": 0.1415909090909091,
      "grad_norm": 0.060943473130464554,
      "learning_rate": 0.00017187713310580204,
      "loss": 0.3655,
      "step": 623
    },
    {
      "epoch": 0.14181818181818182,
      "grad_norm": 0.04136320948600769,
      "learning_rate": 0.0001718316268486917,
      "loss": 0.3105,
      "step": 624
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 0.047183576971292496,
      "learning_rate": 0.00017178612059158134,
      "loss": 0.2806,
      "step": 625
    },
    {
      "epoch": 0.14227272727272727,
      "grad_norm": 0.05503621697425842,
      "learning_rate": 0.000171740614334471,
      "loss": 0.3407,
      "step": 626
    },
    {
      "epoch": 0.1425,
      "grad_norm": 0.06718166917562485,
      "learning_rate": 0.00017169510807736065,
      "loss": 0.4007,
      "step": 627
    },
    {
      "epoch": 0.14272727272727273,
      "grad_norm": 0.05326209217309952,
      "learning_rate": 0.0001716496018202503,
      "loss": 0.3286,
      "step": 628
    },
    {
      "epoch": 0.14295454545454545,
      "grad_norm": 0.04614725708961487,
      "learning_rate": 0.00017160409556313995,
      "loss": 0.3112,
      "step": 629
    },
    {
      "epoch": 0.1431818181818182,
      "grad_norm": 0.055525436997413635,
      "learning_rate": 0.0001715585893060296,
      "loss": 0.3291,
      "step": 630
    },
    {
      "epoch": 0.1434090909090909,
      "grad_norm": 0.06195411831140518,
      "learning_rate": 0.00017151308304891923,
      "loss": 0.4098,
      "step": 631
    },
    {
      "epoch": 0.14363636363636365,
      "grad_norm": 0.057146623730659485,
      "learning_rate": 0.00017146757679180888,
      "loss": 0.3658,
      "step": 632
    },
    {
      "epoch": 0.14386363636363636,
      "grad_norm": 0.05194452032446861,
      "learning_rate": 0.0001714220705346985,
      "loss": 0.3236,
      "step": 633
    },
    {
      "epoch": 0.1440909090909091,
      "grad_norm": 0.05472615733742714,
      "learning_rate": 0.00017137656427758816,
      "loss": 0.3632,
      "step": 634
    },
    {
      "epoch": 0.14431818181818182,
      "grad_norm": 0.05091935768723488,
      "learning_rate": 0.00017133105802047782,
      "loss": 0.3164,
      "step": 635
    },
    {
      "epoch": 0.14454545454545453,
      "grad_norm": 0.05854714661836624,
      "learning_rate": 0.00017128555176336747,
      "loss": 0.3409,
      "step": 636
    },
    {
      "epoch": 0.14477272727272728,
      "grad_norm": 0.061421096324920654,
      "learning_rate": 0.00017124004550625712,
      "loss": 0.3194,
      "step": 637
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05241991579532623,
      "learning_rate": 0.00017119453924914677,
      "loss": 0.3294,
      "step": 638
    },
    {
      "epoch": 0.14522727272727273,
      "grad_norm": 0.04869182035326958,
      "learning_rate": 0.00017114903299203643,
      "loss": 0.3265,
      "step": 639
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.04809802770614624,
      "learning_rate": 0.00017110352673492608,
      "loss": 0.2642,
      "step": 640
    },
    {
      "epoch": 0.1456818181818182,
      "grad_norm": 0.04209277406334877,
      "learning_rate": 0.0001710580204778157,
      "loss": 0.3183,
      "step": 641
    },
    {
      "epoch": 0.1459090909090909,
      "grad_norm": 0.04968772083520889,
      "learning_rate": 0.00017101251422070533,
      "loss": 0.3086,
      "step": 642
    },
    {
      "epoch": 0.14613636363636365,
      "grad_norm": 0.060724783688783646,
      "learning_rate": 0.00017096700796359498,
      "loss": 0.3542,
      "step": 643
    },
    {
      "epoch": 0.14636363636363636,
      "grad_norm": 0.05996377021074295,
      "learning_rate": 0.00017092150170648464,
      "loss": 0.3366,
      "step": 644
    },
    {
      "epoch": 0.14659090909090908,
      "grad_norm": 0.0625680461525917,
      "learning_rate": 0.0001708759954493743,
      "loss": 0.3714,
      "step": 645
    },
    {
      "epoch": 0.14681818181818182,
      "grad_norm": 0.05981293320655823,
      "learning_rate": 0.00017083048919226394,
      "loss": 0.3793,
      "step": 646
    },
    {
      "epoch": 0.14704545454545453,
      "grad_norm": 0.058062467724084854,
      "learning_rate": 0.0001707849829351536,
      "loss": 0.3125,
      "step": 647
    },
    {
      "epoch": 0.14727272727272728,
      "grad_norm": 0.050496675074100494,
      "learning_rate": 0.00017073947667804325,
      "loss": 0.3003,
      "step": 648
    },
    {
      "epoch": 0.1475,
      "grad_norm": 0.04887182265520096,
      "learning_rate": 0.0001706939704209329,
      "loss": 0.3186,
      "step": 649
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.0717572346329689,
      "learning_rate": 0.00017064846416382255,
      "loss": 0.3578,
      "step": 650
    },
    {
      "epoch": 0.14795454545454545,
      "grad_norm": 0.06272096931934357,
      "learning_rate": 0.00017060295790671218,
      "loss": 0.3711,
      "step": 651
    },
    {
      "epoch": 0.1481818181818182,
      "grad_norm": 0.06982682645320892,
      "learning_rate": 0.0001705574516496018,
      "loss": 0.3573,
      "step": 652
    },
    {
      "epoch": 0.1484090909090909,
      "grad_norm": 0.06460107862949371,
      "learning_rate": 0.00017051194539249146,
      "loss": 0.4076,
      "step": 653
    },
    {
      "epoch": 0.14863636363636365,
      "grad_norm": 0.06271602213382721,
      "learning_rate": 0.0001704664391353811,
      "loss": 0.3106,
      "step": 654
    },
    {
      "epoch": 0.14886363636363636,
      "grad_norm": 0.06192607060074806,
      "learning_rate": 0.00017042093287827076,
      "loss": 0.378,
      "step": 655
    },
    {
      "epoch": 0.14909090909090908,
      "grad_norm": 0.06075134873390198,
      "learning_rate": 0.00017037542662116042,
      "loss": 0.3644,
      "step": 656
    },
    {
      "epoch": 0.14931818181818182,
      "grad_norm": 0.05303726717829704,
      "learning_rate": 0.00017032992036405007,
      "loss": 0.2816,
      "step": 657
    },
    {
      "epoch": 0.14954545454545454,
      "grad_norm": 0.05385759845376015,
      "learning_rate": 0.00017028441410693972,
      "loss": 0.372,
      "step": 658
    },
    {
      "epoch": 0.14977272727272728,
      "grad_norm": 0.05149003863334656,
      "learning_rate": 0.00017023890784982938,
      "loss": 0.3031,
      "step": 659
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.04772584140300751,
      "learning_rate": 0.00017019340159271903,
      "loss": 0.3263,
      "step": 660
    },
    {
      "epoch": 0.15022727272727274,
      "grad_norm": 0.05924857035279274,
      "learning_rate": 0.00017014789533560865,
      "loss": 0.3165,
      "step": 661
    },
    {
      "epoch": 0.15045454545454545,
      "grad_norm": 0.049085017293691635,
      "learning_rate": 0.00017010238907849828,
      "loss": 0.3235,
      "step": 662
    },
    {
      "epoch": 0.1506818181818182,
      "grad_norm": 0.05407789722084999,
      "learning_rate": 0.00017005688282138793,
      "loss": 0.3713,
      "step": 663
    },
    {
      "epoch": 0.1509090909090909,
      "grad_norm": 0.05446453019976616,
      "learning_rate": 0.00017001137656427759,
      "loss": 0.315,
      "step": 664
    },
    {
      "epoch": 0.15113636363636362,
      "grad_norm": 0.0632946640253067,
      "learning_rate": 0.00016996587030716724,
      "loss": 0.353,
      "step": 665
    },
    {
      "epoch": 0.15136363636363637,
      "grad_norm": 0.07353118807077408,
      "learning_rate": 0.0001699203640500569,
      "loss": 0.3913,
      "step": 666
    },
    {
      "epoch": 0.15159090909090908,
      "grad_norm": 0.049267079681158066,
      "learning_rate": 0.00016987485779294654,
      "loss": 0.3482,
      "step": 667
    },
    {
      "epoch": 0.15181818181818182,
      "grad_norm": 0.044569388031959534,
      "learning_rate": 0.0001698293515358362,
      "loss": 0.3336,
      "step": 668
    },
    {
      "epoch": 0.15204545454545454,
      "grad_norm": 0.05819074809551239,
      "learning_rate": 0.00016978384527872585,
      "loss": 0.3631,
      "step": 669
    },
    {
      "epoch": 0.15227272727272728,
      "grad_norm": 0.07492619752883911,
      "learning_rate": 0.0001697383390216155,
      "loss": 0.3757,
      "step": 670
    },
    {
      "epoch": 0.1525,
      "grad_norm": 0.04158688709139824,
      "learning_rate": 0.00016969283276450513,
      "loss": 0.3112,
      "step": 671
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.05429660901427269,
      "learning_rate": 0.00016964732650739475,
      "loss": 0.3258,
      "step": 672
    },
    {
      "epoch": 0.15295454545454545,
      "grad_norm": 0.04771880432963371,
      "learning_rate": 0.0001696018202502844,
      "loss": 0.3306,
      "step": 673
    },
    {
      "epoch": 0.15318181818181817,
      "grad_norm": 0.049478307366371155,
      "learning_rate": 0.00016955631399317406,
      "loss": 0.2919,
      "step": 674
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.0683707520365715,
      "learning_rate": 0.0001695108077360637,
      "loss": 0.4057,
      "step": 675
    },
    {
      "epoch": 0.15363636363636363,
      "grad_norm": 0.051583826541900635,
      "learning_rate": 0.00016946530147895337,
      "loss": 0.3788,
      "step": 676
    },
    {
      "epoch": 0.15386363636363637,
      "grad_norm": 0.07213503867387772,
      "learning_rate": 0.00016941979522184302,
      "loss": 0.3954,
      "step": 677
    },
    {
      "epoch": 0.15409090909090908,
      "grad_norm": 0.04620792344212532,
      "learning_rate": 0.00016937428896473267,
      "loss": 0.3651,
      "step": 678
    },
    {
      "epoch": 0.15431818181818183,
      "grad_norm": 0.0552268847823143,
      "learning_rate": 0.00016932878270762232,
      "loss": 0.3165,
      "step": 679
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.05495699122548103,
      "learning_rate": 0.00016928327645051198,
      "loss": 0.3014,
      "step": 680
    },
    {
      "epoch": 0.15477272727272728,
      "grad_norm": 0.04824705421924591,
      "learning_rate": 0.0001692377701934016,
      "loss": 0.3218,
      "step": 681
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.05055674910545349,
      "learning_rate": 0.00016919226393629123,
      "loss": 0.3357,
      "step": 682
    },
    {
      "epoch": 0.1552272727272727,
      "grad_norm": 0.04987814649939537,
      "learning_rate": 0.00016914675767918088,
      "loss": 0.2972,
      "step": 683
    },
    {
      "epoch": 0.15545454545454546,
      "grad_norm": 0.04470771923661232,
      "learning_rate": 0.00016910125142207053,
      "loss": 0.3435,
      "step": 684
    },
    {
      "epoch": 0.15568181818181817,
      "grad_norm": 0.06620042026042938,
      "learning_rate": 0.00016905574516496019,
      "loss": 0.3701,
      "step": 685
    },
    {
      "epoch": 0.1559090909090909,
      "grad_norm": 0.057900458574295044,
      "learning_rate": 0.00016901023890784984,
      "loss": 0.3849,
      "step": 686
    },
    {
      "epoch": 0.15613636363636363,
      "grad_norm": 0.053663454949855804,
      "learning_rate": 0.0001689647326507395,
      "loss": 0.35,
      "step": 687
    },
    {
      "epoch": 0.15636363636363637,
      "grad_norm": 0.05149849131703377,
      "learning_rate": 0.00016891922639362914,
      "loss": 0.3542,
      "step": 688
    },
    {
      "epoch": 0.1565909090909091,
      "grad_norm": 0.07190616428852081,
      "learning_rate": 0.0001688737201365188,
      "loss": 0.3854,
      "step": 689
    },
    {
      "epoch": 0.15681818181818183,
      "grad_norm": 0.0683574229478836,
      "learning_rate": 0.00016882821387940842,
      "loss": 0.3489,
      "step": 690
    },
    {
      "epoch": 0.15704545454545454,
      "grad_norm": 0.04662236198782921,
      "learning_rate": 0.00016878270762229808,
      "loss": 0.2784,
      "step": 691
    },
    {
      "epoch": 0.1572727272727273,
      "grad_norm": 0.04901484027504921,
      "learning_rate": 0.0001687372013651877,
      "loss": 0.3019,
      "step": 692
    },
    {
      "epoch": 0.1575,
      "grad_norm": 0.05487454682588577,
      "learning_rate": 0.00016869169510807735,
      "loss": 0.3566,
      "step": 693
    },
    {
      "epoch": 0.15772727272727272,
      "grad_norm": 0.04756467789411545,
      "learning_rate": 0.000168646188850967,
      "loss": 0.3013,
      "step": 694
    },
    {
      "epoch": 0.15795454545454546,
      "grad_norm": 0.051016587764024734,
      "learning_rate": 0.00016860068259385666,
      "loss": 0.3501,
      "step": 695
    },
    {
      "epoch": 0.15818181818181817,
      "grad_norm": 0.056129828095436096,
      "learning_rate": 0.0001685551763367463,
      "loss": 0.3858,
      "step": 696
    },
    {
      "epoch": 0.15840909090909092,
      "grad_norm": 0.058773089200258255,
      "learning_rate": 0.00016850967007963597,
      "loss": 0.3575,
      "step": 697
    },
    {
      "epoch": 0.15863636363636363,
      "grad_norm": 0.047778673470020294,
      "learning_rate": 0.00016846416382252562,
      "loss": 0.3614,
      "step": 698
    },
    {
      "epoch": 0.15886363636363637,
      "grad_norm": 0.0655776634812355,
      "learning_rate": 0.00016841865756541527,
      "loss": 0.3442,
      "step": 699
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.06375526636838913,
      "learning_rate": 0.0001683731513083049,
      "loss": 0.3718,
      "step": 700
    },
    {
      "epoch": 0.15931818181818183,
      "grad_norm": 0.05072938650846481,
      "learning_rate": 0.00016832764505119455,
      "loss": 0.3477,
      "step": 701
    },
    {
      "epoch": 0.15954545454545455,
      "grad_norm": 0.04759516566991806,
      "learning_rate": 0.00016828213879408418,
      "loss": 0.3344,
      "step": 702
    },
    {
      "epoch": 0.15977272727272726,
      "grad_norm": 0.04754124954342842,
      "learning_rate": 0.00016823663253697383,
      "loss": 0.3453,
      "step": 703
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.05051784962415695,
      "learning_rate": 0.00016819112627986348,
      "loss": 0.3037,
      "step": 704
    },
    {
      "epoch": 0.16022727272727272,
      "grad_norm": 0.04781116545200348,
      "learning_rate": 0.00016814562002275313,
      "loss": 0.3825,
      "step": 705
    },
    {
      "epoch": 0.16045454545454546,
      "grad_norm": 0.06228088214993477,
      "learning_rate": 0.0001681001137656428,
      "loss": 0.3655,
      "step": 706
    },
    {
      "epoch": 0.16068181818181818,
      "grad_norm": 0.03945764899253845,
      "learning_rate": 0.00016805460750853244,
      "loss": 0.2974,
      "step": 707
    },
    {
      "epoch": 0.16090909090909092,
      "grad_norm": 0.04145853966474533,
      "learning_rate": 0.0001680091012514221,
      "loss": 0.3059,
      "step": 708
    },
    {
      "epoch": 0.16113636363636363,
      "grad_norm": 0.05445891246199608,
      "learning_rate": 0.00016796359499431175,
      "loss": 0.3978,
      "step": 709
    },
    {
      "epoch": 0.16136363636363638,
      "grad_norm": 0.05979711934924126,
      "learning_rate": 0.00016791808873720137,
      "loss": 0.4373,
      "step": 710
    },
    {
      "epoch": 0.1615909090909091,
      "grad_norm": 0.05456708371639252,
      "learning_rate": 0.00016787258248009102,
      "loss": 0.3667,
      "step": 711
    },
    {
      "epoch": 0.1618181818181818,
      "grad_norm": 0.058849405497312546,
      "learning_rate": 0.00016782707622298065,
      "loss": 0.3386,
      "step": 712
    },
    {
      "epoch": 0.16204545454545455,
      "grad_norm": 0.048006489872932434,
      "learning_rate": 0.0001677815699658703,
      "loss": 0.3238,
      "step": 713
    },
    {
      "epoch": 0.16227272727272726,
      "grad_norm": 0.0489887110888958,
      "learning_rate": 0.00016773606370875996,
      "loss": 0.3292,
      "step": 714
    },
    {
      "epoch": 0.1625,
      "grad_norm": 0.053661249577999115,
      "learning_rate": 0.0001676905574516496,
      "loss": 0.3311,
      "step": 715
    },
    {
      "epoch": 0.16272727272727272,
      "grad_norm": 0.04666732996702194,
      "learning_rate": 0.00016764505119453926,
      "loss": 0.2916,
      "step": 716
    },
    {
      "epoch": 0.16295454545454546,
      "grad_norm": 0.0495598241686821,
      "learning_rate": 0.00016759954493742891,
      "loss": 0.316,
      "step": 717
    },
    {
      "epoch": 0.16318181818181818,
      "grad_norm": 0.053129009902477264,
      "learning_rate": 0.00016755403868031857,
      "loss": 0.358,
      "step": 718
    },
    {
      "epoch": 0.16340909090909092,
      "grad_norm": 0.05350131541490555,
      "learning_rate": 0.00016750853242320822,
      "loss": 0.359,
      "step": 719
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.055807631462812424,
      "learning_rate": 0.00016746302616609785,
      "loss": 0.3898,
      "step": 720
    },
    {
      "epoch": 0.16386363636363635,
      "grad_norm": 0.06868327409029007,
      "learning_rate": 0.0001674175199089875,
      "loss": 0.4065,
      "step": 721
    },
    {
      "epoch": 0.1640909090909091,
      "grad_norm": 0.05556929111480713,
      "learning_rate": 0.00016737201365187712,
      "loss": 0.3219,
      "step": 722
    },
    {
      "epoch": 0.1643181818181818,
      "grad_norm": 0.04989233613014221,
      "learning_rate": 0.00016732650739476678,
      "loss": 0.3017,
      "step": 723
    },
    {
      "epoch": 0.16454545454545455,
      "grad_norm": 0.06053221598267555,
      "learning_rate": 0.00016728100113765643,
      "loss": 0.3056,
      "step": 724
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.07302062213420868,
      "learning_rate": 0.00016723549488054608,
      "loss": 0.4236,
      "step": 725
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.06785126775503159,
      "learning_rate": 0.00016718998862343573,
      "loss": 0.3859,
      "step": 726
    },
    {
      "epoch": 0.16522727272727272,
      "grad_norm": 0.05414620041847229,
      "learning_rate": 0.0001671444823663254,
      "loss": 0.3283,
      "step": 727
    },
    {
      "epoch": 0.16545454545454547,
      "grad_norm": 0.05031726136803627,
      "learning_rate": 0.00016709897610921504,
      "loss": 0.2908,
      "step": 728
    },
    {
      "epoch": 0.16568181818181818,
      "grad_norm": 0.0691761001944542,
      "learning_rate": 0.0001670534698521047,
      "loss": 0.3696,
      "step": 729
    },
    {
      "epoch": 0.16590909090909092,
      "grad_norm": 0.059505023062229156,
      "learning_rate": 0.00016700796359499432,
      "loss": 0.2887,
      "step": 730
    },
    {
      "epoch": 0.16613636363636364,
      "grad_norm": 0.07114806026220322,
      "learning_rate": 0.00016696245733788397,
      "loss": 0.4094,
      "step": 731
    },
    {
      "epoch": 0.16636363636363635,
      "grad_norm": 0.0418657548725605,
      "learning_rate": 0.0001669169510807736,
      "loss": 0.2542,
      "step": 732
    },
    {
      "epoch": 0.1665909090909091,
      "grad_norm": 0.05604863911867142,
      "learning_rate": 0.00016687144482366325,
      "loss": 0.3537,
      "step": 733
    },
    {
      "epoch": 0.1668181818181818,
      "grad_norm": 0.05349872633814812,
      "learning_rate": 0.0001668259385665529,
      "loss": 0.3229,
      "step": 734
    },
    {
      "epoch": 0.16704545454545455,
      "grad_norm": 0.07007014006376266,
      "learning_rate": 0.00016678043230944256,
      "loss": 0.3781,
      "step": 735
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.07898525893688202,
      "learning_rate": 0.0001667349260523322,
      "loss": 0.4008,
      "step": 736
    },
    {
      "epoch": 0.1675,
      "grad_norm": 0.05266769975423813,
      "learning_rate": 0.00016668941979522186,
      "loss": 0.3485,
      "step": 737
    },
    {
      "epoch": 0.16772727272727272,
      "grad_norm": 0.03601418808102608,
      "learning_rate": 0.00016664391353811151,
      "loss": 0.2767,
      "step": 738
    },
    {
      "epoch": 0.16795454545454547,
      "grad_norm": 0.04856547713279724,
      "learning_rate": 0.00016659840728100117,
      "loss": 0.2945,
      "step": 739
    },
    {
      "epoch": 0.16818181818181818,
      "grad_norm": 0.04983215406537056,
      "learning_rate": 0.0001665529010238908,
      "loss": 0.3199,
      "step": 740
    },
    {
      "epoch": 0.1684090909090909,
      "grad_norm": 0.05132536217570305,
      "learning_rate": 0.00016650739476678045,
      "loss": 0.2966,
      "step": 741
    },
    {
      "epoch": 0.16863636363636364,
      "grad_norm": 0.06551304459571838,
      "learning_rate": 0.00016646188850967007,
      "loss": 0.4182,
      "step": 742
    },
    {
      "epoch": 0.16886363636363635,
      "grad_norm": 0.04910104721784592,
      "learning_rate": 0.00016641638225255972,
      "loss": 0.3062,
      "step": 743
    },
    {
      "epoch": 0.1690909090909091,
      "grad_norm": 0.05944471061229706,
      "learning_rate": 0.00016637087599544938,
      "loss": 0.3433,
      "step": 744
    },
    {
      "epoch": 0.1693181818181818,
      "grad_norm": 0.05583633854985237,
      "learning_rate": 0.00016632536973833903,
      "loss": 0.3303,
      "step": 745
    },
    {
      "epoch": 0.16954545454545455,
      "grad_norm": 0.04542640969157219,
      "learning_rate": 0.00016627986348122868,
      "loss": 0.2898,
      "step": 746
    },
    {
      "epoch": 0.16977272727272727,
      "grad_norm": 0.05844502151012421,
      "learning_rate": 0.00016623435722411834,
      "loss": 0.3682,
      "step": 747
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.06401263922452927,
      "learning_rate": 0.000166188850967008,
      "loss": 0.3376,
      "step": 748
    },
    {
      "epoch": 0.17022727272727273,
      "grad_norm": 0.05186304450035095,
      "learning_rate": 0.00016614334470989761,
      "loss": 0.3381,
      "step": 749
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 0.06347573548555374,
      "learning_rate": 0.00016609783845278727,
      "loss": 0.333,
      "step": 750
    },
    {
      "epoch": 0.17068181818181818,
      "grad_norm": 0.06312379986047745,
      "learning_rate": 0.00016605233219567692,
      "loss": 0.3602,
      "step": 751
    },
    {
      "epoch": 0.1709090909090909,
      "grad_norm": 0.04955970495939255,
      "learning_rate": 0.00016600682593856655,
      "loss": 0.3625,
      "step": 752
    },
    {
      "epoch": 0.17113636363636364,
      "grad_norm": 0.060198042541742325,
      "learning_rate": 0.0001659613196814562,
      "loss": 0.3672,
      "step": 753
    },
    {
      "epoch": 0.17136363636363636,
      "grad_norm": 0.0526868961751461,
      "learning_rate": 0.00016591581342434585,
      "loss": 0.2978,
      "step": 754
    },
    {
      "epoch": 0.1715909090909091,
      "grad_norm": 0.055692095309495926,
      "learning_rate": 0.0001658703071672355,
      "loss": 0.3712,
      "step": 755
    },
    {
      "epoch": 0.17181818181818181,
      "grad_norm": 0.055218908935785294,
      "learning_rate": 0.00016582480091012516,
      "loss": 0.3379,
      "step": 756
    },
    {
      "epoch": 0.17204545454545456,
      "grad_norm": 0.05624740570783615,
      "learning_rate": 0.0001657792946530148,
      "loss": 0.3521,
      "step": 757
    },
    {
      "epoch": 0.17227272727272727,
      "grad_norm": 0.055312030017375946,
      "learning_rate": 0.00016573378839590446,
      "loss": 0.3591,
      "step": 758
    },
    {
      "epoch": 0.1725,
      "grad_norm": 0.059243094176054,
      "learning_rate": 0.0001656882821387941,
      "loss": 0.3478,
      "step": 759
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 0.037806835025548935,
      "learning_rate": 0.00016564277588168374,
      "loss": 0.2935,
      "step": 760
    },
    {
      "epoch": 0.17295454545454544,
      "grad_norm": 0.0628545805811882,
      "learning_rate": 0.0001655972696245734,
      "loss": 0.3575,
      "step": 761
    },
    {
      "epoch": 0.1731818181818182,
      "grad_norm": 0.046628110110759735,
      "learning_rate": 0.00016555176336746302,
      "loss": 0.357,
      "step": 762
    },
    {
      "epoch": 0.1734090909090909,
      "grad_norm": 0.06315165758132935,
      "learning_rate": 0.00016550625711035267,
      "loss": 0.3287,
      "step": 763
    },
    {
      "epoch": 0.17363636363636364,
      "grad_norm": 0.05345473811030388,
      "learning_rate": 0.00016546075085324233,
      "loss": 0.3173,
      "step": 764
    },
    {
      "epoch": 0.17386363636363636,
      "grad_norm": 0.04327929764986038,
      "learning_rate": 0.00016541524459613198,
      "loss": 0.2977,
      "step": 765
    },
    {
      "epoch": 0.1740909090909091,
      "grad_norm": 0.049467429518699646,
      "learning_rate": 0.00016536973833902163,
      "loss": 0.3249,
      "step": 766
    },
    {
      "epoch": 0.17431818181818182,
      "grad_norm": 0.05180700123310089,
      "learning_rate": 0.00016532423208191128,
      "loss": 0.3073,
      "step": 767
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.04720958694815636,
      "learning_rate": 0.00016527872582480094,
      "loss": 0.3221,
      "step": 768
    },
    {
      "epoch": 0.17477272727272727,
      "grad_norm": 0.050289396196603775,
      "learning_rate": 0.00016523321956769056,
      "loss": 0.2968,
      "step": 769
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.06181797385215759,
      "learning_rate": 0.00016518771331058022,
      "loss": 0.3753,
      "step": 770
    },
    {
      "epoch": 0.17522727272727273,
      "grad_norm": 0.04864038527011871,
      "learning_rate": 0.00016514220705346987,
      "loss": 0.3067,
      "step": 771
    },
    {
      "epoch": 0.17545454545454545,
      "grad_norm": 0.04852176085114479,
      "learning_rate": 0.0001650967007963595,
      "loss": 0.3478,
      "step": 772
    },
    {
      "epoch": 0.1756818181818182,
      "grad_norm": 0.047703664749860764,
      "learning_rate": 0.00016505119453924915,
      "loss": 0.3633,
      "step": 773
    },
    {
      "epoch": 0.1759090909090909,
      "grad_norm": 0.052320342510938644,
      "learning_rate": 0.0001650056882821388,
      "loss": 0.3555,
      "step": 774
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.06729578226804733,
      "learning_rate": 0.00016496018202502845,
      "loss": 0.2787,
      "step": 775
    },
    {
      "epoch": 0.17636363636363636,
      "grad_norm": 0.04901544749736786,
      "learning_rate": 0.0001649146757679181,
      "loss": 0.3748,
      "step": 776
    },
    {
      "epoch": 0.1765909090909091,
      "grad_norm": 0.049776267260313034,
      "learning_rate": 0.00016486916951080776,
      "loss": 0.2991,
      "step": 777
    },
    {
      "epoch": 0.17681818181818182,
      "grad_norm": 0.0451652929186821,
      "learning_rate": 0.0001648236632536974,
      "loss": 0.3199,
      "step": 778
    },
    {
      "epoch": 0.17704545454545453,
      "grad_norm": 0.0566459596157074,
      "learning_rate": 0.00016477815699658704,
      "loss": 0.3487,
      "step": 779
    },
    {
      "epoch": 0.17727272727272728,
      "grad_norm": 0.04229636490345001,
      "learning_rate": 0.0001647326507394767,
      "loss": 0.2845,
      "step": 780
    },
    {
      "epoch": 0.1775,
      "grad_norm": 0.06166988983750343,
      "learning_rate": 0.00016468714448236634,
      "loss": 0.3538,
      "step": 781
    },
    {
      "epoch": 0.17772727272727273,
      "grad_norm": 0.04866776615381241,
      "learning_rate": 0.00016464163822525597,
      "loss": 0.3381,
      "step": 782
    },
    {
      "epoch": 0.17795454545454545,
      "grad_norm": 0.06027895584702492,
      "learning_rate": 0.00016459613196814562,
      "loss": 0.341,
      "step": 783
    },
    {
      "epoch": 0.1781818181818182,
      "grad_norm": 0.05311094969511032,
      "learning_rate": 0.00016455062571103527,
      "loss": 0.3927,
      "step": 784
    },
    {
      "epoch": 0.1784090909090909,
      "grad_norm": 0.04267008602619171,
      "learning_rate": 0.00016450511945392493,
      "loss": 0.2685,
      "step": 785
    },
    {
      "epoch": 0.17863636363636365,
      "grad_norm": 0.04879410192370415,
      "learning_rate": 0.00016445961319681458,
      "loss": 0.2717,
      "step": 786
    },
    {
      "epoch": 0.17886363636363636,
      "grad_norm": 0.06050579994916916,
      "learning_rate": 0.00016441410693970423,
      "loss": 0.2878,
      "step": 787
    },
    {
      "epoch": 0.17909090909090908,
      "grad_norm": 0.06512876600027084,
      "learning_rate": 0.00016436860068259386,
      "loss": 0.3341,
      "step": 788
    },
    {
      "epoch": 0.17931818181818182,
      "grad_norm": 0.059758421033620834,
      "learning_rate": 0.0001643230944254835,
      "loss": 0.3542,
      "step": 789
    },
    {
      "epoch": 0.17954545454545454,
      "grad_norm": 0.0518556572496891,
      "learning_rate": 0.00016427758816837316,
      "loss": 0.3498,
      "step": 790
    },
    {
      "epoch": 0.17977272727272728,
      "grad_norm": 0.059901025146245956,
      "learning_rate": 0.00016423208191126282,
      "loss": 0.2855,
      "step": 791
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.05736062675714493,
      "learning_rate": 0.00016418657565415244,
      "loss": 0.3406,
      "step": 792
    },
    {
      "epoch": 0.18022727272727274,
      "grad_norm": 0.07890550792217255,
      "learning_rate": 0.0001641410693970421,
      "loss": 0.3754,
      "step": 793
    },
    {
      "epoch": 0.18045454545454545,
      "grad_norm": 0.033188994973897934,
      "learning_rate": 0.00016409556313993175,
      "loss": 0.2529,
      "step": 794
    },
    {
      "epoch": 0.1806818181818182,
      "grad_norm": 0.07051311433315277,
      "learning_rate": 0.0001640500568828214,
      "loss": 0.4098,
      "step": 795
    },
    {
      "epoch": 0.1809090909090909,
      "grad_norm": 0.05607768893241882,
      "learning_rate": 0.00016400455062571105,
      "loss": 0.34,
      "step": 796
    },
    {
      "epoch": 0.18113636363636362,
      "grad_norm": 0.060617756098508835,
      "learning_rate": 0.0001639590443686007,
      "loss": 0.3313,
      "step": 797
    },
    {
      "epoch": 0.18136363636363637,
      "grad_norm": 0.06274748593568802,
      "learning_rate": 0.00016391353811149033,
      "loss": 0.3279,
      "step": 798
    },
    {
      "epoch": 0.18159090909090908,
      "grad_norm": 0.046027593314647675,
      "learning_rate": 0.00016386803185437998,
      "loss": 0.3554,
      "step": 799
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.038642097264528275,
      "learning_rate": 0.00016382252559726964,
      "loss": 0.3078,
      "step": 800
    },
    {
      "epoch": 0.18204545454545454,
      "grad_norm": 0.0613618828356266,
      "learning_rate": 0.0001637770193401593,
      "loss": 0.3819,
      "step": 801
    },
    {
      "epoch": 0.18227272727272728,
      "grad_norm": 0.04452934116125107,
      "learning_rate": 0.00016373151308304892,
      "loss": 0.3336,
      "step": 802
    },
    {
      "epoch": 0.1825,
      "grad_norm": 0.05428885668516159,
      "learning_rate": 0.00016368600682593857,
      "loss": 0.2733,
      "step": 803
    },
    {
      "epoch": 0.18272727272727274,
      "grad_norm": 0.0510227344930172,
      "learning_rate": 0.00016364050056882822,
      "loss": 0.3636,
      "step": 804
    },
    {
      "epoch": 0.18295454545454545,
      "grad_norm": 0.044412583112716675,
      "learning_rate": 0.00016359499431171787,
      "loss": 0.2849,
      "step": 805
    },
    {
      "epoch": 0.1831818181818182,
      "grad_norm": 0.046168625354766846,
      "learning_rate": 0.00016354948805460753,
      "loss": 0.294,
      "step": 806
    },
    {
      "epoch": 0.1834090909090909,
      "grad_norm": 0.05774259567260742,
      "learning_rate": 0.00016350398179749718,
      "loss": 0.3578,
      "step": 807
    },
    {
      "epoch": 0.18363636363636363,
      "grad_norm": 0.05895288288593292,
      "learning_rate": 0.0001634584755403868,
      "loss": 0.3192,
      "step": 808
    },
    {
      "epoch": 0.18386363636363637,
      "grad_norm": 0.07010826468467712,
      "learning_rate": 0.00016341296928327646,
      "loss": 0.4702,
      "step": 809
    },
    {
      "epoch": 0.18409090909090908,
      "grad_norm": 0.053292594850063324,
      "learning_rate": 0.0001633674630261661,
      "loss": 0.3281,
      "step": 810
    },
    {
      "epoch": 0.18431818181818183,
      "grad_norm": 0.06343355774879456,
      "learning_rate": 0.00016332195676905576,
      "loss": 0.3842,
      "step": 811
    },
    {
      "epoch": 0.18454545454545454,
      "grad_norm": 0.04511595889925957,
      "learning_rate": 0.0001632764505119454,
      "loss": 0.3064,
      "step": 812
    },
    {
      "epoch": 0.18477272727272728,
      "grad_norm": 0.05535171180963516,
      "learning_rate": 0.00016323094425483504,
      "loss": 0.3971,
      "step": 813
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.03474530950188637,
      "learning_rate": 0.0001631854379977247,
      "loss": 0.2653,
      "step": 814
    },
    {
      "epoch": 0.18522727272727274,
      "grad_norm": 0.05624151602387428,
      "learning_rate": 0.00016313993174061435,
      "loss": 0.3971,
      "step": 815
    },
    {
      "epoch": 0.18545454545454546,
      "grad_norm": 0.044423770159482956,
      "learning_rate": 0.000163094425483504,
      "loss": 0.3417,
      "step": 816
    },
    {
      "epoch": 0.18568181818181817,
      "grad_norm": 0.047040969133377075,
      "learning_rate": 0.00016304891922639363,
      "loss": 0.2868,
      "step": 817
    },
    {
      "epoch": 0.1859090909090909,
      "grad_norm": 0.05674547702074051,
      "learning_rate": 0.00016300341296928328,
      "loss": 0.3353,
      "step": 818
    },
    {
      "epoch": 0.18613636363636363,
      "grad_norm": 0.04245217517018318,
      "learning_rate": 0.00016295790671217293,
      "loss": 0.2943,
      "step": 819
    },
    {
      "epoch": 0.18636363636363637,
      "grad_norm": 0.04507127031683922,
      "learning_rate": 0.00016291240045506259,
      "loss": 0.2735,
      "step": 820
    },
    {
      "epoch": 0.18659090909090909,
      "grad_norm": 0.05311017110943794,
      "learning_rate": 0.00016286689419795224,
      "loss": 0.366,
      "step": 821
    },
    {
      "epoch": 0.18681818181818183,
      "grad_norm": 0.05076324939727783,
      "learning_rate": 0.00016282138794084186,
      "loss": 0.3606,
      "step": 822
    },
    {
      "epoch": 0.18704545454545454,
      "grad_norm": 0.05529582127928734,
      "learning_rate": 0.00016277588168373152,
      "loss": 0.3797,
      "step": 823
    },
    {
      "epoch": 0.18727272727272729,
      "grad_norm": 0.05686294659972191,
      "learning_rate": 0.00016273037542662117,
      "loss": 0.3423,
      "step": 824
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.05630234628915787,
      "learning_rate": 0.00016268486916951082,
      "loss": 0.3544,
      "step": 825
    },
    {
      "epoch": 0.18772727272727271,
      "grad_norm": 0.047267068177461624,
      "learning_rate": 0.00016263936291240047,
      "loss": 0.3619,
      "step": 826
    },
    {
      "epoch": 0.18795454545454546,
      "grad_norm": 0.06053685396909714,
      "learning_rate": 0.0001625938566552901,
      "loss": 0.3998,
      "step": 827
    },
    {
      "epoch": 0.18818181818181817,
      "grad_norm": 0.03932052105665207,
      "learning_rate": 0.00016254835039817975,
      "loss": 0.255,
      "step": 828
    },
    {
      "epoch": 0.18840909090909091,
      "grad_norm": 0.05517212301492691,
      "learning_rate": 0.0001625028441410694,
      "loss": 0.3451,
      "step": 829
    },
    {
      "epoch": 0.18863636363636363,
      "grad_norm": 0.06156338006258011,
      "learning_rate": 0.00016245733788395906,
      "loss": 0.388,
      "step": 830
    },
    {
      "epoch": 0.18886363636363637,
      "grad_norm": 0.04520844668149948,
      "learning_rate": 0.0001624118316268487,
      "loss": 0.3268,
      "step": 831
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.04223661124706268,
      "learning_rate": 0.00016236632536973834,
      "loss": 0.2909,
      "step": 832
    },
    {
      "epoch": 0.18931818181818183,
      "grad_norm": 0.06923125684261322,
      "learning_rate": 0.000162320819112628,
      "loss": 0.3399,
      "step": 833
    },
    {
      "epoch": 0.18954545454545454,
      "grad_norm": 0.05666259303689003,
      "learning_rate": 0.00016227531285551764,
      "loss": 0.338,
      "step": 834
    },
    {
      "epoch": 0.18977272727272726,
      "grad_norm": 0.06741177290678024,
      "learning_rate": 0.0001622298065984073,
      "loss": 0.3141,
      "step": 835
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.057317573577165604,
      "learning_rate": 0.00016218430034129695,
      "loss": 0.3774,
      "step": 836
    },
    {
      "epoch": 0.19022727272727272,
      "grad_norm": 0.06780240684747696,
      "learning_rate": 0.00016213879408418657,
      "loss": 0.4038,
      "step": 837
    },
    {
      "epoch": 0.19045454545454546,
      "grad_norm": 0.04463310167193413,
      "learning_rate": 0.00016209328782707623,
      "loss": 0.2901,
      "step": 838
    },
    {
      "epoch": 0.19068181818181817,
      "grad_norm": 0.04919439181685448,
      "learning_rate": 0.00016204778156996588,
      "loss": 0.3528,
      "step": 839
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.0499819852411747,
      "learning_rate": 0.00016200227531285553,
      "loss": 0.3005,
      "step": 840
    },
    {
      "epoch": 0.19113636363636363,
      "grad_norm": 0.05358916148543358,
      "learning_rate": 0.00016195676905574516,
      "loss": 0.3349,
      "step": 841
    },
    {
      "epoch": 0.19136363636363637,
      "grad_norm": 0.051109638065099716,
      "learning_rate": 0.0001619112627986348,
      "loss": 0.2963,
      "step": 842
    },
    {
      "epoch": 0.1915909090909091,
      "grad_norm": 0.06641097366809845,
      "learning_rate": 0.00016186575654152446,
      "loss": 0.4256,
      "step": 843
    },
    {
      "epoch": 0.1918181818181818,
      "grad_norm": 0.05359811335802078,
      "learning_rate": 0.00016182025028441412,
      "loss": 0.2803,
      "step": 844
    },
    {
      "epoch": 0.19204545454545455,
      "grad_norm": 0.04256449267268181,
      "learning_rate": 0.00016177474402730377,
      "loss": 0.2768,
      "step": 845
    },
    {
      "epoch": 0.19227272727272726,
      "grad_norm": 0.043535005301237106,
      "learning_rate": 0.00016172923777019342,
      "loss": 0.304,
      "step": 846
    },
    {
      "epoch": 0.1925,
      "grad_norm": 0.05266573652625084,
      "learning_rate": 0.00016168373151308305,
      "loss": 0.2934,
      "step": 847
    },
    {
      "epoch": 0.19272727272727272,
      "grad_norm": 0.07303544133901596,
      "learning_rate": 0.0001616382252559727,
      "loss": 0.3295,
      "step": 848
    },
    {
      "epoch": 0.19295454545454546,
      "grad_norm": 0.0589459128677845,
      "learning_rate": 0.00016159271899886235,
      "loss": 0.3319,
      "step": 849
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.05436190962791443,
      "learning_rate": 0.000161547212741752,
      "loss": 0.3697,
      "step": 850
    },
    {
      "epoch": 0.19340909090909092,
      "grad_norm": 0.06262142211198807,
      "learning_rate": 0.00016150170648464163,
      "loss": 0.415,
      "step": 851
    },
    {
      "epoch": 0.19363636363636363,
      "grad_norm": 0.04639735817909241,
      "learning_rate": 0.00016145620022753129,
      "loss": 0.3189,
      "step": 852
    },
    {
      "epoch": 0.19386363636363638,
      "grad_norm": 0.05301347002387047,
      "learning_rate": 0.00016141069397042094,
      "loss": 0.3779,
      "step": 853
    },
    {
      "epoch": 0.1940909090909091,
      "grad_norm": 0.037535376846790314,
      "learning_rate": 0.0001613651877133106,
      "loss": 0.273,
      "step": 854
    },
    {
      "epoch": 0.1943181818181818,
      "grad_norm": 0.047840721905231476,
      "learning_rate": 0.00016131968145620024,
      "loss": 0.2839,
      "step": 855
    },
    {
      "epoch": 0.19454545454545455,
      "grad_norm": 0.04903968423604965,
      "learning_rate": 0.00016127417519908987,
      "loss": 0.3014,
      "step": 856
    },
    {
      "epoch": 0.19477272727272726,
      "grad_norm": 0.04709925130009651,
      "learning_rate": 0.00016122866894197952,
      "loss": 0.3258,
      "step": 857
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.050712235271930695,
      "learning_rate": 0.00016118316268486918,
      "loss": 0.2899,
      "step": 858
    },
    {
      "epoch": 0.19522727272727272,
      "grad_norm": 0.05772736668586731,
      "learning_rate": 0.00016113765642775883,
      "loss": 0.3469,
      "step": 859
    },
    {
      "epoch": 0.19545454545454546,
      "grad_norm": 0.04756464809179306,
      "learning_rate": 0.00016109215017064848,
      "loss": 0.3237,
      "step": 860
    },
    {
      "epoch": 0.19568181818181818,
      "grad_norm": 0.04650149866938591,
      "learning_rate": 0.0001610466439135381,
      "loss": 0.3202,
      "step": 861
    },
    {
      "epoch": 0.19590909090909092,
      "grad_norm": 0.057420067489147186,
      "learning_rate": 0.00016100113765642776,
      "loss": 0.3833,
      "step": 862
    },
    {
      "epoch": 0.19613636363636364,
      "grad_norm": 0.05411507561802864,
      "learning_rate": 0.0001609556313993174,
      "loss": 0.3938,
      "step": 863
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.05157485976815224,
      "learning_rate": 0.00016091012514220707,
      "loss": 0.3503,
      "step": 864
    },
    {
      "epoch": 0.1965909090909091,
      "grad_norm": 0.05887864530086517,
      "learning_rate": 0.00016086461888509672,
      "loss": 0.3769,
      "step": 865
    },
    {
      "epoch": 0.1968181818181818,
      "grad_norm": 0.05056656524538994,
      "learning_rate": 0.00016081911262798634,
      "loss": 0.331,
      "step": 866
    },
    {
      "epoch": 0.19704545454545455,
      "grad_norm": 0.052351582795381546,
      "learning_rate": 0.000160773606370876,
      "loss": 0.328,
      "step": 867
    },
    {
      "epoch": 0.19727272727272727,
      "grad_norm": 0.038511235266923904,
      "learning_rate": 0.00016072810011376565,
      "loss": 0.2873,
      "step": 868
    },
    {
      "epoch": 0.1975,
      "grad_norm": 0.07424595952033997,
      "learning_rate": 0.0001606825938566553,
      "loss": 0.3897,
      "step": 869
    },
    {
      "epoch": 0.19772727272727272,
      "grad_norm": 0.04991047829389572,
      "learning_rate": 0.00016063708759954495,
      "loss": 0.3428,
      "step": 870
    },
    {
      "epoch": 0.19795454545454547,
      "grad_norm": 0.05528421327471733,
      "learning_rate": 0.00016059158134243458,
      "loss": 0.3425,
      "step": 871
    },
    {
      "epoch": 0.19818181818181818,
      "grad_norm": 0.05177208036184311,
      "learning_rate": 0.00016054607508532423,
      "loss": 0.3697,
      "step": 872
    },
    {
      "epoch": 0.1984090909090909,
      "grad_norm": 0.05535276234149933,
      "learning_rate": 0.00016050056882821389,
      "loss": 0.3511,
      "step": 873
    },
    {
      "epoch": 0.19863636363636364,
      "grad_norm": 0.047293804585933685,
      "learning_rate": 0.00016045506257110354,
      "loss": 0.3264,
      "step": 874
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.05517047643661499,
      "learning_rate": 0.0001604095563139932,
      "loss": 0.2901,
      "step": 875
    },
    {
      "epoch": 0.1990909090909091,
      "grad_norm": 0.036567676812410355,
      "learning_rate": 0.00016036405005688282,
      "loss": 0.2268,
      "step": 876
    },
    {
      "epoch": 0.1993181818181818,
      "grad_norm": 0.059910256415605545,
      "learning_rate": 0.00016031854379977247,
      "loss": 0.3827,
      "step": 877
    },
    {
      "epoch": 0.19954545454545455,
      "grad_norm": 0.04666285216808319,
      "learning_rate": 0.00016027303754266212,
      "loss": 0.2929,
      "step": 878
    },
    {
      "epoch": 0.19977272727272727,
      "grad_norm": 0.05046035349369049,
      "learning_rate": 0.00016022753128555178,
      "loss": 0.3407,
      "step": 879
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.04464413970708847,
      "learning_rate": 0.00016018202502844143,
      "loss": 0.3037,
      "step": 880
    },
    {
      "epoch": 0.20022727272727273,
      "grad_norm": 0.0419984795153141,
      "learning_rate": 0.00016013651877133105,
      "loss": 0.279,
      "step": 881
    },
    {
      "epoch": 0.20045454545454544,
      "grad_norm": 0.04794275015592575,
      "learning_rate": 0.0001600910125142207,
      "loss": 0.3208,
      "step": 882
    },
    {
      "epoch": 0.20068181818181818,
      "grad_norm": 0.054374609142541885,
      "learning_rate": 0.00016004550625711036,
      "loss": 0.4274,
      "step": 883
    },
    {
      "epoch": 0.2009090909090909,
      "grad_norm": 0.042877331376075745,
      "learning_rate": 0.00016,
      "loss": 0.3016,
      "step": 884
    },
    {
      "epoch": 0.20113636363636364,
      "grad_norm": 0.07098899781703949,
      "learning_rate": 0.00015995449374288964,
      "loss": 0.3822,
      "step": 885
    },
    {
      "epoch": 0.20136363636363636,
      "grad_norm": 0.055094704031944275,
      "learning_rate": 0.0001599089874857793,
      "loss": 0.2919,
      "step": 886
    },
    {
      "epoch": 0.2015909090909091,
      "grad_norm": 0.045493703335523605,
      "learning_rate": 0.00015986348122866894,
      "loss": 0.3401,
      "step": 887
    },
    {
      "epoch": 0.2018181818181818,
      "grad_norm": 0.04131355881690979,
      "learning_rate": 0.0001598179749715586,
      "loss": 0.3274,
      "step": 888
    },
    {
      "epoch": 0.20204545454545456,
      "grad_norm": 0.055312629789114,
      "learning_rate": 0.00015977246871444825,
      "loss": 0.3335,
      "step": 889
    },
    {
      "epoch": 0.20227272727272727,
      "grad_norm": 0.056070346385240555,
      "learning_rate": 0.0001597269624573379,
      "loss": 0.357,
      "step": 890
    },
    {
      "epoch": 0.2025,
      "grad_norm": 0.059025056660175323,
      "learning_rate": 0.00015968145620022753,
      "loss": 0.319,
      "step": 891
    },
    {
      "epoch": 0.20272727272727273,
      "grad_norm": 0.04636312276124954,
      "learning_rate": 0.00015963594994311718,
      "loss": 0.3459,
      "step": 892
    },
    {
      "epoch": 0.20295454545454544,
      "grad_norm": 0.05555397644639015,
      "learning_rate": 0.00015959044368600683,
      "loss": 0.3049,
      "step": 893
    },
    {
      "epoch": 0.20318181818181819,
      "grad_norm": 0.0608048290014267,
      "learning_rate": 0.0001595449374288965,
      "loss": 0.3941,
      "step": 894
    },
    {
      "epoch": 0.2034090909090909,
      "grad_norm": 0.06034574285149574,
      "learning_rate": 0.0001594994311717861,
      "loss": 0.349,
      "step": 895
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.06895223259925842,
      "learning_rate": 0.00015945392491467577,
      "loss": 0.4385,
      "step": 896
    },
    {
      "epoch": 0.20386363636363636,
      "grad_norm": 0.041755445301532745,
      "learning_rate": 0.00015940841865756542,
      "loss": 0.3093,
      "step": 897
    },
    {
      "epoch": 0.2040909090909091,
      "grad_norm": 0.03277130424976349,
      "learning_rate": 0.00015936291240045507,
      "loss": 0.2097,
      "step": 898
    },
    {
      "epoch": 0.20431818181818182,
      "grad_norm": 0.05316631495952606,
      "learning_rate": 0.00015931740614334472,
      "loss": 0.3337,
      "step": 899
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.06359098106622696,
      "learning_rate": 0.00015927189988623438,
      "loss": 0.3685,
      "step": 900
    },
    {
      "epoch": 0.20477272727272727,
      "grad_norm": 0.056648723781108856,
      "learning_rate": 0.000159226393629124,
      "loss": 0.3494,
      "step": 901
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.053680408746004105,
      "learning_rate": 0.00015918088737201366,
      "loss": 0.3899,
      "step": 902
    },
    {
      "epoch": 0.20522727272727273,
      "grad_norm": 0.03528129309415817,
      "learning_rate": 0.0001591353811149033,
      "loss": 0.2589,
      "step": 903
    },
    {
      "epoch": 0.20545454545454545,
      "grad_norm": 0.05127793177962303,
      "learning_rate": 0.00015908987485779296,
      "loss": 0.3752,
      "step": 904
    },
    {
      "epoch": 0.2056818181818182,
      "grad_norm": 0.05331303924322128,
      "learning_rate": 0.0001590443686006826,
      "loss": 0.3966,
      "step": 905
    },
    {
      "epoch": 0.2059090909090909,
      "grad_norm": 0.05976024642586708,
      "learning_rate": 0.00015899886234357224,
      "loss": 0.3929,
      "step": 906
    },
    {
      "epoch": 0.20613636363636365,
      "grad_norm": 0.04610550031065941,
      "learning_rate": 0.0001589533560864619,
      "loss": 0.3449,
      "step": 907
    },
    {
      "epoch": 0.20636363636363636,
      "grad_norm": 0.06312715262174606,
      "learning_rate": 0.00015890784982935155,
      "loss": 0.3916,
      "step": 908
    },
    {
      "epoch": 0.2065909090909091,
      "grad_norm": 0.05852748826146126,
      "learning_rate": 0.0001588623435722412,
      "loss": 0.3332,
      "step": 909
    },
    {
      "epoch": 0.20681818181818182,
      "grad_norm": 0.04625469446182251,
      "learning_rate": 0.00015881683731513085,
      "loss": 0.3047,
      "step": 910
    },
    {
      "epoch": 0.20704545454545453,
      "grad_norm": 0.05111689493060112,
      "learning_rate": 0.00015877133105802048,
      "loss": 0.3643,
      "step": 911
    },
    {
      "epoch": 0.20727272727272728,
      "grad_norm": 0.054213881492614746,
      "learning_rate": 0.00015872582480091013,
      "loss": 0.348,
      "step": 912
    },
    {
      "epoch": 0.2075,
      "grad_norm": 0.05889725685119629,
      "learning_rate": 0.00015868031854379978,
      "loss": 0.3688,
      "step": 913
    },
    {
      "epoch": 0.20772727272727273,
      "grad_norm": 0.052949026226997375,
      "learning_rate": 0.00015863481228668944,
      "loss": 0.3697,
      "step": 914
    },
    {
      "epoch": 0.20795454545454545,
      "grad_norm": 0.04342753812670708,
      "learning_rate": 0.00015858930602957906,
      "loss": 0.298,
      "step": 915
    },
    {
      "epoch": 0.2081818181818182,
      "grad_norm": 0.05068732798099518,
      "learning_rate": 0.00015854379977246871,
      "loss": 0.3229,
      "step": 916
    },
    {
      "epoch": 0.2084090909090909,
      "grad_norm": 0.05776228383183479,
      "learning_rate": 0.00015849829351535837,
      "loss": 0.2954,
      "step": 917
    },
    {
      "epoch": 0.20863636363636365,
      "grad_norm": 0.04241127520799637,
      "learning_rate": 0.00015845278725824802,
      "loss": 0.3066,
      "step": 918
    },
    {
      "epoch": 0.20886363636363636,
      "grad_norm": 0.03868207335472107,
      "learning_rate": 0.00015840728100113767,
      "loss": 0.2917,
      "step": 919
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.04559526965022087,
      "learning_rate": 0.00015836177474402732,
      "loss": 0.3064,
      "step": 920
    },
    {
      "epoch": 0.20931818181818182,
      "grad_norm": 0.06052743270993233,
      "learning_rate": 0.00015831626848691695,
      "loss": 0.3832,
      "step": 921
    },
    {
      "epoch": 0.20954545454545453,
      "grad_norm": 0.05789889395236969,
      "learning_rate": 0.0001582707622298066,
      "loss": 0.3652,
      "step": 922
    },
    {
      "epoch": 0.20977272727272728,
      "grad_norm": 0.04986319690942764,
      "learning_rate": 0.00015822525597269626,
      "loss": 0.3069,
      "step": 923
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.0647767037153244,
      "learning_rate": 0.00015817974971558588,
      "loss": 0.4152,
      "step": 924
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.04324861615896225,
      "learning_rate": 0.00015813424345847553,
      "loss": 0.2889,
      "step": 925
    },
    {
      "epoch": 0.21045454545454545,
      "grad_norm": 0.04548974707722664,
      "learning_rate": 0.0001580887372013652,
      "loss": 0.3581,
      "step": 926
    },
    {
      "epoch": 0.2106818181818182,
      "grad_norm": 0.04680611565709114,
      "learning_rate": 0.00015804323094425484,
      "loss": 0.2725,
      "step": 927
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.05331077799201012,
      "learning_rate": 0.0001579977246871445,
      "loss": 0.3239,
      "step": 928
    },
    {
      "epoch": 0.21113636363636365,
      "grad_norm": 0.052672598510980606,
      "learning_rate": 0.00015795221843003415,
      "loss": 0.3518,
      "step": 929
    },
    {
      "epoch": 0.21136363636363636,
      "grad_norm": 0.0477849617600441,
      "learning_rate": 0.0001579067121729238,
      "loss": 0.2939,
      "step": 930
    },
    {
      "epoch": 0.21159090909090908,
      "grad_norm": 0.05962635576725006,
      "learning_rate": 0.00015786120591581342,
      "loss": 0.3403,
      "step": 931
    },
    {
      "epoch": 0.21181818181818182,
      "grad_norm": 0.06152494624257088,
      "learning_rate": 0.00015781569965870308,
      "loss": 0.3945,
      "step": 932
    },
    {
      "epoch": 0.21204545454545454,
      "grad_norm": 0.0567784458398819,
      "learning_rate": 0.00015777019340159273,
      "loss": 0.3419,
      "step": 933
    },
    {
      "epoch": 0.21227272727272728,
      "grad_norm": 0.0490976981818676,
      "learning_rate": 0.00015772468714448236,
      "loss": 0.3435,
      "step": 934
    },
    {
      "epoch": 0.2125,
      "grad_norm": 0.04966757819056511,
      "learning_rate": 0.000157679180887372,
      "loss": 0.3454,
      "step": 935
    },
    {
      "epoch": 0.21272727272727274,
      "grad_norm": 0.05098139867186546,
      "learning_rate": 0.00015763367463026166,
      "loss": 0.3262,
      "step": 936
    },
    {
      "epoch": 0.21295454545454545,
      "grad_norm": 0.05843770503997803,
      "learning_rate": 0.00015758816837315131,
      "loss": 0.3345,
      "step": 937
    },
    {
      "epoch": 0.2131818181818182,
      "grad_norm": 0.06512696295976639,
      "learning_rate": 0.00015754266211604097,
      "loss": 0.3678,
      "step": 938
    },
    {
      "epoch": 0.2134090909090909,
      "grad_norm": 0.0481264591217041,
      "learning_rate": 0.00015749715585893062,
      "loss": 0.2799,
      "step": 939
    },
    {
      "epoch": 0.21363636363636362,
      "grad_norm": 0.04819418117403984,
      "learning_rate": 0.00015745164960182027,
      "loss": 0.3354,
      "step": 940
    },
    {
      "epoch": 0.21386363636363637,
      "grad_norm": 0.04326421394944191,
      "learning_rate": 0.0001574061433447099,
      "loss": 0.3363,
      "step": 941
    },
    {
      "epoch": 0.21409090909090908,
      "grad_norm": 0.050839614123106,
      "learning_rate": 0.00015736063708759955,
      "loss": 0.3668,
      "step": 942
    },
    {
      "epoch": 0.21431818181818182,
      "grad_norm": 0.040888406336307526,
      "learning_rate": 0.0001573151308304892,
      "loss": 0.2811,
      "step": 943
    },
    {
      "epoch": 0.21454545454545454,
      "grad_norm": 0.04507839307188988,
      "learning_rate": 0.00015726962457337883,
      "loss": 0.2988,
      "step": 944
    },
    {
      "epoch": 0.21477272727272728,
      "grad_norm": 0.04188751429319382,
      "learning_rate": 0.00015722411831626848,
      "loss": 0.2906,
      "step": 945
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.06098105013370514,
      "learning_rate": 0.00015717861205915814,
      "loss": 0.388,
      "step": 946
    },
    {
      "epoch": 0.21522727272727274,
      "grad_norm": 0.05698183551430702,
      "learning_rate": 0.0001571331058020478,
      "loss": 0.297,
      "step": 947
    },
    {
      "epoch": 0.21545454545454545,
      "grad_norm": 0.04502779617905617,
      "learning_rate": 0.00015708759954493744,
      "loss": 0.3098,
      "step": 948
    },
    {
      "epoch": 0.21568181818181817,
      "grad_norm": 0.054026734083890915,
      "learning_rate": 0.0001570420932878271,
      "loss": 0.3465,
      "step": 949
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.04816406965255737,
      "learning_rate": 0.00015699658703071675,
      "loss": 0.3329,
      "step": 950
    },
    {
      "epoch": 0.21613636363636363,
      "grad_norm": 0.0525628887116909,
      "learning_rate": 0.00015695108077360637,
      "loss": 0.3362,
      "step": 951
    },
    {
      "epoch": 0.21636363636363637,
      "grad_norm": 0.058627210557460785,
      "learning_rate": 0.00015690557451649603,
      "loss": 0.3547,
      "step": 952
    },
    {
      "epoch": 0.21659090909090908,
      "grad_norm": 0.041054271161556244,
      "learning_rate": 0.00015686006825938568,
      "loss": 0.2984,
      "step": 953
    },
    {
      "epoch": 0.21681818181818183,
      "grad_norm": 0.04346924647688866,
      "learning_rate": 0.0001568145620022753,
      "loss": 0.3295,
      "step": 954
    },
    {
      "epoch": 0.21704545454545454,
      "grad_norm": 0.03624286502599716,
      "learning_rate": 0.00015676905574516496,
      "loss": 0.2596,
      "step": 955
    },
    {
      "epoch": 0.21727272727272728,
      "grad_norm": 0.0634152740240097,
      "learning_rate": 0.0001567235494880546,
      "loss": 0.384,
      "step": 956
    },
    {
      "epoch": 0.2175,
      "grad_norm": 0.05100034549832344,
      "learning_rate": 0.00015667804323094426,
      "loss": 0.3307,
      "step": 957
    },
    {
      "epoch": 0.2177272727272727,
      "grad_norm": 0.0560111328959465,
      "learning_rate": 0.00015663253697383392,
      "loss": 0.3385,
      "step": 958
    },
    {
      "epoch": 0.21795454545454546,
      "grad_norm": 0.045753639191389084,
      "learning_rate": 0.00015658703071672357,
      "loss": 0.353,
      "step": 959
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.04823987931013107,
      "learning_rate": 0.00015654152445961322,
      "loss": 0.3478,
      "step": 960
    },
    {
      "epoch": 0.2184090909090909,
      "grad_norm": 0.04239252209663391,
      "learning_rate": 0.00015649601820250285,
      "loss": 0.272,
      "step": 961
    },
    {
      "epoch": 0.21863636363636363,
      "grad_norm": 0.05657105892896652,
      "learning_rate": 0.0001564505119453925,
      "loss": 0.366,
      "step": 962
    },
    {
      "epoch": 0.21886363636363637,
      "grad_norm": 0.04965642839670181,
      "learning_rate": 0.00015640500568828213,
      "loss": 0.3657,
      "step": 963
    },
    {
      "epoch": 0.2190909090909091,
      "grad_norm": 0.04545160010457039,
      "learning_rate": 0.00015635949943117178,
      "loss": 0.2943,
      "step": 964
    },
    {
      "epoch": 0.21931818181818183,
      "grad_norm": 0.045495323836803436,
      "learning_rate": 0.00015631399317406143,
      "loss": 0.3132,
      "step": 965
    },
    {
      "epoch": 0.21954545454545454,
      "grad_norm": 0.05670557916164398,
      "learning_rate": 0.00015626848691695108,
      "loss": 0.3827,
      "step": 966
    },
    {
      "epoch": 0.2197727272727273,
      "grad_norm": 0.05870869755744934,
      "learning_rate": 0.00015622298065984074,
      "loss": 0.3466,
      "step": 967
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.04699856787919998,
      "learning_rate": 0.0001561774744027304,
      "loss": 0.3696,
      "step": 968
    },
    {
      "epoch": 0.22022727272727272,
      "grad_norm": 0.06219085305929184,
      "learning_rate": 0.00015613196814562004,
      "loss": 0.3644,
      "step": 969
    },
    {
      "epoch": 0.22045454545454546,
      "grad_norm": 0.04454927518963814,
      "learning_rate": 0.0001560864618885097,
      "loss": 0.2882,
      "step": 970
    },
    {
      "epoch": 0.22068181818181817,
      "grad_norm": 0.05823848769068718,
      "learning_rate": 0.00015604095563139932,
      "loss": 0.4021,
      "step": 971
    },
    {
      "epoch": 0.22090909090909092,
      "grad_norm": 0.044798169285058975,
      "learning_rate": 0.00015599544937428897,
      "loss": 0.3059,
      "step": 972
    },
    {
      "epoch": 0.22113636363636363,
      "grad_norm": 0.04427289217710495,
      "learning_rate": 0.0001559499431171786,
      "loss": 0.3342,
      "step": 973
    },
    {
      "epoch": 0.22136363636363637,
      "grad_norm": 0.03685920685529709,
      "learning_rate": 0.00015590443686006825,
      "loss": 0.2713,
      "step": 974
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.050293974578380585,
      "learning_rate": 0.0001558589306029579,
      "loss": 0.2897,
      "step": 975
    },
    {
      "epoch": 0.22181818181818183,
      "grad_norm": 0.04682685062289238,
      "learning_rate": 0.00015581342434584756,
      "loss": 0.2948,
      "step": 976
    },
    {
      "epoch": 0.22204545454545455,
      "grad_norm": 0.04992052912712097,
      "learning_rate": 0.0001557679180887372,
      "loss": 0.3404,
      "step": 977
    },
    {
      "epoch": 0.22227272727272726,
      "grad_norm": 0.05936732888221741,
      "learning_rate": 0.00015572241183162686,
      "loss": 0.3874,
      "step": 978
    },
    {
      "epoch": 0.2225,
      "grad_norm": 0.04734231159090996,
      "learning_rate": 0.00015567690557451652,
      "loss": 0.3546,
      "step": 979
    },
    {
      "epoch": 0.22272727272727272,
      "grad_norm": 0.053730420768260956,
      "learning_rate": 0.00015563139931740617,
      "loss": 0.3635,
      "step": 980
    },
    {
      "epoch": 0.22295454545454546,
      "grad_norm": 0.05051266402006149,
      "learning_rate": 0.0001555858930602958,
      "loss": 0.2816,
      "step": 981
    },
    {
      "epoch": 0.22318181818181818,
      "grad_norm": 0.036892618983983994,
      "learning_rate": 0.00015554038680318545,
      "loss": 0.311,
      "step": 982
    },
    {
      "epoch": 0.22340909090909092,
      "grad_norm": 0.054335031658411026,
      "learning_rate": 0.00015549488054607507,
      "loss": 0.3426,
      "step": 983
    },
    {
      "epoch": 0.22363636363636363,
      "grad_norm": 0.06099246069788933,
      "learning_rate": 0.00015544937428896473,
      "loss": 0.3643,
      "step": 984
    },
    {
      "epoch": 0.22386363636363638,
      "grad_norm": 0.07359123229980469,
      "learning_rate": 0.00015540386803185438,
      "loss": 0.3189,
      "step": 985
    },
    {
      "epoch": 0.2240909090909091,
      "grad_norm": 0.07155158370733261,
      "learning_rate": 0.00015535836177474403,
      "loss": 0.3674,
      "step": 986
    },
    {
      "epoch": 0.2243181818181818,
      "grad_norm": 0.05691498517990112,
      "learning_rate": 0.00015531285551763368,
      "loss": 0.3602,
      "step": 987
    },
    {
      "epoch": 0.22454545454545455,
      "grad_norm": 0.05431467667222023,
      "learning_rate": 0.00015526734926052334,
      "loss": 0.339,
      "step": 988
    },
    {
      "epoch": 0.22477272727272726,
      "grad_norm": 0.06273798644542694,
      "learning_rate": 0.000155221843003413,
      "loss": 0.2772,
      "step": 989
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.05694238841533661,
      "learning_rate": 0.00015517633674630264,
      "loss": 0.3492,
      "step": 990
    },
    {
      "epoch": 0.22522727272727272,
      "grad_norm": 0.054709065705537796,
      "learning_rate": 0.00015513083048919227,
      "loss": 0.3209,
      "step": 991
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.05178690329194069,
      "learning_rate": 0.00015508532423208192,
      "loss": 0.3223,
      "step": 992
    },
    {
      "epoch": 0.22568181818181818,
      "grad_norm": 0.04903488606214523,
      "learning_rate": 0.00015503981797497155,
      "loss": 0.3554,
      "step": 993
    },
    {
      "epoch": 0.22590909090909092,
      "grad_norm": 0.046185534447431564,
      "learning_rate": 0.0001549943117178612,
      "loss": 0.3174,
      "step": 994
    },
    {
      "epoch": 0.22613636363636364,
      "grad_norm": 0.05120740458369255,
      "learning_rate": 0.00015494880546075085,
      "loss": 0.31,
      "step": 995
    },
    {
      "epoch": 0.22636363636363635,
      "grad_norm": 0.043780405074357986,
      "learning_rate": 0.0001549032992036405,
      "loss": 0.305,
      "step": 996
    },
    {
      "epoch": 0.2265909090909091,
      "grad_norm": 0.053988661617040634,
      "learning_rate": 0.00015485779294653016,
      "loss": 0.3716,
      "step": 997
    },
    {
      "epoch": 0.2268181818181818,
      "grad_norm": 0.039500489830970764,
      "learning_rate": 0.0001548122866894198,
      "loss": 0.2558,
      "step": 998
    },
    {
      "epoch": 0.22704545454545455,
      "grad_norm": 0.05082618072628975,
      "learning_rate": 0.00015476678043230946,
      "loss": 0.3413,
      "step": 999
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.06079257279634476,
      "learning_rate": 0.00015472127417519912,
      "loss": 0.3361,
      "step": 1000
    },
    {
      "epoch": 0.2275,
      "grad_norm": 0.05013292655348778,
      "learning_rate": 0.00015467576791808874,
      "loss": 0.3078,
      "step": 1001
    },
    {
      "epoch": 0.22772727272727272,
      "grad_norm": 0.04363971948623657,
      "learning_rate": 0.00015463026166097837,
      "loss": 0.3391,
      "step": 1002
    },
    {
      "epoch": 0.22795454545454547,
      "grad_norm": 0.0590062290430069,
      "learning_rate": 0.00015458475540386802,
      "loss": 0.332,
      "step": 1003
    },
    {
      "epoch": 0.22818181818181818,
      "grad_norm": 0.059424202889204025,
      "learning_rate": 0.00015453924914675767,
      "loss": 0.3816,
      "step": 1004
    },
    {
      "epoch": 0.22840909090909092,
      "grad_norm": 0.06013992056250572,
      "learning_rate": 0.00015449374288964733,
      "loss": 0.3919,
      "step": 1005
    },
    {
      "epoch": 0.22863636363636364,
      "grad_norm": 0.04826166108250618,
      "learning_rate": 0.00015444823663253698,
      "loss": 0.2615,
      "step": 1006
    },
    {
      "epoch": 0.22886363636363635,
      "grad_norm": 0.048552531749010086,
      "learning_rate": 0.00015440273037542663,
      "loss": 0.3334,
      "step": 1007
    },
    {
      "epoch": 0.2290909090909091,
      "grad_norm": 0.045169491320848465,
      "learning_rate": 0.00015435722411831629,
      "loss": 0.3023,
      "step": 1008
    },
    {
      "epoch": 0.2293181818181818,
      "grad_norm": 0.0485631488263607,
      "learning_rate": 0.00015431171786120594,
      "loss": 0.3695,
      "step": 1009
    },
    {
      "epoch": 0.22954545454545455,
      "grad_norm": 0.0538962259888649,
      "learning_rate": 0.0001542662116040956,
      "loss": 0.3889,
      "step": 1010
    },
    {
      "epoch": 0.22977272727272727,
      "grad_norm": 0.04937811195850372,
      "learning_rate": 0.00015422070534698522,
      "loss": 0.343,
      "step": 1011
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.04880243539810181,
      "learning_rate": 0.00015417519908987484,
      "loss": 0.3128,
      "step": 1012
    },
    {
      "epoch": 0.23022727272727272,
      "grad_norm": 0.05847138538956642,
      "learning_rate": 0.0001541296928327645,
      "loss": 0.4025,
      "step": 1013
    },
    {
      "epoch": 0.23045454545454547,
      "grad_norm": 0.07983632385730743,
      "learning_rate": 0.00015408418657565415,
      "loss": 0.4078,
      "step": 1014
    },
    {
      "epoch": 0.23068181818181818,
      "grad_norm": 0.05415288731455803,
      "learning_rate": 0.0001540386803185438,
      "loss": 0.3703,
      "step": 1015
    },
    {
      "epoch": 0.2309090909090909,
      "grad_norm": 0.0506913885474205,
      "learning_rate": 0.00015399317406143345,
      "loss": 0.3146,
      "step": 1016
    },
    {
      "epoch": 0.23113636363636364,
      "grad_norm": 0.05285130813717842,
      "learning_rate": 0.0001539476678043231,
      "loss": 0.3462,
      "step": 1017
    },
    {
      "epoch": 0.23136363636363635,
      "grad_norm": 0.06267892569303513,
      "learning_rate": 0.00015390216154721276,
      "loss": 0.3993,
      "step": 1018
    },
    {
      "epoch": 0.2315909090909091,
      "grad_norm": 0.0698784589767456,
      "learning_rate": 0.0001538566552901024,
      "loss": 0.4267,
      "step": 1019
    },
    {
      "epoch": 0.2318181818181818,
      "grad_norm": 0.05869384855031967,
      "learning_rate": 0.00015381114903299206,
      "loss": 0.3617,
      "step": 1020
    },
    {
      "epoch": 0.23204545454545455,
      "grad_norm": 0.04601072147488594,
      "learning_rate": 0.0001537656427758817,
      "loss": 0.3331,
      "step": 1021
    },
    {
      "epoch": 0.23227272727272727,
      "grad_norm": 0.042078327387571335,
      "learning_rate": 0.00015372013651877132,
      "loss": 0.3111,
      "step": 1022
    },
    {
      "epoch": 0.2325,
      "grad_norm": 0.08189074695110321,
      "learning_rate": 0.00015367463026166097,
      "loss": 0.4173,
      "step": 1023
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.06542783230543137,
      "learning_rate": 0.00015362912400455062,
      "loss": 0.3117,
      "step": 1024
    },
    {
      "epoch": 0.23295454545454544,
      "grad_norm": 0.05313609167933464,
      "learning_rate": 0.00015358361774744027,
      "loss": 0.3277,
      "step": 1025
    },
    {
      "epoch": 0.23318181818181818,
      "grad_norm": 0.050780221819877625,
      "learning_rate": 0.00015353811149032993,
      "loss": 0.2634,
      "step": 1026
    },
    {
      "epoch": 0.2334090909090909,
      "grad_norm": 0.0648922249674797,
      "learning_rate": 0.00015349260523321958,
      "loss": 0.3873,
      "step": 1027
    },
    {
      "epoch": 0.23363636363636364,
      "grad_norm": 0.08010167628526688,
      "learning_rate": 0.00015344709897610923,
      "loss": 0.3229,
      "step": 1028
    },
    {
      "epoch": 0.23386363636363636,
      "grad_norm": 0.07082549482584,
      "learning_rate": 0.00015340159271899889,
      "loss": 0.3874,
      "step": 1029
    },
    {
      "epoch": 0.2340909090909091,
      "grad_norm": 0.06068166345357895,
      "learning_rate": 0.00015335608646188854,
      "loss": 0.3332,
      "step": 1030
    },
    {
      "epoch": 0.23431818181818181,
      "grad_norm": 0.0527944341301918,
      "learning_rate": 0.00015331058020477816,
      "loss": 0.291,
      "step": 1031
    },
    {
      "epoch": 0.23454545454545456,
      "grad_norm": 0.04793271794915199,
      "learning_rate": 0.0001532650739476678,
      "loss": 0.3447,
      "step": 1032
    },
    {
      "epoch": 0.23477272727272727,
      "grad_norm": 0.05885392054915428,
      "learning_rate": 0.00015321956769055744,
      "loss": 0.3807,
      "step": 1033
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.059015512466430664,
      "learning_rate": 0.0001531740614334471,
      "loss": 0.3502,
      "step": 1034
    },
    {
      "epoch": 0.23522727272727273,
      "grad_norm": 0.06202736124396324,
      "learning_rate": 0.00015312855517633675,
      "loss": 0.3553,
      "step": 1035
    },
    {
      "epoch": 0.23545454545454544,
      "grad_norm": 0.055277980864048004,
      "learning_rate": 0.0001530830489192264,
      "loss": 0.3812,
      "step": 1036
    },
    {
      "epoch": 0.2356818181818182,
      "grad_norm": 0.04796876013278961,
      "learning_rate": 0.00015303754266211605,
      "loss": 0.2671,
      "step": 1037
    },
    {
      "epoch": 0.2359090909090909,
      "grad_norm": 0.04226892814040184,
      "learning_rate": 0.0001529920364050057,
      "loss": 0.3258,
      "step": 1038
    },
    {
      "epoch": 0.23613636363636364,
      "grad_norm": 0.0440179705619812,
      "learning_rate": 0.00015294653014789536,
      "loss": 0.3181,
      "step": 1039
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.06063133478164673,
      "learning_rate": 0.000152901023890785,
      "loss": 0.3001,
      "step": 1040
    },
    {
      "epoch": 0.2365909090909091,
      "grad_norm": 0.04878731071949005,
      "learning_rate": 0.00015285551763367464,
      "loss": 0.3551,
      "step": 1041
    },
    {
      "epoch": 0.23681818181818182,
      "grad_norm": 0.05834653601050377,
      "learning_rate": 0.00015281001137656426,
      "loss": 0.3894,
      "step": 1042
    },
    {
      "epoch": 0.23704545454545456,
      "grad_norm": 0.06465832144021988,
      "learning_rate": 0.00015276450511945392,
      "loss": 0.3759,
      "step": 1043
    },
    {
      "epoch": 0.23727272727272727,
      "grad_norm": 0.04606267437338829,
      "learning_rate": 0.00015271899886234357,
      "loss": 0.2645,
      "step": 1044
    },
    {
      "epoch": 0.2375,
      "grad_norm": 0.050115540623664856,
      "learning_rate": 0.00015267349260523322,
      "loss": 0.2911,
      "step": 1045
    },
    {
      "epoch": 0.23772727272727273,
      "grad_norm": 0.05919560417532921,
      "learning_rate": 0.00015262798634812288,
      "loss": 0.3925,
      "step": 1046
    },
    {
      "epoch": 0.23795454545454545,
      "grad_norm": 0.06500229984521866,
      "learning_rate": 0.00015258248009101253,
      "loss": 0.3703,
      "step": 1047
    },
    {
      "epoch": 0.2381818181818182,
      "grad_norm": 0.061262816190719604,
      "learning_rate": 0.00015253697383390218,
      "loss": 0.2929,
      "step": 1048
    },
    {
      "epoch": 0.2384090909090909,
      "grad_norm": 0.0543428473174572,
      "learning_rate": 0.00015249146757679183,
      "loss": 0.3171,
      "step": 1049
    },
    {
      "epoch": 0.23863636363636365,
      "grad_norm": 0.044818125665187836,
      "learning_rate": 0.0001524459613196815,
      "loss": 0.3147,
      "step": 1050
    },
    {
      "epoch": 0.23886363636363636,
      "grad_norm": 0.052678219974040985,
      "learning_rate": 0.0001524004550625711,
      "loss": 0.3417,
      "step": 1051
    },
    {
      "epoch": 0.2390909090909091,
      "grad_norm": 0.04124799370765686,
      "learning_rate": 0.00015235494880546074,
      "loss": 0.2954,
      "step": 1052
    },
    {
      "epoch": 0.23931818181818182,
      "grad_norm": 0.05589941889047623,
      "learning_rate": 0.0001523094425483504,
      "loss": 0.3193,
      "step": 1053
    },
    {
      "epoch": 0.23954545454545453,
      "grad_norm": 0.07528873533010483,
      "learning_rate": 0.00015226393629124004,
      "loss": 0.3258,
      "step": 1054
    },
    {
      "epoch": 0.23977272727272728,
      "grad_norm": 0.0785321295261383,
      "learning_rate": 0.0001522184300341297,
      "loss": 0.3676,
      "step": 1055
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.052604399621486664,
      "learning_rate": 0.00015217292377701935,
      "loss": 0.3898,
      "step": 1056
    },
    {
      "epoch": 0.24022727272727273,
      "grad_norm": 0.05303303152322769,
      "learning_rate": 0.000152127417519909,
      "loss": 0.3033,
      "step": 1057
    },
    {
      "epoch": 0.24045454545454545,
      "grad_norm": 0.0414179190993309,
      "learning_rate": 0.00015208191126279865,
      "loss": 0.3166,
      "step": 1058
    },
    {
      "epoch": 0.2406818181818182,
      "grad_norm": 0.05693769082427025,
      "learning_rate": 0.0001520364050056883,
      "loss": 0.3271,
      "step": 1059
    },
    {
      "epoch": 0.2409090909090909,
      "grad_norm": 0.04768839478492737,
      "learning_rate": 0.00015199089874857793,
      "loss": 0.3382,
      "step": 1060
    },
    {
      "epoch": 0.24113636363636365,
      "grad_norm": 0.05825741961598396,
      "learning_rate": 0.00015194539249146759,
      "loss": 0.3771,
      "step": 1061
    },
    {
      "epoch": 0.24136363636363636,
      "grad_norm": 0.05580364540219307,
      "learning_rate": 0.0001518998862343572,
      "loss": 0.355,
      "step": 1062
    },
    {
      "epoch": 0.24159090909090908,
      "grad_norm": 0.03830614313483238,
      "learning_rate": 0.00015185437997724686,
      "loss": 0.2349,
      "step": 1063
    },
    {
      "epoch": 0.24181818181818182,
      "grad_norm": 0.05586633458733559,
      "learning_rate": 0.00015180887372013652,
      "loss": 0.3647,
      "step": 1064
    },
    {
      "epoch": 0.24204545454545454,
      "grad_norm": 0.055350128561258316,
      "learning_rate": 0.00015176336746302617,
      "loss": 0.2963,
      "step": 1065
    },
    {
      "epoch": 0.24227272727272728,
      "grad_norm": 0.05506625398993492,
      "learning_rate": 0.00015171786120591582,
      "loss": 0.3365,
      "step": 1066
    },
    {
      "epoch": 0.2425,
      "grad_norm": 0.05884145572781563,
      "learning_rate": 0.00015167235494880548,
      "loss": 0.3632,
      "step": 1067
    },
    {
      "epoch": 0.24272727272727274,
      "grad_norm": 0.04856441915035248,
      "learning_rate": 0.00015162684869169513,
      "loss": 0.329,
      "step": 1068
    },
    {
      "epoch": 0.24295454545454545,
      "grad_norm": 0.08892274647951126,
      "learning_rate": 0.00015158134243458478,
      "loss": 0.428,
      "step": 1069
    },
    {
      "epoch": 0.2431818181818182,
      "grad_norm": 0.06816234439611435,
      "learning_rate": 0.0001515358361774744,
      "loss": 0.3749,
      "step": 1070
    },
    {
      "epoch": 0.2434090909090909,
      "grad_norm": 0.04765118658542633,
      "learning_rate": 0.00015149032992036406,
      "loss": 0.3192,
      "step": 1071
    },
    {
      "epoch": 0.24363636363636362,
      "grad_norm": 0.05578562617301941,
      "learning_rate": 0.00015144482366325369,
      "loss": 0.3449,
      "step": 1072
    },
    {
      "epoch": 0.24386363636363637,
      "grad_norm": 0.046164218336343765,
      "learning_rate": 0.00015139931740614334,
      "loss": 0.3197,
      "step": 1073
    },
    {
      "epoch": 0.24409090909090908,
      "grad_norm": 0.05042706057429314,
      "learning_rate": 0.000151353811149033,
      "loss": 0.3603,
      "step": 1074
    },
    {
      "epoch": 0.24431818181818182,
      "grad_norm": 0.043461401015520096,
      "learning_rate": 0.00015130830489192264,
      "loss": 0.2908,
      "step": 1075
    },
    {
      "epoch": 0.24454545454545454,
      "grad_norm": 0.03743657469749451,
      "learning_rate": 0.0001512627986348123,
      "loss": 0.2585,
      "step": 1076
    },
    {
      "epoch": 0.24477272727272728,
      "grad_norm": 0.04241703450679779,
      "learning_rate": 0.00015121729237770195,
      "loss": 0.274,
      "step": 1077
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.051255181431770325,
      "learning_rate": 0.0001511717861205916,
      "loss": 0.3262,
      "step": 1078
    },
    {
      "epoch": 0.24522727272727274,
      "grad_norm": 0.05226384103298187,
      "learning_rate": 0.00015112627986348126,
      "loss": 0.3359,
      "step": 1079
    },
    {
      "epoch": 0.24545454545454545,
      "grad_norm": 0.09051525592803955,
      "learning_rate": 0.00015108077360637088,
      "loss": 0.4714,
      "step": 1080
    },
    {
      "epoch": 0.2456818181818182,
      "grad_norm": 0.050082024186849594,
      "learning_rate": 0.00015103526734926053,
      "loss": 0.3277,
      "step": 1081
    },
    {
      "epoch": 0.2459090909090909,
      "grad_norm": 0.054801762104034424,
      "learning_rate": 0.00015098976109215016,
      "loss": 0.3421,
      "step": 1082
    },
    {
      "epoch": 0.24613636363636363,
      "grad_norm": 0.05244303122162819,
      "learning_rate": 0.0001509442548350398,
      "loss": 0.4058,
      "step": 1083
    },
    {
      "epoch": 0.24636363636363637,
      "grad_norm": 0.05213812366127968,
      "learning_rate": 0.00015089874857792947,
      "loss": 0.3355,
      "step": 1084
    },
    {
      "epoch": 0.24659090909090908,
      "grad_norm": 0.05328328534960747,
      "learning_rate": 0.00015085324232081912,
      "loss": 0.3589,
      "step": 1085
    },
    {
      "epoch": 0.24681818181818183,
      "grad_norm": 0.05394173786044121,
      "learning_rate": 0.00015080773606370877,
      "loss": 0.3896,
      "step": 1086
    },
    {
      "epoch": 0.24704545454545454,
      "grad_norm": 0.04924602061510086,
      "learning_rate": 0.00015076222980659842,
      "loss": 0.3477,
      "step": 1087
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 0.048418715596199036,
      "learning_rate": 0.00015071672354948808,
      "loss": 0.3476,
      "step": 1088
    },
    {
      "epoch": 0.2475,
      "grad_norm": 0.08172371983528137,
      "learning_rate": 0.00015067121729237773,
      "loss": 0.3811,
      "step": 1089
    },
    {
      "epoch": 0.24772727272727274,
      "grad_norm": 0.056571345776319504,
      "learning_rate": 0.00015062571103526736,
      "loss": 0.3667,
      "step": 1090
    },
    {
      "epoch": 0.24795454545454546,
      "grad_norm": 0.05969979241490364,
      "learning_rate": 0.000150580204778157,
      "loss": 0.3495,
      "step": 1091
    },
    {
      "epoch": 0.24818181818181817,
      "grad_norm": 0.060296300798654556,
      "learning_rate": 0.00015053469852104663,
      "loss": 0.363,
      "step": 1092
    },
    {
      "epoch": 0.2484090909090909,
      "grad_norm": 0.05077523738145828,
      "learning_rate": 0.0001504891922639363,
      "loss": 0.3097,
      "step": 1093
    },
    {
      "epoch": 0.24863636363636363,
      "grad_norm": 0.05029234662652016,
      "learning_rate": 0.00015044368600682594,
      "loss": 0.3106,
      "step": 1094
    },
    {
      "epoch": 0.24886363636363637,
      "grad_norm": 0.06396473199129105,
      "learning_rate": 0.0001503981797497156,
      "loss": 0.3483,
      "step": 1095
    },
    {
      "epoch": 0.24909090909090909,
      "grad_norm": 0.05353758484125137,
      "learning_rate": 0.00015035267349260525,
      "loss": 0.3474,
      "step": 1096
    },
    {
      "epoch": 0.24931818181818183,
      "grad_norm": 0.058167051523923874,
      "learning_rate": 0.0001503071672354949,
      "loss": 0.3722,
      "step": 1097
    },
    {
      "epoch": 0.24954545454545454,
      "grad_norm": 0.04708968847990036,
      "learning_rate": 0.00015026166097838455,
      "loss": 0.3317,
      "step": 1098
    },
    {
      "epoch": 0.24977272727272729,
      "grad_norm": 0.05889478325843811,
      "learning_rate": 0.00015021615472127418,
      "loss": 0.3906,
      "step": 1099
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.045746952295303345,
      "learning_rate": 0.00015017064846416383,
      "loss": 0.2796,
      "step": 1100
    },
    {
      "epoch": 0.25022727272727274,
      "grad_norm": 0.04402293264865875,
      "learning_rate": 0.00015012514220705348,
      "loss": 0.3461,
      "step": 1101
    },
    {
      "epoch": 0.25045454545454543,
      "grad_norm": 0.04867814853787422,
      "learning_rate": 0.0001500796359499431,
      "loss": 0.3848,
      "step": 1102
    },
    {
      "epoch": 0.2506818181818182,
      "grad_norm": 0.04142260551452637,
      "learning_rate": 0.00015003412969283276,
      "loss": 0.3186,
      "step": 1103
    },
    {
      "epoch": 0.2509090909090909,
      "grad_norm": 0.045895904302597046,
      "learning_rate": 0.00014998862343572241,
      "loss": 0.3197,
      "step": 1104
    },
    {
      "epoch": 0.25113636363636366,
      "grad_norm": 0.05524754151701927,
      "learning_rate": 0.00014994311717861207,
      "loss": 0.3838,
      "step": 1105
    },
    {
      "epoch": 0.25136363636363634,
      "grad_norm": 0.05364646390080452,
      "learning_rate": 0.00014989761092150172,
      "loss": 0.359,
      "step": 1106
    },
    {
      "epoch": 0.2515909090909091,
      "grad_norm": 0.04102576896548271,
      "learning_rate": 0.00014985210466439137,
      "loss": 0.3113,
      "step": 1107
    },
    {
      "epoch": 0.25181818181818183,
      "grad_norm": 0.032144833356142044,
      "learning_rate": 0.00014980659840728102,
      "loss": 0.2898,
      "step": 1108
    },
    {
      "epoch": 0.2520454545454546,
      "grad_norm": 0.04504949599504471,
      "learning_rate": 0.00014976109215017065,
      "loss": 0.3162,
      "step": 1109
    },
    {
      "epoch": 0.25227272727272726,
      "grad_norm": 0.041284263134002686,
      "learning_rate": 0.0001497155858930603,
      "loss": 0.3182,
      "step": 1110
    },
    {
      "epoch": 0.2525,
      "grad_norm": 0.05003737658262253,
      "learning_rate": 0.00014967007963594996,
      "loss": 0.2917,
      "step": 1111
    },
    {
      "epoch": 0.25272727272727274,
      "grad_norm": 0.03576623648405075,
      "learning_rate": 0.00014962457337883958,
      "loss": 0.247,
      "step": 1112
    },
    {
      "epoch": 0.25295454545454543,
      "grad_norm": 0.0486665815114975,
      "learning_rate": 0.00014957906712172923,
      "loss": 0.3664,
      "step": 1113
    },
    {
      "epoch": 0.2531818181818182,
      "grad_norm": 0.048033472150564194,
      "learning_rate": 0.0001495335608646189,
      "loss": 0.2872,
      "step": 1114
    },
    {
      "epoch": 0.2534090909090909,
      "grad_norm": 0.04024321213364601,
      "learning_rate": 0.00014948805460750854,
      "loss": 0.3364,
      "step": 1115
    },
    {
      "epoch": 0.25363636363636366,
      "grad_norm": 0.04600448161363602,
      "learning_rate": 0.0001494425483503982,
      "loss": 0.2905,
      "step": 1116
    },
    {
      "epoch": 0.25386363636363635,
      "grad_norm": 0.041853953152894974,
      "learning_rate": 0.00014939704209328785,
      "loss": 0.3577,
      "step": 1117
    },
    {
      "epoch": 0.2540909090909091,
      "grad_norm": 0.044177304953336716,
      "learning_rate": 0.0001493515358361775,
      "loss": 0.35,
      "step": 1118
    },
    {
      "epoch": 0.25431818181818183,
      "grad_norm": 0.04555118829011917,
      "learning_rate": 0.00014930602957906712,
      "loss": 0.3286,
      "step": 1119
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.04290781170129776,
      "learning_rate": 0.00014926052332195678,
      "loss": 0.3207,
      "step": 1120
    },
    {
      "epoch": 0.25477272727272726,
      "grad_norm": 0.055047497153282166,
      "learning_rate": 0.00014921501706484643,
      "loss": 0.367,
      "step": 1121
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.06535155326128006,
      "learning_rate": 0.00014916951080773606,
      "loss": 0.3835,
      "step": 1122
    },
    {
      "epoch": 0.25522727272727275,
      "grad_norm": 0.050651516765356064,
      "learning_rate": 0.0001491240045506257,
      "loss": 0.3627,
      "step": 1123
    },
    {
      "epoch": 0.25545454545454543,
      "grad_norm": 0.041194867342710495,
      "learning_rate": 0.00014907849829351536,
      "loss": 0.2692,
      "step": 1124
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 0.04668048396706581,
      "learning_rate": 0.00014903299203640501,
      "loss": 0.3109,
      "step": 1125
    },
    {
      "epoch": 0.2559090909090909,
      "grad_norm": 0.04562264680862427,
      "learning_rate": 0.00014898748577929467,
      "loss": 0.277,
      "step": 1126
    },
    {
      "epoch": 0.25613636363636366,
      "grad_norm": 0.05330352112650871,
      "learning_rate": 0.00014894197952218432,
      "loss": 0.3627,
      "step": 1127
    },
    {
      "epoch": 0.25636363636363635,
      "grad_norm": 0.05581072345376015,
      "learning_rate": 0.00014889647326507397,
      "loss": 0.4077,
      "step": 1128
    },
    {
      "epoch": 0.2565909090909091,
      "grad_norm": 0.05012259632349014,
      "learning_rate": 0.0001488509670079636,
      "loss": 0.3697,
      "step": 1129
    },
    {
      "epoch": 0.25681818181818183,
      "grad_norm": 0.04436125606298447,
      "learning_rate": 0.00014880546075085325,
      "loss": 0.3512,
      "step": 1130
    },
    {
      "epoch": 0.2570454545454545,
      "grad_norm": 0.059555381536483765,
      "learning_rate": 0.0001487599544937429,
      "loss": 0.316,
      "step": 1131
    },
    {
      "epoch": 0.25727272727272726,
      "grad_norm": 0.049308035522699356,
      "learning_rate": 0.00014871444823663253,
      "loss": 0.3694,
      "step": 1132
    },
    {
      "epoch": 0.2575,
      "grad_norm": 0.03801034390926361,
      "learning_rate": 0.00014866894197952218,
      "loss": 0.2907,
      "step": 1133
    },
    {
      "epoch": 0.25772727272727275,
      "grad_norm": 0.03770895674824715,
      "learning_rate": 0.00014862343572241184,
      "loss": 0.2867,
      "step": 1134
    },
    {
      "epoch": 0.25795454545454544,
      "grad_norm": 0.054651517421007156,
      "learning_rate": 0.0001485779294653015,
      "loss": 0.3837,
      "step": 1135
    },
    {
      "epoch": 0.2581818181818182,
      "grad_norm": 0.06270774453878403,
      "learning_rate": 0.00014853242320819114,
      "loss": 0.3749,
      "step": 1136
    },
    {
      "epoch": 0.2584090909090909,
      "grad_norm": 0.048792459070682526,
      "learning_rate": 0.0001484869169510808,
      "loss": 0.3001,
      "step": 1137
    },
    {
      "epoch": 0.2586363636363636,
      "grad_norm": 0.041021741926670074,
      "learning_rate": 0.00014844141069397042,
      "loss": 0.2964,
      "step": 1138
    },
    {
      "epoch": 0.25886363636363635,
      "grad_norm": 0.06738507747650146,
      "learning_rate": 0.00014839590443686007,
      "loss": 0.4465,
      "step": 1139
    },
    {
      "epoch": 0.2590909090909091,
      "grad_norm": 0.04274272173643112,
      "learning_rate": 0.00014835039817974973,
      "loss": 0.3229,
      "step": 1140
    },
    {
      "epoch": 0.25931818181818184,
      "grad_norm": 0.05912815034389496,
      "learning_rate": 0.00014830489192263938,
      "loss": 0.3694,
      "step": 1141
    },
    {
      "epoch": 0.2595454545454545,
      "grad_norm": 0.059695400297641754,
      "learning_rate": 0.000148259385665529,
      "loss": 0.387,
      "step": 1142
    },
    {
      "epoch": 0.25977272727272727,
      "grad_norm": 0.053633712232112885,
      "learning_rate": 0.00014821387940841866,
      "loss": 0.4012,
      "step": 1143
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.07314980030059814,
      "learning_rate": 0.0001481683731513083,
      "loss": 0.4075,
      "step": 1144
    },
    {
      "epoch": 0.26022727272727275,
      "grad_norm": 0.041344329714775085,
      "learning_rate": 0.00014812286689419796,
      "loss": 0.2776,
      "step": 1145
    },
    {
      "epoch": 0.26045454545454544,
      "grad_norm": 0.06068355590105057,
      "learning_rate": 0.00014807736063708762,
      "loss": 0.3356,
      "step": 1146
    },
    {
      "epoch": 0.2606818181818182,
      "grad_norm": 0.046033598482608795,
      "learning_rate": 0.00014803185437997727,
      "loss": 0.3153,
      "step": 1147
    },
    {
      "epoch": 0.2609090909090909,
      "grad_norm": 0.04499511420726776,
      "learning_rate": 0.0001479863481228669,
      "loss": 0.3013,
      "step": 1148
    },
    {
      "epoch": 0.2611363636363636,
      "grad_norm": 0.05874022841453552,
      "learning_rate": 0.00014794084186575655,
      "loss": 0.3464,
      "step": 1149
    },
    {
      "epoch": 0.26136363636363635,
      "grad_norm": 0.05134930834174156,
      "learning_rate": 0.0001478953356086462,
      "loss": 0.3342,
      "step": 1150
    },
    {
      "epoch": 0.2615909090909091,
      "grad_norm": 0.04622047767043114,
      "learning_rate": 0.00014784982935153585,
      "loss": 0.3066,
      "step": 1151
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.04532278701663017,
      "learning_rate": 0.00014780432309442548,
      "loss": 0.3182,
      "step": 1152
    },
    {
      "epoch": 0.2620454545454545,
      "grad_norm": 0.037825122475624084,
      "learning_rate": 0.00014775881683731513,
      "loss": 0.2931,
      "step": 1153
    },
    {
      "epoch": 0.26227272727272727,
      "grad_norm": 0.0537375845015049,
      "learning_rate": 0.00014771331058020478,
      "loss": 0.3451,
      "step": 1154
    },
    {
      "epoch": 0.2625,
      "grad_norm": 0.051331039518117905,
      "learning_rate": 0.00014766780432309444,
      "loss": 0.3192,
      "step": 1155
    },
    {
      "epoch": 0.26272727272727275,
      "grad_norm": 0.05420779064297676,
      "learning_rate": 0.0001476222980659841,
      "loss": 0.3466,
      "step": 1156
    },
    {
      "epoch": 0.26295454545454544,
      "grad_norm": 0.046380579471588135,
      "learning_rate": 0.00014757679180887374,
      "loss": 0.3325,
      "step": 1157
    },
    {
      "epoch": 0.2631818181818182,
      "grad_norm": 0.04233935475349426,
      "learning_rate": 0.00014753128555176337,
      "loss": 0.3163,
      "step": 1158
    },
    {
      "epoch": 0.2634090909090909,
      "grad_norm": 0.05084409564733505,
      "learning_rate": 0.00014748577929465302,
      "loss": 0.315,
      "step": 1159
    },
    {
      "epoch": 0.2636363636363636,
      "grad_norm": 0.041118815541267395,
      "learning_rate": 0.00014744027303754267,
      "loss": 0.2814,
      "step": 1160
    },
    {
      "epoch": 0.26386363636363636,
      "grad_norm": 0.03312254697084427,
      "learning_rate": 0.00014739476678043233,
      "loss": 0.2554,
      "step": 1161
    },
    {
      "epoch": 0.2640909090909091,
      "grad_norm": 0.0485900416970253,
      "learning_rate": 0.00014734926052332195,
      "loss": 0.3051,
      "step": 1162
    },
    {
      "epoch": 0.26431818181818184,
      "grad_norm": 0.03773272782564163,
      "learning_rate": 0.0001473037542662116,
      "loss": 0.2859,
      "step": 1163
    },
    {
      "epoch": 0.26454545454545453,
      "grad_norm": 0.05091146379709244,
      "learning_rate": 0.00014725824800910126,
      "loss": 0.2715,
      "step": 1164
    },
    {
      "epoch": 0.26477272727272727,
      "grad_norm": 0.06060809642076492,
      "learning_rate": 0.0001472127417519909,
      "loss": 0.3333,
      "step": 1165
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.048716217279434204,
      "learning_rate": 0.00014716723549488056,
      "loss": 0.3394,
      "step": 1166
    },
    {
      "epoch": 0.2652272727272727,
      "grad_norm": 0.052615631371736526,
      "learning_rate": 0.0001471217292377702,
      "loss": 0.3414,
      "step": 1167
    },
    {
      "epoch": 0.26545454545454544,
      "grad_norm": 0.04602702707052231,
      "learning_rate": 0.00014707622298065984,
      "loss": 0.3217,
      "step": 1168
    },
    {
      "epoch": 0.2656818181818182,
      "grad_norm": 0.05358685553073883,
      "learning_rate": 0.0001470307167235495,
      "loss": 0.3806,
      "step": 1169
    },
    {
      "epoch": 0.26590909090909093,
      "grad_norm": 0.04710347577929497,
      "learning_rate": 0.00014698521046643915,
      "loss": 0.3384,
      "step": 1170
    },
    {
      "epoch": 0.2661363636363636,
      "grad_norm": 0.05517728999257088,
      "learning_rate": 0.0001469397042093288,
      "loss": 0.368,
      "step": 1171
    },
    {
      "epoch": 0.26636363636363636,
      "grad_norm": 0.04307381436228752,
      "learning_rate": 0.00014689419795221843,
      "loss": 0.2523,
      "step": 1172
    },
    {
      "epoch": 0.2665909090909091,
      "grad_norm": 0.04835224896669388,
      "learning_rate": 0.00014684869169510808,
      "loss": 0.3406,
      "step": 1173
    },
    {
      "epoch": 0.26681818181818184,
      "grad_norm": 0.04644716903567314,
      "learning_rate": 0.00014680318543799773,
      "loss": 0.3184,
      "step": 1174
    },
    {
      "epoch": 0.26704545454545453,
      "grad_norm": 0.04612021520733833,
      "learning_rate": 0.00014675767918088738,
      "loss": 0.3252,
      "step": 1175
    },
    {
      "epoch": 0.2672727272727273,
      "grad_norm": 0.060614388436079025,
      "learning_rate": 0.00014671217292377704,
      "loss": 0.3165,
      "step": 1176
    },
    {
      "epoch": 0.2675,
      "grad_norm": 0.04389652609825134,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.2728,
      "step": 1177
    },
    {
      "epoch": 0.2677272727272727,
      "grad_norm": 0.04840393364429474,
      "learning_rate": 0.00014662116040955632,
      "loss": 0.3721,
      "step": 1178
    },
    {
      "epoch": 0.26795454545454545,
      "grad_norm": 0.04685106873512268,
      "learning_rate": 0.00014657565415244597,
      "loss": 0.2915,
      "step": 1179
    },
    {
      "epoch": 0.2681818181818182,
      "grad_norm": 0.03584365174174309,
      "learning_rate": 0.00014653014789533562,
      "loss": 0.3065,
      "step": 1180
    },
    {
      "epoch": 0.26840909090909093,
      "grad_norm": 0.05087418854236603,
      "learning_rate": 0.00014648464163822527,
      "loss": 0.3345,
      "step": 1181
    },
    {
      "epoch": 0.2686363636363636,
      "grad_norm": 0.051312752068042755,
      "learning_rate": 0.0001464391353811149,
      "loss": 0.3788,
      "step": 1182
    },
    {
      "epoch": 0.26886363636363636,
      "grad_norm": 0.05034313723444939,
      "learning_rate": 0.00014639362912400455,
      "loss": 0.3097,
      "step": 1183
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.05879879742860794,
      "learning_rate": 0.0001463481228668942,
      "loss": 0.3934,
      "step": 1184
    },
    {
      "epoch": 0.26931818181818185,
      "grad_norm": 0.059171147644519806,
      "learning_rate": 0.00014630261660978386,
      "loss": 0.3472,
      "step": 1185
    },
    {
      "epoch": 0.26954545454545453,
      "grad_norm": 0.05561733618378639,
      "learning_rate": 0.0001462571103526735,
      "loss": 0.3377,
      "step": 1186
    },
    {
      "epoch": 0.2697727272727273,
      "grad_norm": 0.04619526118040085,
      "learning_rate": 0.00014621160409556314,
      "loss": 0.3038,
      "step": 1187
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.05390362814068794,
      "learning_rate": 0.0001461660978384528,
      "loss": 0.3618,
      "step": 1188
    },
    {
      "epoch": 0.2702272727272727,
      "grad_norm": 0.05159072205424309,
      "learning_rate": 0.00014612059158134244,
      "loss": 0.391,
      "step": 1189
    },
    {
      "epoch": 0.27045454545454545,
      "grad_norm": 0.05180025473237038,
      "learning_rate": 0.0001460750853242321,
      "loss": 0.3632,
      "step": 1190
    },
    {
      "epoch": 0.2706818181818182,
      "grad_norm": 0.06503872573375702,
      "learning_rate": 0.00014602957906712175,
      "loss": 0.3572,
      "step": 1191
    },
    {
      "epoch": 0.27090909090909093,
      "grad_norm": 0.059967607259750366,
      "learning_rate": 0.00014598407281001137,
      "loss": 0.4038,
      "step": 1192
    },
    {
      "epoch": 0.2711363636363636,
      "grad_norm": 0.037988413125276566,
      "learning_rate": 0.00014593856655290103,
      "loss": 0.2597,
      "step": 1193
    },
    {
      "epoch": 0.27136363636363636,
      "grad_norm": 0.06584204733371735,
      "learning_rate": 0.00014589306029579068,
      "loss": 0.4166,
      "step": 1194
    },
    {
      "epoch": 0.2715909090909091,
      "grad_norm": 0.05019306391477585,
      "learning_rate": 0.00014584755403868033,
      "loss": 0.3542,
      "step": 1195
    },
    {
      "epoch": 0.2718181818181818,
      "grad_norm": 0.05403446778655052,
      "learning_rate": 0.00014580204778156999,
      "loss": 0.3847,
      "step": 1196
    },
    {
      "epoch": 0.27204545454545453,
      "grad_norm": 0.04375725984573364,
      "learning_rate": 0.0001457565415244596,
      "loss": 0.3078,
      "step": 1197
    },
    {
      "epoch": 0.2722727272727273,
      "grad_norm": 0.04229685291647911,
      "learning_rate": 0.00014571103526734926,
      "loss": 0.3106,
      "step": 1198
    },
    {
      "epoch": 0.2725,
      "grad_norm": 0.05596330389380455,
      "learning_rate": 0.00014566552901023892,
      "loss": 0.3936,
      "step": 1199
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.043099503964185715,
      "learning_rate": 0.00014562002275312857,
      "loss": 0.3478,
      "step": 1200
    },
    {
      "epoch": 0.27295454545454545,
      "grad_norm": 0.04089329019188881,
      "learning_rate": 0.00014557451649601822,
      "loss": 0.3574,
      "step": 1201
    },
    {
      "epoch": 0.2731818181818182,
      "grad_norm": 0.054439250379800797,
      "learning_rate": 0.00014552901023890785,
      "loss": 0.3611,
      "step": 1202
    },
    {
      "epoch": 0.27340909090909093,
      "grad_norm": 0.056466348469257355,
      "learning_rate": 0.0001454835039817975,
      "loss": 0.4086,
      "step": 1203
    },
    {
      "epoch": 0.2736363636363636,
      "grad_norm": 0.04884357377886772,
      "learning_rate": 0.00014543799772468715,
      "loss": 0.301,
      "step": 1204
    },
    {
      "epoch": 0.27386363636363636,
      "grad_norm": 0.06964311003684998,
      "learning_rate": 0.0001453924914675768,
      "loss": 0.3726,
      "step": 1205
    },
    {
      "epoch": 0.2740909090909091,
      "grad_norm": 0.053577352315187454,
      "learning_rate": 0.00014534698521046643,
      "loss": 0.3402,
      "step": 1206
    },
    {
      "epoch": 0.2743181818181818,
      "grad_norm": 0.05884185805916786,
      "learning_rate": 0.00014530147895335608,
      "loss": 0.3828,
      "step": 1207
    },
    {
      "epoch": 0.27454545454545454,
      "grad_norm": 0.05244440212845802,
      "learning_rate": 0.00014525597269624574,
      "loss": 0.348,
      "step": 1208
    },
    {
      "epoch": 0.2747727272727273,
      "grad_norm": 0.06930254399776459,
      "learning_rate": 0.0001452104664391354,
      "loss": 0.3928,
      "step": 1209
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.04070502519607544,
      "learning_rate": 0.00014516496018202504,
      "loss": 0.3205,
      "step": 1210
    },
    {
      "epoch": 0.2752272727272727,
      "grad_norm": 0.05429571866989136,
      "learning_rate": 0.0001451194539249147,
      "loss": 0.3368,
      "step": 1211
    },
    {
      "epoch": 0.27545454545454545,
      "grad_norm": 0.04017975181341171,
      "learning_rate": 0.00014507394766780432,
      "loss": 0.2819,
      "step": 1212
    },
    {
      "epoch": 0.2756818181818182,
      "grad_norm": 0.042952459305524826,
      "learning_rate": 0.00014502844141069397,
      "loss": 0.321,
      "step": 1213
    },
    {
      "epoch": 0.2759090909090909,
      "grad_norm": 0.042821042239665985,
      "learning_rate": 0.00014498293515358363,
      "loss": 0.3203,
      "step": 1214
    },
    {
      "epoch": 0.2761363636363636,
      "grad_norm": 0.046992238610982895,
      "learning_rate": 0.00014493742889647328,
      "loss": 0.3537,
      "step": 1215
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.05150729417800903,
      "learning_rate": 0.0001448919226393629,
      "loss": 0.3882,
      "step": 1216
    },
    {
      "epoch": 0.2765909090909091,
      "grad_norm": 0.04729556664824486,
      "learning_rate": 0.00014484641638225256,
      "loss": 0.3532,
      "step": 1217
    },
    {
      "epoch": 0.2768181818181818,
      "grad_norm": 0.048744961619377136,
      "learning_rate": 0.0001448009101251422,
      "loss": 0.3425,
      "step": 1218
    },
    {
      "epoch": 0.27704545454545454,
      "grad_norm": 0.0499008409678936,
      "learning_rate": 0.00014475540386803186,
      "loss": 0.3386,
      "step": 1219
    },
    {
      "epoch": 0.2772727272727273,
      "grad_norm": 0.05848833918571472,
      "learning_rate": 0.00014470989761092152,
      "loss": 0.3551,
      "step": 1220
    },
    {
      "epoch": 0.2775,
      "grad_norm": 0.044026050716638565,
      "learning_rate": 0.00014466439135381117,
      "loss": 0.2806,
      "step": 1221
    },
    {
      "epoch": 0.2777272727272727,
      "grad_norm": 0.041194818913936615,
      "learning_rate": 0.0001446188850967008,
      "loss": 0.3081,
      "step": 1222
    },
    {
      "epoch": 0.27795454545454545,
      "grad_norm": 0.05307938531041145,
      "learning_rate": 0.00014457337883959045,
      "loss": 0.3454,
      "step": 1223
    },
    {
      "epoch": 0.2781818181818182,
      "grad_norm": 0.04757620766758919,
      "learning_rate": 0.0001445278725824801,
      "loss": 0.3772,
      "step": 1224
    },
    {
      "epoch": 0.2784090909090909,
      "grad_norm": 0.04932534322142601,
      "learning_rate": 0.00014448236632536975,
      "loss": 0.2882,
      "step": 1225
    },
    {
      "epoch": 0.2786363636363636,
      "grad_norm": 0.04555797204375267,
      "learning_rate": 0.00014443686006825938,
      "loss": 0.3338,
      "step": 1226
    },
    {
      "epoch": 0.27886363636363637,
      "grad_norm": 0.060651905834674835,
      "learning_rate": 0.00014439135381114903,
      "loss": 0.3565,
      "step": 1227
    },
    {
      "epoch": 0.2790909090909091,
      "grad_norm": 0.05393769219517708,
      "learning_rate": 0.00014434584755403869,
      "loss": 0.3712,
      "step": 1228
    },
    {
      "epoch": 0.2793181818181818,
      "grad_norm": 0.0646137148141861,
      "learning_rate": 0.00014430034129692834,
      "loss": 0.427,
      "step": 1229
    },
    {
      "epoch": 0.27954545454545454,
      "grad_norm": 0.041475776582956314,
      "learning_rate": 0.000144254835039818,
      "loss": 0.3063,
      "step": 1230
    },
    {
      "epoch": 0.2797727272727273,
      "grad_norm": 0.047030411660671234,
      "learning_rate": 0.00014420932878270764,
      "loss": 0.2976,
      "step": 1231
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.060965802520513535,
      "learning_rate": 0.00014416382252559727,
      "loss": 0.3437,
      "step": 1232
    },
    {
      "epoch": 0.2802272727272727,
      "grad_norm": 0.04046865552663803,
      "learning_rate": 0.00014411831626848692,
      "loss": 0.3407,
      "step": 1233
    },
    {
      "epoch": 0.28045454545454546,
      "grad_norm": 0.062066514045000076,
      "learning_rate": 0.00014407281001137658,
      "loss": 0.3532,
      "step": 1234
    },
    {
      "epoch": 0.2806818181818182,
      "grad_norm": 0.0440547801554203,
      "learning_rate": 0.00014402730375426623,
      "loss": 0.3092,
      "step": 1235
    },
    {
      "epoch": 0.2809090909090909,
      "grad_norm": 0.05130080506205559,
      "learning_rate": 0.00014398179749715585,
      "loss": 0.344,
      "step": 1236
    },
    {
      "epoch": 0.28113636363636363,
      "grad_norm": 0.047072023153305054,
      "learning_rate": 0.0001439362912400455,
      "loss": 0.3498,
      "step": 1237
    },
    {
      "epoch": 0.28136363636363637,
      "grad_norm": 0.0346621572971344,
      "learning_rate": 0.00014389078498293516,
      "loss": 0.2585,
      "step": 1238
    },
    {
      "epoch": 0.2815909090909091,
      "grad_norm": 0.0430200919508934,
      "learning_rate": 0.0001438452787258248,
      "loss": 0.3236,
      "step": 1239
    },
    {
      "epoch": 0.2818181818181818,
      "grad_norm": 0.05029089003801346,
      "learning_rate": 0.00014379977246871447,
      "loss": 0.3655,
      "step": 1240
    },
    {
      "epoch": 0.28204545454545454,
      "grad_norm": 0.05768948793411255,
      "learning_rate": 0.00014375426621160412,
      "loss": 0.3399,
      "step": 1241
    },
    {
      "epoch": 0.2822727272727273,
      "grad_norm": 0.05058986321091652,
      "learning_rate": 0.00014370875995449374,
      "loss": 0.3407,
      "step": 1242
    },
    {
      "epoch": 0.2825,
      "grad_norm": 0.06205827370285988,
      "learning_rate": 0.0001436632536973834,
      "loss": 0.3796,
      "step": 1243
    },
    {
      "epoch": 0.2827272727272727,
      "grad_norm": 0.0422271303832531,
      "learning_rate": 0.00014361774744027305,
      "loss": 0.3216,
      "step": 1244
    },
    {
      "epoch": 0.28295454545454546,
      "grad_norm": 0.06455659121274948,
      "learning_rate": 0.00014357224118316268,
      "loss": 0.4329,
      "step": 1245
    },
    {
      "epoch": 0.2831818181818182,
      "grad_norm": 0.05175439268350601,
      "learning_rate": 0.00014352673492605233,
      "loss": 0.3665,
      "step": 1246
    },
    {
      "epoch": 0.2834090909090909,
      "grad_norm": 0.07855518162250519,
      "learning_rate": 0.00014348122866894198,
      "loss": 0.4432,
      "step": 1247
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.04333902522921562,
      "learning_rate": 0.00014343572241183163,
      "loss": 0.3715,
      "step": 1248
    },
    {
      "epoch": 0.2838636363636364,
      "grad_norm": 0.048798106610774994,
      "learning_rate": 0.00014339021615472129,
      "loss": 0.3334,
      "step": 1249
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 0.05284671112895012,
      "learning_rate": 0.00014334470989761094,
      "loss": 0.3701,
      "step": 1250
    },
    {
      "epoch": 0.2843181818181818,
      "grad_norm": 0.05218333750963211,
      "learning_rate": 0.0001432992036405006,
      "loss": 0.3652,
      "step": 1251
    },
    {
      "epoch": 0.28454545454545455,
      "grad_norm": 0.04501025006175041,
      "learning_rate": 0.00014325369738339022,
      "loss": 0.3358,
      "step": 1252
    },
    {
      "epoch": 0.2847727272727273,
      "grad_norm": 0.04536863788962364,
      "learning_rate": 0.00014320819112627987,
      "loss": 0.3337,
      "step": 1253
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.05169113725423813,
      "learning_rate": 0.00014316268486916952,
      "loss": 0.3097,
      "step": 1254
    },
    {
      "epoch": 0.2852272727272727,
      "grad_norm": 0.04884982481598854,
      "learning_rate": 0.00014311717861205915,
      "loss": 0.3385,
      "step": 1255
    },
    {
      "epoch": 0.28545454545454546,
      "grad_norm": 0.0509166419506073,
      "learning_rate": 0.0001430716723549488,
      "loss": 0.35,
      "step": 1256
    },
    {
      "epoch": 0.2856818181818182,
      "grad_norm": 0.04871850833296776,
      "learning_rate": 0.00014302616609783845,
      "loss": 0.3095,
      "step": 1257
    },
    {
      "epoch": 0.2859090909090909,
      "grad_norm": 0.06693635135889053,
      "learning_rate": 0.0001429806598407281,
      "loss": 0.4093,
      "step": 1258
    },
    {
      "epoch": 0.28613636363636363,
      "grad_norm": 0.04439307004213333,
      "learning_rate": 0.00014293515358361776,
      "loss": 0.3496,
      "step": 1259
    },
    {
      "epoch": 0.2863636363636364,
      "grad_norm": 0.04326217249035835,
      "learning_rate": 0.0001428896473265074,
      "loss": 0.3296,
      "step": 1260
    },
    {
      "epoch": 0.2865909090909091,
      "grad_norm": 0.050816986709833145,
      "learning_rate": 0.00014284414106939707,
      "loss": 0.3281,
      "step": 1261
    },
    {
      "epoch": 0.2868181818181818,
      "grad_norm": 0.04861665889620781,
      "learning_rate": 0.0001427986348122867,
      "loss": 0.3356,
      "step": 1262
    },
    {
      "epoch": 0.28704545454545455,
      "grad_norm": 0.045255545526742935,
      "learning_rate": 0.00014275312855517634,
      "loss": 0.3462,
      "step": 1263
    },
    {
      "epoch": 0.2872727272727273,
      "grad_norm": 0.04573866352438927,
      "learning_rate": 0.000142707622298066,
      "loss": 0.2993,
      "step": 1264
    },
    {
      "epoch": 0.2875,
      "grad_norm": 0.07694487273693085,
      "learning_rate": 0.00014266211604095562,
      "loss": 0.3574,
      "step": 1265
    },
    {
      "epoch": 0.2877272727272727,
      "grad_norm": 0.042826004326343536,
      "learning_rate": 0.00014261660978384528,
      "loss": 0.274,
      "step": 1266
    },
    {
      "epoch": 0.28795454545454546,
      "grad_norm": 0.0455264076590538,
      "learning_rate": 0.00014257110352673493,
      "loss": 0.2948,
      "step": 1267
    },
    {
      "epoch": 0.2881818181818182,
      "grad_norm": 0.0572604238986969,
      "learning_rate": 0.00014252559726962458,
      "loss": 0.3566,
      "step": 1268
    },
    {
      "epoch": 0.2884090909090909,
      "grad_norm": 0.07139255106449127,
      "learning_rate": 0.00014248009101251423,
      "loss": 0.4085,
      "step": 1269
    },
    {
      "epoch": 0.28863636363636364,
      "grad_norm": 0.04491877555847168,
      "learning_rate": 0.0001424345847554039,
      "loss": 0.3327,
      "step": 1270
    },
    {
      "epoch": 0.2888636363636364,
      "grad_norm": 0.0475236214697361,
      "learning_rate": 0.00014238907849829354,
      "loss": 0.3722,
      "step": 1271
    },
    {
      "epoch": 0.28909090909090907,
      "grad_norm": 0.054823312908411026,
      "learning_rate": 0.00014234357224118317,
      "loss": 0.3699,
      "step": 1272
    },
    {
      "epoch": 0.2893181818181818,
      "grad_norm": 0.05056796595454216,
      "learning_rate": 0.00014229806598407282,
      "loss": 0.3003,
      "step": 1273
    },
    {
      "epoch": 0.28954545454545455,
      "grad_norm": 0.05463255196809769,
      "learning_rate": 0.00014225255972696244,
      "loss": 0.3813,
      "step": 1274
    },
    {
      "epoch": 0.2897727272727273,
      "grad_norm": 0.05337943881750107,
      "learning_rate": 0.0001422070534698521,
      "loss": 0.3523,
      "step": 1275
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.043044257909059525,
      "learning_rate": 0.00014216154721274175,
      "loss": 0.3208,
      "step": 1276
    },
    {
      "epoch": 0.2902272727272727,
      "grad_norm": 0.04796142876148224,
      "learning_rate": 0.0001421160409556314,
      "loss": 0.2894,
      "step": 1277
    },
    {
      "epoch": 0.29045454545454547,
      "grad_norm": 0.051173318177461624,
      "learning_rate": 0.00014207053469852106,
      "loss": 0.301,
      "step": 1278
    },
    {
      "epoch": 0.2906818181818182,
      "grad_norm": 0.03768596425652504,
      "learning_rate": 0.0001420250284414107,
      "loss": 0.2632,
      "step": 1279
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.05678822100162506,
      "learning_rate": 0.00014197952218430036,
      "loss": 0.3383,
      "step": 1280
    },
    {
      "epoch": 0.29113636363636364,
      "grad_norm": 0.06410352140665054,
      "learning_rate": 0.00014193401592719001,
      "loss": 0.3362,
      "step": 1281
    },
    {
      "epoch": 0.2913636363636364,
      "grad_norm": 0.055614445358514786,
      "learning_rate": 0.00014188850967007964,
      "loss": 0.3661,
      "step": 1282
    },
    {
      "epoch": 0.29159090909090907,
      "grad_norm": 0.048437803983688354,
      "learning_rate": 0.0001418430034129693,
      "loss": 0.3441,
      "step": 1283
    },
    {
      "epoch": 0.2918181818181818,
      "grad_norm": 0.052894264459609985,
      "learning_rate": 0.00014179749715585892,
      "loss": 0.3548,
      "step": 1284
    },
    {
      "epoch": 0.29204545454545455,
      "grad_norm": 0.043190110474824905,
      "learning_rate": 0.00014175199089874857,
      "loss": 0.3311,
      "step": 1285
    },
    {
      "epoch": 0.2922727272727273,
      "grad_norm": 0.05339846387505531,
      "learning_rate": 0.00014170648464163822,
      "loss": 0.3716,
      "step": 1286
    },
    {
      "epoch": 0.2925,
      "grad_norm": 0.03945322707295418,
      "learning_rate": 0.00014166097838452788,
      "loss": 0.2799,
      "step": 1287
    },
    {
      "epoch": 0.2927272727272727,
      "grad_norm": 0.046576034277677536,
      "learning_rate": 0.00014161547212741753,
      "loss": 0.3029,
      "step": 1288
    },
    {
      "epoch": 0.29295454545454547,
      "grad_norm": 0.05763890594244003,
      "learning_rate": 0.00014156996587030718,
      "loss": 0.376,
      "step": 1289
    },
    {
      "epoch": 0.29318181818181815,
      "grad_norm": 0.05772281438112259,
      "learning_rate": 0.00014152445961319684,
      "loss": 0.3494,
      "step": 1290
    },
    {
      "epoch": 0.2934090909090909,
      "grad_norm": 0.046994633972644806,
      "learning_rate": 0.00014147895335608646,
      "loss": 0.3325,
      "step": 1291
    },
    {
      "epoch": 0.29363636363636364,
      "grad_norm": 0.05569080635905266,
      "learning_rate": 0.00014143344709897611,
      "loss": 0.3668,
      "step": 1292
    },
    {
      "epoch": 0.2938636363636364,
      "grad_norm": 0.04922254756093025,
      "learning_rate": 0.00014138794084186577,
      "loss": 0.3318,
      "step": 1293
    },
    {
      "epoch": 0.29409090909090907,
      "grad_norm": 0.038502492010593414,
      "learning_rate": 0.0001413424345847554,
      "loss": 0.313,
      "step": 1294
    },
    {
      "epoch": 0.2943181818181818,
      "grad_norm": 0.04613108187913895,
      "learning_rate": 0.00014129692832764505,
      "loss": 0.3796,
      "step": 1295
    },
    {
      "epoch": 0.29454545454545455,
      "grad_norm": 0.04466770216822624,
      "learning_rate": 0.0001412514220705347,
      "loss": 0.3366,
      "step": 1296
    },
    {
      "epoch": 0.2947727272727273,
      "grad_norm": 0.04226236045360565,
      "learning_rate": 0.00014120591581342435,
      "loss": 0.2911,
      "step": 1297
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.04424625262618065,
      "learning_rate": 0.000141160409556314,
      "loss": 0.3024,
      "step": 1298
    },
    {
      "epoch": 0.2952272727272727,
      "grad_norm": 0.04767828434705734,
      "learning_rate": 0.00014111490329920366,
      "loss": 0.3156,
      "step": 1299
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.056844744831323624,
      "learning_rate": 0.0001410693970420933,
      "loss": 0.3428,
      "step": 1300
    },
    {
      "epoch": 0.29568181818181816,
      "grad_norm": 0.05338824540376663,
      "learning_rate": 0.00014102389078498293,
      "loss": 0.319,
      "step": 1301
    },
    {
      "epoch": 0.2959090909090909,
      "grad_norm": 0.04187279939651489,
      "learning_rate": 0.0001409783845278726,
      "loss": 0.316,
      "step": 1302
    },
    {
      "epoch": 0.29613636363636364,
      "grad_norm": 0.055106375366449356,
      "learning_rate": 0.00014093287827076224,
      "loss": 0.3296,
      "step": 1303
    },
    {
      "epoch": 0.2963636363636364,
      "grad_norm": 0.06012343615293503,
      "learning_rate": 0.00014088737201365187,
      "loss": 0.3345,
      "step": 1304
    },
    {
      "epoch": 0.29659090909090907,
      "grad_norm": 0.04663519188761711,
      "learning_rate": 0.00014084186575654152,
      "loss": 0.3372,
      "step": 1305
    },
    {
      "epoch": 0.2968181818181818,
      "grad_norm": 0.06169543415307999,
      "learning_rate": 0.00014079635949943117,
      "loss": 0.3781,
      "step": 1306
    },
    {
      "epoch": 0.29704545454545456,
      "grad_norm": 0.04190332442522049,
      "learning_rate": 0.00014075085324232082,
      "loss": 0.3192,
      "step": 1307
    },
    {
      "epoch": 0.2972727272727273,
      "grad_norm": 0.05121518671512604,
      "learning_rate": 0.00014070534698521048,
      "loss": 0.3523,
      "step": 1308
    },
    {
      "epoch": 0.2975,
      "grad_norm": 0.05107321962714195,
      "learning_rate": 0.00014065984072810013,
      "loss": 0.3854,
      "step": 1309
    },
    {
      "epoch": 0.29772727272727273,
      "grad_norm": 0.048848334699869156,
      "learning_rate": 0.00014061433447098978,
      "loss": 0.3224,
      "step": 1310
    },
    {
      "epoch": 0.29795454545454547,
      "grad_norm": 0.04722585529088974,
      "learning_rate": 0.0001405688282138794,
      "loss": 0.3444,
      "step": 1311
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 0.03419908508658409,
      "learning_rate": 0.00014052332195676906,
      "loss": 0.28,
      "step": 1312
    },
    {
      "epoch": 0.2984090909090909,
      "grad_norm": 0.04842032864689827,
      "learning_rate": 0.0001404778156996587,
      "loss": 0.2995,
      "step": 1313
    },
    {
      "epoch": 0.29863636363636364,
      "grad_norm": 0.05001218989491463,
      "learning_rate": 0.00014043230944254834,
      "loss": 0.3144,
      "step": 1314
    },
    {
      "epoch": 0.2988636363636364,
      "grad_norm": 0.039821308106184006,
      "learning_rate": 0.000140386803185438,
      "loss": 0.3191,
      "step": 1315
    },
    {
      "epoch": 0.2990909090909091,
      "grad_norm": 0.04699790105223656,
      "learning_rate": 0.00014034129692832765,
      "loss": 0.3239,
      "step": 1316
    },
    {
      "epoch": 0.2993181818181818,
      "grad_norm": 0.0524866096675396,
      "learning_rate": 0.0001402957906712173,
      "loss": 0.3217,
      "step": 1317
    },
    {
      "epoch": 0.29954545454545456,
      "grad_norm": 0.057706739753484726,
      "learning_rate": 0.00014025028441410695,
      "loss": 0.3935,
      "step": 1318
    },
    {
      "epoch": 0.29977272727272725,
      "grad_norm": 0.04872773960232735,
      "learning_rate": 0.0001402047781569966,
      "loss": 0.3556,
      "step": 1319
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.045685626566410065,
      "learning_rate": 0.00014015927189988626,
      "loss": 0.3695,
      "step": 1320
    },
    {
      "epoch": 0.30022727272727273,
      "grad_norm": 0.04586789384484291,
      "learning_rate": 0.00014011376564277588,
      "loss": 0.2939,
      "step": 1321
    },
    {
      "epoch": 0.3004545454545455,
      "grad_norm": 0.056955140084028244,
      "learning_rate": 0.00014006825938566554,
      "loss": 0.3213,
      "step": 1322
    },
    {
      "epoch": 0.30068181818181816,
      "grad_norm": 0.05254703760147095,
      "learning_rate": 0.00014002275312855516,
      "loss": 0.3318,
      "step": 1323
    },
    {
      "epoch": 0.3009090909090909,
      "grad_norm": 0.05204993113875389,
      "learning_rate": 0.00013997724687144481,
      "loss": 0.3293,
      "step": 1324
    },
    {
      "epoch": 0.30113636363636365,
      "grad_norm": 0.04962174966931343,
      "learning_rate": 0.00013993174061433447,
      "loss": 0.3622,
      "step": 1325
    },
    {
      "epoch": 0.3013636363636364,
      "grad_norm": 0.05045890808105469,
      "learning_rate": 0.00013988623435722412,
      "loss": 0.3414,
      "step": 1326
    },
    {
      "epoch": 0.3015909090909091,
      "grad_norm": 0.04637077823281288,
      "learning_rate": 0.00013984072810011377,
      "loss": 0.2863,
      "step": 1327
    },
    {
      "epoch": 0.3018181818181818,
      "grad_norm": 0.03734632208943367,
      "learning_rate": 0.00013979522184300343,
      "loss": 0.2808,
      "step": 1328
    },
    {
      "epoch": 0.30204545454545456,
      "grad_norm": 0.040347471833229065,
      "learning_rate": 0.00013974971558589308,
      "loss": 0.2981,
      "step": 1329
    },
    {
      "epoch": 0.30227272727272725,
      "grad_norm": 0.055843230336904526,
      "learning_rate": 0.00013970420932878273,
      "loss": 0.3692,
      "step": 1330
    },
    {
      "epoch": 0.3025,
      "grad_norm": 0.06366299837827682,
      "learning_rate": 0.00013965870307167236,
      "loss": 0.3849,
      "step": 1331
    },
    {
      "epoch": 0.30272727272727273,
      "grad_norm": 0.04307505115866661,
      "learning_rate": 0.000139613196814562,
      "loss": 0.3198,
      "step": 1332
    },
    {
      "epoch": 0.3029545454545455,
      "grad_norm": 0.03728164732456207,
      "learning_rate": 0.00013956769055745164,
      "loss": 0.3355,
      "step": 1333
    },
    {
      "epoch": 0.30318181818181816,
      "grad_norm": 0.034691303968429565,
      "learning_rate": 0.0001395221843003413,
      "loss": 0.3105,
      "step": 1334
    },
    {
      "epoch": 0.3034090909090909,
      "grad_norm": 0.05011534318327904,
      "learning_rate": 0.00013947667804323094,
      "loss": 0.3157,
      "step": 1335
    },
    {
      "epoch": 0.30363636363636365,
      "grad_norm": 0.04055241867899895,
      "learning_rate": 0.0001394311717861206,
      "loss": 0.3314,
      "step": 1336
    },
    {
      "epoch": 0.3038636363636364,
      "grad_norm": 0.046587880700826645,
      "learning_rate": 0.00013938566552901025,
      "loss": 0.2461,
      "step": 1337
    },
    {
      "epoch": 0.3040909090909091,
      "grad_norm": 0.04825815185904503,
      "learning_rate": 0.0001393401592718999,
      "loss": 0.3635,
      "step": 1338
    },
    {
      "epoch": 0.3043181818181818,
      "grad_norm": 0.036846768110990524,
      "learning_rate": 0.00013929465301478955,
      "loss": 0.273,
      "step": 1339
    },
    {
      "epoch": 0.30454545454545456,
      "grad_norm": 0.040811654180288315,
      "learning_rate": 0.0001392491467576792,
      "loss": 0.3032,
      "step": 1340
    },
    {
      "epoch": 0.30477272727272725,
      "grad_norm": 0.06294192373752594,
      "learning_rate": 0.00013920364050056883,
      "loss": 0.3426,
      "step": 1341
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.05481129139661789,
      "learning_rate": 0.00013915813424345848,
      "loss": 0.362,
      "step": 1342
    },
    {
      "epoch": 0.30522727272727274,
      "grad_norm": 0.04478297755122185,
      "learning_rate": 0.0001391126279863481,
      "loss": 0.3231,
      "step": 1343
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 0.04666372388601303,
      "learning_rate": 0.00013906712172923776,
      "loss": 0.3457,
      "step": 1344
    },
    {
      "epoch": 0.30568181818181817,
      "grad_norm": 0.06431163847446442,
      "learning_rate": 0.00013902161547212741,
      "loss": 0.4373,
      "step": 1345
    },
    {
      "epoch": 0.3059090909090909,
      "grad_norm": 0.05292198061943054,
      "learning_rate": 0.00013897610921501707,
      "loss": 0.3518,
      "step": 1346
    },
    {
      "epoch": 0.30613636363636365,
      "grad_norm": 0.0431504026055336,
      "learning_rate": 0.00013893060295790672,
      "loss": 0.3347,
      "step": 1347
    },
    {
      "epoch": 0.30636363636363634,
      "grad_norm": 0.05954775959253311,
      "learning_rate": 0.00013888509670079637,
      "loss": 0.3479,
      "step": 1348
    },
    {
      "epoch": 0.3065909090909091,
      "grad_norm": 0.05334458127617836,
      "learning_rate": 0.00013883959044368603,
      "loss": 0.3228,
      "step": 1349
    },
    {
      "epoch": 0.3068181818181818,
      "grad_norm": 0.04193422570824623,
      "learning_rate": 0.00013879408418657568,
      "loss": 0.2971,
      "step": 1350
    },
    {
      "epoch": 0.30704545454545457,
      "grad_norm": 0.04518503323197365,
      "learning_rate": 0.0001387485779294653,
      "loss": 0.3589,
      "step": 1351
    },
    {
      "epoch": 0.30727272727272725,
      "grad_norm": 0.05051480978727341,
      "learning_rate": 0.00013870307167235496,
      "loss": 0.3234,
      "step": 1352
    },
    {
      "epoch": 0.3075,
      "grad_norm": 0.04963666573166847,
      "learning_rate": 0.00013865756541524458,
      "loss": 0.3287,
      "step": 1353
    },
    {
      "epoch": 0.30772727272727274,
      "grad_norm": 0.05102600157260895,
      "learning_rate": 0.00013861205915813424,
      "loss": 0.3126,
      "step": 1354
    },
    {
      "epoch": 0.3079545454545455,
      "grad_norm": 0.04574282094836235,
      "learning_rate": 0.0001385665529010239,
      "loss": 0.3168,
      "step": 1355
    },
    {
      "epoch": 0.30818181818181817,
      "grad_norm": 0.04766511172056198,
      "learning_rate": 0.00013852104664391354,
      "loss": 0.3427,
      "step": 1356
    },
    {
      "epoch": 0.3084090909090909,
      "grad_norm": 0.04921765252947807,
      "learning_rate": 0.0001384755403868032,
      "loss": 0.3375,
      "step": 1357
    },
    {
      "epoch": 0.30863636363636365,
      "grad_norm": 0.044401634484529495,
      "learning_rate": 0.00013843003412969285,
      "loss": 0.3218,
      "step": 1358
    },
    {
      "epoch": 0.30886363636363634,
      "grad_norm": 0.052988819777965546,
      "learning_rate": 0.0001383845278725825,
      "loss": 0.3702,
      "step": 1359
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.04302345588803291,
      "learning_rate": 0.00013833902161547215,
      "loss": 0.2956,
      "step": 1360
    },
    {
      "epoch": 0.3093181818181818,
      "grad_norm": 0.0473000593483448,
      "learning_rate": 0.00013829351535836178,
      "loss": 0.3525,
      "step": 1361
    },
    {
      "epoch": 0.30954545454545457,
      "grad_norm": 0.04506748542189598,
      "learning_rate": 0.00013824800910125143,
      "loss": 0.3525,
      "step": 1362
    },
    {
      "epoch": 0.30977272727272726,
      "grad_norm": 0.047761477530002594,
      "learning_rate": 0.00013820250284414106,
      "loss": 0.3339,
      "step": 1363
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.053466182202100754,
      "learning_rate": 0.0001381569965870307,
      "loss": 0.3416,
      "step": 1364
    },
    {
      "epoch": 0.31022727272727274,
      "grad_norm": 0.04708332195878029,
      "learning_rate": 0.00013811149032992036,
      "loss": 0.3934,
      "step": 1365
    },
    {
      "epoch": 0.3104545454545454,
      "grad_norm": 0.05167948082089424,
      "learning_rate": 0.00013806598407281002,
      "loss": 0.3373,
      "step": 1366
    },
    {
      "epoch": 0.31068181818181817,
      "grad_norm": 0.06102229654788971,
      "learning_rate": 0.00013802047781569967,
      "loss": 0.4159,
      "step": 1367
    },
    {
      "epoch": 0.3109090909090909,
      "grad_norm": 0.04445217177271843,
      "learning_rate": 0.00013797497155858932,
      "loss": 0.3265,
      "step": 1368
    },
    {
      "epoch": 0.31113636363636366,
      "grad_norm": 0.04093257337808609,
      "learning_rate": 0.00013792946530147897,
      "loss": 0.3414,
      "step": 1369
    },
    {
      "epoch": 0.31136363636363634,
      "grad_norm": 0.06020238623023033,
      "learning_rate": 0.00013788395904436863,
      "loss": 0.3613,
      "step": 1370
    },
    {
      "epoch": 0.3115909090909091,
      "grad_norm": 0.05316759645938873,
      "learning_rate": 0.00013783845278725825,
      "loss": 0.3586,
      "step": 1371
    },
    {
      "epoch": 0.3118181818181818,
      "grad_norm": 0.04250766336917877,
      "learning_rate": 0.0001377929465301479,
      "loss": 0.3355,
      "step": 1372
    },
    {
      "epoch": 0.31204545454545457,
      "grad_norm": 0.03988335281610489,
      "learning_rate": 0.00013774744027303753,
      "loss": 0.2855,
      "step": 1373
    },
    {
      "epoch": 0.31227272727272726,
      "grad_norm": 0.04036162421107292,
      "learning_rate": 0.00013770193401592718,
      "loss": 0.321,
      "step": 1374
    },
    {
      "epoch": 0.3125,
      "grad_norm": 0.047701381146907806,
      "learning_rate": 0.00013765642775881684,
      "loss": 0.315,
      "step": 1375
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.051986634731292725,
      "learning_rate": 0.0001376109215017065,
      "loss": 0.3835,
      "step": 1376
    },
    {
      "epoch": 0.31295454545454543,
      "grad_norm": 0.05783778801560402,
      "learning_rate": 0.00013756541524459614,
      "loss": 0.3739,
      "step": 1377
    },
    {
      "epoch": 0.3131818181818182,
      "grad_norm": 0.046588487923145294,
      "learning_rate": 0.0001375199089874858,
      "loss": 0.3747,
      "step": 1378
    },
    {
      "epoch": 0.3134090909090909,
      "grad_norm": 0.06281426548957825,
      "learning_rate": 0.00013747440273037545,
      "loss": 0.3617,
      "step": 1379
    },
    {
      "epoch": 0.31363636363636366,
      "grad_norm": 0.04518168792128563,
      "learning_rate": 0.0001374288964732651,
      "loss": 0.3474,
      "step": 1380
    },
    {
      "epoch": 0.31386363636363634,
      "grad_norm": 0.05689510703086853,
      "learning_rate": 0.00013738339021615473,
      "loss": 0.3323,
      "step": 1381
    },
    {
      "epoch": 0.3140909090909091,
      "grad_norm": 0.041262418031692505,
      "learning_rate": 0.00013733788395904438,
      "loss": 0.3096,
      "step": 1382
    },
    {
      "epoch": 0.31431818181818183,
      "grad_norm": 0.04338095337152481,
      "learning_rate": 0.000137292377701934,
      "loss": 0.3337,
      "step": 1383
    },
    {
      "epoch": 0.3145454545454546,
      "grad_norm": 0.06260799616575241,
      "learning_rate": 0.00013724687144482366,
      "loss": 0.3381,
      "step": 1384
    },
    {
      "epoch": 0.31477272727272726,
      "grad_norm": 0.05078873783349991,
      "learning_rate": 0.0001372013651877133,
      "loss": 0.3514,
      "step": 1385
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.05658841133117676,
      "learning_rate": 0.00013715585893060296,
      "loss": 0.3648,
      "step": 1386
    },
    {
      "epoch": 0.31522727272727274,
      "grad_norm": 0.07055522501468658,
      "learning_rate": 0.00013711035267349262,
      "loss": 0.4381,
      "step": 1387
    },
    {
      "epoch": 0.31545454545454543,
      "grad_norm": 0.04371548444032669,
      "learning_rate": 0.00013706484641638227,
      "loss": 0.3088,
      "step": 1388
    },
    {
      "epoch": 0.3156818181818182,
      "grad_norm": 0.04266763851046562,
      "learning_rate": 0.00013701934015927192,
      "loss": 0.3103,
      "step": 1389
    },
    {
      "epoch": 0.3159090909090909,
      "grad_norm": 0.05341744050383568,
      "learning_rate": 0.00013697383390216157,
      "loss": 0.3363,
      "step": 1390
    },
    {
      "epoch": 0.31613636363636366,
      "grad_norm": 0.05231712386012077,
      "learning_rate": 0.0001369283276450512,
      "loss": 0.3709,
      "step": 1391
    },
    {
      "epoch": 0.31636363636363635,
      "grad_norm": 0.040596913546323776,
      "learning_rate": 0.00013688282138794085,
      "loss": 0.3367,
      "step": 1392
    },
    {
      "epoch": 0.3165909090909091,
      "grad_norm": 0.035040996968746185,
      "learning_rate": 0.00013683731513083048,
      "loss": 0.2897,
      "step": 1393
    },
    {
      "epoch": 0.31681818181818183,
      "grad_norm": 0.033685728907585144,
      "learning_rate": 0.00013679180887372013,
      "loss": 0.2815,
      "step": 1394
    },
    {
      "epoch": 0.3170454545454545,
      "grad_norm": 0.040215976536273956,
      "learning_rate": 0.00013674630261660978,
      "loss": 0.3662,
      "step": 1395
    },
    {
      "epoch": 0.31727272727272726,
      "grad_norm": 0.03692411258816719,
      "learning_rate": 0.00013670079635949944,
      "loss": 0.2857,
      "step": 1396
    },
    {
      "epoch": 0.3175,
      "grad_norm": 0.040907010436058044,
      "learning_rate": 0.0001366552901023891,
      "loss": 0.3092,
      "step": 1397
    },
    {
      "epoch": 0.31772727272727275,
      "grad_norm": 0.03878585249185562,
      "learning_rate": 0.00013660978384527874,
      "loss": 0.3179,
      "step": 1398
    },
    {
      "epoch": 0.31795454545454543,
      "grad_norm": 0.05743950977921486,
      "learning_rate": 0.0001365642775881684,
      "loss": 0.4031,
      "step": 1399
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.06375186890363693,
      "learning_rate": 0.00013651877133105805,
      "loss": 0.3764,
      "step": 1400
    },
    {
      "epoch": 0.3184090909090909,
      "grad_norm": 0.044991955161094666,
      "learning_rate": 0.00013647326507394767,
      "loss": 0.3488,
      "step": 1401
    },
    {
      "epoch": 0.31863636363636366,
      "grad_norm": 0.05797114223241806,
      "learning_rate": 0.00013642775881683733,
      "loss": 0.313,
      "step": 1402
    },
    {
      "epoch": 0.31886363636363635,
      "grad_norm": 0.046460069715976715,
      "learning_rate": 0.00013638225255972695,
      "loss": 0.3322,
      "step": 1403
    },
    {
      "epoch": 0.3190909090909091,
      "grad_norm": 0.04701470211148262,
      "learning_rate": 0.0001363367463026166,
      "loss": 0.3723,
      "step": 1404
    },
    {
      "epoch": 0.31931818181818183,
      "grad_norm": 0.048992376774549484,
      "learning_rate": 0.00013629124004550626,
      "loss": 0.3339,
      "step": 1405
    },
    {
      "epoch": 0.3195454545454545,
      "grad_norm": 0.047628097236156464,
      "learning_rate": 0.0001362457337883959,
      "loss": 0.2545,
      "step": 1406
    },
    {
      "epoch": 0.31977272727272726,
      "grad_norm": 0.050132643431425095,
      "learning_rate": 0.00013620022753128556,
      "loss": 0.3426,
      "step": 1407
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.051244329661130905,
      "learning_rate": 0.00013615472127417522,
      "loss": 0.3408,
      "step": 1408
    },
    {
      "epoch": 0.32022727272727275,
      "grad_norm": 0.05654081702232361,
      "learning_rate": 0.00013610921501706487,
      "loss": 0.3823,
      "step": 1409
    },
    {
      "epoch": 0.32045454545454544,
      "grad_norm": 0.038281556218862534,
      "learning_rate": 0.00013606370875995452,
      "loss": 0.2712,
      "step": 1410
    },
    {
      "epoch": 0.3206818181818182,
      "grad_norm": 0.05295109003782272,
      "learning_rate": 0.00013601820250284415,
      "loss": 0.3979,
      "step": 1411
    },
    {
      "epoch": 0.3209090909090909,
      "grad_norm": 0.05081770941615105,
      "learning_rate": 0.0001359726962457338,
      "loss": 0.3492,
      "step": 1412
    },
    {
      "epoch": 0.3211363636363636,
      "grad_norm": 0.04764523729681969,
      "learning_rate": 0.00013592718998862343,
      "loss": 0.3644,
      "step": 1413
    },
    {
      "epoch": 0.32136363636363635,
      "grad_norm": 0.04458856210112572,
      "learning_rate": 0.00013588168373151308,
      "loss": 0.3037,
      "step": 1414
    },
    {
      "epoch": 0.3215909090909091,
      "grad_norm": 0.0662066638469696,
      "learning_rate": 0.00013583617747440273,
      "loss": 0.4074,
      "step": 1415
    },
    {
      "epoch": 0.32181818181818184,
      "grad_norm": 0.06435299664735794,
      "learning_rate": 0.00013579067121729239,
      "loss": 0.4369,
      "step": 1416
    },
    {
      "epoch": 0.3220454545454545,
      "grad_norm": 0.03929336741566658,
      "learning_rate": 0.00013574516496018204,
      "loss": 0.2874,
      "step": 1417
    },
    {
      "epoch": 0.32227272727272727,
      "grad_norm": 0.04300936311483383,
      "learning_rate": 0.0001356996587030717,
      "loss": 0.3293,
      "step": 1418
    },
    {
      "epoch": 0.3225,
      "grad_norm": 0.07049990445375443,
      "learning_rate": 0.00013565415244596134,
      "loss": 0.4341,
      "step": 1419
    },
    {
      "epoch": 0.32272727272727275,
      "grad_norm": 0.04638851806521416,
      "learning_rate": 0.00013560864618885097,
      "loss": 0.3224,
      "step": 1420
    },
    {
      "epoch": 0.32295454545454544,
      "grad_norm": 0.048862431198358536,
      "learning_rate": 0.00013556313993174062,
      "loss": 0.339,
      "step": 1421
    },
    {
      "epoch": 0.3231818181818182,
      "grad_norm": 0.05889064446091652,
      "learning_rate": 0.00013551763367463028,
      "loss": 0.3537,
      "step": 1422
    },
    {
      "epoch": 0.3234090909090909,
      "grad_norm": 0.05429699644446373,
      "learning_rate": 0.0001354721274175199,
      "loss": 0.352,
      "step": 1423
    },
    {
      "epoch": 0.3236363636363636,
      "grad_norm": 0.03912684693932533,
      "learning_rate": 0.00013542662116040955,
      "loss": 0.2889,
      "step": 1424
    },
    {
      "epoch": 0.32386363636363635,
      "grad_norm": 0.04963366687297821,
      "learning_rate": 0.0001353811149032992,
      "loss": 0.3375,
      "step": 1425
    },
    {
      "epoch": 0.3240909090909091,
      "grad_norm": 0.055523958057165146,
      "learning_rate": 0.00013533560864618886,
      "loss": 0.3283,
      "step": 1426
    },
    {
      "epoch": 0.32431818181818184,
      "grad_norm": 0.052502576261758804,
      "learning_rate": 0.0001352901023890785,
      "loss": 0.3991,
      "step": 1427
    },
    {
      "epoch": 0.3245454545454545,
      "grad_norm": 0.056796230375766754,
      "learning_rate": 0.00013524459613196817,
      "loss": 0.3843,
      "step": 1428
    },
    {
      "epoch": 0.32477272727272727,
      "grad_norm": 0.05903881415724754,
      "learning_rate": 0.00013519908987485782,
      "loss": 0.375,
      "step": 1429
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.04677531495690346,
      "learning_rate": 0.00013515358361774744,
      "loss": 0.3472,
      "step": 1430
    },
    {
      "epoch": 0.32522727272727275,
      "grad_norm": 0.05803538113832474,
      "learning_rate": 0.0001351080773606371,
      "loss": 0.3937,
      "step": 1431
    },
    {
      "epoch": 0.32545454545454544,
      "grad_norm": 0.051087167114019394,
      "learning_rate": 0.00013506257110352675,
      "loss": 0.3674,
      "step": 1432
    },
    {
      "epoch": 0.3256818181818182,
      "grad_norm": 0.04971909150481224,
      "learning_rate": 0.00013501706484641638,
      "loss": 0.3609,
      "step": 1433
    },
    {
      "epoch": 0.3259090909090909,
      "grad_norm": 0.04916159808635712,
      "learning_rate": 0.00013497155858930603,
      "loss": 0.351,
      "step": 1434
    },
    {
      "epoch": 0.3261363636363636,
      "grad_norm": 0.05312107503414154,
      "learning_rate": 0.00013492605233219568,
      "loss": 0.343,
      "step": 1435
    },
    {
      "epoch": 0.32636363636363636,
      "grad_norm": 0.046172741800546646,
      "learning_rate": 0.00013488054607508533,
      "loss": 0.2804,
      "step": 1436
    },
    {
      "epoch": 0.3265909090909091,
      "grad_norm": 0.04870181530714035,
      "learning_rate": 0.00013483503981797499,
      "loss": 0.3734,
      "step": 1437
    },
    {
      "epoch": 0.32681818181818184,
      "grad_norm": 0.04082459956407547,
      "learning_rate": 0.00013478953356086464,
      "loss": 0.2937,
      "step": 1438
    },
    {
      "epoch": 0.32704545454545453,
      "grad_norm": 0.04617973789572716,
      "learning_rate": 0.0001347440273037543,
      "loss": 0.3364,
      "step": 1439
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.04383332282304764,
      "learning_rate": 0.00013469852104664392,
      "loss": 0.2724,
      "step": 1440
    },
    {
      "epoch": 0.3275,
      "grad_norm": 0.041816987097263336,
      "learning_rate": 0.00013465301478953357,
      "loss": 0.3087,
      "step": 1441
    },
    {
      "epoch": 0.3277272727272727,
      "grad_norm": 0.040178731083869934,
      "learning_rate": 0.00013460750853242322,
      "loss": 0.3587,
      "step": 1442
    },
    {
      "epoch": 0.32795454545454544,
      "grad_norm": 0.03504369407892227,
      "learning_rate": 0.00013456200227531285,
      "loss": 0.2685,
      "step": 1443
    },
    {
      "epoch": 0.3281818181818182,
      "grad_norm": 0.03997238725423813,
      "learning_rate": 0.0001345164960182025,
      "loss": 0.3103,
      "step": 1444
    },
    {
      "epoch": 0.32840909090909093,
      "grad_norm": 0.05266071856021881,
      "learning_rate": 0.00013447098976109215,
      "loss": 0.4065,
      "step": 1445
    },
    {
      "epoch": 0.3286363636363636,
      "grad_norm": 0.04262052848935127,
      "learning_rate": 0.0001344254835039818,
      "loss": 0.3146,
      "step": 1446
    },
    {
      "epoch": 0.32886363636363636,
      "grad_norm": 0.05657539516687393,
      "learning_rate": 0.00013437997724687146,
      "loss": 0.4005,
      "step": 1447
    },
    {
      "epoch": 0.3290909090909091,
      "grad_norm": 0.04646344482898712,
      "learning_rate": 0.0001343344709897611,
      "loss": 0.3193,
      "step": 1448
    },
    {
      "epoch": 0.32931818181818184,
      "grad_norm": 0.0485767126083374,
      "learning_rate": 0.00013428896473265074,
      "loss": 0.3464,
      "step": 1449
    },
    {
      "epoch": 0.32954545454545453,
      "grad_norm": 0.05691293254494667,
      "learning_rate": 0.0001342434584755404,
      "loss": 0.3942,
      "step": 1450
    },
    {
      "epoch": 0.3297727272727273,
      "grad_norm": 0.04877130687236786,
      "learning_rate": 0.00013419795221843004,
      "loss": 0.3429,
      "step": 1451
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.04104260355234146,
      "learning_rate": 0.00013415244596131967,
      "loss": 0.2654,
      "step": 1452
    },
    {
      "epoch": 0.3302272727272727,
      "grad_norm": 0.041071582585573196,
      "learning_rate": 0.00013410693970420932,
      "loss": 0.3135,
      "step": 1453
    },
    {
      "epoch": 0.33045454545454545,
      "grad_norm": 0.048844870179891586,
      "learning_rate": 0.00013406143344709898,
      "loss": 0.3331,
      "step": 1454
    },
    {
      "epoch": 0.3306818181818182,
      "grad_norm": 0.047908980399370193,
      "learning_rate": 0.00013401592718998863,
      "loss": 0.3715,
      "step": 1455
    },
    {
      "epoch": 0.33090909090909093,
      "grad_norm": 0.04715540260076523,
      "learning_rate": 0.00013397042093287828,
      "loss": 0.3432,
      "step": 1456
    },
    {
      "epoch": 0.3311363636363636,
      "grad_norm": 0.04428289830684662,
      "learning_rate": 0.00013392491467576793,
      "loss": 0.3325,
      "step": 1457
    },
    {
      "epoch": 0.33136363636363636,
      "grad_norm": 0.043489985167980194,
      "learning_rate": 0.0001338794084186576,
      "loss": 0.3395,
      "step": 1458
    },
    {
      "epoch": 0.3315909090909091,
      "grad_norm": 0.044827982783317566,
      "learning_rate": 0.0001338339021615472,
      "loss": 0.3142,
      "step": 1459
    },
    {
      "epoch": 0.33181818181818185,
      "grad_norm": 0.046183276921510696,
      "learning_rate": 0.00013378839590443687,
      "loss": 0.2921,
      "step": 1460
    },
    {
      "epoch": 0.33204545454545453,
      "grad_norm": 0.053195804357528687,
      "learning_rate": 0.00013374288964732652,
      "loss": 0.2957,
      "step": 1461
    },
    {
      "epoch": 0.3322727272727273,
      "grad_norm": 0.06884892284870148,
      "learning_rate": 0.00013369738339021614,
      "loss": 0.4069,
      "step": 1462
    },
    {
      "epoch": 0.3325,
      "grad_norm": 0.05221031233668327,
      "learning_rate": 0.0001336518771331058,
      "loss": 0.3418,
      "step": 1463
    },
    {
      "epoch": 0.3327272727272727,
      "grad_norm": 0.05340420827269554,
      "learning_rate": 0.00013360637087599545,
      "loss": 0.3635,
      "step": 1464
    },
    {
      "epoch": 0.33295454545454545,
      "grad_norm": 0.04423922300338745,
      "learning_rate": 0.0001335608646188851,
      "loss": 0.3133,
      "step": 1465
    },
    {
      "epoch": 0.3331818181818182,
      "grad_norm": 0.04402126744389534,
      "learning_rate": 0.00013351535836177476,
      "loss": 0.3596,
      "step": 1466
    },
    {
      "epoch": 0.33340909090909093,
      "grad_norm": 0.04832261800765991,
      "learning_rate": 0.0001334698521046644,
      "loss": 0.3342,
      "step": 1467
    },
    {
      "epoch": 0.3336363636363636,
      "grad_norm": 0.04057307541370392,
      "learning_rate": 0.00013342434584755406,
      "loss": 0.2946,
      "step": 1468
    },
    {
      "epoch": 0.33386363636363636,
      "grad_norm": 0.050913870334625244,
      "learning_rate": 0.0001333788395904437,
      "loss": 0.3796,
      "step": 1469
    },
    {
      "epoch": 0.3340909090909091,
      "grad_norm": 0.052370037883520126,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.3315,
      "step": 1470
    },
    {
      "epoch": 0.3343181818181818,
      "grad_norm": 0.044982362538576126,
      "learning_rate": 0.000133287827076223,
      "loss": 0.3795,
      "step": 1471
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 0.049538761377334595,
      "learning_rate": 0.00013324232081911262,
      "loss": 0.3492,
      "step": 1472
    },
    {
      "epoch": 0.3347727272727273,
      "grad_norm": 0.04331023246049881,
      "learning_rate": 0.00013319681456200227,
      "loss": 0.3338,
      "step": 1473
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.03997064009308815,
      "learning_rate": 0.00013315130830489192,
      "loss": 0.3322,
      "step": 1474
    },
    {
      "epoch": 0.3352272727272727,
      "grad_norm": 0.04292447492480278,
      "learning_rate": 0.00013310580204778158,
      "loss": 0.3553,
      "step": 1475
    },
    {
      "epoch": 0.33545454545454545,
      "grad_norm": 0.03795981779694557,
      "learning_rate": 0.00013306029579067123,
      "loss": 0.2983,
      "step": 1476
    },
    {
      "epoch": 0.3356818181818182,
      "grad_norm": 0.05072234198451042,
      "learning_rate": 0.00013301478953356088,
      "loss": 0.3168,
      "step": 1477
    },
    {
      "epoch": 0.33590909090909093,
      "grad_norm": 0.046256400644779205,
      "learning_rate": 0.00013296928327645054,
      "loss": 0.3495,
      "step": 1478
    },
    {
      "epoch": 0.3361363636363636,
      "grad_norm": 0.045281510800123215,
      "learning_rate": 0.00013292377701934016,
      "loss": 0.3116,
      "step": 1479
    },
    {
      "epoch": 0.33636363636363636,
      "grad_norm": 0.06448352336883545,
      "learning_rate": 0.00013287827076222981,
      "loss": 0.3488,
      "step": 1480
    },
    {
      "epoch": 0.3365909090909091,
      "grad_norm": 0.043521903455257416,
      "learning_rate": 0.00013283276450511947,
      "loss": 0.3415,
      "step": 1481
    },
    {
      "epoch": 0.3368181818181818,
      "grad_norm": 0.06377352029085159,
      "learning_rate": 0.0001327872582480091,
      "loss": 0.4287,
      "step": 1482
    },
    {
      "epoch": 0.33704545454545454,
      "grad_norm": 0.05603628233075142,
      "learning_rate": 0.00013274175199089875,
      "loss": 0.3693,
      "step": 1483
    },
    {
      "epoch": 0.3372727272727273,
      "grad_norm": 0.03954354673624039,
      "learning_rate": 0.0001326962457337884,
      "loss": 0.2156,
      "step": 1484
    },
    {
      "epoch": 0.3375,
      "grad_norm": 0.05176463723182678,
      "learning_rate": 0.00013265073947667805,
      "loss": 0.3388,
      "step": 1485
    },
    {
      "epoch": 0.3377272727272727,
      "grad_norm": 0.044624991714954376,
      "learning_rate": 0.0001326052332195677,
      "loss": 0.3217,
      "step": 1486
    },
    {
      "epoch": 0.33795454545454545,
      "grad_norm": 0.04828763008117676,
      "learning_rate": 0.00013255972696245736,
      "loss": 0.3022,
      "step": 1487
    },
    {
      "epoch": 0.3381818181818182,
      "grad_norm": 0.03820483759045601,
      "learning_rate": 0.00013251422070534698,
      "loss": 0.2991,
      "step": 1488
    },
    {
      "epoch": 0.3384090909090909,
      "grad_norm": 0.042591895908117294,
      "learning_rate": 0.00013246871444823663,
      "loss": 0.3317,
      "step": 1489
    },
    {
      "epoch": 0.3386363636363636,
      "grad_norm": 0.0377265065908432,
      "learning_rate": 0.0001324232081911263,
      "loss": 0.3109,
      "step": 1490
    },
    {
      "epoch": 0.33886363636363637,
      "grad_norm": 0.05183181166648865,
      "learning_rate": 0.00013237770193401594,
      "loss": 0.3208,
      "step": 1491
    },
    {
      "epoch": 0.3390909090909091,
      "grad_norm": 0.056807730346918106,
      "learning_rate": 0.00013233219567690557,
      "loss": 0.3363,
      "step": 1492
    },
    {
      "epoch": 0.3393181818181818,
      "grad_norm": 0.03565094992518425,
      "learning_rate": 0.00013228668941979522,
      "loss": 0.2675,
      "step": 1493
    },
    {
      "epoch": 0.33954545454545454,
      "grad_norm": 0.06545750051736832,
      "learning_rate": 0.00013224118316268487,
      "loss": 0.3752,
      "step": 1494
    },
    {
      "epoch": 0.3397727272727273,
      "grad_norm": 0.03701506555080414,
      "learning_rate": 0.00013219567690557452,
      "loss": 0.2313,
      "step": 1495
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.038464806973934174,
      "learning_rate": 0.00013215017064846418,
      "loss": 0.3125,
      "step": 1496
    },
    {
      "epoch": 0.3402272727272727,
      "grad_norm": 0.03553036227822304,
      "learning_rate": 0.00013210466439135383,
      "loss": 0.3003,
      "step": 1497
    },
    {
      "epoch": 0.34045454545454545,
      "grad_norm": 0.04684792086482048,
      "learning_rate": 0.00013205915813424346,
      "loss": 0.3171,
      "step": 1498
    },
    {
      "epoch": 0.3406818181818182,
      "grad_norm": 0.03880953788757324,
      "learning_rate": 0.0001320136518771331,
      "loss": 0.3166,
      "step": 1499
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.0475190207362175,
      "learning_rate": 0.00013196814562002276,
      "loss": 0.3388,
      "step": 1500
    },
    {
      "epoch": 0.3411363636363636,
      "grad_norm": 0.04148665815591812,
      "learning_rate": 0.00013192263936291241,
      "loss": 0.3171,
      "step": 1501
    },
    {
      "epoch": 0.34136363636363637,
      "grad_norm": 0.049905817955732346,
      "learning_rate": 0.00013187713310580204,
      "loss": 0.3769,
      "step": 1502
    },
    {
      "epoch": 0.3415909090909091,
      "grad_norm": 0.05307742953300476,
      "learning_rate": 0.0001318316268486917,
      "loss": 0.3747,
      "step": 1503
    },
    {
      "epoch": 0.3418181818181818,
      "grad_norm": 0.04568866640329361,
      "learning_rate": 0.00013178612059158135,
      "loss": 0.3424,
      "step": 1504
    },
    {
      "epoch": 0.34204545454545454,
      "grad_norm": 0.047443196177482605,
      "learning_rate": 0.000131740614334471,
      "loss": 0.314,
      "step": 1505
    },
    {
      "epoch": 0.3422727272727273,
      "grad_norm": 0.056016940623521805,
      "learning_rate": 0.00013169510807736065,
      "loss": 0.3731,
      "step": 1506
    },
    {
      "epoch": 0.3425,
      "grad_norm": 0.04729209467768669,
      "learning_rate": 0.0001316496018202503,
      "loss": 0.3285,
      "step": 1507
    },
    {
      "epoch": 0.3427272727272727,
      "grad_norm": 0.04201473668217659,
      "learning_rate": 0.00013160409556313993,
      "loss": 0.3333,
      "step": 1508
    },
    {
      "epoch": 0.34295454545454546,
      "grad_norm": 0.03342646360397339,
      "learning_rate": 0.00013155858930602958,
      "loss": 0.246,
      "step": 1509
    },
    {
      "epoch": 0.3431818181818182,
      "grad_norm": 0.05297074094414711,
      "learning_rate": 0.00013151308304891924,
      "loss": 0.3667,
      "step": 1510
    },
    {
      "epoch": 0.3434090909090909,
      "grad_norm": 0.03872619941830635,
      "learning_rate": 0.0001314675767918089,
      "loss": 0.2657,
      "step": 1511
    },
    {
      "epoch": 0.34363636363636363,
      "grad_norm": 0.03782868757843971,
      "learning_rate": 0.00013142207053469851,
      "loss": 0.2928,
      "step": 1512
    },
    {
      "epoch": 0.34386363636363637,
      "grad_norm": 0.03721117600798607,
      "learning_rate": 0.00013137656427758817,
      "loss": 0.285,
      "step": 1513
    },
    {
      "epoch": 0.3440909090909091,
      "grad_norm": 0.04950733110308647,
      "learning_rate": 0.00013133105802047782,
      "loss": 0.2626,
      "step": 1514
    },
    {
      "epoch": 0.3443181818181818,
      "grad_norm": 0.04309536889195442,
      "learning_rate": 0.00013128555176336747,
      "loss": 0.3565,
      "step": 1515
    },
    {
      "epoch": 0.34454545454545454,
      "grad_norm": 0.04480297118425369,
      "learning_rate": 0.00013124004550625713,
      "loss": 0.3091,
      "step": 1516
    },
    {
      "epoch": 0.3447727272727273,
      "grad_norm": 0.033964261412620544,
      "learning_rate": 0.00013119453924914675,
      "loss": 0.2481,
      "step": 1517
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.04610935226082802,
      "learning_rate": 0.0001311490329920364,
      "loss": 0.3373,
      "step": 1518
    },
    {
      "epoch": 0.3452272727272727,
      "grad_norm": 0.03730078041553497,
      "learning_rate": 0.00013110352673492606,
      "loss": 0.2728,
      "step": 1519
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.04372229054570198,
      "learning_rate": 0.0001310580204778157,
      "loss": 0.3127,
      "step": 1520
    },
    {
      "epoch": 0.3456818181818182,
      "grad_norm": 0.032847385853528976,
      "learning_rate": 0.00013101251422070536,
      "loss": 0.2779,
      "step": 1521
    },
    {
      "epoch": 0.3459090909090909,
      "grad_norm": 0.04578927904367447,
      "learning_rate": 0.000130967007963595,
      "loss": 0.371,
      "step": 1522
    },
    {
      "epoch": 0.34613636363636363,
      "grad_norm": 0.05256958305835724,
      "learning_rate": 0.00013092150170648464,
      "loss": 0.3827,
      "step": 1523
    },
    {
      "epoch": 0.3463636363636364,
      "grad_norm": 0.03320566937327385,
      "learning_rate": 0.0001308759954493743,
      "loss": 0.2365,
      "step": 1524
    },
    {
      "epoch": 0.3465909090909091,
      "grad_norm": 0.051310401409864426,
      "learning_rate": 0.00013083048919226395,
      "loss": 0.3471,
      "step": 1525
    },
    {
      "epoch": 0.3468181818181818,
      "grad_norm": 0.033088233321905136,
      "learning_rate": 0.0001307849829351536,
      "loss": 0.2363,
      "step": 1526
    },
    {
      "epoch": 0.34704545454545455,
      "grad_norm": 0.04989631101489067,
      "learning_rate": 0.00013073947667804323,
      "loss": 0.3556,
      "step": 1527
    },
    {
      "epoch": 0.3472727272727273,
      "grad_norm": 0.06955236941576004,
      "learning_rate": 0.00013069397042093288,
      "loss": 0.413,
      "step": 1528
    },
    {
      "epoch": 0.3475,
      "grad_norm": 0.04058228060603142,
      "learning_rate": 0.00013064846416382253,
      "loss": 0.3009,
      "step": 1529
    },
    {
      "epoch": 0.3477272727272727,
      "grad_norm": 0.03236660361289978,
      "learning_rate": 0.00013060295790671218,
      "loss": 0.2564,
      "step": 1530
    },
    {
      "epoch": 0.34795454545454546,
      "grad_norm": 0.03850946202874184,
      "learning_rate": 0.00013055745164960184,
      "loss": 0.2507,
      "step": 1531
    },
    {
      "epoch": 0.3481818181818182,
      "grad_norm": 0.03902802616357803,
      "learning_rate": 0.00013051194539249146,
      "loss": 0.2803,
      "step": 1532
    },
    {
      "epoch": 0.3484090909090909,
      "grad_norm": 0.04129612073302269,
      "learning_rate": 0.00013046643913538111,
      "loss": 0.3452,
      "step": 1533
    },
    {
      "epoch": 0.34863636363636363,
      "grad_norm": 0.0608266144990921,
      "learning_rate": 0.00013042093287827077,
      "loss": 0.4334,
      "step": 1534
    },
    {
      "epoch": 0.3488636363636364,
      "grad_norm": 0.041843462735414505,
      "learning_rate": 0.00013037542662116042,
      "loss": 0.3135,
      "step": 1535
    },
    {
      "epoch": 0.3490909090909091,
      "grad_norm": 0.04169870540499687,
      "learning_rate": 0.00013032992036405007,
      "loss": 0.3753,
      "step": 1536
    },
    {
      "epoch": 0.3493181818181818,
      "grad_norm": 0.048582274466753006,
      "learning_rate": 0.0001302844141069397,
      "loss": 0.3041,
      "step": 1537
    },
    {
      "epoch": 0.34954545454545455,
      "grad_norm": 0.055961690843105316,
      "learning_rate": 0.00013023890784982935,
      "loss": 0.3205,
      "step": 1538
    },
    {
      "epoch": 0.3497727272727273,
      "grad_norm": 0.05285101756453514,
      "learning_rate": 0.000130193401592719,
      "loss": 0.358,
      "step": 1539
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.05039598420262337,
      "learning_rate": 0.00013014789533560866,
      "loss": 0.337,
      "step": 1540
    },
    {
      "epoch": 0.3502272727272727,
      "grad_norm": 0.0500762015581131,
      "learning_rate": 0.0001301023890784983,
      "loss": 0.3462,
      "step": 1541
    },
    {
      "epoch": 0.35045454545454546,
      "grad_norm": 0.06632298976182938,
      "learning_rate": 0.00013005688282138794,
      "loss": 0.3543,
      "step": 1542
    },
    {
      "epoch": 0.3506818181818182,
      "grad_norm": 0.046442046761512756,
      "learning_rate": 0.0001300113765642776,
      "loss": 0.3292,
      "step": 1543
    },
    {
      "epoch": 0.3509090909090909,
      "grad_norm": 0.03962089121341705,
      "learning_rate": 0.00012996587030716724,
      "loss": 0.2891,
      "step": 1544
    },
    {
      "epoch": 0.35113636363636364,
      "grad_norm": 0.05683829262852669,
      "learning_rate": 0.0001299203640500569,
      "loss": 0.3188,
      "step": 1545
    },
    {
      "epoch": 0.3513636363636364,
      "grad_norm": 0.04803222417831421,
      "learning_rate": 0.00012987485779294655,
      "loss": 0.3619,
      "step": 1546
    },
    {
      "epoch": 0.35159090909090907,
      "grad_norm": 0.06338608264923096,
      "learning_rate": 0.00012982935153583617,
      "loss": 0.3583,
      "step": 1547
    },
    {
      "epoch": 0.3518181818181818,
      "grad_norm": 0.04132939875125885,
      "learning_rate": 0.00012978384527872583,
      "loss": 0.2917,
      "step": 1548
    },
    {
      "epoch": 0.35204545454545455,
      "grad_norm": 0.04761924222111702,
      "learning_rate": 0.00012973833902161548,
      "loss": 0.3469,
      "step": 1549
    },
    {
      "epoch": 0.3522727272727273,
      "grad_norm": 0.04837239906191826,
      "learning_rate": 0.00012969283276450513,
      "loss": 0.3022,
      "step": 1550
    },
    {
      "epoch": 0.3525,
      "grad_norm": 0.055399682372808456,
      "learning_rate": 0.00012964732650739478,
      "loss": 0.3339,
      "step": 1551
    },
    {
      "epoch": 0.3527272727272727,
      "grad_norm": 0.0582544282078743,
      "learning_rate": 0.0001296018202502844,
      "loss": 0.3745,
      "step": 1552
    },
    {
      "epoch": 0.35295454545454547,
      "grad_norm": 0.044436015188694,
      "learning_rate": 0.00012955631399317406,
      "loss": 0.3641,
      "step": 1553
    },
    {
      "epoch": 0.3531818181818182,
      "grad_norm": 0.04922989755868912,
      "learning_rate": 0.00012951080773606372,
      "loss": 0.3441,
      "step": 1554
    },
    {
      "epoch": 0.3534090909090909,
      "grad_norm": 0.04337049648165703,
      "learning_rate": 0.00012946530147895337,
      "loss": 0.2976,
      "step": 1555
    },
    {
      "epoch": 0.35363636363636364,
      "grad_norm": 0.06812050193548203,
      "learning_rate": 0.000129419795221843,
      "loss": 0.3788,
      "step": 1556
    },
    {
      "epoch": 0.3538636363636364,
      "grad_norm": 0.061449915170669556,
      "learning_rate": 0.00012937428896473265,
      "loss": 0.3949,
      "step": 1557
    },
    {
      "epoch": 0.35409090909090907,
      "grad_norm": 0.047943662852048874,
      "learning_rate": 0.0001293287827076223,
      "loss": 0.3384,
      "step": 1558
    },
    {
      "epoch": 0.3543181818181818,
      "grad_norm": 0.04311993345618248,
      "learning_rate": 0.00012928327645051195,
      "loss": 0.2917,
      "step": 1559
    },
    {
      "epoch": 0.35454545454545455,
      "grad_norm": 0.039006832987070084,
      "learning_rate": 0.0001292377701934016,
      "loss": 0.2783,
      "step": 1560
    },
    {
      "epoch": 0.3547727272727273,
      "grad_norm": 0.05038827657699585,
      "learning_rate": 0.00012919226393629126,
      "loss": 0.359,
      "step": 1561
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.04202396422624588,
      "learning_rate": 0.00012914675767918088,
      "loss": 0.3054,
      "step": 1562
    },
    {
      "epoch": 0.3552272727272727,
      "grad_norm": 0.05420859158039093,
      "learning_rate": 0.00012910125142207054,
      "loss": 0.3703,
      "step": 1563
    },
    {
      "epoch": 0.35545454545454547,
      "grad_norm": 0.046283263713121414,
      "learning_rate": 0.0001290557451649602,
      "loss": 0.3052,
      "step": 1564
    },
    {
      "epoch": 0.35568181818181815,
      "grad_norm": 0.045386359095573425,
      "learning_rate": 0.00012901023890784984,
      "loss": 0.276,
      "step": 1565
    },
    {
      "epoch": 0.3559090909090909,
      "grad_norm": 0.04883816838264465,
      "learning_rate": 0.00012896473265073947,
      "loss": 0.3486,
      "step": 1566
    },
    {
      "epoch": 0.35613636363636364,
      "grad_norm": 0.054816242307424545,
      "learning_rate": 0.00012891922639362912,
      "loss": 0.3708,
      "step": 1567
    },
    {
      "epoch": 0.3563636363636364,
      "grad_norm": 0.04695812612771988,
      "learning_rate": 0.00012887372013651877,
      "loss": 0.3448,
      "step": 1568
    },
    {
      "epoch": 0.35659090909090907,
      "grad_norm": 0.040235139429569244,
      "learning_rate": 0.00012882821387940843,
      "loss": 0.3076,
      "step": 1569
    },
    {
      "epoch": 0.3568181818181818,
      "grad_norm": 0.04508699104189873,
      "learning_rate": 0.00012878270762229808,
      "loss": 0.3088,
      "step": 1570
    },
    {
      "epoch": 0.35704545454545455,
      "grad_norm": 0.06617467105388641,
      "learning_rate": 0.00012873720136518773,
      "loss": 0.3305,
      "step": 1571
    },
    {
      "epoch": 0.3572727272727273,
      "grad_norm": 0.04114747792482376,
      "learning_rate": 0.00012869169510807736,
      "loss": 0.3149,
      "step": 1572
    },
    {
      "epoch": 0.3575,
      "grad_norm": 0.04492200165987015,
      "learning_rate": 0.000128646188850967,
      "loss": 0.3222,
      "step": 1573
    },
    {
      "epoch": 0.3577272727272727,
      "grad_norm": 0.046987805515527725,
      "learning_rate": 0.00012860068259385666,
      "loss": 0.367,
      "step": 1574
    },
    {
      "epoch": 0.35795454545454547,
      "grad_norm": 0.04073883593082428,
      "learning_rate": 0.00012855517633674632,
      "loss": 0.3078,
      "step": 1575
    },
    {
      "epoch": 0.35818181818181816,
      "grad_norm": 0.06199023500084877,
      "learning_rate": 0.00012850967007963594,
      "loss": 0.3528,
      "step": 1576
    },
    {
      "epoch": 0.3584090909090909,
      "grad_norm": 0.0458553209900856,
      "learning_rate": 0.0001284641638225256,
      "loss": 0.2936,
      "step": 1577
    },
    {
      "epoch": 0.35863636363636364,
      "grad_norm": 0.05107702687382698,
      "learning_rate": 0.00012841865756541525,
      "loss": 0.3349,
      "step": 1578
    },
    {
      "epoch": 0.3588636363636364,
      "grad_norm": 0.044087305665016174,
      "learning_rate": 0.0001283731513083049,
      "loss": 0.2981,
      "step": 1579
    },
    {
      "epoch": 0.35909090909090907,
      "grad_norm": 0.05722725763916969,
      "learning_rate": 0.00012832764505119455,
      "loss": 0.3595,
      "step": 1580
    },
    {
      "epoch": 0.3593181818181818,
      "grad_norm": 0.04645616561174393,
      "learning_rate": 0.0001282821387940842,
      "loss": 0.3144,
      "step": 1581
    },
    {
      "epoch": 0.35954545454545456,
      "grad_norm": 0.04060624539852142,
      "learning_rate": 0.00012823663253697383,
      "loss": 0.3257,
      "step": 1582
    },
    {
      "epoch": 0.3597727272727273,
      "grad_norm": 0.06286461651325226,
      "learning_rate": 0.00012819112627986348,
      "loss": 0.3671,
      "step": 1583
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.043882761150598526,
      "learning_rate": 0.00012814562002275314,
      "loss": 0.3271,
      "step": 1584
    },
    {
      "epoch": 0.36022727272727273,
      "grad_norm": 0.05044231936335564,
      "learning_rate": 0.0001281001137656428,
      "loss": 0.3611,
      "step": 1585
    },
    {
      "epoch": 0.36045454545454547,
      "grad_norm": 0.06453859061002731,
      "learning_rate": 0.00012805460750853242,
      "loss": 0.3628,
      "step": 1586
    },
    {
      "epoch": 0.36068181818181816,
      "grad_norm": 0.04637627676129341,
      "learning_rate": 0.00012800910125142207,
      "loss": 0.322,
      "step": 1587
    },
    {
      "epoch": 0.3609090909090909,
      "grad_norm": 0.03686480596661568,
      "learning_rate": 0.00012796359499431172,
      "loss": 0.2873,
      "step": 1588
    },
    {
      "epoch": 0.36113636363636364,
      "grad_norm": 0.04526785761117935,
      "learning_rate": 0.00012791808873720137,
      "loss": 0.3022,
      "step": 1589
    },
    {
      "epoch": 0.3613636363636364,
      "grad_norm": 0.04075682908296585,
      "learning_rate": 0.00012787258248009103,
      "loss": 0.3069,
      "step": 1590
    },
    {
      "epoch": 0.3615909090909091,
      "grad_norm": 0.04299049824476242,
      "learning_rate": 0.00012782707622298068,
      "loss": 0.3388,
      "step": 1591
    },
    {
      "epoch": 0.3618181818181818,
      "grad_norm": 0.048923641443252563,
      "learning_rate": 0.0001277815699658703,
      "loss": 0.296,
      "step": 1592
    },
    {
      "epoch": 0.36204545454545456,
      "grad_norm": 0.04216573014855385,
      "learning_rate": 0.00012773606370875996,
      "loss": 0.2958,
      "step": 1593
    },
    {
      "epoch": 0.36227272727272725,
      "grad_norm": 0.04798383265733719,
      "learning_rate": 0.0001276905574516496,
      "loss": 0.3638,
      "step": 1594
    },
    {
      "epoch": 0.3625,
      "grad_norm": 0.03127703443169594,
      "learning_rate": 0.00012764505119453924,
      "loss": 0.2564,
      "step": 1595
    },
    {
      "epoch": 0.36272727272727273,
      "grad_norm": 0.045237526297569275,
      "learning_rate": 0.0001275995449374289,
      "loss": 0.2943,
      "step": 1596
    },
    {
      "epoch": 0.3629545454545455,
      "grad_norm": 0.04798236861824989,
      "learning_rate": 0.00012755403868031854,
      "loss": 0.3942,
      "step": 1597
    },
    {
      "epoch": 0.36318181818181816,
      "grad_norm": 0.053661372512578964,
      "learning_rate": 0.0001275085324232082,
      "loss": 0.3355,
      "step": 1598
    },
    {
      "epoch": 0.3634090909090909,
      "grad_norm": 0.04121607542037964,
      "learning_rate": 0.00012746302616609785,
      "loss": 0.2643,
      "step": 1599
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.0510333888232708,
      "learning_rate": 0.0001274175199089875,
      "loss": 0.3291,
      "step": 1600
    },
    {
      "epoch": 0.3638636363636364,
      "grad_norm": 0.05545340105891228,
      "learning_rate": 0.00012737201365187715,
      "loss": 0.2879,
      "step": 1601
    },
    {
      "epoch": 0.3640909090909091,
      "grad_norm": 0.04116753861308098,
      "learning_rate": 0.00012732650739476678,
      "loss": 0.3296,
      "step": 1602
    },
    {
      "epoch": 0.3643181818181818,
      "grad_norm": 0.04127884283661842,
      "learning_rate": 0.00012728100113765643,
      "loss": 0.272,
      "step": 1603
    },
    {
      "epoch": 0.36454545454545456,
      "grad_norm": 0.04543406143784523,
      "learning_rate": 0.00012723549488054609,
      "loss": 0.3258,
      "step": 1604
    },
    {
      "epoch": 0.36477272727272725,
      "grad_norm": 0.05436177924275398,
      "learning_rate": 0.0001271899886234357,
      "loss": 0.356,
      "step": 1605
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.041266005486249924,
      "learning_rate": 0.00012714448236632536,
      "loss": 0.34,
      "step": 1606
    },
    {
      "epoch": 0.36522727272727273,
      "grad_norm": 0.0543077290058136,
      "learning_rate": 0.00012709897610921502,
      "loss": 0.3611,
      "step": 1607
    },
    {
      "epoch": 0.3654545454545455,
      "grad_norm": 0.05709484592080116,
      "learning_rate": 0.00012705346985210467,
      "loss": 0.356,
      "step": 1608
    },
    {
      "epoch": 0.36568181818181816,
      "grad_norm": 0.05633305013179779,
      "learning_rate": 0.00012700796359499432,
      "loss": 0.3663,
      "step": 1609
    },
    {
      "epoch": 0.3659090909090909,
      "grad_norm": 0.06031415984034538,
      "learning_rate": 0.00012696245733788398,
      "loss": 0.3737,
      "step": 1610
    },
    {
      "epoch": 0.36613636363636365,
      "grad_norm": 0.04843110963702202,
      "learning_rate": 0.00012691695108077363,
      "loss": 0.3499,
      "step": 1611
    },
    {
      "epoch": 0.3663636363636364,
      "grad_norm": 0.047511931508779526,
      "learning_rate": 0.00012687144482366325,
      "loss": 0.3415,
      "step": 1612
    },
    {
      "epoch": 0.3665909090909091,
      "grad_norm": 0.041437387466430664,
      "learning_rate": 0.0001268259385665529,
      "loss": 0.3187,
      "step": 1613
    },
    {
      "epoch": 0.3668181818181818,
      "grad_norm": 0.03990170732140541,
      "learning_rate": 0.00012678043230944256,
      "loss": 0.28,
      "step": 1614
    },
    {
      "epoch": 0.36704545454545456,
      "grad_norm": 0.04944140464067459,
      "learning_rate": 0.00012673492605233219,
      "loss": 0.3637,
      "step": 1615
    },
    {
      "epoch": 0.36727272727272725,
      "grad_norm": 0.04284515231847763,
      "learning_rate": 0.00012668941979522184,
      "loss": 0.3503,
      "step": 1616
    },
    {
      "epoch": 0.3675,
      "grad_norm": 0.038432423025369644,
      "learning_rate": 0.0001266439135381115,
      "loss": 0.3086,
      "step": 1617
    },
    {
      "epoch": 0.36772727272727274,
      "grad_norm": 0.05918826535344124,
      "learning_rate": 0.00012659840728100114,
      "loss": 0.4042,
      "step": 1618
    },
    {
      "epoch": 0.3679545454545455,
      "grad_norm": 0.03172678500413895,
      "learning_rate": 0.0001265529010238908,
      "loss": 0.276,
      "step": 1619
    },
    {
      "epoch": 0.36818181818181817,
      "grad_norm": 0.04716943949460983,
      "learning_rate": 0.00012650739476678045,
      "loss": 0.3226,
      "step": 1620
    },
    {
      "epoch": 0.3684090909090909,
      "grad_norm": 0.05191921442747116,
      "learning_rate": 0.0001264618885096701,
      "loss": 0.4121,
      "step": 1621
    },
    {
      "epoch": 0.36863636363636365,
      "grad_norm": 0.036293528974056244,
      "learning_rate": 0.00012641638225255973,
      "loss": 0.3194,
      "step": 1622
    },
    {
      "epoch": 0.36886363636363634,
      "grad_norm": 0.04539438337087631,
      "learning_rate": 0.00012637087599544938,
      "loss": 0.2847,
      "step": 1623
    },
    {
      "epoch": 0.3690909090909091,
      "grad_norm": 0.0516371950507164,
      "learning_rate": 0.000126325369738339,
      "loss": 0.3482,
      "step": 1624
    },
    {
      "epoch": 0.3693181818181818,
      "grad_norm": 0.04577389732003212,
      "learning_rate": 0.00012627986348122866,
      "loss": 0.3435,
      "step": 1625
    },
    {
      "epoch": 0.36954545454545457,
      "grad_norm": 0.037127383053302765,
      "learning_rate": 0.0001262343572241183,
      "loss": 0.2936,
      "step": 1626
    },
    {
      "epoch": 0.36977272727272725,
      "grad_norm": 0.03921964019536972,
      "learning_rate": 0.00012618885096700796,
      "loss": 0.3001,
      "step": 1627
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.05181223899126053,
      "learning_rate": 0.00012614334470989762,
      "loss": 0.3364,
      "step": 1628
    },
    {
      "epoch": 0.37022727272727274,
      "grad_norm": 0.050251305103302,
      "learning_rate": 0.00012609783845278727,
      "loss": 0.3577,
      "step": 1629
    },
    {
      "epoch": 0.3704545454545455,
      "grad_norm": 0.05017516016960144,
      "learning_rate": 0.00012605233219567692,
      "loss": 0.3658,
      "step": 1630
    },
    {
      "epoch": 0.37068181818181817,
      "grad_norm": 0.04938841983675957,
      "learning_rate": 0.00012600682593856658,
      "loss": 0.3649,
      "step": 1631
    },
    {
      "epoch": 0.3709090909090909,
      "grad_norm": 0.0477607399225235,
      "learning_rate": 0.0001259613196814562,
      "loss": 0.3486,
      "step": 1632
    },
    {
      "epoch": 0.37113636363636365,
      "grad_norm": 0.04059687629342079,
      "learning_rate": 0.00012591581342434585,
      "loss": 0.3206,
      "step": 1633
    },
    {
      "epoch": 0.37136363636363634,
      "grad_norm": 0.040450021624565125,
      "learning_rate": 0.00012587030716723548,
      "loss": 0.3376,
      "step": 1634
    },
    {
      "epoch": 0.3715909090909091,
      "grad_norm": 0.04667782410979271,
      "learning_rate": 0.00012582480091012513,
      "loss": 0.286,
      "step": 1635
    },
    {
      "epoch": 0.3718181818181818,
      "grad_norm": 0.047461770474910736,
      "learning_rate": 0.00012577929465301479,
      "loss": 0.391,
      "step": 1636
    },
    {
      "epoch": 0.37204545454545457,
      "grad_norm": 0.06001793593168259,
      "learning_rate": 0.00012573378839590444,
      "loss": 0.3158,
      "step": 1637
    },
    {
      "epoch": 0.37227272727272726,
      "grad_norm": 0.04367893561720848,
      "learning_rate": 0.0001256882821387941,
      "loss": 0.3248,
      "step": 1638
    },
    {
      "epoch": 0.3725,
      "grad_norm": 0.04012477770447731,
      "learning_rate": 0.00012564277588168374,
      "loss": 0.2827,
      "step": 1639
    },
    {
      "epoch": 0.37272727272727274,
      "grad_norm": 0.052837636321783066,
      "learning_rate": 0.0001255972696245734,
      "loss": 0.3216,
      "step": 1640
    },
    {
      "epoch": 0.3729545454545454,
      "grad_norm": 0.04662797972559929,
      "learning_rate": 0.00012555176336746305,
      "loss": 0.3055,
      "step": 1641
    },
    {
      "epoch": 0.37318181818181817,
      "grad_norm": 0.04359141364693642,
      "learning_rate": 0.00012550625711035268,
      "loss": 0.3009,
      "step": 1642
    },
    {
      "epoch": 0.3734090909090909,
      "grad_norm": 0.04370607063174248,
      "learning_rate": 0.00012546075085324233,
      "loss": 0.2849,
      "step": 1643
    },
    {
      "epoch": 0.37363636363636366,
      "grad_norm": 0.0325692854821682,
      "learning_rate": 0.00012541524459613195,
      "loss": 0.2425,
      "step": 1644
    },
    {
      "epoch": 0.37386363636363634,
      "grad_norm": 0.05249257758259773,
      "learning_rate": 0.0001253697383390216,
      "loss": 0.3525,
      "step": 1645
    },
    {
      "epoch": 0.3740909090909091,
      "grad_norm": 0.04662397503852844,
      "learning_rate": 0.00012532423208191126,
      "loss": 0.3459,
      "step": 1646
    },
    {
      "epoch": 0.3743181818181818,
      "grad_norm": 0.048797860741615295,
      "learning_rate": 0.0001252787258248009,
      "loss": 0.378,
      "step": 1647
    },
    {
      "epoch": 0.37454545454545457,
      "grad_norm": 0.04469747468829155,
      "learning_rate": 0.00012523321956769057,
      "loss": 0.312,
      "step": 1648
    },
    {
      "epoch": 0.37477272727272726,
      "grad_norm": 0.04194385185837746,
      "learning_rate": 0.00012518771331058022,
      "loss": 0.2989,
      "step": 1649
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.050061117857694626,
      "learning_rate": 0.00012514220705346987,
      "loss": 0.3026,
      "step": 1650
    },
    {
      "epoch": 0.37522727272727274,
      "grad_norm": 0.04386874660849571,
      "learning_rate": 0.00012509670079635952,
      "loss": 0.3411,
      "step": 1651
    },
    {
      "epoch": 0.37545454545454543,
      "grad_norm": 0.04484940320253372,
      "learning_rate": 0.00012505119453924915,
      "loss": 0.338,
      "step": 1652
    },
    {
      "epoch": 0.3756818181818182,
      "grad_norm": 0.03379763290286064,
      "learning_rate": 0.0001250056882821388,
      "loss": 0.2908,
      "step": 1653
    },
    {
      "epoch": 0.3759090909090909,
      "grad_norm": 0.04675324261188507,
      "learning_rate": 0.00012496018202502843,
      "loss": 0.2963,
      "step": 1654
    },
    {
      "epoch": 0.37613636363636366,
      "grad_norm": 0.035425685346126556,
      "learning_rate": 0.00012491467576791808,
      "loss": 0.3013,
      "step": 1655
    },
    {
      "epoch": 0.37636363636363634,
      "grad_norm": 0.03831182047724724,
      "learning_rate": 0.00012486916951080773,
      "loss": 0.2583,
      "step": 1656
    },
    {
      "epoch": 0.3765909090909091,
      "grad_norm": 0.04609563574194908,
      "learning_rate": 0.0001248236632536974,
      "loss": 0.3563,
      "step": 1657
    },
    {
      "epoch": 0.37681818181818183,
      "grad_norm": 0.040149785578250885,
      "learning_rate": 0.00012477815699658704,
      "loss": 0.3045,
      "step": 1658
    },
    {
      "epoch": 0.3770454545454546,
      "grad_norm": 0.04020337760448456,
      "learning_rate": 0.0001247326507394767,
      "loss": 0.3174,
      "step": 1659
    },
    {
      "epoch": 0.37727272727272726,
      "grad_norm": 0.04266367480158806,
      "learning_rate": 0.00012468714448236635,
      "loss": 0.3495,
      "step": 1660
    },
    {
      "epoch": 0.3775,
      "grad_norm": 0.03426619991660118,
      "learning_rate": 0.000124641638225256,
      "loss": 0.2973,
      "step": 1661
    },
    {
      "epoch": 0.37772727272727274,
      "grad_norm": 0.04031279683113098,
      "learning_rate": 0.00012459613196814562,
      "loss": 0.3089,
      "step": 1662
    },
    {
      "epoch": 0.37795454545454543,
      "grad_norm": 0.04539664834737778,
      "learning_rate": 0.00012455062571103525,
      "loss": 0.3332,
      "step": 1663
    },
    {
      "epoch": 0.3781818181818182,
      "grad_norm": 0.038926828652620316,
      "learning_rate": 0.0001245051194539249,
      "loss": 0.3157,
      "step": 1664
    },
    {
      "epoch": 0.3784090909090909,
      "grad_norm": 0.03659219667315483,
      "learning_rate": 0.00012445961319681456,
      "loss": 0.3197,
      "step": 1665
    },
    {
      "epoch": 0.37863636363636366,
      "grad_norm": 0.038739532232284546,
      "learning_rate": 0.0001244141069397042,
      "loss": 0.2956,
      "step": 1666
    },
    {
      "epoch": 0.37886363636363635,
      "grad_norm": 0.04585547372698784,
      "learning_rate": 0.00012436860068259386,
      "loss": 0.4098,
      "step": 1667
    },
    {
      "epoch": 0.3790909090909091,
      "grad_norm": 0.0410926453769207,
      "learning_rate": 0.00012432309442548351,
      "loss": 0.32,
      "step": 1668
    },
    {
      "epoch": 0.37931818181818183,
      "grad_norm": 0.05202524736523628,
      "learning_rate": 0.00012427758816837317,
      "loss": 0.3452,
      "step": 1669
    },
    {
      "epoch": 0.3795454545454545,
      "grad_norm": 0.04587319493293762,
      "learning_rate": 0.00012423208191126282,
      "loss": 0.3712,
      "step": 1670
    },
    {
      "epoch": 0.37977272727272726,
      "grad_norm": 0.036333151161670685,
      "learning_rate": 0.00012418657565415247,
      "loss": 0.2775,
      "step": 1671
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.043427664786577225,
      "learning_rate": 0.0001241410693970421,
      "loss": 0.3374,
      "step": 1672
    },
    {
      "epoch": 0.38022727272727275,
      "grad_norm": 0.04089196026325226,
      "learning_rate": 0.00012409556313993172,
      "loss": 0.3169,
      "step": 1673
    },
    {
      "epoch": 0.38045454545454543,
      "grad_norm": 0.04223395884037018,
      "learning_rate": 0.00012405005688282138,
      "loss": 0.3441,
      "step": 1674
    },
    {
      "epoch": 0.3806818181818182,
      "grad_norm": 0.04028167203068733,
      "learning_rate": 0.00012400455062571103,
      "loss": 0.3118,
      "step": 1675
    },
    {
      "epoch": 0.3809090909090909,
      "grad_norm": 0.05313415080308914,
      "learning_rate": 0.00012395904436860068,
      "loss": 0.3315,
      "step": 1676
    },
    {
      "epoch": 0.38113636363636366,
      "grad_norm": 0.04281110316514969,
      "learning_rate": 0.00012391353811149033,
      "loss": 0.2761,
      "step": 1677
    },
    {
      "epoch": 0.38136363636363635,
      "grad_norm": 0.037792544811964035,
      "learning_rate": 0.00012386803185438,
      "loss": 0.2318,
      "step": 1678
    },
    {
      "epoch": 0.3815909090909091,
      "grad_norm": 0.046369992196559906,
      "learning_rate": 0.00012382252559726964,
      "loss": 0.2865,
      "step": 1679
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.05609967187047005,
      "learning_rate": 0.0001237770193401593,
      "loss": 0.3789,
      "step": 1680
    },
    {
      "epoch": 0.3820454545454545,
      "grad_norm": 0.04253961145877838,
      "learning_rate": 0.00012373151308304895,
      "loss": 0.338,
      "step": 1681
    },
    {
      "epoch": 0.38227272727272726,
      "grad_norm": 0.03935279697179794,
      "learning_rate": 0.00012368600682593857,
      "loss": 0.3545,
      "step": 1682
    },
    {
      "epoch": 0.3825,
      "grad_norm": 0.03355735167860985,
      "learning_rate": 0.0001236405005688282,
      "loss": 0.2842,
      "step": 1683
    },
    {
      "epoch": 0.38272727272727275,
      "grad_norm": 0.055170658975839615,
      "learning_rate": 0.00012359499431171785,
      "loss": 0.3366,
      "step": 1684
    },
    {
      "epoch": 0.38295454545454544,
      "grad_norm": 0.05960501730442047,
      "learning_rate": 0.0001235494880546075,
      "loss": 0.3589,
      "step": 1685
    },
    {
      "epoch": 0.3831818181818182,
      "grad_norm": 0.049229707568883896,
      "learning_rate": 0.00012350398179749716,
      "loss": 0.301,
      "step": 1686
    },
    {
      "epoch": 0.3834090909090909,
      "grad_norm": 0.04942586272954941,
      "learning_rate": 0.0001234584755403868,
      "loss": 0.3779,
      "step": 1687
    },
    {
      "epoch": 0.3836363636363636,
      "grad_norm": 0.04524031654000282,
      "learning_rate": 0.00012341296928327646,
      "loss": 0.3251,
      "step": 1688
    },
    {
      "epoch": 0.38386363636363635,
      "grad_norm": 0.054354555904865265,
      "learning_rate": 0.00012336746302616611,
      "loss": 0.3373,
      "step": 1689
    },
    {
      "epoch": 0.3840909090909091,
      "grad_norm": 0.053184326738119125,
      "learning_rate": 0.00012332195676905577,
      "loss": 0.3334,
      "step": 1690
    },
    {
      "epoch": 0.38431818181818184,
      "grad_norm": 0.05175337940454483,
      "learning_rate": 0.00012327645051194542,
      "loss": 0.3497,
      "step": 1691
    },
    {
      "epoch": 0.3845454545454545,
      "grad_norm": 0.052650898694992065,
      "learning_rate": 0.00012323094425483505,
      "loss": 0.3089,
      "step": 1692
    },
    {
      "epoch": 0.38477272727272727,
      "grad_norm": 0.0520278736948967,
      "learning_rate": 0.00012318543799772467,
      "loss": 0.4087,
      "step": 1693
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.037517618387937546,
      "learning_rate": 0.00012313993174061432,
      "loss": 0.2966,
      "step": 1694
    },
    {
      "epoch": 0.38522727272727275,
      "grad_norm": 0.04074477404356003,
      "learning_rate": 0.00012309442548350398,
      "loss": 0.3316,
      "step": 1695
    },
    {
      "epoch": 0.38545454545454544,
      "grad_norm": 0.03900190815329552,
      "learning_rate": 0.00012304891922639363,
      "loss": 0.2661,
      "step": 1696
    },
    {
      "epoch": 0.3856818181818182,
      "grad_norm": 0.04924069344997406,
      "learning_rate": 0.00012300341296928328,
      "loss": 0.3496,
      "step": 1697
    },
    {
      "epoch": 0.3859090909090909,
      "grad_norm": 0.05012072995305061,
      "learning_rate": 0.00012295790671217294,
      "loss": 0.3108,
      "step": 1698
    },
    {
      "epoch": 0.3861363636363636,
      "grad_norm": 0.04863515868782997,
      "learning_rate": 0.0001229124004550626,
      "loss": 0.3225,
      "step": 1699
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.04370320960879326,
      "learning_rate": 0.00012286689419795224,
      "loss": 0.3008,
      "step": 1700
    },
    {
      "epoch": 0.3865909090909091,
      "grad_norm": 0.03962087631225586,
      "learning_rate": 0.0001228213879408419,
      "loss": 0.308,
      "step": 1701
    },
    {
      "epoch": 0.38681818181818184,
      "grad_norm": 0.041883986443281174,
      "learning_rate": 0.00012277588168373152,
      "loss": 0.3187,
      "step": 1702
    },
    {
      "epoch": 0.3870454545454545,
      "grad_norm": 0.06058477982878685,
      "learning_rate": 0.00012273037542662115,
      "loss": 0.4056,
      "step": 1703
    },
    {
      "epoch": 0.38727272727272727,
      "grad_norm": 0.04733597859740257,
      "learning_rate": 0.0001226848691695108,
      "loss": 0.3085,
      "step": 1704
    },
    {
      "epoch": 0.3875,
      "grad_norm": 0.052243199199438095,
      "learning_rate": 0.00012263936291240045,
      "loss": 0.3399,
      "step": 1705
    },
    {
      "epoch": 0.38772727272727275,
      "grad_norm": 0.04216041788458824,
      "learning_rate": 0.0001225938566552901,
      "loss": 0.3308,
      "step": 1706
    },
    {
      "epoch": 0.38795454545454544,
      "grad_norm": 0.04848930612206459,
      "learning_rate": 0.00012254835039817976,
      "loss": 0.3464,
      "step": 1707
    },
    {
      "epoch": 0.3881818181818182,
      "grad_norm": 0.051122747361660004,
      "learning_rate": 0.0001225028441410694,
      "loss": 0.366,
      "step": 1708
    },
    {
      "epoch": 0.3884090909090909,
      "grad_norm": 0.051691554486751556,
      "learning_rate": 0.00012245733788395906,
      "loss": 0.3231,
      "step": 1709
    },
    {
      "epoch": 0.3886363636363636,
      "grad_norm": 0.0509880930185318,
      "learning_rate": 0.00012241183162684872,
      "loss": 0.3779,
      "step": 1710
    },
    {
      "epoch": 0.38886363636363636,
      "grad_norm": 0.03650668263435364,
      "learning_rate": 0.00012236632536973837,
      "loss": 0.2674,
      "step": 1711
    },
    {
      "epoch": 0.3890909090909091,
      "grad_norm": 0.051568157970905304,
      "learning_rate": 0.000122320819112628,
      "loss": 0.3523,
      "step": 1712
    },
    {
      "epoch": 0.38931818181818184,
      "grad_norm": 0.03472307696938515,
      "learning_rate": 0.00012227531285551762,
      "loss": 0.235,
      "step": 1713
    },
    {
      "epoch": 0.38954545454545453,
      "grad_norm": 0.04318653419613838,
      "learning_rate": 0.00012222980659840727,
      "loss": 0.3144,
      "step": 1714
    },
    {
      "epoch": 0.38977272727272727,
      "grad_norm": 0.04776621609926224,
      "learning_rate": 0.00012218430034129693,
      "loss": 0.2933,
      "step": 1715
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.04687734320759773,
      "learning_rate": 0.00012213879408418658,
      "loss": 0.3261,
      "step": 1716
    },
    {
      "epoch": 0.3902272727272727,
      "grad_norm": 0.04370983690023422,
      "learning_rate": 0.00012209328782707623,
      "loss": 0.3601,
      "step": 1717
    },
    {
      "epoch": 0.39045454545454544,
      "grad_norm": 0.058957479894161224,
      "learning_rate": 0.00012204778156996588,
      "loss": 0.4009,
      "step": 1718
    },
    {
      "epoch": 0.3906818181818182,
      "grad_norm": 0.04398082569241524,
      "learning_rate": 0.00012200227531285552,
      "loss": 0.3656,
      "step": 1719
    },
    {
      "epoch": 0.39090909090909093,
      "grad_norm": 0.06306666135787964,
      "learning_rate": 0.00012195676905574518,
      "loss": 0.4286,
      "step": 1720
    },
    {
      "epoch": 0.3911363636363636,
      "grad_norm": 0.040047671645879745,
      "learning_rate": 0.00012191126279863483,
      "loss": 0.2698,
      "step": 1721
    },
    {
      "epoch": 0.39136363636363636,
      "grad_norm": 0.04876500368118286,
      "learning_rate": 0.00012186575654152445,
      "loss": 0.3425,
      "step": 1722
    },
    {
      "epoch": 0.3915909090909091,
      "grad_norm": 0.041274379938840866,
      "learning_rate": 0.00012182025028441411,
      "loss": 0.2897,
      "step": 1723
    },
    {
      "epoch": 0.39181818181818184,
      "grad_norm": 0.04444827511906624,
      "learning_rate": 0.00012177474402730376,
      "loss": 0.3336,
      "step": 1724
    },
    {
      "epoch": 0.39204545454545453,
      "grad_norm": 0.032784201204776764,
      "learning_rate": 0.0001217292377701934,
      "loss": 0.2383,
      "step": 1725
    },
    {
      "epoch": 0.3922727272727273,
      "grad_norm": 0.04777374491095543,
      "learning_rate": 0.00012168373151308305,
      "loss": 0.3336,
      "step": 1726
    },
    {
      "epoch": 0.3925,
      "grad_norm": 0.041806016117334366,
      "learning_rate": 0.0001216382252559727,
      "loss": 0.2815,
      "step": 1727
    },
    {
      "epoch": 0.3927272727272727,
      "grad_norm": 0.04949020594358444,
      "learning_rate": 0.00012159271899886236,
      "loss": 0.3505,
      "step": 1728
    },
    {
      "epoch": 0.39295454545454545,
      "grad_norm": 0.04975499212741852,
      "learning_rate": 0.000121547212741752,
      "loss": 0.3372,
      "step": 1729
    },
    {
      "epoch": 0.3931818181818182,
      "grad_norm": 0.046528272330760956,
      "learning_rate": 0.00012150170648464165,
      "loss": 0.3533,
      "step": 1730
    },
    {
      "epoch": 0.39340909090909093,
      "grad_norm": 0.047123972326517105,
      "learning_rate": 0.00012145620022753128,
      "loss": 0.343,
      "step": 1731
    },
    {
      "epoch": 0.3936363636363636,
      "grad_norm": 0.03780811280012131,
      "learning_rate": 0.00012141069397042093,
      "loss": 0.2762,
      "step": 1732
    },
    {
      "epoch": 0.39386363636363636,
      "grad_norm": 0.0641237124800682,
      "learning_rate": 0.00012136518771331058,
      "loss": 0.3556,
      "step": 1733
    },
    {
      "epoch": 0.3940909090909091,
      "grad_norm": 0.038904789835214615,
      "learning_rate": 0.00012131968145620023,
      "loss": 0.313,
      "step": 1734
    },
    {
      "epoch": 0.39431818181818185,
      "grad_norm": 0.03759413957595825,
      "learning_rate": 0.00012127417519908987,
      "loss": 0.3019,
      "step": 1735
    },
    {
      "epoch": 0.39454545454545453,
      "grad_norm": 0.03929264470934868,
      "learning_rate": 0.00012122866894197953,
      "loss": 0.3152,
      "step": 1736
    },
    {
      "epoch": 0.3947727272727273,
      "grad_norm": 0.051957082003355026,
      "learning_rate": 0.00012118316268486918,
      "loss": 0.3804,
      "step": 1737
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.04757228121161461,
      "learning_rate": 0.00012113765642775883,
      "loss": 0.3627,
      "step": 1738
    },
    {
      "epoch": 0.3952272727272727,
      "grad_norm": 0.03545765206217766,
      "learning_rate": 0.00012109215017064847,
      "loss": 0.2874,
      "step": 1739
    },
    {
      "epoch": 0.39545454545454545,
      "grad_norm": 0.04124819114804268,
      "learning_rate": 0.00012104664391353812,
      "loss": 0.2948,
      "step": 1740
    },
    {
      "epoch": 0.3956818181818182,
      "grad_norm": 0.035744838416576385,
      "learning_rate": 0.00012100113765642775,
      "loss": 0.2651,
      "step": 1741
    },
    {
      "epoch": 0.39590909090909093,
      "grad_norm": 0.03872270882129669,
      "learning_rate": 0.0001209556313993174,
      "loss": 0.3205,
      "step": 1742
    },
    {
      "epoch": 0.3961363636363636,
      "grad_norm": 0.04198403283953667,
      "learning_rate": 0.00012091012514220706,
      "loss": 0.3739,
      "step": 1743
    },
    {
      "epoch": 0.39636363636363636,
      "grad_norm": 0.060734812170267105,
      "learning_rate": 0.00012086461888509671,
      "loss": 0.3342,
      "step": 1744
    },
    {
      "epoch": 0.3965909090909091,
      "grad_norm": 0.05910944193601608,
      "learning_rate": 0.00012081911262798635,
      "loss": 0.3992,
      "step": 1745
    },
    {
      "epoch": 0.3968181818181818,
      "grad_norm": 0.03912090137600899,
      "learning_rate": 0.000120773606370876,
      "loss": 0.3239,
      "step": 1746
    },
    {
      "epoch": 0.39704545454545453,
      "grad_norm": 0.04576539993286133,
      "learning_rate": 0.00012072810011376565,
      "loss": 0.3321,
      "step": 1747
    },
    {
      "epoch": 0.3972727272727273,
      "grad_norm": 0.04146098718047142,
      "learning_rate": 0.0001206825938566553,
      "loss": 0.3391,
      "step": 1748
    },
    {
      "epoch": 0.3975,
      "grad_norm": 0.03719697520136833,
      "learning_rate": 0.00012063708759954494,
      "loss": 0.3238,
      "step": 1749
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 0.05877279117703438,
      "learning_rate": 0.0001205915813424346,
      "loss": 0.3587,
      "step": 1750
    },
    {
      "epoch": 0.39795454545454545,
      "grad_norm": 0.04226422309875488,
      "learning_rate": 0.00012054607508532422,
      "loss": 0.2559,
      "step": 1751
    },
    {
      "epoch": 0.3981818181818182,
      "grad_norm": 0.04910193756222725,
      "learning_rate": 0.00012050056882821388,
      "loss": 0.2983,
      "step": 1752
    },
    {
      "epoch": 0.39840909090909093,
      "grad_norm": 0.0399356409907341,
      "learning_rate": 0.00012045506257110353,
      "loss": 0.2779,
      "step": 1753
    },
    {
      "epoch": 0.3986363636363636,
      "grad_norm": 0.039144061505794525,
      "learning_rate": 0.00012040955631399318,
      "loss": 0.3138,
      "step": 1754
    },
    {
      "epoch": 0.39886363636363636,
      "grad_norm": 0.05997250974178314,
      "learning_rate": 0.00012036405005688282,
      "loss": 0.385,
      "step": 1755
    },
    {
      "epoch": 0.3990909090909091,
      "grad_norm": 0.048030413687229156,
      "learning_rate": 0.00012031854379977247,
      "loss": 0.3589,
      "step": 1756
    },
    {
      "epoch": 0.3993181818181818,
      "grad_norm": 0.05687295272946358,
      "learning_rate": 0.00012027303754266213,
      "loss": 0.3027,
      "step": 1757
    },
    {
      "epoch": 0.39954545454545454,
      "grad_norm": 0.05576326325535774,
      "learning_rate": 0.00012022753128555178,
      "loss": 0.3855,
      "step": 1758
    },
    {
      "epoch": 0.3997727272727273,
      "grad_norm": 0.03797220066189766,
      "learning_rate": 0.00012018202502844142,
      "loss": 0.271,
      "step": 1759
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.04809077829122543,
      "learning_rate": 0.00012013651877133107,
      "loss": 0.3149,
      "step": 1760
    },
    {
      "epoch": 0.4002272727272727,
      "grad_norm": 0.04795681685209274,
      "learning_rate": 0.0001200910125142207,
      "loss": 0.3247,
      "step": 1761
    },
    {
      "epoch": 0.40045454545454545,
      "grad_norm": 0.04205434024333954,
      "learning_rate": 0.00012004550625711035,
      "loss": 0.3376,
      "step": 1762
    },
    {
      "epoch": 0.4006818181818182,
      "grad_norm": 0.048238374292850494,
      "learning_rate": 0.00012,
      "loss": 0.3587,
      "step": 1763
    },
    {
      "epoch": 0.4009090909090909,
      "grad_norm": 0.0504581518471241,
      "learning_rate": 0.00011995449374288966,
      "loss": 0.3278,
      "step": 1764
    },
    {
      "epoch": 0.4011363636363636,
      "grad_norm": 0.04286123067140579,
      "learning_rate": 0.0001199089874857793,
      "loss": 0.2927,
      "step": 1765
    },
    {
      "epoch": 0.40136363636363637,
      "grad_norm": 0.04142994433641434,
      "learning_rate": 0.00011986348122866895,
      "loss": 0.3179,
      "step": 1766
    },
    {
      "epoch": 0.4015909090909091,
      "grad_norm": 0.03940782696008682,
      "learning_rate": 0.0001198179749715586,
      "loss": 0.258,
      "step": 1767
    },
    {
      "epoch": 0.4018181818181818,
      "grad_norm": 0.05007043853402138,
      "learning_rate": 0.00011977246871444825,
      "loss": 0.3631,
      "step": 1768
    },
    {
      "epoch": 0.40204545454545454,
      "grad_norm": 0.05880333110690117,
      "learning_rate": 0.00011972696245733789,
      "loss": 0.4162,
      "step": 1769
    },
    {
      "epoch": 0.4022727272727273,
      "grad_norm": 0.032729972153902054,
      "learning_rate": 0.00011968145620022753,
      "loss": 0.2572,
      "step": 1770
    },
    {
      "epoch": 0.4025,
      "grad_norm": 0.03336600959300995,
      "learning_rate": 0.00011963594994311717,
      "loss": 0.3158,
      "step": 1771
    },
    {
      "epoch": 0.4027272727272727,
      "grad_norm": 0.04603033512830734,
      "learning_rate": 0.00011959044368600682,
      "loss": 0.3423,
      "step": 1772
    },
    {
      "epoch": 0.40295454545454545,
      "grad_norm": 0.046433255076408386,
      "learning_rate": 0.00011954493742889648,
      "loss": 0.3463,
      "step": 1773
    },
    {
      "epoch": 0.4031818181818182,
      "grad_norm": 0.048422880470752716,
      "learning_rate": 0.00011949943117178613,
      "loss": 0.3316,
      "step": 1774
    },
    {
      "epoch": 0.4034090909090909,
      "grad_norm": 0.045754574239254,
      "learning_rate": 0.00011945392491467577,
      "loss": 0.2991,
      "step": 1775
    },
    {
      "epoch": 0.4036363636363636,
      "grad_norm": 0.0555819533765316,
      "learning_rate": 0.00011940841865756542,
      "loss": 0.416,
      "step": 1776
    },
    {
      "epoch": 0.40386363636363637,
      "grad_norm": 0.05455661937594414,
      "learning_rate": 0.00011936291240045507,
      "loss": 0.3753,
      "step": 1777
    },
    {
      "epoch": 0.4040909090909091,
      "grad_norm": 0.042918115854263306,
      "learning_rate": 0.00011931740614334473,
      "loss": 0.3551,
      "step": 1778
    },
    {
      "epoch": 0.4043181818181818,
      "grad_norm": 0.036201708018779755,
      "learning_rate": 0.00011927189988623437,
      "loss": 0.2765,
      "step": 1779
    },
    {
      "epoch": 0.40454545454545454,
      "grad_norm": 0.044104646891355515,
      "learning_rate": 0.000119226393629124,
      "loss": 0.3027,
      "step": 1780
    },
    {
      "epoch": 0.4047727272727273,
      "grad_norm": 0.06453083455562592,
      "learning_rate": 0.00011918088737201365,
      "loss": 0.3504,
      "step": 1781
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.051997359842061996,
      "learning_rate": 0.0001191353811149033,
      "loss": 0.2906,
      "step": 1782
    },
    {
      "epoch": 0.4052272727272727,
      "grad_norm": 0.051586493849754333,
      "learning_rate": 0.00011908987485779295,
      "loss": 0.3421,
      "step": 1783
    },
    {
      "epoch": 0.40545454545454546,
      "grad_norm": 0.05107744038105011,
      "learning_rate": 0.0001190443686006826,
      "loss": 0.3127,
      "step": 1784
    },
    {
      "epoch": 0.4056818181818182,
      "grad_norm": 0.06320788711309433,
      "learning_rate": 0.00011899886234357224,
      "loss": 0.3679,
      "step": 1785
    },
    {
      "epoch": 0.4059090909090909,
      "grad_norm": 0.059589363634586334,
      "learning_rate": 0.0001189533560864619,
      "loss": 0.3229,
      "step": 1786
    },
    {
      "epoch": 0.40613636363636363,
      "grad_norm": 0.04672563076019287,
      "learning_rate": 0.00011890784982935155,
      "loss": 0.32,
      "step": 1787
    },
    {
      "epoch": 0.40636363636363637,
      "grad_norm": 0.05221832916140556,
      "learning_rate": 0.0001188623435722412,
      "loss": 0.3591,
      "step": 1788
    },
    {
      "epoch": 0.4065909090909091,
      "grad_norm": 0.03689046576619148,
      "learning_rate": 0.00011881683731513084,
      "loss": 0.293,
      "step": 1789
    },
    {
      "epoch": 0.4068181818181818,
      "grad_norm": 0.04500074312090874,
      "learning_rate": 0.00011877133105802048,
      "loss": 0.3125,
      "step": 1790
    },
    {
      "epoch": 0.40704545454545454,
      "grad_norm": 0.041685573756694794,
      "learning_rate": 0.00011872582480091012,
      "loss": 0.2872,
      "step": 1791
    },
    {
      "epoch": 0.4072727272727273,
      "grad_norm": 0.03134389594197273,
      "learning_rate": 0.00011868031854379977,
      "loss": 0.2461,
      "step": 1792
    },
    {
      "epoch": 0.4075,
      "grad_norm": 0.03649045526981354,
      "learning_rate": 0.00011863481228668942,
      "loss": 0.2958,
      "step": 1793
    },
    {
      "epoch": 0.4077272727272727,
      "grad_norm": 0.04649856314063072,
      "learning_rate": 0.00011858930602957908,
      "loss": 0.3587,
      "step": 1794
    },
    {
      "epoch": 0.40795454545454546,
      "grad_norm": 0.049979113042354584,
      "learning_rate": 0.00011854379977246872,
      "loss": 0.3455,
      "step": 1795
    },
    {
      "epoch": 0.4081818181818182,
      "grad_norm": 0.05538741499185562,
      "learning_rate": 0.00011849829351535837,
      "loss": 0.3863,
      "step": 1796
    },
    {
      "epoch": 0.4084090909090909,
      "grad_norm": 0.04744844511151314,
      "learning_rate": 0.00011845278725824802,
      "loss": 0.3775,
      "step": 1797
    },
    {
      "epoch": 0.40863636363636363,
      "grad_norm": 0.04261592775583267,
      "learning_rate": 0.00011840728100113768,
      "loss": 0.3452,
      "step": 1798
    },
    {
      "epoch": 0.4088636363636364,
      "grad_norm": 0.04006766155362129,
      "learning_rate": 0.0001183617747440273,
      "loss": 0.3088,
      "step": 1799
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.05425448715686798,
      "learning_rate": 0.00011831626848691695,
      "loss": 0.3492,
      "step": 1800
    },
    {
      "epoch": 0.4093181818181818,
      "grad_norm": 0.04915492609143257,
      "learning_rate": 0.0001182707622298066,
      "loss": 0.36,
      "step": 1801
    },
    {
      "epoch": 0.40954545454545455,
      "grad_norm": 0.03623940423130989,
      "learning_rate": 0.00011822525597269625,
      "loss": 0.3027,
      "step": 1802
    },
    {
      "epoch": 0.4097727272727273,
      "grad_norm": 0.05187109857797623,
      "learning_rate": 0.0001181797497155859,
      "loss": 0.3455,
      "step": 1803
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.05229822173714638,
      "learning_rate": 0.00011813424345847555,
      "loss": 0.3837,
      "step": 1804
    },
    {
      "epoch": 0.4102272727272727,
      "grad_norm": 0.05451567843556404,
      "learning_rate": 0.00011808873720136519,
      "loss": 0.3254,
      "step": 1805
    },
    {
      "epoch": 0.41045454545454546,
      "grad_norm": 0.06789551675319672,
      "learning_rate": 0.00011804323094425484,
      "loss": 0.3927,
      "step": 1806
    },
    {
      "epoch": 0.4106818181818182,
      "grad_norm": 0.04353492707014084,
      "learning_rate": 0.0001179977246871445,
      "loss": 0.3378,
      "step": 1807
    },
    {
      "epoch": 0.4109090909090909,
      "grad_norm": 0.05170479416847229,
      "learning_rate": 0.00011795221843003415,
      "loss": 0.3627,
      "step": 1808
    },
    {
      "epoch": 0.41113636363636363,
      "grad_norm": 0.04669490084052086,
      "learning_rate": 0.00011790671217292378,
      "loss": 0.3368,
      "step": 1809
    },
    {
      "epoch": 0.4113636363636364,
      "grad_norm": 0.05800682678818703,
      "learning_rate": 0.00011786120591581343,
      "loss": 0.4066,
      "step": 1810
    },
    {
      "epoch": 0.4115909090909091,
      "grad_norm": 0.05536837503314018,
      "learning_rate": 0.00011781569965870307,
      "loss": 0.3239,
      "step": 1811
    },
    {
      "epoch": 0.4118181818181818,
      "grad_norm": 0.04858149588108063,
      "learning_rate": 0.00011777019340159272,
      "loss": 0.3745,
      "step": 1812
    },
    {
      "epoch": 0.41204545454545455,
      "grad_norm": 0.047475360333919525,
      "learning_rate": 0.00011772468714448237,
      "loss": 0.3011,
      "step": 1813
    },
    {
      "epoch": 0.4122727272727273,
      "grad_norm": 0.04845808818936348,
      "learning_rate": 0.00011767918088737203,
      "loss": 0.3831,
      "step": 1814
    },
    {
      "epoch": 0.4125,
      "grad_norm": 0.04713859036564827,
      "learning_rate": 0.00011763367463026167,
      "loss": 0.3195,
      "step": 1815
    },
    {
      "epoch": 0.4127272727272727,
      "grad_norm": 0.052696969360113144,
      "learning_rate": 0.00011758816837315132,
      "loss": 0.3763,
      "step": 1816
    },
    {
      "epoch": 0.41295454545454546,
      "grad_norm": 0.053692180663347244,
      "learning_rate": 0.00011754266211604097,
      "loss": 0.4014,
      "step": 1817
    },
    {
      "epoch": 0.4131818181818182,
      "grad_norm": 0.04897477105259895,
      "learning_rate": 0.00011749715585893062,
      "loss": 0.3371,
      "step": 1818
    },
    {
      "epoch": 0.4134090909090909,
      "grad_norm": 0.04992774873971939,
      "learning_rate": 0.00011745164960182025,
      "loss": 0.3378,
      "step": 1819
    },
    {
      "epoch": 0.41363636363636364,
      "grad_norm": 0.040068380534648895,
      "learning_rate": 0.0001174061433447099,
      "loss": 0.2827,
      "step": 1820
    },
    {
      "epoch": 0.4138636363636364,
      "grad_norm": 0.05615273490548134,
      "learning_rate": 0.00011736063708759954,
      "loss": 0.3415,
      "step": 1821
    },
    {
      "epoch": 0.41409090909090907,
      "grad_norm": 0.05430204048752785,
      "learning_rate": 0.0001173151308304892,
      "loss": 0.3419,
      "step": 1822
    },
    {
      "epoch": 0.4143181818181818,
      "grad_norm": 0.04181826487183571,
      "learning_rate": 0.00011726962457337885,
      "loss": 0.3115,
      "step": 1823
    },
    {
      "epoch": 0.41454545454545455,
      "grad_norm": 0.0639314129948616,
      "learning_rate": 0.0001172241183162685,
      "loss": 0.4156,
      "step": 1824
    },
    {
      "epoch": 0.4147727272727273,
      "grad_norm": 0.04261999949812889,
      "learning_rate": 0.00011717861205915814,
      "loss": 0.3505,
      "step": 1825
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.048186250030994415,
      "learning_rate": 0.00011713310580204779,
      "loss": 0.3572,
      "step": 1826
    },
    {
      "epoch": 0.4152272727272727,
      "grad_norm": 0.04628761485219002,
      "learning_rate": 0.00011708759954493744,
      "loss": 0.3387,
      "step": 1827
    },
    {
      "epoch": 0.41545454545454547,
      "grad_norm": 0.04931390658020973,
      "learning_rate": 0.0001170420932878271,
      "loss": 0.3563,
      "step": 1828
    },
    {
      "epoch": 0.4156818181818182,
      "grad_norm": 0.03912121802568436,
      "learning_rate": 0.00011699658703071672,
      "loss": 0.3337,
      "step": 1829
    },
    {
      "epoch": 0.4159090909090909,
      "grad_norm": 0.04259447380900383,
      "learning_rate": 0.00011695108077360638,
      "loss": 0.3434,
      "step": 1830
    },
    {
      "epoch": 0.41613636363636364,
      "grad_norm": 0.038478948175907135,
      "learning_rate": 0.00011690557451649602,
      "loss": 0.3217,
      "step": 1831
    },
    {
      "epoch": 0.4163636363636364,
      "grad_norm": 0.049615491181612015,
      "learning_rate": 0.00011686006825938567,
      "loss": 0.3138,
      "step": 1832
    },
    {
      "epoch": 0.41659090909090907,
      "grad_norm": 0.0416199266910553,
      "learning_rate": 0.00011681456200227532,
      "loss": 0.3075,
      "step": 1833
    },
    {
      "epoch": 0.4168181818181818,
      "grad_norm": 0.04910849407315254,
      "learning_rate": 0.00011676905574516497,
      "loss": 0.3399,
      "step": 1834
    },
    {
      "epoch": 0.41704545454545455,
      "grad_norm": 0.045225415378808975,
      "learning_rate": 0.00011672354948805461,
      "loss": 0.338,
      "step": 1835
    },
    {
      "epoch": 0.4172727272727273,
      "grad_norm": 0.04833371564745903,
      "learning_rate": 0.00011667804323094427,
      "loss": 0.32,
      "step": 1836
    },
    {
      "epoch": 0.4175,
      "grad_norm": 0.060404710471630096,
      "learning_rate": 0.00011663253697383392,
      "loss": 0.4092,
      "step": 1837
    },
    {
      "epoch": 0.4177272727272727,
      "grad_norm": 0.046882666647434235,
      "learning_rate": 0.00011658703071672354,
      "loss": 0.3374,
      "step": 1838
    },
    {
      "epoch": 0.41795454545454547,
      "grad_norm": 0.03624522686004639,
      "learning_rate": 0.0001165415244596132,
      "loss": 0.2899,
      "step": 1839
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.04723746329545975,
      "learning_rate": 0.00011649601820250285,
      "loss": 0.3001,
      "step": 1840
    },
    {
      "epoch": 0.4184090909090909,
      "grad_norm": 0.043871987611055374,
      "learning_rate": 0.00011645051194539249,
      "loss": 0.2824,
      "step": 1841
    },
    {
      "epoch": 0.41863636363636364,
      "grad_norm": 0.03988252580165863,
      "learning_rate": 0.00011640500568828214,
      "loss": 0.3009,
      "step": 1842
    },
    {
      "epoch": 0.4188636363636364,
      "grad_norm": 0.0448974184691906,
      "learning_rate": 0.0001163594994311718,
      "loss": 0.3452,
      "step": 1843
    },
    {
      "epoch": 0.41909090909090907,
      "grad_norm": 0.0461704395711422,
      "learning_rate": 0.00011631399317406145,
      "loss": 0.3114,
      "step": 1844
    },
    {
      "epoch": 0.4193181818181818,
      "grad_norm": 0.047427840530872345,
      "learning_rate": 0.00011626848691695109,
      "loss": 0.3273,
      "step": 1845
    },
    {
      "epoch": 0.41954545454545455,
      "grad_norm": 0.05106772482395172,
      "learning_rate": 0.00011622298065984074,
      "loss": 0.3612,
      "step": 1846
    },
    {
      "epoch": 0.4197727272727273,
      "grad_norm": 0.04247497394680977,
      "learning_rate": 0.00011617747440273039,
      "loss": 0.3268,
      "step": 1847
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.0456056222319603,
      "learning_rate": 0.00011613196814562002,
      "loss": 0.3237,
      "step": 1848
    },
    {
      "epoch": 0.4202272727272727,
      "grad_norm": 0.049024056643247604,
      "learning_rate": 0.00011608646188850967,
      "loss": 0.3336,
      "step": 1849
    },
    {
      "epoch": 0.42045454545454547,
      "grad_norm": 0.05829944834113121,
      "learning_rate": 0.00011604095563139932,
      "loss": 0.332,
      "step": 1850
    },
    {
      "epoch": 0.42068181818181816,
      "grad_norm": 0.04829518869519234,
      "learning_rate": 0.00011599544937428896,
      "loss": 0.3689,
      "step": 1851
    },
    {
      "epoch": 0.4209090909090909,
      "grad_norm": 0.051221225410699844,
      "learning_rate": 0.00011594994311717862,
      "loss": 0.3077,
      "step": 1852
    },
    {
      "epoch": 0.42113636363636364,
      "grad_norm": 0.03794638812541962,
      "learning_rate": 0.00011590443686006827,
      "loss": 0.3101,
      "step": 1853
    },
    {
      "epoch": 0.4213636363636364,
      "grad_norm": 0.057329464703798294,
      "learning_rate": 0.00011585893060295792,
      "loss": 0.3615,
      "step": 1854
    },
    {
      "epoch": 0.42159090909090907,
      "grad_norm": 0.03395719826221466,
      "learning_rate": 0.00011581342434584756,
      "loss": 0.2523,
      "step": 1855
    },
    {
      "epoch": 0.4218181818181818,
      "grad_norm": 0.053214479237794876,
      "learning_rate": 0.00011576791808873721,
      "loss": 0.3303,
      "step": 1856
    },
    {
      "epoch": 0.42204545454545456,
      "grad_norm": 0.038354359567165375,
      "learning_rate": 0.00011572241183162687,
      "loss": 0.3373,
      "step": 1857
    },
    {
      "epoch": 0.4222727272727273,
      "grad_norm": 0.046228859573602676,
      "learning_rate": 0.00011567690557451649,
      "loss": 0.338,
      "step": 1858
    },
    {
      "epoch": 0.4225,
      "grad_norm": 0.03435347229242325,
      "learning_rate": 0.00011563139931740615,
      "loss": 0.2949,
      "step": 1859
    },
    {
      "epoch": 0.42272727272727273,
      "grad_norm": 0.046058062463998795,
      "learning_rate": 0.0001155858930602958,
      "loss": 0.316,
      "step": 1860
    },
    {
      "epoch": 0.42295454545454547,
      "grad_norm": 0.0556061677634716,
      "learning_rate": 0.00011554038680318544,
      "loss": 0.3204,
      "step": 1861
    },
    {
      "epoch": 0.42318181818181816,
      "grad_norm": 0.04883354529738426,
      "learning_rate": 0.00011549488054607509,
      "loss": 0.3236,
      "step": 1862
    },
    {
      "epoch": 0.4234090909090909,
      "grad_norm": 0.04952630028128624,
      "learning_rate": 0.00011544937428896474,
      "loss": 0.3586,
      "step": 1863
    },
    {
      "epoch": 0.42363636363636364,
      "grad_norm": 0.06334218382835388,
      "learning_rate": 0.0001154038680318544,
      "loss": 0.4142,
      "step": 1864
    },
    {
      "epoch": 0.4238636363636364,
      "grad_norm": 0.044616278260946274,
      "learning_rate": 0.00011535836177474403,
      "loss": 0.335,
      "step": 1865
    },
    {
      "epoch": 0.4240909090909091,
      "grad_norm": 0.043547168374061584,
      "learning_rate": 0.00011531285551763369,
      "loss": 0.3357,
      "step": 1866
    },
    {
      "epoch": 0.4243181818181818,
      "grad_norm": 0.04788513854146004,
      "learning_rate": 0.00011526734926052334,
      "loss": 0.3572,
      "step": 1867
    },
    {
      "epoch": 0.42454545454545456,
      "grad_norm": 0.05111490190029144,
      "learning_rate": 0.00011522184300341297,
      "loss": 0.3767,
      "step": 1868
    },
    {
      "epoch": 0.42477272727272725,
      "grad_norm": 0.04159024730324745,
      "learning_rate": 0.00011517633674630262,
      "loss": 0.3389,
      "step": 1869
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.05636376887559891,
      "learning_rate": 0.00011513083048919226,
      "loss": 0.3785,
      "step": 1870
    },
    {
      "epoch": 0.42522727272727273,
      "grad_norm": 0.05623386427760124,
      "learning_rate": 0.00011508532423208191,
      "loss": 0.3445,
      "step": 1871
    },
    {
      "epoch": 0.4254545454545455,
      "grad_norm": 0.03608570247888565,
      "learning_rate": 0.00011503981797497156,
      "loss": 0.2898,
      "step": 1872
    },
    {
      "epoch": 0.42568181818181816,
      "grad_norm": 0.04469833895564079,
      "learning_rate": 0.00011499431171786122,
      "loss": 0.3223,
      "step": 1873
    },
    {
      "epoch": 0.4259090909090909,
      "grad_norm": 0.0521853081882,
      "learning_rate": 0.00011494880546075087,
      "loss": 0.3477,
      "step": 1874
    },
    {
      "epoch": 0.42613636363636365,
      "grad_norm": 0.0427861362695694,
      "learning_rate": 0.00011490329920364051,
      "loss": 0.2996,
      "step": 1875
    },
    {
      "epoch": 0.4263636363636364,
      "grad_norm": 0.05443332716822624,
      "learning_rate": 0.00011485779294653016,
      "loss": 0.369,
      "step": 1876
    },
    {
      "epoch": 0.4265909090909091,
      "grad_norm": 0.04635034129023552,
      "learning_rate": 0.00011481228668941979,
      "loss": 0.3118,
      "step": 1877
    },
    {
      "epoch": 0.4268181818181818,
      "grad_norm": 0.052906185388565063,
      "learning_rate": 0.00011476678043230944,
      "loss": 0.3685,
      "step": 1878
    },
    {
      "epoch": 0.42704545454545456,
      "grad_norm": 0.046213217079639435,
      "learning_rate": 0.00011472127417519909,
      "loss": 0.3321,
      "step": 1879
    },
    {
      "epoch": 0.42727272727272725,
      "grad_norm": 0.052711281925439835,
      "learning_rate": 0.00011467576791808873,
      "loss": 0.4026,
      "step": 1880
    },
    {
      "epoch": 0.4275,
      "grad_norm": 0.04823652282357216,
      "learning_rate": 0.00011463026166097839,
      "loss": 0.3162,
      "step": 1881
    },
    {
      "epoch": 0.42772727272727273,
      "grad_norm": 0.051241084933280945,
      "learning_rate": 0.00011458475540386804,
      "loss": 0.3358,
      "step": 1882
    },
    {
      "epoch": 0.4279545454545455,
      "grad_norm": 0.04364319518208504,
      "learning_rate": 0.00011453924914675769,
      "loss": 0.3321,
      "step": 1883
    },
    {
      "epoch": 0.42818181818181816,
      "grad_norm": 0.04285767301917076,
      "learning_rate": 0.00011449374288964734,
      "loss": 0.3003,
      "step": 1884
    },
    {
      "epoch": 0.4284090909090909,
      "grad_norm": 0.0544043704867363,
      "learning_rate": 0.00011444823663253698,
      "loss": 0.3485,
      "step": 1885
    },
    {
      "epoch": 0.42863636363636365,
      "grad_norm": 0.03489375859498978,
      "learning_rate": 0.00011440273037542664,
      "loss": 0.308,
      "step": 1886
    },
    {
      "epoch": 0.4288636363636364,
      "grad_norm": 0.04281314089894295,
      "learning_rate": 0.00011435722411831626,
      "loss": 0.339,
      "step": 1887
    },
    {
      "epoch": 0.4290909090909091,
      "grad_norm": 0.04739785194396973,
      "learning_rate": 0.00011431171786120591,
      "loss": 0.331,
      "step": 1888
    },
    {
      "epoch": 0.4293181818181818,
      "grad_norm": 0.05263015255331993,
      "learning_rate": 0.00011426621160409557,
      "loss": 0.3163,
      "step": 1889
    },
    {
      "epoch": 0.42954545454545456,
      "grad_norm": 0.06766960024833679,
      "learning_rate": 0.0001142207053469852,
      "loss": 0.339,
      "step": 1890
    },
    {
      "epoch": 0.42977272727272725,
      "grad_norm": 0.046411942690610886,
      "learning_rate": 0.00011417519908987486,
      "loss": 0.3492,
      "step": 1891
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.0368167944252491,
      "learning_rate": 0.00011412969283276451,
      "loss": 0.2936,
      "step": 1892
    },
    {
      "epoch": 0.43022727272727274,
      "grad_norm": 0.03104228712618351,
      "learning_rate": 0.00011408418657565416,
      "loss": 0.2532,
      "step": 1893
    },
    {
      "epoch": 0.4304545454545455,
      "grad_norm": 0.05060487240552902,
      "learning_rate": 0.00011403868031854382,
      "loss": 0.3103,
      "step": 1894
    },
    {
      "epoch": 0.43068181818181817,
      "grad_norm": 0.047322701662778854,
      "learning_rate": 0.00011399317406143346,
      "loss": 0.3048,
      "step": 1895
    },
    {
      "epoch": 0.4309090909090909,
      "grad_norm": 0.046005964279174805,
      "learning_rate": 0.00011394766780432311,
      "loss": 0.3614,
      "step": 1896
    },
    {
      "epoch": 0.43113636363636365,
      "grad_norm": 0.05550999194383621,
      "learning_rate": 0.00011390216154721274,
      "loss": 0.3541,
      "step": 1897
    },
    {
      "epoch": 0.43136363636363634,
      "grad_norm": 0.03811538591980934,
      "learning_rate": 0.00011385665529010239,
      "loss": 0.3015,
      "step": 1898
    },
    {
      "epoch": 0.4315909090909091,
      "grad_norm": 0.034495264291763306,
      "learning_rate": 0.00011381114903299204,
      "loss": 0.3025,
      "step": 1899
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.04002221301198006,
      "learning_rate": 0.00011376564277588168,
      "loss": 0.293,
      "step": 1900
    },
    {
      "epoch": 0.43204545454545457,
      "grad_norm": 0.03945394605398178,
      "learning_rate": 0.00011372013651877133,
      "loss": 0.2521,
      "step": 1901
    },
    {
      "epoch": 0.43227272727272725,
      "grad_norm": 0.05547461658716202,
      "learning_rate": 0.00011367463026166099,
      "loss": 0.4078,
      "step": 1902
    },
    {
      "epoch": 0.4325,
      "grad_norm": 0.047881823033094406,
      "learning_rate": 0.00011362912400455064,
      "loss": 0.3447,
      "step": 1903
    },
    {
      "epoch": 0.43272727272727274,
      "grad_norm": 0.06209666654467583,
      "learning_rate": 0.00011358361774744029,
      "loss": 0.3459,
      "step": 1904
    },
    {
      "epoch": 0.4329545454545455,
      "grad_norm": 0.04771680384874344,
      "learning_rate": 0.00011353811149032993,
      "loss": 0.2927,
      "step": 1905
    },
    {
      "epoch": 0.43318181818181817,
      "grad_norm": 0.0401420071721077,
      "learning_rate": 0.00011349260523321956,
      "loss": 0.3214,
      "step": 1906
    },
    {
      "epoch": 0.4334090909090909,
      "grad_norm": 0.04103347286581993,
      "learning_rate": 0.00011344709897610921,
      "loss": 0.332,
      "step": 1907
    },
    {
      "epoch": 0.43363636363636365,
      "grad_norm": 0.05197093263268471,
      "learning_rate": 0.00011340159271899886,
      "loss": 0.358,
      "step": 1908
    },
    {
      "epoch": 0.43386363636363634,
      "grad_norm": 0.04115625098347664,
      "learning_rate": 0.00011335608646188852,
      "loss": 0.3094,
      "step": 1909
    },
    {
      "epoch": 0.4340909090909091,
      "grad_norm": 0.04135548695921898,
      "learning_rate": 0.00011331058020477815,
      "loss": 0.309,
      "step": 1910
    },
    {
      "epoch": 0.4343181818181818,
      "grad_norm": 0.05895936116576195,
      "learning_rate": 0.00011326507394766781,
      "loss": 0.3522,
      "step": 1911
    },
    {
      "epoch": 0.43454545454545457,
      "grad_norm": 0.04246867820620537,
      "learning_rate": 0.00011321956769055746,
      "loss": 0.3356,
      "step": 1912
    },
    {
      "epoch": 0.43477272727272726,
      "grad_norm": 0.04404661804437637,
      "learning_rate": 0.00011317406143344711,
      "loss": 0.3109,
      "step": 1913
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.053780194371938705,
      "learning_rate": 0.00011312855517633677,
      "loss": 0.3908,
      "step": 1914
    },
    {
      "epoch": 0.43522727272727274,
      "grad_norm": 0.046446289867162704,
      "learning_rate": 0.0001130830489192264,
      "loss": 0.3317,
      "step": 1915
    },
    {
      "epoch": 0.4354545454545454,
      "grad_norm": 0.05427779629826546,
      "learning_rate": 0.00011303754266211603,
      "loss": 0.4159,
      "step": 1916
    },
    {
      "epoch": 0.43568181818181817,
      "grad_norm": 0.044722218066453934,
      "learning_rate": 0.00011299203640500568,
      "loss": 0.2839,
      "step": 1917
    },
    {
      "epoch": 0.4359090909090909,
      "grad_norm": 0.05663720890879631,
      "learning_rate": 0.00011294653014789534,
      "loss": 0.3257,
      "step": 1918
    },
    {
      "epoch": 0.43613636363636366,
      "grad_norm": 0.04731487110257149,
      "learning_rate": 0.00011290102389078499,
      "loss": 0.3406,
      "step": 1919
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.05786198750138283,
      "learning_rate": 0.00011285551763367463,
      "loss": 0.3693,
      "step": 1920
    },
    {
      "epoch": 0.4365909090909091,
      "grad_norm": 0.05931772664189339,
      "learning_rate": 0.00011281001137656428,
      "loss": 0.3176,
      "step": 1921
    },
    {
      "epoch": 0.4368181818181818,
      "grad_norm": 0.04723905026912689,
      "learning_rate": 0.00011276450511945393,
      "loss": 0.3252,
      "step": 1922
    },
    {
      "epoch": 0.43704545454545457,
      "grad_norm": 0.044732678681612015,
      "learning_rate": 0.00011271899886234359,
      "loss": 0.3347,
      "step": 1923
    },
    {
      "epoch": 0.43727272727272726,
      "grad_norm": 0.04150921106338501,
      "learning_rate": 0.00011267349260523324,
      "loss": 0.3421,
      "step": 1924
    },
    {
      "epoch": 0.4375,
      "grad_norm": 0.04549099877476692,
      "learning_rate": 0.00011262798634812288,
      "loss": 0.3558,
      "step": 1925
    },
    {
      "epoch": 0.43772727272727274,
      "grad_norm": 0.049181174486875534,
      "learning_rate": 0.0001125824800910125,
      "loss": 0.3637,
      "step": 1926
    },
    {
      "epoch": 0.43795454545454543,
      "grad_norm": 0.04356206953525543,
      "learning_rate": 0.00011253697383390216,
      "loss": 0.3523,
      "step": 1927
    },
    {
      "epoch": 0.4381818181818182,
      "grad_norm": 0.038874153047800064,
      "learning_rate": 0.00011249146757679181,
      "loss": 0.3384,
      "step": 1928
    },
    {
      "epoch": 0.4384090909090909,
      "grad_norm": 0.038923975080251694,
      "learning_rate": 0.00011244596131968146,
      "loss": 0.3328,
      "step": 1929
    },
    {
      "epoch": 0.43863636363636366,
      "grad_norm": 0.05584532395005226,
      "learning_rate": 0.0001124004550625711,
      "loss": 0.3423,
      "step": 1930
    },
    {
      "epoch": 0.43886363636363634,
      "grad_norm": 0.044169940054416656,
      "learning_rate": 0.00011235494880546076,
      "loss": 0.3434,
      "step": 1931
    },
    {
      "epoch": 0.4390909090909091,
      "grad_norm": 0.05106537044048309,
      "learning_rate": 0.00011230944254835041,
      "loss": 0.342,
      "step": 1932
    },
    {
      "epoch": 0.43931818181818183,
      "grad_norm": 0.05092627555131912,
      "learning_rate": 0.00011226393629124006,
      "loss": 0.335,
      "step": 1933
    },
    {
      "epoch": 0.4395454545454546,
      "grad_norm": 0.04924767091870308,
      "learning_rate": 0.0001122184300341297,
      "loss": 0.3403,
      "step": 1934
    },
    {
      "epoch": 0.43977272727272726,
      "grad_norm": 0.03946886956691742,
      "learning_rate": 0.00011217292377701935,
      "loss": 0.2903,
      "step": 1935
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.05170134827494621,
      "learning_rate": 0.00011212741751990898,
      "loss": 0.3828,
      "step": 1936
    },
    {
      "epoch": 0.44022727272727274,
      "grad_norm": 0.056850627064704895,
      "learning_rate": 0.00011208191126279863,
      "loss": 0.3859,
      "step": 1937
    },
    {
      "epoch": 0.44045454545454543,
      "grad_norm": 0.04579874500632286,
      "learning_rate": 0.00011203640500568828,
      "loss": 0.359,
      "step": 1938
    },
    {
      "epoch": 0.4406818181818182,
      "grad_norm": 0.04902057349681854,
      "learning_rate": 0.00011199089874857794,
      "loss": 0.3467,
      "step": 1939
    },
    {
      "epoch": 0.4409090909090909,
      "grad_norm": 0.05142161250114441,
      "learning_rate": 0.00011194539249146758,
      "loss": 0.3706,
      "step": 1940
    },
    {
      "epoch": 0.44113636363636366,
      "grad_norm": 0.04415449872612953,
      "learning_rate": 0.00011189988623435723,
      "loss": 0.3238,
      "step": 1941
    },
    {
      "epoch": 0.44136363636363635,
      "grad_norm": 0.04506359249353409,
      "learning_rate": 0.00011185437997724688,
      "loss": 0.3893,
      "step": 1942
    },
    {
      "epoch": 0.4415909090909091,
      "grad_norm": 0.050632767379283905,
      "learning_rate": 0.00011180887372013653,
      "loss": 0.3971,
      "step": 1943
    },
    {
      "epoch": 0.44181818181818183,
      "grad_norm": 0.04537934437394142,
      "learning_rate": 0.00011176336746302617,
      "loss": 0.3527,
      "step": 1944
    },
    {
      "epoch": 0.4420454545454545,
      "grad_norm": 0.04308007284998894,
      "learning_rate": 0.00011171786120591581,
      "loss": 0.3013,
      "step": 1945
    },
    {
      "epoch": 0.44227272727272726,
      "grad_norm": 0.048161160200834274,
      "learning_rate": 0.00011167235494880545,
      "loss": 0.3279,
      "step": 1946
    },
    {
      "epoch": 0.4425,
      "grad_norm": 0.04290423542261124,
      "learning_rate": 0.0001116268486916951,
      "loss": 0.3015,
      "step": 1947
    },
    {
      "epoch": 0.44272727272727275,
      "grad_norm": 0.0487707145512104,
      "learning_rate": 0.00011158134243458476,
      "loss": 0.3247,
      "step": 1948
    },
    {
      "epoch": 0.44295454545454543,
      "grad_norm": 0.04202340915799141,
      "learning_rate": 0.00011153583617747441,
      "loss": 0.2761,
      "step": 1949
    },
    {
      "epoch": 0.4431818181818182,
      "grad_norm": 0.051712412387132645,
      "learning_rate": 0.00011149032992036405,
      "loss": 0.3486,
      "step": 1950
    },
    {
      "epoch": 0.4434090909090909,
      "grad_norm": 0.04676646739244461,
      "learning_rate": 0.0001114448236632537,
      "loss": 0.3669,
      "step": 1951
    },
    {
      "epoch": 0.44363636363636366,
      "grad_norm": 0.03736395761370659,
      "learning_rate": 0.00011139931740614336,
      "loss": 0.2894,
      "step": 1952
    },
    {
      "epoch": 0.44386363636363635,
      "grad_norm": 0.0392412431538105,
      "learning_rate": 0.00011135381114903301,
      "loss": 0.3267,
      "step": 1953
    },
    {
      "epoch": 0.4440909090909091,
      "grad_norm": 0.05032466724514961,
      "learning_rate": 0.00011130830489192265,
      "loss": 0.3563,
      "step": 1954
    },
    {
      "epoch": 0.44431818181818183,
      "grad_norm": 0.054816070944070816,
      "learning_rate": 0.00011126279863481229,
      "loss": 0.37,
      "step": 1955
    },
    {
      "epoch": 0.4445454545454545,
      "grad_norm": 0.044126804918050766,
      "learning_rate": 0.00011121729237770193,
      "loss": 0.3072,
      "step": 1956
    },
    {
      "epoch": 0.44477272727272726,
      "grad_norm": 0.04269106313586235,
      "learning_rate": 0.00011117178612059158,
      "loss": 0.2906,
      "step": 1957
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.03816011920571327,
      "learning_rate": 0.00011112627986348123,
      "loss": 0.2833,
      "step": 1958
    },
    {
      "epoch": 0.44522727272727275,
      "grad_norm": 0.0485759936273098,
      "learning_rate": 0.00011108077360637088,
      "loss": 0.3288,
      "step": 1959
    },
    {
      "epoch": 0.44545454545454544,
      "grad_norm": 0.045488063246011734,
      "learning_rate": 0.00011103526734926052,
      "loss": 0.3236,
      "step": 1960
    },
    {
      "epoch": 0.4456818181818182,
      "grad_norm": 0.0474432036280632,
      "learning_rate": 0.00011098976109215018,
      "loss": 0.348,
      "step": 1961
    },
    {
      "epoch": 0.4459090909090909,
      "grad_norm": 0.046728361397981644,
      "learning_rate": 0.00011094425483503983,
      "loss": 0.2945,
      "step": 1962
    },
    {
      "epoch": 0.4461363636363636,
      "grad_norm": 0.038120172917842865,
      "learning_rate": 0.00011089874857792948,
      "loss": 0.2875,
      "step": 1963
    },
    {
      "epoch": 0.44636363636363635,
      "grad_norm": 0.048715654760599136,
      "learning_rate": 0.00011085324232081912,
      "loss": 0.3351,
      "step": 1964
    },
    {
      "epoch": 0.4465909090909091,
      "grad_norm": 0.044526781886816025,
      "learning_rate": 0.00011080773606370876,
      "loss": 0.3319,
      "step": 1965
    },
    {
      "epoch": 0.44681818181818184,
      "grad_norm": 0.03693170100450516,
      "learning_rate": 0.0001107622298065984,
      "loss": 0.2918,
      "step": 1966
    },
    {
      "epoch": 0.4470454545454545,
      "grad_norm": 0.0501646064221859,
      "learning_rate": 0.00011071672354948805,
      "loss": 0.3855,
      "step": 1967
    },
    {
      "epoch": 0.44727272727272727,
      "grad_norm": 0.04491223022341728,
      "learning_rate": 0.0001106712172923777,
      "loss": 0.3372,
      "step": 1968
    },
    {
      "epoch": 0.4475,
      "grad_norm": 0.042663730680942535,
      "learning_rate": 0.00011062571103526736,
      "loss": 0.3594,
      "step": 1969
    },
    {
      "epoch": 0.44772727272727275,
      "grad_norm": 0.052843593060970306,
      "learning_rate": 0.000110580204778157,
      "loss": 0.3538,
      "step": 1970
    },
    {
      "epoch": 0.44795454545454544,
      "grad_norm": 0.04842924699187279,
      "learning_rate": 0.00011053469852104665,
      "loss": 0.3738,
      "step": 1971
    },
    {
      "epoch": 0.4481818181818182,
      "grad_norm": 0.05492115393280983,
      "learning_rate": 0.0001104891922639363,
      "loss": 0.407,
      "step": 1972
    },
    {
      "epoch": 0.4484090909090909,
      "grad_norm": 0.04775489866733551,
      "learning_rate": 0.00011044368600682596,
      "loss": 0.3503,
      "step": 1973
    },
    {
      "epoch": 0.4486363636363636,
      "grad_norm": 0.05104738846421242,
      "learning_rate": 0.00011039817974971558,
      "loss": 0.3026,
      "step": 1974
    },
    {
      "epoch": 0.44886363636363635,
      "grad_norm": 0.05409299209713936,
      "learning_rate": 0.00011035267349260524,
      "loss": 0.3137,
      "step": 1975
    },
    {
      "epoch": 0.4490909090909091,
      "grad_norm": 0.043621327728033066,
      "learning_rate": 0.00011030716723549487,
      "loss": 0.2994,
      "step": 1976
    },
    {
      "epoch": 0.44931818181818184,
      "grad_norm": 0.04757525399327278,
      "learning_rate": 0.00011026166097838453,
      "loss": 0.3464,
      "step": 1977
    },
    {
      "epoch": 0.4495454545454545,
      "grad_norm": 0.05195918679237366,
      "learning_rate": 0.00011021615472127418,
      "loss": 0.3386,
      "step": 1978
    },
    {
      "epoch": 0.44977272727272727,
      "grad_norm": 0.0483512245118618,
      "learning_rate": 0.00011017064846416383,
      "loss": 0.3656,
      "step": 1979
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.04719811677932739,
      "learning_rate": 0.00011012514220705347,
      "loss": 0.39,
      "step": 1980
    },
    {
      "epoch": 0.45022727272727275,
      "grad_norm": 0.05131116881966591,
      "learning_rate": 0.00011007963594994313,
      "loss": 0.4032,
      "step": 1981
    },
    {
      "epoch": 0.45045454545454544,
      "grad_norm": 0.03963945806026459,
      "learning_rate": 0.00011003412969283278,
      "loss": 0.2405,
      "step": 1982
    },
    {
      "epoch": 0.4506818181818182,
      "grad_norm": 0.05286601185798645,
      "learning_rate": 0.00010998862343572243,
      "loss": 0.3267,
      "step": 1983
    },
    {
      "epoch": 0.4509090909090909,
      "grad_norm": 0.03907521441578865,
      "learning_rate": 0.00010994311717861206,
      "loss": 0.2737,
      "step": 1984
    },
    {
      "epoch": 0.4511363636363636,
      "grad_norm": 0.03907031938433647,
      "learning_rate": 0.00010989761092150171,
      "loss": 0.3133,
      "step": 1985
    },
    {
      "epoch": 0.45136363636363636,
      "grad_norm": 0.040685344487428665,
      "learning_rate": 0.00010985210466439135,
      "loss": 0.3199,
      "step": 1986
    },
    {
      "epoch": 0.4515909090909091,
      "grad_norm": 0.049562081694602966,
      "learning_rate": 0.000109806598407281,
      "loss": 0.3703,
      "step": 1987
    },
    {
      "epoch": 0.45181818181818184,
      "grad_norm": 0.04127030074596405,
      "learning_rate": 0.00010976109215017065,
      "loss": 0.3139,
      "step": 1988
    },
    {
      "epoch": 0.45204545454545453,
      "grad_norm": 0.044861990958452225,
      "learning_rate": 0.00010971558589306031,
      "loss": 0.3281,
      "step": 1989
    },
    {
      "epoch": 0.45227272727272727,
      "grad_norm": 0.051635053008794785,
      "learning_rate": 0.00010967007963594995,
      "loss": 0.305,
      "step": 1990
    },
    {
      "epoch": 0.4525,
      "grad_norm": 0.047060806304216385,
      "learning_rate": 0.0001096245733788396,
      "loss": 0.3094,
      "step": 1991
    },
    {
      "epoch": 0.4527272727272727,
      "grad_norm": 0.04442396014928818,
      "learning_rate": 0.00010957906712172925,
      "loss": 0.3683,
      "step": 1992
    },
    {
      "epoch": 0.45295454545454544,
      "grad_norm": 0.044102370738983154,
      "learning_rate": 0.0001095335608646189,
      "loss": 0.306,
      "step": 1993
    },
    {
      "epoch": 0.4531818181818182,
      "grad_norm": 0.046188920736312866,
      "learning_rate": 0.00010948805460750853,
      "loss": 0.3524,
      "step": 1994
    },
    {
      "epoch": 0.45340909090909093,
      "grad_norm": 0.038169555366039276,
      "learning_rate": 0.00010944254835039818,
      "loss": 0.2879,
      "step": 1995
    },
    {
      "epoch": 0.4536363636363636,
      "grad_norm": 0.038682498037815094,
      "learning_rate": 0.00010939704209328782,
      "loss": 0.3263,
      "step": 1996
    },
    {
      "epoch": 0.45386363636363636,
      "grad_norm": 0.04119795933365822,
      "learning_rate": 0.00010935153583617748,
      "loss": 0.3297,
      "step": 1997
    },
    {
      "epoch": 0.4540909090909091,
      "grad_norm": 0.05514074116945267,
      "learning_rate": 0.00010930602957906713,
      "loss": 0.3929,
      "step": 1998
    },
    {
      "epoch": 0.45431818181818184,
      "grad_norm": 0.03768210858106613,
      "learning_rate": 0.00010926052332195678,
      "loss": 0.2745,
      "step": 1999
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.049648649990558624,
      "learning_rate": 0.00010921501706484642,
      "loss": 0.345,
      "step": 2000
    },
    {
      "epoch": 0.4547727272727273,
      "grad_norm": 0.045281942933797836,
      "learning_rate": 0.00010916951080773607,
      "loss": 0.3412,
      "step": 2001
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.03840486705303192,
      "learning_rate": 0.00010912400455062573,
      "loss": 0.2858,
      "step": 2002
    },
    {
      "epoch": 0.4552272727272727,
      "grad_norm": 0.04530542343854904,
      "learning_rate": 0.00010907849829351538,
      "loss": 0.3319,
      "step": 2003
    },
    {
      "epoch": 0.45545454545454545,
      "grad_norm": 0.051991116255521774,
      "learning_rate": 0.000109032992036405,
      "loss": 0.4047,
      "step": 2004
    },
    {
      "epoch": 0.4556818181818182,
      "grad_norm": 0.05022348836064339,
      "learning_rate": 0.00010898748577929466,
      "loss": 0.3961,
      "step": 2005
    },
    {
      "epoch": 0.45590909090909093,
      "grad_norm": 0.05021204799413681,
      "learning_rate": 0.0001089419795221843,
      "loss": 0.3498,
      "step": 2006
    },
    {
      "epoch": 0.4561363636363636,
      "grad_norm": 0.04338550567626953,
      "learning_rate": 0.00010889647326507395,
      "loss": 0.3465,
      "step": 2007
    },
    {
      "epoch": 0.45636363636363636,
      "grad_norm": 0.04564230144023895,
      "learning_rate": 0.0001088509670079636,
      "loss": 0.3486,
      "step": 2008
    },
    {
      "epoch": 0.4565909090909091,
      "grad_norm": 0.041321102529764175,
      "learning_rate": 0.00010880546075085325,
      "loss": 0.304,
      "step": 2009
    },
    {
      "epoch": 0.45681818181818185,
      "grad_norm": 0.04700330272316933,
      "learning_rate": 0.0001087599544937429,
      "loss": 0.3482,
      "step": 2010
    },
    {
      "epoch": 0.45704545454545453,
      "grad_norm": 0.04571773111820221,
      "learning_rate": 0.00010871444823663255,
      "loss": 0.3469,
      "step": 2011
    },
    {
      "epoch": 0.4572727272727273,
      "grad_norm": 0.04362776130437851,
      "learning_rate": 0.0001086689419795222,
      "loss": 0.3354,
      "step": 2012
    },
    {
      "epoch": 0.4575,
      "grad_norm": 0.05423455312848091,
      "learning_rate": 0.00010862343572241183,
      "loss": 0.3667,
      "step": 2013
    },
    {
      "epoch": 0.4577272727272727,
      "grad_norm": 0.0492481030523777,
      "learning_rate": 0.00010857792946530148,
      "loss": 0.3945,
      "step": 2014
    },
    {
      "epoch": 0.45795454545454545,
      "grad_norm": 0.05959620326757431,
      "learning_rate": 0.00010853242320819113,
      "loss": 0.3608,
      "step": 2015
    },
    {
      "epoch": 0.4581818181818182,
      "grad_norm": 0.042563747614622116,
      "learning_rate": 0.00010848691695108077,
      "loss": 0.255,
      "step": 2016
    },
    {
      "epoch": 0.45840909090909093,
      "grad_norm": 0.0418601855635643,
      "learning_rate": 0.00010844141069397042,
      "loss": 0.3403,
      "step": 2017
    },
    {
      "epoch": 0.4586363636363636,
      "grad_norm": 0.03596413880586624,
      "learning_rate": 0.00010839590443686008,
      "loss": 0.2585,
      "step": 2018
    },
    {
      "epoch": 0.45886363636363636,
      "grad_norm": 0.04172803461551666,
      "learning_rate": 0.00010835039817974973,
      "loss": 0.3058,
      "step": 2019
    },
    {
      "epoch": 0.4590909090909091,
      "grad_norm": 0.05605189874768257,
      "learning_rate": 0.00010830489192263937,
      "loss": 0.3524,
      "step": 2020
    },
    {
      "epoch": 0.4593181818181818,
      "grad_norm": 0.042847901582717896,
      "learning_rate": 0.00010825938566552902,
      "loss": 0.2966,
      "step": 2021
    },
    {
      "epoch": 0.45954545454545453,
      "grad_norm": 0.046575721353292465,
      "learning_rate": 0.00010821387940841867,
      "loss": 0.3561,
      "step": 2022
    },
    {
      "epoch": 0.4597727272727273,
      "grad_norm": 0.04498271271586418,
      "learning_rate": 0.0001081683731513083,
      "loss": 0.3136,
      "step": 2023
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.04413421079516411,
      "learning_rate": 0.00010812286689419795,
      "loss": 0.3348,
      "step": 2024
    },
    {
      "epoch": 0.4602272727272727,
      "grad_norm": 0.06470197439193726,
      "learning_rate": 0.0001080773606370876,
      "loss": 0.3423,
      "step": 2025
    },
    {
      "epoch": 0.46045454545454545,
      "grad_norm": 0.03921094536781311,
      "learning_rate": 0.00010803185437997724,
      "loss": 0.2641,
      "step": 2026
    },
    {
      "epoch": 0.4606818181818182,
      "grad_norm": 0.0498017817735672,
      "learning_rate": 0.0001079863481228669,
      "loss": 0.3489,
      "step": 2027
    },
    {
      "epoch": 0.46090909090909093,
      "grad_norm": 0.05064656212925911,
      "learning_rate": 0.00010794084186575655,
      "loss": 0.3121,
      "step": 2028
    },
    {
      "epoch": 0.4611363636363636,
      "grad_norm": 0.04458962753415108,
      "learning_rate": 0.0001078953356086462,
      "loss": 0.3536,
      "step": 2029
    },
    {
      "epoch": 0.46136363636363636,
      "grad_norm": 0.05034244805574417,
      "learning_rate": 0.00010784982935153584,
      "loss": 0.3494,
      "step": 2030
    },
    {
      "epoch": 0.4615909090909091,
      "grad_norm": 0.03685525804758072,
      "learning_rate": 0.0001078043230944255,
      "loss": 0.329,
      "step": 2031
    },
    {
      "epoch": 0.4618181818181818,
      "grad_norm": 0.060079459100961685,
      "learning_rate": 0.00010775881683731515,
      "loss": 0.4246,
      "step": 2032
    },
    {
      "epoch": 0.46204545454545454,
      "grad_norm": 0.048833269625902176,
      "learning_rate": 0.00010771331058020477,
      "loss": 0.3464,
      "step": 2033
    },
    {
      "epoch": 0.4622727272727273,
      "grad_norm": 0.03650151193141937,
      "learning_rate": 0.00010766780432309443,
      "loss": 0.2752,
      "step": 2034
    },
    {
      "epoch": 0.4625,
      "grad_norm": 0.05233864486217499,
      "learning_rate": 0.00010762229806598408,
      "loss": 0.372,
      "step": 2035
    },
    {
      "epoch": 0.4627272727272727,
      "grad_norm": 0.05637507513165474,
      "learning_rate": 0.00010757679180887372,
      "loss": 0.3903,
      "step": 2036
    },
    {
      "epoch": 0.46295454545454545,
      "grad_norm": 0.049613069742918015,
      "learning_rate": 0.00010753128555176337,
      "loss": 0.4062,
      "step": 2037
    },
    {
      "epoch": 0.4631818181818182,
      "grad_norm": 0.04563459753990173,
      "learning_rate": 0.00010748577929465302,
      "loss": 0.3382,
      "step": 2038
    },
    {
      "epoch": 0.4634090909090909,
      "grad_norm": 0.05545395240187645,
      "learning_rate": 0.00010744027303754268,
      "loss": 0.3454,
      "step": 2039
    },
    {
      "epoch": 0.4636363636363636,
      "grad_norm": 0.04714414104819298,
      "learning_rate": 0.00010739476678043232,
      "loss": 0.3515,
      "step": 2040
    },
    {
      "epoch": 0.46386363636363637,
      "grad_norm": 0.05742161348462105,
      "learning_rate": 0.00010734926052332197,
      "loss": 0.3623,
      "step": 2041
    },
    {
      "epoch": 0.4640909090909091,
      "grad_norm": 0.0449850857257843,
      "learning_rate": 0.00010730375426621162,
      "loss": 0.3205,
      "step": 2042
    },
    {
      "epoch": 0.4643181818181818,
      "grad_norm": 0.04733743891119957,
      "learning_rate": 0.00010725824800910125,
      "loss": 0.3894,
      "step": 2043
    },
    {
      "epoch": 0.46454545454545454,
      "grad_norm": 0.06778273731470108,
      "learning_rate": 0.0001072127417519909,
      "loss": 0.3609,
      "step": 2044
    },
    {
      "epoch": 0.4647727272727273,
      "grad_norm": 0.03555956110358238,
      "learning_rate": 0.00010716723549488055,
      "loss": 0.2998,
      "step": 2045
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.046410176903009415,
      "learning_rate": 0.00010712172923777019,
      "loss": 0.3359,
      "step": 2046
    },
    {
      "epoch": 0.4652272727272727,
      "grad_norm": 0.05914676561951637,
      "learning_rate": 0.00010707622298065985,
      "loss": 0.4142,
      "step": 2047
    },
    {
      "epoch": 0.46545454545454545,
      "grad_norm": 0.0768519788980484,
      "learning_rate": 0.0001070307167235495,
      "loss": 0.427,
      "step": 2048
    },
    {
      "epoch": 0.4656818181818182,
      "grad_norm": 0.04232228547334671,
      "learning_rate": 0.00010698521046643915,
      "loss": 0.317,
      "step": 2049
    },
    {
      "epoch": 0.4659090909090909,
      "grad_norm": 0.04672637954354286,
      "learning_rate": 0.00010693970420932879,
      "loss": 0.3213,
      "step": 2050
    },
    {
      "epoch": 0.4661363636363636,
      "grad_norm": 0.04855330288410187,
      "learning_rate": 0.00010689419795221844,
      "loss": 0.3394,
      "step": 2051
    },
    {
      "epoch": 0.46636363636363637,
      "grad_norm": 0.04489944130182266,
      "learning_rate": 0.00010684869169510807,
      "loss": 0.3079,
      "step": 2052
    },
    {
      "epoch": 0.4665909090909091,
      "grad_norm": 0.04005248844623566,
      "learning_rate": 0.00010680318543799772,
      "loss": 0.2797,
      "step": 2053
    },
    {
      "epoch": 0.4668181818181818,
      "grad_norm": 0.04764307662844658,
      "learning_rate": 0.00010675767918088737,
      "loss": 0.3596,
      "step": 2054
    },
    {
      "epoch": 0.46704545454545454,
      "grad_norm": 0.038149747997522354,
      "learning_rate": 0.00010671217292377703,
      "loss": 0.3081,
      "step": 2055
    },
    {
      "epoch": 0.4672727272727273,
      "grad_norm": 0.04753819480538368,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.3393,
      "step": 2056
    },
    {
      "epoch": 0.4675,
      "grad_norm": 0.05553460866212845,
      "learning_rate": 0.00010662116040955632,
      "loss": 0.393,
      "step": 2057
    },
    {
      "epoch": 0.4677272727272727,
      "grad_norm": 0.041425228118896484,
      "learning_rate": 0.00010657565415244597,
      "loss": 0.3259,
      "step": 2058
    },
    {
      "epoch": 0.46795454545454546,
      "grad_norm": 0.0438973642885685,
      "learning_rate": 0.00010653014789533562,
      "loss": 0.368,
      "step": 2059
    },
    {
      "epoch": 0.4681818181818182,
      "grad_norm": 0.04906953126192093,
      "learning_rate": 0.00010648464163822526,
      "loss": 0.3215,
      "step": 2060
    },
    {
      "epoch": 0.4684090909090909,
      "grad_norm": 0.055101729929447174,
      "learning_rate": 0.00010643913538111492,
      "loss": 0.3637,
      "step": 2061
    },
    {
      "epoch": 0.46863636363636363,
      "grad_norm": 0.05549878254532814,
      "learning_rate": 0.00010639362912400454,
      "loss": 0.3741,
      "step": 2062
    },
    {
      "epoch": 0.46886363636363637,
      "grad_norm": 0.05189386382699013,
      "learning_rate": 0.0001063481228668942,
      "loss": 0.3845,
      "step": 2063
    },
    {
      "epoch": 0.4690909090909091,
      "grad_norm": 0.04516393691301346,
      "learning_rate": 0.00010630261660978385,
      "loss": 0.3363,
      "step": 2064
    },
    {
      "epoch": 0.4693181818181818,
      "grad_norm": 0.053089987486600876,
      "learning_rate": 0.0001062571103526735,
      "loss": 0.3807,
      "step": 2065
    },
    {
      "epoch": 0.46954545454545454,
      "grad_norm": 0.04246990382671356,
      "learning_rate": 0.00010621160409556314,
      "loss": 0.3107,
      "step": 2066
    },
    {
      "epoch": 0.4697727272727273,
      "grad_norm": 0.044066861271858215,
      "learning_rate": 0.00010616609783845279,
      "loss": 0.2746,
      "step": 2067
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.03791055828332901,
      "learning_rate": 0.00010612059158134245,
      "loss": 0.2902,
      "step": 2068
    },
    {
      "epoch": 0.4702272727272727,
      "grad_norm": 0.046641893684864044,
      "learning_rate": 0.0001060750853242321,
      "loss": 0.3391,
      "step": 2069
    },
    {
      "epoch": 0.47045454545454546,
      "grad_norm": 0.052074071019887924,
      "learning_rate": 0.00010602957906712174,
      "loss": 0.3304,
      "step": 2070
    },
    {
      "epoch": 0.4706818181818182,
      "grad_norm": 0.04115854203701019,
      "learning_rate": 0.00010598407281001139,
      "loss": 0.3423,
      "step": 2071
    },
    {
      "epoch": 0.4709090909090909,
      "grad_norm": 0.0395631305873394,
      "learning_rate": 0.00010593856655290102,
      "loss": 0.3171,
      "step": 2072
    },
    {
      "epoch": 0.47113636363636363,
      "grad_norm": 0.034189604222774506,
      "learning_rate": 0.00010589306029579067,
      "loss": 0.265,
      "step": 2073
    },
    {
      "epoch": 0.4713636363636364,
      "grad_norm": 0.04606730863451958,
      "learning_rate": 0.00010584755403868032,
      "loss": 0.4057,
      "step": 2074
    },
    {
      "epoch": 0.4715909090909091,
      "grad_norm": 0.052760303020477295,
      "learning_rate": 0.00010580204778156998,
      "loss": 0.3096,
      "step": 2075
    },
    {
      "epoch": 0.4718181818181818,
      "grad_norm": 0.0477910079061985,
      "learning_rate": 0.00010575654152445961,
      "loss": 0.3567,
      "step": 2076
    },
    {
      "epoch": 0.47204545454545455,
      "grad_norm": 0.05015682056546211,
      "learning_rate": 0.00010571103526734927,
      "loss": 0.3361,
      "step": 2077
    },
    {
      "epoch": 0.4722727272727273,
      "grad_norm": 0.04956921935081482,
      "learning_rate": 0.00010566552901023892,
      "loss": 0.4098,
      "step": 2078
    },
    {
      "epoch": 0.4725,
      "grad_norm": 0.051199235022068024,
      "learning_rate": 0.00010562002275312857,
      "loss": 0.3704,
      "step": 2079
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.05887390300631523,
      "learning_rate": 0.00010557451649601821,
      "loss": 0.4016,
      "step": 2080
    },
    {
      "epoch": 0.47295454545454546,
      "grad_norm": 0.050086285918951035,
      "learning_rate": 0.00010552901023890784,
      "loss": 0.3037,
      "step": 2081
    },
    {
      "epoch": 0.4731818181818182,
      "grad_norm": 0.05484550818800926,
      "learning_rate": 0.00010548350398179749,
      "loss": 0.3298,
      "step": 2082
    },
    {
      "epoch": 0.4734090909090909,
      "grad_norm": 0.04884128272533417,
      "learning_rate": 0.00010543799772468714,
      "loss": 0.3573,
      "step": 2083
    },
    {
      "epoch": 0.47363636363636363,
      "grad_norm": 0.04287891089916229,
      "learning_rate": 0.0001053924914675768,
      "loss": 0.3021,
      "step": 2084
    },
    {
      "epoch": 0.4738636363636364,
      "grad_norm": 0.03198334947228432,
      "learning_rate": 0.00010534698521046645,
      "loss": 0.268,
      "step": 2085
    },
    {
      "epoch": 0.4740909090909091,
      "grad_norm": 0.03999868407845497,
      "learning_rate": 0.00010530147895335609,
      "loss": 0.3065,
      "step": 2086
    },
    {
      "epoch": 0.4743181818181818,
      "grad_norm": 0.04137587919831276,
      "learning_rate": 0.00010525597269624574,
      "loss": 0.2796,
      "step": 2087
    },
    {
      "epoch": 0.47454545454545455,
      "grad_norm": 0.06067280098795891,
      "learning_rate": 0.0001052104664391354,
      "loss": 0.4082,
      "step": 2088
    },
    {
      "epoch": 0.4747727272727273,
      "grad_norm": 0.043556999415159225,
      "learning_rate": 0.00010516496018202505,
      "loss": 0.2819,
      "step": 2089
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.043116211891174316,
      "learning_rate": 0.00010511945392491469,
      "loss": 0.3351,
      "step": 2090
    },
    {
      "epoch": 0.4752272727272727,
      "grad_norm": 0.02861710824072361,
      "learning_rate": 0.00010507394766780431,
      "loss": 0.2762,
      "step": 2091
    },
    {
      "epoch": 0.47545454545454546,
      "grad_norm": 0.040520068258047104,
      "learning_rate": 0.00010502844141069396,
      "loss": 0.3296,
      "step": 2092
    },
    {
      "epoch": 0.4756818181818182,
      "grad_norm": 0.030249696224927902,
      "learning_rate": 0.00010498293515358362,
      "loss": 0.2572,
      "step": 2093
    },
    {
      "epoch": 0.4759090909090909,
      "grad_norm": 0.042397912591695786,
      "learning_rate": 0.00010493742889647327,
      "loss": 0.3045,
      "step": 2094
    },
    {
      "epoch": 0.47613636363636364,
      "grad_norm": 0.04683072119951248,
      "learning_rate": 0.00010489192263936291,
      "loss": 0.3014,
      "step": 2095
    },
    {
      "epoch": 0.4763636363636364,
      "grad_norm": 0.04590091109275818,
      "learning_rate": 0.00010484641638225256,
      "loss": 0.2889,
      "step": 2096
    },
    {
      "epoch": 0.47659090909090907,
      "grad_norm": 0.04359249770641327,
      "learning_rate": 0.00010480091012514222,
      "loss": 0.3107,
      "step": 2097
    },
    {
      "epoch": 0.4768181818181818,
      "grad_norm": 0.049927353858947754,
      "learning_rate": 0.00010475540386803187,
      "loss": 0.3067,
      "step": 2098
    },
    {
      "epoch": 0.47704545454545455,
      "grad_norm": 0.0552348792552948,
      "learning_rate": 0.00010470989761092152,
      "loss": 0.3497,
      "step": 2099
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.04500812664628029,
      "learning_rate": 0.00010466439135381116,
      "loss": 0.3225,
      "step": 2100
    },
    {
      "epoch": 0.4775,
      "grad_norm": 0.04049305617809296,
      "learning_rate": 0.00010461888509670079,
      "loss": 0.2747,
      "step": 2101
    },
    {
      "epoch": 0.4777272727272727,
      "grad_norm": 0.05639177933335304,
      "learning_rate": 0.00010457337883959044,
      "loss": 0.3177,
      "step": 2102
    },
    {
      "epoch": 0.47795454545454547,
      "grad_norm": 0.04902176186442375,
      "learning_rate": 0.00010452787258248009,
      "loss": 0.3177,
      "step": 2103
    },
    {
      "epoch": 0.4781818181818182,
      "grad_norm": 0.05912293866276741,
      "learning_rate": 0.00010448236632536974,
      "loss": 0.4164,
      "step": 2104
    },
    {
      "epoch": 0.4784090909090909,
      "grad_norm": 0.04391709342598915,
      "learning_rate": 0.00010443686006825938,
      "loss": 0.3176,
      "step": 2105
    },
    {
      "epoch": 0.47863636363636364,
      "grad_norm": 0.0298987478017807,
      "learning_rate": 0.00010439135381114904,
      "loss": 0.2851,
      "step": 2106
    },
    {
      "epoch": 0.4788636363636364,
      "grad_norm": 0.05021084100008011,
      "learning_rate": 0.00010434584755403869,
      "loss": 0.3463,
      "step": 2107
    },
    {
      "epoch": 0.47909090909090907,
      "grad_norm": 0.04034895449876785,
      "learning_rate": 0.00010430034129692834,
      "loss": 0.3321,
      "step": 2108
    },
    {
      "epoch": 0.4793181818181818,
      "grad_norm": 0.05336899682879448,
      "learning_rate": 0.000104254835039818,
      "loss": 0.3452,
      "step": 2109
    },
    {
      "epoch": 0.47954545454545455,
      "grad_norm": 0.05933058261871338,
      "learning_rate": 0.00010420932878270763,
      "loss": 0.3381,
      "step": 2110
    },
    {
      "epoch": 0.4797727272727273,
      "grad_norm": 0.04521196335554123,
      "learning_rate": 0.00010416382252559726,
      "loss": 0.3483,
      "step": 2111
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.05945233255624771,
      "learning_rate": 0.00010411831626848691,
      "loss": 0.3597,
      "step": 2112
    },
    {
      "epoch": 0.4802272727272727,
      "grad_norm": 0.04149797931313515,
      "learning_rate": 0.00010407281001137657,
      "loss": 0.3079,
      "step": 2113
    },
    {
      "epoch": 0.48045454545454547,
      "grad_norm": 0.04771694540977478,
      "learning_rate": 0.00010402730375426622,
      "loss": 0.3294,
      "step": 2114
    },
    {
      "epoch": 0.48068181818181815,
      "grad_norm": 0.056595779955387115,
      "learning_rate": 0.00010398179749715586,
      "loss": 0.3524,
      "step": 2115
    },
    {
      "epoch": 0.4809090909090909,
      "grad_norm": 0.061529990285634995,
      "learning_rate": 0.00010393629124004551,
      "loss": 0.405,
      "step": 2116
    },
    {
      "epoch": 0.48113636363636364,
      "grad_norm": 0.04603906348347664,
      "learning_rate": 0.00010389078498293516,
      "loss": 0.2711,
      "step": 2117
    },
    {
      "epoch": 0.4813636363636364,
      "grad_norm": 0.04983168840408325,
      "learning_rate": 0.00010384527872582482,
      "loss": 0.3673,
      "step": 2118
    },
    {
      "epoch": 0.48159090909090907,
      "grad_norm": 0.06541743874549866,
      "learning_rate": 0.00010379977246871447,
      "loss": 0.3881,
      "step": 2119
    },
    {
      "epoch": 0.4818181818181818,
      "grad_norm": 0.04019864276051521,
      "learning_rate": 0.0001037542662116041,
      "loss": 0.2827,
      "step": 2120
    },
    {
      "epoch": 0.48204545454545455,
      "grad_norm": 0.04342533275485039,
      "learning_rate": 0.00010370875995449373,
      "loss": 0.3363,
      "step": 2121
    },
    {
      "epoch": 0.4822727272727273,
      "grad_norm": 0.05240562930703163,
      "learning_rate": 0.00010366325369738339,
      "loss": 0.3558,
      "step": 2122
    },
    {
      "epoch": 0.4825,
      "grad_norm": 0.03124803490936756,
      "learning_rate": 0.00010361774744027304,
      "loss": 0.272,
      "step": 2123
    },
    {
      "epoch": 0.4827272727272727,
      "grad_norm": 0.033652834594249725,
      "learning_rate": 0.00010357224118316269,
      "loss": 0.2523,
      "step": 2124
    },
    {
      "epoch": 0.48295454545454547,
      "grad_norm": 0.057131074368953705,
      "learning_rate": 0.00010352673492605233,
      "loss": 0.3098,
      "step": 2125
    },
    {
      "epoch": 0.48318181818181816,
      "grad_norm": 0.04023115709424019,
      "learning_rate": 0.00010348122866894198,
      "loss": 0.312,
      "step": 2126
    },
    {
      "epoch": 0.4834090909090909,
      "grad_norm": 0.044420026242733,
      "learning_rate": 0.00010343572241183164,
      "loss": 0.31,
      "step": 2127
    },
    {
      "epoch": 0.48363636363636364,
      "grad_norm": 0.043875087052583694,
      "learning_rate": 0.00010339021615472129,
      "loss": 0.34,
      "step": 2128
    },
    {
      "epoch": 0.4838636363636364,
      "grad_norm": 0.04704499617218971,
      "learning_rate": 0.00010334470989761094,
      "loss": 0.3251,
      "step": 2129
    },
    {
      "epoch": 0.48409090909090907,
      "grad_norm": 0.04197055473923683,
      "learning_rate": 0.00010329920364050057,
      "loss": 0.2834,
      "step": 2130
    },
    {
      "epoch": 0.4843181818181818,
      "grad_norm": 0.043072570115327835,
      "learning_rate": 0.00010325369738339021,
      "loss": 0.3384,
      "step": 2131
    },
    {
      "epoch": 0.48454545454545456,
      "grad_norm": 0.04834172874689102,
      "learning_rate": 0.00010320819112627986,
      "loss": 0.3387,
      "step": 2132
    },
    {
      "epoch": 0.4847727272727273,
      "grad_norm": 0.041735224425792694,
      "learning_rate": 0.00010316268486916951,
      "loss": 0.3333,
      "step": 2133
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.04557831212878227,
      "learning_rate": 0.00010311717861205917,
      "loss": 0.2882,
      "step": 2134
    },
    {
      "epoch": 0.48522727272727273,
      "grad_norm": 0.04736560955643654,
      "learning_rate": 0.0001030716723549488,
      "loss": 0.3422,
      "step": 2135
    },
    {
      "epoch": 0.48545454545454547,
      "grad_norm": 0.052232492715120316,
      "learning_rate": 0.00010302616609783846,
      "loss": 0.359,
      "step": 2136
    },
    {
      "epoch": 0.48568181818181816,
      "grad_norm": 0.044971246272325516,
      "learning_rate": 0.00010298065984072811,
      "loss": 0.3611,
      "step": 2137
    },
    {
      "epoch": 0.4859090909090909,
      "grad_norm": 0.0410076342523098,
      "learning_rate": 0.00010293515358361776,
      "loss": 0.3231,
      "step": 2138
    },
    {
      "epoch": 0.48613636363636364,
      "grad_norm": 0.04207301139831543,
      "learning_rate": 0.00010288964732650742,
      "loss": 0.2597,
      "step": 2139
    },
    {
      "epoch": 0.4863636363636364,
      "grad_norm": 0.04389970749616623,
      "learning_rate": 0.00010284414106939704,
      "loss": 0.3532,
      "step": 2140
    },
    {
      "epoch": 0.4865909090909091,
      "grad_norm": 0.05231154337525368,
      "learning_rate": 0.00010279863481228668,
      "loss": 0.3888,
      "step": 2141
    },
    {
      "epoch": 0.4868181818181818,
      "grad_norm": 0.059170570224523544,
      "learning_rate": 0.00010275312855517633,
      "loss": 0.4193,
      "step": 2142
    },
    {
      "epoch": 0.48704545454545456,
      "grad_norm": 0.03601829707622528,
      "learning_rate": 0.00010270762229806599,
      "loss": 0.2948,
      "step": 2143
    },
    {
      "epoch": 0.48727272727272725,
      "grad_norm": 0.05891947075724602,
      "learning_rate": 0.00010266211604095564,
      "loss": 0.3171,
      "step": 2144
    },
    {
      "epoch": 0.4875,
      "grad_norm": 0.03988565877079964,
      "learning_rate": 0.00010261660978384528,
      "loss": 0.3022,
      "step": 2145
    },
    {
      "epoch": 0.48772727272727273,
      "grad_norm": 0.044906049966812134,
      "learning_rate": 0.00010257110352673493,
      "loss": 0.3227,
      "step": 2146
    },
    {
      "epoch": 0.4879545454545455,
      "grad_norm": 0.04421217739582062,
      "learning_rate": 0.00010252559726962459,
      "loss": 0.3394,
      "step": 2147
    },
    {
      "epoch": 0.48818181818181816,
      "grad_norm": 0.04324917867779732,
      "learning_rate": 0.00010248009101251424,
      "loss": 0.3276,
      "step": 2148
    },
    {
      "epoch": 0.4884090909090909,
      "grad_norm": 0.04212573170661926,
      "learning_rate": 0.00010243458475540389,
      "loss": 0.3128,
      "step": 2149
    },
    {
      "epoch": 0.48863636363636365,
      "grad_norm": 0.05452375486493111,
      "learning_rate": 0.00010238907849829352,
      "loss": 0.3757,
      "step": 2150
    },
    {
      "epoch": 0.4888636363636364,
      "grad_norm": 0.0622163750231266,
      "learning_rate": 0.00010234357224118316,
      "loss": 0.3746,
      "step": 2151
    },
    {
      "epoch": 0.4890909090909091,
      "grad_norm": 0.049334585666656494,
      "learning_rate": 0.00010229806598407281,
      "loss": 0.3858,
      "step": 2152
    },
    {
      "epoch": 0.4893181818181818,
      "grad_norm": 0.05319709703326225,
      "learning_rate": 0.00010225255972696246,
      "loss": 0.3757,
      "step": 2153
    },
    {
      "epoch": 0.48954545454545456,
      "grad_norm": 0.048169612884521484,
      "learning_rate": 0.00010220705346985211,
      "loss": 0.347,
      "step": 2154
    },
    {
      "epoch": 0.48977272727272725,
      "grad_norm": 0.04213601350784302,
      "learning_rate": 0.00010216154721274175,
      "loss": 0.3602,
      "step": 2155
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.050751809030771255,
      "learning_rate": 0.0001021160409556314,
      "loss": 0.3845,
      "step": 2156
    },
    {
      "epoch": 0.49022727272727273,
      "grad_norm": 0.05280030146241188,
      "learning_rate": 0.00010207053469852106,
      "loss": 0.3235,
      "step": 2157
    },
    {
      "epoch": 0.4904545454545455,
      "grad_norm": 0.044915419071912766,
      "learning_rate": 0.00010202502844141071,
      "loss": 0.3227,
      "step": 2158
    },
    {
      "epoch": 0.49068181818181816,
      "grad_norm": 0.042856376618146896,
      "learning_rate": 0.00010197952218430034,
      "loss": 0.301,
      "step": 2159
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.034014761447906494,
      "learning_rate": 0.00010193401592718999,
      "loss": 0.2516,
      "step": 2160
    },
    {
      "epoch": 0.49113636363636365,
      "grad_norm": 0.04830172657966614,
      "learning_rate": 0.00010188850967007963,
      "loss": 0.2876,
      "step": 2161
    },
    {
      "epoch": 0.4913636363636364,
      "grad_norm": 0.04979773238301277,
      "learning_rate": 0.00010184300341296928,
      "loss": 0.3734,
      "step": 2162
    },
    {
      "epoch": 0.4915909090909091,
      "grad_norm": 0.04973607510328293,
      "learning_rate": 0.00010179749715585894,
      "loss": 0.3644,
      "step": 2163
    },
    {
      "epoch": 0.4918181818181818,
      "grad_norm": 0.054454777389764786,
      "learning_rate": 0.00010175199089874859,
      "loss": 0.3469,
      "step": 2164
    },
    {
      "epoch": 0.49204545454545456,
      "grad_norm": 0.029921354725956917,
      "learning_rate": 0.00010170648464163823,
      "loss": 0.2373,
      "step": 2165
    },
    {
      "epoch": 0.49227272727272725,
      "grad_norm": 0.04030961915850639,
      "learning_rate": 0.00010166097838452788,
      "loss": 0.3164,
      "step": 2166
    },
    {
      "epoch": 0.4925,
      "grad_norm": 0.04507369175553322,
      "learning_rate": 0.00010161547212741753,
      "loss": 0.3522,
      "step": 2167
    },
    {
      "epoch": 0.49272727272727274,
      "grad_norm": 0.04788937047123909,
      "learning_rate": 0.00010156996587030719,
      "loss": 0.3619,
      "step": 2168
    },
    {
      "epoch": 0.4929545454545455,
      "grad_norm": 0.03446674719452858,
      "learning_rate": 0.00010152445961319681,
      "loss": 0.2803,
      "step": 2169
    },
    {
      "epoch": 0.49318181818181817,
      "grad_norm": 0.03885728120803833,
      "learning_rate": 0.00010147895335608646,
      "loss": 0.3432,
      "step": 2170
    },
    {
      "epoch": 0.4934090909090909,
      "grad_norm": 0.044494736939668655,
      "learning_rate": 0.0001014334470989761,
      "loss": 0.3467,
      "step": 2171
    },
    {
      "epoch": 0.49363636363636365,
      "grad_norm": 0.06075918674468994,
      "learning_rate": 0.00010138794084186576,
      "loss": 0.3204,
      "step": 2172
    },
    {
      "epoch": 0.49386363636363634,
      "grad_norm": 0.05766715109348297,
      "learning_rate": 0.00010134243458475541,
      "loss": 0.3813,
      "step": 2173
    },
    {
      "epoch": 0.4940909090909091,
      "grad_norm": 0.04105735942721367,
      "learning_rate": 0.00010129692832764506,
      "loss": 0.2993,
      "step": 2174
    },
    {
      "epoch": 0.4943181818181818,
      "grad_norm": 0.04750300943851471,
      "learning_rate": 0.0001012514220705347,
      "loss": 0.3293,
      "step": 2175
    },
    {
      "epoch": 0.49454545454545457,
      "grad_norm": 0.04867810010910034,
      "learning_rate": 0.00010120591581342435,
      "loss": 0.3321,
      "step": 2176
    },
    {
      "epoch": 0.49477272727272725,
      "grad_norm": 0.03995518758893013,
      "learning_rate": 0.00010116040955631401,
      "loss": 0.3153,
      "step": 2177
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.04206039011478424,
      "learning_rate": 0.00010111490329920366,
      "loss": 0.3452,
      "step": 2178
    },
    {
      "epoch": 0.49522727272727274,
      "grad_norm": 0.04398530721664429,
      "learning_rate": 0.00010106939704209329,
      "loss": 0.3531,
      "step": 2179
    },
    {
      "epoch": 0.4954545454545455,
      "grad_norm": 0.038679104298353195,
      "learning_rate": 0.00010102389078498294,
      "loss": 0.3375,
      "step": 2180
    },
    {
      "epoch": 0.49568181818181817,
      "grad_norm": 0.04801320657134056,
      "learning_rate": 0.00010097838452787258,
      "loss": 0.2944,
      "step": 2181
    },
    {
      "epoch": 0.4959090909090909,
      "grad_norm": 0.04376261308789253,
      "learning_rate": 0.00010093287827076223,
      "loss": 0.2498,
      "step": 2182
    },
    {
      "epoch": 0.49613636363636365,
      "grad_norm": 0.03484663367271423,
      "learning_rate": 0.00010088737201365188,
      "loss": 0.2591,
      "step": 2183
    },
    {
      "epoch": 0.49636363636363634,
      "grad_norm": 0.058853838592767715,
      "learning_rate": 0.00010084186575654154,
      "loss": 0.3342,
      "step": 2184
    },
    {
      "epoch": 0.4965909090909091,
      "grad_norm": 0.03984813392162323,
      "learning_rate": 0.00010079635949943118,
      "loss": 0.2887,
      "step": 2185
    },
    {
      "epoch": 0.4968181818181818,
      "grad_norm": 0.06304723024368286,
      "learning_rate": 0.00010075085324232083,
      "loss": 0.4343,
      "step": 2186
    },
    {
      "epoch": 0.49704545454545457,
      "grad_norm": 0.0342320054769516,
      "learning_rate": 0.00010070534698521048,
      "loss": 0.3047,
      "step": 2187
    },
    {
      "epoch": 0.49727272727272726,
      "grad_norm": 0.045963354408741,
      "learning_rate": 0.0001006598407281001,
      "loss": 0.3149,
      "step": 2188
    },
    {
      "epoch": 0.4975,
      "grad_norm": 0.037451911717653275,
      "learning_rate": 0.00010061433447098976,
      "loss": 0.2979,
      "step": 2189
    },
    {
      "epoch": 0.49772727272727274,
      "grad_norm": 0.04883195459842682,
      "learning_rate": 0.00010056882821387941,
      "loss": 0.317,
      "step": 2190
    },
    {
      "epoch": 0.4979545454545454,
      "grad_norm": 0.04557783529162407,
      "learning_rate": 0.00010052332195676905,
      "loss": 0.3393,
      "step": 2191
    },
    {
      "epoch": 0.49818181818181817,
      "grad_norm": 0.06264685094356537,
      "learning_rate": 0.0001004778156996587,
      "loss": 0.3792,
      "step": 2192
    },
    {
      "epoch": 0.4984090909090909,
      "grad_norm": 0.05560827627778053,
      "learning_rate": 0.00010043230944254836,
      "loss": 0.3663,
      "step": 2193
    },
    {
      "epoch": 0.49863636363636366,
      "grad_norm": 0.032391391694545746,
      "learning_rate": 0.00010038680318543801,
      "loss": 0.2148,
      "step": 2194
    },
    {
      "epoch": 0.49886363636363634,
      "grad_norm": 0.03999614343047142,
      "learning_rate": 0.00010034129692832765,
      "loss": 0.3204,
      "step": 2195
    },
    {
      "epoch": 0.4990909090909091,
      "grad_norm": 0.041831377893686295,
      "learning_rate": 0.0001002957906712173,
      "loss": 0.3163,
      "step": 2196
    },
    {
      "epoch": 0.4993181818181818,
      "grad_norm": 0.033558640629053116,
      "learning_rate": 0.00010025028441410695,
      "loss": 0.2838,
      "step": 2197
    },
    {
      "epoch": 0.49954545454545457,
      "grad_norm": 0.05597800761461258,
      "learning_rate": 0.00010020477815699658,
      "loss": 0.3603,
      "step": 2198
    },
    {
      "epoch": 0.49977272727272726,
      "grad_norm": 0.04402537643909454,
      "learning_rate": 0.00010015927189988623,
      "loss": 0.3413,
      "step": 2199
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.04170617833733559,
      "learning_rate": 0.00010011376564277589,
      "loss": 0.3515,
      "step": 2200
    },
    {
      "epoch": 0.5002272727272727,
      "grad_norm": 0.03976845741271973,
      "learning_rate": 0.00010006825938566553,
      "loss": 0.3526,
      "step": 2201
    },
    {
      "epoch": 0.5004545454545455,
      "grad_norm": 0.044225629419088364,
      "learning_rate": 0.00010002275312855518,
      "loss": 0.3864,
      "step": 2202
    },
    {
      "epoch": 0.5006818181818182,
      "grad_norm": 0.038943443447351456,
      "learning_rate": 9.997724687144483e-05,
      "loss": 0.289,
      "step": 2203
    },
    {
      "epoch": 0.5009090909090909,
      "grad_norm": 0.051239874213933945,
      "learning_rate": 9.993174061433448e-05,
      "loss": 0.3058,
      "step": 2204
    },
    {
      "epoch": 0.5011363636363636,
      "grad_norm": 0.0538112111389637,
      "learning_rate": 9.988623435722412e-05,
      "loss": 0.3641,
      "step": 2205
    },
    {
      "epoch": 0.5013636363636363,
      "grad_norm": 0.040914177894592285,
      "learning_rate": 9.984072810011376e-05,
      "loss": 0.3266,
      "step": 2206
    },
    {
      "epoch": 0.5015909090909091,
      "grad_norm": 0.04464450851082802,
      "learning_rate": 9.979522184300342e-05,
      "loss": 0.3486,
      "step": 2207
    },
    {
      "epoch": 0.5018181818181818,
      "grad_norm": 0.05526770278811455,
      "learning_rate": 9.974971558589307e-05,
      "loss": 0.336,
      "step": 2208
    },
    {
      "epoch": 0.5020454545454546,
      "grad_norm": 0.04257398843765259,
      "learning_rate": 9.970420932878272e-05,
      "loss": 0.2999,
      "step": 2209
    },
    {
      "epoch": 0.5022727272727273,
      "grad_norm": 0.03740955889225006,
      "learning_rate": 9.965870307167236e-05,
      "loss": 0.314,
      "step": 2210
    },
    {
      "epoch": 0.5025,
      "grad_norm": 0.03838466852903366,
      "learning_rate": 9.9613196814562e-05,
      "loss": 0.2374,
      "step": 2211
    },
    {
      "epoch": 0.5027272727272727,
      "grad_norm": 0.054642219096422195,
      "learning_rate": 9.956769055745165e-05,
      "loss": 0.3902,
      "step": 2212
    },
    {
      "epoch": 0.5029545454545454,
      "grad_norm": 0.05011266469955444,
      "learning_rate": 9.95221843003413e-05,
      "loss": 0.3904,
      "step": 2213
    },
    {
      "epoch": 0.5031818181818182,
      "grad_norm": 0.050847724080085754,
      "learning_rate": 9.947667804323096e-05,
      "loss": 0.3431,
      "step": 2214
    },
    {
      "epoch": 0.5034090909090909,
      "grad_norm": 0.03429364413022995,
      "learning_rate": 9.94311717861206e-05,
      "loss": 0.2542,
      "step": 2215
    },
    {
      "epoch": 0.5036363636363637,
      "grad_norm": 0.054261352866888046,
      "learning_rate": 9.938566552901024e-05,
      "loss": 0.3128,
      "step": 2216
    },
    {
      "epoch": 0.5038636363636364,
      "grad_norm": 0.03952290117740631,
      "learning_rate": 9.934015927189989e-05,
      "loss": 0.2854,
      "step": 2217
    },
    {
      "epoch": 0.5040909090909091,
      "grad_norm": 0.05325596407055855,
      "learning_rate": 9.929465301478954e-05,
      "loss": 0.3942,
      "step": 2218
    },
    {
      "epoch": 0.5043181818181818,
      "grad_norm": 0.04852036386728287,
      "learning_rate": 9.92491467576792e-05,
      "loss": 0.3718,
      "step": 2219
    },
    {
      "epoch": 0.5045454545454545,
      "grad_norm": 0.03755400702357292,
      "learning_rate": 9.920364050056883e-05,
      "loss": 0.2893,
      "step": 2220
    },
    {
      "epoch": 0.5047727272727273,
      "grad_norm": 0.04955723136663437,
      "learning_rate": 9.915813424345847e-05,
      "loss": 0.3184,
      "step": 2221
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.038261424750089645,
      "learning_rate": 9.911262798634813e-05,
      "loss": 0.2789,
      "step": 2222
    },
    {
      "epoch": 0.5052272727272727,
      "grad_norm": 0.0534668043255806,
      "learning_rate": 9.906712172923778e-05,
      "loss": 0.4078,
      "step": 2223
    },
    {
      "epoch": 0.5054545454545455,
      "grad_norm": 0.03669346868991852,
      "learning_rate": 9.902161547212743e-05,
      "loss": 0.2506,
      "step": 2224
    },
    {
      "epoch": 0.5056818181818182,
      "grad_norm": 0.043505460023880005,
      "learning_rate": 9.897610921501707e-05,
      "loss": 0.312,
      "step": 2225
    },
    {
      "epoch": 0.5059090909090909,
      "grad_norm": 0.04190162196755409,
      "learning_rate": 9.893060295790671e-05,
      "loss": 0.3387,
      "step": 2226
    },
    {
      "epoch": 0.5061363636363636,
      "grad_norm": 0.04367084428668022,
      "learning_rate": 9.888509670079636e-05,
      "loss": 0.3336,
      "step": 2227
    },
    {
      "epoch": 0.5063636363636363,
      "grad_norm": 0.03849984332919121,
      "learning_rate": 9.883959044368602e-05,
      "loss": 0.3218,
      "step": 2228
    },
    {
      "epoch": 0.5065909090909091,
      "grad_norm": 0.05457264557480812,
      "learning_rate": 9.879408418657567e-05,
      "loss": 0.4058,
      "step": 2229
    },
    {
      "epoch": 0.5068181818181818,
      "grad_norm": 0.05164181441068649,
      "learning_rate": 9.874857792946531e-05,
      "loss": 0.4351,
      "step": 2230
    },
    {
      "epoch": 0.5070454545454546,
      "grad_norm": 0.04852722957730293,
      "learning_rate": 9.870307167235495e-05,
      "loss": 0.2975,
      "step": 2231
    },
    {
      "epoch": 0.5072727272727273,
      "grad_norm": 0.04660382494330406,
      "learning_rate": 9.86575654152446e-05,
      "loss": 0.3531,
      "step": 2232
    },
    {
      "epoch": 0.5075,
      "grad_norm": 0.0472610704600811,
      "learning_rate": 9.861205915813425e-05,
      "loss": 0.3149,
      "step": 2233
    },
    {
      "epoch": 0.5077272727272727,
      "grad_norm": 0.040160395205020905,
      "learning_rate": 9.85665529010239e-05,
      "loss": 0.3195,
      "step": 2234
    },
    {
      "epoch": 0.5079545454545454,
      "grad_norm": 0.056138329207897186,
      "learning_rate": 9.852104664391355e-05,
      "loss": 0.3731,
      "step": 2235
    },
    {
      "epoch": 0.5081818181818182,
      "grad_norm": 0.05644037202000618,
      "learning_rate": 9.847554038680318e-05,
      "loss": 0.3967,
      "step": 2236
    },
    {
      "epoch": 0.5084090909090909,
      "grad_norm": 0.04655447229743004,
      "learning_rate": 9.843003412969284e-05,
      "loss": 0.3379,
      "step": 2237
    },
    {
      "epoch": 0.5086363636363637,
      "grad_norm": 0.04442010819911957,
      "learning_rate": 9.838452787258249e-05,
      "loss": 0.3323,
      "step": 2238
    },
    {
      "epoch": 0.5088636363636364,
      "grad_norm": 0.06057637184858322,
      "learning_rate": 9.833902161547213e-05,
      "loss": 0.427,
      "step": 2239
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.08079715073108673,
      "learning_rate": 9.829351535836178e-05,
      "loss": 0.3849,
      "step": 2240
    },
    {
      "epoch": 0.5093181818181818,
      "grad_norm": 0.04708721488714218,
      "learning_rate": 9.824800910125142e-05,
      "loss": 0.3448,
      "step": 2241
    },
    {
      "epoch": 0.5095454545454545,
      "grad_norm": 0.03867344930768013,
      "learning_rate": 9.820250284414107e-05,
      "loss": 0.2818,
      "step": 2242
    },
    {
      "epoch": 0.5097727272727273,
      "grad_norm": 0.04472899064421654,
      "learning_rate": 9.815699658703073e-05,
      "loss": 0.3606,
      "step": 2243
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05110976845026016,
      "learning_rate": 9.811149032992037e-05,
      "loss": 0.3273,
      "step": 2244
    },
    {
      "epoch": 0.5102272727272728,
      "grad_norm": 0.03775862604379654,
      "learning_rate": 9.806598407281002e-05,
      "loss": 0.3366,
      "step": 2245
    },
    {
      "epoch": 0.5104545454545455,
      "grad_norm": 0.0635090246796608,
      "learning_rate": 9.802047781569966e-05,
      "loss": 0.4307,
      "step": 2246
    },
    {
      "epoch": 0.5106818181818182,
      "grad_norm": 0.05697647109627724,
      "learning_rate": 9.797497155858931e-05,
      "loss": 0.379,
      "step": 2247
    },
    {
      "epoch": 0.5109090909090909,
      "grad_norm": 0.05100737884640694,
      "learning_rate": 9.792946530147896e-05,
      "loss": 0.3821,
      "step": 2248
    },
    {
      "epoch": 0.5111363636363636,
      "grad_norm": 0.03914007544517517,
      "learning_rate": 9.78839590443686e-05,
      "loss": 0.3661,
      "step": 2249
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 0.04007449001073837,
      "learning_rate": 9.783845278725826e-05,
      "loss": 0.3358,
      "step": 2250
    },
    {
      "epoch": 0.5115909090909091,
      "grad_norm": 0.05580057203769684,
      "learning_rate": 9.77929465301479e-05,
      "loss": 0.3882,
      "step": 2251
    },
    {
      "epoch": 0.5118181818181818,
      "grad_norm": 0.0455157533288002,
      "learning_rate": 9.774744027303755e-05,
      "loss": 0.3958,
      "step": 2252
    },
    {
      "epoch": 0.5120454545454546,
      "grad_norm": 0.0761418417096138,
      "learning_rate": 9.77019340159272e-05,
      "loss": 0.4908,
      "step": 2253
    },
    {
      "epoch": 0.5122727272727273,
      "grad_norm": 0.05110824108123779,
      "learning_rate": 9.765642775881684e-05,
      "loss": 0.3261,
      "step": 2254
    },
    {
      "epoch": 0.5125,
      "grad_norm": 0.047234516590833664,
      "learning_rate": 9.761092150170649e-05,
      "loss": 0.3648,
      "step": 2255
    },
    {
      "epoch": 0.5127272727272727,
      "grad_norm": 0.03861096873879433,
      "learning_rate": 9.756541524459613e-05,
      "loss": 0.2771,
      "step": 2256
    },
    {
      "epoch": 0.5129545454545454,
      "grad_norm": 0.048503004014492035,
      "learning_rate": 9.751990898748579e-05,
      "loss": 0.3542,
      "step": 2257
    },
    {
      "epoch": 0.5131818181818182,
      "grad_norm": 0.06092571094632149,
      "learning_rate": 9.747440273037544e-05,
      "loss": 0.3817,
      "step": 2258
    },
    {
      "epoch": 0.5134090909090909,
      "grad_norm": 0.04336049407720566,
      "learning_rate": 9.742889647326508e-05,
      "loss": 0.307,
      "step": 2259
    },
    {
      "epoch": 0.5136363636363637,
      "grad_norm": 0.050364959985017776,
      "learning_rate": 9.738339021615473e-05,
      "loss": 0.3517,
      "step": 2260
    },
    {
      "epoch": 0.5138636363636364,
      "grad_norm": 0.04248999431729317,
      "learning_rate": 9.733788395904437e-05,
      "loss": 0.2998,
      "step": 2261
    },
    {
      "epoch": 0.514090909090909,
      "grad_norm": 0.03912470489740372,
      "learning_rate": 9.729237770193402e-05,
      "loss": 0.2929,
      "step": 2262
    },
    {
      "epoch": 0.5143181818181818,
      "grad_norm": 0.04167678952217102,
      "learning_rate": 9.724687144482368e-05,
      "loss": 0.3045,
      "step": 2263
    },
    {
      "epoch": 0.5145454545454545,
      "grad_norm": 0.05105339363217354,
      "learning_rate": 9.720136518771331e-05,
      "loss": 0.3263,
      "step": 2264
    },
    {
      "epoch": 0.5147727272727273,
      "grad_norm": 0.034482140094041824,
      "learning_rate": 9.715585893060297e-05,
      "loss": 0.2864,
      "step": 2265
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.04287823289632797,
      "learning_rate": 9.71103526734926e-05,
      "loss": 0.3062,
      "step": 2266
    },
    {
      "epoch": 0.5152272727272728,
      "grad_norm": 0.03681912273168564,
      "learning_rate": 9.706484641638226e-05,
      "loss": 0.2859,
      "step": 2267
    },
    {
      "epoch": 0.5154545454545455,
      "grad_norm": 0.037004806101322174,
      "learning_rate": 9.701934015927191e-05,
      "loss": 0.3458,
      "step": 2268
    },
    {
      "epoch": 0.5156818181818181,
      "grad_norm": 0.051304832100868225,
      "learning_rate": 9.697383390216155e-05,
      "loss": 0.3497,
      "step": 2269
    },
    {
      "epoch": 0.5159090909090909,
      "grad_norm": 0.04271228238940239,
      "learning_rate": 9.69283276450512e-05,
      "loss": 0.3495,
      "step": 2270
    },
    {
      "epoch": 0.5161363636363636,
      "grad_norm": 0.039448849856853485,
      "learning_rate": 9.688282138794084e-05,
      "loss": 0.3181,
      "step": 2271
    },
    {
      "epoch": 0.5163636363636364,
      "grad_norm": 0.04457559064030647,
      "learning_rate": 9.68373151308305e-05,
      "loss": 0.351,
      "step": 2272
    },
    {
      "epoch": 0.5165909090909091,
      "grad_norm": 0.039506904780864716,
      "learning_rate": 9.679180887372014e-05,
      "loss": 0.3507,
      "step": 2273
    },
    {
      "epoch": 0.5168181818181818,
      "grad_norm": 0.04535943269729614,
      "learning_rate": 9.674630261660979e-05,
      "loss": 0.3608,
      "step": 2274
    },
    {
      "epoch": 0.5170454545454546,
      "grad_norm": 0.048590756952762604,
      "learning_rate": 9.670079635949944e-05,
      "loss": 0.3543,
      "step": 2275
    },
    {
      "epoch": 0.5172727272727272,
      "grad_norm": 0.047624148428440094,
      "learning_rate": 9.665529010238908e-05,
      "loss": 0.3332,
      "step": 2276
    },
    {
      "epoch": 0.5175,
      "grad_norm": 0.03860611468553543,
      "learning_rate": 9.660978384527873e-05,
      "loss": 0.29,
      "step": 2277
    },
    {
      "epoch": 0.5177272727272727,
      "grad_norm": 0.04198507219552994,
      "learning_rate": 9.656427758816837e-05,
      "loss": 0.3255,
      "step": 2278
    },
    {
      "epoch": 0.5179545454545454,
      "grad_norm": 0.0358518622815609,
      "learning_rate": 9.651877133105803e-05,
      "loss": 0.2653,
      "step": 2279
    },
    {
      "epoch": 0.5181818181818182,
      "grad_norm": 0.032974015921354294,
      "learning_rate": 9.647326507394768e-05,
      "loss": 0.2738,
      "step": 2280
    },
    {
      "epoch": 0.5184090909090909,
      "grad_norm": 0.05472086742520332,
      "learning_rate": 9.642775881683732e-05,
      "loss": 0.3853,
      "step": 2281
    },
    {
      "epoch": 0.5186363636363637,
      "grad_norm": 0.042829789221286774,
      "learning_rate": 9.638225255972697e-05,
      "loss": 0.3465,
      "step": 2282
    },
    {
      "epoch": 0.5188636363636364,
      "grad_norm": 0.05284688621759415,
      "learning_rate": 9.633674630261661e-05,
      "loss": 0.2818,
      "step": 2283
    },
    {
      "epoch": 0.519090909090909,
      "grad_norm": 0.03744533285498619,
      "learning_rate": 9.629124004550626e-05,
      "loss": 0.3199,
      "step": 2284
    },
    {
      "epoch": 0.5193181818181818,
      "grad_norm": 0.05076959356665611,
      "learning_rate": 9.624573378839592e-05,
      "loss": 0.3709,
      "step": 2285
    },
    {
      "epoch": 0.5195454545454545,
      "grad_norm": 0.04721629619598389,
      "learning_rate": 9.620022753128555e-05,
      "loss": 0.2995,
      "step": 2286
    },
    {
      "epoch": 0.5197727272727273,
      "grad_norm": 0.04796355962753296,
      "learning_rate": 9.615472127417521e-05,
      "loss": 0.3196,
      "step": 2287
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.04709908738732338,
      "learning_rate": 9.610921501706485e-05,
      "loss": 0.3488,
      "step": 2288
    },
    {
      "epoch": 0.5202272727272728,
      "grad_norm": 0.054819926619529724,
      "learning_rate": 9.60637087599545e-05,
      "loss": 0.3886,
      "step": 2289
    },
    {
      "epoch": 0.5204545454545455,
      "grad_norm": 0.047758542001247406,
      "learning_rate": 9.601820250284415e-05,
      "loss": 0.3614,
      "step": 2290
    },
    {
      "epoch": 0.5206818181818181,
      "grad_norm": 0.05584680289030075,
      "learning_rate": 9.597269624573379e-05,
      "loss": 0.3487,
      "step": 2291
    },
    {
      "epoch": 0.5209090909090909,
      "grad_norm": 0.04278184473514557,
      "learning_rate": 9.592718998862344e-05,
      "loss": 0.3349,
      "step": 2292
    },
    {
      "epoch": 0.5211363636363636,
      "grad_norm": 0.04120109975337982,
      "learning_rate": 9.588168373151308e-05,
      "loss": 0.3171,
      "step": 2293
    },
    {
      "epoch": 0.5213636363636364,
      "grad_norm": 0.04937288165092468,
      "learning_rate": 9.583617747440274e-05,
      "loss": 0.3165,
      "step": 2294
    },
    {
      "epoch": 0.5215909090909091,
      "grad_norm": 0.04759606719017029,
      "learning_rate": 9.579067121729239e-05,
      "loss": 0.2926,
      "step": 2295
    },
    {
      "epoch": 0.5218181818181818,
      "grad_norm": 0.047259170562028885,
      "learning_rate": 9.574516496018203e-05,
      "loss": 0.3454,
      "step": 2296
    },
    {
      "epoch": 0.5220454545454546,
      "grad_norm": 0.04766722023487091,
      "learning_rate": 9.569965870307168e-05,
      "loss": 0.3308,
      "step": 2297
    },
    {
      "epoch": 0.5222727272727272,
      "grad_norm": 0.05112061649560928,
      "learning_rate": 9.565415244596132e-05,
      "loss": 0.3272,
      "step": 2298
    },
    {
      "epoch": 0.5225,
      "grad_norm": 0.05214579403400421,
      "learning_rate": 9.560864618885097e-05,
      "loss": 0.3563,
      "step": 2299
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.05114701762795448,
      "learning_rate": 9.556313993174063e-05,
      "loss": 0.3501,
      "step": 2300
    },
    {
      "epoch": 0.5229545454545454,
      "grad_norm": 0.03870251029729843,
      "learning_rate": 9.551763367463027e-05,
      "loss": 0.3078,
      "step": 2301
    },
    {
      "epoch": 0.5231818181818182,
      "grad_norm": 0.032509759068489075,
      "learning_rate": 9.547212741751992e-05,
      "loss": 0.2662,
      "step": 2302
    },
    {
      "epoch": 0.5234090909090909,
      "grad_norm": 0.03521360084414482,
      "learning_rate": 9.542662116040956e-05,
      "loss": 0.31,
      "step": 2303
    },
    {
      "epoch": 0.5236363636363637,
      "grad_norm": 0.04851812869310379,
      "learning_rate": 9.538111490329921e-05,
      "loss": 0.3529,
      "step": 2304
    },
    {
      "epoch": 0.5238636363636363,
      "grad_norm": 0.04919256269931793,
      "learning_rate": 9.533560864618886e-05,
      "loss": 0.3391,
      "step": 2305
    },
    {
      "epoch": 0.524090909090909,
      "grad_norm": 0.035535361617803574,
      "learning_rate": 9.52901023890785e-05,
      "loss": 0.2936,
      "step": 2306
    },
    {
      "epoch": 0.5243181818181818,
      "grad_norm": 0.0529620386660099,
      "learning_rate": 9.524459613196816e-05,
      "loss": 0.3483,
      "step": 2307
    },
    {
      "epoch": 0.5245454545454545,
      "grad_norm": 0.03808970749378204,
      "learning_rate": 9.51990898748578e-05,
      "loss": 0.2729,
      "step": 2308
    },
    {
      "epoch": 0.5247727272727273,
      "grad_norm": 0.043305207043886185,
      "learning_rate": 9.515358361774745e-05,
      "loss": 0.276,
      "step": 2309
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.0469038188457489,
      "learning_rate": 9.51080773606371e-05,
      "loss": 0.3883,
      "step": 2310
    },
    {
      "epoch": 0.5252272727272728,
      "grad_norm": 0.04482362046837807,
      "learning_rate": 9.506257110352674e-05,
      "loss": 0.3261,
      "step": 2311
    },
    {
      "epoch": 0.5254545454545455,
      "grad_norm": 0.03408219665288925,
      "learning_rate": 9.501706484641638e-05,
      "loss": 0.3057,
      "step": 2312
    },
    {
      "epoch": 0.5256818181818181,
      "grad_norm": 0.048545725643634796,
      "learning_rate": 9.497155858930603e-05,
      "loss": 0.3187,
      "step": 2313
    },
    {
      "epoch": 0.5259090909090909,
      "grad_norm": 0.04114669933915138,
      "learning_rate": 9.492605233219568e-05,
      "loss": 0.3538,
      "step": 2314
    },
    {
      "epoch": 0.5261363636363636,
      "grad_norm": 0.043596167117357254,
      "learning_rate": 9.488054607508534e-05,
      "loss": 0.3176,
      "step": 2315
    },
    {
      "epoch": 0.5263636363636364,
      "grad_norm": 0.04672814533114433,
      "learning_rate": 9.483503981797498e-05,
      "loss": 0.3186,
      "step": 2316
    },
    {
      "epoch": 0.5265909090909091,
      "grad_norm": 0.03309139609336853,
      "learning_rate": 9.478953356086462e-05,
      "loss": 0.2934,
      "step": 2317
    },
    {
      "epoch": 0.5268181818181819,
      "grad_norm": 0.06000376492738724,
      "learning_rate": 9.474402730375427e-05,
      "loss": 0.3849,
      "step": 2318
    },
    {
      "epoch": 0.5270454545454546,
      "grad_norm": 0.06202666461467743,
      "learning_rate": 9.469852104664392e-05,
      "loss": 0.3897,
      "step": 2319
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.03997862711548805,
      "learning_rate": 9.465301478953356e-05,
      "loss": 0.2926,
      "step": 2320
    },
    {
      "epoch": 0.5275,
      "grad_norm": 0.0393032431602478,
      "learning_rate": 9.460750853242321e-05,
      "loss": 0.3258,
      "step": 2321
    },
    {
      "epoch": 0.5277272727272727,
      "grad_norm": 0.04268544167280197,
      "learning_rate": 9.456200227531285e-05,
      "loss": 0.3396,
      "step": 2322
    },
    {
      "epoch": 0.5279545454545455,
      "grad_norm": 0.03580775111913681,
      "learning_rate": 9.45164960182025e-05,
      "loss": 0.2859,
      "step": 2323
    },
    {
      "epoch": 0.5281818181818182,
      "grad_norm": 0.037722181528806686,
      "learning_rate": 9.447098976109216e-05,
      "loss": 0.2747,
      "step": 2324
    },
    {
      "epoch": 0.5284090909090909,
      "grad_norm": 0.03443947434425354,
      "learning_rate": 9.44254835039818e-05,
      "loss": 0.2634,
      "step": 2325
    },
    {
      "epoch": 0.5286363636363637,
      "grad_norm": 0.05600469931960106,
      "learning_rate": 9.437997724687145e-05,
      "loss": 0.3838,
      "step": 2326
    },
    {
      "epoch": 0.5288636363636363,
      "grad_norm": 0.04073145240545273,
      "learning_rate": 9.433447098976109e-05,
      "loss": 0.2839,
      "step": 2327
    },
    {
      "epoch": 0.5290909090909091,
      "grad_norm": 0.052461784332990646,
      "learning_rate": 9.428896473265074e-05,
      "loss": 0.3165,
      "step": 2328
    },
    {
      "epoch": 0.5293181818181818,
      "grad_norm": 0.053194113075733185,
      "learning_rate": 9.42434584755404e-05,
      "loss": 0.3349,
      "step": 2329
    },
    {
      "epoch": 0.5295454545454545,
      "grad_norm": 0.037940703332424164,
      "learning_rate": 9.419795221843003e-05,
      "loss": 0.2985,
      "step": 2330
    },
    {
      "epoch": 0.5297727272727273,
      "grad_norm": 0.04863239452242851,
      "learning_rate": 9.415244596131969e-05,
      "loss": 0.3675,
      "step": 2331
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.03735240176320076,
      "learning_rate": 9.410693970420933e-05,
      "loss": 0.31,
      "step": 2332
    },
    {
      "epoch": 0.5302272727272728,
      "grad_norm": 0.04468366876244545,
      "learning_rate": 9.406143344709898e-05,
      "loss": 0.3724,
      "step": 2333
    },
    {
      "epoch": 0.5304545454545454,
      "grad_norm": 0.03723405301570892,
      "learning_rate": 9.401592718998863e-05,
      "loss": 0.3071,
      "step": 2334
    },
    {
      "epoch": 0.5306818181818181,
      "grad_norm": 0.04911583662033081,
      "learning_rate": 9.397042093287827e-05,
      "loss": 0.3217,
      "step": 2335
    },
    {
      "epoch": 0.5309090909090909,
      "grad_norm": 0.04517252743244171,
      "learning_rate": 9.392491467576792e-05,
      "loss": 0.3291,
      "step": 2336
    },
    {
      "epoch": 0.5311363636363636,
      "grad_norm": 0.04001307860016823,
      "learning_rate": 9.387940841865756e-05,
      "loss": 0.3112,
      "step": 2337
    },
    {
      "epoch": 0.5313636363636364,
      "grad_norm": 0.03733208030462265,
      "learning_rate": 9.383390216154722e-05,
      "loss": 0.2791,
      "step": 2338
    },
    {
      "epoch": 0.5315909090909091,
      "grad_norm": 0.044410381466150284,
      "learning_rate": 9.378839590443687e-05,
      "loss": 0.3829,
      "step": 2339
    },
    {
      "epoch": 0.5318181818181819,
      "grad_norm": 0.03983081504702568,
      "learning_rate": 9.374288964732651e-05,
      "loss": 0.2862,
      "step": 2340
    },
    {
      "epoch": 0.5320454545454546,
      "grad_norm": 0.04698042944073677,
      "learning_rate": 9.369738339021616e-05,
      "loss": 0.3621,
      "step": 2341
    },
    {
      "epoch": 0.5322727272727272,
      "grad_norm": 0.04087714105844498,
      "learning_rate": 9.36518771331058e-05,
      "loss": 0.3501,
      "step": 2342
    },
    {
      "epoch": 0.5325,
      "grad_norm": 0.047410998493433,
      "learning_rate": 9.360637087599545e-05,
      "loss": 0.3357,
      "step": 2343
    },
    {
      "epoch": 0.5327272727272727,
      "grad_norm": 0.06449227780103683,
      "learning_rate": 9.35608646188851e-05,
      "loss": 0.4142,
      "step": 2344
    },
    {
      "epoch": 0.5329545454545455,
      "grad_norm": 0.041884370148181915,
      "learning_rate": 9.351535836177475e-05,
      "loss": 0.3531,
      "step": 2345
    },
    {
      "epoch": 0.5331818181818182,
      "grad_norm": 0.0452740378677845,
      "learning_rate": 9.346985210466438e-05,
      "loss": 0.3512,
      "step": 2346
    },
    {
      "epoch": 0.5334090909090909,
      "grad_norm": 0.03763394430279732,
      "learning_rate": 9.342434584755404e-05,
      "loss": 0.3146,
      "step": 2347
    },
    {
      "epoch": 0.5336363636363637,
      "grad_norm": 0.044841088354587555,
      "learning_rate": 9.337883959044369e-05,
      "loss": 0.2799,
      "step": 2348
    },
    {
      "epoch": 0.5338636363636363,
      "grad_norm": 0.0446464829146862,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.3315,
      "step": 2349
    },
    {
      "epoch": 0.5340909090909091,
      "grad_norm": 0.050111230462789536,
      "learning_rate": 9.328782707622298e-05,
      "loss": 0.3236,
      "step": 2350
    },
    {
      "epoch": 0.5343181818181818,
      "grad_norm": 0.05086266249418259,
      "learning_rate": 9.324232081911262e-05,
      "loss": 0.3045,
      "step": 2351
    },
    {
      "epoch": 0.5345454545454545,
      "grad_norm": 0.04425038397312164,
      "learning_rate": 9.319681456200227e-05,
      "loss": 0.2979,
      "step": 2352
    },
    {
      "epoch": 0.5347727272727273,
      "grad_norm": 0.04461679235100746,
      "learning_rate": 9.315130830489193e-05,
      "loss": 0.3325,
      "step": 2353
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.03645143285393715,
      "learning_rate": 9.310580204778158e-05,
      "loss": 0.2647,
      "step": 2354
    },
    {
      "epoch": 0.5352272727272728,
      "grad_norm": 0.044199056923389435,
      "learning_rate": 9.306029579067122e-05,
      "loss": 0.3416,
      "step": 2355
    },
    {
      "epoch": 0.5354545454545454,
      "grad_norm": 0.06121133267879486,
      "learning_rate": 9.301478953356086e-05,
      "loss": 0.3731,
      "step": 2356
    },
    {
      "epoch": 0.5356818181818181,
      "grad_norm": 0.05247044563293457,
      "learning_rate": 9.296928327645051e-05,
      "loss": 0.3689,
      "step": 2357
    },
    {
      "epoch": 0.5359090909090909,
      "grad_norm": 0.041264962404966354,
      "learning_rate": 9.292377701934016e-05,
      "loss": 0.2742,
      "step": 2358
    },
    {
      "epoch": 0.5361363636363636,
      "grad_norm": 0.040611665695905685,
      "learning_rate": 9.287827076222982e-05,
      "loss": 0.3234,
      "step": 2359
    },
    {
      "epoch": 0.5363636363636364,
      "grad_norm": 0.05249686911702156,
      "learning_rate": 9.283276450511946e-05,
      "loss": 0.3681,
      "step": 2360
    },
    {
      "epoch": 0.5365909090909091,
      "grad_norm": 0.04238241910934448,
      "learning_rate": 9.27872582480091e-05,
      "loss": 0.3304,
      "step": 2361
    },
    {
      "epoch": 0.5368181818181819,
      "grad_norm": 0.04122272878885269,
      "learning_rate": 9.274175199089875e-05,
      "loss": 0.3616,
      "step": 2362
    },
    {
      "epoch": 0.5370454545454545,
      "grad_norm": 0.20398670434951782,
      "learning_rate": 9.26962457337884e-05,
      "loss": 0.3141,
      "step": 2363
    },
    {
      "epoch": 0.5372727272727272,
      "grad_norm": 0.2835211157798767,
      "learning_rate": 9.265073947667805e-05,
      "loss": 0.3453,
      "step": 2364
    },
    {
      "epoch": 0.5375,
      "grad_norm": 0.22956915199756622,
      "learning_rate": 9.26052332195677e-05,
      "loss": 0.3038,
      "step": 2365
    },
    {
      "epoch": 0.5377272727272727,
      "grad_norm": 0.14018166065216064,
      "learning_rate": 9.255972696245733e-05,
      "loss": 0.3765,
      "step": 2366
    },
    {
      "epoch": 0.5379545454545455,
      "grad_norm": 0.09922967106103897,
      "learning_rate": 9.251422070534699e-05,
      "loss": 0.3229,
      "step": 2367
    },
    {
      "epoch": 0.5381818181818182,
      "grad_norm": 0.06463140994310379,
      "learning_rate": 9.246871444823664e-05,
      "loss": 0.3161,
      "step": 2368
    },
    {
      "epoch": 0.538409090909091,
      "grad_norm": 0.0659654438495636,
      "learning_rate": 9.242320819112629e-05,
      "loss": 0.3148,
      "step": 2369
    },
    {
      "epoch": 0.5386363636363637,
      "grad_norm": 0.06665686517953873,
      "learning_rate": 9.237770193401593e-05,
      "loss": 0.3357,
      "step": 2370
    },
    {
      "epoch": 0.5388636363636363,
      "grad_norm": 0.04022049531340599,
      "learning_rate": 9.233219567690557e-05,
      "loss": 0.3223,
      "step": 2371
    },
    {
      "epoch": 0.5390909090909091,
      "grad_norm": 0.0522187240421772,
      "learning_rate": 9.228668941979522e-05,
      "loss": 0.3561,
      "step": 2372
    },
    {
      "epoch": 0.5393181818181818,
      "grad_norm": 0.05622069165110588,
      "learning_rate": 9.224118316268488e-05,
      "loss": 0.332,
      "step": 2373
    },
    {
      "epoch": 0.5395454545454546,
      "grad_norm": 0.07096760720014572,
      "learning_rate": 9.219567690557453e-05,
      "loss": 0.3985,
      "step": 2374
    },
    {
      "epoch": 0.5397727272727273,
      "grad_norm": 0.03126692399382591,
      "learning_rate": 9.215017064846417e-05,
      "loss": 0.2718,
      "step": 2375
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.055666759610176086,
      "learning_rate": 9.21046643913538e-05,
      "loss": 0.3535,
      "step": 2376
    },
    {
      "epoch": 0.5402272727272728,
      "grad_norm": 0.06851425021886826,
      "learning_rate": 9.205915813424346e-05,
      "loss": 0.3886,
      "step": 2377
    },
    {
      "epoch": 0.5404545454545454,
      "grad_norm": 0.04645088315010071,
      "learning_rate": 9.201365187713311e-05,
      "loss": 0.3194,
      "step": 2378
    },
    {
      "epoch": 0.5406818181818182,
      "grad_norm": 0.04830773547291756,
      "learning_rate": 9.196814562002277e-05,
      "loss": 0.3541,
      "step": 2379
    },
    {
      "epoch": 0.5409090909090909,
      "grad_norm": 0.04606984555721283,
      "learning_rate": 9.19226393629124e-05,
      "loss": 0.2981,
      "step": 2380
    },
    {
      "epoch": 0.5411363636363636,
      "grad_norm": 0.05612559616565704,
      "learning_rate": 9.187713310580204e-05,
      "loss": 0.3712,
      "step": 2381
    },
    {
      "epoch": 0.5413636363636364,
      "grad_norm": 0.04754822701215744,
      "learning_rate": 9.18316268486917e-05,
      "loss": 0.3157,
      "step": 2382
    },
    {
      "epoch": 0.5415909090909091,
      "grad_norm": 0.038958072662353516,
      "learning_rate": 9.178612059158135e-05,
      "loss": 0.2726,
      "step": 2383
    },
    {
      "epoch": 0.5418181818181819,
      "grad_norm": 0.04147126153111458,
      "learning_rate": 9.1740614334471e-05,
      "loss": 0.3381,
      "step": 2384
    },
    {
      "epoch": 0.5420454545454545,
      "grad_norm": 0.04544706642627716,
      "learning_rate": 9.169510807736064e-05,
      "loss": 0.3312,
      "step": 2385
    },
    {
      "epoch": 0.5422727272727272,
      "grad_norm": 0.048192352056503296,
      "learning_rate": 9.164960182025028e-05,
      "loss": 0.3097,
      "step": 2386
    },
    {
      "epoch": 0.5425,
      "grad_norm": 0.040118224918842316,
      "learning_rate": 9.160409556313993e-05,
      "loss": 0.3445,
      "step": 2387
    },
    {
      "epoch": 0.5427272727272727,
      "grad_norm": 0.06119534373283386,
      "learning_rate": 9.155858930602959e-05,
      "loss": 0.341,
      "step": 2388
    },
    {
      "epoch": 0.5429545454545455,
      "grad_norm": 0.04920550808310509,
      "learning_rate": 9.151308304891924e-05,
      "loss": 0.3337,
      "step": 2389
    },
    {
      "epoch": 0.5431818181818182,
      "grad_norm": 0.040329039096832275,
      "learning_rate": 9.146757679180888e-05,
      "loss": 0.3266,
      "step": 2390
    },
    {
      "epoch": 0.543409090909091,
      "grad_norm": 0.035280756652355194,
      "learning_rate": 9.142207053469852e-05,
      "loss": 0.291,
      "step": 2391
    },
    {
      "epoch": 0.5436363636363636,
      "grad_norm": 0.04864901676774025,
      "learning_rate": 9.137656427758817e-05,
      "loss": 0.3994,
      "step": 2392
    },
    {
      "epoch": 0.5438636363636363,
      "grad_norm": 0.037616852670907974,
      "learning_rate": 9.133105802047782e-05,
      "loss": 0.2953,
      "step": 2393
    },
    {
      "epoch": 0.5440909090909091,
      "grad_norm": 0.0404331311583519,
      "learning_rate": 9.128555176336748e-05,
      "loss": 0.3154,
      "step": 2394
    },
    {
      "epoch": 0.5443181818181818,
      "grad_norm": 0.04494714364409447,
      "learning_rate": 9.124004550625712e-05,
      "loss": 0.3484,
      "step": 2395
    },
    {
      "epoch": 0.5445454545454546,
      "grad_norm": 0.04172704741358757,
      "learning_rate": 9.119453924914675e-05,
      "loss": 0.2968,
      "step": 2396
    },
    {
      "epoch": 0.5447727272727273,
      "grad_norm": 0.04416598007082939,
      "learning_rate": 9.114903299203641e-05,
      "loss": 0.3409,
      "step": 2397
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.05249395594000816,
      "learning_rate": 9.110352673492606e-05,
      "loss": 0.2986,
      "step": 2398
    },
    {
      "epoch": 0.5452272727272728,
      "grad_norm": 0.049386098980903625,
      "learning_rate": 9.105802047781571e-05,
      "loss": 0.3535,
      "step": 2399
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.039089031517505646,
      "learning_rate": 9.101251422070535e-05,
      "loss": 0.3435,
      "step": 2400
    },
    {
      "epoch": 0.5456818181818182,
      "grad_norm": 0.03550330549478531,
      "learning_rate": 9.096700796359499e-05,
      "loss": 0.3061,
      "step": 2401
    },
    {
      "epoch": 0.5459090909090909,
      "grad_norm": 0.03454815223813057,
      "learning_rate": 9.092150170648464e-05,
      "loss": 0.2771,
      "step": 2402
    },
    {
      "epoch": 0.5461363636363636,
      "grad_norm": 0.04756045714020729,
      "learning_rate": 9.08759954493743e-05,
      "loss": 0.3714,
      "step": 2403
    },
    {
      "epoch": 0.5463636363636364,
      "grad_norm": 0.04310760647058487,
      "learning_rate": 9.083048919226395e-05,
      "loss": 0.319,
      "step": 2404
    },
    {
      "epoch": 0.5465909090909091,
      "grad_norm": 0.03872068598866463,
      "learning_rate": 9.078498293515359e-05,
      "loss": 0.307,
      "step": 2405
    },
    {
      "epoch": 0.5468181818181819,
      "grad_norm": 0.04763583466410637,
      "learning_rate": 9.073947667804323e-05,
      "loss": 0.3636,
      "step": 2406
    },
    {
      "epoch": 0.5470454545454545,
      "grad_norm": 0.04918039217591286,
      "learning_rate": 9.069397042093288e-05,
      "loss": 0.3765,
      "step": 2407
    },
    {
      "epoch": 0.5472727272727272,
      "grad_norm": 0.04194159433245659,
      "learning_rate": 9.064846416382253e-05,
      "loss": 0.3191,
      "step": 2408
    },
    {
      "epoch": 0.5475,
      "grad_norm": 0.05095461755990982,
      "learning_rate": 9.060295790671219e-05,
      "loss": 0.3771,
      "step": 2409
    },
    {
      "epoch": 0.5477272727272727,
      "grad_norm": 0.04563412070274353,
      "learning_rate": 9.055745164960183e-05,
      "loss": 0.3223,
      "step": 2410
    },
    {
      "epoch": 0.5479545454545455,
      "grad_norm": 0.05367506667971611,
      "learning_rate": 9.051194539249147e-05,
      "loss": 0.2775,
      "step": 2411
    },
    {
      "epoch": 0.5481818181818182,
      "grad_norm": 0.04424304887652397,
      "learning_rate": 9.046643913538112e-05,
      "loss": 0.3453,
      "step": 2412
    },
    {
      "epoch": 0.548409090909091,
      "grad_norm": 0.04696132615208626,
      "learning_rate": 9.042093287827077e-05,
      "loss": 0.3042,
      "step": 2413
    },
    {
      "epoch": 0.5486363636363636,
      "grad_norm": 0.03856992349028587,
      "learning_rate": 9.037542662116041e-05,
      "loss": 0.3159,
      "step": 2414
    },
    {
      "epoch": 0.5488636363636363,
      "grad_norm": 0.04333637282252312,
      "learning_rate": 9.032992036405006e-05,
      "loss": 0.3683,
      "step": 2415
    },
    {
      "epoch": 0.5490909090909091,
      "grad_norm": 0.044052854180336,
      "learning_rate": 9.02844141069397e-05,
      "loss": 0.3226,
      "step": 2416
    },
    {
      "epoch": 0.5493181818181818,
      "grad_norm": 0.05803820490837097,
      "learning_rate": 9.023890784982936e-05,
      "loss": 0.3762,
      "step": 2417
    },
    {
      "epoch": 0.5495454545454546,
      "grad_norm": 0.03572515770792961,
      "learning_rate": 9.019340159271901e-05,
      "loss": 0.3134,
      "step": 2418
    },
    {
      "epoch": 0.5497727272727273,
      "grad_norm": 0.049780137836933136,
      "learning_rate": 9.014789533560865e-05,
      "loss": 0.4234,
      "step": 2419
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.03738061711192131,
      "learning_rate": 9.01023890784983e-05,
      "loss": 0.314,
      "step": 2420
    },
    {
      "epoch": 0.5502272727272727,
      "grad_norm": 0.04457889497280121,
      "learning_rate": 9.005688282138794e-05,
      "loss": 0.2789,
      "step": 2421
    },
    {
      "epoch": 0.5504545454545454,
      "grad_norm": 0.04624832049012184,
      "learning_rate": 9.001137656427759e-05,
      "loss": 0.3468,
      "step": 2422
    },
    {
      "epoch": 0.5506818181818182,
      "grad_norm": 0.06080752611160278,
      "learning_rate": 8.996587030716725e-05,
      "loss": 0.3254,
      "step": 2423
    },
    {
      "epoch": 0.5509090909090909,
      "grad_norm": 0.04821435362100601,
      "learning_rate": 8.992036405005688e-05,
      "loss": 0.219,
      "step": 2424
    },
    {
      "epoch": 0.5511363636363636,
      "grad_norm": 0.0422913134098053,
      "learning_rate": 8.987485779294654e-05,
      "loss": 0.302,
      "step": 2425
    },
    {
      "epoch": 0.5513636363636364,
      "grad_norm": 0.04082252085208893,
      "learning_rate": 8.982935153583618e-05,
      "loss": 0.3167,
      "step": 2426
    },
    {
      "epoch": 0.5515909090909091,
      "grad_norm": 0.034905076026916504,
      "learning_rate": 8.978384527872583e-05,
      "loss": 0.2224,
      "step": 2427
    },
    {
      "epoch": 0.5518181818181818,
      "grad_norm": 0.048698827624320984,
      "learning_rate": 8.973833902161548e-05,
      "loss": 0.3271,
      "step": 2428
    },
    {
      "epoch": 0.5520454545454545,
      "grad_norm": 0.06574980914592743,
      "learning_rate": 8.969283276450512e-05,
      "loss": 0.392,
      "step": 2429
    },
    {
      "epoch": 0.5522727272727272,
      "grad_norm": 0.045612797141075134,
      "learning_rate": 8.964732650739477e-05,
      "loss": 0.3395,
      "step": 2430
    },
    {
      "epoch": 0.5525,
      "grad_norm": 0.0390956811606884,
      "learning_rate": 8.960182025028441e-05,
      "loss": 0.3562,
      "step": 2431
    },
    {
      "epoch": 0.5527272727272727,
      "grad_norm": 0.04003079980611801,
      "learning_rate": 8.955631399317407e-05,
      "loss": 0.2938,
      "step": 2432
    },
    {
      "epoch": 0.5529545454545455,
      "grad_norm": 0.05047561600804329,
      "learning_rate": 8.951080773606372e-05,
      "loss": 0.3597,
      "step": 2433
    },
    {
      "epoch": 0.5531818181818182,
      "grad_norm": 0.037012048065662384,
      "learning_rate": 8.946530147895336e-05,
      "loss": 0.2641,
      "step": 2434
    },
    {
      "epoch": 0.553409090909091,
      "grad_norm": 0.04293552413582802,
      "learning_rate": 8.941979522184301e-05,
      "loss": 0.3383,
      "step": 2435
    },
    {
      "epoch": 0.5536363636363636,
      "grad_norm": 0.054086048156023026,
      "learning_rate": 8.937428896473265e-05,
      "loss": 0.3531,
      "step": 2436
    },
    {
      "epoch": 0.5538636363636363,
      "grad_norm": 0.04540390893816948,
      "learning_rate": 8.93287827076223e-05,
      "loss": 0.3579,
      "step": 2437
    },
    {
      "epoch": 0.5540909090909091,
      "grad_norm": 0.052327245473861694,
      "learning_rate": 8.928327645051196e-05,
      "loss": 0.3279,
      "step": 2438
    },
    {
      "epoch": 0.5543181818181818,
      "grad_norm": 0.042150478810071945,
      "learning_rate": 8.92377701934016e-05,
      "loss": 0.3216,
      "step": 2439
    },
    {
      "epoch": 0.5545454545454546,
      "grad_norm": 0.047993406653404236,
      "learning_rate": 8.919226393629125e-05,
      "loss": 0.3202,
      "step": 2440
    },
    {
      "epoch": 0.5547727272727273,
      "grad_norm": 0.04159804806113243,
      "learning_rate": 8.914675767918089e-05,
      "loss": 0.3256,
      "step": 2441
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.03895799070596695,
      "learning_rate": 8.910125142207054e-05,
      "loss": 0.2699,
      "step": 2442
    },
    {
      "epoch": 0.5552272727272727,
      "grad_norm": 0.04824887588620186,
      "learning_rate": 8.90557451649602e-05,
      "loss": 0.36,
      "step": 2443
    },
    {
      "epoch": 0.5554545454545454,
      "grad_norm": 0.035987723618745804,
      "learning_rate": 8.901023890784983e-05,
      "loss": 0.2912,
      "step": 2444
    },
    {
      "epoch": 0.5556818181818182,
      "grad_norm": 0.038909364491701126,
      "learning_rate": 8.896473265073949e-05,
      "loss": 0.2749,
      "step": 2445
    },
    {
      "epoch": 0.5559090909090909,
      "grad_norm": 0.03924454376101494,
      "learning_rate": 8.891922639362912e-05,
      "loss": 0.2902,
      "step": 2446
    },
    {
      "epoch": 0.5561363636363637,
      "grad_norm": 0.039769794791936874,
      "learning_rate": 8.887372013651878e-05,
      "loss": 0.3023,
      "step": 2447
    },
    {
      "epoch": 0.5563636363636364,
      "grad_norm": 0.052514512091875076,
      "learning_rate": 8.882821387940843e-05,
      "loss": 0.3499,
      "step": 2448
    },
    {
      "epoch": 0.5565909090909091,
      "grad_norm": 0.0424124076962471,
      "learning_rate": 8.878270762229807e-05,
      "loss": 0.3004,
      "step": 2449
    },
    {
      "epoch": 0.5568181818181818,
      "grad_norm": 0.05079163238406181,
      "learning_rate": 8.873720136518772e-05,
      "loss": 0.3769,
      "step": 2450
    },
    {
      "epoch": 0.5570454545454545,
      "grad_norm": 0.050629984587430954,
      "learning_rate": 8.869169510807736e-05,
      "loss": 0.38,
      "step": 2451
    },
    {
      "epoch": 0.5572727272727273,
      "grad_norm": 0.055495258420705795,
      "learning_rate": 8.864618885096701e-05,
      "loss": 0.3803,
      "step": 2452
    },
    {
      "epoch": 0.5575,
      "grad_norm": 0.049492765218019485,
      "learning_rate": 8.860068259385665e-05,
      "loss": 0.3647,
      "step": 2453
    },
    {
      "epoch": 0.5577272727272727,
      "grad_norm": 0.05351686105132103,
      "learning_rate": 8.85551763367463e-05,
      "loss": 0.3881,
      "step": 2454
    },
    {
      "epoch": 0.5579545454545455,
      "grad_norm": 0.051035039126873016,
      "learning_rate": 8.850967007963596e-05,
      "loss": 0.3633,
      "step": 2455
    },
    {
      "epoch": 0.5581818181818182,
      "grad_norm": 0.045156240463256836,
      "learning_rate": 8.84641638225256e-05,
      "loss": 0.3655,
      "step": 2456
    },
    {
      "epoch": 0.5584090909090909,
      "grad_norm": 0.03756885230541229,
      "learning_rate": 8.841865756541525e-05,
      "loss": 0.2983,
      "step": 2457
    },
    {
      "epoch": 0.5586363636363636,
      "grad_norm": 0.04506169632077217,
      "learning_rate": 8.837315130830489e-05,
      "loss": 0.3531,
      "step": 2458
    },
    {
      "epoch": 0.5588636363636363,
      "grad_norm": 0.05013304203748703,
      "learning_rate": 8.832764505119454e-05,
      "loss": 0.3284,
      "step": 2459
    },
    {
      "epoch": 0.5590909090909091,
      "grad_norm": 0.04448818787932396,
      "learning_rate": 8.82821387940842e-05,
      "loss": 0.308,
      "step": 2460
    },
    {
      "epoch": 0.5593181818181818,
      "grad_norm": 0.03258204087615013,
      "learning_rate": 8.823663253697384e-05,
      "loss": 0.2636,
      "step": 2461
    },
    {
      "epoch": 0.5595454545454546,
      "grad_norm": 0.039205409586429596,
      "learning_rate": 8.819112627986349e-05,
      "loss": 0.2895,
      "step": 2462
    },
    {
      "epoch": 0.5597727272727273,
      "grad_norm": 0.05321814492344856,
      "learning_rate": 8.814562002275313e-05,
      "loss": 0.4064,
      "step": 2463
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.055225882679224014,
      "learning_rate": 8.810011376564278e-05,
      "loss": 0.3472,
      "step": 2464
    },
    {
      "epoch": 0.5602272727272727,
      "grad_norm": 0.040731169283390045,
      "learning_rate": 8.805460750853243e-05,
      "loss": 0.3172,
      "step": 2465
    },
    {
      "epoch": 0.5604545454545454,
      "grad_norm": 0.030828574672341347,
      "learning_rate": 8.800910125142207e-05,
      "loss": 0.2388,
      "step": 2466
    },
    {
      "epoch": 0.5606818181818182,
      "grad_norm": 0.04251570254564285,
      "learning_rate": 8.796359499431173e-05,
      "loss": 0.3345,
      "step": 2467
    },
    {
      "epoch": 0.5609090909090909,
      "grad_norm": 0.043096303939819336,
      "learning_rate": 8.791808873720136e-05,
      "loss": 0.3221,
      "step": 2468
    },
    {
      "epoch": 0.5611363636363637,
      "grad_norm": 0.04119977727532387,
      "learning_rate": 8.787258248009102e-05,
      "loss": 0.3213,
      "step": 2469
    },
    {
      "epoch": 0.5613636363636364,
      "grad_norm": 0.03635723888874054,
      "learning_rate": 8.782707622298067e-05,
      "loss": 0.2671,
      "step": 2470
    },
    {
      "epoch": 0.5615909090909091,
      "grad_norm": 0.05743768811225891,
      "learning_rate": 8.778156996587031e-05,
      "loss": 0.4242,
      "step": 2471
    },
    {
      "epoch": 0.5618181818181818,
      "grad_norm": 0.04527974873781204,
      "learning_rate": 8.773606370875996e-05,
      "loss": 0.3203,
      "step": 2472
    },
    {
      "epoch": 0.5620454545454545,
      "grad_norm": 0.03496996685862541,
      "learning_rate": 8.76905574516496e-05,
      "loss": 0.2643,
      "step": 2473
    },
    {
      "epoch": 0.5622727272727273,
      "grad_norm": 0.04919397085905075,
      "learning_rate": 8.764505119453925e-05,
      "loss": 0.359,
      "step": 2474
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.0450814813375473,
      "learning_rate": 8.759954493742891e-05,
      "loss": 0.3556,
      "step": 2475
    },
    {
      "epoch": 0.5627272727272727,
      "grad_norm": 0.03035290166735649,
      "learning_rate": 8.755403868031855e-05,
      "loss": 0.2268,
      "step": 2476
    },
    {
      "epoch": 0.5629545454545455,
      "grad_norm": 0.04116421565413475,
      "learning_rate": 8.75085324232082e-05,
      "loss": 0.3055,
      "step": 2477
    },
    {
      "epoch": 0.5631818181818182,
      "grad_norm": 0.05097690969705582,
      "learning_rate": 8.746302616609784e-05,
      "loss": 0.333,
      "step": 2478
    },
    {
      "epoch": 0.5634090909090909,
      "grad_norm": 0.04734857380390167,
      "learning_rate": 8.741751990898749e-05,
      "loss": 0.3696,
      "step": 2479
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.043905939906835556,
      "learning_rate": 8.737201365187714e-05,
      "loss": 0.3185,
      "step": 2480
    },
    {
      "epoch": 0.5638636363636363,
      "grad_norm": 0.05184626206755638,
      "learning_rate": 8.732650739476678e-05,
      "loss": 0.3066,
      "step": 2481
    },
    {
      "epoch": 0.5640909090909091,
      "grad_norm": 0.051196664571762085,
      "learning_rate": 8.728100113765644e-05,
      "loss": 0.3732,
      "step": 2482
    },
    {
      "epoch": 0.5643181818181818,
      "grad_norm": 0.043604232370853424,
      "learning_rate": 8.723549488054608e-05,
      "loss": 0.3123,
      "step": 2483
    },
    {
      "epoch": 0.5645454545454546,
      "grad_norm": 0.03812866657972336,
      "learning_rate": 8.718998862343573e-05,
      "loss": 0.3117,
      "step": 2484
    },
    {
      "epoch": 0.5647727272727273,
      "grad_norm": 0.045499563217163086,
      "learning_rate": 8.714448236632538e-05,
      "loss": 0.3797,
      "step": 2485
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.043802134692668915,
      "learning_rate": 8.709897610921502e-05,
      "loss": 0.3177,
      "step": 2486
    },
    {
      "epoch": 0.5652272727272727,
      "grad_norm": 0.0448325090110302,
      "learning_rate": 8.705346985210466e-05,
      "loss": 0.338,
      "step": 2487
    },
    {
      "epoch": 0.5654545454545454,
      "grad_norm": 0.05854802578687668,
      "learning_rate": 8.700796359499431e-05,
      "loss": 0.4186,
      "step": 2488
    },
    {
      "epoch": 0.5656818181818182,
      "grad_norm": 0.03979148343205452,
      "learning_rate": 8.696245733788397e-05,
      "loss": 0.3156,
      "step": 2489
    },
    {
      "epoch": 0.5659090909090909,
      "grad_norm": 0.041579268872737885,
      "learning_rate": 8.691695108077362e-05,
      "loss": 0.3335,
      "step": 2490
    },
    {
      "epoch": 0.5661363636363637,
      "grad_norm": 0.03636973351240158,
      "learning_rate": 8.687144482366326e-05,
      "loss": 0.3224,
      "step": 2491
    },
    {
      "epoch": 0.5663636363636364,
      "grad_norm": 0.043180808424949646,
      "learning_rate": 8.68259385665529e-05,
      "loss": 0.3412,
      "step": 2492
    },
    {
      "epoch": 0.5665909090909091,
      "grad_norm": 0.04060819745063782,
      "learning_rate": 8.678043230944255e-05,
      "loss": 0.2681,
      "step": 2493
    },
    {
      "epoch": 0.5668181818181818,
      "grad_norm": 0.0491841658949852,
      "learning_rate": 8.67349260523322e-05,
      "loss": 0.3844,
      "step": 2494
    },
    {
      "epoch": 0.5670454545454545,
      "grad_norm": 0.055144939571619034,
      "learning_rate": 8.668941979522186e-05,
      "loss": 0.3648,
      "step": 2495
    },
    {
      "epoch": 0.5672727272727273,
      "grad_norm": 0.04465050622820854,
      "learning_rate": 8.66439135381115e-05,
      "loss": 0.3654,
      "step": 2496
    },
    {
      "epoch": 0.5675,
      "grad_norm": 0.04132450371980667,
      "learning_rate": 8.659840728100113e-05,
      "loss": 0.2434,
      "step": 2497
    },
    {
      "epoch": 0.5677272727272727,
      "grad_norm": 0.05575209856033325,
      "learning_rate": 8.655290102389079e-05,
      "loss": 0.3847,
      "step": 2498
    },
    {
      "epoch": 0.5679545454545455,
      "grad_norm": 0.043405357748270035,
      "learning_rate": 8.650739476678044e-05,
      "loss": 0.3623,
      "step": 2499
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.04170853644609451,
      "learning_rate": 8.646188850967009e-05,
      "loss": 0.3525,
      "step": 2500
    },
    {
      "epoch": 0.5684090909090909,
      "grad_norm": 0.057258620858192444,
      "learning_rate": 8.641638225255973e-05,
      "loss": 0.3633,
      "step": 2501
    },
    {
      "epoch": 0.5686363636363636,
      "grad_norm": 0.043321289122104645,
      "learning_rate": 8.637087599544937e-05,
      "loss": 0.3165,
      "step": 2502
    },
    {
      "epoch": 0.5688636363636363,
      "grad_norm": 0.09568918496370316,
      "learning_rate": 8.632536973833902e-05,
      "loss": 0.3888,
      "step": 2503
    },
    {
      "epoch": 0.5690909090909091,
      "grad_norm": 0.04522230848670006,
      "learning_rate": 8.627986348122868e-05,
      "loss": 0.3124,
      "step": 2504
    },
    {
      "epoch": 0.5693181818181818,
      "grad_norm": 0.05088042840361595,
      "learning_rate": 8.623435722411833e-05,
      "loss": 0.37,
      "step": 2505
    },
    {
      "epoch": 0.5695454545454546,
      "grad_norm": 0.04790797457098961,
      "learning_rate": 8.618885096700797e-05,
      "loss": 0.3645,
      "step": 2506
    },
    {
      "epoch": 0.5697727272727273,
      "grad_norm": 0.039726193994283676,
      "learning_rate": 8.614334470989761e-05,
      "loss": 0.3166,
      "step": 2507
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.044649992138147354,
      "learning_rate": 8.609783845278726e-05,
      "loss": 0.3036,
      "step": 2508
    },
    {
      "epoch": 0.5702272727272727,
      "grad_norm": 0.041151970624923706,
      "learning_rate": 8.605233219567691e-05,
      "loss": 0.2889,
      "step": 2509
    },
    {
      "epoch": 0.5704545454545454,
      "grad_norm": 0.043286703526973724,
      "learning_rate": 8.600682593856657e-05,
      "loss": 0.3272,
      "step": 2510
    },
    {
      "epoch": 0.5706818181818182,
      "grad_norm": 0.040543168783187866,
      "learning_rate": 8.59613196814562e-05,
      "loss": 0.3017,
      "step": 2511
    },
    {
      "epoch": 0.5709090909090909,
      "grad_norm": 0.048166729509830475,
      "learning_rate": 8.591581342434584e-05,
      "loss": 0.3504,
      "step": 2512
    },
    {
      "epoch": 0.5711363636363637,
      "grad_norm": 0.0387699119746685,
      "learning_rate": 8.58703071672355e-05,
      "loss": 0.2407,
      "step": 2513
    },
    {
      "epoch": 0.5713636363636364,
      "grad_norm": 0.058612074702978134,
      "learning_rate": 8.582480091012515e-05,
      "loss": 0.382,
      "step": 2514
    },
    {
      "epoch": 0.571590909090909,
      "grad_norm": 0.046435266733169556,
      "learning_rate": 8.57792946530148e-05,
      "loss": 0.362,
      "step": 2515
    },
    {
      "epoch": 0.5718181818181818,
      "grad_norm": 0.04208442568778992,
      "learning_rate": 8.573378839590444e-05,
      "loss": 0.3202,
      "step": 2516
    },
    {
      "epoch": 0.5720454545454545,
      "grad_norm": 0.04269922897219658,
      "learning_rate": 8.568828213879408e-05,
      "loss": 0.3579,
      "step": 2517
    },
    {
      "epoch": 0.5722727272727273,
      "grad_norm": 0.05197906121611595,
      "learning_rate": 8.564277588168373e-05,
      "loss": 0.3657,
      "step": 2518
    },
    {
      "epoch": 0.5725,
      "grad_norm": 0.058257196098566055,
      "learning_rate": 8.559726962457339e-05,
      "loss": 0.3264,
      "step": 2519
    },
    {
      "epoch": 0.5727272727272728,
      "grad_norm": 0.050837934017181396,
      "learning_rate": 8.555176336746304e-05,
      "loss": 0.3331,
      "step": 2520
    },
    {
      "epoch": 0.5729545454545455,
      "grad_norm": 0.05502363666892052,
      "learning_rate": 8.550625711035267e-05,
      "loss": 0.3728,
      "step": 2521
    },
    {
      "epoch": 0.5731818181818182,
      "grad_norm": 0.038898713886737823,
      "learning_rate": 8.546075085324232e-05,
      "loss": 0.3237,
      "step": 2522
    },
    {
      "epoch": 0.5734090909090909,
      "grad_norm": 0.039281874895095825,
      "learning_rate": 8.541524459613197e-05,
      "loss": 0.3226,
      "step": 2523
    },
    {
      "epoch": 0.5736363636363636,
      "grad_norm": 0.039815161377191544,
      "learning_rate": 8.536973833902162e-05,
      "loss": 0.3332,
      "step": 2524
    },
    {
      "epoch": 0.5738636363636364,
      "grad_norm": 0.04446668177843094,
      "learning_rate": 8.532423208191128e-05,
      "loss": 0.3085,
      "step": 2525
    },
    {
      "epoch": 0.5740909090909091,
      "grad_norm": 0.03757189214229584,
      "learning_rate": 8.52787258248009e-05,
      "loss": 0.3179,
      "step": 2526
    },
    {
      "epoch": 0.5743181818181818,
      "grad_norm": 0.0451384112238884,
      "learning_rate": 8.523321956769056e-05,
      "loss": 0.3149,
      "step": 2527
    },
    {
      "epoch": 0.5745454545454546,
      "grad_norm": 0.0629161074757576,
      "learning_rate": 8.518771331058021e-05,
      "loss": 0.3781,
      "step": 2528
    },
    {
      "epoch": 0.5747727272727273,
      "grad_norm": 0.033429913222789764,
      "learning_rate": 8.514220705346986e-05,
      "loss": 0.2821,
      "step": 2529
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.03986752778291702,
      "learning_rate": 8.509670079635951e-05,
      "loss": 0.3421,
      "step": 2530
    },
    {
      "epoch": 0.5752272727272727,
      "grad_norm": 0.04969961568713188,
      "learning_rate": 8.505119453924914e-05,
      "loss": 0.3674,
      "step": 2531
    },
    {
      "epoch": 0.5754545454545454,
      "grad_norm": 0.041148003190755844,
      "learning_rate": 8.500568828213879e-05,
      "loss": 0.2939,
      "step": 2532
    },
    {
      "epoch": 0.5756818181818182,
      "grad_norm": 0.04064299538731575,
      "learning_rate": 8.496018202502845e-05,
      "loss": 0.3151,
      "step": 2533
    },
    {
      "epoch": 0.5759090909090909,
      "grad_norm": 0.04123539477586746,
      "learning_rate": 8.49146757679181e-05,
      "loss": 0.3201,
      "step": 2534
    },
    {
      "epoch": 0.5761363636363637,
      "grad_norm": 0.05097935348749161,
      "learning_rate": 8.486916951080775e-05,
      "loss": 0.401,
      "step": 2535
    },
    {
      "epoch": 0.5763636363636364,
      "grad_norm": 0.044248420745134354,
      "learning_rate": 8.482366325369738e-05,
      "loss": 0.3154,
      "step": 2536
    },
    {
      "epoch": 0.576590909090909,
      "grad_norm": 0.037531524896621704,
      "learning_rate": 8.477815699658703e-05,
      "loss": 0.3028,
      "step": 2537
    },
    {
      "epoch": 0.5768181818181818,
      "grad_norm": 0.04013829678297043,
      "learning_rate": 8.473265073947668e-05,
      "loss": 0.3097,
      "step": 2538
    },
    {
      "epoch": 0.5770454545454545,
      "grad_norm": 0.04952767863869667,
      "learning_rate": 8.468714448236634e-05,
      "loss": 0.3552,
      "step": 2539
    },
    {
      "epoch": 0.5772727272727273,
      "grad_norm": 0.036704402416944504,
      "learning_rate": 8.464163822525599e-05,
      "loss": 0.2895,
      "step": 2540
    },
    {
      "epoch": 0.5775,
      "grad_norm": 0.04143761470913887,
      "learning_rate": 8.459613196814561e-05,
      "loss": 0.3576,
      "step": 2541
    },
    {
      "epoch": 0.5777272727272728,
      "grad_norm": 0.040117304772138596,
      "learning_rate": 8.455062571103527e-05,
      "loss": 0.2491,
      "step": 2542
    },
    {
      "epoch": 0.5779545454545455,
      "grad_norm": 0.03794423118233681,
      "learning_rate": 8.450511945392492e-05,
      "loss": 0.3083,
      "step": 2543
    },
    {
      "epoch": 0.5781818181818181,
      "grad_norm": 0.059864431619644165,
      "learning_rate": 8.445961319681457e-05,
      "loss": 0.3982,
      "step": 2544
    },
    {
      "epoch": 0.5784090909090909,
      "grad_norm": 0.036635227501392365,
      "learning_rate": 8.441410693970421e-05,
      "loss": 0.2873,
      "step": 2545
    },
    {
      "epoch": 0.5786363636363636,
      "grad_norm": 0.04355365037918091,
      "learning_rate": 8.436860068259385e-05,
      "loss": 0.3255,
      "step": 2546
    },
    {
      "epoch": 0.5788636363636364,
      "grad_norm": 0.043303243815898895,
      "learning_rate": 8.43230944254835e-05,
      "loss": 0.292,
      "step": 2547
    },
    {
      "epoch": 0.5790909090909091,
      "grad_norm": 0.045498158782720566,
      "learning_rate": 8.427758816837316e-05,
      "loss": 0.3197,
      "step": 2548
    },
    {
      "epoch": 0.5793181818181818,
      "grad_norm": 0.04650585353374481,
      "learning_rate": 8.423208191126281e-05,
      "loss": 0.2979,
      "step": 2549
    },
    {
      "epoch": 0.5795454545454546,
      "grad_norm": 0.03921448811888695,
      "learning_rate": 8.418657565415245e-05,
      "loss": 0.3253,
      "step": 2550
    },
    {
      "epoch": 0.5797727272727272,
      "grad_norm": 0.0483570396900177,
      "learning_rate": 8.414106939704209e-05,
      "loss": 0.3104,
      "step": 2551
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.04855151101946831,
      "learning_rate": 8.409556313993174e-05,
      "loss": 0.3254,
      "step": 2552
    },
    {
      "epoch": 0.5802272727272727,
      "grad_norm": 0.03799441456794739,
      "learning_rate": 8.40500568828214e-05,
      "loss": 0.2935,
      "step": 2553
    },
    {
      "epoch": 0.5804545454545454,
      "grad_norm": 0.04383976384997368,
      "learning_rate": 8.400455062571105e-05,
      "loss": 0.33,
      "step": 2554
    },
    {
      "epoch": 0.5806818181818182,
      "grad_norm": 0.042530328035354614,
      "learning_rate": 8.395904436860069e-05,
      "loss": 0.3657,
      "step": 2555
    },
    {
      "epoch": 0.5809090909090909,
      "grad_norm": 0.036989953368902206,
      "learning_rate": 8.391353811149032e-05,
      "loss": 0.2674,
      "step": 2556
    },
    {
      "epoch": 0.5811363636363637,
      "grad_norm": 0.05987214297056198,
      "learning_rate": 8.386803185437998e-05,
      "loss": 0.3945,
      "step": 2557
    },
    {
      "epoch": 0.5813636363636364,
      "grad_norm": 0.03806024417281151,
      "learning_rate": 8.382252559726963e-05,
      "loss": 0.2643,
      "step": 2558
    },
    {
      "epoch": 0.581590909090909,
      "grad_norm": 0.03769386187195778,
      "learning_rate": 8.377701934015928e-05,
      "loss": 0.319,
      "step": 2559
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.06820598989725113,
      "learning_rate": 8.373151308304892e-05,
      "loss": 0.3661,
      "step": 2560
    },
    {
      "epoch": 0.5820454545454545,
      "grad_norm": 0.040733423084020615,
      "learning_rate": 8.368600682593856e-05,
      "loss": 0.3341,
      "step": 2561
    },
    {
      "epoch": 0.5822727272727273,
      "grad_norm": 0.03234471380710602,
      "learning_rate": 8.364050056882821e-05,
      "loss": 0.247,
      "step": 2562
    },
    {
      "epoch": 0.5825,
      "grad_norm": 0.04014536738395691,
      "learning_rate": 8.359499431171787e-05,
      "loss": 0.3155,
      "step": 2563
    },
    {
      "epoch": 0.5827272727272728,
      "grad_norm": 0.04063049331307411,
      "learning_rate": 8.354948805460752e-05,
      "loss": 0.2523,
      "step": 2564
    },
    {
      "epoch": 0.5829545454545455,
      "grad_norm": 0.044616490602493286,
      "learning_rate": 8.350398179749716e-05,
      "loss": 0.3428,
      "step": 2565
    },
    {
      "epoch": 0.5831818181818181,
      "grad_norm": 0.04895507171750069,
      "learning_rate": 8.34584755403868e-05,
      "loss": 0.3665,
      "step": 2566
    },
    {
      "epoch": 0.5834090909090909,
      "grad_norm": 0.04236582666635513,
      "learning_rate": 8.341296928327645e-05,
      "loss": 0.2654,
      "step": 2567
    },
    {
      "epoch": 0.5836363636363636,
      "grad_norm": 0.048884317278862,
      "learning_rate": 8.33674630261661e-05,
      "loss": 0.3571,
      "step": 2568
    },
    {
      "epoch": 0.5838636363636364,
      "grad_norm": 0.03579084202647209,
      "learning_rate": 8.332195676905576e-05,
      "loss": 0.2849,
      "step": 2569
    },
    {
      "epoch": 0.5840909090909091,
      "grad_norm": 0.08381199091672897,
      "learning_rate": 8.32764505119454e-05,
      "loss": 0.3467,
      "step": 2570
    },
    {
      "epoch": 0.5843181818181818,
      "grad_norm": 0.04760388657450676,
      "learning_rate": 8.323094425483504e-05,
      "loss": 0.3206,
      "step": 2571
    },
    {
      "epoch": 0.5845454545454546,
      "grad_norm": 0.03914305940270424,
      "learning_rate": 8.318543799772469e-05,
      "loss": 0.2958,
      "step": 2572
    },
    {
      "epoch": 0.5847727272727272,
      "grad_norm": 0.03382854163646698,
      "learning_rate": 8.313993174061434e-05,
      "loss": 0.3205,
      "step": 2573
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.036386631429195404,
      "learning_rate": 8.3094425483504e-05,
      "loss": 0.2576,
      "step": 2574
    },
    {
      "epoch": 0.5852272727272727,
      "grad_norm": 0.047733623534440994,
      "learning_rate": 8.304891922639363e-05,
      "loss": 0.3326,
      "step": 2575
    },
    {
      "epoch": 0.5854545454545454,
      "grad_norm": 0.048304177820682526,
      "learning_rate": 8.300341296928327e-05,
      "loss": 0.3028,
      "step": 2576
    },
    {
      "epoch": 0.5856818181818182,
      "grad_norm": 0.041141390800476074,
      "learning_rate": 8.295790671217293e-05,
      "loss": 0.3049,
      "step": 2577
    },
    {
      "epoch": 0.5859090909090909,
      "grad_norm": 0.036371078342199326,
      "learning_rate": 8.291240045506258e-05,
      "loss": 0.2955,
      "step": 2578
    },
    {
      "epoch": 0.5861363636363637,
      "grad_norm": 0.0499536506831646,
      "learning_rate": 8.286689419795223e-05,
      "loss": 0.331,
      "step": 2579
    },
    {
      "epoch": 0.5863636363636363,
      "grad_norm": 0.03658992424607277,
      "learning_rate": 8.282138794084187e-05,
      "loss": 0.2927,
      "step": 2580
    },
    {
      "epoch": 0.586590909090909,
      "grad_norm": 0.054146334528923035,
      "learning_rate": 8.277588168373151e-05,
      "loss": 0.3344,
      "step": 2581
    },
    {
      "epoch": 0.5868181818181818,
      "grad_norm": 0.0473199188709259,
      "learning_rate": 8.273037542662116e-05,
      "loss": 0.3141,
      "step": 2582
    },
    {
      "epoch": 0.5870454545454545,
      "grad_norm": 0.04583476111292839,
      "learning_rate": 8.268486916951082e-05,
      "loss": 0.3187,
      "step": 2583
    },
    {
      "epoch": 0.5872727272727273,
      "grad_norm": 0.05571365728974342,
      "learning_rate": 8.263936291240047e-05,
      "loss": 0.3877,
      "step": 2584
    },
    {
      "epoch": 0.5875,
      "grad_norm": 0.0404680036008358,
      "learning_rate": 8.259385665529011e-05,
      "loss": 0.359,
      "step": 2585
    },
    {
      "epoch": 0.5877272727272728,
      "grad_norm": 0.04788783937692642,
      "learning_rate": 8.254835039817975e-05,
      "loss": 0.3385,
      "step": 2586
    },
    {
      "epoch": 0.5879545454545455,
      "grad_norm": 0.04112514480948448,
      "learning_rate": 8.25028441410694e-05,
      "loss": 0.2957,
      "step": 2587
    },
    {
      "epoch": 0.5881818181818181,
      "grad_norm": 0.04740780591964722,
      "learning_rate": 8.245733788395905e-05,
      "loss": 0.363,
      "step": 2588
    },
    {
      "epoch": 0.5884090909090909,
      "grad_norm": 0.040006574243307114,
      "learning_rate": 8.24118316268487e-05,
      "loss": 0.3244,
      "step": 2589
    },
    {
      "epoch": 0.5886363636363636,
      "grad_norm": 0.034271225333213806,
      "learning_rate": 8.236632536973834e-05,
      "loss": 0.303,
      "step": 2590
    },
    {
      "epoch": 0.5888636363636364,
      "grad_norm": 0.04189480096101761,
      "learning_rate": 8.232081911262798e-05,
      "loss": 0.3014,
      "step": 2591
    },
    {
      "epoch": 0.5890909090909091,
      "grad_norm": 0.05436770245432854,
      "learning_rate": 8.227531285551764e-05,
      "loss": 0.4147,
      "step": 2592
    },
    {
      "epoch": 0.5893181818181819,
      "grad_norm": 0.04895022511482239,
      "learning_rate": 8.222980659840729e-05,
      "loss": 0.3396,
      "step": 2593
    },
    {
      "epoch": 0.5895454545454546,
      "grad_norm": 0.04726364463567734,
      "learning_rate": 8.218430034129693e-05,
      "loss": 0.3647,
      "step": 2594
    },
    {
      "epoch": 0.5897727272727272,
      "grad_norm": 0.055632688105106354,
      "learning_rate": 8.213879408418658e-05,
      "loss": 0.3412,
      "step": 2595
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.05884196609258652,
      "learning_rate": 8.209328782707622e-05,
      "loss": 0.4082,
      "step": 2596
    },
    {
      "epoch": 0.5902272727272727,
      "grad_norm": 0.04346765577793121,
      "learning_rate": 8.204778156996587e-05,
      "loss": 0.3122,
      "step": 2597
    },
    {
      "epoch": 0.5904545454545455,
      "grad_norm": 0.055980075150728226,
      "learning_rate": 8.200227531285553e-05,
      "loss": 0.3635,
      "step": 2598
    },
    {
      "epoch": 0.5906818181818182,
      "grad_norm": 0.039576053619384766,
      "learning_rate": 8.195676905574517e-05,
      "loss": 0.3106,
      "step": 2599
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.05052582919597626,
      "learning_rate": 8.191126279863482e-05,
      "loss": 0.3347,
      "step": 2600
    },
    {
      "epoch": 0.5911363636363637,
      "grad_norm": 0.04188387095928192,
      "learning_rate": 8.186575654152446e-05,
      "loss": 0.307,
      "step": 2601
    },
    {
      "epoch": 0.5913636363636363,
      "grad_norm": 0.030940573662519455,
      "learning_rate": 8.182025028441411e-05,
      "loss": 0.2578,
      "step": 2602
    },
    {
      "epoch": 0.5915909090909091,
      "grad_norm": 0.043565940111875534,
      "learning_rate": 8.177474402730376e-05,
      "loss": 0.3551,
      "step": 2603
    },
    {
      "epoch": 0.5918181818181818,
      "grad_norm": 0.029581956565380096,
      "learning_rate": 8.17292377701934e-05,
      "loss": 0.2383,
      "step": 2604
    },
    {
      "epoch": 0.5920454545454545,
      "grad_norm": 0.061234090477228165,
      "learning_rate": 8.168373151308306e-05,
      "loss": 0.3426,
      "step": 2605
    },
    {
      "epoch": 0.5922727272727273,
      "grad_norm": 0.04672860726714134,
      "learning_rate": 8.16382252559727e-05,
      "loss": 0.3278,
      "step": 2606
    },
    {
      "epoch": 0.5925,
      "grad_norm": 0.03335396945476532,
      "learning_rate": 8.159271899886235e-05,
      "loss": 0.2953,
      "step": 2607
    },
    {
      "epoch": 0.5927272727272728,
      "grad_norm": 0.044954512268304825,
      "learning_rate": 8.1547212741752e-05,
      "loss": 0.3438,
      "step": 2608
    },
    {
      "epoch": 0.5929545454545454,
      "grad_norm": 0.034464024007320404,
      "learning_rate": 8.150170648464164e-05,
      "loss": 0.304,
      "step": 2609
    },
    {
      "epoch": 0.5931818181818181,
      "grad_norm": 0.052297379821538925,
      "learning_rate": 8.145620022753129e-05,
      "loss": 0.3898,
      "step": 2610
    },
    {
      "epoch": 0.5934090909090909,
      "grad_norm": 0.047278452664613724,
      "learning_rate": 8.141069397042093e-05,
      "loss": 0.3519,
      "step": 2611
    },
    {
      "epoch": 0.5936363636363636,
      "grad_norm": 0.036994483321905136,
      "learning_rate": 8.136518771331058e-05,
      "loss": 0.2601,
      "step": 2612
    },
    {
      "epoch": 0.5938636363636364,
      "grad_norm": 0.06113859638571739,
      "learning_rate": 8.131968145620024e-05,
      "loss": 0.3823,
      "step": 2613
    },
    {
      "epoch": 0.5940909090909091,
      "grad_norm": 0.04199809581041336,
      "learning_rate": 8.127417519908988e-05,
      "loss": 0.3272,
      "step": 2614
    },
    {
      "epoch": 0.5943181818181819,
      "grad_norm": 0.047335341572761536,
      "learning_rate": 8.122866894197953e-05,
      "loss": 0.377,
      "step": 2615
    },
    {
      "epoch": 0.5945454545454546,
      "grad_norm": 0.03619258850812912,
      "learning_rate": 8.118316268486917e-05,
      "loss": 0.313,
      "step": 2616
    },
    {
      "epoch": 0.5947727272727272,
      "grad_norm": 0.05097242817282677,
      "learning_rate": 8.113765642775882e-05,
      "loss": 0.355,
      "step": 2617
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.05419512465596199,
      "learning_rate": 8.109215017064847e-05,
      "loss": 0.3817,
      "step": 2618
    },
    {
      "epoch": 0.5952272727272727,
      "grad_norm": 0.04093782231211662,
      "learning_rate": 8.104664391353811e-05,
      "loss": 0.3438,
      "step": 2619
    },
    {
      "epoch": 0.5954545454545455,
      "grad_norm": 0.041196178644895554,
      "learning_rate": 8.100113765642777e-05,
      "loss": 0.3717,
      "step": 2620
    },
    {
      "epoch": 0.5956818181818182,
      "grad_norm": 0.045739129185676575,
      "learning_rate": 8.09556313993174e-05,
      "loss": 0.3399,
      "step": 2621
    },
    {
      "epoch": 0.5959090909090909,
      "grad_norm": 0.05188732594251633,
      "learning_rate": 8.091012514220706e-05,
      "loss": 0.3523,
      "step": 2622
    },
    {
      "epoch": 0.5961363636363637,
      "grad_norm": 0.036615025252103806,
      "learning_rate": 8.086461888509671e-05,
      "loss": 0.2847,
      "step": 2623
    },
    {
      "epoch": 0.5963636363636363,
      "grad_norm": 0.04780898243188858,
      "learning_rate": 8.081911262798635e-05,
      "loss": 0.3406,
      "step": 2624
    },
    {
      "epoch": 0.5965909090909091,
      "grad_norm": 0.0540618933737278,
      "learning_rate": 8.0773606370876e-05,
      "loss": 0.3813,
      "step": 2625
    },
    {
      "epoch": 0.5968181818181818,
      "grad_norm": 0.04420878738164902,
      "learning_rate": 8.072810011376564e-05,
      "loss": 0.313,
      "step": 2626
    },
    {
      "epoch": 0.5970454545454545,
      "grad_norm": 0.04862278699874878,
      "learning_rate": 8.06825938566553e-05,
      "loss": 0.3653,
      "step": 2627
    },
    {
      "epoch": 0.5972727272727273,
      "grad_norm": 0.04694753512740135,
      "learning_rate": 8.063708759954493e-05,
      "loss": 0.3445,
      "step": 2628
    },
    {
      "epoch": 0.5975,
      "grad_norm": 0.03385518491268158,
      "learning_rate": 8.059158134243459e-05,
      "loss": 0.2659,
      "step": 2629
    },
    {
      "epoch": 0.5977272727272728,
      "grad_norm": 0.031149843707680702,
      "learning_rate": 8.054607508532424e-05,
      "loss": 0.2524,
      "step": 2630
    },
    {
      "epoch": 0.5979545454545454,
      "grad_norm": 0.03816481679677963,
      "learning_rate": 8.050056882821388e-05,
      "loss": 0.2872,
      "step": 2631
    },
    {
      "epoch": 0.5981818181818181,
      "grad_norm": 0.05067161098122597,
      "learning_rate": 8.045506257110353e-05,
      "loss": 0.3327,
      "step": 2632
    },
    {
      "epoch": 0.5984090909090909,
      "grad_norm": 0.042884379625320435,
      "learning_rate": 8.040955631399317e-05,
      "loss": 0.3376,
      "step": 2633
    },
    {
      "epoch": 0.5986363636363636,
      "grad_norm": 0.05573621764779091,
      "learning_rate": 8.036405005688282e-05,
      "loss": 0.3185,
      "step": 2634
    },
    {
      "epoch": 0.5988636363636364,
      "grad_norm": 0.038734711706638336,
      "learning_rate": 8.031854379977248e-05,
      "loss": 0.3038,
      "step": 2635
    },
    {
      "epoch": 0.5990909090909091,
      "grad_norm": 0.05280989408493042,
      "learning_rate": 8.027303754266212e-05,
      "loss": 0.3437,
      "step": 2636
    },
    {
      "epoch": 0.5993181818181819,
      "grad_norm": 0.05615770444273949,
      "learning_rate": 8.022753128555177e-05,
      "loss": 0.389,
      "step": 2637
    },
    {
      "epoch": 0.5995454545454545,
      "grad_norm": 0.04227077215909958,
      "learning_rate": 8.018202502844141e-05,
      "loss": 0.3374,
      "step": 2638
    },
    {
      "epoch": 0.5997727272727272,
      "grad_norm": 0.04292809218168259,
      "learning_rate": 8.013651877133106e-05,
      "loss": 0.3495,
      "step": 2639
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.03690032660961151,
      "learning_rate": 8.009101251422071e-05,
      "loss": 0.2896,
      "step": 2640
    },
    {
      "epoch": 0.6002272727272727,
      "grad_norm": 0.03435523062944412,
      "learning_rate": 8.004550625711035e-05,
      "loss": 0.2507,
      "step": 2641
    },
    {
      "epoch": 0.6004545454545455,
      "grad_norm": 0.047965019941329956,
      "learning_rate": 8e-05,
      "loss": 0.3672,
      "step": 2642
    },
    {
      "epoch": 0.6006818181818182,
      "grad_norm": 0.04107862710952759,
      "learning_rate": 7.995449374288965e-05,
      "loss": 0.3148,
      "step": 2643
    },
    {
      "epoch": 0.600909090909091,
      "grad_norm": 0.045650042593479156,
      "learning_rate": 7.99089874857793e-05,
      "loss": 0.3402,
      "step": 2644
    },
    {
      "epoch": 0.6011363636363637,
      "grad_norm": 0.03483961150050163,
      "learning_rate": 7.986348122866895e-05,
      "loss": 0.2375,
      "step": 2645
    },
    {
      "epoch": 0.6013636363636363,
      "grad_norm": 0.03893333300948143,
      "learning_rate": 7.981797497155859e-05,
      "loss": 0.3061,
      "step": 2646
    },
    {
      "epoch": 0.6015909090909091,
      "grad_norm": 0.047410715371370316,
      "learning_rate": 7.977246871444824e-05,
      "loss": 0.3323,
      "step": 2647
    },
    {
      "epoch": 0.6018181818181818,
      "grad_norm": 0.06560482084751129,
      "learning_rate": 7.972696245733788e-05,
      "loss": 0.4404,
      "step": 2648
    },
    {
      "epoch": 0.6020454545454546,
      "grad_norm": 0.035855282098054886,
      "learning_rate": 7.968145620022754e-05,
      "loss": 0.3016,
      "step": 2649
    },
    {
      "epoch": 0.6022727272727273,
      "grad_norm": 0.03679380193352699,
      "learning_rate": 7.963594994311719e-05,
      "loss": 0.3072,
      "step": 2650
    },
    {
      "epoch": 0.6025,
      "grad_norm": 0.04363729804754257,
      "learning_rate": 7.959044368600683e-05,
      "loss": 0.3208,
      "step": 2651
    },
    {
      "epoch": 0.6027272727272728,
      "grad_norm": 0.046679046005010605,
      "learning_rate": 7.954493742889648e-05,
      "loss": 0.3072,
      "step": 2652
    },
    {
      "epoch": 0.6029545454545454,
      "grad_norm": 0.05176132544875145,
      "learning_rate": 7.949943117178612e-05,
      "loss": 0.3721,
      "step": 2653
    },
    {
      "epoch": 0.6031818181818182,
      "grad_norm": 0.037478744983673096,
      "learning_rate": 7.945392491467577e-05,
      "loss": 0.3248,
      "step": 2654
    },
    {
      "epoch": 0.6034090909090909,
      "grad_norm": 0.038074661046266556,
      "learning_rate": 7.940841865756543e-05,
      "loss": 0.3014,
      "step": 2655
    },
    {
      "epoch": 0.6036363636363636,
      "grad_norm": 0.03312057629227638,
      "learning_rate": 7.936291240045506e-05,
      "loss": 0.2707,
      "step": 2656
    },
    {
      "epoch": 0.6038636363636364,
      "grad_norm": 0.0389600433409214,
      "learning_rate": 7.931740614334472e-05,
      "loss": 0.3085,
      "step": 2657
    },
    {
      "epoch": 0.6040909090909091,
      "grad_norm": 0.050178412348032,
      "learning_rate": 7.927189988623436e-05,
      "loss": 0.4048,
      "step": 2658
    },
    {
      "epoch": 0.6043181818181819,
      "grad_norm": 0.04313383996486664,
      "learning_rate": 7.922639362912401e-05,
      "loss": 0.3049,
      "step": 2659
    },
    {
      "epoch": 0.6045454545454545,
      "grad_norm": 0.05666664242744446,
      "learning_rate": 7.918088737201366e-05,
      "loss": 0.3624,
      "step": 2660
    },
    {
      "epoch": 0.6047727272727272,
      "grad_norm": 0.0434880293905735,
      "learning_rate": 7.91353811149033e-05,
      "loss": 0.3046,
      "step": 2661
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03724883124232292,
      "learning_rate": 7.908987485779294e-05,
      "loss": 0.2987,
      "step": 2662
    },
    {
      "epoch": 0.6052272727272727,
      "grad_norm": 0.03733421489596367,
      "learning_rate": 7.90443686006826e-05,
      "loss": 0.3075,
      "step": 2663
    },
    {
      "epoch": 0.6054545454545455,
      "grad_norm": 0.04554947465658188,
      "learning_rate": 7.899886234357225e-05,
      "loss": 0.3132,
      "step": 2664
    },
    {
      "epoch": 0.6056818181818182,
      "grad_norm": 0.04682934656739235,
      "learning_rate": 7.89533560864619e-05,
      "loss": 0.3557,
      "step": 2665
    },
    {
      "epoch": 0.605909090909091,
      "grad_norm": 0.04927109181880951,
      "learning_rate": 7.890784982935154e-05,
      "loss": 0.3364,
      "step": 2666
    },
    {
      "epoch": 0.6061363636363636,
      "grad_norm": 0.03825940936803818,
      "learning_rate": 7.886234357224118e-05,
      "loss": 0.2821,
      "step": 2667
    },
    {
      "epoch": 0.6063636363636363,
      "grad_norm": 0.047704100608825684,
      "learning_rate": 7.881683731513083e-05,
      "loss": 0.2999,
      "step": 2668
    },
    {
      "epoch": 0.6065909090909091,
      "grad_norm": 0.0555671751499176,
      "learning_rate": 7.877133105802048e-05,
      "loss": 0.3984,
      "step": 2669
    },
    {
      "epoch": 0.6068181818181818,
      "grad_norm": 0.03915809094905853,
      "learning_rate": 7.872582480091014e-05,
      "loss": 0.3238,
      "step": 2670
    },
    {
      "epoch": 0.6070454545454546,
      "grad_norm": 0.04861104115843773,
      "learning_rate": 7.868031854379978e-05,
      "loss": 0.3357,
      "step": 2671
    },
    {
      "epoch": 0.6072727272727273,
      "grad_norm": 0.03549502417445183,
      "learning_rate": 7.863481228668941e-05,
      "loss": 0.2662,
      "step": 2672
    },
    {
      "epoch": 0.6075,
      "grad_norm": 0.05548553541302681,
      "learning_rate": 7.858930602957907e-05,
      "loss": 0.3678,
      "step": 2673
    },
    {
      "epoch": 0.6077272727272728,
      "grad_norm": 0.03291289135813713,
      "learning_rate": 7.854379977246872e-05,
      "loss": 0.2926,
      "step": 2674
    },
    {
      "epoch": 0.6079545454545454,
      "grad_norm": 0.03697890043258667,
      "learning_rate": 7.849829351535837e-05,
      "loss": 0.3266,
      "step": 2675
    },
    {
      "epoch": 0.6081818181818182,
      "grad_norm": 0.04241756722331047,
      "learning_rate": 7.845278725824801e-05,
      "loss": 0.3376,
      "step": 2676
    },
    {
      "epoch": 0.6084090909090909,
      "grad_norm": 0.04730013385415077,
      "learning_rate": 7.840728100113765e-05,
      "loss": 0.3291,
      "step": 2677
    },
    {
      "epoch": 0.6086363636363636,
      "grad_norm": 0.046798139810562134,
      "learning_rate": 7.83617747440273e-05,
      "loss": 0.3476,
      "step": 2678
    },
    {
      "epoch": 0.6088636363636364,
      "grad_norm": 0.05226924270391464,
      "learning_rate": 7.831626848691696e-05,
      "loss": 0.3475,
      "step": 2679
    },
    {
      "epoch": 0.6090909090909091,
      "grad_norm": 0.05127134919166565,
      "learning_rate": 7.827076222980661e-05,
      "loss": 0.3413,
      "step": 2680
    },
    {
      "epoch": 0.6093181818181819,
      "grad_norm": 0.04594359174370766,
      "learning_rate": 7.822525597269625e-05,
      "loss": 0.3807,
      "step": 2681
    },
    {
      "epoch": 0.6095454545454545,
      "grad_norm": 0.04551464691758156,
      "learning_rate": 7.817974971558589e-05,
      "loss": 0.3618,
      "step": 2682
    },
    {
      "epoch": 0.6097727272727272,
      "grad_norm": 0.04524872824549675,
      "learning_rate": 7.813424345847554e-05,
      "loss": 0.3455,
      "step": 2683
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.03701983019709587,
      "learning_rate": 7.80887372013652e-05,
      "loss": 0.3165,
      "step": 2684
    },
    {
      "epoch": 0.6102272727272727,
      "grad_norm": 0.036259233951568604,
      "learning_rate": 7.804323094425485e-05,
      "loss": 0.2987,
      "step": 2685
    },
    {
      "epoch": 0.6104545454545455,
      "grad_norm": 0.039614900946617126,
      "learning_rate": 7.799772468714449e-05,
      "loss": 0.3218,
      "step": 2686
    },
    {
      "epoch": 0.6106818181818182,
      "grad_norm": 0.03725675493478775,
      "learning_rate": 7.795221843003413e-05,
      "loss": 0.2779,
      "step": 2687
    },
    {
      "epoch": 0.610909090909091,
      "grad_norm": 0.04469167813658714,
      "learning_rate": 7.790671217292378e-05,
      "loss": 0.3359,
      "step": 2688
    },
    {
      "epoch": 0.6111363636363636,
      "grad_norm": 0.04304732009768486,
      "learning_rate": 7.786120591581343e-05,
      "loss": 0.3639,
      "step": 2689
    },
    {
      "epoch": 0.6113636363636363,
      "grad_norm": 0.05171196907758713,
      "learning_rate": 7.781569965870308e-05,
      "loss": 0.3469,
      "step": 2690
    },
    {
      "epoch": 0.6115909090909091,
      "grad_norm": 0.047846369445323944,
      "learning_rate": 7.777019340159272e-05,
      "loss": 0.3581,
      "step": 2691
    },
    {
      "epoch": 0.6118181818181818,
      "grad_norm": 0.05776961147785187,
      "learning_rate": 7.772468714448236e-05,
      "loss": 0.4393,
      "step": 2692
    },
    {
      "epoch": 0.6120454545454546,
      "grad_norm": 0.0489489883184433,
      "learning_rate": 7.767918088737202e-05,
      "loss": 0.3493,
      "step": 2693
    },
    {
      "epoch": 0.6122727272727273,
      "grad_norm": 0.04931299760937691,
      "learning_rate": 7.763367463026167e-05,
      "loss": 0.3454,
      "step": 2694
    },
    {
      "epoch": 0.6125,
      "grad_norm": 0.05611244589090347,
      "learning_rate": 7.758816837315132e-05,
      "loss": 0.3511,
      "step": 2695
    },
    {
      "epoch": 0.6127272727272727,
      "grad_norm": 0.04614510014653206,
      "learning_rate": 7.754266211604096e-05,
      "loss": 0.4047,
      "step": 2696
    },
    {
      "epoch": 0.6129545454545454,
      "grad_norm": 0.03969016671180725,
      "learning_rate": 7.74971558589306e-05,
      "loss": 0.2981,
      "step": 2697
    },
    {
      "epoch": 0.6131818181818182,
      "grad_norm": 0.0539778508245945,
      "learning_rate": 7.745164960182025e-05,
      "loss": 0.3926,
      "step": 2698
    },
    {
      "epoch": 0.6134090909090909,
      "grad_norm": 0.04549933597445488,
      "learning_rate": 7.74061433447099e-05,
      "loss": 0.3158,
      "step": 2699
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.05618242174386978,
      "learning_rate": 7.736063708759956e-05,
      "loss": 0.4017,
      "step": 2700
    },
    {
      "epoch": 0.6138636363636364,
      "grad_norm": 0.04819086566567421,
      "learning_rate": 7.731513083048918e-05,
      "loss": 0.3505,
      "step": 2701
    },
    {
      "epoch": 0.6140909090909091,
      "grad_norm": 0.04527885466814041,
      "learning_rate": 7.726962457337884e-05,
      "loss": 0.2914,
      "step": 2702
    },
    {
      "epoch": 0.6143181818181818,
      "grad_norm": 0.04987364262342453,
      "learning_rate": 7.722411831626849e-05,
      "loss": 0.34,
      "step": 2703
    },
    {
      "epoch": 0.6145454545454545,
      "grad_norm": 0.035870570689439774,
      "learning_rate": 7.717861205915814e-05,
      "loss": 0.2919,
      "step": 2704
    },
    {
      "epoch": 0.6147727272727272,
      "grad_norm": 0.03649461641907692,
      "learning_rate": 7.71331058020478e-05,
      "loss": 0.2647,
      "step": 2705
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.06627005338668823,
      "learning_rate": 7.708759954493742e-05,
      "loss": 0.3668,
      "step": 2706
    },
    {
      "epoch": 0.6152272727272727,
      "grad_norm": 0.05070336163043976,
      "learning_rate": 7.704209328782707e-05,
      "loss": 0.3356,
      "step": 2707
    },
    {
      "epoch": 0.6154545454545455,
      "grad_norm": 0.04101758077740669,
      "learning_rate": 7.699658703071673e-05,
      "loss": 0.3615,
      "step": 2708
    },
    {
      "epoch": 0.6156818181818182,
      "grad_norm": 0.05451972037553787,
      "learning_rate": 7.695108077360638e-05,
      "loss": 0.3707,
      "step": 2709
    },
    {
      "epoch": 0.615909090909091,
      "grad_norm": 0.040815744549036026,
      "learning_rate": 7.690557451649603e-05,
      "loss": 0.3309,
      "step": 2710
    },
    {
      "epoch": 0.6161363636363636,
      "grad_norm": 0.033164188265800476,
      "learning_rate": 7.686006825938566e-05,
      "loss": 0.3283,
      "step": 2711
    },
    {
      "epoch": 0.6163636363636363,
      "grad_norm": 0.039872776716947556,
      "learning_rate": 7.681456200227531e-05,
      "loss": 0.3177,
      "step": 2712
    },
    {
      "epoch": 0.6165909090909091,
      "grad_norm": 0.04064684361219406,
      "learning_rate": 7.676905574516496e-05,
      "loss": 0.3096,
      "step": 2713
    },
    {
      "epoch": 0.6168181818181818,
      "grad_norm": 0.06319069117307663,
      "learning_rate": 7.672354948805462e-05,
      "loss": 0.3822,
      "step": 2714
    },
    {
      "epoch": 0.6170454545454546,
      "grad_norm": 0.05425270274281502,
      "learning_rate": 7.667804323094427e-05,
      "loss": 0.3778,
      "step": 2715
    },
    {
      "epoch": 0.6172727272727273,
      "grad_norm": 0.044924505054950714,
      "learning_rate": 7.66325369738339e-05,
      "loss": 0.3868,
      "step": 2716
    },
    {
      "epoch": 0.6175,
      "grad_norm": 0.042955659329891205,
      "learning_rate": 7.658703071672355e-05,
      "loss": 0.3703,
      "step": 2717
    },
    {
      "epoch": 0.6177272727272727,
      "grad_norm": 0.04318264499306679,
      "learning_rate": 7.65415244596132e-05,
      "loss": 0.2951,
      "step": 2718
    },
    {
      "epoch": 0.6179545454545454,
      "grad_norm": 0.049925949424505234,
      "learning_rate": 7.649601820250285e-05,
      "loss": 0.3507,
      "step": 2719
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.046137720346450806,
      "learning_rate": 7.64505119453925e-05,
      "loss": 0.2435,
      "step": 2720
    },
    {
      "epoch": 0.6184090909090909,
      "grad_norm": 0.04260685294866562,
      "learning_rate": 7.640500568828213e-05,
      "loss": 0.3266,
      "step": 2721
    },
    {
      "epoch": 0.6186363636363637,
      "grad_norm": 0.04283687099814415,
      "learning_rate": 7.635949943117178e-05,
      "loss": 0.3197,
      "step": 2722
    },
    {
      "epoch": 0.6188636363636364,
      "grad_norm": 0.04664098843932152,
      "learning_rate": 7.631399317406144e-05,
      "loss": 0.3056,
      "step": 2723
    },
    {
      "epoch": 0.6190909090909091,
      "grad_norm": 0.040477849543094635,
      "learning_rate": 7.626848691695109e-05,
      "loss": 0.2991,
      "step": 2724
    },
    {
      "epoch": 0.6193181818181818,
      "grad_norm": 0.037026241421699524,
      "learning_rate": 7.622298065984074e-05,
      "loss": 0.2924,
      "step": 2725
    },
    {
      "epoch": 0.6195454545454545,
      "grad_norm": 0.056492798030376434,
      "learning_rate": 7.617747440273037e-05,
      "loss": 0.3907,
      "step": 2726
    },
    {
      "epoch": 0.6197727272727273,
      "grad_norm": 0.036386746913194656,
      "learning_rate": 7.613196814562002e-05,
      "loss": 0.2662,
      "step": 2727
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.051519304513931274,
      "learning_rate": 7.608646188850967e-05,
      "loss": 0.3507,
      "step": 2728
    },
    {
      "epoch": 0.6202272727272727,
      "grad_norm": 0.04093021899461746,
      "learning_rate": 7.604095563139933e-05,
      "loss": 0.3188,
      "step": 2729
    },
    {
      "epoch": 0.6204545454545455,
      "grad_norm": 0.05053611099720001,
      "learning_rate": 7.599544937428897e-05,
      "loss": 0.3413,
      "step": 2730
    },
    {
      "epoch": 0.6206818181818182,
      "grad_norm": 0.04209880158305168,
      "learning_rate": 7.59499431171786e-05,
      "loss": 0.305,
      "step": 2731
    },
    {
      "epoch": 0.6209090909090909,
      "grad_norm": 0.04037933051586151,
      "learning_rate": 7.590443686006826e-05,
      "loss": 0.3079,
      "step": 2732
    },
    {
      "epoch": 0.6211363636363636,
      "grad_norm": 0.05348985642194748,
      "learning_rate": 7.585893060295791e-05,
      "loss": 0.3016,
      "step": 2733
    },
    {
      "epoch": 0.6213636363636363,
      "grad_norm": 0.033299606293439865,
      "learning_rate": 7.581342434584756e-05,
      "loss": 0.2707,
      "step": 2734
    },
    {
      "epoch": 0.6215909090909091,
      "grad_norm": 0.05063723772764206,
      "learning_rate": 7.57679180887372e-05,
      "loss": 0.3548,
      "step": 2735
    },
    {
      "epoch": 0.6218181818181818,
      "grad_norm": 0.04401235654950142,
      "learning_rate": 7.572241183162684e-05,
      "loss": 0.3098,
      "step": 2736
    },
    {
      "epoch": 0.6220454545454546,
      "grad_norm": 0.05198250710964203,
      "learning_rate": 7.56769055745165e-05,
      "loss": 0.3685,
      "step": 2737
    },
    {
      "epoch": 0.6222727272727273,
      "grad_norm": 0.042213648557662964,
      "learning_rate": 7.563139931740615e-05,
      "loss": 0.3569,
      "step": 2738
    },
    {
      "epoch": 0.6225,
      "grad_norm": 0.04890548437833786,
      "learning_rate": 7.55858930602958e-05,
      "loss": 0.3863,
      "step": 2739
    },
    {
      "epoch": 0.6227272727272727,
      "grad_norm": 0.05060432851314545,
      "learning_rate": 7.554038680318544e-05,
      "loss": 0.3001,
      "step": 2740
    },
    {
      "epoch": 0.6229545454545454,
      "grad_norm": 0.05577363073825836,
      "learning_rate": 7.549488054607508e-05,
      "loss": 0.3966,
      "step": 2741
    },
    {
      "epoch": 0.6231818181818182,
      "grad_norm": 0.04244941473007202,
      "learning_rate": 7.544937428896473e-05,
      "loss": 0.2948,
      "step": 2742
    },
    {
      "epoch": 0.6234090909090909,
      "grad_norm": 0.04093707725405693,
      "learning_rate": 7.540386803185439e-05,
      "loss": 0.337,
      "step": 2743
    },
    {
      "epoch": 0.6236363636363637,
      "grad_norm": 0.03858267143368721,
      "learning_rate": 7.535836177474404e-05,
      "loss": 0.255,
      "step": 2744
    },
    {
      "epoch": 0.6238636363636364,
      "grad_norm": 0.06444703042507172,
      "learning_rate": 7.531285551763368e-05,
      "loss": 0.4057,
      "step": 2745
    },
    {
      "epoch": 0.6240909090909091,
      "grad_norm": 0.04091079905629158,
      "learning_rate": 7.526734926052332e-05,
      "loss": 0.3193,
      "step": 2746
    },
    {
      "epoch": 0.6243181818181818,
      "grad_norm": 0.05108948424458504,
      "learning_rate": 7.522184300341297e-05,
      "loss": 0.3883,
      "step": 2747
    },
    {
      "epoch": 0.6245454545454545,
      "grad_norm": 0.04280523955821991,
      "learning_rate": 7.517633674630262e-05,
      "loss": 0.3451,
      "step": 2748
    },
    {
      "epoch": 0.6247727272727273,
      "grad_norm": 0.04509797692298889,
      "learning_rate": 7.513083048919228e-05,
      "loss": 0.3426,
      "step": 2749
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.044513363391160965,
      "learning_rate": 7.508532423208191e-05,
      "loss": 0.3387,
      "step": 2750
    },
    {
      "epoch": 0.6252272727272727,
      "grad_norm": 0.052760977298021317,
      "learning_rate": 7.503981797497155e-05,
      "loss": 0.3652,
      "step": 2751
    },
    {
      "epoch": 0.6254545454545455,
      "grad_norm": 0.04031458869576454,
      "learning_rate": 7.499431171786121e-05,
      "loss": 0.2439,
      "step": 2752
    },
    {
      "epoch": 0.6256818181818182,
      "grad_norm": 0.036530300974845886,
      "learning_rate": 7.494880546075086e-05,
      "loss": 0.3036,
      "step": 2753
    },
    {
      "epoch": 0.6259090909090909,
      "grad_norm": 0.045009516179561615,
      "learning_rate": 7.490329920364051e-05,
      "loss": 0.3121,
      "step": 2754
    },
    {
      "epoch": 0.6261363636363636,
      "grad_norm": 0.0380096435546875,
      "learning_rate": 7.485779294653015e-05,
      "loss": 0.3384,
      "step": 2755
    },
    {
      "epoch": 0.6263636363636363,
      "grad_norm": 0.05470291152596474,
      "learning_rate": 7.481228668941979e-05,
      "loss": 0.3694,
      "step": 2756
    },
    {
      "epoch": 0.6265909090909091,
      "grad_norm": 0.036528173834085464,
      "learning_rate": 7.476678043230944e-05,
      "loss": 0.2975,
      "step": 2757
    },
    {
      "epoch": 0.6268181818181818,
      "grad_norm": 0.040134016424417496,
      "learning_rate": 7.47212741751991e-05,
      "loss": 0.3114,
      "step": 2758
    },
    {
      "epoch": 0.6270454545454546,
      "grad_norm": 0.03437262400984764,
      "learning_rate": 7.467576791808875e-05,
      "loss": 0.2269,
      "step": 2759
    },
    {
      "epoch": 0.6272727272727273,
      "grad_norm": 0.05505632609128952,
      "learning_rate": 7.463026166097839e-05,
      "loss": 0.3153,
      "step": 2760
    },
    {
      "epoch": 0.6275,
      "grad_norm": 0.054343245923519135,
      "learning_rate": 7.458475540386803e-05,
      "loss": 0.3514,
      "step": 2761
    },
    {
      "epoch": 0.6277272727272727,
      "grad_norm": 0.049364831298589706,
      "learning_rate": 7.453924914675768e-05,
      "loss": 0.3444,
      "step": 2762
    },
    {
      "epoch": 0.6279545454545454,
      "grad_norm": 0.04553850367665291,
      "learning_rate": 7.449374288964733e-05,
      "loss": 0.3536,
      "step": 2763
    },
    {
      "epoch": 0.6281818181818182,
      "grad_norm": 0.04310271888971329,
      "learning_rate": 7.444823663253699e-05,
      "loss": 0.3372,
      "step": 2764
    },
    {
      "epoch": 0.6284090909090909,
      "grad_norm": 0.05961887910962105,
      "learning_rate": 7.440273037542663e-05,
      "loss": 0.3604,
      "step": 2765
    },
    {
      "epoch": 0.6286363636363637,
      "grad_norm": 0.04744341969490051,
      "learning_rate": 7.435722411831626e-05,
      "loss": 0.3749,
      "step": 2766
    },
    {
      "epoch": 0.6288636363636364,
      "grad_norm": 0.05536109581589699,
      "learning_rate": 7.431171786120592e-05,
      "loss": 0.3236,
      "step": 2767
    },
    {
      "epoch": 0.6290909090909091,
      "grad_norm": 0.03714514896273613,
      "learning_rate": 7.426621160409557e-05,
      "loss": 0.2785,
      "step": 2768
    },
    {
      "epoch": 0.6293181818181818,
      "grad_norm": 0.03540889546275139,
      "learning_rate": 7.422070534698521e-05,
      "loss": 0.3129,
      "step": 2769
    },
    {
      "epoch": 0.6295454545454545,
      "grad_norm": 0.04526791349053383,
      "learning_rate": 7.417519908987486e-05,
      "loss": 0.3606,
      "step": 2770
    },
    {
      "epoch": 0.6297727272727273,
      "grad_norm": 0.04208831116557121,
      "learning_rate": 7.41296928327645e-05,
      "loss": 0.3155,
      "step": 2771
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.04401251673698425,
      "learning_rate": 7.408418657565415e-05,
      "loss": 0.3375,
      "step": 2772
    },
    {
      "epoch": 0.6302272727272727,
      "grad_norm": 0.03779878094792366,
      "learning_rate": 7.403868031854381e-05,
      "loss": 0.3092,
      "step": 2773
    },
    {
      "epoch": 0.6304545454545455,
      "grad_norm": 0.045713216066360474,
      "learning_rate": 7.399317406143345e-05,
      "loss": 0.3534,
      "step": 2774
    },
    {
      "epoch": 0.6306818181818182,
      "grad_norm": 0.052778780460357666,
      "learning_rate": 7.39476678043231e-05,
      "loss": 0.3895,
      "step": 2775
    },
    {
      "epoch": 0.6309090909090909,
      "grad_norm": 0.043278150260448456,
      "learning_rate": 7.390216154721274e-05,
      "loss": 0.3426,
      "step": 2776
    },
    {
      "epoch": 0.6311363636363636,
      "grad_norm": 0.04501956328749657,
      "learning_rate": 7.385665529010239e-05,
      "loss": 0.3745,
      "step": 2777
    },
    {
      "epoch": 0.6313636363636363,
      "grad_norm": 0.03758113458752632,
      "learning_rate": 7.381114903299204e-05,
      "loss": 0.2975,
      "step": 2778
    },
    {
      "epoch": 0.6315909090909091,
      "grad_norm": 0.034987322986125946,
      "learning_rate": 7.376564277588168e-05,
      "loss": 0.2953,
      "step": 2779
    },
    {
      "epoch": 0.6318181818181818,
      "grad_norm": 0.04900176450610161,
      "learning_rate": 7.372013651877134e-05,
      "loss": 0.3539,
      "step": 2780
    },
    {
      "epoch": 0.6320454545454546,
      "grad_norm": 0.04651649668812752,
      "learning_rate": 7.367463026166098e-05,
      "loss": 0.312,
      "step": 2781
    },
    {
      "epoch": 0.6322727272727273,
      "grad_norm": 0.041127488017082214,
      "learning_rate": 7.362912400455063e-05,
      "loss": 0.3219,
      "step": 2782
    },
    {
      "epoch": 0.6325,
      "grad_norm": 0.04417211934924126,
      "learning_rate": 7.358361774744028e-05,
      "loss": 0.3133,
      "step": 2783
    },
    {
      "epoch": 0.6327272727272727,
      "grad_norm": 0.060173992067575455,
      "learning_rate": 7.353811149032992e-05,
      "loss": 0.3846,
      "step": 2784
    },
    {
      "epoch": 0.6329545454545454,
      "grad_norm": 0.046695783734321594,
      "learning_rate": 7.349260523321957e-05,
      "loss": 0.3019,
      "step": 2785
    },
    {
      "epoch": 0.6331818181818182,
      "grad_norm": 0.04340016841888428,
      "learning_rate": 7.344709897610921e-05,
      "loss": 0.3479,
      "step": 2786
    },
    {
      "epoch": 0.6334090909090909,
      "grad_norm": 0.03493201360106468,
      "learning_rate": 7.340159271899887e-05,
      "loss": 0.2726,
      "step": 2787
    },
    {
      "epoch": 0.6336363636363637,
      "grad_norm": 0.04538392648100853,
      "learning_rate": 7.335608646188852e-05,
      "loss": 0.3356,
      "step": 2788
    },
    {
      "epoch": 0.6338636363636364,
      "grad_norm": 0.03752899542450905,
      "learning_rate": 7.331058020477816e-05,
      "loss": 0.2936,
      "step": 2789
    },
    {
      "epoch": 0.634090909090909,
      "grad_norm": 0.047237515449523926,
      "learning_rate": 7.326507394766781e-05,
      "loss": 0.3709,
      "step": 2790
    },
    {
      "epoch": 0.6343181818181818,
      "grad_norm": 0.050931014120578766,
      "learning_rate": 7.321956769055745e-05,
      "loss": 0.3559,
      "step": 2791
    },
    {
      "epoch": 0.6345454545454545,
      "grad_norm": 0.03619816154241562,
      "learning_rate": 7.31740614334471e-05,
      "loss": 0.2846,
      "step": 2792
    },
    {
      "epoch": 0.6347727272727273,
      "grad_norm": 0.04451953247189522,
      "learning_rate": 7.312855517633676e-05,
      "loss": 0.3569,
      "step": 2793
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.03866488113999367,
      "learning_rate": 7.30830489192264e-05,
      "loss": 0.3483,
      "step": 2794
    },
    {
      "epoch": 0.6352272727272728,
      "grad_norm": 0.027511265128850937,
      "learning_rate": 7.303754266211605e-05,
      "loss": 0.2361,
      "step": 2795
    },
    {
      "epoch": 0.6354545454545455,
      "grad_norm": 0.04923921450972557,
      "learning_rate": 7.299203640500569e-05,
      "loss": 0.3559,
      "step": 2796
    },
    {
      "epoch": 0.6356818181818182,
      "grad_norm": 0.04437587410211563,
      "learning_rate": 7.294653014789534e-05,
      "loss": 0.3249,
      "step": 2797
    },
    {
      "epoch": 0.6359090909090909,
      "grad_norm": 0.037714604288339615,
      "learning_rate": 7.290102389078499e-05,
      "loss": 0.342,
      "step": 2798
    },
    {
      "epoch": 0.6361363636363636,
      "grad_norm": 0.04854809492826462,
      "learning_rate": 7.285551763367463e-05,
      "loss": 0.3454,
      "step": 2799
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.04628119617700577,
      "learning_rate": 7.281001137656428e-05,
      "loss": 0.3128,
      "step": 2800
    },
    {
      "epoch": 0.6365909090909091,
      "grad_norm": 0.04147684574127197,
      "learning_rate": 7.276450511945392e-05,
      "loss": 0.3336,
      "step": 2801
    },
    {
      "epoch": 0.6368181818181818,
      "grad_norm": 0.04855012893676758,
      "learning_rate": 7.271899886234358e-05,
      "loss": 0.3579,
      "step": 2802
    },
    {
      "epoch": 0.6370454545454546,
      "grad_norm": 0.04528716951608658,
      "learning_rate": 7.267349260523322e-05,
      "loss": 0.3393,
      "step": 2803
    },
    {
      "epoch": 0.6372727272727273,
      "grad_norm": 0.03902215510606766,
      "learning_rate": 7.262798634812287e-05,
      "loss": 0.3309,
      "step": 2804
    },
    {
      "epoch": 0.6375,
      "grad_norm": 0.04498939961194992,
      "learning_rate": 7.258248009101252e-05,
      "loss": 0.3242,
      "step": 2805
    },
    {
      "epoch": 0.6377272727272727,
      "grad_norm": 0.05325821042060852,
      "learning_rate": 7.253697383390216e-05,
      "loss": 0.3312,
      "step": 2806
    },
    {
      "epoch": 0.6379545454545454,
      "grad_norm": 0.03211808577179909,
      "learning_rate": 7.249146757679181e-05,
      "loss": 0.2617,
      "step": 2807
    },
    {
      "epoch": 0.6381818181818182,
      "grad_norm": 0.04453587904572487,
      "learning_rate": 7.244596131968145e-05,
      "loss": 0.3467,
      "step": 2808
    },
    {
      "epoch": 0.6384090909090909,
      "grad_norm": 0.044735074043273926,
      "learning_rate": 7.24004550625711e-05,
      "loss": 0.386,
      "step": 2809
    },
    {
      "epoch": 0.6386363636363637,
      "grad_norm": 0.05040987208485603,
      "learning_rate": 7.235494880546076e-05,
      "loss": 0.3684,
      "step": 2810
    },
    {
      "epoch": 0.6388636363636364,
      "grad_norm": 0.04833945259451866,
      "learning_rate": 7.23094425483504e-05,
      "loss": 0.3334,
      "step": 2811
    },
    {
      "epoch": 0.639090909090909,
      "grad_norm": 0.03879893198609352,
      "learning_rate": 7.226393629124005e-05,
      "loss": 0.2805,
      "step": 2812
    },
    {
      "epoch": 0.6393181818181818,
      "grad_norm": 0.03719165921211243,
      "learning_rate": 7.221843003412969e-05,
      "loss": 0.2901,
      "step": 2813
    },
    {
      "epoch": 0.6395454545454545,
      "grad_norm": 0.04800201579928398,
      "learning_rate": 7.217292377701934e-05,
      "loss": 0.3497,
      "step": 2814
    },
    {
      "epoch": 0.6397727272727273,
      "grad_norm": 0.06942682713270187,
      "learning_rate": 7.2127417519909e-05,
      "loss": 0.4128,
      "step": 2815
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.03944556042551994,
      "learning_rate": 7.208191126279863e-05,
      "loss": 0.2615,
      "step": 2816
    },
    {
      "epoch": 0.6402272727272728,
      "grad_norm": 0.04889759048819542,
      "learning_rate": 7.203640500568829e-05,
      "loss": 0.3742,
      "step": 2817
    },
    {
      "epoch": 0.6404545454545455,
      "grad_norm": 0.04094905033707619,
      "learning_rate": 7.199089874857793e-05,
      "loss": 0.3262,
      "step": 2818
    },
    {
      "epoch": 0.6406818181818181,
      "grad_norm": 0.035550154745578766,
      "learning_rate": 7.194539249146758e-05,
      "loss": 0.2859,
      "step": 2819
    },
    {
      "epoch": 0.6409090909090909,
      "grad_norm": 0.05604635179042816,
      "learning_rate": 7.189988623435723e-05,
      "loss": 0.4091,
      "step": 2820
    },
    {
      "epoch": 0.6411363636363636,
      "grad_norm": 0.0444202646613121,
      "learning_rate": 7.185437997724687e-05,
      "loss": 0.3658,
      "step": 2821
    },
    {
      "epoch": 0.6413636363636364,
      "grad_norm": 0.04988972470164299,
      "learning_rate": 7.180887372013652e-05,
      "loss": 0.3999,
      "step": 2822
    },
    {
      "epoch": 0.6415909090909091,
      "grad_norm": 0.05154109001159668,
      "learning_rate": 7.176336746302616e-05,
      "loss": 0.3488,
      "step": 2823
    },
    {
      "epoch": 0.6418181818181818,
      "grad_norm": 0.044026605784893036,
      "learning_rate": 7.171786120591582e-05,
      "loss": 0.3134,
      "step": 2824
    },
    {
      "epoch": 0.6420454545454546,
      "grad_norm": 0.044491443783044815,
      "learning_rate": 7.167235494880547e-05,
      "loss": 0.3562,
      "step": 2825
    },
    {
      "epoch": 0.6422727272727272,
      "grad_norm": 0.07536882907152176,
      "learning_rate": 7.162684869169511e-05,
      "loss": 0.3765,
      "step": 2826
    },
    {
      "epoch": 0.6425,
      "grad_norm": 0.04279308393597603,
      "learning_rate": 7.158134243458476e-05,
      "loss": 0.3488,
      "step": 2827
    },
    {
      "epoch": 0.6427272727272727,
      "grad_norm": 0.05005805194377899,
      "learning_rate": 7.15358361774744e-05,
      "loss": 0.3533,
      "step": 2828
    },
    {
      "epoch": 0.6429545454545454,
      "grad_norm": 0.03764354810118675,
      "learning_rate": 7.149032992036405e-05,
      "loss": 0.3015,
      "step": 2829
    },
    {
      "epoch": 0.6431818181818182,
      "grad_norm": 0.05273505672812462,
      "learning_rate": 7.14448236632537e-05,
      "loss": 0.348,
      "step": 2830
    },
    {
      "epoch": 0.6434090909090909,
      "grad_norm": 0.0331215001642704,
      "learning_rate": 7.139931740614335e-05,
      "loss": 0.2736,
      "step": 2831
    },
    {
      "epoch": 0.6436363636363637,
      "grad_norm": 0.03644606098532677,
      "learning_rate": 7.1353811149033e-05,
      "loss": 0.2802,
      "step": 2832
    },
    {
      "epoch": 0.6438636363636364,
      "grad_norm": 0.06050390005111694,
      "learning_rate": 7.130830489192264e-05,
      "loss": 0.3953,
      "step": 2833
    },
    {
      "epoch": 0.644090909090909,
      "grad_norm": 0.031256288290023804,
      "learning_rate": 7.126279863481229e-05,
      "loss": 0.2562,
      "step": 2834
    },
    {
      "epoch": 0.6443181818181818,
      "grad_norm": 0.05576604977250099,
      "learning_rate": 7.121729237770194e-05,
      "loss": 0.3444,
      "step": 2835
    },
    {
      "epoch": 0.6445454545454545,
      "grad_norm": 0.043176330626010895,
      "learning_rate": 7.117178612059158e-05,
      "loss": 0.3338,
      "step": 2836
    },
    {
      "epoch": 0.6447727272727273,
      "grad_norm": 0.041824255138635635,
      "learning_rate": 7.112627986348122e-05,
      "loss": 0.3118,
      "step": 2837
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.048286862671375275,
      "learning_rate": 7.108077360637087e-05,
      "loss": 0.3521,
      "step": 2838
    },
    {
      "epoch": 0.6452272727272728,
      "grad_norm": 0.039626166224479675,
      "learning_rate": 7.103526734926053e-05,
      "loss": 0.3158,
      "step": 2839
    },
    {
      "epoch": 0.6454545454545455,
      "grad_norm": 0.05070041865110397,
      "learning_rate": 7.098976109215018e-05,
      "loss": 0.3855,
      "step": 2840
    },
    {
      "epoch": 0.6456818181818181,
      "grad_norm": 0.04842561483383179,
      "learning_rate": 7.094425483503982e-05,
      "loss": 0.3804,
      "step": 2841
    },
    {
      "epoch": 0.6459090909090909,
      "grad_norm": 0.052102696150541306,
      "learning_rate": 7.089874857792946e-05,
      "loss": 0.3749,
      "step": 2842
    },
    {
      "epoch": 0.6461363636363636,
      "grad_norm": 0.05225467309355736,
      "learning_rate": 7.085324232081911e-05,
      "loss": 0.3601,
      "step": 2843
    },
    {
      "epoch": 0.6463636363636364,
      "grad_norm": 0.04459305852651596,
      "learning_rate": 7.080773606370876e-05,
      "loss": 0.3439,
      "step": 2844
    },
    {
      "epoch": 0.6465909090909091,
      "grad_norm": 0.06535139679908752,
      "learning_rate": 7.076222980659842e-05,
      "loss": 0.3816,
      "step": 2845
    },
    {
      "epoch": 0.6468181818181818,
      "grad_norm": 0.05756933614611626,
      "learning_rate": 7.071672354948806e-05,
      "loss": 0.3543,
      "step": 2846
    },
    {
      "epoch": 0.6470454545454546,
      "grad_norm": 0.04411180689930916,
      "learning_rate": 7.06712172923777e-05,
      "loss": 0.3001,
      "step": 2847
    },
    {
      "epoch": 0.6472727272727272,
      "grad_norm": 0.0473790168762207,
      "learning_rate": 7.062571103526735e-05,
      "loss": 0.3244,
      "step": 2848
    },
    {
      "epoch": 0.6475,
      "grad_norm": 0.04281078279018402,
      "learning_rate": 7.0580204778157e-05,
      "loss": 0.3151,
      "step": 2849
    },
    {
      "epoch": 0.6477272727272727,
      "grad_norm": 0.033301059156656265,
      "learning_rate": 7.053469852104665e-05,
      "loss": 0.2673,
      "step": 2850
    },
    {
      "epoch": 0.6479545454545454,
      "grad_norm": 0.042777206748723984,
      "learning_rate": 7.04891922639363e-05,
      "loss": 0.3177,
      "step": 2851
    },
    {
      "epoch": 0.6481818181818182,
      "grad_norm": 0.04482106491923332,
      "learning_rate": 7.044368600682593e-05,
      "loss": 0.353,
      "step": 2852
    },
    {
      "epoch": 0.6484090909090909,
      "grad_norm": 0.05930124223232269,
      "learning_rate": 7.039817974971559e-05,
      "loss": 0.3954,
      "step": 2853
    },
    {
      "epoch": 0.6486363636363637,
      "grad_norm": 0.05336655303835869,
      "learning_rate": 7.035267349260524e-05,
      "loss": 0.3682,
      "step": 2854
    },
    {
      "epoch": 0.6488636363636363,
      "grad_norm": 0.04577713459730148,
      "learning_rate": 7.030716723549489e-05,
      "loss": 0.3458,
      "step": 2855
    },
    {
      "epoch": 0.649090909090909,
      "grad_norm": 0.04111273214221001,
      "learning_rate": 7.026166097838453e-05,
      "loss": 0.3213,
      "step": 2856
    },
    {
      "epoch": 0.6493181818181818,
      "grad_norm": 0.04195830598473549,
      "learning_rate": 7.021615472127417e-05,
      "loss": 0.2883,
      "step": 2857
    },
    {
      "epoch": 0.6495454545454545,
      "grad_norm": 0.04853122681379318,
      "learning_rate": 7.017064846416382e-05,
      "loss": 0.3577,
      "step": 2858
    },
    {
      "epoch": 0.6497727272727273,
      "grad_norm": 0.03137439861893654,
      "learning_rate": 7.012514220705348e-05,
      "loss": 0.2948,
      "step": 2859
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.050342854112386703,
      "learning_rate": 7.007963594994313e-05,
      "loss": 0.352,
      "step": 2860
    },
    {
      "epoch": 0.6502272727272728,
      "grad_norm": 0.042265068739652634,
      "learning_rate": 7.003412969283277e-05,
      "loss": 0.3447,
      "step": 2861
    },
    {
      "epoch": 0.6504545454545455,
      "grad_norm": 0.05316461622714996,
      "learning_rate": 6.998862343572241e-05,
      "loss": 0.3561,
      "step": 2862
    },
    {
      "epoch": 0.6506818181818181,
      "grad_norm": 0.051369357854127884,
      "learning_rate": 6.994311717861206e-05,
      "loss": 0.3954,
      "step": 2863
    },
    {
      "epoch": 0.6509090909090909,
      "grad_norm": 0.041197311133146286,
      "learning_rate": 6.989761092150171e-05,
      "loss": 0.3465,
      "step": 2864
    },
    {
      "epoch": 0.6511363636363636,
      "grad_norm": 0.045625004917383194,
      "learning_rate": 6.985210466439137e-05,
      "loss": 0.343,
      "step": 2865
    },
    {
      "epoch": 0.6513636363636364,
      "grad_norm": 0.04181218147277832,
      "learning_rate": 6.9806598407281e-05,
      "loss": 0.2824,
      "step": 2866
    },
    {
      "epoch": 0.6515909090909091,
      "grad_norm": 0.04256588593125343,
      "learning_rate": 6.976109215017064e-05,
      "loss": 0.3316,
      "step": 2867
    },
    {
      "epoch": 0.6518181818181819,
      "grad_norm": 0.040261972695589066,
      "learning_rate": 6.97155858930603e-05,
      "loss": 0.3239,
      "step": 2868
    },
    {
      "epoch": 0.6520454545454546,
      "grad_norm": 0.03986216336488724,
      "learning_rate": 6.967007963594995e-05,
      "loss": 0.2945,
      "step": 2869
    },
    {
      "epoch": 0.6522727272727272,
      "grad_norm": 0.03595683351159096,
      "learning_rate": 6.96245733788396e-05,
      "loss": 0.2695,
      "step": 2870
    },
    {
      "epoch": 0.6525,
      "grad_norm": 0.04857223108410835,
      "learning_rate": 6.957906712172924e-05,
      "loss": 0.3467,
      "step": 2871
    },
    {
      "epoch": 0.6527272727272727,
      "grad_norm": 0.055455636233091354,
      "learning_rate": 6.953356086461888e-05,
      "loss": 0.3831,
      "step": 2872
    },
    {
      "epoch": 0.6529545454545455,
      "grad_norm": 0.04169510677456856,
      "learning_rate": 6.948805460750853e-05,
      "loss": 0.2809,
      "step": 2873
    },
    {
      "epoch": 0.6531818181818182,
      "grad_norm": 0.035690512508153915,
      "learning_rate": 6.944254835039819e-05,
      "loss": 0.2665,
      "step": 2874
    },
    {
      "epoch": 0.6534090909090909,
      "grad_norm": 0.038178037852048874,
      "learning_rate": 6.939704209328784e-05,
      "loss": 0.2803,
      "step": 2875
    },
    {
      "epoch": 0.6536363636363637,
      "grad_norm": 0.03840872272849083,
      "learning_rate": 6.935153583617748e-05,
      "loss": 0.3061,
      "step": 2876
    },
    {
      "epoch": 0.6538636363636363,
      "grad_norm": 0.043458834290504456,
      "learning_rate": 6.930602957906712e-05,
      "loss": 0.2966,
      "step": 2877
    },
    {
      "epoch": 0.6540909090909091,
      "grad_norm": 0.05503400042653084,
      "learning_rate": 6.926052332195677e-05,
      "loss": 0.3754,
      "step": 2878
    },
    {
      "epoch": 0.6543181818181818,
      "grad_norm": 0.04481200501322746,
      "learning_rate": 6.921501706484642e-05,
      "loss": 0.339,
      "step": 2879
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.03278643637895584,
      "learning_rate": 6.916951080773608e-05,
      "loss": 0.2843,
      "step": 2880
    },
    {
      "epoch": 0.6547727272727273,
      "grad_norm": 0.04120047390460968,
      "learning_rate": 6.912400455062572e-05,
      "loss": 0.294,
      "step": 2881
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.042742807418107986,
      "learning_rate": 6.907849829351536e-05,
      "loss": 0.2869,
      "step": 2882
    },
    {
      "epoch": 0.6552272727272728,
      "grad_norm": 0.04624083638191223,
      "learning_rate": 6.903299203640501e-05,
      "loss": 0.3764,
      "step": 2883
    },
    {
      "epoch": 0.6554545454545454,
      "grad_norm": 0.05257001146674156,
      "learning_rate": 6.898748577929466e-05,
      "loss": 0.381,
      "step": 2884
    },
    {
      "epoch": 0.6556818181818181,
      "grad_norm": 0.06396210938692093,
      "learning_rate": 6.894197952218431e-05,
      "loss": 0.4131,
      "step": 2885
    },
    {
      "epoch": 0.6559090909090909,
      "grad_norm": 0.039051495492458344,
      "learning_rate": 6.889647326507395e-05,
      "loss": 0.3339,
      "step": 2886
    },
    {
      "epoch": 0.6561363636363636,
      "grad_norm": 0.05156106501817703,
      "learning_rate": 6.885096700796359e-05,
      "loss": 0.3358,
      "step": 2887
    },
    {
      "epoch": 0.6563636363636364,
      "grad_norm": 0.057149242609739304,
      "learning_rate": 6.880546075085324e-05,
      "loss": 0.393,
      "step": 2888
    },
    {
      "epoch": 0.6565909090909091,
      "grad_norm": 0.04637680575251579,
      "learning_rate": 6.87599544937429e-05,
      "loss": 0.3316,
      "step": 2889
    },
    {
      "epoch": 0.6568181818181819,
      "grad_norm": 0.04905783012509346,
      "learning_rate": 6.871444823663255e-05,
      "loss": 0.3729,
      "step": 2890
    },
    {
      "epoch": 0.6570454545454546,
      "grad_norm": 0.052311815321445465,
      "learning_rate": 6.866894197952219e-05,
      "loss": 0.3139,
      "step": 2891
    },
    {
      "epoch": 0.6572727272727272,
      "grad_norm": 0.04739370942115784,
      "learning_rate": 6.862343572241183e-05,
      "loss": 0.3453,
      "step": 2892
    },
    {
      "epoch": 0.6575,
      "grad_norm": 0.05848636105656624,
      "learning_rate": 6.857792946530148e-05,
      "loss": 0.3924,
      "step": 2893
    },
    {
      "epoch": 0.6577272727272727,
      "grad_norm": 0.04426657035946846,
      "learning_rate": 6.853242320819113e-05,
      "loss": 0.3094,
      "step": 2894
    },
    {
      "epoch": 0.6579545454545455,
      "grad_norm": 0.059848710894584656,
      "learning_rate": 6.848691695108079e-05,
      "loss": 0.3467,
      "step": 2895
    },
    {
      "epoch": 0.6581818181818182,
      "grad_norm": 0.03709283843636513,
      "learning_rate": 6.844141069397043e-05,
      "loss": 0.3412,
      "step": 2896
    },
    {
      "epoch": 0.6584090909090909,
      "grad_norm": 0.06190615892410278,
      "learning_rate": 6.839590443686007e-05,
      "loss": 0.3706,
      "step": 2897
    },
    {
      "epoch": 0.6586363636363637,
      "grad_norm": 0.05710068345069885,
      "learning_rate": 6.835039817974972e-05,
      "loss": 0.3447,
      "step": 2898
    },
    {
      "epoch": 0.6588636363636363,
      "grad_norm": 0.04736001044511795,
      "learning_rate": 6.830489192263937e-05,
      "loss": 0.3477,
      "step": 2899
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.045741572976112366,
      "learning_rate": 6.825938566552902e-05,
      "loss": 0.2943,
      "step": 2900
    },
    {
      "epoch": 0.6593181818181818,
      "grad_norm": 0.03721242770552635,
      "learning_rate": 6.821387940841866e-05,
      "loss": 0.2951,
      "step": 2901
    },
    {
      "epoch": 0.6595454545454545,
      "grad_norm": 0.048145752400159836,
      "learning_rate": 6.81683731513083e-05,
      "loss": 0.3315,
      "step": 2902
    },
    {
      "epoch": 0.6597727272727273,
      "grad_norm": 0.06498754769563675,
      "learning_rate": 6.812286689419796e-05,
      "loss": 0.3629,
      "step": 2903
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.03843231499195099,
      "learning_rate": 6.807736063708761e-05,
      "loss": 0.2956,
      "step": 2904
    },
    {
      "epoch": 0.6602272727272728,
      "grad_norm": 0.0470726452767849,
      "learning_rate": 6.803185437997726e-05,
      "loss": 0.348,
      "step": 2905
    },
    {
      "epoch": 0.6604545454545454,
      "grad_norm": 0.04070558026432991,
      "learning_rate": 6.79863481228669e-05,
      "loss": 0.3442,
      "step": 2906
    },
    {
      "epoch": 0.6606818181818181,
      "grad_norm": 0.043805211782455444,
      "learning_rate": 6.794084186575654e-05,
      "loss": 0.3158,
      "step": 2907
    },
    {
      "epoch": 0.6609090909090909,
      "grad_norm": 0.061071135103702545,
      "learning_rate": 6.789533560864619e-05,
      "loss": 0.363,
      "step": 2908
    },
    {
      "epoch": 0.6611363636363636,
      "grad_norm": 0.04296181723475456,
      "learning_rate": 6.784982935153585e-05,
      "loss": 0.3457,
      "step": 2909
    },
    {
      "epoch": 0.6613636363636364,
      "grad_norm": 0.030854806303977966,
      "learning_rate": 6.780432309442548e-05,
      "loss": 0.2679,
      "step": 2910
    },
    {
      "epoch": 0.6615909090909091,
      "grad_norm": 0.04293530434370041,
      "learning_rate": 6.775881683731514e-05,
      "loss": 0.3273,
      "step": 2911
    },
    {
      "epoch": 0.6618181818181819,
      "grad_norm": 0.04383622482419014,
      "learning_rate": 6.771331058020478e-05,
      "loss": 0.2958,
      "step": 2912
    },
    {
      "epoch": 0.6620454545454545,
      "grad_norm": 0.06309066712856293,
      "learning_rate": 6.766780432309443e-05,
      "loss": 0.3559,
      "step": 2913
    },
    {
      "epoch": 0.6622727272727272,
      "grad_norm": 0.04351595416665077,
      "learning_rate": 6.762229806598408e-05,
      "loss": 0.2954,
      "step": 2914
    },
    {
      "epoch": 0.6625,
      "grad_norm": 0.04581614211201668,
      "learning_rate": 6.757679180887372e-05,
      "loss": 0.3451,
      "step": 2915
    },
    {
      "epoch": 0.6627272727272727,
      "grad_norm": 0.057472456246614456,
      "learning_rate": 6.753128555176337e-05,
      "loss": 0.3799,
      "step": 2916
    },
    {
      "epoch": 0.6629545454545455,
      "grad_norm": 0.05035492405295372,
      "learning_rate": 6.748577929465301e-05,
      "loss": 0.3305,
      "step": 2917
    },
    {
      "epoch": 0.6631818181818182,
      "grad_norm": 0.04916174337267876,
      "learning_rate": 6.744027303754267e-05,
      "loss": 0.3524,
      "step": 2918
    },
    {
      "epoch": 0.663409090909091,
      "grad_norm": 0.03632113337516785,
      "learning_rate": 6.739476678043232e-05,
      "loss": 0.3012,
      "step": 2919
    },
    {
      "epoch": 0.6636363636363637,
      "grad_norm": 0.04434434697031975,
      "learning_rate": 6.734926052332196e-05,
      "loss": 0.3553,
      "step": 2920
    },
    {
      "epoch": 0.6638636363636363,
      "grad_norm": 0.04762091487646103,
      "learning_rate": 6.730375426621161e-05,
      "loss": 0.3134,
      "step": 2921
    },
    {
      "epoch": 0.6640909090909091,
      "grad_norm": 0.052004002034664154,
      "learning_rate": 6.725824800910125e-05,
      "loss": 0.3553,
      "step": 2922
    },
    {
      "epoch": 0.6643181818181818,
      "grad_norm": 0.045570194721221924,
      "learning_rate": 6.72127417519909e-05,
      "loss": 0.3653,
      "step": 2923
    },
    {
      "epoch": 0.6645454545454546,
      "grad_norm": 0.04587901383638382,
      "learning_rate": 6.716723549488056e-05,
      "loss": 0.3523,
      "step": 2924
    },
    {
      "epoch": 0.6647727272727273,
      "grad_norm": 0.03744888678193092,
      "learning_rate": 6.71217292377702e-05,
      "loss": 0.3533,
      "step": 2925
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.0557846873998642,
      "learning_rate": 6.707622298065984e-05,
      "loss": 0.389,
      "step": 2926
    },
    {
      "epoch": 0.6652272727272728,
      "grad_norm": 0.04403961822390556,
      "learning_rate": 6.703071672354949e-05,
      "loss": 0.3339,
      "step": 2927
    },
    {
      "epoch": 0.6654545454545454,
      "grad_norm": 0.03642701357603073,
      "learning_rate": 6.698521046643914e-05,
      "loss": 0.3139,
      "step": 2928
    },
    {
      "epoch": 0.6656818181818182,
      "grad_norm": 0.038636162877082825,
      "learning_rate": 6.69397042093288e-05,
      "loss": 0.2746,
      "step": 2929
    },
    {
      "epoch": 0.6659090909090909,
      "grad_norm": 0.042870763689279556,
      "learning_rate": 6.689419795221843e-05,
      "loss": 0.3238,
      "step": 2930
    },
    {
      "epoch": 0.6661363636363636,
      "grad_norm": 0.041139744222164154,
      "learning_rate": 6.684869169510807e-05,
      "loss": 0.3154,
      "step": 2931
    },
    {
      "epoch": 0.6663636363636364,
      "grad_norm": 0.06431057304143906,
      "learning_rate": 6.680318543799772e-05,
      "loss": 0.3834,
      "step": 2932
    },
    {
      "epoch": 0.6665909090909091,
      "grad_norm": 0.04747094213962555,
      "learning_rate": 6.675767918088738e-05,
      "loss": 0.292,
      "step": 2933
    },
    {
      "epoch": 0.6668181818181819,
      "grad_norm": 0.035055745393037796,
      "learning_rate": 6.671217292377703e-05,
      "loss": 0.2652,
      "step": 2934
    },
    {
      "epoch": 0.6670454545454545,
      "grad_norm": 0.04372799023985863,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.3168,
      "step": 2935
    },
    {
      "epoch": 0.6672727272727272,
      "grad_norm": 0.048069607466459274,
      "learning_rate": 6.662116040955631e-05,
      "loss": 0.3706,
      "step": 2936
    },
    {
      "epoch": 0.6675,
      "grad_norm": 0.045292939990758896,
      "learning_rate": 6.657565415244596e-05,
      "loss": 0.3758,
      "step": 2937
    },
    {
      "epoch": 0.6677272727272727,
      "grad_norm": 0.0427582822740078,
      "learning_rate": 6.653014789533561e-05,
      "loss": 0.3039,
      "step": 2938
    },
    {
      "epoch": 0.6679545454545455,
      "grad_norm": 0.037753116339445114,
      "learning_rate": 6.648464163822527e-05,
      "loss": 0.299,
      "step": 2939
    },
    {
      "epoch": 0.6681818181818182,
      "grad_norm": 0.04118061810731888,
      "learning_rate": 6.643913538111491e-05,
      "loss": 0.3097,
      "step": 2940
    },
    {
      "epoch": 0.668409090909091,
      "grad_norm": 0.03582935780286789,
      "learning_rate": 6.639362912400455e-05,
      "loss": 0.3053,
      "step": 2941
    },
    {
      "epoch": 0.6686363636363636,
      "grad_norm": 0.06290178745985031,
      "learning_rate": 6.63481228668942e-05,
      "loss": 0.3797,
      "step": 2942
    },
    {
      "epoch": 0.6688636363636363,
      "grad_norm": 0.03780393674969673,
      "learning_rate": 6.630261660978385e-05,
      "loss": 0.3185,
      "step": 2943
    },
    {
      "epoch": 0.6690909090909091,
      "grad_norm": 0.04176173359155655,
      "learning_rate": 6.625711035267349e-05,
      "loss": 0.3207,
      "step": 2944
    },
    {
      "epoch": 0.6693181818181818,
      "grad_norm": 0.04364683851599693,
      "learning_rate": 6.621160409556314e-05,
      "loss": 0.326,
      "step": 2945
    },
    {
      "epoch": 0.6695454545454546,
      "grad_norm": 0.04026846960186958,
      "learning_rate": 6.616609783845278e-05,
      "loss": 0.3313,
      "step": 2946
    },
    {
      "epoch": 0.6697727272727273,
      "grad_norm": 0.046641990542411804,
      "learning_rate": 6.612059158134244e-05,
      "loss": 0.3874,
      "step": 2947
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.0345272459089756,
      "learning_rate": 6.607508532423209e-05,
      "loss": 0.3004,
      "step": 2948
    },
    {
      "epoch": 0.6702272727272728,
      "grad_norm": 0.03992011398077011,
      "learning_rate": 6.602957906712173e-05,
      "loss": 0.3188,
      "step": 2949
    },
    {
      "epoch": 0.6704545454545454,
      "grad_norm": 0.0341065488755703,
      "learning_rate": 6.598407281001138e-05,
      "loss": 0.2932,
      "step": 2950
    },
    {
      "epoch": 0.6706818181818182,
      "grad_norm": 0.03986286744475365,
      "learning_rate": 6.593856655290102e-05,
      "loss": 0.3375,
      "step": 2951
    },
    {
      "epoch": 0.6709090909090909,
      "grad_norm": 0.03806113079190254,
      "learning_rate": 6.589306029579067e-05,
      "loss": 0.2758,
      "step": 2952
    },
    {
      "epoch": 0.6711363636363636,
      "grad_norm": 0.04723039269447327,
      "learning_rate": 6.584755403868033e-05,
      "loss": 0.3735,
      "step": 2953
    },
    {
      "epoch": 0.6713636363636364,
      "grad_norm": 0.05021679401397705,
      "learning_rate": 6.580204778156997e-05,
      "loss": 0.4096,
      "step": 2954
    },
    {
      "epoch": 0.6715909090909091,
      "grad_norm": 0.03725738078355789,
      "learning_rate": 6.575654152445962e-05,
      "loss": 0.3064,
      "step": 2955
    },
    {
      "epoch": 0.6718181818181819,
      "grad_norm": 0.04612589254975319,
      "learning_rate": 6.571103526734926e-05,
      "loss": 0.295,
      "step": 2956
    },
    {
      "epoch": 0.6720454545454545,
      "grad_norm": 0.03191820904612541,
      "learning_rate": 6.566552901023891e-05,
      "loss": 0.2994,
      "step": 2957
    },
    {
      "epoch": 0.6722727272727272,
      "grad_norm": 0.04837524890899658,
      "learning_rate": 6.562002275312856e-05,
      "loss": 0.4015,
      "step": 2958
    },
    {
      "epoch": 0.6725,
      "grad_norm": 0.0345211997628212,
      "learning_rate": 6.55745164960182e-05,
      "loss": 0.2711,
      "step": 2959
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.04709383845329285,
      "learning_rate": 6.552901023890785e-05,
      "loss": 0.3644,
      "step": 2960
    },
    {
      "epoch": 0.6729545454545455,
      "grad_norm": 0.030950874090194702,
      "learning_rate": 6.54835039817975e-05,
      "loss": 0.2797,
      "step": 2961
    },
    {
      "epoch": 0.6731818181818182,
      "grad_norm": 0.05786209926009178,
      "learning_rate": 6.543799772468715e-05,
      "loss": 0.3799,
      "step": 2962
    },
    {
      "epoch": 0.673409090909091,
      "grad_norm": 0.04132579267024994,
      "learning_rate": 6.53924914675768e-05,
      "loss": 0.3257,
      "step": 2963
    },
    {
      "epoch": 0.6736363636363636,
      "grad_norm": 0.05077922344207764,
      "learning_rate": 6.534698521046644e-05,
      "loss": 0.3543,
      "step": 2964
    },
    {
      "epoch": 0.6738636363636363,
      "grad_norm": 0.04217836260795593,
      "learning_rate": 6.530147895335609e-05,
      "loss": 0.369,
      "step": 2965
    },
    {
      "epoch": 0.6740909090909091,
      "grad_norm": 0.03840663656592369,
      "learning_rate": 6.525597269624573e-05,
      "loss": 0.2731,
      "step": 2966
    },
    {
      "epoch": 0.6743181818181818,
      "grad_norm": 0.044030021876096725,
      "learning_rate": 6.521046643913538e-05,
      "loss": 0.353,
      "step": 2967
    },
    {
      "epoch": 0.6745454545454546,
      "grad_norm": 0.04492020979523659,
      "learning_rate": 6.516496018202504e-05,
      "loss": 0.36,
      "step": 2968
    },
    {
      "epoch": 0.6747727272727273,
      "grad_norm": 0.028128428384661674,
      "learning_rate": 6.511945392491468e-05,
      "loss": 0.2599,
      "step": 2969
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.04749785363674164,
      "learning_rate": 6.507394766780433e-05,
      "loss": 0.3687,
      "step": 2970
    },
    {
      "epoch": 0.6752272727272727,
      "grad_norm": 0.04005297273397446,
      "learning_rate": 6.502844141069397e-05,
      "loss": 0.3087,
      "step": 2971
    },
    {
      "epoch": 0.6754545454545454,
      "grad_norm": 0.04574259743094444,
      "learning_rate": 6.498293515358362e-05,
      "loss": 0.3442,
      "step": 2972
    },
    {
      "epoch": 0.6756818181818182,
      "grad_norm": 0.030957315117120743,
      "learning_rate": 6.493742889647327e-05,
      "loss": 0.2618,
      "step": 2973
    },
    {
      "epoch": 0.6759090909090909,
      "grad_norm": 0.03911226987838745,
      "learning_rate": 6.489192263936291e-05,
      "loss": 0.3291,
      "step": 2974
    },
    {
      "epoch": 0.6761363636363636,
      "grad_norm": 0.0388355553150177,
      "learning_rate": 6.484641638225257e-05,
      "loss": 0.282,
      "step": 2975
    },
    {
      "epoch": 0.6763636363636364,
      "grad_norm": 0.04295816272497177,
      "learning_rate": 6.48009101251422e-05,
      "loss": 0.3131,
      "step": 2976
    },
    {
      "epoch": 0.6765909090909091,
      "grad_norm": 0.034908127039670944,
      "learning_rate": 6.475540386803186e-05,
      "loss": 0.2357,
      "step": 2977
    },
    {
      "epoch": 0.6768181818181818,
      "grad_norm": 0.041562553495168686,
      "learning_rate": 6.47098976109215e-05,
      "loss": 0.3461,
      "step": 2978
    },
    {
      "epoch": 0.6770454545454545,
      "grad_norm": 0.03978218138217926,
      "learning_rate": 6.466439135381115e-05,
      "loss": 0.2803,
      "step": 2979
    },
    {
      "epoch": 0.6772727272727272,
      "grad_norm": 0.031292952597141266,
      "learning_rate": 6.46188850967008e-05,
      "loss": 0.3179,
      "step": 2980
    },
    {
      "epoch": 0.6775,
      "grad_norm": 0.04057624936103821,
      "learning_rate": 6.457337883959044e-05,
      "loss": 0.3207,
      "step": 2981
    },
    {
      "epoch": 0.6777272727272727,
      "grad_norm": 0.053892944008111954,
      "learning_rate": 6.45278725824801e-05,
      "loss": 0.3833,
      "step": 2982
    },
    {
      "epoch": 0.6779545454545455,
      "grad_norm": 0.04781254008412361,
      "learning_rate": 6.448236632536973e-05,
      "loss": 0.427,
      "step": 2983
    },
    {
      "epoch": 0.6781818181818182,
      "grad_norm": 0.04851337894797325,
      "learning_rate": 6.443686006825939e-05,
      "loss": 0.3488,
      "step": 2984
    },
    {
      "epoch": 0.678409090909091,
      "grad_norm": 0.04182145372033119,
      "learning_rate": 6.439135381114904e-05,
      "loss": 0.2943,
      "step": 2985
    },
    {
      "epoch": 0.6786363636363636,
      "grad_norm": 0.04486522078514099,
      "learning_rate": 6.434584755403868e-05,
      "loss": 0.3111,
      "step": 2986
    },
    {
      "epoch": 0.6788636363636363,
      "grad_norm": 0.04797576740384102,
      "learning_rate": 6.430034129692833e-05,
      "loss": 0.3265,
      "step": 2987
    },
    {
      "epoch": 0.6790909090909091,
      "grad_norm": 0.028563471511006355,
      "learning_rate": 6.425483503981797e-05,
      "loss": 0.2644,
      "step": 2988
    },
    {
      "epoch": 0.6793181818181818,
      "grad_norm": 0.05209807679057121,
      "learning_rate": 6.420932878270762e-05,
      "loss": 0.3578,
      "step": 2989
    },
    {
      "epoch": 0.6795454545454546,
      "grad_norm": 0.03969042748212814,
      "learning_rate": 6.416382252559728e-05,
      "loss": 0.2852,
      "step": 2990
    },
    {
      "epoch": 0.6797727272727273,
      "grad_norm": 0.05822049826383591,
      "learning_rate": 6.411831626848692e-05,
      "loss": 0.3585,
      "step": 2991
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.04135769605636597,
      "learning_rate": 6.407281001137657e-05,
      "loss": 0.3109,
      "step": 2992
    },
    {
      "epoch": 0.6802272727272727,
      "grad_norm": 0.03383467718958855,
      "learning_rate": 6.402730375426621e-05,
      "loss": 0.2806,
      "step": 2993
    },
    {
      "epoch": 0.6804545454545454,
      "grad_norm": 0.03541249781847,
      "learning_rate": 6.398179749715586e-05,
      "loss": 0.2908,
      "step": 2994
    },
    {
      "epoch": 0.6806818181818182,
      "grad_norm": 0.04515707492828369,
      "learning_rate": 6.393629124004551e-05,
      "loss": 0.3432,
      "step": 2995
    },
    {
      "epoch": 0.6809090909090909,
      "grad_norm": 0.047141317278146744,
      "learning_rate": 6.389078498293515e-05,
      "loss": 0.3277,
      "step": 2996
    },
    {
      "epoch": 0.6811363636363637,
      "grad_norm": 0.05064641311764717,
      "learning_rate": 6.38452787258248e-05,
      "loss": 0.3269,
      "step": 2997
    },
    {
      "epoch": 0.6813636363636364,
      "grad_norm": 0.043479111045598984,
      "learning_rate": 6.379977246871445e-05,
      "loss": 0.3314,
      "step": 2998
    },
    {
      "epoch": 0.6815909090909091,
      "grad_norm": 0.030383329838514328,
      "learning_rate": 6.37542662116041e-05,
      "loss": 0.2718,
      "step": 2999
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.04602197930216789,
      "learning_rate": 6.370875995449375e-05,
      "loss": 0.3738,
      "step": 3000
    },
    {
      "epoch": 0.6820454545454545,
      "grad_norm": 0.0467560850083828,
      "learning_rate": 6.366325369738339e-05,
      "loss": 0.3556,
      "step": 3001
    },
    {
      "epoch": 0.6822727272727273,
      "grad_norm": 0.04588838294148445,
      "learning_rate": 6.361774744027304e-05,
      "loss": 0.324,
      "step": 3002
    },
    {
      "epoch": 0.6825,
      "grad_norm": 0.05154119059443474,
      "learning_rate": 6.357224118316268e-05,
      "loss": 0.337,
      "step": 3003
    },
    {
      "epoch": 0.6827272727272727,
      "grad_norm": 0.04925146698951721,
      "learning_rate": 6.352673492605233e-05,
      "loss": 0.4163,
      "step": 3004
    },
    {
      "epoch": 0.6829545454545455,
      "grad_norm": 0.06674438714981079,
      "learning_rate": 6.348122866894199e-05,
      "loss": 0.4115,
      "step": 3005
    },
    {
      "epoch": 0.6831818181818182,
      "grad_norm": 0.05426591634750366,
      "learning_rate": 6.343572241183163e-05,
      "loss": 0.3862,
      "step": 3006
    },
    {
      "epoch": 0.6834090909090909,
      "grad_norm": 0.03855019435286522,
      "learning_rate": 6.339021615472128e-05,
      "loss": 0.3428,
      "step": 3007
    },
    {
      "epoch": 0.6836363636363636,
      "grad_norm": 0.034126292914152145,
      "learning_rate": 6.334470989761092e-05,
      "loss": 0.296,
      "step": 3008
    },
    {
      "epoch": 0.6838636363636363,
      "grad_norm": 0.040534935891628265,
      "learning_rate": 6.329920364050057e-05,
      "loss": 0.3283,
      "step": 3009
    },
    {
      "epoch": 0.6840909090909091,
      "grad_norm": 0.047299403697252274,
      "learning_rate": 6.325369738339022e-05,
      "loss": 0.3604,
      "step": 3010
    },
    {
      "epoch": 0.6843181818181818,
      "grad_norm": 0.036074381321668625,
      "learning_rate": 6.320819112627986e-05,
      "loss": 0.2603,
      "step": 3011
    },
    {
      "epoch": 0.6845454545454546,
      "grad_norm": 0.047468751668930054,
      "learning_rate": 6.31626848691695e-05,
      "loss": 0.3312,
      "step": 3012
    },
    {
      "epoch": 0.6847727272727273,
      "grad_norm": 0.03921429440379143,
      "learning_rate": 6.311717861205916e-05,
      "loss": 0.3455,
      "step": 3013
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.047058358788490295,
      "learning_rate": 6.307167235494881e-05,
      "loss": 0.3388,
      "step": 3014
    },
    {
      "epoch": 0.6852272727272727,
      "grad_norm": 0.0435125008225441,
      "learning_rate": 6.302616609783846e-05,
      "loss": 0.3441,
      "step": 3015
    },
    {
      "epoch": 0.6854545454545454,
      "grad_norm": 0.03767762705683708,
      "learning_rate": 6.29806598407281e-05,
      "loss": 0.3446,
      "step": 3016
    },
    {
      "epoch": 0.6856818181818182,
      "grad_norm": 0.043884776532649994,
      "learning_rate": 6.293515358361774e-05,
      "loss": 0.3193,
      "step": 3017
    },
    {
      "epoch": 0.6859090909090909,
      "grad_norm": 0.035926029086112976,
      "learning_rate": 6.288964732650739e-05,
      "loss": 0.2357,
      "step": 3018
    },
    {
      "epoch": 0.6861363636363637,
      "grad_norm": 0.04829630255699158,
      "learning_rate": 6.284414106939705e-05,
      "loss": 0.3565,
      "step": 3019
    },
    {
      "epoch": 0.6863636363636364,
      "grad_norm": 0.06019017472863197,
      "learning_rate": 6.27986348122867e-05,
      "loss": 0.4042,
      "step": 3020
    },
    {
      "epoch": 0.6865909090909091,
      "grad_norm": 0.04950113594532013,
      "learning_rate": 6.275312855517634e-05,
      "loss": 0.3537,
      "step": 3021
    },
    {
      "epoch": 0.6868181818181818,
      "grad_norm": 0.04469682648777962,
      "learning_rate": 6.270762229806598e-05,
      "loss": 0.333,
      "step": 3022
    },
    {
      "epoch": 0.6870454545454545,
      "grad_norm": 0.03374237194657326,
      "learning_rate": 6.266211604095563e-05,
      "loss": 0.285,
      "step": 3023
    },
    {
      "epoch": 0.6872727272727273,
      "grad_norm": 0.055702563375234604,
      "learning_rate": 6.261660978384528e-05,
      "loss": 0.4386,
      "step": 3024
    },
    {
      "epoch": 0.6875,
      "grad_norm": 0.055257175117731094,
      "learning_rate": 6.257110352673494e-05,
      "loss": 0.3677,
      "step": 3025
    },
    {
      "epoch": 0.6877272727272727,
      "grad_norm": 0.04962689429521561,
      "learning_rate": 6.252559726962457e-05,
      "loss": 0.3306,
      "step": 3026
    },
    {
      "epoch": 0.6879545454545455,
      "grad_norm": 0.04755143076181412,
      "learning_rate": 6.248009101251421e-05,
      "loss": 0.3674,
      "step": 3027
    },
    {
      "epoch": 0.6881818181818182,
      "grad_norm": 0.04052084684371948,
      "learning_rate": 6.243458475540387e-05,
      "loss": 0.327,
      "step": 3028
    },
    {
      "epoch": 0.6884090909090909,
      "grad_norm": 0.05612554773688316,
      "learning_rate": 6.238907849829352e-05,
      "loss": 0.3504,
      "step": 3029
    },
    {
      "epoch": 0.6886363636363636,
      "grad_norm": 0.044430505484342575,
      "learning_rate": 6.234357224118317e-05,
      "loss": 0.3113,
      "step": 3030
    },
    {
      "epoch": 0.6888636363636363,
      "grad_norm": 0.04169986769556999,
      "learning_rate": 6.229806598407281e-05,
      "loss": 0.3244,
      "step": 3031
    },
    {
      "epoch": 0.6890909090909091,
      "grad_norm": 0.04744824394583702,
      "learning_rate": 6.225255972696245e-05,
      "loss": 0.3166,
      "step": 3032
    },
    {
      "epoch": 0.6893181818181818,
      "grad_norm": 0.04598201438784599,
      "learning_rate": 6.22070534698521e-05,
      "loss": 0.3698,
      "step": 3033
    },
    {
      "epoch": 0.6895454545454546,
      "grad_norm": 0.040005989372730255,
      "learning_rate": 6.216154721274176e-05,
      "loss": 0.2758,
      "step": 3034
    },
    {
      "epoch": 0.6897727272727273,
      "grad_norm": 0.050599850714206696,
      "learning_rate": 6.211604095563141e-05,
      "loss": 0.3691,
      "step": 3035
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.039134345948696136,
      "learning_rate": 6.207053469852105e-05,
      "loss": 0.3314,
      "step": 3036
    },
    {
      "epoch": 0.6902272727272727,
      "grad_norm": 0.03426176682114601,
      "learning_rate": 6.202502844141069e-05,
      "loss": 0.2672,
      "step": 3037
    },
    {
      "epoch": 0.6904545454545454,
      "grad_norm": 0.045508310198783875,
      "learning_rate": 6.197952218430034e-05,
      "loss": 0.3453,
      "step": 3038
    },
    {
      "epoch": 0.6906818181818182,
      "grad_norm": 0.03983089327812195,
      "learning_rate": 6.193401592719e-05,
      "loss": 0.3027,
      "step": 3039
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.03748992085456848,
      "learning_rate": 6.188850967007965e-05,
      "loss": 0.2935,
      "step": 3040
    },
    {
      "epoch": 0.6911363636363637,
      "grad_norm": 0.04343271628022194,
      "learning_rate": 6.184300341296929e-05,
      "loss": 0.3637,
      "step": 3041
    },
    {
      "epoch": 0.6913636363636364,
      "grad_norm": 0.0719372108578682,
      "learning_rate": 6.179749715585893e-05,
      "loss": 0.4121,
      "step": 3042
    },
    {
      "epoch": 0.6915909090909091,
      "grad_norm": 0.03739996999502182,
      "learning_rate": 6.175199089874858e-05,
      "loss": 0.3154,
      "step": 3043
    },
    {
      "epoch": 0.6918181818181818,
      "grad_norm": 0.0407349169254303,
      "learning_rate": 6.170648464163823e-05,
      "loss": 0.3408,
      "step": 3044
    },
    {
      "epoch": 0.6920454545454545,
      "grad_norm": 0.045247793197631836,
      "learning_rate": 6.166097838452788e-05,
      "loss": 0.3068,
      "step": 3045
    },
    {
      "epoch": 0.6922727272727273,
      "grad_norm": 0.047269295901060104,
      "learning_rate": 6.161547212741752e-05,
      "loss": 0.323,
      "step": 3046
    },
    {
      "epoch": 0.6925,
      "grad_norm": 0.04041052982211113,
      "learning_rate": 6.156996587030716e-05,
      "loss": 0.3241,
      "step": 3047
    },
    {
      "epoch": 0.6927272727272727,
      "grad_norm": 0.04238259419798851,
      "learning_rate": 6.152445961319682e-05,
      "loss": 0.3287,
      "step": 3048
    },
    {
      "epoch": 0.6929545454545455,
      "grad_norm": 0.033577658236026764,
      "learning_rate": 6.147895335608647e-05,
      "loss": 0.2891,
      "step": 3049
    },
    {
      "epoch": 0.6931818181818182,
      "grad_norm": 0.04540354385972023,
      "learning_rate": 6.143344709897612e-05,
      "loss": 0.329,
      "step": 3050
    },
    {
      "epoch": 0.6934090909090909,
      "grad_norm": 0.03945846110582352,
      "learning_rate": 6.138794084186576e-05,
      "loss": 0.2972,
      "step": 3051
    },
    {
      "epoch": 0.6936363636363636,
      "grad_norm": 0.03984808176755905,
      "learning_rate": 6.13424345847554e-05,
      "loss": 0.3687,
      "step": 3052
    },
    {
      "epoch": 0.6938636363636363,
      "grad_norm": 0.05303914472460747,
      "learning_rate": 6.129692832764505e-05,
      "loss": 0.4138,
      "step": 3053
    },
    {
      "epoch": 0.6940909090909091,
      "grad_norm": 0.036347877234220505,
      "learning_rate": 6.12514220705347e-05,
      "loss": 0.2722,
      "step": 3054
    },
    {
      "epoch": 0.6943181818181818,
      "grad_norm": 0.04871609807014465,
      "learning_rate": 6.120591581342436e-05,
      "loss": 0.3168,
      "step": 3055
    },
    {
      "epoch": 0.6945454545454546,
      "grad_norm": 0.03970422223210335,
      "learning_rate": 6.1160409556314e-05,
      "loss": 0.3267,
      "step": 3056
    },
    {
      "epoch": 0.6947727272727273,
      "grad_norm": 0.039732590317726135,
      "learning_rate": 6.111490329920364e-05,
      "loss": 0.3354,
      "step": 3057
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.05756215751171112,
      "learning_rate": 6.106939704209329e-05,
      "loss": 0.3609,
      "step": 3058
    },
    {
      "epoch": 0.6952272727272727,
      "grad_norm": 0.04450906440615654,
      "learning_rate": 6.102389078498294e-05,
      "loss": 0.3601,
      "step": 3059
    },
    {
      "epoch": 0.6954545454545454,
      "grad_norm": 0.05214545130729675,
      "learning_rate": 6.097838452787259e-05,
      "loss": 0.3631,
      "step": 3060
    },
    {
      "epoch": 0.6956818181818182,
      "grad_norm": 0.041906993836164474,
      "learning_rate": 6.093287827076223e-05,
      "loss": 0.3174,
      "step": 3061
    },
    {
      "epoch": 0.6959090909090909,
      "grad_norm": 0.04152935743331909,
      "learning_rate": 6.088737201365188e-05,
      "loss": 0.3049,
      "step": 3062
    },
    {
      "epoch": 0.6961363636363637,
      "grad_norm": 0.02868211641907692,
      "learning_rate": 6.0841865756541526e-05,
      "loss": 0.226,
      "step": 3063
    },
    {
      "epoch": 0.6963636363636364,
      "grad_norm": 0.046370238065719604,
      "learning_rate": 6.079635949943118e-05,
      "loss": 0.3124,
      "step": 3064
    },
    {
      "epoch": 0.696590909090909,
      "grad_norm": 0.051024485379457474,
      "learning_rate": 6.0750853242320825e-05,
      "loss": 0.3822,
      "step": 3065
    },
    {
      "epoch": 0.6968181818181818,
      "grad_norm": 0.042605817317962646,
      "learning_rate": 6.0705346985210464e-05,
      "loss": 0.3347,
      "step": 3066
    },
    {
      "epoch": 0.6970454545454545,
      "grad_norm": 0.05672384053468704,
      "learning_rate": 6.065984072810012e-05,
      "loss": 0.376,
      "step": 3067
    },
    {
      "epoch": 0.6972727272727273,
      "grad_norm": 0.05252623185515404,
      "learning_rate": 6.061433447098976e-05,
      "loss": 0.3227,
      "step": 3068
    },
    {
      "epoch": 0.6975,
      "grad_norm": 0.04621163010597229,
      "learning_rate": 6.0568828213879416e-05,
      "loss": 0.3323,
      "step": 3069
    },
    {
      "epoch": 0.6977272727272728,
      "grad_norm": 0.05391298234462738,
      "learning_rate": 6.052332195676906e-05,
      "loss": 0.3211,
      "step": 3070
    },
    {
      "epoch": 0.6979545454545455,
      "grad_norm": 0.03571013733744621,
      "learning_rate": 6.04778156996587e-05,
      "loss": 0.3134,
      "step": 3071
    },
    {
      "epoch": 0.6981818181818182,
      "grad_norm": 0.0463717095553875,
      "learning_rate": 6.0432309442548354e-05,
      "loss": 0.3849,
      "step": 3072
    },
    {
      "epoch": 0.6984090909090909,
      "grad_norm": 0.038959767669439316,
      "learning_rate": 6.0386803185438e-05,
      "loss": 0.3088,
      "step": 3073
    },
    {
      "epoch": 0.6986363636363636,
      "grad_norm": 0.04949076846241951,
      "learning_rate": 6.034129692832765e-05,
      "loss": 0.3806,
      "step": 3074
    },
    {
      "epoch": 0.6988636363636364,
      "grad_norm": 0.04489092528820038,
      "learning_rate": 6.02957906712173e-05,
      "loss": 0.3114,
      "step": 3075
    },
    {
      "epoch": 0.6990909090909091,
      "grad_norm": 0.042042236775159836,
      "learning_rate": 6.025028441410694e-05,
      "loss": 0.3205,
      "step": 3076
    },
    {
      "epoch": 0.6993181818181818,
      "grad_norm": 0.05363032594323158,
      "learning_rate": 6.020477815699659e-05,
      "loss": 0.347,
      "step": 3077
    },
    {
      "epoch": 0.6995454545454546,
      "grad_norm": 0.038992177695035934,
      "learning_rate": 6.015927189988624e-05,
      "loss": 0.3194,
      "step": 3078
    },
    {
      "epoch": 0.6997727272727273,
      "grad_norm": 0.04343625158071518,
      "learning_rate": 6.011376564277589e-05,
      "loss": 0.3418,
      "step": 3079
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04790128767490387,
      "learning_rate": 6.0068259385665536e-05,
      "loss": 0.3878,
      "step": 3080
    },
    {
      "epoch": 0.7002272727272727,
      "grad_norm": 0.037443000823259354,
      "learning_rate": 6.0022753128555175e-05,
      "loss": 0.3169,
      "step": 3081
    },
    {
      "epoch": 0.7004545454545454,
      "grad_norm": 0.04010668769478798,
      "learning_rate": 5.997724687144483e-05,
      "loss": 0.237,
      "step": 3082
    },
    {
      "epoch": 0.7006818181818182,
      "grad_norm": 0.03785908222198486,
      "learning_rate": 5.9931740614334474e-05,
      "loss": 0.3033,
      "step": 3083
    },
    {
      "epoch": 0.7009090909090909,
      "grad_norm": 0.04636668413877487,
      "learning_rate": 5.988623435722413e-05,
      "loss": 0.3471,
      "step": 3084
    },
    {
      "epoch": 0.7011363636363637,
      "grad_norm": 0.06225995346903801,
      "learning_rate": 5.9840728100113766e-05,
      "loss": 0.3995,
      "step": 3085
    },
    {
      "epoch": 0.7013636363636364,
      "grad_norm": 0.04166080802679062,
      "learning_rate": 5.979522184300341e-05,
      "loss": 0.2806,
      "step": 3086
    },
    {
      "epoch": 0.701590909090909,
      "grad_norm": 0.03494837135076523,
      "learning_rate": 5.9749715585893065e-05,
      "loss": 0.2591,
      "step": 3087
    },
    {
      "epoch": 0.7018181818181818,
      "grad_norm": 0.056171875447034836,
      "learning_rate": 5.970420932878271e-05,
      "loss": 0.3959,
      "step": 3088
    },
    {
      "epoch": 0.7020454545454545,
      "grad_norm": 0.03915070742368698,
      "learning_rate": 5.9658703071672364e-05,
      "loss": 0.3431,
      "step": 3089
    },
    {
      "epoch": 0.7022727272727273,
      "grad_norm": 0.034737806767225266,
      "learning_rate": 5.9613196814562e-05,
      "loss": 0.3182,
      "step": 3090
    },
    {
      "epoch": 0.7025,
      "grad_norm": 0.05287560448050499,
      "learning_rate": 5.956769055745165e-05,
      "loss": 0.3824,
      "step": 3091
    },
    {
      "epoch": 0.7027272727272728,
      "grad_norm": 0.04053273797035217,
      "learning_rate": 5.95221843003413e-05,
      "loss": 0.3588,
      "step": 3092
    },
    {
      "epoch": 0.7029545454545455,
      "grad_norm": 0.04133259132504463,
      "learning_rate": 5.947667804323095e-05,
      "loss": 0.3114,
      "step": 3093
    },
    {
      "epoch": 0.7031818181818181,
      "grad_norm": 0.04413623362779617,
      "learning_rate": 5.94311717861206e-05,
      "loss": 0.3527,
      "step": 3094
    },
    {
      "epoch": 0.7034090909090909,
      "grad_norm": 0.04899100959300995,
      "learning_rate": 5.938566552901024e-05,
      "loss": 0.3515,
      "step": 3095
    },
    {
      "epoch": 0.7036363636363636,
      "grad_norm": 0.03662437945604324,
      "learning_rate": 5.9340159271899886e-05,
      "loss": 0.3396,
      "step": 3096
    },
    {
      "epoch": 0.7038636363636364,
      "grad_norm": 0.03976376727223396,
      "learning_rate": 5.929465301478954e-05,
      "loss": 0.2662,
      "step": 3097
    },
    {
      "epoch": 0.7040909090909091,
      "grad_norm": 0.04257198050618172,
      "learning_rate": 5.9249146757679185e-05,
      "loss": 0.3082,
      "step": 3098
    },
    {
      "epoch": 0.7043181818181818,
      "grad_norm": 0.04519634321331978,
      "learning_rate": 5.920364050056884e-05,
      "loss": 0.2925,
      "step": 3099
    },
    {
      "epoch": 0.7045454545454546,
      "grad_norm": 0.03848335146903992,
      "learning_rate": 5.915813424345848e-05,
      "loss": 0.2874,
      "step": 3100
    },
    {
      "epoch": 0.7047727272727272,
      "grad_norm": 0.0597069077193737,
      "learning_rate": 5.911262798634812e-05,
      "loss": 0.407,
      "step": 3101
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.0457504540681839,
      "learning_rate": 5.9067121729237776e-05,
      "loss": 0.3424,
      "step": 3102
    },
    {
      "epoch": 0.7052272727272727,
      "grad_norm": 0.03362394869327545,
      "learning_rate": 5.902161547212742e-05,
      "loss": 0.293,
      "step": 3103
    },
    {
      "epoch": 0.7054545454545454,
      "grad_norm": 0.04617618769407272,
      "learning_rate": 5.8976109215017075e-05,
      "loss": 0.3641,
      "step": 3104
    },
    {
      "epoch": 0.7056818181818182,
      "grad_norm": 0.049266595393419266,
      "learning_rate": 5.8930602957906714e-05,
      "loss": 0.3358,
      "step": 3105
    },
    {
      "epoch": 0.7059090909090909,
      "grad_norm": 0.04999023303389549,
      "learning_rate": 5.888509670079636e-05,
      "loss": 0.3592,
      "step": 3106
    },
    {
      "epoch": 0.7061363636363637,
      "grad_norm": 0.039701901376247406,
      "learning_rate": 5.883959044368601e-05,
      "loss": 0.3067,
      "step": 3107
    },
    {
      "epoch": 0.7063636363636364,
      "grad_norm": 0.03234534338116646,
      "learning_rate": 5.879408418657566e-05,
      "loss": 0.2528,
      "step": 3108
    },
    {
      "epoch": 0.706590909090909,
      "grad_norm": 0.0466301366686821,
      "learning_rate": 5.874857792946531e-05,
      "loss": 0.3506,
      "step": 3109
    },
    {
      "epoch": 0.7068181818181818,
      "grad_norm": 0.04766053333878517,
      "learning_rate": 5.870307167235495e-05,
      "loss": 0.367,
      "step": 3110
    },
    {
      "epoch": 0.7070454545454545,
      "grad_norm": 0.03705993667244911,
      "learning_rate": 5.86575654152446e-05,
      "loss": 0.2767,
      "step": 3111
    },
    {
      "epoch": 0.7072727272727273,
      "grad_norm": 0.04355904087424278,
      "learning_rate": 5.861205915813425e-05,
      "loss": 0.3505,
      "step": 3112
    },
    {
      "epoch": 0.7075,
      "grad_norm": 0.036550216376781464,
      "learning_rate": 5.8566552901023896e-05,
      "loss": 0.3304,
      "step": 3113
    },
    {
      "epoch": 0.7077272727272728,
      "grad_norm": 0.03426952660083771,
      "learning_rate": 5.852104664391355e-05,
      "loss": 0.2875,
      "step": 3114
    },
    {
      "epoch": 0.7079545454545455,
      "grad_norm": 0.034044742584228516,
      "learning_rate": 5.847554038680319e-05,
      "loss": 0.2931,
      "step": 3115
    },
    {
      "epoch": 0.7081818181818181,
      "grad_norm": 0.05137660726904869,
      "learning_rate": 5.8430034129692834e-05,
      "loss": 0.3645,
      "step": 3116
    },
    {
      "epoch": 0.7084090909090909,
      "grad_norm": 0.03304807469248772,
      "learning_rate": 5.838452787258249e-05,
      "loss": 0.2719,
      "step": 3117
    },
    {
      "epoch": 0.7086363636363636,
      "grad_norm": 0.03910323232412338,
      "learning_rate": 5.833902161547213e-05,
      "loss": 0.3074,
      "step": 3118
    },
    {
      "epoch": 0.7088636363636364,
      "grad_norm": 0.0411003939807415,
      "learning_rate": 5.829351535836177e-05,
      "loss": 0.2833,
      "step": 3119
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.03677636384963989,
      "learning_rate": 5.8248009101251425e-05,
      "loss": 0.3213,
      "step": 3120
    },
    {
      "epoch": 0.7093181818181818,
      "grad_norm": 0.05301830545067787,
      "learning_rate": 5.820250284414107e-05,
      "loss": 0.3303,
      "step": 3121
    },
    {
      "epoch": 0.7095454545454546,
      "grad_norm": 0.056418538093566895,
      "learning_rate": 5.8156996587030724e-05,
      "loss": 0.3799,
      "step": 3122
    },
    {
      "epoch": 0.7097727272727272,
      "grad_norm": 0.039551664143800735,
      "learning_rate": 5.811149032992037e-05,
      "loss": 0.3132,
      "step": 3123
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.03691725805401802,
      "learning_rate": 5.806598407281001e-05,
      "loss": 0.3223,
      "step": 3124
    },
    {
      "epoch": 0.7102272727272727,
      "grad_norm": 0.038535334169864655,
      "learning_rate": 5.802047781569966e-05,
      "loss": 0.3232,
      "step": 3125
    },
    {
      "epoch": 0.7104545454545454,
      "grad_norm": 0.03840351849794388,
      "learning_rate": 5.797497155858931e-05,
      "loss": 0.2796,
      "step": 3126
    },
    {
      "epoch": 0.7106818181818182,
      "grad_norm": 0.04093136638402939,
      "learning_rate": 5.792946530147896e-05,
      "loss": 0.3984,
      "step": 3127
    },
    {
      "epoch": 0.7109090909090909,
      "grad_norm": 0.04573161527514458,
      "learning_rate": 5.788395904436861e-05,
      "loss": 0.3512,
      "step": 3128
    },
    {
      "epoch": 0.7111363636363637,
      "grad_norm": 0.05054424703121185,
      "learning_rate": 5.7838452787258246e-05,
      "loss": 0.3492,
      "step": 3129
    },
    {
      "epoch": 0.7113636363636363,
      "grad_norm": 0.05276267230510712,
      "learning_rate": 5.77929465301479e-05,
      "loss": 0.4176,
      "step": 3130
    },
    {
      "epoch": 0.711590909090909,
      "grad_norm": 0.04882363602519035,
      "learning_rate": 5.7747440273037545e-05,
      "loss": 0.2826,
      "step": 3131
    },
    {
      "epoch": 0.7118181818181818,
      "grad_norm": 0.04394467547535896,
      "learning_rate": 5.77019340159272e-05,
      "loss": 0.3677,
      "step": 3132
    },
    {
      "epoch": 0.7120454545454545,
      "grad_norm": 0.04265286400914192,
      "learning_rate": 5.7656427758816844e-05,
      "loss": 0.3711,
      "step": 3133
    },
    {
      "epoch": 0.7122727272727273,
      "grad_norm": 0.0481991171836853,
      "learning_rate": 5.761092150170648e-05,
      "loss": 0.3756,
      "step": 3134
    },
    {
      "epoch": 0.7125,
      "grad_norm": 0.04605596512556076,
      "learning_rate": 5.756541524459613e-05,
      "loss": 0.3064,
      "step": 3135
    },
    {
      "epoch": 0.7127272727272728,
      "grad_norm": 0.03511311113834381,
      "learning_rate": 5.751990898748578e-05,
      "loss": 0.2944,
      "step": 3136
    },
    {
      "epoch": 0.7129545454545455,
      "grad_norm": 0.04652288928627968,
      "learning_rate": 5.7474402730375435e-05,
      "loss": 0.3436,
      "step": 3137
    },
    {
      "epoch": 0.7131818181818181,
      "grad_norm": 0.043377481400966644,
      "learning_rate": 5.742889647326508e-05,
      "loss": 0.3218,
      "step": 3138
    },
    {
      "epoch": 0.7134090909090909,
      "grad_norm": 0.04254431277513504,
      "learning_rate": 5.738339021615472e-05,
      "loss": 0.3515,
      "step": 3139
    },
    {
      "epoch": 0.7136363636363636,
      "grad_norm": 0.05057039111852646,
      "learning_rate": 5.7337883959044366e-05,
      "loss": 0.3382,
      "step": 3140
    },
    {
      "epoch": 0.7138636363636364,
      "grad_norm": 0.04751644283533096,
      "learning_rate": 5.729237770193402e-05,
      "loss": 0.3848,
      "step": 3141
    },
    {
      "epoch": 0.7140909090909091,
      "grad_norm": 0.04476802423596382,
      "learning_rate": 5.724687144482367e-05,
      "loss": 0.3285,
      "step": 3142
    },
    {
      "epoch": 0.7143181818181819,
      "grad_norm": 0.04168366268277168,
      "learning_rate": 5.720136518771332e-05,
      "loss": 0.3457,
      "step": 3143
    },
    {
      "epoch": 0.7145454545454546,
      "grad_norm": 0.03456321358680725,
      "learning_rate": 5.715585893060296e-05,
      "loss": 0.2935,
      "step": 3144
    },
    {
      "epoch": 0.7147727272727272,
      "grad_norm": 0.05610737204551697,
      "learning_rate": 5.71103526734926e-05,
      "loss": 0.3273,
      "step": 3145
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.04053691774606705,
      "learning_rate": 5.7064846416382256e-05,
      "loss": 0.3057,
      "step": 3146
    },
    {
      "epoch": 0.7152272727272727,
      "grad_norm": 0.05826857313513756,
      "learning_rate": 5.701934015927191e-05,
      "loss": 0.3674,
      "step": 3147
    },
    {
      "epoch": 0.7154545454545455,
      "grad_norm": 0.042825061827898026,
      "learning_rate": 5.6973833902161555e-05,
      "loss": 0.3422,
      "step": 3148
    },
    {
      "epoch": 0.7156818181818182,
      "grad_norm": 0.05607610195875168,
      "learning_rate": 5.6928327645051194e-05,
      "loss": 0.3586,
      "step": 3149
    },
    {
      "epoch": 0.7159090909090909,
      "grad_norm": 0.039003193378448486,
      "learning_rate": 5.688282138794084e-05,
      "loss": 0.3028,
      "step": 3150
    },
    {
      "epoch": 0.7161363636363637,
      "grad_norm": 0.04208961874246597,
      "learning_rate": 5.683731513083049e-05,
      "loss": 0.3126,
      "step": 3151
    },
    {
      "epoch": 0.7163636363636363,
      "grad_norm": 0.038469091057777405,
      "learning_rate": 5.6791808873720146e-05,
      "loss": 0.2893,
      "step": 3152
    },
    {
      "epoch": 0.7165909090909091,
      "grad_norm": 0.03470239415764809,
      "learning_rate": 5.674630261660978e-05,
      "loss": 0.2621,
      "step": 3153
    },
    {
      "epoch": 0.7168181818181818,
      "grad_norm": 0.05054019019007683,
      "learning_rate": 5.670079635949943e-05,
      "loss": 0.3608,
      "step": 3154
    },
    {
      "epoch": 0.7170454545454545,
      "grad_norm": 0.04996197670698166,
      "learning_rate": 5.665529010238908e-05,
      "loss": 0.3031,
      "step": 3155
    },
    {
      "epoch": 0.7172727272727273,
      "grad_norm": 0.037530526518821716,
      "learning_rate": 5.660978384527873e-05,
      "loss": 0.3321,
      "step": 3156
    },
    {
      "epoch": 0.7175,
      "grad_norm": 0.06289158761501312,
      "learning_rate": 5.656427758816838e-05,
      "loss": 0.4165,
      "step": 3157
    },
    {
      "epoch": 0.7177272727272728,
      "grad_norm": 0.03738054260611534,
      "learning_rate": 5.6518771331058015e-05,
      "loss": 0.2658,
      "step": 3158
    },
    {
      "epoch": 0.7179545454545454,
      "grad_norm": 0.03655735403299332,
      "learning_rate": 5.647326507394767e-05,
      "loss": 0.2983,
      "step": 3159
    },
    {
      "epoch": 0.7181818181818181,
      "grad_norm": 0.040507253259420395,
      "learning_rate": 5.6427758816837314e-05,
      "loss": 0.3582,
      "step": 3160
    },
    {
      "epoch": 0.7184090909090909,
      "grad_norm": 0.042610976845026016,
      "learning_rate": 5.638225255972697e-05,
      "loss": 0.3252,
      "step": 3161
    },
    {
      "epoch": 0.7186363636363636,
      "grad_norm": 0.04333989694714546,
      "learning_rate": 5.633674630261662e-05,
      "loss": 0.3027,
      "step": 3162
    },
    {
      "epoch": 0.7188636363636364,
      "grad_norm": 0.031821638345718384,
      "learning_rate": 5.629124004550625e-05,
      "loss": 0.2528,
      "step": 3163
    },
    {
      "epoch": 0.7190909090909091,
      "grad_norm": 0.046370744705200195,
      "learning_rate": 5.6245733788395905e-05,
      "loss": 0.3677,
      "step": 3164
    },
    {
      "epoch": 0.7193181818181819,
      "grad_norm": 0.0324489064514637,
      "learning_rate": 5.620022753128555e-05,
      "loss": 0.2874,
      "step": 3165
    },
    {
      "epoch": 0.7195454545454546,
      "grad_norm": 0.04602809622883797,
      "learning_rate": 5.6154721274175204e-05,
      "loss": 0.3181,
      "step": 3166
    },
    {
      "epoch": 0.7197727272727272,
      "grad_norm": 0.048589423298835754,
      "learning_rate": 5.610921501706485e-05,
      "loss": 0.3083,
      "step": 3167
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.05190141499042511,
      "learning_rate": 5.606370875995449e-05,
      "loss": 0.3519,
      "step": 3168
    },
    {
      "epoch": 0.7202272727272727,
      "grad_norm": 0.054101768881082535,
      "learning_rate": 5.601820250284414e-05,
      "loss": 0.3443,
      "step": 3169
    },
    {
      "epoch": 0.7204545454545455,
      "grad_norm": 0.045702170580625534,
      "learning_rate": 5.597269624573379e-05,
      "loss": 0.31,
      "step": 3170
    },
    {
      "epoch": 0.7206818181818182,
      "grad_norm": 0.05173933133482933,
      "learning_rate": 5.592718998862344e-05,
      "loss": 0.3697,
      "step": 3171
    },
    {
      "epoch": 0.7209090909090909,
      "grad_norm": 0.038879863917827606,
      "learning_rate": 5.588168373151309e-05,
      "loss": 0.2833,
      "step": 3172
    },
    {
      "epoch": 0.7211363636363637,
      "grad_norm": 0.0501195564866066,
      "learning_rate": 5.5836177474402726e-05,
      "loss": 0.3545,
      "step": 3173
    },
    {
      "epoch": 0.7213636363636363,
      "grad_norm": 0.04555993527173996,
      "learning_rate": 5.579067121729238e-05,
      "loss": 0.3646,
      "step": 3174
    },
    {
      "epoch": 0.7215909090909091,
      "grad_norm": 0.03852003440260887,
      "learning_rate": 5.5745164960182025e-05,
      "loss": 0.3097,
      "step": 3175
    },
    {
      "epoch": 0.7218181818181818,
      "grad_norm": 0.03576847165822983,
      "learning_rate": 5.569965870307168e-05,
      "loss": 0.2745,
      "step": 3176
    },
    {
      "epoch": 0.7220454545454545,
      "grad_norm": 0.047173358500003815,
      "learning_rate": 5.5654152445961324e-05,
      "loss": 0.3306,
      "step": 3177
    },
    {
      "epoch": 0.7222727272727273,
      "grad_norm": 0.053929008543491364,
      "learning_rate": 5.560864618885096e-05,
      "loss": 0.3675,
      "step": 3178
    },
    {
      "epoch": 0.7225,
      "grad_norm": 0.051668234169483185,
      "learning_rate": 5.5563139931740616e-05,
      "loss": 0.3452,
      "step": 3179
    },
    {
      "epoch": 0.7227272727272728,
      "grad_norm": 0.0658537745475769,
      "learning_rate": 5.551763367463026e-05,
      "loss": 0.3898,
      "step": 3180
    },
    {
      "epoch": 0.7229545454545454,
      "grad_norm": 0.057847049087285995,
      "learning_rate": 5.5472127417519915e-05,
      "loss": 0.3616,
      "step": 3181
    },
    {
      "epoch": 0.7231818181818181,
      "grad_norm": 0.03713643178343773,
      "learning_rate": 5.542662116040956e-05,
      "loss": 0.3253,
      "step": 3182
    },
    {
      "epoch": 0.7234090909090909,
      "grad_norm": 0.03679679334163666,
      "learning_rate": 5.53811149032992e-05,
      "loss": 0.2733,
      "step": 3183
    },
    {
      "epoch": 0.7236363636363636,
      "grad_norm": 0.0342373326420784,
      "learning_rate": 5.533560864618885e-05,
      "loss": 0.2646,
      "step": 3184
    },
    {
      "epoch": 0.7238636363636364,
      "grad_norm": 0.046108074486255646,
      "learning_rate": 5.52901023890785e-05,
      "loss": 0.3485,
      "step": 3185
    },
    {
      "epoch": 0.7240909090909091,
      "grad_norm": 0.038774993270635605,
      "learning_rate": 5.524459613196815e-05,
      "loss": 0.3266,
      "step": 3186
    },
    {
      "epoch": 0.7243181818181819,
      "grad_norm": 0.042932406067848206,
      "learning_rate": 5.519908987485779e-05,
      "loss": 0.3461,
      "step": 3187
    },
    {
      "epoch": 0.7245454545454545,
      "grad_norm": 0.04970385879278183,
      "learning_rate": 5.515358361774744e-05,
      "loss": 0.3664,
      "step": 3188
    },
    {
      "epoch": 0.7247727272727272,
      "grad_norm": 0.03642341122031212,
      "learning_rate": 5.510807736063709e-05,
      "loss": 0.2502,
      "step": 3189
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.045520804822444916,
      "learning_rate": 5.5062571103526736e-05,
      "loss": 0.3533,
      "step": 3190
    },
    {
      "epoch": 0.7252272727272727,
      "grad_norm": 0.047688014805316925,
      "learning_rate": 5.501706484641639e-05,
      "loss": 0.3356,
      "step": 3191
    },
    {
      "epoch": 0.7254545454545455,
      "grad_norm": 0.04532748460769653,
      "learning_rate": 5.497155858930603e-05,
      "loss": 0.3621,
      "step": 3192
    },
    {
      "epoch": 0.7256818181818182,
      "grad_norm": 0.031292881816625595,
      "learning_rate": 5.4926052332195674e-05,
      "loss": 0.2252,
      "step": 3193
    },
    {
      "epoch": 0.725909090909091,
      "grad_norm": 0.07152464240789413,
      "learning_rate": 5.488054607508533e-05,
      "loss": 0.3866,
      "step": 3194
    },
    {
      "epoch": 0.7261363636363637,
      "grad_norm": 0.049687594175338745,
      "learning_rate": 5.483503981797497e-05,
      "loss": 0.3553,
      "step": 3195
    },
    {
      "epoch": 0.7263636363636363,
      "grad_norm": 0.04707005247473717,
      "learning_rate": 5.4789533560864626e-05,
      "loss": 0.3197,
      "step": 3196
    },
    {
      "epoch": 0.7265909090909091,
      "grad_norm": 0.051653604954481125,
      "learning_rate": 5.4744027303754265e-05,
      "loss": 0.3595,
      "step": 3197
    },
    {
      "epoch": 0.7268181818181818,
      "grad_norm": 0.048174917697906494,
      "learning_rate": 5.469852104664391e-05,
      "loss": 0.4094,
      "step": 3198
    },
    {
      "epoch": 0.7270454545454546,
      "grad_norm": 0.0542309433221817,
      "learning_rate": 5.4653014789533564e-05,
      "loss": 0.4063,
      "step": 3199
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.05139361694455147,
      "learning_rate": 5.460750853242321e-05,
      "loss": 0.3219,
      "step": 3200
    },
    {
      "epoch": 0.7275,
      "grad_norm": 0.03459598869085312,
      "learning_rate": 5.456200227531286e-05,
      "loss": 0.336,
      "step": 3201
    },
    {
      "epoch": 0.7277272727272728,
      "grad_norm": 0.050910331308841705,
      "learning_rate": 5.45164960182025e-05,
      "loss": 0.3736,
      "step": 3202
    },
    {
      "epoch": 0.7279545454545454,
      "grad_norm": 0.03827660158276558,
      "learning_rate": 5.447098976109215e-05,
      "loss": 0.3087,
      "step": 3203
    },
    {
      "epoch": 0.7281818181818182,
      "grad_norm": 0.05400250852108002,
      "learning_rate": 5.44254835039818e-05,
      "loss": 0.4093,
      "step": 3204
    },
    {
      "epoch": 0.7284090909090909,
      "grad_norm": 0.04052681103348732,
      "learning_rate": 5.437997724687145e-05,
      "loss": 0.2825,
      "step": 3205
    },
    {
      "epoch": 0.7286363636363636,
      "grad_norm": 0.03505311906337738,
      "learning_rate": 5.43344709897611e-05,
      "loss": 0.2483,
      "step": 3206
    },
    {
      "epoch": 0.7288636363636364,
      "grad_norm": 0.04776837304234505,
      "learning_rate": 5.428896473265074e-05,
      "loss": 0.3599,
      "step": 3207
    },
    {
      "epoch": 0.7290909090909091,
      "grad_norm": 0.0373004712164402,
      "learning_rate": 5.4243458475540385e-05,
      "loss": 0.2867,
      "step": 3208
    },
    {
      "epoch": 0.7293181818181819,
      "grad_norm": 0.04396767541766167,
      "learning_rate": 5.419795221843004e-05,
      "loss": 0.332,
      "step": 3209
    },
    {
      "epoch": 0.7295454545454545,
      "grad_norm": 0.04010498523712158,
      "learning_rate": 5.4152445961319684e-05,
      "loss": 0.2912,
      "step": 3210
    },
    {
      "epoch": 0.7297727272727272,
      "grad_norm": 0.03290904685854912,
      "learning_rate": 5.410693970420934e-05,
      "loss": 0.2994,
      "step": 3211
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.041105497628450394,
      "learning_rate": 5.4061433447098976e-05,
      "loss": 0.3171,
      "step": 3212
    },
    {
      "epoch": 0.7302272727272727,
      "grad_norm": 0.03727000206708908,
      "learning_rate": 5.401592718998862e-05,
      "loss": 0.3035,
      "step": 3213
    },
    {
      "epoch": 0.7304545454545455,
      "grad_norm": 0.04054355248808861,
      "learning_rate": 5.3970420932878275e-05,
      "loss": 0.3287,
      "step": 3214
    },
    {
      "epoch": 0.7306818181818182,
      "grad_norm": 0.05087818577885628,
      "learning_rate": 5.392491467576792e-05,
      "loss": 0.3815,
      "step": 3215
    },
    {
      "epoch": 0.730909090909091,
      "grad_norm": 0.04088186100125313,
      "learning_rate": 5.3879408418657574e-05,
      "loss": 0.3029,
      "step": 3216
    },
    {
      "epoch": 0.7311363636363636,
      "grad_norm": 0.04367775842547417,
      "learning_rate": 5.383390216154721e-05,
      "loss": 0.322,
      "step": 3217
    },
    {
      "epoch": 0.7313636363636363,
      "grad_norm": 0.04466797783970833,
      "learning_rate": 5.378839590443686e-05,
      "loss": 0.3578,
      "step": 3218
    },
    {
      "epoch": 0.7315909090909091,
      "grad_norm": 0.05083281546831131,
      "learning_rate": 5.374288964732651e-05,
      "loss": 0.3802,
      "step": 3219
    },
    {
      "epoch": 0.7318181818181818,
      "grad_norm": 0.04508867859840393,
      "learning_rate": 5.369738339021616e-05,
      "loss": 0.3375,
      "step": 3220
    },
    {
      "epoch": 0.7320454545454546,
      "grad_norm": 0.05720143765211105,
      "learning_rate": 5.365187713310581e-05,
      "loss": 0.38,
      "step": 3221
    },
    {
      "epoch": 0.7322727272727273,
      "grad_norm": 0.05282015725970268,
      "learning_rate": 5.360637087599545e-05,
      "loss": 0.3284,
      "step": 3222
    },
    {
      "epoch": 0.7325,
      "grad_norm": 0.039768069982528687,
      "learning_rate": 5.3560864618885096e-05,
      "loss": 0.3249,
      "step": 3223
    },
    {
      "epoch": 0.7327272727272728,
      "grad_norm": 0.048542559146881104,
      "learning_rate": 5.351535836177475e-05,
      "loss": 0.3128,
      "step": 3224
    },
    {
      "epoch": 0.7329545454545454,
      "grad_norm": 0.04011926054954529,
      "learning_rate": 5.3469852104664395e-05,
      "loss": 0.3247,
      "step": 3225
    },
    {
      "epoch": 0.7331818181818182,
      "grad_norm": 0.037018582224845886,
      "learning_rate": 5.3424345847554034e-05,
      "loss": 0.3065,
      "step": 3226
    },
    {
      "epoch": 0.7334090909090909,
      "grad_norm": 0.05179281532764435,
      "learning_rate": 5.337883959044369e-05,
      "loss": 0.35,
      "step": 3227
    },
    {
      "epoch": 0.7336363636363636,
      "grad_norm": 0.041449014097452164,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.3476,
      "step": 3228
    },
    {
      "epoch": 0.7338636363636364,
      "grad_norm": 0.04895860329270363,
      "learning_rate": 5.3287827076222986e-05,
      "loss": 0.3675,
      "step": 3229
    },
    {
      "epoch": 0.7340909090909091,
      "grad_norm": 0.04739925265312195,
      "learning_rate": 5.324232081911263e-05,
      "loss": 0.2805,
      "step": 3230
    },
    {
      "epoch": 0.7343181818181819,
      "grad_norm": 0.04848140478134155,
      "learning_rate": 5.319681456200227e-05,
      "loss": 0.305,
      "step": 3231
    },
    {
      "epoch": 0.7345454545454545,
      "grad_norm": 0.037840183824300766,
      "learning_rate": 5.3151308304891924e-05,
      "loss": 0.2967,
      "step": 3232
    },
    {
      "epoch": 0.7347727272727272,
      "grad_norm": 0.05546080693602562,
      "learning_rate": 5.310580204778157e-05,
      "loss": 0.4205,
      "step": 3233
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.046103157103061676,
      "learning_rate": 5.306029579067122e-05,
      "loss": 0.3252,
      "step": 3234
    },
    {
      "epoch": 0.7352272727272727,
      "grad_norm": 0.03689876198768616,
      "learning_rate": 5.301478953356087e-05,
      "loss": 0.2843,
      "step": 3235
    },
    {
      "epoch": 0.7354545454545455,
      "grad_norm": 0.03134288638830185,
      "learning_rate": 5.296928327645051e-05,
      "loss": 0.2497,
      "step": 3236
    },
    {
      "epoch": 0.7356818181818182,
      "grad_norm": 0.034941188991069794,
      "learning_rate": 5.292377701934016e-05,
      "loss": 0.3156,
      "step": 3237
    },
    {
      "epoch": 0.735909090909091,
      "grad_norm": 0.05414591729640961,
      "learning_rate": 5.287827076222981e-05,
      "loss": 0.3753,
      "step": 3238
    },
    {
      "epoch": 0.7361363636363636,
      "grad_norm": 0.044647350907325745,
      "learning_rate": 5.283276450511946e-05,
      "loss": 0.317,
      "step": 3239
    },
    {
      "epoch": 0.7363636363636363,
      "grad_norm": 0.059833284467458725,
      "learning_rate": 5.2787258248009106e-05,
      "loss": 0.3827,
      "step": 3240
    },
    {
      "epoch": 0.7365909090909091,
      "grad_norm": 0.03607886657118797,
      "learning_rate": 5.2741751990898745e-05,
      "loss": 0.2768,
      "step": 3241
    },
    {
      "epoch": 0.7368181818181818,
      "grad_norm": 0.040068987756967545,
      "learning_rate": 5.26962457337884e-05,
      "loss": 0.3196,
      "step": 3242
    },
    {
      "epoch": 0.7370454545454546,
      "grad_norm": 0.06589725613594055,
      "learning_rate": 5.2650739476678044e-05,
      "loss": 0.4314,
      "step": 3243
    },
    {
      "epoch": 0.7372727272727273,
      "grad_norm": 0.04184318706393242,
      "learning_rate": 5.26052332195677e-05,
      "loss": 0.3521,
      "step": 3244
    },
    {
      "epoch": 0.7375,
      "grad_norm": 0.05686474218964577,
      "learning_rate": 5.255972696245734e-05,
      "loss": 0.3927,
      "step": 3245
    },
    {
      "epoch": 0.7377272727272727,
      "grad_norm": 0.04366303235292435,
      "learning_rate": 5.251422070534698e-05,
      "loss": 0.3111,
      "step": 3246
    },
    {
      "epoch": 0.7379545454545454,
      "grad_norm": 0.038717515766620636,
      "learning_rate": 5.2468714448236635e-05,
      "loss": 0.2936,
      "step": 3247
    },
    {
      "epoch": 0.7381818181818182,
      "grad_norm": 0.04451271519064903,
      "learning_rate": 5.242320819112628e-05,
      "loss": 0.3234,
      "step": 3248
    },
    {
      "epoch": 0.7384090909090909,
      "grad_norm": 0.03974974900484085,
      "learning_rate": 5.2377701934015934e-05,
      "loss": 0.3339,
      "step": 3249
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 0.042598873376846313,
      "learning_rate": 5.233219567690558e-05,
      "loss": 0.356,
      "step": 3250
    },
    {
      "epoch": 0.7388636363636364,
      "grad_norm": 0.05213845893740654,
      "learning_rate": 5.228668941979522e-05,
      "loss": 0.3743,
      "step": 3251
    },
    {
      "epoch": 0.7390909090909091,
      "grad_norm": 0.04525569826364517,
      "learning_rate": 5.224118316268487e-05,
      "loss": 0.2906,
      "step": 3252
    },
    {
      "epoch": 0.7393181818181818,
      "grad_norm": 0.048404593020677567,
      "learning_rate": 5.219567690557452e-05,
      "loss": 0.2911,
      "step": 3253
    },
    {
      "epoch": 0.7395454545454545,
      "grad_norm": 0.042441725730895996,
      "learning_rate": 5.215017064846417e-05,
      "loss": 0.2979,
      "step": 3254
    },
    {
      "epoch": 0.7397727272727272,
      "grad_norm": 0.05521942675113678,
      "learning_rate": 5.210466439135382e-05,
      "loss": 0.3229,
      "step": 3255
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04225998371839523,
      "learning_rate": 5.2059158134243456e-05,
      "loss": 0.3561,
      "step": 3256
    },
    {
      "epoch": 0.7402272727272727,
      "grad_norm": 0.03531862050294876,
      "learning_rate": 5.201365187713311e-05,
      "loss": 0.282,
      "step": 3257
    },
    {
      "epoch": 0.7404545454545455,
      "grad_norm": 0.045036572962999344,
      "learning_rate": 5.1968145620022755e-05,
      "loss": 0.3087,
      "step": 3258
    },
    {
      "epoch": 0.7406818181818182,
      "grad_norm": 0.0430404432117939,
      "learning_rate": 5.192263936291241e-05,
      "loss": 0.3262,
      "step": 3259
    },
    {
      "epoch": 0.740909090909091,
      "grad_norm": 0.04114843159914017,
      "learning_rate": 5.187713310580205e-05,
      "loss": 0.2766,
      "step": 3260
    },
    {
      "epoch": 0.7411363636363636,
      "grad_norm": 0.0518774688243866,
      "learning_rate": 5.183162684869169e-05,
      "loss": 0.3466,
      "step": 3261
    },
    {
      "epoch": 0.7413636363636363,
      "grad_norm": 0.04380517452955246,
      "learning_rate": 5.1786120591581346e-05,
      "loss": 0.3301,
      "step": 3262
    },
    {
      "epoch": 0.7415909090909091,
      "grad_norm": 0.04431610554456711,
      "learning_rate": 5.174061433447099e-05,
      "loss": 0.3343,
      "step": 3263
    },
    {
      "epoch": 0.7418181818181818,
      "grad_norm": 0.0455411896109581,
      "learning_rate": 5.1695108077360645e-05,
      "loss": 0.3173,
      "step": 3264
    },
    {
      "epoch": 0.7420454545454546,
      "grad_norm": 0.04046223312616348,
      "learning_rate": 5.1649601820250284e-05,
      "loss": 0.2976,
      "step": 3265
    },
    {
      "epoch": 0.7422727272727273,
      "grad_norm": 0.040975067764520645,
      "learning_rate": 5.160409556313993e-05,
      "loss": 0.29,
      "step": 3266
    },
    {
      "epoch": 0.7425,
      "grad_norm": 0.04078025370836258,
      "learning_rate": 5.155858930602958e-05,
      "loss": 0.3141,
      "step": 3267
    },
    {
      "epoch": 0.7427272727272727,
      "grad_norm": 0.04999512434005737,
      "learning_rate": 5.151308304891923e-05,
      "loss": 0.3158,
      "step": 3268
    },
    {
      "epoch": 0.7429545454545454,
      "grad_norm": 0.049107279628515244,
      "learning_rate": 5.146757679180888e-05,
      "loss": 0.3474,
      "step": 3269
    },
    {
      "epoch": 0.7431818181818182,
      "grad_norm": 0.044398047029972076,
      "learning_rate": 5.142207053469852e-05,
      "loss": 0.3391,
      "step": 3270
    },
    {
      "epoch": 0.7434090909090909,
      "grad_norm": 0.04208934307098389,
      "learning_rate": 5.137656427758817e-05,
      "loss": 0.3284,
      "step": 3271
    },
    {
      "epoch": 0.7436363636363637,
      "grad_norm": 0.035869911313056946,
      "learning_rate": 5.133105802047782e-05,
      "loss": 0.3034,
      "step": 3272
    },
    {
      "epoch": 0.7438636363636364,
      "grad_norm": 0.03146059066057205,
      "learning_rate": 5.1285551763367466e-05,
      "loss": 0.2532,
      "step": 3273
    },
    {
      "epoch": 0.7440909090909091,
      "grad_norm": 0.037069883197546005,
      "learning_rate": 5.124004550625712e-05,
      "loss": 0.2828,
      "step": 3274
    },
    {
      "epoch": 0.7443181818181818,
      "grad_norm": 0.04904509335756302,
      "learning_rate": 5.119453924914676e-05,
      "loss": 0.3023,
      "step": 3275
    },
    {
      "epoch": 0.7445454545454545,
      "grad_norm": 0.04268087446689606,
      "learning_rate": 5.1149032992036404e-05,
      "loss": 0.3258,
      "step": 3276
    },
    {
      "epoch": 0.7447727272727273,
      "grad_norm": 0.05200641602277756,
      "learning_rate": 5.110352673492606e-05,
      "loss": 0.342,
      "step": 3277
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.05376240238547325,
      "learning_rate": 5.10580204778157e-05,
      "loss": 0.3612,
      "step": 3278
    },
    {
      "epoch": 0.7452272727272727,
      "grad_norm": 0.04261403903365135,
      "learning_rate": 5.1012514220705356e-05,
      "loss": 0.301,
      "step": 3279
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.029349133372306824,
      "learning_rate": 5.0967007963594995e-05,
      "loss": 0.2233,
      "step": 3280
    },
    {
      "epoch": 0.7456818181818182,
      "grad_norm": 0.041209977120161057,
      "learning_rate": 5.092150170648464e-05,
      "loss": 0.3122,
      "step": 3281
    },
    {
      "epoch": 0.7459090909090909,
      "grad_norm": 0.05126865580677986,
      "learning_rate": 5.0875995449374294e-05,
      "loss": 0.3497,
      "step": 3282
    },
    {
      "epoch": 0.7461363636363636,
      "grad_norm": 0.051491718739271164,
      "learning_rate": 5.083048919226394e-05,
      "loss": 0.3065,
      "step": 3283
    },
    {
      "epoch": 0.7463636363636363,
      "grad_norm": 0.044793132692575455,
      "learning_rate": 5.078498293515359e-05,
      "loss": 0.3589,
      "step": 3284
    },
    {
      "epoch": 0.7465909090909091,
      "grad_norm": 0.0563136525452137,
      "learning_rate": 5.073947667804323e-05,
      "loss": 0.3796,
      "step": 3285
    },
    {
      "epoch": 0.7468181818181818,
      "grad_norm": 0.04061931371688843,
      "learning_rate": 5.069397042093288e-05,
      "loss": 0.3026,
      "step": 3286
    },
    {
      "epoch": 0.7470454545454546,
      "grad_norm": 0.04145891219377518,
      "learning_rate": 5.064846416382253e-05,
      "loss": 0.3361,
      "step": 3287
    },
    {
      "epoch": 0.7472727272727273,
      "grad_norm": 0.04538711532950401,
      "learning_rate": 5.060295790671218e-05,
      "loss": 0.345,
      "step": 3288
    },
    {
      "epoch": 0.7475,
      "grad_norm": 0.04168466106057167,
      "learning_rate": 5.055745164960183e-05,
      "loss": 0.3271,
      "step": 3289
    },
    {
      "epoch": 0.7477272727272727,
      "grad_norm": 0.04662155732512474,
      "learning_rate": 5.051194539249147e-05,
      "loss": 0.2825,
      "step": 3290
    },
    {
      "epoch": 0.7479545454545454,
      "grad_norm": 0.047104738652706146,
      "learning_rate": 5.0466439135381115e-05,
      "loss": 0.3502,
      "step": 3291
    },
    {
      "epoch": 0.7481818181818182,
      "grad_norm": 0.042889319360256195,
      "learning_rate": 5.042093287827077e-05,
      "loss": 0.313,
      "step": 3292
    },
    {
      "epoch": 0.7484090909090909,
      "grad_norm": 0.046108342707157135,
      "learning_rate": 5.0375426621160414e-05,
      "loss": 0.334,
      "step": 3293
    },
    {
      "epoch": 0.7486363636363637,
      "grad_norm": 0.038603585213422775,
      "learning_rate": 5.032992036405005e-05,
      "loss": 0.3597,
      "step": 3294
    },
    {
      "epoch": 0.7488636363636364,
      "grad_norm": 0.03964380919933319,
      "learning_rate": 5.0284414106939706e-05,
      "loss": 0.3187,
      "step": 3295
    },
    {
      "epoch": 0.7490909090909091,
      "grad_norm": 0.05015178024768829,
      "learning_rate": 5.023890784982935e-05,
      "loss": 0.3187,
      "step": 3296
    },
    {
      "epoch": 0.7493181818181818,
      "grad_norm": 0.05977901443839073,
      "learning_rate": 5.0193401592719005e-05,
      "loss": 0.401,
      "step": 3297
    },
    {
      "epoch": 0.7495454545454545,
      "grad_norm": 0.05673258379101753,
      "learning_rate": 5.014789533560865e-05,
      "loss": 0.3675,
      "step": 3298
    },
    {
      "epoch": 0.7497727272727273,
      "grad_norm": 0.036141667515039444,
      "learning_rate": 5.010238907849829e-05,
      "loss": 0.3387,
      "step": 3299
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.03529158979654312,
      "learning_rate": 5.005688282138794e-05,
      "loss": 0.2919,
      "step": 3300
    },
    {
      "epoch": 0.7502272727272727,
      "grad_norm": 0.04656480625271797,
      "learning_rate": 5.001137656427759e-05,
      "loss": 0.3548,
      "step": 3301
    },
    {
      "epoch": 0.7504545454545455,
      "grad_norm": 0.038495320826768875,
      "learning_rate": 4.996587030716724e-05,
      "loss": 0.3033,
      "step": 3302
    },
    {
      "epoch": 0.7506818181818182,
      "grad_norm": 0.04966064170002937,
      "learning_rate": 4.992036405005688e-05,
      "loss": 0.3252,
      "step": 3303
    },
    {
      "epoch": 0.7509090909090909,
      "grad_norm": 0.05148323252797127,
      "learning_rate": 4.9874857792946534e-05,
      "loss": 0.3757,
      "step": 3304
    },
    {
      "epoch": 0.7511363636363636,
      "grad_norm": 0.03979851305484772,
      "learning_rate": 4.982935153583618e-05,
      "loss": 0.3066,
      "step": 3305
    },
    {
      "epoch": 0.7513636363636363,
      "grad_norm": 0.03749043866991997,
      "learning_rate": 4.9783845278725826e-05,
      "loss": 0.3294,
      "step": 3306
    },
    {
      "epoch": 0.7515909090909091,
      "grad_norm": 0.050998836755752563,
      "learning_rate": 4.973833902161548e-05,
      "loss": 0.3626,
      "step": 3307
    },
    {
      "epoch": 0.7518181818181818,
      "grad_norm": 0.036826588213443756,
      "learning_rate": 4.969283276450512e-05,
      "loss": 0.2773,
      "step": 3308
    },
    {
      "epoch": 0.7520454545454546,
      "grad_norm": 0.052377067506313324,
      "learning_rate": 4.964732650739477e-05,
      "loss": 0.3821,
      "step": 3309
    },
    {
      "epoch": 0.7522727272727273,
      "grad_norm": 0.040848586708307266,
      "learning_rate": 4.960182025028442e-05,
      "loss": 0.3237,
      "step": 3310
    },
    {
      "epoch": 0.7525,
      "grad_norm": 0.03996974229812622,
      "learning_rate": 4.955631399317406e-05,
      "loss": 0.2792,
      "step": 3311
    },
    {
      "epoch": 0.7527272727272727,
      "grad_norm": 0.045931510627269745,
      "learning_rate": 4.9510807736063716e-05,
      "loss": 0.3458,
      "step": 3312
    },
    {
      "epoch": 0.7529545454545454,
      "grad_norm": 0.04023032635450363,
      "learning_rate": 4.9465301478953355e-05,
      "loss": 0.36,
      "step": 3313
    },
    {
      "epoch": 0.7531818181818182,
      "grad_norm": 0.050588205456733704,
      "learning_rate": 4.941979522184301e-05,
      "loss": 0.3833,
      "step": 3314
    },
    {
      "epoch": 0.7534090909090909,
      "grad_norm": 0.04169943556189537,
      "learning_rate": 4.9374288964732654e-05,
      "loss": 0.3382,
      "step": 3315
    },
    {
      "epoch": 0.7536363636363637,
      "grad_norm": 0.04311099648475647,
      "learning_rate": 4.93287827076223e-05,
      "loss": 0.3507,
      "step": 3316
    },
    {
      "epoch": 0.7538636363636364,
      "grad_norm": 0.05406004562973976,
      "learning_rate": 4.928327645051195e-05,
      "loss": 0.3412,
      "step": 3317
    },
    {
      "epoch": 0.7540909090909091,
      "grad_norm": 0.041175469756126404,
      "learning_rate": 4.923777019340159e-05,
      "loss": 0.3224,
      "step": 3318
    },
    {
      "epoch": 0.7543181818181818,
      "grad_norm": 0.03207414597272873,
      "learning_rate": 4.9192263936291245e-05,
      "loss": 0.2839,
      "step": 3319
    },
    {
      "epoch": 0.7545454545454545,
      "grad_norm": 0.03449886664748192,
      "learning_rate": 4.914675767918089e-05,
      "loss": 0.3001,
      "step": 3320
    },
    {
      "epoch": 0.7547727272727273,
      "grad_norm": 0.04189315810799599,
      "learning_rate": 4.910125142207054e-05,
      "loss": 0.3157,
      "step": 3321
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.04579810053110123,
      "learning_rate": 4.905574516496018e-05,
      "loss": 0.3043,
      "step": 3322
    },
    {
      "epoch": 0.7552272727272727,
      "grad_norm": 0.03461270034313202,
      "learning_rate": 4.901023890784983e-05,
      "loss": 0.2967,
      "step": 3323
    },
    {
      "epoch": 0.7554545454545455,
      "grad_norm": 0.03484295681118965,
      "learning_rate": 4.896473265073948e-05,
      "loss": 0.2771,
      "step": 3324
    },
    {
      "epoch": 0.7556818181818182,
      "grad_norm": 0.05317224562168121,
      "learning_rate": 4.891922639362913e-05,
      "loss": 0.3195,
      "step": 3325
    },
    {
      "epoch": 0.7559090909090909,
      "grad_norm": 0.04622328653931618,
      "learning_rate": 4.8873720136518774e-05,
      "loss": 0.3076,
      "step": 3326
    },
    {
      "epoch": 0.7561363636363636,
      "grad_norm": 0.05814629793167114,
      "learning_rate": 4.882821387940842e-05,
      "loss": 0.3817,
      "step": 3327
    },
    {
      "epoch": 0.7563636363636363,
      "grad_norm": 0.041318222880363464,
      "learning_rate": 4.8782707622298066e-05,
      "loss": 0.3006,
      "step": 3328
    },
    {
      "epoch": 0.7565909090909091,
      "grad_norm": 0.04369170963764191,
      "learning_rate": 4.873720136518772e-05,
      "loss": 0.3386,
      "step": 3329
    },
    {
      "epoch": 0.7568181818181818,
      "grad_norm": 0.04957570880651474,
      "learning_rate": 4.8691695108077365e-05,
      "loss": 0.3127,
      "step": 3330
    },
    {
      "epoch": 0.7570454545454546,
      "grad_norm": 0.03950819373130798,
      "learning_rate": 4.864618885096701e-05,
      "loss": 0.3146,
      "step": 3331
    },
    {
      "epoch": 0.7572727272727273,
      "grad_norm": 0.03614150732755661,
      "learning_rate": 4.860068259385666e-05,
      "loss": 0.3234,
      "step": 3332
    },
    {
      "epoch": 0.7575,
      "grad_norm": 0.04508577287197113,
      "learning_rate": 4.85551763367463e-05,
      "loss": 0.3769,
      "step": 3333
    },
    {
      "epoch": 0.7577272727272727,
      "grad_norm": 0.042927391827106476,
      "learning_rate": 4.8509670079635956e-05,
      "loss": 0.32,
      "step": 3334
    },
    {
      "epoch": 0.7579545454545454,
      "grad_norm": 0.05014277249574661,
      "learning_rate": 4.84641638225256e-05,
      "loss": 0.3356,
      "step": 3335
    },
    {
      "epoch": 0.7581818181818182,
      "grad_norm": 0.03854062408208847,
      "learning_rate": 4.841865756541525e-05,
      "loss": 0.2803,
      "step": 3336
    },
    {
      "epoch": 0.7584090909090909,
      "grad_norm": 0.054411381483078,
      "learning_rate": 4.8373151308304894e-05,
      "loss": 0.3737,
      "step": 3337
    },
    {
      "epoch": 0.7586363636363637,
      "grad_norm": 0.045890625566244125,
      "learning_rate": 4.832764505119454e-05,
      "loss": 0.3341,
      "step": 3338
    },
    {
      "epoch": 0.7588636363636364,
      "grad_norm": 0.03829517215490341,
      "learning_rate": 4.8282138794084186e-05,
      "loss": 0.2678,
      "step": 3339
    },
    {
      "epoch": 0.759090909090909,
      "grad_norm": 0.03660409152507782,
      "learning_rate": 4.823663253697384e-05,
      "loss": 0.2623,
      "step": 3340
    },
    {
      "epoch": 0.7593181818181818,
      "grad_norm": 0.060414042323827744,
      "learning_rate": 4.8191126279863485e-05,
      "loss": 0.3554,
      "step": 3341
    },
    {
      "epoch": 0.7595454545454545,
      "grad_norm": 0.03567233309149742,
      "learning_rate": 4.814562002275313e-05,
      "loss": 0.3395,
      "step": 3342
    },
    {
      "epoch": 0.7597727272727273,
      "grad_norm": 0.05893357843160629,
      "learning_rate": 4.810011376564278e-05,
      "loss": 0.3168,
      "step": 3343
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.0381912887096405,
      "learning_rate": 4.805460750853242e-05,
      "loss": 0.3296,
      "step": 3344
    },
    {
      "epoch": 0.7602272727272728,
      "grad_norm": 0.044327542185783386,
      "learning_rate": 4.8009101251422076e-05,
      "loss": 0.3241,
      "step": 3345
    },
    {
      "epoch": 0.7604545454545455,
      "grad_norm": 0.03249712288379669,
      "learning_rate": 4.796359499431172e-05,
      "loss": 0.2802,
      "step": 3346
    },
    {
      "epoch": 0.7606818181818182,
      "grad_norm": 0.05022013559937477,
      "learning_rate": 4.791808873720137e-05,
      "loss": 0.343,
      "step": 3347
    },
    {
      "epoch": 0.7609090909090909,
      "grad_norm": 0.05231600999832153,
      "learning_rate": 4.7872582480091014e-05,
      "loss": 0.3691,
      "step": 3348
    },
    {
      "epoch": 0.7611363636363636,
      "grad_norm": 0.05282265320420265,
      "learning_rate": 4.782707622298066e-05,
      "loss": 0.3516,
      "step": 3349
    },
    {
      "epoch": 0.7613636363636364,
      "grad_norm": 0.03323456645011902,
      "learning_rate": 4.778156996587031e-05,
      "loss": 0.2943,
      "step": 3350
    },
    {
      "epoch": 0.7615909090909091,
      "grad_norm": 0.04926815256476402,
      "learning_rate": 4.773606370875996e-05,
      "loss": 0.3399,
      "step": 3351
    },
    {
      "epoch": 0.7618181818181818,
      "grad_norm": 0.054567862302064896,
      "learning_rate": 4.7690557451649605e-05,
      "loss": 0.4078,
      "step": 3352
    },
    {
      "epoch": 0.7620454545454546,
      "grad_norm": 0.04519619420170784,
      "learning_rate": 4.764505119453925e-05,
      "loss": 0.3574,
      "step": 3353
    },
    {
      "epoch": 0.7622727272727273,
      "grad_norm": 0.04696352407336235,
      "learning_rate": 4.75995449374289e-05,
      "loss": 0.3634,
      "step": 3354
    },
    {
      "epoch": 0.7625,
      "grad_norm": 0.03044140338897705,
      "learning_rate": 4.755403868031855e-05,
      "loss": 0.2763,
      "step": 3355
    },
    {
      "epoch": 0.7627272727272727,
      "grad_norm": 0.03837421163916588,
      "learning_rate": 4.750853242320819e-05,
      "loss": 0.3264,
      "step": 3356
    },
    {
      "epoch": 0.7629545454545454,
      "grad_norm": 0.10087200254201889,
      "learning_rate": 4.746302616609784e-05,
      "loss": 0.3742,
      "step": 3357
    },
    {
      "epoch": 0.7631818181818182,
      "grad_norm": 0.035614632070064545,
      "learning_rate": 4.741751990898749e-05,
      "loss": 0.3089,
      "step": 3358
    },
    {
      "epoch": 0.7634090909090909,
      "grad_norm": 0.043029557913541794,
      "learning_rate": 4.7372013651877134e-05,
      "loss": 0.3625,
      "step": 3359
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.04148014634847641,
      "learning_rate": 4.732650739476678e-05,
      "loss": 0.3364,
      "step": 3360
    },
    {
      "epoch": 0.7638636363636364,
      "grad_norm": 0.03887486457824707,
      "learning_rate": 4.7281001137656426e-05,
      "loss": 0.293,
      "step": 3361
    },
    {
      "epoch": 0.764090909090909,
      "grad_norm": 0.055473510175943375,
      "learning_rate": 4.723549488054608e-05,
      "loss": 0.3434,
      "step": 3362
    },
    {
      "epoch": 0.7643181818181818,
      "grad_norm": 0.05361267551779747,
      "learning_rate": 4.7189988623435725e-05,
      "loss": 0.3588,
      "step": 3363
    },
    {
      "epoch": 0.7645454545454545,
      "grad_norm": 0.044373150914907455,
      "learning_rate": 4.714448236632537e-05,
      "loss": 0.3593,
      "step": 3364
    },
    {
      "epoch": 0.7647727272727273,
      "grad_norm": 0.04339654743671417,
      "learning_rate": 4.709897610921502e-05,
      "loss": 0.3772,
      "step": 3365
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.03983788564801216,
      "learning_rate": 4.705346985210466e-05,
      "loss": 0.3381,
      "step": 3366
    },
    {
      "epoch": 0.7652272727272728,
      "grad_norm": 0.05078991502523422,
      "learning_rate": 4.7007963594994316e-05,
      "loss": 0.4186,
      "step": 3367
    },
    {
      "epoch": 0.7654545454545455,
      "grad_norm": 0.04870925098657608,
      "learning_rate": 4.696245733788396e-05,
      "loss": 0.3338,
      "step": 3368
    },
    {
      "epoch": 0.7656818181818181,
      "grad_norm": 0.04853779822587967,
      "learning_rate": 4.691695108077361e-05,
      "loss": 0.3431,
      "step": 3369
    },
    {
      "epoch": 0.7659090909090909,
      "grad_norm": 0.04764489457011223,
      "learning_rate": 4.6871444823663254e-05,
      "loss": 0.3597,
      "step": 3370
    },
    {
      "epoch": 0.7661363636363636,
      "grad_norm": 0.043426308780908585,
      "learning_rate": 4.68259385665529e-05,
      "loss": 0.3576,
      "step": 3371
    },
    {
      "epoch": 0.7663636363636364,
      "grad_norm": 0.03879319876432419,
      "learning_rate": 4.678043230944255e-05,
      "loss": 0.313,
      "step": 3372
    },
    {
      "epoch": 0.7665909090909091,
      "grad_norm": 0.04098302870988846,
      "learning_rate": 4.673492605233219e-05,
      "loss": 0.311,
      "step": 3373
    },
    {
      "epoch": 0.7668181818181818,
      "grad_norm": 0.031674839556217194,
      "learning_rate": 4.6689419795221845e-05,
      "loss": 0.2701,
      "step": 3374
    },
    {
      "epoch": 0.7670454545454546,
      "grad_norm": 0.04814426600933075,
      "learning_rate": 4.664391353811149e-05,
      "loss": 0.3424,
      "step": 3375
    },
    {
      "epoch": 0.7672727272727272,
      "grad_norm": 0.05349160358309746,
      "learning_rate": 4.659840728100114e-05,
      "loss": 0.3632,
      "step": 3376
    },
    {
      "epoch": 0.7675,
      "grad_norm": 0.04355219006538391,
      "learning_rate": 4.655290102389079e-05,
      "loss": 0.3079,
      "step": 3377
    },
    {
      "epoch": 0.7677272727272727,
      "grad_norm": 0.03579828515648842,
      "learning_rate": 4.650739476678043e-05,
      "loss": 0.3444,
      "step": 3378
    },
    {
      "epoch": 0.7679545454545454,
      "grad_norm": 0.04605809971690178,
      "learning_rate": 4.646188850967008e-05,
      "loss": 0.3204,
      "step": 3379
    },
    {
      "epoch": 0.7681818181818182,
      "grad_norm": 0.0442025326192379,
      "learning_rate": 4.641638225255973e-05,
      "loss": 0.2967,
      "step": 3380
    },
    {
      "epoch": 0.7684090909090909,
      "grad_norm": 0.058856233954429626,
      "learning_rate": 4.6370875995449374e-05,
      "loss": 0.4091,
      "step": 3381
    },
    {
      "epoch": 0.7686363636363637,
      "grad_norm": 0.042573608458042145,
      "learning_rate": 4.632536973833903e-05,
      "loss": 0.3755,
      "step": 3382
    },
    {
      "epoch": 0.7688636363636364,
      "grad_norm": 0.04871304705739021,
      "learning_rate": 4.6279863481228666e-05,
      "loss": 0.3187,
      "step": 3383
    },
    {
      "epoch": 0.769090909090909,
      "grad_norm": 0.050216738134622574,
      "learning_rate": 4.623435722411832e-05,
      "loss": 0.3821,
      "step": 3384
    },
    {
      "epoch": 0.7693181818181818,
      "grad_norm": 0.038175150752067566,
      "learning_rate": 4.6188850967007965e-05,
      "loss": 0.3074,
      "step": 3385
    },
    {
      "epoch": 0.7695454545454545,
      "grad_norm": 0.04508250951766968,
      "learning_rate": 4.614334470989761e-05,
      "loss": 0.3827,
      "step": 3386
    },
    {
      "epoch": 0.7697727272727273,
      "grad_norm": 0.04015674442052841,
      "learning_rate": 4.6097838452787264e-05,
      "loss": 0.3258,
      "step": 3387
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.04659293591976166,
      "learning_rate": 4.60523321956769e-05,
      "loss": 0.3168,
      "step": 3388
    },
    {
      "epoch": 0.7702272727272728,
      "grad_norm": 0.0482487678527832,
      "learning_rate": 4.6006825938566556e-05,
      "loss": 0.314,
      "step": 3389
    },
    {
      "epoch": 0.7704545454545455,
      "grad_norm": 0.04228971526026726,
      "learning_rate": 4.59613196814562e-05,
      "loss": 0.2963,
      "step": 3390
    },
    {
      "epoch": 0.7706818181818181,
      "grad_norm": 0.046166062355041504,
      "learning_rate": 4.591581342434585e-05,
      "loss": 0.3728,
      "step": 3391
    },
    {
      "epoch": 0.7709090909090909,
      "grad_norm": 0.040350206196308136,
      "learning_rate": 4.58703071672355e-05,
      "loss": 0.2861,
      "step": 3392
    },
    {
      "epoch": 0.7711363636363636,
      "grad_norm": 0.03974081203341484,
      "learning_rate": 4.582480091012514e-05,
      "loss": 0.2883,
      "step": 3393
    },
    {
      "epoch": 0.7713636363636364,
      "grad_norm": 0.0445442721247673,
      "learning_rate": 4.577929465301479e-05,
      "loss": 0.3433,
      "step": 3394
    },
    {
      "epoch": 0.7715909090909091,
      "grad_norm": 0.03432069718837738,
      "learning_rate": 4.573378839590444e-05,
      "loss": 0.2713,
      "step": 3395
    },
    {
      "epoch": 0.7718181818181818,
      "grad_norm": 0.03707334026694298,
      "learning_rate": 4.5688282138794085e-05,
      "loss": 0.32,
      "step": 3396
    },
    {
      "epoch": 0.7720454545454546,
      "grad_norm": 0.03400633484125137,
      "learning_rate": 4.564277588168374e-05,
      "loss": 0.2983,
      "step": 3397
    },
    {
      "epoch": 0.7722727272727272,
      "grad_norm": 0.048412784934043884,
      "learning_rate": 4.559726962457338e-05,
      "loss": 0.3398,
      "step": 3398
    },
    {
      "epoch": 0.7725,
      "grad_norm": 0.04671423137187958,
      "learning_rate": 4.555176336746303e-05,
      "loss": 0.3256,
      "step": 3399
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.039214540272951126,
      "learning_rate": 4.5506257110352676e-05,
      "loss": 0.301,
      "step": 3400
    },
    {
      "epoch": 0.7729545454545454,
      "grad_norm": 0.050326280295848846,
      "learning_rate": 4.546075085324232e-05,
      "loss": 0.3231,
      "step": 3401
    },
    {
      "epoch": 0.7731818181818182,
      "grad_norm": 0.06355173885822296,
      "learning_rate": 4.5415244596131975e-05,
      "loss": 0.4157,
      "step": 3402
    },
    {
      "epoch": 0.7734090909090909,
      "grad_norm": 0.054626405239105225,
      "learning_rate": 4.5369738339021614e-05,
      "loss": 0.3798,
      "step": 3403
    },
    {
      "epoch": 0.7736363636363637,
      "grad_norm": 0.043866273015737534,
      "learning_rate": 4.532423208191127e-05,
      "loss": 0.3093,
      "step": 3404
    },
    {
      "epoch": 0.7738636363636363,
      "grad_norm": 0.04258228465914726,
      "learning_rate": 4.527872582480091e-05,
      "loss": 0.3097,
      "step": 3405
    },
    {
      "epoch": 0.774090909090909,
      "grad_norm": 0.0385441780090332,
      "learning_rate": 4.523321956769056e-05,
      "loss": 0.3164,
      "step": 3406
    },
    {
      "epoch": 0.7743181818181818,
      "grad_norm": 0.043679650872945786,
      "learning_rate": 4.5187713310580205e-05,
      "loss": 0.3245,
      "step": 3407
    },
    {
      "epoch": 0.7745454545454545,
      "grad_norm": 0.04957399517297745,
      "learning_rate": 4.514220705346985e-05,
      "loss": 0.3724,
      "step": 3408
    },
    {
      "epoch": 0.7747727272727273,
      "grad_norm": 0.04974577575922012,
      "learning_rate": 4.5096700796359504e-05,
      "loss": 0.3232,
      "step": 3409
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.03408299759030342,
      "learning_rate": 4.505119453924915e-05,
      "loss": 0.2596,
      "step": 3410
    },
    {
      "epoch": 0.7752272727272728,
      "grad_norm": 0.0438985750079155,
      "learning_rate": 4.5005688282138796e-05,
      "loss": 0.344,
      "step": 3411
    },
    {
      "epoch": 0.7754545454545455,
      "grad_norm": 0.029948977753520012,
      "learning_rate": 4.496018202502844e-05,
      "loss": 0.2724,
      "step": 3412
    },
    {
      "epoch": 0.7756818181818181,
      "grad_norm": 0.040568675845861435,
      "learning_rate": 4.491467576791809e-05,
      "loss": 0.3209,
      "step": 3413
    },
    {
      "epoch": 0.7759090909090909,
      "grad_norm": 0.040412239730358124,
      "learning_rate": 4.486916951080774e-05,
      "loss": 0.3556,
      "step": 3414
    },
    {
      "epoch": 0.7761363636363636,
      "grad_norm": 0.04610581323504448,
      "learning_rate": 4.482366325369739e-05,
      "loss": 0.3689,
      "step": 3415
    },
    {
      "epoch": 0.7763636363636364,
      "grad_norm": 0.037515223026275635,
      "learning_rate": 4.477815699658703e-05,
      "loss": 0.301,
      "step": 3416
    },
    {
      "epoch": 0.7765909090909091,
      "grad_norm": 0.05730167031288147,
      "learning_rate": 4.473265073947668e-05,
      "loss": 0.3486,
      "step": 3417
    },
    {
      "epoch": 0.7768181818181819,
      "grad_norm": 0.04174020513892174,
      "learning_rate": 4.4687144482366325e-05,
      "loss": 0.3354,
      "step": 3418
    },
    {
      "epoch": 0.7770454545454546,
      "grad_norm": 0.04458318278193474,
      "learning_rate": 4.464163822525598e-05,
      "loss": 0.3385,
      "step": 3419
    },
    {
      "epoch": 0.7772727272727272,
      "grad_norm": 0.04019860923290253,
      "learning_rate": 4.4596131968145624e-05,
      "loss": 0.3189,
      "step": 3420
    },
    {
      "epoch": 0.7775,
      "grad_norm": 0.04974035173654556,
      "learning_rate": 4.455062571103527e-05,
      "loss": 0.3344,
      "step": 3421
    },
    {
      "epoch": 0.7777272727272727,
      "grad_norm": 0.0475735068321228,
      "learning_rate": 4.4505119453924916e-05,
      "loss": 0.3085,
      "step": 3422
    },
    {
      "epoch": 0.7779545454545455,
      "grad_norm": 0.03852982819080353,
      "learning_rate": 4.445961319681456e-05,
      "loss": 0.2748,
      "step": 3423
    },
    {
      "epoch": 0.7781818181818182,
      "grad_norm": 0.04332166165113449,
      "learning_rate": 4.4414106939704215e-05,
      "loss": 0.3702,
      "step": 3424
    },
    {
      "epoch": 0.7784090909090909,
      "grad_norm": 0.0416545644402504,
      "learning_rate": 4.436860068259386e-05,
      "loss": 0.3349,
      "step": 3425
    },
    {
      "epoch": 0.7786363636363637,
      "grad_norm": 0.045616935938596725,
      "learning_rate": 4.432309442548351e-05,
      "loss": 0.3351,
      "step": 3426
    },
    {
      "epoch": 0.7788636363636363,
      "grad_norm": 0.03677498176693916,
      "learning_rate": 4.427758816837315e-05,
      "loss": 0.2747,
      "step": 3427
    },
    {
      "epoch": 0.7790909090909091,
      "grad_norm": 0.04140321537852287,
      "learning_rate": 4.42320819112628e-05,
      "loss": 0.2986,
      "step": 3428
    },
    {
      "epoch": 0.7793181818181818,
      "grad_norm": 0.04089152067899704,
      "learning_rate": 4.4186575654152445e-05,
      "loss": 0.3337,
      "step": 3429
    },
    {
      "epoch": 0.7795454545454545,
      "grad_norm": 0.03585590794682503,
      "learning_rate": 4.41410693970421e-05,
      "loss": 0.2766,
      "step": 3430
    },
    {
      "epoch": 0.7797727272727273,
      "grad_norm": 0.043849170207977295,
      "learning_rate": 4.4095563139931744e-05,
      "loss": 0.3497,
      "step": 3431
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04283023253083229,
      "learning_rate": 4.405005688282139e-05,
      "loss": 0.3508,
      "step": 3432
    },
    {
      "epoch": 0.7802272727272728,
      "grad_norm": 0.04460924118757248,
      "learning_rate": 4.4004550625711036e-05,
      "loss": 0.3717,
      "step": 3433
    },
    {
      "epoch": 0.7804545454545454,
      "grad_norm": 0.03447150066494942,
      "learning_rate": 4.395904436860068e-05,
      "loss": 0.3003,
      "step": 3434
    },
    {
      "epoch": 0.7806818181818181,
      "grad_norm": 0.03430330380797386,
      "learning_rate": 4.3913538111490335e-05,
      "loss": 0.2641,
      "step": 3435
    },
    {
      "epoch": 0.7809090909090909,
      "grad_norm": 0.06330829858779907,
      "learning_rate": 4.386803185437998e-05,
      "loss": 0.3452,
      "step": 3436
    },
    {
      "epoch": 0.7811363636363636,
      "grad_norm": 0.04107923433184624,
      "learning_rate": 4.382252559726963e-05,
      "loss": 0.3148,
      "step": 3437
    },
    {
      "epoch": 0.7813636363636364,
      "grad_norm": 0.03702794387936592,
      "learning_rate": 4.377701934015927e-05,
      "loss": 0.3037,
      "step": 3438
    },
    {
      "epoch": 0.7815909090909091,
      "grad_norm": 0.044847819954156876,
      "learning_rate": 4.373151308304892e-05,
      "loss": 0.3279,
      "step": 3439
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.032725587487220764,
      "learning_rate": 4.368600682593857e-05,
      "loss": 0.2574,
      "step": 3440
    },
    {
      "epoch": 0.7820454545454546,
      "grad_norm": 0.042206741869449615,
      "learning_rate": 4.364050056882822e-05,
      "loss": 0.3119,
      "step": 3441
    },
    {
      "epoch": 0.7822727272727272,
      "grad_norm": 0.04005986824631691,
      "learning_rate": 4.3594994311717864e-05,
      "loss": 0.3223,
      "step": 3442
    },
    {
      "epoch": 0.7825,
      "grad_norm": 0.03855567425489426,
      "learning_rate": 4.354948805460751e-05,
      "loss": 0.2928,
      "step": 3443
    },
    {
      "epoch": 0.7827272727272727,
      "grad_norm": 0.03668515384197235,
      "learning_rate": 4.3503981797497156e-05,
      "loss": 0.318,
      "step": 3444
    },
    {
      "epoch": 0.7829545454545455,
      "grad_norm": 0.05037274584174156,
      "learning_rate": 4.345847554038681e-05,
      "loss": 0.3825,
      "step": 3445
    },
    {
      "epoch": 0.7831818181818182,
      "grad_norm": 0.05021633952856064,
      "learning_rate": 4.341296928327645e-05,
      "loss": 0.3445,
      "step": 3446
    },
    {
      "epoch": 0.7834090909090909,
      "grad_norm": 0.03500920906662941,
      "learning_rate": 4.33674630261661e-05,
      "loss": 0.2566,
      "step": 3447
    },
    {
      "epoch": 0.7836363636363637,
      "grad_norm": 0.07723961025476456,
      "learning_rate": 4.332195676905575e-05,
      "loss": 0.4264,
      "step": 3448
    },
    {
      "epoch": 0.7838636363636363,
      "grad_norm": 0.0531775988638401,
      "learning_rate": 4.327645051194539e-05,
      "loss": 0.336,
      "step": 3449
    },
    {
      "epoch": 0.7840909090909091,
      "grad_norm": 0.042121220380067825,
      "learning_rate": 4.3230944254835046e-05,
      "loss": 0.3504,
      "step": 3450
    },
    {
      "epoch": 0.7843181818181818,
      "grad_norm": 0.03901024907827377,
      "learning_rate": 4.3185437997724685e-05,
      "loss": 0.3117,
      "step": 3451
    },
    {
      "epoch": 0.7845454545454545,
      "grad_norm": 0.04474498704075813,
      "learning_rate": 4.313993174061434e-05,
      "loss": 0.3502,
      "step": 3452
    },
    {
      "epoch": 0.7847727272727273,
      "grad_norm": 0.06330586969852448,
      "learning_rate": 4.3094425483503984e-05,
      "loss": 0.3571,
      "step": 3453
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.056624654680490494,
      "learning_rate": 4.304891922639363e-05,
      "loss": 0.4205,
      "step": 3454
    },
    {
      "epoch": 0.7852272727272728,
      "grad_norm": 0.03841058164834976,
      "learning_rate": 4.300341296928328e-05,
      "loss": 0.2759,
      "step": 3455
    },
    {
      "epoch": 0.7854545454545454,
      "grad_norm": 0.042464494705200195,
      "learning_rate": 4.295790671217292e-05,
      "loss": 0.3476,
      "step": 3456
    },
    {
      "epoch": 0.7856818181818181,
      "grad_norm": 0.04655182734131813,
      "learning_rate": 4.2912400455062575e-05,
      "loss": 0.2717,
      "step": 3457
    },
    {
      "epoch": 0.7859090909090909,
      "grad_norm": 0.052356570959091187,
      "learning_rate": 4.286689419795222e-05,
      "loss": 0.3678,
      "step": 3458
    },
    {
      "epoch": 0.7861363636363636,
      "grad_norm": 0.05042549967765808,
      "learning_rate": 4.282138794084187e-05,
      "loss": 0.3368,
      "step": 3459
    },
    {
      "epoch": 0.7863636363636364,
      "grad_norm": 0.049101412296295166,
      "learning_rate": 4.277588168373152e-05,
      "loss": 0.3545,
      "step": 3460
    },
    {
      "epoch": 0.7865909090909091,
      "grad_norm": 0.04130124673247337,
      "learning_rate": 4.273037542662116e-05,
      "loss": 0.2991,
      "step": 3461
    },
    {
      "epoch": 0.7868181818181819,
      "grad_norm": 0.05426903814077377,
      "learning_rate": 4.268486916951081e-05,
      "loss": 0.3658,
      "step": 3462
    },
    {
      "epoch": 0.7870454545454545,
      "grad_norm": 0.04441164806485176,
      "learning_rate": 4.263936291240045e-05,
      "loss": 0.3269,
      "step": 3463
    },
    {
      "epoch": 0.7872727272727272,
      "grad_norm": 0.04398239776492119,
      "learning_rate": 4.2593856655290104e-05,
      "loss": 0.3136,
      "step": 3464
    },
    {
      "epoch": 0.7875,
      "grad_norm": 0.04082394763827324,
      "learning_rate": 4.254835039817976e-05,
      "loss": 0.3343,
      "step": 3465
    },
    {
      "epoch": 0.7877272727272727,
      "grad_norm": 0.051381468772888184,
      "learning_rate": 4.2502844141069396e-05,
      "loss": 0.3752,
      "step": 3466
    },
    {
      "epoch": 0.7879545454545455,
      "grad_norm": 0.058570731431245804,
      "learning_rate": 4.245733788395905e-05,
      "loss": 0.4156,
      "step": 3467
    },
    {
      "epoch": 0.7881818181818182,
      "grad_norm": 0.037387337535619736,
      "learning_rate": 4.241183162684869e-05,
      "loss": 0.2796,
      "step": 3468
    },
    {
      "epoch": 0.788409090909091,
      "grad_norm": 0.041390422731637955,
      "learning_rate": 4.236632536973834e-05,
      "loss": 0.2802,
      "step": 3469
    },
    {
      "epoch": 0.7886363636363637,
      "grad_norm": 0.040246520191431046,
      "learning_rate": 4.2320819112627994e-05,
      "loss": 0.2877,
      "step": 3470
    },
    {
      "epoch": 0.7888636363636363,
      "grad_norm": 0.04382556676864624,
      "learning_rate": 4.227531285551763e-05,
      "loss": 0.3134,
      "step": 3471
    },
    {
      "epoch": 0.7890909090909091,
      "grad_norm": 0.03709733858704567,
      "learning_rate": 4.2229806598407286e-05,
      "loss": 0.2556,
      "step": 3472
    },
    {
      "epoch": 0.7893181818181818,
      "grad_norm": 0.037211038172245026,
      "learning_rate": 4.2184300341296925e-05,
      "loss": 0.2934,
      "step": 3473
    },
    {
      "epoch": 0.7895454545454546,
      "grad_norm": 0.05602097138762474,
      "learning_rate": 4.213879408418658e-05,
      "loss": 0.3556,
      "step": 3474
    },
    {
      "epoch": 0.7897727272727273,
      "grad_norm": 0.048653487116098404,
      "learning_rate": 4.2093287827076224e-05,
      "loss": 0.3731,
      "step": 3475
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.05117229372262955,
      "learning_rate": 4.204778156996587e-05,
      "loss": 0.3706,
      "step": 3476
    },
    {
      "epoch": 0.7902272727272728,
      "grad_norm": 0.046434227377176285,
      "learning_rate": 4.200227531285552e-05,
      "loss": 0.3357,
      "step": 3477
    },
    {
      "epoch": 0.7904545454545454,
      "grad_norm": 0.042857181280851364,
      "learning_rate": 4.195676905574516e-05,
      "loss": 0.3295,
      "step": 3478
    },
    {
      "epoch": 0.7906818181818182,
      "grad_norm": 0.04990006983280182,
      "learning_rate": 4.1911262798634815e-05,
      "loss": 0.3145,
      "step": 3479
    },
    {
      "epoch": 0.7909090909090909,
      "grad_norm": 0.04942294582724571,
      "learning_rate": 4.186575654152446e-05,
      "loss": 0.3739,
      "step": 3480
    },
    {
      "epoch": 0.7911363636363636,
      "grad_norm": 0.04001873359084129,
      "learning_rate": 4.182025028441411e-05,
      "loss": 0.359,
      "step": 3481
    },
    {
      "epoch": 0.7913636363636364,
      "grad_norm": 0.04370416700839996,
      "learning_rate": 4.177474402730376e-05,
      "loss": 0.2823,
      "step": 3482
    },
    {
      "epoch": 0.7915909090909091,
      "grad_norm": 0.04166734963655472,
      "learning_rate": 4.17292377701934e-05,
      "loss": 0.3404,
      "step": 3483
    },
    {
      "epoch": 0.7918181818181819,
      "grad_norm": 0.04049460589885712,
      "learning_rate": 4.168373151308305e-05,
      "loss": 0.3061,
      "step": 3484
    },
    {
      "epoch": 0.7920454545454545,
      "grad_norm": 0.042129699140787125,
      "learning_rate": 4.16382252559727e-05,
      "loss": 0.2981,
      "step": 3485
    },
    {
      "epoch": 0.7922727272727272,
      "grad_norm": 0.04242798313498497,
      "learning_rate": 4.1592718998862344e-05,
      "loss": 0.2911,
      "step": 3486
    },
    {
      "epoch": 0.7925,
      "grad_norm": 0.046355802565813065,
      "learning_rate": 4.1547212741752e-05,
      "loss": 0.382,
      "step": 3487
    },
    {
      "epoch": 0.7927272727272727,
      "grad_norm": 0.04481297731399536,
      "learning_rate": 4.1501706484641636e-05,
      "loss": 0.3184,
      "step": 3488
    },
    {
      "epoch": 0.7929545454545455,
      "grad_norm": 0.04164711385965347,
      "learning_rate": 4.145620022753129e-05,
      "loss": 0.3376,
      "step": 3489
    },
    {
      "epoch": 0.7931818181818182,
      "grad_norm": 0.0634385496377945,
      "learning_rate": 4.1410693970420935e-05,
      "loss": 0.3479,
      "step": 3490
    },
    {
      "epoch": 0.793409090909091,
      "grad_norm": 0.03728441521525383,
      "learning_rate": 4.136518771331058e-05,
      "loss": 0.3126,
      "step": 3491
    },
    {
      "epoch": 0.7936363636363636,
      "grad_norm": 0.04876379668712616,
      "learning_rate": 4.1319681456200234e-05,
      "loss": 0.3564,
      "step": 3492
    },
    {
      "epoch": 0.7938636363636363,
      "grad_norm": 0.048412177711725235,
      "learning_rate": 4.1274175199089873e-05,
      "loss": 0.3668,
      "step": 3493
    },
    {
      "epoch": 0.7940909090909091,
      "grad_norm": 0.05047299712896347,
      "learning_rate": 4.1228668941979526e-05,
      "loss": 0.3368,
      "step": 3494
    },
    {
      "epoch": 0.7943181818181818,
      "grad_norm": 0.03888048604130745,
      "learning_rate": 4.118316268486917e-05,
      "loss": 0.2672,
      "step": 3495
    },
    {
      "epoch": 0.7945454545454546,
      "grad_norm": 0.038801152259111404,
      "learning_rate": 4.113765642775882e-05,
      "loss": 0.3468,
      "step": 3496
    },
    {
      "epoch": 0.7947727272727273,
      "grad_norm": 0.04328472912311554,
      "learning_rate": 4.1092150170648464e-05,
      "loss": 0.348,
      "step": 3497
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.04055706039071083,
      "learning_rate": 4.104664391353811e-05,
      "loss": 0.3225,
      "step": 3498
    },
    {
      "epoch": 0.7952272727272728,
      "grad_norm": 0.03791654855012894,
      "learning_rate": 4.100113765642776e-05,
      "loss": 0.251,
      "step": 3499
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 0.054446615278720856,
      "learning_rate": 4.095563139931741e-05,
      "loss": 0.3507,
      "step": 3500
    }
  ],
  "logging_steps": 1,
  "max_steps": 4400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.140396707892347e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
