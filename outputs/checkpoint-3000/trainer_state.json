{
  "best_metric": 1.1555155515670776,
  "best_model_checkpoint": "outputs/checkpoint-1500",
  "epoch": 3.0,
  "eval_steps": 100,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 0.07153336703777313,
      "learning_rate": 0.0004,
      "loss": 0.3265,
      "step": 1
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.07364838570356369,
      "learning_rate": 0.0008,
      "loss": 0.3142,
      "step": 2
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.12498562783002853,
      "learning_rate": 0.0012,
      "loss": 0.3848,
      "step": 3
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.18664319813251495,
      "learning_rate": 0.0016,
      "loss": 0.3473,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.0481288433074951,
      "learning_rate": 0.002,
      "loss": 0.5477,
      "step": 5
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.36579111218452454,
      "learning_rate": 0.0019999977920603196,
      "loss": 0.4836,
      "step": 6
    },
    {
      "epoch": 0.014,
      "grad_norm": 2.3086600303649902,
      "learning_rate": 0.0019999911682510277,
      "loss": 0.5702,
      "step": 7
    },
    {
      "epoch": 0.016,
      "grad_norm": 22.525005340576172,
      "learning_rate": 0.001999980128601375,
      "loss": 1.0408,
      "step": 8
    },
    {
      "epoch": 0.018,
      "grad_norm": 21.848602294921875,
      "learning_rate": 0.00199996467316011,
      "loss": 3.2526,
      "step": 9
    },
    {
      "epoch": 0.02,
      "grad_norm": 14.757415771484375,
      "learning_rate": 0.001999944801995484,
      "loss": 1.8052,
      "step": 10
    },
    {
      "epoch": 0.022,
      "grad_norm": 17.1826171875,
      "learning_rate": 0.0019999205151952438,
      "loss": 2.3941,
      "step": 11
    },
    {
      "epoch": 0.024,
      "grad_norm": 6.885984420776367,
      "learning_rate": 0.001999891812866638,
      "loss": 0.9802,
      "step": 12
    },
    {
      "epoch": 0.026,
      "grad_norm": 10.152630805969238,
      "learning_rate": 0.0019998586951364126,
      "loss": 1.7751,
      "step": 13
    },
    {
      "epoch": 0.028,
      "grad_norm": 7.794259548187256,
      "learning_rate": 0.0019998211621508107,
      "loss": 0.9923,
      "step": 14
    },
    {
      "epoch": 0.03,
      "grad_norm": 9.997300148010254,
      "learning_rate": 0.0019997792140755742,
      "loss": 1.235,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 6.218258380889893,
      "learning_rate": 0.0019997328510959413,
      "loss": 0.8716,
      "step": 16
    },
    {
      "epoch": 0.034,
      "grad_norm": 7.197497844696045,
      "learning_rate": 0.001999682073416644,
      "loss": 0.9204,
      "step": 17
    },
    {
      "epoch": 0.036,
      "grad_norm": 40.00090789794922,
      "learning_rate": 0.0019996268812619107,
      "loss": 2.4472,
      "step": 18
    },
    {
      "epoch": 0.038,
      "grad_norm": 5.918007850646973,
      "learning_rate": 0.001999567274875464,
      "loss": 0.8146,
      "step": 19
    },
    {
      "epoch": 0.04,
      "grad_norm": 14.861492156982422,
      "learning_rate": 0.0019995032545205176,
      "loss": 1.3966,
      "step": 20
    },
    {
      "epoch": 0.042,
      "grad_norm": 15.856351852416992,
      "learning_rate": 0.0019994348204797788,
      "loss": 0.982,
      "step": 21
    },
    {
      "epoch": 0.044,
      "grad_norm": 11.071501731872559,
      "learning_rate": 0.001999361973055443,
      "loss": 1.5479,
      "step": 22
    },
    {
      "epoch": 0.046,
      "grad_norm": 108.72428894042969,
      "learning_rate": 0.001999284712569196,
      "loss": 3.0887,
      "step": 23
    },
    {
      "epoch": 0.048,
      "grad_norm": 59.42506408691406,
      "learning_rate": 0.0019992030393622107,
      "loss": 3.3124,
      "step": 24
    },
    {
      "epoch": 0.05,
      "grad_norm": 5.313709735870361,
      "learning_rate": 0.0019991169537951466,
      "loss": 0.9667,
      "step": 25
    },
    {
      "epoch": 0.052,
      "grad_norm": 13.352042198181152,
      "learning_rate": 0.001999026456248147,
      "loss": 1.5735,
      "step": 26
    },
    {
      "epoch": 0.054,
      "grad_norm": 4.222155570983887,
      "learning_rate": 0.001998931547120838,
      "loss": 1.113,
      "step": 27
    },
    {
      "epoch": 0.056,
      "grad_norm": 8.537115097045898,
      "learning_rate": 0.0019988322268323267,
      "loss": 1.0243,
      "step": 28
    },
    {
      "epoch": 0.058,
      "grad_norm": 1.7223957777023315,
      "learning_rate": 0.0019987284958211996,
      "loss": 0.6193,
      "step": 29
    },
    {
      "epoch": 0.06,
      "grad_norm": 18.858154296875,
      "learning_rate": 0.0019986203545455205,
      "loss": 1.6633,
      "step": 30
    },
    {
      "epoch": 0.062,
      "grad_norm": 14.651820182800293,
      "learning_rate": 0.001998507803482828,
      "loss": 2.0824,
      "step": 31
    },
    {
      "epoch": 0.064,
      "grad_norm": 33.13444900512695,
      "learning_rate": 0.0019983908431301343,
      "loss": 2.237,
      "step": 32
    },
    {
      "epoch": 0.066,
      "grad_norm": 49.49964141845703,
      "learning_rate": 0.001998269474003922,
      "loss": 3.2682,
      "step": 33
    },
    {
      "epoch": 0.068,
      "grad_norm": 11.279411315917969,
      "learning_rate": 0.0019981436966401427,
      "loss": 1.8529,
      "step": 34
    },
    {
      "epoch": 0.07,
      "grad_norm": 11.400317192077637,
      "learning_rate": 0.0019980135115942135,
      "loss": 1.2975,
      "step": 35
    },
    {
      "epoch": 0.072,
      "grad_norm": 2.172196865081787,
      "learning_rate": 0.0019978789194410166,
      "loss": 0.667,
      "step": 36
    },
    {
      "epoch": 0.074,
      "grad_norm": 4.283565998077393,
      "learning_rate": 0.0019977399207748944,
      "loss": 0.7487,
      "step": 37
    },
    {
      "epoch": 0.076,
      "grad_norm": 16.038217544555664,
      "learning_rate": 0.0019975965162096483,
      "loss": 1.1877,
      "step": 38
    },
    {
      "epoch": 0.078,
      "grad_norm": 42.251346588134766,
      "learning_rate": 0.0019974487063785353,
      "loss": 1.4782,
      "step": 39
    },
    {
      "epoch": 0.08,
      "grad_norm": 38.56047058105469,
      "learning_rate": 0.0019972964919342663,
      "loss": 5.3519,
      "step": 40
    },
    {
      "epoch": 0.082,
      "grad_norm": 99.82169342041016,
      "learning_rate": 0.0019971398735490016,
      "loss": 13.7152,
      "step": 41
    },
    {
      "epoch": 0.084,
      "grad_norm": 53.84722900390625,
      "learning_rate": 0.001996978851914349,
      "loss": 14.451,
      "step": 42
    },
    {
      "epoch": 0.086,
      "grad_norm": 23.021757125854492,
      "learning_rate": 0.0019968134277413606,
      "loss": 10.9631,
      "step": 43
    },
    {
      "epoch": 0.088,
      "grad_norm": 30.860822677612305,
      "learning_rate": 0.0019966436017605296,
      "loss": 12.2249,
      "step": 44
    },
    {
      "epoch": 0.09,
      "grad_norm": 26.272480010986328,
      "learning_rate": 0.0019964693747217873,
      "loss": 10.3854,
      "step": 45
    },
    {
      "epoch": 0.092,
      "grad_norm": 25.899433135986328,
      "learning_rate": 0.0019962907473944995,
      "loss": 11.9361,
      "step": 46
    },
    {
      "epoch": 0.094,
      "grad_norm": 16.461830139160156,
      "learning_rate": 0.001996107720567462,
      "loss": 10.2164,
      "step": 47
    },
    {
      "epoch": 0.096,
      "grad_norm": 7.193535804748535,
      "learning_rate": 0.0019959202950489,
      "loss": 5.7087,
      "step": 48
    },
    {
      "epoch": 0.098,
      "grad_norm": 33.34125900268555,
      "learning_rate": 0.0019957284716664615,
      "loss": 11.7368,
      "step": 49
    },
    {
      "epoch": 0.1,
      "grad_norm": 24.017000198364258,
      "learning_rate": 0.001995532251267216,
      "loss": 5.1569,
      "step": 50
    },
    {
      "epoch": 0.102,
      "grad_norm": 54.824546813964844,
      "learning_rate": 0.0019953316347176486,
      "loss": 15.9276,
      "step": 51
    },
    {
      "epoch": 0.104,
      "grad_norm": 35.168212890625,
      "learning_rate": 0.001995126622903658,
      "loss": 12.9034,
      "step": 52
    },
    {
      "epoch": 0.106,
      "grad_norm": 22.955968856811523,
      "learning_rate": 0.0019949172167305516,
      "loss": 8.2095,
      "step": 53
    },
    {
      "epoch": 0.108,
      "grad_norm": 11.206243515014648,
      "learning_rate": 0.001994703417123042,
      "loss": 5.0177,
      "step": 54
    },
    {
      "epoch": 0.11,
      "grad_norm": 25.80901527404785,
      "learning_rate": 0.0019944852250252418,
      "loss": 10.7141,
      "step": 55
    },
    {
      "epoch": 0.112,
      "grad_norm": 26.53685760498047,
      "learning_rate": 0.0019942626414006614,
      "loss": 8.0554,
      "step": 56
    },
    {
      "epoch": 0.114,
      "grad_norm": 26.4266300201416,
      "learning_rate": 0.0019940356672322034,
      "loss": 8.1232,
      "step": 57
    },
    {
      "epoch": 0.116,
      "grad_norm": 27.160112380981445,
      "learning_rate": 0.0019938043035221584,
      "loss": 8.1943,
      "step": 58
    },
    {
      "epoch": 0.118,
      "grad_norm": 14.713798522949219,
      "learning_rate": 0.0019935685512922005,
      "loss": 6.0858,
      "step": 59
    },
    {
      "epoch": 0.12,
      "grad_norm": 37.84613800048828,
      "learning_rate": 0.0019933284115833828,
      "loss": 6.1043,
      "step": 60
    },
    {
      "epoch": 0.122,
      "grad_norm": 36.95431900024414,
      "learning_rate": 0.001993083885456134,
      "loss": 5.8995,
      "step": 61
    },
    {
      "epoch": 0.124,
      "grad_norm": 37.088035583496094,
      "learning_rate": 0.001992834973990251,
      "loss": 7.143,
      "step": 62
    },
    {
      "epoch": 0.126,
      "grad_norm": 39.99118423461914,
      "learning_rate": 0.0019925816782848976,
      "loss": 6.7677,
      "step": 63
    },
    {
      "epoch": 0.128,
      "grad_norm": 8.601700782775879,
      "learning_rate": 0.0019923239994585965,
      "loss": 4.354,
      "step": 64
    },
    {
      "epoch": 0.13,
      "grad_norm": 76.58305358886719,
      "learning_rate": 0.0019920619386492268,
      "loss": 8.6727,
      "step": 65
    },
    {
      "epoch": 0.132,
      "grad_norm": 215.24545288085938,
      "learning_rate": 0.0019917954970140174,
      "loss": 19.3494,
      "step": 66
    },
    {
      "epoch": 0.134,
      "grad_norm": 317.1894836425781,
      "learning_rate": 0.001991524675729542,
      "loss": 13.2993,
      "step": 67
    },
    {
      "epoch": 0.136,
      "grad_norm": 63.986480712890625,
      "learning_rate": 0.001991249475991715,
      "loss": 8.1727,
      "step": 68
    },
    {
      "epoch": 0.138,
      "grad_norm": 61.14423370361328,
      "learning_rate": 0.001990969899015785,
      "loss": 18.8724,
      "step": 69
    },
    {
      "epoch": 0.14,
      "grad_norm": 48.705894470214844,
      "learning_rate": 0.0019906859460363307,
      "loss": 18.8833,
      "step": 70
    },
    {
      "epoch": 0.142,
      "grad_norm": 44.769229888916016,
      "learning_rate": 0.001990397618307254,
      "loss": 9.431,
      "step": 71
    },
    {
      "epoch": 0.144,
      "grad_norm": 36.0777702331543,
      "learning_rate": 0.001990104917101775,
      "loss": 9.1747,
      "step": 72
    },
    {
      "epoch": 0.146,
      "grad_norm": 20.258756637573242,
      "learning_rate": 0.0019898078437124273,
      "loss": 9.9993,
      "step": 73
    },
    {
      "epoch": 0.148,
      "grad_norm": 19.348947525024414,
      "learning_rate": 0.001989506399451051,
      "loss": 8.0574,
      "step": 74
    },
    {
      "epoch": 0.15,
      "grad_norm": 12.677506446838379,
      "learning_rate": 0.0019892005856487877,
      "loss": 4.6704,
      "step": 75
    },
    {
      "epoch": 0.152,
      "grad_norm": 7.288967132568359,
      "learning_rate": 0.0019888904036560744,
      "loss": 4.3465,
      "step": 76
    },
    {
      "epoch": 0.154,
      "grad_norm": 25.731111526489258,
      "learning_rate": 0.0019885758548426366,
      "loss": 4.551,
      "step": 77
    },
    {
      "epoch": 0.156,
      "grad_norm": 10.50670051574707,
      "learning_rate": 0.001988256940597485,
      "loss": 4.0567,
      "step": 78
    },
    {
      "epoch": 0.158,
      "grad_norm": 38.44298553466797,
      "learning_rate": 0.0019879336623289056,
      "loss": 5.0879,
      "step": 79
    },
    {
      "epoch": 0.16,
      "grad_norm": 18.668872833251953,
      "learning_rate": 0.0019876060214644568,
      "loss": 5.8952,
      "step": 80
    },
    {
      "epoch": 0.162,
      "grad_norm": 10.860904693603516,
      "learning_rate": 0.0019872740194509606,
      "loss": 4.8303,
      "step": 81
    },
    {
      "epoch": 0.164,
      "grad_norm": 16.271303176879883,
      "learning_rate": 0.0019869376577544983,
      "loss": 4.5477,
      "step": 82
    },
    {
      "epoch": 0.166,
      "grad_norm": 16.538307189941406,
      "learning_rate": 0.001986596937860402,
      "loss": 4.1279,
      "step": 83
    },
    {
      "epoch": 0.168,
      "grad_norm": 10.17186450958252,
      "learning_rate": 0.0019862518612732503,
      "loss": 3.4794,
      "step": 84
    },
    {
      "epoch": 0.17,
      "grad_norm": 126.83295440673828,
      "learning_rate": 0.0019859024295168595,
      "loss": 3.9457,
      "step": 85
    },
    {
      "epoch": 0.172,
      "grad_norm": 12.776949882507324,
      "learning_rate": 0.001985548644134278,
      "loss": 4.4403,
      "step": 86
    },
    {
      "epoch": 0.174,
      "grad_norm": 17.375917434692383,
      "learning_rate": 0.0019851905066877794,
      "loss": 4.2164,
      "step": 87
    },
    {
      "epoch": 0.176,
      "grad_norm": 8.938773155212402,
      "learning_rate": 0.0019848280187588557,
      "loss": 3.6192,
      "step": 88
    },
    {
      "epoch": 0.178,
      "grad_norm": 5.6368088722229,
      "learning_rate": 0.0019844611819482094,
      "loss": 3.4678,
      "step": 89
    },
    {
      "epoch": 0.18,
      "grad_norm": 6.4656782150268555,
      "learning_rate": 0.0019840899978757483,
      "loss": 3.3958,
      "step": 90
    },
    {
      "epoch": 0.182,
      "grad_norm": 3.5705130100250244,
      "learning_rate": 0.0019837144681805756,
      "loss": 3.1315,
      "step": 91
    },
    {
      "epoch": 0.184,
      "grad_norm": 4.337969779968262,
      "learning_rate": 0.0019833345945209856,
      "loss": 3.2196,
      "step": 92
    },
    {
      "epoch": 0.186,
      "grad_norm": 4.525212287902832,
      "learning_rate": 0.001982950378574455,
      "loss": 3.3266,
      "step": 93
    },
    {
      "epoch": 0.188,
      "grad_norm": 10.478811264038086,
      "learning_rate": 0.0019825618220376344,
      "loss": 3.6558,
      "step": 94
    },
    {
      "epoch": 0.19,
      "grad_norm": 2.0926499366760254,
      "learning_rate": 0.0019821689266263424,
      "loss": 2.8955,
      "step": 95
    },
    {
      "epoch": 0.192,
      "grad_norm": 7.8299784660339355,
      "learning_rate": 0.0019817716940755585,
      "loss": 3.3969,
      "step": 96
    },
    {
      "epoch": 0.194,
      "grad_norm": 21.893238067626953,
      "learning_rate": 0.0019813701261394137,
      "loss": 6.187,
      "step": 97
    },
    {
      "epoch": 0.196,
      "grad_norm": 12.930903434753418,
      "learning_rate": 0.001980964224591183,
      "loss": 4.349,
      "step": 98
    },
    {
      "epoch": 0.198,
      "grad_norm": 6.7530388832092285,
      "learning_rate": 0.0019805539912232783,
      "loss": 3.2752,
      "step": 99
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.532876491546631,
      "learning_rate": 0.0019801394278472417,
      "loss": 3.7424,
      "step": 100
    },
    {
      "epoch": 0.2,
      "eval_loss": 3.5269076824188232,
      "eval_runtime": 228.953,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 100
    },
    {
      "epoch": 0.202,
      "grad_norm": 5.784553050994873,
      "learning_rate": 0.0019797205362937346,
      "loss": 3.3643,
      "step": 101
    },
    {
      "epoch": 0.204,
      "grad_norm": 9.127640724182129,
      "learning_rate": 0.0019792973184125317,
      "loss": 3.704,
      "step": 102
    },
    {
      "epoch": 0.206,
      "grad_norm": 11.89439868927002,
      "learning_rate": 0.001978869776072512,
      "loss": 3.7486,
      "step": 103
    },
    {
      "epoch": 0.208,
      "grad_norm": 3.653444528579712,
      "learning_rate": 0.0019784379111616505,
      "loss": 3.1518,
      "step": 104
    },
    {
      "epoch": 0.21,
      "grad_norm": 6.55711555480957,
      "learning_rate": 0.001978001725587011,
      "loss": 3.7818,
      "step": 105
    },
    {
      "epoch": 0.212,
      "grad_norm": 5.933317184448242,
      "learning_rate": 0.001977561221274737,
      "loss": 3.4243,
      "step": 106
    },
    {
      "epoch": 0.214,
      "grad_norm": 6.509677886962891,
      "learning_rate": 0.001977116400170041,
      "loss": 3.2879,
      "step": 107
    },
    {
      "epoch": 0.216,
      "grad_norm": 4.7551188468933105,
      "learning_rate": 0.0019766672642372,
      "loss": 3.221,
      "step": 108
    },
    {
      "epoch": 0.218,
      "grad_norm": 5.226152420043945,
      "learning_rate": 0.0019762138154595446,
      "loss": 3.4831,
      "step": 109
    },
    {
      "epoch": 0.22,
      "grad_norm": 5.412924289703369,
      "learning_rate": 0.0019757560558394493,
      "loss": 3.2454,
      "step": 110
    },
    {
      "epoch": 0.222,
      "grad_norm": 2.984659433364868,
      "learning_rate": 0.0019752939873983254,
      "loss": 3.0154,
      "step": 111
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.7997300624847412,
      "learning_rate": 0.0019748276121766117,
      "loss": 2.984,
      "step": 112
    },
    {
      "epoch": 0.226,
      "grad_norm": 4.594057083129883,
      "learning_rate": 0.001974356932233764,
      "loss": 2.9406,
      "step": 113
    },
    {
      "epoch": 0.228,
      "grad_norm": 3.41648268699646,
      "learning_rate": 0.0019738819496482496,
      "loss": 3.0348,
      "step": 114
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.4951449632644653,
      "learning_rate": 0.0019734026665175334,
      "loss": 2.7256,
      "step": 115
    },
    {
      "epoch": 0.232,
      "grad_norm": 1.9623335599899292,
      "learning_rate": 0.001972919084958072,
      "loss": 2.8238,
      "step": 116
    },
    {
      "epoch": 0.234,
      "grad_norm": 1.9828375577926636,
      "learning_rate": 0.001972431207105303,
      "loss": 2.6557,
      "step": 117
    },
    {
      "epoch": 0.236,
      "grad_norm": 2.3744359016418457,
      "learning_rate": 0.001971939035113636,
      "loss": 2.6103,
      "step": 118
    },
    {
      "epoch": 0.238,
      "grad_norm": 3.2531588077545166,
      "learning_rate": 0.0019714425711564445,
      "loss": 2.7168,
      "step": 119
    },
    {
      "epoch": 0.24,
      "grad_norm": 7.147645950317383,
      "learning_rate": 0.001970941817426052,
      "loss": 2.9605,
      "step": 120
    },
    {
      "epoch": 0.242,
      "grad_norm": 2.484334945678711,
      "learning_rate": 0.001970436776133727,
      "loss": 2.7385,
      "step": 121
    },
    {
      "epoch": 0.244,
      "grad_norm": 1.369204044342041,
      "learning_rate": 0.0019699274495096715,
      "loss": 2.569,
      "step": 122
    },
    {
      "epoch": 0.246,
      "grad_norm": 10.312801361083984,
      "learning_rate": 0.0019694138398030094,
      "loss": 3.4052,
      "step": 123
    },
    {
      "epoch": 0.248,
      "grad_norm": 9.634992599487305,
      "learning_rate": 0.00196889594928178,
      "loss": 3.2181,
      "step": 124
    },
    {
      "epoch": 0.25,
      "grad_norm": 6.4653096199035645,
      "learning_rate": 0.001968373780232924,
      "loss": 2.8706,
      "step": 125
    },
    {
      "epoch": 0.252,
      "grad_norm": 4.661166667938232,
      "learning_rate": 0.0019678473349622793,
      "loss": 2.9823,
      "step": 126
    },
    {
      "epoch": 0.254,
      "grad_norm": 11.554218292236328,
      "learning_rate": 0.0019673166157945627,
      "loss": 3.067,
      "step": 127
    },
    {
      "epoch": 0.256,
      "grad_norm": 12.641718864440918,
      "learning_rate": 0.001966781625073367,
      "loss": 2.8902,
      "step": 128
    },
    {
      "epoch": 0.258,
      "grad_norm": 2.1051597595214844,
      "learning_rate": 0.001966242365161146,
      "loss": 2.687,
      "step": 129
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.1415337324142456,
      "learning_rate": 0.0019656988384392075,
      "loss": 2.6062,
      "step": 130
    },
    {
      "epoch": 0.262,
      "grad_norm": 7.867302417755127,
      "learning_rate": 0.0019651510473076986,
      "loss": 3.0761,
      "step": 131
    },
    {
      "epoch": 0.264,
      "grad_norm": 8.365605354309082,
      "learning_rate": 0.0019645989941856,
      "loss": 3.1222,
      "step": 132
    },
    {
      "epoch": 0.266,
      "grad_norm": 2.648066282272339,
      "learning_rate": 0.0019640426815107108,
      "loss": 2.7301,
      "step": 133
    },
    {
      "epoch": 0.268,
      "grad_norm": 4.7227349281311035,
      "learning_rate": 0.001963482111739641,
      "loss": 2.7605,
      "step": 134
    },
    {
      "epoch": 0.27,
      "grad_norm": 3.2076096534729004,
      "learning_rate": 0.0019629172873477994,
      "loss": 2.7632,
      "step": 135
    },
    {
      "epoch": 0.272,
      "grad_norm": 2.9289326667785645,
      "learning_rate": 0.0019623482108293818,
      "loss": 2.6788,
      "step": 136
    },
    {
      "epoch": 0.274,
      "grad_norm": 3.5195915699005127,
      "learning_rate": 0.001961774884697362,
      "loss": 2.5345,
      "step": 137
    },
    {
      "epoch": 0.276,
      "grad_norm": 2.2036643028259277,
      "learning_rate": 0.001961197311483479,
      "loss": 2.614,
      "step": 138
    },
    {
      "epoch": 0.278,
      "grad_norm": 2.5068509578704834,
      "learning_rate": 0.001960615493738226,
      "loss": 2.5876,
      "step": 139
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.586123943328857,
      "learning_rate": 0.0019600294340308398,
      "loss": 2.5494,
      "step": 140
    },
    {
      "epoch": 0.282,
      "grad_norm": 2.2469773292541504,
      "learning_rate": 0.0019594391349492903,
      "loss": 2.4626,
      "step": 141
    },
    {
      "epoch": 0.284,
      "grad_norm": 3.114971399307251,
      "learning_rate": 0.001958844599100266,
      "loss": 2.5529,
      "step": 142
    },
    {
      "epoch": 0.286,
      "grad_norm": 3.3518009185791016,
      "learning_rate": 0.0019582458291091663,
      "loss": 2.4463,
      "step": 143
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.3204469680786133,
      "learning_rate": 0.001957642827620087,
      "loss": 2.3298,
      "step": 144
    },
    {
      "epoch": 0.29,
      "grad_norm": 2.791255235671997,
      "learning_rate": 0.00195703559729581,
      "loss": 2.4031,
      "step": 145
    },
    {
      "epoch": 0.292,
      "grad_norm": 2.419426918029785,
      "learning_rate": 0.00195642414081779,
      "loss": 2.2956,
      "step": 146
    },
    {
      "epoch": 0.294,
      "grad_norm": 3.620445728302002,
      "learning_rate": 0.0019558084608861472,
      "loss": 2.3149,
      "step": 147
    },
    {
      "epoch": 0.296,
      "grad_norm": 3.8125832080841064,
      "learning_rate": 0.001955188560219648,
      "loss": 2.3881,
      "step": 148
    },
    {
      "epoch": 0.298,
      "grad_norm": 5.389157772064209,
      "learning_rate": 0.0019545644415557,
      "loss": 2.4915,
      "step": 149
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.9504411220550537,
      "learning_rate": 0.001953936107650336,
      "loss": 2.3557,
      "step": 150
    },
    {
      "epoch": 0.302,
      "grad_norm": 4.520712375640869,
      "learning_rate": 0.001953303561278202,
      "loss": 2.4204,
      "step": 151
    },
    {
      "epoch": 0.304,
      "grad_norm": 2.5971944332122803,
      "learning_rate": 0.0019526668052325467,
      "loss": 2.277,
      "step": 152
    },
    {
      "epoch": 0.306,
      "grad_norm": 4.897330284118652,
      "learning_rate": 0.001952025842325208,
      "loss": 2.3802,
      "step": 153
    },
    {
      "epoch": 0.308,
      "grad_norm": 7.155513763427734,
      "learning_rate": 0.0019513806753866014,
      "loss": 2.5903,
      "step": 154
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.4044182300567627,
      "learning_rate": 0.0019507313072657055,
      "loss": 2.3653,
      "step": 155
    },
    {
      "epoch": 0.312,
      "grad_norm": 2.7513935565948486,
      "learning_rate": 0.0019500777408300517,
      "loss": 2.5413,
      "step": 156
    },
    {
      "epoch": 0.314,
      "grad_norm": 3.6750152111053467,
      "learning_rate": 0.001949419978965711,
      "loss": 2.4324,
      "step": 157
    },
    {
      "epoch": 0.316,
      "grad_norm": 2.800503730773926,
      "learning_rate": 0.00194875802457728,
      "loss": 2.4592,
      "step": 158
    },
    {
      "epoch": 0.318,
      "grad_norm": 1.5488383769989014,
      "learning_rate": 0.0019480918805878697,
      "loss": 2.5536,
      "step": 159
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.892378807067871,
      "learning_rate": 0.001947421549939091,
      "loss": 2.3528,
      "step": 160
    },
    {
      "epoch": 0.322,
      "grad_norm": 2.5410561561584473,
      "learning_rate": 0.0019467470355910438,
      "loss": 2.5435,
      "step": 161
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.5102311372756958,
      "learning_rate": 0.0019460683405223018,
      "loss": 2.3692,
      "step": 162
    },
    {
      "epoch": 0.326,
      "grad_norm": 1.8860889673233032,
      "learning_rate": 0.001945385467729901,
      "loss": 2.4393,
      "step": 163
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.9577900171279907,
      "learning_rate": 0.0019446984202293246,
      "loss": 2.6922,
      "step": 164
    },
    {
      "epoch": 0.33,
      "grad_norm": 2.702770948410034,
      "learning_rate": 0.0019440072010544918,
      "loss": 2.4289,
      "step": 165
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.7343031167984009,
      "learning_rate": 0.001943311813257743,
      "loss": 2.4545,
      "step": 166
    },
    {
      "epoch": 0.334,
      "grad_norm": 2.591649055480957,
      "learning_rate": 0.001942612259909827,
      "loss": 2.3311,
      "step": 167
    },
    {
      "epoch": 0.336,
      "grad_norm": 4.664798736572266,
      "learning_rate": 0.0019419085440998871,
      "loss": 2.3579,
      "step": 168
    },
    {
      "epoch": 0.338,
      "grad_norm": 2.376110792160034,
      "learning_rate": 0.0019412006689354469,
      "loss": 2.3356,
      "step": 169
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.4358927011489868,
      "learning_rate": 0.0019404886375423982,
      "loss": 2.5352,
      "step": 170
    },
    {
      "epoch": 0.342,
      "grad_norm": 5.066070556640625,
      "learning_rate": 0.0019397724530649857,
      "loss": 2.286,
      "step": 171
    },
    {
      "epoch": 0.344,
      "grad_norm": 3.3609206676483154,
      "learning_rate": 0.0019390521186657935,
      "loss": 2.4734,
      "step": 172
    },
    {
      "epoch": 0.346,
      "grad_norm": 2.534966230392456,
      "learning_rate": 0.001938327637525731,
      "loss": 2.3758,
      "step": 173
    },
    {
      "epoch": 0.348,
      "grad_norm": 3.235281467437744,
      "learning_rate": 0.0019375990128440205,
      "loss": 2.2871,
      "step": 174
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.8142403960227966,
      "learning_rate": 0.0019368662478381799,
      "loss": 2.3591,
      "step": 175
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.0178661346435547,
      "learning_rate": 0.001936129345744011,
      "loss": 2.2127,
      "step": 176
    },
    {
      "epoch": 0.354,
      "grad_norm": 1.2104151248931885,
      "learning_rate": 0.0019353883098155854,
      "loss": 2.3315,
      "step": 177
    },
    {
      "epoch": 0.356,
      "grad_norm": 1.7122215032577515,
      "learning_rate": 0.0019346431433252273,
      "loss": 2.2473,
      "step": 178
    },
    {
      "epoch": 0.358,
      "grad_norm": 0.9045205116271973,
      "learning_rate": 0.001933893849563503,
      "loss": 2.2175,
      "step": 179
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.8639039993286133,
      "learning_rate": 0.0019331404318392025,
      "loss": 2.0355,
      "step": 180
    },
    {
      "epoch": 0.362,
      "grad_norm": 0.5442056655883789,
      "learning_rate": 0.0019323828934793284,
      "loss": 2.1411,
      "step": 181
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.9039509296417236,
      "learning_rate": 0.0019316212378290782,
      "loss": 2.1988,
      "step": 182
    },
    {
      "epoch": 0.366,
      "grad_norm": 1.0469768047332764,
      "learning_rate": 0.0019308554682518312,
      "loss": 2.2151,
      "step": 183
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.0894259214401245,
      "learning_rate": 0.001930085588129134,
      "loss": 2.2926,
      "step": 184
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.074076771736145,
      "learning_rate": 0.0019293116008606837,
      "loss": 2.2157,
      "step": 185
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.7083254456520081,
      "learning_rate": 0.0019285335098643153,
      "loss": 2.0883,
      "step": 186
    },
    {
      "epoch": 0.374,
      "grad_norm": 0.7717963457107544,
      "learning_rate": 0.0019277513185759845,
      "loss": 2.0789,
      "step": 187
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.0702821016311646,
      "learning_rate": 0.001926965030449754,
      "loss": 2.0811,
      "step": 188
    },
    {
      "epoch": 0.378,
      "grad_norm": 1.4277511835098267,
      "learning_rate": 0.0019261746489577765,
      "loss": 2.0163,
      "step": 189
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.046269178390503,
      "learning_rate": 0.001925380177590282,
      "loss": 2.0642,
      "step": 190
    },
    {
      "epoch": 0.382,
      "grad_norm": 2.1891238689422607,
      "learning_rate": 0.0019245816198555604,
      "loss": 2.1662,
      "step": 191
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.068668842315674,
      "learning_rate": 0.0019237789792799457,
      "loss": 2.0996,
      "step": 192
    },
    {
      "epoch": 0.386,
      "grad_norm": 2.1617050170898438,
      "learning_rate": 0.001922972259407802,
      "loss": 2.11,
      "step": 193
    },
    {
      "epoch": 0.388,
      "grad_norm": 3.4916043281555176,
      "learning_rate": 0.0019221614638015075,
      "loss": 2.0878,
      "step": 194
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.109224319458008,
      "learning_rate": 0.001921346596041437,
      "loss": 2.1832,
      "step": 195
    },
    {
      "epoch": 0.392,
      "grad_norm": 4.173354625701904,
      "learning_rate": 0.0019205276597259483,
      "loss": 2.2866,
      "step": 196
    },
    {
      "epoch": 0.394,
      "grad_norm": 1.5933623313903809,
      "learning_rate": 0.0019197046584713661,
      "loss": 2.153,
      "step": 197
    },
    {
      "epoch": 0.396,
      "grad_norm": 1.0274534225463867,
      "learning_rate": 0.0019188775959119641,
      "loss": 1.9966,
      "step": 198
    },
    {
      "epoch": 0.398,
      "grad_norm": 1.349075436592102,
      "learning_rate": 0.0019180464756999509,
      "loss": 1.9761,
      "step": 199
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.169824481010437,
      "learning_rate": 0.001917211301505453,
      "loss": 2.3021,
      "step": 200
    },
    {
      "epoch": 0.4,
      "eval_loss": 2.087155342102051,
      "eval_runtime": 228.7401,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 200
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.9232484102249146,
      "learning_rate": 0.001916372077016499,
      "loss": 2.004,
      "step": 201
    },
    {
      "epoch": 0.404,
      "grad_norm": 4.446470260620117,
      "learning_rate": 0.0019155288059390027,
      "loss": 2.0448,
      "step": 202
    },
    {
      "epoch": 0.406,
      "grad_norm": 2.6597437858581543,
      "learning_rate": 0.0019146814919967481,
      "loss": 2.0506,
      "step": 203
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.728100538253784,
      "learning_rate": 0.0019138301389313708,
      "loss": 2.1125,
      "step": 204
    },
    {
      "epoch": 0.41,
      "grad_norm": 1.0705533027648926,
      "learning_rate": 0.0019129747505023437,
      "loss": 1.9811,
      "step": 205
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.311711311340332,
      "learning_rate": 0.0019121153304869584,
      "loss": 1.8992,
      "step": 206
    },
    {
      "epoch": 0.414,
      "grad_norm": 1.3047494888305664,
      "learning_rate": 0.0019112518826803098,
      "loss": 1.8774,
      "step": 207
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.7055065631866455,
      "learning_rate": 0.00191038441089528,
      "loss": 2.0167,
      "step": 208
    },
    {
      "epoch": 0.418,
      "grad_norm": 5.122485160827637,
      "learning_rate": 0.0019095129189625193,
      "loss": 2.3276,
      "step": 209
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.042346954345703,
      "learning_rate": 0.0019086374107304311,
      "loss": 1.9327,
      "step": 210
    },
    {
      "epoch": 0.422,
      "grad_norm": 2.1639699935913086,
      "learning_rate": 0.0019077578900651541,
      "loss": 2.1737,
      "step": 211
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.4633746147155762,
      "learning_rate": 0.0019068743608505454,
      "loss": 2.1236,
      "step": 212
    },
    {
      "epoch": 0.426,
      "grad_norm": 1.1507340669631958,
      "learning_rate": 0.0019059868269881636,
      "loss": 1.9466,
      "step": 213
    },
    {
      "epoch": 0.428,
      "grad_norm": 22.211172103881836,
      "learning_rate": 0.0019050952923972508,
      "loss": 2.2025,
      "step": 214
    },
    {
      "epoch": 0.43,
      "grad_norm": 12.042900085449219,
      "learning_rate": 0.0019041997610147166,
      "loss": 3.1032,
      "step": 215
    },
    {
      "epoch": 0.432,
      "grad_norm": 6.174982070922852,
      "learning_rate": 0.0019033002367951192,
      "loss": 2.6387,
      "step": 216
    },
    {
      "epoch": 0.434,
      "grad_norm": 1.4321067333221436,
      "learning_rate": 0.0019023967237106491,
      "loss": 2.1359,
      "step": 217
    },
    {
      "epoch": 0.436,
      "grad_norm": 8.086593627929688,
      "learning_rate": 0.0019014892257511117,
      "loss": 2.6089,
      "step": 218
    },
    {
      "epoch": 0.438,
      "grad_norm": 6.714529514312744,
      "learning_rate": 0.0019005777469239076,
      "loss": 2.895,
      "step": 219
    },
    {
      "epoch": 0.44,
      "grad_norm": 4.731200695037842,
      "learning_rate": 0.001899662291254018,
      "loss": 2.8271,
      "step": 220
    },
    {
      "epoch": 0.442,
      "grad_norm": 13.793353080749512,
      "learning_rate": 0.0018987428627839842,
      "loss": 2.4418,
      "step": 221
    },
    {
      "epoch": 0.444,
      "grad_norm": 2.278334617614746,
      "learning_rate": 0.0018978194655738915,
      "loss": 2.0087,
      "step": 222
    },
    {
      "epoch": 0.446,
      "grad_norm": 5.2318806648254395,
      "learning_rate": 0.0018968921037013512,
      "loss": 2.0781,
      "step": 223
    },
    {
      "epoch": 0.448,
      "grad_norm": 9.879586219787598,
      "learning_rate": 0.0018959607812614806,
      "loss": 2.2921,
      "step": 224
    },
    {
      "epoch": 0.45,
      "grad_norm": 3.0180296897888184,
      "learning_rate": 0.0018950255023668877,
      "loss": 2.1912,
      "step": 225
    },
    {
      "epoch": 0.452,
      "grad_norm": 2.880418300628662,
      "learning_rate": 0.0018940862711476511,
      "loss": 2.2629,
      "step": 226
    },
    {
      "epoch": 0.454,
      "grad_norm": 1.419576644897461,
      "learning_rate": 0.0018931430917513029,
      "loss": 2.0631,
      "step": 227
    },
    {
      "epoch": 0.456,
      "grad_norm": 1.007103681564331,
      "learning_rate": 0.001892195968342809,
      "loss": 1.9057,
      "step": 228
    },
    {
      "epoch": 0.458,
      "grad_norm": 1.1325498819351196,
      "learning_rate": 0.0018912449051045525,
      "loss": 1.9321,
      "step": 229
    },
    {
      "epoch": 0.46,
      "grad_norm": 2.100541114807129,
      "learning_rate": 0.0018902899062363141,
      "loss": 2.0121,
      "step": 230
    },
    {
      "epoch": 0.462,
      "grad_norm": 1.1221510171890259,
      "learning_rate": 0.0018893309759552529,
      "loss": 1.938,
      "step": 231
    },
    {
      "epoch": 0.464,
      "grad_norm": 1.1299188137054443,
      "learning_rate": 0.0018883681184958898,
      "loss": 2.1734,
      "step": 232
    },
    {
      "epoch": 0.466,
      "grad_norm": 1.4051908254623413,
      "learning_rate": 0.0018874013381100874,
      "loss": 1.846,
      "step": 233
    },
    {
      "epoch": 0.468,
      "grad_norm": 1.5859426259994507,
      "learning_rate": 0.0018864306390670308,
      "loss": 1.8241,
      "step": 234
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.8983497023582458,
      "learning_rate": 0.0018854560256532098,
      "loss": 1.8387,
      "step": 235
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.0259063243865967,
      "learning_rate": 0.0018844775021724003,
      "loss": 1.9667,
      "step": 236
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.7260911464691162,
      "learning_rate": 0.0018834950729456432,
      "loss": 1.9189,
      "step": 237
    },
    {
      "epoch": 0.476,
      "grad_norm": 1.650926947593689,
      "learning_rate": 0.001882508742311228,
      "loss": 1.7305,
      "step": 238
    },
    {
      "epoch": 0.478,
      "grad_norm": 1.172369360923767,
      "learning_rate": 0.0018815185146246716,
      "loss": 1.8847,
      "step": 239
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.6931760907173157,
      "learning_rate": 0.0018805243942587,
      "loss": 1.8689,
      "step": 240
    },
    {
      "epoch": 0.482,
      "grad_norm": 1.2832351922988892,
      "learning_rate": 0.0018795263856032287,
      "loss": 1.9057,
      "step": 241
    },
    {
      "epoch": 0.484,
      "grad_norm": 26.47721290588379,
      "learning_rate": 0.0018785244930653439,
      "loss": 1.9442,
      "step": 242
    },
    {
      "epoch": 0.486,
      "grad_norm": 0.9876003861427307,
      "learning_rate": 0.0018775187210692814,
      "loss": 1.8215,
      "step": 243
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.9097820520401001,
      "learning_rate": 0.0018765090740564098,
      "loss": 1.855,
      "step": 244
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.9427711963653564,
      "learning_rate": 0.001875495556485208,
      "loss": 1.9431,
      "step": 245
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.7460276484489441,
      "learning_rate": 0.001874478172831248,
      "loss": 1.8116,
      "step": 246
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.9198756814002991,
      "learning_rate": 0.0018734569275871726,
      "loss": 1.8921,
      "step": 247
    },
    {
      "epoch": 0.496,
      "grad_norm": 1.4212478399276733,
      "learning_rate": 0.0018724318252626776,
      "loss": 2.0372,
      "step": 248
    },
    {
      "epoch": 0.498,
      "grad_norm": 0.5720387697219849,
      "learning_rate": 0.0018714028703844914,
      "loss": 1.6899,
      "step": 249
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.8207013010978699,
      "learning_rate": 0.0018703700674963545,
      "loss": 1.8318,
      "step": 250
    },
    {
      "epoch": 0.502,
      "grad_norm": 1.0384647846221924,
      "learning_rate": 0.0018693334211590006,
      "loss": 1.8637,
      "step": 251
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.8766223192214966,
      "learning_rate": 0.0018682929359501337,
      "loss": 1.7961,
      "step": 252
    },
    {
      "epoch": 0.506,
      "grad_norm": 1.039932131767273,
      "learning_rate": 0.0018672486164644116,
      "loss": 1.7555,
      "step": 253
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.5402951836585999,
      "learning_rate": 0.0018662004673134231,
      "loss": 1.757,
      "step": 254
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.9596356749534607,
      "learning_rate": 0.0018651484931256684,
      "loss": 1.6867,
      "step": 255
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.8282240629196167,
      "learning_rate": 0.0018640926985465387,
      "loss": 1.6481,
      "step": 256
    },
    {
      "epoch": 0.514,
      "grad_norm": 1.3981173038482666,
      "learning_rate": 0.0018630330882382952,
      "loss": 1.8429,
      "step": 257
    },
    {
      "epoch": 0.516,
      "grad_norm": 1.1158251762390137,
      "learning_rate": 0.0018619696668800492,
      "loss": 1.8297,
      "step": 258
    },
    {
      "epoch": 0.518,
      "grad_norm": 1.6066783666610718,
      "learning_rate": 0.0018609024391677417,
      "loss": 1.7256,
      "step": 259
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.7062008380889893,
      "learning_rate": 0.0018598314098141207,
      "loss": 1.7927,
      "step": 260
    },
    {
      "epoch": 0.522,
      "grad_norm": 2.072417736053467,
      "learning_rate": 0.0018587565835487233,
      "loss": 1.8325,
      "step": 261
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.968880295753479,
      "learning_rate": 0.0018576779651178522,
      "loss": 1.771,
      "step": 262
    },
    {
      "epoch": 0.526,
      "grad_norm": 1.589495062828064,
      "learning_rate": 0.0018565955592845563,
      "loss": 1.8607,
      "step": 263
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.3808385133743286,
      "learning_rate": 0.0018555093708286093,
      "loss": 1.814,
      "step": 264
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.6895244717597961,
      "learning_rate": 0.0018544194045464887,
      "loss": 1.7371,
      "step": 265
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.941152572631836,
      "learning_rate": 0.0018533256652513534,
      "loss": 2.0481,
      "step": 266
    },
    {
      "epoch": 0.534,
      "grad_norm": 1.556852102279663,
      "learning_rate": 0.001852228157773025,
      "loss": 1.7803,
      "step": 267
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.5502138137817383,
      "learning_rate": 0.0018511268869579635,
      "loss": 1.7977,
      "step": 268
    },
    {
      "epoch": 0.538,
      "grad_norm": 11.519356727600098,
      "learning_rate": 0.001850021857669248,
      "loss": 1.8868,
      "step": 269
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.5086493492126465,
      "learning_rate": 0.0018489130747865548,
      "loss": 2.0024,
      "step": 270
    },
    {
      "epoch": 0.542,
      "grad_norm": 2.329739809036255,
      "learning_rate": 0.0018478005432061352,
      "loss": 1.6887,
      "step": 271
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.3066250085830688,
      "learning_rate": 0.0018466842678407944,
      "loss": 1.8537,
      "step": 272
    },
    {
      "epoch": 0.546,
      "grad_norm": 2.0700197219848633,
      "learning_rate": 0.00184556425361987,
      "loss": 2.0206,
      "step": 273
    },
    {
      "epoch": 0.548,
      "grad_norm": 2.1918694972991943,
      "learning_rate": 0.0018444405054892092,
      "loss": 1.7804,
      "step": 274
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9389934539794922,
      "learning_rate": 0.001843313028411149,
      "loss": 1.7287,
      "step": 275
    },
    {
      "epoch": 0.552,
      "grad_norm": 2.0404369831085205,
      "learning_rate": 0.0018421818273644912,
      "loss": 1.7987,
      "step": 276
    },
    {
      "epoch": 0.554,
      "grad_norm": 1.5645990371704102,
      "learning_rate": 0.001841046907344484,
      "loss": 1.7326,
      "step": 277
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.0867469310760498,
      "learning_rate": 0.0018399082733627965,
      "loss": 1.8908,
      "step": 278
    },
    {
      "epoch": 0.558,
      "grad_norm": 1.9863090515136719,
      "learning_rate": 0.0018387659304474994,
      "loss": 1.8284,
      "step": 279
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.9959880113601685,
      "learning_rate": 0.0018376198836430415,
      "loss": 1.7495,
      "step": 280
    },
    {
      "epoch": 0.562,
      "grad_norm": 1.2685647010803223,
      "learning_rate": 0.0018364701380102267,
      "loss": 1.5932,
      "step": 281
    },
    {
      "epoch": 0.564,
      "grad_norm": 3.379605770111084,
      "learning_rate": 0.0018353166986261936,
      "loss": 1.8427,
      "step": 282
    },
    {
      "epoch": 0.566,
      "grad_norm": 1.1339600086212158,
      "learning_rate": 0.0018341595705843906,
      "loss": 1.7817,
      "step": 283
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.6808093786239624,
      "learning_rate": 0.001832998758994556,
      "loss": 1.7777,
      "step": 284
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.0846030712127686,
      "learning_rate": 0.0018318342689826936,
      "loss": 1.5967,
      "step": 285
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.857460081577301,
      "learning_rate": 0.001830666105691051,
      "loss": 1.7492,
      "step": 286
    },
    {
      "epoch": 0.574,
      "grad_norm": 18.6268253326416,
      "learning_rate": 0.0018294942742780964,
      "loss": 2.3627,
      "step": 287
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.662473201751709,
      "learning_rate": 0.0018283187799184957,
      "loss": 1.8249,
      "step": 288
    },
    {
      "epoch": 0.578,
      "grad_norm": 1.8290801048278809,
      "learning_rate": 0.0018271396278030903,
      "loss": 1.81,
      "step": 289
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.5882046222686768,
      "learning_rate": 0.0018259568231388736,
      "loss": 2.0656,
      "step": 290
    },
    {
      "epoch": 0.582,
      "grad_norm": 2.7158992290496826,
      "learning_rate": 0.0018247703711489684,
      "loss": 1.7376,
      "step": 291
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.7360192537307739,
      "learning_rate": 0.0018235802770726038,
      "loss": 1.6592,
      "step": 292
    },
    {
      "epoch": 0.586,
      "grad_norm": 0.9611104726791382,
      "learning_rate": 0.0018223865461650913,
      "loss": 1.6154,
      "step": 293
    },
    {
      "epoch": 0.588,
      "grad_norm": 1.0467870235443115,
      "learning_rate": 0.0018211891836978028,
      "loss": 1.7841,
      "step": 294
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.962512731552124,
      "learning_rate": 0.001819988194958146,
      "loss": 1.794,
      "step": 295
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.1482343673706055,
      "learning_rate": 0.0018187835852495429,
      "loss": 1.7645,
      "step": 296
    },
    {
      "epoch": 0.594,
      "grad_norm": 0.7849602699279785,
      "learning_rate": 0.0018175753598914047,
      "loss": 1.82,
      "step": 297
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.269761562347412,
      "learning_rate": 0.0018163635242191083,
      "loss": 1.7728,
      "step": 298
    },
    {
      "epoch": 0.598,
      "grad_norm": 0.8502991199493408,
      "learning_rate": 0.0018151480835839743,
      "loss": 1.6464,
      "step": 299
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5836257934570312,
      "learning_rate": 0.0018139290433532413,
      "loss": 1.652,
      "step": 300
    },
    {
      "epoch": 0.6,
      "eval_loss": 1.7823143005371094,
      "eval_runtime": 228.8832,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 300
    },
    {
      "epoch": 0.602,
      "grad_norm": 1.5231127738952637,
      "learning_rate": 0.0018127064089100446,
      "loss": 1.8211,
      "step": 301
    },
    {
      "epoch": 0.604,
      "grad_norm": 1.0323700904846191,
      "learning_rate": 0.00181148018565339,
      "loss": 1.7974,
      "step": 302
    },
    {
      "epoch": 0.606,
      "grad_norm": 1.9231871366500854,
      "learning_rate": 0.001810250378998132,
      "loss": 1.8587,
      "step": 303
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.168794870376587,
      "learning_rate": 0.0018090169943749475,
      "loss": 1.7838,
      "step": 304
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.2404541969299316,
      "learning_rate": 0.001807780037230315,
      "loss": 1.8,
      "step": 305
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.8690732717514038,
      "learning_rate": 0.0018065395130264874,
      "loss": 1.6905,
      "step": 306
    },
    {
      "epoch": 0.614,
      "grad_norm": 0.57569420337677,
      "learning_rate": 0.0018052954272414707,
      "loss": 1.7044,
      "step": 307
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.8931528925895691,
      "learning_rate": 0.0018040477853689969,
      "loss": 1.6956,
      "step": 308
    },
    {
      "epoch": 0.618,
      "grad_norm": 0.9971257448196411,
      "learning_rate": 0.0018027965929185024,
      "loss": 1.8003,
      "step": 309
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.4838508367538452,
      "learning_rate": 0.0018015418554151023,
      "loss": 1.5543,
      "step": 310
    },
    {
      "epoch": 0.622,
      "grad_norm": 1.077484130859375,
      "learning_rate": 0.0018002835783995652,
      "loss": 1.8775,
      "step": 311
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.4516801834106445,
      "learning_rate": 0.0017990217674282915,
      "loss": 1.8355,
      "step": 312
    },
    {
      "epoch": 0.626,
      "grad_norm": 1.5076583623886108,
      "learning_rate": 0.001797756428073286,
      "loss": 1.6261,
      "step": 313
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.9125683903694153,
      "learning_rate": 0.0017964875659221343,
      "loss": 1.7385,
      "step": 314
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.3437045812606812,
      "learning_rate": 0.0017952151865779792,
      "loss": 1.5429,
      "step": 315
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.8631296157836914,
      "learning_rate": 0.0017939392956594932,
      "loss": 1.6427,
      "step": 316
    },
    {
      "epoch": 0.634,
      "grad_norm": 1.231150507926941,
      "learning_rate": 0.0017926598988008582,
      "loss": 1.6562,
      "step": 317
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.7557599544525146,
      "learning_rate": 0.0017913770016517354,
      "loss": 1.6754,
      "step": 318
    },
    {
      "epoch": 0.638,
      "grad_norm": 1.099472165107727,
      "learning_rate": 0.0017900906098772444,
      "loss": 1.6691,
      "step": 319
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.3493553400039673,
      "learning_rate": 0.0017888007291579355,
      "loss": 1.6849,
      "step": 320
    },
    {
      "epoch": 0.642,
      "grad_norm": 0.7711889743804932,
      "learning_rate": 0.001787507365189767,
      "loss": 1.702,
      "step": 321
    },
    {
      "epoch": 0.644,
      "grad_norm": 1.1909146308898926,
      "learning_rate": 0.0017862105236840775,
      "loss": 1.6445,
      "step": 322
    },
    {
      "epoch": 0.646,
      "grad_norm": 1.202271819114685,
      "learning_rate": 0.001784910210367563,
      "loss": 1.7829,
      "step": 323
    },
    {
      "epoch": 0.648,
      "grad_norm": 1.0791819095611572,
      "learning_rate": 0.0017836064309822502,
      "loss": 1.5777,
      "step": 324
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.8754224181175232,
      "learning_rate": 0.0017822991912854714,
      "loss": 1.6147,
      "step": 325
    },
    {
      "epoch": 0.652,
      "grad_norm": 1.19380521774292,
      "learning_rate": 0.0017809884970498395,
      "loss": 1.7346,
      "step": 326
    },
    {
      "epoch": 0.654,
      "grad_norm": 1.2211077213287354,
      "learning_rate": 0.0017796743540632223,
      "loss": 1.6115,
      "step": 327
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.0480109453201294,
      "learning_rate": 0.0017783567681287167,
      "loss": 1.6735,
      "step": 328
    },
    {
      "epoch": 0.658,
      "grad_norm": 0.9083231091499329,
      "learning_rate": 0.001777035745064623,
      "loss": 1.6116,
      "step": 329
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.7638611793518066,
      "learning_rate": 0.0017757112907044199,
      "loss": 1.6513,
      "step": 330
    },
    {
      "epoch": 0.662,
      "grad_norm": 1.1647852659225464,
      "learning_rate": 0.001774383410896738,
      "loss": 1.6887,
      "step": 331
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.099047064781189,
      "learning_rate": 0.001773052111505334,
      "loss": 1.7065,
      "step": 332
    },
    {
      "epoch": 0.666,
      "grad_norm": 1.370642900466919,
      "learning_rate": 0.0017717173984090658,
      "loss": 1.7755,
      "step": 333
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.6246787905693054,
      "learning_rate": 0.0017703792775018655,
      "loss": 1.577,
      "step": 334
    },
    {
      "epoch": 0.67,
      "grad_norm": 1.0504446029663086,
      "learning_rate": 0.0017690377546927133,
      "loss": 1.6766,
      "step": 335
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.7460998892784119,
      "learning_rate": 0.001767692835905612,
      "loss": 1.4748,
      "step": 336
    },
    {
      "epoch": 0.674,
      "grad_norm": 1.4882992506027222,
      "learning_rate": 0.001766344527079561,
      "loss": 1.695,
      "step": 337
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.9611985683441162,
      "learning_rate": 0.0017649928341685298,
      "loss": 1.7733,
      "step": 338
    },
    {
      "epoch": 0.678,
      "grad_norm": 1.264172077178955,
      "learning_rate": 0.0017636377631414302,
      "loss": 1.6448,
      "step": 339
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.1381560564041138,
      "learning_rate": 0.0017622793199820932,
      "loss": 1.6442,
      "step": 340
    },
    {
      "epoch": 0.682,
      "grad_norm": 0.9429733753204346,
      "learning_rate": 0.0017609175106892395,
      "loss": 1.597,
      "step": 341
    },
    {
      "epoch": 0.684,
      "grad_norm": 1.1724679470062256,
      "learning_rate": 0.001759552341276455,
      "loss": 1.6193,
      "step": 342
    },
    {
      "epoch": 0.686,
      "grad_norm": 0.9145501255989075,
      "learning_rate": 0.0017581838177721627,
      "loss": 1.6621,
      "step": 343
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.8912887573242188,
      "learning_rate": 0.0017568119462195977,
      "loss": 1.6034,
      "step": 344
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.8403961658477783,
      "learning_rate": 0.0017554367326767792,
      "loss": 1.5582,
      "step": 345
    },
    {
      "epoch": 0.692,
      "grad_norm": 5.842499256134033,
      "learning_rate": 0.0017540581832164838,
      "loss": 1.7362,
      "step": 346
    },
    {
      "epoch": 0.694,
      "grad_norm": 2.252243757247925,
      "learning_rate": 0.0017526763039262207,
      "loss": 1.6522,
      "step": 347
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.9733878970146179,
      "learning_rate": 0.0017512911009082012,
      "loss": 1.6934,
      "step": 348
    },
    {
      "epoch": 0.698,
      "grad_norm": 1.1161422729492188,
      "learning_rate": 0.0017499025802793146,
      "loss": 1.739,
      "step": 349
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.8673473596572876,
      "learning_rate": 0.001748510748171101,
      "loss": 1.5813,
      "step": 350
    },
    {
      "epoch": 0.702,
      "grad_norm": 0.8545043468475342,
      "learning_rate": 0.0017471156107297233,
      "loss": 1.6128,
      "step": 351
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.7536791563034058,
      "learning_rate": 0.0017457171741159395,
      "loss": 1.7518,
      "step": 352
    },
    {
      "epoch": 0.706,
      "grad_norm": 0.9054182171821594,
      "learning_rate": 0.0017443154445050772,
      "loss": 1.6668,
      "step": 353
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.9451432228088379,
      "learning_rate": 0.0017429104280870056,
      "loss": 1.5543,
      "step": 354
    },
    {
      "epoch": 0.71,
      "grad_norm": 4.472679138183594,
      "learning_rate": 0.001741502131066107,
      "loss": 1.7556,
      "step": 355
    },
    {
      "epoch": 0.712,
      "grad_norm": 1.3667792081832886,
      "learning_rate": 0.001740090559661252,
      "loss": 1.7082,
      "step": 356
    },
    {
      "epoch": 0.714,
      "grad_norm": 0.6175951957702637,
      "learning_rate": 0.001738675720105769,
      "loss": 1.593,
      "step": 357
    },
    {
      "epoch": 0.716,
      "grad_norm": 29.68411636352539,
      "learning_rate": 0.001737257618647419,
      "loss": 1.6585,
      "step": 358
    },
    {
      "epoch": 0.718,
      "grad_norm": 4.170158863067627,
      "learning_rate": 0.0017358362615483669,
      "loss": 1.838,
      "step": 359
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.307258129119873,
      "learning_rate": 0.0017344116550851542,
      "loss": 1.6608,
      "step": 360
    },
    {
      "epoch": 0.722,
      "grad_norm": 3.2212538719177246,
      "learning_rate": 0.0017329838055486716,
      "loss": 1.7082,
      "step": 361
    },
    {
      "epoch": 0.724,
      "grad_norm": 1.291123867034912,
      "learning_rate": 0.0017315527192441297,
      "loss": 1.5484,
      "step": 362
    },
    {
      "epoch": 0.726,
      "grad_norm": 0.9879775643348694,
      "learning_rate": 0.0017301184024910332,
      "loss": 1.6113,
      "step": 363
    },
    {
      "epoch": 0.728,
      "grad_norm": 2.1589083671569824,
      "learning_rate": 0.0017286808616231522,
      "loss": 1.5939,
      "step": 364
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.538110613822937,
      "learning_rate": 0.0017272401029884933,
      "loss": 1.6761,
      "step": 365
    },
    {
      "epoch": 0.732,
      "grad_norm": 1.4945731163024902,
      "learning_rate": 0.0017257961329492728,
      "loss": 1.5987,
      "step": 366
    },
    {
      "epoch": 0.734,
      "grad_norm": 1.0853922367095947,
      "learning_rate": 0.001724348957881889,
      "loss": 1.6008,
      "step": 367
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.185198426246643,
      "learning_rate": 0.0017228985841768914,
      "loss": 1.6473,
      "step": 368
    },
    {
      "epoch": 0.738,
      "grad_norm": 2.011244535446167,
      "learning_rate": 0.0017214450182389558,
      "loss": 1.702,
      "step": 369
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.352113127708435,
      "learning_rate": 0.0017199882664868538,
      "loss": 1.5394,
      "step": 370
    },
    {
      "epoch": 0.742,
      "grad_norm": 1.401039719581604,
      "learning_rate": 0.0017185283353534258,
      "loss": 1.6784,
      "step": 371
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.957292377948761,
      "learning_rate": 0.0017170652312855513,
      "loss": 1.5743,
      "step": 372
    },
    {
      "epoch": 0.746,
      "grad_norm": 1.1197599172592163,
      "learning_rate": 0.0017155989607441212,
      "loss": 1.6577,
      "step": 373
    },
    {
      "epoch": 0.748,
      "grad_norm": 1.1531860828399658,
      "learning_rate": 0.0017141295302040094,
      "loss": 1.5859,
      "step": 374
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.8067148327827454,
      "learning_rate": 0.0017126569461540441,
      "loss": 1.7011,
      "step": 375
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.7454786896705627,
      "learning_rate": 0.0017111812150969788,
      "loss": 1.6242,
      "step": 376
    },
    {
      "epoch": 0.754,
      "grad_norm": 0.8122573494911194,
      "learning_rate": 0.0017097023435494636,
      "loss": 1.65,
      "step": 377
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.8289828300476074,
      "learning_rate": 0.001708220338042017,
      "loss": 1.4458,
      "step": 378
    },
    {
      "epoch": 0.758,
      "grad_norm": 1.8177695274353027,
      "learning_rate": 0.0017067352051189967,
      "loss": 1.5271,
      "step": 379
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9217049479484558,
      "learning_rate": 0.0017052469513385699,
      "loss": 1.5631,
      "step": 380
    },
    {
      "epoch": 0.762,
      "grad_norm": 0.8236808180809021,
      "learning_rate": 0.0017037555832726864,
      "loss": 1.5464,
      "step": 381
    },
    {
      "epoch": 0.764,
      "grad_norm": 2.1468148231506348,
      "learning_rate": 0.0017022611075070474,
      "loss": 1.6201,
      "step": 382
    },
    {
      "epoch": 0.766,
      "grad_norm": 1.1715935468673706,
      "learning_rate": 0.0017007635306410774,
      "loss": 1.6249,
      "step": 383
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.9796546101570129,
      "learning_rate": 0.0016992628592878956,
      "loss": 1.4883,
      "step": 384
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.1490569114685059,
      "learning_rate": 0.0016977591000742853,
      "loss": 1.6485,
      "step": 385
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.8662044405937195,
      "learning_rate": 0.0016962522596406663,
      "loss": 1.4815,
      "step": 386
    },
    {
      "epoch": 0.774,
      "grad_norm": 1.4506090879440308,
      "learning_rate": 0.0016947423446410635,
      "loss": 1.6829,
      "step": 387
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.8883365392684937,
      "learning_rate": 0.0016932293617430796,
      "loss": 1.5468,
      "step": 388
    },
    {
      "epoch": 0.778,
      "grad_norm": 1.3998026847839355,
      "learning_rate": 0.0016917133176278648,
      "loss": 1.5558,
      "step": 389
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.281177282333374,
      "learning_rate": 0.0016901942189900866,
      "loss": 1.5969,
      "step": 390
    },
    {
      "epoch": 0.782,
      "grad_norm": 1.1527841091156006,
      "learning_rate": 0.001688672072537902,
      "loss": 1.5642,
      "step": 391
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.9075772166252136,
      "learning_rate": 0.0016871468849929253,
      "loss": 1.5742,
      "step": 392
    },
    {
      "epoch": 0.786,
      "grad_norm": 1.0418435335159302,
      "learning_rate": 0.0016856186630902013,
      "loss": 1.4203,
      "step": 393
    },
    {
      "epoch": 0.788,
      "grad_norm": 1.427114725112915,
      "learning_rate": 0.001684087413578173,
      "loss": 1.5263,
      "step": 394
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.8930515050888062,
      "learning_rate": 0.0016825531432186542,
      "loss": 1.5345,
      "step": 395
    },
    {
      "epoch": 0.792,
      "grad_norm": 2.4928700923919678,
      "learning_rate": 0.0016810158587867972,
      "loss": 1.438,
      "step": 396
    },
    {
      "epoch": 0.794,
      "grad_norm": 1.422499656677246,
      "learning_rate": 0.001679475567071065,
      "loss": 1.7737,
      "step": 397
    },
    {
      "epoch": 0.796,
      "grad_norm": 1.565308690071106,
      "learning_rate": 0.0016779322748731995,
      "loss": 1.608,
      "step": 398
    },
    {
      "epoch": 0.798,
      "grad_norm": 3.0083491802215576,
      "learning_rate": 0.001676385989008193,
      "loss": 1.5348,
      "step": 399
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.825927257537842,
      "learning_rate": 0.0016748367163042577,
      "loss": 1.894,
      "step": 400
    },
    {
      "epoch": 0.8,
      "eval_loss": 2.663140058517456,
      "eval_runtime": 228.5861,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 400
    },
    {
      "epoch": 0.802,
      "grad_norm": 49.084259033203125,
      "learning_rate": 0.0016732844636027945,
      "loss": 2.8966,
      "step": 401
    },
    {
      "epoch": 0.804,
      "grad_norm": 7.208386421203613,
      "learning_rate": 0.0016717292377583647,
      "loss": 2.3815,
      "step": 402
    },
    {
      "epoch": 0.806,
      "grad_norm": 11.825223922729492,
      "learning_rate": 0.0016701710456386572,
      "loss": 2.3087,
      "step": 403
    },
    {
      "epoch": 0.808,
      "grad_norm": 18.69350814819336,
      "learning_rate": 0.0016686098941244613,
      "loss": 2.1081,
      "step": 404
    },
    {
      "epoch": 0.81,
      "grad_norm": 4.945819854736328,
      "learning_rate": 0.0016670457901096327,
      "loss": 2.0841,
      "step": 405
    },
    {
      "epoch": 0.812,
      "grad_norm": 2.2382442951202393,
      "learning_rate": 0.001665478740501067,
      "loss": 1.9495,
      "step": 406
    },
    {
      "epoch": 0.814,
      "grad_norm": 3.30540132522583,
      "learning_rate": 0.0016639087522186658,
      "loss": 2.0836,
      "step": 407
    },
    {
      "epoch": 0.816,
      "grad_norm": 2.6894960403442383,
      "learning_rate": 0.0016623358321953077,
      "loss": 1.7824,
      "step": 408
    },
    {
      "epoch": 0.818,
      "grad_norm": 1.501424789428711,
      "learning_rate": 0.0016607599873768183,
      "loss": 1.7852,
      "step": 409
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.4573293924331665,
      "learning_rate": 0.0016591812247219377,
      "loss": 1.7624,
      "step": 410
    },
    {
      "epoch": 0.822,
      "grad_norm": 1.5187232494354248,
      "learning_rate": 0.0016575995512022922,
      "loss": 1.7677,
      "step": 411
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.3021838665008545,
      "learning_rate": 0.00165601497380236,
      "loss": 1.5918,
      "step": 412
    },
    {
      "epoch": 0.826,
      "grad_norm": 1.3647332191467285,
      "learning_rate": 0.0016544274995194447,
      "loss": 1.728,
      "step": 413
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.6052151918411255,
      "learning_rate": 0.0016528371353636408,
      "loss": 1.6632,
      "step": 414
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.3614898920059204,
      "learning_rate": 0.0016512438883578046,
      "loss": 1.7832,
      "step": 415
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.2418807744979858,
      "learning_rate": 0.0016496477655375227,
      "loss": 1.7268,
      "step": 416
    },
    {
      "epoch": 0.834,
      "grad_norm": 1.1295928955078125,
      "learning_rate": 0.0016480487739510808,
      "loss": 1.7145,
      "step": 417
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.997776448726654,
      "learning_rate": 0.0016464469206594332,
      "loss": 1.5929,
      "step": 418
    },
    {
      "epoch": 0.838,
      "grad_norm": 2.637593984603882,
      "learning_rate": 0.0016448422127361706,
      "loss": 1.5882,
      "step": 419
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.6778512001037598,
      "learning_rate": 0.0016432346572674897,
      "loss": 1.5691,
      "step": 420
    },
    {
      "epoch": 0.842,
      "grad_norm": 1.4109376668930054,
      "learning_rate": 0.001641624261352161,
      "loss": 1.7181,
      "step": 421
    },
    {
      "epoch": 0.844,
      "grad_norm": 2.1076979637145996,
      "learning_rate": 0.0016400110321014992,
      "loss": 1.5405,
      "step": 422
    },
    {
      "epoch": 0.846,
      "grad_norm": 1.2131354808807373,
      "learning_rate": 0.00163839497663933,
      "loss": 1.6033,
      "step": 423
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.8180492520332336,
      "learning_rate": 0.001636776102101959,
      "loss": 1.6174,
      "step": 424
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.8554771542549133,
      "learning_rate": 0.0016351544156381413,
      "loss": 1.6058,
      "step": 425
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.9781724214553833,
      "learning_rate": 0.0016335299244090479,
      "loss": 1.6724,
      "step": 426
    },
    {
      "epoch": 0.854,
      "grad_norm": 0.9397765398025513,
      "learning_rate": 0.001631902635588237,
      "loss": 1.667,
      "step": 427
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.7167592644691467,
      "learning_rate": 0.0016302725563616192,
      "loss": 1.7178,
      "step": 428
    },
    {
      "epoch": 0.858,
      "grad_norm": 1.1971646547317505,
      "learning_rate": 0.001628639693927428,
      "loss": 1.5612,
      "step": 429
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.537182331085205,
      "learning_rate": 0.0016270040554961867,
      "loss": 1.6573,
      "step": 430
    },
    {
      "epoch": 0.862,
      "grad_norm": 1.295730710029602,
      "learning_rate": 0.0016253656482906776,
      "loss": 1.8103,
      "step": 431
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.6355118155479431,
      "learning_rate": 0.0016237244795459084,
      "loss": 1.4355,
      "step": 432
    },
    {
      "epoch": 0.866,
      "grad_norm": 0.9493564367294312,
      "learning_rate": 0.0016220805565090837,
      "loss": 1.7143,
      "step": 433
    },
    {
      "epoch": 0.868,
      "grad_norm": 1.1721752882003784,
      "learning_rate": 0.0016204338864395681,
      "loss": 1.6398,
      "step": 434
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.8405128717422485,
      "learning_rate": 0.0016187844766088583,
      "loss": 1.4582,
      "step": 435
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.164881467819214,
      "learning_rate": 0.0016171323343005498,
      "loss": 1.7084,
      "step": 436
    },
    {
      "epoch": 0.874,
      "grad_norm": 2.0078232288360596,
      "learning_rate": 0.0016154774668103028,
      "loss": 1.668,
      "step": 437
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.6471284627914429,
      "learning_rate": 0.0016138198814458127,
      "loss": 1.5394,
      "step": 438
    },
    {
      "epoch": 0.878,
      "grad_norm": 257.2276306152344,
      "learning_rate": 0.0016121595855267765,
      "loss": 1.6876,
      "step": 439
    },
    {
      "epoch": 0.88,
      "grad_norm": 4.955711364746094,
      "learning_rate": 0.0016104965863848616,
      "loss": 1.8844,
      "step": 440
    },
    {
      "epoch": 0.882,
      "grad_norm": 2.479661226272583,
      "learning_rate": 0.0016088308913636703,
      "loss": 1.8095,
      "step": 441
    },
    {
      "epoch": 0.884,
      "grad_norm": 3.6681101322174072,
      "learning_rate": 0.0016071625078187112,
      "loss": 1.8407,
      "step": 442
    },
    {
      "epoch": 0.886,
      "grad_norm": 4.782470226287842,
      "learning_rate": 0.0016054914431173652,
      "loss": 1.8935,
      "step": 443
    },
    {
      "epoch": 0.888,
      "grad_norm": 30.53533172607422,
      "learning_rate": 0.0016038177046388523,
      "loss": 1.7284,
      "step": 444
    },
    {
      "epoch": 0.89,
      "grad_norm": 17.501195907592773,
      "learning_rate": 0.0016021412997741992,
      "loss": 1.7687,
      "step": 445
    },
    {
      "epoch": 0.892,
      "grad_norm": 3.643260955810547,
      "learning_rate": 0.0016004622359262085,
      "loss": 1.756,
      "step": 446
    },
    {
      "epoch": 0.894,
      "grad_norm": 8.269022941589355,
      "learning_rate": 0.0015987805205094226,
      "loss": 1.8604,
      "step": 447
    },
    {
      "epoch": 0.896,
      "grad_norm": 3.765413761138916,
      "learning_rate": 0.0015970961609500945,
      "loss": 1.7478,
      "step": 448
    },
    {
      "epoch": 0.898,
      "grad_norm": 1.3105733394622803,
      "learning_rate": 0.0015954091646861524,
      "loss": 1.6538,
      "step": 449
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.932565450668335,
      "learning_rate": 0.0015937195391671688,
      "loss": 1.6795,
      "step": 450
    },
    {
      "epoch": 0.902,
      "grad_norm": 2.3548834323883057,
      "learning_rate": 0.0015920272918543256,
      "loss": 1.7642,
      "step": 451
    },
    {
      "epoch": 0.904,
      "grad_norm": 1.330745816230774,
      "learning_rate": 0.0015903324302203835,
      "loss": 1.4647,
      "step": 452
    },
    {
      "epoch": 0.906,
      "grad_norm": 1.658000111579895,
      "learning_rate": 0.001588634961749646,
      "loss": 1.7556,
      "step": 453
    },
    {
      "epoch": 0.908,
      "grad_norm": 3.387324810028076,
      "learning_rate": 0.0015869348939379303,
      "loss": 1.6538,
      "step": 454
    },
    {
      "epoch": 0.91,
      "grad_norm": 152.97799682617188,
      "learning_rate": 0.0015852322342925294,
      "loss": 1.7838,
      "step": 455
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.4772089719772339,
      "learning_rate": 0.0015835269903321841,
      "loss": 1.6393,
      "step": 456
    },
    {
      "epoch": 0.914,
      "grad_norm": 1.7114837169647217,
      "learning_rate": 0.0015818191695870453,
      "loss": 1.7409,
      "step": 457
    },
    {
      "epoch": 0.916,
      "grad_norm": 1.6915321350097656,
      "learning_rate": 0.0015801087795986437,
      "loss": 1.7024,
      "step": 458
    },
    {
      "epoch": 0.918,
      "grad_norm": 2.0051369667053223,
      "learning_rate": 0.0015783958279198549,
      "loss": 1.7917,
      "step": 459
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.8461072444915771,
      "learning_rate": 0.0015766803221148673,
      "loss": 1.6529,
      "step": 460
    },
    {
      "epoch": 0.922,
      "grad_norm": 1.5127614736557007,
      "learning_rate": 0.001574962269759147,
      "loss": 1.6524,
      "step": 461
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.6698948740959167,
      "learning_rate": 0.0015732416784394064,
      "loss": 1.7163,
      "step": 462
    },
    {
      "epoch": 0.926,
      "grad_norm": 1.6204802989959717,
      "learning_rate": 0.0015715185557535689,
      "loss": 1.5534,
      "step": 463
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.6415321826934814,
      "learning_rate": 0.0015697929093107365,
      "loss": 1.6848,
      "step": 464
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.0659085512161255,
      "learning_rate": 0.0015680647467311557,
      "loss": 1.5771,
      "step": 465
    },
    {
      "epoch": 0.932,
      "grad_norm": 1.336102843284607,
      "learning_rate": 0.0015663340756461844,
      "loss": 1.566,
      "step": 466
    },
    {
      "epoch": 0.934,
      "grad_norm": 1.5215941667556763,
      "learning_rate": 0.0015646009036982566,
      "loss": 1.8299,
      "step": 467
    },
    {
      "epoch": 0.936,
      "grad_norm": 2.7674427032470703,
      "learning_rate": 0.0015628652385408508,
      "loss": 1.6396,
      "step": 468
    },
    {
      "epoch": 0.938,
      "grad_norm": 1.0575882196426392,
      "learning_rate": 0.001561127087838455,
      "loss": 1.4491,
      "step": 469
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.733551263809204,
      "learning_rate": 0.0015593864592665333,
      "loss": 1.8176,
      "step": 470
    },
    {
      "epoch": 0.942,
      "grad_norm": 1.4474126100540161,
      "learning_rate": 0.001557643360511491,
      "loss": 1.615,
      "step": 471
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.8422797322273254,
      "learning_rate": 0.0015558977992706424,
      "loss": 1.5498,
      "step": 472
    },
    {
      "epoch": 0.946,
      "grad_norm": 1.0337456464767456,
      "learning_rate": 0.001554149783252175,
      "loss": 1.6324,
      "step": 473
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.8075729608535767,
      "learning_rate": 0.0015523993201751166,
      "loss": 1.5321,
      "step": 474
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.2668739557266235,
      "learning_rate": 0.0015506464177693008,
      "loss": 1.7793,
      "step": 475
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.7997688055038452,
      "learning_rate": 0.001548891083775334,
      "loss": 1.7718,
      "step": 476
    },
    {
      "epoch": 0.954,
      "grad_norm": 1.2308053970336914,
      "learning_rate": 0.001547133325944559,
      "loss": 1.6295,
      "step": 477
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.8287737965583801,
      "learning_rate": 0.0015453731520390214,
      "loss": 1.582,
      "step": 478
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.7936168313026428,
      "learning_rate": 0.0015436105698314383,
      "loss": 1.6437,
      "step": 479
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.871144413948059,
      "learning_rate": 0.001541845587105159,
      "loss": 1.7393,
      "step": 480
    },
    {
      "epoch": 0.962,
      "grad_norm": 1.0065170526504517,
      "learning_rate": 0.001540078211654135,
      "loss": 1.5133,
      "step": 481
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.7463515996932983,
      "learning_rate": 0.0015383084512828825,
      "loss": 1.5742,
      "step": 482
    },
    {
      "epoch": 0.966,
      "grad_norm": 1.4624966382980347,
      "learning_rate": 0.0015365363138064498,
      "loss": 1.5693,
      "step": 483
    },
    {
      "epoch": 0.968,
      "grad_norm": 2.0347917079925537,
      "learning_rate": 0.0015347618070503826,
      "loss": 1.638,
      "step": 484
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.8479105234146118,
      "learning_rate": 0.0015329849388506886,
      "loss": 1.5787,
      "step": 485
    },
    {
      "epoch": 0.972,
      "grad_norm": 1.2716195583343506,
      "learning_rate": 0.0015312057170538034,
      "loss": 1.4339,
      "step": 486
    },
    {
      "epoch": 0.974,
      "grad_norm": 2.105614185333252,
      "learning_rate": 0.0015294241495165558,
      "loss": 1.6836,
      "step": 487
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.3805021047592163,
      "learning_rate": 0.0015276402441061327,
      "loss": 1.6813,
      "step": 488
    },
    {
      "epoch": 0.978,
      "grad_norm": 1.1508023738861084,
      "learning_rate": 0.0015258540087000458,
      "loss": 1.5458,
      "step": 489
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.9013211727142334,
      "learning_rate": 0.001524065451186095,
      "loss": 1.3457,
      "step": 490
    },
    {
      "epoch": 0.982,
      "grad_norm": 0.956143319606781,
      "learning_rate": 0.0015222745794623341,
      "loss": 1.601,
      "step": 491
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.786769688129425,
      "learning_rate": 0.0015204814014370372,
      "loss": 1.5378,
      "step": 492
    },
    {
      "epoch": 0.986,
      "grad_norm": 0.7278709411621094,
      "learning_rate": 0.0015186859250286616,
      "loss": 1.4091,
      "step": 493
    },
    {
      "epoch": 0.988,
      "grad_norm": 1.3844939470291138,
      "learning_rate": 0.0015168881581658147,
      "loss": 1.6194,
      "step": 494
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.487123727798462,
      "learning_rate": 0.0015150881087872183,
      "loss": 1.6873,
      "step": 495
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.6781864762306213,
      "learning_rate": 0.0015132857848416733,
      "loss": 1.5885,
      "step": 496
    },
    {
      "epoch": 0.994,
      "grad_norm": 1.135419487953186,
      "learning_rate": 0.0015114811942880243,
      "loss": 1.594,
      "step": 497
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.0823935270309448,
      "learning_rate": 0.0015096743450951258,
      "loss": 1.614,
      "step": 498
    },
    {
      "epoch": 0.998,
      "grad_norm": 0.9972948431968689,
      "learning_rate": 0.0015078652452418062,
      "loss": 1.4762,
      "step": 499
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7504764795303345,
      "learning_rate": 0.0015060539027168317,
      "loss": 1.4587,
      "step": 500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.5547724962234497,
      "eval_runtime": 228.9372,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 500
    },
    {
      "epoch": 1.002,
      "grad_norm": 1.1780281066894531,
      "learning_rate": 0.0015042403255188723,
      "loss": 1.4629,
      "step": 501
    },
    {
      "epoch": 1.004,
      "grad_norm": 1.1023913621902466,
      "learning_rate": 0.0015024245216564668,
      "loss": 1.5572,
      "step": 502
    },
    {
      "epoch": 1.006,
      "grad_norm": 1.5896475315093994,
      "learning_rate": 0.0015006064991479853,
      "loss": 1.6457,
      "step": 503
    },
    {
      "epoch": 1.008,
      "grad_norm": 1.242985486984253,
      "learning_rate": 0.0014987862660215965,
      "loss": 1.6242,
      "step": 504
    },
    {
      "epoch": 1.01,
      "grad_norm": 1.5107595920562744,
      "learning_rate": 0.0014969638303152296,
      "loss": 1.4774,
      "step": 505
    },
    {
      "epoch": 1.012,
      "grad_norm": 1.1105130910873413,
      "learning_rate": 0.0014951392000765412,
      "loss": 1.4446,
      "step": 506
    },
    {
      "epoch": 1.014,
      "grad_norm": 1.0065275430679321,
      "learning_rate": 0.0014933123833628787,
      "loss": 1.5513,
      "step": 507
    },
    {
      "epoch": 1.016,
      "grad_norm": 1.1883883476257324,
      "learning_rate": 0.0014914833882412432,
      "loss": 1.4275,
      "step": 508
    },
    {
      "epoch": 1.018,
      "grad_norm": 1.639533281326294,
      "learning_rate": 0.0014896522227882578,
      "loss": 1.639,
      "step": 509
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.9548944234848022,
      "learning_rate": 0.0014878188950901274,
      "loss": 1.5666,
      "step": 510
    },
    {
      "epoch": 1.022,
      "grad_norm": 1.0928623676300049,
      "learning_rate": 0.001485983413242606,
      "loss": 1.5923,
      "step": 511
    },
    {
      "epoch": 1.024,
      "grad_norm": 1.169904112815857,
      "learning_rate": 0.0014841457853509606,
      "loss": 1.4162,
      "step": 512
    },
    {
      "epoch": 1.026,
      "grad_norm": 0.9876121878623962,
      "learning_rate": 0.0014823060195299335,
      "loss": 1.6376,
      "step": 513
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.9296040534973145,
      "learning_rate": 0.0014804641239037095,
      "loss": 1.4292,
      "step": 514
    },
    {
      "epoch": 1.03,
      "grad_norm": 1.4444328546524048,
      "learning_rate": 0.0014786201066058766,
      "loss": 1.6087,
      "step": 515
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.707074761390686,
      "learning_rate": 0.001476773975779393,
      "loss": 1.696,
      "step": 516
    },
    {
      "epoch": 1.034,
      "grad_norm": 1.3413773775100708,
      "learning_rate": 0.0014749257395765502,
      "loss": 1.5488,
      "step": 517
    },
    {
      "epoch": 1.036,
      "grad_norm": 1.2088373899459839,
      "learning_rate": 0.0014730754061589356,
      "loss": 1.4281,
      "step": 518
    },
    {
      "epoch": 1.038,
      "grad_norm": 1.576151728630066,
      "learning_rate": 0.0014712229836973986,
      "loss": 1.5066,
      "step": 519
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.3051886558532715,
      "learning_rate": 0.0014693684803720138,
      "loss": 1.5675,
      "step": 520
    },
    {
      "epoch": 1.042,
      "grad_norm": 10.94564151763916,
      "learning_rate": 0.0014675119043720435,
      "loss": 1.6284,
      "step": 521
    },
    {
      "epoch": 1.044,
      "grad_norm": 10.788118362426758,
      "learning_rate": 0.0014656532638959035,
      "loss": 1.6135,
      "step": 522
    },
    {
      "epoch": 1.046,
      "grad_norm": 2.6469173431396484,
      "learning_rate": 0.001463792567151126,
      "loss": 1.6918,
      "step": 523
    },
    {
      "epoch": 1.048,
      "grad_norm": 35.90705490112305,
      "learning_rate": 0.0014619298223543236,
      "loss": 1.6402,
      "step": 524
    },
    {
      "epoch": 1.05,
      "grad_norm": 4.172820568084717,
      "learning_rate": 0.0014600650377311522,
      "loss": 1.4812,
      "step": 525
    },
    {
      "epoch": 1.052,
      "grad_norm": 1.5786224603652954,
      "learning_rate": 0.001458198221516276,
      "loss": 1.4829,
      "step": 526
    },
    {
      "epoch": 1.054,
      "grad_norm": 4.870033264160156,
      "learning_rate": 0.0014563293819533298,
      "loss": 1.6162,
      "step": 527
    },
    {
      "epoch": 1.056,
      "grad_norm": 2.5753307342529297,
      "learning_rate": 0.0014544585272948842,
      "loss": 1.6945,
      "step": 528
    },
    {
      "epoch": 1.058,
      "grad_norm": 2.546924114227295,
      "learning_rate": 0.0014525856658024075,
      "loss": 1.7231,
      "step": 529
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.9857914447784424,
      "learning_rate": 0.0014507108057462297,
      "loss": 1.6715,
      "step": 530
    },
    {
      "epoch": 1.062,
      "grad_norm": 1.2026174068450928,
      "learning_rate": 0.0014488339554055072,
      "loss": 1.4322,
      "step": 531
    },
    {
      "epoch": 1.064,
      "grad_norm": 6.831431865692139,
      "learning_rate": 0.0014469551230681843,
      "loss": 1.3952,
      "step": 532
    },
    {
      "epoch": 1.066,
      "grad_norm": 5.6800127029418945,
      "learning_rate": 0.0014450743170309583,
      "loss": 1.8683,
      "step": 533
    },
    {
      "epoch": 1.068,
      "grad_norm": 4.73813009262085,
      "learning_rate": 0.0014431915455992415,
      "loss": 1.82,
      "step": 534
    },
    {
      "epoch": 1.07,
      "grad_norm": 4.033404350280762,
      "learning_rate": 0.001441306817087125,
      "loss": 1.6389,
      "step": 535
    },
    {
      "epoch": 1.072,
      "grad_norm": 5.071736812591553,
      "learning_rate": 0.0014394201398173437,
      "loss": 1.6672,
      "step": 536
    },
    {
      "epoch": 1.074,
      "grad_norm": 1.3827528953552246,
      "learning_rate": 0.0014375315221212357,
      "loss": 1.5036,
      "step": 537
    },
    {
      "epoch": 1.076,
      "grad_norm": 2.5762128829956055,
      "learning_rate": 0.001435640972338709,
      "loss": 1.5455,
      "step": 538
    },
    {
      "epoch": 1.078,
      "grad_norm": 1.3640952110290527,
      "learning_rate": 0.001433748498818204,
      "loss": 1.5559,
      "step": 539
    },
    {
      "epoch": 1.08,
      "grad_norm": 6.8236083984375,
      "learning_rate": 0.0014318541099166556,
      "loss": 1.4586,
      "step": 540
    },
    {
      "epoch": 1.082,
      "grad_norm": 0.9722149968147278,
      "learning_rate": 0.0014299578139994557,
      "loss": 1.5188,
      "step": 541
    },
    {
      "epoch": 1.084,
      "grad_norm": 5.86314582824707,
      "learning_rate": 0.0014280596194404186,
      "loss": 1.4754,
      "step": 542
    },
    {
      "epoch": 1.086,
      "grad_norm": 0.9638668298721313,
      "learning_rate": 0.001426159534621743,
      "loss": 1.5417,
      "step": 543
    },
    {
      "epoch": 1.088,
      "grad_norm": 1.4243652820587158,
      "learning_rate": 0.0014242575679339737,
      "loss": 1.4833,
      "step": 544
    },
    {
      "epoch": 1.09,
      "grad_norm": 18.954627990722656,
      "learning_rate": 0.0014223537277759666,
      "loss": 1.468,
      "step": 545
    },
    {
      "epoch": 1.092,
      "grad_norm": 1.109449028968811,
      "learning_rate": 0.0014204480225548494,
      "loss": 1.5664,
      "step": 546
    },
    {
      "epoch": 1.094,
      "grad_norm": 0.8905270099639893,
      "learning_rate": 0.0014185404606859874,
      "loss": 1.5251,
      "step": 547
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.262162208557129,
      "learning_rate": 0.0014166310505929435,
      "loss": 1.5743,
      "step": 548
    },
    {
      "epoch": 1.098,
      "grad_norm": 0.8477277755737305,
      "learning_rate": 0.0014147198007074416,
      "loss": 1.5832,
      "step": 549
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.8544580936431885,
      "learning_rate": 0.0014128067194693315,
      "loss": 1.6135,
      "step": 550
    },
    {
      "epoch": 1.102,
      "grad_norm": 1.6738367080688477,
      "learning_rate": 0.0014108918153265485,
      "loss": 1.41,
      "step": 551
    },
    {
      "epoch": 1.104,
      "grad_norm": 1.5051926374435425,
      "learning_rate": 0.001408975096735078,
      "loss": 1.6038,
      "step": 552
    },
    {
      "epoch": 1.106,
      "grad_norm": 1.3606088161468506,
      "learning_rate": 0.0014070565721589195,
      "loss": 1.6568,
      "step": 553
    },
    {
      "epoch": 1.108,
      "grad_norm": 1.434888243675232,
      "learning_rate": 0.0014051362500700447,
      "loss": 1.5436,
      "step": 554
    },
    {
      "epoch": 1.11,
      "grad_norm": 2.0266904830932617,
      "learning_rate": 0.0014032141389483648,
      "loss": 1.5003,
      "step": 555
    },
    {
      "epoch": 1.112,
      "grad_norm": 18.30324935913086,
      "learning_rate": 0.0014012902472816907,
      "loss": 1.5255,
      "step": 556
    },
    {
      "epoch": 1.114,
      "grad_norm": 6.174690246582031,
      "learning_rate": 0.0013993645835656955,
      "loss": 1.9058,
      "step": 557
    },
    {
      "epoch": 1.116,
      "grad_norm": 7.176418304443359,
      "learning_rate": 0.0013974371563038785,
      "loss": 2.2636,
      "step": 558
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 4.957699298858643,
      "learning_rate": 0.0013955079740075255,
      "loss": 2.0708,
      "step": 559
    },
    {
      "epoch": 1.12,
      "grad_norm": 20.495866775512695,
      "learning_rate": 0.0013935770451956732,
      "loss": 1.6678,
      "step": 560
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 9.747633934020996,
      "learning_rate": 0.0013916443783950694,
      "loss": 1.8024,
      "step": 561
    },
    {
      "epoch": 1.124,
      "grad_norm": 13.572236061096191,
      "learning_rate": 0.0013897099821401384,
      "loss": 2.239,
      "step": 562
    },
    {
      "epoch": 1.126,
      "grad_norm": 5.918147087097168,
      "learning_rate": 0.0013877738649729406,
      "loss": 2.621,
      "step": 563
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.567435622215271,
      "learning_rate": 0.0013858360354431355,
      "loss": 2.1715,
      "step": 564
    },
    {
      "epoch": 1.13,
      "grad_norm": 5.028715133666992,
      "learning_rate": 0.0013838965021079445,
      "loss": 1.8252,
      "step": 565
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 4.168030738830566,
      "learning_rate": 0.0013819552735321134,
      "loss": 2.018,
      "step": 566
    },
    {
      "epoch": 1.134,
      "grad_norm": 2.295701503753662,
      "learning_rate": 0.001380012358287873,
      "loss": 1.941,
      "step": 567
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.4259865283966064,
      "learning_rate": 0.0013780677649549026,
      "loss": 1.7363,
      "step": 568
    },
    {
      "epoch": 1.138,
      "grad_norm": 1.300613522529602,
      "learning_rate": 0.0013761215021202914,
      "loss": 1.8905,
      "step": 569
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.816460132598877,
      "learning_rate": 0.001374173578378502,
      "loss": 1.583,
      "step": 570
    },
    {
      "epoch": 1.142,
      "grad_norm": 1.21202552318573,
      "learning_rate": 0.0013722240023313307,
      "loss": 1.5779,
      "step": 571
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.182912826538086,
      "learning_rate": 0.0013702727825878694,
      "loss": 1.7754,
      "step": 572
    },
    {
      "epoch": 1.146,
      "grad_norm": 0.9406577348709106,
      "learning_rate": 0.0013683199277644694,
      "loss": 1.6909,
      "step": 573
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.9332537651062012,
      "learning_rate": 0.001366365446484702,
      "loss": 1.6569,
      "step": 574
    },
    {
      "epoch": 1.15,
      "grad_norm": 1.0008440017700195,
      "learning_rate": 0.0013644093473793213,
      "loss": 1.6609,
      "step": 575
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.8391348719596863,
      "learning_rate": 0.0013624516390862242,
      "loss": 1.6126,
      "step": 576
    },
    {
      "epoch": 1.154,
      "grad_norm": 1.2672902345657349,
      "learning_rate": 0.0013604923302504147,
      "loss": 1.7475,
      "step": 577
    },
    {
      "epoch": 1.156,
      "grad_norm": 1.0147987604141235,
      "learning_rate": 0.0013585314295239644,
      "loss": 1.5618,
      "step": 578
    },
    {
      "epoch": 1.158,
      "grad_norm": 1.1782443523406982,
      "learning_rate": 0.0013565689455659737,
      "loss": 1.6344,
      "step": 579
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.077957034111023,
      "learning_rate": 0.0013546048870425357,
      "loss": 1.5838,
      "step": 580
    },
    {
      "epoch": 1.162,
      "grad_norm": 2.2564644813537598,
      "learning_rate": 0.0013526392626266957,
      "loss": 1.6375,
      "step": 581
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.196020483970642,
      "learning_rate": 0.0013506720809984137,
      "loss": 1.6993,
      "step": 582
    },
    {
      "epoch": 1.166,
      "grad_norm": 1.3285222053527832,
      "learning_rate": 0.001348703350844527,
      "loss": 1.5924,
      "step": 583
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.7322521209716797,
      "learning_rate": 0.0013467330808587098,
      "loss": 1.7281,
      "step": 584
    },
    {
      "epoch": 1.17,
      "grad_norm": 1.1363630294799805,
      "learning_rate": 0.001344761279741437,
      "loss": 1.5261,
      "step": 585
    },
    {
      "epoch": 1.172,
      "grad_norm": 1.2277910709381104,
      "learning_rate": 0.001342787956199945,
      "loss": 1.7118,
      "step": 586
    },
    {
      "epoch": 1.174,
      "grad_norm": 0.9945822358131409,
      "learning_rate": 0.001340813118948191,
      "loss": 1.6094,
      "step": 587
    },
    {
      "epoch": 1.176,
      "grad_norm": 2.0086967945098877,
      "learning_rate": 0.0013388367767068199,
      "loss": 1.645,
      "step": 588
    },
    {
      "epoch": 1.178,
      "grad_norm": 1.126505732536316,
      "learning_rate": 0.0013368589382031196,
      "loss": 1.6289,
      "step": 589
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.1656737327575684,
      "learning_rate": 0.0013348796121709862,
      "loss": 1.5489,
      "step": 590
    },
    {
      "epoch": 1.182,
      "grad_norm": 1.0129072666168213,
      "learning_rate": 0.0013328988073508853,
      "loss": 1.6256,
      "step": 591
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.9766730070114136,
      "learning_rate": 0.001330916532489811,
      "loss": 1.5712,
      "step": 592
    },
    {
      "epoch": 1.186,
      "grad_norm": 1.049911618232727,
      "learning_rate": 0.001328932796341251,
      "loss": 1.6635,
      "step": 593
    },
    {
      "epoch": 1.188,
      "grad_norm": 1.2256388664245605,
      "learning_rate": 0.0013269476076651447,
      "loss": 1.5562,
      "step": 594
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.9173448085784912,
      "learning_rate": 0.0013249609752278453,
      "loss": 1.5869,
      "step": 595
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.167176604270935,
      "learning_rate": 0.0013229729078020822,
      "loss": 1.4529,
      "step": 596
    },
    {
      "epoch": 1.194,
      "grad_norm": 0.7670183777809143,
      "learning_rate": 0.0013209834141669212,
      "loss": 1.5984,
      "step": 597
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.9808332920074463,
      "learning_rate": 0.0013189925031077267,
      "loss": 1.5442,
      "step": 598
    },
    {
      "epoch": 1.198,
      "grad_norm": 1.0966367721557617,
      "learning_rate": 0.0013170001834161209,
      "loss": 1.4578,
      "step": 599
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.2338659763336182,
      "learning_rate": 0.001315006463889948,
      "loss": 1.608,
      "step": 600
    },
    {
      "epoch": 1.2,
      "eval_loss": 1.5334744453430176,
      "eval_runtime": 229.0031,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 600
    },
    {
      "epoch": 1.202,
      "grad_norm": 1.4098153114318848,
      "learning_rate": 0.0013130113533332325,
      "loss": 1.652,
      "step": 601
    },
    {
      "epoch": 1.204,
      "grad_norm": 4.585978984832764,
      "learning_rate": 0.0013110148605561419,
      "loss": 1.5079,
      "step": 602
    },
    {
      "epoch": 1.206,
      "grad_norm": 1.1779398918151855,
      "learning_rate": 0.0013090169943749475,
      "loss": 1.4973,
      "step": 603
    },
    {
      "epoch": 1.208,
      "grad_norm": 1.004794716835022,
      "learning_rate": 0.0013070177636119854,
      "loss": 1.5554,
      "step": 604
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.9892309904098511,
      "learning_rate": 0.0013050171770956176,
      "loss": 1.5823,
      "step": 605
    },
    {
      "epoch": 1.212,
      "grad_norm": 2.153183937072754,
      "learning_rate": 0.0013030152436601927,
      "loss": 1.453,
      "step": 606
    },
    {
      "epoch": 1.214,
      "grad_norm": 1.1621555089950562,
      "learning_rate": 0.0013010119721460073,
      "loss": 1.5634,
      "step": 607
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.8639954328536987,
      "learning_rate": 0.0012990073713992662,
      "loss": 1.3446,
      "step": 608
    },
    {
      "epoch": 1.218,
      "grad_norm": 0.9370217323303223,
      "learning_rate": 0.0012970014502720452,
      "loss": 1.5977,
      "step": 609
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.5776727199554443,
      "learning_rate": 0.0012949942176222495,
      "loss": 1.7048,
      "step": 610
    },
    {
      "epoch": 1.222,
      "grad_norm": 1.0274442434310913,
      "learning_rate": 0.0012929856823135771,
      "loss": 1.5001,
      "step": 611
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.8040680289268494,
      "learning_rate": 0.0012909758532154765,
      "loss": 1.5553,
      "step": 612
    },
    {
      "epoch": 1.226,
      "grad_norm": 1.5417072772979736,
      "learning_rate": 0.0012889647392031111,
      "loss": 1.5585,
      "step": 613
    },
    {
      "epoch": 1.228,
      "grad_norm": 1.0702061653137207,
      "learning_rate": 0.0012869523491573181,
      "loss": 1.4095,
      "step": 614
    },
    {
      "epoch": 1.23,
      "grad_norm": 1.9772862195968628,
      "learning_rate": 0.0012849386919645686,
      "loss": 1.7985,
      "step": 615
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.4403539896011353,
      "learning_rate": 0.00128292377651693,
      "loss": 1.5532,
      "step": 616
    },
    {
      "epoch": 1.234,
      "grad_norm": 1.5148303508758545,
      "learning_rate": 0.001280907611712026,
      "loss": 1.492,
      "step": 617
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.1462969779968262,
      "learning_rate": 0.001278890206452997,
      "loss": 1.5264,
      "step": 618
    },
    {
      "epoch": 1.238,
      "grad_norm": 1.2081433534622192,
      "learning_rate": 0.0012768715696484616,
      "loss": 1.6925,
      "step": 619
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.8226962089538574,
      "learning_rate": 0.0012748517102124754,
      "loss": 1.4884,
      "step": 620
    },
    {
      "epoch": 1.242,
      "grad_norm": 1.627720832824707,
      "learning_rate": 0.0012728306370644953,
      "loss": 1.5379,
      "step": 621
    },
    {
      "epoch": 1.244,
      "grad_norm": 1.1967682838439941,
      "learning_rate": 0.0012708083591293359,
      "loss": 1.5264,
      "step": 622
    },
    {
      "epoch": 1.246,
      "grad_norm": 0.9124382734298706,
      "learning_rate": 0.0012687848853371322,
      "loss": 1.5769,
      "step": 623
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.6862711906433105,
      "learning_rate": 0.001266760224623301,
      "loss": 1.441,
      "step": 624
    },
    {
      "epoch": 1.25,
      "grad_norm": 2.2149643898010254,
      "learning_rate": 0.0012647343859284997,
      "loss": 1.6024,
      "step": 625
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.9566590189933777,
      "learning_rate": 0.0012627073781985869,
      "loss": 1.5418,
      "step": 626
    },
    {
      "epoch": 1.254,
      "grad_norm": 1.0358984470367432,
      "learning_rate": 0.001260679210384585,
      "loss": 1.4036,
      "step": 627
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.0196791887283325,
      "learning_rate": 0.0012586498914426382,
      "loss": 1.5592,
      "step": 628
    },
    {
      "epoch": 1.258,
      "grad_norm": 0.9406511783599854,
      "learning_rate": 0.0012566194303339738,
      "loss": 1.3775,
      "step": 629
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.7432499527931213,
      "learning_rate": 0.0012545878360248632,
      "loss": 1.5476,
      "step": 630
    },
    {
      "epoch": 1.262,
      "grad_norm": 0.7222140431404114,
      "learning_rate": 0.001252555117486582,
      "loss": 1.3878,
      "step": 631
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.2760851383209229,
      "learning_rate": 0.00125052128369537,
      "loss": 1.4156,
      "step": 632
    },
    {
      "epoch": 1.266,
      "grad_norm": 1.6329388618469238,
      "learning_rate": 0.001248486343632392,
      "loss": 1.5439,
      "step": 633
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.8072954416275024,
      "learning_rate": 0.0012464503062836975,
      "loss": 1.5009,
      "step": 634
    },
    {
      "epoch": 1.27,
      "grad_norm": 1.111607551574707,
      "learning_rate": 0.0012444131806401816,
      "loss": 1.4491,
      "step": 635
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.7511457204818726,
      "learning_rate": 0.001242374975697546,
      "loss": 1.372,
      "step": 636
    },
    {
      "epoch": 1.274,
      "grad_norm": 1.320016860961914,
      "learning_rate": 0.0012403357004562574,
      "loss": 1.4534,
      "step": 637
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.958395779132843,
      "learning_rate": 0.0012382953639215096,
      "loss": 1.3985,
      "step": 638
    },
    {
      "epoch": 1.278,
      "grad_norm": 0.6014586687088013,
      "learning_rate": 0.0012362539751031823,
      "loss": 1.449,
      "step": 639
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.2806674242019653,
      "learning_rate": 0.0012342115430158023,
      "loss": 1.4922,
      "step": 640
    },
    {
      "epoch": 1.282,
      "grad_norm": 0.6254866719245911,
      "learning_rate": 0.0012321680766785035,
      "loss": 1.5013,
      "step": 641
    },
    {
      "epoch": 1.284,
      "grad_norm": 1.0840685367584229,
      "learning_rate": 0.0012301235851149865,
      "loss": 1.7222,
      "step": 642
    },
    {
      "epoch": 1.286,
      "grad_norm": 0.7782782912254333,
      "learning_rate": 0.0012280780773534794,
      "loss": 1.4106,
      "step": 643
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.9271112680435181,
      "learning_rate": 0.001226031562426698,
      "loss": 1.5154,
      "step": 644
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.7043904066085815,
      "learning_rate": 0.0012239840493718048,
      "loss": 1.3854,
      "step": 645
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.8749702572822571,
      "learning_rate": 0.001221935547230371,
      "loss": 1.4865,
      "step": 646
    },
    {
      "epoch": 1.294,
      "grad_norm": 0.9267475605010986,
      "learning_rate": 0.0012198860650483344,
      "loss": 1.4511,
      "step": 647
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.5016639232635498,
      "learning_rate": 0.0012178356118759618,
      "loss": 1.3611,
      "step": 648
    },
    {
      "epoch": 1.298,
      "grad_norm": 0.618175745010376,
      "learning_rate": 0.0012157841967678062,
      "loss": 1.4025,
      "step": 649
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.0168097019195557,
      "learning_rate": 0.0012137318287826697,
      "loss": 1.5556,
      "step": 650
    },
    {
      "epoch": 1.302,
      "grad_norm": 2.2081856727600098,
      "learning_rate": 0.0012116785169835617,
      "loss": 1.4574,
      "step": 651
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.8473395109176636,
      "learning_rate": 0.0012096242704376598,
      "loss": 1.465,
      "step": 652
    },
    {
      "epoch": 1.306,
      "grad_norm": 0.8876979351043701,
      "learning_rate": 0.0012075690982162677,
      "loss": 1.5276,
      "step": 653
    },
    {
      "epoch": 1.308,
      "grad_norm": 1.0146435499191284,
      "learning_rate": 0.001205513009394779,
      "loss": 1.5959,
      "step": 654
    },
    {
      "epoch": 1.31,
      "grad_norm": 5.470303058624268,
      "learning_rate": 0.001203456013052634,
      "loss": 1.5064,
      "step": 655
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.755900502204895,
      "learning_rate": 0.0012013981182732796,
      "loss": 1.5524,
      "step": 656
    },
    {
      "epoch": 1.314,
      "grad_norm": 3.6215336322784424,
      "learning_rate": 0.0011993393341441319,
      "loss": 1.5399,
      "step": 657
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.1844679117202759,
      "learning_rate": 0.0011972796697565322,
      "loss": 1.5394,
      "step": 658
    },
    {
      "epoch": 1.318,
      "grad_norm": 0.919153094291687,
      "learning_rate": 0.0011952191342057103,
      "loss": 1.4475,
      "step": 659
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.9697510600090027,
      "learning_rate": 0.0011931577365907433,
      "loss": 1.5113,
      "step": 660
    },
    {
      "epoch": 1.322,
      "grad_norm": 1.5506731271743774,
      "learning_rate": 0.0011910954860145137,
      "loss": 1.5614,
      "step": 661
    },
    {
      "epoch": 1.324,
      "grad_norm": 2.0964367389678955,
      "learning_rate": 0.0011890323915836713,
      "loss": 1.5209,
      "step": 662
    },
    {
      "epoch": 1.326,
      "grad_norm": 1.372867465019226,
      "learning_rate": 0.0011869684624085924,
      "loss": 1.4731,
      "step": 663
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.275971531867981,
      "learning_rate": 0.001184903707603339,
      "loss": 1.512,
      "step": 664
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.9447258114814758,
      "learning_rate": 0.0011828381362856196,
      "loss": 1.4427,
      "step": 665
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.9499161243438721,
      "learning_rate": 0.0011807717575767474,
      "loss": 1.5645,
      "step": 666
    },
    {
      "epoch": 1.334,
      "grad_norm": 0.5635765790939331,
      "learning_rate": 0.001178704580601602,
      "loss": 1.564,
      "step": 667
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.2151210308074951,
      "learning_rate": 0.0011766366144885876,
      "loss": 1.6054,
      "step": 668
    },
    {
      "epoch": 1.338,
      "grad_norm": 0.792966902256012,
      "learning_rate": 0.0011745678683695927,
      "loss": 1.568,
      "step": 669
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.1206854581832886,
      "learning_rate": 0.0011724983513799504,
      "loss": 1.4106,
      "step": 670
    },
    {
      "epoch": 1.342,
      "grad_norm": 0.6191392540931702,
      "learning_rate": 0.0011704280726583989,
      "loss": 1.488,
      "step": 671
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.9175344705581665,
      "learning_rate": 0.0011683570413470383,
      "loss": 1.5011,
      "step": 672
    },
    {
      "epoch": 1.346,
      "grad_norm": 0.8753415942192078,
      "learning_rate": 0.0011662852665912942,
      "loss": 1.4305,
      "step": 673
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 1.0487139225006104,
      "learning_rate": 0.0011642127575398728,
      "loss": 1.359,
      "step": 674
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.091314435005188,
      "learning_rate": 0.0011621395233447247,
      "loss": 1.4001,
      "step": 675
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.6114835143089294,
      "learning_rate": 0.001160065573161002,
      "loss": 1.5224,
      "step": 676
    },
    {
      "epoch": 1.354,
      "grad_norm": 1.3227825164794922,
      "learning_rate": 0.001157990916147018,
      "loss": 1.4068,
      "step": 677
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 1.2252479791641235,
      "learning_rate": 0.0011559155614642082,
      "loss": 1.5362,
      "step": 678
    },
    {
      "epoch": 1.358,
      "grad_norm": 0.8008044958114624,
      "learning_rate": 0.0011538395182770886,
      "loss": 1.4365,
      "step": 679
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.922588586807251,
      "learning_rate": 0.0011517627957532152,
      "loss": 1.5931,
      "step": 680
    },
    {
      "epoch": 1.362,
      "grad_norm": 0.90240079164505,
      "learning_rate": 0.0011496854030631444,
      "loss": 1.3875,
      "step": 681
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 1.2543224096298218,
      "learning_rate": 0.0011476073493803913,
      "loss": 1.625,
      "step": 682
    },
    {
      "epoch": 1.366,
      "grad_norm": 0.8563950061798096,
      "learning_rate": 0.0011455286438813907,
      "loss": 1.4075,
      "step": 683
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.7726670503616333,
      "learning_rate": 0.001143449295745455,
      "loss": 1.4097,
      "step": 684
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.7912070155143738,
      "learning_rate": 0.0011413693141547351,
      "loss": 1.4951,
      "step": 685
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.6634076237678528,
      "learning_rate": 0.001139288708294178,
      "loss": 1.4517,
      "step": 686
    },
    {
      "epoch": 1.374,
      "grad_norm": 1.0928665399551392,
      "learning_rate": 0.0011372074873514893,
      "loss": 1.3551,
      "step": 687
    },
    {
      "epoch": 1.376,
      "grad_norm": 1.1028386354446411,
      "learning_rate": 0.0011351256605170886,
      "loss": 1.5235,
      "step": 688
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 0.6663061380386353,
      "learning_rate": 0.0011330432369840726,
      "loss": 1.364,
      "step": 689
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.0999181270599365,
      "learning_rate": 0.0011309602259481726,
      "loss": 1.4756,
      "step": 690
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 0.931542694568634,
      "learning_rate": 0.001128876636607713,
      "loss": 1.5298,
      "step": 691
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.8382184505462646,
      "learning_rate": 0.0011267924781635742,
      "loss": 1.4457,
      "step": 692
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 0.7070698738098145,
      "learning_rate": 0.0011247077598191479,
      "loss": 1.4244,
      "step": 693
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.1126073598861694,
      "learning_rate": 0.0011226224907802983,
      "loss": 1.4628,
      "step": 694
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.8875011801719666,
      "learning_rate": 0.0011205366802553229,
      "loss": 1.3634,
      "step": 695
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.8479020595550537,
      "learning_rate": 0.001118450337454909,
      "loss": 1.3608,
      "step": 696
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 1.254683017730713,
      "learning_rate": 0.0011163634715920946,
      "loss": 1.4841,
      "step": 697
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.8564544320106506,
      "learning_rate": 0.0011142760918822275,
      "loss": 1.3678,
      "step": 698
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 0.5995428562164307,
      "learning_rate": 0.0011121882075429248,
      "loss": 1.3635,
      "step": 699
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.453490138053894,
      "learning_rate": 0.0011100998277940315,
      "loss": 1.4779,
      "step": 700
    },
    {
      "epoch": 1.4,
      "eval_loss": 1.4298977851867676,
      "eval_runtime": 228.9961,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 700
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 0.9146859049797058,
      "learning_rate": 0.0011080109618575816,
      "loss": 1.4162,
      "step": 701
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.8046936392784119,
      "learning_rate": 0.001105921618957754,
      "loss": 1.3655,
      "step": 702
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 1.1970430612564087,
      "learning_rate": 0.0011038318083208354,
      "loss": 1.4462,
      "step": 703
    },
    {
      "epoch": 1.408,
      "grad_norm": 3.2156169414520264,
      "learning_rate": 0.0011017415391751774,
      "loss": 1.5788,
      "step": 704
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.9793831706047058,
      "learning_rate": 0.0010996508207511565,
      "loss": 1.365,
      "step": 705
    },
    {
      "epoch": 1.412,
      "grad_norm": 17.40815544128418,
      "learning_rate": 0.001097559662281133,
      "loss": 1.4031,
      "step": 706
    },
    {
      "epoch": 1.414,
      "grad_norm": 1.0826998949050903,
      "learning_rate": 0.0010954680729994102,
      "loss": 1.5355,
      "step": 707
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.2123832702636719,
      "learning_rate": 0.0010933760621421942,
      "loss": 1.45,
      "step": 708
    },
    {
      "epoch": 1.418,
      "grad_norm": 1.2673410177230835,
      "learning_rate": 0.0010912836389475526,
      "loss": 1.4811,
      "step": 709
    },
    {
      "epoch": 1.42,
      "grad_norm": 2.361619710922241,
      "learning_rate": 0.0010891908126553738,
      "loss": 1.4186,
      "step": 710
    },
    {
      "epoch": 1.422,
      "grad_norm": 1.041494607925415,
      "learning_rate": 0.0010870975925073262,
      "loss": 1.3588,
      "step": 711
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.0763741731643677,
      "learning_rate": 0.0010850039877468172,
      "loss": 1.4442,
      "step": 712
    },
    {
      "epoch": 1.426,
      "grad_norm": 0.9534597396850586,
      "learning_rate": 0.0010829100076189533,
      "loss": 1.4104,
      "step": 713
    },
    {
      "epoch": 1.428,
      "grad_norm": 1.2166271209716797,
      "learning_rate": 0.0010808156613704978,
      "loss": 1.5201,
      "step": 714
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.6957953572273254,
      "learning_rate": 0.0010787209582498315,
      "loss": 1.4098,
      "step": 715
    },
    {
      "epoch": 1.432,
      "grad_norm": 2.021681547164917,
      "learning_rate": 0.00107662590750691,
      "loss": 1.5112,
      "step": 716
    },
    {
      "epoch": 1.434,
      "grad_norm": 0.8147386312484741,
      "learning_rate": 0.0010745305183932252,
      "loss": 1.5166,
      "step": 717
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.8298325538635254,
      "learning_rate": 0.0010724348001617625,
      "loss": 1.4171,
      "step": 718
    },
    {
      "epoch": 1.438,
      "grad_norm": 1.4096384048461914,
      "learning_rate": 0.0010703387620669606,
      "loss": 1.4327,
      "step": 719
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.309157371520996,
      "learning_rate": 0.0010682424133646711,
      "loss": 1.3511,
      "step": 720
    },
    {
      "epoch": 1.442,
      "grad_norm": 2.130545139312744,
      "learning_rate": 0.0010661457633121168,
      "loss": 1.3861,
      "step": 721
    },
    {
      "epoch": 1.444,
      "grad_norm": 1.616632342338562,
      "learning_rate": 0.0010640488211678513,
      "loss": 1.5558,
      "step": 722
    },
    {
      "epoch": 1.446,
      "grad_norm": 1.047716498374939,
      "learning_rate": 0.0010619515961917186,
      "loss": 1.4592,
      "step": 723
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.9202664494514465,
      "learning_rate": 0.0010598540976448107,
      "loss": 1.4707,
      "step": 724
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.661590814590454,
      "learning_rate": 0.0010577563347894286,
      "loss": 1.4189,
      "step": 725
    },
    {
      "epoch": 1.452,
      "grad_norm": 1.6137840747833252,
      "learning_rate": 0.0010556583168890396,
      "loss": 1.4834,
      "step": 726
    },
    {
      "epoch": 1.454,
      "grad_norm": 1.4684791564941406,
      "learning_rate": 0.0010535600532082373,
      "loss": 1.5801,
      "step": 727
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.6331346035003662,
      "learning_rate": 0.001051461553012702,
      "loss": 1.3842,
      "step": 728
    },
    {
      "epoch": 1.458,
      "grad_norm": 0.7180582284927368,
      "learning_rate": 0.001049362825569157,
      "loss": 1.4263,
      "step": 729
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.9320597052574158,
      "learning_rate": 0.0010472638801453287,
      "loss": 1.4636,
      "step": 730
    },
    {
      "epoch": 1.462,
      "grad_norm": 4.579135894775391,
      "learning_rate": 0.0010451647260099081,
      "loss": 1.3875,
      "step": 731
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.3215413093566895,
      "learning_rate": 0.0010430653724325058,
      "loss": 1.416,
      "step": 732
    },
    {
      "epoch": 1.466,
      "grad_norm": 1.0294979810714722,
      "learning_rate": 0.0010409658286836144,
      "loss": 1.3392,
      "step": 733
    },
    {
      "epoch": 1.468,
      "grad_norm": 1.1025134325027466,
      "learning_rate": 0.0010388661040345655,
      "loss": 1.4547,
      "step": 734
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.8343940377235413,
      "learning_rate": 0.0010367662077574898,
      "loss": 1.3415,
      "step": 735
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.2966703176498413,
      "learning_rate": 0.0010346661491252762,
      "loss": 1.4648,
      "step": 736
    },
    {
      "epoch": 1.474,
      "grad_norm": 0.7439001202583313,
      "learning_rate": 0.0010325659374115302,
      "loss": 1.3244,
      "step": 737
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.9620084762573242,
      "learning_rate": 0.001030465581890533,
      "loss": 1.3488,
      "step": 738
    },
    {
      "epoch": 1.478,
      "grad_norm": 0.9699628949165344,
      "learning_rate": 0.001028365091837202,
      "loss": 1.4863,
      "step": 739
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.9144236445426941,
      "learning_rate": 0.0010262644765270472,
      "loss": 1.4333,
      "step": 740
    },
    {
      "epoch": 1.482,
      "grad_norm": 0.8218076229095459,
      "learning_rate": 0.0010241637452361324,
      "loss": 1.3048,
      "step": 741
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.9960999488830566,
      "learning_rate": 0.0010220629072410338,
      "loss": 1.4169,
      "step": 742
    },
    {
      "epoch": 1.486,
      "grad_norm": 0.8271291851997375,
      "learning_rate": 0.0010199619718187984,
      "loss": 1.3285,
      "step": 743
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.013687252998352,
      "learning_rate": 0.001017860948246904,
      "loss": 1.5443,
      "step": 744
    },
    {
      "epoch": 1.49,
      "grad_norm": 1.2359248399734497,
      "learning_rate": 0.0010157598458032165,
      "loss": 1.4351,
      "step": 745
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.8602597713470459,
      "learning_rate": 0.001013658673765951,
      "loss": 1.4661,
      "step": 746
    },
    {
      "epoch": 1.494,
      "grad_norm": 1.023962378501892,
      "learning_rate": 0.0010115574414136304,
      "loss": 1.3939,
      "step": 747
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.651198148727417,
      "learning_rate": 0.0010094561580250426,
      "loss": 1.4309,
      "step": 748
    },
    {
      "epoch": 1.498,
      "grad_norm": 0.9243757128715515,
      "learning_rate": 0.0010073548328792016,
      "loss": 1.2947,
      "step": 749
    },
    {
      "epoch": 1.5,
      "grad_norm": 2.1267635822296143,
      "learning_rate": 0.0010052534752553063,
      "loss": 1.4072,
      "step": 750
    },
    {
      "epoch": 1.502,
      "grad_norm": 2.218168258666992,
      "learning_rate": 0.0010031520944326975,
      "loss": 1.5654,
      "step": 751
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.5017553567886353,
      "learning_rate": 0.0010010506996908201,
      "loss": 1.6466,
      "step": 752
    },
    {
      "epoch": 1.506,
      "grad_norm": 1.3307305574417114,
      "learning_rate": 0.0009989493003091801,
      "loss": 1.489,
      "step": 753
    },
    {
      "epoch": 1.508,
      "grad_norm": 3.102775812149048,
      "learning_rate": 0.0009968479055673027,
      "loss": 1.485,
      "step": 754
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.1093436479568481,
      "learning_rate": 0.000994746524744694,
      "loss": 1.3985,
      "step": 755
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.0472590923309326,
      "learning_rate": 0.0009926451671207984,
      "loss": 1.4356,
      "step": 756
    },
    {
      "epoch": 1.514,
      "grad_norm": 0.7667849063873291,
      "learning_rate": 0.0009905438419749576,
      "loss": 1.4919,
      "step": 757
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.5584935545921326,
      "learning_rate": 0.00098844255858637,
      "loss": 1.4791,
      "step": 758
    },
    {
      "epoch": 1.518,
      "grad_norm": 0.9921008944511414,
      "learning_rate": 0.000986341326234049,
      "loss": 1.5479,
      "step": 759
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.8702777028083801,
      "learning_rate": 0.0009842401541967838,
      "loss": 1.5382,
      "step": 760
    },
    {
      "epoch": 1.522,
      "grad_norm": 0.643673300743103,
      "learning_rate": 0.0009821390517530963,
      "loss": 1.3854,
      "step": 761
    },
    {
      "epoch": 1.524,
      "grad_norm": 1.310935139656067,
      "learning_rate": 0.0009800380281812016,
      "loss": 1.419,
      "step": 762
    },
    {
      "epoch": 1.526,
      "grad_norm": 0.7780073285102844,
      "learning_rate": 0.0009779370927589666,
      "loss": 1.4644,
      "step": 763
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.2970837354660034,
      "learning_rate": 0.000975836254763868,
      "loss": 1.4596,
      "step": 764
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.7681534290313721,
      "learning_rate": 0.000973735523472953,
      "loss": 1.4772,
      "step": 765
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.5821118354797363,
      "learning_rate": 0.0009716349081627981,
      "loss": 1.3598,
      "step": 766
    },
    {
      "epoch": 1.534,
      "grad_norm": 1.0757631063461304,
      "learning_rate": 0.0009695344181094668,
      "loss": 1.405,
      "step": 767
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.5479196310043335,
      "learning_rate": 0.0009674340625884701,
      "loss": 1.4294,
      "step": 768
    },
    {
      "epoch": 1.538,
      "grad_norm": 0.6241746544837952,
      "learning_rate": 0.000965333850874724,
      "loss": 1.359,
      "step": 769
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.6383098363876343,
      "learning_rate": 0.0009632337922425105,
      "loss": 1.3162,
      "step": 770
    },
    {
      "epoch": 1.542,
      "grad_norm": 0.9284193515777588,
      "learning_rate": 0.0009611338959654346,
      "loss": 1.3229,
      "step": 771
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.8746408224105835,
      "learning_rate": 0.0009590341713163857,
      "loss": 1.4197,
      "step": 772
    },
    {
      "epoch": 1.546,
      "grad_norm": 0.8771669864654541,
      "learning_rate": 0.0009569346275674944,
      "loss": 1.4023,
      "step": 773
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.6774560213088989,
      "learning_rate": 0.0009548352739900921,
      "loss": 1.3769,
      "step": 774
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.0013619661331177,
      "learning_rate": 0.0009527361198546714,
      "loss": 1.4379,
      "step": 775
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.9200623035430908,
      "learning_rate": 0.0009506371744308432,
      "loss": 1.4775,
      "step": 776
    },
    {
      "epoch": 1.554,
      "grad_norm": 0.9188187122344971,
      "learning_rate": 0.0009485384469872979,
      "loss": 1.4906,
      "step": 777
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.7449846863746643,
      "learning_rate": 0.0009464399467917625,
      "loss": 1.4393,
      "step": 778
    },
    {
      "epoch": 1.558,
      "grad_norm": 0.9869486093521118,
      "learning_rate": 0.0009443416831109608,
      "loss": 1.4814,
      "step": 779
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.9344437718391418,
      "learning_rate": 0.0009422436652105717,
      "loss": 1.3643,
      "step": 780
    },
    {
      "epoch": 1.562,
      "grad_norm": 0.862073540687561,
      "learning_rate": 0.0009401459023551894,
      "loss": 1.3963,
      "step": 781
    },
    {
      "epoch": 1.564,
      "grad_norm": 1.139992117881775,
      "learning_rate": 0.0009380484038082813,
      "loss": 1.3761,
      "step": 782
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 0.9760828614234924,
      "learning_rate": 0.0009359511788321485,
      "loss": 1.4262,
      "step": 783
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.6173897981643677,
      "learning_rate": 0.0009338542366878834,
      "loss": 1.3022,
      "step": 784
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 1.2682507038116455,
      "learning_rate": 0.0009317575866353291,
      "loss": 1.4142,
      "step": 785
    },
    {
      "epoch": 1.572,
      "grad_norm": 1.0130372047424316,
      "learning_rate": 0.0009296612379330396,
      "loss": 1.2608,
      "step": 786
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 0.7193174362182617,
      "learning_rate": 0.0009275651998382377,
      "loss": 1.3368,
      "step": 787
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.0688443183898926,
      "learning_rate": 0.0009254694816067747,
      "loss": 1.3775,
      "step": 788
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 0.5633663535118103,
      "learning_rate": 0.0009233740924930904,
      "loss": 1.262,
      "step": 789
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.3359781503677368,
      "learning_rate": 0.0009212790417501688,
      "loss": 1.4616,
      "step": 790
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 0.8400145769119263,
      "learning_rate": 0.0009191843386295022,
      "loss": 1.3133,
      "step": 791
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.7709165215492249,
      "learning_rate": 0.0009170899923810469,
      "loss": 1.4751,
      "step": 792
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 1.0307276248931885,
      "learning_rate": 0.0009149960122531826,
      "loss": 1.4164,
      "step": 793
    },
    {
      "epoch": 1.588,
      "grad_norm": 1.6415058374404907,
      "learning_rate": 0.0009129024074926743,
      "loss": 1.5702,
      "step": 794
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 1.1347970962524414,
      "learning_rate": 0.0009108091873446264,
      "loss": 1.3426,
      "step": 795
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.7205044627189636,
      "learning_rate": 0.0009087163610524475,
      "loss": 1.3236,
      "step": 796
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 1.1659268140792847,
      "learning_rate": 0.0009066239378578059,
      "loss": 1.4707,
      "step": 797
    },
    {
      "epoch": 1.596,
      "grad_norm": 1.2072844505310059,
      "learning_rate": 0.00090453192700059,
      "loss": 1.2708,
      "step": 798
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 0.879864513874054,
      "learning_rate": 0.0009024403377188673,
      "loss": 1.3622,
      "step": 799
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.7675594091415405,
      "learning_rate": 0.0009003491792488438,
      "loss": 1.3649,
      "step": 800
    },
    {
      "epoch": 1.6,
      "eval_loss": 1.4199143648147583,
      "eval_runtime": 228.8149,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 800
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 1.2268770933151245,
      "learning_rate": 0.0008982584608248225,
      "loss": 1.5325,
      "step": 801
    },
    {
      "epoch": 1.604,
      "grad_norm": 1.4975658655166626,
      "learning_rate": 0.0008961681916791646,
      "loss": 1.441,
      "step": 802
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 1.3009151220321655,
      "learning_rate": 0.0008940783810422461,
      "loss": 1.4345,
      "step": 803
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.3389434814453125,
      "learning_rate": 0.0008919890381424188,
      "loss": 1.474,
      "step": 804
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.6857113838195801,
      "learning_rate": 0.0008899001722059687,
      "loss": 1.3578,
      "step": 805
    },
    {
      "epoch": 1.612,
      "grad_norm": 1.199568748474121,
      "learning_rate": 0.0008878117924570754,
      "loss": 1.4888,
      "step": 806
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 0.8412646055221558,
      "learning_rate": 0.0008857239081177725,
      "loss": 1.2545,
      "step": 807
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.155876874923706,
      "learning_rate": 0.0008836365284079056,
      "loss": 1.434,
      "step": 808
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 1.0153887271881104,
      "learning_rate": 0.0008815496625450912,
      "loss": 1.3138,
      "step": 809
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.630708634853363,
      "learning_rate": 0.0008794633197446771,
      "loss": 1.458,
      "step": 810
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 1.09669828414917,
      "learning_rate": 0.0008773775092197017,
      "loss": 1.2675,
      "step": 811
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.0127789974212646,
      "learning_rate": 0.0008752922401808523,
      "loss": 1.3657,
      "step": 812
    },
    {
      "epoch": 1.626,
      "grad_norm": 0.9662420153617859,
      "learning_rate": 0.0008732075218364258,
      "loss": 1.2541,
      "step": 813
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.8105437755584717,
      "learning_rate": 0.0008711233633922871,
      "loss": 1.4149,
      "step": 814
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.8095434904098511,
      "learning_rate": 0.0008690397740518279,
      "loss": 1.4131,
      "step": 815
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.0878397226333618,
      "learning_rate": 0.0008669567630159276,
      "loss": 1.3907,
      "step": 816
    },
    {
      "epoch": 1.634,
      "grad_norm": 0.706540584564209,
      "learning_rate": 0.0008648743394829115,
      "loss": 1.305,
      "step": 817
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.874624490737915,
      "learning_rate": 0.0008627925126485108,
      "loss": 1.3637,
      "step": 818
    },
    {
      "epoch": 1.638,
      "grad_norm": 1.2189394235610962,
      "learning_rate": 0.0008607112917058222,
      "loss": 1.3615,
      "step": 819
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.5655207633972168,
      "learning_rate": 0.0008586306858452652,
      "loss": 1.2832,
      "step": 820
    },
    {
      "epoch": 1.642,
      "grad_norm": 0.6573654413223267,
      "learning_rate": 0.0008565507042545451,
      "loss": 1.2938,
      "step": 821
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.6890116930007935,
      "learning_rate": 0.0008544713561186095,
      "loss": 1.3863,
      "step": 822
    },
    {
      "epoch": 1.646,
      "grad_norm": 0.7140735387802124,
      "learning_rate": 0.0008523926506196085,
      "loss": 1.4188,
      "step": 823
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.6521961688995361,
      "learning_rate": 0.0008503145969368561,
      "loss": 1.3513,
      "step": 824
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.7746971249580383,
      "learning_rate": 0.000848237204246785,
      "loss": 1.3303,
      "step": 825
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.7463425397872925,
      "learning_rate": 0.0008461604817229115,
      "loss": 1.3033,
      "step": 826
    },
    {
      "epoch": 1.654,
      "grad_norm": 0.780606746673584,
      "learning_rate": 0.0008440844385357918,
      "loss": 1.3075,
      "step": 827
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.7568067312240601,
      "learning_rate": 0.0008420090838529822,
      "loss": 1.428,
      "step": 828
    },
    {
      "epoch": 1.658,
      "grad_norm": 0.5553720593452454,
      "learning_rate": 0.0008399344268389981,
      "loss": 1.2664,
      "step": 829
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.7965715527534485,
      "learning_rate": 0.0008378604766552756,
      "loss": 1.4849,
      "step": 830
    },
    {
      "epoch": 1.662,
      "grad_norm": 0.8815155029296875,
      "learning_rate": 0.0008357872424601272,
      "loss": 1.3942,
      "step": 831
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.8622299432754517,
      "learning_rate": 0.000833714733408706,
      "loss": 1.4476,
      "step": 832
    },
    {
      "epoch": 1.666,
      "grad_norm": 1.224653959274292,
      "learning_rate": 0.0008316429586529614,
      "loss": 1.347,
      "step": 833
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.6635845899581909,
      "learning_rate": 0.0008295719273416011,
      "loss": 1.2042,
      "step": 834
    },
    {
      "epoch": 1.67,
      "grad_norm": 1.0669357776641846,
      "learning_rate": 0.0008275016486200497,
      "loss": 1.4224,
      "step": 835
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.9831957221031189,
      "learning_rate": 0.0008254321316304075,
      "loss": 1.3628,
      "step": 836
    },
    {
      "epoch": 1.674,
      "grad_norm": 0.8810164928436279,
      "learning_rate": 0.0008233633855114126,
      "loss": 1.3366,
      "step": 837
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.9591448903083801,
      "learning_rate": 0.000821295419398398,
      "loss": 1.3587,
      "step": 838
    },
    {
      "epoch": 1.678,
      "grad_norm": 1.301722764968872,
      "learning_rate": 0.0008192282424232527,
      "loss": 1.384,
      "step": 839
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.0059534311294556,
      "learning_rate": 0.000817161863714381,
      "loss": 1.2518,
      "step": 840
    },
    {
      "epoch": 1.682,
      "grad_norm": 0.8883270025253296,
      "learning_rate": 0.0008150962923966614,
      "loss": 1.3703,
      "step": 841
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.779270589351654,
      "learning_rate": 0.0008130315375914079,
      "loss": 1.1661,
      "step": 842
    },
    {
      "epoch": 1.686,
      "grad_norm": 1.5723451375961304,
      "learning_rate": 0.0008109676084163289,
      "loss": 1.274,
      "step": 843
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.1115530729293823,
      "learning_rate": 0.0008089045139854865,
      "loss": 1.4051,
      "step": 844
    },
    {
      "epoch": 1.69,
      "grad_norm": 1.487997055053711,
      "learning_rate": 0.000806842263409257,
      "loss": 1.6282,
      "step": 845
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.7704240679740906,
      "learning_rate": 0.0008047808657942897,
      "loss": 1.3359,
      "step": 846
    },
    {
      "epoch": 1.694,
      "grad_norm": 0.8952732682228088,
      "learning_rate": 0.0008027203302434679,
      "loss": 1.3409,
      "step": 847
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.237674355506897,
      "learning_rate": 0.0008006606658558684,
      "loss": 1.3672,
      "step": 848
    },
    {
      "epoch": 1.698,
      "grad_norm": 0.8390824198722839,
      "learning_rate": 0.0007986018817267203,
      "loss": 1.3574,
      "step": 849
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.6503231525421143,
      "learning_rate": 0.0007965439869473664,
      "loss": 1.4412,
      "step": 850
    },
    {
      "epoch": 1.702,
      "grad_norm": 1.0847594738006592,
      "learning_rate": 0.0007944869906052211,
      "loss": 1.3912,
      "step": 851
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.8952080011367798,
      "learning_rate": 0.0007924309017837325,
      "loss": 1.3962,
      "step": 852
    },
    {
      "epoch": 1.706,
      "grad_norm": 0.9757710695266724,
      "learning_rate": 0.0007903757295623406,
      "loss": 1.3785,
      "step": 853
    },
    {
      "epoch": 1.708,
      "grad_norm": 1.7027065753936768,
      "learning_rate": 0.0007883214830164383,
      "loss": 1.4927,
      "step": 854
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.850113570690155,
      "learning_rate": 0.0007862681712173304,
      "loss": 1.3763,
      "step": 855
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.7790515422821045,
      "learning_rate": 0.000784215803232194,
      "loss": 1.2991,
      "step": 856
    },
    {
      "epoch": 1.714,
      "grad_norm": 0.8169605731964111,
      "learning_rate": 0.0007821643881240386,
      "loss": 1.2691,
      "step": 857
    },
    {
      "epoch": 1.716,
      "grad_norm": 1.2181730270385742,
      "learning_rate": 0.0007801139349516656,
      "loss": 1.3741,
      "step": 858
    },
    {
      "epoch": 1.718,
      "grad_norm": 0.996147871017456,
      "learning_rate": 0.000778064452769629,
      "loss": 1.3971,
      "step": 859
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.7795325517654419,
      "learning_rate": 0.0007760159506281955,
      "loss": 1.3294,
      "step": 860
    },
    {
      "epoch": 1.722,
      "grad_norm": 0.7389630079269409,
      "learning_rate": 0.0007739684375733022,
      "loss": 1.3196,
      "step": 861
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.884056031703949,
      "learning_rate": 0.0007719219226465208,
      "loss": 1.295,
      "step": 862
    },
    {
      "epoch": 1.726,
      "grad_norm": 0.8399612903594971,
      "learning_rate": 0.0007698764148850137,
      "loss": 1.3094,
      "step": 863
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.8431825041770935,
      "learning_rate": 0.0007678319233214965,
      "loss": 1.3578,
      "step": 864
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.9728361964225769,
      "learning_rate": 0.000765788456984198,
      "loss": 1.2389,
      "step": 865
    },
    {
      "epoch": 1.732,
      "grad_norm": 1.6672265529632568,
      "learning_rate": 0.0007637460248968177,
      "loss": 1.2292,
      "step": 866
    },
    {
      "epoch": 1.734,
      "grad_norm": 1.0032707452774048,
      "learning_rate": 0.0007617046360784905,
      "loss": 1.2686,
      "step": 867
    },
    {
      "epoch": 1.736,
      "grad_norm": 2.4142684936523438,
      "learning_rate": 0.0007596642995437426,
      "loss": 1.2912,
      "step": 868
    },
    {
      "epoch": 1.738,
      "grad_norm": 1.2539838552474976,
      "learning_rate": 0.0007576250243024542,
      "loss": 1.4946,
      "step": 869
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.4700801372528076,
      "learning_rate": 0.0007555868193598187,
      "loss": 1.31,
      "step": 870
    },
    {
      "epoch": 1.742,
      "grad_norm": 2.9111111164093018,
      "learning_rate": 0.0007535496937163031,
      "loss": 1.3829,
      "step": 871
    },
    {
      "epoch": 1.744,
      "grad_norm": 2.5368053913116455,
      "learning_rate": 0.0007515136563676083,
      "loss": 1.4724,
      "step": 872
    },
    {
      "epoch": 1.746,
      "grad_norm": 1.983561396598816,
      "learning_rate": 0.0007494787163046299,
      "loss": 1.359,
      "step": 873
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.4596829414367676,
      "learning_rate": 0.000747444882513418,
      "loss": 1.42,
      "step": 874
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.2161825895309448,
      "learning_rate": 0.0007454121639751371,
      "loss": 1.3272,
      "step": 875
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.9522807598114014,
      "learning_rate": 0.0007433805696660267,
      "loss": 1.3882,
      "step": 876
    },
    {
      "epoch": 1.754,
      "grad_norm": 1.2763506174087524,
      "learning_rate": 0.000741350108557362,
      "loss": 1.3371,
      "step": 877
    },
    {
      "epoch": 1.756,
      "grad_norm": 1.5125024318695068,
      "learning_rate": 0.0007393207896154151,
      "loss": 1.3435,
      "step": 878
    },
    {
      "epoch": 1.758,
      "grad_norm": 0.9390172958374023,
      "learning_rate": 0.000737292621801413,
      "loss": 1.3229,
      "step": 879
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.598934531211853,
      "learning_rate": 0.0007352656140715006,
      "loss": 1.3302,
      "step": 880
    },
    {
      "epoch": 1.762,
      "grad_norm": 1.881211280822754,
      "learning_rate": 0.0007332397753766993,
      "loss": 1.2742,
      "step": 881
    },
    {
      "epoch": 1.764,
      "grad_norm": 1.1944831609725952,
      "learning_rate": 0.000731215114662868,
      "loss": 1.2787,
      "step": 882
    },
    {
      "epoch": 1.766,
      "grad_norm": 0.725105881690979,
      "learning_rate": 0.0007291916408706643,
      "loss": 1.4093,
      "step": 883
    },
    {
      "epoch": 1.768,
      "grad_norm": 2.9108726978302,
      "learning_rate": 0.0007271693629355047,
      "loss": 1.3575,
      "step": 884
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.931761622428894,
      "learning_rate": 0.0007251482897875244,
      "loss": 1.242,
      "step": 885
    },
    {
      "epoch": 1.772,
      "grad_norm": 1.1507843732833862,
      "learning_rate": 0.0007231284303515389,
      "loss": 1.431,
      "step": 886
    },
    {
      "epoch": 1.774,
      "grad_norm": 1.2774746417999268,
      "learning_rate": 0.0007211097935470031,
      "loss": 1.2422,
      "step": 887
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.8730714917182922,
      "learning_rate": 0.0007190923882879742,
      "loss": 1.3481,
      "step": 888
    },
    {
      "epoch": 1.778,
      "grad_norm": 0.7727015018463135,
      "learning_rate": 0.0007170762234830699,
      "loss": 1.3902,
      "step": 889
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.6107262969017029,
      "learning_rate": 0.0007150613080354315,
      "loss": 1.2238,
      "step": 890
    },
    {
      "epoch": 1.782,
      "grad_norm": 0.8112109303474426,
      "learning_rate": 0.0007130476508426822,
      "loss": 1.2232,
      "step": 891
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.118578553199768,
      "learning_rate": 0.0007110352607968889,
      "loss": 1.3424,
      "step": 892
    },
    {
      "epoch": 1.786,
      "grad_norm": 1.05631422996521,
      "learning_rate": 0.0007090241467845237,
      "loss": 1.5517,
      "step": 893
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.9813739657402039,
      "learning_rate": 0.0007070143176864231,
      "loss": 1.4033,
      "step": 894
    },
    {
      "epoch": 1.79,
      "grad_norm": 1.1957199573516846,
      "learning_rate": 0.0007050057823777502,
      "loss": 1.4746,
      "step": 895
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.3153221607208252,
      "learning_rate": 0.0007029985497279549,
      "loss": 1.3135,
      "step": 896
    },
    {
      "epoch": 1.794,
      "grad_norm": 1.3569353818893433,
      "learning_rate": 0.000700992628600734,
      "loss": 1.4059,
      "step": 897
    },
    {
      "epoch": 1.796,
      "grad_norm": 1.11194908618927,
      "learning_rate": 0.0006989880278539931,
      "loss": 1.2228,
      "step": 898
    },
    {
      "epoch": 1.798,
      "grad_norm": 1.030229926109314,
      "learning_rate": 0.0006969847563398075,
      "loss": 1.3092,
      "step": 899
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.8848022222518921,
      "learning_rate": 0.0006949828229043824,
      "loss": 1.4191,
      "step": 900
    },
    {
      "epoch": 1.8,
      "eval_loss": 1.3276846408843994,
      "eval_runtime": 228.9366,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 900
    },
    {
      "epoch": 1.802,
      "grad_norm": 0.9436598420143127,
      "learning_rate": 0.0006929822363880149,
      "loss": 1.2792,
      "step": 901
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.2280137538909912,
      "learning_rate": 0.0006909830056250527,
      "loss": 1.3061,
      "step": 902
    },
    {
      "epoch": 1.806,
      "grad_norm": 1.796993374824524,
      "learning_rate": 0.0006889851394438584,
      "loss": 1.3197,
      "step": 903
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.8879406452178955,
      "learning_rate": 0.0006869886466667679,
      "loss": 1.392,
      "step": 904
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.4102129936218262,
      "learning_rate": 0.0006849935361100521,
      "loss": 1.4399,
      "step": 905
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.4425201416015625,
      "learning_rate": 0.0006829998165838793,
      "loss": 1.4412,
      "step": 906
    },
    {
      "epoch": 1.814,
      "grad_norm": 1.2780940532684326,
      "learning_rate": 0.0006810074968922736,
      "loss": 1.3747,
      "step": 907
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 6.555848121643066,
      "learning_rate": 0.0006790165858330788,
      "loss": 1.3669,
      "step": 908
    },
    {
      "epoch": 1.818,
      "grad_norm": 1.1609936952590942,
      "learning_rate": 0.0006770270921979179,
      "loss": 1.3253,
      "step": 909
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 3.1202189922332764,
      "learning_rate": 0.0006750390247721548,
      "loss": 1.3365,
      "step": 910
    },
    {
      "epoch": 1.822,
      "grad_norm": 1.0136892795562744,
      "learning_rate": 0.0006730523923348556,
      "loss": 1.5277,
      "step": 911
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.246198058128357,
      "learning_rate": 0.0006710672036587491,
      "loss": 1.3412,
      "step": 912
    },
    {
      "epoch": 1.826,
      "grad_norm": 1.0435909032821655,
      "learning_rate": 0.0006690834675101889,
      "loss": 1.3737,
      "step": 913
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.7781681418418884,
      "learning_rate": 0.0006671011926491151,
      "loss": 1.3023,
      "step": 914
    },
    {
      "epoch": 1.83,
      "grad_norm": 1.7740411758422852,
      "learning_rate": 0.0006651203878290139,
      "loss": 1.4406,
      "step": 915
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.7292230129241943,
      "learning_rate": 0.0006631410617968807,
      "loss": 1.3168,
      "step": 916
    },
    {
      "epoch": 1.834,
      "grad_norm": 1.0872355699539185,
      "learning_rate": 0.0006611632232931804,
      "loss": 1.2335,
      "step": 917
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.7062076926231384,
      "learning_rate": 0.000659186881051809,
      "loss": 1.2892,
      "step": 918
    },
    {
      "epoch": 1.838,
      "grad_norm": 0.7790378928184509,
      "learning_rate": 0.0006572120438000553,
      "loss": 1.2587,
      "step": 919
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.9674646258354187,
      "learning_rate": 0.0006552387202585629,
      "loss": 1.353,
      "step": 920
    },
    {
      "epoch": 1.842,
      "grad_norm": 1.7633609771728516,
      "learning_rate": 0.0006532669191412905,
      "loss": 1.2777,
      "step": 921
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.206470012664795,
      "learning_rate": 0.0006512966491554735,
      "loss": 1.4467,
      "step": 922
    },
    {
      "epoch": 1.846,
      "grad_norm": 0.6656424403190613,
      "learning_rate": 0.0006493279190015865,
      "loss": 1.4155,
      "step": 923
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 1.0454294681549072,
      "learning_rate": 0.0006473607373733044,
      "loss": 1.3616,
      "step": 924
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.8079620599746704,
      "learning_rate": 0.0006453951129574643,
      "loss": 1.3604,
      "step": 925
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.9291304349899292,
      "learning_rate": 0.0006434310544340265,
      "loss": 1.3812,
      "step": 926
    },
    {
      "epoch": 1.854,
      "grad_norm": 1.4500478506088257,
      "learning_rate": 0.000641468570476036,
      "loss": 1.3826,
      "step": 927
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.0876476764678955,
      "learning_rate": 0.0006395076697495854,
      "loss": 1.2446,
      "step": 928
    },
    {
      "epoch": 1.858,
      "grad_norm": 1.0329389572143555,
      "learning_rate": 0.000637548360913776,
      "loss": 1.3059,
      "step": 929
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.6684793829917908,
      "learning_rate": 0.0006355906526206787,
      "loss": 1.2859,
      "step": 930
    },
    {
      "epoch": 1.862,
      "grad_norm": 0.8531168699264526,
      "learning_rate": 0.0006336345535152976,
      "loss": 1.3027,
      "step": 931
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.7595357298851013,
      "learning_rate": 0.0006316800722355307,
      "loss": 1.3367,
      "step": 932
    },
    {
      "epoch": 1.866,
      "grad_norm": 0.739008367061615,
      "learning_rate": 0.0006297272174121309,
      "loss": 1.2969,
      "step": 933
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.7292867302894592,
      "learning_rate": 0.0006277759976686697,
      "loss": 1.2914,
      "step": 934
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.7147024869918823,
      "learning_rate": 0.0006258264216214977,
      "loss": 1.3277,
      "step": 935
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1.1261142492294312,
      "learning_rate": 0.0006238784978797084,
      "loss": 1.2831,
      "step": 936
    },
    {
      "epoch": 1.874,
      "grad_norm": 0.678737998008728,
      "learning_rate": 0.0006219322350450978,
      "loss": 1.237,
      "step": 937
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.8743345141410828,
      "learning_rate": 0.0006199876417121272,
      "loss": 1.2751,
      "step": 938
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 1.2182174921035767,
      "learning_rate": 0.0006180447264678868,
      "loss": 1.3753,
      "step": 939
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.374388337135315,
      "learning_rate": 0.0006161034978920554,
      "loss": 1.3174,
      "step": 940
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 1.2746751308441162,
      "learning_rate": 0.0006141639645568645,
      "loss": 1.3387,
      "step": 941
    },
    {
      "epoch": 1.884,
      "grad_norm": 1.9022868871688843,
      "learning_rate": 0.0006122261350270598,
      "loss": 1.4045,
      "step": 942
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 1.1478205919265747,
      "learning_rate": 0.0006102900178598616,
      "loss": 1.4117,
      "step": 943
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.0603108406066895,
      "learning_rate": 0.0006083556216049306,
      "loss": 1.3974,
      "step": 944
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 1.196036696434021,
      "learning_rate": 0.0006064229548043272,
      "loss": 1.3568,
      "step": 945
    },
    {
      "epoch": 1.892,
      "grad_norm": 3.8378682136535645,
      "learning_rate": 0.0006044920259924747,
      "loss": 1.3475,
      "step": 946
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 0.8932080268859863,
      "learning_rate": 0.0006025628436961218,
      "loss": 1.3069,
      "step": 947
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.7816734910011292,
      "learning_rate": 0.0006006354164343047,
      "loss": 1.2726,
      "step": 948
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 1.0551565885543823,
      "learning_rate": 0.0005987097527183096,
      "loss": 1.234,
      "step": 949
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.8642141819000244,
      "learning_rate": 0.0005967858610516353,
      "loss": 1.2678,
      "step": 950
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 0.7811188101768494,
      "learning_rate": 0.0005948637499299554,
      "loss": 1.3113,
      "step": 951
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.2245465517044067,
      "learning_rate": 0.000592943427841081,
      "loss": 1.5042,
      "step": 952
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 1.601223349571228,
      "learning_rate": 0.000591024903264922,
      "loss": 1.2924,
      "step": 953
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.655718207359314,
      "learning_rate": 0.0005891081846734518,
      "loss": 1.2352,
      "step": 954
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.6960938572883606,
      "learning_rate": 0.0005871932805306688,
      "loss": 1.3543,
      "step": 955
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.2658891677856445,
      "learning_rate": 0.0005852801992925585,
      "loss": 1.3392,
      "step": 956
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 0.9146814346313477,
      "learning_rate": 0.0005833689494070569,
      "loss": 1.2642,
      "step": 957
    },
    {
      "epoch": 1.916,
      "grad_norm": 1.0535380840301514,
      "learning_rate": 0.0005814595393140126,
      "loss": 1.2815,
      "step": 958
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 0.5757153630256653,
      "learning_rate": 0.0005795519774451505,
      "loss": 1.2035,
      "step": 959
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.7727646827697754,
      "learning_rate": 0.0005776462722240337,
      "loss": 1.2489,
      "step": 960
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 0.6635884046554565,
      "learning_rate": 0.0005757424320660264,
      "loss": 1.1938,
      "step": 961
    },
    {
      "epoch": 1.924,
      "grad_norm": 1.0937458276748657,
      "learning_rate": 0.0005738404653782571,
      "loss": 1.329,
      "step": 962
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 1.3761671781539917,
      "learning_rate": 0.0005719403805595815,
      "loss": 1.2351,
      "step": 963
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.1333712339401245,
      "learning_rate": 0.0005700421860005447,
      "loss": 1.358,
      "step": 964
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 1.2402344942092896,
      "learning_rate": 0.0005681458900833447,
      "loss": 1.2129,
      "step": 965
    },
    {
      "epoch": 1.932,
      "grad_norm": 1.0103952884674072,
      "learning_rate": 0.0005662515011817959,
      "loss": 1.2593,
      "step": 966
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 1.0767766237258911,
      "learning_rate": 0.0005643590276612909,
      "loss": 1.2415,
      "step": 967
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.0340982675552368,
      "learning_rate": 0.0005624684778787646,
      "loss": 1.2964,
      "step": 968
    },
    {
      "epoch": 1.938,
      "grad_norm": 0.7146506309509277,
      "learning_rate": 0.0005605798601826566,
      "loss": 1.2694,
      "step": 969
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.8860993385314941,
      "learning_rate": 0.0005586931829128749,
      "loss": 1.2318,
      "step": 970
    },
    {
      "epoch": 1.942,
      "grad_norm": 1.193990707397461,
      "learning_rate": 0.0005568084544007588,
      "loss": 1.3248,
      "step": 971
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.2207388877868652,
      "learning_rate": 0.0005549256829690418,
      "loss": 1.2085,
      "step": 972
    },
    {
      "epoch": 1.946,
      "grad_norm": 0.8959752917289734,
      "learning_rate": 0.0005530448769318157,
      "loss": 1.2504,
      "step": 973
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.9239142537117004,
      "learning_rate": 0.0005511660445944929,
      "loss": 1.3325,
      "step": 974
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.2312010526657104,
      "learning_rate": 0.0005492891942537703,
      "loss": 1.2116,
      "step": 975
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.001709222793579,
      "learning_rate": 0.0005474143341975928,
      "loss": 1.1787,
      "step": 976
    },
    {
      "epoch": 1.954,
      "grad_norm": 0.6679803133010864,
      "learning_rate": 0.0005455414727051159,
      "loss": 1.2397,
      "step": 977
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.9275016784667969,
      "learning_rate": 0.0005436706180466702,
      "loss": 1.3225,
      "step": 978
    },
    {
      "epoch": 1.958,
      "grad_norm": 0.7664514183998108,
      "learning_rate": 0.0005418017784837243,
      "loss": 1.3626,
      "step": 979
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.8553375005722046,
      "learning_rate": 0.0005399349622688479,
      "loss": 1.2627,
      "step": 980
    },
    {
      "epoch": 1.962,
      "grad_norm": 1.1441752910614014,
      "learning_rate": 0.0005380701776456766,
      "loss": 1.2463,
      "step": 981
    },
    {
      "epoch": 1.964,
      "grad_norm": 1.3491289615631104,
      "learning_rate": 0.000536207432848874,
      "loss": 1.3563,
      "step": 982
    },
    {
      "epoch": 1.966,
      "grad_norm": 1.1952950954437256,
      "learning_rate": 0.0005343467361040966,
      "loss": 1.3492,
      "step": 983
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.8893725275993347,
      "learning_rate": 0.0005324880956279567,
      "loss": 1.2664,
      "step": 984
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.9221228361129761,
      "learning_rate": 0.0005306315196279864,
      "loss": 1.2169,
      "step": 985
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.9341763854026794,
      "learning_rate": 0.0005287770163026013,
      "loss": 1.3376,
      "step": 986
    },
    {
      "epoch": 1.974,
      "grad_norm": 0.7157062888145447,
      "learning_rate": 0.0005269245938410646,
      "loss": 1.2016,
      "step": 987
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.1518175601959229,
      "learning_rate": 0.00052507426042345,
      "loss": 1.2765,
      "step": 988
    },
    {
      "epoch": 1.978,
      "grad_norm": 1.3914059400558472,
      "learning_rate": 0.0005232260242206071,
      "loss": 1.2225,
      "step": 989
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.4036592245101929,
      "learning_rate": 0.0005213798933941236,
      "loss": 1.255,
      "step": 990
    },
    {
      "epoch": 1.982,
      "grad_norm": 0.9078657031059265,
      "learning_rate": 0.0005195358760962907,
      "loss": 1.258,
      "step": 991
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.0889135599136353,
      "learning_rate": 0.0005176939804700664,
      "loss": 1.2556,
      "step": 992
    },
    {
      "epoch": 1.986,
      "grad_norm": 0.593640148639679,
      "learning_rate": 0.0005158542146490399,
      "loss": 1.2488,
      "step": 993
    },
    {
      "epoch": 1.988,
      "grad_norm": 1.2038726806640625,
      "learning_rate": 0.0005140165867573939,
      "loss": 1.4031,
      "step": 994
    },
    {
      "epoch": 1.99,
      "grad_norm": 1.2918527126312256,
      "learning_rate": 0.0005121811049098728,
      "loss": 1.3647,
      "step": 995
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.6026954054832458,
      "learning_rate": 0.0005103477772117424,
      "loss": 1.2078,
      "step": 996
    },
    {
      "epoch": 1.994,
      "grad_norm": 0.7251142859458923,
      "learning_rate": 0.0005085166117587567,
      "loss": 1.2726,
      "step": 997
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.4736086130142212,
      "learning_rate": 0.0005066876166371219,
      "loss": 1.2976,
      "step": 998
    },
    {
      "epoch": 1.998,
      "grad_norm": 0.6697493195533752,
      "learning_rate": 0.0005048607999234587,
      "loss": 1.2542,
      "step": 999
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1312150955200195,
      "learning_rate": 0.0005030361696847705,
      "loss": 1.281,
      "step": 1000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2809292078018188,
      "eval_runtime": 228.91,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1000
    },
    {
      "epoch": 2.002,
      "grad_norm": 1.260601282119751,
      "learning_rate": 0.0005012137339784037,
      "loss": 1.3179,
      "step": 1001
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.6387666463851929,
      "learning_rate": 0.0004993935008520146,
      "loss": 1.3127,
      "step": 1002
    },
    {
      "epoch": 2.006,
      "grad_norm": 0.7132006883621216,
      "learning_rate": 0.0004975754783435336,
      "loss": 1.2519,
      "step": 1003
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.8542735576629639,
      "learning_rate": 0.0004957596744811279,
      "loss": 1.2733,
      "step": 1004
    },
    {
      "epoch": 2.01,
      "grad_norm": 2.048156261444092,
      "learning_rate": 0.0004939460972831684,
      "loss": 1.2338,
      "step": 1005
    },
    {
      "epoch": 2.012,
      "grad_norm": 1.0398590564727783,
      "learning_rate": 0.000492134754758194,
      "loss": 1.2664,
      "step": 1006
    },
    {
      "epoch": 2.014,
      "grad_norm": 1.0643529891967773,
      "learning_rate": 0.0004903256549048742,
      "loss": 1.3588,
      "step": 1007
    },
    {
      "epoch": 2.016,
      "grad_norm": 1.962151288986206,
      "learning_rate": 0.0004885188057119762,
      "loss": 1.3598,
      "step": 1008
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.764815092086792,
      "learning_rate": 0.00048671421515832726,
      "loss": 1.3851,
      "step": 1009
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.5039572715759277,
      "learning_rate": 0.00048491189121278167,
      "loss": 1.2565,
      "step": 1010
    },
    {
      "epoch": 2.022,
      "grad_norm": 0.8041626214981079,
      "learning_rate": 0.0004831118418341852,
      "loss": 1.3229,
      "step": 1011
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.8319966197013855,
      "learning_rate": 0.0004813140749713384,
      "loss": 1.0954,
      "step": 1012
    },
    {
      "epoch": 2.026,
      "grad_norm": 1.1450577974319458,
      "learning_rate": 0.0004795185985629632,
      "loss": 1.2874,
      "step": 1013
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.6299481987953186,
      "learning_rate": 0.0004777254205376662,
      "loss": 1.2191,
      "step": 1014
    },
    {
      "epoch": 2.03,
      "grad_norm": 1.7483670711517334,
      "learning_rate": 0.0004759345488139054,
      "loss": 1.3568,
      "step": 1015
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.8059095144271851,
      "learning_rate": 0.00047414599129995405,
      "loss": 1.2062,
      "step": 1016
    },
    {
      "epoch": 2.034,
      "grad_norm": 2.583371162414551,
      "learning_rate": 0.00047235975589386713,
      "loss": 1.246,
      "step": 1017
    },
    {
      "epoch": 2.036,
      "grad_norm": 1.5294904708862305,
      "learning_rate": 0.00047057585048344467,
      "loss": 1.1983,
      "step": 1018
    },
    {
      "epoch": 2.038,
      "grad_norm": 1.1150835752487183,
      "learning_rate": 0.0004687942829461969,
      "loss": 1.2,
      "step": 1019
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.95988529920578,
      "learning_rate": 0.00046701506114931157,
      "loss": 1.2262,
      "step": 1020
    },
    {
      "epoch": 2.042,
      "grad_norm": 0.7685225605964661,
      "learning_rate": 0.0004652381929496172,
      "loss": 1.1958,
      "step": 1021
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.9924214482307434,
      "learning_rate": 0.00046346368619355007,
      "loss": 1.2098,
      "step": 1022
    },
    {
      "epoch": 2.046,
      "grad_norm": 0.4949361979961395,
      "learning_rate": 0.00046169154871711804,
      "loss": 1.2011,
      "step": 1023
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.6212730407714844,
      "learning_rate": 0.0004599217883458655,
      "loss": 1.2585,
      "step": 1024
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.6872054934501648,
      "learning_rate": 0.00045815441289484126,
      "loss": 1.2574,
      "step": 1025
    },
    {
      "epoch": 2.052,
      "grad_norm": 1.3634370565414429,
      "learning_rate": 0.00045638943016856204,
      "loss": 1.239,
      "step": 1026
    },
    {
      "epoch": 2.054,
      "grad_norm": 1.48817777633667,
      "learning_rate": 0.0004546268479609783,
      "loss": 1.279,
      "step": 1027
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.0784010887145996,
      "learning_rate": 0.00045286667405544114,
      "loss": 1.2757,
      "step": 1028
    },
    {
      "epoch": 2.058,
      "grad_norm": 1.4315115213394165,
      "learning_rate": 0.0004511089162246661,
      "loss": 1.2706,
      "step": 1029
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.48414483666419983,
      "learning_rate": 0.00044935358223069925,
      "loss": 1.1502,
      "step": 1030
    },
    {
      "epoch": 2.062,
      "grad_norm": 0.6347913146018982,
      "learning_rate": 0.0004476006798248837,
      "loss": 1.2065,
      "step": 1031
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.2326308488845825,
      "learning_rate": 0.00044585021674782533,
      "loss": 1.1202,
      "step": 1032
    },
    {
      "epoch": 2.066,
      "grad_norm": 2.6398048400878906,
      "learning_rate": 0.0004441022007293575,
      "loss": 1.1204,
      "step": 1033
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.9222638010978699,
      "learning_rate": 0.0004423566394885091,
      "loss": 1.2411,
      "step": 1034
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.82872074842453,
      "learning_rate": 0.0004406135407334668,
      "loss": 1.2404,
      "step": 1035
    },
    {
      "epoch": 2.072,
      "grad_norm": 1.5441759824752808,
      "learning_rate": 0.000438872912161545,
      "loss": 1.3331,
      "step": 1036
    },
    {
      "epoch": 2.074,
      "grad_norm": 0.9413723945617676,
      "learning_rate": 0.0004371347614591493,
      "loss": 1.2449,
      "step": 1037
    },
    {
      "epoch": 2.076,
      "grad_norm": 1.0385771989822388,
      "learning_rate": 0.0004353990963017433,
      "loss": 1.243,
      "step": 1038
    },
    {
      "epoch": 2.078,
      "grad_norm": 0.767503023147583,
      "learning_rate": 0.0004336659243538159,
      "loss": 1.1864,
      "step": 1039
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.1973607540130615,
      "learning_rate": 0.0004319352532688443,
      "loss": 1.3591,
      "step": 1040
    },
    {
      "epoch": 2.082,
      "grad_norm": 1.0760889053344727,
      "learning_rate": 0.00043020709068926366,
      "loss": 1.1311,
      "step": 1041
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.7123854756355286,
      "learning_rate": 0.00042848144424643134,
      "loss": 1.3394,
      "step": 1042
    },
    {
      "epoch": 2.086,
      "grad_norm": 0.8518358469009399,
      "learning_rate": 0.0004267583215605939,
      "loss": 1.2188,
      "step": 1043
    },
    {
      "epoch": 2.088,
      "grad_norm": 1.6319721937179565,
      "learning_rate": 0.0004250377302408531,
      "loss": 1.3699,
      "step": 1044
    },
    {
      "epoch": 2.09,
      "grad_norm": 32.976619720458984,
      "learning_rate": 0.0004233196778851329,
      "loss": 1.2541,
      "step": 1045
    },
    {
      "epoch": 2.092,
      "grad_norm": 1.1725808382034302,
      "learning_rate": 0.0004216041720801451,
      "loss": 1.189,
      "step": 1046
    },
    {
      "epoch": 2.094,
      "grad_norm": 0.895893931388855,
      "learning_rate": 0.00041989122040135654,
      "loss": 1.2404,
      "step": 1047
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.6142410039901733,
      "learning_rate": 0.00041818083041295486,
      "loss": 1.2162,
      "step": 1048
    },
    {
      "epoch": 2.098,
      "grad_norm": 0.7102110981941223,
      "learning_rate": 0.0004164730096678161,
      "loss": 1.2413,
      "step": 1049
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.7504299283027649,
      "learning_rate": 0.00041476776570747065,
      "loss": 1.2609,
      "step": 1050
    },
    {
      "epoch": 2.102,
      "grad_norm": 0.7273263931274414,
      "learning_rate": 0.00041306510606207005,
      "loss": 1.2355,
      "step": 1051
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.358378529548645,
      "learning_rate": 0.00041136503825035396,
      "loss": 1.3182,
      "step": 1052
    },
    {
      "epoch": 2.106,
      "grad_norm": 0.9387801289558411,
      "learning_rate": 0.00040966756977961683,
      "loss": 1.2397,
      "step": 1053
    },
    {
      "epoch": 2.108,
      "grad_norm": 1.3348349332809448,
      "learning_rate": 0.00040797270814567447,
      "loss": 1.3666,
      "step": 1054
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.8824655413627625,
      "learning_rate": 0.00040628046083283133,
      "loss": 1.2033,
      "step": 1055
    },
    {
      "epoch": 2.112,
      "grad_norm": 357.9854431152344,
      "learning_rate": 0.0004045908353138477,
      "loss": 1.2353,
      "step": 1056
    },
    {
      "epoch": 2.114,
      "grad_norm": 0.8159214854240417,
      "learning_rate": 0.0004029038390499057,
      "loss": 1.3021,
      "step": 1057
    },
    {
      "epoch": 2.116,
      "grad_norm": 1.2118078470230103,
      "learning_rate": 0.0004012194794905775,
      "loss": 1.316,
      "step": 1058
    },
    {
      "epoch": 2.118,
      "grad_norm": 1.910038709640503,
      "learning_rate": 0.0003995377640737917,
      "loss": 1.2416,
      "step": 1059
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.7945699095726013,
      "learning_rate": 0.00039785870022580075,
      "loss": 1.1367,
      "step": 1060
    },
    {
      "epoch": 2.122,
      "grad_norm": 1.9691715240478516,
      "learning_rate": 0.0003961822953611478,
      "loss": 1.3261,
      "step": 1061
    },
    {
      "epoch": 2.124,
      "grad_norm": 2.208477258682251,
      "learning_rate": 0.00039450855688263485,
      "loss": 1.1206,
      "step": 1062
    },
    {
      "epoch": 2.126,
      "grad_norm": 1.433426856994629,
      "learning_rate": 0.00039283749218128883,
      "loss": 1.2391,
      "step": 1063
    },
    {
      "epoch": 2.128,
      "grad_norm": 1.8800711631774902,
      "learning_rate": 0.00039116910863633037,
      "loss": 1.3815,
      "step": 1064
    },
    {
      "epoch": 2.13,
      "grad_norm": 1.4295191764831543,
      "learning_rate": 0.00038950341361513875,
      "loss": 1.4407,
      "step": 1065
    },
    {
      "epoch": 2.132,
      "grad_norm": 3.106860876083374,
      "learning_rate": 0.0003878404144732234,
      "loss": 1.2233,
      "step": 1066
    },
    {
      "epoch": 2.134,
      "grad_norm": 0.6160525679588318,
      "learning_rate": 0.00038618011855418743,
      "loss": 1.3117,
      "step": 1067
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.9381053447723389,
      "learning_rate": 0.0003845225331896974,
      "loss": 1.2647,
      "step": 1068
    },
    {
      "epoch": 2.138,
      "grad_norm": 0.8617138862609863,
      "learning_rate": 0.00038286766569945076,
      "loss": 1.1629,
      "step": 1069
    },
    {
      "epoch": 2.14,
      "grad_norm": 2.7235515117645264,
      "learning_rate": 0.00038121552339114164,
      "loss": 1.2817,
      "step": 1070
    },
    {
      "epoch": 2.142,
      "grad_norm": 5.825747966766357,
      "learning_rate": 0.0003795661135604319,
      "loss": 1.2191,
      "step": 1071
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.6456683874130249,
      "learning_rate": 0.00037791944349091646,
      "loss": 1.2362,
      "step": 1072
    },
    {
      "epoch": 2.146,
      "grad_norm": 0.8307990431785583,
      "learning_rate": 0.0003762755204540914,
      "loss": 1.1669,
      "step": 1073
    },
    {
      "epoch": 2.148,
      "grad_norm": 1.1561943292617798,
      "learning_rate": 0.0003746343517093229,
      "loss": 1.248,
      "step": 1074
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.7672144174575806,
      "learning_rate": 0.0003729959445038136,
      "loss": 1.1625,
      "step": 1075
    },
    {
      "epoch": 2.152,
      "grad_norm": 1.4660394191741943,
      "learning_rate": 0.00037136030607257196,
      "loss": 1.1508,
      "step": 1076
    },
    {
      "epoch": 2.154,
      "grad_norm": 0.900613009929657,
      "learning_rate": 0.00036972744363838073,
      "loss": 1.3045,
      "step": 1077
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.9442479610443115,
      "learning_rate": 0.000368097364411763,
      "loss": 1.1733,
      "step": 1078
    },
    {
      "epoch": 2.158,
      "grad_norm": 0.7328196167945862,
      "learning_rate": 0.00036647007559095204,
      "loss": 1.1997,
      "step": 1079
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.6013145446777344,
      "learning_rate": 0.00036484558436185934,
      "loss": 1.1934,
      "step": 1080
    },
    {
      "epoch": 2.162,
      "grad_norm": 0.6774169206619263,
      "learning_rate": 0.000363223897898041,
      "loss": 1.1324,
      "step": 1081
    },
    {
      "epoch": 2.164,
      "grad_norm": 1.394971489906311,
      "learning_rate": 0.00036160502336067004,
      "loss": 1.5174,
      "step": 1082
    },
    {
      "epoch": 2.166,
      "grad_norm": 1.6546937227249146,
      "learning_rate": 0.00035998896789850066,
      "loss": 1.1927,
      "step": 1083
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.968611478805542,
      "learning_rate": 0.0003583757386478389,
      "loss": 1.2004,
      "step": 1084
    },
    {
      "epoch": 2.17,
      "grad_norm": 3.8247435092926025,
      "learning_rate": 0.0003567653427325107,
      "loss": 1.2455,
      "step": 1085
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.6899540424346924,
      "learning_rate": 0.00035515778726382964,
      "loss": 1.2122,
      "step": 1086
    },
    {
      "epoch": 2.174,
      "grad_norm": 0.870061993598938,
      "learning_rate": 0.00035355307934056666,
      "loss": 1.3438,
      "step": 1087
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.6223017573356628,
      "learning_rate": 0.00035195122604891904,
      "loss": 1.268,
      "step": 1088
    },
    {
      "epoch": 2.178,
      "grad_norm": 0.6044483184814453,
      "learning_rate": 0.00035035223446247733,
      "loss": 1.2567,
      "step": 1089
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.0229498147964478,
      "learning_rate": 0.0003487561116421958,
      "loss": 1.1847,
      "step": 1090
    },
    {
      "epoch": 2.182,
      "grad_norm": 0.7255319952964783,
      "learning_rate": 0.0003471628646363597,
      "loss": 1.2119,
      "step": 1091
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.9413844347000122,
      "learning_rate": 0.00034557250048055575,
      "loss": 1.2227,
      "step": 1092
    },
    {
      "epoch": 2.186,
      "grad_norm": 1.0329883098602295,
      "learning_rate": 0.00034398502619764,
      "loss": 1.1254,
      "step": 1093
    },
    {
      "epoch": 2.188,
      "grad_norm": 1.2365890741348267,
      "learning_rate": 0.00034240044879770806,
      "loss": 1.1678,
      "step": 1094
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9060811400413513,
      "learning_rate": 0.0003408187752780624,
      "loss": 1.3044,
      "step": 1095
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.8601967692375183,
      "learning_rate": 0.00033924001262318205,
      "loss": 1.2258,
      "step": 1096
    },
    {
      "epoch": 2.194,
      "grad_norm": 0.9049960970878601,
      "learning_rate": 0.00033766416780469256,
      "loss": 1.2378,
      "step": 1097
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.8570452332496643,
      "learning_rate": 0.00033609124778133425,
      "loss": 1.246,
      "step": 1098
    },
    {
      "epoch": 2.198,
      "grad_norm": 0.8404725193977356,
      "learning_rate": 0.000334521259498933,
      "loss": 1.1667,
      "step": 1099
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.7540146112442017,
      "learning_rate": 0.0003329542098903674,
      "loss": 1.2456,
      "step": 1100
    },
    {
      "epoch": 2.2,
      "eval_loss": 1.223738431930542,
      "eval_runtime": 228.9309,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1100
    },
    {
      "epoch": 2.202,
      "grad_norm": 0.8972026705741882,
      "learning_rate": 0.00033139010587553906,
      "loss": 1.1779,
      "step": 1101
    },
    {
      "epoch": 2.204,
      "grad_norm": 1.1864666938781738,
      "learning_rate": 0.0003298289543613429,
      "loss": 1.2101,
      "step": 1102
    },
    {
      "epoch": 2.206,
      "grad_norm": 1.7911183834075928,
      "learning_rate": 0.00032827076224163556,
      "loss": 1.2475,
      "step": 1103
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.6897879242897034,
      "learning_rate": 0.0003267155363972052,
      "loss": 1.2554,
      "step": 1104
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.0919909477233887,
      "learning_rate": 0.00032516328369574247,
      "loss": 1.1958,
      "step": 1105
    },
    {
      "epoch": 2.212,
      "grad_norm": 1.0828170776367188,
      "learning_rate": 0.0003236140109918071,
      "loss": 1.2156,
      "step": 1106
    },
    {
      "epoch": 2.214,
      "grad_norm": 0.8996362686157227,
      "learning_rate": 0.0003220677251268008,
      "loss": 1.2282,
      "step": 1107
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.7800846099853516,
      "learning_rate": 0.0003205244329289354,
      "loss": 1.2108,
      "step": 1108
    },
    {
      "epoch": 2.218,
      "grad_norm": 1.8131059408187866,
      "learning_rate": 0.0003189841412132027,
      "loss": 1.3953,
      "step": 1109
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.8126987218856812,
      "learning_rate": 0.0003174468567813461,
      "loss": 1.181,
      "step": 1110
    },
    {
      "epoch": 2.222,
      "grad_norm": 0.8871451020240784,
      "learning_rate": 0.0003159125864218272,
      "loss": 1.1773,
      "step": 1111
    },
    {
      "epoch": 2.224,
      "grad_norm": 2.5925538539886475,
      "learning_rate": 0.0003143813369097991,
      "loss": 1.1603,
      "step": 1112
    },
    {
      "epoch": 2.226,
      "grad_norm": 1.192246437072754,
      "learning_rate": 0.0003128531150070749,
      "loss": 1.1741,
      "step": 1113
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.990601122379303,
      "learning_rate": 0.00031132792746209836,
      "loss": 1.2121,
      "step": 1114
    },
    {
      "epoch": 2.23,
      "grad_norm": 1.4492919445037842,
      "learning_rate": 0.0003098057810099135,
      "loss": 1.3008,
      "step": 1115
    },
    {
      "epoch": 2.232,
      "grad_norm": 1.7242426872253418,
      "learning_rate": 0.00030828668237213553,
      "loss": 1.2176,
      "step": 1116
    },
    {
      "epoch": 2.234,
      "grad_norm": 1.2671180963516235,
      "learning_rate": 0.00030677063825692067,
      "loss": 1.1601,
      "step": 1117
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.9625851511955261,
      "learning_rate": 0.0003052576553589368,
      "loss": 1.2086,
      "step": 1118
    },
    {
      "epoch": 2.238,
      "grad_norm": 0.7944760918617249,
      "learning_rate": 0.00030374774035933405,
      "loss": 1.1492,
      "step": 1119
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.1856731176376343,
      "learning_rate": 0.0003022408999257148,
      "loss": 1.1949,
      "step": 1120
    },
    {
      "epoch": 2.242,
      "grad_norm": 1.2841112613677979,
      "learning_rate": 0.00030073714071210454,
      "loss": 1.1844,
      "step": 1121
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2.0286865234375,
      "learning_rate": 0.0002992364693589228,
      "loss": 1.2361,
      "step": 1122
    },
    {
      "epoch": 2.246,
      "grad_norm": 0.8142291903495789,
      "learning_rate": 0.00029773889249295294,
      "loss": 1.2189,
      "step": 1123
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.08571457862854,
      "learning_rate": 0.0002962444167273138,
      "loss": 1.1834,
      "step": 1124
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.0786292552947998,
      "learning_rate": 0.0002947530486614303,
      "loss": 1.3278,
      "step": 1125
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.8638723492622375,
      "learning_rate": 0.0002932647948810037,
      "loss": 1.31,
      "step": 1126
    },
    {
      "epoch": 2.254,
      "grad_norm": 1.383068323135376,
      "learning_rate": 0.0002917796619579831,
      "loss": 1.1838,
      "step": 1127
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.8946739435195923,
      "learning_rate": 0.0002902976564505365,
      "loss": 1.0781,
      "step": 1128
    },
    {
      "epoch": 2.258,
      "grad_norm": 1.1314265727996826,
      "learning_rate": 0.00028881878490302125,
      "loss": 1.1956,
      "step": 1129
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.7673801183700562,
      "learning_rate": 0.0002873430538459559,
      "loss": 1.2525,
      "step": 1130
    },
    {
      "epoch": 2.262,
      "grad_norm": 2.440962314605713,
      "learning_rate": 0.00028587046979599066,
      "loss": 1.0903,
      "step": 1131
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.3382593393325806,
      "learning_rate": 0.00028440103925587903,
      "loss": 1.2138,
      "step": 1132
    },
    {
      "epoch": 2.266,
      "grad_norm": 2.586690664291382,
      "learning_rate": 0.0002829347687144489,
      "loss": 1.2251,
      "step": 1133
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.9078938364982605,
      "learning_rate": 0.00028147166464657426,
      "loss": 1.1711,
      "step": 1134
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.8347866535186768,
      "learning_rate": 0.00028001173351314626,
      "loss": 1.3059,
      "step": 1135
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.9196872711181641,
      "learning_rate": 0.00027855498176104434,
      "loss": 1.2093,
      "step": 1136
    },
    {
      "epoch": 2.274,
      "grad_norm": 1.1543989181518555,
      "learning_rate": 0.0002771014158231088,
      "loss": 1.1454,
      "step": 1137
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.8811867237091064,
      "learning_rate": 0.0002756510421181112,
      "loss": 1.2892,
      "step": 1138
    },
    {
      "epoch": 2.278,
      "grad_norm": 1.1446958780288696,
      "learning_rate": 0.0002742038670507271,
      "loss": 1.3311,
      "step": 1139
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.0929967164993286,
      "learning_rate": 0.0002727598970115068,
      "loss": 1.1647,
      "step": 1140
    },
    {
      "epoch": 2.282,
      "grad_norm": 0.6311975717544556,
      "learning_rate": 0.00027131913837684797,
      "loss": 1.2614,
      "step": 1141
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.6510522961616516,
      "learning_rate": 0.0002698815975089668,
      "loss": 1.2589,
      "step": 1142
    },
    {
      "epoch": 2.286,
      "grad_norm": 0.6521551609039307,
      "learning_rate": 0.00026844728075587043,
      "loss": 1.1729,
      "step": 1143
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.7907068729400635,
      "learning_rate": 0.00026701619445132854,
      "loss": 1.1549,
      "step": 1144
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.73386549949646,
      "learning_rate": 0.00026558834491484574,
      "loss": 1.1601,
      "step": 1145
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.9883718490600586,
      "learning_rate": 0.00026416373845163343,
      "loss": 1.1738,
      "step": 1146
    },
    {
      "epoch": 2.294,
      "grad_norm": 1.001053810119629,
      "learning_rate": 0.00026274238135258113,
      "loss": 1.2483,
      "step": 1147
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.8780947327613831,
      "learning_rate": 0.000261324279894231,
      "loss": 1.2073,
      "step": 1148
    },
    {
      "epoch": 2.298,
      "grad_norm": 0.6157861351966858,
      "learning_rate": 0.00025990944033874806,
      "loss": 1.106,
      "step": 1149
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.897179901599884,
      "learning_rate": 0.00025849786893389294,
      "loss": 1.2062,
      "step": 1150
    },
    {
      "epoch": 2.302,
      "grad_norm": 1.0566297769546509,
      "learning_rate": 0.0002570895719129949,
      "loss": 1.1022,
      "step": 1151
    },
    {
      "epoch": 2.304,
      "grad_norm": 1.2217146158218384,
      "learning_rate": 0.00025568455549492306,
      "loss": 1.2528,
      "step": 1152
    },
    {
      "epoch": 2.306,
      "grad_norm": 1.6479134559631348,
      "learning_rate": 0.0002542828258840606,
      "loss": 1.3081,
      "step": 1153
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.9183220267295837,
      "learning_rate": 0.0002528843892702768,
      "loss": 1.346,
      "step": 1154
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.8748323321342468,
      "learning_rate": 0.0002514892518288988,
      "loss": 1.1674,
      "step": 1155
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.7848238945007324,
      "learning_rate": 0.0002500974197206857,
      "loss": 1.1249,
      "step": 1156
    },
    {
      "epoch": 2.314,
      "grad_norm": 0.9999386668205261,
      "learning_rate": 0.00024870889909179927,
      "loss": 1.2195,
      "step": 1157
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.6051103472709656,
      "learning_rate": 0.0002473236960737794,
      "loss": 1.2,
      "step": 1158
    },
    {
      "epoch": 2.318,
      "grad_norm": 1.2164219617843628,
      "learning_rate": 0.0002459418167835159,
      "loss": 1.3409,
      "step": 1159
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.3100993633270264,
      "learning_rate": 0.00024456326732322074,
      "loss": 1.1904,
      "step": 1160
    },
    {
      "epoch": 2.322,
      "grad_norm": 1.7274528741836548,
      "learning_rate": 0.00024318805378040242,
      "loss": 1.2029,
      "step": 1161
    },
    {
      "epoch": 2.324,
      "grad_norm": 16.41606330871582,
      "learning_rate": 0.00024181618222783742,
      "loss": 1.3167,
      "step": 1162
    },
    {
      "epoch": 2.326,
      "grad_norm": 1.058780312538147,
      "learning_rate": 0.00024044765872354524,
      "loss": 1.4933,
      "step": 1163
    },
    {
      "epoch": 2.328,
      "grad_norm": 2.2237493991851807,
      "learning_rate": 0.00023908248931076037,
      "loss": 1.2102,
      "step": 1164
    },
    {
      "epoch": 2.33,
      "grad_norm": 1.9099235534667969,
      "learning_rate": 0.0002377206800179068,
      "loss": 1.1465,
      "step": 1165
    },
    {
      "epoch": 2.332,
      "grad_norm": 1.6235295534133911,
      "learning_rate": 0.00023636223685857005,
      "loss": 1.1739,
      "step": 1166
    },
    {
      "epoch": 2.334,
      "grad_norm": 1.0623067617416382,
      "learning_rate": 0.00023500716583147065,
      "loss": 1.1586,
      "step": 1167
    },
    {
      "epoch": 2.336,
      "grad_norm": 1.1860811710357666,
      "learning_rate": 0.0002336554729204391,
      "loss": 1.0989,
      "step": 1168
    },
    {
      "epoch": 2.338,
      "grad_norm": 0.9525923728942871,
      "learning_rate": 0.0002323071640943879,
      "loss": 1.3638,
      "step": 1169
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.1978726387023926,
      "learning_rate": 0.00023096224530728672,
      "loss": 1.1981,
      "step": 1170
    },
    {
      "epoch": 2.342,
      "grad_norm": 0.895959734916687,
      "learning_rate": 0.00022962072249813482,
      "loss": 1.2132,
      "step": 1171
    },
    {
      "epoch": 2.344,
      "grad_norm": 1.0326093435287476,
      "learning_rate": 0.00022828260159093438,
      "loss": 1.2563,
      "step": 1172
    },
    {
      "epoch": 2.346,
      "grad_norm": 1.0601415634155273,
      "learning_rate": 0.0002269478884946663,
      "loss": 1.19,
      "step": 1173
    },
    {
      "epoch": 2.348,
      "grad_norm": 1.3614258766174316,
      "learning_rate": 0.00022561658910326244,
      "loss": 1.2478,
      "step": 1174
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.7380846738815308,
      "learning_rate": 0.0002242887092955801,
      "loss": 1.1957,
      "step": 1175
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.3279728889465332,
      "learning_rate": 0.0002229642549353772,
      "loss": 1.1717,
      "step": 1176
    },
    {
      "epoch": 2.354,
      "grad_norm": 2.3561320304870605,
      "learning_rate": 0.00022164323187128342,
      "loss": 1.1995,
      "step": 1177
    },
    {
      "epoch": 2.356,
      "grad_norm": 1.1536132097244263,
      "learning_rate": 0.00022032564593677773,
      "loss": 1.2791,
      "step": 1178
    },
    {
      "epoch": 2.358,
      "grad_norm": 0.8926586508750916,
      "learning_rate": 0.0002190115029501606,
      "loss": 1.3008,
      "step": 1179
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.8653767108917236,
      "learning_rate": 0.00021770080871452856,
      "loss": 1.2259,
      "step": 1180
    },
    {
      "epoch": 2.362,
      "grad_norm": 0.7813012003898621,
      "learning_rate": 0.00021639356901774986,
      "loss": 1.1438,
      "step": 1181
    },
    {
      "epoch": 2.364,
      "grad_norm": 1.0221229791641235,
      "learning_rate": 0.0002150897896324373,
      "loss": 1.1518,
      "step": 1182
    },
    {
      "epoch": 2.366,
      "grad_norm": 1.5640069246292114,
      "learning_rate": 0.00021378947631592283,
      "loss": 1.2404,
      "step": 1183
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.1806979179382324,
      "learning_rate": 0.0002124926348102333,
      "loss": 1.2573,
      "step": 1184
    },
    {
      "epoch": 2.37,
      "grad_norm": 1.0675095319747925,
      "learning_rate": 0.0002111992708420646,
      "loss": 1.1921,
      "step": 1185
    },
    {
      "epoch": 2.372,
      "grad_norm": 1.0778300762176514,
      "learning_rate": 0.00020990939012275556,
      "loss": 1.2647,
      "step": 1186
    },
    {
      "epoch": 2.374,
      "grad_norm": 1.5715465545654297,
      "learning_rate": 0.0002086229983482646,
      "loss": 1.2083,
      "step": 1187
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.8666690587997437,
      "learning_rate": 0.00020734010119914192,
      "loss": 1.1344,
      "step": 1188
    },
    {
      "epoch": 2.378,
      "grad_norm": 0.7715266942977905,
      "learning_rate": 0.00020606070434050672,
      "loss": 1.2488,
      "step": 1189
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.51156485080719,
      "learning_rate": 0.00020478481342202126,
      "loss": 1.1775,
      "step": 1190
    },
    {
      "epoch": 2.382,
      "grad_norm": 0.9266802668571472,
      "learning_rate": 0.0002035124340778659,
      "loss": 1.1112,
      "step": 1191
    },
    {
      "epoch": 2.384,
      "grad_norm": 2.1647675037384033,
      "learning_rate": 0.00020224357192671428,
      "loss": 1.1344,
      "step": 1192
    },
    {
      "epoch": 2.386,
      "grad_norm": 1.1963342428207397,
      "learning_rate": 0.00020097823257170868,
      "loss": 1.2974,
      "step": 1193
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.888167679309845,
      "learning_rate": 0.0001997164216004349,
      "loss": 1.1153,
      "step": 1194
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.2145764827728271,
      "learning_rate": 0.00019845814458489808,
      "loss": 1.193,
      "step": 1195
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.7170302271842957,
      "learning_rate": 0.00019720340708149776,
      "loss": 1.1619,
      "step": 1196
    },
    {
      "epoch": 2.394,
      "grad_norm": 0.770683228969574,
      "learning_rate": 0.0001959522146310032,
      "loss": 1.1974,
      "step": 1197
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.5985816717147827,
      "learning_rate": 0.00019470457275852949,
      "loss": 1.2185,
      "step": 1198
    },
    {
      "epoch": 2.398,
      "grad_norm": 0.46223515272140503,
      "learning_rate": 0.0001934604869735126,
      "loss": 1.2039,
      "step": 1199
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.7575311660766602,
      "learning_rate": 0.00019221996276968522,
      "loss": 1.1736,
      "step": 1200
    },
    {
      "epoch": 2.4,
      "eval_loss": 1.194549560546875,
      "eval_runtime": 228.9122,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1200
    },
    {
      "epoch": 2.402,
      "grad_norm": 2.340369701385498,
      "learning_rate": 0.00019098300562505265,
      "loss": 1.2523,
      "step": 1201
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.8958458304405212,
      "learning_rate": 0.00018974962100186833,
      "loss": 1.2081,
      "step": 1202
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.7341375350952148,
      "learning_rate": 0.00018851981434660992,
      "loss": 1.1425,
      "step": 1203
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.8987573385238647,
      "learning_rate": 0.00018729359108995546,
      "loss": 1.3301,
      "step": 1204
    },
    {
      "epoch": 2.41,
      "grad_norm": 1.9877313375473022,
      "learning_rate": 0.00018607095664675865,
      "loss": 1.2504,
      "step": 1205
    },
    {
      "epoch": 2.412,
      "grad_norm": 1.1533316373825073,
      "learning_rate": 0.00018485191641602595,
      "loss": 1.1762,
      "step": 1206
    },
    {
      "epoch": 2.414,
      "grad_norm": 0.9168343544006348,
      "learning_rate": 0.00018363647578089183,
      "loss": 1.2108,
      "step": 1207
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.7675278186798096,
      "learning_rate": 0.0001824246401085955,
      "loss": 1.2572,
      "step": 1208
    },
    {
      "epoch": 2.418,
      "grad_norm": 1.125416874885559,
      "learning_rate": 0.00018121641475045704,
      "loss": 1.3603,
      "step": 1209
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.622018814086914,
      "learning_rate": 0.000180011805041854,
      "loss": 1.1742,
      "step": 1210
    },
    {
      "epoch": 2.422,
      "grad_norm": 0.9470522403717041,
      "learning_rate": 0.00017881081630219742,
      "loss": 1.1954,
      "step": 1211
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.5685069561004639,
      "learning_rate": 0.00017761345383490878,
      "loss": 1.1596,
      "step": 1212
    },
    {
      "epoch": 2.426,
      "grad_norm": 1.3513908386230469,
      "learning_rate": 0.00017641972292739628,
      "loss": 1.1191,
      "step": 1213
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.859259843826294,
      "learning_rate": 0.00017522962885103144,
      "loss": 1.2207,
      "step": 1214
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.6933746337890625,
      "learning_rate": 0.00017404317686112636,
      "loss": 1.1441,
      "step": 1215
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.8926721811294556,
      "learning_rate": 0.00017286037219690976,
      "loss": 1.2511,
      "step": 1216
    },
    {
      "epoch": 2.434,
      "grad_norm": 29.02939796447754,
      "learning_rate": 0.00017168122008150456,
      "loss": 1.2496,
      "step": 1217
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.8222680687904358,
      "learning_rate": 0.0001705057257219036,
      "loss": 1.1759,
      "step": 1218
    },
    {
      "epoch": 2.438,
      "grad_norm": 0.925614058971405,
      "learning_rate": 0.00016933389430894897,
      "loss": 1.229,
      "step": 1219
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.7584233283996582,
      "learning_rate": 0.00016816573101730638,
      "loss": 1.1383,
      "step": 1220
    },
    {
      "epoch": 2.442,
      "grad_norm": 0.648844838142395,
      "learning_rate": 0.00016700124100544413,
      "loss": 1.134,
      "step": 1221
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.8547199964523315,
      "learning_rate": 0.00016584042941560973,
      "loss": 1.1964,
      "step": 1222
    },
    {
      "epoch": 2.446,
      "grad_norm": 1.2093325853347778,
      "learning_rate": 0.00016468330137380693,
      "loss": 1.2303,
      "step": 1223
    },
    {
      "epoch": 2.448,
      "grad_norm": 1.7031357288360596,
      "learning_rate": 0.00016352986198977327,
      "loss": 1.0827,
      "step": 1224
    },
    {
      "epoch": 2.45,
      "grad_norm": 1.1039471626281738,
      "learning_rate": 0.0001623801163569585,
      "loss": 1.1898,
      "step": 1225
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.6647872924804688,
      "learning_rate": 0.0001612340695525004,
      "loss": 1.1611,
      "step": 1226
    },
    {
      "epoch": 2.454,
      "grad_norm": 4.50335168838501,
      "learning_rate": 0.00016009172663720351,
      "loss": 1.2987,
      "step": 1227
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.6854565143585205,
      "learning_rate": 0.00015895309265551638,
      "loss": 1.2032,
      "step": 1228
    },
    {
      "epoch": 2.458,
      "grad_norm": 0.9440429210662842,
      "learning_rate": 0.00015781817263550867,
      "loss": 1.1548,
      "step": 1229
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.8159842491149902,
      "learning_rate": 0.00015668697158885102,
      "loss": 1.1853,
      "step": 1230
    },
    {
      "epoch": 2.462,
      "grad_norm": 0.4479374587535858,
      "learning_rate": 0.00015555949451079054,
      "loss": 1.0976,
      "step": 1231
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.8630422949790955,
      "learning_rate": 0.00015443574638013003,
      "loss": 1.1626,
      "step": 1232
    },
    {
      "epoch": 2.466,
      "grad_norm": 0.6196279525756836,
      "learning_rate": 0.0001533157321592058,
      "loss": 1.1344,
      "step": 1233
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.6253308653831482,
      "learning_rate": 0.00015219945679386505,
      "loss": 1.0728,
      "step": 1234
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.5779390931129456,
      "learning_rate": 0.00015108692521344526,
      "loss": 1.1569,
      "step": 1235
    },
    {
      "epoch": 2.472,
      "grad_norm": 1.565556287765503,
      "learning_rate": 0.00014997814233075202,
      "loss": 1.2026,
      "step": 1236
    },
    {
      "epoch": 2.474,
      "grad_norm": 2.460574150085449,
      "learning_rate": 0.00014887311304203666,
      "loss": 1.1981,
      "step": 1237
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.7537539601325989,
      "learning_rate": 0.00014777184222697538,
      "loss": 1.1608,
      "step": 1238
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 1.20552396774292,
      "learning_rate": 0.00014667433474864678,
      "loss": 1.1489,
      "step": 1239
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.125731110572815,
      "learning_rate": 0.00014558059545351142,
      "loss": 1.2251,
      "step": 1240
    },
    {
      "epoch": 2.482,
      "grad_norm": 1.267386555671692,
      "learning_rate": 0.00014449062917139055,
      "loss": 1.2781,
      "step": 1241
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.6703099012374878,
      "learning_rate": 0.00014340444071544368,
      "loss": 1.1919,
      "step": 1242
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 0.9751828908920288,
      "learning_rate": 0.00014232203488214812,
      "loss": 1.2864,
      "step": 1243
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.8636718988418579,
      "learning_rate": 0.00014124341645127702,
      "loss": 1.1735,
      "step": 1244
    },
    {
      "epoch": 2.49,
      "grad_norm": 6.062546730041504,
      "learning_rate": 0.00014016859018587958,
      "loss": 1.1584,
      "step": 1245
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.698689341545105,
      "learning_rate": 0.00013909756083225843,
      "loss": 1.1289,
      "step": 1246
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 1.3429627418518066,
      "learning_rate": 0.00013803033311995074,
      "loss": 1.1173,
      "step": 1247
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.8070347309112549,
      "learning_rate": 0.00013696691176170507,
      "loss": 1.1749,
      "step": 1248
    },
    {
      "epoch": 2.498,
      "grad_norm": 0.7851744890213013,
      "learning_rate": 0.00013590730145346153,
      "loss": 1.1598,
      "step": 1249
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.1090298891067505,
      "learning_rate": 0.00013485150687433166,
      "loss": 1.2535,
      "step": 1250
    },
    {
      "epoch": 2.502,
      "grad_norm": 0.5813297033309937,
      "learning_rate": 0.00013379953268657696,
      "loss": 1.2851,
      "step": 1251
    },
    {
      "epoch": 2.504,
      "grad_norm": 2.107870578765869,
      "learning_rate": 0.00013275138353558823,
      "loss": 1.2918,
      "step": 1252
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 0.9784218072891235,
      "learning_rate": 0.00013170706404986644,
      "loss": 1.2997,
      "step": 1253
    },
    {
      "epoch": 2.508,
      "grad_norm": 1.3459959030151367,
      "learning_rate": 0.00013066657884099964,
      "loss": 1.1573,
      "step": 1254
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.2722069025039673,
      "learning_rate": 0.0001296299325036454,
      "loss": 1.3228,
      "step": 1255
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.9677184820175171,
      "learning_rate": 0.00012859712961550874,
      "loss": 1.158,
      "step": 1256
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 1.1988757848739624,
      "learning_rate": 0.0001275681747373224,
      "loss": 1.2889,
      "step": 1257
    },
    {
      "epoch": 2.516,
      "grad_norm": 1.0743237733840942,
      "learning_rate": 0.0001265430724128277,
      "loss": 1.1659,
      "step": 1258
    },
    {
      "epoch": 2.518,
      "grad_norm": 1.6720274686813354,
      "learning_rate": 0.00012552182716875227,
      "loss": 1.2034,
      "step": 1259
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.0819056034088135,
      "learning_rate": 0.00012450444351479195,
      "loss": 1.1648,
      "step": 1260
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 1.2290116548538208,
      "learning_rate": 0.00012349092594359036,
      "loss": 1.2257,
      "step": 1261
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.71037757396698,
      "learning_rate": 0.0001224812789307187,
      "loss": 1.1063,
      "step": 1262
    },
    {
      "epoch": 2.526,
      "grad_norm": 0.7787315845489502,
      "learning_rate": 0.00012147550693465636,
      "loss": 1.2588,
      "step": 1263
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.5615307688713074,
      "learning_rate": 0.0001204736143967714,
      "loss": 1.1599,
      "step": 1264
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 1.0727591514587402,
      "learning_rate": 0.00011947560574130011,
      "loss": 1.2504,
      "step": 1265
    },
    {
      "epoch": 2.532,
      "grad_norm": 1.118167757987976,
      "learning_rate": 0.00011848148537532844,
      "loss": 1.1261,
      "step": 1266
    },
    {
      "epoch": 2.534,
      "grad_norm": 0.7857189774513245,
      "learning_rate": 0.000117491257688772,
      "loss": 1.3343,
      "step": 1267
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.8771981000900269,
      "learning_rate": 0.00011650492705435678,
      "loss": 1.2004,
      "step": 1268
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 1.996174931526184,
      "learning_rate": 0.00011552249782759983,
      "loss": 1.171,
      "step": 1269
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.4410886764526367,
      "learning_rate": 0.0001145439743467902,
      "loss": 1.2244,
      "step": 1270
    },
    {
      "epoch": 2.542,
      "grad_norm": 0.5451167821884155,
      "learning_rate": 0.00011356936093296944,
      "loss": 1.3519,
      "step": 1271
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.1310733556747437,
      "learning_rate": 0.00011259866188991275,
      "loss": 1.3504,
      "step": 1272
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 0.6089124083518982,
      "learning_rate": 0.00011163188150411019,
      "loss": 1.0943,
      "step": 1273
    },
    {
      "epoch": 2.548,
      "grad_norm": 1.431448221206665,
      "learning_rate": 0.0001106690240447471,
      "loss": 1.2376,
      "step": 1274
    },
    {
      "epoch": 2.55,
      "grad_norm": 2.38862681388855,
      "learning_rate": 0.00010971009376368612,
      "loss": 1.1949,
      "step": 1275
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.7030392289161682,
      "learning_rate": 0.00010875509489544744,
      "loss": 1.2059,
      "step": 1276
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 1.024477243423462,
      "learning_rate": 0.00010780403165719088,
      "loss": 1.2012,
      "step": 1277
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.9852086305618286,
      "learning_rate": 0.0001068569082486972,
      "loss": 1.1099,
      "step": 1278
    },
    {
      "epoch": 2.558,
      "grad_norm": 1.1219964027404785,
      "learning_rate": 0.00010591372885234885,
      "loss": 1.2262,
      "step": 1279
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.1026920080184937,
      "learning_rate": 0.0001049744976331124,
      "loss": 1.1235,
      "step": 1280
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 0.9077086448669434,
      "learning_rate": 0.00010403921873851951,
      "loss": 1.0908,
      "step": 1281
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.6095170378684998,
      "learning_rate": 0.00010310789629864903,
      "loss": 1.1181,
      "step": 1282
    },
    {
      "epoch": 2.566,
      "grad_norm": 1.296194076538086,
      "learning_rate": 0.00010218053442610842,
      "loss": 1.2078,
      "step": 1283
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.7417304515838623,
      "learning_rate": 0.00010125713721601593,
      "loss": 1.2147,
      "step": 1284
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.5865510106086731,
      "learning_rate": 0.00010033770874598224,
      "loss": 1.204,
      "step": 1285
    },
    {
      "epoch": 2.572,
      "grad_norm": 1.1880731582641602,
      "learning_rate": 9.942225307609243e-05,
      "loss": 1.1858,
      "step": 1286
    },
    {
      "epoch": 2.574,
      "grad_norm": 0.7451639175415039,
      "learning_rate": 9.851077424888844e-05,
      "loss": 1.1187,
      "step": 1287
    },
    {
      "epoch": 2.576,
      "grad_norm": 1.0617671012878418,
      "learning_rate": 9.760327628935073e-05,
      "loss": 1.2949,
      "step": 1288
    },
    {
      "epoch": 2.578,
      "grad_norm": 1.3907320499420166,
      "learning_rate": 9.669976320488083e-05,
      "loss": 1.2577,
      "step": 1289
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.2007839679718018,
      "learning_rate": 9.580023898528345e-05,
      "loss": 1.1211,
      "step": 1290
    },
    {
      "epoch": 2.582,
      "grad_norm": 0.5458554625511169,
      "learning_rate": 9.490470760274917e-05,
      "loss": 1.2962,
      "step": 1291
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.9216965436935425,
      "learning_rate": 9.401317301183654e-05,
      "loss": 1.3402,
      "step": 1292
    },
    {
      "epoch": 2.586,
      "grad_norm": 1.0911626815795898,
      "learning_rate": 9.31256391494546e-05,
      "loss": 1.1126,
      "step": 1293
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.5937939286231995,
      "learning_rate": 9.224210993484605e-05,
      "loss": 1.1431,
      "step": 1294
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.7490249276161194,
      "learning_rate": 9.136258926956886e-05,
      "loss": 1.1252,
      "step": 1295
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.013742446899414,
      "learning_rate": 9.048708103748071e-05,
      "loss": 1.0415,
      "step": 1296
    },
    {
      "epoch": 2.594,
      "grad_norm": 1.856061577796936,
      "learning_rate": 8.961558910472e-05,
      "loss": 1.0978,
      "step": 1297
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.8585401177406311,
      "learning_rate": 8.874811731969023e-05,
      "loss": 1.1062,
      "step": 1298
    },
    {
      "epoch": 2.598,
      "grad_norm": 2.844616651535034,
      "learning_rate": 8.788466951304208e-05,
      "loss": 1.1392,
      "step": 1299
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.9072121977806091,
      "learning_rate": 8.702524949765645e-05,
      "loss": 1.1951,
      "step": 1300
    },
    {
      "epoch": 2.6,
      "eval_loss": 1.1694953441619873,
      "eval_runtime": 228.8019,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1300
    },
    {
      "epoch": 2.602,
      "grad_norm": 1.4769704341888428,
      "learning_rate": 8.616986106862911e-05,
      "loss": 1.076,
      "step": 1301
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.6576958894729614,
      "learning_rate": 8.531850800325181e-05,
      "loss": 1.1688,
      "step": 1302
    },
    {
      "epoch": 2.606,
      "grad_norm": 1.004211187362671,
      "learning_rate": 8.447119406099702e-05,
      "loss": 1.0932,
      "step": 1303
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.6466251611709595,
      "learning_rate": 8.36279229835012e-05,
      "loss": 1.1715,
      "step": 1304
    },
    {
      "epoch": 2.61,
      "grad_norm": 2.2290782928466797,
      "learning_rate": 8.278869849454718e-05,
      "loss": 1.1155,
      "step": 1305
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.9843249320983887,
      "learning_rate": 8.19535243000491e-05,
      "loss": 1.2098,
      "step": 1306
    },
    {
      "epoch": 2.614,
      "grad_norm": 1.1420575380325317,
      "learning_rate": 8.112240408803583e-05,
      "loss": 1.1696,
      "step": 1307
    },
    {
      "epoch": 2.616,
      "grad_norm": 1.3407143354415894,
      "learning_rate": 8.029534152863383e-05,
      "loss": 1.1886,
      "step": 1308
    },
    {
      "epoch": 2.618,
      "grad_norm": 1.0619971752166748,
      "learning_rate": 7.947234027405159e-05,
      "loss": 1.0815,
      "step": 1309
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.7099370956420898,
      "learning_rate": 7.865340395856324e-05,
      "loss": 1.1569,
      "step": 1310
    },
    {
      "epoch": 2.622,
      "grad_norm": 1.1468371152877808,
      "learning_rate": 7.783853619849279e-05,
      "loss": 1.2434,
      "step": 1311
    },
    {
      "epoch": 2.624,
      "grad_norm": 1.1868855953216553,
      "learning_rate": 7.702774059219786e-05,
      "loss": 1.1602,
      "step": 1312
    },
    {
      "epoch": 2.626,
      "grad_norm": 1.156132459640503,
      "learning_rate": 7.622102072005432e-05,
      "loss": 1.1276,
      "step": 1313
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.8502592444419861,
      "learning_rate": 7.54183801444398e-05,
      "loss": 1.1712,
      "step": 1314
    },
    {
      "epoch": 2.63,
      "grad_norm": 3.326014518737793,
      "learning_rate": 7.461982240971799e-05,
      "loss": 1.3045,
      "step": 1315
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.8292366862297058,
      "learning_rate": 7.382535104222366e-05,
      "loss": 1.1541,
      "step": 1316
    },
    {
      "epoch": 2.634,
      "grad_norm": 0.9914650917053223,
      "learning_rate": 7.303496955024625e-05,
      "loss": 1.164,
      "step": 1317
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.7969197034835815,
      "learning_rate": 7.224868142401542e-05,
      "loss": 1.0908,
      "step": 1318
    },
    {
      "epoch": 2.638,
      "grad_norm": 6.267918586730957,
      "learning_rate": 7.146649013568484e-05,
      "loss": 1.2079,
      "step": 1319
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.4954153299331665,
      "learning_rate": 7.068839913931646e-05,
      "loss": 1.3111,
      "step": 1320
    },
    {
      "epoch": 2.642,
      "grad_norm": 1.8461318016052246,
      "learning_rate": 6.991441187086633e-05,
      "loss": 1.1322,
      "step": 1321
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.9345418214797974,
      "learning_rate": 6.914453174816904e-05,
      "loss": 1.2473,
      "step": 1322
    },
    {
      "epoch": 2.646,
      "grad_norm": 1.4575597047805786,
      "learning_rate": 6.837876217092198e-05,
      "loss": 1.1524,
      "step": 1323
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.8355869650840759,
      "learning_rate": 6.761710652067177e-05,
      "loss": 1.1094,
      "step": 1324
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.9907805323600769,
      "learning_rate": 6.685956816079752e-05,
      "loss": 1.2773,
      "step": 1325
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.6780723333358765,
      "learning_rate": 6.610615043649714e-05,
      "loss": 1.202,
      "step": 1326
    },
    {
      "epoch": 2.654,
      "grad_norm": 1.2528053522109985,
      "learning_rate": 6.535685667477264e-05,
      "loss": 1.1038,
      "step": 1327
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.3824992179870605,
      "learning_rate": 6.46116901844147e-05,
      "loss": 1.0558,
      "step": 1328
    },
    {
      "epoch": 2.658,
      "grad_norm": 1.2255935668945312,
      "learning_rate": 6.387065425598882e-05,
      "loss": 1.2347,
      "step": 1329
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.6575255990028381,
      "learning_rate": 6.313375216182038e-05,
      "loss": 1.152,
      "step": 1330
    },
    {
      "epoch": 2.662,
      "grad_norm": 1.5571699142456055,
      "learning_rate": 6.240098715597975e-05,
      "loss": 1.1597,
      "step": 1331
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.2184648513793945,
      "learning_rate": 6.1672362474269e-05,
      "loss": 1.0664,
      "step": 1332
    },
    {
      "epoch": 2.666,
      "grad_norm": 0.9221116304397583,
      "learning_rate": 6.094788133420681e-05,
      "loss": 1.1845,
      "step": 1333
    },
    {
      "epoch": 2.668,
      "grad_norm": 2.531715154647827,
      "learning_rate": 6.022754693501431e-05,
      "loss": 1.1337,
      "step": 1334
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.8316018581390381,
      "learning_rate": 5.95113624576018e-05,
      "loss": 1.2518,
      "step": 1335
    },
    {
      "epoch": 2.672,
      "grad_norm": 1.0826908349990845,
      "learning_rate": 5.879933106455304e-05,
      "loss": 1.1686,
      "step": 1336
    },
    {
      "epoch": 2.674,
      "grad_norm": 0.7599675059318542,
      "learning_rate": 5.8091455900113e-05,
      "loss": 1.1548,
      "step": 1337
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.6325603723526001,
      "learning_rate": 5.7387740090172894e-05,
      "loss": 1.0962,
      "step": 1338
    },
    {
      "epoch": 2.678,
      "grad_norm": 1.436640739440918,
      "learning_rate": 5.668818674225684e-05,
      "loss": 1.2327,
      "step": 1339
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.2399711608886719,
      "learning_rate": 5.599279894550824e-05,
      "loss": 1.187,
      "step": 1340
    },
    {
      "epoch": 2.682,
      "grad_norm": 0.6330413222312927,
      "learning_rate": 5.530157977067552e-05,
      "loss": 1.162,
      "step": 1341
    },
    {
      "epoch": 2.684,
      "grad_norm": 1.1452242136001587,
      "learning_rate": 5.461453227009916e-05,
      "loss": 1.2129,
      "step": 1342
    },
    {
      "epoch": 2.686,
      "grad_norm": 0.6809900999069214,
      "learning_rate": 5.393165947769818e-05,
      "loss": 1.136,
      "step": 1343
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.8221299648284912,
      "learning_rate": 5.3252964408956216e-05,
      "loss": 1.1498,
      "step": 1344
    },
    {
      "epoch": 2.69,
      "grad_norm": 1.3665423393249512,
      "learning_rate": 5.257845006090911e-05,
      "loss": 1.203,
      "step": 1345
    },
    {
      "epoch": 2.692,
      "grad_norm": 1.6640723943710327,
      "learning_rate": 5.1908119412130586e-05,
      "loss": 1.2439,
      "step": 1346
    },
    {
      "epoch": 2.694,
      "grad_norm": 3.4374876022338867,
      "learning_rate": 5.124197542272002e-05,
      "loss": 1.1002,
      "step": 1347
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.6711872816085815,
      "learning_rate": 5.058002103428905e-05,
      "loss": 1.208,
      "step": 1348
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.9059156775474548,
      "learning_rate": 4.992225916994819e-05,
      "loss": 1.1195,
      "step": 1349
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.8150299787521362,
      "learning_rate": 4.9268692734294464e-05,
      "loss": 1.2466,
      "step": 1350
    },
    {
      "epoch": 2.702,
      "grad_norm": 0.8379353880882263,
      "learning_rate": 4.8619324613398576e-05,
      "loss": 1.1719,
      "step": 1351
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.0148833990097046,
      "learning_rate": 4.797415767479174e-05,
      "loss": 1.098,
      "step": 1352
    },
    {
      "epoch": 2.706,
      "grad_norm": 1.976536512374878,
      "learning_rate": 4.733319476745335e-05,
      "loss": 1.107,
      "step": 1353
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.6910558342933655,
      "learning_rate": 4.6696438721798184e-05,
      "loss": 1.0683,
      "step": 1354
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.5749010443687439,
      "learning_rate": 4.6063892349664236e-05,
      "loss": 1.037,
      "step": 1355
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.7348378896713257,
      "learning_rate": 4.543555844429992e-05,
      "loss": 1.1438,
      "step": 1356
    },
    {
      "epoch": 2.714,
      "grad_norm": 0.5497878789901733,
      "learning_rate": 4.481143978035196e-05,
      "loss": 1.0427,
      "step": 1357
    },
    {
      "epoch": 2.716,
      "grad_norm": 1.050514817237854,
      "learning_rate": 4.41915391138531e-05,
      "loss": 1.2128,
      "step": 1358
    },
    {
      "epoch": 2.718,
      "grad_norm": 0.7912823557853699,
      "learning_rate": 4.357585918220986e-05,
      "loss": 1.1452,
      "step": 1359
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.7404230237007141,
      "learning_rate": 4.2964402704190555e-05,
      "loss": 1.0887,
      "step": 1360
    },
    {
      "epoch": 2.722,
      "grad_norm": 1.248901605606079,
      "learning_rate": 4.235717237991321e-05,
      "loss": 1.1772,
      "step": 1361
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.5380420088768005,
      "learning_rate": 4.175417089083378e-05,
      "loss": 1.1233,
      "step": 1362
    },
    {
      "epoch": 2.726,
      "grad_norm": 1.2176626920700073,
      "learning_rate": 4.115540089973402e-05,
      "loss": 1.2243,
      "step": 1363
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.5942589640617371,
      "learning_rate": 4.0560865050709997e-05,
      "loss": 1.2372,
      "step": 1364
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.3833152055740356,
      "learning_rate": 3.997056596916038e-05,
      "loss": 1.1564,
      "step": 1365
    },
    {
      "epoch": 2.732,
      "grad_norm": 1.006684422492981,
      "learning_rate": 3.9384506261774365e-05,
      "loss": 1.1664,
      "step": 1366
    },
    {
      "epoch": 2.734,
      "grad_norm": 0.6274227499961853,
      "learning_rate": 3.8802688516521356e-05,
      "loss": 1.1101,
      "step": 1367
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.8958070874214172,
      "learning_rate": 3.822511530263806e-05,
      "loss": 1.1547,
      "step": 1368
    },
    {
      "epoch": 2.738,
      "grad_norm": 1.6800442934036255,
      "learning_rate": 3.765178917061818e-05,
      "loss": 1.1318,
      "step": 1369
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.835320770740509,
      "learning_rate": 3.7082712652200864e-05,
      "loss": 1.07,
      "step": 1370
    },
    {
      "epoch": 2.742,
      "grad_norm": 0.7992783188819885,
      "learning_rate": 3.651788826035895e-05,
      "loss": 1.171,
      "step": 1371
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.8570391535758972,
      "learning_rate": 3.595731848928929e-05,
      "loss": 1.2374,
      "step": 1372
    },
    {
      "epoch": 2.746,
      "grad_norm": 0.7907513380050659,
      "learning_rate": 3.54010058144002e-05,
      "loss": 1.1599,
      "step": 1373
    },
    {
      "epoch": 2.748,
      "grad_norm": 1.5618098974227905,
      "learning_rate": 3.484895269230137e-05,
      "loss": 1.1437,
      "step": 1374
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.86674964427948,
      "learning_rate": 3.430116156079277e-05,
      "loss": 1.2415,
      "step": 1375
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.6564878821372986,
      "learning_rate": 3.375763483885386e-05,
      "loss": 1.1287,
      "step": 1376
    },
    {
      "epoch": 2.754,
      "grad_norm": 0.744292140007019,
      "learning_rate": 3.321837492663304e-05,
      "loss": 1.2203,
      "step": 1377
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 1.8992972373962402,
      "learning_rate": 3.268338420543726e-05,
      "loss": 1.1842,
      "step": 1378
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.9391039609909058,
      "learning_rate": 3.215266503772085e-05,
      "loss": 1.2196,
      "step": 1379
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.9436885118484497,
      "learning_rate": 3.162621976707558e-05,
      "loss": 1.1597,
      "step": 1380
    },
    {
      "epoch": 2.762,
      "grad_norm": 0.9579997062683105,
      "learning_rate": 3.1104050718220536e-05,
      "loss": 1.1682,
      "step": 1381
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.819248616695404,
      "learning_rate": 3.0586160196990894e-05,
      "loss": 1.1388,
      "step": 1382
    },
    {
      "epoch": 2.766,
      "grad_norm": 1.0031737089157104,
      "learning_rate": 3.0072550490328753e-05,
      "loss": 1.2027,
      "step": 1383
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.8007104396820068,
      "learning_rate": 2.9563223866272858e-05,
      "loss": 1.0349,
      "step": 1384
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.8350557088851929,
      "learning_rate": 2.905818257394799e-05,
      "loss": 1.1658,
      "step": 1385
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.7059655785560608,
      "learning_rate": 2.855742884355561e-05,
      "loss": 1.1704,
      "step": 1386
    },
    {
      "epoch": 2.774,
      "grad_norm": 0.745569109916687,
      "learning_rate": 2.806096488636367e-05,
      "loss": 1.1242,
      "step": 1387
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.737017035484314,
      "learning_rate": 2.756879289469716e-05,
      "loss": 1.1425,
      "step": 1388
    },
    {
      "epoch": 2.778,
      "grad_norm": 1.0294549465179443,
      "learning_rate": 2.7080915041928332e-05,
      "loss": 1.1602,
      "step": 1389
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.783542275428772,
      "learning_rate": 2.6597333482466847e-05,
      "loss": 1.1624,
      "step": 1390
    },
    {
      "epoch": 2.782,
      "grad_norm": 1.0813747644424438,
      "learning_rate": 2.6118050351750635e-05,
      "loss": 1.193,
      "step": 1391
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.9776589870452881,
      "learning_rate": 2.564306776623593e-05,
      "loss": 1.1478,
      "step": 1392
    },
    {
      "epoch": 2.786,
      "grad_norm": 0.7292658090591431,
      "learning_rate": 2.5172387823388708e-05,
      "loss": 1.2287,
      "step": 1393
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 1.4828966856002808,
      "learning_rate": 2.470601260167471e-05,
      "loss": 1.1607,
      "step": 1394
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.5667781829833984,
      "learning_rate": 2.424394416055076e-05,
      "loss": 1.2155,
      "step": 1395
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.6823787093162537,
      "learning_rate": 2.3786184540455446e-05,
      "loss": 1.2241,
      "step": 1396
    },
    {
      "epoch": 2.794,
      "grad_norm": 0.7523752450942993,
      "learning_rate": 2.3332735762799817e-05,
      "loss": 1.1011,
      "step": 1397
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 1.2191928625106812,
      "learning_rate": 2.2883599829959134e-05,
      "loss": 1.1675,
      "step": 1398
    },
    {
      "epoch": 2.798,
      "grad_norm": 1.0180639028549194,
      "learning_rate": 2.2438778725263232e-05,
      "loss": 1.0716,
      "step": 1399
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.7864862680435181,
      "learning_rate": 2.1998274412988628e-05,
      "loss": 1.1271,
      "step": 1400
    },
    {
      "epoch": 2.8,
      "eval_loss": 1.1610379219055176,
      "eval_runtime": 223.9142,
      "eval_samples_per_second": 0.447,
      "eval_steps_per_second": 0.447,
      "step": 1400
    },
    {
      "epoch": 2.802,
      "grad_norm": 2.5688652992248535,
      "learning_rate": 2.1562088838349425e-05,
      "loss": 1.1633,
      "step": 1401
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 1.2911415100097656,
      "learning_rate": 2.11302239274882e-05,
      "loss": 1.1148,
      "step": 1402
    },
    {
      "epoch": 2.806,
      "grad_norm": 0.8174794912338257,
      "learning_rate": 2.0702681587468353e-05,
      "loss": 1.1953,
      "step": 1403
    },
    {
      "epoch": 2.808,
      "grad_norm": 1.8233996629714966,
      "learning_rate": 2.027946370626532e-05,
      "loss": 1.2738,
      "step": 1404
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.6656189560890198,
      "learning_rate": 1.986057215275816e-05,
      "loss": 1.1692,
      "step": 1405
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.8911401629447937,
      "learning_rate": 1.9446008776721645e-05,
      "loss": 1.0925,
      "step": 1406
    },
    {
      "epoch": 2.814,
      "grad_norm": 0.8339460492134094,
      "learning_rate": 1.9035775408817403e-05,
      "loss": 1.1008,
      "step": 1407
    },
    {
      "epoch": 2.816,
      "grad_norm": 1.2991700172424316,
      "learning_rate": 1.8629873860586568e-05,
      "loss": 1.2658,
      "step": 1408
    },
    {
      "epoch": 2.818,
      "grad_norm": 2.8445186614990234,
      "learning_rate": 1.822830592444147e-05,
      "loss": 1.1376,
      "step": 1409
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.6518149375915527,
      "learning_rate": 1.7831073373657525e-05,
      "loss": 1.1286,
      "step": 1410
    },
    {
      "epoch": 2.822,
      "grad_norm": 1.2559632062911987,
      "learning_rate": 1.74381779623658e-05,
      "loss": 1.0833,
      "step": 1411
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.6040147542953491,
      "learning_rate": 1.7049621425545115e-05,
      "loss": 1.1359,
      "step": 1412
    },
    {
      "epoch": 2.826,
      "grad_norm": 0.8154429197311401,
      "learning_rate": 1.6665405479014294e-05,
      "loss": 1.1746,
      "step": 1413
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.7796658873558044,
      "learning_rate": 1.6285531819424494e-05,
      "loss": 1.0685,
      "step": 1414
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.6060484051704407,
      "learning_rate": 1.5910002124251975e-05,
      "loss": 1.2162,
      "step": 1415
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.607424259185791,
      "learning_rate": 1.5538818051790583e-05,
      "loss": 1.1406,
      "step": 1416
    },
    {
      "epoch": 2.834,
      "grad_norm": 0.9277681708335876,
      "learning_rate": 1.5171981241144494e-05,
      "loss": 1.2053,
      "step": 1417
    },
    {
      "epoch": 2.836,
      "grad_norm": 1.2125309705734253,
      "learning_rate": 1.480949331222059e-05,
      "loss": 1.1157,
      "step": 1418
    },
    {
      "epoch": 2.838,
      "grad_norm": 0.7117336988449097,
      "learning_rate": 1.4451355865722105e-05,
      "loss": 1.0427,
      "step": 1419
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.6711523532867432,
      "learning_rate": 1.4097570483140642e-05,
      "loss": 1.22,
      "step": 1420
    },
    {
      "epoch": 2.842,
      "grad_norm": 0.7561403512954712,
      "learning_rate": 1.3748138726749737e-05,
      "loss": 1.1805,
      "step": 1421
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.7788792252540588,
      "learning_rate": 1.3403062139598077e-05,
      "loss": 1.1433,
      "step": 1422
    },
    {
      "epoch": 2.846,
      "grad_norm": 0.5204653739929199,
      "learning_rate": 1.3062342245501958e-05,
      "loss": 1.0782,
      "step": 1423
    },
    {
      "epoch": 2.848,
      "grad_norm": 2.019362688064575,
      "learning_rate": 1.2725980549039506e-05,
      "loss": 1.1485,
      "step": 1424
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.9201071858406067,
      "learning_rate": 1.239397853554336e-05,
      "loss": 1.1673,
      "step": 1425
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.9530510306358337,
      "learning_rate": 1.206633767109444e-05,
      "loss": 1.118,
      "step": 1426
    },
    {
      "epoch": 2.854,
      "grad_norm": 0.6416634321212769,
      "learning_rate": 1.1743059402515077e-05,
      "loss": 1.0943,
      "step": 1427
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.49372297525405884,
      "learning_rate": 1.142414515736323e-05,
      "loss": 1.0853,
      "step": 1428
    },
    {
      "epoch": 2.858,
      "grad_norm": 0.5072737336158752,
      "learning_rate": 1.1109596343925721e-05,
      "loss": 1.0929,
      "step": 1429
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.2669074535369873,
      "learning_rate": 1.0799414351212234e-05,
      "loss": 1.2154,
      "step": 1430
    },
    {
      "epoch": 2.862,
      "grad_norm": 0.8839138150215149,
      "learning_rate": 1.0493600548948879e-05,
      "loss": 1.1126,
      "step": 1431
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.4417452812194824,
      "learning_rate": 1.019215628757264e-05,
      "loss": 1.2775,
      "step": 1432
    },
    {
      "epoch": 2.866,
      "grad_norm": 1.3309309482574463,
      "learning_rate": 9.895082898224939e-06,
      "loss": 1.1456,
      "step": 1433
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.7321348190307617,
      "learning_rate": 9.602381692746077e-06,
      "loss": 1.1673,
      "step": 1434
    },
    {
      "epoch": 2.87,
      "grad_norm": 5.895889759063721,
      "learning_rate": 9.314053963669244e-06,
      "loss": 1.125,
      "step": 1435
    },
    {
      "epoch": 2.872,
      "grad_norm": 2.055471420288086,
      "learning_rate": 9.030100984214861e-06,
      "loss": 1.1266,
      "step": 1436
    },
    {
      "epoch": 2.874,
      "grad_norm": 1.0695024728775024,
      "learning_rate": 8.750524008285132e-06,
      "loss": 1.2282,
      "step": 1437
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.8988561630249023,
      "learning_rate": 8.475324270458163e-06,
      "loss": 1.2413,
      "step": 1438
    },
    {
      "epoch": 2.878,
      "grad_norm": 1.0401649475097656,
      "learning_rate": 8.204502985982854e-06,
      "loss": 1.256,
      "step": 1439
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.081560730934143,
      "learning_rate": 7.93806135077324e-06,
      "loss": 1.231,
      "step": 1440
    },
    {
      "epoch": 2.882,
      "grad_norm": 0.9232778549194336,
      "learning_rate": 7.676000541403494e-06,
      "loss": 1.2558,
      "step": 1441
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.6926286816596985,
      "learning_rate": 7.418321715102705e-06,
      "loss": 1.0527,
      "step": 1442
    },
    {
      "epoch": 2.886,
      "grad_norm": 0.7142324447631836,
      "learning_rate": 7.165026009749109e-06,
      "loss": 1.1729,
      "step": 1443
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.6389784216880798,
      "learning_rate": 6.9161145438662035e-06,
      "loss": 1.1365,
      "step": 1444
    },
    {
      "epoch": 2.89,
      "grad_norm": 1.0790841579437256,
      "learning_rate": 6.6715884166170806e-06,
      "loss": 1.1468,
      "step": 1445
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.8875596523284912,
      "learning_rate": 6.431448707799436e-06,
      "loss": 1.1949,
      "step": 1446
    },
    {
      "epoch": 2.894,
      "grad_norm": 0.8199006915092468,
      "learning_rate": 6.195696477841462e-06,
      "loss": 1.1639,
      "step": 1447
    },
    {
      "epoch": 2.896,
      "grad_norm": 1.0412195920944214,
      "learning_rate": 5.964332767796399e-06,
      "loss": 1.1549,
      "step": 1448
    },
    {
      "epoch": 2.898,
      "grad_norm": 1.182554841041565,
      "learning_rate": 5.737358599338438e-06,
      "loss": 1.1761,
      "step": 1449
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.5320535898208618,
      "learning_rate": 5.514774974758274e-06,
      "loss": 1.2214,
      "step": 1450
    },
    {
      "epoch": 2.902,
      "grad_norm": 0.732118546962738,
      "learning_rate": 5.2965828769582225e-06,
      "loss": 1.1452,
      "step": 1451
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.6332452297210693,
      "learning_rate": 5.082783269448443e-06,
      "loss": 1.1254,
      "step": 1452
    },
    {
      "epoch": 2.906,
      "grad_norm": 0.7130746841430664,
      "learning_rate": 4.873377096342058e-06,
      "loss": 1.1247,
      "step": 1453
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.6948849558830261,
      "learning_rate": 4.668365282351372e-06,
      "loss": 1.1196,
      "step": 1454
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.5575879216194153,
      "learning_rate": 4.467748732783994e-06,
      "loss": 1.1283,
      "step": 1455
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.8460903763771057,
      "learning_rate": 4.271528333538388e-06,
      "loss": 1.1877,
      "step": 1456
    },
    {
      "epoch": 2.914,
      "grad_norm": 0.9577768445014954,
      "learning_rate": 4.079704951100105e-06,
      "loss": 1.0543,
      "step": 1457
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.7064356803894043,
      "learning_rate": 3.892279432538115e-06,
      "loss": 1.1348,
      "step": 1458
    },
    {
      "epoch": 2.918,
      "grad_norm": 0.7258855104446411,
      "learning_rate": 3.7092526055008126e-06,
      "loss": 1.1667,
      "step": 1459
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.7106546759605408,
      "learning_rate": 3.5306252782126846e-06,
      "loss": 1.1296,
      "step": 1460
    },
    {
      "epoch": 2.922,
      "grad_norm": 1.5663115978240967,
      "learning_rate": 3.3563982394704262e-06,
      "loss": 1.1795,
      "step": 1461
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.9657493233680725,
      "learning_rate": 3.1865722586397196e-06,
      "loss": 1.2408,
      "step": 1462
    },
    {
      "epoch": 2.926,
      "grad_norm": 0.6003562211990356,
      "learning_rate": 3.0211480856513484e-06,
      "loss": 1.1528,
      "step": 1463
    },
    {
      "epoch": 2.928,
      "grad_norm": 1.1179860830307007,
      "learning_rate": 2.860126450998646e-06,
      "loss": 1.2068,
      "step": 1464
    },
    {
      "epoch": 2.93,
      "grad_norm": 1.0323444604873657,
      "learning_rate": 2.7035080657338287e-06,
      "loss": 1.1822,
      "step": 1465
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.5777507424354553,
      "learning_rate": 2.5512936214646675e-06,
      "loss": 1.1221,
      "step": 1466
    },
    {
      "epoch": 2.934,
      "grad_norm": 0.854200005531311,
      "learning_rate": 2.403483790351824e-06,
      "loss": 1.1476,
      "step": 1467
    },
    {
      "epoch": 2.936,
      "grad_norm": 1.024603009223938,
      "learning_rate": 2.260079225105627e-06,
      "loss": 1.1415,
      "step": 1468
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 0.7916037440299988,
      "learning_rate": 2.1210805589834128e-06,
      "loss": 1.1817,
      "step": 1469
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.7348964214324951,
      "learning_rate": 1.986488405786524e-06,
      "loss": 1.2178,
      "step": 1470
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.6546357870101929,
      "learning_rate": 1.8563033598575363e-06,
      "loss": 1.223,
      "step": 1471
    },
    {
      "epoch": 2.944,
      "grad_norm": 3.553351640701294,
      "learning_rate": 1.7305259960781471e-06,
      "loss": 1.1742,
      "step": 1472
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 1.0078611373901367,
      "learning_rate": 1.6091568698658465e-06,
      "loss": 1.2014,
      "step": 1473
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.7355649471282959,
      "learning_rate": 1.4921965171720286e-06,
      "loss": 1.1466,
      "step": 1474
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.846461832523346,
      "learning_rate": 1.379645454479661e-06,
      "loss": 1.1856,
      "step": 1475
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.7628452777862549,
      "learning_rate": 1.2715041788003978e-06,
      "loss": 1.1743,
      "step": 1476
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 0.649842381477356,
      "learning_rate": 1.1677731676733582e-06,
      "loss": 1.1431,
      "step": 1477
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.8774417042732239,
      "learning_rate": 1.0684528791621296e-06,
      "loss": 1.0199,
      "step": 1478
    },
    {
      "epoch": 2.958,
      "grad_norm": 2.2225441932678223,
      "learning_rate": 9.735437518528788e-07,
      "loss": 1.1591,
      "step": 1479
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.0392374992370605,
      "learning_rate": 8.830462048531329e-07,
      "loss": 1.248,
      "step": 1480
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 0.919577956199646,
      "learning_rate": 7.969606377890015e-07,
      "loss": 1.1361,
      "step": 1481
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.735273003578186,
      "learning_rate": 7.15287430803957e-07,
      "loss": 1.0976,
      "step": 1482
    },
    {
      "epoch": 2.966,
      "grad_norm": 0.6329562067985535,
      "learning_rate": 6.380269445571685e-07,
      "loss": 1.1432,
      "step": 1483
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.9103863835334778,
      "learning_rate": 5.651795202213927e-07,
      "loss": 1.1865,
      "step": 1484
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.6425986886024475,
      "learning_rate": 4.967454794823078e-07,
      "loss": 1.1343,
      "step": 1485
    },
    {
      "epoch": 2.972,
      "grad_norm": 1.6602259874343872,
      "learning_rate": 4.3272512453618184e-07,
      "loss": 1.1156,
      "step": 1486
    },
    {
      "epoch": 2.974,
      "grad_norm": 0.9613580703735352,
      "learning_rate": 3.7311873808931753e-07,
      "loss": 1.1746,
      "step": 1487
    },
    {
      "epoch": 2.976,
      "grad_norm": 1.0959935188293457,
      "learning_rate": 3.179265833562761e-07,
      "loss": 1.141,
      "step": 1488
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 0.7517024874687195,
      "learning_rate": 2.671489040589892e-07,
      "loss": 1.1198,
      "step": 1489
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.7404323816299438,
      "learning_rate": 2.2078592442553725e-07,
      "loss": 1.0939,
      "step": 1490
    },
    {
      "epoch": 2.982,
      "grad_norm": 1.3180049657821655,
      "learning_rate": 1.788378491891507e-07,
      "loss": 1.3143,
      "step": 1491
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.7441008687019348,
      "learning_rate": 1.413048635876546e-07,
      "loss": 1.2151,
      "step": 1492
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 0.5124862790107727,
      "learning_rate": 1.0818713336202546e-07,
      "loss": 1.1712,
      "step": 1493
    },
    {
      "epoch": 2.988,
      "grad_norm": 1.3090676069259644,
      "learning_rate": 7.948480475616915e-08,
      "loss": 1.2475,
      "step": 1494
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.714987576007843,
      "learning_rate": 5.5198004516254786e-08,
      "loss": 1.1672,
      "step": 1495
    },
    {
      "epoch": 2.992,
      "grad_norm": 2.2725274562835693,
      "learning_rate": 3.532683988971552e-08,
      "loss": 1.1098,
      "step": 1496
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 0.7502026557922363,
      "learning_rate": 1.987139862524856e-08,
      "loss": 1.1846,
      "step": 1497
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.8905120491981506,
      "learning_rate": 8.831748972371045e-09,
      "loss": 1.1747,
      "step": 1498
    },
    {
      "epoch": 2.998,
      "grad_norm": 0.8033991456031799,
      "learning_rate": 2.2079396805319007e-09,
      "loss": 1.0997,
      "step": 1499
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.6741493940353394,
      "learning_rate": 0.0,
      "loss": 1.1053,
      "step": 1500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.1555155515670776,
      "eval_runtime": 224.04,
      "eval_samples_per_second": 0.446,
      "eval_steps_per_second": 0.446,
      "step": 1500
    },
    {
      "epoch": 1.501,
      "grad_norm": 0.8133410215377808,
      "learning_rate": 0.0013842267553958371,
      "loss": 1.2356,
      "step": 1501
    },
    {
      "epoch": 1.502,
      "grad_norm": 0.6617940068244934,
      "learning_rate": 0.001383500619123451,
      "loss": 1.1462,
      "step": 1502
    },
    {
      "epoch": 1.5030000000000001,
      "grad_norm": 2.3453407287597656,
      "learning_rate": 0.0013827742456960976,
      "loss": 1.4405,
      "step": 1503
    },
    {
      "epoch": 1.504,
      "grad_norm": 2.8662374019622803,
      "learning_rate": 0.0013820476355629631,
      "loss": 1.2603,
      "step": 1504
    },
    {
      "epoch": 1.505,
      "grad_norm": 1.5953236818313599,
      "learning_rate": 0.00138132078917338,
      "loss": 1.2857,
      "step": 1505
    },
    {
      "epoch": 1.506,
      "grad_norm": 1.7661161422729492,
      "learning_rate": 0.001380593706976826,
      "loss": 1.1834,
      "step": 1506
    },
    {
      "epoch": 1.5070000000000001,
      "grad_norm": 2.572188138961792,
      "learning_rate": 0.0013798663894229258,
      "loss": 1.3632,
      "step": 1507
    },
    {
      "epoch": 1.508,
      "grad_norm": 5.48185396194458,
      "learning_rate": 0.0013791388369614487,
      "loss": 1.3646,
      "step": 1508
    },
    {
      "epoch": 1.509,
      "grad_norm": 2.005331039428711,
      "learning_rate": 0.0013784110500423103,
      "loss": 1.4138,
      "step": 1509
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.275559902191162,
      "learning_rate": 0.0013776830291155702,
      "loss": 1.4265,
      "step": 1510
    },
    {
      "epoch": 1.5110000000000001,
      "grad_norm": 1.4847660064697266,
      "learning_rate": 0.001376954774631433,
      "loss": 1.3283,
      "step": 1511
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.387174367904663,
      "learning_rate": 0.001376226287040248,
      "loss": 1.3209,
      "step": 1512
    },
    {
      "epoch": 1.513,
      "grad_norm": 1.4440841674804688,
      "learning_rate": 0.0013754975667925088,
      "loss": 1.2498,
      "step": 1513
    },
    {
      "epoch": 1.514,
      "grad_norm": 1.1633447408676147,
      "learning_rate": 0.0013747686143388519,
      "loss": 1.4318,
      "step": 1514
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.7884296178817749,
      "learning_rate": 0.0013740394301300584,
      "loss": 1.3055,
      "step": 1515
    },
    {
      "epoch": 1.516,
      "grad_norm": 1.0449048280715942,
      "learning_rate": 0.0013733100146170521,
      "loss": 1.2938,
      "step": 1516
    },
    {
      "epoch": 1.517,
      "grad_norm": 1.4566857814788818,
      "learning_rate": 0.0013725803682509005,
      "loss": 1.4398,
      "step": 1517
    },
    {
      "epoch": 1.518,
      "grad_norm": 1.3273966312408447,
      "learning_rate": 0.0013718504914828134,
      "loss": 1.3495,
      "step": 1518
    },
    {
      "epoch": 1.5190000000000001,
      "grad_norm": 1.7001383304595947,
      "learning_rate": 0.0013711203847641428,
      "loss": 1.3766,
      "step": 1519
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.3257654905319214,
      "learning_rate": 0.0013703900485463835,
      "loss": 1.4103,
      "step": 1520
    },
    {
      "epoch": 1.521,
      "grad_norm": 0.9972306489944458,
      "learning_rate": 0.0013696594832811717,
      "loss": 1.3438,
      "step": 1521
    },
    {
      "epoch": 1.522,
      "grad_norm": 1.0351508855819702,
      "learning_rate": 0.0013689286894202862,
      "loss": 1.4426,
      "step": 1522
    },
    {
      "epoch": 1.5230000000000001,
      "grad_norm": 1.594399333000183,
      "learning_rate": 0.0013681976674156456,
      "loss": 1.3435,
      "step": 1523
    },
    {
      "epoch": 1.524,
      "grad_norm": 1.3165593147277832,
      "learning_rate": 0.0013674664177193107,
      "loss": 1.2924,
      "step": 1524
    },
    {
      "epoch": 1.525,
      "grad_norm": 4.527373313903809,
      "learning_rate": 0.0013667349407834833,
      "loss": 1.4128,
      "step": 1525
    },
    {
      "epoch": 1.526,
      "grad_norm": 2.724705696105957,
      "learning_rate": 0.0013660032370605047,
      "loss": 1.3638,
      "step": 1526
    },
    {
      "epoch": 1.5270000000000001,
      "grad_norm": 3.662099838256836,
      "learning_rate": 0.0013652713070028573,
      "loss": 1.287,
      "step": 1527
    },
    {
      "epoch": 1.528,
      "grad_norm": 3.194838285446167,
      "learning_rate": 0.0013645391510631632,
      "loss": 1.5526,
      "step": 1528
    },
    {
      "epoch": 1.529,
      "grad_norm": 2.8401806354522705,
      "learning_rate": 0.0013638067696941838,
      "loss": 1.4718,
      "step": 1529
    },
    {
      "epoch": 1.53,
      "grad_norm": 1.3552093505859375,
      "learning_rate": 0.0013630741633488212,
      "loss": 1.4234,
      "step": 1530
    },
    {
      "epoch": 1.5310000000000001,
      "grad_norm": 1.331689715385437,
      "learning_rate": 0.0013623413324801148,
      "loss": 1.2516,
      "step": 1531
    },
    {
      "epoch": 1.532,
      "grad_norm": 1.8251910209655762,
      "learning_rate": 0.0013616082775412437,
      "loss": 1.3436,
      "step": 1532
    },
    {
      "epoch": 1.533,
      "grad_norm": 1.5365194082260132,
      "learning_rate": 0.001360874998985526,
      "loss": 1.4484,
      "step": 1533
    },
    {
      "epoch": 1.534,
      "grad_norm": 1.462229609489441,
      "learning_rate": 0.001360141497266418,
      "loss": 1.3019,
      "step": 1534
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 1.0574485063552856,
      "learning_rate": 0.0013594077728375127,
      "loss": 1.4144,
      "step": 1535
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.837862014770508,
      "learning_rate": 0.0013586738261525428,
      "loss": 1.4461,
      "step": 1536
    },
    {
      "epoch": 1.537,
      "grad_norm": 1.3071051836013794,
      "learning_rate": 0.001357939657665377,
      "loss": 1.4693,
      "step": 1537
    },
    {
      "epoch": 1.538,
      "grad_norm": 1.6273024082183838,
      "learning_rate": 0.0013572052678300218,
      "loss": 1.2732,
      "step": 1538
    },
    {
      "epoch": 1.5390000000000001,
      "grad_norm": 2.3100295066833496,
      "learning_rate": 0.00135647065710062,
      "loss": 1.432,
      "step": 1539
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.3386813402175903,
      "learning_rate": 0.0013557358259314518,
      "loss": 1.337,
      "step": 1540
    },
    {
      "epoch": 1.541,
      "grad_norm": 1.696024775505066,
      "learning_rate": 0.0013550007747769332,
      "loss": 1.3368,
      "step": 1541
    },
    {
      "epoch": 1.542,
      "grad_norm": 3.467660427093506,
      "learning_rate": 0.0013542655040916162,
      "loss": 1.2876,
      "step": 1542
    },
    {
      "epoch": 1.5430000000000001,
      "grad_norm": 2.164419412612915,
      "learning_rate": 0.001353530014330189,
      "loss": 1.3249,
      "step": 1543
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.239678144454956,
      "learning_rate": 0.0013527943059474749,
      "loss": 1.2718,
      "step": 1544
    },
    {
      "epoch": 1.545,
      "grad_norm": 3.1091604232788086,
      "learning_rate": 0.0013520583793984323,
      "loss": 1.3995,
      "step": 1545
    },
    {
      "epoch": 1.546,
      "grad_norm": 1.9072716236114502,
      "learning_rate": 0.0013513222351381547,
      "loss": 1.3616,
      "step": 1546
    },
    {
      "epoch": 1.5470000000000002,
      "grad_norm": 1.8505897521972656,
      "learning_rate": 0.0013505858736218704,
      "loss": 1.459,
      "step": 1547
    },
    {
      "epoch": 1.548,
      "grad_norm": 2.194211959838867,
      "learning_rate": 0.001349849295304942,
      "loss": 1.4668,
      "step": 1548
    },
    {
      "epoch": 1.549,
      "grad_norm": 2.380948066711426,
      "learning_rate": 0.0013491125006428657,
      "loss": 1.3652,
      "step": 1549
    },
    {
      "epoch": 1.55,
      "grad_norm": 2.246948003768921,
      "learning_rate": 0.001348375490091272,
      "loss": 1.4033,
      "step": 1550
    },
    {
      "epoch": 1.5510000000000002,
      "grad_norm": 1.165330171585083,
      "learning_rate": 0.0013476382641059244,
      "loss": 1.3541,
      "step": 1551
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.9233968257904053,
      "learning_rate": 0.0013469008231427207,
      "loss": 1.501,
      "step": 1552
    },
    {
      "epoch": 1.553,
      "grad_norm": 2.422292470932007,
      "learning_rate": 0.0013461631676576903,
      "loss": 1.239,
      "step": 1553
    },
    {
      "epoch": 1.554,
      "grad_norm": 1.8645907640457153,
      "learning_rate": 0.001345425298106996,
      "loss": 1.4118,
      "step": 1554
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 1.5084692239761353,
      "learning_rate": 0.0013446872149469329,
      "loss": 1.4074,
      "step": 1555
    },
    {
      "epoch": 1.556,
      "grad_norm": 1.038651943206787,
      "learning_rate": 0.0013439489186339282,
      "loss": 1.3391,
      "step": 1556
    },
    {
      "epoch": 1.557,
      "grad_norm": 1.8026055097579956,
      "learning_rate": 0.0013432104096245407,
      "loss": 1.359,
      "step": 1557
    },
    {
      "epoch": 1.558,
      "grad_norm": 1.5985759496688843,
      "learning_rate": 0.001342471688375461,
      "loss": 1.4839,
      "step": 1558
    },
    {
      "epoch": 1.5590000000000002,
      "grad_norm": 0.9540688395500183,
      "learning_rate": 0.0013417327553435104,
      "loss": 1.4771,
      "step": 1559
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.1222068071365356,
      "learning_rate": 0.0013409936109856422,
      "loss": 1.3361,
      "step": 1560
    },
    {
      "epoch": 1.561,
      "grad_norm": 0.8232514262199402,
      "learning_rate": 0.0013402542557589396,
      "loss": 1.3672,
      "step": 1561
    },
    {
      "epoch": 1.562,
      "grad_norm": 1.8682128190994263,
      "learning_rate": 0.001339514690120616,
      "loss": 1.3415,
      "step": 1562
    },
    {
      "epoch": 1.563,
      "grad_norm": 0.8800725340843201,
      "learning_rate": 0.0013387749145280159,
      "loss": 1.2621,
      "step": 1563
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.853518545627594,
      "learning_rate": 0.0013380349294386125,
      "loss": 1.2274,
      "step": 1564
    },
    {
      "epoch": 1.565,
      "grad_norm": 1.8346112966537476,
      "learning_rate": 0.0013372947353100092,
      "loss": 1.4451,
      "step": 1565
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 0.9486215710639954,
      "learning_rate": 0.0013365543325999387,
      "loss": 1.4269,
      "step": 1566
    },
    {
      "epoch": 1.567,
      "grad_norm": 1.5637927055358887,
      "learning_rate": 0.0013358137217662626,
      "loss": 1.3428,
      "step": 1567
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.5138124227523804,
      "learning_rate": 0.0013350729032669704,
      "loss": 1.3335,
      "step": 1568
    },
    {
      "epoch": 1.569,
      "grad_norm": 3.68990421295166,
      "learning_rate": 0.0013343318775601819,
      "loss": 1.414,
      "step": 1569
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 1.1938157081604004,
      "learning_rate": 0.001333590645104143,
      "loss": 1.3964,
      "step": 1570
    },
    {
      "epoch": 1.571,
      "grad_norm": 8.924056053161621,
      "learning_rate": 0.001332849206357229,
      "loss": 1.3772,
      "step": 1571
    },
    {
      "epoch": 1.572,
      "grad_norm": 3.133645534515381,
      "learning_rate": 0.0013321075617779411,
      "loss": 1.5165,
      "step": 1572
    },
    {
      "epoch": 1.573,
      "grad_norm": 2.085651159286499,
      "learning_rate": 0.0013313657118249104,
      "loss": 1.4023,
      "step": 1573
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 1.7382588386535645,
      "learning_rate": 0.0013306236569568916,
      "loss": 1.432,
      "step": 1574
    },
    {
      "epoch": 1.575,
      "grad_norm": 7.597526550292969,
      "learning_rate": 0.0013298813976327694,
      "loss": 1.3641,
      "step": 1575
    },
    {
      "epoch": 1.576,
      "grad_norm": 3.4429268836975098,
      "learning_rate": 0.0013291389343115525,
      "loss": 1.4809,
      "step": 1576
    },
    {
      "epoch": 1.577,
      "grad_norm": 2.0054945945739746,
      "learning_rate": 0.0013283962674523768,
      "loss": 1.4148,
      "step": 1577
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 1.4585071802139282,
      "learning_rate": 0.0013276533975145046,
      "loss": 1.4549,
      "step": 1578
    },
    {
      "epoch": 1.579,
      "grad_norm": 2.7654526233673096,
      "learning_rate": 0.0013269103249573227,
      "loss": 1.3768,
      "step": 1579
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.7047297954559326,
      "learning_rate": 0.0013261670502403436,
      "loss": 1.37,
      "step": 1580
    },
    {
      "epoch": 1.581,
      "grad_norm": 1.3921886682510376,
      "learning_rate": 0.001325423573823205,
      "loss": 1.4386,
      "step": 1581
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 2.7423152923583984,
      "learning_rate": 0.0013246798961656693,
      "loss": 1.4947,
      "step": 1582
    },
    {
      "epoch": 1.583,
      "grad_norm": 3.2414565086364746,
      "learning_rate": 0.0013239360177276227,
      "loss": 1.3863,
      "step": 1583
    },
    {
      "epoch": 1.584,
      "grad_norm": 1.790519118309021,
      "learning_rate": 0.001323191938969077,
      "loss": 1.4659,
      "step": 1584
    },
    {
      "epoch": 1.585,
      "grad_norm": 2.560166358947754,
      "learning_rate": 0.0013224476603501663,
      "loss": 1.3764,
      "step": 1585
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 2.452143669128418,
      "learning_rate": 0.0013217031823311487,
      "loss": 1.5147,
      "step": 1586
    },
    {
      "epoch": 1.587,
      "grad_norm": 3.2104477882385254,
      "learning_rate": 0.0013209585053724072,
      "loss": 1.5761,
      "step": 1587
    },
    {
      "epoch": 1.588,
      "grad_norm": 2.318929672241211,
      "learning_rate": 0.001320213629934445,
      "loss": 1.4004,
      "step": 1588
    },
    {
      "epoch": 1.589,
      "grad_norm": 14.460332870483398,
      "learning_rate": 0.0013194685564778905,
      "loss": 1.4148,
      "step": 1589
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 54.08332443237305,
      "learning_rate": 0.0013187232854634933,
      "loss": 1.2641,
      "step": 1590
    },
    {
      "epoch": 1.591,
      "grad_norm": 37.941673278808594,
      "learning_rate": 0.0013179778173521253,
      "loss": 1.4916,
      "step": 1591
    },
    {
      "epoch": 1.592,
      "grad_norm": 8.581616401672363,
      "learning_rate": 0.0013172321526047806,
      "loss": 1.597,
      "step": 1592
    },
    {
      "epoch": 1.593,
      "grad_norm": 20.353742599487305,
      "learning_rate": 0.001316486291682575,
      "loss": 1.5798,
      "step": 1593
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 21.160390853881836,
      "learning_rate": 0.0013157402350467448,
      "loss": 1.6176,
      "step": 1594
    },
    {
      "epoch": 1.595,
      "grad_norm": 23.064008712768555,
      "learning_rate": 0.0013149939831586484,
      "loss": 1.5376,
      "step": 1595
    },
    {
      "epoch": 1.596,
      "grad_norm": 27.977338790893555,
      "learning_rate": 0.0013142475364797644,
      "loss": 1.5948,
      "step": 1596
    },
    {
      "epoch": 1.597,
      "grad_norm": 5.446643829345703,
      "learning_rate": 0.0013135008954716916,
      "loss": 1.5249,
      "step": 1597
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 31.170053482055664,
      "learning_rate": 0.0013127540605961492,
      "loss": 1.6235,
      "step": 1598
    },
    {
      "epoch": 1.599,
      "grad_norm": 23.808481216430664,
      "learning_rate": 0.0013120070323149767,
      "loss": 2.1324,
      "step": 1599
    },
    {
      "epoch": 1.6,
      "grad_norm": 8.979668617248535,
      "learning_rate": 0.0013112598110901327,
      "loss": 1.6629,
      "step": 1600
    },
    {
      "epoch": 1.6,
      "eval_loss": 1.5831520557403564,
      "eval_runtime": 225.6001,
      "eval_samples_per_second": 0.443,
      "eval_steps_per_second": 0.443,
      "step": 1600
    },
    {
      "epoch": 1.601,
      "grad_norm": 99.63268280029297,
      "learning_rate": 0.0013105123973836954,
      "loss": 1.6497,
      "step": 1601
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 14.37511920928955,
      "learning_rate": 0.0013097647916578618,
      "loss": 1.7323,
      "step": 1602
    },
    {
      "epoch": 1.603,
      "grad_norm": 12.515687942504883,
      "learning_rate": 0.0013090169943749475,
      "loss": 1.7369,
      "step": 1603
    },
    {
      "epoch": 1.604,
      "grad_norm": 12.225214004516602,
      "learning_rate": 0.001308269005997387,
      "loss": 1.6949,
      "step": 1604
    },
    {
      "epoch": 1.605,
      "grad_norm": 56.12985610961914,
      "learning_rate": 0.001307520826987733,
      "loss": 2.032,
      "step": 1605
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 55.232845306396484,
      "learning_rate": 0.0013067724578086557,
      "loss": 2.2346,
      "step": 1606
    },
    {
      "epoch": 1.607,
      "grad_norm": 31.583345413208008,
      "learning_rate": 0.0013060238989229425,
      "loss": 1.7821,
      "step": 1607
    },
    {
      "epoch": 1.608,
      "grad_norm": 61.106441497802734,
      "learning_rate": 0.0013052751507934997,
      "loss": 2.3537,
      "step": 1608
    },
    {
      "epoch": 1.609,
      "grad_norm": 41.946205139160156,
      "learning_rate": 0.0013045262138833488,
      "loss": 1.5846,
      "step": 1609
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 13.061322212219238,
      "learning_rate": 0.0013037770886556292,
      "loss": 1.8167,
      "step": 1610
    },
    {
      "epoch": 1.611,
      "grad_norm": 26.806888580322266,
      "learning_rate": 0.0013030277755735965,
      "loss": 1.7043,
      "step": 1611
    },
    {
      "epoch": 1.612,
      "grad_norm": 38.20295715332031,
      "learning_rate": 0.0013022782751006218,
      "loss": 1.7454,
      "step": 1612
    },
    {
      "epoch": 1.613,
      "grad_norm": 79.63111114501953,
      "learning_rate": 0.001301528587700193,
      "loss": 1.7128,
      "step": 1613
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 15.6775541305542,
      "learning_rate": 0.0013007787138359136,
      "loss": 1.8516,
      "step": 1614
    },
    {
      "epoch": 1.615,
      "grad_norm": 7.115856170654297,
      "learning_rate": 0.0013000286539715015,
      "loss": 2.0461,
      "step": 1615
    },
    {
      "epoch": 1.616,
      "grad_norm": 5.742143154144287,
      "learning_rate": 0.00129927840857079,
      "loss": 1.9296,
      "step": 1616
    },
    {
      "epoch": 1.617,
      "grad_norm": 15.487201690673828,
      "learning_rate": 0.001298527978097728,
      "loss": 1.9617,
      "step": 1617
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 8.427897453308105,
      "learning_rate": 0.0012977773630163778,
      "loss": 2.1209,
      "step": 1618
    },
    {
      "epoch": 1.619,
      "grad_norm": 3.6233420372009277,
      "learning_rate": 0.0012970265637909165,
      "loss": 1.9061,
      "step": 1619
    },
    {
      "epoch": 1.62,
      "grad_norm": 10.391929626464844,
      "learning_rate": 0.001296275580885634,
      "loss": 1.8879,
      "step": 1620
    },
    {
      "epoch": 1.621,
      "grad_norm": 4.500280857086182,
      "learning_rate": 0.0012955244147649354,
      "loss": 1.9757,
      "step": 1621
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 1.8226414918899536,
      "learning_rate": 0.0012947730658933378,
      "loss": 1.8652,
      "step": 1622
    },
    {
      "epoch": 1.623,
      "grad_norm": 64.99397277832031,
      "learning_rate": 0.0012940215347354723,
      "loss": 1.7527,
      "step": 1623
    },
    {
      "epoch": 1.624,
      "grad_norm": 3.7382407188415527,
      "learning_rate": 0.0012932698217560815,
      "loss": 1.8237,
      "step": 1624
    },
    {
      "epoch": 1.625,
      "grad_norm": 3.491297960281372,
      "learning_rate": 0.0012925179274200214,
      "loss": 1.5813,
      "step": 1625
    },
    {
      "epoch": 1.626,
      "grad_norm": 2.0993175506591797,
      "learning_rate": 0.0012917658521922602,
      "loss": 1.5589,
      "step": 1626
    },
    {
      "epoch": 1.627,
      "grad_norm": 2.5703868865966797,
      "learning_rate": 0.0012910135965378776,
      "loss": 1.5971,
      "step": 1627
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 18.810274124145508,
      "learning_rate": 0.0012902611609220645,
      "loss": 1.6256,
      "step": 1628
    },
    {
      "epoch": 1.629,
      "grad_norm": 3.109557867050171,
      "learning_rate": 0.0012895085458101239,
      "loss": 1.6533,
      "step": 1629
    },
    {
      "epoch": 1.63,
      "grad_norm": 2.582906723022461,
      "learning_rate": 0.0012887557516674695,
      "loss": 1.5005,
      "step": 1630
    },
    {
      "epoch": 1.631,
      "grad_norm": 2.005925416946411,
      "learning_rate": 0.0012880027789596254,
      "loss": 1.7378,
      "step": 1631
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 3.1285176277160645,
      "learning_rate": 0.0012872496281522263,
      "loss": 1.5563,
      "step": 1632
    },
    {
      "epoch": 1.633,
      "grad_norm": 38.813209533691406,
      "learning_rate": 0.0012864962997110173,
      "loss": 1.5886,
      "step": 1633
    },
    {
      "epoch": 1.634,
      "grad_norm": 8.667654991149902,
      "learning_rate": 0.001285742794101853,
      "loss": 1.7724,
      "step": 1634
    },
    {
      "epoch": 1.635,
      "grad_norm": 1.5273585319519043,
      "learning_rate": 0.0012849891117906978,
      "loss": 1.5044,
      "step": 1635
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 3.389345645904541,
      "learning_rate": 0.0012842352532436254,
      "loss": 1.6102,
      "step": 1636
    },
    {
      "epoch": 1.637,
      "grad_norm": 1.1431598663330078,
      "learning_rate": 0.0012834812189268178,
      "loss": 1.4613,
      "step": 1637
    },
    {
      "epoch": 1.638,
      "grad_norm": 4.826280117034912,
      "learning_rate": 0.001282727009306567,
      "loss": 1.4701,
      "step": 1638
    },
    {
      "epoch": 1.639,
      "grad_norm": 1.796424150466919,
      "learning_rate": 0.0012819726248492723,
      "loss": 1.6355,
      "step": 1639
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.4390616416931152,
      "learning_rate": 0.0012812180660214411,
      "loss": 1.4433,
      "step": 1640
    },
    {
      "epoch": 1.641,
      "grad_norm": 3.6475870609283447,
      "learning_rate": 0.00128046333328969,
      "loss": 1.4875,
      "step": 1641
    },
    {
      "epoch": 1.642,
      "grad_norm": 2.2089807987213135,
      "learning_rate": 0.0012797084271207412,
      "loss": 1.492,
      "step": 1642
    },
    {
      "epoch": 1.643,
      "grad_norm": 2.5002379417419434,
      "learning_rate": 0.0012789533479814251,
      "loss": 1.442,
      "step": 1643
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 1.5737745761871338,
      "learning_rate": 0.0012781980963386798,
      "loss": 1.6491,
      "step": 1644
    },
    {
      "epoch": 1.645,
      "grad_norm": 2.017683982849121,
      "learning_rate": 0.001277442672659549,
      "loss": 1.5954,
      "step": 1645
    },
    {
      "epoch": 1.646,
      "grad_norm": 3.2848258018493652,
      "learning_rate": 0.0012766870774111828,
      "loss": 1.3829,
      "step": 1646
    },
    {
      "epoch": 1.647,
      "grad_norm": 3.1538405418395996,
      "learning_rate": 0.001275931311060838,
      "loss": 1.6665,
      "step": 1647
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.8784143924713135,
      "learning_rate": 0.0012751753740758772,
      "loss": 1.37,
      "step": 1648
    },
    {
      "epoch": 1.649,
      "grad_norm": 1.845929503440857,
      "learning_rate": 0.0012744192669237678,
      "loss": 1.3709,
      "step": 1649
    },
    {
      "epoch": 1.65,
      "grad_norm": 3.4564151763916016,
      "learning_rate": 0.0012736629900720832,
      "loss": 1.5665,
      "step": 1650
    },
    {
      "epoch": 1.651,
      "grad_norm": 3.1717841625213623,
      "learning_rate": 0.0012729065439885008,
      "loss": 1.472,
      "step": 1651
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 2.5115177631378174,
      "learning_rate": 0.001272149929140804,
      "loss": 1.4977,
      "step": 1652
    },
    {
      "epoch": 1.653,
      "grad_norm": 4.540896415710449,
      "learning_rate": 0.0012713931459968797,
      "loss": 1.5857,
      "step": 1653
    },
    {
      "epoch": 1.654,
      "grad_norm": 5.461775302886963,
      "learning_rate": 0.001270636195024719,
      "loss": 1.6919,
      "step": 1654
    },
    {
      "epoch": 1.655,
      "grad_norm": 32.64865493774414,
      "learning_rate": 0.0012698790766924165,
      "loss": 1.6409,
      "step": 1655
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 4.4767632484436035,
      "learning_rate": 0.001269121791468171,
      "loss": 1.7436,
      "step": 1656
    },
    {
      "epoch": 1.657,
      "grad_norm": 10.761202812194824,
      "learning_rate": 0.0012683643398202838,
      "loss": 1.7989,
      "step": 1657
    },
    {
      "epoch": 1.658,
      "grad_norm": 3.5251433849334717,
      "learning_rate": 0.0012676067222171595,
      "loss": 1.5409,
      "step": 1658
    },
    {
      "epoch": 1.659,
      "grad_norm": 17.24062156677246,
      "learning_rate": 0.0012668489391273053,
      "loss": 1.5992,
      "step": 1659
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 4.515651702880859,
      "learning_rate": 0.0012660909910193303,
      "loss": 1.5986,
      "step": 1660
    },
    {
      "epoch": 1.661,
      "grad_norm": 4.558638095855713,
      "learning_rate": 0.0012653328783619466,
      "loss": 1.7148,
      "step": 1661
    },
    {
      "epoch": 1.662,
      "grad_norm": 418.7220458984375,
      "learning_rate": 0.0012645746016239673,
      "loss": 1.7305,
      "step": 1662
    },
    {
      "epoch": 1.663,
      "grad_norm": 21.727540969848633,
      "learning_rate": 0.001263816161274307,
      "loss": 1.8606,
      "step": 1663
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 3.3352036476135254,
      "learning_rate": 0.0012630575577819813,
      "loss": 1.62,
      "step": 1664
    },
    {
      "epoch": 1.665,
      "grad_norm": 37.06242370605469,
      "learning_rate": 0.001262298791616108,
      "loss": 1.6453,
      "step": 1665
    },
    {
      "epoch": 1.666,
      "grad_norm": 13.741649627685547,
      "learning_rate": 0.0012615398632459037,
      "loss": 1.6197,
      "step": 1666
    },
    {
      "epoch": 1.667,
      "grad_norm": 85.21553802490234,
      "learning_rate": 0.0012607807731406862,
      "loss": 1.4887,
      "step": 1667
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 20.75127410888672,
      "learning_rate": 0.0012600215217698737,
      "loss": 1.73,
      "step": 1668
    },
    {
      "epoch": 1.669,
      "grad_norm": 9.689576148986816,
      "learning_rate": 0.0012592621096029829,
      "loss": 1.6177,
      "step": 1669
    },
    {
      "epoch": 1.67,
      "grad_norm": 9.342446327209473,
      "learning_rate": 0.0012585025371096307,
      "loss": 1.6929,
      "step": 1670
    },
    {
      "epoch": 1.671,
      "grad_norm": 7.2074174880981445,
      "learning_rate": 0.0012577428047595342,
      "loss": 1.8389,
      "step": 1671
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 17.09326171875,
      "learning_rate": 0.0012569829130225072,
      "loss": 1.9016,
      "step": 1672
    },
    {
      "epoch": 1.673,
      "grad_norm": 41.8323860168457,
      "learning_rate": 0.0012562228623684637,
      "loss": 1.7912,
      "step": 1673
    },
    {
      "epoch": 1.674,
      "grad_norm": 19.464069366455078,
      "learning_rate": 0.0012554626532674154,
      "loss": 1.8324,
      "step": 1674
    },
    {
      "epoch": 1.675,
      "grad_norm": 19.801382064819336,
      "learning_rate": 0.001254702286189472,
      "loss": 1.9647,
      "step": 1675
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 55.39860153198242,
      "learning_rate": 0.0012539417616048403,
      "loss": 1.7975,
      "step": 1676
    },
    {
      "epoch": 1.677,
      "grad_norm": 55.56830978393555,
      "learning_rate": 0.001253181079983826,
      "loss": 2.094,
      "step": 1677
    },
    {
      "epoch": 1.678,
      "grad_norm": 25.663455963134766,
      "learning_rate": 0.0012524202417968304,
      "loss": 1.8509,
      "step": 1678
    },
    {
      "epoch": 1.679,
      "grad_norm": 125.12039184570312,
      "learning_rate": 0.0012516592475143521,
      "loss": 1.784,
      "step": 1679
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.7736990451812744,
      "learning_rate": 0.0012508980976069875,
      "loss": 1.7021,
      "step": 1680
    },
    {
      "epoch": 1.681,
      "grad_norm": 3.083232879638672,
      "learning_rate": 0.0012501367925454266,
      "loss": 1.7066,
      "step": 1681
    },
    {
      "epoch": 1.682,
      "grad_norm": 1.626430869102478,
      "learning_rate": 0.0012493753328004578,
      "loss": 1.5916,
      "step": 1682
    },
    {
      "epoch": 1.683,
      "grad_norm": 6.552485466003418,
      "learning_rate": 0.001248613718842964,
      "loss": 1.6785,
      "step": 1683
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 31.878313064575195,
      "learning_rate": 0.0012478519511439235,
      "loss": 1.8454,
      "step": 1684
    },
    {
      "epoch": 1.685,
      "grad_norm": 82.54534149169922,
      "learning_rate": 0.0012470900301744102,
      "loss": 1.7897,
      "step": 1685
    },
    {
      "epoch": 1.686,
      "grad_norm": 35.43092727661133,
      "learning_rate": 0.0012463279564055916,
      "loss": 1.983,
      "step": 1686
    },
    {
      "epoch": 1.687,
      "grad_norm": 11.856733322143555,
      "learning_rate": 0.0012455657303087316,
      "loss": 1.9474,
      "step": 1687
    },
    {
      "epoch": 1.688,
      "grad_norm": 83.39401245117188,
      "learning_rate": 0.0012448033523551865,
      "loss": 1.9852,
      "step": 1688
    },
    {
      "epoch": 1.689,
      "grad_norm": 25.475431442260742,
      "learning_rate": 0.0012440408230164073,
      "loss": 1.9543,
      "step": 1689
    },
    {
      "epoch": 1.69,
      "grad_norm": 29.354530334472656,
      "learning_rate": 0.0012432781427639387,
      "loss": 2.0004,
      "step": 1690
    },
    {
      "epoch": 1.6909999999999998,
      "grad_norm": 80.25934600830078,
      "learning_rate": 0.0012425153120694183,
      "loss": 2.2081,
      "step": 1691
    },
    {
      "epoch": 1.692,
      "grad_norm": 26.90117073059082,
      "learning_rate": 0.0012417523314045777,
      "loss": 2.5796,
      "step": 1692
    },
    {
      "epoch": 1.693,
      "grad_norm": 79.5951919555664,
      "learning_rate": 0.0012409892012412397,
      "loss": 2.5153,
      "step": 1693
    },
    {
      "epoch": 1.694,
      "grad_norm": 95.8365249633789,
      "learning_rate": 0.0012402259220513205,
      "loss": 3.2112,
      "step": 1694
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 299.24102783203125,
      "learning_rate": 0.0012394624943068287,
      "loss": 3.7645,
      "step": 1695
    },
    {
      "epoch": 1.696,
      "grad_norm": 889.02099609375,
      "learning_rate": 0.0012386989184798645,
      "loss": 5.5183,
      "step": 1696
    },
    {
      "epoch": 1.697,
      "grad_norm": 101.28903198242188,
      "learning_rate": 0.0012379351950426187,
      "loss": 5.9757,
      "step": 1697
    },
    {
      "epoch": 1.698,
      "grad_norm": 223.9195098876953,
      "learning_rate": 0.0012371713244673755,
      "loss": 5.6296,
      "step": 1698
    },
    {
      "epoch": 1.6989999999999998,
      "grad_norm": 460.6154479980469,
      "learning_rate": 0.0012364073072265078,
      "loss": 5.2641,
      "step": 1699
    },
    {
      "epoch": 1.7,
      "grad_norm": 142.74156188964844,
      "learning_rate": 0.0012356431437924806,
      "loss": 4.0027,
      "step": 1700
    },
    {
      "epoch": 1.7,
      "eval_loss": 4.945338249206543,
      "eval_runtime": 225.0952,
      "eval_samples_per_second": 0.444,
      "eval_steps_per_second": 0.444,
      "step": 1700
    },
    {
      "epoch": 1.701,
      "grad_norm": 787.7314453125,
      "learning_rate": 0.0012348788346378493,
      "loss": 4.8643,
      "step": 1701
    },
    {
      "epoch": 1.702,
      "grad_norm": 178.2046356201172,
      "learning_rate": 0.0012341143802352586,
      "loss": 6.0293,
      "step": 1702
    },
    {
      "epoch": 1.7029999999999998,
      "grad_norm": 1239.896728515625,
      "learning_rate": 0.0012333497810574437,
      "loss": 5.2566,
      "step": 1703
    },
    {
      "epoch": 1.704,
      "grad_norm": 536.195068359375,
      "learning_rate": 0.0012325850375772289,
      "loss": 7.1902,
      "step": 1704
    },
    {
      "epoch": 1.705,
      "grad_norm": 1078.02001953125,
      "learning_rate": 0.0012318201502675285,
      "loss": 8.6362,
      "step": 1705
    },
    {
      "epoch": 1.706,
      "grad_norm": 541.3268432617188,
      "learning_rate": 0.0012310551196013444,
      "loss": 6.6556,
      "step": 1706
    },
    {
      "epoch": 1.7069999999999999,
      "grad_norm": 231.36483764648438,
      "learning_rate": 0.0012302899460517685,
      "loss": 7.0438,
      "step": 1707
    },
    {
      "epoch": 1.708,
      "grad_norm": 832.8728637695312,
      "learning_rate": 0.0012295246300919802,
      "loss": 6.6319,
      "step": 1708
    },
    {
      "epoch": 1.709,
      "grad_norm": 883.405517578125,
      "learning_rate": 0.0012287591721952475,
      "loss": 6.1636,
      "step": 1709
    },
    {
      "epoch": 1.71,
      "grad_norm": 1912.224853515625,
      "learning_rate": 0.0012279935728349261,
      "loss": 5.4987,
      "step": 1710
    },
    {
      "epoch": 1.7109999999999999,
      "grad_norm": 136.58465576171875,
      "learning_rate": 0.0012272278324844583,
      "loss": 5.2671,
      "step": 1711
    },
    {
      "epoch": 1.712,
      "grad_norm": 283.4630432128906,
      "learning_rate": 0.001226461951617375,
      "loss": 4.8829,
      "step": 1712
    },
    {
      "epoch": 1.713,
      "grad_norm": 685.913330078125,
      "learning_rate": 0.001225695930707293,
      "loss": 5.3168,
      "step": 1713
    },
    {
      "epoch": 1.714,
      "grad_norm": 209.23114013671875,
      "learning_rate": 0.001224929770227916,
      "loss": 5.3018,
      "step": 1714
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 206.27096557617188,
      "learning_rate": 0.0012241634706530345,
      "loss": 5.3581,
      "step": 1715
    },
    {
      "epoch": 1.716,
      "grad_norm": 409.3028259277344,
      "learning_rate": 0.001223397032456524,
      "loss": 5.6831,
      "step": 1716
    },
    {
      "epoch": 1.717,
      "grad_norm": 181.0166473388672,
      "learning_rate": 0.001222630456112346,
      "loss": 4.7892,
      "step": 1717
    },
    {
      "epoch": 1.718,
      "grad_norm": 472.8269348144531,
      "learning_rate": 0.0012218637420945483,
      "loss": 6.0043,
      "step": 1718
    },
    {
      "epoch": 1.7189999999999999,
      "grad_norm": 201.9632568359375,
      "learning_rate": 0.001221096890877263,
      "loss": 6.8597,
      "step": 1719
    },
    {
      "epoch": 1.72,
      "grad_norm": 533.552978515625,
      "learning_rate": 0.001220329902934707,
      "loss": 6.4989,
      "step": 1720
    },
    {
      "epoch": 1.721,
      "grad_norm": 1927.36279296875,
      "learning_rate": 0.0012195627787411822,
      "loss": 5.8516,
      "step": 1721
    },
    {
      "epoch": 1.722,
      "grad_norm": 2932.4208984375,
      "learning_rate": 0.001218795518771075,
      "loss": 6.4913,
      "step": 1722
    },
    {
      "epoch": 1.7229999999999999,
      "grad_norm": 443.0720520019531,
      "learning_rate": 0.001218028123498855,
      "loss": 5.5086,
      "step": 1723
    },
    {
      "epoch": 1.724,
      "grad_norm": 7383.27587890625,
      "learning_rate": 0.0012172605933990755,
      "loss": 5.9157,
      "step": 1724
    },
    {
      "epoch": 1.725,
      "grad_norm": 454.60968017578125,
      "learning_rate": 0.0012164929289463736,
      "loss": 5.4076,
      "step": 1725
    },
    {
      "epoch": 1.726,
      "grad_norm": 234.9728546142578,
      "learning_rate": 0.0012157251306154698,
      "loss": 4.4506,
      "step": 1726
    },
    {
      "epoch": 1.7269999999999999,
      "grad_norm": 2380.610595703125,
      "learning_rate": 0.0012149571988811663,
      "loss": 4.0414,
      "step": 1727
    },
    {
      "epoch": 1.728,
      "grad_norm": 109.3624267578125,
      "learning_rate": 0.0012141891342183492,
      "loss": 4.4436,
      "step": 1728
    },
    {
      "epoch": 1.729,
      "grad_norm": 111.5252456665039,
      "learning_rate": 0.0012134209371019852,
      "loss": 3.985,
      "step": 1729
    },
    {
      "epoch": 1.73,
      "grad_norm": 61.928279876708984,
      "learning_rate": 0.0012126526080071243,
      "loss": 4.1855,
      "step": 1730
    },
    {
      "epoch": 1.7309999999999999,
      "grad_norm": 16.637149810791016,
      "learning_rate": 0.0012118841474088977,
      "loss": 3.9769,
      "step": 1731
    },
    {
      "epoch": 1.732,
      "grad_norm": 23.26910972595215,
      "learning_rate": 0.0012111155557825173,
      "loss": 3.5891,
      "step": 1732
    },
    {
      "epoch": 1.733,
      "grad_norm": 31.810861587524414,
      "learning_rate": 0.001210346833603277,
      "loss": 3.3963,
      "step": 1733
    },
    {
      "epoch": 1.734,
      "grad_norm": 136.84811401367188,
      "learning_rate": 0.0012095779813465506,
      "loss": 3.1549,
      "step": 1734
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 23.37020492553711,
      "learning_rate": 0.001208808999487793,
      "loss": 3.1647,
      "step": 1735
    },
    {
      "epoch": 1.736,
      "grad_norm": 11.403839111328125,
      "learning_rate": 0.0012080398885025388,
      "loss": 2.9852,
      "step": 1736
    },
    {
      "epoch": 1.737,
      "grad_norm": 8.853170394897461,
      "learning_rate": 0.0012072706488664028,
      "loss": 2.6547,
      "step": 1737
    },
    {
      "epoch": 1.738,
      "grad_norm": 35.97683334350586,
      "learning_rate": 0.0012065012810550783,
      "loss": 2.5873,
      "step": 1738
    },
    {
      "epoch": 1.7389999999999999,
      "grad_norm": 10.109292030334473,
      "learning_rate": 0.0012057317855443395,
      "loss": 2.7447,
      "step": 1739
    },
    {
      "epoch": 1.74,
      "grad_norm": 11.151836395263672,
      "learning_rate": 0.0012049621628100388,
      "loss": 2.7259,
      "step": 1740
    },
    {
      "epoch": 1.741,
      "grad_norm": 7.310627460479736,
      "learning_rate": 0.0012041924133281072,
      "loss": 2.4808,
      "step": 1741
    },
    {
      "epoch": 1.742,
      "grad_norm": 19.80479621887207,
      "learning_rate": 0.001203422537574554,
      "loss": 3.2495,
      "step": 1742
    },
    {
      "epoch": 1.7429999999999999,
      "grad_norm": 10.275568008422852,
      "learning_rate": 0.0012026525360254664,
      "loss": 3.0442,
      "step": 1743
    },
    {
      "epoch": 1.744,
      "grad_norm": 10.059927940368652,
      "learning_rate": 0.0012018824091570102,
      "loss": 2.6801,
      "step": 1744
    },
    {
      "epoch": 1.745,
      "grad_norm": 156.8206329345703,
      "learning_rate": 0.0012011121574454283,
      "loss": 2.8506,
      "step": 1745
    },
    {
      "epoch": 1.746,
      "grad_norm": 12.556529998779297,
      "learning_rate": 0.0012003417813670402,
      "loss": 2.8949,
      "step": 1746
    },
    {
      "epoch": 1.7469999999999999,
      "grad_norm": 6.781261920928955,
      "learning_rate": 0.0011995712813982432,
      "loss": 2.6274,
      "step": 1747
    },
    {
      "epoch": 1.748,
      "grad_norm": 75.21794128417969,
      "learning_rate": 0.0011988006580155104,
      "loss": 2.5175,
      "step": 1748
    },
    {
      "epoch": 1.749,
      "grad_norm": 60.375144958496094,
      "learning_rate": 0.0011980299116953923,
      "loss": 2.8037,
      "step": 1749
    },
    {
      "epoch": 1.75,
      "grad_norm": 35.615440368652344,
      "learning_rate": 0.0011972590429145142,
      "loss": 3.4184,
      "step": 1750
    },
    {
      "epoch": 1.751,
      "grad_norm": 12.143954277038574,
      "learning_rate": 0.0011964880521495784,
      "loss": 3.0845,
      "step": 1751
    },
    {
      "epoch": 1.752,
      "grad_norm": 17.5909366607666,
      "learning_rate": 0.0011957169398773611,
      "loss": 2.6548,
      "step": 1752
    },
    {
      "epoch": 1.7530000000000001,
      "grad_norm": 4.842310428619385,
      "learning_rate": 0.0011949457065747147,
      "loss": 2.6986,
      "step": 1753
    },
    {
      "epoch": 1.754,
      "grad_norm": 26.42704963684082,
      "learning_rate": 0.001194174352718567,
      "loss": 2.665,
      "step": 1754
    },
    {
      "epoch": 1.755,
      "grad_norm": 101.18809509277344,
      "learning_rate": 0.0011934028787859185,
      "loss": 2.7567,
      "step": 1755
    },
    {
      "epoch": 1.756,
      "grad_norm": 5.503548622131348,
      "learning_rate": 0.0011926312852538455,
      "loss": 2.7899,
      "step": 1756
    },
    {
      "epoch": 1.7570000000000001,
      "grad_norm": 4.929245471954346,
      "learning_rate": 0.0011918595725994975,
      "loss": 2.82,
      "step": 1757
    },
    {
      "epoch": 1.758,
      "grad_norm": 5.952747344970703,
      "learning_rate": 0.001191087741300099,
      "loss": 2.8953,
      "step": 1758
    },
    {
      "epoch": 1.759,
      "grad_norm": 7.529831409454346,
      "learning_rate": 0.0011903157918329455,
      "loss": 2.6888,
      "step": 1759
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.0171375274658203,
      "learning_rate": 0.0011895437246754073,
      "loss": 2.3014,
      "step": 1760
    },
    {
      "epoch": 1.7610000000000001,
      "grad_norm": 15.198833465576172,
      "learning_rate": 0.0011887715403049275,
      "loss": 2.4446,
      "step": 1761
    },
    {
      "epoch": 1.762,
      "grad_norm": 10.671675682067871,
      "learning_rate": 0.001187999239199021,
      "loss": 2.6761,
      "step": 1762
    },
    {
      "epoch": 1.763,
      "grad_norm": 4.789581775665283,
      "learning_rate": 0.0011872268218352752,
      "loss": 2.5422,
      "step": 1763
    },
    {
      "epoch": 1.764,
      "grad_norm": 3.844783067703247,
      "learning_rate": 0.001186454288691349,
      "loss": 2.8102,
      "step": 1764
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 2.564565420150757,
      "learning_rate": 0.0011856816402449732,
      "loss": 2.6049,
      "step": 1765
    },
    {
      "epoch": 1.766,
      "grad_norm": 6.6315999031066895,
      "learning_rate": 0.0011849088769739508,
      "loss": 2.4042,
      "step": 1766
    },
    {
      "epoch": 1.767,
      "grad_norm": 5.723902702331543,
      "learning_rate": 0.0011841359993561543,
      "loss": 2.2172,
      "step": 1767
    },
    {
      "epoch": 1.768,
      "grad_norm": 2.6102874279022217,
      "learning_rate": 0.0011833630078695275,
      "loss": 2.3494,
      "step": 1768
    },
    {
      "epoch": 1.7690000000000001,
      "grad_norm": 27.266441345214844,
      "learning_rate": 0.0011825899029920847,
      "loss": 2.4933,
      "step": 1769
    },
    {
      "epoch": 1.77,
      "grad_norm": 8.408110618591309,
      "learning_rate": 0.0011818166852019109,
      "loss": 2.7537,
      "step": 1770
    },
    {
      "epoch": 1.771,
      "grad_norm": 9.458531379699707,
      "learning_rate": 0.0011810433549771593,
      "loss": 2.5334,
      "step": 1771
    },
    {
      "epoch": 1.772,
      "grad_norm": 5.190684795379639,
      "learning_rate": 0.0011802699127960545,
      "loss": 2.417,
      "step": 1772
    },
    {
      "epoch": 1.7730000000000001,
      "grad_norm": 2.0747687816619873,
      "learning_rate": 0.0011794963591368892,
      "loss": 2.1475,
      "step": 1773
    },
    {
      "epoch": 1.774,
      "grad_norm": 4.343659400939941,
      "learning_rate": 0.0011787226944780249,
      "loss": 2.048,
      "step": 1774
    },
    {
      "epoch": 1.775,
      "grad_norm": 1.9141016006469727,
      "learning_rate": 0.0011779489192978929,
      "loss": 2.0176,
      "step": 1775
    },
    {
      "epoch": 1.776,
      "grad_norm": 1.153128743171692,
      "learning_rate": 0.0011771750340749919,
      "loss": 1.9577,
      "step": 1776
    },
    {
      "epoch": 1.7770000000000001,
      "grad_norm": 2.2148897647857666,
      "learning_rate": 0.0011764010392878884,
      "loss": 1.9876,
      "step": 1777
    },
    {
      "epoch": 1.778,
      "grad_norm": 1.260347843170166,
      "learning_rate": 0.0011756269354152174,
      "loss": 1.7879,
      "step": 1778
    },
    {
      "epoch": 1.779,
      "grad_norm": 2.9833977222442627,
      "learning_rate": 0.0011748527229356813,
      "loss": 1.8297,
      "step": 1779
    },
    {
      "epoch": 1.78,
      "grad_norm": 2.397146224975586,
      "learning_rate": 0.001174078402328049,
      "loss": 1.9988,
      "step": 1780
    },
    {
      "epoch": 1.7810000000000001,
      "grad_norm": 1.5171681642532349,
      "learning_rate": 0.0011733039740711574,
      "loss": 1.8328,
      "step": 1781
    },
    {
      "epoch": 1.782,
      "grad_norm": 2.8218438625335693,
      "learning_rate": 0.0011725294386439084,
      "loss": 1.8004,
      "step": 1782
    },
    {
      "epoch": 1.783,
      "grad_norm": 4.360485076904297,
      "learning_rate": 0.0011717547965252712,
      "loss": 1.919,
      "step": 1783
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.8981132507324219,
      "learning_rate": 0.0011709800481942815,
      "loss": 1.8775,
      "step": 1784
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 5.763084888458252,
      "learning_rate": 0.0011702051941300396,
      "loss": 2.1122,
      "step": 1785
    },
    {
      "epoch": 1.786,
      "grad_norm": 4.581227779388428,
      "learning_rate": 0.0011694302348117113,
      "loss": 2.0786,
      "step": 1786
    },
    {
      "epoch": 1.787,
      "grad_norm": 2.848451614379883,
      "learning_rate": 0.0011686551707185279,
      "loss": 1.9533,
      "step": 1787
    },
    {
      "epoch": 1.788,
      "grad_norm": 13.069598197937012,
      "learning_rate": 0.0011678800023297857,
      "loss": 1.9107,
      "step": 1788
    },
    {
      "epoch": 1.7890000000000001,
      "grad_norm": 3.325319290161133,
      "learning_rate": 0.0011671047301248447,
      "loss": 1.8321,
      "step": 1789
    },
    {
      "epoch": 1.79,
      "grad_norm": 3.638889789581299,
      "learning_rate": 0.0011663293545831302,
      "loss": 2.2023,
      "step": 1790
    },
    {
      "epoch": 1.791,
      "grad_norm": 3.643972873687744,
      "learning_rate": 0.00116555387618413,
      "loss": 1.9502,
      "step": 1791
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.6285336017608643,
      "learning_rate": 0.0011647782954073967,
      "loss": 1.6971,
      "step": 1792
    },
    {
      "epoch": 1.7930000000000001,
      "grad_norm": 10.886690139770508,
      "learning_rate": 0.0011640026127325458,
      "loss": 1.8922,
      "step": 1793
    },
    {
      "epoch": 1.794,
      "grad_norm": 4.015232086181641,
      "learning_rate": 0.0011632268286392558,
      "loss": 1.868,
      "step": 1794
    },
    {
      "epoch": 1.795,
      "grad_norm": 3.186452865600586,
      "learning_rate": 0.0011624509436072674,
      "loss": 1.895,
      "step": 1795
    },
    {
      "epoch": 1.796,
      "grad_norm": 3.194711208343506,
      "learning_rate": 0.001161674958116385,
      "loss": 1.7726,
      "step": 1796
    },
    {
      "epoch": 1.7970000000000002,
      "grad_norm": 2.8773491382598877,
      "learning_rate": 0.0011608988726464742,
      "loss": 1.8216,
      "step": 1797
    },
    {
      "epoch": 1.798,
      "grad_norm": 2.8864634037017822,
      "learning_rate": 0.001160122687677462,
      "loss": 1.7876,
      "step": 1798
    },
    {
      "epoch": 1.799,
      "grad_norm": 2.2938287258148193,
      "learning_rate": 0.0011593464036893382,
      "loss": 1.74,
      "step": 1799
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.5498666763305664,
      "learning_rate": 0.0011585700211621526,
      "loss": 1.7033,
      "step": 1800
    },
    {
      "epoch": 1.8,
      "eval_loss": 1.940671443939209,
      "eval_runtime": 224.7977,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 1800
    },
    {
      "epoch": 1.8010000000000002,
      "grad_norm": 4.207131385803223,
      "learning_rate": 0.0011577935405760165,
      "loss": 1.969,
      "step": 1801
    },
    {
      "epoch": 1.802,
      "grad_norm": 2.9592013359069824,
      "learning_rate": 0.0011570169624111025,
      "loss": 1.8176,
      "step": 1802
    },
    {
      "epoch": 1.803,
      "grad_norm": 4.708759784698486,
      "learning_rate": 0.0011562402871476423,
      "loss": 1.705,
      "step": 1803
    },
    {
      "epoch": 1.804,
      "grad_norm": 4.260100364685059,
      "learning_rate": 0.0011554635152659278,
      "loss": 1.8041,
      "step": 1804
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 3.2616021633148193,
      "learning_rate": 0.001154686647246312,
      "loss": 1.905,
      "step": 1805
    },
    {
      "epoch": 1.806,
      "grad_norm": 2.740452289581299,
      "learning_rate": 0.001153909683569206,
      "loss": 1.8131,
      "step": 1806
    },
    {
      "epoch": 1.807,
      "grad_norm": 2.5884337425231934,
      "learning_rate": 0.00115313262471508,
      "loss": 1.7512,
      "step": 1807
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.7709746360778809,
      "learning_rate": 0.0011523554711644643,
      "loss": 1.717,
      "step": 1808
    },
    {
      "epoch": 1.8090000000000002,
      "grad_norm": 2.160339117050171,
      "learning_rate": 0.0011515782233979465,
      "loss": 1.5638,
      "step": 1809
    },
    {
      "epoch": 1.81,
      "grad_norm": 5.029852867126465,
      "learning_rate": 0.001150800881896173,
      "loss": 1.6954,
      "step": 1810
    },
    {
      "epoch": 1.811,
      "grad_norm": 1.4971349239349365,
      "learning_rate": 0.0011500234471398486,
      "loss": 1.755,
      "step": 1811
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.2045434713363647,
      "learning_rate": 0.0011492459196097344,
      "loss": 1.6213,
      "step": 1812
    },
    {
      "epoch": 1.813,
      "grad_norm": 38.888553619384766,
      "learning_rate": 0.0011484682997866499,
      "loss": 1.6229,
      "step": 1813
    },
    {
      "epoch": 1.814,
      "grad_norm": 2.335677146911621,
      "learning_rate": 0.001147690588151472,
      "loss": 1.7133,
      "step": 1814
    },
    {
      "epoch": 1.815,
      "grad_norm": 2.1326136589050293,
      "learning_rate": 0.001146912785185134,
      "loss": 1.6016,
      "step": 1815
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 4.5206217765808105,
      "learning_rate": 0.001146134891368625,
      "loss": 1.9228,
      "step": 1816
    },
    {
      "epoch": 1.817,
      "grad_norm": 3.620067834854126,
      "learning_rate": 0.001145356907182991,
      "loss": 1.8507,
      "step": 1817
    },
    {
      "epoch": 1.818,
      "grad_norm": 1.4519381523132324,
      "learning_rate": 0.001144578833109334,
      "loss": 1.679,
      "step": 1818
    },
    {
      "epoch": 1.819,
      "grad_norm": 4.341037750244141,
      "learning_rate": 0.0011438006696288109,
      "loss": 1.8317,
      "step": 1819
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 11.043856620788574,
      "learning_rate": 0.001143022417222635,
      "loss": 1.6645,
      "step": 1820
    },
    {
      "epoch": 1.821,
      "grad_norm": 3.9448931217193604,
      "learning_rate": 0.001142244076372073,
      "loss": 1.7007,
      "step": 1821
    },
    {
      "epoch": 1.822,
      "grad_norm": 5.005965232849121,
      "learning_rate": 0.0011414656475584478,
      "loss": 1.8964,
      "step": 1822
    },
    {
      "epoch": 1.823,
      "grad_norm": 4.563599586486816,
      "learning_rate": 0.0011406871312631362,
      "loss": 1.8143,
      "step": 1823
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 27.375612258911133,
      "learning_rate": 0.0011399085279675688,
      "loss": 1.9798,
      "step": 1824
    },
    {
      "epoch": 1.825,
      "grad_norm": 4.567429542541504,
      "learning_rate": 0.0011391298381532296,
      "loss": 1.8255,
      "step": 1825
    },
    {
      "epoch": 1.826,
      "grad_norm": 6.5909743309021,
      "learning_rate": 0.0011383510623016574,
      "loss": 1.7623,
      "step": 1826
    },
    {
      "epoch": 1.827,
      "grad_norm": 4.328863143920898,
      "learning_rate": 0.0011375722008944438,
      "loss": 1.9526,
      "step": 1827
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 5.5246734619140625,
      "learning_rate": 0.0011367932544132323,
      "loss": 1.8986,
      "step": 1828
    },
    {
      "epoch": 1.829,
      "grad_norm": 2.882007122039795,
      "learning_rate": 0.0011360142233397198,
      "loss": 1.6244,
      "step": 1829
    },
    {
      "epoch": 1.83,
      "grad_norm": 7.338501453399658,
      "learning_rate": 0.0011352351081556557,
      "loss": 1.7214,
      "step": 1830
    },
    {
      "epoch": 1.831,
      "grad_norm": 2.984440565109253,
      "learning_rate": 0.001134455909342841,
      "loss": 1.6801,
      "step": 1831
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 15.036918640136719,
      "learning_rate": 0.0011336766273831286,
      "loss": 1.9361,
      "step": 1832
    },
    {
      "epoch": 1.833,
      "grad_norm": 41.20704650878906,
      "learning_rate": 0.0011328972627584228,
      "loss": 1.8621,
      "step": 1833
    },
    {
      "epoch": 1.834,
      "grad_norm": 13.963446617126465,
      "learning_rate": 0.0011321178159506792,
      "loss": 1.776,
      "step": 1834
    },
    {
      "epoch": 1.835,
      "grad_norm": 5.255237102508545,
      "learning_rate": 0.0011313382874419031,
      "loss": 1.7732,
      "step": 1835
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 6.045709609985352,
      "learning_rate": 0.0011305586777141526,
      "loss": 1.5579,
      "step": 1836
    },
    {
      "epoch": 1.837,
      "grad_norm": 3.5632030963897705,
      "learning_rate": 0.0011297789872495333,
      "loss": 1.6316,
      "step": 1837
    },
    {
      "epoch": 1.838,
      "grad_norm": 16.60182762145996,
      "learning_rate": 0.0011289992165302034,
      "loss": 1.9804,
      "step": 1838
    },
    {
      "epoch": 1.839,
      "grad_norm": 7.402459621429443,
      "learning_rate": 0.0011282193660383684,
      "loss": 1.8574,
      "step": 1839
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 5.38160514831543,
      "learning_rate": 0.0011274394362562846,
      "loss": 1.7231,
      "step": 1840
    },
    {
      "epoch": 1.841,
      "grad_norm": 2.2871687412261963,
      "learning_rate": 0.001126659427666257,
      "loss": 1.9555,
      "step": 1841
    },
    {
      "epoch": 1.842,
      "grad_norm": 3.254150152206421,
      "learning_rate": 0.001125879340750639,
      "loss": 1.6523,
      "step": 1842
    },
    {
      "epoch": 1.843,
      "grad_norm": 1.81564462184906,
      "learning_rate": 0.0011250991759918323,
      "loss": 1.6294,
      "step": 1843
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.625213861465454,
      "learning_rate": 0.001124318933872288,
      "loss": 1.7691,
      "step": 1844
    },
    {
      "epoch": 1.845,
      "grad_norm": 10.575579643249512,
      "learning_rate": 0.0011235386148745034,
      "loss": 1.5544,
      "step": 1845
    },
    {
      "epoch": 1.846,
      "grad_norm": 2.7437856197357178,
      "learning_rate": 0.0011227582194810242,
      "loss": 1.5405,
      "step": 1846
    },
    {
      "epoch": 1.847,
      "grad_norm": 2.4432671070098877,
      "learning_rate": 0.0011219777481744435,
      "loss": 1.8375,
      "step": 1847
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 4.385117053985596,
      "learning_rate": 0.0011211972014374008,
      "loss": 1.7973,
      "step": 1848
    },
    {
      "epoch": 1.849,
      "grad_norm": 8.45837688446045,
      "learning_rate": 0.0011204165797525825,
      "loss": 1.7966,
      "step": 1849
    },
    {
      "epoch": 1.85,
      "grad_norm": 5.054619789123535,
      "learning_rate": 0.0011196358836027215,
      "loss": 1.8835,
      "step": 1850
    },
    {
      "epoch": 1.851,
      "grad_norm": 1.896239161491394,
      "learning_rate": 0.0011188551134705966,
      "loss": 1.7001,
      "step": 1851
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 3.3278026580810547,
      "learning_rate": 0.001118074269839032,
      "loss": 1.7585,
      "step": 1852
    },
    {
      "epoch": 1.853,
      "grad_norm": 2.090301752090454,
      "learning_rate": 0.0011172933531908983,
      "loss": 1.5255,
      "step": 1853
    },
    {
      "epoch": 1.854,
      "grad_norm": 4.916884422302246,
      "learning_rate": 0.0011165123640091105,
      "loss": 1.6846,
      "step": 1854
    },
    {
      "epoch": 1.855,
      "grad_norm": 2.779242992401123,
      "learning_rate": 0.0011157313027766278,
      "loss": 1.5526,
      "step": 1855
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 3.3209636211395264,
      "learning_rate": 0.0011149501699764558,
      "loss": 1.5923,
      "step": 1856
    },
    {
      "epoch": 1.857,
      "grad_norm": 2.5899996757507324,
      "learning_rate": 0.0011141689660916431,
      "loss": 1.5932,
      "step": 1857
    },
    {
      "epoch": 1.858,
      "grad_norm": 1.3287222385406494,
      "learning_rate": 0.0011133876916052821,
      "loss": 1.4896,
      "step": 1858
    },
    {
      "epoch": 1.859,
      "grad_norm": 5.744210243225098,
      "learning_rate": 0.0011126063470005096,
      "loss": 1.6479,
      "step": 1859
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.7642158269882202,
      "learning_rate": 0.0011118249327605055,
      "loss": 1.5974,
      "step": 1860
    },
    {
      "epoch": 1.861,
      "grad_norm": 2.520876169204712,
      "learning_rate": 0.001111043449368492,
      "loss": 1.5537,
      "step": 1861
    },
    {
      "epoch": 1.862,
      "grad_norm": 1.6557824611663818,
      "learning_rate": 0.0011102618973077355,
      "loss": 1.7574,
      "step": 1862
    },
    {
      "epoch": 1.863,
      "grad_norm": 6.676003932952881,
      "learning_rate": 0.0011094802770615438,
      "loss": 1.7279,
      "step": 1863
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 2.6055893898010254,
      "learning_rate": 0.0011086985891132668,
      "loss": 1.5842,
      "step": 1864
    },
    {
      "epoch": 1.865,
      "grad_norm": 2.8629772663116455,
      "learning_rate": 0.001107916833946297,
      "loss": 1.7254,
      "step": 1865
    },
    {
      "epoch": 1.866,
      "grad_norm": 5.5585784912109375,
      "learning_rate": 0.0011071350120440684,
      "loss": 1.8807,
      "step": 1866
    },
    {
      "epoch": 1.867,
      "grad_norm": 8.07001781463623,
      "learning_rate": 0.0011063531238900556,
      "loss": 1.7392,
      "step": 1867
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 3.863013982772827,
      "learning_rate": 0.0011055711699677743,
      "loss": 1.5946,
      "step": 1868
    },
    {
      "epoch": 1.869,
      "grad_norm": 5.237558841705322,
      "learning_rate": 0.0011047891507607814,
      "loss": 1.7099,
      "step": 1869
    },
    {
      "epoch": 1.87,
      "grad_norm": 26.098127365112305,
      "learning_rate": 0.0011040070667526736,
      "loss": 1.8462,
      "step": 1870
    },
    {
      "epoch": 1.871,
      "grad_norm": 2.6784651279449463,
      "learning_rate": 0.0011032249184270886,
      "loss": 1.6139,
      "step": 1871
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 3.7228481769561768,
      "learning_rate": 0.0011024427062677027,
      "loss": 1.6288,
      "step": 1872
    },
    {
      "epoch": 1.873,
      "grad_norm": 2.936776638031006,
      "learning_rate": 0.001101660430758232,
      "loss": 1.61,
      "step": 1873
    },
    {
      "epoch": 1.874,
      "grad_norm": 3.059346914291382,
      "learning_rate": 0.0011008780923824327,
      "loss": 1.5113,
      "step": 1874
    },
    {
      "epoch": 1.875,
      "grad_norm": 5.175826072692871,
      "learning_rate": 0.0011000956916240984,
      "loss": 1.7202,
      "step": 1875
    },
    {
      "epoch": 1.876,
      "grad_norm": 6.132888317108154,
      "learning_rate": 0.0010993132289670625,
      "loss": 1.5912,
      "step": 1876
    },
    {
      "epoch": 1.877,
      "grad_norm": 13344.4296875,
      "learning_rate": 0.001098530704895196,
      "loss": 1.6754,
      "step": 1877
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 2.6978349685668945,
      "learning_rate": 0.001097748119892408,
      "loss": 1.7081,
      "step": 1878
    },
    {
      "epoch": 1.879,
      "grad_norm": 4.345241069793701,
      "learning_rate": 0.0010969654744426454,
      "loss": 1.6125,
      "step": 1879
    },
    {
      "epoch": 1.88,
      "grad_norm": 96.92253875732422,
      "learning_rate": 0.0010961827690298927,
      "loss": 1.9752,
      "step": 1880
    },
    {
      "epoch": 1.881,
      "grad_norm": 46.566566467285156,
      "learning_rate": 0.001095400004138171,
      "loss": 1.9408,
      "step": 1881
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 55.298500061035156,
      "learning_rate": 0.0010946171802515385,
      "loss": 1.9927,
      "step": 1882
    },
    {
      "epoch": 1.883,
      "grad_norm": 63.81329345703125,
      "learning_rate": 0.0010938342978540896,
      "loss": 1.8504,
      "step": 1883
    },
    {
      "epoch": 1.884,
      "grad_norm": 7.9214019775390625,
      "learning_rate": 0.0010930513574299555,
      "loss": 1.8041,
      "step": 1884
    },
    {
      "epoch": 1.885,
      "grad_norm": 2.449136734008789,
      "learning_rate": 0.001092268359463302,
      "loss": 1.7966,
      "step": 1885
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 4.568121433258057,
      "learning_rate": 0.0010914853044383323,
      "loss": 1.7697,
      "step": 1886
    },
    {
      "epoch": 1.887,
      "grad_norm": 7.2304277420043945,
      "learning_rate": 0.0010907021928392832,
      "loss": 1.7045,
      "step": 1887
    },
    {
      "epoch": 1.888,
      "grad_norm": 5.4387946128845215,
      "learning_rate": 0.0010899190251504277,
      "loss": 1.9018,
      "step": 1888
    },
    {
      "epoch": 1.889,
      "grad_norm": 3.1722540855407715,
      "learning_rate": 0.0010891358018560728,
      "loss": 1.6533,
      "step": 1889
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 83.14036560058594,
      "learning_rate": 0.0010883525234405603,
      "loss": 1.8189,
      "step": 1890
    },
    {
      "epoch": 1.891,
      "grad_norm": 2.122941732406616,
      "learning_rate": 0.001087569190388265,
      "loss": 1.6862,
      "step": 1891
    },
    {
      "epoch": 1.892,
      "grad_norm": 11.470270156860352,
      "learning_rate": 0.0010867858031835976,
      "loss": 1.641,
      "step": 1892
    },
    {
      "epoch": 1.893,
      "grad_norm": 2.7602767944335938,
      "learning_rate": 0.0010860023623110002,
      "loss": 1.5961,
      "step": 1893
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 1.5947973728179932,
      "learning_rate": 0.001085218868254949,
      "loss": 1.5919,
      "step": 1894
    },
    {
      "epoch": 1.895,
      "grad_norm": 2.298036813735962,
      "learning_rate": 0.0010844353214999535,
      "loss": 1.6951,
      "step": 1895
    },
    {
      "epoch": 1.896,
      "grad_norm": 3.785090208053589,
      "learning_rate": 0.0010836517225305546,
      "loss": 1.8778,
      "step": 1896
    },
    {
      "epoch": 1.897,
      "grad_norm": 3.3158726692199707,
      "learning_rate": 0.0010828680718313268,
      "loss": 1.7425,
      "step": 1897
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 2.815657615661621,
      "learning_rate": 0.0010820843698868758,
      "loss": 1.5585,
      "step": 1898
    },
    {
      "epoch": 1.899,
      "grad_norm": 1.4932399988174438,
      "learning_rate": 0.0010813006171818392,
      "loss": 1.4507,
      "step": 1899
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.5154931545257568,
      "learning_rate": 0.0010805168142008852,
      "loss": 1.6717,
      "step": 1900
    },
    {
      "epoch": 1.9,
      "eval_loss": 1.6078267097473145,
      "eval_runtime": 224.9667,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 1900
    },
    {
      "epoch": 1.901,
      "grad_norm": 1.4221597909927368,
      "learning_rate": 0.0010797329614287152,
      "loss": 1.5716,
      "step": 1901
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 4.617467403411865,
      "learning_rate": 0.0010789490593500592,
      "loss": 1.6102,
      "step": 1902
    },
    {
      "epoch": 1.903,
      "grad_norm": 4.095471382141113,
      "learning_rate": 0.0010781651084496786,
      "loss": 1.5666,
      "step": 1903
    },
    {
      "epoch": 1.904,
      "grad_norm": 2.263638973236084,
      "learning_rate": 0.0010773811092123654,
      "loss": 1.5587,
      "step": 1904
    },
    {
      "epoch": 1.905,
      "grad_norm": 2.3014638423919678,
      "learning_rate": 0.0010765970621229401,
      "loss": 1.5531,
      "step": 1905
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 1.4016764163970947,
      "learning_rate": 0.0010758129676662544,
      "loss": 1.6166,
      "step": 1906
    },
    {
      "epoch": 1.907,
      "grad_norm": 3.29880952835083,
      "learning_rate": 0.0010750288263271888,
      "loss": 1.743,
      "step": 1907
    },
    {
      "epoch": 1.908,
      "grad_norm": 1.4141709804534912,
      "learning_rate": 0.0010742446385906522,
      "loss": 1.5385,
      "step": 1908
    },
    {
      "epoch": 1.909,
      "grad_norm": 1.5037620067596436,
      "learning_rate": 0.001073460404941582,
      "loss": 1.6305,
      "step": 1909
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 2.914543628692627,
      "learning_rate": 0.001072676125864946,
      "loss": 1.662,
      "step": 1910
    },
    {
      "epoch": 1.911,
      "grad_norm": 1.4044408798217773,
      "learning_rate": 0.0010718918018457375,
      "loss": 1.5467,
      "step": 1911
    },
    {
      "epoch": 1.912,
      "grad_norm": 11.13939094543457,
      "learning_rate": 0.0010711074333689791,
      "loss": 1.5296,
      "step": 1912
    },
    {
      "epoch": 1.913,
      "grad_norm": 2.232361316680908,
      "learning_rate": 0.0010703230209197206,
      "loss": 1.8026,
      "step": 1913
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 7.502827167510986,
      "learning_rate": 0.0010695385649830392,
      "loss": 1.6294,
      "step": 1914
    },
    {
      "epoch": 1.915,
      "grad_norm": 2.6874780654907227,
      "learning_rate": 0.001068754066044038,
      "loss": 1.6687,
      "step": 1915
    },
    {
      "epoch": 1.916,
      "grad_norm": 35.015201568603516,
      "learning_rate": 0.0010679695245878482,
      "loss": 1.5657,
      "step": 1916
    },
    {
      "epoch": 1.917,
      "grad_norm": 1.3852882385253906,
      "learning_rate": 0.001067184941099626,
      "loss": 1.5463,
      "step": 1917
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 32.71429443359375,
      "learning_rate": 0.0010664003160645546,
      "loss": 1.5162,
      "step": 1918
    },
    {
      "epoch": 1.919,
      "grad_norm": 2.354548931121826,
      "learning_rate": 0.001065615649967842,
      "loss": 1.7517,
      "step": 1919
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.8102329969406128,
      "learning_rate": 0.0010648309432947224,
      "loss": 1.6801,
      "step": 1920
    },
    {
      "epoch": 1.921,
      "grad_norm": 15.913873672485352,
      "learning_rate": 0.001064046196530454,
      "loss": 1.5342,
      "step": 1921
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 2.909952163696289,
      "learning_rate": 0.001063261410160321,
      "loss": 1.5144,
      "step": 1922
    },
    {
      "epoch": 1.923,
      "grad_norm": 1.3615608215332031,
      "learning_rate": 0.0010624765846696316,
      "loss": 1.4892,
      "step": 1923
    },
    {
      "epoch": 1.924,
      "grad_norm": 2.8303382396698,
      "learning_rate": 0.001061691720543718,
      "loss": 1.5296,
      "step": 1924
    },
    {
      "epoch": 1.925,
      "grad_norm": 2.4425036907196045,
      "learning_rate": 0.0010609068182679366,
      "loss": 1.714,
      "step": 1925
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 4.0808634757995605,
      "learning_rate": 0.0010601218783276671,
      "loss": 1.4687,
      "step": 1926
    },
    {
      "epoch": 1.927,
      "grad_norm": 1.7291889190673828,
      "learning_rate": 0.0010593369012083128,
      "loss": 1.6811,
      "step": 1927
    },
    {
      "epoch": 1.928,
      "grad_norm": 2.180635452270508,
      "learning_rate": 0.0010585518873952996,
      "loss": 1.5572,
      "step": 1928
    },
    {
      "epoch": 1.929,
      "grad_norm": 2.3465394973754883,
      "learning_rate": 0.0010577668373740767,
      "loss": 1.6923,
      "step": 1929
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 8.664457321166992,
      "learning_rate": 0.0010569817516301147,
      "loss": 1.5322,
      "step": 1930
    },
    {
      "epoch": 1.931,
      "grad_norm": 3.4724502563476562,
      "learning_rate": 0.0010561966306489076,
      "loss": 1.6443,
      "step": 1931
    },
    {
      "epoch": 1.932,
      "grad_norm": 3.1047887802124023,
      "learning_rate": 0.00105541147491597,
      "loss": 1.6292,
      "step": 1932
    },
    {
      "epoch": 1.933,
      "grad_norm": 2.5450379848480225,
      "learning_rate": 0.001054626284916839,
      "loss": 1.5463,
      "step": 1933
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 11.015823364257812,
      "learning_rate": 0.0010538410611370718,
      "loss": 1.4688,
      "step": 1934
    },
    {
      "epoch": 1.935,
      "grad_norm": 124.77635955810547,
      "learning_rate": 0.0010530558040622471,
      "loss": 1.5404,
      "step": 1935
    },
    {
      "epoch": 1.936,
      "grad_norm": 51.51686096191406,
      "learning_rate": 0.0010522705141779644,
      "loss": 1.7585,
      "step": 1936
    },
    {
      "epoch": 1.937,
      "grad_norm": 922.4570922851562,
      "learning_rate": 0.0010514851919698437,
      "loss": 1.6323,
      "step": 1937
    },
    {
      "epoch": 1.938,
      "grad_norm": 5831.80029296875,
      "learning_rate": 0.001050699837923524,
      "loss": 1.8167,
      "step": 1938
    },
    {
      "epoch": 1.939,
      "grad_norm": 109.6670913696289,
      "learning_rate": 0.0010499144525246643,
      "loss": 2.0077,
      "step": 1939
    },
    {
      "epoch": 1.94,
      "grad_norm": 132.7557830810547,
      "learning_rate": 0.0010491290362589433,
      "loss": 2.2626,
      "step": 1940
    },
    {
      "epoch": 1.9409999999999998,
      "grad_norm": 56.38319778442383,
      "learning_rate": 0.0010483435896120592,
      "loss": 2.4838,
      "step": 1941
    },
    {
      "epoch": 1.942,
      "grad_norm": 709.5214233398438,
      "learning_rate": 0.0010475581130697284,
      "loss": 2.5028,
      "step": 1942
    },
    {
      "epoch": 1.943,
      "grad_norm": 70.43374633789062,
      "learning_rate": 0.0010467726071176854,
      "loss": 2.8989,
      "step": 1943
    },
    {
      "epoch": 1.944,
      "grad_norm": 71.10095977783203,
      "learning_rate": 0.0010459870722416833,
      "loss": 3.3768,
      "step": 1944
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 213.35336303710938,
      "learning_rate": 0.0010452015089274938,
      "loss": 3.3768,
      "step": 1945
    },
    {
      "epoch": 1.946,
      "grad_norm": 21.644729614257812,
      "learning_rate": 0.0010444159176609053,
      "loss": 3.8047,
      "step": 1946
    },
    {
      "epoch": 1.947,
      "grad_norm": 90.76285552978516,
      "learning_rate": 0.0010436302989277233,
      "loss": 3.4374,
      "step": 1947
    },
    {
      "epoch": 1.948,
      "grad_norm": 22.17421531677246,
      "learning_rate": 0.0010428446532137713,
      "loss": 3.4468,
      "step": 1948
    },
    {
      "epoch": 1.9489999999999998,
      "grad_norm": 55.269500732421875,
      "learning_rate": 0.0010420589810048887,
      "loss": 3.2541,
      "step": 1949
    },
    {
      "epoch": 1.95,
      "grad_norm": 84.01691436767578,
      "learning_rate": 0.0010412732827869311,
      "loss": 2.6146,
      "step": 1950
    },
    {
      "epoch": 1.951,
      "grad_norm": 1721.322998046875,
      "learning_rate": 0.001040487559045771,
      "loss": 2.6405,
      "step": 1951
    },
    {
      "epoch": 1.952,
      "grad_norm": 35.058563232421875,
      "learning_rate": 0.001039701810267296,
      "loss": 2.3121,
      "step": 1952
    },
    {
      "epoch": 1.9529999999999998,
      "grad_norm": 167.04629516601562,
      "learning_rate": 0.0010389160369374095,
      "loss": 2.241,
      "step": 1953
    },
    {
      "epoch": 1.954,
      "grad_norm": 2395.482177734375,
      "learning_rate": 0.00103813023954203,
      "loss": 2.2081,
      "step": 1954
    },
    {
      "epoch": 1.955,
      "grad_norm": 375.2223205566406,
      "learning_rate": 0.0010373444185670913,
      "loss": 2.1419,
      "step": 1955
    },
    {
      "epoch": 1.956,
      "grad_norm": 21.668712615966797,
      "learning_rate": 0.0010365585744985405,
      "loss": 2.3124,
      "step": 1956
    },
    {
      "epoch": 1.9569999999999999,
      "grad_norm": 909.6898803710938,
      "learning_rate": 0.0010357727078223404,
      "loss": 2.1239,
      "step": 1957
    },
    {
      "epoch": 1.958,
      "grad_norm": 449.3364562988281,
      "learning_rate": 0.0010349868190244673,
      "loss": 1.992,
      "step": 1958
    },
    {
      "epoch": 1.959,
      "grad_norm": 462.24139404296875,
      "learning_rate": 0.001034200908590911,
      "loss": 1.9293,
      "step": 1959
    },
    {
      "epoch": 1.96,
      "grad_norm": 48.26146697998047,
      "learning_rate": 0.0010334149770076747,
      "loss": 2.1794,
      "step": 1960
    },
    {
      "epoch": 1.9609999999999999,
      "grad_norm": 32.366554260253906,
      "learning_rate": 0.0010326290247607746,
      "loss": 2.0142,
      "step": 1961
    },
    {
      "epoch": 1.962,
      "grad_norm": 139.3802490234375,
      "learning_rate": 0.0010318430523362405,
      "loss": 1.9112,
      "step": 1962
    },
    {
      "epoch": 1.963,
      "grad_norm": 68.60526275634766,
      "learning_rate": 0.0010310570602201133,
      "loss": 2.0599,
      "step": 1963
    },
    {
      "epoch": 1.964,
      "grad_norm": 167.15940856933594,
      "learning_rate": 0.0010302710488984473,
      "loss": 2.0471,
      "step": 1964
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 614.7646484375,
      "learning_rate": 0.001029485018857308,
      "loss": 2.115,
      "step": 1965
    },
    {
      "epoch": 1.966,
      "grad_norm": 94.95227813720703,
      "learning_rate": 0.0010286989705827723,
      "loss": 2.0466,
      "step": 1966
    },
    {
      "epoch": 1.967,
      "grad_norm": 237.3173370361328,
      "learning_rate": 0.0010279129045609294,
      "loss": 2.2324,
      "step": 1967
    },
    {
      "epoch": 1.968,
      "grad_norm": 161.57159423828125,
      "learning_rate": 0.0010271268212778786,
      "loss": 2.1898,
      "step": 1968
    },
    {
      "epoch": 1.9689999999999999,
      "grad_norm": 153.25840759277344,
      "learning_rate": 0.0010263407212197295,
      "loss": 2.2304,
      "step": 1969
    },
    {
      "epoch": 1.97,
      "grad_norm": 202.4735565185547,
      "learning_rate": 0.0010255546048726035,
      "loss": 2.2049,
      "step": 1970
    },
    {
      "epoch": 1.971,
      "grad_norm": 231.9862518310547,
      "learning_rate": 0.0010247684727226307,
      "loss": 2.1374,
      "step": 1971
    },
    {
      "epoch": 1.972,
      "grad_norm": 132.025390625,
      "learning_rate": 0.0010239823252559516,
      "loss": 2.3463,
      "step": 1972
    },
    {
      "epoch": 1.9729999999999999,
      "grad_norm": 69.98054504394531,
      "learning_rate": 0.0010231961629587163,
      "loss": 2.1668,
      "step": 1973
    },
    {
      "epoch": 1.974,
      "grad_norm": 129.3926239013672,
      "learning_rate": 0.0010224099863170835,
      "loss": 2.2868,
      "step": 1974
    },
    {
      "epoch": 1.975,
      "grad_norm": 52.12026596069336,
      "learning_rate": 0.0010216237958172213,
      "loss": 2.3381,
      "step": 1975
    },
    {
      "epoch": 1.976,
      "grad_norm": 22.95089340209961,
      "learning_rate": 0.001020837591945306,
      "loss": 2.2062,
      "step": 1976
    },
    {
      "epoch": 1.9769999999999999,
      "grad_norm": 147.02685546875,
      "learning_rate": 0.0010200513751875227,
      "loss": 2.3976,
      "step": 1977
    },
    {
      "epoch": 1.978,
      "grad_norm": 47.65959930419922,
      "learning_rate": 0.001019265146030064,
      "loss": 2.4801,
      "step": 1978
    },
    {
      "epoch": 1.979,
      "grad_norm": 236.10980224609375,
      "learning_rate": 0.00101847890495913,
      "loss": 2.3908,
      "step": 1979
    },
    {
      "epoch": 1.98,
      "grad_norm": 656.5185546875,
      "learning_rate": 0.001017692652460929,
      "loss": 2.1316,
      "step": 1980
    },
    {
      "epoch": 1.9809999999999999,
      "grad_norm": 126.65050506591797,
      "learning_rate": 0.001016906389021675,
      "loss": 2.1071,
      "step": 1981
    },
    {
      "epoch": 1.982,
      "grad_norm": 73.7005615234375,
      "learning_rate": 0.0010161201151275905,
      "loss": 2.2919,
      "step": 1982
    },
    {
      "epoch": 1.983,
      "grad_norm": 92.16385650634766,
      "learning_rate": 0.0010153338312649028,
      "loss": 2.3625,
      "step": 1983
    },
    {
      "epoch": 1.984,
      "grad_norm": 1142.21630859375,
      "learning_rate": 0.0010145475379198464,
      "loss": 2.2814,
      "step": 1984
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 458.3420104980469,
      "learning_rate": 0.0010137612355786618,
      "loss": 2.3573,
      "step": 1985
    },
    {
      "epoch": 1.986,
      "grad_norm": 2023.1580810546875,
      "learning_rate": 0.001012974924727594,
      "loss": 2.4806,
      "step": 1986
    },
    {
      "epoch": 1.987,
      "grad_norm": 468.3153076171875,
      "learning_rate": 0.0010121886058528943,
      "loss": 2.8082,
      "step": 1987
    },
    {
      "epoch": 1.988,
      "grad_norm": 43006.48828125,
      "learning_rate": 0.0010114022794408184,
      "loss": 3.0133,
      "step": 1988
    },
    {
      "epoch": 1.9889999999999999,
      "grad_norm": 82010.609375,
      "learning_rate": 0.0010106159459776273,
      "loss": 4.0289,
      "step": 1989
    },
    {
      "epoch": 1.99,
      "grad_norm": 37383.734375,
      "learning_rate": 0.0010098296059495848,
      "loss": 4.7605,
      "step": 1990
    },
    {
      "epoch": 1.991,
      "grad_norm": 46306.80859375,
      "learning_rate": 0.0010090432598429608,
      "loss": 6.6402,
      "step": 1991
    },
    {
      "epoch": 1.992,
      "grad_norm": 127402.1015625,
      "learning_rate": 0.0010082569081440278,
      "loss": 6.5539,
      "step": 1992
    },
    {
      "epoch": 1.9929999999999999,
      "grad_norm": 365793.84375,
      "learning_rate": 0.0010074705513390615,
      "loss": 9.6828,
      "step": 1993
    },
    {
      "epoch": 1.994,
      "grad_norm": 363088.0,
      "learning_rate": 0.0010066841899143424,
      "loss": 15.7443,
      "step": 1994
    },
    {
      "epoch": 1.995,
      "grad_norm": 652688.125,
      "learning_rate": 0.0010058978243561514,
      "loss": 12.7235,
      "step": 1995
    },
    {
      "epoch": 1.996,
      "grad_norm": 24178.90234375,
      "learning_rate": 0.0010051114551507736,
      "loss": 7.8588,
      "step": 1996
    },
    {
      "epoch": 1.9969999999999999,
      "grad_norm": 8407500.0,
      "learning_rate": 0.0010043250827844966,
      "loss": 8.0491,
      "step": 1997
    },
    {
      "epoch": 1.998,
      "grad_norm": 698338.5625,
      "learning_rate": 0.0010035387077436087,
      "loss": 6.9494,
      "step": 1998
    },
    {
      "epoch": 1.999,
      "grad_norm": 22149292.0,
      "learning_rate": 0.0010027523305144003,
      "loss": 7.1158,
      "step": 1999
    },
    {
      "epoch": 2.0,
      "grad_norm": 227109.625,
      "learning_rate": 0.0010019659515831643,
      "loss": 7.2387,
      "step": 2000
    },
    {
      "epoch": 2.0,
      "eval_loss": 6.662415027618408,
      "eval_runtime": 224.8999,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 2000
    },
    {
      "epoch": 2.001,
      "grad_norm": 2503520.0,
      "learning_rate": 0.001001179571436193,
      "loss": 5.8734,
      "step": 2001
    },
    {
      "epoch": 2.002,
      "grad_norm": 154895.5,
      "learning_rate": 0.00100039319055978,
      "loss": 4.9813,
      "step": 2002
    },
    {
      "epoch": 2.003,
      "grad_norm": 465073.0625,
      "learning_rate": 0.00099960680944022,
      "loss": 4.6467,
      "step": 2003
    },
    {
      "epoch": 2.004,
      "grad_norm": 302248.0625,
      "learning_rate": 0.0009988204285638074,
      "loss": 4.7306,
      "step": 2004
    },
    {
      "epoch": 2.005,
      "grad_norm": 146716.90625,
      "learning_rate": 0.0009980340484168357,
      "loss": 4.237,
      "step": 2005
    },
    {
      "epoch": 2.006,
      "grad_norm": 1938719.125,
      "learning_rate": 0.0009972476694855997,
      "loss": 4.7455,
      "step": 2006
    },
    {
      "epoch": 2.007,
      "grad_norm": 179238.3125,
      "learning_rate": 0.0009964612922563915,
      "loss": 4.5704,
      "step": 2007
    },
    {
      "epoch": 2.008,
      "grad_norm": 51547.14453125,
      "learning_rate": 0.0009956749172155036,
      "loss": 4.3508,
      "step": 2008
    },
    {
      "epoch": 2.009,
      "grad_norm": 14162.6396484375,
      "learning_rate": 0.0009948885448492265,
      "loss": 4.0925,
      "step": 2009
    },
    {
      "epoch": 2.01,
      "grad_norm": 22939.904296875,
      "learning_rate": 0.0009941021756438488,
      "loss": 3.6675,
      "step": 2010
    },
    {
      "epoch": 2.011,
      "grad_norm": 3066.794677734375,
      "learning_rate": 0.000993315810085658,
      "loss": 3.6667,
      "step": 2011
    },
    {
      "epoch": 2.012,
      "grad_norm": 30558.55859375,
      "learning_rate": 0.0009925294486609383,
      "loss": 4.1836,
      "step": 2012
    },
    {
      "epoch": 2.013,
      "grad_norm": 48866.84765625,
      "learning_rate": 0.0009917430918559723,
      "loss": 3.3344,
      "step": 2013
    },
    {
      "epoch": 2.014,
      "grad_norm": 54204.03515625,
      "learning_rate": 0.0009909567401570393,
      "loss": 3.9184,
      "step": 2014
    },
    {
      "epoch": 2.015,
      "grad_norm": 19796.60546875,
      "learning_rate": 0.0009901703940504154,
      "loss": 3.8921,
      "step": 2015
    },
    {
      "epoch": 2.016,
      "grad_norm": 2670.1728515625,
      "learning_rate": 0.000989384054022373,
      "loss": 3.8515,
      "step": 2016
    },
    {
      "epoch": 2.017,
      "grad_norm": 19140.42578125,
      "learning_rate": 0.0009885977205591817,
      "loss": 4.5273,
      "step": 2017
    },
    {
      "epoch": 2.018,
      "grad_norm": 3769.029052734375,
      "learning_rate": 0.000987811394147106,
      "loss": 4.0212,
      "step": 2018
    },
    {
      "epoch": 2.019,
      "grad_norm": 5608.498046875,
      "learning_rate": 0.0009870250752724062,
      "loss": 4.3269,
      "step": 2019
    },
    {
      "epoch": 2.02,
      "grad_norm": 1037.9415283203125,
      "learning_rate": 0.000986238764421338,
      "loss": 4.3794,
      "step": 2020
    },
    {
      "epoch": 2.021,
      "grad_norm": 2633.600341796875,
      "learning_rate": 0.0009854524620801536,
      "loss": 4.4858,
      "step": 2021
    },
    {
      "epoch": 2.022,
      "grad_norm": 1531.60009765625,
      "learning_rate": 0.0009846661687350974,
      "loss": 4.3235,
      "step": 2022
    },
    {
      "epoch": 2.023,
      "grad_norm": 3110.239013671875,
      "learning_rate": 0.0009838798848724097,
      "loss": 4.1683,
      "step": 2023
    },
    {
      "epoch": 2.024,
      "grad_norm": 1586.6075439453125,
      "learning_rate": 0.0009830936109783252,
      "loss": 4.3955,
      "step": 2024
    },
    {
      "epoch": 2.025,
      "grad_norm": 4821.84765625,
      "learning_rate": 0.0009823073475390713,
      "loss": 4.4032,
      "step": 2025
    },
    {
      "epoch": 2.026,
      "grad_norm": 48864.53515625,
      "learning_rate": 0.0009815210950408703,
      "loss": 4.5167,
      "step": 2026
    },
    {
      "epoch": 2.027,
      "grad_norm": 1437.666015625,
      "learning_rate": 0.0009807348539699365,
      "loss": 4.5992,
      "step": 2027
    },
    {
      "epoch": 2.028,
      "grad_norm": 12462.703125,
      "learning_rate": 0.0009799486248124774,
      "loss": 4.8335,
      "step": 2028
    },
    {
      "epoch": 2.029,
      "grad_norm": 9709.171875,
      "learning_rate": 0.0009791624080546938,
      "loss": 4.5216,
      "step": 2029
    },
    {
      "epoch": 2.03,
      "grad_norm": 636.6102905273438,
      "learning_rate": 0.0009783762041827787,
      "loss": 4.3067,
      "step": 2030
    },
    {
      "epoch": 2.031,
      "grad_norm": 959.7393188476562,
      "learning_rate": 0.0009775900136829167,
      "loss": 4.312,
      "step": 2031
    },
    {
      "epoch": 2.032,
      "grad_norm": 3882.083984375,
      "learning_rate": 0.0009768038370412838,
      "loss": 4.2824,
      "step": 2032
    },
    {
      "epoch": 2.033,
      "grad_norm": 5867.3896484375,
      "learning_rate": 0.0009760176747440485,
      "loss": 4.145,
      "step": 2033
    },
    {
      "epoch": 2.034,
      "grad_norm": 1855.6595458984375,
      "learning_rate": 0.0009752315272773695,
      "loss": 4.2415,
      "step": 2034
    },
    {
      "epoch": 2.035,
      "grad_norm": 34162.76171875,
      "learning_rate": 0.0009744453951273968,
      "loss": 4.1136,
      "step": 2035
    },
    {
      "epoch": 2.036,
      "grad_norm": 14052.9755859375,
      "learning_rate": 0.0009736592787802709,
      "loss": 4.0318,
      "step": 2036
    },
    {
      "epoch": 2.037,
      "grad_norm": 35532.40234375,
      "learning_rate": 0.0009728731787221219,
      "loss": 4.1296,
      "step": 2037
    },
    {
      "epoch": 2.038,
      "grad_norm": 29138.96484375,
      "learning_rate": 0.0009720870954390704,
      "loss": 4.2397,
      "step": 2038
    },
    {
      "epoch": 2.039,
      "grad_norm": 30113.513671875,
      "learning_rate": 0.0009713010294172276,
      "loss": 4.5177,
      "step": 2039
    },
    {
      "epoch": 2.04,
      "grad_norm": 3101.745361328125,
      "learning_rate": 0.0009705149811426923,
      "loss": 4.2266,
      "step": 2040
    },
    {
      "epoch": 2.041,
      "grad_norm": 70915.2109375,
      "learning_rate": 0.0009697289511015528,
      "loss": 4.4308,
      "step": 2041
    },
    {
      "epoch": 2.042,
      "grad_norm": 3548.315185546875,
      "learning_rate": 0.0009689429397798869,
      "loss": 4.77,
      "step": 2042
    },
    {
      "epoch": 2.043,
      "grad_norm": 541.5228271484375,
      "learning_rate": 0.0009681569476637596,
      "loss": 4.8701,
      "step": 2043
    },
    {
      "epoch": 2.044,
      "grad_norm": 12434.701171875,
      "learning_rate": 0.0009673709752392255,
      "loss": 4.9086,
      "step": 2044
    },
    {
      "epoch": 2.045,
      "grad_norm": 290.00189208984375,
      "learning_rate": 0.0009665850229923258,
      "loss": 4.8502,
      "step": 2045
    },
    {
      "epoch": 2.046,
      "grad_norm": 5008.62158203125,
      "learning_rate": 0.0009657990914090893,
      "loss": 4.5844,
      "step": 2046
    },
    {
      "epoch": 2.047,
      "grad_norm": 12394.9296875,
      "learning_rate": 0.0009650131809755325,
      "loss": 4.8997,
      "step": 2047
    },
    {
      "epoch": 2.048,
      "grad_norm": 3818.894775390625,
      "learning_rate": 0.0009642272921776596,
      "loss": 4.856,
      "step": 2048
    },
    {
      "epoch": 2.049,
      "grad_norm": 20422.69921875,
      "learning_rate": 0.0009634414255014598,
      "loss": 5.0242,
      "step": 2049
    },
    {
      "epoch": 2.05,
      "grad_norm": 4725.158203125,
      "learning_rate": 0.000962655581432909,
      "loss": 4.9588,
      "step": 2050
    },
    {
      "epoch": 2.051,
      "grad_norm": 415.8332824707031,
      "learning_rate": 0.00096186976045797,
      "loss": 5.1715,
      "step": 2051
    },
    {
      "epoch": 2.052,
      "grad_norm": 658.1959228515625,
      "learning_rate": 0.0009610839630625905,
      "loss": 5.5914,
      "step": 2052
    },
    {
      "epoch": 2.053,
      "grad_norm": 944.5579833984375,
      "learning_rate": 0.0009602981897327042,
      "loss": 5.1985,
      "step": 2053
    },
    {
      "epoch": 2.054,
      "grad_norm": 103870.6015625,
      "learning_rate": 0.0009595124409542294,
      "loss": 5.4908,
      "step": 2054
    },
    {
      "epoch": 2.055,
      "grad_norm": 3464.63525390625,
      "learning_rate": 0.0009587267172130689,
      "loss": 5.2239,
      "step": 2055
    },
    {
      "epoch": 2.056,
      "grad_norm": 3248.443359375,
      "learning_rate": 0.0009579410189951114,
      "loss": 5.3976,
      "step": 2056
    },
    {
      "epoch": 2.057,
      "grad_norm": 506.87298583984375,
      "learning_rate": 0.0009571553467862288,
      "loss": 5.2664,
      "step": 2057
    },
    {
      "epoch": 2.058,
      "grad_norm": 2656.46533203125,
      "learning_rate": 0.0009563697010722768,
      "loss": 5.3913,
      "step": 2058
    },
    {
      "epoch": 2.059,
      "grad_norm": 234.47396850585938,
      "learning_rate": 0.0009555840823390948,
      "loss": 5.0831,
      "step": 2059
    },
    {
      "epoch": 2.06,
      "grad_norm": 116.79641723632812,
      "learning_rate": 0.0009547984910725064,
      "loss": 5.577,
      "step": 2060
    },
    {
      "epoch": 2.061,
      "grad_norm": 22265.7578125,
      "learning_rate": 0.0009540129277583167,
      "loss": 5.6441,
      "step": 2061
    },
    {
      "epoch": 2.062,
      "grad_norm": 294.0448913574219,
      "learning_rate": 0.0009532273928823151,
      "loss": 5.3905,
      "step": 2062
    },
    {
      "epoch": 2.063,
      "grad_norm": 5471.56591796875,
      "learning_rate": 0.0009524418869302722,
      "loss": 5.422,
      "step": 2063
    },
    {
      "epoch": 2.064,
      "grad_norm": 532.7857666015625,
      "learning_rate": 0.0009516564103879408,
      "loss": 5.4103,
      "step": 2064
    },
    {
      "epoch": 2.065,
      "grad_norm": 256.9206237792969,
      "learning_rate": 0.0009508709637410566,
      "loss": 5.2254,
      "step": 2065
    },
    {
      "epoch": 2.066,
      "grad_norm": 382.46490478515625,
      "learning_rate": 0.0009500855474753359,
      "loss": 5.0989,
      "step": 2066
    },
    {
      "epoch": 2.067,
      "grad_norm": 1579.2015380859375,
      "learning_rate": 0.0009493001620764763,
      "loss": 5.2416,
      "step": 2067
    },
    {
      "epoch": 2.068,
      "grad_norm": 1766.6175537109375,
      "learning_rate": 0.0009485148080301565,
      "loss": 4.8694,
      "step": 2068
    },
    {
      "epoch": 2.069,
      "grad_norm": 4209.7685546875,
      "learning_rate": 0.0009477294858220356,
      "loss": 4.9194,
      "step": 2069
    },
    {
      "epoch": 2.07,
      "grad_norm": 3604.35595703125,
      "learning_rate": 0.000946944195937753,
      "loss": 4.5945,
      "step": 2070
    },
    {
      "epoch": 2.071,
      "grad_norm": 9592.001953125,
      "learning_rate": 0.0009461589388629287,
      "loss": 4.5654,
      "step": 2071
    },
    {
      "epoch": 2.072,
      "grad_norm": 40172.66015625,
      "learning_rate": 0.0009453737150831616,
      "loss": 4.5909,
      "step": 2072
    },
    {
      "epoch": 2.073,
      "grad_norm": 2144.188720703125,
      "learning_rate": 0.0009445885250840301,
      "loss": 4.6244,
      "step": 2073
    },
    {
      "epoch": 2.074,
      "grad_norm": 2224.881103515625,
      "learning_rate": 0.0009438033693510925,
      "loss": 4.7478,
      "step": 2074
    },
    {
      "epoch": 2.075,
      "grad_norm": 1393.7774658203125,
      "learning_rate": 0.0009430182483698853,
      "loss": 4.663,
      "step": 2075
    },
    {
      "epoch": 2.076,
      "grad_norm": 5364.875,
      "learning_rate": 0.0009422331626259235,
      "loss": 4.6952,
      "step": 2076
    },
    {
      "epoch": 2.077,
      "grad_norm": 1557.14501953125,
      "learning_rate": 0.0009414481126047005,
      "loss": 4.6882,
      "step": 2077
    },
    {
      "epoch": 2.078,
      "grad_norm": 355.6181335449219,
      "learning_rate": 0.0009406630987916875,
      "loss": 4.6505,
      "step": 2078
    },
    {
      "epoch": 2.079,
      "grad_norm": 8760.5419921875,
      "learning_rate": 0.000939878121672333,
      "loss": 4.6988,
      "step": 2079
    },
    {
      "epoch": 2.08,
      "grad_norm": 1459.196044921875,
      "learning_rate": 0.0009390931817320638,
      "loss": 4.3739,
      "step": 2080
    },
    {
      "epoch": 2.081,
      "grad_norm": 2812.476318359375,
      "learning_rate": 0.0009383082794562823,
      "loss": 4.6874,
      "step": 2081
    },
    {
      "epoch": 2.082,
      "grad_norm": 119.74500274658203,
      "learning_rate": 0.0009375234153303684,
      "loss": 4.4448,
      "step": 2082
    },
    {
      "epoch": 2.083,
      "grad_norm": 275.3872375488281,
      "learning_rate": 0.000936738589839679,
      "loss": 4.751,
      "step": 2083
    },
    {
      "epoch": 2.084,
      "grad_norm": 1317.364990234375,
      "learning_rate": 0.0009359538034695462,
      "loss": 4.547,
      "step": 2084
    },
    {
      "epoch": 2.085,
      "grad_norm": 1842.9642333984375,
      "learning_rate": 0.0009351690567052779,
      "loss": 4.4149,
      "step": 2085
    },
    {
      "epoch": 2.086,
      "grad_norm": 214.5895233154297,
      "learning_rate": 0.0009343843500321581,
      "loss": 4.3529,
      "step": 2086
    },
    {
      "epoch": 2.087,
      "grad_norm": 476.66778564453125,
      "learning_rate": 0.0009335996839354457,
      "loss": 4.5325,
      "step": 2087
    },
    {
      "epoch": 2.088,
      "grad_norm": 491.9464416503906,
      "learning_rate": 0.0009328150589003741,
      "loss": 4.0235,
      "step": 2088
    },
    {
      "epoch": 2.089,
      "grad_norm": 873.5132446289062,
      "learning_rate": 0.0009320304754121522,
      "loss": 4.2963,
      "step": 2089
    },
    {
      "epoch": 2.09,
      "grad_norm": 1354.940673828125,
      "learning_rate": 0.0009312459339559619,
      "loss": 4.3038,
      "step": 2090
    },
    {
      "epoch": 2.091,
      "grad_norm": 112.52726745605469,
      "learning_rate": 0.0009304614350169611,
      "loss": 4.1584,
      "step": 2091
    },
    {
      "epoch": 2.092,
      "grad_norm": 303.1717224121094,
      "learning_rate": 0.0009296769790802794,
      "loss": 4.2096,
      "step": 2092
    },
    {
      "epoch": 2.093,
      "grad_norm": 89.2029800415039,
      "learning_rate": 0.000928892566631021,
      "loss": 4.1587,
      "step": 2093
    },
    {
      "epoch": 2.094,
      "grad_norm": 2048.8037109375,
      "learning_rate": 0.0009281081981542626,
      "loss": 4.1683,
      "step": 2094
    },
    {
      "epoch": 2.095,
      "grad_norm": 8720.9873046875,
      "learning_rate": 0.0009273238741350542,
      "loss": 4.1045,
      "step": 2095
    },
    {
      "epoch": 2.096,
      "grad_norm": 165305.484375,
      "learning_rate": 0.000926539595058418,
      "loss": 3.7982,
      "step": 2096
    },
    {
      "epoch": 2.097,
      "grad_norm": 53.59455871582031,
      "learning_rate": 0.0009257553614093481,
      "loss": 3.8365,
      "step": 2097
    },
    {
      "epoch": 2.098,
      "grad_norm": 797.9557495117188,
      "learning_rate": 0.0009249711736728115,
      "loss": 3.7388,
      "step": 2098
    },
    {
      "epoch": 2.099,
      "grad_norm": 125.56859588623047,
      "learning_rate": 0.0009241870323337453,
      "loss": 3.8515,
      "step": 2099
    },
    {
      "epoch": 2.1,
      "grad_norm": 1176.4979248046875,
      "learning_rate": 0.0009234029378770599,
      "loss": 3.8647,
      "step": 2100
    },
    {
      "epoch": 2.1,
      "eval_loss": 3.8805019855499268,
      "eval_runtime": 224.7927,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 2100
    },
    {
      "epoch": 2.101,
      "grad_norm": 482.5692443847656,
      "learning_rate": 0.0009226188907876348,
      "loss": 3.8557,
      "step": 2101
    },
    {
      "epoch": 2.102,
      "grad_norm": 41227.62109375,
      "learning_rate": 0.0009218348915503214,
      "loss": 3.9835,
      "step": 2102
    },
    {
      "epoch": 2.103,
      "grad_norm": 401.8652038574219,
      "learning_rate": 0.0009210509406499407,
      "loss": 3.8907,
      "step": 2103
    },
    {
      "epoch": 2.104,
      "grad_norm": 962.0974731445312,
      "learning_rate": 0.000920267038571285,
      "loss": 3.9534,
      "step": 2104
    },
    {
      "epoch": 2.105,
      "grad_norm": 1422.712646484375,
      "learning_rate": 0.0009194831857991148,
      "loss": 3.8415,
      "step": 2105
    },
    {
      "epoch": 2.106,
      "grad_norm": 684.1978149414062,
      "learning_rate": 0.0009186993828181612,
      "loss": 3.9121,
      "step": 2106
    },
    {
      "epoch": 2.107,
      "grad_norm": 206.97805786132812,
      "learning_rate": 0.0009179156301131246,
      "loss": 3.8635,
      "step": 2107
    },
    {
      "epoch": 2.108,
      "grad_norm": 436.52117919921875,
      "learning_rate": 0.0009171319281686732,
      "loss": 3.7857,
      "step": 2108
    },
    {
      "epoch": 2.109,
      "grad_norm": 240.82614135742188,
      "learning_rate": 0.0009163482774694454,
      "loss": 3.466,
      "step": 2109
    },
    {
      "epoch": 2.11,
      "grad_norm": 800.3075561523438,
      "learning_rate": 0.0009155646785000466,
      "loss": 3.7925,
      "step": 2110
    },
    {
      "epoch": 2.111,
      "grad_norm": 189.14993286132812,
      "learning_rate": 0.0009147811317450511,
      "loss": 3.5648,
      "step": 2111
    },
    {
      "epoch": 2.112,
      "grad_norm": 1453.82080078125,
      "learning_rate": 0.0009139976376889999,
      "loss": 3.5081,
      "step": 2112
    },
    {
      "epoch": 2.113,
      "grad_norm": 266.6135559082031,
      "learning_rate": 0.0009132141968164026,
      "loss": 3.4731,
      "step": 2113
    },
    {
      "epoch": 2.114,
      "grad_norm": 284.1903991699219,
      "learning_rate": 0.0009124308096117351,
      "loss": 3.4067,
      "step": 2114
    },
    {
      "epoch": 2.115,
      "grad_norm": 221.66281127929688,
      "learning_rate": 0.0009116474765594402,
      "loss": 3.3704,
      "step": 2115
    },
    {
      "epoch": 2.116,
      "grad_norm": 102.30709075927734,
      "learning_rate": 0.0009108641981439275,
      "loss": 3.39,
      "step": 2116
    },
    {
      "epoch": 2.117,
      "grad_norm": 112.96664428710938,
      "learning_rate": 0.0009100809748495722,
      "loss": 3.2838,
      "step": 2117
    },
    {
      "epoch": 2.118,
      "grad_norm": 126.50260925292969,
      "learning_rate": 0.0009092978071607166,
      "loss": 3.3787,
      "step": 2118
    },
    {
      "epoch": 2.1189999999999998,
      "grad_norm": 709.3699951171875,
      "learning_rate": 0.0009085146955616679,
      "loss": 3.4922,
      "step": 2119
    },
    {
      "epoch": 2.12,
      "grad_norm": 1361.759033203125,
      "learning_rate": 0.0009077316405366981,
      "loss": 3.1521,
      "step": 2120
    },
    {
      "epoch": 2.121,
      "grad_norm": 105.28962707519531,
      "learning_rate": 0.0009069486425700448,
      "loss": 3.5455,
      "step": 2121
    },
    {
      "epoch": 2.122,
      "grad_norm": 175.96192932128906,
      "learning_rate": 0.0009061657021459106,
      "loss": 3.2651,
      "step": 2122
    },
    {
      "epoch": 2.123,
      "grad_norm": 225.81480407714844,
      "learning_rate": 0.000905382819748462,
      "loss": 3.4267,
      "step": 2123
    },
    {
      "epoch": 2.124,
      "grad_norm": 106.30233001708984,
      "learning_rate": 0.0009045999958618292,
      "loss": 3.1153,
      "step": 2124
    },
    {
      "epoch": 2.125,
      "grad_norm": 127.90862274169922,
      "learning_rate": 0.0009038172309701072,
      "loss": 3.3677,
      "step": 2125
    },
    {
      "epoch": 2.126,
      "grad_norm": 257.11663818359375,
      "learning_rate": 0.0009030345255573545,
      "loss": 3.1231,
      "step": 2126
    },
    {
      "epoch": 2.127,
      "grad_norm": 283.62451171875,
      "learning_rate": 0.000902251880107592,
      "loss": 3.2224,
      "step": 2127
    },
    {
      "epoch": 2.128,
      "grad_norm": 7503.0908203125,
      "learning_rate": 0.0009014692951048042,
      "loss": 3.1175,
      "step": 2128
    },
    {
      "epoch": 2.129,
      "grad_norm": 88.80368041992188,
      "learning_rate": 0.0009006867710329378,
      "loss": 3.2476,
      "step": 2129
    },
    {
      "epoch": 2.13,
      "grad_norm": 8250.4150390625,
      "learning_rate": 0.0008999043083759016,
      "loss": 3.2538,
      "step": 2130
    },
    {
      "epoch": 2.1310000000000002,
      "grad_norm": 343.4037170410156,
      "learning_rate": 0.0008991219076175676,
      "loss": 3.2402,
      "step": 2131
    },
    {
      "epoch": 2.132,
      "grad_norm": 313.4421081542969,
      "learning_rate": 0.0008983395692417681,
      "loss": 3.0469,
      "step": 2132
    },
    {
      "epoch": 2.133,
      "grad_norm": 41.32957458496094,
      "learning_rate": 0.0008975572937322976,
      "loss": 3.1612,
      "step": 2133
    },
    {
      "epoch": 2.134,
      "grad_norm": 274.1636047363281,
      "learning_rate": 0.0008967750815729113,
      "loss": 3.0925,
      "step": 2134
    },
    {
      "epoch": 2.135,
      "grad_norm": 4929.1162109375,
      "learning_rate": 0.0008959929332473262,
      "loss": 3.3158,
      "step": 2135
    },
    {
      "epoch": 2.136,
      "grad_norm": 51.92389678955078,
      "learning_rate": 0.0008952108492392186,
      "loss": 2.987,
      "step": 2136
    },
    {
      "epoch": 2.137,
      "grad_norm": 200.73574829101562,
      "learning_rate": 0.0008944288300322258,
      "loss": 3.1034,
      "step": 2137
    },
    {
      "epoch": 2.138,
      "grad_norm": 98.36006164550781,
      "learning_rate": 0.0008936468761099448,
      "loss": 3.3312,
      "step": 2138
    },
    {
      "epoch": 2.1390000000000002,
      "grad_norm": 218.27621459960938,
      "learning_rate": 0.0008928649879559317,
      "loss": 2.9972,
      "step": 2139
    },
    {
      "epoch": 2.14,
      "grad_norm": 81.85137939453125,
      "learning_rate": 0.000892083166053703,
      "loss": 3.0474,
      "step": 2140
    },
    {
      "epoch": 2.141,
      "grad_norm": 188.39794921875,
      "learning_rate": 0.0008913014108867335,
      "loss": 2.9011,
      "step": 2141
    },
    {
      "epoch": 2.142,
      "grad_norm": 470.2411193847656,
      "learning_rate": 0.0008905197229384566,
      "loss": 2.9069,
      "step": 2142
    },
    {
      "epoch": 2.143,
      "grad_norm": 42.344669342041016,
      "learning_rate": 0.0008897381026922645,
      "loss": 3.0112,
      "step": 2143
    },
    {
      "epoch": 2.144,
      "grad_norm": 1097.74951171875,
      "learning_rate": 0.000888956550631508,
      "loss": 3.0787,
      "step": 2144
    },
    {
      "epoch": 2.145,
      "grad_norm": 47.53384017944336,
      "learning_rate": 0.0008881750672394946,
      "loss": 3.0739,
      "step": 2145
    },
    {
      "epoch": 2.146,
      "grad_norm": 278.112548828125,
      "learning_rate": 0.0008873936529994904,
      "loss": 2.973,
      "step": 2146
    },
    {
      "epoch": 2.147,
      "grad_norm": 31.852628707885742,
      "learning_rate": 0.000886612308394718,
      "loss": 2.9159,
      "step": 2147
    },
    {
      "epoch": 2.148,
      "grad_norm": 6211.861328125,
      "learning_rate": 0.000885831033908357,
      "loss": 2.915,
      "step": 2148
    },
    {
      "epoch": 2.149,
      "grad_norm": 130.12110900878906,
      "learning_rate": 0.0008850498300235443,
      "loss": 2.9299,
      "step": 2149
    },
    {
      "epoch": 2.15,
      "grad_norm": 824.2913818359375,
      "learning_rate": 0.0008842686972233725,
      "loss": 2.8387,
      "step": 2150
    },
    {
      "epoch": 2.151,
      "grad_norm": 224.25445556640625,
      "learning_rate": 0.0008834876359908901,
      "loss": 2.9419,
      "step": 2151
    },
    {
      "epoch": 2.152,
      "grad_norm": 218.99090576171875,
      "learning_rate": 0.0008827066468091018,
      "loss": 3.0587,
      "step": 2152
    },
    {
      "epoch": 2.153,
      "grad_norm": 214.59420776367188,
      "learning_rate": 0.000881925730160968,
      "loss": 3.0409,
      "step": 2153
    },
    {
      "epoch": 2.154,
      "grad_norm": 42.67775344848633,
      "learning_rate": 0.0008811448865294035,
      "loss": 2.9296,
      "step": 2154
    },
    {
      "epoch": 2.155,
      "grad_norm": 203.0156707763672,
      "learning_rate": 0.0008803641163972787,
      "loss": 3.0265,
      "step": 2155
    },
    {
      "epoch": 2.156,
      "grad_norm": 38.91101837158203,
      "learning_rate": 0.0008795834202474178,
      "loss": 2.9275,
      "step": 2156
    },
    {
      "epoch": 2.157,
      "grad_norm": 112.84730529785156,
      "learning_rate": 0.0008788027985625993,
      "loss": 2.8252,
      "step": 2157
    },
    {
      "epoch": 2.158,
      "grad_norm": 41.23545455932617,
      "learning_rate": 0.0008780222518255568,
      "loss": 2.8794,
      "step": 2158
    },
    {
      "epoch": 2.159,
      "grad_norm": 206.37164306640625,
      "learning_rate": 0.000877241780518976,
      "loss": 2.7764,
      "step": 2159
    },
    {
      "epoch": 2.16,
      "grad_norm": 91.78655242919922,
      "learning_rate": 0.0008764613851254967,
      "loss": 2.7972,
      "step": 2160
    },
    {
      "epoch": 2.161,
      "grad_norm": 629.0062866210938,
      "learning_rate": 0.000875681066127712,
      "loss": 2.9007,
      "step": 2161
    },
    {
      "epoch": 2.162,
      "grad_norm": 565.5755004882812,
      "learning_rate": 0.0008749008240081677,
      "loss": 3.0163,
      "step": 2162
    },
    {
      "epoch": 2.163,
      "grad_norm": 85.41349029541016,
      "learning_rate": 0.0008741206592493611,
      "loss": 2.7815,
      "step": 2163
    },
    {
      "epoch": 2.164,
      "grad_norm": 165.3278045654297,
      "learning_rate": 0.0008733405723337432,
      "loss": 2.7801,
      "step": 2164
    },
    {
      "epoch": 2.165,
      "grad_norm": 278.9884033203125,
      "learning_rate": 0.0008725605637437155,
      "loss": 2.8821,
      "step": 2165
    },
    {
      "epoch": 2.166,
      "grad_norm": 1021.3570556640625,
      "learning_rate": 0.0008717806339616316,
      "loss": 2.868,
      "step": 2166
    },
    {
      "epoch": 2.167,
      "grad_norm": 409.4934997558594,
      "learning_rate": 0.000871000783469797,
      "loss": 2.792,
      "step": 2167
    },
    {
      "epoch": 2.168,
      "grad_norm": 372.87890625,
      "learning_rate": 0.0008702210127504667,
      "loss": 2.9053,
      "step": 2168
    },
    {
      "epoch": 2.169,
      "grad_norm": 571.7699584960938,
      "learning_rate": 0.0008694413222858477,
      "loss": 2.946,
      "step": 2169
    },
    {
      "epoch": 2.17,
      "grad_norm": 180.0287322998047,
      "learning_rate": 0.0008686617125580968,
      "loss": 3.0168,
      "step": 2170
    },
    {
      "epoch": 2.171,
      "grad_norm": 596.3489990234375,
      "learning_rate": 0.0008678821840493212,
      "loss": 2.9778,
      "step": 2171
    },
    {
      "epoch": 2.172,
      "grad_norm": 762.7310180664062,
      "learning_rate": 0.0008671027372415772,
      "loss": 2.9886,
      "step": 2172
    },
    {
      "epoch": 2.173,
      "grad_norm": 680.9160766601562,
      "learning_rate": 0.0008663233726168715,
      "loss": 2.9921,
      "step": 2173
    },
    {
      "epoch": 2.174,
      "grad_norm": 727.5517578125,
      "learning_rate": 0.0008655440906571592,
      "loss": 3.0775,
      "step": 2174
    },
    {
      "epoch": 2.175,
      "grad_norm": 1342.992919921875,
      "learning_rate": 0.0008647648918443444,
      "loss": 3.1567,
      "step": 2175
    },
    {
      "epoch": 2.176,
      "grad_norm": 840.3110961914062,
      "learning_rate": 0.0008639857766602805,
      "loss": 3.3796,
      "step": 2176
    },
    {
      "epoch": 2.177,
      "grad_norm": 1241.484375,
      "learning_rate": 0.000863206745586768,
      "loss": 3.0692,
      "step": 2177
    },
    {
      "epoch": 2.178,
      "grad_norm": 1159.0638427734375,
      "learning_rate": 0.0008624277991055562,
      "loss": 2.9154,
      "step": 2178
    },
    {
      "epoch": 2.179,
      "grad_norm": 3591.170654296875,
      "learning_rate": 0.0008616489376983424,
      "loss": 2.9943,
      "step": 2179
    },
    {
      "epoch": 2.18,
      "grad_norm": 717.09765625,
      "learning_rate": 0.0008608701618467705,
      "loss": 3.07,
      "step": 2180
    },
    {
      "epoch": 2.181,
      "grad_norm": 825.808349609375,
      "learning_rate": 0.0008600914720324316,
      "loss": 2.9145,
      "step": 2181
    },
    {
      "epoch": 2.182,
      "grad_norm": 323.4488830566406,
      "learning_rate": 0.0008593128687368641,
      "loss": 2.9498,
      "step": 2182
    },
    {
      "epoch": 2.183,
      "grad_norm": 1054.50537109375,
      "learning_rate": 0.0008585343524415524,
      "loss": 2.9286,
      "step": 2183
    },
    {
      "epoch": 2.184,
      "grad_norm": 1267.560791015625,
      "learning_rate": 0.0008577559236279272,
      "loss": 2.925,
      "step": 2184
    },
    {
      "epoch": 2.185,
      "grad_norm": 2369.644775390625,
      "learning_rate": 0.0008569775827773655,
      "loss": 2.9825,
      "step": 2185
    },
    {
      "epoch": 2.186,
      "grad_norm": 808.91845703125,
      "learning_rate": 0.0008561993303711893,
      "loss": 2.9356,
      "step": 2186
    },
    {
      "epoch": 2.187,
      "grad_norm": 592.9122924804688,
      "learning_rate": 0.000855421166890666,
      "loss": 2.7446,
      "step": 2187
    },
    {
      "epoch": 2.188,
      "grad_norm": 484.9397888183594,
      "learning_rate": 0.000854643092817009,
      "loss": 2.8819,
      "step": 2188
    },
    {
      "epoch": 2.189,
      "grad_norm": 310.0497741699219,
      "learning_rate": 0.0008538651086313753,
      "loss": 2.8197,
      "step": 2189
    },
    {
      "epoch": 2.19,
      "grad_norm": 318.3031311035156,
      "learning_rate": 0.0008530872148148662,
      "loss": 2.9458,
      "step": 2190
    },
    {
      "epoch": 2.191,
      "grad_norm": 2931.881103515625,
      "learning_rate": 0.0008523094118485279,
      "loss": 3.2161,
      "step": 2191
    },
    {
      "epoch": 2.192,
      "grad_norm": 1219.9034423828125,
      "learning_rate": 0.0008515317002133503,
      "loss": 3.2882,
      "step": 2192
    },
    {
      "epoch": 2.193,
      "grad_norm": 438.03985595703125,
      "learning_rate": 0.0008507540803902661,
      "loss": 3.4086,
      "step": 2193
    },
    {
      "epoch": 2.194,
      "grad_norm": 2111.239990234375,
      "learning_rate": 0.000849976552860152,
      "loss": 3.5559,
      "step": 2194
    },
    {
      "epoch": 2.195,
      "grad_norm": 248.02757263183594,
      "learning_rate": 0.0008491991181038268,
      "loss": 3.8282,
      "step": 2195
    },
    {
      "epoch": 2.196,
      "grad_norm": 2735.255859375,
      "learning_rate": 0.0008484217766020532,
      "loss": 3.7547,
      "step": 2196
    },
    {
      "epoch": 2.197,
      "grad_norm": 305.1540222167969,
      "learning_rate": 0.0008476445288355356,
      "loss": 3.8331,
      "step": 2197
    },
    {
      "epoch": 2.198,
      "grad_norm": 2903.077392578125,
      "learning_rate": 0.00084686737528492,
      "loss": 4.0129,
      "step": 2198
    },
    {
      "epoch": 2.199,
      "grad_norm": 716.6660766601562,
      "learning_rate": 0.0008460903164307942,
      "loss": 4.1047,
      "step": 2199
    },
    {
      "epoch": 2.2,
      "grad_norm": 69490.9609375,
      "learning_rate": 0.000845313352753688,
      "loss": 4.0792,
      "step": 2200
    },
    {
      "epoch": 2.2,
      "eval_loss": 4.21364164352417,
      "eval_runtime": 224.538,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 2200
    },
    {
      "epoch": 2.201,
      "grad_norm": 1042.4229736328125,
      "learning_rate": 0.0008445364847340724,
      "loss": 4.3576,
      "step": 2201
    },
    {
      "epoch": 2.202,
      "grad_norm": 2820.588134765625,
      "learning_rate": 0.0008437597128523581,
      "loss": 4.2952,
      "step": 2202
    },
    {
      "epoch": 2.203,
      "grad_norm": 1889.1131591796875,
      "learning_rate": 0.000842983037588898,
      "loss": 4.3246,
      "step": 2203
    },
    {
      "epoch": 2.204,
      "grad_norm": 121.09799194335938,
      "learning_rate": 0.0008422064594239833,
      "loss": 4.0499,
      "step": 2204
    },
    {
      "epoch": 2.205,
      "grad_norm": 543.288330078125,
      "learning_rate": 0.0008414299788378475,
      "loss": 4.1311,
      "step": 2205
    },
    {
      "epoch": 2.206,
      "grad_norm": 2977.7939453125,
      "learning_rate": 0.000840653596310662,
      "loss": 4.1764,
      "step": 2206
    },
    {
      "epoch": 2.207,
      "grad_norm": 183.03558349609375,
      "learning_rate": 0.0008398773123225382,
      "loss": 4.363,
      "step": 2207
    },
    {
      "epoch": 2.208,
      "grad_norm": 465.9208068847656,
      "learning_rate": 0.0008391011273535259,
      "loss": 4.3646,
      "step": 2208
    },
    {
      "epoch": 2.209,
      "grad_norm": 1812.3004150390625,
      "learning_rate": 0.0008383250418836151,
      "loss": 4.4919,
      "step": 2209
    },
    {
      "epoch": 2.21,
      "grad_norm": 29548.494140625,
      "learning_rate": 0.0008375490563927328,
      "loss": 4.2832,
      "step": 2210
    },
    {
      "epoch": 2.211,
      "grad_norm": 2086.6435546875,
      "learning_rate": 0.0008367731713607445,
      "loss": 4.3863,
      "step": 2211
    },
    {
      "epoch": 2.212,
      "grad_norm": 4083.216796875,
      "learning_rate": 0.0008359973872674544,
      "loss": 4.4846,
      "step": 2212
    },
    {
      "epoch": 2.213,
      "grad_norm": 334.6087646484375,
      "learning_rate": 0.0008352217045926033,
      "loss": 4.2749,
      "step": 2213
    },
    {
      "epoch": 2.214,
      "grad_norm": 858.3285522460938,
      "learning_rate": 0.00083444612381587,
      "loss": 4.4547,
      "step": 2214
    },
    {
      "epoch": 2.215,
      "grad_norm": 86.49554443359375,
      "learning_rate": 0.00083367064541687,
      "loss": 4.4697,
      "step": 2215
    },
    {
      "epoch": 2.216,
      "grad_norm": 175.2657012939453,
      "learning_rate": 0.0008328952698751554,
      "loss": 4.4079,
      "step": 2216
    },
    {
      "epoch": 2.217,
      "grad_norm": 143.2085418701172,
      "learning_rate": 0.0008321199976702145,
      "loss": 4.253,
      "step": 2217
    },
    {
      "epoch": 2.218,
      "grad_norm": 24.116147994995117,
      "learning_rate": 0.0008313448292814723,
      "loss": 4.162,
      "step": 2218
    },
    {
      "epoch": 2.219,
      "grad_norm": 201.4631805419922,
      "learning_rate": 0.000830569765188289,
      "loss": 4.0933,
      "step": 2219
    },
    {
      "epoch": 2.22,
      "grad_norm": 379.58990478515625,
      "learning_rate": 0.0008297948058699608,
      "loss": 3.7803,
      "step": 2220
    },
    {
      "epoch": 2.221,
      "grad_norm": 158.09120178222656,
      "learning_rate": 0.0008290199518057188,
      "loss": 4.0356,
      "step": 2221
    },
    {
      "epoch": 2.222,
      "grad_norm": 306.7885437011719,
      "learning_rate": 0.0008282452034747287,
      "loss": 3.9436,
      "step": 2222
    },
    {
      "epoch": 2.223,
      "grad_norm": 782.9981689453125,
      "learning_rate": 0.0008274705613560918,
      "loss": 4.0859,
      "step": 2223
    },
    {
      "epoch": 2.224,
      "grad_norm": 956.1825561523438,
      "learning_rate": 0.000826696025928843,
      "loss": 4.0545,
      "step": 2224
    },
    {
      "epoch": 2.225,
      "grad_norm": 11290.7021484375,
      "learning_rate": 0.0008259215976719511,
      "loss": 4.133,
      "step": 2225
    },
    {
      "epoch": 2.226,
      "grad_norm": 1845.21240234375,
      "learning_rate": 0.0008251472770643189,
      "loss": 4.1657,
      "step": 2226
    },
    {
      "epoch": 2.227,
      "grad_norm": 3751.506591796875,
      "learning_rate": 0.0008243730645847828,
      "loss": 4.0408,
      "step": 2227
    },
    {
      "epoch": 2.228,
      "grad_norm": 251.25656127929688,
      "learning_rate": 0.0008235989607121118,
      "loss": 4.2391,
      "step": 2228
    },
    {
      "epoch": 2.229,
      "grad_norm": 89.23355102539062,
      "learning_rate": 0.0008228249659250086,
      "loss": 4.3573,
      "step": 2229
    },
    {
      "epoch": 2.23,
      "grad_norm": 282.61151123046875,
      "learning_rate": 0.000822051080702107,
      "loss": 4.0375,
      "step": 2230
    },
    {
      "epoch": 2.231,
      "grad_norm": 410.9374084472656,
      "learning_rate": 0.0008212773055219752,
      "loss": 3.7629,
      "step": 2231
    },
    {
      "epoch": 2.232,
      "grad_norm": 796.7774658203125,
      "learning_rate": 0.0008205036408631109,
      "loss": 3.5568,
      "step": 2232
    },
    {
      "epoch": 2.233,
      "grad_norm": 768.7659912109375,
      "learning_rate": 0.0008197300872039456,
      "loss": 3.3846,
      "step": 2233
    },
    {
      "epoch": 2.234,
      "grad_norm": 450.2730407714844,
      "learning_rate": 0.0008189566450228408,
      "loss": 3.323,
      "step": 2234
    },
    {
      "epoch": 2.235,
      "grad_norm": 151.3351593017578,
      "learning_rate": 0.0008181833147980894,
      "loss": 3.44,
      "step": 2235
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 243.51768493652344,
      "learning_rate": 0.0008174100970079153,
      "loss": 3.4004,
      "step": 2236
    },
    {
      "epoch": 2.237,
      "grad_norm": 175.26060485839844,
      "learning_rate": 0.0008166369921304726,
      "loss": 3.4909,
      "step": 2237
    },
    {
      "epoch": 2.238,
      "grad_norm": 368.3842468261719,
      "learning_rate": 0.000815864000643846,
      "loss": 3.5202,
      "step": 2238
    },
    {
      "epoch": 2.239,
      "grad_norm": 789.0948486328125,
      "learning_rate": 0.0008150911230260492,
      "loss": 3.4764,
      "step": 2239
    },
    {
      "epoch": 2.24,
      "grad_norm": 200.17213439941406,
      "learning_rate": 0.0008143183597550266,
      "loss": 3.5767,
      "step": 2240
    },
    {
      "epoch": 2.241,
      "grad_norm": 1916.7647705078125,
      "learning_rate": 0.0008135457113086511,
      "loss": 3.5553,
      "step": 2241
    },
    {
      "epoch": 2.242,
      "grad_norm": 69.29048919677734,
      "learning_rate": 0.0008127731781647252,
      "loss": 3.6282,
      "step": 2242
    },
    {
      "epoch": 2.243,
      "grad_norm": 51.693660736083984,
      "learning_rate": 0.0008120007608009793,
      "loss": 3.6375,
      "step": 2243
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 31.471208572387695,
      "learning_rate": 0.0008112284596950727,
      "loss": 3.5689,
      "step": 2244
    },
    {
      "epoch": 2.245,
      "grad_norm": 192.68153381347656,
      "learning_rate": 0.0008104562753245929,
      "loss": 3.4918,
      "step": 2245
    },
    {
      "epoch": 2.246,
      "grad_norm": 198.35911560058594,
      "learning_rate": 0.0008096842081670547,
      "loss": 3.4369,
      "step": 2246
    },
    {
      "epoch": 2.247,
      "grad_norm": 90.47209167480469,
      "learning_rate": 0.0008089122586999015,
      "loss": 3.4041,
      "step": 2247
    },
    {
      "epoch": 2.248,
      "grad_norm": 418.5999755859375,
      "learning_rate": 0.0008081404274005022,
      "loss": 3.4529,
      "step": 2248
    },
    {
      "epoch": 2.249,
      "grad_norm": 74.20178985595703,
      "learning_rate": 0.0008073687147461547,
      "loss": 3.4152,
      "step": 2249
    },
    {
      "epoch": 2.25,
      "grad_norm": 156.7352752685547,
      "learning_rate": 0.0008065971212140817,
      "loss": 3.5567,
      "step": 2250
    },
    {
      "epoch": 2.251,
      "grad_norm": 129.822998046875,
      "learning_rate": 0.0008058256472814333,
      "loss": 3.6814,
      "step": 2251
    },
    {
      "epoch": 2.252,
      "grad_norm": 940.2536010742188,
      "learning_rate": 0.0008050542934252854,
      "loss": 3.3957,
      "step": 2252
    },
    {
      "epoch": 2.253,
      "grad_norm": 563.2747802734375,
      "learning_rate": 0.0008042830601226392,
      "loss": 3.365,
      "step": 2253
    },
    {
      "epoch": 2.254,
      "grad_norm": 92.72300720214844,
      "learning_rate": 0.000803511947850422,
      "loss": 3.4911,
      "step": 2254
    },
    {
      "epoch": 2.255,
      "grad_norm": 714.958984375,
      "learning_rate": 0.0008027409570854858,
      "loss": 3.2972,
      "step": 2255
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 56.6925163269043,
      "learning_rate": 0.0008019700883046079,
      "loss": 3.3232,
      "step": 2256
    },
    {
      "epoch": 2.257,
      "grad_norm": 198.65904235839844,
      "learning_rate": 0.0008011993419844895,
      "loss": 3.2509,
      "step": 2257
    },
    {
      "epoch": 2.258,
      "grad_norm": 78.09991455078125,
      "learning_rate": 0.000800428718601757,
      "loss": 3.2098,
      "step": 2258
    },
    {
      "epoch": 2.259,
      "grad_norm": 67.01753234863281,
      "learning_rate": 0.0007996582186329598,
      "loss": 3.3496,
      "step": 2259
    },
    {
      "epoch": 2.26,
      "grad_norm": 12.010086059570312,
      "learning_rate": 0.0007988878425545719,
      "loss": 3.2277,
      "step": 2260
    },
    {
      "epoch": 2.261,
      "grad_norm": 86.60587310791016,
      "learning_rate": 0.00079811759084299,
      "loss": 3.1872,
      "step": 2261
    },
    {
      "epoch": 2.262,
      "grad_norm": 119.46276092529297,
      "learning_rate": 0.0007973474639745337,
      "loss": 3.2115,
      "step": 2262
    },
    {
      "epoch": 2.263,
      "grad_norm": 522.0625,
      "learning_rate": 0.0007965774624254464,
      "loss": 3.2088,
      "step": 2263
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 52.61154556274414,
      "learning_rate": 0.000795807586671893,
      "loss": 3.3056,
      "step": 2264
    },
    {
      "epoch": 2.265,
      "grad_norm": 70.5064468383789,
      "learning_rate": 0.0007950378371899609,
      "loss": 3.1078,
      "step": 2265
    },
    {
      "epoch": 2.266,
      "grad_norm": 41.24247360229492,
      "learning_rate": 0.0007942682144556604,
      "loss": 3.1903,
      "step": 2266
    },
    {
      "epoch": 2.267,
      "grad_norm": 144.9609375,
      "learning_rate": 0.0007934987189449218,
      "loss": 3.1593,
      "step": 2267
    },
    {
      "epoch": 2.268,
      "grad_norm": 38.99873733520508,
      "learning_rate": 0.0007927293511335976,
      "loss": 3.1592,
      "step": 2268
    },
    {
      "epoch": 2.269,
      "grad_norm": 94.77314758300781,
      "learning_rate": 0.0007919601114974614,
      "loss": 3.1893,
      "step": 2269
    },
    {
      "epoch": 2.27,
      "grad_norm": 68.86539459228516,
      "learning_rate": 0.0007911910005122073,
      "loss": 3.1286,
      "step": 2270
    },
    {
      "epoch": 2.271,
      "grad_norm": 38.38703536987305,
      "learning_rate": 0.0007904220186534494,
      "loss": 3.2192,
      "step": 2271
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 138.52122497558594,
      "learning_rate": 0.0007896531663967232,
      "loss": 2.9937,
      "step": 2272
    },
    {
      "epoch": 2.273,
      "grad_norm": 212.48480224609375,
      "learning_rate": 0.0007888844442174829,
      "loss": 3.1681,
      "step": 2273
    },
    {
      "epoch": 2.274,
      "grad_norm": 83.43106079101562,
      "learning_rate": 0.0007881158525911023,
      "loss": 2.9796,
      "step": 2274
    },
    {
      "epoch": 2.275,
      "grad_norm": 289.51690673828125,
      "learning_rate": 0.0007873473919928757,
      "loss": 3.1229,
      "step": 2275
    },
    {
      "epoch": 2.276,
      "grad_norm": 88.41728973388672,
      "learning_rate": 0.000786579062898015,
      "loss": 3.1205,
      "step": 2276
    },
    {
      "epoch": 2.277,
      "grad_norm": 448.4065856933594,
      "learning_rate": 0.0007858108657816512,
      "loss": 3.0939,
      "step": 2277
    },
    {
      "epoch": 2.278,
      "grad_norm": 38.85513687133789,
      "learning_rate": 0.0007850428011188339,
      "loss": 3.0281,
      "step": 2278
    },
    {
      "epoch": 2.279,
      "grad_norm": 85.30377197265625,
      "learning_rate": 0.0007842748693845306,
      "loss": 2.956,
      "step": 2279
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 484.7276306152344,
      "learning_rate": 0.0007835070710536267,
      "loss": 2.935,
      "step": 2280
    },
    {
      "epoch": 2.281,
      "grad_norm": 520.1919555664062,
      "learning_rate": 0.000782739406600925,
      "loss": 2.9104,
      "step": 2281
    },
    {
      "epoch": 2.282,
      "grad_norm": 1042.3780517578125,
      "learning_rate": 0.0007819718765011454,
      "loss": 2.9649,
      "step": 2282
    },
    {
      "epoch": 2.283,
      "grad_norm": 201.5940704345703,
      "learning_rate": 0.0007812044812289248,
      "loss": 2.9371,
      "step": 2283
    },
    {
      "epoch": 2.284,
      "grad_norm": 114.09599304199219,
      "learning_rate": 0.0007804372212588175,
      "loss": 2.9318,
      "step": 2284
    },
    {
      "epoch": 2.285,
      "grad_norm": 880.4490966796875,
      "learning_rate": 0.0007796700970652931,
      "loss": 2.9851,
      "step": 2285
    },
    {
      "epoch": 2.286,
      "grad_norm": 111.09066772460938,
      "learning_rate": 0.0007789031091227372,
      "loss": 3.022,
      "step": 2286
    },
    {
      "epoch": 2.287,
      "grad_norm": 860.5923461914062,
      "learning_rate": 0.0007781362579054519,
      "loss": 2.8936,
      "step": 2287
    },
    {
      "epoch": 2.288,
      "grad_norm": 73.37416076660156,
      "learning_rate": 0.0007773695438876542,
      "loss": 2.918,
      "step": 2288
    },
    {
      "epoch": 2.289,
      "grad_norm": 86.94256591796875,
      "learning_rate": 0.0007766029675434765,
      "loss": 3.0581,
      "step": 2289
    },
    {
      "epoch": 2.29,
      "grad_norm": 102.88270568847656,
      "learning_rate": 0.0007758365293469659,
      "loss": 3.0639,
      "step": 2290
    },
    {
      "epoch": 2.291,
      "grad_norm": 181.92669677734375,
      "learning_rate": 0.0007750702297720838,
      "loss": 2.9665,
      "step": 2291
    },
    {
      "epoch": 2.292,
      "grad_norm": 117.92321014404297,
      "learning_rate": 0.0007743040692927068,
      "loss": 2.9233,
      "step": 2292
    },
    {
      "epoch": 2.293,
      "grad_norm": 112.53759002685547,
      "learning_rate": 0.0007735380483826251,
      "loss": 2.9059,
      "step": 2293
    },
    {
      "epoch": 2.294,
      "grad_norm": 63.05339813232422,
      "learning_rate": 0.0007727721675155419,
      "loss": 3.0264,
      "step": 2294
    },
    {
      "epoch": 2.295,
      "grad_norm": 472.6817626953125,
      "learning_rate": 0.0007720064271650741,
      "loss": 2.9517,
      "step": 2295
    },
    {
      "epoch": 2.296,
      "grad_norm": 48.97197341918945,
      "learning_rate": 0.0007712408278047527,
      "loss": 3.1098,
      "step": 2296
    },
    {
      "epoch": 2.297,
      "grad_norm": 93.17953491210938,
      "learning_rate": 0.0007704753699080198,
      "loss": 3.0684,
      "step": 2297
    },
    {
      "epoch": 2.298,
      "grad_norm": 799.3899536132812,
      "learning_rate": 0.0007697100539482318,
      "loss": 3.0936,
      "step": 2298
    },
    {
      "epoch": 2.299,
      "grad_norm": 51.03506851196289,
      "learning_rate": 0.000768944880398656,
      "loss": 3.1279,
      "step": 2299
    },
    {
      "epoch": 2.3,
      "grad_norm": 219.45010375976562,
      "learning_rate": 0.0007681798497324717,
      "loss": 2.9472,
      "step": 2300
    },
    {
      "epoch": 2.3,
      "eval_loss": 3.016315221786499,
      "eval_runtime": 224.4878,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 2300
    },
    {
      "epoch": 2.301,
      "grad_norm": 239.0636444091797,
      "learning_rate": 0.0007674149624227709,
      "loss": 3.0853,
      "step": 2301
    },
    {
      "epoch": 2.302,
      "grad_norm": 3546.395263671875,
      "learning_rate": 0.0007666502189425564,
      "loss": 3.0254,
      "step": 2302
    },
    {
      "epoch": 2.303,
      "grad_norm": 369.2489318847656,
      "learning_rate": 0.0007658856197647416,
      "loss": 3.0169,
      "step": 2303
    },
    {
      "epoch": 2.304,
      "grad_norm": 178.0493927001953,
      "learning_rate": 0.0007651211653621508,
      "loss": 3.0465,
      "step": 2304
    },
    {
      "epoch": 2.305,
      "grad_norm": 85.18441009521484,
      "learning_rate": 0.0007643568562075195,
      "loss": 2.8745,
      "step": 2305
    },
    {
      "epoch": 2.306,
      "grad_norm": 968.6985473632812,
      "learning_rate": 0.0007635926927734924,
      "loss": 3.0034,
      "step": 2306
    },
    {
      "epoch": 2.307,
      "grad_norm": 14.225621223449707,
      "learning_rate": 0.000762828675532625,
      "loss": 3.0275,
      "step": 2307
    },
    {
      "epoch": 2.308,
      "grad_norm": 457.5213623046875,
      "learning_rate": 0.0007620648049573815,
      "loss": 2.9378,
      "step": 2308
    },
    {
      "epoch": 2.309,
      "grad_norm": 259.9063720703125,
      "learning_rate": 0.0007613010815201357,
      "loss": 3.0685,
      "step": 2309
    },
    {
      "epoch": 2.31,
      "grad_norm": 42.31601333618164,
      "learning_rate": 0.0007605375056931712,
      "loss": 2.8232,
      "step": 2310
    },
    {
      "epoch": 2.311,
      "grad_norm": 60.49738311767578,
      "learning_rate": 0.0007597740779486795,
      "loss": 2.9485,
      "step": 2311
    },
    {
      "epoch": 2.312,
      "grad_norm": 480.9681091308594,
      "learning_rate": 0.0007590107987587607,
      "loss": 2.9773,
      "step": 2312
    },
    {
      "epoch": 2.313,
      "grad_norm": 408.07562255859375,
      "learning_rate": 0.0007582476685954225,
      "loss": 2.8764,
      "step": 2313
    },
    {
      "epoch": 2.314,
      "grad_norm": 7567.97900390625,
      "learning_rate": 0.0007574846879305816,
      "loss": 2.9324,
      "step": 2314
    },
    {
      "epoch": 2.315,
      "grad_norm": 165.28138732910156,
      "learning_rate": 0.0007567218572360615,
      "loss": 2.9174,
      "step": 2315
    },
    {
      "epoch": 2.316,
      "grad_norm": 33.94816589355469,
      "learning_rate": 0.000755959176983593,
      "loss": 3.0388,
      "step": 2316
    },
    {
      "epoch": 2.317,
      "grad_norm": 466.745361328125,
      "learning_rate": 0.0007551966476448139,
      "loss": 2.8828,
      "step": 2317
    },
    {
      "epoch": 2.318,
      "grad_norm": 3392.503173828125,
      "learning_rate": 0.0007544342696912685,
      "loss": 2.9751,
      "step": 2318
    },
    {
      "epoch": 2.319,
      "grad_norm": 107.96302795410156,
      "learning_rate": 0.0007536720435944082,
      "loss": 2.979,
      "step": 2319
    },
    {
      "epoch": 2.32,
      "grad_norm": 3949.671142578125,
      "learning_rate": 0.0007529099698255901,
      "loss": 3.0004,
      "step": 2320
    },
    {
      "epoch": 2.321,
      "grad_norm": 276.1885681152344,
      "learning_rate": 0.0007521480488560766,
      "loss": 2.9563,
      "step": 2321
    },
    {
      "epoch": 2.322,
      "grad_norm": 343.3539123535156,
      "learning_rate": 0.0007513862811570362,
      "loss": 2.8973,
      "step": 2322
    },
    {
      "epoch": 2.323,
      "grad_norm": 740.1602783203125,
      "learning_rate": 0.0007506246671995423,
      "loss": 2.8768,
      "step": 2323
    },
    {
      "epoch": 2.324,
      "grad_norm": 54.69899368286133,
      "learning_rate": 0.0007498632074545735,
      "loss": 2.9751,
      "step": 2324
    },
    {
      "epoch": 2.325,
      "grad_norm": 124.30557250976562,
      "learning_rate": 0.000749101902393013,
      "loss": 2.9224,
      "step": 2325
    },
    {
      "epoch": 2.326,
      "grad_norm": 278.3680419921875,
      "learning_rate": 0.0007483407524856476,
      "loss": 2.8274,
      "step": 2326
    },
    {
      "epoch": 2.327,
      "grad_norm": 97.62198638916016,
      "learning_rate": 0.0007475797582031698,
      "loss": 2.8198,
      "step": 2327
    },
    {
      "epoch": 2.328,
      "grad_norm": 175.32481384277344,
      "learning_rate": 0.0007468189200161741,
      "loss": 2.78,
      "step": 2328
    },
    {
      "epoch": 2.329,
      "grad_norm": 405.3992004394531,
      "learning_rate": 0.0007460582383951597,
      "loss": 2.7967,
      "step": 2329
    },
    {
      "epoch": 2.33,
      "grad_norm": 4509.8701171875,
      "learning_rate": 0.0007452977138105283,
      "loss": 2.8294,
      "step": 2330
    },
    {
      "epoch": 2.331,
      "grad_norm": 1129.18505859375,
      "learning_rate": 0.0007445373467325847,
      "loss": 2.7742,
      "step": 2331
    },
    {
      "epoch": 2.332,
      "grad_norm": 213.5106201171875,
      "learning_rate": 0.0007437771376315364,
      "loss": 2.9237,
      "step": 2332
    },
    {
      "epoch": 2.333,
      "grad_norm": 84.36347198486328,
      "learning_rate": 0.0007430170869774928,
      "loss": 2.8484,
      "step": 2333
    },
    {
      "epoch": 2.334,
      "grad_norm": 191.85122680664062,
      "learning_rate": 0.0007422571952404662,
      "loss": 2.8506,
      "step": 2334
    },
    {
      "epoch": 2.335,
      "grad_norm": 174.29617309570312,
      "learning_rate": 0.000741497462890369,
      "loss": 2.741,
      "step": 2335
    },
    {
      "epoch": 2.336,
      "grad_norm": 111.31939697265625,
      "learning_rate": 0.0007407378903970174,
      "loss": 2.8169,
      "step": 2336
    },
    {
      "epoch": 2.337,
      "grad_norm": 51.21194839477539,
      "learning_rate": 0.0007399784782301267,
      "loss": 2.9383,
      "step": 2337
    },
    {
      "epoch": 2.338,
      "grad_norm": 6013.87890625,
      "learning_rate": 0.0007392192268593138,
      "loss": 2.8279,
      "step": 2338
    },
    {
      "epoch": 2.339,
      "grad_norm": 416.6298828125,
      "learning_rate": 0.0007384601367540964,
      "loss": 2.925,
      "step": 2339
    },
    {
      "epoch": 2.34,
      "grad_norm": 259.35809326171875,
      "learning_rate": 0.0007377012083838921,
      "loss": 2.7649,
      "step": 2340
    },
    {
      "epoch": 2.341,
      "grad_norm": 819.2926635742188,
      "learning_rate": 0.0007369424422180187,
      "loss": 2.8927,
      "step": 2341
    },
    {
      "epoch": 2.342,
      "grad_norm": 1001.9140014648438,
      "learning_rate": 0.0007361838387256932,
      "loss": 2.9239,
      "step": 2342
    },
    {
      "epoch": 2.343,
      "grad_norm": 303.5827331542969,
      "learning_rate": 0.0007354253983760331,
      "loss": 2.7899,
      "step": 2343
    },
    {
      "epoch": 2.344,
      "grad_norm": 81.01651000976562,
      "learning_rate": 0.0007346671216380534,
      "loss": 2.7884,
      "step": 2344
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 333.492919921875,
      "learning_rate": 0.0007339090089806697,
      "loss": 2.7699,
      "step": 2345
    },
    {
      "epoch": 2.346,
      "grad_norm": 875.2186279296875,
      "learning_rate": 0.0007331510608726949,
      "loss": 2.8037,
      "step": 2346
    },
    {
      "epoch": 2.347,
      "grad_norm": 150.97239685058594,
      "learning_rate": 0.0007323932777828408,
      "loss": 2.7831,
      "step": 2347
    },
    {
      "epoch": 2.348,
      "grad_norm": 36.28826141357422,
      "learning_rate": 0.0007316356601797162,
      "loss": 2.8037,
      "step": 2348
    },
    {
      "epoch": 2.349,
      "grad_norm": 1064.624267578125,
      "learning_rate": 0.0007308782085318293,
      "loss": 2.778,
      "step": 2349
    },
    {
      "epoch": 2.35,
      "grad_norm": 66.91719818115234,
      "learning_rate": 0.0007301209233075837,
      "loss": 2.9267,
      "step": 2350
    },
    {
      "epoch": 2.351,
      "grad_norm": 80.24117279052734,
      "learning_rate": 0.0007293638049752812,
      "loss": 2.8071,
      "step": 2351
    },
    {
      "epoch": 2.352,
      "grad_norm": 109.81642150878906,
      "learning_rate": 0.0007286068540031205,
      "loss": 2.9281,
      "step": 2352
    },
    {
      "epoch": 2.3529999999999998,
      "grad_norm": 49.005008697509766,
      "learning_rate": 0.0007278500708591959,
      "loss": 2.7742,
      "step": 2353
    },
    {
      "epoch": 2.354,
      "grad_norm": 137.73486328125,
      "learning_rate": 0.0007270934560114992,
      "loss": 2.7513,
      "step": 2354
    },
    {
      "epoch": 2.355,
      "grad_norm": 212.9202423095703,
      "learning_rate": 0.0007263370099279172,
      "loss": 2.7529,
      "step": 2355
    },
    {
      "epoch": 2.356,
      "grad_norm": 23.371421813964844,
      "learning_rate": 0.0007255807330762325,
      "loss": 2.8359,
      "step": 2356
    },
    {
      "epoch": 2.357,
      "grad_norm": 42.55233383178711,
      "learning_rate": 0.0007248246259241229,
      "loss": 2.8929,
      "step": 2357
    },
    {
      "epoch": 2.358,
      "grad_norm": 272.8756408691406,
      "learning_rate": 0.0007240686889391621,
      "loss": 2.8783,
      "step": 2358
    },
    {
      "epoch": 2.359,
      "grad_norm": 9283.2275390625,
      "learning_rate": 0.0007233129225888176,
      "loss": 2.8395,
      "step": 2359
    },
    {
      "epoch": 2.36,
      "grad_norm": 221.8492889404297,
      "learning_rate": 0.0007225573273404513,
      "loss": 2.9215,
      "step": 2360
    },
    {
      "epoch": 2.3609999999999998,
      "grad_norm": 701.1039428710938,
      "learning_rate": 0.0007218019036613201,
      "loss": 2.8062,
      "step": 2361
    },
    {
      "epoch": 2.362,
      "grad_norm": 86.1174545288086,
      "learning_rate": 0.0007210466520185748,
      "loss": 2.9183,
      "step": 2362
    },
    {
      "epoch": 2.363,
      "grad_norm": 2499.812255859375,
      "learning_rate": 0.0007202915728792592,
      "loss": 2.9491,
      "step": 2363
    },
    {
      "epoch": 2.364,
      "grad_norm": 25.15692901611328,
      "learning_rate": 0.0007195366667103102,
      "loss": 2.7681,
      "step": 2364
    },
    {
      "epoch": 2.365,
      "grad_norm": 15227.0634765625,
      "learning_rate": 0.0007187819339785588,
      "loss": 2.847,
      "step": 2365
    },
    {
      "epoch": 2.366,
      "grad_norm": 50.54779052734375,
      "learning_rate": 0.0007180273751507279,
      "loss": 2.9891,
      "step": 2366
    },
    {
      "epoch": 2.367,
      "grad_norm": 3147.231689453125,
      "learning_rate": 0.0007172729906934332,
      "loss": 2.8018,
      "step": 2367
    },
    {
      "epoch": 2.368,
      "grad_norm": 136.18296813964844,
      "learning_rate": 0.0007165187810731823,
      "loss": 2.8549,
      "step": 2368
    },
    {
      "epoch": 2.3689999999999998,
      "grad_norm": 84.59404754638672,
      "learning_rate": 0.000715764746756375,
      "loss": 2.9311,
      "step": 2369
    },
    {
      "epoch": 2.37,
      "grad_norm": 41.60192108154297,
      "learning_rate": 0.0007150108882093021,
      "loss": 2.7776,
      "step": 2370
    },
    {
      "epoch": 2.371,
      "grad_norm": 51.31373596191406,
      "learning_rate": 0.000714257205898147,
      "loss": 2.9626,
      "step": 2371
    },
    {
      "epoch": 2.372,
      "grad_norm": 94.65605163574219,
      "learning_rate": 0.0007135037002889829,
      "loss": 2.8462,
      "step": 2372
    },
    {
      "epoch": 2.373,
      "grad_norm": 647.8455200195312,
      "learning_rate": 0.0007127503718477738,
      "loss": 2.7751,
      "step": 2373
    },
    {
      "epoch": 2.374,
      "grad_norm": 32.18935775756836,
      "learning_rate": 0.0007119972210403749,
      "loss": 2.7803,
      "step": 2374
    },
    {
      "epoch": 2.375,
      "grad_norm": 32.961334228515625,
      "learning_rate": 0.0007112442483325306,
      "loss": 2.7432,
      "step": 2375
    },
    {
      "epoch": 2.376,
      "grad_norm": 68.39087677001953,
      "learning_rate": 0.0007104914541898763,
      "loss": 2.7281,
      "step": 2376
    },
    {
      "epoch": 2.377,
      "grad_norm": 360.92706298828125,
      "learning_rate": 0.0007097388390779358,
      "loss": 2.8082,
      "step": 2377
    },
    {
      "epoch": 2.378,
      "grad_norm": 52.54648208618164,
      "learning_rate": 0.0007089864034621228,
      "loss": 2.8575,
      "step": 2378
    },
    {
      "epoch": 2.379,
      "grad_norm": 32.3131103515625,
      "learning_rate": 0.0007082341478077396,
      "loss": 2.9053,
      "step": 2379
    },
    {
      "epoch": 2.38,
      "grad_norm": 62.149070739746094,
      "learning_rate": 0.0007074820725799786,
      "loss": 2.8327,
      "step": 2380
    },
    {
      "epoch": 2.3810000000000002,
      "grad_norm": 241.89703369140625,
      "learning_rate": 0.0007067301782439188,
      "loss": 2.7627,
      "step": 2381
    },
    {
      "epoch": 2.382,
      "grad_norm": 23.98751449584961,
      "learning_rate": 0.000705978465264528,
      "loss": 2.7279,
      "step": 2382
    },
    {
      "epoch": 2.383,
      "grad_norm": 37.542503356933594,
      "learning_rate": 0.0007052269341066623,
      "loss": 2.837,
      "step": 2383
    },
    {
      "epoch": 2.384,
      "grad_norm": 104.1024398803711,
      "learning_rate": 0.0007044755852350648,
      "loss": 2.759,
      "step": 2384
    },
    {
      "epoch": 2.385,
      "grad_norm": 85.04728698730469,
      "learning_rate": 0.0007037244191143661,
      "loss": 2.8019,
      "step": 2385
    },
    {
      "epoch": 2.386,
      "grad_norm": 249.87106323242188,
      "learning_rate": 0.000702973436209084,
      "loss": 2.7766,
      "step": 2386
    },
    {
      "epoch": 2.387,
      "grad_norm": 96.15292358398438,
      "learning_rate": 0.0007022226369836224,
      "loss": 2.769,
      "step": 2387
    },
    {
      "epoch": 2.388,
      "grad_norm": 1568.155029296875,
      "learning_rate": 0.0007014720219022718,
      "loss": 2.8711,
      "step": 2388
    },
    {
      "epoch": 2.3890000000000002,
      "grad_norm": 27.800323486328125,
      "learning_rate": 0.00070072159142921,
      "loss": 2.8422,
      "step": 2389
    },
    {
      "epoch": 2.39,
      "grad_norm": 904.8232421875,
      "learning_rate": 0.0006999713460284988,
      "loss": 2.9907,
      "step": 2390
    },
    {
      "epoch": 2.391,
      "grad_norm": 36.05681610107422,
      "learning_rate": 0.0006992212861640868,
      "loss": 2.9215,
      "step": 2391
    },
    {
      "epoch": 2.392,
      "grad_norm": 34.87647247314453,
      "learning_rate": 0.0006984714122998072,
      "loss": 2.858,
      "step": 2392
    },
    {
      "epoch": 2.393,
      "grad_norm": 13.762845993041992,
      "learning_rate": 0.0006977217248993785,
      "loss": 2.8449,
      "step": 2393
    },
    {
      "epoch": 2.394,
      "grad_norm": 97.51490783691406,
      "learning_rate": 0.000696972224426404,
      "loss": 2.8346,
      "step": 2394
    },
    {
      "epoch": 2.395,
      "grad_norm": 50.858821868896484,
      "learning_rate": 0.0006962229113443712,
      "loss": 2.9319,
      "step": 2395
    },
    {
      "epoch": 2.396,
      "grad_norm": 41.73222351074219,
      "learning_rate": 0.0006954737861166512,
      "loss": 2.7809,
      "step": 2396
    },
    {
      "epoch": 2.3970000000000002,
      "grad_norm": 53.1003303527832,
      "learning_rate": 0.0006947248492065003,
      "loss": 2.7925,
      "step": 2397
    },
    {
      "epoch": 2.398,
      "grad_norm": 19.130577087402344,
      "learning_rate": 0.0006939761010770574,
      "loss": 2.7785,
      "step": 2398
    },
    {
      "epoch": 2.399,
      "grad_norm": 12.854414939880371,
      "learning_rate": 0.0006932275421913446,
      "loss": 2.7005,
      "step": 2399
    },
    {
      "epoch": 2.4,
      "grad_norm": 27.217500686645508,
      "learning_rate": 0.0006924791730122671,
      "loss": 2.8159,
      "step": 2400
    },
    {
      "epoch": 2.4,
      "eval_loss": 2.76357364654541,
      "eval_runtime": 224.6451,
      "eval_samples_per_second": 0.445,
      "eval_steps_per_second": 0.445,
      "step": 2400
    },
    {
      "epoch": 2.401,
      "grad_norm": 56.47343826293945,
      "learning_rate": 0.0006917309940026131,
      "loss": 2.6826,
      "step": 2401
    },
    {
      "epoch": 2.402,
      "grad_norm": 34.850830078125,
      "learning_rate": 0.0006909830056250527,
      "loss": 2.6498,
      "step": 2402
    },
    {
      "epoch": 2.403,
      "grad_norm": 36.14690399169922,
      "learning_rate": 0.0006902352083421386,
      "loss": 2.7253,
      "step": 2403
    },
    {
      "epoch": 2.404,
      "grad_norm": 81.2342758178711,
      "learning_rate": 0.0006894876026163051,
      "loss": 2.7781,
      "step": 2404
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 28.67473030090332,
      "learning_rate": 0.0006887401889098672,
      "loss": 2.6874,
      "step": 2405
    },
    {
      "epoch": 2.406,
      "grad_norm": 92.59819793701172,
      "learning_rate": 0.0006879929676850232,
      "loss": 2.8462,
      "step": 2406
    },
    {
      "epoch": 2.407,
      "grad_norm": 316.3427429199219,
      "learning_rate": 0.0006872459394038508,
      "loss": 2.6097,
      "step": 2407
    },
    {
      "epoch": 2.408,
      "grad_norm": 41.891841888427734,
      "learning_rate": 0.0006864991045283086,
      "loss": 2.6226,
      "step": 2408
    },
    {
      "epoch": 2.409,
      "grad_norm": 141.5034637451172,
      "learning_rate": 0.000685752463520236,
      "loss": 2.7336,
      "step": 2409
    },
    {
      "epoch": 2.41,
      "grad_norm": 71.45449829101562,
      "learning_rate": 0.0006850060168413518,
      "loss": 2.6923,
      "step": 2410
    },
    {
      "epoch": 2.411,
      "grad_norm": 8675.044921875,
      "learning_rate": 0.0006842597649532552,
      "loss": 2.7023,
      "step": 2411
    },
    {
      "epoch": 2.412,
      "grad_norm": 88.37380981445312,
      "learning_rate": 0.0006835137083174254,
      "loss": 2.7387,
      "step": 2412
    },
    {
      "epoch": 2.413,
      "grad_norm": 50.20368576049805,
      "learning_rate": 0.0006827678473952197,
      "loss": 2.7639,
      "step": 2413
    },
    {
      "epoch": 2.414,
      "grad_norm": 15.551692962646484,
      "learning_rate": 0.0006820221826478747,
      "loss": 2.7344,
      "step": 2414
    },
    {
      "epoch": 2.415,
      "grad_norm": 240.33877563476562,
      "learning_rate": 0.0006812767145365069,
      "loss": 2.8384,
      "step": 2415
    },
    {
      "epoch": 2.416,
      "grad_norm": 854.1293334960938,
      "learning_rate": 0.0006805314435221096,
      "loss": 2.7991,
      "step": 2416
    },
    {
      "epoch": 2.417,
      "grad_norm": 59.9614372253418,
      "learning_rate": 0.0006797863700655548,
      "loss": 2.6339,
      "step": 2417
    },
    {
      "epoch": 2.418,
      "grad_norm": 81487.6015625,
      "learning_rate": 0.0006790414946275931,
      "loss": 2.7113,
      "step": 2418
    },
    {
      "epoch": 2.419,
      "grad_norm": 610.162841796875,
      "learning_rate": 0.0006782968176688514,
      "loss": 2.8879,
      "step": 2419
    },
    {
      "epoch": 2.42,
      "grad_norm": 237.66433715820312,
      "learning_rate": 0.000677552339649834,
      "loss": 2.7145,
      "step": 2420
    },
    {
      "epoch": 2.421,
      "grad_norm": 133.9853057861328,
      "learning_rate": 0.0006768080610309234,
      "loss": 2.6644,
      "step": 2421
    },
    {
      "epoch": 2.422,
      "grad_norm": 196.92726135253906,
      "learning_rate": 0.0006760639822723776,
      "loss": 2.7147,
      "step": 2422
    },
    {
      "epoch": 2.423,
      "grad_norm": 62.67776107788086,
      "learning_rate": 0.000675320103834331,
      "loss": 2.7088,
      "step": 2423
    },
    {
      "epoch": 2.424,
      "grad_norm": 177.14743041992188,
      "learning_rate": 0.000674576426176795,
      "loss": 2.748,
      "step": 2424
    },
    {
      "epoch": 2.425,
      "grad_norm": 44.67744827270508,
      "learning_rate": 0.0006738329497596565,
      "loss": 2.6787,
      "step": 2425
    },
    {
      "epoch": 2.426,
      "grad_norm": 56.59333801269531,
      "learning_rate": 0.0006730896750426773,
      "loss": 2.8359,
      "step": 2426
    },
    {
      "epoch": 2.427,
      "grad_norm": 709.3442993164062,
      "learning_rate": 0.0006723466024854955,
      "loss": 2.706,
      "step": 2427
    },
    {
      "epoch": 2.428,
      "grad_norm": 402.1654052734375,
      "learning_rate": 0.0006716037325476233,
      "loss": 2.7802,
      "step": 2428
    },
    {
      "epoch": 2.429,
      "grad_norm": 107.9085922241211,
      "learning_rate": 0.0006708610656884478,
      "loss": 2.7327,
      "step": 2429
    },
    {
      "epoch": 2.43,
      "grad_norm": 22.58144187927246,
      "learning_rate": 0.000670118602367231,
      "loss": 2.9525,
      "step": 2430
    },
    {
      "epoch": 2.431,
      "grad_norm": 12.678072929382324,
      "learning_rate": 0.0006693763430431083,
      "loss": 2.5675,
      "step": 2431
    },
    {
      "epoch": 2.432,
      "grad_norm": 664.8987426757812,
      "learning_rate": 0.00066863428817509,
      "loss": 2.7032,
      "step": 2432
    },
    {
      "epoch": 2.433,
      "grad_norm": 31714.29296875,
      "learning_rate": 0.0006678924382220586,
      "loss": 2.6917,
      "step": 2433
    },
    {
      "epoch": 2.434,
      "grad_norm": 20.44282341003418,
      "learning_rate": 0.0006671507936427713,
      "loss": 2.69,
      "step": 2434
    },
    {
      "epoch": 2.435,
      "grad_norm": 491.37200927734375,
      "learning_rate": 0.0006664093548958569,
      "loss": 2.7678,
      "step": 2435
    },
    {
      "epoch": 2.436,
      "grad_norm": 91.40434265136719,
      "learning_rate": 0.0006656681224398183,
      "loss": 2.6936,
      "step": 2436
    },
    {
      "epoch": 2.437,
      "grad_norm": 31.210193634033203,
      "learning_rate": 0.0006649270967330296,
      "loss": 2.6545,
      "step": 2437
    },
    {
      "epoch": 2.438,
      "grad_norm": 224.21029663085938,
      "learning_rate": 0.0006641862782337378,
      "loss": 2.7157,
      "step": 2438
    },
    {
      "epoch": 2.439,
      "grad_norm": 59.068546295166016,
      "learning_rate": 0.0006634456674000617,
      "loss": 2.7543,
      "step": 2439
    },
    {
      "epoch": 2.44,
      "grad_norm": 173.48239135742188,
      "learning_rate": 0.0006627052646899908,
      "loss": 2.8286,
      "step": 2440
    },
    {
      "epoch": 2.441,
      "grad_norm": 36.1848258972168,
      "learning_rate": 0.0006619650705613878,
      "loss": 2.914,
      "step": 2441
    },
    {
      "epoch": 2.442,
      "grad_norm": 13.215519905090332,
      "learning_rate": 0.0006612250854719842,
      "loss": 2.669,
      "step": 2442
    },
    {
      "epoch": 2.443,
      "grad_norm": 64.42835235595703,
      "learning_rate": 0.000660485309879384,
      "loss": 2.773,
      "step": 2443
    },
    {
      "epoch": 2.444,
      "grad_norm": 36.225162506103516,
      "learning_rate": 0.0006597457442410605,
      "loss": 2.5969,
      "step": 2444
    },
    {
      "epoch": 2.445,
      "grad_norm": 190.8259735107422,
      "learning_rate": 0.0006590063890143578,
      "loss": 2.5568,
      "step": 2445
    },
    {
      "epoch": 2.446,
      "grad_norm": 32.04839324951172,
      "learning_rate": 0.0006582672446564898,
      "loss": 2.7634,
      "step": 2446
    },
    {
      "epoch": 2.447,
      "grad_norm": 25.252656936645508,
      "learning_rate": 0.0006575283116245394,
      "loss": 2.8312,
      "step": 2447
    },
    {
      "epoch": 2.448,
      "grad_norm": 41.249839782714844,
      "learning_rate": 0.0006567895903754597,
      "loss": 2.7804,
      "step": 2448
    },
    {
      "epoch": 2.449,
      "grad_norm": 19.065248489379883,
      "learning_rate": 0.0006560510813660718,
      "loss": 2.7663,
      "step": 2449
    },
    {
      "epoch": 2.45,
      "grad_norm": 10.574759483337402,
      "learning_rate": 0.000655312785053067,
      "loss": 2.7299,
      "step": 2450
    },
    {
      "epoch": 2.451,
      "grad_norm": 13624.755859375,
      "learning_rate": 0.000654574701893004,
      "loss": 2.6651,
      "step": 2451
    },
    {
      "epoch": 2.452,
      "grad_norm": 41.73727035522461,
      "learning_rate": 0.0006538368323423099,
      "loss": 2.7869,
      "step": 2452
    },
    {
      "epoch": 2.453,
      "grad_norm": 8.08720588684082,
      "learning_rate": 0.0006530991768572793,
      "loss": 2.6617,
      "step": 2453
    },
    {
      "epoch": 2.454,
      "grad_norm": 62.91663360595703,
      "learning_rate": 0.0006523617358940757,
      "loss": 2.8069,
      "step": 2454
    },
    {
      "epoch": 2.455,
      "grad_norm": 11.764848709106445,
      "learning_rate": 0.0006516245099087286,
      "loss": 2.7371,
      "step": 2455
    },
    {
      "epoch": 2.456,
      "grad_norm": 24.288494110107422,
      "learning_rate": 0.0006508874993571348,
      "loss": 2.6888,
      "step": 2456
    },
    {
      "epoch": 2.457,
      "grad_norm": 10.66264820098877,
      "learning_rate": 0.0006501507046950585,
      "loss": 2.7867,
      "step": 2457
    },
    {
      "epoch": 2.458,
      "grad_norm": 3015.938232421875,
      "learning_rate": 0.0006494141263781297,
      "loss": 2.7651,
      "step": 2458
    },
    {
      "epoch": 2.459,
      "grad_norm": 22.566675186157227,
      "learning_rate": 0.0006486777648618453,
      "loss": 2.7257,
      "step": 2459
    },
    {
      "epoch": 2.46,
      "grad_norm": 37.11832809448242,
      "learning_rate": 0.0006479416206015678,
      "loss": 2.6662,
      "step": 2460
    },
    {
      "epoch": 2.461,
      "grad_norm": 512.3594360351562,
      "learning_rate": 0.0006472056940525254,
      "loss": 2.6385,
      "step": 2461
    },
    {
      "epoch": 2.462,
      "grad_norm": 13.219647407531738,
      "learning_rate": 0.0006464699856698111,
      "loss": 2.7676,
      "step": 2462
    },
    {
      "epoch": 2.463,
      "grad_norm": 8.7278470993042,
      "learning_rate": 0.0006457344959083839,
      "loss": 2.7754,
      "step": 2463
    },
    {
      "epoch": 2.464,
      "grad_norm": 31.129119873046875,
      "learning_rate": 0.0006449992252230672,
      "loss": 2.6534,
      "step": 2464
    },
    {
      "epoch": 2.465,
      "grad_norm": 33.58213424682617,
      "learning_rate": 0.0006442641740685484,
      "loss": 2.6124,
      "step": 2465
    },
    {
      "epoch": 2.466,
      "grad_norm": 142.19854736328125,
      "learning_rate": 0.0006435293428993798,
      "loss": 2.6527,
      "step": 2466
    },
    {
      "epoch": 2.467,
      "grad_norm": 39.823272705078125,
      "learning_rate": 0.0006427947321699783,
      "loss": 2.5846,
      "step": 2467
    },
    {
      "epoch": 2.468,
      "grad_norm": 74.06969451904297,
      "learning_rate": 0.000642060342334623,
      "loss": 2.5256,
      "step": 2468
    },
    {
      "epoch": 2.469,
      "grad_norm": 79.87765502929688,
      "learning_rate": 0.0006413261738474572,
      "loss": 2.6413,
      "step": 2469
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 14.864399909973145,
      "learning_rate": 0.0006405922271624873,
      "loss": 2.7598,
      "step": 2470
    },
    {
      "epoch": 2.471,
      "grad_norm": 25.93107032775879,
      "learning_rate": 0.0006398585027335823,
      "loss": 2.6217,
      "step": 2471
    },
    {
      "epoch": 2.472,
      "grad_norm": 99.70986938476562,
      "learning_rate": 0.000639125001014474,
      "loss": 2.7195,
      "step": 2472
    },
    {
      "epoch": 2.473,
      "grad_norm": 43.38667297363281,
      "learning_rate": 0.0006383917224587566,
      "loss": 2.6645,
      "step": 2473
    },
    {
      "epoch": 2.474,
      "grad_norm": 19.47126579284668,
      "learning_rate": 0.0006376586675198857,
      "loss": 2.5934,
      "step": 2474
    },
    {
      "epoch": 2.475,
      "grad_norm": 140.77915954589844,
      "learning_rate": 0.0006369258366511789,
      "loss": 2.594,
      "step": 2475
    },
    {
      "epoch": 2.476,
      "grad_norm": 33.73828887939453,
      "learning_rate": 0.000636193230305816,
      "loss": 2.5713,
      "step": 2476
    },
    {
      "epoch": 2.477,
      "grad_norm": 21.3123779296875,
      "learning_rate": 0.0006354608489368368,
      "loss": 2.7423,
      "step": 2477
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 300.55010986328125,
      "learning_rate": 0.0006347286929971427,
      "loss": 2.5503,
      "step": 2478
    },
    {
      "epoch": 2.479,
      "grad_norm": 958841.8125,
      "learning_rate": 0.0006339967629394956,
      "loss": 2.6006,
      "step": 2479
    },
    {
      "epoch": 2.48,
      "grad_norm": 58.792274475097656,
      "learning_rate": 0.000633265059216517,
      "loss": 2.6532,
      "step": 2480
    },
    {
      "epoch": 2.481,
      "grad_norm": 644.8206176757812,
      "learning_rate": 0.0006325335822806895,
      "loss": 2.6439,
      "step": 2481
    },
    {
      "epoch": 2.482,
      "grad_norm": 111.07088470458984,
      "learning_rate": 0.0006318023325843547,
      "loss": 2.6494,
      "step": 2482
    },
    {
      "epoch": 2.483,
      "grad_norm": 183.01010131835938,
      "learning_rate": 0.0006310713105797144,
      "loss": 2.5969,
      "step": 2483
    },
    {
      "epoch": 2.484,
      "grad_norm": 19.20134162902832,
      "learning_rate": 0.0006303405167188282,
      "loss": 2.5743,
      "step": 2484
    },
    {
      "epoch": 2.485,
      "grad_norm": 26.58098793029785,
      "learning_rate": 0.0006296099514536167,
      "loss": 2.6112,
      "step": 2485
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 111.18085479736328,
      "learning_rate": 0.0006288796152358573,
      "loss": 2.5669,
      "step": 2486
    },
    {
      "epoch": 2.487,
      "grad_norm": 20.505720138549805,
      "learning_rate": 0.0006281495085171868,
      "loss": 2.6673,
      "step": 2487
    },
    {
      "epoch": 2.488,
      "grad_norm": 2443.1611328125,
      "learning_rate": 0.0006274196317490997,
      "loss": 2.6294,
      "step": 2488
    },
    {
      "epoch": 2.489,
      "grad_norm": 90.05949401855469,
      "learning_rate": 0.0006266899853829479,
      "loss": 2.5877,
      "step": 2489
    },
    {
      "epoch": 2.49,
      "grad_norm": 153.57469177246094,
      "learning_rate": 0.000625960569869942,
      "loss": 2.5022,
      "step": 2490
    },
    {
      "epoch": 2.491,
      "grad_norm": 49.409969329833984,
      "learning_rate": 0.0006252313856611485,
      "loss": 2.6295,
      "step": 2491
    },
    {
      "epoch": 2.492,
      "grad_norm": 185.83168029785156,
      "learning_rate": 0.0006245024332074916,
      "loss": 2.5708,
      "step": 2492
    },
    {
      "epoch": 2.493,
      "grad_norm": 37.01203918457031,
      "learning_rate": 0.0006237737129597518,
      "loss": 2.5353,
      "step": 2493
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 319.10089111328125,
      "learning_rate": 0.000623045225368567,
      "loss": 2.5769,
      "step": 2494
    },
    {
      "epoch": 2.495,
      "grad_norm": 309.1163635253906,
      "learning_rate": 0.0006223169708844299,
      "loss": 2.617,
      "step": 2495
    },
    {
      "epoch": 2.496,
      "grad_norm": 47.3286018371582,
      "learning_rate": 0.0006215889499576897,
      "loss": 2.6757,
      "step": 2496
    },
    {
      "epoch": 2.497,
      "grad_norm": 738.7969970703125,
      "learning_rate": 0.0006208611630385514,
      "loss": 2.5967,
      "step": 2497
    },
    {
      "epoch": 2.498,
      "grad_norm": 663.3078002929688,
      "learning_rate": 0.0006201336105770745,
      "loss": 2.6852,
      "step": 2498
    },
    {
      "epoch": 2.499,
      "grad_norm": 83.53768920898438,
      "learning_rate": 0.0006194062930231743,
      "loss": 2.5677,
      "step": 2499
    },
    {
      "epoch": 2.5,
      "grad_norm": 88.2625503540039,
      "learning_rate": 0.0006186792108266204,
      "loss": 2.6519,
      "step": 2500
    },
    {
      "epoch": 2.5,
      "eval_loss": 2.660719633102417,
      "eval_runtime": 222.727,
      "eval_samples_per_second": 0.449,
      "eval_steps_per_second": 0.449,
      "step": 2500
    },
    {
      "epoch": 2.501,
      "grad_norm": 2758.340576171875,
      "learning_rate": 0.0006179523644370366,
      "loss": 2.666,
      "step": 2501
    },
    {
      "epoch": 2.502,
      "grad_norm": 88.84339141845703,
      "learning_rate": 0.0006172257543039023,
      "loss": 2.7517,
      "step": 2502
    },
    {
      "epoch": 2.503,
      "grad_norm": 253.29783630371094,
      "learning_rate": 0.0006164993808765492,
      "loss": 2.6534,
      "step": 2503
    },
    {
      "epoch": 2.504,
      "grad_norm": 1287.312744140625,
      "learning_rate": 0.000615773244604163,
      "loss": 2.6238,
      "step": 2504
    },
    {
      "epoch": 2.505,
      "grad_norm": 103.20113372802734,
      "learning_rate": 0.0006150473459357831,
      "loss": 2.6414,
      "step": 2505
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 12595.9921875,
      "learning_rate": 0.000614321685320302,
      "loss": 2.565,
      "step": 2506
    },
    {
      "epoch": 2.507,
      "grad_norm": 158.97547912597656,
      "learning_rate": 0.0006135962632064643,
      "loss": 2.6202,
      "step": 2507
    },
    {
      "epoch": 2.508,
      "grad_norm": 281.71099853515625,
      "learning_rate": 0.000612871080042868,
      "loss": 2.5043,
      "step": 2508
    },
    {
      "epoch": 2.509,
      "grad_norm": 421.6109313964844,
      "learning_rate": 0.0006121461362779628,
      "loss": 2.6074,
      "step": 2509
    },
    {
      "epoch": 2.51,
      "grad_norm": 97.48194885253906,
      "learning_rate": 0.0006114214323600503,
      "loss": 2.5592,
      "step": 2510
    },
    {
      "epoch": 2.511,
      "grad_norm": 244.94635009765625,
      "learning_rate": 0.0006106969687372845,
      "loss": 2.7128,
      "step": 2511
    },
    {
      "epoch": 2.512,
      "grad_norm": 730.0665893554688,
      "learning_rate": 0.0006099727458576701,
      "loss": 2.6409,
      "step": 2512
    },
    {
      "epoch": 2.513,
      "grad_norm": 73.66366577148438,
      "learning_rate": 0.0006092487641690625,
      "loss": 2.5963,
      "step": 2513
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 378.87274169921875,
      "learning_rate": 0.0006085250241191694,
      "loss": 2.6139,
      "step": 2514
    },
    {
      "epoch": 2.515,
      "grad_norm": 296.863525390625,
      "learning_rate": 0.0006078015261555479,
      "loss": 2.6017,
      "step": 2515
    },
    {
      "epoch": 2.516,
      "grad_norm": 239.82806396484375,
      "learning_rate": 0.0006070782707256053,
      "loss": 2.6622,
      "step": 2516
    },
    {
      "epoch": 2.517,
      "grad_norm": 278.5904235839844,
      "learning_rate": 0.0006063552582765999,
      "loss": 2.5694,
      "step": 2517
    },
    {
      "epoch": 2.518,
      "grad_norm": 766.5254516601562,
      "learning_rate": 0.0006056324892556388,
      "loss": 2.5426,
      "step": 2518
    },
    {
      "epoch": 2.519,
      "grad_norm": 108.6833724975586,
      "learning_rate": 0.0006049099641096787,
      "loss": 2.6274,
      "step": 2519
    },
    {
      "epoch": 2.52,
      "grad_norm": 225.3260498046875,
      "learning_rate": 0.0006041876832855267,
      "loss": 2.5989,
      "step": 2520
    },
    {
      "epoch": 2.521,
      "grad_norm": 374.34368896484375,
      "learning_rate": 0.0006034656472298373,
      "loss": 2.5314,
      "step": 2521
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 21.738554000854492,
      "learning_rate": 0.0006027438563891139,
      "loss": 2.5743,
      "step": 2522
    },
    {
      "epoch": 2.523,
      "grad_norm": 112.2898178100586,
      "learning_rate": 0.000602022311209709,
      "loss": 2.6236,
      "step": 2523
    },
    {
      "epoch": 2.524,
      "grad_norm": 47.786224365234375,
      "learning_rate": 0.0006013010121378224,
      "loss": 2.6039,
      "step": 2524
    },
    {
      "epoch": 2.525,
      "grad_norm": 747.930908203125,
      "learning_rate": 0.0006005799596195021,
      "loss": 2.6614,
      "step": 2525
    },
    {
      "epoch": 2.526,
      "grad_norm": 41.435550689697266,
      "learning_rate": 0.0005998591541006437,
      "loss": 2.6595,
      "step": 2526
    },
    {
      "epoch": 2.527,
      "grad_norm": 759.9549560546875,
      "learning_rate": 0.0005991385960269896,
      "loss": 2.5824,
      "step": 2527
    },
    {
      "epoch": 2.528,
      "grad_norm": 269.84796142578125,
      "learning_rate": 0.0005984182858441295,
      "loss": 2.631,
      "step": 2528
    },
    {
      "epoch": 2.529,
      "grad_norm": 1306.521240234375,
      "learning_rate": 0.0005976982239975005,
      "loss": 2.6365,
      "step": 2529
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 577.6260986328125,
      "learning_rate": 0.0005969784109323849,
      "loss": 2.5852,
      "step": 2530
    },
    {
      "epoch": 2.531,
      "grad_norm": 1998.1419677734375,
      "learning_rate": 0.0005962588470939116,
      "loss": 2.6226,
      "step": 2531
    },
    {
      "epoch": 2.532,
      "grad_norm": 640.9766845703125,
      "learning_rate": 0.0005955395329270559,
      "loss": 2.6359,
      "step": 2532
    },
    {
      "epoch": 2.533,
      "grad_norm": 132.55604553222656,
      "learning_rate": 0.0005948204688766378,
      "loss": 2.4172,
      "step": 2533
    },
    {
      "epoch": 2.534,
      "grad_norm": 83.58187866210938,
      "learning_rate": 0.0005941016553873232,
      "loss": 2.7385,
      "step": 2534
    },
    {
      "epoch": 2.535,
      "grad_norm": 91.94696044921875,
      "learning_rate": 0.0005933830929036232,
      "loss": 2.5185,
      "step": 2535
    },
    {
      "epoch": 2.536,
      "grad_norm": 44.04692459106445,
      "learning_rate": 0.0005926647818698931,
      "loss": 2.4729,
      "step": 2536
    },
    {
      "epoch": 2.537,
      "grad_norm": 208.3658905029297,
      "learning_rate": 0.0005919467227303332,
      "loss": 2.5315,
      "step": 2537
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 104.03922271728516,
      "learning_rate": 0.0005912289159289883,
      "loss": 2.504,
      "step": 2538
    },
    {
      "epoch": 2.539,
      "grad_norm": 188.49258422851562,
      "learning_rate": 0.0005905113619097461,
      "loss": 2.4917,
      "step": 2539
    },
    {
      "epoch": 2.54,
      "grad_norm": 108.67495727539062,
      "learning_rate": 0.0005897940611163389,
      "loss": 2.6136,
      "step": 2540
    },
    {
      "epoch": 2.541,
      "grad_norm": 367.756103515625,
      "learning_rate": 0.0005890770139923422,
      "loss": 2.7271,
      "step": 2541
    },
    {
      "epoch": 2.542,
      "grad_norm": 151.5581817626953,
      "learning_rate": 0.0005883602209811741,
      "loss": 2.4805,
      "step": 2542
    },
    {
      "epoch": 2.543,
      "grad_norm": 940.7109375,
      "learning_rate": 0.0005876436825260967,
      "loss": 2.6978,
      "step": 2543
    },
    {
      "epoch": 2.544,
      "grad_norm": 731.6663208007812,
      "learning_rate": 0.0005869273990702135,
      "loss": 2.6007,
      "step": 2544
    },
    {
      "epoch": 2.545,
      "grad_norm": 310.7831726074219,
      "learning_rate": 0.0005862113710564705,
      "loss": 2.6495,
      "step": 2545
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 187.88836669921875,
      "learning_rate": 0.0005854955989276566,
      "loss": 2.4391,
      "step": 2546
    },
    {
      "epoch": 2.547,
      "grad_norm": 170.1297149658203,
      "learning_rate": 0.0005847800831264018,
      "loss": 2.688,
      "step": 2547
    },
    {
      "epoch": 2.548,
      "grad_norm": 52.75763702392578,
      "learning_rate": 0.0005840648240951778,
      "loss": 2.6987,
      "step": 2548
    },
    {
      "epoch": 2.549,
      "grad_norm": 561.7080688476562,
      "learning_rate": 0.0005833498222762972,
      "loss": 2.4875,
      "step": 2549
    },
    {
      "epoch": 2.55,
      "grad_norm": 68.68708801269531,
      "learning_rate": 0.0005826350781119134,
      "loss": 2.6981,
      "step": 2550
    },
    {
      "epoch": 2.551,
      "grad_norm": 27.133365631103516,
      "learning_rate": 0.0005819205920440214,
      "loss": 2.603,
      "step": 2551
    },
    {
      "epoch": 2.552,
      "grad_norm": 59.42658233642578,
      "learning_rate": 0.0005812063645144559,
      "loss": 2.5563,
      "step": 2552
    },
    {
      "epoch": 2.553,
      "grad_norm": 21.526620864868164,
      "learning_rate": 0.0005804923959648916,
      "loss": 2.5912,
      "step": 2553
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 35.38502883911133,
      "learning_rate": 0.000579778686836843,
      "loss": 2.5769,
      "step": 2554
    },
    {
      "epoch": 2.555,
      "grad_norm": 35.3024787902832,
      "learning_rate": 0.0005790652375716652,
      "loss": 2.5842,
      "step": 2555
    },
    {
      "epoch": 2.556,
      "grad_norm": 61.241947174072266,
      "learning_rate": 0.0005783520486105509,
      "loss": 2.445,
      "step": 2556
    },
    {
      "epoch": 2.557,
      "grad_norm": 53.684329986572266,
      "learning_rate": 0.0005776391203945339,
      "loss": 2.5482,
      "step": 2557
    },
    {
      "epoch": 2.558,
      "grad_norm": 37.37528610229492,
      "learning_rate": 0.0005769264533644849,
      "loss": 2.5896,
      "step": 2558
    },
    {
      "epoch": 2.559,
      "grad_norm": 101.98355102539062,
      "learning_rate": 0.000576214047961114,
      "loss": 2.4804,
      "step": 2559
    },
    {
      "epoch": 2.56,
      "grad_norm": 60.19645690917969,
      "learning_rate": 0.0005755019046249694,
      "loss": 2.5871,
      "step": 2560
    },
    {
      "epoch": 2.561,
      "grad_norm": 29.609785079956055,
      "learning_rate": 0.0005747900237964371,
      "loss": 2.51,
      "step": 2561
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 60.4664421081543,
      "learning_rate": 0.0005740784059157404,
      "loss": 2.6761,
      "step": 2562
    },
    {
      "epoch": 2.5629999999999997,
      "grad_norm": 11.812042236328125,
      "learning_rate": 0.0005733670514229406,
      "loss": 2.684,
      "step": 2563
    },
    {
      "epoch": 2.564,
      "grad_norm": 179.94288635253906,
      "learning_rate": 0.0005726559607579368,
      "loss": 2.4813,
      "step": 2564
    },
    {
      "epoch": 2.565,
      "grad_norm": 34.68281555175781,
      "learning_rate": 0.0005719451343604633,
      "loss": 2.4202,
      "step": 2565
    },
    {
      "epoch": 2.566,
      "grad_norm": 77.88746643066406,
      "learning_rate": 0.0005712345726700921,
      "loss": 2.5758,
      "step": 2566
    },
    {
      "epoch": 2.567,
      "grad_norm": 399.78021240234375,
      "learning_rate": 0.000570524276126231,
      "loss": 2.5819,
      "step": 2567
    },
    {
      "epoch": 2.568,
      "grad_norm": 61.30812454223633,
      "learning_rate": 0.0005698142451681237,
      "loss": 2.5141,
      "step": 2568
    },
    {
      "epoch": 2.569,
      "grad_norm": 76.54069519042969,
      "learning_rate": 0.0005691044802348508,
      "loss": 2.472,
      "step": 2569
    },
    {
      "epoch": 2.57,
      "grad_norm": 74.17704010009766,
      "learning_rate": 0.000568394981765327,
      "loss": 2.5304,
      "step": 2570
    },
    {
      "epoch": 2.5709999999999997,
      "grad_norm": 486.00567626953125,
      "learning_rate": 0.0005676857501983026,
      "loss": 2.5629,
      "step": 2571
    },
    {
      "epoch": 2.572,
      "grad_norm": 7693.37548828125,
      "learning_rate": 0.0005669767859723635,
      "loss": 2.5291,
      "step": 2572
    },
    {
      "epoch": 2.573,
      "grad_norm": 77.58704376220703,
      "learning_rate": 0.00056626808952593,
      "loss": 2.5126,
      "step": 2573
    },
    {
      "epoch": 2.574,
      "grad_norm": 41.809146881103516,
      "learning_rate": 0.0005655596612972558,
      "loss": 2.4659,
      "step": 2574
    },
    {
      "epoch": 2.575,
      "grad_norm": 59.51445388793945,
      "learning_rate": 0.0005648515017244305,
      "loss": 2.4374,
      "step": 2575
    },
    {
      "epoch": 2.576,
      "grad_norm": 41.36544418334961,
      "learning_rate": 0.0005641436112453761,
      "loss": 2.4183,
      "step": 2576
    },
    {
      "epoch": 2.577,
      "grad_norm": 23095.81640625,
      "learning_rate": 0.0005634359902978488,
      "loss": 2.5298,
      "step": 2577
    },
    {
      "epoch": 2.578,
      "grad_norm": 499.4382019042969,
      "learning_rate": 0.0005627286393194382,
      "loss": 2.4146,
      "step": 2578
    },
    {
      "epoch": 2.5789999999999997,
      "grad_norm": 209.02484130859375,
      "learning_rate": 0.0005620215587475666,
      "loss": 2.4957,
      "step": 2579
    },
    {
      "epoch": 2.58,
      "grad_norm": 340.8682861328125,
      "learning_rate": 0.0005613147490194887,
      "loss": 2.5172,
      "step": 2580
    },
    {
      "epoch": 2.581,
      "grad_norm": 495.4203186035156,
      "learning_rate": 0.0005606082105722932,
      "loss": 2.4014,
      "step": 2581
    },
    {
      "epoch": 2.582,
      "grad_norm": 1342.35498046875,
      "learning_rate": 0.0005599019438429001,
      "loss": 2.5693,
      "step": 2582
    },
    {
      "epoch": 2.583,
      "grad_norm": 93.22061920166016,
      "learning_rate": 0.0005591959492680613,
      "loss": 2.5511,
      "step": 2583
    },
    {
      "epoch": 2.584,
      "grad_norm": 13.562908172607422,
      "learning_rate": 0.0005584902272843602,
      "loss": 2.5122,
      "step": 2584
    },
    {
      "epoch": 2.585,
      "grad_norm": 1454.306396484375,
      "learning_rate": 0.0005577847783282122,
      "loss": 2.6329,
      "step": 2585
    },
    {
      "epoch": 2.586,
      "grad_norm": 105.24771118164062,
      "learning_rate": 0.000557079602835863,
      "loss": 2.4849,
      "step": 2586
    },
    {
      "epoch": 2.5869999999999997,
      "grad_norm": 72.36042785644531,
      "learning_rate": 0.0005563747012433907,
      "loss": 2.4375,
      "step": 2587
    },
    {
      "epoch": 2.588,
      "grad_norm": 304.1009216308594,
      "learning_rate": 0.0005556700739867027,
      "loss": 2.4908,
      "step": 2588
    },
    {
      "epoch": 2.589,
      "grad_norm": 147.94361877441406,
      "learning_rate": 0.0005549657215015367,
      "loss": 2.4952,
      "step": 2589
    },
    {
      "epoch": 2.59,
      "grad_norm": 52.44453811645508,
      "learning_rate": 0.0005542616442234618,
      "loss": 2.4263,
      "step": 2590
    },
    {
      "epoch": 2.591,
      "grad_norm": 37.526100158691406,
      "learning_rate": 0.0005535578425878755,
      "loss": 2.3868,
      "step": 2591
    },
    {
      "epoch": 2.592,
      "grad_norm": 37.990657806396484,
      "learning_rate": 0.0005528543170300053,
      "loss": 2.3926,
      "step": 2592
    },
    {
      "epoch": 2.593,
      "grad_norm": 231.2610626220703,
      "learning_rate": 0.0005521510679849087,
      "loss": 2.4942,
      "step": 2593
    },
    {
      "epoch": 2.594,
      "grad_norm": 31.657278060913086,
      "learning_rate": 0.0005514480958874711,
      "loss": 2.4953,
      "step": 2594
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 252.78651428222656,
      "learning_rate": 0.0005507454011724072,
      "loss": 2.515,
      "step": 2595
    },
    {
      "epoch": 2.596,
      "grad_norm": 1200.4713134765625,
      "learning_rate": 0.0005500429842742602,
      "loss": 2.6036,
      "step": 2596
    },
    {
      "epoch": 2.597,
      "grad_norm": 1949.2130126953125,
      "learning_rate": 0.0005493408456274007,
      "loss": 2.4582,
      "step": 2597
    },
    {
      "epoch": 2.598,
      "grad_norm": 230.12022399902344,
      "learning_rate": 0.000548638985666029,
      "loss": 2.5484,
      "step": 2598
    },
    {
      "epoch": 2.599,
      "grad_norm": 468.1412353515625,
      "learning_rate": 0.000547937404824171,
      "loss": 2.4423,
      "step": 2599
    },
    {
      "epoch": 2.6,
      "grad_norm": 140.35714721679688,
      "learning_rate": 0.0005472361035356819,
      "loss": 2.5261,
      "step": 2600
    },
    {
      "epoch": 2.6,
      "eval_loss": 2.4791643619537354,
      "eval_runtime": 221.4552,
      "eval_samples_per_second": 0.452,
      "eval_steps_per_second": 0.452,
      "step": 2600
    },
    {
      "epoch": 2.601,
      "grad_norm": 23.284181594848633,
      "learning_rate": 0.0005465350822342425,
      "loss": 2.5701,
      "step": 2601
    },
    {
      "epoch": 2.602,
      "grad_norm": 60.451778411865234,
      "learning_rate": 0.0005458343413533613,
      "loss": 2.5014,
      "step": 2602
    },
    {
      "epoch": 2.6029999999999998,
      "grad_norm": 69.7957534790039,
      "learning_rate": 0.000545133881326373,
      "loss": 2.5073,
      "step": 2603
    },
    {
      "epoch": 2.604,
      "grad_norm": 139.98788452148438,
      "learning_rate": 0.0005444337025864384,
      "loss": 2.5066,
      "step": 2604
    },
    {
      "epoch": 2.605,
      "grad_norm": 79.68655395507812,
      "learning_rate": 0.0005437338055665455,
      "loss": 2.5865,
      "step": 2605
    },
    {
      "epoch": 2.606,
      "grad_norm": 21.034801483154297,
      "learning_rate": 0.0005430341906995064,
      "loss": 2.3937,
      "step": 2606
    },
    {
      "epoch": 2.607,
      "grad_norm": 45.95082473754883,
      "learning_rate": 0.0005423348584179607,
      "loss": 2.4541,
      "step": 2607
    },
    {
      "epoch": 2.608,
      "grad_norm": 22.005754470825195,
      "learning_rate": 0.0005416358091543716,
      "loss": 2.4018,
      "step": 2608
    },
    {
      "epoch": 2.609,
      "grad_norm": 847.2581176757812,
      "learning_rate": 0.0005409370433410277,
      "loss": 2.4467,
      "step": 2609
    },
    {
      "epoch": 2.61,
      "grad_norm": 82.23355102539062,
      "learning_rate": 0.0005402385614100423,
      "loss": 2.365,
      "step": 2610
    },
    {
      "epoch": 2.6109999999999998,
      "grad_norm": 1934.79931640625,
      "learning_rate": 0.000539540363793354,
      "loss": 2.4709,
      "step": 2611
    },
    {
      "epoch": 2.612,
      "grad_norm": 20.157363891601562,
      "learning_rate": 0.0005388424509227248,
      "loss": 2.52,
      "step": 2612
    },
    {
      "epoch": 2.613,
      "grad_norm": 13.514885902404785,
      "learning_rate": 0.0005381448232297401,
      "loss": 2.5165,
      "step": 2613
    },
    {
      "epoch": 2.614,
      "grad_norm": 13.736686706542969,
      "learning_rate": 0.0005374474811458101,
      "loss": 2.3357,
      "step": 2614
    },
    {
      "epoch": 2.615,
      "grad_norm": 149.38409423828125,
      "learning_rate": 0.0005367504251021673,
      "loss": 2.466,
      "step": 2615
    },
    {
      "epoch": 2.616,
      "grad_norm": 25.389795303344727,
      "learning_rate": 0.0005360536555298682,
      "loss": 2.4215,
      "step": 2616
    },
    {
      "epoch": 2.617,
      "grad_norm": 27.242158889770508,
      "learning_rate": 0.0005353571728597921,
      "loss": 2.4151,
      "step": 2617
    },
    {
      "epoch": 2.618,
      "grad_norm": 22.081838607788086,
      "learning_rate": 0.0005346609775226406,
      "loss": 2.3174,
      "step": 2618
    },
    {
      "epoch": 2.6189999999999998,
      "grad_norm": 148.23240661621094,
      "learning_rate": 0.000533965069948937,
      "loss": 2.3647,
      "step": 2619
    },
    {
      "epoch": 2.62,
      "grad_norm": 64.2441635131836,
      "learning_rate": 0.0005332694505690277,
      "loss": 2.3496,
      "step": 2620
    },
    {
      "epoch": 2.621,
      "grad_norm": 7.421729564666748,
      "learning_rate": 0.0005325741198130802,
      "loss": 2.3775,
      "step": 2621
    },
    {
      "epoch": 2.622,
      "grad_norm": 15.813545227050781,
      "learning_rate": 0.0005318790781110836,
      "loss": 2.5323,
      "step": 2622
    },
    {
      "epoch": 2.623,
      "grad_norm": 40.903690338134766,
      "learning_rate": 0.0005311843258928489,
      "loss": 2.5128,
      "step": 2623
    },
    {
      "epoch": 2.624,
      "grad_norm": 98.05545806884766,
      "learning_rate": 0.000530489863588007,
      "loss": 2.5205,
      "step": 2624
    },
    {
      "epoch": 2.625,
      "grad_norm": 14.299042701721191,
      "learning_rate": 0.0005297956916260108,
      "loss": 2.3237,
      "step": 2625
    },
    {
      "epoch": 2.626,
      "grad_norm": 8.095552444458008,
      "learning_rate": 0.0005291018104361326,
      "loss": 2.3849,
      "step": 2626
    },
    {
      "epoch": 2.627,
      "grad_norm": 7.789309978485107,
      "learning_rate": 0.0005284082204474651,
      "loss": 2.3739,
      "step": 2627
    },
    {
      "epoch": 2.628,
      "grad_norm": 32.08073043823242,
      "learning_rate": 0.0005277149220889209,
      "loss": 2.4351,
      "step": 2628
    },
    {
      "epoch": 2.629,
      "grad_norm": 47.40580749511719,
      "learning_rate": 0.0005270219157892331,
      "loss": 2.3927,
      "step": 2629
    },
    {
      "epoch": 2.63,
      "grad_norm": 13.119359016418457,
      "learning_rate": 0.0005263292019769531,
      "loss": 2.468,
      "step": 2630
    },
    {
      "epoch": 2.6310000000000002,
      "grad_norm": 45.55007553100586,
      "learning_rate": 0.0005256367810804518,
      "loss": 2.3227,
      "step": 2631
    },
    {
      "epoch": 2.632,
      "grad_norm": 173.7067108154297,
      "learning_rate": 0.0005249446535279188,
      "loss": 2.3279,
      "step": 2632
    },
    {
      "epoch": 2.633,
      "grad_norm": 145.9557647705078,
      "learning_rate": 0.0005242528197473631,
      "loss": 2.4568,
      "step": 2633
    },
    {
      "epoch": 2.634,
      "grad_norm": 55.02260971069336,
      "learning_rate": 0.0005235612801666107,
      "loss": 2.3126,
      "step": 2634
    },
    {
      "epoch": 2.635,
      "grad_norm": 41.10543441772461,
      "learning_rate": 0.0005228700352133071,
      "loss": 2.3364,
      "step": 2635
    },
    {
      "epoch": 2.636,
      "grad_norm": 29.35865020751953,
      "learning_rate": 0.0005221790853149147,
      "loss": 2.3744,
      "step": 2636
    },
    {
      "epoch": 2.637,
      "grad_norm": 11.914557456970215,
      "learning_rate": 0.0005214884308987136,
      "loss": 2.3675,
      "step": 2637
    },
    {
      "epoch": 2.638,
      "grad_norm": 10.46717643737793,
      "learning_rate": 0.0005207980723918012,
      "loss": 2.4175,
      "step": 2638
    },
    {
      "epoch": 2.6390000000000002,
      "grad_norm": 9.074422836303711,
      "learning_rate": 0.0005201080102210918,
      "loss": 2.4363,
      "step": 2639
    },
    {
      "epoch": 2.64,
      "grad_norm": 524.8789672851562,
      "learning_rate": 0.0005194182448133162,
      "loss": 2.388,
      "step": 2640
    },
    {
      "epoch": 2.641,
      "grad_norm": 29.85433578491211,
      "learning_rate": 0.0005187287765950229,
      "loss": 2.534,
      "step": 2641
    },
    {
      "epoch": 2.642,
      "grad_norm": 70.17790985107422,
      "learning_rate": 0.0005180396059925755,
      "loss": 2.3106,
      "step": 2642
    },
    {
      "epoch": 2.643,
      "grad_norm": 11.362831115722656,
      "learning_rate": 0.0005173507334321538,
      "loss": 2.1777,
      "step": 2643
    },
    {
      "epoch": 2.644,
      "grad_norm": 32.9367561340332,
      "learning_rate": 0.0005166621593397535,
      "loss": 2.2449,
      "step": 2644
    },
    {
      "epoch": 2.645,
      "grad_norm": 95.65739440917969,
      "learning_rate": 0.0005159738841411852,
      "loss": 2.373,
      "step": 2645
    },
    {
      "epoch": 2.646,
      "grad_norm": 28.125843048095703,
      "learning_rate": 0.0005152859082620748,
      "loss": 2.382,
      "step": 2646
    },
    {
      "epoch": 2.6470000000000002,
      "grad_norm": 62.967464447021484,
      "learning_rate": 0.0005145982321278641,
      "loss": 2.3925,
      "step": 2647
    },
    {
      "epoch": 2.648,
      "grad_norm": 30.32352066040039,
      "learning_rate": 0.0005139108561638084,
      "loss": 2.3465,
      "step": 2648
    },
    {
      "epoch": 2.649,
      "grad_norm": 14.796099662780762,
      "learning_rate": 0.0005132237807949777,
      "loss": 2.4175,
      "step": 2649
    },
    {
      "epoch": 2.65,
      "grad_norm": 7.925904750823975,
      "learning_rate": 0.0005125370064462558,
      "loss": 2.2119,
      "step": 2650
    },
    {
      "epoch": 2.651,
      "grad_norm": 9.909093856811523,
      "learning_rate": 0.0005118505335423414,
      "loss": 2.3058,
      "step": 2651
    },
    {
      "epoch": 2.652,
      "grad_norm": 64.24073791503906,
      "learning_rate": 0.0005111643625077454,
      "loss": 2.3198,
      "step": 2652
    },
    {
      "epoch": 2.653,
      "grad_norm": 26.21586036682129,
      "learning_rate": 0.0005104784937667935,
      "loss": 2.3644,
      "step": 2653
    },
    {
      "epoch": 2.654,
      "grad_norm": 4.5762224197387695,
      "learning_rate": 0.0005097929277436233,
      "loss": 2.3278,
      "step": 2654
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 44.13794708251953,
      "learning_rate": 0.0005091076648621856,
      "loss": 2.385,
      "step": 2655
    },
    {
      "epoch": 2.656,
      "grad_norm": 119.50316619873047,
      "learning_rate": 0.0005084227055462435,
      "loss": 2.5541,
      "step": 2656
    },
    {
      "epoch": 2.657,
      "grad_norm": 573.301513671875,
      "learning_rate": 0.0005077380502193725,
      "loss": 2.4443,
      "step": 2657
    },
    {
      "epoch": 2.658,
      "grad_norm": 5.430088520050049,
      "learning_rate": 0.0005070536993049608,
      "loss": 2.5348,
      "step": 2658
    },
    {
      "epoch": 2.659,
      "grad_norm": 237.3760223388672,
      "learning_rate": 0.0005063696532262071,
      "loss": 2.3476,
      "step": 2659
    },
    {
      "epoch": 2.66,
      "grad_norm": 10.056231498718262,
      "learning_rate": 0.0005056859124061231,
      "loss": 2.3296,
      "step": 2660
    },
    {
      "epoch": 2.661,
      "grad_norm": 17.03553009033203,
      "learning_rate": 0.0005050024772675301,
      "loss": 2.5212,
      "step": 2661
    },
    {
      "epoch": 2.662,
      "grad_norm": 1783.8583984375,
      "learning_rate": 0.0005043193482330617,
      "loss": 2.4801,
      "step": 2662
    },
    {
      "epoch": 2.6630000000000003,
      "grad_norm": 14.807062149047852,
      "learning_rate": 0.0005036365257251613,
      "loss": 2.4209,
      "step": 2663
    },
    {
      "epoch": 2.664,
      "grad_norm": 201.0807647705078,
      "learning_rate": 0.0005029540101660829,
      "loss": 2.5215,
      "step": 2664
    },
    {
      "epoch": 2.665,
      "grad_norm": 19.99355125427246,
      "learning_rate": 0.0005022718019778915,
      "loss": 2.4531,
      "step": 2665
    },
    {
      "epoch": 2.666,
      "grad_norm": 30.60120964050293,
      "learning_rate": 0.0005015899015824612,
      "loss": 2.3391,
      "step": 2666
    },
    {
      "epoch": 2.667,
      "grad_norm": 254.57208251953125,
      "learning_rate": 0.0005009083094014755,
      "loss": 2.3969,
      "step": 2667
    },
    {
      "epoch": 2.668,
      "grad_norm": 25.084915161132812,
      "learning_rate": 0.0005002270258564284,
      "loss": 2.4584,
      "step": 2668
    },
    {
      "epoch": 2.669,
      "grad_norm": 23.253419876098633,
      "learning_rate": 0.0004995460513686223,
      "loss": 2.3741,
      "step": 2669
    },
    {
      "epoch": 2.67,
      "grad_norm": 34.84798049926758,
      "learning_rate": 0.0004988653863591681,
      "loss": 2.3387,
      "step": 2670
    },
    {
      "epoch": 2.6710000000000003,
      "grad_norm": 33.747493743896484,
      "learning_rate": 0.0004981850312489866,
      "loss": 2.3781,
      "step": 2671
    },
    {
      "epoch": 2.672,
      "grad_norm": 24.12771224975586,
      "learning_rate": 0.0004975049864588058,
      "loss": 2.5462,
      "step": 2672
    },
    {
      "epoch": 2.673,
      "grad_norm": 42.475833892822266,
      "learning_rate": 0.0004968252524091621,
      "loss": 2.4878,
      "step": 2673
    },
    {
      "epoch": 2.674,
      "grad_norm": 287.88409423828125,
      "learning_rate": 0.0004961458295203999,
      "loss": 2.3444,
      "step": 2674
    },
    {
      "epoch": 2.675,
      "grad_norm": 47.64751052856445,
      "learning_rate": 0.0004954667182126707,
      "loss": 2.3024,
      "step": 2675
    },
    {
      "epoch": 2.676,
      "grad_norm": 34.23588180541992,
      "learning_rate": 0.0004947879189059341,
      "loss": 2.4401,
      "step": 2676
    },
    {
      "epoch": 2.677,
      "grad_norm": 67.68582916259766,
      "learning_rate": 0.0004941094320199568,
      "loss": 2.4669,
      "step": 2677
    },
    {
      "epoch": 2.678,
      "grad_norm": 153.8092041015625,
      "learning_rate": 0.0004934312579743113,
      "loss": 2.4727,
      "step": 2678
    },
    {
      "epoch": 2.6790000000000003,
      "grad_norm": 565.7080688476562,
      "learning_rate": 0.0004927533971883775,
      "loss": 2.4417,
      "step": 2679
    },
    {
      "epoch": 2.68,
      "grad_norm": 2731.791259765625,
      "learning_rate": 0.0004920758500813412,
      "loss": 2.5675,
      "step": 2680
    },
    {
      "epoch": 2.681,
      "grad_norm": 845.3548583984375,
      "learning_rate": 0.0004913986170721941,
      "loss": 2.3791,
      "step": 2681
    },
    {
      "epoch": 2.682,
      "grad_norm": 124.88505554199219,
      "learning_rate": 0.0004907216985797338,
      "loss": 2.4656,
      "step": 2682
    },
    {
      "epoch": 2.683,
      "grad_norm": 59.84575271606445,
      "learning_rate": 0.0004900450950225642,
      "loss": 2.5164,
      "step": 2683
    },
    {
      "epoch": 2.684,
      "grad_norm": 46.38576126098633,
      "learning_rate": 0.0004893688068190932,
      "loss": 2.398,
      "step": 2684
    },
    {
      "epoch": 2.685,
      "grad_norm": 113.03352355957031,
      "learning_rate": 0.0004886928343875341,
      "loss": 2.4022,
      "step": 2685
    },
    {
      "epoch": 2.686,
      "grad_norm": 191.374267578125,
      "learning_rate": 0.0004880171781459055,
      "loss": 2.3479,
      "step": 2686
    },
    {
      "epoch": 2.6870000000000003,
      "grad_norm": 244.92286682128906,
      "learning_rate": 0.00048734183851202996,
      "loss": 2.5428,
      "step": 2687
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 42.20957565307617,
      "learning_rate": 0.00048666681590353366,
      "loss": 2.4881,
      "step": 2688
    },
    {
      "epoch": 2.689,
      "grad_norm": 3166.81982421875,
      "learning_rate": 0.00048599211073784823,
      "loss": 2.3998,
      "step": 2689
    },
    {
      "epoch": 2.69,
      "grad_norm": 96.02172088623047,
      "learning_rate": 0.00048531772343220783,
      "loss": 2.3809,
      "step": 2690
    },
    {
      "epoch": 2.691,
      "grad_norm": 38.41667556762695,
      "learning_rate": 0.00048464365440365045,
      "loss": 2.3963,
      "step": 2691
    },
    {
      "epoch": 2.692,
      "grad_norm": 249.33523559570312,
      "learning_rate": 0.00048396990406901686,
      "loss": 2.312,
      "step": 2692
    },
    {
      "epoch": 2.693,
      "grad_norm": 142.09121704101562,
      "learning_rate": 0.00048329647284495105,
      "loss": 2.4777,
      "step": 2693
    },
    {
      "epoch": 2.694,
      "grad_norm": 53.786128997802734,
      "learning_rate": 0.00048262336114789995,
      "loss": 2.309,
      "step": 2694
    },
    {
      "epoch": 2.695,
      "grad_norm": 227.7447509765625,
      "learning_rate": 0.00048195056939411295,
      "loss": 2.3548,
      "step": 2695
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 87.26122283935547,
      "learning_rate": 0.0004812780979996411,
      "loss": 2.4253,
      "step": 2696
    },
    {
      "epoch": 2.697,
      "grad_norm": 378.882080078125,
      "learning_rate": 0.00048060594738033737,
      "loss": 2.347,
      "step": 2697
    },
    {
      "epoch": 2.698,
      "grad_norm": 334.91436767578125,
      "learning_rate": 0.0004799341179518565,
      "loss": 2.4347,
      "step": 2698
    },
    {
      "epoch": 2.699,
      "grad_norm": 158.5510711669922,
      "learning_rate": 0.00047926261012965464,
      "loss": 2.399,
      "step": 2699
    },
    {
      "epoch": 2.7,
      "grad_norm": 753.9526977539062,
      "learning_rate": 0.00047859142432898883,
      "loss": 2.4112,
      "step": 2700
    },
    {
      "epoch": 2.7,
      "eval_loss": 2.398365020751953,
      "eval_runtime": 221.7152,
      "eval_samples_per_second": 0.451,
      "eval_steps_per_second": 0.451,
      "step": 2700
    },
    {
      "epoch": 2.701,
      "grad_norm": 831.3394775390625,
      "learning_rate": 0.00047792056096491775,
      "loss": 2.5335,
      "step": 2701
    },
    {
      "epoch": 2.702,
      "grad_norm": 74.29889678955078,
      "learning_rate": 0.0004772500204522994,
      "loss": 2.3983,
      "step": 2702
    },
    {
      "epoch": 2.703,
      "grad_norm": 35.00475311279297,
      "learning_rate": 0.00047657980320579374,
      "loss": 2.375,
      "step": 2703
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 236.7357635498047,
      "learning_rate": 0.0004759099096398595,
      "loss": 2.3825,
      "step": 2704
    },
    {
      "epoch": 2.705,
      "grad_norm": 11093.400390625,
      "learning_rate": 0.0004752403401687556,
      "loss": 2.7538,
      "step": 2705
    },
    {
      "epoch": 2.706,
      "grad_norm": 158.70066833496094,
      "learning_rate": 0.0004745710952065404,
      "loss": 2.4127,
      "step": 2706
    },
    {
      "epoch": 2.707,
      "grad_norm": 1838.7646484375,
      "learning_rate": 0.0004739021751670725,
      "loss": 2.5143,
      "step": 2707
    },
    {
      "epoch": 2.708,
      "grad_norm": 598.204833984375,
      "learning_rate": 0.00047323358046400844,
      "loss": 2.3534,
      "step": 2708
    },
    {
      "epoch": 2.709,
      "grad_norm": 25.703332901000977,
      "learning_rate": 0.00047256531151080417,
      "loss": 2.233,
      "step": 2709
    },
    {
      "epoch": 2.71,
      "grad_norm": 29.954635620117188,
      "learning_rate": 0.000471897368720714,
      "loss": 2.3813,
      "step": 2710
    },
    {
      "epoch": 2.711,
      "grad_norm": 15.391302108764648,
      "learning_rate": 0.00047122975250679,
      "loss": 2.206,
      "step": 2711
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 101.17560577392578,
      "learning_rate": 0.00047056246328188356,
      "loss": 2.2226,
      "step": 2712
    },
    {
      "epoch": 2.713,
      "grad_norm": 219.00164794921875,
      "learning_rate": 0.00046989550145864345,
      "loss": 2.3046,
      "step": 2713
    },
    {
      "epoch": 2.714,
      "grad_norm": 177.5283203125,
      "learning_rate": 0.0004692288674495152,
      "loss": 2.2771,
      "step": 2714
    },
    {
      "epoch": 2.715,
      "grad_norm": 30.078750610351562,
      "learning_rate": 0.00046856256166674225,
      "loss": 2.2478,
      "step": 2715
    },
    {
      "epoch": 2.716,
      "grad_norm": 884.384765625,
      "learning_rate": 0.00046789658452236495,
      "loss": 2.6254,
      "step": 2716
    },
    {
      "epoch": 2.717,
      "grad_norm": 371.98980712890625,
      "learning_rate": 0.00046723093642822,
      "loss": 2.8043,
      "step": 2717
    },
    {
      "epoch": 2.718,
      "grad_norm": 31.335004806518555,
      "learning_rate": 0.00046656561779594174,
      "loss": 2.3889,
      "step": 2718
    },
    {
      "epoch": 2.719,
      "grad_norm": 759.0619506835938,
      "learning_rate": 0.00046590062903695983,
      "loss": 2.5968,
      "step": 2719
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 27.388715744018555,
      "learning_rate": 0.00046523597056249966,
      "loss": 2.2765,
      "step": 2720
    },
    {
      "epoch": 2.721,
      "grad_norm": 74.64006042480469,
      "learning_rate": 0.00046457164278358357,
      "loss": 2.3453,
      "step": 2721
    },
    {
      "epoch": 2.722,
      "grad_norm": 1347.9852294921875,
      "learning_rate": 0.00046390764611102856,
      "loss": 2.4022,
      "step": 2722
    },
    {
      "epoch": 2.723,
      "grad_norm": 154.10316467285156,
      "learning_rate": 0.0004632439809554468,
      "loss": 2.5778,
      "step": 2723
    },
    {
      "epoch": 2.724,
      "grad_norm": 79.67169189453125,
      "learning_rate": 0.0004625806477272455,
      "loss": 2.4684,
      "step": 2724
    },
    {
      "epoch": 2.725,
      "grad_norm": 76.19547271728516,
      "learning_rate": 0.0004619176468366274,
      "loss": 2.2556,
      "step": 2725
    },
    {
      "epoch": 2.726,
      "grad_norm": 344.0086669921875,
      "learning_rate": 0.00046125497869358876,
      "loss": 2.6776,
      "step": 2726
    },
    {
      "epoch": 2.727,
      "grad_norm": 439.5830383300781,
      "learning_rate": 0.00046059264370792053,
      "loss": 2.3672,
      "step": 2727
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 216.55419921875,
      "learning_rate": 0.00045993064228920733,
      "loss": 2.7853,
      "step": 2728
    },
    {
      "epoch": 2.729,
      "grad_norm": 140.1542510986328,
      "learning_rate": 0.0004592689748468275,
      "loss": 2.4793,
      "step": 2729
    },
    {
      "epoch": 2.73,
      "grad_norm": 876.0005493164062,
      "learning_rate": 0.0004586076417899534,
      "loss": 2.3581,
      "step": 2730
    },
    {
      "epoch": 2.731,
      "grad_norm": 3498.130615234375,
      "learning_rate": 0.00045794664352755057,
      "loss": 2.4882,
      "step": 2731
    },
    {
      "epoch": 2.732,
      "grad_norm": 3384.0009765625,
      "learning_rate": 0.0004572859804683769,
      "loss": 2.3745,
      "step": 2732
    },
    {
      "epoch": 2.733,
      "grad_norm": 437.16448974609375,
      "learning_rate": 0.00045662565302098326,
      "loss": 2.3132,
      "step": 2733
    },
    {
      "epoch": 2.734,
      "grad_norm": 272.13336181640625,
      "learning_rate": 0.000455965661593713,
      "loss": 2.4591,
      "step": 2734
    },
    {
      "epoch": 2.735,
      "grad_norm": 348.69677734375,
      "learning_rate": 0.00045530600659470134,
      "loss": 2.568,
      "step": 2735
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 296.9554443359375,
      "learning_rate": 0.0004546466884318766,
      "loss": 2.5549,
      "step": 2736
    },
    {
      "epoch": 2.737,
      "grad_norm": 97.33748626708984,
      "learning_rate": 0.00045398770751295717,
      "loss": 2.9196,
      "step": 2737
    },
    {
      "epoch": 2.738,
      "grad_norm": 26.628767013549805,
      "learning_rate": 0.0004533290642454546,
      "loss": 2.3074,
      "step": 2738
    },
    {
      "epoch": 2.739,
      "grad_norm": 54.396385192871094,
      "learning_rate": 0.00045267075903667023,
      "loss": 2.5118,
      "step": 2739
    },
    {
      "epoch": 2.74,
      "grad_norm": 51.147701263427734,
      "learning_rate": 0.0004520127922936971,
      "loss": 2.6488,
      "step": 2740
    },
    {
      "epoch": 2.741,
      "grad_norm": 169.58547973632812,
      "learning_rate": 0.00045135516442341853,
      "loss": 2.327,
      "step": 2741
    },
    {
      "epoch": 2.742,
      "grad_norm": 19.70209503173828,
      "learning_rate": 0.0004506978758325081,
      "loss": 2.428,
      "step": 2742
    },
    {
      "epoch": 2.743,
      "grad_norm": 496.9162292480469,
      "learning_rate": 0.00045004092692743094,
      "loss": 2.4968,
      "step": 2743
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 482.25616455078125,
      "learning_rate": 0.0004493843181144406,
      "loss": 2.3149,
      "step": 2744
    },
    {
      "epoch": 2.745,
      "grad_norm": 281.5605773925781,
      "learning_rate": 0.0004487280497995808,
      "loss": 2.3237,
      "step": 2745
    },
    {
      "epoch": 2.746,
      "grad_norm": 1019.6764526367188,
      "learning_rate": 0.00044807212238868457,
      "loss": 2.5728,
      "step": 2746
    },
    {
      "epoch": 2.747,
      "grad_norm": 285.2435607910156,
      "learning_rate": 0.0004474165362873749,
      "loss": 2.5635,
      "step": 2747
    },
    {
      "epoch": 2.748,
      "grad_norm": 43.13798522949219,
      "learning_rate": 0.0004467612919010624,
      "loss": 2.4936,
      "step": 2748
    },
    {
      "epoch": 2.749,
      "grad_norm": 78.1955337524414,
      "learning_rate": 0.0004461063896349479,
      "loss": 2.3572,
      "step": 2749
    },
    {
      "epoch": 2.75,
      "grad_norm": 75.775390625,
      "learning_rate": 0.0004454518298940196,
      "loss": 2.4527,
      "step": 2750
    },
    {
      "epoch": 2.751,
      "grad_norm": 112.57640838623047,
      "learning_rate": 0.00044479761308305386,
      "loss": 2.6993,
      "step": 2751
    },
    {
      "epoch": 2.752,
      "grad_norm": 239.7730255126953,
      "learning_rate": 0.00044414373960661546,
      "loss": 2.4867,
      "step": 2752
    },
    {
      "epoch": 2.753,
      "grad_norm": 69.82917022705078,
      "learning_rate": 0.0004434902098690563,
      "loss": 2.5506,
      "step": 2753
    },
    {
      "epoch": 2.754,
      "grad_norm": 14.825581550598145,
      "learning_rate": 0.00044283702427451677,
      "loss": 2.2981,
      "step": 2754
    },
    {
      "epoch": 2.755,
      "grad_norm": 83.99708557128906,
      "learning_rate": 0.00044218418322692287,
      "loss": 2.3297,
      "step": 2755
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 37.477630615234375,
      "learning_rate": 0.00044153168712998946,
      "loss": 2.2934,
      "step": 2756
    },
    {
      "epoch": 2.757,
      "grad_norm": 121.7288818359375,
      "learning_rate": 0.00044087953638721647,
      "loss": 2.4356,
      "step": 2757
    },
    {
      "epoch": 2.758,
      "grad_norm": 429.339599609375,
      "learning_rate": 0.0004402277314018908,
      "loss": 2.6346,
      "step": 2758
    },
    {
      "epoch": 2.759,
      "grad_norm": 4037.59375,
      "learning_rate": 0.0004395762725770852,
      "loss": 2.3087,
      "step": 2759
    },
    {
      "epoch": 2.76,
      "grad_norm": 909.62109375,
      "learning_rate": 0.00043892516031565957,
      "loss": 2.276,
      "step": 2760
    },
    {
      "epoch": 2.761,
      "grad_norm": 93.7209243774414,
      "learning_rate": 0.00043827439502025826,
      "loss": 2.3323,
      "step": 2761
    },
    {
      "epoch": 2.762,
      "grad_norm": 20.720073699951172,
      "learning_rate": 0.00043762397709331137,
      "loss": 2.3374,
      "step": 2762
    },
    {
      "epoch": 2.763,
      "grad_norm": 140.30645751953125,
      "learning_rate": 0.0004369739069370343,
      "loss": 2.5012,
      "step": 2763
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 87.67245483398438,
      "learning_rate": 0.00043632418495342696,
      "loss": 2.5831,
      "step": 2764
    },
    {
      "epoch": 2.765,
      "grad_norm": 18.0341796875,
      "learning_rate": 0.00043567481154427524,
      "loss": 2.5904,
      "step": 2765
    },
    {
      "epoch": 2.766,
      "grad_norm": 279.1724548339844,
      "learning_rate": 0.0004350257871111476,
      "loss": 2.4278,
      "step": 2766
    },
    {
      "epoch": 2.767,
      "grad_norm": 28.855939865112305,
      "learning_rate": 0.00043437711205539866,
      "loss": 2.279,
      "step": 2767
    },
    {
      "epoch": 2.768,
      "grad_norm": 94.209716796875,
      "learning_rate": 0.0004337287867781654,
      "loss": 2.4838,
      "step": 2768
    },
    {
      "epoch": 2.769,
      "grad_norm": 159.83395385742188,
      "learning_rate": 0.00043308081168036926,
      "loss": 2.3842,
      "step": 2769
    },
    {
      "epoch": 2.77,
      "grad_norm": 136.06251525878906,
      "learning_rate": 0.000432433187162715,
      "loss": 2.3617,
      "step": 2770
    },
    {
      "epoch": 2.771,
      "grad_norm": 32.60847473144531,
      "learning_rate": 0.00043178591362569,
      "loss": 2.3676,
      "step": 2771
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 50.33320617675781,
      "learning_rate": 0.0004311389914695657,
      "loss": 2.5371,
      "step": 2772
    },
    {
      "epoch": 2.773,
      "grad_norm": 231.62158203125,
      "learning_rate": 0.000430492421094396,
      "loss": 2.4513,
      "step": 2773
    },
    {
      "epoch": 2.774,
      "grad_norm": 113.32378387451172,
      "learning_rate": 0.0004298462029000165,
      "loss": 2.4909,
      "step": 2774
    },
    {
      "epoch": 2.775,
      "grad_norm": 485.0134582519531,
      "learning_rate": 0.0004292003372860457,
      "loss": 2.3999,
      "step": 2775
    },
    {
      "epoch": 2.776,
      "grad_norm": 599.5899047851562,
      "learning_rate": 0.00042855482465188366,
      "loss": 2.2663,
      "step": 2776
    },
    {
      "epoch": 2.777,
      "grad_norm": 97.26655578613281,
      "learning_rate": 0.00042790966539671193,
      "loss": 2.2812,
      "step": 2777
    },
    {
      "epoch": 2.778,
      "grad_norm": 559.96435546875,
      "learning_rate": 0.00042726485991949483,
      "loss": 2.2745,
      "step": 2778
    },
    {
      "epoch": 2.779,
      "grad_norm": 303.20892333984375,
      "learning_rate": 0.00042662040861897655,
      "loss": 2.3637,
      "step": 2779
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1293.95947265625,
      "learning_rate": 0.0004259763118936827,
      "loss": 2.4095,
      "step": 2780
    },
    {
      "epoch": 2.781,
      "grad_norm": 69.33599090576172,
      "learning_rate": 0.0004253325701419194,
      "loss": 2.3559,
      "step": 2781
    },
    {
      "epoch": 2.782,
      "grad_norm": 17.085384368896484,
      "learning_rate": 0.00042468918376177415,
      "loss": 2.2894,
      "step": 2782
    },
    {
      "epoch": 2.783,
      "grad_norm": 64.71652221679688,
      "learning_rate": 0.0004240461531511136,
      "loss": 2.5482,
      "step": 2783
    },
    {
      "epoch": 2.784,
      "grad_norm": 355.8529052734375,
      "learning_rate": 0.0004234034787075847,
      "loss": 2.4267,
      "step": 2784
    },
    {
      "epoch": 2.785,
      "grad_norm": 276.05450439453125,
      "learning_rate": 0.0004227611608286147,
      "loss": 2.5912,
      "step": 2785
    },
    {
      "epoch": 2.786,
      "grad_norm": 21.081605911254883,
      "learning_rate": 0.0004221191999114098,
      "loss": 2.3685,
      "step": 2786
    },
    {
      "epoch": 2.787,
      "grad_norm": 71.7578125,
      "learning_rate": 0.0004214775963529555,
      "loss": 2.5466,
      "step": 2787
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 68.62333679199219,
      "learning_rate": 0.00042083635055001643,
      "loss": 2.3044,
      "step": 2788
    },
    {
      "epoch": 2.789,
      "grad_norm": 48.465518951416016,
      "learning_rate": 0.0004201954628991356,
      "loss": 2.4579,
      "step": 2789
    },
    {
      "epoch": 2.79,
      "grad_norm": 310.71466064453125,
      "learning_rate": 0.0004195549337966352,
      "loss": 2.4677,
      "step": 2790
    },
    {
      "epoch": 2.791,
      "grad_norm": 32.544654846191406,
      "learning_rate": 0.00041891476363861593,
      "loss": 2.2704,
      "step": 2791
    },
    {
      "epoch": 2.792,
      "grad_norm": 252.73960876464844,
      "learning_rate": 0.0004182749528209555,
      "loss": 2.3136,
      "step": 2792
    },
    {
      "epoch": 2.793,
      "grad_norm": 284.3739013671875,
      "learning_rate": 0.00041763550173930984,
      "loss": 2.361,
      "step": 2793
    },
    {
      "epoch": 2.794,
      "grad_norm": 439.00360107421875,
      "learning_rate": 0.00041699641078911257,
      "loss": 2.6158,
      "step": 2794
    },
    {
      "epoch": 2.795,
      "grad_norm": 45.37565231323242,
      "learning_rate": 0.00041635768036557395,
      "loss": 2.3422,
      "step": 2795
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 38.91007995605469,
      "learning_rate": 0.0004157193108636829,
      "loss": 2.4385,
      "step": 2796
    },
    {
      "epoch": 2.797,
      "grad_norm": 103.50369262695312,
      "learning_rate": 0.00041508130267820344,
      "loss": 2.3414,
      "step": 2797
    },
    {
      "epoch": 2.798,
      "grad_norm": 131.7727508544922,
      "learning_rate": 0.0004144436562036771,
      "loss": 2.4743,
      "step": 2798
    },
    {
      "epoch": 2.799,
      "grad_norm": 95.0635757446289,
      "learning_rate": 0.0004138063718344207,
      "loss": 2.2817,
      "step": 2799
    },
    {
      "epoch": 2.8,
      "grad_norm": 35.899635314941406,
      "learning_rate": 0.000413169449964529,
      "loss": 2.2787,
      "step": 2800
    },
    {
      "epoch": 2.8,
      "eval_loss": 2.3438215255737305,
      "eval_runtime": 221.4813,
      "eval_samples_per_second": 0.452,
      "eval_steps_per_second": 0.452,
      "step": 2800
    },
    {
      "epoch": 2.801,
      "grad_norm": 112.64262390136719,
      "learning_rate": 0.00041253289098787074,
      "loss": 2.4022,
      "step": 2801
    },
    {
      "epoch": 2.802,
      "grad_norm": 17.592937469482422,
      "learning_rate": 0.00041189669529809094,
      "loss": 2.3514,
      "step": 2802
    },
    {
      "epoch": 2.803,
      "grad_norm": 121.91743469238281,
      "learning_rate": 0.00041126086328861045,
      "loss": 2.3151,
      "step": 2803
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 126.11660766601562,
      "learning_rate": 0.0004106253953526247,
      "loss": 2.2286,
      "step": 2804
    },
    {
      "epoch": 2.805,
      "grad_norm": 346.52203369140625,
      "learning_rate": 0.0004099902918831038,
      "loss": 2.2895,
      "step": 2805
    },
    {
      "epoch": 2.806,
      "grad_norm": 187.33383178710938,
      "learning_rate": 0.00040935555327279273,
      "loss": 2.3057,
      "step": 2806
    },
    {
      "epoch": 2.807,
      "grad_norm": 267.4583740234375,
      "learning_rate": 0.00040872117991421066,
      "loss": 2.2765,
      "step": 2807
    },
    {
      "epoch": 2.808,
      "grad_norm": 21.759883880615234,
      "learning_rate": 0.0004080871721996513,
      "loss": 2.3344,
      "step": 2808
    },
    {
      "epoch": 2.809,
      "grad_norm": 257.4724426269531,
      "learning_rate": 0.0004074535305211825,
      "loss": 2.2762,
      "step": 2809
    },
    {
      "epoch": 2.81,
      "grad_norm": 116.6666488647461,
      "learning_rate": 0.0004068202552706448,
      "loss": 2.3212,
      "step": 2810
    },
    {
      "epoch": 2.811,
      "grad_norm": 126.99402618408203,
      "learning_rate": 0.0004061873468396526,
      "loss": 2.3555,
      "step": 2811
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 59.76203918457031,
      "learning_rate": 0.0004055548056195937,
      "loss": 2.3649,
      "step": 2812
    },
    {
      "epoch": 2.8129999999999997,
      "grad_norm": 50.39413833618164,
      "learning_rate": 0.00040492263200162795,
      "loss": 2.3437,
      "step": 2813
    },
    {
      "epoch": 2.814,
      "grad_norm": 41.724327087402344,
      "learning_rate": 0.0004042908263766893,
      "loss": 2.4525,
      "step": 2814
    },
    {
      "epoch": 2.815,
      "grad_norm": 163.38717651367188,
      "learning_rate": 0.0004036593891354832,
      "loss": 2.2223,
      "step": 2815
    },
    {
      "epoch": 2.816,
      "grad_norm": 43.049278259277344,
      "learning_rate": 0.0004030283206684868,
      "loss": 2.3107,
      "step": 2816
    },
    {
      "epoch": 2.817,
      "grad_norm": 1559.2459716796875,
      "learning_rate": 0.0004023976213659508,
      "loss": 2.1973,
      "step": 2817
    },
    {
      "epoch": 2.818,
      "grad_norm": 157.11279296875,
      "learning_rate": 0.00040176729161789615,
      "loss": 2.3351,
      "step": 2818
    },
    {
      "epoch": 2.819,
      "grad_norm": 82.86060333251953,
      "learning_rate": 0.0004011373318141156,
      "loss": 2.3502,
      "step": 2819
    },
    {
      "epoch": 2.82,
      "grad_norm": 152.24917602539062,
      "learning_rate": 0.00040050774234417385,
      "loss": 2.2598,
      "step": 2820
    },
    {
      "epoch": 2.8209999999999997,
      "grad_norm": 32.7861328125,
      "learning_rate": 0.0003998785235974058,
      "loss": 2.2682,
      "step": 2821
    },
    {
      "epoch": 2.822,
      "grad_norm": 310.4176025390625,
      "learning_rate": 0.00039924967596291727,
      "loss": 2.2528,
      "step": 2822
    },
    {
      "epoch": 2.823,
      "grad_norm": 16.38443374633789,
      "learning_rate": 0.0003986211998295847,
      "loss": 2.312,
      "step": 2823
    },
    {
      "epoch": 2.824,
      "grad_norm": 225.95169067382812,
      "learning_rate": 0.0003979930955860548,
      "loss": 2.59,
      "step": 2824
    },
    {
      "epoch": 2.825,
      "grad_norm": 165.09002685546875,
      "learning_rate": 0.0003973653636207437,
      "loss": 2.4264,
      "step": 2825
    },
    {
      "epoch": 2.826,
      "grad_norm": 98.38671875,
      "learning_rate": 0.0003967380043218385,
      "loss": 2.6258,
      "step": 2826
    },
    {
      "epoch": 2.827,
      "grad_norm": 1574.578369140625,
      "learning_rate": 0.00039611101807729545,
      "loss": 2.4265,
      "step": 2827
    },
    {
      "epoch": 2.828,
      "grad_norm": 15.522262573242188,
      "learning_rate": 0.00039548440527483955,
      "loss": 2.3466,
      "step": 2828
    },
    {
      "epoch": 2.8289999999999997,
      "grad_norm": 121.92875671386719,
      "learning_rate": 0.00039485816630196514,
      "loss": 2.4567,
      "step": 2829
    },
    {
      "epoch": 2.83,
      "grad_norm": 387.6595458984375,
      "learning_rate": 0.00039423230154593536,
      "loss": 2.4578,
      "step": 2830
    },
    {
      "epoch": 2.831,
      "grad_norm": 35.231895446777344,
      "learning_rate": 0.0003936068113937819,
      "loss": 2.3478,
      "step": 2831
    },
    {
      "epoch": 2.832,
      "grad_norm": 131.68190002441406,
      "learning_rate": 0.0003929816962323054,
      "loss": 2.3937,
      "step": 2832
    },
    {
      "epoch": 2.833,
      "grad_norm": 85.98857116699219,
      "learning_rate": 0.0003923569564480736,
      "loss": 2.2404,
      "step": 2833
    },
    {
      "epoch": 2.834,
      "grad_norm": 53.381221771240234,
      "learning_rate": 0.0003917325924274231,
      "loss": 2.3603,
      "step": 2834
    },
    {
      "epoch": 2.835,
      "grad_norm": 43.091163635253906,
      "learning_rate": 0.00039110860455645745,
      "loss": 2.2741,
      "step": 2835
    },
    {
      "epoch": 2.836,
      "grad_norm": 172.35287475585938,
      "learning_rate": 0.0003904849932210478,
      "loss": 2.303,
      "step": 2836
    },
    {
      "epoch": 2.8369999999999997,
      "grad_norm": 965.9765014648438,
      "learning_rate": 0.00038986175880683196,
      "loss": 2.4964,
      "step": 2837
    },
    {
      "epoch": 2.838,
      "grad_norm": 160.11949157714844,
      "learning_rate": 0.00038923890169921603,
      "loss": 2.3374,
      "step": 2838
    },
    {
      "epoch": 2.839,
      "grad_norm": 157.95281982421875,
      "learning_rate": 0.00038861642228337146,
      "loss": 2.358,
      "step": 2839
    },
    {
      "epoch": 2.84,
      "grad_norm": 62.82260513305664,
      "learning_rate": 0.0003879943209442366,
      "loss": 2.322,
      "step": 2840
    },
    {
      "epoch": 2.841,
      "grad_norm": 117.0413818359375,
      "learning_rate": 0.00038737259806651583,
      "loss": 2.3189,
      "step": 2841
    },
    {
      "epoch": 2.842,
      "grad_norm": 62.728885650634766,
      "learning_rate": 0.0003867512540346795,
      "loss": 2.3494,
      "step": 2842
    },
    {
      "epoch": 2.843,
      "grad_norm": 77.02944946289062,
      "learning_rate": 0.00038613028923296434,
      "loss": 2.3808,
      "step": 2843
    },
    {
      "epoch": 2.844,
      "grad_norm": 81.27517700195312,
      "learning_rate": 0.00038550970404537145,
      "loss": 2.3513,
      "step": 2844
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 3135.03759765625,
      "learning_rate": 0.0003848894988556685,
      "loss": 2.2553,
      "step": 2845
    },
    {
      "epoch": 2.846,
      "grad_norm": 411.2885437011719,
      "learning_rate": 0.000384269674047387,
      "loss": 2.2698,
      "step": 2846
    },
    {
      "epoch": 2.847,
      "grad_norm": 1043.125732421875,
      "learning_rate": 0.0003836502300038236,
      "loss": 2.3033,
      "step": 2847
    },
    {
      "epoch": 2.848,
      "grad_norm": 63.222774505615234,
      "learning_rate": 0.00038303116710803974,
      "loss": 2.2333,
      "step": 2848
    },
    {
      "epoch": 2.849,
      "grad_norm": 28.081504821777344,
      "learning_rate": 0.0003824124857428606,
      "loss": 2.3951,
      "step": 2849
    },
    {
      "epoch": 2.85,
      "grad_norm": 224.98471069335938,
      "learning_rate": 0.00038179418629087647,
      "loss": 2.292,
      "step": 2850
    },
    {
      "epoch": 2.851,
      "grad_norm": 804.302734375,
      "learning_rate": 0.00038117626913444015,
      "loss": 2.1908,
      "step": 2851
    },
    {
      "epoch": 2.852,
      "grad_norm": 14555.4716796875,
      "learning_rate": 0.0003805587346556693,
      "loss": 2.5258,
      "step": 2852
    },
    {
      "epoch": 2.8529999999999998,
      "grad_norm": 2723.244140625,
      "learning_rate": 0.000379941583236444,
      "loss": 2.282,
      "step": 2853
    },
    {
      "epoch": 2.854,
      "grad_norm": 136.8495330810547,
      "learning_rate": 0.00037932481525840777,
      "loss": 2.2291,
      "step": 2854
    },
    {
      "epoch": 2.855,
      "grad_norm": 353.6197814941406,
      "learning_rate": 0.0003787084311029666,
      "loss": 2.2828,
      "step": 2855
    },
    {
      "epoch": 2.856,
      "grad_norm": 85.02141571044922,
      "learning_rate": 0.00037809243115129035,
      "loss": 2.403,
      "step": 2856
    },
    {
      "epoch": 2.857,
      "grad_norm": 208.92196655273438,
      "learning_rate": 0.00037747681578431013,
      "loss": 2.1859,
      "step": 2857
    },
    {
      "epoch": 2.858,
      "grad_norm": 88.73731231689453,
      "learning_rate": 0.00037686158538271944,
      "loss": 2.2846,
      "step": 2858
    },
    {
      "epoch": 2.859,
      "grad_norm": 1302.44921875,
      "learning_rate": 0.00037624674032697393,
      "loss": 2.3991,
      "step": 2859
    },
    {
      "epoch": 2.86,
      "grad_norm": 994.6530151367188,
      "learning_rate": 0.0003756322809972905,
      "loss": 2.2877,
      "step": 2860
    },
    {
      "epoch": 2.8609999999999998,
      "grad_norm": 153.8837890625,
      "learning_rate": 0.00037501820777364857,
      "loss": 2.1811,
      "step": 2861
    },
    {
      "epoch": 2.862,
      "grad_norm": 2598.40625,
      "learning_rate": 0.0003744045210357875,
      "loss": 2.3296,
      "step": 2862
    },
    {
      "epoch": 2.863,
      "grad_norm": 484.2564697265625,
      "learning_rate": 0.00037379122116320884,
      "loss": 2.3488,
      "step": 2863
    },
    {
      "epoch": 2.864,
      "grad_norm": 49.9347038269043,
      "learning_rate": 0.0003731783085351741,
      "loss": 2.2273,
      "step": 2864
    },
    {
      "epoch": 2.865,
      "grad_norm": 318.9468078613281,
      "learning_rate": 0.0003725657835307055,
      "loss": 2.3921,
      "step": 2865
    },
    {
      "epoch": 2.866,
      "grad_norm": 26.71765899658203,
      "learning_rate": 0.00037195364652858545,
      "loss": 2.3737,
      "step": 2866
    },
    {
      "epoch": 2.867,
      "grad_norm": 1151.98291015625,
      "learning_rate": 0.0003713418979073565,
      "loss": 2.2786,
      "step": 2867
    },
    {
      "epoch": 2.868,
      "grad_norm": 201.17100524902344,
      "learning_rate": 0.0003707305380453213,
      "loss": 2.188,
      "step": 2868
    },
    {
      "epoch": 2.8689999999999998,
      "grad_norm": 76.95872497558594,
      "learning_rate": 0.0003701195673205422,
      "loss": 2.2756,
      "step": 2869
    },
    {
      "epoch": 2.87,
      "grad_norm": 98.45975494384766,
      "learning_rate": 0.0003695089861108403,
      "loss": 2.2473,
      "step": 2870
    },
    {
      "epoch": 2.871,
      "grad_norm": 138.19497680664062,
      "learning_rate": 0.0003688987947937962,
      "loss": 2.2019,
      "step": 2871
    },
    {
      "epoch": 2.872,
      "grad_norm": 184.606689453125,
      "learning_rate": 0.0003682889937467493,
      "loss": 2.1918,
      "step": 2872
    },
    {
      "epoch": 2.873,
      "grad_norm": 36.886077880859375,
      "learning_rate": 0.0003676795833467972,
      "loss": 2.3892,
      "step": 2873
    },
    {
      "epoch": 2.874,
      "grad_norm": 19.9386043548584,
      "learning_rate": 0.0003670705639707972,
      "loss": 2.3148,
      "step": 2874
    },
    {
      "epoch": 2.875,
      "grad_norm": 251.2616729736328,
      "learning_rate": 0.0003664619359953637,
      "loss": 2.1561,
      "step": 2875
    },
    {
      "epoch": 2.876,
      "grad_norm": 47.75984191894531,
      "learning_rate": 0.00036585369979686924,
      "loss": 2.3887,
      "step": 2876
    },
    {
      "epoch": 2.877,
      "grad_norm": 3363.921630859375,
      "learning_rate": 0.000365245855751444,
      "loss": 2.3675,
      "step": 2877
    },
    {
      "epoch": 2.878,
      "grad_norm": 428.92047119140625,
      "learning_rate": 0.0003646384042349764,
      "loss": 2.342,
      "step": 2878
    },
    {
      "epoch": 2.879,
      "grad_norm": 274.34564208984375,
      "learning_rate": 0.00036403134562311103,
      "loss": 2.5073,
      "step": 2879
    },
    {
      "epoch": 2.88,
      "grad_norm": 229.16290283203125,
      "learning_rate": 0.00036342468029125063,
      "loss": 2.2468,
      "step": 2880
    },
    {
      "epoch": 2.8810000000000002,
      "grad_norm": 1128.968505859375,
      "learning_rate": 0.00036281840861455397,
      "loss": 2.1343,
      "step": 2881
    },
    {
      "epoch": 2.882,
      "grad_norm": 95.54401397705078,
      "learning_rate": 0.0003622125309679364,
      "loss": 2.2582,
      "step": 2882
    },
    {
      "epoch": 2.883,
      "grad_norm": 82.22989654541016,
      "learning_rate": 0.00036160704772607,
      "loss": 2.2961,
      "step": 2883
    },
    {
      "epoch": 2.884,
      "grad_norm": 110.866455078125,
      "learning_rate": 0.00036100195926338255,
      "loss": 2.1969,
      "step": 2884
    },
    {
      "epoch": 2.885,
      "grad_norm": 287.2178649902344,
      "learning_rate": 0.00036039726595405754,
      "loss": 2.2767,
      "step": 2885
    },
    {
      "epoch": 2.886,
      "grad_norm": 200.03089904785156,
      "learning_rate": 0.000359792968172035,
      "loss": 2.2228,
      "step": 2886
    },
    {
      "epoch": 2.887,
      "grad_norm": 63.302207946777344,
      "learning_rate": 0.00035918906629101,
      "loss": 2.403,
      "step": 2887
    },
    {
      "epoch": 2.888,
      "grad_norm": 69.2291259765625,
      "learning_rate": 0.0003585855606844323,
      "loss": 2.2343,
      "step": 2888
    },
    {
      "epoch": 2.8890000000000002,
      "grad_norm": 65.76239013671875,
      "learning_rate": 0.0003579824517255068,
      "loss": 2.4243,
      "step": 2889
    },
    {
      "epoch": 2.89,
      "grad_norm": 244.17678833007812,
      "learning_rate": 0.0003573797397871934,
      "loss": 2.4119,
      "step": 2890
    },
    {
      "epoch": 2.891,
      "grad_norm": 3117.452392578125,
      "learning_rate": 0.0003567774252422059,
      "loss": 2.1963,
      "step": 2891
    },
    {
      "epoch": 2.892,
      "grad_norm": 54.438804626464844,
      "learning_rate": 0.00035617550846301347,
      "loss": 2.1956,
      "step": 2892
    },
    {
      "epoch": 2.893,
      "grad_norm": 77.52177429199219,
      "learning_rate": 0.0003555739898218382,
      "loss": 2.3629,
      "step": 2893
    },
    {
      "epoch": 2.894,
      "grad_norm": 90.33613586425781,
      "learning_rate": 0.00035497286969065646,
      "loss": 2.3294,
      "step": 2894
    },
    {
      "epoch": 2.895,
      "grad_norm": 188.95449829101562,
      "learning_rate": 0.0003543721484411976,
      "loss": 2.2949,
      "step": 2895
    },
    {
      "epoch": 2.896,
      "grad_norm": 58.136260986328125,
      "learning_rate": 0.00035377182644494564,
      "loss": 2.3486,
      "step": 2896
    },
    {
      "epoch": 2.8970000000000002,
      "grad_norm": 57.97993850708008,
      "learning_rate": 0.00035317190407313607,
      "loss": 2.2188,
      "step": 2897
    },
    {
      "epoch": 2.898,
      "grad_norm": 200.08799743652344,
      "learning_rate": 0.0003525723816967588,
      "loss": 2.293,
      "step": 2898
    },
    {
      "epoch": 2.899,
      "grad_norm": 93.2708511352539,
      "learning_rate": 0.00035197325968655514,
      "loss": 2.0468,
      "step": 2899
    },
    {
      "epoch": 2.9,
      "grad_norm": 47.85997009277344,
      "learning_rate": 0.00035137453841301946,
      "loss": 2.3459,
      "step": 2900
    },
    {
      "epoch": 2.9,
      "eval_loss": 2.2742867469787598,
      "eval_runtime": 221.5783,
      "eval_samples_per_second": 0.451,
      "eval_steps_per_second": 0.451,
      "step": 2900
    },
    {
      "epoch": 2.901,
      "grad_norm": 116.9361572265625,
      "learning_rate": 0.0003507762182463982,
      "loss": 2.2721,
      "step": 2901
    },
    {
      "epoch": 2.902,
      "grad_norm": 111.39865112304688,
      "learning_rate": 0.00035017829955668944,
      "loss": 2.1931,
      "step": 2902
    },
    {
      "epoch": 2.903,
      "grad_norm": 115.74037170410156,
      "learning_rate": 0.00034958078271364325,
      "loss": 2.3021,
      "step": 2903
    },
    {
      "epoch": 2.904,
      "grad_norm": 83.28931427001953,
      "learning_rate": 0.0003489836680867614,
      "loss": 2.3237,
      "step": 2904
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 41.189002990722656,
      "learning_rate": 0.00034838695604529714,
      "loss": 2.2516,
      "step": 2905
    },
    {
      "epoch": 2.906,
      "grad_norm": 55.90492630004883,
      "learning_rate": 0.000347790646958254,
      "loss": 2.2045,
      "step": 2906
    },
    {
      "epoch": 2.907,
      "grad_norm": 88.84587860107422,
      "learning_rate": 0.0003471947411943867,
      "loss": 2.5126,
      "step": 2907
    },
    {
      "epoch": 2.908,
      "grad_norm": 95.39675903320312,
      "learning_rate": 0.0003465992391222006,
      "loss": 2.199,
      "step": 2908
    },
    {
      "epoch": 2.909,
      "grad_norm": 235.5797576904297,
      "learning_rate": 0.00034600414110995083,
      "loss": 2.4531,
      "step": 2909
    },
    {
      "epoch": 2.91,
      "grad_norm": 162.09413146972656,
      "learning_rate": 0.00034540944752564406,
      "loss": 2.4026,
      "step": 2910
    },
    {
      "epoch": 2.911,
      "grad_norm": 15.291885375976562,
      "learning_rate": 0.0003448151587370356,
      "loss": 2.1592,
      "step": 2911
    },
    {
      "epoch": 2.912,
      "grad_norm": 202.4222869873047,
      "learning_rate": 0.0003442212751116305,
      "loss": 2.1955,
      "step": 2912
    },
    {
      "epoch": 2.9130000000000003,
      "grad_norm": 2176.24462890625,
      "learning_rate": 0.0003436277970166841,
      "loss": 2.0693,
      "step": 2913
    },
    {
      "epoch": 2.914,
      "grad_norm": 29.37827491760254,
      "learning_rate": 0.0003430347248192004,
      "loss": 2.3609,
      "step": 2914
    },
    {
      "epoch": 2.915,
      "grad_norm": 190.04637145996094,
      "learning_rate": 0.0003424420588859318,
      "loss": 2.2811,
      "step": 2915
    },
    {
      "epoch": 2.916,
      "grad_norm": 34.940940856933594,
      "learning_rate": 0.00034184979958338103,
      "loss": 2.4457,
      "step": 2916
    },
    {
      "epoch": 2.917,
      "grad_norm": 15.235306739807129,
      "learning_rate": 0.0003412579472777979,
      "loss": 2.1736,
      "step": 2917
    },
    {
      "epoch": 2.918,
      "grad_norm": 710.7598266601562,
      "learning_rate": 0.00034066650233518126,
      "loss": 2.3293,
      "step": 2918
    },
    {
      "epoch": 2.919,
      "grad_norm": 68.69015502929688,
      "learning_rate": 0.0003400754651212776,
      "loss": 2.1693,
      "step": 2919
    },
    {
      "epoch": 2.92,
      "grad_norm": 49.28615951538086,
      "learning_rate": 0.0003394848360015815,
      "loss": 2.2614,
      "step": 2920
    },
    {
      "epoch": 2.9210000000000003,
      "grad_norm": 43.60612106323242,
      "learning_rate": 0.0003388946153413357,
      "loss": 2.2882,
      "step": 2921
    },
    {
      "epoch": 2.922,
      "grad_norm": 155.3382568359375,
      "learning_rate": 0.0003383048035055293,
      "loss": 2.249,
      "step": 2922
    },
    {
      "epoch": 2.923,
      "grad_norm": 28.334657669067383,
      "learning_rate": 0.0003377154008588997,
      "loss": 2.3147,
      "step": 2923
    },
    {
      "epoch": 2.924,
      "grad_norm": 14.442163467407227,
      "learning_rate": 0.0003371264077659304,
      "loss": 2.2648,
      "step": 2924
    },
    {
      "epoch": 2.925,
      "grad_norm": 202.52688598632812,
      "learning_rate": 0.000336537824590852,
      "loss": 2.1672,
      "step": 2925
    },
    {
      "epoch": 2.926,
      "grad_norm": 345.4459228515625,
      "learning_rate": 0.0003359496516976415,
      "loss": 2.4193,
      "step": 2926
    },
    {
      "epoch": 2.927,
      "grad_norm": 218.76834106445312,
      "learning_rate": 0.0003353618894500219,
      "loss": 2.3064,
      "step": 2927
    },
    {
      "epoch": 2.928,
      "grad_norm": 27.573463439941406,
      "learning_rate": 0.00033477453821146333,
      "loss": 2.1762,
      "step": 2928
    },
    {
      "epoch": 2.9290000000000003,
      "grad_norm": 300.3991394042969,
      "learning_rate": 0.00033418759834518054,
      "loss": 2.3294,
      "step": 2929
    },
    {
      "epoch": 2.93,
      "grad_norm": 33.25945281982422,
      "learning_rate": 0.0003336010702141341,
      "loss": 2.4067,
      "step": 2930
    },
    {
      "epoch": 2.931,
      "grad_norm": 51.922054290771484,
      "learning_rate": 0.00033301495418103103,
      "loss": 2.1445,
      "step": 2931
    },
    {
      "epoch": 2.932,
      "grad_norm": 940.12060546875,
      "learning_rate": 0.0003324292506083221,
      "loss": 2.3352,
      "step": 2932
    },
    {
      "epoch": 2.933,
      "grad_norm": 199.8150634765625,
      "learning_rate": 0.00033184395985820346,
      "loss": 2.267,
      "step": 2933
    },
    {
      "epoch": 2.934,
      "grad_norm": 61.633968353271484,
      "learning_rate": 0.00033125908229261693,
      "loss": 2.1561,
      "step": 2934
    },
    {
      "epoch": 2.935,
      "grad_norm": 12.179117202758789,
      "learning_rate": 0.0003306746182732475,
      "loss": 2.2336,
      "step": 2935
    },
    {
      "epoch": 2.936,
      "grad_norm": 33.253028869628906,
      "learning_rate": 0.00033009056816152494,
      "loss": 2.2358,
      "step": 2936
    },
    {
      "epoch": 2.9370000000000003,
      "grad_norm": 9.937772750854492,
      "learning_rate": 0.00032950693231862315,
      "loss": 2.36,
      "step": 2937
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 24.17580223083496,
      "learning_rate": 0.00032892371110545946,
      "loss": 2.3906,
      "step": 2938
    },
    {
      "epoch": 2.939,
      "grad_norm": 505.6628723144531,
      "learning_rate": 0.0003283409048826955,
      "loss": 2.1806,
      "step": 2939
    },
    {
      "epoch": 2.94,
      "grad_norm": 31.493898391723633,
      "learning_rate": 0.000327758514010736,
      "loss": 2.2484,
      "step": 2940
    },
    {
      "epoch": 2.941,
      "grad_norm": 19.521379470825195,
      "learning_rate": 0.00032717653884972855,
      "loss": 2.2413,
      "step": 2941
    },
    {
      "epoch": 2.942,
      "grad_norm": 27.100170135498047,
      "learning_rate": 0.0003265949797595638,
      "loss": 2.2554,
      "step": 2942
    },
    {
      "epoch": 2.943,
      "grad_norm": 48.91145706176758,
      "learning_rate": 0.0003260138370998751,
      "loss": 2.201,
      "step": 2943
    },
    {
      "epoch": 2.944,
      "grad_norm": 522.4818725585938,
      "learning_rate": 0.0003254331112300384,
      "loss": 2.1952,
      "step": 2944
    },
    {
      "epoch": 2.945,
      "grad_norm": 87.58114624023438,
      "learning_rate": 0.00032485280250917135,
      "loss": 2.3717,
      "step": 2945
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 177.25216674804688,
      "learning_rate": 0.00032427291129613503,
      "loss": 2.3508,
      "step": 2946
    },
    {
      "epoch": 2.947,
      "grad_norm": 53.841854095458984,
      "learning_rate": 0.00032369343794953043,
      "loss": 2.2075,
      "step": 2947
    },
    {
      "epoch": 2.948,
      "grad_norm": 17.91213035583496,
      "learning_rate": 0.0003231143828277021,
      "loss": 2.308,
      "step": 2948
    },
    {
      "epoch": 2.949,
      "grad_norm": 532.0741577148438,
      "learning_rate": 0.0003225357462887344,
      "loss": 2.1674,
      "step": 2949
    },
    {
      "epoch": 2.95,
      "grad_norm": 109.68450164794922,
      "learning_rate": 0.0003219575286904536,
      "loss": 2.2529,
      "step": 2950
    },
    {
      "epoch": 2.951,
      "grad_norm": 55.98528289794922,
      "learning_rate": 0.00032137973039042636,
      "loss": 2.2992,
      "step": 2951
    },
    {
      "epoch": 2.952,
      "grad_norm": 76.97501373291016,
      "learning_rate": 0.00032080235174596094,
      "loss": 2.5395,
      "step": 2952
    },
    {
      "epoch": 2.953,
      "grad_norm": 46.54322814941406,
      "learning_rate": 0.0003202253931141054,
      "loss": 2.1275,
      "step": 2953
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 20.291173934936523,
      "learning_rate": 0.0003196488548516482,
      "loss": 2.1412,
      "step": 2954
    },
    {
      "epoch": 2.955,
      "grad_norm": 10.142783164978027,
      "learning_rate": 0.00031907273731511776,
      "loss": 2.1138,
      "step": 2955
    },
    {
      "epoch": 2.956,
      "grad_norm": 693.90185546875,
      "learning_rate": 0.00031849704086078226,
      "loss": 2.1522,
      "step": 2956
    },
    {
      "epoch": 2.957,
      "grad_norm": 91.37716674804688,
      "learning_rate": 0.00031792176584464983,
      "loss": 2.1032,
      "step": 2957
    },
    {
      "epoch": 2.958,
      "grad_norm": 10.453752517700195,
      "learning_rate": 0.0003173469126224682,
      "loss": 2.2822,
      "step": 2958
    },
    {
      "epoch": 2.959,
      "grad_norm": 9.095865249633789,
      "learning_rate": 0.0003167724815497237,
      "loss": 2.2247,
      "step": 2959
    },
    {
      "epoch": 2.96,
      "grad_norm": 342.07794189453125,
      "learning_rate": 0.0003161984729816415,
      "loss": 2.1073,
      "step": 2960
    },
    {
      "epoch": 2.961,
      "grad_norm": 155.63250732421875,
      "learning_rate": 0.00031562488727318605,
      "loss": 2.4002,
      "step": 2961
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 75.17495727539062,
      "learning_rate": 0.00031505172477905996,
      "loss": 2.2288,
      "step": 2962
    },
    {
      "epoch": 2.963,
      "grad_norm": 92.15203094482422,
      "learning_rate": 0.00031447898585370383,
      "loss": 2.1854,
      "step": 2963
    },
    {
      "epoch": 2.964,
      "grad_norm": 8.049671173095703,
      "learning_rate": 0.00031390667085129734,
      "loss": 2.1805,
      "step": 2964
    },
    {
      "epoch": 2.965,
      "grad_norm": 33.97770309448242,
      "learning_rate": 0.00031333478012575677,
      "loss": 2.211,
      "step": 2965
    },
    {
      "epoch": 2.966,
      "grad_norm": 40.818702697753906,
      "learning_rate": 0.0003127633140307373,
      "loss": 2.0568,
      "step": 2966
    },
    {
      "epoch": 2.967,
      "grad_norm": 23.736873626708984,
      "learning_rate": 0.0003121922729196305,
      "loss": 2.2217,
      "step": 2967
    },
    {
      "epoch": 2.968,
      "grad_norm": 33.66810989379883,
      "learning_rate": 0.00031162165714556544,
      "loss": 2.0982,
      "step": 2968
    },
    {
      "epoch": 2.969,
      "grad_norm": 106.42457580566406,
      "learning_rate": 0.00031105146706140796,
      "loss": 2.1381,
      "step": 2969
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 119.19579315185547,
      "learning_rate": 0.00031048170301976153,
      "loss": 2.6072,
      "step": 2970
    },
    {
      "epoch": 2.971,
      "grad_norm": 55.35552215576172,
      "learning_rate": 0.0003099123653729653,
      "loss": 2.2823,
      "step": 2971
    },
    {
      "epoch": 2.972,
      "grad_norm": 189.0391387939453,
      "learning_rate": 0.00030934345447309485,
      "loss": 2.3178,
      "step": 2972
    },
    {
      "epoch": 2.973,
      "grad_norm": 32.91483688354492,
      "learning_rate": 0.0003087749706719617,
      "loss": 2.2384,
      "step": 2973
    },
    {
      "epoch": 2.974,
      "grad_norm": 44.492637634277344,
      "learning_rate": 0.00030820691432111426,
      "loss": 2.1992,
      "step": 2974
    },
    {
      "epoch": 2.975,
      "grad_norm": 25.438106536865234,
      "learning_rate": 0.0003076392857718351,
      "loss": 2.3094,
      "step": 2975
    },
    {
      "epoch": 2.976,
      "grad_norm": 126.07438659667969,
      "learning_rate": 0.0003070720853751437,
      "loss": 2.5452,
      "step": 2976
    },
    {
      "epoch": 2.977,
      "grad_norm": 134.79830932617188,
      "learning_rate": 0.0003065053134817939,
      "loss": 2.2261,
      "step": 2977
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 21.958316802978516,
      "learning_rate": 0.0003059389704422747,
      "loss": 2.0802,
      "step": 2978
    },
    {
      "epoch": 2.979,
      "grad_norm": 115.87623596191406,
      "learning_rate": 0.0003053730566068099,
      "loss": 2.181,
      "step": 2979
    },
    {
      "epoch": 2.98,
      "grad_norm": 15.395658493041992,
      "learning_rate": 0.0003048075723253577,
      "loss": 2.1193,
      "step": 2980
    },
    {
      "epoch": 2.981,
      "grad_norm": 31.219717025756836,
      "learning_rate": 0.0003042425179476115,
      "loss": 2.2697,
      "step": 2981
    },
    {
      "epoch": 2.982,
      "grad_norm": 1565.6529541015625,
      "learning_rate": 0.0003036778938229976,
      "loss": 2.2081,
      "step": 2982
    },
    {
      "epoch": 2.983,
      "grad_norm": 27.557601928710938,
      "learning_rate": 0.0003031137003006775,
      "loss": 2.103,
      "step": 2983
    },
    {
      "epoch": 2.984,
      "grad_norm": 10259.4619140625,
      "learning_rate": 0.00030254993772954554,
      "loss": 2.3506,
      "step": 2984
    },
    {
      "epoch": 2.985,
      "grad_norm": 57.5016975402832,
      "learning_rate": 0.00030198660645822985,
      "loss": 2.2189,
      "step": 2985
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 7.678838729858398,
      "learning_rate": 0.0003014237068350917,
      "loss": 2.105,
      "step": 2986
    },
    {
      "epoch": 2.987,
      "grad_norm": 55.526859283447266,
      "learning_rate": 0.00030086123920822525,
      "loss": 2.3113,
      "step": 2987
    },
    {
      "epoch": 2.988,
      "grad_norm": 31.44150161743164,
      "learning_rate": 0.0003002992039254586,
      "loss": 2.2141,
      "step": 2988
    },
    {
      "epoch": 2.989,
      "grad_norm": 103.41869354248047,
      "learning_rate": 0.00029973760133435135,
      "loss": 2.0362,
      "step": 2989
    },
    {
      "epoch": 2.99,
      "grad_norm": 18.5892391204834,
      "learning_rate": 0.0002991764317821958,
      "loss": 2.1164,
      "step": 2990
    },
    {
      "epoch": 2.991,
      "grad_norm": 18.207670211791992,
      "learning_rate": 0.0002986156956160162,
      "loss": 2.1904,
      "step": 2991
    },
    {
      "epoch": 2.992,
      "grad_norm": 30.264623641967773,
      "learning_rate": 0.0002980553931825698,
      "loss": 2.1576,
      "step": 2992
    },
    {
      "epoch": 2.993,
      "grad_norm": 33.411861419677734,
      "learning_rate": 0.0002974955248283445,
      "loss": 2.1944,
      "step": 2993
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 45.895355224609375,
      "learning_rate": 0.00029693609089956064,
      "loss": 2.1125,
      "step": 2994
    },
    {
      "epoch": 2.995,
      "grad_norm": 10.773179054260254,
      "learning_rate": 0.00029637709174216943,
      "loss": 2.0582,
      "step": 2995
    },
    {
      "epoch": 2.996,
      "grad_norm": 39.67872619628906,
      "learning_rate": 0.000295818527701853,
      "loss": 2.2099,
      "step": 2996
    },
    {
      "epoch": 2.997,
      "grad_norm": 51.428524017333984,
      "learning_rate": 0.00029526039912402505,
      "loss": 2.1946,
      "step": 2997
    },
    {
      "epoch": 2.998,
      "grad_norm": 82.86080932617188,
      "learning_rate": 0.000294702706353829,
      "loss": 2.3241,
      "step": 2998
    },
    {
      "epoch": 2.999,
      "grad_norm": 11.231776237487793,
      "learning_rate": 0.0002941454497361403,
      "loss": 2.2086,
      "step": 2999
    },
    {
      "epoch": 3.0,
      "grad_norm": 23.51589012145996,
      "learning_rate": 0.0002935886296155631,
      "loss": 2.1337,
      "step": 3000
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.179903268814087,
      "eval_runtime": 225.062,
      "eval_samples_per_second": 0.444,
      "eval_steps_per_second": 0.444,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 4000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.973680513151107e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
