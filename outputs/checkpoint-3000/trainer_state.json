{
  "best_metric": 1.337794542312622,
  "best_model_checkpoint": "outputs/checkpoint-1500",
  "epoch": 3.0,
  "eval_steps": 100,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 0.10689288377761841,
      "learning_rate": 0.0004,
      "loss": 0.4097,
      "step": 1
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.084578737616539,
      "learning_rate": 0.0008,
      "loss": 0.3448,
      "step": 2
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.11432426422834396,
      "learning_rate": 0.0012,
      "loss": 0.4483,
      "step": 3
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.97259521484375,
      "learning_rate": 0.0016,
      "loss": 0.4519,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.9730566143989563,
      "learning_rate": 0.002,
      "loss": 0.4221,
      "step": 5
    },
    {
      "epoch": 0.012,
      "grad_norm": 1.8814690113067627,
      "learning_rate": 0.0019999977920603196,
      "loss": 0.6301,
      "step": 6
    },
    {
      "epoch": 0.014,
      "grad_norm": 58.9294319152832,
      "learning_rate": 0.0019999911682510277,
      "loss": 9.8013,
      "step": 7
    },
    {
      "epoch": 0.016,
      "grad_norm": 8.579010963439941,
      "learning_rate": 0.001999980128601375,
      "loss": 1.2972,
      "step": 8
    },
    {
      "epoch": 0.018,
      "grad_norm": 7.820326328277588,
      "learning_rate": 0.00199996467316011,
      "loss": 0.8264,
      "step": 9
    },
    {
      "epoch": 0.02,
      "grad_norm": 14.161247253417969,
      "learning_rate": 0.001999944801995484,
      "loss": 1.6032,
      "step": 10
    },
    {
      "epoch": 0.022,
      "grad_norm": 30.037368774414062,
      "learning_rate": 0.0019999205151952438,
      "loss": 2.1843,
      "step": 11
    },
    {
      "epoch": 0.024,
      "grad_norm": 8.311636924743652,
      "learning_rate": 0.001999891812866638,
      "loss": 1.5917,
      "step": 12
    },
    {
      "epoch": 0.026,
      "grad_norm": 8.509608268737793,
      "learning_rate": 0.0019998586951364126,
      "loss": 1.1972,
      "step": 13
    },
    {
      "epoch": 0.028,
      "grad_norm": 4.354803085327148,
      "learning_rate": 0.0019998211621508107,
      "loss": 0.7045,
      "step": 14
    },
    {
      "epoch": 0.03,
      "grad_norm": 5.143829345703125,
      "learning_rate": 0.0019997792140755742,
      "loss": 0.7788,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 7.363183498382568,
      "learning_rate": 0.0019997328510959413,
      "loss": 0.8836,
      "step": 16
    },
    {
      "epoch": 0.034,
      "grad_norm": 46.73540496826172,
      "learning_rate": 0.001999682073416644,
      "loss": 4.8299,
      "step": 17
    },
    {
      "epoch": 0.036,
      "grad_norm": 15.875617027282715,
      "learning_rate": 0.0019996268812619107,
      "loss": 3.2444,
      "step": 18
    },
    {
      "epoch": 0.038,
      "grad_norm": 8.972566604614258,
      "learning_rate": 0.001999567274875464,
      "loss": 2.1526,
      "step": 19
    },
    {
      "epoch": 0.04,
      "grad_norm": 10.888273239135742,
      "learning_rate": 0.0019995032545205176,
      "loss": 1.068,
      "step": 20
    },
    {
      "epoch": 0.042,
      "grad_norm": 18.81281852722168,
      "learning_rate": 0.0019994348204797788,
      "loss": 0.8374,
      "step": 21
    },
    {
      "epoch": 0.044,
      "grad_norm": 4.79962158203125,
      "learning_rate": 0.001999361973055443,
      "loss": 0.821,
      "step": 22
    },
    {
      "epoch": 0.046,
      "grad_norm": 13.699286460876465,
      "learning_rate": 0.001999284712569196,
      "loss": 1.0775,
      "step": 23
    },
    {
      "epoch": 0.048,
      "grad_norm": 61.513160705566406,
      "learning_rate": 0.0019992030393622107,
      "loss": 2.0139,
      "step": 24
    },
    {
      "epoch": 0.05,
      "grad_norm": 12.2156400680542,
      "learning_rate": 0.0019991169537951466,
      "loss": 1.3322,
      "step": 25
    },
    {
      "epoch": 0.052,
      "grad_norm": 23.329771041870117,
      "learning_rate": 0.001999026456248147,
      "loss": 1.6115,
      "step": 26
    },
    {
      "epoch": 0.054,
      "grad_norm": 12.838418006896973,
      "learning_rate": 0.001998931547120838,
      "loss": 1.3898,
      "step": 27
    },
    {
      "epoch": 0.056,
      "grad_norm": 7.564198017120361,
      "learning_rate": 0.0019988322268323267,
      "loss": 0.8453,
      "step": 28
    },
    {
      "epoch": 0.058,
      "grad_norm": 2.7777209281921387,
      "learning_rate": 0.0019987284958211996,
      "loss": 0.6977,
      "step": 29
    },
    {
      "epoch": 0.06,
      "grad_norm": 9.555514335632324,
      "learning_rate": 0.0019986203545455205,
      "loss": 0.9694,
      "step": 30
    },
    {
      "epoch": 0.062,
      "grad_norm": 5.669062614440918,
      "learning_rate": 0.001998507803482828,
      "loss": 0.7723,
      "step": 31
    },
    {
      "epoch": 0.064,
      "grad_norm": 25.26238441467285,
      "learning_rate": 0.0019983908431301343,
      "loss": 1.5027,
      "step": 32
    },
    {
      "epoch": 0.066,
      "grad_norm": 13.2509126663208,
      "learning_rate": 0.001998269474003922,
      "loss": 0.9809,
      "step": 33
    },
    {
      "epoch": 0.068,
      "grad_norm": 6.424870014190674,
      "learning_rate": 0.0019981436966401427,
      "loss": 1.1792,
      "step": 34
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.950324296951294,
      "learning_rate": 0.0019980135115942135,
      "loss": 0.6505,
      "step": 35
    },
    {
      "epoch": 0.072,
      "grad_norm": 9.499361038208008,
      "learning_rate": 0.0019978789194410166,
      "loss": 0.7031,
      "step": 36
    },
    {
      "epoch": 0.074,
      "grad_norm": 10.464120864868164,
      "learning_rate": 0.0019977399207748944,
      "loss": 0.703,
      "step": 37
    },
    {
      "epoch": 0.076,
      "grad_norm": 30.07783317565918,
      "learning_rate": 0.0019975965162096483,
      "loss": 1.1381,
      "step": 38
    },
    {
      "epoch": 0.078,
      "grad_norm": 2.327071189880371,
      "learning_rate": 0.0019974487063785353,
      "loss": 0.6775,
      "step": 39
    },
    {
      "epoch": 0.08,
      "grad_norm": 6.3857927322387695,
      "learning_rate": 0.0019972964919342663,
      "loss": 1.1466,
      "step": 40
    },
    {
      "epoch": 0.082,
      "grad_norm": 5.035772800445557,
      "learning_rate": 0.0019971398735490016,
      "loss": 0.9265,
      "step": 41
    },
    {
      "epoch": 0.084,
      "grad_norm": 14.275304794311523,
      "learning_rate": 0.001996978851914349,
      "loss": 0.7842,
      "step": 42
    },
    {
      "epoch": 0.086,
      "grad_norm": 16.341625213623047,
      "learning_rate": 0.0019968134277413606,
      "loss": 1.1048,
      "step": 43
    },
    {
      "epoch": 0.088,
      "grad_norm": 22.216005325317383,
      "learning_rate": 0.0019966436017605296,
      "loss": 1.0039,
      "step": 44
    },
    {
      "epoch": 0.09,
      "grad_norm": 45.441246032714844,
      "learning_rate": 0.0019964693747217873,
      "loss": 1.2813,
      "step": 45
    },
    {
      "epoch": 0.092,
      "grad_norm": 10.186388969421387,
      "learning_rate": 0.0019962907473944995,
      "loss": 1.1187,
      "step": 46
    },
    {
      "epoch": 0.094,
      "grad_norm": 96.56367492675781,
      "learning_rate": 0.001996107720567462,
      "loss": 12.869,
      "step": 47
    },
    {
      "epoch": 0.096,
      "grad_norm": 52.198055267333984,
      "learning_rate": 0.0019959202950489,
      "loss": 15.0148,
      "step": 48
    },
    {
      "epoch": 0.098,
      "grad_norm": 717.4691162109375,
      "learning_rate": 0.0019957284716664615,
      "loss": 10.8338,
      "step": 49
    },
    {
      "epoch": 0.1,
      "grad_norm": 77.10630798339844,
      "learning_rate": 0.001995532251267216,
      "loss": 19.4512,
      "step": 50
    },
    {
      "epoch": 0.102,
      "grad_norm": 156.91049194335938,
      "learning_rate": 0.0019953316347176486,
      "loss": 30.6445,
      "step": 51
    },
    {
      "epoch": 0.104,
      "grad_norm": 33.72825241088867,
      "learning_rate": 0.001995126622903658,
      "loss": 10.2449,
      "step": 52
    },
    {
      "epoch": 0.106,
      "grad_norm": 105.81783294677734,
      "learning_rate": 0.0019949172167305516,
      "loss": 17.5685,
      "step": 53
    },
    {
      "epoch": 0.108,
      "grad_norm": 25.370473861694336,
      "learning_rate": 0.001994703417123042,
      "loss": 9.3568,
      "step": 54
    },
    {
      "epoch": 0.11,
      "grad_norm": 41.368839263916016,
      "learning_rate": 0.0019944852250252418,
      "loss": 17.6755,
      "step": 55
    },
    {
      "epoch": 0.112,
      "grad_norm": 29.67848014831543,
      "learning_rate": 0.0019942626414006614,
      "loss": 15.9476,
      "step": 56
    },
    {
      "epoch": 0.114,
      "grad_norm": 28.006908416748047,
      "learning_rate": 0.0019940356672322034,
      "loss": 8.7011,
      "step": 57
    },
    {
      "epoch": 0.116,
      "grad_norm": 31.808700561523438,
      "learning_rate": 0.0019938043035221584,
      "loss": 7.725,
      "step": 58
    },
    {
      "epoch": 0.118,
      "grad_norm": 25.565980911254883,
      "learning_rate": 0.0019935685512922005,
      "loss": 11.1486,
      "step": 59
    },
    {
      "epoch": 0.12,
      "grad_norm": 19.46499252319336,
      "learning_rate": 0.0019933284115833828,
      "loss": 8.1667,
      "step": 60
    },
    {
      "epoch": 0.122,
      "grad_norm": 27.20265769958496,
      "learning_rate": 0.001993083885456134,
      "loss": 9.1384,
      "step": 61
    },
    {
      "epoch": 0.124,
      "grad_norm": 19.032459259033203,
      "learning_rate": 0.001992834973990251,
      "loss": 7.3177,
      "step": 62
    },
    {
      "epoch": 0.126,
      "grad_norm": 19.91191864013672,
      "learning_rate": 0.0019925816782848976,
      "loss": 7.9379,
      "step": 63
    },
    {
      "epoch": 0.128,
      "grad_norm": 14.774715423583984,
      "learning_rate": 0.0019923239994585965,
      "loss": 7.4823,
      "step": 64
    },
    {
      "epoch": 0.13,
      "grad_norm": 15.784680366516113,
      "learning_rate": 0.0019920619386492268,
      "loss": 5.7869,
      "step": 65
    },
    {
      "epoch": 0.132,
      "grad_norm": 26.500076293945312,
      "learning_rate": 0.0019917954970140174,
      "loss": 6.8697,
      "step": 66
    },
    {
      "epoch": 0.134,
      "grad_norm": 10.829269409179688,
      "learning_rate": 0.001991524675729542,
      "loss": 6.2622,
      "step": 67
    },
    {
      "epoch": 0.136,
      "grad_norm": 7.936374664306641,
      "learning_rate": 0.001991249475991715,
      "loss": 5.5851,
      "step": 68
    },
    {
      "epoch": 0.138,
      "grad_norm": 18.465625762939453,
      "learning_rate": 0.001990969899015785,
      "loss": 5.9923,
      "step": 69
    },
    {
      "epoch": 0.14,
      "grad_norm": 15.627571105957031,
      "learning_rate": 0.0019906859460363307,
      "loss": 5.1435,
      "step": 70
    },
    {
      "epoch": 0.142,
      "grad_norm": 17.541229248046875,
      "learning_rate": 0.001990397618307254,
      "loss": 5.3498,
      "step": 71
    },
    {
      "epoch": 0.144,
      "grad_norm": 16.8480167388916,
      "learning_rate": 0.001990104917101775,
      "loss": 5.3807,
      "step": 72
    },
    {
      "epoch": 0.146,
      "grad_norm": 8.606659889221191,
      "learning_rate": 0.0019898078437124273,
      "loss": 4.6418,
      "step": 73
    },
    {
      "epoch": 0.148,
      "grad_norm": 21.818483352661133,
      "learning_rate": 0.001989506399451051,
      "loss": 4.9773,
      "step": 74
    },
    {
      "epoch": 0.15,
      "grad_norm": 10.904372215270996,
      "learning_rate": 0.0019892005856487877,
      "loss": 4.4199,
      "step": 75
    },
    {
      "epoch": 0.152,
      "grad_norm": 8.838079452514648,
      "learning_rate": 0.0019888904036560744,
      "loss": 4.2805,
      "step": 76
    },
    {
      "epoch": 0.154,
      "grad_norm": 8.869036674499512,
      "learning_rate": 0.0019885758548426366,
      "loss": 4.2187,
      "step": 77
    },
    {
      "epoch": 0.156,
      "grad_norm": 6.551239967346191,
      "learning_rate": 0.001988256940597485,
      "loss": 4.1691,
      "step": 78
    },
    {
      "epoch": 0.158,
      "grad_norm": 7.6529669761657715,
      "learning_rate": 0.0019879336623289056,
      "loss": 4.086,
      "step": 79
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.090555667877197,
      "learning_rate": 0.0019876060214644568,
      "loss": 4.1824,
      "step": 80
    },
    {
      "epoch": 0.162,
      "grad_norm": 15.530783653259277,
      "learning_rate": 0.0019872740194509606,
      "loss": 4.5367,
      "step": 81
    },
    {
      "epoch": 0.164,
      "grad_norm": 14.215782165527344,
      "learning_rate": 0.0019869376577544983,
      "loss": 4.4915,
      "step": 82
    },
    {
      "epoch": 0.166,
      "grad_norm": 6.154689311981201,
      "learning_rate": 0.001986596937860402,
      "loss": 3.9938,
      "step": 83
    },
    {
      "epoch": 0.168,
      "grad_norm": 4.92926025390625,
      "learning_rate": 0.0019862518612732503,
      "loss": 4.1385,
      "step": 84
    },
    {
      "epoch": 0.17,
      "grad_norm": 6.206085205078125,
      "learning_rate": 0.0019859024295168595,
      "loss": 4.1449,
      "step": 85
    },
    {
      "epoch": 0.172,
      "grad_norm": 7.200077533721924,
      "learning_rate": 0.001985548644134278,
      "loss": 4.1029,
      "step": 86
    },
    {
      "epoch": 0.174,
      "grad_norm": 10.180184364318848,
      "learning_rate": 0.0019851905066877794,
      "loss": 4.1244,
      "step": 87
    },
    {
      "epoch": 0.176,
      "grad_norm": 3.760026216506958,
      "learning_rate": 0.0019848280187588557,
      "loss": 4.0017,
      "step": 88
    },
    {
      "epoch": 0.178,
      "grad_norm": 6.265959739685059,
      "learning_rate": 0.0019844611819482094,
      "loss": 3.8437,
      "step": 89
    },
    {
      "epoch": 0.18,
      "grad_norm": 13.777705192565918,
      "learning_rate": 0.0019840899978757483,
      "loss": 4.2669,
      "step": 90
    },
    {
      "epoch": 0.182,
      "grad_norm": 10.786234855651855,
      "learning_rate": 0.0019837144681805756,
      "loss": 4.3479,
      "step": 91
    },
    {
      "epoch": 0.184,
      "grad_norm": 2.257171869277954,
      "learning_rate": 0.0019833345945209856,
      "loss": 3.9824,
      "step": 92
    },
    {
      "epoch": 0.186,
      "grad_norm": 9.063436508178711,
      "learning_rate": 0.001982950378574455,
      "loss": 4.1283,
      "step": 93
    },
    {
      "epoch": 0.188,
      "grad_norm": 7.847458839416504,
      "learning_rate": 0.0019825618220376344,
      "loss": 4.1823,
      "step": 94
    },
    {
      "epoch": 0.19,
      "grad_norm": 5.439370155334473,
      "learning_rate": 0.0019821689266263424,
      "loss": 4.1364,
      "step": 95
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.997363567352295,
      "learning_rate": 0.0019817716940755585,
      "loss": 3.9247,
      "step": 96
    },
    {
      "epoch": 0.194,
      "grad_norm": 15.107873916625977,
      "learning_rate": 0.0019813701261394137,
      "loss": 4.2997,
      "step": 97
    },
    {
      "epoch": 0.196,
      "grad_norm": 11.800546646118164,
      "learning_rate": 0.001980964224591183,
      "loss": 4.0786,
      "step": 98
    },
    {
      "epoch": 0.198,
      "grad_norm": 2.9633309841156006,
      "learning_rate": 0.0019805539912232783,
      "loss": 4.0079,
      "step": 99
    },
    {
      "epoch": 0.2,
      "grad_norm": 9.26484489440918,
      "learning_rate": 0.0019801394278472417,
      "loss": 4.2895,
      "step": 100
    },
    {
      "epoch": 0.2,
      "eval_loss": 4.087549209594727,
      "eval_runtime": 212.8216,
      "eval_samples_per_second": 0.47,
      "eval_steps_per_second": 0.47,
      "step": 100
    },
    {
      "epoch": 0.202,
      "grad_norm": 5.097751140594482,
      "learning_rate": 0.0019797205362937346,
      "loss": 4.0492,
      "step": 101
    },
    {
      "epoch": 0.204,
      "grad_norm": 6.0146164894104,
      "learning_rate": 0.0019792973184125317,
      "loss": 4.2848,
      "step": 102
    },
    {
      "epoch": 0.206,
      "grad_norm": 3.73445200920105,
      "learning_rate": 0.001978869776072512,
      "loss": 4.0175,
      "step": 103
    },
    {
      "epoch": 0.208,
      "grad_norm": 8.104863166809082,
      "learning_rate": 0.0019784379111616505,
      "loss": 4.2369,
      "step": 104
    },
    {
      "epoch": 0.21,
      "grad_norm": 5.603906631469727,
      "learning_rate": 0.001978001725587011,
      "loss": 4.1577,
      "step": 105
    },
    {
      "epoch": 0.212,
      "grad_norm": 8.156097412109375,
      "learning_rate": 0.001977561221274737,
      "loss": 4.2612,
      "step": 106
    },
    {
      "epoch": 0.214,
      "grad_norm": 7.257375240325928,
      "learning_rate": 0.001977116400170041,
      "loss": 4.1095,
      "step": 107
    },
    {
      "epoch": 0.216,
      "grad_norm": 3.9450461864471436,
      "learning_rate": 0.0019766672642372,
      "loss": 3.9466,
      "step": 108
    },
    {
      "epoch": 0.218,
      "grad_norm": 7.384090423583984,
      "learning_rate": 0.0019762138154595446,
      "loss": 4.1546,
      "step": 109
    },
    {
      "epoch": 0.22,
      "grad_norm": 6.359889507293701,
      "learning_rate": 0.0019757560558394493,
      "loss": 4.0289,
      "step": 110
    },
    {
      "epoch": 0.222,
      "grad_norm": 3.884133815765381,
      "learning_rate": 0.0019752939873983254,
      "loss": 3.8395,
      "step": 111
    },
    {
      "epoch": 0.224,
      "grad_norm": 4.552613735198975,
      "learning_rate": 0.0019748276121766117,
      "loss": 3.9525,
      "step": 112
    },
    {
      "epoch": 0.226,
      "grad_norm": 3.474287986755371,
      "learning_rate": 0.001974356932233764,
      "loss": 3.9419,
      "step": 113
    },
    {
      "epoch": 0.228,
      "grad_norm": 3.940126657485962,
      "learning_rate": 0.0019738819496482496,
      "loss": 3.8488,
      "step": 114
    },
    {
      "epoch": 0.23,
      "grad_norm": 2.0929813385009766,
      "learning_rate": 0.0019734026665175334,
      "loss": 3.8036,
      "step": 115
    },
    {
      "epoch": 0.232,
      "grad_norm": 1.3861103057861328,
      "learning_rate": 0.001972919084958072,
      "loss": 3.7407,
      "step": 116
    },
    {
      "epoch": 0.234,
      "grad_norm": 4.450469017028809,
      "learning_rate": 0.001972431207105303,
      "loss": 3.9112,
      "step": 117
    },
    {
      "epoch": 0.236,
      "grad_norm": 1.7306631803512573,
      "learning_rate": 0.001971939035113636,
      "loss": 3.7819,
      "step": 118
    },
    {
      "epoch": 0.238,
      "grad_norm": 4.272606372833252,
      "learning_rate": 0.0019714425711564445,
      "loss": 4.0269,
      "step": 119
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.8772478103637695,
      "learning_rate": 0.001970941817426052,
      "loss": 4.0817,
      "step": 120
    },
    {
      "epoch": 0.242,
      "grad_norm": 2.8320887088775635,
      "learning_rate": 0.001970436776133727,
      "loss": 3.9081,
      "step": 121
    },
    {
      "epoch": 0.244,
      "grad_norm": 2.3876097202301025,
      "learning_rate": 0.0019699274495096715,
      "loss": 3.7979,
      "step": 122
    },
    {
      "epoch": 0.246,
      "grad_norm": 5.722553730010986,
      "learning_rate": 0.0019694138398030094,
      "loss": 3.9644,
      "step": 123
    },
    {
      "epoch": 0.248,
      "grad_norm": 5.810420989990234,
      "learning_rate": 0.00196889594928178,
      "loss": 3.8785,
      "step": 124
    },
    {
      "epoch": 0.25,
      "grad_norm": 2.580946207046509,
      "learning_rate": 0.001968373780232924,
      "loss": 3.6782,
      "step": 125
    },
    {
      "epoch": 0.252,
      "grad_norm": 5.660831451416016,
      "learning_rate": 0.0019678473349622793,
      "loss": 3.9596,
      "step": 126
    },
    {
      "epoch": 0.254,
      "grad_norm": 5.553674697875977,
      "learning_rate": 0.0019673166157945627,
      "loss": 3.939,
      "step": 127
    },
    {
      "epoch": 0.256,
      "grad_norm": 4.46774959564209,
      "learning_rate": 0.001966781625073367,
      "loss": 3.7774,
      "step": 128
    },
    {
      "epoch": 0.258,
      "grad_norm": 3.4952552318573,
      "learning_rate": 0.001966242365161146,
      "loss": 3.897,
      "step": 129
    },
    {
      "epoch": 0.26,
      "grad_norm": 2.1385550498962402,
      "learning_rate": 0.0019656988384392075,
      "loss": 3.8415,
      "step": 130
    },
    {
      "epoch": 0.262,
      "grad_norm": 3.746845245361328,
      "learning_rate": 0.0019651510473076986,
      "loss": 3.9401,
      "step": 131
    },
    {
      "epoch": 0.264,
      "grad_norm": 2.8245339393615723,
      "learning_rate": 0.0019645989941856,
      "loss": 3.7972,
      "step": 132
    },
    {
      "epoch": 0.266,
      "grad_norm": 2.157331705093384,
      "learning_rate": 0.0019640426815107108,
      "loss": 3.9159,
      "step": 133
    },
    {
      "epoch": 0.268,
      "grad_norm": 2.539757490158081,
      "learning_rate": 0.001963482111739641,
      "loss": 3.8109,
      "step": 134
    },
    {
      "epoch": 0.27,
      "grad_norm": 2.3521595001220703,
      "learning_rate": 0.0019629172873477994,
      "loss": 3.702,
      "step": 135
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.6652417182922363,
      "learning_rate": 0.0019623482108293818,
      "loss": 3.7841,
      "step": 136
    },
    {
      "epoch": 0.274,
      "grad_norm": 4.989634037017822,
      "learning_rate": 0.001961774884697362,
      "loss": 4.0426,
      "step": 137
    },
    {
      "epoch": 0.276,
      "grad_norm": 3.9169418811798096,
      "learning_rate": 0.001961197311483479,
      "loss": 3.8312,
      "step": 138
    },
    {
      "epoch": 0.278,
      "grad_norm": 3.4740068912506104,
      "learning_rate": 0.001960615493738226,
      "loss": 3.9132,
      "step": 139
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.341305255889893,
      "learning_rate": 0.0019600294340308398,
      "loss": 3.901,
      "step": 140
    },
    {
      "epoch": 0.282,
      "grad_norm": 2.2857134342193604,
      "learning_rate": 0.0019594391349492903,
      "loss": 3.7663,
      "step": 141
    },
    {
      "epoch": 0.284,
      "grad_norm": 6.500133037567139,
      "learning_rate": 0.001958844599100266,
      "loss": 3.9017,
      "step": 142
    },
    {
      "epoch": 0.286,
      "grad_norm": 7.2627482414245605,
      "learning_rate": 0.0019582458291091663,
      "loss": 3.8526,
      "step": 143
    },
    {
      "epoch": 0.288,
      "grad_norm": 3.119201898574829,
      "learning_rate": 0.001957642827620087,
      "loss": 3.8306,
      "step": 144
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.42329216003418,
      "learning_rate": 0.00195703559729581,
      "loss": 3.9998,
      "step": 145
    },
    {
      "epoch": 0.292,
      "grad_norm": 4.796652793884277,
      "learning_rate": 0.00195642414081779,
      "loss": 4.0325,
      "step": 146
    },
    {
      "epoch": 0.294,
      "grad_norm": 2.2950246334075928,
      "learning_rate": 0.0019558084608861472,
      "loss": 3.6906,
      "step": 147
    },
    {
      "epoch": 0.296,
      "grad_norm": 2.27897047996521,
      "learning_rate": 0.001955188560219648,
      "loss": 3.847,
      "step": 148
    },
    {
      "epoch": 0.298,
      "grad_norm": 1.9712156057357788,
      "learning_rate": 0.0019545644415557,
      "loss": 3.7141,
      "step": 149
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.4513416290283203,
      "learning_rate": 0.001953936107650336,
      "loss": 3.9255,
      "step": 150
    },
    {
      "epoch": 0.302,
      "grad_norm": 1.1574450731277466,
      "learning_rate": 0.001953303561278202,
      "loss": 3.7583,
      "step": 151
    },
    {
      "epoch": 0.304,
      "grad_norm": 2.919924020767212,
      "learning_rate": 0.0019526668052325467,
      "loss": 3.9023,
      "step": 152
    },
    {
      "epoch": 0.306,
      "grad_norm": 1.943543553352356,
      "learning_rate": 0.001952025842325208,
      "loss": 3.74,
      "step": 153
    },
    {
      "epoch": 0.308,
      "grad_norm": 2.065889358520508,
      "learning_rate": 0.0019513806753866014,
      "loss": 3.5713,
      "step": 154
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.885834693908691,
      "learning_rate": 0.0019507313072657055,
      "loss": 3.5013,
      "step": 155
    },
    {
      "epoch": 0.312,
      "grad_norm": 16.16520118713379,
      "learning_rate": 0.0019500777408300517,
      "loss": 3.8774,
      "step": 156
    },
    {
      "epoch": 0.314,
      "grad_norm": 8.877775192260742,
      "learning_rate": 0.001949419978965711,
      "loss": 3.8821,
      "step": 157
    },
    {
      "epoch": 0.316,
      "grad_norm": 5.928879737854004,
      "learning_rate": 0.00194875802457728,
      "loss": 4.2104,
      "step": 158
    },
    {
      "epoch": 0.318,
      "grad_norm": 17.222402572631836,
      "learning_rate": 0.0019480918805878697,
      "loss": 5.2891,
      "step": 159
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.500422239303589,
      "learning_rate": 0.001947421549939091,
      "loss": 4.0763,
      "step": 160
    },
    {
      "epoch": 0.322,
      "grad_norm": 4.208552837371826,
      "learning_rate": 0.0019467470355910438,
      "loss": 3.9651,
      "step": 161
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.4003832340240479,
      "learning_rate": 0.0019460683405223018,
      "loss": 3.7846,
      "step": 162
    },
    {
      "epoch": 0.326,
      "grad_norm": 1.5884822607040405,
      "learning_rate": 0.001945385467729901,
      "loss": 3.7764,
      "step": 163
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.6281615495681763,
      "learning_rate": 0.0019446984202293246,
      "loss": 3.8944,
      "step": 164
    },
    {
      "epoch": 0.33,
      "grad_norm": 3.3991734981536865,
      "learning_rate": 0.0019440072010544918,
      "loss": 3.7867,
      "step": 165
    },
    {
      "epoch": 0.332,
      "grad_norm": 2.9237446784973145,
      "learning_rate": 0.001943311813257743,
      "loss": 3.4943,
      "step": 166
    },
    {
      "epoch": 0.334,
      "grad_norm": 2.031006097793579,
      "learning_rate": 0.001942612259909827,
      "loss": 3.5111,
      "step": 167
    },
    {
      "epoch": 0.336,
      "grad_norm": 2.070072650909424,
      "learning_rate": 0.0019419085440998871,
      "loss": 3.351,
      "step": 168
    },
    {
      "epoch": 0.338,
      "grad_norm": 3.250882148742676,
      "learning_rate": 0.0019412006689354469,
      "loss": 3.2741,
      "step": 169
    },
    {
      "epoch": 0.34,
      "grad_norm": 3.0531368255615234,
      "learning_rate": 0.0019404886375423982,
      "loss": 3.413,
      "step": 170
    },
    {
      "epoch": 0.342,
      "grad_norm": 2.219747304916382,
      "learning_rate": 0.0019397724530649857,
      "loss": 3.3275,
      "step": 171
    },
    {
      "epoch": 0.344,
      "grad_norm": 2.7933156490325928,
      "learning_rate": 0.0019390521186657935,
      "loss": 3.1357,
      "step": 172
    },
    {
      "epoch": 0.346,
      "grad_norm": 2.1470651626586914,
      "learning_rate": 0.001938327637525731,
      "loss": 3.089,
      "step": 173
    },
    {
      "epoch": 0.348,
      "grad_norm": 41.50885772705078,
      "learning_rate": 0.0019375990128440205,
      "loss": 7.7493,
      "step": 174
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.727687120437622,
      "learning_rate": 0.0019368662478381799,
      "loss": 2.9603,
      "step": 175
    },
    {
      "epoch": 0.352,
      "grad_norm": 6.069608688354492,
      "learning_rate": 0.001936129345744011,
      "loss": 3.1332,
      "step": 176
    },
    {
      "epoch": 0.354,
      "grad_norm": 1.7369825839996338,
      "learning_rate": 0.0019353883098155854,
      "loss": 2.9489,
      "step": 177
    },
    {
      "epoch": 0.356,
      "grad_norm": 155.95303344726562,
      "learning_rate": 0.0019346431433252273,
      "loss": 3.2016,
      "step": 178
    },
    {
      "epoch": 0.358,
      "grad_norm": 6.919866561889648,
      "learning_rate": 0.001933893849563503,
      "loss": 3.587,
      "step": 179
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.6674349308013916,
      "learning_rate": 0.0019331404318392025,
      "loss": 3.4117,
      "step": 180
    },
    {
      "epoch": 0.362,
      "grad_norm": 4.165578365325928,
      "learning_rate": 0.0019323828934793284,
      "loss": 3.1524,
      "step": 181
    },
    {
      "epoch": 0.364,
      "grad_norm": 2.273158311843872,
      "learning_rate": 0.0019316212378290782,
      "loss": 2.9548,
      "step": 182
    },
    {
      "epoch": 0.366,
      "grad_norm": 1.829839825630188,
      "learning_rate": 0.0019308554682518312,
      "loss": 2.9177,
      "step": 183
    },
    {
      "epoch": 0.368,
      "grad_norm": 5.258911609649658,
      "learning_rate": 0.001930085588129134,
      "loss": 3.0985,
      "step": 184
    },
    {
      "epoch": 0.37,
      "grad_norm": 2.2935218811035156,
      "learning_rate": 0.0019293116008606837,
      "loss": 2.8774,
      "step": 185
    },
    {
      "epoch": 0.372,
      "grad_norm": 6.381641387939453,
      "learning_rate": 0.0019285335098643153,
      "loss": 3.2244,
      "step": 186
    },
    {
      "epoch": 0.374,
      "grad_norm": 1.5829997062683105,
      "learning_rate": 0.0019277513185759845,
      "loss": 2.882,
      "step": 187
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.4570904970169067,
      "learning_rate": 0.001926965030449754,
      "loss": 2.8957,
      "step": 188
    },
    {
      "epoch": 0.378,
      "grad_norm": 1.3911361694335938,
      "learning_rate": 0.0019261746489577765,
      "loss": 2.8609,
      "step": 189
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.4556010961532593,
      "learning_rate": 0.001925380177590282,
      "loss": 2.8871,
      "step": 190
    },
    {
      "epoch": 0.382,
      "grad_norm": 2.356713056564331,
      "learning_rate": 0.0019245816198555604,
      "loss": 2.8865,
      "step": 191
    },
    {
      "epoch": 0.384,
      "grad_norm": 3.208085298538208,
      "learning_rate": 0.0019237789792799457,
      "loss": 2.8733,
      "step": 192
    },
    {
      "epoch": 0.386,
      "grad_norm": 3.3887312412261963,
      "learning_rate": 0.001922972259407802,
      "loss": 2.8097,
      "step": 193
    },
    {
      "epoch": 0.388,
      "grad_norm": 1.5897456407546997,
      "learning_rate": 0.0019221614638015075,
      "loss": 2.785,
      "step": 194
    },
    {
      "epoch": 0.39,
      "grad_norm": 5.513909339904785,
      "learning_rate": 0.001921346596041437,
      "loss": 3.4739,
      "step": 195
    },
    {
      "epoch": 0.392,
      "grad_norm": 5.400357246398926,
      "learning_rate": 0.0019205276597259483,
      "loss": 3.1785,
      "step": 196
    },
    {
      "epoch": 0.394,
      "grad_norm": 2.5335726737976074,
      "learning_rate": 0.0019197046584713661,
      "loss": 2.7758,
      "step": 197
    },
    {
      "epoch": 0.396,
      "grad_norm": 2.600646495819092,
      "learning_rate": 0.0019188775959119641,
      "loss": 2.8635,
      "step": 198
    },
    {
      "epoch": 0.398,
      "grad_norm": 2.237241506576538,
      "learning_rate": 0.0019180464756999509,
      "loss": 2.7868,
      "step": 199
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.106405258178711,
      "learning_rate": 0.001917211301505453,
      "loss": 2.8547,
      "step": 200
    },
    {
      "epoch": 0.4,
      "eval_loss": 2.7569472789764404,
      "eval_runtime": 214.4024,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 200
    },
    {
      "epoch": 0.402,
      "grad_norm": 2.437791109085083,
      "learning_rate": 0.001916372077016499,
      "loss": 2.7562,
      "step": 201
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.8614029884338379,
      "learning_rate": 0.0019155288059390027,
      "loss": 2.6753,
      "step": 202
    },
    {
      "epoch": 0.406,
      "grad_norm": 1.6191282272338867,
      "learning_rate": 0.0019146814919967481,
      "loss": 2.7567,
      "step": 203
    },
    {
      "epoch": 0.408,
      "grad_norm": 1.5468201637268066,
      "learning_rate": 0.0019138301389313708,
      "loss": 2.5477,
      "step": 204
    },
    {
      "epoch": 0.41,
      "grad_norm": 2.241537094116211,
      "learning_rate": 0.0019129747505023437,
      "loss": 2.6143,
      "step": 205
    },
    {
      "epoch": 0.412,
      "grad_norm": 2.3169503211975098,
      "learning_rate": 0.0019121153304869584,
      "loss": 2.6276,
      "step": 206
    },
    {
      "epoch": 0.414,
      "grad_norm": 1.4660946130752563,
      "learning_rate": 0.0019112518826803098,
      "loss": 2.6481,
      "step": 207
    },
    {
      "epoch": 0.416,
      "grad_norm": 3.670999765396118,
      "learning_rate": 0.00191038441089528,
      "loss": 2.7575,
      "step": 208
    },
    {
      "epoch": 0.418,
      "grad_norm": 2.1378672122955322,
      "learning_rate": 0.0019095129189625193,
      "loss": 2.8118,
      "step": 209
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.478317379951477,
      "learning_rate": 0.0019086374107304311,
      "loss": 2.6267,
      "step": 210
    },
    {
      "epoch": 0.422,
      "grad_norm": 2.1900315284729004,
      "learning_rate": 0.0019077578900651541,
      "loss": 2.6118,
      "step": 211
    },
    {
      "epoch": 0.424,
      "grad_norm": 3.023249864578247,
      "learning_rate": 0.0019068743608505454,
      "loss": 2.6327,
      "step": 212
    },
    {
      "epoch": 0.426,
      "grad_norm": 2.378028154373169,
      "learning_rate": 0.0019059868269881636,
      "loss": 2.667,
      "step": 213
    },
    {
      "epoch": 0.428,
      "grad_norm": 1.3596745729446411,
      "learning_rate": 0.0019050952923972508,
      "loss": 2.6828,
      "step": 214
    },
    {
      "epoch": 0.43,
      "grad_norm": 2.4050638675689697,
      "learning_rate": 0.0019041997610147166,
      "loss": 2.5688,
      "step": 215
    },
    {
      "epoch": 0.432,
      "grad_norm": 1.5172045230865479,
      "learning_rate": 0.0019033002367951192,
      "loss": 2.5397,
      "step": 216
    },
    {
      "epoch": 0.434,
      "grad_norm": 3.7499849796295166,
      "learning_rate": 0.0019023967237106491,
      "loss": 2.5462,
      "step": 217
    },
    {
      "epoch": 0.436,
      "grad_norm": 3.0412256717681885,
      "learning_rate": 0.0019014892257511117,
      "loss": 2.7246,
      "step": 218
    },
    {
      "epoch": 0.438,
      "grad_norm": 1.3770488500595093,
      "learning_rate": 0.0019005777469239076,
      "loss": 2.6576,
      "step": 219
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.6912823915481567,
      "learning_rate": 0.001899662291254018,
      "loss": 2.5478,
      "step": 220
    },
    {
      "epoch": 0.442,
      "grad_norm": 2.3487942218780518,
      "learning_rate": 0.0018987428627839842,
      "loss": 2.6124,
      "step": 221
    },
    {
      "epoch": 0.444,
      "grad_norm": 1.2562780380249023,
      "learning_rate": 0.0018978194655738915,
      "loss": 2.4719,
      "step": 222
    },
    {
      "epoch": 0.446,
      "grad_norm": 1.8632471561431885,
      "learning_rate": 0.0018968921037013512,
      "loss": 2.4906,
      "step": 223
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.065873384475708,
      "learning_rate": 0.0018959607812614806,
      "loss": 2.4836,
      "step": 224
    },
    {
      "epoch": 0.45,
      "grad_norm": 1.686170220375061,
      "learning_rate": 0.0018950255023668877,
      "loss": 2.5545,
      "step": 225
    },
    {
      "epoch": 0.452,
      "grad_norm": 2.3977906703948975,
      "learning_rate": 0.0018940862711476511,
      "loss": 2.5263,
      "step": 226
    },
    {
      "epoch": 0.454,
      "grad_norm": 2.699784994125366,
      "learning_rate": 0.0018931430917513029,
      "loss": 2.4123,
      "step": 227
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.9838353991508484,
      "learning_rate": 0.001892195968342809,
      "loss": 2.3526,
      "step": 228
    },
    {
      "epoch": 0.458,
      "grad_norm": 1.9762444496154785,
      "learning_rate": 0.0018912449051045525,
      "loss": 2.6635,
      "step": 229
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.9106740951538086,
      "learning_rate": 0.0018902899062363141,
      "loss": 2.4518,
      "step": 230
    },
    {
      "epoch": 0.462,
      "grad_norm": 2.0589303970336914,
      "learning_rate": 0.0018893309759552529,
      "loss": 2.3058,
      "step": 231
    },
    {
      "epoch": 0.464,
      "grad_norm": 1.4612406492233276,
      "learning_rate": 0.0018883681184958898,
      "loss": 2.2631,
      "step": 232
    },
    {
      "epoch": 0.466,
      "grad_norm": 2.042137861251831,
      "learning_rate": 0.0018874013381100874,
      "loss": 2.3887,
      "step": 233
    },
    {
      "epoch": 0.468,
      "grad_norm": 2.9777297973632812,
      "learning_rate": 0.0018864306390670308,
      "loss": 2.3859,
      "step": 234
    },
    {
      "epoch": 0.47,
      "grad_norm": 3.476142406463623,
      "learning_rate": 0.0018854560256532098,
      "loss": 2.3627,
      "step": 235
    },
    {
      "epoch": 0.472,
      "grad_norm": 7.374683856964111,
      "learning_rate": 0.0018844775021724003,
      "loss": 2.3102,
      "step": 236
    },
    {
      "epoch": 0.474,
      "grad_norm": 3.586138963699341,
      "learning_rate": 0.0018834950729456432,
      "loss": 2.3128,
      "step": 237
    },
    {
      "epoch": 0.476,
      "grad_norm": 1.4701393842697144,
      "learning_rate": 0.001882508742311228,
      "loss": 2.3391,
      "step": 238
    },
    {
      "epoch": 0.478,
      "grad_norm": 1.3705323934555054,
      "learning_rate": 0.0018815185146246716,
      "loss": 2.0927,
      "step": 239
    },
    {
      "epoch": 0.48,
      "grad_norm": 4.723278045654297,
      "learning_rate": 0.0018805243942587,
      "loss": 2.399,
      "step": 240
    },
    {
      "epoch": 0.482,
      "grad_norm": 2.4533042907714844,
      "learning_rate": 0.0018795263856032287,
      "loss": 2.1253,
      "step": 241
    },
    {
      "epoch": 0.484,
      "grad_norm": 3.28692626953125,
      "learning_rate": 0.0018785244930653439,
      "loss": 2.3791,
      "step": 242
    },
    {
      "epoch": 0.486,
      "grad_norm": 1.6356228590011597,
      "learning_rate": 0.0018775187210692814,
      "loss": 2.1259,
      "step": 243
    },
    {
      "epoch": 0.488,
      "grad_norm": 4.296216011047363,
      "learning_rate": 0.0018765090740564098,
      "loss": 2.7276,
      "step": 244
    },
    {
      "epoch": 0.49,
      "grad_norm": 3.812382936477661,
      "learning_rate": 0.001875495556485208,
      "loss": 2.4598,
      "step": 245
    },
    {
      "epoch": 0.492,
      "grad_norm": 2.5472209453582764,
      "learning_rate": 0.001874478172831248,
      "loss": 2.1219,
      "step": 246
    },
    {
      "epoch": 0.494,
      "grad_norm": 2.093242645263672,
      "learning_rate": 0.0018734569275871726,
      "loss": 2.2515,
      "step": 247
    },
    {
      "epoch": 0.496,
      "grad_norm": 2.889310359954834,
      "learning_rate": 0.0018724318252626776,
      "loss": 2.1555,
      "step": 248
    },
    {
      "epoch": 0.498,
      "grad_norm": 1.836767554283142,
      "learning_rate": 0.0018714028703844914,
      "loss": 2.3279,
      "step": 249
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.9833530187606812,
      "learning_rate": 0.0018703700674963545,
      "loss": 2.2268,
      "step": 250
    },
    {
      "epoch": 0.502,
      "grad_norm": 1.4897099733352661,
      "learning_rate": 0.0018693334211590006,
      "loss": 2.2728,
      "step": 251
    },
    {
      "epoch": 0.504,
      "grad_norm": 1.941544771194458,
      "learning_rate": 0.0018682929359501337,
      "loss": 2.0025,
      "step": 252
    },
    {
      "epoch": 0.506,
      "grad_norm": 2.063384771347046,
      "learning_rate": 0.0018672486164644116,
      "loss": 2.114,
      "step": 253
    },
    {
      "epoch": 0.508,
      "grad_norm": 9.12896728515625,
      "learning_rate": 0.0018662004673134231,
      "loss": 2.2348,
      "step": 254
    },
    {
      "epoch": 0.51,
      "grad_norm": 2.3671436309814453,
      "learning_rate": 0.0018651484931256684,
      "loss": 2.2072,
      "step": 255
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.0431063175201416,
      "learning_rate": 0.0018640926985465387,
      "loss": 2.134,
      "step": 256
    },
    {
      "epoch": 0.514,
      "grad_norm": 2.4492526054382324,
      "learning_rate": 0.0018630330882382952,
      "loss": 2.163,
      "step": 257
    },
    {
      "epoch": 0.516,
      "grad_norm": 3.0998520851135254,
      "learning_rate": 0.0018619696668800492,
      "loss": 2.1412,
      "step": 258
    },
    {
      "epoch": 0.518,
      "grad_norm": 1.4419025182724,
      "learning_rate": 0.0018609024391677417,
      "loss": 2.0474,
      "step": 259
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.4526324272155762,
      "learning_rate": 0.0018598314098141207,
      "loss": 2.2092,
      "step": 260
    },
    {
      "epoch": 0.522,
      "grad_norm": 0.7093802094459534,
      "learning_rate": 0.0018587565835487233,
      "loss": 2.035,
      "step": 261
    },
    {
      "epoch": 0.524,
      "grad_norm": 2.053166151046753,
      "learning_rate": 0.0018576779651178522,
      "loss": 2.0889,
      "step": 262
    },
    {
      "epoch": 0.526,
      "grad_norm": 1.7750377655029297,
      "learning_rate": 0.0018565955592845563,
      "loss": 1.9983,
      "step": 263
    },
    {
      "epoch": 0.528,
      "grad_norm": 2.8194613456726074,
      "learning_rate": 0.0018555093708286093,
      "loss": 2.2395,
      "step": 264
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.8581326007843018,
      "learning_rate": 0.0018544194045464887,
      "loss": 2.1074,
      "step": 265
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.79025399684906,
      "learning_rate": 0.0018533256652513534,
      "loss": 1.9372,
      "step": 266
    },
    {
      "epoch": 0.534,
      "grad_norm": 1.0047465562820435,
      "learning_rate": 0.001852228157773025,
      "loss": 2.0341,
      "step": 267
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.472951889038086,
      "learning_rate": 0.0018511268869579635,
      "loss": 2.0,
      "step": 268
    },
    {
      "epoch": 0.538,
      "grad_norm": 0.8167721033096313,
      "learning_rate": 0.001850021857669248,
      "loss": 2.0282,
      "step": 269
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.3056704998016357,
      "learning_rate": 0.0018489130747865548,
      "loss": 1.9908,
      "step": 270
    },
    {
      "epoch": 0.542,
      "grad_norm": 0.7072440981864929,
      "learning_rate": 0.0018478005432061352,
      "loss": 2.009,
      "step": 271
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.9551014304161072,
      "learning_rate": 0.0018466842678407944,
      "loss": 1.8771,
      "step": 272
    },
    {
      "epoch": 0.546,
      "grad_norm": 2.015329599380493,
      "learning_rate": 0.00184556425361987,
      "loss": 1.9382,
      "step": 273
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.6865448355674744,
      "learning_rate": 0.0018444405054892092,
      "loss": 1.9094,
      "step": 274
    },
    {
      "epoch": 0.55,
      "grad_norm": 2.1745128631591797,
      "learning_rate": 0.001843313028411149,
      "loss": 1.9736,
      "step": 275
    },
    {
      "epoch": 0.552,
      "grad_norm": 1.3340539932250977,
      "learning_rate": 0.0018421818273644912,
      "loss": 1.9336,
      "step": 276
    },
    {
      "epoch": 0.554,
      "grad_norm": 1.5305424928665161,
      "learning_rate": 0.001841046907344484,
      "loss": 1.9325,
      "step": 277
    },
    {
      "epoch": 0.556,
      "grad_norm": 4.358243942260742,
      "learning_rate": 0.0018399082733627965,
      "loss": 2.0402,
      "step": 278
    },
    {
      "epoch": 0.558,
      "grad_norm": 1.6777774095535278,
      "learning_rate": 0.0018387659304474994,
      "loss": 1.8606,
      "step": 279
    },
    {
      "epoch": 0.56,
      "grad_norm": 4.904858589172363,
      "learning_rate": 0.0018376198836430415,
      "loss": 2.0832,
      "step": 280
    },
    {
      "epoch": 0.562,
      "grad_norm": 3.934642791748047,
      "learning_rate": 0.0018364701380102267,
      "loss": 2.0568,
      "step": 281
    },
    {
      "epoch": 0.564,
      "grad_norm": 2.538179874420166,
      "learning_rate": 0.0018353166986261936,
      "loss": 2.0483,
      "step": 282
    },
    {
      "epoch": 0.566,
      "grad_norm": 1.9202468395233154,
      "learning_rate": 0.0018341595705843906,
      "loss": 1.9563,
      "step": 283
    },
    {
      "epoch": 0.568,
      "grad_norm": 2.8745357990264893,
      "learning_rate": 0.001832998758994556,
      "loss": 2.2928,
      "step": 284
    },
    {
      "epoch": 0.57,
      "grad_norm": 4.064741611480713,
      "learning_rate": 0.0018318342689826936,
      "loss": 2.2056,
      "step": 285
    },
    {
      "epoch": 0.572,
      "grad_norm": 1.2869336605072021,
      "learning_rate": 0.001830666105691051,
      "loss": 1.8522,
      "step": 286
    },
    {
      "epoch": 0.574,
      "grad_norm": 8.110916137695312,
      "learning_rate": 0.0018294942742780964,
      "loss": 2.1132,
      "step": 287
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.7317262887954712,
      "learning_rate": 0.0018283187799184957,
      "loss": 2.1377,
      "step": 288
    },
    {
      "epoch": 0.578,
      "grad_norm": 0.7028880715370178,
      "learning_rate": 0.0018271396278030903,
      "loss": 1.8794,
      "step": 289
    },
    {
      "epoch": 0.58,
      "grad_norm": 7.359745025634766,
      "learning_rate": 0.0018259568231388736,
      "loss": 2.0676,
      "step": 290
    },
    {
      "epoch": 0.582,
      "grad_norm": 3.3747036457061768,
      "learning_rate": 0.0018247703711489684,
      "loss": 1.9294,
      "step": 291
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.7845973372459412,
      "learning_rate": 0.0018235802770726038,
      "loss": 1.8017,
      "step": 292
    },
    {
      "epoch": 0.586,
      "grad_norm": 3.866828441619873,
      "learning_rate": 0.0018223865461650913,
      "loss": 2.0374,
      "step": 293
    },
    {
      "epoch": 0.588,
      "grad_norm": 3.943230152130127,
      "learning_rate": 0.0018211891836978028,
      "loss": 1.9961,
      "step": 294
    },
    {
      "epoch": 0.59,
      "grad_norm": 1.4381682872772217,
      "learning_rate": 0.001819988194958146,
      "loss": 1.9866,
      "step": 295
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.2843194007873535,
      "learning_rate": 0.0018187835852495429,
      "loss": 1.8435,
      "step": 296
    },
    {
      "epoch": 0.594,
      "grad_norm": 2.097268581390381,
      "learning_rate": 0.0018175753598914047,
      "loss": 1.8696,
      "step": 297
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.7812860012054443,
      "learning_rate": 0.0018163635242191083,
      "loss": 1.9102,
      "step": 298
    },
    {
      "epoch": 0.598,
      "grad_norm": 1.0270416736602783,
      "learning_rate": 0.0018151480835839743,
      "loss": 1.8752,
      "step": 299
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.0740411281585693,
      "learning_rate": 0.0018139290433532413,
      "loss": 1.7481,
      "step": 300
    },
    {
      "epoch": 0.6,
      "eval_loss": 2.003680944442749,
      "eval_runtime": 214.6073,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 300
    },
    {
      "epoch": 0.602,
      "grad_norm": 5.275888442993164,
      "learning_rate": 0.0018127064089100446,
      "loss": 1.9528,
      "step": 301
    },
    {
      "epoch": 0.604,
      "grad_norm": 5.195688247680664,
      "learning_rate": 0.00181148018565339,
      "loss": 1.9675,
      "step": 302
    },
    {
      "epoch": 0.606,
      "grad_norm": 1.0743874311447144,
      "learning_rate": 0.001810250378998132,
      "loss": 1.9155,
      "step": 303
    },
    {
      "epoch": 0.608,
      "grad_norm": 6.450048923492432,
      "learning_rate": 0.0018090169943749475,
      "loss": 2.0287,
      "step": 304
    },
    {
      "epoch": 0.61,
      "grad_norm": 2.6360714435577393,
      "learning_rate": 0.001807780037230315,
      "loss": 2.2253,
      "step": 305
    },
    {
      "epoch": 0.612,
      "grad_norm": 1.815761685371399,
      "learning_rate": 0.0018065395130264874,
      "loss": 1.8471,
      "step": 306
    },
    {
      "epoch": 0.614,
      "grad_norm": 1.7224658727645874,
      "learning_rate": 0.0018052954272414707,
      "loss": 1.9919,
      "step": 307
    },
    {
      "epoch": 0.616,
      "grad_norm": 3.783226490020752,
      "learning_rate": 0.0018040477853689969,
      "loss": 1.9178,
      "step": 308
    },
    {
      "epoch": 0.618,
      "grad_norm": 1.4003031253814697,
      "learning_rate": 0.0018027965929185024,
      "loss": 1.8215,
      "step": 309
    },
    {
      "epoch": 0.62,
      "grad_norm": 3.2184195518493652,
      "learning_rate": 0.0018015418554151023,
      "loss": 1.9776,
      "step": 310
    },
    {
      "epoch": 0.622,
      "grad_norm": 2.235565185546875,
      "learning_rate": 0.0018002835783995652,
      "loss": 1.9936,
      "step": 311
    },
    {
      "epoch": 0.624,
      "grad_norm": 3.2797882556915283,
      "learning_rate": 0.0017990217674282915,
      "loss": 1.8816,
      "step": 312
    },
    {
      "epoch": 0.626,
      "grad_norm": 2.7622275352478027,
      "learning_rate": 0.001797756428073286,
      "loss": 1.8565,
      "step": 313
    },
    {
      "epoch": 0.628,
      "grad_norm": 1.1475543975830078,
      "learning_rate": 0.0017964875659221343,
      "loss": 1.9739,
      "step": 314
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.6624130010604858,
      "learning_rate": 0.0017952151865779792,
      "loss": 1.8346,
      "step": 315
    },
    {
      "epoch": 0.632,
      "grad_norm": 1.735350489616394,
      "learning_rate": 0.0017939392956594932,
      "loss": 1.9842,
      "step": 316
    },
    {
      "epoch": 0.634,
      "grad_norm": 1.3645603656768799,
      "learning_rate": 0.0017926598988008582,
      "loss": 1.7467,
      "step": 317
    },
    {
      "epoch": 0.636,
      "grad_norm": 1.7190128564834595,
      "learning_rate": 0.0017913770016517354,
      "loss": 1.8572,
      "step": 318
    },
    {
      "epoch": 0.638,
      "grad_norm": 2.079439640045166,
      "learning_rate": 0.0017900906098772444,
      "loss": 1.9157,
      "step": 319
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.0893893241882324,
      "learning_rate": 0.0017888007291579355,
      "loss": 1.7867,
      "step": 320
    },
    {
      "epoch": 0.642,
      "grad_norm": 3.7517189979553223,
      "learning_rate": 0.001787507365189767,
      "loss": 1.968,
      "step": 321
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.8603603839874268,
      "learning_rate": 0.0017862105236840775,
      "loss": 1.5925,
      "step": 322
    },
    {
      "epoch": 0.646,
      "grad_norm": 4.840493679046631,
      "learning_rate": 0.001784910210367563,
      "loss": 1.837,
      "step": 323
    },
    {
      "epoch": 0.648,
      "grad_norm": 6.102051258087158,
      "learning_rate": 0.0017836064309822502,
      "loss": 2.11,
      "step": 324
    },
    {
      "epoch": 0.65,
      "grad_norm": 2.0164759159088135,
      "learning_rate": 0.0017822991912854714,
      "loss": 1.9396,
      "step": 325
    },
    {
      "epoch": 0.652,
      "grad_norm": 1.9613151550292969,
      "learning_rate": 0.0017809884970498395,
      "loss": 1.8424,
      "step": 326
    },
    {
      "epoch": 0.654,
      "grad_norm": 2.6625077724456787,
      "learning_rate": 0.0017796743540632223,
      "loss": 1.9326,
      "step": 327
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.5835576057434082,
      "learning_rate": 0.0017783567681287167,
      "loss": 1.8285,
      "step": 328
    },
    {
      "epoch": 0.658,
      "grad_norm": 2.4700918197631836,
      "learning_rate": 0.001777035745064623,
      "loss": 1.9104,
      "step": 329
    },
    {
      "epoch": 0.66,
      "grad_norm": 5.008753299713135,
      "learning_rate": 0.0017757112907044199,
      "loss": 1.9535,
      "step": 330
    },
    {
      "epoch": 0.662,
      "grad_norm": 32.69684600830078,
      "learning_rate": 0.001774383410896738,
      "loss": 3.2069,
      "step": 331
    },
    {
      "epoch": 0.664,
      "grad_norm": 11.677919387817383,
      "learning_rate": 0.001773052111505334,
      "loss": 2.6595,
      "step": 332
    },
    {
      "epoch": 0.666,
      "grad_norm": 3.8162200450897217,
      "learning_rate": 0.0017717173984090658,
      "loss": 2.051,
      "step": 333
    },
    {
      "epoch": 0.668,
      "grad_norm": 2.73925518989563,
      "learning_rate": 0.0017703792775018655,
      "loss": 2.0028,
      "step": 334
    },
    {
      "epoch": 0.67,
      "grad_norm": 2.778430223464966,
      "learning_rate": 0.0017690377546927133,
      "loss": 1.8003,
      "step": 335
    },
    {
      "epoch": 0.672,
      "grad_norm": 2.3258018493652344,
      "learning_rate": 0.001767692835905612,
      "loss": 1.9884,
      "step": 336
    },
    {
      "epoch": 0.674,
      "grad_norm": 2.6141955852508545,
      "learning_rate": 0.001766344527079561,
      "loss": 1.9602,
      "step": 337
    },
    {
      "epoch": 0.676,
      "grad_norm": 1.6637415885925293,
      "learning_rate": 0.0017649928341685298,
      "loss": 1.7876,
      "step": 338
    },
    {
      "epoch": 0.678,
      "grad_norm": 6.889964580535889,
      "learning_rate": 0.0017636377631414302,
      "loss": 2.0717,
      "step": 339
    },
    {
      "epoch": 0.68,
      "grad_norm": 7.607306480407715,
      "learning_rate": 0.0017622793199820932,
      "loss": 2.0901,
      "step": 340
    },
    {
      "epoch": 0.682,
      "grad_norm": 3.831721544265747,
      "learning_rate": 0.0017609175106892395,
      "loss": 1.9786,
      "step": 341
    },
    {
      "epoch": 0.684,
      "grad_norm": 32.60588836669922,
      "learning_rate": 0.001759552341276455,
      "loss": 2.0475,
      "step": 342
    },
    {
      "epoch": 0.686,
      "grad_norm": 1.3389657735824585,
      "learning_rate": 0.0017581838177721627,
      "loss": 2.0577,
      "step": 343
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.9935059547424316,
      "learning_rate": 0.0017568119462195977,
      "loss": 1.924,
      "step": 344
    },
    {
      "epoch": 0.69,
      "grad_norm": 2.3636181354522705,
      "learning_rate": 0.0017554367326767792,
      "loss": 1.8887,
      "step": 345
    },
    {
      "epoch": 0.692,
      "grad_norm": 1.1704517602920532,
      "learning_rate": 0.0017540581832164838,
      "loss": 1.8875,
      "step": 346
    },
    {
      "epoch": 0.694,
      "grad_norm": 1.3260549306869507,
      "learning_rate": 0.0017526763039262207,
      "loss": 1.9067,
      "step": 347
    },
    {
      "epoch": 0.696,
      "grad_norm": 4.988547325134277,
      "learning_rate": 0.0017512911009082012,
      "loss": 1.8703,
      "step": 348
    },
    {
      "epoch": 0.698,
      "grad_norm": 0.893297016620636,
      "learning_rate": 0.0017499025802793146,
      "loss": 1.7209,
      "step": 349
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.1977099180221558,
      "learning_rate": 0.001748510748171101,
      "loss": 2.001,
      "step": 350
    },
    {
      "epoch": 0.702,
      "grad_norm": 1.6388975381851196,
      "learning_rate": 0.0017471156107297233,
      "loss": 1.9552,
      "step": 351
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.1636892557144165,
      "learning_rate": 0.0017457171741159395,
      "loss": 1.7975,
      "step": 352
    },
    {
      "epoch": 0.706,
      "grad_norm": 3.763468027114868,
      "learning_rate": 0.0017443154445050772,
      "loss": 2.2477,
      "step": 353
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.8547747135162354,
      "learning_rate": 0.0017429104280870056,
      "loss": 1.889,
      "step": 354
    },
    {
      "epoch": 0.71,
      "grad_norm": 2.295851469039917,
      "learning_rate": 0.001741502131066107,
      "loss": 1.9083,
      "step": 355
    },
    {
      "epoch": 0.712,
      "grad_norm": 1.607290506362915,
      "learning_rate": 0.001740090559661252,
      "loss": 1.8177,
      "step": 356
    },
    {
      "epoch": 0.714,
      "grad_norm": 1.0779179334640503,
      "learning_rate": 0.001738675720105769,
      "loss": 1.8203,
      "step": 357
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.7504029273986816,
      "learning_rate": 0.001737257618647419,
      "loss": 1.8988,
      "step": 358
    },
    {
      "epoch": 0.718,
      "grad_norm": 0.9955078959465027,
      "learning_rate": 0.0017358362615483669,
      "loss": 1.7721,
      "step": 359
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.8653132915496826,
      "learning_rate": 0.0017344116550851542,
      "loss": 1.8222,
      "step": 360
    },
    {
      "epoch": 0.722,
      "grad_norm": 2.4027767181396484,
      "learning_rate": 0.0017329838055486716,
      "loss": 1.874,
      "step": 361
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.6823976635932922,
      "learning_rate": 0.0017315527192441297,
      "loss": 1.7065,
      "step": 362
    },
    {
      "epoch": 0.726,
      "grad_norm": 1.3852829933166504,
      "learning_rate": 0.0017301184024910332,
      "loss": 1.7914,
      "step": 363
    },
    {
      "epoch": 0.728,
      "grad_norm": 1.1623455286026,
      "learning_rate": 0.0017286808616231522,
      "loss": 1.9187,
      "step": 364
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.7269585132598877,
      "learning_rate": 0.0017272401029884933,
      "loss": 1.6587,
      "step": 365
    },
    {
      "epoch": 0.732,
      "grad_norm": 1.0623549222946167,
      "learning_rate": 0.0017257961329492728,
      "loss": 1.7461,
      "step": 366
    },
    {
      "epoch": 0.734,
      "grad_norm": 0.7915670275688171,
      "learning_rate": 0.001724348957881889,
      "loss": 1.6873,
      "step": 367
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.885283350944519,
      "learning_rate": 0.0017228985841768914,
      "loss": 1.8064,
      "step": 368
    },
    {
      "epoch": 0.738,
      "grad_norm": 2.573652982711792,
      "learning_rate": 0.0017214450182389558,
      "loss": 1.7142,
      "step": 369
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.9665259718894958,
      "learning_rate": 0.0017199882664868538,
      "loss": 1.6398,
      "step": 370
    },
    {
      "epoch": 0.742,
      "grad_norm": 1.4628034830093384,
      "learning_rate": 0.0017185283353534258,
      "loss": 1.745,
      "step": 371
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.9098427295684814,
      "learning_rate": 0.0017170652312855513,
      "loss": 1.7481,
      "step": 372
    },
    {
      "epoch": 0.746,
      "grad_norm": 2.0527310371398926,
      "learning_rate": 0.0017155989607441212,
      "loss": 1.7107,
      "step": 373
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.6249220967292786,
      "learning_rate": 0.0017141295302040094,
      "loss": 1.6916,
      "step": 374
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.7852041721343994,
      "learning_rate": 0.0017126569461540441,
      "loss": 1.7347,
      "step": 375
    },
    {
      "epoch": 0.752,
      "grad_norm": 1.0229157209396362,
      "learning_rate": 0.0017111812150969788,
      "loss": 1.7095,
      "step": 376
    },
    {
      "epoch": 0.754,
      "grad_norm": 3.277080774307251,
      "learning_rate": 0.0017097023435494636,
      "loss": 1.7845,
      "step": 377
    },
    {
      "epoch": 0.756,
      "grad_norm": 2.4981095790863037,
      "learning_rate": 0.001708220338042017,
      "loss": 1.9226,
      "step": 378
    },
    {
      "epoch": 0.758,
      "grad_norm": 1.3083505630493164,
      "learning_rate": 0.0017067352051189967,
      "loss": 1.8607,
      "step": 379
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9758284687995911,
      "learning_rate": 0.0017052469513385699,
      "loss": 1.6036,
      "step": 380
    },
    {
      "epoch": 0.762,
      "grad_norm": 0.9818079471588135,
      "learning_rate": 0.0017037555832726864,
      "loss": 1.6395,
      "step": 381
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.9793897271156311,
      "learning_rate": 0.0017022611075070474,
      "loss": 1.6707,
      "step": 382
    },
    {
      "epoch": 0.766,
      "grad_norm": 2.269204616546631,
      "learning_rate": 0.0017007635306410774,
      "loss": 1.7388,
      "step": 383
    },
    {
      "epoch": 0.768,
      "grad_norm": 4.338377952575684,
      "learning_rate": 0.0016992628592878956,
      "loss": 2.1587,
      "step": 384
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.5375699996948242,
      "learning_rate": 0.0016977591000742853,
      "loss": 1.8932,
      "step": 385
    },
    {
      "epoch": 0.772,
      "grad_norm": 8.069236755371094,
      "learning_rate": 0.0016962522596406663,
      "loss": 2.53,
      "step": 386
    },
    {
      "epoch": 0.774,
      "grad_norm": 2.6307308673858643,
      "learning_rate": 0.0016947423446410635,
      "loss": 1.8614,
      "step": 387
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.9871082901954651,
      "learning_rate": 0.0016932293617430796,
      "loss": 1.6775,
      "step": 388
    },
    {
      "epoch": 0.778,
      "grad_norm": 0.8349642753601074,
      "learning_rate": 0.0016917133176278648,
      "loss": 1.7829,
      "step": 389
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.161250114440918,
      "learning_rate": 0.0016901942189900866,
      "loss": 1.7373,
      "step": 390
    },
    {
      "epoch": 0.782,
      "grad_norm": 5.251394748687744,
      "learning_rate": 0.001688672072537902,
      "loss": 2.2039,
      "step": 391
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.060323715209961,
      "learning_rate": 0.0016871468849929253,
      "loss": 1.8332,
      "step": 392
    },
    {
      "epoch": 0.786,
      "grad_norm": 4.304510593414307,
      "learning_rate": 0.0016856186630902013,
      "loss": 1.8011,
      "step": 393
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.9932770729064941,
      "learning_rate": 0.001684087413578173,
      "loss": 1.8166,
      "step": 394
    },
    {
      "epoch": 0.79,
      "grad_norm": 2.6222524642944336,
      "learning_rate": 0.0016825531432186542,
      "loss": 1.7034,
      "step": 395
    },
    {
      "epoch": 0.792,
      "grad_norm": 1.5862082242965698,
      "learning_rate": 0.0016810158587867972,
      "loss": 1.6519,
      "step": 396
    },
    {
      "epoch": 0.794,
      "grad_norm": 1.0093141794204712,
      "learning_rate": 0.001679475567071065,
      "loss": 1.8318,
      "step": 397
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.8446143269538879,
      "learning_rate": 0.0016779322748731995,
      "loss": 1.7538,
      "step": 398
    },
    {
      "epoch": 0.798,
      "grad_norm": 1.2604455947875977,
      "learning_rate": 0.001676385989008193,
      "loss": 1.6276,
      "step": 399
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.107374668121338,
      "learning_rate": 0.0016748367163042577,
      "loss": 1.7125,
      "step": 400
    },
    {
      "epoch": 0.8,
      "eval_loss": 1.6870700120925903,
      "eval_runtime": 214.3754,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 400
    },
    {
      "epoch": 0.802,
      "grad_norm": 0.9980140924453735,
      "learning_rate": 0.0016732844636027945,
      "loss": 1.7083,
      "step": 401
    },
    {
      "epoch": 0.804,
      "grad_norm": 1.0976005792617798,
      "learning_rate": 0.0016717292377583647,
      "loss": 1.6384,
      "step": 402
    },
    {
      "epoch": 0.806,
      "grad_norm": 0.840270459651947,
      "learning_rate": 0.0016701710456386572,
      "loss": 1.5619,
      "step": 403
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.4119371175765991,
      "learning_rate": 0.0016686098941244613,
      "loss": 1.7134,
      "step": 404
    },
    {
      "epoch": 0.81,
      "grad_norm": 1.0908564329147339,
      "learning_rate": 0.0016670457901096327,
      "loss": 1.6122,
      "step": 405
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.7185947299003601,
      "learning_rate": 0.001665478740501067,
      "loss": 1.6279,
      "step": 406
    },
    {
      "epoch": 0.814,
      "grad_norm": 1.0175795555114746,
      "learning_rate": 0.0016639087522186658,
      "loss": 1.731,
      "step": 407
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.622015655040741,
      "learning_rate": 0.0016623358321953077,
      "loss": 1.6275,
      "step": 408
    },
    {
      "epoch": 0.818,
      "grad_norm": 2.3017263412475586,
      "learning_rate": 0.0016607599873768183,
      "loss": 1.6204,
      "step": 409
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.6710883975028992,
      "learning_rate": 0.0016591812247219377,
      "loss": 1.5506,
      "step": 410
    },
    {
      "epoch": 0.822,
      "grad_norm": 1.627700686454773,
      "learning_rate": 0.0016575995512022922,
      "loss": 1.6255,
      "step": 411
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.7355836629867554,
      "learning_rate": 0.00165601497380236,
      "loss": 1.6399,
      "step": 412
    },
    {
      "epoch": 0.826,
      "grad_norm": 2.001927375793457,
      "learning_rate": 0.0016544274995194447,
      "loss": 1.7612,
      "step": 413
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.0764375925064087,
      "learning_rate": 0.0016528371353636408,
      "loss": 1.7356,
      "step": 414
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.4450421333312988,
      "learning_rate": 0.0016512438883578046,
      "loss": 1.676,
      "step": 415
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.6432273387908936,
      "learning_rate": 0.0016496477655375227,
      "loss": 1.636,
      "step": 416
    },
    {
      "epoch": 0.834,
      "grad_norm": 1.0768322944641113,
      "learning_rate": 0.0016480487739510808,
      "loss": 1.5854,
      "step": 417
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.8151595592498779,
      "learning_rate": 0.0016464469206594332,
      "loss": 1.4546,
      "step": 418
    },
    {
      "epoch": 0.838,
      "grad_norm": 0.9124859571456909,
      "learning_rate": 0.0016448422127361706,
      "loss": 1.6126,
      "step": 419
    },
    {
      "epoch": 0.84,
      "grad_norm": 6.529543876647949,
      "learning_rate": 0.0016432346572674897,
      "loss": 1.93,
      "step": 420
    },
    {
      "epoch": 0.842,
      "grad_norm": 0.8101893067359924,
      "learning_rate": 0.001641624261352161,
      "loss": 1.5545,
      "step": 421
    },
    {
      "epoch": 0.844,
      "grad_norm": 1.8469046354293823,
      "learning_rate": 0.0016400110321014992,
      "loss": 1.69,
      "step": 422
    },
    {
      "epoch": 0.846,
      "grad_norm": 0.8216381669044495,
      "learning_rate": 0.00163839497663933,
      "loss": 1.6131,
      "step": 423
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.7511263489723206,
      "learning_rate": 0.001636776102101959,
      "loss": 1.6033,
      "step": 424
    },
    {
      "epoch": 0.85,
      "grad_norm": 2.3163321018218994,
      "learning_rate": 0.0016351544156381413,
      "loss": 1.7244,
      "step": 425
    },
    {
      "epoch": 0.852,
      "grad_norm": 1.5037988424301147,
      "learning_rate": 0.0016335299244090479,
      "loss": 1.8275,
      "step": 426
    },
    {
      "epoch": 0.854,
      "grad_norm": 1.2051609754562378,
      "learning_rate": 0.001631902635588237,
      "loss": 1.6854,
      "step": 427
    },
    {
      "epoch": 0.856,
      "grad_norm": 1.697174310684204,
      "learning_rate": 0.0016302725563616192,
      "loss": 1.6551,
      "step": 428
    },
    {
      "epoch": 0.858,
      "grad_norm": 0.8688215613365173,
      "learning_rate": 0.001628639693927428,
      "loss": 1.5874,
      "step": 429
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.0057523250579834,
      "learning_rate": 0.0016270040554961867,
      "loss": 1.5663,
      "step": 430
    },
    {
      "epoch": 0.862,
      "grad_norm": 2.0619215965270996,
      "learning_rate": 0.0016253656482906776,
      "loss": 1.7617,
      "step": 431
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.1441365480422974,
      "learning_rate": 0.0016237244795459084,
      "loss": 1.7541,
      "step": 432
    },
    {
      "epoch": 0.866,
      "grad_norm": 1.4916423559188843,
      "learning_rate": 0.0016220805565090837,
      "loss": 1.7173,
      "step": 433
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.615585446357727,
      "learning_rate": 0.0016204338864395681,
      "loss": 1.4905,
      "step": 434
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.9163062572479248,
      "learning_rate": 0.0016187844766088583,
      "loss": 1.7224,
      "step": 435
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.127387523651123,
      "learning_rate": 0.0016171323343005498,
      "loss": 1.7009,
      "step": 436
    },
    {
      "epoch": 0.874,
      "grad_norm": 1.0791295766830444,
      "learning_rate": 0.0016154774668103028,
      "loss": 1.8419,
      "step": 437
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.912263035774231,
      "learning_rate": 0.0016138198814458127,
      "loss": 1.8933,
      "step": 438
    },
    {
      "epoch": 0.878,
      "grad_norm": 1.158734679222107,
      "learning_rate": 0.0016121595855267765,
      "loss": 1.658,
      "step": 439
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.345826506614685,
      "learning_rate": 0.0016104965863848616,
      "loss": 1.7477,
      "step": 440
    },
    {
      "epoch": 0.882,
      "grad_norm": 1.1859692335128784,
      "learning_rate": 0.0016088308913636703,
      "loss": 1.6299,
      "step": 441
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.8147404789924622,
      "learning_rate": 0.0016071625078187112,
      "loss": 1.6201,
      "step": 442
    },
    {
      "epoch": 0.886,
      "grad_norm": 1.1995457410812378,
      "learning_rate": 0.0016054914431173652,
      "loss": 1.7564,
      "step": 443
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.6705223917961121,
      "learning_rate": 0.0016038177046388523,
      "loss": 1.6002,
      "step": 444
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.8955385684967041,
      "learning_rate": 0.0016021412997741992,
      "loss": 1.6455,
      "step": 445
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.8938408493995667,
      "learning_rate": 0.0016004622359262085,
      "loss": 1.6318,
      "step": 446
    },
    {
      "epoch": 0.894,
      "grad_norm": 0.9655106663703918,
      "learning_rate": 0.0015987805205094226,
      "loss": 1.6183,
      "step": 447
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.5059157609939575,
      "learning_rate": 0.0015970961609500945,
      "loss": 1.6875,
      "step": 448
    },
    {
      "epoch": 0.898,
      "grad_norm": 1.1567450761795044,
      "learning_rate": 0.0015954091646861524,
      "loss": 1.7872,
      "step": 449
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.5212448835372925,
      "learning_rate": 0.0015937195391671688,
      "loss": 1.4988,
      "step": 450
    },
    {
      "epoch": 0.902,
      "grad_norm": 0.6901001930236816,
      "learning_rate": 0.0015920272918543256,
      "loss": 1.4974,
      "step": 451
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.9730527997016907,
      "learning_rate": 0.0015903324302203835,
      "loss": 1.701,
      "step": 452
    },
    {
      "epoch": 0.906,
      "grad_norm": 1.233077883720398,
      "learning_rate": 0.001588634961749646,
      "loss": 1.6632,
      "step": 453
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.9076414704322815,
      "learning_rate": 0.0015869348939379303,
      "loss": 1.7176,
      "step": 454
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.7884234189987183,
      "learning_rate": 0.0015852322342925294,
      "loss": 1.6069,
      "step": 455
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.8357170820236206,
      "learning_rate": 0.0015835269903321841,
      "loss": 1.5761,
      "step": 456
    },
    {
      "epoch": 0.914,
      "grad_norm": 1.3029191493988037,
      "learning_rate": 0.0015818191695870453,
      "loss": 1.596,
      "step": 457
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.6854164004325867,
      "learning_rate": 0.0015801087795986437,
      "loss": 1.6264,
      "step": 458
    },
    {
      "epoch": 0.918,
      "grad_norm": 0.7721836566925049,
      "learning_rate": 0.0015783958279198549,
      "loss": 1.6502,
      "step": 459
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.0228633880615234,
      "learning_rate": 0.0015766803221148673,
      "loss": 1.5197,
      "step": 460
    },
    {
      "epoch": 0.922,
      "grad_norm": 1.3726686239242554,
      "learning_rate": 0.001574962269759147,
      "loss": 1.664,
      "step": 461
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.7643293738365173,
      "learning_rate": 0.0015732416784394064,
      "loss": 1.5809,
      "step": 462
    },
    {
      "epoch": 0.926,
      "grad_norm": 1.9945961236953735,
      "learning_rate": 0.0015715185557535689,
      "loss": 1.6394,
      "step": 463
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.0209664106369019,
      "learning_rate": 0.0015697929093107365,
      "loss": 1.622,
      "step": 464
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.9324619770050049,
      "learning_rate": 0.0015680647467311557,
      "loss": 1.5666,
      "step": 465
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.6060009598731995,
      "learning_rate": 0.0015663340756461844,
      "loss": 1.5431,
      "step": 466
    },
    {
      "epoch": 0.934,
      "grad_norm": 0.886229932308197,
      "learning_rate": 0.0015646009036982566,
      "loss": 1.62,
      "step": 467
    },
    {
      "epoch": 0.936,
      "grad_norm": 1.2148733139038086,
      "learning_rate": 0.0015628652385408508,
      "loss": 1.5792,
      "step": 468
    },
    {
      "epoch": 0.938,
      "grad_norm": 0.8120684623718262,
      "learning_rate": 0.001561127087838455,
      "loss": 1.5737,
      "step": 469
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.3758594989776611,
      "learning_rate": 0.0015593864592665333,
      "loss": 1.7367,
      "step": 470
    },
    {
      "epoch": 0.942,
      "grad_norm": 0.6993634104728699,
      "learning_rate": 0.001557643360511491,
      "loss": 1.6344,
      "step": 471
    },
    {
      "epoch": 0.944,
      "grad_norm": 1.1151020526885986,
      "learning_rate": 0.0015558977992706424,
      "loss": 1.7954,
      "step": 472
    },
    {
      "epoch": 0.946,
      "grad_norm": 0.7481717467308044,
      "learning_rate": 0.001554149783252175,
      "loss": 1.5323,
      "step": 473
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.5172485709190369,
      "learning_rate": 0.0015523993201751166,
      "loss": 1.4871,
      "step": 474
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.779988169670105,
      "learning_rate": 0.0015506464177693008,
      "loss": 1.6718,
      "step": 475
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.4168262183666229,
      "learning_rate": 0.001548891083775334,
      "loss": 1.531,
      "step": 476
    },
    {
      "epoch": 0.954,
      "grad_norm": 1.2350027561187744,
      "learning_rate": 0.001547133325944559,
      "loss": 1.7321,
      "step": 477
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.724673867225647,
      "learning_rate": 0.0015453731520390214,
      "loss": 1.5948,
      "step": 478
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.5346187949180603,
      "learning_rate": 0.0015436105698314383,
      "loss": 1.472,
      "step": 479
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.4682496786117554,
      "learning_rate": 0.001541845587105159,
      "loss": 1.6441,
      "step": 480
    },
    {
      "epoch": 0.962,
      "grad_norm": 2.213994264602661,
      "learning_rate": 0.001540078211654135,
      "loss": 1.6948,
      "step": 481
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.8542896509170532,
      "learning_rate": 0.0015383084512828825,
      "loss": 1.5571,
      "step": 482
    },
    {
      "epoch": 0.966,
      "grad_norm": 2.3443305492401123,
      "learning_rate": 0.0015365363138064498,
      "loss": 1.7269,
      "step": 483
    },
    {
      "epoch": 0.968,
      "grad_norm": 1.587734341621399,
      "learning_rate": 0.0015347618070503826,
      "loss": 1.6466,
      "step": 484
    },
    {
      "epoch": 0.97,
      "grad_norm": 2.913365364074707,
      "learning_rate": 0.0015329849388506886,
      "loss": 1.6634,
      "step": 485
    },
    {
      "epoch": 0.972,
      "grad_norm": 3.4718799591064453,
      "learning_rate": 0.0015312057170538034,
      "loss": 1.5876,
      "step": 486
    },
    {
      "epoch": 0.974,
      "grad_norm": 1.608209252357483,
      "learning_rate": 0.0015294241495165558,
      "loss": 1.6582,
      "step": 487
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.3516077995300293,
      "learning_rate": 0.0015276402441061327,
      "loss": 1.6834,
      "step": 488
    },
    {
      "epoch": 0.978,
      "grad_norm": 1.17630136013031,
      "learning_rate": 0.0015258540087000458,
      "loss": 1.6876,
      "step": 489
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.838168740272522,
      "learning_rate": 0.001524065451186095,
      "loss": 1.6304,
      "step": 490
    },
    {
      "epoch": 0.982,
      "grad_norm": 1.3479702472686768,
      "learning_rate": 0.0015222745794623341,
      "loss": 1.5602,
      "step": 491
    },
    {
      "epoch": 0.984,
      "grad_norm": 1.3308022022247314,
      "learning_rate": 0.0015204814014370372,
      "loss": 1.7265,
      "step": 492
    },
    {
      "epoch": 0.986,
      "grad_norm": 1.4333711862564087,
      "learning_rate": 0.0015186859250286616,
      "loss": 1.5488,
      "step": 493
    },
    {
      "epoch": 0.988,
      "grad_norm": 1.618591547012329,
      "learning_rate": 0.0015168881581658147,
      "loss": 1.5633,
      "step": 494
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.9249601364135742,
      "learning_rate": 0.0015150881087872183,
      "loss": 1.577,
      "step": 495
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.8206844925880432,
      "learning_rate": 0.0015132857848416733,
      "loss": 1.4645,
      "step": 496
    },
    {
      "epoch": 0.994,
      "grad_norm": 1.3637114763259888,
      "learning_rate": 0.0015114811942880243,
      "loss": 1.7958,
      "step": 497
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.790530264377594,
      "learning_rate": 0.0015096743450951258,
      "loss": 1.5936,
      "step": 498
    },
    {
      "epoch": 0.998,
      "grad_norm": 1.008440613746643,
      "learning_rate": 0.0015078652452418062,
      "loss": 1.6055,
      "step": 499
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7593508958816528,
      "learning_rate": 0.0015060539027168317,
      "loss": 1.4607,
      "step": 500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.6271374225616455,
      "eval_runtime": 214.3874,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 500
    },
    {
      "epoch": 1.002,
      "grad_norm": 1.205501914024353,
      "learning_rate": 0.0015042403255188723,
      "loss": 1.5722,
      "step": 501
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.634405255317688,
      "learning_rate": 0.0015024245216564668,
      "loss": 1.601,
      "step": 502
    },
    {
      "epoch": 1.006,
      "grad_norm": 0.9298604130744934,
      "learning_rate": 0.0015006064991479853,
      "loss": 1.582,
      "step": 503
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.8196508288383484,
      "learning_rate": 0.0014987862660215965,
      "loss": 1.56,
      "step": 504
    },
    {
      "epoch": 1.01,
      "grad_norm": 1.0075864791870117,
      "learning_rate": 0.0014969638303152296,
      "loss": 1.6164,
      "step": 505
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.7396460175514221,
      "learning_rate": 0.0014951392000765412,
      "loss": 1.5136,
      "step": 506
    },
    {
      "epoch": 1.014,
      "grad_norm": 0.7073444724082947,
      "learning_rate": 0.0014933123833628787,
      "loss": 1.5841,
      "step": 507
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.8354134559631348,
      "learning_rate": 0.0014914833882412432,
      "loss": 1.4263,
      "step": 508
    },
    {
      "epoch": 1.018,
      "grad_norm": 0.6901289224624634,
      "learning_rate": 0.0014896522227882578,
      "loss": 1.526,
      "step": 509
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.7296534776687622,
      "learning_rate": 0.0014878188950901274,
      "loss": 1.5687,
      "step": 510
    },
    {
      "epoch": 1.022,
      "grad_norm": 1.2748843431472778,
      "learning_rate": 0.001485983413242606,
      "loss": 1.6422,
      "step": 511
    },
    {
      "epoch": 1.024,
      "grad_norm": 1.0742613077163696,
      "learning_rate": 0.0014841457853509606,
      "loss": 1.8038,
      "step": 512
    },
    {
      "epoch": 1.026,
      "grad_norm": 0.6803113222122192,
      "learning_rate": 0.0014823060195299335,
      "loss": 1.6663,
      "step": 513
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.6649814248085022,
      "learning_rate": 0.0014804641239037095,
      "loss": 1.5097,
      "step": 514
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.738261342048645,
      "learning_rate": 0.0014786201066058766,
      "loss": 1.6313,
      "step": 515
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.48604682087898254,
      "learning_rate": 0.001476773975779393,
      "loss": 1.5008,
      "step": 516
    },
    {
      "epoch": 1.034,
      "grad_norm": 0.6591385006904602,
      "learning_rate": 0.0014749257395765502,
      "loss": 1.6775,
      "step": 517
    },
    {
      "epoch": 1.036,
      "grad_norm": 1.7361490726470947,
      "learning_rate": 0.0014730754061589356,
      "loss": 1.5251,
      "step": 518
    },
    {
      "epoch": 1.038,
      "grad_norm": 1.619252324104309,
      "learning_rate": 0.0014712229836973986,
      "loss": 1.5712,
      "step": 519
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.7671758532524109,
      "learning_rate": 0.0014693684803720138,
      "loss": 1.4464,
      "step": 520
    },
    {
      "epoch": 1.042,
      "grad_norm": 2.795330762863159,
      "learning_rate": 0.0014675119043720435,
      "loss": 1.591,
      "step": 521
    },
    {
      "epoch": 1.044,
      "grad_norm": 1.3196300268173218,
      "learning_rate": 0.0014656532638959035,
      "loss": 1.5629,
      "step": 522
    },
    {
      "epoch": 1.046,
      "grad_norm": 4.266744613647461,
      "learning_rate": 0.001463792567151126,
      "loss": 1.8354,
      "step": 523
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.5281530618667603,
      "learning_rate": 0.0014619298223543236,
      "loss": 1.5498,
      "step": 524
    },
    {
      "epoch": 1.05,
      "grad_norm": 1.5498563051223755,
      "learning_rate": 0.0014600650377311522,
      "loss": 1.6483,
      "step": 525
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.7990263104438782,
      "learning_rate": 0.001458198221516276,
      "loss": 1.4867,
      "step": 526
    },
    {
      "epoch": 1.054,
      "grad_norm": 0.5594714879989624,
      "learning_rate": 0.0014563293819533298,
      "loss": 1.4876,
      "step": 527
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.9182326793670654,
      "learning_rate": 0.0014544585272948842,
      "loss": 1.5092,
      "step": 528
    },
    {
      "epoch": 1.058,
      "grad_norm": 0.8903492093086243,
      "learning_rate": 0.0014525856658024075,
      "loss": 1.5639,
      "step": 529
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.6092816591262817,
      "learning_rate": 0.0014507108057462297,
      "loss": 1.7153,
      "step": 530
    },
    {
      "epoch": 1.062,
      "grad_norm": 0.7232649326324463,
      "learning_rate": 0.0014488339554055072,
      "loss": 1.4633,
      "step": 531
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.3952579498291016,
      "learning_rate": 0.0014469551230681843,
      "loss": 1.5419,
      "step": 532
    },
    {
      "epoch": 1.066,
      "grad_norm": 1.0832993984222412,
      "learning_rate": 0.0014450743170309583,
      "loss": 1.5867,
      "step": 533
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.44389602541923523,
      "learning_rate": 0.0014431915455992415,
      "loss": 1.5219,
      "step": 534
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.8583162426948547,
      "learning_rate": 0.001441306817087125,
      "loss": 1.4884,
      "step": 535
    },
    {
      "epoch": 1.072,
      "grad_norm": 4.769372940063477,
      "learning_rate": 0.0014394201398173437,
      "loss": 1.5344,
      "step": 536
    },
    {
      "epoch": 1.074,
      "grad_norm": 0.8283169269561768,
      "learning_rate": 0.0014375315221212357,
      "loss": 1.5141,
      "step": 537
    },
    {
      "epoch": 1.076,
      "grad_norm": 1.0631530284881592,
      "learning_rate": 0.001435640972338709,
      "loss": 1.4876,
      "step": 538
    },
    {
      "epoch": 1.078,
      "grad_norm": 0.5937294960021973,
      "learning_rate": 0.001433748498818204,
      "loss": 1.4777,
      "step": 539
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.1033775806427,
      "learning_rate": 0.0014318541099166556,
      "loss": 1.6316,
      "step": 540
    },
    {
      "epoch": 1.082,
      "grad_norm": 0.7797374725341797,
      "learning_rate": 0.0014299578139994557,
      "loss": 1.4406,
      "step": 541
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.7851663827896118,
      "learning_rate": 0.0014280596194404186,
      "loss": 1.5579,
      "step": 542
    },
    {
      "epoch": 1.086,
      "grad_norm": 0.6168704032897949,
      "learning_rate": 0.001426159534621743,
      "loss": 1.63,
      "step": 543
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.7600914239883423,
      "learning_rate": 0.0014242575679339737,
      "loss": 1.5442,
      "step": 544
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.7727041840553284,
      "learning_rate": 0.0014223537277759666,
      "loss": 1.4086,
      "step": 545
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.8336670398712158,
      "learning_rate": 0.0014204480225548494,
      "loss": 1.4938,
      "step": 546
    },
    {
      "epoch": 1.094,
      "grad_norm": 0.7369433045387268,
      "learning_rate": 0.0014185404606859874,
      "loss": 1.5229,
      "step": 547
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.6885262131690979,
      "learning_rate": 0.0014166310505929435,
      "loss": 1.4505,
      "step": 548
    },
    {
      "epoch": 1.098,
      "grad_norm": 0.9393014907836914,
      "learning_rate": 0.0014147198007074416,
      "loss": 1.5442,
      "step": 549
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.6964300870895386,
      "learning_rate": 0.0014128067194693315,
      "loss": 1.5046,
      "step": 550
    },
    {
      "epoch": 1.102,
      "grad_norm": 0.8833647966384888,
      "learning_rate": 0.0014108918153265485,
      "loss": 1.5731,
      "step": 551
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.570319414138794,
      "learning_rate": 0.001408975096735078,
      "loss": 1.4884,
      "step": 552
    },
    {
      "epoch": 1.106,
      "grad_norm": 0.9699033498764038,
      "learning_rate": 0.0014070565721589195,
      "loss": 1.5347,
      "step": 553
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.8320421576499939,
      "learning_rate": 0.0014051362500700447,
      "loss": 1.527,
      "step": 554
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.9120113849639893,
      "learning_rate": 0.0014032141389483648,
      "loss": 1.5519,
      "step": 555
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.8450369238853455,
      "learning_rate": 0.0014012902472816907,
      "loss": 1.5618,
      "step": 556
    },
    {
      "epoch": 1.114,
      "grad_norm": 1.0834895372390747,
      "learning_rate": 0.0013993645835656955,
      "loss": 1.478,
      "step": 557
    },
    {
      "epoch": 1.116,
      "grad_norm": 1.089161992073059,
      "learning_rate": 0.0013974371563038785,
      "loss": 1.5301,
      "step": 558
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 1.2123653888702393,
      "learning_rate": 0.0013955079740075255,
      "loss": 1.6316,
      "step": 559
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.3235057592391968,
      "learning_rate": 0.0013935770451956732,
      "loss": 1.6886,
      "step": 560
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 1.3889553546905518,
      "learning_rate": 0.0013916443783950694,
      "loss": 1.4117,
      "step": 561
    },
    {
      "epoch": 1.124,
      "grad_norm": 1.0884977579116821,
      "learning_rate": 0.0013897099821401384,
      "loss": 1.5831,
      "step": 562
    },
    {
      "epoch": 1.126,
      "grad_norm": 1.734725832939148,
      "learning_rate": 0.0013877738649729406,
      "loss": 1.5397,
      "step": 563
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.051685094833374,
      "learning_rate": 0.0013858360354431355,
      "loss": 1.3951,
      "step": 564
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.7148035764694214,
      "learning_rate": 0.0013838965021079445,
      "loss": 1.4142,
      "step": 565
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 3.925785779953003,
      "learning_rate": 0.0013819552735321134,
      "loss": 1.634,
      "step": 566
    },
    {
      "epoch": 1.134,
      "grad_norm": 2.450209617614746,
      "learning_rate": 0.001380012358287873,
      "loss": 1.6243,
      "step": 567
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.096943974494934,
      "learning_rate": 0.0013780677649549026,
      "loss": 1.608,
      "step": 568
    },
    {
      "epoch": 1.138,
      "grad_norm": 0.9614190459251404,
      "learning_rate": 0.0013761215021202914,
      "loss": 1.4842,
      "step": 569
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.6853218078613281,
      "learning_rate": 0.001374173578378502,
      "loss": 1.5308,
      "step": 570
    },
    {
      "epoch": 1.142,
      "grad_norm": 1.5343223810195923,
      "learning_rate": 0.0013722240023313307,
      "loss": 1.5686,
      "step": 571
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.0183743238449097,
      "learning_rate": 0.0013702727825878694,
      "loss": 1.5182,
      "step": 572
    },
    {
      "epoch": 1.146,
      "grad_norm": 0.9103183150291443,
      "learning_rate": 0.0013683199277644694,
      "loss": 1.4594,
      "step": 573
    },
    {
      "epoch": 1.148,
      "grad_norm": 1.0501893758773804,
      "learning_rate": 0.001366365446484702,
      "loss": 1.523,
      "step": 574
    },
    {
      "epoch": 1.15,
      "grad_norm": 1.0200424194335938,
      "learning_rate": 0.0013644093473793213,
      "loss": 1.5426,
      "step": 575
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.6856241822242737,
      "learning_rate": 0.0013624516390862242,
      "loss": 1.4857,
      "step": 576
    },
    {
      "epoch": 1.154,
      "grad_norm": 1.2282978296279907,
      "learning_rate": 0.0013604923302504147,
      "loss": 1.529,
      "step": 577
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.9312200546264648,
      "learning_rate": 0.0013585314295239644,
      "loss": 1.4829,
      "step": 578
    },
    {
      "epoch": 1.158,
      "grad_norm": 0.766345739364624,
      "learning_rate": 0.0013565689455659737,
      "loss": 1.4366,
      "step": 579
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.4265974760055542,
      "learning_rate": 0.0013546048870425357,
      "loss": 1.5441,
      "step": 580
    },
    {
      "epoch": 1.162,
      "grad_norm": 0.8021628856658936,
      "learning_rate": 0.0013526392626266957,
      "loss": 1.3872,
      "step": 581
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.0302900075912476,
      "learning_rate": 0.0013506720809984137,
      "loss": 1.3975,
      "step": 582
    },
    {
      "epoch": 1.166,
      "grad_norm": 1.0238227844238281,
      "learning_rate": 0.001348703350844527,
      "loss": 1.4352,
      "step": 583
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.8627533316612244,
      "learning_rate": 0.0013467330808587098,
      "loss": 1.4442,
      "step": 584
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.9747784733772278,
      "learning_rate": 0.001344761279741437,
      "loss": 1.479,
      "step": 585
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.8183448314666748,
      "learning_rate": 0.001342787956199945,
      "loss": 1.4692,
      "step": 586
    },
    {
      "epoch": 1.174,
      "grad_norm": 1.4237550497055054,
      "learning_rate": 0.001340813118948191,
      "loss": 1.5126,
      "step": 587
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.9317252039909363,
      "learning_rate": 0.0013388367767068199,
      "loss": 1.5136,
      "step": 588
    },
    {
      "epoch": 1.178,
      "grad_norm": 1.3487240076065063,
      "learning_rate": 0.0013368589382031196,
      "loss": 1.75,
      "step": 589
    },
    {
      "epoch": 1.18,
      "grad_norm": 3.7067391872406006,
      "learning_rate": 0.0013348796121709862,
      "loss": 1.9358,
      "step": 590
    },
    {
      "epoch": 1.182,
      "grad_norm": 1.646085500717163,
      "learning_rate": 0.0013328988073508853,
      "loss": 1.7336,
      "step": 591
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.712536096572876,
      "learning_rate": 0.001330916532489811,
      "loss": 1.4971,
      "step": 592
    },
    {
      "epoch": 1.186,
      "grad_norm": 2.0292530059814453,
      "learning_rate": 0.001328932796341251,
      "loss": 1.5749,
      "step": 593
    },
    {
      "epoch": 1.188,
      "grad_norm": 1.6234846115112305,
      "learning_rate": 0.0013269476076651447,
      "loss": 1.5809,
      "step": 594
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.7909966111183167,
      "learning_rate": 0.0013249609752278453,
      "loss": 1.8087,
      "step": 595
    },
    {
      "epoch": 1.192,
      "grad_norm": 3.1314401626586914,
      "learning_rate": 0.0013229729078020822,
      "loss": 1.713,
      "step": 596
    },
    {
      "epoch": 1.194,
      "grad_norm": 2.025517702102661,
      "learning_rate": 0.0013209834141669212,
      "loss": 1.5573,
      "step": 597
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.7303367853164673,
      "learning_rate": 0.0013189925031077267,
      "loss": 1.5775,
      "step": 598
    },
    {
      "epoch": 1.198,
      "grad_norm": 2.1844327449798584,
      "learning_rate": 0.0013170001834161209,
      "loss": 1.5227,
      "step": 599
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.679616928100586,
      "learning_rate": 0.001315006463889948,
      "loss": 1.4922,
      "step": 600
    },
    {
      "epoch": 1.2,
      "eval_loss": 1.5207263231277466,
      "eval_runtime": 214.4495,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 600
    },
    {
      "epoch": 1.202,
      "grad_norm": 1.8939892053604126,
      "learning_rate": 0.0013130113533332325,
      "loss": 1.44,
      "step": 601
    },
    {
      "epoch": 1.204,
      "grad_norm": 1.3521263599395752,
      "learning_rate": 0.0013110148605561419,
      "loss": 1.5456,
      "step": 602
    },
    {
      "epoch": 1.206,
      "grad_norm": 0.6536515355110168,
      "learning_rate": 0.0013090169943749475,
      "loss": 1.4085,
      "step": 603
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.8049366474151611,
      "learning_rate": 0.0013070177636119854,
      "loss": 1.5414,
      "step": 604
    },
    {
      "epoch": 1.21,
      "grad_norm": 1.0448697805404663,
      "learning_rate": 0.0013050171770956176,
      "loss": 1.553,
      "step": 605
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.6233279705047607,
      "learning_rate": 0.0013030152436601927,
      "loss": 1.4912,
      "step": 606
    },
    {
      "epoch": 1.214,
      "grad_norm": 0.9252703785896301,
      "learning_rate": 0.0013010119721460073,
      "loss": 1.5203,
      "step": 607
    },
    {
      "epoch": 1.216,
      "grad_norm": 1.067465901374817,
      "learning_rate": 0.0012990073713992662,
      "loss": 1.4392,
      "step": 608
    },
    {
      "epoch": 1.218,
      "grad_norm": 0.9960492253303528,
      "learning_rate": 0.0012970014502720452,
      "loss": 1.4184,
      "step": 609
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.838870644569397,
      "learning_rate": 0.0012949942176222495,
      "loss": 1.5207,
      "step": 610
    },
    {
      "epoch": 1.222,
      "grad_norm": 0.6400659680366516,
      "learning_rate": 0.0012929856823135771,
      "loss": 1.4307,
      "step": 611
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.9828018546104431,
      "learning_rate": 0.0012909758532154765,
      "loss": 1.4479,
      "step": 612
    },
    {
      "epoch": 1.226,
      "grad_norm": 0.49979540705680847,
      "learning_rate": 0.0012889647392031111,
      "loss": 1.3569,
      "step": 613
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.9973685145378113,
      "learning_rate": 0.0012869523491573181,
      "loss": 1.338,
      "step": 614
    },
    {
      "epoch": 1.23,
      "grad_norm": 1.107810616493225,
      "learning_rate": 0.0012849386919645686,
      "loss": 1.5736,
      "step": 615
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.7010527849197388,
      "learning_rate": 0.00128292377651693,
      "loss": 1.3293,
      "step": 616
    },
    {
      "epoch": 1.234,
      "grad_norm": 1.5058403015136719,
      "learning_rate": 0.001280907611712026,
      "loss": 1.4498,
      "step": 617
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.0360924005508423,
      "learning_rate": 0.001278890206452997,
      "loss": 1.438,
      "step": 618
    },
    {
      "epoch": 1.238,
      "grad_norm": 2.6615748405456543,
      "learning_rate": 0.0012768715696484616,
      "loss": 1.5642,
      "step": 619
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.7696471214294434,
      "learning_rate": 0.0012748517102124754,
      "loss": 1.7921,
      "step": 620
    },
    {
      "epoch": 1.242,
      "grad_norm": 12.848731994628906,
      "learning_rate": 0.0012728306370644953,
      "loss": 1.5088,
      "step": 621
    },
    {
      "epoch": 1.244,
      "grad_norm": 3.6457509994506836,
      "learning_rate": 0.0012708083591293359,
      "loss": 1.6941,
      "step": 622
    },
    {
      "epoch": 1.246,
      "grad_norm": 2.4187493324279785,
      "learning_rate": 0.0012687848853371322,
      "loss": 1.6919,
      "step": 623
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.1969783306121826,
      "learning_rate": 0.001266760224623301,
      "loss": 1.5346,
      "step": 624
    },
    {
      "epoch": 1.25,
      "grad_norm": 2.176222324371338,
      "learning_rate": 0.0012647343859284997,
      "loss": 1.5846,
      "step": 625
    },
    {
      "epoch": 1.252,
      "grad_norm": 5.262096881866455,
      "learning_rate": 0.0012627073781985869,
      "loss": 1.9709,
      "step": 626
    },
    {
      "epoch": 1.254,
      "grad_norm": 4.360698699951172,
      "learning_rate": 0.001260679210384585,
      "loss": 1.7226,
      "step": 627
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.7103630304336548,
      "learning_rate": 0.0012586498914426382,
      "loss": 1.6307,
      "step": 628
    },
    {
      "epoch": 1.258,
      "grad_norm": 4.928110599517822,
      "learning_rate": 0.0012566194303339738,
      "loss": 1.657,
      "step": 629
    },
    {
      "epoch": 1.26,
      "grad_norm": 5.49196720123291,
      "learning_rate": 0.0012545878360248632,
      "loss": 1.9959,
      "step": 630
    },
    {
      "epoch": 1.262,
      "grad_norm": 2.905571222305298,
      "learning_rate": 0.001252555117486582,
      "loss": 1.8512,
      "step": 631
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.2420706748962402,
      "learning_rate": 0.00125052128369537,
      "loss": 1.6069,
      "step": 632
    },
    {
      "epoch": 1.266,
      "grad_norm": 2.296504259109497,
      "learning_rate": 0.001248486343632392,
      "loss": 1.7063,
      "step": 633
    },
    {
      "epoch": 1.268,
      "grad_norm": 2.008080005645752,
      "learning_rate": 0.0012464503062836975,
      "loss": 1.6525,
      "step": 634
    },
    {
      "epoch": 1.27,
      "grad_norm": 1.0306347608566284,
      "learning_rate": 0.0012444131806401816,
      "loss": 1.5021,
      "step": 635
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.8091539144515991,
      "learning_rate": 0.001242374975697546,
      "loss": 1.6458,
      "step": 636
    },
    {
      "epoch": 1.274,
      "grad_norm": 11.321674346923828,
      "learning_rate": 0.0012403357004562574,
      "loss": 1.8671,
      "step": 637
    },
    {
      "epoch": 1.276,
      "grad_norm": 4.3780646324157715,
      "learning_rate": 0.0012382953639215096,
      "loss": 1.9523,
      "step": 638
    },
    {
      "epoch": 1.278,
      "grad_norm": 2.681748151779175,
      "learning_rate": 0.0012362539751031823,
      "loss": 1.838,
      "step": 639
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.1924892663955688,
      "learning_rate": 0.0012342115430158023,
      "loss": 1.6606,
      "step": 640
    },
    {
      "epoch": 1.282,
      "grad_norm": 0.9641377329826355,
      "learning_rate": 0.0012321680766785035,
      "loss": 1.4663,
      "step": 641
    },
    {
      "epoch": 1.284,
      "grad_norm": 2.9504504203796387,
      "learning_rate": 0.0012301235851149865,
      "loss": 1.684,
      "step": 642
    },
    {
      "epoch": 1.286,
      "grad_norm": 3.340707302093506,
      "learning_rate": 0.0012280780773534794,
      "loss": 1.5573,
      "step": 643
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.498490571975708,
      "learning_rate": 0.001226031562426698,
      "loss": 1.5673,
      "step": 644
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.9991964101791382,
      "learning_rate": 0.0012239840493718048,
      "loss": 1.4699,
      "step": 645
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.933310329914093,
      "learning_rate": 0.001221935547230371,
      "loss": 1.6146,
      "step": 646
    },
    {
      "epoch": 1.294,
      "grad_norm": 0.9594003558158875,
      "learning_rate": 0.0012198860650483344,
      "loss": 1.634,
      "step": 647
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.7746263146400452,
      "learning_rate": 0.0012178356118759618,
      "loss": 1.6222,
      "step": 648
    },
    {
      "epoch": 1.298,
      "grad_norm": 0.625359833240509,
      "learning_rate": 0.0012157841967678062,
      "loss": 1.4413,
      "step": 649
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.1077287197113037,
      "learning_rate": 0.0012137318287826697,
      "loss": 1.4771,
      "step": 650
    },
    {
      "epoch": 1.302,
      "grad_norm": 1.7371857166290283,
      "learning_rate": 0.0012116785169835617,
      "loss": 1.3868,
      "step": 651
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.9401227831840515,
      "learning_rate": 0.0012096242704376598,
      "loss": 1.6386,
      "step": 652
    },
    {
      "epoch": 1.306,
      "grad_norm": 0.869844913482666,
      "learning_rate": 0.0012075690982162677,
      "loss": 1.5166,
      "step": 653
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.9565454721450806,
      "learning_rate": 0.001205513009394779,
      "loss": 1.4702,
      "step": 654
    },
    {
      "epoch": 1.31,
      "grad_norm": 1.1212310791015625,
      "learning_rate": 0.001203456013052634,
      "loss": 1.5127,
      "step": 655
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.8388131260871887,
      "learning_rate": 0.0012013981182732796,
      "loss": 1.3669,
      "step": 656
    },
    {
      "epoch": 1.314,
      "grad_norm": 0.7918674945831299,
      "learning_rate": 0.0011993393341441319,
      "loss": 1.4991,
      "step": 657
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.0832988023757935,
      "learning_rate": 0.0011972796697565322,
      "loss": 1.4449,
      "step": 658
    },
    {
      "epoch": 1.318,
      "grad_norm": 0.9111741185188293,
      "learning_rate": 0.0011952191342057103,
      "loss": 1.5338,
      "step": 659
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.3830983638763428,
      "learning_rate": 0.0011931577365907433,
      "loss": 1.5942,
      "step": 660
    },
    {
      "epoch": 1.322,
      "grad_norm": 1.7621819972991943,
      "learning_rate": 0.0011910954860145137,
      "loss": 1.5423,
      "step": 661
    },
    {
      "epoch": 1.324,
      "grad_norm": 1.515956163406372,
      "learning_rate": 0.0011890323915836713,
      "loss": 1.5662,
      "step": 662
    },
    {
      "epoch": 1.326,
      "grad_norm": 1.328957438468933,
      "learning_rate": 0.0011869684624085924,
      "loss": 1.5498,
      "step": 663
    },
    {
      "epoch": 1.328,
      "grad_norm": 2.0307154655456543,
      "learning_rate": 0.001184903707603339,
      "loss": 1.6181,
      "step": 664
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.9863181710243225,
      "learning_rate": 0.0011828381362856196,
      "loss": 1.6577,
      "step": 665
    },
    {
      "epoch": 1.332,
      "grad_norm": 1.3236749172210693,
      "learning_rate": 0.0011807717575767474,
      "loss": 1.4079,
      "step": 666
    },
    {
      "epoch": 1.334,
      "grad_norm": 0.9640764594078064,
      "learning_rate": 0.001178704580601602,
      "loss": 1.4859,
      "step": 667
    },
    {
      "epoch": 1.336,
      "grad_norm": 4.211453914642334,
      "learning_rate": 0.0011766366144885876,
      "loss": 1.5344,
      "step": 668
    },
    {
      "epoch": 1.338,
      "grad_norm": 1.9062633514404297,
      "learning_rate": 0.0011745678683695927,
      "loss": 1.6305,
      "step": 669
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.306708812713623,
      "learning_rate": 0.0011724983513799504,
      "loss": 1.5094,
      "step": 670
    },
    {
      "epoch": 1.342,
      "grad_norm": 0.7347769737243652,
      "learning_rate": 0.0011704280726583989,
      "loss": 1.3929,
      "step": 671
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.8568170666694641,
      "learning_rate": 0.0011683570413470383,
      "loss": 1.516,
      "step": 672
    },
    {
      "epoch": 1.346,
      "grad_norm": 21.6617374420166,
      "learning_rate": 0.0011662852665912942,
      "loss": 1.3881,
      "step": 673
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 2.2507829666137695,
      "learning_rate": 0.0011642127575398728,
      "loss": 1.5245,
      "step": 674
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.010426640510559,
      "learning_rate": 0.0011621395233447247,
      "loss": 1.573,
      "step": 675
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.6868935227394104,
      "learning_rate": 0.001160065573161002,
      "loss": 1.3855,
      "step": 676
    },
    {
      "epoch": 1.354,
      "grad_norm": 0.9738574028015137,
      "learning_rate": 0.001157990916147018,
      "loss": 1.4052,
      "step": 677
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.9656386375427246,
      "learning_rate": 0.0011559155614642082,
      "loss": 1.5112,
      "step": 678
    },
    {
      "epoch": 1.358,
      "grad_norm": 1.3561025857925415,
      "learning_rate": 0.0011538395182770886,
      "loss": 1.6838,
      "step": 679
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.7986552119255066,
      "learning_rate": 0.0011517627957532152,
      "loss": 1.5018,
      "step": 680
    },
    {
      "epoch": 1.362,
      "grad_norm": 1.0794037580490112,
      "learning_rate": 0.0011496854030631444,
      "loss": 1.5597,
      "step": 681
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.7846861481666565,
      "learning_rate": 0.0011476073493803913,
      "loss": 1.4139,
      "step": 682
    },
    {
      "epoch": 1.366,
      "grad_norm": 1.4470276832580566,
      "learning_rate": 0.0011455286438813907,
      "loss": 1.6748,
      "step": 683
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.4817890226840973,
      "learning_rate": 0.001143449295745455,
      "loss": 1.436,
      "step": 684
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.9955976009368896,
      "learning_rate": 0.0011413693141547351,
      "loss": 1.4622,
      "step": 685
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.7798761129379272,
      "learning_rate": 0.001139288708294178,
      "loss": 1.444,
      "step": 686
    },
    {
      "epoch": 1.374,
      "grad_norm": 1.641672134399414,
      "learning_rate": 0.0011372074873514893,
      "loss": 1.4551,
      "step": 687
    },
    {
      "epoch": 1.376,
      "grad_norm": 1.1990059614181519,
      "learning_rate": 0.0011351256605170886,
      "loss": 1.4465,
      "step": 688
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 2.000030040740967,
      "learning_rate": 0.0011330432369840726,
      "loss": 1.4691,
      "step": 689
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.8572368621826172,
      "learning_rate": 0.0011309602259481726,
      "loss": 1.5367,
      "step": 690
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 0.9438456296920776,
      "learning_rate": 0.001128876636607713,
      "loss": 1.5388,
      "step": 691
    },
    {
      "epoch": 1.384,
      "grad_norm": 77.23890686035156,
      "learning_rate": 0.0011267924781635742,
      "loss": 1.5305,
      "step": 692
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 2.959594488143921,
      "learning_rate": 0.0011247077598191479,
      "loss": 1.6947,
      "step": 693
    },
    {
      "epoch": 1.388,
      "grad_norm": 2.411048173904419,
      "learning_rate": 0.0011226224907802983,
      "loss": 1.5116,
      "step": 694
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 1.988042950630188,
      "learning_rate": 0.0011205366802553229,
      "loss": 1.4429,
      "step": 695
    },
    {
      "epoch": 1.392,
      "grad_norm": 2.053224563598633,
      "learning_rate": 0.001118450337454909,
      "loss": 1.4356,
      "step": 696
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 21.536684036254883,
      "learning_rate": 0.0011163634715920946,
      "loss": 1.7551,
      "step": 697
    },
    {
      "epoch": 1.396,
      "grad_norm": 30.567840576171875,
      "learning_rate": 0.0011142760918822275,
      "loss": 1.5435,
      "step": 698
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 1.7349222898483276,
      "learning_rate": 0.0011121882075429248,
      "loss": 1.6761,
      "step": 699
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.095776915550232,
      "learning_rate": 0.0011100998277940315,
      "loss": 1.5178,
      "step": 700
    },
    {
      "epoch": 1.4,
      "eval_loss": 1.5696918964385986,
      "eval_runtime": 214.4088,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 700
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 0.8140184283256531,
      "learning_rate": 0.0011080109618575816,
      "loss": 1.4909,
      "step": 701
    },
    {
      "epoch": 1.404,
      "grad_norm": 2.4946422576904297,
      "learning_rate": 0.001105921618957754,
      "loss": 1.4999,
      "step": 702
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 0.7247132658958435,
      "learning_rate": 0.0011038318083208354,
      "loss": 1.4534,
      "step": 703
    },
    {
      "epoch": 1.408,
      "grad_norm": 1.0901159048080444,
      "learning_rate": 0.0011017415391751774,
      "loss": 1.6057,
      "step": 704
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.8720489740371704,
      "learning_rate": 0.0010996508207511565,
      "loss": 1.628,
      "step": 705
    },
    {
      "epoch": 1.412,
      "grad_norm": 2.2418439388275146,
      "learning_rate": 0.001097559662281133,
      "loss": 1.5612,
      "step": 706
    },
    {
      "epoch": 1.414,
      "grad_norm": 1.5775916576385498,
      "learning_rate": 0.0010954680729994102,
      "loss": 1.6154,
      "step": 707
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.5391318798065186,
      "learning_rate": 0.0010933760621421942,
      "loss": 1.5782,
      "step": 708
    },
    {
      "epoch": 1.418,
      "grad_norm": 1.1897944211959839,
      "learning_rate": 0.0010912836389475526,
      "loss": 1.6005,
      "step": 709
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.0130244493484497,
      "learning_rate": 0.0010891908126553738,
      "loss": 1.4584,
      "step": 710
    },
    {
      "epoch": 1.422,
      "grad_norm": 0.7512016892433167,
      "learning_rate": 0.0010870975925073262,
      "loss": 1.5697,
      "step": 711
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.7540568709373474,
      "learning_rate": 0.0010850039877468172,
      "loss": 1.4577,
      "step": 712
    },
    {
      "epoch": 1.426,
      "grad_norm": 1.2762846946716309,
      "learning_rate": 0.0010829100076189533,
      "loss": 1.5599,
      "step": 713
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.7124900817871094,
      "learning_rate": 0.0010808156613704978,
      "loss": 1.4744,
      "step": 714
    },
    {
      "epoch": 1.43,
      "grad_norm": 1.1814709901809692,
      "learning_rate": 0.0010787209582498315,
      "loss": 1.5389,
      "step": 715
    },
    {
      "epoch": 1.432,
      "grad_norm": 25.760520935058594,
      "learning_rate": 0.00107662590750691,
      "loss": 1.7403,
      "step": 716
    },
    {
      "epoch": 1.434,
      "grad_norm": 10.139492988586426,
      "learning_rate": 0.0010745305183932252,
      "loss": 1.7343,
      "step": 717
    },
    {
      "epoch": 1.436,
      "grad_norm": 6.487475872039795,
      "learning_rate": 0.0010724348001617625,
      "loss": 1.6695,
      "step": 718
    },
    {
      "epoch": 1.438,
      "grad_norm": 2.0771548748016357,
      "learning_rate": 0.0010703387620669606,
      "loss": 1.5628,
      "step": 719
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.416749358177185,
      "learning_rate": 0.0010682424133646711,
      "loss": 1.648,
      "step": 720
    },
    {
      "epoch": 1.442,
      "grad_norm": 2.5810751914978027,
      "learning_rate": 0.0010661457633121168,
      "loss": 1.6992,
      "step": 721
    },
    {
      "epoch": 1.444,
      "grad_norm": 3.5189168453216553,
      "learning_rate": 0.0010640488211678513,
      "loss": 1.4564,
      "step": 722
    },
    {
      "epoch": 1.446,
      "grad_norm": 1.1717554330825806,
      "learning_rate": 0.0010619515961917186,
      "loss": 1.6169,
      "step": 723
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.2007709741592407,
      "learning_rate": 0.0010598540976448107,
      "loss": 1.6097,
      "step": 724
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.4806538820266724,
      "learning_rate": 0.0010577563347894286,
      "loss": 1.4878,
      "step": 725
    },
    {
      "epoch": 1.452,
      "grad_norm": 4.5568318367004395,
      "learning_rate": 0.0010556583168890396,
      "loss": 1.4329,
      "step": 726
    },
    {
      "epoch": 1.454,
      "grad_norm": 1.7754147052764893,
      "learning_rate": 0.0010535600532082373,
      "loss": 1.5352,
      "step": 727
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.9842751026153564,
      "learning_rate": 0.001051461553012702,
      "loss": 1.6694,
      "step": 728
    },
    {
      "epoch": 1.458,
      "grad_norm": 6.4055280685424805,
      "learning_rate": 0.001049362825569157,
      "loss": 1.5307,
      "step": 729
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.5879045724868774,
      "learning_rate": 0.0010472638801453287,
      "loss": 1.5526,
      "step": 730
    },
    {
      "epoch": 1.462,
      "grad_norm": 1.3526092767715454,
      "learning_rate": 0.0010451647260099081,
      "loss": 1.4754,
      "step": 731
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.152818202972412,
      "learning_rate": 0.0010430653724325058,
      "loss": 1.57,
      "step": 732
    },
    {
      "epoch": 1.466,
      "grad_norm": 1.1795146465301514,
      "learning_rate": 0.0010409658286836144,
      "loss": 1.6064,
      "step": 733
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.7354073524475098,
      "learning_rate": 0.0010388661040345655,
      "loss": 1.4352,
      "step": 734
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.8395440578460693,
      "learning_rate": 0.0010367662077574898,
      "loss": 1.4612,
      "step": 735
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.0683525800704956,
      "learning_rate": 0.0010346661491252762,
      "loss": 1.5254,
      "step": 736
    },
    {
      "epoch": 1.474,
      "grad_norm": 1.034247636795044,
      "learning_rate": 0.0010325659374115302,
      "loss": 1.4198,
      "step": 737
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.9488519430160522,
      "learning_rate": 0.001030465581890533,
      "loss": 1.454,
      "step": 738
    },
    {
      "epoch": 1.478,
      "grad_norm": 1.5998680591583252,
      "learning_rate": 0.001028365091837202,
      "loss": 1.5867,
      "step": 739
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.3366897106170654,
      "learning_rate": 0.0010262644765270472,
      "loss": 1.4723,
      "step": 740
    },
    {
      "epoch": 1.482,
      "grad_norm": 1.3579224348068237,
      "learning_rate": 0.0010241637452361324,
      "loss": 1.4632,
      "step": 741
    },
    {
      "epoch": 1.484,
      "grad_norm": 1.5431468486785889,
      "learning_rate": 0.0010220629072410338,
      "loss": 1.5438,
      "step": 742
    },
    {
      "epoch": 1.486,
      "grad_norm": 1.317745566368103,
      "learning_rate": 0.0010199619718187984,
      "loss": 1.4438,
      "step": 743
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.8799940943717957,
      "learning_rate": 0.001017860948246904,
      "loss": 1.5014,
      "step": 744
    },
    {
      "epoch": 1.49,
      "grad_norm": 2.3853650093078613,
      "learning_rate": 0.0010157598458032165,
      "loss": 1.4235,
      "step": 745
    },
    {
      "epoch": 1.492,
      "grad_norm": 5.949159622192383,
      "learning_rate": 0.001013658673765951,
      "loss": 1.5707,
      "step": 746
    },
    {
      "epoch": 1.494,
      "grad_norm": 4.077223777770996,
      "learning_rate": 0.0010115574414136304,
      "loss": 1.5376,
      "step": 747
    },
    {
      "epoch": 1.496,
      "grad_norm": 35.37705612182617,
      "learning_rate": 0.0010094561580250426,
      "loss": 1.6115,
      "step": 748
    },
    {
      "epoch": 1.498,
      "grad_norm": 1.2324196100234985,
      "learning_rate": 0.0010073548328792016,
      "loss": 1.4924,
      "step": 749
    },
    {
      "epoch": 1.5,
      "grad_norm": 2.054457664489746,
      "learning_rate": 0.0010052534752553063,
      "loss": 1.4891,
      "step": 750
    },
    {
      "epoch": 1.502,
      "grad_norm": 4.201944351196289,
      "learning_rate": 0.0010031520944326975,
      "loss": 1.4596,
      "step": 751
    },
    {
      "epoch": 1.504,
      "grad_norm": 3.367177963256836,
      "learning_rate": 0.0010010506996908201,
      "loss": 1.6081,
      "step": 752
    },
    {
      "epoch": 1.506,
      "grad_norm": 1.8854448795318604,
      "learning_rate": 0.0009989493003091801,
      "loss": 1.6795,
      "step": 753
    },
    {
      "epoch": 1.508,
      "grad_norm": 2.1709611415863037,
      "learning_rate": 0.0009968479055673027,
      "loss": 1.5977,
      "step": 754
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.2877486944198608,
      "learning_rate": 0.000994746524744694,
      "loss": 1.6101,
      "step": 755
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.6381034255027771,
      "learning_rate": 0.0009926451671207984,
      "loss": 1.3935,
      "step": 756
    },
    {
      "epoch": 1.514,
      "grad_norm": 0.7148578763008118,
      "learning_rate": 0.0009905438419749576,
      "loss": 1.4442,
      "step": 757
    },
    {
      "epoch": 1.516,
      "grad_norm": 1.3118648529052734,
      "learning_rate": 0.00098844255858637,
      "loss": 1.5258,
      "step": 758
    },
    {
      "epoch": 1.518,
      "grad_norm": 5.327488899230957,
      "learning_rate": 0.000986341326234049,
      "loss": 1.6163,
      "step": 759
    },
    {
      "epoch": 1.52,
      "grad_norm": 22.10264015197754,
      "learning_rate": 0.0009842401541967838,
      "loss": 1.5014,
      "step": 760
    },
    {
      "epoch": 1.522,
      "grad_norm": 1.2698864936828613,
      "learning_rate": 0.0009821390517530963,
      "loss": 1.5853,
      "step": 761
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.867561936378479,
      "learning_rate": 0.0009800380281812016,
      "loss": 1.4995,
      "step": 762
    },
    {
      "epoch": 1.526,
      "grad_norm": 2.608764410018921,
      "learning_rate": 0.0009779370927589666,
      "loss": 1.4531,
      "step": 763
    },
    {
      "epoch": 1.528,
      "grad_norm": 5.973611831665039,
      "learning_rate": 0.000975836254763868,
      "loss": 1.4774,
      "step": 764
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.7885600924491882,
      "learning_rate": 0.000973735523472953,
      "loss": 1.3932,
      "step": 765
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.7970895171165466,
      "learning_rate": 0.0009716349081627981,
      "loss": 1.4605,
      "step": 766
    },
    {
      "epoch": 1.534,
      "grad_norm": 2.4307854175567627,
      "learning_rate": 0.0009695344181094668,
      "loss": 1.5054,
      "step": 767
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.452188491821289,
      "learning_rate": 0.0009674340625884701,
      "loss": 1.4724,
      "step": 768
    },
    {
      "epoch": 1.538,
      "grad_norm": 1.009121298789978,
      "learning_rate": 0.000965333850874724,
      "loss": 1.4229,
      "step": 769
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.5645102262496948,
      "learning_rate": 0.0009632337922425105,
      "loss": 1.3906,
      "step": 770
    },
    {
      "epoch": 1.542,
      "grad_norm": 0.9645180106163025,
      "learning_rate": 0.0009611338959654346,
      "loss": 1.5119,
      "step": 771
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.2136708498001099,
      "learning_rate": 0.0009590341713163857,
      "loss": 1.4093,
      "step": 772
    },
    {
      "epoch": 1.546,
      "grad_norm": 1.1827385425567627,
      "learning_rate": 0.0009569346275674944,
      "loss": 1.5164,
      "step": 773
    },
    {
      "epoch": 1.548,
      "grad_norm": 2.0329174995422363,
      "learning_rate": 0.0009548352739900921,
      "loss": 1.4611,
      "step": 774
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.7641578912734985,
      "learning_rate": 0.0009527361198546714,
      "loss": 1.5087,
      "step": 775
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.1707295179367065,
      "learning_rate": 0.0009506371744308432,
      "loss": 1.4964,
      "step": 776
    },
    {
      "epoch": 1.554,
      "grad_norm": 4.12968111038208,
      "learning_rate": 0.0009485384469872979,
      "loss": 1.471,
      "step": 777
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.9595455527305603,
      "learning_rate": 0.0009464399467917625,
      "loss": 1.4624,
      "step": 778
    },
    {
      "epoch": 1.558,
      "grad_norm": 1.1019920110702515,
      "learning_rate": 0.0009443416831109608,
      "loss": 1.5478,
      "step": 779
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.6350457668304443,
      "learning_rate": 0.0009422436652105717,
      "loss": 1.5448,
      "step": 780
    },
    {
      "epoch": 1.562,
      "grad_norm": 7.272396564483643,
      "learning_rate": 0.0009401459023551894,
      "loss": 1.4428,
      "step": 781
    },
    {
      "epoch": 1.564,
      "grad_norm": 8.242218017578125,
      "learning_rate": 0.0009380484038082813,
      "loss": 1.6193,
      "step": 782
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 9.524410247802734,
      "learning_rate": 0.0009359511788321485,
      "loss": 1.5599,
      "step": 783
    },
    {
      "epoch": 1.568,
      "grad_norm": 2.5511655807495117,
      "learning_rate": 0.0009338542366878834,
      "loss": 1.489,
      "step": 784
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 2.6868138313293457,
      "learning_rate": 0.0009317575866353291,
      "loss": 1.5198,
      "step": 785
    },
    {
      "epoch": 1.572,
      "grad_norm": 101.96644592285156,
      "learning_rate": 0.0009296612379330396,
      "loss": 1.6743,
      "step": 786
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 7.78405237197876,
      "learning_rate": 0.0009275651998382377,
      "loss": 1.9701,
      "step": 787
    },
    {
      "epoch": 1.576,
      "grad_norm": 11.000903129577637,
      "learning_rate": 0.0009254694816067747,
      "loss": 2.1882,
      "step": 788
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 7.380127429962158,
      "learning_rate": 0.0009233740924930904,
      "loss": 1.7877,
      "step": 789
    },
    {
      "epoch": 1.58,
      "grad_norm": 4.609099864959717,
      "learning_rate": 0.0009212790417501688,
      "loss": 1.7073,
      "step": 790
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 1.3356188535690308,
      "learning_rate": 0.0009191843386295022,
      "loss": 1.5465,
      "step": 791
    },
    {
      "epoch": 1.584,
      "grad_norm": 23.33120346069336,
      "learning_rate": 0.0009170899923810469,
      "loss": 1.5832,
      "step": 792
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 58.38941955566406,
      "learning_rate": 0.0009149960122531826,
      "loss": 1.7487,
      "step": 793
    },
    {
      "epoch": 1.588,
      "grad_norm": 12.325865745544434,
      "learning_rate": 0.0009129024074926743,
      "loss": 1.9019,
      "step": 794
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 28.898420333862305,
      "learning_rate": 0.0009108091873446264,
      "loss": 2.1595,
      "step": 795
    },
    {
      "epoch": 1.592,
      "grad_norm": 73.7504653930664,
      "learning_rate": 0.0009087163610524475,
      "loss": 2.1749,
      "step": 796
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 35.02983474731445,
      "learning_rate": 0.0009066239378578059,
      "loss": 2.3957,
      "step": 797
    },
    {
      "epoch": 1.596,
      "grad_norm": 81.32318115234375,
      "learning_rate": 0.00090453192700059,
      "loss": 2.5798,
      "step": 798
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 141.25570678710938,
      "learning_rate": 0.0009024403377188673,
      "loss": 2.6916,
      "step": 799
    },
    {
      "epoch": 1.6,
      "grad_norm": 17.840110778808594,
      "learning_rate": 0.0009003491792488438,
      "loss": 2.5692,
      "step": 800
    },
    {
      "epoch": 1.6,
      "eval_loss": 2.2407565116882324,
      "eval_runtime": 214.282,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 800
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 48.37796401977539,
      "learning_rate": 0.0008982584608248225,
      "loss": 2.2994,
      "step": 801
    },
    {
      "epoch": 1.604,
      "grad_norm": 14.269560813903809,
      "learning_rate": 0.0008961681916791646,
      "loss": 2.001,
      "step": 802
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 6.728115558624268,
      "learning_rate": 0.0008940783810422461,
      "loss": 2.1108,
      "step": 803
    },
    {
      "epoch": 1.608,
      "grad_norm": 19.41750717163086,
      "learning_rate": 0.0008919890381424188,
      "loss": 1.6708,
      "step": 804
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 11.517719268798828,
      "learning_rate": 0.0008899001722059687,
      "loss": 1.8466,
      "step": 805
    },
    {
      "epoch": 1.612,
      "grad_norm": 9.078011512756348,
      "learning_rate": 0.0008878117924570754,
      "loss": 1.572,
      "step": 806
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 28.866743087768555,
      "learning_rate": 0.0008857239081177725,
      "loss": 1.7963,
      "step": 807
    },
    {
      "epoch": 1.616,
      "grad_norm": 13.14944839477539,
      "learning_rate": 0.0008836365284079056,
      "loss": 1.8541,
      "step": 808
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 9.996728897094727,
      "learning_rate": 0.0008815496625450912,
      "loss": 2.2119,
      "step": 809
    },
    {
      "epoch": 1.62,
      "grad_norm": 6.062660217285156,
      "learning_rate": 0.0008794633197446771,
      "loss": 1.9436,
      "step": 810
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 2.3640620708465576,
      "learning_rate": 0.0008773775092197017,
      "loss": 1.7412,
      "step": 811
    },
    {
      "epoch": 1.624,
      "grad_norm": 4.631687164306641,
      "learning_rate": 0.0008752922401808523,
      "loss": 1.7818,
      "step": 812
    },
    {
      "epoch": 1.626,
      "grad_norm": 2.7470266819000244,
      "learning_rate": 0.0008732075218364258,
      "loss": 1.5716,
      "step": 813
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 4.47988748550415,
      "learning_rate": 0.0008711233633922871,
      "loss": 1.7007,
      "step": 814
    },
    {
      "epoch": 1.63,
      "grad_norm": 6.721288681030273,
      "learning_rate": 0.0008690397740518279,
      "loss": 1.7174,
      "step": 815
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 3.242349147796631,
      "learning_rate": 0.0008669567630159276,
      "loss": 1.796,
      "step": 816
    },
    {
      "epoch": 1.634,
      "grad_norm": 15.195856094360352,
      "learning_rate": 0.0008648743394829115,
      "loss": 1.6604,
      "step": 817
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 13.340115547180176,
      "learning_rate": 0.0008627925126485108,
      "loss": 1.6819,
      "step": 818
    },
    {
      "epoch": 1.638,
      "grad_norm": 1.7245080471038818,
      "learning_rate": 0.0008607112917058222,
      "loss": 1.5656,
      "step": 819
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 3.144718647003174,
      "learning_rate": 0.0008586306858452652,
      "loss": 1.5941,
      "step": 820
    },
    {
      "epoch": 1.642,
      "grad_norm": 21.61688804626465,
      "learning_rate": 0.0008565507042545451,
      "loss": 1.5827,
      "step": 821
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 1.1872217655181885,
      "learning_rate": 0.0008544713561186095,
      "loss": 1.4441,
      "step": 822
    },
    {
      "epoch": 1.646,
      "grad_norm": 2.269529104232788,
      "learning_rate": 0.0008523926506196085,
      "loss": 1.4965,
      "step": 823
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.9779706001281738,
      "learning_rate": 0.0008503145969368561,
      "loss": 1.6053,
      "step": 824
    },
    {
      "epoch": 1.65,
      "grad_norm": 29.761091232299805,
      "learning_rate": 0.000848237204246785,
      "loss": 1.5464,
      "step": 825
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 1.7063935995101929,
      "learning_rate": 0.0008461604817229115,
      "loss": 1.4211,
      "step": 826
    },
    {
      "epoch": 1.654,
      "grad_norm": 2.272029161453247,
      "learning_rate": 0.0008440844385357918,
      "loss": 1.6903,
      "step": 827
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 13.610429763793945,
      "learning_rate": 0.0008420090838529822,
      "loss": 1.4824,
      "step": 828
    },
    {
      "epoch": 1.658,
      "grad_norm": 33.74708557128906,
      "learning_rate": 0.0008399344268389981,
      "loss": 1.7348,
      "step": 829
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 14.206526756286621,
      "learning_rate": 0.0008378604766552756,
      "loss": 1.4247,
      "step": 830
    },
    {
      "epoch": 1.662,
      "grad_norm": 19.819808959960938,
      "learning_rate": 0.0008357872424601272,
      "loss": 1.6841,
      "step": 831
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 71.7949447631836,
      "learning_rate": 0.000833714733408706,
      "loss": 1.4819,
      "step": 832
    },
    {
      "epoch": 1.666,
      "grad_norm": 8.017165184020996,
      "learning_rate": 0.0008316429586529614,
      "loss": 1.5602,
      "step": 833
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 2.0714120864868164,
      "learning_rate": 0.0008295719273416011,
      "loss": 1.4916,
      "step": 834
    },
    {
      "epoch": 1.67,
      "grad_norm": 3.407994508743286,
      "learning_rate": 0.0008275016486200497,
      "loss": 1.5099,
      "step": 835
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 8.282629013061523,
      "learning_rate": 0.0008254321316304075,
      "loss": 1.3816,
      "step": 836
    },
    {
      "epoch": 1.674,
      "grad_norm": 3.074880599975586,
      "learning_rate": 0.0008233633855114126,
      "loss": 1.4936,
      "step": 837
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 6.475584506988525,
      "learning_rate": 0.000821295419398398,
      "loss": 1.7472,
      "step": 838
    },
    {
      "epoch": 1.678,
      "grad_norm": 20.74003028869629,
      "learning_rate": 0.0008192282424232527,
      "loss": 1.5469,
      "step": 839
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 3.8707430362701416,
      "learning_rate": 0.000817161863714381,
      "loss": 1.5302,
      "step": 840
    },
    {
      "epoch": 1.682,
      "grad_norm": 4.777573108673096,
      "learning_rate": 0.0008150962923966614,
      "loss": 1.5955,
      "step": 841
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 3.8949050903320312,
      "learning_rate": 0.0008130315375914079,
      "loss": 1.5296,
      "step": 842
    },
    {
      "epoch": 1.686,
      "grad_norm": 1.1312519311904907,
      "learning_rate": 0.0008109676084163289,
      "loss": 1.603,
      "step": 843
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.562816858291626,
      "learning_rate": 0.0008089045139854865,
      "loss": 1.509,
      "step": 844
    },
    {
      "epoch": 1.69,
      "grad_norm": 6.405933856964111,
      "learning_rate": 0.000806842263409257,
      "loss": 1.5375,
      "step": 845
    },
    {
      "epoch": 1.692,
      "grad_norm": 6.9349799156188965,
      "learning_rate": 0.0008047808657942897,
      "loss": 1.5194,
      "step": 846
    },
    {
      "epoch": 1.694,
      "grad_norm": 4.943783283233643,
      "learning_rate": 0.0008027203302434679,
      "loss": 1.5293,
      "step": 847
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.1825695037841797,
      "learning_rate": 0.0008006606658558684,
      "loss": 1.4527,
      "step": 848
    },
    {
      "epoch": 1.698,
      "grad_norm": 3.378629207611084,
      "learning_rate": 0.0007986018817267203,
      "loss": 1.4631,
      "step": 849
    },
    {
      "epoch": 1.7,
      "grad_norm": 3.5011415481567383,
      "learning_rate": 0.0007965439869473664,
      "loss": 1.5072,
      "step": 850
    },
    {
      "epoch": 1.702,
      "grad_norm": 8.230931282043457,
      "learning_rate": 0.0007944869906052211,
      "loss": 1.5273,
      "step": 851
    },
    {
      "epoch": 1.704,
      "grad_norm": 7.447128772735596,
      "learning_rate": 0.0007924309017837325,
      "loss": 1.6786,
      "step": 852
    },
    {
      "epoch": 1.706,
      "grad_norm": 33.416534423828125,
      "learning_rate": 0.0007903757295623406,
      "loss": 1.5271,
      "step": 853
    },
    {
      "epoch": 1.708,
      "grad_norm": 2.048920154571533,
      "learning_rate": 0.0007883214830164383,
      "loss": 1.5724,
      "step": 854
    },
    {
      "epoch": 1.71,
      "grad_norm": 10.351691246032715,
      "learning_rate": 0.0007862681712173304,
      "loss": 1.4816,
      "step": 855
    },
    {
      "epoch": 1.712,
      "grad_norm": 2.407349109649658,
      "learning_rate": 0.000784215803232194,
      "loss": 1.6178,
      "step": 856
    },
    {
      "epoch": 1.714,
      "grad_norm": 1.5445252656936646,
      "learning_rate": 0.0007821643881240386,
      "loss": 1.4997,
      "step": 857
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.795928418636322,
      "learning_rate": 0.0007801139349516656,
      "loss": 1.3919,
      "step": 858
    },
    {
      "epoch": 1.718,
      "grad_norm": 1.1059379577636719,
      "learning_rate": 0.000778064452769629,
      "loss": 1.5208,
      "step": 859
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.994702160358429,
      "learning_rate": 0.0007760159506281955,
      "loss": 1.532,
      "step": 860
    },
    {
      "epoch": 1.722,
      "grad_norm": 0.844563364982605,
      "learning_rate": 0.0007739684375733022,
      "loss": 1.3817,
      "step": 861
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.9446215629577637,
      "learning_rate": 0.0007719219226465208,
      "loss": 1.486,
      "step": 862
    },
    {
      "epoch": 1.726,
      "grad_norm": 4.215527534484863,
      "learning_rate": 0.0007698764148850137,
      "loss": 1.6375,
      "step": 863
    },
    {
      "epoch": 1.728,
      "grad_norm": 1.3529897928237915,
      "learning_rate": 0.0007678319233214965,
      "loss": 1.4986,
      "step": 864
    },
    {
      "epoch": 1.73,
      "grad_norm": 2.08817458152771,
      "learning_rate": 0.000765788456984198,
      "loss": 1.5676,
      "step": 865
    },
    {
      "epoch": 1.732,
      "grad_norm": 1.182197093963623,
      "learning_rate": 0.0007637460248968177,
      "loss": 1.4646,
      "step": 866
    },
    {
      "epoch": 1.734,
      "grad_norm": 0.9678610563278198,
      "learning_rate": 0.0007617046360784905,
      "loss": 1.4411,
      "step": 867
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.1221449375152588,
      "learning_rate": 0.0007596642995437426,
      "loss": 1.4542,
      "step": 868
    },
    {
      "epoch": 1.738,
      "grad_norm": 1.123806118965149,
      "learning_rate": 0.0007576250243024542,
      "loss": 1.4853,
      "step": 869
    },
    {
      "epoch": 1.74,
      "grad_norm": 9.944730758666992,
      "learning_rate": 0.0007555868193598187,
      "loss": 1.3784,
      "step": 870
    },
    {
      "epoch": 1.742,
      "grad_norm": 3.692779064178467,
      "learning_rate": 0.0007535496937163031,
      "loss": 1.435,
      "step": 871
    },
    {
      "epoch": 1.744,
      "grad_norm": 1.9897056818008423,
      "learning_rate": 0.0007515136563676083,
      "loss": 1.4748,
      "step": 872
    },
    {
      "epoch": 1.746,
      "grad_norm": 0.9185799360275269,
      "learning_rate": 0.0007494787163046299,
      "loss": 1.4327,
      "step": 873
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.5825870037078857,
      "learning_rate": 0.000747444882513418,
      "loss": 1.4722,
      "step": 874
    },
    {
      "epoch": 1.75,
      "grad_norm": 5.755086898803711,
      "learning_rate": 0.0007454121639751371,
      "loss": 1.6043,
      "step": 875
    },
    {
      "epoch": 1.752,
      "grad_norm": 21.098562240600586,
      "learning_rate": 0.0007433805696660267,
      "loss": 1.4161,
      "step": 876
    },
    {
      "epoch": 1.754,
      "grad_norm": 1.8669989109039307,
      "learning_rate": 0.000741350108557362,
      "loss": 1.3558,
      "step": 877
    },
    {
      "epoch": 1.756,
      "grad_norm": 4.766286373138428,
      "learning_rate": 0.0007393207896154151,
      "loss": 1.4859,
      "step": 878
    },
    {
      "epoch": 1.758,
      "grad_norm": 2.3255534172058105,
      "learning_rate": 0.000737292621801413,
      "loss": 1.4435,
      "step": 879
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.7841176986694336,
      "learning_rate": 0.0007352656140715006,
      "loss": 1.54,
      "step": 880
    },
    {
      "epoch": 1.762,
      "grad_norm": 8.016974449157715,
      "learning_rate": 0.0007332397753766993,
      "loss": 1.4746,
      "step": 881
    },
    {
      "epoch": 1.764,
      "grad_norm": 1.8869394063949585,
      "learning_rate": 0.000731215114662868,
      "loss": 1.5705,
      "step": 882
    },
    {
      "epoch": 1.766,
      "grad_norm": 4.451976776123047,
      "learning_rate": 0.0007291916408706643,
      "loss": 1.4006,
      "step": 883
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.6487804651260376,
      "learning_rate": 0.0007271693629355047,
      "loss": 1.4304,
      "step": 884
    },
    {
      "epoch": 1.77,
      "grad_norm": 58.15054702758789,
      "learning_rate": 0.0007251482897875244,
      "loss": 1.5985,
      "step": 885
    },
    {
      "epoch": 1.772,
      "grad_norm": 4.326127529144287,
      "learning_rate": 0.0007231284303515389,
      "loss": 1.5466,
      "step": 886
    },
    {
      "epoch": 1.774,
      "grad_norm": 27.96722984313965,
      "learning_rate": 0.0007211097935470031,
      "loss": 1.7916,
      "step": 887
    },
    {
      "epoch": 1.776,
      "grad_norm": 95.85236358642578,
      "learning_rate": 0.0007190923882879742,
      "loss": 1.9747,
      "step": 888
    },
    {
      "epoch": 1.778,
      "grad_norm": 14.07789134979248,
      "learning_rate": 0.0007170762234830699,
      "loss": 2.2084,
      "step": 889
    },
    {
      "epoch": 1.78,
      "grad_norm": 13.510293006896973,
      "learning_rate": 0.0007150613080354315,
      "loss": 2.7873,
      "step": 890
    },
    {
      "epoch": 1.782,
      "grad_norm": 400.1896057128906,
      "learning_rate": 0.0007130476508426822,
      "loss": 3.3308,
      "step": 891
    },
    {
      "epoch": 1.784,
      "grad_norm": 50.701515197753906,
      "learning_rate": 0.0007110352607968889,
      "loss": 2.8992,
      "step": 892
    },
    {
      "epoch": 1.786,
      "grad_norm": 98.7392578125,
      "learning_rate": 0.0007090241467845237,
      "loss": 2.9253,
      "step": 893
    },
    {
      "epoch": 1.788,
      "grad_norm": 80.5271987915039,
      "learning_rate": 0.0007070143176864231,
      "loss": 2.4711,
      "step": 894
    },
    {
      "epoch": 1.79,
      "grad_norm": 34.16621017456055,
      "learning_rate": 0.0007050057823777502,
      "loss": 2.4705,
      "step": 895
    },
    {
      "epoch": 1.792,
      "grad_norm": 7.3398637771606445,
      "learning_rate": 0.0007029985497279549,
      "loss": 1.9628,
      "step": 896
    },
    {
      "epoch": 1.794,
      "grad_norm": 58.549373626708984,
      "learning_rate": 0.000700992628600734,
      "loss": 2.0145,
      "step": 897
    },
    {
      "epoch": 1.796,
      "grad_norm": 73.24082946777344,
      "learning_rate": 0.0006989880278539931,
      "loss": 1.9017,
      "step": 898
    },
    {
      "epoch": 1.798,
      "grad_norm": 252.36817932128906,
      "learning_rate": 0.0006969847563398075,
      "loss": 2.134,
      "step": 899
    },
    {
      "epoch": 1.8,
      "grad_norm": 34.20063018798828,
      "learning_rate": 0.0006949828229043824,
      "loss": 2.1301,
      "step": 900
    },
    {
      "epoch": 1.8,
      "eval_loss": 2.345210552215576,
      "eval_runtime": 214.0214,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 900
    },
    {
      "epoch": 1.802,
      "grad_norm": 378.43408203125,
      "learning_rate": 0.0006929822363880149,
      "loss": 2.4237,
      "step": 901
    },
    {
      "epoch": 1.804,
      "grad_norm": 75.12552642822266,
      "learning_rate": 0.0006909830056250527,
      "loss": 2.6279,
      "step": 902
    },
    {
      "epoch": 1.806,
      "grad_norm": 555.5459594726562,
      "learning_rate": 0.0006889851394438584,
      "loss": 2.9552,
      "step": 903
    },
    {
      "epoch": 1.808,
      "grad_norm": 123.79924774169922,
      "learning_rate": 0.0006869886466667679,
      "loss": 2.7536,
      "step": 904
    },
    {
      "epoch": 1.81,
      "grad_norm": 115.86539459228516,
      "learning_rate": 0.0006849935361100521,
      "loss": 2.4412,
      "step": 905
    },
    {
      "epoch": 1.812,
      "grad_norm": 290.3843994140625,
      "learning_rate": 0.0006829998165838793,
      "loss": 2.5562,
      "step": 906
    },
    {
      "epoch": 1.814,
      "grad_norm": 298.1366271972656,
      "learning_rate": 0.0006810074968922736,
      "loss": 2.5759,
      "step": 907
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 5430.40625,
      "learning_rate": 0.0006790165858330788,
      "loss": 2.4451,
      "step": 908
    },
    {
      "epoch": 1.818,
      "grad_norm": 124.36465454101562,
      "learning_rate": 0.0006770270921979179,
      "loss": 2.3223,
      "step": 909
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 60.6462287902832,
      "learning_rate": 0.0006750390247721548,
      "loss": 2.4109,
      "step": 910
    },
    {
      "epoch": 1.822,
      "grad_norm": 4.053603172302246,
      "learning_rate": 0.0006730523923348556,
      "loss": 2.2926,
      "step": 911
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 5.211324214935303,
      "learning_rate": 0.0006710672036587491,
      "loss": 2.3051,
      "step": 912
    },
    {
      "epoch": 1.826,
      "grad_norm": 15.224791526794434,
      "learning_rate": 0.0006690834675101889,
      "loss": 2.2234,
      "step": 913
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 2.8254148960113525,
      "learning_rate": 0.0006671011926491151,
      "loss": 1.8605,
      "step": 914
    },
    {
      "epoch": 1.83,
      "grad_norm": 7.651598930358887,
      "learning_rate": 0.0006651203878290139,
      "loss": 1.9744,
      "step": 915
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 7.268939018249512,
      "learning_rate": 0.0006631410617968807,
      "loss": 1.9531,
      "step": 916
    },
    {
      "epoch": 1.834,
      "grad_norm": 3.1726479530334473,
      "learning_rate": 0.0006611632232931804,
      "loss": 1.7861,
      "step": 917
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 17.083696365356445,
      "learning_rate": 0.000659186881051809,
      "loss": 1.9178,
      "step": 918
    },
    {
      "epoch": 1.838,
      "grad_norm": 15.038009643554688,
      "learning_rate": 0.0006572120438000553,
      "loss": 1.6645,
      "step": 919
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 13.742317199707031,
      "learning_rate": 0.0006552387202585629,
      "loss": 1.6311,
      "step": 920
    },
    {
      "epoch": 1.842,
      "grad_norm": 37.85024642944336,
      "learning_rate": 0.0006532669191412905,
      "loss": 1.9488,
      "step": 921
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 36.19845962524414,
      "learning_rate": 0.0006512966491554735,
      "loss": 1.5994,
      "step": 922
    },
    {
      "epoch": 1.846,
      "grad_norm": 3.4349143505096436,
      "learning_rate": 0.0006493279190015865,
      "loss": 1.681,
      "step": 923
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 41.06438064575195,
      "learning_rate": 0.0006473607373733044,
      "loss": 1.6733,
      "step": 924
    },
    {
      "epoch": 1.85,
      "grad_norm": 1.7293332815170288,
      "learning_rate": 0.0006453951129574643,
      "loss": 1.6234,
      "step": 925
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 9.494985580444336,
      "learning_rate": 0.0006434310544340265,
      "loss": 1.777,
      "step": 926
    },
    {
      "epoch": 1.854,
      "grad_norm": 10.394780158996582,
      "learning_rate": 0.000641468570476036,
      "loss": 1.7781,
      "step": 927
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 15.506834983825684,
      "learning_rate": 0.0006395076697495854,
      "loss": 1.8478,
      "step": 928
    },
    {
      "epoch": 1.858,
      "grad_norm": 8.168585777282715,
      "learning_rate": 0.000637548360913776,
      "loss": 1.6346,
      "step": 929
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 106.42713165283203,
      "learning_rate": 0.0006355906526206787,
      "loss": 1.6926,
      "step": 930
    },
    {
      "epoch": 1.862,
      "grad_norm": 55.49125671386719,
      "learning_rate": 0.0006336345535152976,
      "loss": 1.788,
      "step": 931
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 70.17772674560547,
      "learning_rate": 0.0006316800722355307,
      "loss": 1.7082,
      "step": 932
    },
    {
      "epoch": 1.866,
      "grad_norm": 57.44357681274414,
      "learning_rate": 0.0006297272174121309,
      "loss": 1.7852,
      "step": 933
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 14.523950576782227,
      "learning_rate": 0.0006277759976686697,
      "loss": 2.2287,
      "step": 934
    },
    {
      "epoch": 1.87,
      "grad_norm": 3302.64697265625,
      "learning_rate": 0.0006258264216214977,
      "loss": 1.9393,
      "step": 935
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 30.581106185913086,
      "learning_rate": 0.0006238784978797084,
      "loss": 2.144,
      "step": 936
    },
    {
      "epoch": 1.874,
      "grad_norm": 52.3514289855957,
      "learning_rate": 0.0006219322350450978,
      "loss": 2.0037,
      "step": 937
    },
    {
      "epoch": 1.876,
      "grad_norm": 29.055465698242188,
      "learning_rate": 0.0006199876417121272,
      "loss": 2.3007,
      "step": 938
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 741.95751953125,
      "learning_rate": 0.0006180447264678868,
      "loss": 2.3864,
      "step": 939
    },
    {
      "epoch": 1.88,
      "grad_norm": 58.31224060058594,
      "learning_rate": 0.0006161034978920554,
      "loss": 2.3199,
      "step": 940
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 117.21582794189453,
      "learning_rate": 0.0006141639645568645,
      "loss": 2.1517,
      "step": 941
    },
    {
      "epoch": 1.884,
      "grad_norm": 67.04757690429688,
      "learning_rate": 0.0006122261350270598,
      "loss": 2.207,
      "step": 942
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 447.41424560546875,
      "learning_rate": 0.0006102900178598616,
      "loss": 2.1461,
      "step": 943
    },
    {
      "epoch": 1.888,
      "grad_norm": 18.130468368530273,
      "learning_rate": 0.0006083556216049306,
      "loss": 2.1989,
      "step": 944
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 33.526004791259766,
      "learning_rate": 0.0006064229548043272,
      "loss": 1.9499,
      "step": 945
    },
    {
      "epoch": 1.892,
      "grad_norm": 87.45211029052734,
      "learning_rate": 0.0006044920259924747,
      "loss": 2.0906,
      "step": 946
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 15.18132209777832,
      "learning_rate": 0.0006025628436961218,
      "loss": 1.8194,
      "step": 947
    },
    {
      "epoch": 1.896,
      "grad_norm": 22.744964599609375,
      "learning_rate": 0.0006006354164343047,
      "loss": 1.8887,
      "step": 948
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 28.585121154785156,
      "learning_rate": 0.0005987097527183096,
      "loss": 1.8146,
      "step": 949
    },
    {
      "epoch": 1.9,
      "grad_norm": 9.077740669250488,
      "learning_rate": 0.0005967858610516353,
      "loss": 1.7937,
      "step": 950
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 227.0623779296875,
      "learning_rate": 0.0005948637499299554,
      "loss": 1.6703,
      "step": 951
    },
    {
      "epoch": 1.904,
      "grad_norm": 2.8956592082977295,
      "learning_rate": 0.000592943427841081,
      "loss": 1.7615,
      "step": 952
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 181.50982666015625,
      "learning_rate": 0.000591024903264922,
      "loss": 1.6551,
      "step": 953
    },
    {
      "epoch": 1.908,
      "grad_norm": 2.908289909362793,
      "learning_rate": 0.0005891081846734518,
      "loss": 1.4402,
      "step": 954
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 81.5860366821289,
      "learning_rate": 0.0005871932805306688,
      "loss": 1.5092,
      "step": 955
    },
    {
      "epoch": 1.912,
      "grad_norm": 10.863962173461914,
      "learning_rate": 0.0005852801992925585,
      "loss": 1.5404,
      "step": 956
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 2.995004892349243,
      "learning_rate": 0.0005833689494070569,
      "loss": 1.5863,
      "step": 957
    },
    {
      "epoch": 1.916,
      "grad_norm": 7.065956115722656,
      "learning_rate": 0.0005814595393140126,
      "loss": 1.637,
      "step": 958
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 4.38847541809082,
      "learning_rate": 0.0005795519774451505,
      "loss": 1.5939,
      "step": 959
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.312093734741211,
      "learning_rate": 0.0005776462722240337,
      "loss": 1.5247,
      "step": 960
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 2.2841317653656006,
      "learning_rate": 0.0005757424320660264,
      "loss": 1.6104,
      "step": 961
    },
    {
      "epoch": 1.924,
      "grad_norm": 20.16775894165039,
      "learning_rate": 0.0005738404653782571,
      "loss": 1.6177,
      "step": 962
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 2.6429145336151123,
      "learning_rate": 0.0005719403805595815,
      "loss": 1.6197,
      "step": 963
    },
    {
      "epoch": 1.928,
      "grad_norm": 9.14059066772461,
      "learning_rate": 0.0005700421860005447,
      "loss": 1.7448,
      "step": 964
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 12.026653289794922,
      "learning_rate": 0.0005681458900833447,
      "loss": 1.5514,
      "step": 965
    },
    {
      "epoch": 1.932,
      "grad_norm": 8.637431144714355,
      "learning_rate": 0.0005662515011817959,
      "loss": 1.5823,
      "step": 966
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 4.933592796325684,
      "learning_rate": 0.0005643590276612909,
      "loss": 1.5478,
      "step": 967
    },
    {
      "epoch": 1.936,
      "grad_norm": 9.467641830444336,
      "learning_rate": 0.0005624684778787646,
      "loss": 1.6035,
      "step": 968
    },
    {
      "epoch": 1.938,
      "grad_norm": 4.525832653045654,
      "learning_rate": 0.0005605798601826566,
      "loss": 1.7239,
      "step": 969
    },
    {
      "epoch": 1.94,
      "grad_norm": 10.075789451599121,
      "learning_rate": 0.0005586931829128749,
      "loss": 1.6273,
      "step": 970
    },
    {
      "epoch": 1.942,
      "grad_norm": 17.82392120361328,
      "learning_rate": 0.0005568084544007588,
      "loss": 1.5424,
      "step": 971
    },
    {
      "epoch": 1.944,
      "grad_norm": 9.692423820495605,
      "learning_rate": 0.0005549256829690418,
      "loss": 1.7986,
      "step": 972
    },
    {
      "epoch": 1.946,
      "grad_norm": 5.173174858093262,
      "learning_rate": 0.0005530448769318157,
      "loss": 1.6826,
      "step": 973
    },
    {
      "epoch": 1.948,
      "grad_norm": 2.847698450088501,
      "learning_rate": 0.0005511660445944929,
      "loss": 1.6159,
      "step": 974
    },
    {
      "epoch": 1.95,
      "grad_norm": 8.724321365356445,
      "learning_rate": 0.0005492891942537703,
      "loss": 1.6602,
      "step": 975
    },
    {
      "epoch": 1.952,
      "grad_norm": 2.0084946155548096,
      "learning_rate": 0.0005474143341975928,
      "loss": 1.4815,
      "step": 976
    },
    {
      "epoch": 1.954,
      "grad_norm": 16.506189346313477,
      "learning_rate": 0.0005455414727051159,
      "loss": 1.5718,
      "step": 977
    },
    {
      "epoch": 1.956,
      "grad_norm": 28.590187072753906,
      "learning_rate": 0.0005436706180466702,
      "loss": 1.6707,
      "step": 978
    },
    {
      "epoch": 1.958,
      "grad_norm": 8.729909896850586,
      "learning_rate": 0.0005418017784837243,
      "loss": 1.6758,
      "step": 979
    },
    {
      "epoch": 1.96,
      "grad_norm": 4.10148811340332,
      "learning_rate": 0.0005399349622688479,
      "loss": 1.6301,
      "step": 980
    },
    {
      "epoch": 1.962,
      "grad_norm": 43.40799331665039,
      "learning_rate": 0.0005380701776456766,
      "loss": 1.5988,
      "step": 981
    },
    {
      "epoch": 1.964,
      "grad_norm": 3.0911152362823486,
      "learning_rate": 0.000536207432848874,
      "loss": 1.6323,
      "step": 982
    },
    {
      "epoch": 1.966,
      "grad_norm": 18.10004425048828,
      "learning_rate": 0.0005343467361040966,
      "loss": 1.6905,
      "step": 983
    },
    {
      "epoch": 1.968,
      "grad_norm": 22.164260864257812,
      "learning_rate": 0.0005324880956279567,
      "loss": 1.4193,
      "step": 984
    },
    {
      "epoch": 1.97,
      "grad_norm": 4.777860164642334,
      "learning_rate": 0.0005306315196279864,
      "loss": 1.7062,
      "step": 985
    },
    {
      "epoch": 1.972,
      "grad_norm": 3.4213640689849854,
      "learning_rate": 0.0005287770163026013,
      "loss": 1.5256,
      "step": 986
    },
    {
      "epoch": 1.974,
      "grad_norm": 12.631621360778809,
      "learning_rate": 0.0005269245938410646,
      "loss": 1.4684,
      "step": 987
    },
    {
      "epoch": 1.976,
      "grad_norm": 7.550547122955322,
      "learning_rate": 0.00052507426042345,
      "loss": 1.6764,
      "step": 988
    },
    {
      "epoch": 1.978,
      "grad_norm": 8.17073917388916,
      "learning_rate": 0.0005232260242206071,
      "loss": 1.5229,
      "step": 989
    },
    {
      "epoch": 1.98,
      "grad_norm": 6.473642826080322,
      "learning_rate": 0.0005213798933941236,
      "loss": 1.4917,
      "step": 990
    },
    {
      "epoch": 1.982,
      "grad_norm": 4.97361946105957,
      "learning_rate": 0.0005195358760962907,
      "loss": 1.6603,
      "step": 991
    },
    {
      "epoch": 1.984,
      "grad_norm": 3.7793288230895996,
      "learning_rate": 0.0005176939804700664,
      "loss": 1.5253,
      "step": 992
    },
    {
      "epoch": 1.986,
      "grad_norm": 3.761855363845825,
      "learning_rate": 0.0005158542146490399,
      "loss": 1.5939,
      "step": 993
    },
    {
      "epoch": 1.988,
      "grad_norm": 3.250903844833374,
      "learning_rate": 0.0005140165867573939,
      "loss": 1.4815,
      "step": 994
    },
    {
      "epoch": 1.99,
      "grad_norm": 7.368410587310791,
      "learning_rate": 0.0005121811049098728,
      "loss": 1.6306,
      "step": 995
    },
    {
      "epoch": 1.992,
      "grad_norm": 14.338052749633789,
      "learning_rate": 0.0005103477772117424,
      "loss": 1.4247,
      "step": 996
    },
    {
      "epoch": 1.994,
      "grad_norm": 24.954118728637695,
      "learning_rate": 0.0005085166117587567,
      "loss": 1.5897,
      "step": 997
    },
    {
      "epoch": 1.996,
      "grad_norm": 3.9838035106658936,
      "learning_rate": 0.0005066876166371219,
      "loss": 1.6934,
      "step": 998
    },
    {
      "epoch": 1.998,
      "grad_norm": 166.58888244628906,
      "learning_rate": 0.0005048607999234587,
      "loss": 1.5473,
      "step": 999
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.6502296924591064,
      "learning_rate": 0.0005030361696847705,
      "loss": 1.5832,
      "step": 1000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.563438892364502,
      "eval_runtime": 214.067,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 1000
    },
    {
      "epoch": 2.002,
      "grad_norm": 3.4037346839904785,
      "learning_rate": 0.0005012137339784037,
      "loss": 1.722,
      "step": 1001
    },
    {
      "epoch": 2.004,
      "grad_norm": 39.31886672973633,
      "learning_rate": 0.0004993935008520146,
      "loss": 1.628,
      "step": 1002
    },
    {
      "epoch": 2.006,
      "grad_norm": 4.5382304191589355,
      "learning_rate": 0.0004975754783435336,
      "loss": 1.5139,
      "step": 1003
    },
    {
      "epoch": 2.008,
      "grad_norm": 2.1087417602539062,
      "learning_rate": 0.0004957596744811279,
      "loss": 1.5523,
      "step": 1004
    },
    {
      "epoch": 2.01,
      "grad_norm": 9.463984489440918,
      "learning_rate": 0.0004939460972831684,
      "loss": 1.5512,
      "step": 1005
    },
    {
      "epoch": 2.012,
      "grad_norm": 2.9235596656799316,
      "learning_rate": 0.000492134754758194,
      "loss": 1.5903,
      "step": 1006
    },
    {
      "epoch": 2.014,
      "grad_norm": 3.206580877304077,
      "learning_rate": 0.0004903256549048742,
      "loss": 1.6572,
      "step": 1007
    },
    {
      "epoch": 2.016,
      "grad_norm": 2.8065872192382812,
      "learning_rate": 0.0004885188057119762,
      "loss": 1.5782,
      "step": 1008
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.7114707231521606,
      "learning_rate": 0.00048671421515832726,
      "loss": 1.42,
      "step": 1009
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.6937483549118042,
      "learning_rate": 0.00048491189121278167,
      "loss": 1.5313,
      "step": 1010
    },
    {
      "epoch": 2.022,
      "grad_norm": 2.692274332046509,
      "learning_rate": 0.0004831118418341852,
      "loss": 1.4477,
      "step": 1011
    },
    {
      "epoch": 2.024,
      "grad_norm": 3.5393991470336914,
      "learning_rate": 0.0004813140749713384,
      "loss": 1.5195,
      "step": 1012
    },
    {
      "epoch": 2.026,
      "grad_norm": 4.279778480529785,
      "learning_rate": 0.0004795185985629632,
      "loss": 1.5575,
      "step": 1013
    },
    {
      "epoch": 2.028,
      "grad_norm": 1.6190822124481201,
      "learning_rate": 0.0004777254205376662,
      "loss": 1.428,
      "step": 1014
    },
    {
      "epoch": 2.03,
      "grad_norm": 2.743257999420166,
      "learning_rate": 0.0004759345488139054,
      "loss": 1.6689,
      "step": 1015
    },
    {
      "epoch": 2.032,
      "grad_norm": 4.509552478790283,
      "learning_rate": 0.00047414599129995405,
      "loss": 1.3887,
      "step": 1016
    },
    {
      "epoch": 2.034,
      "grad_norm": 5.676326274871826,
      "learning_rate": 0.00047235975589386713,
      "loss": 1.5002,
      "step": 1017
    },
    {
      "epoch": 2.036,
      "grad_norm": 7.670284748077393,
      "learning_rate": 0.00047057585048344467,
      "loss": 1.4668,
      "step": 1018
    },
    {
      "epoch": 2.038,
      "grad_norm": 2.8054327964782715,
      "learning_rate": 0.0004687942829461969,
      "loss": 1.4511,
      "step": 1019
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.9116752743721008,
      "learning_rate": 0.00046701506114931157,
      "loss": 1.5063,
      "step": 1020
    },
    {
      "epoch": 2.042,
      "grad_norm": 2.498964548110962,
      "learning_rate": 0.0004652381929496172,
      "loss": 1.5534,
      "step": 1021
    },
    {
      "epoch": 2.044,
      "grad_norm": 5.383687496185303,
      "learning_rate": 0.00046346368619355007,
      "loss": 1.5755,
      "step": 1022
    },
    {
      "epoch": 2.046,
      "grad_norm": 2.5095174312591553,
      "learning_rate": 0.00046169154871711804,
      "loss": 1.4044,
      "step": 1023
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.1678555011749268,
      "learning_rate": 0.0004599217883458655,
      "loss": 1.433,
      "step": 1024
    },
    {
      "epoch": 2.05,
      "grad_norm": 5.671201229095459,
      "learning_rate": 0.00045815441289484126,
      "loss": 1.3849,
      "step": 1025
    },
    {
      "epoch": 2.052,
      "grad_norm": 1.450675129890442,
      "learning_rate": 0.00045638943016856204,
      "loss": 1.4761,
      "step": 1026
    },
    {
      "epoch": 2.054,
      "grad_norm": 2.1340160369873047,
      "learning_rate": 0.0004546268479609783,
      "loss": 1.6184,
      "step": 1027
    },
    {
      "epoch": 2.056,
      "grad_norm": 4.83311653137207,
      "learning_rate": 0.00045286667405544114,
      "loss": 1.4043,
      "step": 1028
    },
    {
      "epoch": 2.058,
      "grad_norm": 2.7088794708251953,
      "learning_rate": 0.0004511089162246661,
      "loss": 1.4613,
      "step": 1029
    },
    {
      "epoch": 2.06,
      "grad_norm": 2.4346585273742676,
      "learning_rate": 0.00044935358223069925,
      "loss": 1.455,
      "step": 1030
    },
    {
      "epoch": 2.062,
      "grad_norm": 3.3637876510620117,
      "learning_rate": 0.0004476006798248837,
      "loss": 1.4415,
      "step": 1031
    },
    {
      "epoch": 2.064,
      "grad_norm": 26.88131332397461,
      "learning_rate": 0.00044585021674782533,
      "loss": 1.5447,
      "step": 1032
    },
    {
      "epoch": 2.066,
      "grad_norm": 1.7136690616607666,
      "learning_rate": 0.0004441022007293575,
      "loss": 1.4931,
      "step": 1033
    },
    {
      "epoch": 2.068,
      "grad_norm": 1.0293596982955933,
      "learning_rate": 0.0004423566394885091,
      "loss": 1.4027,
      "step": 1034
    },
    {
      "epoch": 2.07,
      "grad_norm": 1.316032886505127,
      "learning_rate": 0.0004406135407334668,
      "loss": 1.5342,
      "step": 1035
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.940552294254303,
      "learning_rate": 0.000438872912161545,
      "loss": 1.6179,
      "step": 1036
    },
    {
      "epoch": 2.074,
      "grad_norm": 0.865034282207489,
      "learning_rate": 0.0004371347614591493,
      "loss": 1.3614,
      "step": 1037
    },
    {
      "epoch": 2.076,
      "grad_norm": 4.651411056518555,
      "learning_rate": 0.0004353990963017433,
      "loss": 1.3724,
      "step": 1038
    },
    {
      "epoch": 2.078,
      "grad_norm": 1.1064645051956177,
      "learning_rate": 0.0004336659243538159,
      "loss": 1.4535,
      "step": 1039
    },
    {
      "epoch": 2.08,
      "grad_norm": 2.1733267307281494,
      "learning_rate": 0.0004319352532688443,
      "loss": 1.5165,
      "step": 1040
    },
    {
      "epoch": 2.082,
      "grad_norm": 1.6668314933776855,
      "learning_rate": 0.00043020709068926366,
      "loss": 1.491,
      "step": 1041
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.9473680853843689,
      "learning_rate": 0.00042848144424643134,
      "loss": 1.5055,
      "step": 1042
    },
    {
      "epoch": 2.086,
      "grad_norm": 1.51840341091156,
      "learning_rate": 0.0004267583215605939,
      "loss": 1.4444,
      "step": 1043
    },
    {
      "epoch": 2.088,
      "grad_norm": 1.1447051763534546,
      "learning_rate": 0.0004250377302408531,
      "loss": 1.4875,
      "step": 1044
    },
    {
      "epoch": 2.09,
      "grad_norm": 1.2048993110656738,
      "learning_rate": 0.0004233196778851329,
      "loss": 1.4217,
      "step": 1045
    },
    {
      "epoch": 2.092,
      "grad_norm": 1.5988065004348755,
      "learning_rate": 0.0004216041720801451,
      "loss": 1.5391,
      "step": 1046
    },
    {
      "epoch": 2.094,
      "grad_norm": 1.1412872076034546,
      "learning_rate": 0.00041989122040135654,
      "loss": 1.3524,
      "step": 1047
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.9141467809677124,
      "learning_rate": 0.00041818083041295486,
      "loss": 1.4734,
      "step": 1048
    },
    {
      "epoch": 2.098,
      "grad_norm": 0.676835298538208,
      "learning_rate": 0.0004164730096678161,
      "loss": 1.3642,
      "step": 1049
    },
    {
      "epoch": 2.1,
      "grad_norm": 2.8520095348358154,
      "learning_rate": 0.00041476776570747065,
      "loss": 1.4227,
      "step": 1050
    },
    {
      "epoch": 2.102,
      "grad_norm": 16.849029541015625,
      "learning_rate": 0.00041306510606207005,
      "loss": 1.4855,
      "step": 1051
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.9324301481246948,
      "learning_rate": 0.00041136503825035396,
      "loss": 1.5536,
      "step": 1052
    },
    {
      "epoch": 2.106,
      "grad_norm": 3.298438310623169,
      "learning_rate": 0.00040966756977961683,
      "loss": 1.4295,
      "step": 1053
    },
    {
      "epoch": 2.108,
      "grad_norm": 1.405562162399292,
      "learning_rate": 0.00040797270814567447,
      "loss": 1.4012,
      "step": 1054
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.9409417510032654,
      "learning_rate": 0.00040628046083283133,
      "loss": 1.3966,
      "step": 1055
    },
    {
      "epoch": 2.112,
      "grad_norm": 5.870832443237305,
      "learning_rate": 0.0004045908353138477,
      "loss": 1.4933,
      "step": 1056
    },
    {
      "epoch": 2.114,
      "grad_norm": 7.128233909606934,
      "learning_rate": 0.0004029038390499057,
      "loss": 1.3591,
      "step": 1057
    },
    {
      "epoch": 2.116,
      "grad_norm": 7.1403985023498535,
      "learning_rate": 0.0004012194794905775,
      "loss": 1.5574,
      "step": 1058
    },
    {
      "epoch": 2.118,
      "grad_norm": 6.454950332641602,
      "learning_rate": 0.0003995377640737917,
      "loss": 1.5382,
      "step": 1059
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.1057804822921753,
      "learning_rate": 0.00039785870022580075,
      "loss": 1.4374,
      "step": 1060
    },
    {
      "epoch": 2.122,
      "grad_norm": 1.7634357213974,
      "learning_rate": 0.0003961822953611478,
      "loss": 1.5338,
      "step": 1061
    },
    {
      "epoch": 2.124,
      "grad_norm": 8.173855781555176,
      "learning_rate": 0.00039450855688263485,
      "loss": 1.5708,
      "step": 1062
    },
    {
      "epoch": 2.126,
      "grad_norm": 1.2074002027511597,
      "learning_rate": 0.00039283749218128883,
      "loss": 1.4126,
      "step": 1063
    },
    {
      "epoch": 2.128,
      "grad_norm": 1.9371141195297241,
      "learning_rate": 0.00039116910863633037,
      "loss": 1.3951,
      "step": 1064
    },
    {
      "epoch": 2.13,
      "grad_norm": 33.359153747558594,
      "learning_rate": 0.00038950341361513875,
      "loss": 1.3885,
      "step": 1065
    },
    {
      "epoch": 2.132,
      "grad_norm": 3.599066734313965,
      "learning_rate": 0.0003878404144732234,
      "loss": 1.5311,
      "step": 1066
    },
    {
      "epoch": 2.134,
      "grad_norm": 1.608940839767456,
      "learning_rate": 0.00038618011855418743,
      "loss": 1.3552,
      "step": 1067
    },
    {
      "epoch": 2.136,
      "grad_norm": 1.1430599689483643,
      "learning_rate": 0.0003845225331896974,
      "loss": 1.4299,
      "step": 1068
    },
    {
      "epoch": 2.138,
      "grad_norm": 1.764215111732483,
      "learning_rate": 0.00038286766569945076,
      "loss": 1.407,
      "step": 1069
    },
    {
      "epoch": 2.14,
      "grad_norm": 3.649754762649536,
      "learning_rate": 0.00038121552339114164,
      "loss": 1.2992,
      "step": 1070
    },
    {
      "epoch": 2.142,
      "grad_norm": 3.6045987606048584,
      "learning_rate": 0.0003795661135604319,
      "loss": 1.4668,
      "step": 1071
    },
    {
      "epoch": 2.144,
      "grad_norm": 8.755365371704102,
      "learning_rate": 0.00037791944349091646,
      "loss": 1.3809,
      "step": 1072
    },
    {
      "epoch": 2.146,
      "grad_norm": 1.030545949935913,
      "learning_rate": 0.0003762755204540914,
      "loss": 1.4045,
      "step": 1073
    },
    {
      "epoch": 2.148,
      "grad_norm": 2.1903977394104004,
      "learning_rate": 0.0003746343517093229,
      "loss": 1.3737,
      "step": 1074
    },
    {
      "epoch": 2.15,
      "grad_norm": 4.371890544891357,
      "learning_rate": 0.0003729959445038136,
      "loss": 1.4806,
      "step": 1075
    },
    {
      "epoch": 2.152,
      "grad_norm": 9.237951278686523,
      "learning_rate": 0.00037136030607257196,
      "loss": 1.4682,
      "step": 1076
    },
    {
      "epoch": 2.154,
      "grad_norm": 1.4541581869125366,
      "learning_rate": 0.00036972744363838073,
      "loss": 1.4706,
      "step": 1077
    },
    {
      "epoch": 2.156,
      "grad_norm": 2.0935959815979004,
      "learning_rate": 0.000368097364411763,
      "loss": 1.5443,
      "step": 1078
    },
    {
      "epoch": 2.158,
      "grad_norm": 27.463117599487305,
      "learning_rate": 0.00036647007559095204,
      "loss": 1.4952,
      "step": 1079
    },
    {
      "epoch": 2.16,
      "grad_norm": 18.95720100402832,
      "learning_rate": 0.00036484558436185934,
      "loss": 1.5568,
      "step": 1080
    },
    {
      "epoch": 2.162,
      "grad_norm": 2.086049795150757,
      "learning_rate": 0.000363223897898041,
      "loss": 1.317,
      "step": 1081
    },
    {
      "epoch": 2.164,
      "grad_norm": 5.561103343963623,
      "learning_rate": 0.00036160502336067004,
      "loss": 1.485,
      "step": 1082
    },
    {
      "epoch": 2.166,
      "grad_norm": 0.7889677286148071,
      "learning_rate": 0.00035998896789850066,
      "loss": 1.4709,
      "step": 1083
    },
    {
      "epoch": 2.168,
      "grad_norm": 2.7035467624664307,
      "learning_rate": 0.0003583757386478389,
      "loss": 1.4464,
      "step": 1084
    },
    {
      "epoch": 2.17,
      "grad_norm": 7.7713518142700195,
      "learning_rate": 0.0003567653427325107,
      "loss": 1.3433,
      "step": 1085
    },
    {
      "epoch": 2.172,
      "grad_norm": 1.0579267740249634,
      "learning_rate": 0.00035515778726382964,
      "loss": 1.5421,
      "step": 1086
    },
    {
      "epoch": 2.174,
      "grad_norm": 1.7527494430541992,
      "learning_rate": 0.00035355307934056666,
      "loss": 1.4741,
      "step": 1087
    },
    {
      "epoch": 2.176,
      "grad_norm": 3.2561089992523193,
      "learning_rate": 0.00035195122604891904,
      "loss": 1.5284,
      "step": 1088
    },
    {
      "epoch": 2.178,
      "grad_norm": 2.013047695159912,
      "learning_rate": 0.00035035223446247733,
      "loss": 1.5274,
      "step": 1089
    },
    {
      "epoch": 2.18,
      "grad_norm": 3.55403733253479,
      "learning_rate": 0.0003487561116421958,
      "loss": 1.5074,
      "step": 1090
    },
    {
      "epoch": 2.182,
      "grad_norm": 1.6918343305587769,
      "learning_rate": 0.0003471628646363597,
      "loss": 1.5359,
      "step": 1091
    },
    {
      "epoch": 2.184,
      "grad_norm": 3.640425443649292,
      "learning_rate": 0.00034557250048055575,
      "loss": 1.5086,
      "step": 1092
    },
    {
      "epoch": 2.186,
      "grad_norm": 8.002474784851074,
      "learning_rate": 0.00034398502619764,
      "loss": 1.5793,
      "step": 1093
    },
    {
      "epoch": 2.188,
      "grad_norm": 1.8383015394210815,
      "learning_rate": 0.00034240044879770806,
      "loss": 1.3644,
      "step": 1094
    },
    {
      "epoch": 2.19,
      "grad_norm": 5.637462139129639,
      "learning_rate": 0.0003408187752780624,
      "loss": 1.5348,
      "step": 1095
    },
    {
      "epoch": 2.192,
      "grad_norm": 28.471771240234375,
      "learning_rate": 0.00033924001262318205,
      "loss": 1.4557,
      "step": 1096
    },
    {
      "epoch": 2.194,
      "grad_norm": 5.945024013519287,
      "learning_rate": 0.00033766416780469256,
      "loss": 1.506,
      "step": 1097
    },
    {
      "epoch": 2.196,
      "grad_norm": 15.404632568359375,
      "learning_rate": 0.00033609124778133425,
      "loss": 1.5247,
      "step": 1098
    },
    {
      "epoch": 2.198,
      "grad_norm": 0.9348123669624329,
      "learning_rate": 0.000334521259498933,
      "loss": 1.4086,
      "step": 1099
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.3462345600128174,
      "learning_rate": 0.0003329542098903674,
      "loss": 1.4328,
      "step": 1100
    },
    {
      "epoch": 2.2,
      "eval_loss": 1.4338510036468506,
      "eval_runtime": 214.1156,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 1100
    },
    {
      "epoch": 2.202,
      "grad_norm": 2.245863199234009,
      "learning_rate": 0.00033139010587553906,
      "loss": 1.3668,
      "step": 1101
    },
    {
      "epoch": 2.204,
      "grad_norm": 4.926334857940674,
      "learning_rate": 0.0003298289543613429,
      "loss": 1.5517,
      "step": 1102
    },
    {
      "epoch": 2.206,
      "grad_norm": 3.9432637691497803,
      "learning_rate": 0.00032827076224163556,
      "loss": 1.4232,
      "step": 1103
    },
    {
      "epoch": 2.208,
      "grad_norm": 6.452599048614502,
      "learning_rate": 0.0003267155363972052,
      "loss": 1.4589,
      "step": 1104
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.8233836889266968,
      "learning_rate": 0.00032516328369574247,
      "loss": 1.5985,
      "step": 1105
    },
    {
      "epoch": 2.212,
      "grad_norm": 9.372137069702148,
      "learning_rate": 0.0003236140109918071,
      "loss": 1.4312,
      "step": 1106
    },
    {
      "epoch": 2.214,
      "grad_norm": 0.969922661781311,
      "learning_rate": 0.0003220677251268008,
      "loss": 1.358,
      "step": 1107
    },
    {
      "epoch": 2.216,
      "grad_norm": 2.115934133529663,
      "learning_rate": 0.0003205244329289354,
      "loss": 1.3811,
      "step": 1108
    },
    {
      "epoch": 2.218,
      "grad_norm": 4.14463996887207,
      "learning_rate": 0.0003189841412132027,
      "loss": 1.5168,
      "step": 1109
    },
    {
      "epoch": 2.22,
      "grad_norm": 4.850559711456299,
      "learning_rate": 0.0003174468567813461,
      "loss": 1.4738,
      "step": 1110
    },
    {
      "epoch": 2.222,
      "grad_norm": 2.416461944580078,
      "learning_rate": 0.0003159125864218272,
      "loss": 1.4484,
      "step": 1111
    },
    {
      "epoch": 2.224,
      "grad_norm": 1.490014672279358,
      "learning_rate": 0.0003143813369097991,
      "loss": 1.3883,
      "step": 1112
    },
    {
      "epoch": 2.226,
      "grad_norm": 1.245261311531067,
      "learning_rate": 0.0003128531150070749,
      "loss": 1.4521,
      "step": 1113
    },
    {
      "epoch": 2.228,
      "grad_norm": 1.7160382270812988,
      "learning_rate": 0.00031132792746209836,
      "loss": 1.3933,
      "step": 1114
    },
    {
      "epoch": 2.23,
      "grad_norm": 4.12685489654541,
      "learning_rate": 0.0003098057810099135,
      "loss": 1.3932,
      "step": 1115
    },
    {
      "epoch": 2.232,
      "grad_norm": 1.520469307899475,
      "learning_rate": 0.00030828668237213553,
      "loss": 1.5211,
      "step": 1116
    },
    {
      "epoch": 2.234,
      "grad_norm": 1.852678894996643,
      "learning_rate": 0.00030677063825692067,
      "loss": 1.4074,
      "step": 1117
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 5.4724578857421875,
      "learning_rate": 0.0003052576553589368,
      "loss": 1.3617,
      "step": 1118
    },
    {
      "epoch": 2.238,
      "grad_norm": 1.8117420673370361,
      "learning_rate": 0.00030374774035933405,
      "loss": 1.4311,
      "step": 1119
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.0521152019500732,
      "learning_rate": 0.0003022408999257148,
      "loss": 1.4879,
      "step": 1120
    },
    {
      "epoch": 2.242,
      "grad_norm": 1.000592827796936,
      "learning_rate": 0.00030073714071210454,
      "loss": 1.3666,
      "step": 1121
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 6.549644947052002,
      "learning_rate": 0.0002992364693589228,
      "loss": 1.49,
      "step": 1122
    },
    {
      "epoch": 2.246,
      "grad_norm": 0.7403791546821594,
      "learning_rate": 0.00029773889249295294,
      "loss": 1.3977,
      "step": 1123
    },
    {
      "epoch": 2.248,
      "grad_norm": 6.669475078582764,
      "learning_rate": 0.0002962444167273138,
      "loss": 1.4388,
      "step": 1124
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.3783252239227295,
      "learning_rate": 0.0002947530486614303,
      "loss": 1.4329,
      "step": 1125
    },
    {
      "epoch": 2.252,
      "grad_norm": 1.9638735055923462,
      "learning_rate": 0.0002932647948810037,
      "loss": 1.422,
      "step": 1126
    },
    {
      "epoch": 2.254,
      "grad_norm": 1.7893165349960327,
      "learning_rate": 0.0002917796619579831,
      "loss": 1.3317,
      "step": 1127
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 6.1649932861328125,
      "learning_rate": 0.0002902976564505365,
      "loss": 1.3949,
      "step": 1128
    },
    {
      "epoch": 2.258,
      "grad_norm": 3.4103662967681885,
      "learning_rate": 0.00028881878490302125,
      "loss": 1.5315,
      "step": 1129
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.0407284498214722,
      "learning_rate": 0.0002873430538459559,
      "loss": 1.4355,
      "step": 1130
    },
    {
      "epoch": 2.262,
      "grad_norm": 1.2306675910949707,
      "learning_rate": 0.00028587046979599066,
      "loss": 1.4364,
      "step": 1131
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.864334762096405,
      "learning_rate": 0.00028440103925587903,
      "loss": 1.3661,
      "step": 1132
    },
    {
      "epoch": 2.266,
      "grad_norm": 1.4115577936172485,
      "learning_rate": 0.0002829347687144489,
      "loss": 1.3607,
      "step": 1133
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.9426435828208923,
      "learning_rate": 0.00028147166464657426,
      "loss": 1.3823,
      "step": 1134
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.8431143164634705,
      "learning_rate": 0.00028001173351314626,
      "loss": 1.3589,
      "step": 1135
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.5220171213150024,
      "learning_rate": 0.00027855498176104434,
      "loss": 1.4001,
      "step": 1136
    },
    {
      "epoch": 2.274,
      "grad_norm": 1.4877541065216064,
      "learning_rate": 0.0002771014158231088,
      "loss": 1.3122,
      "step": 1137
    },
    {
      "epoch": 2.276,
      "grad_norm": 3.64432954788208,
      "learning_rate": 0.0002756510421181112,
      "loss": 1.4737,
      "step": 1138
    },
    {
      "epoch": 2.278,
      "grad_norm": 2.4634439945220947,
      "learning_rate": 0.0002742038670507271,
      "loss": 1.4154,
      "step": 1139
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.1168545484542847,
      "learning_rate": 0.0002727598970115068,
      "loss": 1.4352,
      "step": 1140
    },
    {
      "epoch": 2.282,
      "grad_norm": 1.8172204494476318,
      "learning_rate": 0.00027131913837684797,
      "loss": 1.421,
      "step": 1141
    },
    {
      "epoch": 2.284,
      "grad_norm": 5.799687385559082,
      "learning_rate": 0.0002698815975089668,
      "loss": 1.4235,
      "step": 1142
    },
    {
      "epoch": 2.286,
      "grad_norm": 1.3103296756744385,
      "learning_rate": 0.00026844728075587043,
      "loss": 1.3586,
      "step": 1143
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.7836785316467285,
      "learning_rate": 0.00026701619445132854,
      "loss": 1.3252,
      "step": 1144
    },
    {
      "epoch": 2.29,
      "grad_norm": 2.5218122005462646,
      "learning_rate": 0.00026558834491484574,
      "loss": 1.416,
      "step": 1145
    },
    {
      "epoch": 2.292,
      "grad_norm": 1.4250282049179077,
      "learning_rate": 0.00026416373845163343,
      "loss": 1.4146,
      "step": 1146
    },
    {
      "epoch": 2.294,
      "grad_norm": 1.122071385383606,
      "learning_rate": 0.00026274238135258113,
      "loss": 1.618,
      "step": 1147
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.7812299132347107,
      "learning_rate": 0.000261324279894231,
      "loss": 1.3142,
      "step": 1148
    },
    {
      "epoch": 2.298,
      "grad_norm": 1.9164458513259888,
      "learning_rate": 0.00025990944033874806,
      "loss": 1.4713,
      "step": 1149
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.7907142639160156,
      "learning_rate": 0.00025849786893389294,
      "loss": 1.4498,
      "step": 1150
    },
    {
      "epoch": 2.302,
      "grad_norm": 2.0458130836486816,
      "learning_rate": 0.0002570895719129949,
      "loss": 1.4398,
      "step": 1151
    },
    {
      "epoch": 2.304,
      "grad_norm": 2.874405860900879,
      "learning_rate": 0.00025568455549492306,
      "loss": 1.3935,
      "step": 1152
    },
    {
      "epoch": 2.306,
      "grad_norm": 3.337311267852783,
      "learning_rate": 0.0002542828258840606,
      "loss": 1.3853,
      "step": 1153
    },
    {
      "epoch": 2.308,
      "grad_norm": 3.1857752799987793,
      "learning_rate": 0.0002528843892702768,
      "loss": 1.3168,
      "step": 1154
    },
    {
      "epoch": 2.31,
      "grad_norm": 1.036961317062378,
      "learning_rate": 0.0002514892518288988,
      "loss": 1.306,
      "step": 1155
    },
    {
      "epoch": 2.312,
      "grad_norm": 1.5084275007247925,
      "learning_rate": 0.0002500974197206857,
      "loss": 1.3691,
      "step": 1156
    },
    {
      "epoch": 2.314,
      "grad_norm": 1.9796240329742432,
      "learning_rate": 0.00024870889909179927,
      "loss": 1.3298,
      "step": 1157
    },
    {
      "epoch": 2.316,
      "grad_norm": 1.3672115802764893,
      "learning_rate": 0.0002473236960737794,
      "loss": 1.3414,
      "step": 1158
    },
    {
      "epoch": 2.318,
      "grad_norm": 2.9906227588653564,
      "learning_rate": 0.0002459418167835159,
      "loss": 1.4946,
      "step": 1159
    },
    {
      "epoch": 2.32,
      "grad_norm": 6.249600410461426,
      "learning_rate": 0.00024456326732322074,
      "loss": 1.5794,
      "step": 1160
    },
    {
      "epoch": 2.322,
      "grad_norm": 3.2954230308532715,
      "learning_rate": 0.00024318805378040242,
      "loss": 1.4692,
      "step": 1161
    },
    {
      "epoch": 2.324,
      "grad_norm": 1.3668034076690674,
      "learning_rate": 0.00024181618222783742,
      "loss": 1.4446,
      "step": 1162
    },
    {
      "epoch": 2.326,
      "grad_norm": 84.48590850830078,
      "learning_rate": 0.00024044765872354524,
      "loss": 1.3639,
      "step": 1163
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.9840608835220337,
      "learning_rate": 0.00023908248931076037,
      "loss": 1.3641,
      "step": 1164
    },
    {
      "epoch": 2.33,
      "grad_norm": 2.231775999069214,
      "learning_rate": 0.0002377206800179068,
      "loss": 1.3527,
      "step": 1165
    },
    {
      "epoch": 2.332,
      "grad_norm": 1.4675689935684204,
      "learning_rate": 0.00023636223685857005,
      "loss": 1.4075,
      "step": 1166
    },
    {
      "epoch": 2.334,
      "grad_norm": 17.16322898864746,
      "learning_rate": 0.00023500716583147065,
      "loss": 1.3505,
      "step": 1167
    },
    {
      "epoch": 2.336,
      "grad_norm": 1.3022204637527466,
      "learning_rate": 0.0002336554729204391,
      "loss": 1.3346,
      "step": 1168
    },
    {
      "epoch": 2.338,
      "grad_norm": 15.233122825622559,
      "learning_rate": 0.0002323071640943879,
      "loss": 1.4969,
      "step": 1169
    },
    {
      "epoch": 2.34,
      "grad_norm": 3.2118563652038574,
      "learning_rate": 0.00023096224530728672,
      "loss": 1.3437,
      "step": 1170
    },
    {
      "epoch": 2.342,
      "grad_norm": 5.238992214202881,
      "learning_rate": 0.00022962072249813482,
      "loss": 1.4173,
      "step": 1171
    },
    {
      "epoch": 2.344,
      "grad_norm": 2.6543209552764893,
      "learning_rate": 0.00022828260159093438,
      "loss": 1.401,
      "step": 1172
    },
    {
      "epoch": 2.346,
      "grad_norm": 1.7512329816818237,
      "learning_rate": 0.0002269478884946663,
      "loss": 1.435,
      "step": 1173
    },
    {
      "epoch": 2.348,
      "grad_norm": 5.35312032699585,
      "learning_rate": 0.00022561658910326244,
      "loss": 1.4171,
      "step": 1174
    },
    {
      "epoch": 2.35,
      "grad_norm": 1.17755126953125,
      "learning_rate": 0.0002242887092955801,
      "loss": 1.3538,
      "step": 1175
    },
    {
      "epoch": 2.352,
      "grad_norm": 12.431075096130371,
      "learning_rate": 0.0002229642549353772,
      "loss": 1.5489,
      "step": 1176
    },
    {
      "epoch": 2.354,
      "grad_norm": 1.2866507768630981,
      "learning_rate": 0.00022164323187128342,
      "loss": 1.3856,
      "step": 1177
    },
    {
      "epoch": 2.356,
      "grad_norm": 3.983588933944702,
      "learning_rate": 0.00022032564593677773,
      "loss": 1.4198,
      "step": 1178
    },
    {
      "epoch": 2.358,
      "grad_norm": 1.0592381954193115,
      "learning_rate": 0.0002190115029501606,
      "loss": 1.3673,
      "step": 1179
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.112337112426758,
      "learning_rate": 0.00021770080871452856,
      "loss": 1.3436,
      "step": 1180
    },
    {
      "epoch": 2.362,
      "grad_norm": 3.393646478652954,
      "learning_rate": 0.00021639356901774986,
      "loss": 1.3349,
      "step": 1181
    },
    {
      "epoch": 2.364,
      "grad_norm": 3.5008816719055176,
      "learning_rate": 0.0002150897896324373,
      "loss": 1.4573,
      "step": 1182
    },
    {
      "epoch": 2.366,
      "grad_norm": 3.4373040199279785,
      "learning_rate": 0.00021378947631592283,
      "loss": 1.3747,
      "step": 1183
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.36067533493042,
      "learning_rate": 0.0002124926348102333,
      "loss": 1.5025,
      "step": 1184
    },
    {
      "epoch": 2.37,
      "grad_norm": 11.877326965332031,
      "learning_rate": 0.0002111992708420646,
      "loss": 1.3152,
      "step": 1185
    },
    {
      "epoch": 2.372,
      "grad_norm": 2.590623140335083,
      "learning_rate": 0.00020990939012275556,
      "loss": 1.2909,
      "step": 1186
    },
    {
      "epoch": 2.374,
      "grad_norm": 1.0424957275390625,
      "learning_rate": 0.0002086229983482646,
      "loss": 1.3057,
      "step": 1187
    },
    {
      "epoch": 2.376,
      "grad_norm": 1.3119784593582153,
      "learning_rate": 0.00020734010119914192,
      "loss": 1.4027,
      "step": 1188
    },
    {
      "epoch": 2.378,
      "grad_norm": 19.211124420166016,
      "learning_rate": 0.00020606070434050672,
      "loss": 1.3382,
      "step": 1189
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.9535725116729736,
      "learning_rate": 0.00020478481342202126,
      "loss": 1.4569,
      "step": 1190
    },
    {
      "epoch": 2.382,
      "grad_norm": 1.006026268005371,
      "learning_rate": 0.0002035124340778659,
      "loss": 1.4101,
      "step": 1191
    },
    {
      "epoch": 2.384,
      "grad_norm": 1.2013070583343506,
      "learning_rate": 0.00020224357192671428,
      "loss": 1.3783,
      "step": 1192
    },
    {
      "epoch": 2.386,
      "grad_norm": 0.699104368686676,
      "learning_rate": 0.00020097823257170868,
      "loss": 1.2924,
      "step": 1193
    },
    {
      "epoch": 2.388,
      "grad_norm": 1.4269981384277344,
      "learning_rate": 0.0001997164216004349,
      "loss": 1.5835,
      "step": 1194
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.370706558227539,
      "learning_rate": 0.00019845814458489808,
      "loss": 1.4547,
      "step": 1195
    },
    {
      "epoch": 2.392,
      "grad_norm": 2.510904550552368,
      "learning_rate": 0.00019720340708149776,
      "loss": 1.3298,
      "step": 1196
    },
    {
      "epoch": 2.394,
      "grad_norm": 1.6546653509140015,
      "learning_rate": 0.0001959522146310032,
      "loss": 1.4473,
      "step": 1197
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.659518837928772,
      "learning_rate": 0.00019470457275852949,
      "loss": 1.4079,
      "step": 1198
    },
    {
      "epoch": 2.398,
      "grad_norm": 0.821356475353241,
      "learning_rate": 0.0001934604869735126,
      "loss": 1.3335,
      "step": 1199
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.7878851294517517,
      "learning_rate": 0.00019221996276968522,
      "loss": 1.3215,
      "step": 1200
    },
    {
      "epoch": 2.4,
      "eval_loss": 1.390537142753601,
      "eval_runtime": 214.0862,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 1200
    },
    {
      "epoch": 2.402,
      "grad_norm": 2.742572546005249,
      "learning_rate": 0.00019098300562505265,
      "loss": 1.4776,
      "step": 1201
    },
    {
      "epoch": 2.404,
      "grad_norm": 79.19727325439453,
      "learning_rate": 0.00018974962100186833,
      "loss": 1.3067,
      "step": 1202
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.9465988874435425,
      "learning_rate": 0.00018851981434660992,
      "loss": 1.5145,
      "step": 1203
    },
    {
      "epoch": 2.408,
      "grad_norm": 1.5599569082260132,
      "learning_rate": 0.00018729359108995546,
      "loss": 1.3052,
      "step": 1204
    },
    {
      "epoch": 2.41,
      "grad_norm": 2.5035452842712402,
      "learning_rate": 0.00018607095664675865,
      "loss": 1.3915,
      "step": 1205
    },
    {
      "epoch": 2.412,
      "grad_norm": 3.179184913635254,
      "learning_rate": 0.00018485191641602595,
      "loss": 1.5909,
      "step": 1206
    },
    {
      "epoch": 2.414,
      "grad_norm": 0.9914897680282593,
      "learning_rate": 0.00018363647578089183,
      "loss": 1.3817,
      "step": 1207
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.08564293384552,
      "learning_rate": 0.0001824246401085955,
      "loss": 1.3669,
      "step": 1208
    },
    {
      "epoch": 2.418,
      "grad_norm": 1.7766510248184204,
      "learning_rate": 0.00018121641475045704,
      "loss": 1.3412,
      "step": 1209
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.942346215248108,
      "learning_rate": 0.000180011805041854,
      "loss": 1.5579,
      "step": 1210
    },
    {
      "epoch": 2.422,
      "grad_norm": 1.1343406438827515,
      "learning_rate": 0.00017881081630219742,
      "loss": 1.5585,
      "step": 1211
    },
    {
      "epoch": 2.424,
      "grad_norm": 1.5225021839141846,
      "learning_rate": 0.00017761345383490878,
      "loss": 1.4747,
      "step": 1212
    },
    {
      "epoch": 2.426,
      "grad_norm": 2.0287203788757324,
      "learning_rate": 0.00017641972292739628,
      "loss": 1.4818,
      "step": 1213
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.6257117986679077,
      "learning_rate": 0.00017522962885103144,
      "loss": 1.2934,
      "step": 1214
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.8974955081939697,
      "learning_rate": 0.00017404317686112636,
      "loss": 1.2667,
      "step": 1215
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.9077282547950745,
      "learning_rate": 0.00017286037219690976,
      "loss": 1.3243,
      "step": 1216
    },
    {
      "epoch": 2.434,
      "grad_norm": 2.520907163619995,
      "learning_rate": 0.00017168122008150456,
      "loss": 1.4369,
      "step": 1217
    },
    {
      "epoch": 2.436,
      "grad_norm": 1.2192753553390503,
      "learning_rate": 0.0001705057257219036,
      "loss": 1.3105,
      "step": 1218
    },
    {
      "epoch": 2.438,
      "grad_norm": 2.71848201751709,
      "learning_rate": 0.00016933389430894897,
      "loss": 1.4001,
      "step": 1219
    },
    {
      "epoch": 2.44,
      "grad_norm": 4.3209004402160645,
      "learning_rate": 0.00016816573101730638,
      "loss": 1.3435,
      "step": 1220
    },
    {
      "epoch": 2.442,
      "grad_norm": 5.136044502258301,
      "learning_rate": 0.00016700124100544413,
      "loss": 1.4014,
      "step": 1221
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.9541531205177307,
      "learning_rate": 0.00016584042941560973,
      "loss": 1.3025,
      "step": 1222
    },
    {
      "epoch": 2.446,
      "grad_norm": 2.06672739982605,
      "learning_rate": 0.00016468330137380693,
      "loss": 1.3796,
      "step": 1223
    },
    {
      "epoch": 2.448,
      "grad_norm": 1.988333821296692,
      "learning_rate": 0.00016352986198977327,
      "loss": 1.2873,
      "step": 1224
    },
    {
      "epoch": 2.45,
      "grad_norm": 4.3863372802734375,
      "learning_rate": 0.0001623801163569585,
      "loss": 1.4682,
      "step": 1225
    },
    {
      "epoch": 2.452,
      "grad_norm": 1.2710143327713013,
      "learning_rate": 0.0001612340695525004,
      "loss": 1.4536,
      "step": 1226
    },
    {
      "epoch": 2.454,
      "grad_norm": 0.979885995388031,
      "learning_rate": 0.00016009172663720351,
      "loss": 1.4266,
      "step": 1227
    },
    {
      "epoch": 2.456,
      "grad_norm": 1.192293405532837,
      "learning_rate": 0.00015895309265551638,
      "loss": 1.3745,
      "step": 1228
    },
    {
      "epoch": 2.458,
      "grad_norm": 0.9024198055267334,
      "learning_rate": 0.00015781817263550867,
      "loss": 1.3653,
      "step": 1229
    },
    {
      "epoch": 2.46,
      "grad_norm": 5.169487953186035,
      "learning_rate": 0.00015668697158885102,
      "loss": 1.5662,
      "step": 1230
    },
    {
      "epoch": 2.462,
      "grad_norm": 2.833366632461548,
      "learning_rate": 0.00015555949451079054,
      "loss": 1.5548,
      "step": 1231
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.510101318359375,
      "learning_rate": 0.00015443574638013003,
      "loss": 1.3004,
      "step": 1232
    },
    {
      "epoch": 2.466,
      "grad_norm": 1.3593652248382568,
      "learning_rate": 0.0001533157321592058,
      "loss": 1.4241,
      "step": 1233
    },
    {
      "epoch": 2.468,
      "grad_norm": 6.7973785400390625,
      "learning_rate": 0.00015219945679386505,
      "loss": 1.3404,
      "step": 1234
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 77.60971069335938,
      "learning_rate": 0.00015108692521344526,
      "loss": 1.4594,
      "step": 1235
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.9650120735168457,
      "learning_rate": 0.00014997814233075202,
      "loss": 1.3104,
      "step": 1236
    },
    {
      "epoch": 2.474,
      "grad_norm": 1.9385401010513306,
      "learning_rate": 0.00014887311304203666,
      "loss": 1.3915,
      "step": 1237
    },
    {
      "epoch": 2.476,
      "grad_norm": 1.2166950702667236,
      "learning_rate": 0.00014777184222697538,
      "loss": 1.3409,
      "step": 1238
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 2.2099030017852783,
      "learning_rate": 0.00014667433474864678,
      "loss": 1.3817,
      "step": 1239
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.1107075214385986,
      "learning_rate": 0.00014558059545351142,
      "loss": 1.4978,
      "step": 1240
    },
    {
      "epoch": 2.482,
      "grad_norm": 0.726648211479187,
      "learning_rate": 0.00014449062917139055,
      "loss": 1.38,
      "step": 1241
    },
    {
      "epoch": 2.484,
      "grad_norm": 1.0070494413375854,
      "learning_rate": 0.00014340444071544368,
      "loss": 1.2708,
      "step": 1242
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 1.3642923831939697,
      "learning_rate": 0.00014232203488214812,
      "loss": 1.4086,
      "step": 1243
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.7798131108283997,
      "learning_rate": 0.00014124341645127702,
      "loss": 1.3075,
      "step": 1244
    },
    {
      "epoch": 2.49,
      "grad_norm": 1.1125379800796509,
      "learning_rate": 0.00014016859018587958,
      "loss": 1.4453,
      "step": 1245
    },
    {
      "epoch": 2.492,
      "grad_norm": 2.003199338912964,
      "learning_rate": 0.00013909756083225843,
      "loss": 1.3823,
      "step": 1246
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 0.8782459497451782,
      "learning_rate": 0.00013803033311995074,
      "loss": 1.3473,
      "step": 1247
    },
    {
      "epoch": 2.496,
      "grad_norm": 1.2688109874725342,
      "learning_rate": 0.00013696691176170507,
      "loss": 1.2997,
      "step": 1248
    },
    {
      "epoch": 2.498,
      "grad_norm": 1.286109447479248,
      "learning_rate": 0.00013590730145346153,
      "loss": 1.4858,
      "step": 1249
    },
    {
      "epoch": 2.5,
      "grad_norm": 3.677837610244751,
      "learning_rate": 0.00013485150687433166,
      "loss": 1.3544,
      "step": 1250
    },
    {
      "epoch": 2.502,
      "grad_norm": 1.0295273065567017,
      "learning_rate": 0.00013379953268657696,
      "loss": 1.3205,
      "step": 1251
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.8235036730766296,
      "learning_rate": 0.00013275138353558823,
      "loss": 1.2927,
      "step": 1252
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 1.2012337446212769,
      "learning_rate": 0.00013170706404986644,
      "loss": 1.3722,
      "step": 1253
    },
    {
      "epoch": 2.508,
      "grad_norm": 2.96921968460083,
      "learning_rate": 0.00013066657884099964,
      "loss": 1.3787,
      "step": 1254
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.2346760034561157,
      "learning_rate": 0.0001296299325036454,
      "loss": 1.3533,
      "step": 1255
    },
    {
      "epoch": 2.512,
      "grad_norm": 1.2419804334640503,
      "learning_rate": 0.00012859712961550874,
      "loss": 1.3703,
      "step": 1256
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 16.138168334960938,
      "learning_rate": 0.0001275681747373224,
      "loss": 1.3488,
      "step": 1257
    },
    {
      "epoch": 2.516,
      "grad_norm": 1.4424024820327759,
      "learning_rate": 0.0001265430724128277,
      "loss": 1.4275,
      "step": 1258
    },
    {
      "epoch": 2.518,
      "grad_norm": 1.5136431455612183,
      "learning_rate": 0.00012552182716875227,
      "loss": 1.2483,
      "step": 1259
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.675560474395752,
      "learning_rate": 0.00012450444351479195,
      "loss": 1.4182,
      "step": 1260
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 2.881479263305664,
      "learning_rate": 0.00012349092594359036,
      "loss": 1.2755,
      "step": 1261
    },
    {
      "epoch": 2.524,
      "grad_norm": 4.19934606552124,
      "learning_rate": 0.0001224812789307187,
      "loss": 1.3691,
      "step": 1262
    },
    {
      "epoch": 2.526,
      "grad_norm": 1.0508939027786255,
      "learning_rate": 0.00012147550693465636,
      "loss": 1.3693,
      "step": 1263
    },
    {
      "epoch": 2.528,
      "grad_norm": 2.6826016902923584,
      "learning_rate": 0.0001204736143967714,
      "loss": 1.2922,
      "step": 1264
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 6.664820194244385,
      "learning_rate": 0.00011947560574130011,
      "loss": 1.3816,
      "step": 1265
    },
    {
      "epoch": 2.532,
      "grad_norm": 1.193026065826416,
      "learning_rate": 0.00011848148537532844,
      "loss": 1.3439,
      "step": 1266
    },
    {
      "epoch": 2.534,
      "grad_norm": 2.478780746459961,
      "learning_rate": 0.000117491257688772,
      "loss": 1.2486,
      "step": 1267
    },
    {
      "epoch": 2.536,
      "grad_norm": 1.6074475049972534,
      "learning_rate": 0.00011650492705435678,
      "loss": 1.3117,
      "step": 1268
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 2.945525646209717,
      "learning_rate": 0.00011552249782759983,
      "loss": 1.3576,
      "step": 1269
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.9394162893295288,
      "learning_rate": 0.0001145439743467902,
      "loss": 1.3523,
      "step": 1270
    },
    {
      "epoch": 2.542,
      "grad_norm": 2.591676712036133,
      "learning_rate": 0.00011356936093296944,
      "loss": 1.3908,
      "step": 1271
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.2159655094146729,
      "learning_rate": 0.00011259866188991275,
      "loss": 1.3677,
      "step": 1272
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 1.0237839221954346,
      "learning_rate": 0.00011163188150411019,
      "loss": 1.3176,
      "step": 1273
    },
    {
      "epoch": 2.548,
      "grad_norm": 1.2569693326950073,
      "learning_rate": 0.0001106690240447471,
      "loss": 1.3586,
      "step": 1274
    },
    {
      "epoch": 2.55,
      "grad_norm": 1.5829451084136963,
      "learning_rate": 0.00010971009376368612,
      "loss": 1.4981,
      "step": 1275
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.6252260804176331,
      "learning_rate": 0.00010875509489544744,
      "loss": 1.3643,
      "step": 1276
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 3.0096099376678467,
      "learning_rate": 0.00010780403165719088,
      "loss": 1.3205,
      "step": 1277
    },
    {
      "epoch": 2.556,
      "grad_norm": 2.2751290798187256,
      "learning_rate": 0.0001068569082486972,
      "loss": 1.4415,
      "step": 1278
    },
    {
      "epoch": 2.558,
      "grad_norm": 5.4249067306518555,
      "learning_rate": 0.00010591372885234885,
      "loss": 1.2465,
      "step": 1279
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.1251662969589233,
      "learning_rate": 0.0001049744976331124,
      "loss": 1.348,
      "step": 1280
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 1.956041932106018,
      "learning_rate": 0.00010403921873851951,
      "loss": 1.4468,
      "step": 1281
    },
    {
      "epoch": 2.564,
      "grad_norm": 4.138284683227539,
      "learning_rate": 0.00010310789629864903,
      "loss": 1.4556,
      "step": 1282
    },
    {
      "epoch": 2.566,
      "grad_norm": 3.741492509841919,
      "learning_rate": 0.00010218053442610842,
      "loss": 1.3372,
      "step": 1283
    },
    {
      "epoch": 2.568,
      "grad_norm": 1.3607054948806763,
      "learning_rate": 0.00010125713721601593,
      "loss": 1.5063,
      "step": 1284
    },
    {
      "epoch": 2.57,
      "grad_norm": 2.394218921661377,
      "learning_rate": 0.00010033770874598224,
      "loss": 1.4011,
      "step": 1285
    },
    {
      "epoch": 2.572,
      "grad_norm": 3.823643684387207,
      "learning_rate": 9.942225307609243e-05,
      "loss": 1.3738,
      "step": 1286
    },
    {
      "epoch": 2.574,
      "grad_norm": 4.390570163726807,
      "learning_rate": 9.851077424888844e-05,
      "loss": 1.3705,
      "step": 1287
    },
    {
      "epoch": 2.576,
      "grad_norm": 6.685685157775879,
      "learning_rate": 9.760327628935073e-05,
      "loss": 1.3001,
      "step": 1288
    },
    {
      "epoch": 2.578,
      "grad_norm": 1.5184977054595947,
      "learning_rate": 9.669976320488083e-05,
      "loss": 1.3298,
      "step": 1289
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.0268468856811523,
      "learning_rate": 9.580023898528345e-05,
      "loss": 1.3344,
      "step": 1290
    },
    {
      "epoch": 2.582,
      "grad_norm": 1.8232938051223755,
      "learning_rate": 9.490470760274917e-05,
      "loss": 1.3891,
      "step": 1291
    },
    {
      "epoch": 2.584,
      "grad_norm": 1.145608901977539,
      "learning_rate": 9.401317301183654e-05,
      "loss": 1.281,
      "step": 1292
    },
    {
      "epoch": 2.586,
      "grad_norm": 9.23915958404541,
      "learning_rate": 9.31256391494546e-05,
      "loss": 1.4369,
      "step": 1293
    },
    {
      "epoch": 2.588,
      "grad_norm": 2.2033636569976807,
      "learning_rate": 9.224210993484605e-05,
      "loss": 1.276,
      "step": 1294
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.9252792000770569,
      "learning_rate": 9.136258926956886e-05,
      "loss": 1.3404,
      "step": 1295
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.8449146747589111,
      "learning_rate": 9.048708103748071e-05,
      "loss": 1.3672,
      "step": 1296
    },
    {
      "epoch": 2.594,
      "grad_norm": 1.5935146808624268,
      "learning_rate": 8.961558910472e-05,
      "loss": 1.2857,
      "step": 1297
    },
    {
      "epoch": 2.596,
      "grad_norm": 5.209104061126709,
      "learning_rate": 8.874811731969023e-05,
      "loss": 1.4156,
      "step": 1298
    },
    {
      "epoch": 2.598,
      "grad_norm": 1.4442328214645386,
      "learning_rate": 8.788466951304208e-05,
      "loss": 1.3321,
      "step": 1299
    },
    {
      "epoch": 2.6,
      "grad_norm": 28.162939071655273,
      "learning_rate": 8.702524949765645e-05,
      "loss": 1.3796,
      "step": 1300
    },
    {
      "epoch": 2.6,
      "eval_loss": 1.3595499992370605,
      "eval_runtime": 214.1418,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 1300
    },
    {
      "epoch": 2.602,
      "grad_norm": 5.030116081237793,
      "learning_rate": 8.616986106862911e-05,
      "loss": 1.4209,
      "step": 1301
    },
    {
      "epoch": 2.604,
      "grad_norm": 1.7504876852035522,
      "learning_rate": 8.531850800325181e-05,
      "loss": 1.317,
      "step": 1302
    },
    {
      "epoch": 2.606,
      "grad_norm": 1.180922508239746,
      "learning_rate": 8.447119406099702e-05,
      "loss": 1.3054,
      "step": 1303
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.4799851179122925,
      "learning_rate": 8.36279229835012e-05,
      "loss": 1.3564,
      "step": 1304
    },
    {
      "epoch": 2.61,
      "grad_norm": 2.1617701053619385,
      "learning_rate": 8.278869849454718e-05,
      "loss": 1.3999,
      "step": 1305
    },
    {
      "epoch": 2.612,
      "grad_norm": 2.35727858543396,
      "learning_rate": 8.19535243000491e-05,
      "loss": 1.2708,
      "step": 1306
    },
    {
      "epoch": 2.614,
      "grad_norm": 3.5595998764038086,
      "learning_rate": 8.112240408803583e-05,
      "loss": 1.4088,
      "step": 1307
    },
    {
      "epoch": 2.616,
      "grad_norm": 1.4570655822753906,
      "learning_rate": 8.029534152863383e-05,
      "loss": 1.4279,
      "step": 1308
    },
    {
      "epoch": 2.618,
      "grad_norm": 0.6379416584968567,
      "learning_rate": 7.947234027405159e-05,
      "loss": 1.3999,
      "step": 1309
    },
    {
      "epoch": 2.62,
      "grad_norm": 3.2757439613342285,
      "learning_rate": 7.865340395856324e-05,
      "loss": 1.4233,
      "step": 1310
    },
    {
      "epoch": 2.622,
      "grad_norm": 1.986275315284729,
      "learning_rate": 7.783853619849279e-05,
      "loss": 1.3615,
      "step": 1311
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.9616504907608032,
      "learning_rate": 7.702774059219786e-05,
      "loss": 1.3028,
      "step": 1312
    },
    {
      "epoch": 2.626,
      "grad_norm": 6.362893581390381,
      "learning_rate": 7.622102072005432e-05,
      "loss": 1.4429,
      "step": 1313
    },
    {
      "epoch": 2.628,
      "grad_norm": 1.823315143585205,
      "learning_rate": 7.54183801444398e-05,
      "loss": 1.4119,
      "step": 1314
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.6894094347953796,
      "learning_rate": 7.461982240971799e-05,
      "loss": 1.3036,
      "step": 1315
    },
    {
      "epoch": 2.632,
      "grad_norm": 2.77095890045166,
      "learning_rate": 7.382535104222366e-05,
      "loss": 1.2762,
      "step": 1316
    },
    {
      "epoch": 2.634,
      "grad_norm": 1.2500495910644531,
      "learning_rate": 7.303496955024625e-05,
      "loss": 1.428,
      "step": 1317
    },
    {
      "epoch": 2.636,
      "grad_norm": 1.864804744720459,
      "learning_rate": 7.224868142401542e-05,
      "loss": 1.3448,
      "step": 1318
    },
    {
      "epoch": 2.638,
      "grad_norm": 2.599961042404175,
      "learning_rate": 7.146649013568484e-05,
      "loss": 1.414,
      "step": 1319
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.3033865690231323,
      "learning_rate": 7.068839913931646e-05,
      "loss": 1.5037,
      "step": 1320
    },
    {
      "epoch": 2.642,
      "grad_norm": 2.483215570449829,
      "learning_rate": 6.991441187086633e-05,
      "loss": 1.3214,
      "step": 1321
    },
    {
      "epoch": 2.644,
      "grad_norm": 1.0365939140319824,
      "learning_rate": 6.914453174816904e-05,
      "loss": 1.3363,
      "step": 1322
    },
    {
      "epoch": 2.646,
      "grad_norm": 2.0306484699249268,
      "learning_rate": 6.837876217092198e-05,
      "loss": 1.3866,
      "step": 1323
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.8079751133918762,
      "learning_rate": 6.761710652067177e-05,
      "loss": 1.2348,
      "step": 1324
    },
    {
      "epoch": 2.65,
      "grad_norm": 3.1640498638153076,
      "learning_rate": 6.685956816079752e-05,
      "loss": 1.3401,
      "step": 1325
    },
    {
      "epoch": 2.652,
      "grad_norm": 1.9498093128204346,
      "learning_rate": 6.610615043649714e-05,
      "loss": 1.3221,
      "step": 1326
    },
    {
      "epoch": 2.654,
      "grad_norm": 1.089725136756897,
      "learning_rate": 6.535685667477264e-05,
      "loss": 1.36,
      "step": 1327
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.6186802387237549,
      "learning_rate": 6.46116901844147e-05,
      "loss": 1.2846,
      "step": 1328
    },
    {
      "epoch": 2.658,
      "grad_norm": 1.2884800434112549,
      "learning_rate": 6.387065425598882e-05,
      "loss": 1.3023,
      "step": 1329
    },
    {
      "epoch": 2.66,
      "grad_norm": 6.0183210372924805,
      "learning_rate": 6.313375216182038e-05,
      "loss": 1.372,
      "step": 1330
    },
    {
      "epoch": 2.662,
      "grad_norm": 2.9932780265808105,
      "learning_rate": 6.240098715597975e-05,
      "loss": 1.2305,
      "step": 1331
    },
    {
      "epoch": 2.664,
      "grad_norm": 2.2399368286132812,
      "learning_rate": 6.1672362474269e-05,
      "loss": 1.3209,
      "step": 1332
    },
    {
      "epoch": 2.666,
      "grad_norm": 1.535054326057434,
      "learning_rate": 6.094788133420681e-05,
      "loss": 1.4764,
      "step": 1333
    },
    {
      "epoch": 2.668,
      "grad_norm": 1.1301474571228027,
      "learning_rate": 6.022754693501431e-05,
      "loss": 1.3456,
      "step": 1334
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.514668881893158,
      "learning_rate": 5.95113624576018e-05,
      "loss": 1.3718,
      "step": 1335
    },
    {
      "epoch": 2.672,
      "grad_norm": 92.83503723144531,
      "learning_rate": 5.879933106455304e-05,
      "loss": 1.2772,
      "step": 1336
    },
    {
      "epoch": 2.674,
      "grad_norm": 0.7661105394363403,
      "learning_rate": 5.8091455900113e-05,
      "loss": 1.2551,
      "step": 1337
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.6389372944831848,
      "learning_rate": 5.7387740090172894e-05,
      "loss": 1.277,
      "step": 1338
    },
    {
      "epoch": 2.678,
      "grad_norm": 3.9819178581237793,
      "learning_rate": 5.668818674225684e-05,
      "loss": 1.2965,
      "step": 1339
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.752921998500824,
      "learning_rate": 5.599279894550824e-05,
      "loss": 1.2521,
      "step": 1340
    },
    {
      "epoch": 2.682,
      "grad_norm": 0.8815339803695679,
      "learning_rate": 5.530157977067552e-05,
      "loss": 1.3245,
      "step": 1341
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.7386656999588013,
      "learning_rate": 5.461453227009916e-05,
      "loss": 1.3421,
      "step": 1342
    },
    {
      "epoch": 2.686,
      "grad_norm": 1.0428080558776855,
      "learning_rate": 5.393165947769818e-05,
      "loss": 1.3241,
      "step": 1343
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 3.223761558532715,
      "learning_rate": 5.3252964408956216e-05,
      "loss": 1.373,
      "step": 1344
    },
    {
      "epoch": 2.69,
      "grad_norm": 5.212588787078857,
      "learning_rate": 5.257845006090911e-05,
      "loss": 1.3981,
      "step": 1345
    },
    {
      "epoch": 2.692,
      "grad_norm": 3.3970260620117188,
      "learning_rate": 5.1908119412130586e-05,
      "loss": 1.3948,
      "step": 1346
    },
    {
      "epoch": 2.694,
      "grad_norm": 0.7867531776428223,
      "learning_rate": 5.124197542272002e-05,
      "loss": 1.3072,
      "step": 1347
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.8350265026092529,
      "learning_rate": 5.058002103428905e-05,
      "loss": 1.2714,
      "step": 1348
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.8871043920516968,
      "learning_rate": 4.992225916994819e-05,
      "loss": 1.3315,
      "step": 1349
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.6311898231506348,
      "learning_rate": 4.9268692734294464e-05,
      "loss": 1.3898,
      "step": 1350
    },
    {
      "epoch": 2.702,
      "grad_norm": 1.0099232196807861,
      "learning_rate": 4.8619324613398576e-05,
      "loss": 1.3484,
      "step": 1351
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.091483235359192,
      "learning_rate": 4.797415767479174e-05,
      "loss": 1.3433,
      "step": 1352
    },
    {
      "epoch": 2.706,
      "grad_norm": 1.4040899276733398,
      "learning_rate": 4.733319476745335e-05,
      "loss": 1.3964,
      "step": 1353
    },
    {
      "epoch": 2.708,
      "grad_norm": 1.3859364986419678,
      "learning_rate": 4.6696438721798184e-05,
      "loss": 1.2752,
      "step": 1354
    },
    {
      "epoch": 2.71,
      "grad_norm": 2.1104490756988525,
      "learning_rate": 4.6063892349664236e-05,
      "loss": 1.2821,
      "step": 1355
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.7593425512313843,
      "learning_rate": 4.543555844429992e-05,
      "loss": 1.2656,
      "step": 1356
    },
    {
      "epoch": 2.714,
      "grad_norm": 3.693927049636841,
      "learning_rate": 4.481143978035196e-05,
      "loss": 1.3827,
      "step": 1357
    },
    {
      "epoch": 2.716,
      "grad_norm": 2.778764009475708,
      "learning_rate": 4.41915391138531e-05,
      "loss": 1.2736,
      "step": 1358
    },
    {
      "epoch": 2.718,
      "grad_norm": 5.669633865356445,
      "learning_rate": 4.357585918220986e-05,
      "loss": 1.3973,
      "step": 1359
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.197569727897644,
      "learning_rate": 4.2964402704190555e-05,
      "loss": 1.2943,
      "step": 1360
    },
    {
      "epoch": 2.722,
      "grad_norm": 3.537001609802246,
      "learning_rate": 4.235717237991321e-05,
      "loss": 1.4068,
      "step": 1361
    },
    {
      "epoch": 2.724,
      "grad_norm": 1.8374804258346558,
      "learning_rate": 4.175417089083378e-05,
      "loss": 1.2597,
      "step": 1362
    },
    {
      "epoch": 2.726,
      "grad_norm": 0.9423330426216125,
      "learning_rate": 4.115540089973402e-05,
      "loss": 1.3308,
      "step": 1363
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 12.71887493133545,
      "learning_rate": 4.0560865050709997e-05,
      "loss": 1.5039,
      "step": 1364
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.1704639196395874,
      "learning_rate": 3.997056596916038e-05,
      "loss": 1.3503,
      "step": 1365
    },
    {
      "epoch": 2.732,
      "grad_norm": 10.225996017456055,
      "learning_rate": 3.9384506261774365e-05,
      "loss": 1.304,
      "step": 1366
    },
    {
      "epoch": 2.734,
      "grad_norm": 1.2207021713256836,
      "learning_rate": 3.8802688516521356e-05,
      "loss": 1.3535,
      "step": 1367
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 1.7460576295852661,
      "learning_rate": 3.822511530263806e-05,
      "loss": 1.3526,
      "step": 1368
    },
    {
      "epoch": 2.738,
      "grad_norm": 1.4937540292739868,
      "learning_rate": 3.765178917061818e-05,
      "loss": 1.3991,
      "step": 1369
    },
    {
      "epoch": 2.74,
      "grad_norm": 1.1058980226516724,
      "learning_rate": 3.7082712652200864e-05,
      "loss": 1.2873,
      "step": 1370
    },
    {
      "epoch": 2.742,
      "grad_norm": 0.636858344078064,
      "learning_rate": 3.651788826035895e-05,
      "loss": 1.2362,
      "step": 1371
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 2.899345874786377,
      "learning_rate": 3.595731848928929e-05,
      "loss": 1.2612,
      "step": 1372
    },
    {
      "epoch": 2.746,
      "grad_norm": 3.901045560836792,
      "learning_rate": 3.54010058144002e-05,
      "loss": 1.3398,
      "step": 1373
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.5490731596946716,
      "learning_rate": 3.484895269230137e-05,
      "loss": 1.2813,
      "step": 1374
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.810012698173523,
      "learning_rate": 3.430116156079277e-05,
      "loss": 1.3563,
      "step": 1375
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.238973617553711,
      "learning_rate": 3.375763483885386e-05,
      "loss": 1.3575,
      "step": 1376
    },
    {
      "epoch": 2.754,
      "grad_norm": 1.3049594163894653,
      "learning_rate": 3.321837492663304e-05,
      "loss": 1.2417,
      "step": 1377
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 2.5236306190490723,
      "learning_rate": 3.268338420543726e-05,
      "loss": 1.3706,
      "step": 1378
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.9345670938491821,
      "learning_rate": 3.215266503772085e-05,
      "loss": 1.3306,
      "step": 1379
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.0897942781448364,
      "learning_rate": 3.162621976707558e-05,
      "loss": 1.3644,
      "step": 1380
    },
    {
      "epoch": 2.762,
      "grad_norm": 1.5070908069610596,
      "learning_rate": 3.1104050718220536e-05,
      "loss": 1.3146,
      "step": 1381
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 4.440124034881592,
      "learning_rate": 3.0586160196990894e-05,
      "loss": 1.404,
      "step": 1382
    },
    {
      "epoch": 2.766,
      "grad_norm": 0.9047961831092834,
      "learning_rate": 3.0072550490328753e-05,
      "loss": 1.317,
      "step": 1383
    },
    {
      "epoch": 2.768,
      "grad_norm": 3.9423446655273438,
      "learning_rate": 2.9563223866272858e-05,
      "loss": 1.3501,
      "step": 1384
    },
    {
      "epoch": 2.77,
      "grad_norm": 1.0557442903518677,
      "learning_rate": 2.905818257394799e-05,
      "loss": 1.2754,
      "step": 1385
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 2.962283134460449,
      "learning_rate": 2.855742884355561e-05,
      "loss": 1.2917,
      "step": 1386
    },
    {
      "epoch": 2.774,
      "grad_norm": 0.8391584157943726,
      "learning_rate": 2.806096488636367e-05,
      "loss": 1.2917,
      "step": 1387
    },
    {
      "epoch": 2.776,
      "grad_norm": 1.3458783626556396,
      "learning_rate": 2.756879289469716e-05,
      "loss": 1.3776,
      "step": 1388
    },
    {
      "epoch": 2.778,
      "grad_norm": 2.0184123516082764,
      "learning_rate": 2.7080915041928332e-05,
      "loss": 1.2852,
      "step": 1389
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 6.324461460113525,
      "learning_rate": 2.6597333482466847e-05,
      "loss": 1.2679,
      "step": 1390
    },
    {
      "epoch": 2.782,
      "grad_norm": 18.259050369262695,
      "learning_rate": 2.6118050351750635e-05,
      "loss": 1.3268,
      "step": 1391
    },
    {
      "epoch": 2.784,
      "grad_norm": 1.4757558107376099,
      "learning_rate": 2.564306776623593e-05,
      "loss": 1.3103,
      "step": 1392
    },
    {
      "epoch": 2.786,
      "grad_norm": 2.06400465965271,
      "learning_rate": 2.5172387823388708e-05,
      "loss": 1.2772,
      "step": 1393
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 1.2733182907104492,
      "learning_rate": 2.470601260167471e-05,
      "loss": 1.3229,
      "step": 1394
    },
    {
      "epoch": 2.79,
      "grad_norm": 3.055846691131592,
      "learning_rate": 2.424394416055076e-05,
      "loss": 1.3018,
      "step": 1395
    },
    {
      "epoch": 2.792,
      "grad_norm": 2.504307508468628,
      "learning_rate": 2.3786184540455446e-05,
      "loss": 1.2836,
      "step": 1396
    },
    {
      "epoch": 2.794,
      "grad_norm": 9.200118064880371,
      "learning_rate": 2.3332735762799817e-05,
      "loss": 1.3115,
      "step": 1397
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.997795045375824,
      "learning_rate": 2.2883599829959134e-05,
      "loss": 1.2799,
      "step": 1398
    },
    {
      "epoch": 2.798,
      "grad_norm": 0.9573493003845215,
      "learning_rate": 2.2438778725263232e-05,
      "loss": 1.4427,
      "step": 1399
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.0998618602752686,
      "learning_rate": 2.1998274412988628e-05,
      "loss": 1.3221,
      "step": 1400
    },
    {
      "epoch": 2.8,
      "eval_loss": 1.343103289604187,
      "eval_runtime": 214.1688,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 1400
    },
    {
      "epoch": 2.802,
      "grad_norm": 12.443595886230469,
      "learning_rate": 2.1562088838349425e-05,
      "loss": 1.3198,
      "step": 1401
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 5.192984104156494,
      "learning_rate": 2.11302239274882e-05,
      "loss": 1.3363,
      "step": 1402
    },
    {
      "epoch": 2.806,
      "grad_norm": 1.5991159677505493,
      "learning_rate": 2.0702681587468353e-05,
      "loss": 1.2729,
      "step": 1403
    },
    {
      "epoch": 2.808,
      "grad_norm": 2.3537697792053223,
      "learning_rate": 2.027946370626532e-05,
      "loss": 1.3697,
      "step": 1404
    },
    {
      "epoch": 2.81,
      "grad_norm": 1.4656894207000732,
      "learning_rate": 1.986057215275816e-05,
      "loss": 1.2153,
      "step": 1405
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 1.5709891319274902,
      "learning_rate": 1.9446008776721645e-05,
      "loss": 1.4219,
      "step": 1406
    },
    {
      "epoch": 2.814,
      "grad_norm": 0.8406573534011841,
      "learning_rate": 1.9035775408817403e-05,
      "loss": 1.3721,
      "step": 1407
    },
    {
      "epoch": 2.816,
      "grad_norm": 1.8877817392349243,
      "learning_rate": 1.8629873860586568e-05,
      "loss": 1.4287,
      "step": 1408
    },
    {
      "epoch": 2.818,
      "grad_norm": 0.8872346878051758,
      "learning_rate": 1.822830592444147e-05,
      "loss": 1.3604,
      "step": 1409
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.3683223724365234,
      "learning_rate": 1.7831073373657525e-05,
      "loss": 1.2767,
      "step": 1410
    },
    {
      "epoch": 2.822,
      "grad_norm": 0.9616315960884094,
      "learning_rate": 1.74381779623658e-05,
      "loss": 1.289,
      "step": 1411
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.9405974745750427,
      "learning_rate": 1.7049621425545115e-05,
      "loss": 1.463,
      "step": 1412
    },
    {
      "epoch": 2.826,
      "grad_norm": 1.4589669704437256,
      "learning_rate": 1.6665405479014294e-05,
      "loss": 1.2771,
      "step": 1413
    },
    {
      "epoch": 2.828,
      "grad_norm": 2.010152578353882,
      "learning_rate": 1.6285531819424494e-05,
      "loss": 1.3754,
      "step": 1414
    },
    {
      "epoch": 2.83,
      "grad_norm": 1.6784586906433105,
      "learning_rate": 1.5910002124251975e-05,
      "loss": 1.3994,
      "step": 1415
    },
    {
      "epoch": 2.832,
      "grad_norm": 1.5792698860168457,
      "learning_rate": 1.5538818051790583e-05,
      "loss": 1.3409,
      "step": 1416
    },
    {
      "epoch": 2.834,
      "grad_norm": 0.5761178731918335,
      "learning_rate": 1.5171981241144494e-05,
      "loss": 1.3076,
      "step": 1417
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.9182499647140503,
      "learning_rate": 1.480949331222059e-05,
      "loss": 1.2872,
      "step": 1418
    },
    {
      "epoch": 2.838,
      "grad_norm": 1.1850730180740356,
      "learning_rate": 1.4451355865722105e-05,
      "loss": 1.5251,
      "step": 1419
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.2631254196166992,
      "learning_rate": 1.4097570483140642e-05,
      "loss": 1.3074,
      "step": 1420
    },
    {
      "epoch": 2.842,
      "grad_norm": 1.0897152423858643,
      "learning_rate": 1.3748138726749737e-05,
      "loss": 1.2894,
      "step": 1421
    },
    {
      "epoch": 2.844,
      "grad_norm": 18.472623825073242,
      "learning_rate": 1.3403062139598077e-05,
      "loss": 1.4624,
      "step": 1422
    },
    {
      "epoch": 2.846,
      "grad_norm": 1.4134289026260376,
      "learning_rate": 1.3062342245501958e-05,
      "loss": 1.336,
      "step": 1423
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.9725944995880127,
      "learning_rate": 1.2725980549039506e-05,
      "loss": 1.4583,
      "step": 1424
    },
    {
      "epoch": 2.85,
      "grad_norm": 2.405906915664673,
      "learning_rate": 1.239397853554336e-05,
      "loss": 1.4046,
      "step": 1425
    },
    {
      "epoch": 2.852,
      "grad_norm": 1.7689002752304077,
      "learning_rate": 1.206633767109444e-05,
      "loss": 1.3759,
      "step": 1426
    },
    {
      "epoch": 2.854,
      "grad_norm": 1.895449161529541,
      "learning_rate": 1.1743059402515077e-05,
      "loss": 1.2571,
      "step": 1427
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.9228881597518921,
      "learning_rate": 1.142414515736323e-05,
      "loss": 1.4182,
      "step": 1428
    },
    {
      "epoch": 2.858,
      "grad_norm": 0.8139652609825134,
      "learning_rate": 1.1109596343925721e-05,
      "loss": 1.2693,
      "step": 1429
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.859401524066925,
      "learning_rate": 1.0799414351212234e-05,
      "loss": 1.3024,
      "step": 1430
    },
    {
      "epoch": 2.862,
      "grad_norm": 1.2546038627624512,
      "learning_rate": 1.0493600548948879e-05,
      "loss": 1.3209,
      "step": 1431
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.516687273979187,
      "learning_rate": 1.019215628757264e-05,
      "loss": 1.3138,
      "step": 1432
    },
    {
      "epoch": 2.866,
      "grad_norm": 1.00140380859375,
      "learning_rate": 9.895082898224939e-06,
      "loss": 1.4232,
      "step": 1433
    },
    {
      "epoch": 2.868,
      "grad_norm": 1.5032304525375366,
      "learning_rate": 9.602381692746077e-06,
      "loss": 1.4553,
      "step": 1434
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.7812671661376953,
      "learning_rate": 9.314053963669244e-06,
      "loss": 1.3699,
      "step": 1435
    },
    {
      "epoch": 2.872,
      "grad_norm": 1.207388997077942,
      "learning_rate": 9.030100984214861e-06,
      "loss": 1.4412,
      "step": 1436
    },
    {
      "epoch": 2.874,
      "grad_norm": 2.008342981338501,
      "learning_rate": 8.750524008285132e-06,
      "loss": 1.5716,
      "step": 1437
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.9585667848587036,
      "learning_rate": 8.475324270458163e-06,
      "loss": 1.4466,
      "step": 1438
    },
    {
      "epoch": 2.878,
      "grad_norm": 1.134981632232666,
      "learning_rate": 8.204502985982854e-06,
      "loss": 1.3338,
      "step": 1439
    },
    {
      "epoch": 2.88,
      "grad_norm": 8.927032470703125,
      "learning_rate": 7.93806135077324e-06,
      "loss": 1.3625,
      "step": 1440
    },
    {
      "epoch": 2.882,
      "grad_norm": 1.335434079170227,
      "learning_rate": 7.676000541403494e-06,
      "loss": 1.4139,
      "step": 1441
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.9597547054290771,
      "learning_rate": 7.418321715102705e-06,
      "loss": 1.4281,
      "step": 1442
    },
    {
      "epoch": 2.886,
      "grad_norm": 0.808189868927002,
      "learning_rate": 7.165026009749109e-06,
      "loss": 1.3199,
      "step": 1443
    },
    {
      "epoch": 2.888,
      "grad_norm": 1.118133783340454,
      "learning_rate": 6.9161145438662035e-06,
      "loss": 1.4387,
      "step": 1444
    },
    {
      "epoch": 2.89,
      "grad_norm": 87.0388412475586,
      "learning_rate": 6.6715884166170806e-06,
      "loss": 1.2904,
      "step": 1445
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.7275330424308777,
      "learning_rate": 6.431448707799436e-06,
      "loss": 1.3524,
      "step": 1446
    },
    {
      "epoch": 2.894,
      "grad_norm": 2.2339417934417725,
      "learning_rate": 6.195696477841462e-06,
      "loss": 1.3384,
      "step": 1447
    },
    {
      "epoch": 2.896,
      "grad_norm": 25.27467918395996,
      "learning_rate": 5.964332767796399e-06,
      "loss": 1.3687,
      "step": 1448
    },
    {
      "epoch": 2.898,
      "grad_norm": 4.820312976837158,
      "learning_rate": 5.737358599338438e-06,
      "loss": 1.3707,
      "step": 1449
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.6376563310623169,
      "learning_rate": 5.514774974758274e-06,
      "loss": 1.2778,
      "step": 1450
    },
    {
      "epoch": 2.902,
      "grad_norm": 2.850358009338379,
      "learning_rate": 5.2965828769582225e-06,
      "loss": 1.2868,
      "step": 1451
    },
    {
      "epoch": 2.904,
      "grad_norm": 2.3005099296569824,
      "learning_rate": 5.082783269448443e-06,
      "loss": 1.2494,
      "step": 1452
    },
    {
      "epoch": 2.906,
      "grad_norm": 0.9083008170127869,
      "learning_rate": 4.873377096342058e-06,
      "loss": 1.2897,
      "step": 1453
    },
    {
      "epoch": 2.908,
      "grad_norm": 1.5522806644439697,
      "learning_rate": 4.668365282351372e-06,
      "loss": 1.3147,
      "step": 1454
    },
    {
      "epoch": 2.91,
      "grad_norm": 5.130735874176025,
      "learning_rate": 4.467748732783994e-06,
      "loss": 1.3043,
      "step": 1455
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.6406219601631165,
      "learning_rate": 4.271528333538388e-06,
      "loss": 1.3355,
      "step": 1456
    },
    {
      "epoch": 2.914,
      "grad_norm": 1.0875519514083862,
      "learning_rate": 4.079704951100105e-06,
      "loss": 1.2774,
      "step": 1457
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.747660219669342,
      "learning_rate": 3.892279432538115e-06,
      "loss": 1.3939,
      "step": 1458
    },
    {
      "epoch": 2.918,
      "grad_norm": 2.0891664028167725,
      "learning_rate": 3.7092526055008126e-06,
      "loss": 1.3176,
      "step": 1459
    },
    {
      "epoch": 2.92,
      "grad_norm": 2.5676424503326416,
      "learning_rate": 3.5306252782126846e-06,
      "loss": 1.4016,
      "step": 1460
    },
    {
      "epoch": 2.922,
      "grad_norm": 0.7278285026550293,
      "learning_rate": 3.3563982394704262e-06,
      "loss": 1.3902,
      "step": 1461
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.670030951499939,
      "learning_rate": 3.1865722586397196e-06,
      "loss": 1.3108,
      "step": 1462
    },
    {
      "epoch": 2.926,
      "grad_norm": 10.438676834106445,
      "learning_rate": 3.0211480856513484e-06,
      "loss": 1.2853,
      "step": 1463
    },
    {
      "epoch": 2.928,
      "grad_norm": 1.3079588413238525,
      "learning_rate": 2.860126450998646e-06,
      "loss": 1.2407,
      "step": 1464
    },
    {
      "epoch": 2.93,
      "grad_norm": 3.3738210201263428,
      "learning_rate": 2.7035080657338287e-06,
      "loss": 1.335,
      "step": 1465
    },
    {
      "epoch": 2.932,
      "grad_norm": 1.2011739015579224,
      "learning_rate": 2.5512936214646675e-06,
      "loss": 1.3565,
      "step": 1466
    },
    {
      "epoch": 2.934,
      "grad_norm": 0.7751978635787964,
      "learning_rate": 2.403483790351824e-06,
      "loss": 1.3331,
      "step": 1467
    },
    {
      "epoch": 2.936,
      "grad_norm": 1.5108551979064941,
      "learning_rate": 2.260079225105627e-06,
      "loss": 1.2358,
      "step": 1468
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 1.0770336389541626,
      "learning_rate": 2.1210805589834128e-06,
      "loss": 1.359,
      "step": 1469
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.8919159173965454,
      "learning_rate": 1.986488405786524e-06,
      "loss": 1.3466,
      "step": 1470
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.691551148891449,
      "learning_rate": 1.8563033598575363e-06,
      "loss": 1.3146,
      "step": 1471
    },
    {
      "epoch": 2.944,
      "grad_norm": 1.1293741464614868,
      "learning_rate": 1.7305259960781471e-06,
      "loss": 1.344,
      "step": 1472
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 2.367988348007202,
      "learning_rate": 1.6091568698658465e-06,
      "loss": 1.3701,
      "step": 1473
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.8476109504699707,
      "learning_rate": 1.4921965171720286e-06,
      "loss": 1.3309,
      "step": 1474
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.976094126701355,
      "learning_rate": 1.379645454479661e-06,
      "loss": 1.3883,
      "step": 1475
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.664702296257019,
      "learning_rate": 1.2715041788003978e-06,
      "loss": 1.386,
      "step": 1476
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 0.9244662523269653,
      "learning_rate": 1.1677731676733582e-06,
      "loss": 1.3706,
      "step": 1477
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.5202797055244446,
      "learning_rate": 1.0684528791621296e-06,
      "loss": 1.4378,
      "step": 1478
    },
    {
      "epoch": 2.958,
      "grad_norm": 0.7947484254837036,
      "learning_rate": 9.735437518528788e-07,
      "loss": 1.2864,
      "step": 1479
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.7121357917785645,
      "learning_rate": 8.830462048531329e-07,
      "loss": 1.2833,
      "step": 1480
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 0.7734193801879883,
      "learning_rate": 7.969606377890015e-07,
      "loss": 1.2767,
      "step": 1481
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.7108060121536255,
      "learning_rate": 7.15287430803957e-07,
      "loss": 1.3294,
      "step": 1482
    },
    {
      "epoch": 2.966,
      "grad_norm": 1.4653185606002808,
      "learning_rate": 6.380269445571685e-07,
      "loss": 1.4023,
      "step": 1483
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.8117008805274963,
      "learning_rate": 5.651795202213927e-07,
      "loss": 1.2837,
      "step": 1484
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.5074913501739502,
      "learning_rate": 4.967454794823078e-07,
      "loss": 1.2771,
      "step": 1485
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.4854724705219269,
      "learning_rate": 4.3272512453618184e-07,
      "loss": 1.2002,
      "step": 1486
    },
    {
      "epoch": 2.974,
      "grad_norm": 0.7628499269485474,
      "learning_rate": 3.7311873808931753e-07,
      "loss": 1.4247,
      "step": 1487
    },
    {
      "epoch": 2.976,
      "grad_norm": 1.8027769327163696,
      "learning_rate": 3.179265833562761e-07,
      "loss": 1.3979,
      "step": 1488
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 0.8984485864639282,
      "learning_rate": 2.671489040589892e-07,
      "loss": 1.3102,
      "step": 1489
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.888512372970581,
      "learning_rate": 2.2078592442553725e-07,
      "loss": 1.3254,
      "step": 1490
    },
    {
      "epoch": 2.982,
      "grad_norm": 2.759246349334717,
      "learning_rate": 1.788378491891507e-07,
      "loss": 1.4071,
      "step": 1491
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.9490126967430115,
      "learning_rate": 1.413048635876546e-07,
      "loss": 1.286,
      "step": 1492
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 0.7432870268821716,
      "learning_rate": 1.0818713336202546e-07,
      "loss": 1.4186,
      "step": 1493
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.8252647519111633,
      "learning_rate": 7.948480475616915e-08,
      "loss": 1.3255,
      "step": 1494
    },
    {
      "epoch": 2.99,
      "grad_norm": 3.278613567352295,
      "learning_rate": 5.5198004516254786e-08,
      "loss": 1.3459,
      "step": 1495
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.976569414138794,
      "learning_rate": 3.532683988971552e-08,
      "loss": 1.3058,
      "step": 1496
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 0.767716646194458,
      "learning_rate": 1.987139862524856e-08,
      "loss": 1.2692,
      "step": 1497
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.733283281326294,
      "learning_rate": 8.831748972371045e-09,
      "loss": 1.3327,
      "step": 1498
    },
    {
      "epoch": 2.998,
      "grad_norm": 0.9959025979042053,
      "learning_rate": 2.2079396805319007e-09,
      "loss": 1.2714,
      "step": 1499
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.3590599298477173,
      "learning_rate": 0.0,
      "loss": 1.3749,
      "step": 1500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.337794542312622,
      "eval_runtime": 213.9578,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 1500
    },
    {
      "epoch": 1.501,
      "grad_norm": 0.6443982720375061,
      "learning_rate": 0.0013842267553958371,
      "loss": 1.3149,
      "step": 1501
    },
    {
      "epoch": 1.502,
      "grad_norm": 1.749169945716858,
      "learning_rate": 0.001383500619123451,
      "loss": 1.6076,
      "step": 1502
    },
    {
      "epoch": 1.5030000000000001,
      "grad_norm": 67.70389556884766,
      "learning_rate": 0.0013827742456960976,
      "loss": 1.5568,
      "step": 1503
    },
    {
      "epoch": 1.504,
      "grad_norm": 21.07728385925293,
      "learning_rate": 0.0013820476355629631,
      "loss": 1.683,
      "step": 1504
    },
    {
      "epoch": 1.505,
      "grad_norm": 11.539412498474121,
      "learning_rate": 0.00138132078917338,
      "loss": 1.9116,
      "step": 1505
    },
    {
      "epoch": 1.506,
      "grad_norm": 27.270206451416016,
      "learning_rate": 0.001380593706976826,
      "loss": 1.7742,
      "step": 1506
    },
    {
      "epoch": 1.5070000000000001,
      "grad_norm": 4.56522798538208,
      "learning_rate": 0.0013798663894229258,
      "loss": 1.9121,
      "step": 1507
    },
    {
      "epoch": 1.508,
      "grad_norm": 2.1179871559143066,
      "learning_rate": 0.0013791388369614487,
      "loss": 1.7301,
      "step": 1508
    },
    {
      "epoch": 1.509,
      "grad_norm": 4.036448955535889,
      "learning_rate": 0.0013784110500423103,
      "loss": 1.5876,
      "step": 1509
    },
    {
      "epoch": 1.51,
      "grad_norm": 4.444431304931641,
      "learning_rate": 0.0013776830291155702,
      "loss": 1.7465,
      "step": 1510
    },
    {
      "epoch": 1.5110000000000001,
      "grad_norm": 2.2502355575561523,
      "learning_rate": 0.001376954774631433,
      "loss": 1.5942,
      "step": 1511
    },
    {
      "epoch": 1.512,
      "grad_norm": 6.872323513031006,
      "learning_rate": 0.001376226287040248,
      "loss": 1.7945,
      "step": 1512
    },
    {
      "epoch": 1.513,
      "grad_norm": 1.5739001035690308,
      "learning_rate": 0.0013754975667925088,
      "loss": 1.6283,
      "step": 1513
    },
    {
      "epoch": 1.514,
      "grad_norm": 1.346057415008545,
      "learning_rate": 0.0013747686143388519,
      "loss": 1.5933,
      "step": 1514
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 11.441910743713379,
      "learning_rate": 0.0013740394301300584,
      "loss": 1.5168,
      "step": 1515
    },
    {
      "epoch": 1.516,
      "grad_norm": 2.653092384338379,
      "learning_rate": 0.0013733100146170521,
      "loss": 1.698,
      "step": 1516
    },
    {
      "epoch": 1.517,
      "grad_norm": 1.4330004453659058,
      "learning_rate": 0.0013725803682509005,
      "loss": 1.5924,
      "step": 1517
    },
    {
      "epoch": 1.518,
      "grad_norm": 4.514871597290039,
      "learning_rate": 0.0013718504914828134,
      "loss": 1.6095,
      "step": 1518
    },
    {
      "epoch": 1.5190000000000001,
      "grad_norm": 3.1531026363372803,
      "learning_rate": 0.0013711203847641428,
      "loss": 1.5574,
      "step": 1519
    },
    {
      "epoch": 1.52,
      "grad_norm": 2.0564472675323486,
      "learning_rate": 0.0013703900485463835,
      "loss": 1.6375,
      "step": 1520
    },
    {
      "epoch": 1.521,
      "grad_norm": 1.551644206047058,
      "learning_rate": 0.0013696594832811717,
      "loss": 1.6099,
      "step": 1521
    },
    {
      "epoch": 1.522,
      "grad_norm": 1.778922438621521,
      "learning_rate": 0.0013689286894202862,
      "loss": 1.6954,
      "step": 1522
    },
    {
      "epoch": 1.5230000000000001,
      "grad_norm": 7.425695419311523,
      "learning_rate": 0.0013681976674156456,
      "loss": 1.421,
      "step": 1523
    },
    {
      "epoch": 1.524,
      "grad_norm": 20.853782653808594,
      "learning_rate": 0.0013674664177193107,
      "loss": 1.5767,
      "step": 1524
    },
    {
      "epoch": 1.525,
      "grad_norm": 4.462945461273193,
      "learning_rate": 0.0013667349407834833,
      "loss": 1.7535,
      "step": 1525
    },
    {
      "epoch": 1.526,
      "grad_norm": 8.187407493591309,
      "learning_rate": 0.0013660032370605047,
      "loss": 1.5114,
      "step": 1526
    },
    {
      "epoch": 1.5270000000000001,
      "grad_norm": 21.749074935913086,
      "learning_rate": 0.0013652713070028573,
      "loss": 1.6605,
      "step": 1527
    },
    {
      "epoch": 1.528,
      "grad_norm": 221.6731719970703,
      "learning_rate": 0.0013645391510631632,
      "loss": 1.6247,
      "step": 1528
    },
    {
      "epoch": 1.529,
      "grad_norm": 8.873772621154785,
      "learning_rate": 0.0013638067696941838,
      "loss": 1.7457,
      "step": 1529
    },
    {
      "epoch": 1.53,
      "grad_norm": 4.138433456420898,
      "learning_rate": 0.0013630741633488212,
      "loss": 1.543,
      "step": 1530
    },
    {
      "epoch": 1.5310000000000001,
      "grad_norm": 6.674230575561523,
      "learning_rate": 0.0013623413324801148,
      "loss": 1.552,
      "step": 1531
    },
    {
      "epoch": 1.532,
      "grad_norm": 4.276679992675781,
      "learning_rate": 0.0013616082775412437,
      "loss": 1.6594,
      "step": 1532
    },
    {
      "epoch": 1.533,
      "grad_norm": 1.3193862438201904,
      "learning_rate": 0.001360874998985526,
      "loss": 1.5163,
      "step": 1533
    },
    {
      "epoch": 1.534,
      "grad_norm": 2.2899177074432373,
      "learning_rate": 0.001360141497266418,
      "loss": 1.7218,
      "step": 1534
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 1.6798466444015503,
      "learning_rate": 0.0013594077728375127,
      "loss": 1.5562,
      "step": 1535
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.0417227745056152,
      "learning_rate": 0.0013586738261525428,
      "loss": 1.4913,
      "step": 1536
    },
    {
      "epoch": 1.537,
      "grad_norm": 13.495000839233398,
      "learning_rate": 0.001357939657665377,
      "loss": 1.6225,
      "step": 1537
    },
    {
      "epoch": 1.538,
      "grad_norm": 3.1349549293518066,
      "learning_rate": 0.0013572052678300218,
      "loss": 1.8803,
      "step": 1538
    },
    {
      "epoch": 1.5390000000000001,
      "grad_norm": 3.3455395698547363,
      "learning_rate": 0.00135647065710062,
      "loss": 1.7115,
      "step": 1539
    },
    {
      "epoch": 1.54,
      "grad_norm": 2.697233200073242,
      "learning_rate": 0.0013557358259314518,
      "loss": 1.8857,
      "step": 1540
    },
    {
      "epoch": 1.541,
      "grad_norm": 1.3683608770370483,
      "learning_rate": 0.0013550007747769332,
      "loss": 1.7089,
      "step": 1541
    },
    {
      "epoch": 1.542,
      "grad_norm": 1.154591679573059,
      "learning_rate": 0.0013542655040916162,
      "loss": 1.5997,
      "step": 1542
    },
    {
      "epoch": 1.5430000000000001,
      "grad_norm": 4.929232597351074,
      "learning_rate": 0.001353530014330189,
      "loss": 1.7012,
      "step": 1543
    },
    {
      "epoch": 1.544,
      "grad_norm": 4.21314811706543,
      "learning_rate": 0.0013527943059474749,
      "loss": 1.7235,
      "step": 1544
    },
    {
      "epoch": 1.545,
      "grad_norm": 1.326290249824524,
      "learning_rate": 0.0013520583793984323,
      "loss": 1.6045,
      "step": 1545
    },
    {
      "epoch": 1.546,
      "grad_norm": 10.427184104919434,
      "learning_rate": 0.0013513222351381547,
      "loss": 1.5084,
      "step": 1546
    },
    {
      "epoch": 1.5470000000000002,
      "grad_norm": 3.6172542572021484,
      "learning_rate": 0.0013505858736218704,
      "loss": 1.7491,
      "step": 1547
    },
    {
      "epoch": 1.548,
      "grad_norm": 3.1397719383239746,
      "learning_rate": 0.001349849295304942,
      "loss": 1.7283,
      "step": 1548
    },
    {
      "epoch": 1.549,
      "grad_norm": 2.289877414703369,
      "learning_rate": 0.0013491125006428657,
      "loss": 1.6488,
      "step": 1549
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.6383558511734009,
      "learning_rate": 0.001348375490091272,
      "loss": 1.5603,
      "step": 1550
    },
    {
      "epoch": 1.5510000000000002,
      "grad_norm": 2.6318163871765137,
      "learning_rate": 0.0013476382641059244,
      "loss": 1.5461,
      "step": 1551
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.5722626447677612,
      "learning_rate": 0.0013469008231427207,
      "loss": 1.5014,
      "step": 1552
    },
    {
      "epoch": 1.553,
      "grad_norm": 1.6149780750274658,
      "learning_rate": 0.0013461631676576903,
      "loss": 1.5108,
      "step": 1553
    },
    {
      "epoch": 1.554,
      "grad_norm": 2.8002078533172607,
      "learning_rate": 0.001345425298106996,
      "loss": 1.5319,
      "step": 1554
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 2.2293648719787598,
      "learning_rate": 0.0013446872149469329,
      "loss": 1.5339,
      "step": 1555
    },
    {
      "epoch": 1.556,
      "grad_norm": 3.391878843307495,
      "learning_rate": 0.0013439489186339282,
      "loss": 1.4909,
      "step": 1556
    },
    {
      "epoch": 1.557,
      "grad_norm": 1.284062385559082,
      "learning_rate": 0.0013432104096245407,
      "loss": 1.5257,
      "step": 1557
    },
    {
      "epoch": 1.558,
      "grad_norm": 1.798819661140442,
      "learning_rate": 0.001342471688375461,
      "loss": 1.4548,
      "step": 1558
    },
    {
      "epoch": 1.5590000000000002,
      "grad_norm": 1.5892261266708374,
      "learning_rate": 0.0013417327553435104,
      "loss": 1.4993,
      "step": 1559
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.8594083786010742,
      "learning_rate": 0.0013409936109856422,
      "loss": 1.4574,
      "step": 1560
    },
    {
      "epoch": 1.561,
      "grad_norm": 1.455919623374939,
      "learning_rate": 0.0013402542557589396,
      "loss": 1.4735,
      "step": 1561
    },
    {
      "epoch": 1.562,
      "grad_norm": 1.9523935317993164,
      "learning_rate": 0.001339514690120616,
      "loss": 1.6729,
      "step": 1562
    },
    {
      "epoch": 1.563,
      "grad_norm": 1.3425081968307495,
      "learning_rate": 0.0013387749145280159,
      "loss": 1.5581,
      "step": 1563
    },
    {
      "epoch": 1.564,
      "grad_norm": 1.6617040634155273,
      "learning_rate": 0.0013380349294386125,
      "loss": 1.6535,
      "step": 1564
    },
    {
      "epoch": 1.565,
      "grad_norm": 1.2619404792785645,
      "learning_rate": 0.0013372947353100092,
      "loss": 1.4551,
      "step": 1565
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 3.168994665145874,
      "learning_rate": 0.0013365543325999387,
      "loss": 1.4535,
      "step": 1566
    },
    {
      "epoch": 1.567,
      "grad_norm": 1.2252753973007202,
      "learning_rate": 0.0013358137217662626,
      "loss": 1.6202,
      "step": 1567
    },
    {
      "epoch": 1.568,
      "grad_norm": 7.008842468261719,
      "learning_rate": 0.0013350729032669704,
      "loss": 1.4991,
      "step": 1568
    },
    {
      "epoch": 1.569,
      "grad_norm": 4.200235366821289,
      "learning_rate": 0.0013343318775601819,
      "loss": 1.4335,
      "step": 1569
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 2.5551884174346924,
      "learning_rate": 0.001333590645104143,
      "loss": 1.5765,
      "step": 1570
    },
    {
      "epoch": 1.571,
      "grad_norm": 0.928292453289032,
      "learning_rate": 0.001332849206357229,
      "loss": 1.4701,
      "step": 1571
    },
    {
      "epoch": 1.572,
      "grad_norm": 2.191009283065796,
      "learning_rate": 0.0013321075617779411,
      "loss": 1.5185,
      "step": 1572
    },
    {
      "epoch": 1.573,
      "grad_norm": 1.7619465589523315,
      "learning_rate": 0.0013313657118249104,
      "loss": 1.4959,
      "step": 1573
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 1.4704748392105103,
      "learning_rate": 0.0013306236569568916,
      "loss": 1.4363,
      "step": 1574
    },
    {
      "epoch": 1.575,
      "grad_norm": 4.425975799560547,
      "learning_rate": 0.0013298813976327694,
      "loss": 1.5232,
      "step": 1575
    },
    {
      "epoch": 1.576,
      "grad_norm": 13.02016544342041,
      "learning_rate": 0.0013291389343115525,
      "loss": 1.9609,
      "step": 1576
    },
    {
      "epoch": 1.577,
      "grad_norm": 8.86670970916748,
      "learning_rate": 0.0013283962674523768,
      "loss": 1.6937,
      "step": 1577
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 5.77834415435791,
      "learning_rate": 0.0013276533975145046,
      "loss": 1.8205,
      "step": 1578
    },
    {
      "epoch": 1.579,
      "grad_norm": 7.198775768280029,
      "learning_rate": 0.0013269103249573227,
      "loss": 1.7838,
      "step": 1579
    },
    {
      "epoch": 1.58,
      "grad_norm": 3.6669201850891113,
      "learning_rate": 0.0013261670502403436,
      "loss": 1.5856,
      "step": 1580
    },
    {
      "epoch": 1.581,
      "grad_norm": 2.2003626823425293,
      "learning_rate": 0.001325423573823205,
      "loss": 1.8521,
      "step": 1581
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 3.40228271484375,
      "learning_rate": 0.0013246798961656693,
      "loss": 1.4811,
      "step": 1582
    },
    {
      "epoch": 1.583,
      "grad_norm": 2.8566131591796875,
      "learning_rate": 0.0013239360177276227,
      "loss": 1.4859,
      "step": 1583
    },
    {
      "epoch": 1.584,
      "grad_norm": 5.717894554138184,
      "learning_rate": 0.001323191938969077,
      "loss": 1.5374,
      "step": 1584
    },
    {
      "epoch": 1.585,
      "grad_norm": 1.1986660957336426,
      "learning_rate": 0.0013224476603501663,
      "loss": 1.5321,
      "step": 1585
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 1.07758629322052,
      "learning_rate": 0.0013217031823311487,
      "loss": 1.4241,
      "step": 1586
    },
    {
      "epoch": 1.587,
      "grad_norm": 43.159061431884766,
      "learning_rate": 0.0013209585053724072,
      "loss": 1.7312,
      "step": 1587
    },
    {
      "epoch": 1.588,
      "grad_norm": 18.137178421020508,
      "learning_rate": 0.001320213629934445,
      "loss": 1.4956,
      "step": 1588
    },
    {
      "epoch": 1.589,
      "grad_norm": 1.7586041688919067,
      "learning_rate": 0.0013194685564778905,
      "loss": 1.5259,
      "step": 1589
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 1.7326881885528564,
      "learning_rate": 0.0013187232854634933,
      "loss": 1.4449,
      "step": 1590
    },
    {
      "epoch": 1.591,
      "grad_norm": 1.224414348602295,
      "learning_rate": 0.0013179778173521253,
      "loss": 1.4974,
      "step": 1591
    },
    {
      "epoch": 1.592,
      "grad_norm": 2.841806173324585,
      "learning_rate": 0.0013172321526047806,
      "loss": 1.5195,
      "step": 1592
    },
    {
      "epoch": 1.593,
      "grad_norm": 1.6313766241073608,
      "learning_rate": 0.001316486291682575,
      "loss": 1.5808,
      "step": 1593
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 1.8374974727630615,
      "learning_rate": 0.0013157402350467448,
      "loss": 1.4803,
      "step": 1594
    },
    {
      "epoch": 1.595,
      "grad_norm": 19.586538314819336,
      "learning_rate": 0.0013149939831586484,
      "loss": 1.9576,
      "step": 1595
    },
    {
      "epoch": 1.596,
      "grad_norm": 9.422592163085938,
      "learning_rate": 0.0013142475364797644,
      "loss": 1.7831,
      "step": 1596
    },
    {
      "epoch": 1.597,
      "grad_norm": 10.075748443603516,
      "learning_rate": 0.0013135008954716916,
      "loss": 1.7996,
      "step": 1597
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 14.203496932983398,
      "learning_rate": 0.0013127540605961492,
      "loss": 2.1251,
      "step": 1598
    },
    {
      "epoch": 1.599,
      "grad_norm": 5.1939616203308105,
      "learning_rate": 0.0013120070323149767,
      "loss": 1.5367,
      "step": 1599
    },
    {
      "epoch": 1.6,
      "grad_norm": 77.73196411132812,
      "learning_rate": 0.0013112598110901327,
      "loss": 1.716,
      "step": 1600
    },
    {
      "epoch": 1.6,
      "eval_loss": 1.8015620708465576,
      "eval_runtime": 213.3985,
      "eval_samples_per_second": 0.469,
      "eval_steps_per_second": 0.469,
      "step": 1600
    },
    {
      "epoch": 1.601,
      "grad_norm": 288.57269287109375,
      "learning_rate": 0.0013105123973836954,
      "loss": 1.9087,
      "step": 1601
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 3.727754592895508,
      "learning_rate": 0.0013097647916578618,
      "loss": 1.8177,
      "step": 1602
    },
    {
      "epoch": 1.603,
      "grad_norm": 5.392819881439209,
      "learning_rate": 0.0013090169943749475,
      "loss": 1.8787,
      "step": 1603
    },
    {
      "epoch": 1.604,
      "grad_norm": 8.17737865447998,
      "learning_rate": 0.001308269005997387,
      "loss": 1.8296,
      "step": 1604
    },
    {
      "epoch": 1.605,
      "grad_norm": 6.270727157592773,
      "learning_rate": 0.001307520826987733,
      "loss": 1.6701,
      "step": 1605
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 7.529212951660156,
      "learning_rate": 0.0013067724578086557,
      "loss": 1.689,
      "step": 1606
    },
    {
      "epoch": 1.607,
      "grad_norm": 4.764654636383057,
      "learning_rate": 0.0013060238989229425,
      "loss": 1.6049,
      "step": 1607
    },
    {
      "epoch": 1.608,
      "grad_norm": 4.389397144317627,
      "learning_rate": 0.0013052751507934997,
      "loss": 1.6279,
      "step": 1608
    },
    {
      "epoch": 1.609,
      "grad_norm": 2.060213804244995,
      "learning_rate": 0.0013045262138833488,
      "loss": 1.4172,
      "step": 1609
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 1.2376694679260254,
      "learning_rate": 0.0013037770886556292,
      "loss": 1.5394,
      "step": 1610
    },
    {
      "epoch": 1.611,
      "grad_norm": 11.755853652954102,
      "learning_rate": 0.0013030277755735965,
      "loss": 1.4385,
      "step": 1611
    },
    {
      "epoch": 1.612,
      "grad_norm": 1.1559218168258667,
      "learning_rate": 0.0013022782751006218,
      "loss": 1.4645,
      "step": 1612
    },
    {
      "epoch": 1.613,
      "grad_norm": 4.760447025299072,
      "learning_rate": 0.001301528587700193,
      "loss": 1.5065,
      "step": 1613
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 4.074640274047852,
      "learning_rate": 0.0013007787138359136,
      "loss": 1.5419,
      "step": 1614
    },
    {
      "epoch": 1.615,
      "grad_norm": 10.177834510803223,
      "learning_rate": 0.0013000286539715015,
      "loss": 1.6449,
      "step": 1615
    },
    {
      "epoch": 1.616,
      "grad_norm": 7.322189807891846,
      "learning_rate": 0.00129927840857079,
      "loss": 1.7024,
      "step": 1616
    },
    {
      "epoch": 1.617,
      "grad_norm": 37.13429260253906,
      "learning_rate": 0.001298527978097728,
      "loss": 1.5704,
      "step": 1617
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 2.272714853286743,
      "learning_rate": 0.0012977773630163778,
      "loss": 1.6698,
      "step": 1618
    },
    {
      "epoch": 1.619,
      "grad_norm": 32.23123550415039,
      "learning_rate": 0.0012970265637909165,
      "loss": 1.644,
      "step": 1619
    },
    {
      "epoch": 1.62,
      "grad_norm": 3.8991594314575195,
      "learning_rate": 0.001296275580885634,
      "loss": 1.7151,
      "step": 1620
    },
    {
      "epoch": 1.621,
      "grad_norm": 3.363098382949829,
      "learning_rate": 0.0012955244147649354,
      "loss": 1.5949,
      "step": 1621
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 1.3229776620864868,
      "learning_rate": 0.0012947730658933378,
      "loss": 1.544,
      "step": 1622
    },
    {
      "epoch": 1.623,
      "grad_norm": 2.5292294025421143,
      "learning_rate": 0.0012940215347354723,
      "loss": 1.514,
      "step": 1623
    },
    {
      "epoch": 1.624,
      "grad_norm": 3.3249928951263428,
      "learning_rate": 0.0012932698217560815,
      "loss": 1.6474,
      "step": 1624
    },
    {
      "epoch": 1.625,
      "grad_norm": 27.482513427734375,
      "learning_rate": 0.0012925179274200214,
      "loss": 1.5549,
      "step": 1625
    },
    {
      "epoch": 1.626,
      "grad_norm": 46.50253677368164,
      "learning_rate": 0.0012917658521922602,
      "loss": 1.8019,
      "step": 1626
    },
    {
      "epoch": 1.627,
      "grad_norm": 14.863025665283203,
      "learning_rate": 0.0012910135965378776,
      "loss": 1.6718,
      "step": 1627
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 16.962064743041992,
      "learning_rate": 0.0012902611609220645,
      "loss": 1.7457,
      "step": 1628
    },
    {
      "epoch": 1.629,
      "grad_norm": 20.890966415405273,
      "learning_rate": 0.0012895085458101239,
      "loss": 1.7293,
      "step": 1629
    },
    {
      "epoch": 1.63,
      "grad_norm": 27.201440811157227,
      "learning_rate": 0.0012887557516674695,
      "loss": 1.7607,
      "step": 1630
    },
    {
      "epoch": 1.631,
      "grad_norm": 14.272224426269531,
      "learning_rate": 0.0012880027789596254,
      "loss": 1.9817,
      "step": 1631
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 34.64626693725586,
      "learning_rate": 0.0012872496281522263,
      "loss": 1.8367,
      "step": 1632
    },
    {
      "epoch": 1.633,
      "grad_norm": 15.779417991638184,
      "learning_rate": 0.0012864962997110173,
      "loss": 1.995,
      "step": 1633
    },
    {
      "epoch": 1.634,
      "grad_norm": 49.13778305053711,
      "learning_rate": 0.001285742794101853,
      "loss": 2.1536,
      "step": 1634
    },
    {
      "epoch": 1.635,
      "grad_norm": 88.4688720703125,
      "learning_rate": 0.0012849891117906978,
      "loss": 2.6762,
      "step": 1635
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 107.74565887451172,
      "learning_rate": 0.0012842352532436254,
      "loss": 3.0411,
      "step": 1636
    },
    {
      "epoch": 1.637,
      "grad_norm": 328.71826171875,
      "learning_rate": 0.0012834812189268178,
      "loss": 3.1655,
      "step": 1637
    },
    {
      "epoch": 1.638,
      "grad_norm": 4239.3515625,
      "learning_rate": 0.001282727009306567,
      "loss": 3.2272,
      "step": 1638
    },
    {
      "epoch": 1.639,
      "grad_norm": 1527.8321533203125,
      "learning_rate": 0.0012819726248492723,
      "loss": 3.9914,
      "step": 1639
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 2881.525146484375,
      "learning_rate": 0.0012812180660214411,
      "loss": 5.0038,
      "step": 1640
    },
    {
      "epoch": 1.641,
      "grad_norm": 71229.0,
      "learning_rate": 0.00128046333328969,
      "loss": 7.9719,
      "step": 1641
    },
    {
      "epoch": 1.642,
      "grad_norm": 21834.171875,
      "learning_rate": 0.0012797084271207412,
      "loss": 13.9609,
      "step": 1642
    },
    {
      "epoch": 1.643,
      "grad_norm": 42432.01953125,
      "learning_rate": 0.0012789533479814251,
      "loss": 19.1904,
      "step": 1643
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 3468.37939453125,
      "learning_rate": 0.0012781980963386798,
      "loss": 17.6608,
      "step": 1644
    },
    {
      "epoch": 1.645,
      "grad_norm": 693.078369140625,
      "learning_rate": 0.001277442672659549,
      "loss": 15.7353,
      "step": 1645
    },
    {
      "epoch": 1.646,
      "grad_norm": 3120.754638671875,
      "learning_rate": 0.0012766870774111828,
      "loss": 13.4105,
      "step": 1646
    },
    {
      "epoch": 1.647,
      "grad_norm": 24641.4765625,
      "learning_rate": 0.001275931311060838,
      "loss": 12.4024,
      "step": 1647
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1168.8165283203125,
      "learning_rate": 0.0012751753740758772,
      "loss": 10.6396,
      "step": 1648
    },
    {
      "epoch": 1.649,
      "grad_norm": 6910.09423828125,
      "learning_rate": 0.0012744192669237678,
      "loss": 10.0449,
      "step": 1649
    },
    {
      "epoch": 1.65,
      "grad_norm": 714.8561401367188,
      "learning_rate": 0.0012736629900720832,
      "loss": 9.7054,
      "step": 1650
    },
    {
      "epoch": 1.651,
      "grad_norm": 1675.8199462890625,
      "learning_rate": 0.0012729065439885008,
      "loss": 9.4523,
      "step": 1651
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 1239.9365234375,
      "learning_rate": 0.001272149929140804,
      "loss": 9.8559,
      "step": 1652
    },
    {
      "epoch": 1.653,
      "grad_norm": 1612.8778076171875,
      "learning_rate": 0.0012713931459968797,
      "loss": 10.1907,
      "step": 1653
    },
    {
      "epoch": 1.654,
      "grad_norm": 6011.4921875,
      "learning_rate": 0.001270636195024719,
      "loss": 11.1699,
      "step": 1654
    },
    {
      "epoch": 1.655,
      "grad_norm": 411.4080810546875,
      "learning_rate": 0.0012698790766924165,
      "loss": 12.2495,
      "step": 1655
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 1171.630126953125,
      "learning_rate": 0.001269121791468171,
      "loss": 12.1913,
      "step": 1656
    },
    {
      "epoch": 1.657,
      "grad_norm": 3602.524169921875,
      "learning_rate": 0.0012683643398202838,
      "loss": 12.7633,
      "step": 1657
    },
    {
      "epoch": 1.658,
      "grad_norm": 68496.953125,
      "learning_rate": 0.0012676067222171595,
      "loss": 12.0561,
      "step": 1658
    },
    {
      "epoch": 1.659,
      "grad_norm": 12700.458984375,
      "learning_rate": 0.0012668489391273053,
      "loss": 13.1928,
      "step": 1659
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 13997.55078125,
      "learning_rate": 0.0012660909910193303,
      "loss": 13.229,
      "step": 1660
    },
    {
      "epoch": 1.661,
      "grad_norm": 21567.921875,
      "learning_rate": 0.0012653328783619466,
      "loss": 12.5439,
      "step": 1661
    },
    {
      "epoch": 1.662,
      "grad_norm": 9422.1044921875,
      "learning_rate": 0.0012645746016239673,
      "loss": 13.939,
      "step": 1662
    },
    {
      "epoch": 1.663,
      "grad_norm": 4551.93408203125,
      "learning_rate": 0.001263816161274307,
      "loss": 14.0831,
      "step": 1663
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 50491.88671875,
      "learning_rate": 0.0012630575577819813,
      "loss": 16.4141,
      "step": 1664
    },
    {
      "epoch": 1.665,
      "grad_norm": 223851.84375,
      "learning_rate": 0.001262298791616108,
      "loss": 16.6899,
      "step": 1665
    },
    {
      "epoch": 1.666,
      "grad_norm": 214830.8125,
      "learning_rate": 0.0012615398632459037,
      "loss": 17.4392,
      "step": 1666
    },
    {
      "epoch": 1.667,
      "grad_norm": 153337.296875,
      "learning_rate": 0.0012607807731406862,
      "loss": 18.5969,
      "step": 1667
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 41859.25,
      "learning_rate": 0.0012600215217698737,
      "loss": 19.7363,
      "step": 1668
    },
    {
      "epoch": 1.669,
      "grad_norm": 6693.5205078125,
      "learning_rate": 0.0012592621096029829,
      "loss": 20.9747,
      "step": 1669
    },
    {
      "epoch": 1.67,
      "grad_norm": 5117.2138671875,
      "learning_rate": 0.0012585025371096307,
      "loss": 21.6168,
      "step": 1670
    },
    {
      "epoch": 1.671,
      "grad_norm": 16874.33984375,
      "learning_rate": 0.0012577428047595342,
      "loss": 22.6173,
      "step": 1671
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 6636.74365234375,
      "learning_rate": 0.0012569829130225072,
      "loss": 21.1614,
      "step": 1672
    },
    {
      "epoch": 1.673,
      "grad_norm": 41731.40234375,
      "learning_rate": 0.0012562228623684637,
      "loss": 19.6008,
      "step": 1673
    },
    {
      "epoch": 1.674,
      "grad_norm": 2303.76953125,
      "learning_rate": 0.0012554626532674154,
      "loss": 13.4304,
      "step": 1674
    },
    {
      "epoch": 1.675,
      "grad_norm": 8304.7607421875,
      "learning_rate": 0.001254702286189472,
      "loss": 12.4009,
      "step": 1675
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 4740.54638671875,
      "learning_rate": 0.0012539417616048403,
      "loss": 11.7207,
      "step": 1676
    },
    {
      "epoch": 1.677,
      "grad_norm": 278118.3125,
      "learning_rate": 0.001253181079983826,
      "loss": 12.3221,
      "step": 1677
    },
    {
      "epoch": 1.678,
      "grad_norm": 7464.62255859375,
      "learning_rate": 0.0012524202417968304,
      "loss": 14.5878,
      "step": 1678
    },
    {
      "epoch": 1.679,
      "grad_norm": 37938.76953125,
      "learning_rate": 0.0012516592475143521,
      "loss": 21.8011,
      "step": 1679
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 4801.7177734375,
      "learning_rate": 0.0012508980976069875,
      "loss": 20.8218,
      "step": 1680
    },
    {
      "epoch": 1.681,
      "grad_norm": 62285.7734375,
      "learning_rate": 0.0012501367925454266,
      "loss": 24.4888,
      "step": 1681
    },
    {
      "epoch": 1.682,
      "grad_norm": 72501.59375,
      "learning_rate": 0.0012493753328004578,
      "loss": 24.7817,
      "step": 1682
    },
    {
      "epoch": 1.683,
      "grad_norm": 149993.0625,
      "learning_rate": 0.001248613718842964,
      "loss": 25.3438,
      "step": 1683
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 22467.50390625,
      "learning_rate": 0.0012478519511439235,
      "loss": 24.8094,
      "step": 1684
    },
    {
      "epoch": 1.685,
      "grad_norm": 14223.6845703125,
      "learning_rate": 0.0012470900301744102,
      "loss": 24.66,
      "step": 1685
    },
    {
      "epoch": 1.686,
      "grad_norm": 1668.0191650390625,
      "learning_rate": 0.0012463279564055916,
      "loss": 25.5256,
      "step": 1686
    },
    {
      "epoch": 1.687,
      "grad_norm": 2679.5703125,
      "learning_rate": 0.0012455657303087316,
      "loss": 26.875,
      "step": 1687
    },
    {
      "epoch": 1.688,
      "grad_norm": 1203.371826171875,
      "learning_rate": 0.0012448033523551865,
      "loss": 24.5206,
      "step": 1688
    },
    {
      "epoch": 1.689,
      "grad_norm": 620.5856323242188,
      "learning_rate": 0.0012440408230164073,
      "loss": 24.1719,
      "step": 1689
    },
    {
      "epoch": 1.69,
      "grad_norm": 75655.0859375,
      "learning_rate": 0.0012432781427639387,
      "loss": 22.987,
      "step": 1690
    },
    {
      "epoch": 1.6909999999999998,
      "grad_norm": 126.39543914794922,
      "learning_rate": 0.0012425153120694183,
      "loss": 21.0567,
      "step": 1691
    },
    {
      "epoch": 1.692,
      "grad_norm": 17809.2109375,
      "learning_rate": 0.0012417523314045777,
      "loss": 17.3086,
      "step": 1692
    },
    {
      "epoch": 1.693,
      "grad_norm": 199.8449249267578,
      "learning_rate": 0.0012409892012412397,
      "loss": 17.1365,
      "step": 1693
    },
    {
      "epoch": 1.694,
      "grad_norm": 7648.2451171875,
      "learning_rate": 0.0012402259220513205,
      "loss": 16.4359,
      "step": 1694
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 631.8130493164062,
      "learning_rate": 0.0012394624943068287,
      "loss": 15.2949,
      "step": 1695
    },
    {
      "epoch": 1.696,
      "grad_norm": 405.38055419921875,
      "learning_rate": 0.0012386989184798645,
      "loss": 13.1189,
      "step": 1696
    },
    {
      "epoch": 1.697,
      "grad_norm": 19.057497024536133,
      "learning_rate": 0.0012379351950426187,
      "loss": 11.5358,
      "step": 1697
    },
    {
      "epoch": 1.698,
      "grad_norm": 37.72999572753906,
      "learning_rate": 0.0012371713244673755,
      "loss": 12.0707,
      "step": 1698
    },
    {
      "epoch": 1.6989999999999998,
      "grad_norm": 25.673908233642578,
      "learning_rate": 0.0012364073072265078,
      "loss": 10.5263,
      "step": 1699
    },
    {
      "epoch": 1.7,
      "grad_norm": 75.76519012451172,
      "learning_rate": 0.0012356431437924806,
      "loss": 8.6381,
      "step": 1700
    },
    {
      "epoch": 1.7,
      "eval_loss": 7.699985980987549,
      "eval_runtime": 212.4704,
      "eval_samples_per_second": 0.471,
      "eval_steps_per_second": 0.471,
      "step": 1700
    },
    {
      "epoch": 1.701,
      "grad_norm": 19.033693313598633,
      "learning_rate": 0.0012348788346378493,
      "loss": 7.5789,
      "step": 1701
    },
    {
      "epoch": 1.702,
      "grad_norm": 23.637060165405273,
      "learning_rate": 0.0012341143802352586,
      "loss": 8.7489,
      "step": 1702
    },
    {
      "epoch": 1.7029999999999998,
      "grad_norm": 23.709068298339844,
      "learning_rate": 0.0012333497810574437,
      "loss": 7.9878,
      "step": 1703
    },
    {
      "epoch": 1.704,
      "grad_norm": 15.556934356689453,
      "learning_rate": 0.0012325850375772289,
      "loss": 6.4527,
      "step": 1704
    },
    {
      "epoch": 1.705,
      "grad_norm": 27.32005500793457,
      "learning_rate": 0.0012318201502675285,
      "loss": 7.6025,
      "step": 1705
    },
    {
      "epoch": 1.706,
      "grad_norm": 17.535234451293945,
      "learning_rate": 0.0012310551196013444,
      "loss": 7.5816,
      "step": 1706
    },
    {
      "epoch": 1.7069999999999999,
      "grad_norm": 14.30000114440918,
      "learning_rate": 0.0012302899460517685,
      "loss": 6.514,
      "step": 1707
    },
    {
      "epoch": 1.708,
      "grad_norm": 281.0007629394531,
      "learning_rate": 0.0012295246300919802,
      "loss": 8.3476,
      "step": 1708
    },
    {
      "epoch": 1.709,
      "grad_norm": 91.90519714355469,
      "learning_rate": 0.0012287591721952475,
      "loss": 11.1088,
      "step": 1709
    },
    {
      "epoch": 1.71,
      "grad_norm": 36.22467041015625,
      "learning_rate": 0.0012279935728349261,
      "loss": 12.9878,
      "step": 1710
    },
    {
      "epoch": 1.7109999999999999,
      "grad_norm": 31.045881271362305,
      "learning_rate": 0.0012272278324844583,
      "loss": 10.0983,
      "step": 1711
    },
    {
      "epoch": 1.712,
      "grad_norm": 8.71398639678955,
      "learning_rate": 0.001226461951617375,
      "loss": 5.2654,
      "step": 1712
    },
    {
      "epoch": 1.713,
      "grad_norm": 60.80616760253906,
      "learning_rate": 0.001225695930707293,
      "loss": 6.5778,
      "step": 1713
    },
    {
      "epoch": 1.714,
      "grad_norm": 20.461137771606445,
      "learning_rate": 0.001224929770227916,
      "loss": 8.671,
      "step": 1714
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 29.119253158569336,
      "learning_rate": 0.0012241634706530345,
      "loss": 8.67,
      "step": 1715
    },
    {
      "epoch": 1.716,
      "grad_norm": 17.089914321899414,
      "learning_rate": 0.001223397032456524,
      "loss": 7.6969,
      "step": 1716
    },
    {
      "epoch": 1.717,
      "grad_norm": 14.067848205566406,
      "learning_rate": 0.001222630456112346,
      "loss": 5.6894,
      "step": 1717
    },
    {
      "epoch": 1.718,
      "grad_norm": 9.39657974243164,
      "learning_rate": 0.0012218637420945483,
      "loss": 4.3712,
      "step": 1718
    },
    {
      "epoch": 1.7189999999999999,
      "grad_norm": 50.495540618896484,
      "learning_rate": 0.001221096890877263,
      "loss": 5.306,
      "step": 1719
    },
    {
      "epoch": 1.72,
      "grad_norm": 23.18038558959961,
      "learning_rate": 0.001220329902934707,
      "loss": 5.4736,
      "step": 1720
    },
    {
      "epoch": 1.721,
      "grad_norm": 17.31814956665039,
      "learning_rate": 0.0012195627787411822,
      "loss": 4.4632,
      "step": 1721
    },
    {
      "epoch": 1.722,
      "grad_norm": 29.91890525817871,
      "learning_rate": 0.001218795518771075,
      "loss": 5.3147,
      "step": 1722
    },
    {
      "epoch": 1.7229999999999999,
      "grad_norm": 57.5333137512207,
      "learning_rate": 0.001218028123498855,
      "loss": 5.2701,
      "step": 1723
    },
    {
      "epoch": 1.724,
      "grad_norm": 62.996700286865234,
      "learning_rate": 0.0012172605933990755,
      "loss": 5.3123,
      "step": 1724
    },
    {
      "epoch": 1.725,
      "grad_norm": 112.4852294921875,
      "learning_rate": 0.0012164929289463736,
      "loss": 4.9439,
      "step": 1725
    },
    {
      "epoch": 1.726,
      "grad_norm": 18.2054443359375,
      "learning_rate": 0.0012157251306154698,
      "loss": 4.8192,
      "step": 1726
    },
    {
      "epoch": 1.7269999999999999,
      "grad_norm": 577.63037109375,
      "learning_rate": 0.0012149571988811663,
      "loss": 3.9089,
      "step": 1727
    },
    {
      "epoch": 1.728,
      "grad_norm": 55.48242950439453,
      "learning_rate": 0.0012141891342183492,
      "loss": 3.7399,
      "step": 1728
    },
    {
      "epoch": 1.729,
      "grad_norm": 1403.5599365234375,
      "learning_rate": 0.0012134209371019852,
      "loss": 4.0993,
      "step": 1729
    },
    {
      "epoch": 1.73,
      "grad_norm": 525.2500610351562,
      "learning_rate": 0.0012126526080071243,
      "loss": 5.0513,
      "step": 1730
    },
    {
      "epoch": 1.7309999999999999,
      "grad_norm": 215.44590759277344,
      "learning_rate": 0.0012118841474088977,
      "loss": 5.9882,
      "step": 1731
    },
    {
      "epoch": 1.732,
      "grad_norm": 205.88409423828125,
      "learning_rate": 0.0012111155557825173,
      "loss": 6.4715,
      "step": 1732
    },
    {
      "epoch": 1.733,
      "grad_norm": 8921.92578125,
      "learning_rate": 0.001210346833603277,
      "loss": 6.525,
      "step": 1733
    },
    {
      "epoch": 1.734,
      "grad_norm": 331.106201171875,
      "learning_rate": 0.0012095779813465506,
      "loss": 6.5125,
      "step": 1734
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 823.954345703125,
      "learning_rate": 0.001208808999487793,
      "loss": 6.726,
      "step": 1735
    },
    {
      "epoch": 1.736,
      "grad_norm": 624.7827758789062,
      "learning_rate": 0.0012080398885025388,
      "loss": 6.4476,
      "step": 1736
    },
    {
      "epoch": 1.737,
      "grad_norm": 395.9987487792969,
      "learning_rate": 0.0012072706488664028,
      "loss": 6.2298,
      "step": 1737
    },
    {
      "epoch": 1.738,
      "grad_norm": 1844.8570556640625,
      "learning_rate": 0.0012065012810550783,
      "loss": 6.2109,
      "step": 1738
    },
    {
      "epoch": 1.7389999999999999,
      "grad_norm": 530.4279174804688,
      "learning_rate": 0.0012057317855443395,
      "loss": 6.015,
      "step": 1739
    },
    {
      "epoch": 1.74,
      "grad_norm": 454.9089050292969,
      "learning_rate": 0.0012049621628100388,
      "loss": 6.1142,
      "step": 1740
    },
    {
      "epoch": 1.741,
      "grad_norm": 107194.6328125,
      "learning_rate": 0.0012041924133281072,
      "loss": 6.1296,
      "step": 1741
    },
    {
      "epoch": 1.742,
      "grad_norm": 28376.083984375,
      "learning_rate": 0.001203422537574554,
      "loss": 5.9043,
      "step": 1742
    },
    {
      "epoch": 1.7429999999999999,
      "grad_norm": 2105.09326171875,
      "learning_rate": 0.0012026525360254664,
      "loss": 6.287,
      "step": 1743
    },
    {
      "epoch": 1.744,
      "grad_norm": 984355.125,
      "learning_rate": 0.0012018824091570102,
      "loss": 6.6161,
      "step": 1744
    },
    {
      "epoch": 1.745,
      "grad_norm": 2302.50390625,
      "learning_rate": 0.0012011121574454283,
      "loss": 7.6457,
      "step": 1745
    },
    {
      "epoch": 1.746,
      "grad_norm": 613.4978637695312,
      "learning_rate": 0.0012003417813670402,
      "loss": 9.6218,
      "step": 1746
    },
    {
      "epoch": 1.7469999999999999,
      "grad_norm": 1559.5216064453125,
      "learning_rate": 0.0011995712813982432,
      "loss": 9.6438,
      "step": 1747
    },
    {
      "epoch": 1.748,
      "grad_norm": 79.58639526367188,
      "learning_rate": 0.0011988006580155104,
      "loss": 8.1073,
      "step": 1748
    },
    {
      "epoch": 1.749,
      "grad_norm": 512.0629272460938,
      "learning_rate": 0.0011980299116953923,
      "loss": 10.2872,
      "step": 1749
    },
    {
      "epoch": 1.75,
      "grad_norm": 6641.55419921875,
      "learning_rate": 0.0011972590429145142,
      "loss": 6.9958,
      "step": 1750
    },
    {
      "epoch": 1.751,
      "grad_norm": 26.48760223388672,
      "learning_rate": 0.0011964880521495784,
      "loss": 7.0376,
      "step": 1751
    },
    {
      "epoch": 1.752,
      "grad_norm": 9.033005714416504,
      "learning_rate": 0.0011957169398773611,
      "loss": 7.2297,
      "step": 1752
    },
    {
      "epoch": 1.7530000000000001,
      "grad_norm": 11.623011589050293,
      "learning_rate": 0.0011949457065747147,
      "loss": 5.2551,
      "step": 1753
    },
    {
      "epoch": 1.754,
      "grad_norm": 27.164352416992188,
      "learning_rate": 0.001194174352718567,
      "loss": 8.6224,
      "step": 1754
    },
    {
      "epoch": 1.755,
      "grad_norm": 66.00528717041016,
      "learning_rate": 0.0011934028787859185,
      "loss": 6.0677,
      "step": 1755
    },
    {
      "epoch": 1.756,
      "grad_norm": 3.7425057888031006,
      "learning_rate": 0.0011926312852538455,
      "loss": 4.4881,
      "step": 1756
    },
    {
      "epoch": 1.7570000000000001,
      "grad_norm": 11.698280334472656,
      "learning_rate": 0.0011918595725994975,
      "loss": 4.8586,
      "step": 1757
    },
    {
      "epoch": 1.758,
      "grad_norm": 3.998575210571289,
      "learning_rate": 0.001191087741300099,
      "loss": 4.7469,
      "step": 1758
    },
    {
      "epoch": 1.759,
      "grad_norm": 2.8429055213928223,
      "learning_rate": 0.0011903157918329455,
      "loss": 4.4427,
      "step": 1759
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.0008628368377686,
      "learning_rate": 0.0011895437246754073,
      "loss": 4.3212,
      "step": 1760
    },
    {
      "epoch": 1.7610000000000001,
      "grad_norm": 8.74057674407959,
      "learning_rate": 0.0011887715403049275,
      "loss": 4.4561,
      "step": 1761
    },
    {
      "epoch": 1.762,
      "grad_norm": 2.870368242263794,
      "learning_rate": 0.001187999239199021,
      "loss": 4.0435,
      "step": 1762
    },
    {
      "epoch": 1.763,
      "grad_norm": 1.8072865009307861,
      "learning_rate": 0.0011872268218352752,
      "loss": 3.5781,
      "step": 1763
    },
    {
      "epoch": 1.764,
      "grad_norm": 2.556697130203247,
      "learning_rate": 0.001186454288691349,
      "loss": 3.6093,
      "step": 1764
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 2.7839198112487793,
      "learning_rate": 0.0011856816402449732,
      "loss": 3.4724,
      "step": 1765
    },
    {
      "epoch": 1.766,
      "grad_norm": 2.7582571506500244,
      "learning_rate": 0.0011849088769739508,
      "loss": 3.6555,
      "step": 1766
    },
    {
      "epoch": 1.767,
      "grad_norm": 2.5823590755462646,
      "learning_rate": 0.0011841359993561543,
      "loss": 3.5872,
      "step": 1767
    },
    {
      "epoch": 1.768,
      "grad_norm": 5.8380913734436035,
      "learning_rate": 0.0011833630078695275,
      "loss": 3.629,
      "step": 1768
    },
    {
      "epoch": 1.7690000000000001,
      "grad_norm": 4.424500465393066,
      "learning_rate": 0.0011825899029920847,
      "loss": 3.6654,
      "step": 1769
    },
    {
      "epoch": 1.77,
      "grad_norm": 5.626617431640625,
      "learning_rate": 0.0011818166852019109,
      "loss": 3.698,
      "step": 1770
    },
    {
      "epoch": 1.771,
      "grad_norm": 2.6070148944854736,
      "learning_rate": 0.0011810433549771593,
      "loss": 3.4905,
      "step": 1771
    },
    {
      "epoch": 1.772,
      "grad_norm": 5.7373223304748535,
      "learning_rate": 0.0011802699127960545,
      "loss": 3.7401,
      "step": 1772
    },
    {
      "epoch": 1.7730000000000001,
      "grad_norm": 2.209592342376709,
      "learning_rate": 0.0011794963591368892,
      "loss": 3.3823,
      "step": 1773
    },
    {
      "epoch": 1.774,
      "grad_norm": 5.7288737297058105,
      "learning_rate": 0.0011787226944780249,
      "loss": 3.7635,
      "step": 1774
    },
    {
      "epoch": 1.775,
      "grad_norm": 3.466975450515747,
      "learning_rate": 0.0011779489192978929,
      "loss": 3.5652,
      "step": 1775
    },
    {
      "epoch": 1.776,
      "grad_norm": 4.3054914474487305,
      "learning_rate": 0.0011771750340749919,
      "loss": 3.3258,
      "step": 1776
    },
    {
      "epoch": 1.7770000000000001,
      "grad_norm": 2.94077205657959,
      "learning_rate": 0.0011764010392878884,
      "loss": 3.3443,
      "step": 1777
    },
    {
      "epoch": 1.778,
      "grad_norm": 2.421010971069336,
      "learning_rate": 0.0011756269354152174,
      "loss": 3.278,
      "step": 1778
    },
    {
      "epoch": 1.779,
      "grad_norm": 3.973687171936035,
      "learning_rate": 0.0011748527229356813,
      "loss": 3.2698,
      "step": 1779
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.7636990547180176,
      "learning_rate": 0.001174078402328049,
      "loss": 3.2777,
      "step": 1780
    },
    {
      "epoch": 1.7810000000000001,
      "grad_norm": 4.989715576171875,
      "learning_rate": 0.0011733039740711574,
      "loss": 3.2971,
      "step": 1781
    },
    {
      "epoch": 1.782,
      "grad_norm": 4.405601501464844,
      "learning_rate": 0.0011725294386439084,
      "loss": 3.3722,
      "step": 1782
    },
    {
      "epoch": 1.783,
      "grad_norm": 1.984394907951355,
      "learning_rate": 0.0011717547965252712,
      "loss": 3.1622,
      "step": 1783
    },
    {
      "epoch": 1.784,
      "grad_norm": 2.2801289558410645,
      "learning_rate": 0.0011709800481942815,
      "loss": 3.1492,
      "step": 1784
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 1.8595612049102783,
      "learning_rate": 0.0011702051941300396,
      "loss": 3.179,
      "step": 1785
    },
    {
      "epoch": 1.786,
      "grad_norm": 3.8360214233398438,
      "learning_rate": 0.0011694302348117113,
      "loss": 3.3165,
      "step": 1786
    },
    {
      "epoch": 1.787,
      "grad_norm": 2.3562207221984863,
      "learning_rate": 0.0011686551707185279,
      "loss": 3.1139,
      "step": 1787
    },
    {
      "epoch": 1.788,
      "grad_norm": 3.6208810806274414,
      "learning_rate": 0.0011678800023297857,
      "loss": 3.3466,
      "step": 1788
    },
    {
      "epoch": 1.7890000000000001,
      "grad_norm": 2.2822937965393066,
      "learning_rate": 0.0011671047301248447,
      "loss": 3.2352,
      "step": 1789
    },
    {
      "epoch": 1.79,
      "grad_norm": 3.5239198207855225,
      "learning_rate": 0.0011663293545831302,
      "loss": 3.3052,
      "step": 1790
    },
    {
      "epoch": 1.791,
      "grad_norm": 3.4116549491882324,
      "learning_rate": 0.00116555387618413,
      "loss": 3.0745,
      "step": 1791
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.5908770561218262,
      "learning_rate": 0.0011647782954073967,
      "loss": 2.9932,
      "step": 1792
    },
    {
      "epoch": 1.7930000000000001,
      "grad_norm": 2.6759414672851562,
      "learning_rate": 0.0011640026127325458,
      "loss": 3.043,
      "step": 1793
    },
    {
      "epoch": 1.794,
      "grad_norm": 1.7954142093658447,
      "learning_rate": 0.0011632268286392558,
      "loss": 2.978,
      "step": 1794
    },
    {
      "epoch": 1.795,
      "grad_norm": 3.5303902626037598,
      "learning_rate": 0.0011624509436072674,
      "loss": 3.1182,
      "step": 1795
    },
    {
      "epoch": 1.796,
      "grad_norm": 3.6289656162261963,
      "learning_rate": 0.001161674958116385,
      "loss": 3.1756,
      "step": 1796
    },
    {
      "epoch": 1.7970000000000002,
      "grad_norm": 1.0633554458618164,
      "learning_rate": 0.0011608988726464742,
      "loss": 2.9796,
      "step": 1797
    },
    {
      "epoch": 1.798,
      "grad_norm": 1.2385674715042114,
      "learning_rate": 0.001160122687677462,
      "loss": 2.9996,
      "step": 1798
    },
    {
      "epoch": 1.799,
      "grad_norm": 2.7909209728240967,
      "learning_rate": 0.0011593464036893382,
      "loss": 3.0219,
      "step": 1799
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.7528715133666992,
      "learning_rate": 0.0011585700211621526,
      "loss": 2.9409,
      "step": 1800
    },
    {
      "epoch": 1.8,
      "eval_loss": 3.025435209274292,
      "eval_runtime": 212.2275,
      "eval_samples_per_second": 0.471,
      "eval_steps_per_second": 0.471,
      "step": 1800
    },
    {
      "epoch": 1.8010000000000002,
      "grad_norm": 2.487126588821411,
      "learning_rate": 0.0011577935405760165,
      "loss": 2.9913,
      "step": 1801
    },
    {
      "epoch": 1.802,
      "grad_norm": 2.611013889312744,
      "learning_rate": 0.0011570169624111025,
      "loss": 2.9523,
      "step": 1802
    },
    {
      "epoch": 1.803,
      "grad_norm": 1.4215874671936035,
      "learning_rate": 0.0011562402871476423,
      "loss": 3.0346,
      "step": 1803
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.1812372207641602,
      "learning_rate": 0.0011554635152659278,
      "loss": 2.9825,
      "step": 1804
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 1.7312978506088257,
      "learning_rate": 0.001154686647246312,
      "loss": 2.8622,
      "step": 1805
    },
    {
      "epoch": 1.806,
      "grad_norm": 0.8782114386558533,
      "learning_rate": 0.001153909683569206,
      "loss": 2.8888,
      "step": 1806
    },
    {
      "epoch": 1.807,
      "grad_norm": 4.236198902130127,
      "learning_rate": 0.00115313262471508,
      "loss": 3.0873,
      "step": 1807
    },
    {
      "epoch": 1.808,
      "grad_norm": 3.75762677192688,
      "learning_rate": 0.0011523554711644643,
      "loss": 2.9816,
      "step": 1808
    },
    {
      "epoch": 1.8090000000000002,
      "grad_norm": 1.2957465648651123,
      "learning_rate": 0.0011515782233979465,
      "loss": 2.9159,
      "step": 1809
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.4152430295944214,
      "learning_rate": 0.001150800881896173,
      "loss": 2.9899,
      "step": 1810
    },
    {
      "epoch": 1.811,
      "grad_norm": 2.136197328567505,
      "learning_rate": 0.0011500234471398486,
      "loss": 2.9475,
      "step": 1811
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.5133236646652222,
      "learning_rate": 0.0011492459196097344,
      "loss": 2.7688,
      "step": 1812
    },
    {
      "epoch": 1.813,
      "grad_norm": 2.1675097942352295,
      "learning_rate": 0.0011484682997866499,
      "loss": 2.8382,
      "step": 1813
    },
    {
      "epoch": 1.814,
      "grad_norm": 1.7956047058105469,
      "learning_rate": 0.001147690588151472,
      "loss": 2.8315,
      "step": 1814
    },
    {
      "epoch": 1.815,
      "grad_norm": 2.2954258918762207,
      "learning_rate": 0.001146912785185134,
      "loss": 2.9345,
      "step": 1815
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 2.8079891204833984,
      "learning_rate": 0.001146134891368625,
      "loss": 2.8232,
      "step": 1816
    },
    {
      "epoch": 1.817,
      "grad_norm": 1.1724770069122314,
      "learning_rate": 0.001145356907182991,
      "loss": 2.8943,
      "step": 1817
    },
    {
      "epoch": 1.818,
      "grad_norm": 9.949960708618164,
      "learning_rate": 0.001144578833109334,
      "loss": 3.015,
      "step": 1818
    },
    {
      "epoch": 1.819,
      "grad_norm": 0.9242944717407227,
      "learning_rate": 0.0011438006696288109,
      "loss": 2.962,
      "step": 1819
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.2665700912475586,
      "learning_rate": 0.001143022417222635,
      "loss": 2.8327,
      "step": 1820
    },
    {
      "epoch": 1.821,
      "grad_norm": 1.0410375595092773,
      "learning_rate": 0.001142244076372073,
      "loss": 2.7085,
      "step": 1821
    },
    {
      "epoch": 1.822,
      "grad_norm": 1.6213045120239258,
      "learning_rate": 0.0011414656475584478,
      "loss": 2.818,
      "step": 1822
    },
    {
      "epoch": 1.823,
      "grad_norm": 1.1600780487060547,
      "learning_rate": 0.0011406871312631362,
      "loss": 2.8284,
      "step": 1823
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.9083961844444275,
      "learning_rate": 0.0011399085279675688,
      "loss": 2.8375,
      "step": 1824
    },
    {
      "epoch": 1.825,
      "grad_norm": 1.166690707206726,
      "learning_rate": 0.0011391298381532296,
      "loss": 2.6608,
      "step": 1825
    },
    {
      "epoch": 1.826,
      "grad_norm": 1.7542212009429932,
      "learning_rate": 0.0011383510623016574,
      "loss": 2.7547,
      "step": 1826
    },
    {
      "epoch": 1.827,
      "grad_norm": 0.8636561632156372,
      "learning_rate": 0.0011375722008944438,
      "loss": 2.708,
      "step": 1827
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 3.159247636795044,
      "learning_rate": 0.0011367932544132323,
      "loss": 2.8834,
      "step": 1828
    },
    {
      "epoch": 1.829,
      "grad_norm": 1.810393214225769,
      "learning_rate": 0.0011360142233397198,
      "loss": 2.5893,
      "step": 1829
    },
    {
      "epoch": 1.83,
      "grad_norm": 2.090299606323242,
      "learning_rate": 0.0011352351081556557,
      "loss": 2.7333,
      "step": 1830
    },
    {
      "epoch": 1.831,
      "grad_norm": 1.785884976387024,
      "learning_rate": 0.001134455909342841,
      "loss": 2.7091,
      "step": 1831
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 2.151707887649536,
      "learning_rate": 0.0011336766273831286,
      "loss": 2.7479,
      "step": 1832
    },
    {
      "epoch": 1.833,
      "grad_norm": 1.7058470249176025,
      "learning_rate": 0.0011328972627584228,
      "loss": 2.8597,
      "step": 1833
    },
    {
      "epoch": 1.834,
      "grad_norm": 0.7697296738624573,
      "learning_rate": 0.0011321178159506792,
      "loss": 2.6702,
      "step": 1834
    },
    {
      "epoch": 1.835,
      "grad_norm": 2.6109676361083984,
      "learning_rate": 0.0011313382874419031,
      "loss": 2.6717,
      "step": 1835
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.9998352527618408,
      "learning_rate": 0.0011305586777141526,
      "loss": 2.6099,
      "step": 1836
    },
    {
      "epoch": 1.837,
      "grad_norm": 1.3859708309173584,
      "learning_rate": 0.0011297789872495333,
      "loss": 2.6146,
      "step": 1837
    },
    {
      "epoch": 1.838,
      "grad_norm": 0.9965307712554932,
      "learning_rate": 0.0011289992165302034,
      "loss": 2.5794,
      "step": 1838
    },
    {
      "epoch": 1.839,
      "grad_norm": 1.6704047918319702,
      "learning_rate": 0.0011282193660383684,
      "loss": 2.8455,
      "step": 1839
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.1510752439498901,
      "learning_rate": 0.0011274394362562846,
      "loss": 2.5237,
      "step": 1840
    },
    {
      "epoch": 1.841,
      "grad_norm": 1.0799365043640137,
      "learning_rate": 0.001126659427666257,
      "loss": 2.4477,
      "step": 1841
    },
    {
      "epoch": 1.842,
      "grad_norm": 1.2409805059432983,
      "learning_rate": 0.001125879340750639,
      "loss": 2.6483,
      "step": 1842
    },
    {
      "epoch": 1.843,
      "grad_norm": 1.325018048286438,
      "learning_rate": 0.0011250991759918323,
      "loss": 2.6692,
      "step": 1843
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.788233995437622,
      "learning_rate": 0.001124318933872288,
      "loss": 2.6387,
      "step": 1844
    },
    {
      "epoch": 1.845,
      "grad_norm": 1.1156291961669922,
      "learning_rate": 0.0011235386148745034,
      "loss": 2.5068,
      "step": 1845
    },
    {
      "epoch": 1.846,
      "grad_norm": 1.8213248252868652,
      "learning_rate": 0.0011227582194810242,
      "loss": 2.5133,
      "step": 1846
    },
    {
      "epoch": 1.847,
      "grad_norm": 0.9579487442970276,
      "learning_rate": 0.0011219777481744435,
      "loss": 2.5024,
      "step": 1847
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 1.0216553211212158,
      "learning_rate": 0.0011211972014374008,
      "loss": 2.4423,
      "step": 1848
    },
    {
      "epoch": 1.849,
      "grad_norm": 0.7215990424156189,
      "learning_rate": 0.0011204165797525825,
      "loss": 2.5252,
      "step": 1849
    },
    {
      "epoch": 1.85,
      "grad_norm": 1.0830681324005127,
      "learning_rate": 0.0011196358836027215,
      "loss": 2.5447,
      "step": 1850
    },
    {
      "epoch": 1.851,
      "grad_norm": 0.6792945861816406,
      "learning_rate": 0.0011188551134705966,
      "loss": 2.4325,
      "step": 1851
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.8861304521560669,
      "learning_rate": 0.001118074269839032,
      "loss": 2.3891,
      "step": 1852
    },
    {
      "epoch": 1.853,
      "grad_norm": 0.940210223197937,
      "learning_rate": 0.0011172933531908983,
      "loss": 2.4862,
      "step": 1853
    },
    {
      "epoch": 1.854,
      "grad_norm": 3.6636886596679688,
      "learning_rate": 0.0011165123640091105,
      "loss": 2.5423,
      "step": 1854
    },
    {
      "epoch": 1.855,
      "grad_norm": 1.4311590194702148,
      "learning_rate": 0.0011157313027766278,
      "loss": 2.6537,
      "step": 1855
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 2.2358360290527344,
      "learning_rate": 0.0011149501699764558,
      "loss": 2.5394,
      "step": 1856
    },
    {
      "epoch": 1.857,
      "grad_norm": 1.6814078092575073,
      "learning_rate": 0.0011141689660916431,
      "loss": 2.5616,
      "step": 1857
    },
    {
      "epoch": 1.858,
      "grad_norm": 2.241743803024292,
      "learning_rate": 0.0011133876916052821,
      "loss": 2.6509,
      "step": 1858
    },
    {
      "epoch": 1.859,
      "grad_norm": 1.6202369928359985,
      "learning_rate": 0.0011126063470005096,
      "loss": 2.4932,
      "step": 1859
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 3.5286340713500977,
      "learning_rate": 0.0011118249327605055,
      "loss": 2.6338,
      "step": 1860
    },
    {
      "epoch": 1.861,
      "grad_norm": 2.619840383529663,
      "learning_rate": 0.001111043449368492,
      "loss": 2.6298,
      "step": 1861
    },
    {
      "epoch": 1.862,
      "grad_norm": 1.6275994777679443,
      "learning_rate": 0.0011102618973077355,
      "loss": 2.4829,
      "step": 1862
    },
    {
      "epoch": 1.863,
      "grad_norm": 1.3724879026412964,
      "learning_rate": 0.0011094802770615438,
      "loss": 2.5937,
      "step": 1863
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 1.2918893098831177,
      "learning_rate": 0.0011086985891132668,
      "loss": 2.5442,
      "step": 1864
    },
    {
      "epoch": 1.865,
      "grad_norm": 1.2495535612106323,
      "learning_rate": 0.001107916833946297,
      "loss": 2.4542,
      "step": 1865
    },
    {
      "epoch": 1.866,
      "grad_norm": 2.655107021331787,
      "learning_rate": 0.0011071350120440684,
      "loss": 2.5001,
      "step": 1866
    },
    {
      "epoch": 1.867,
      "grad_norm": 2.159034013748169,
      "learning_rate": 0.0011063531238900556,
      "loss": 2.4962,
      "step": 1867
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.8774974346160889,
      "learning_rate": 0.0011055711699677743,
      "loss": 2.4564,
      "step": 1868
    },
    {
      "epoch": 1.869,
      "grad_norm": 1.6422783136367798,
      "learning_rate": 0.0011047891507607814,
      "loss": 2.6833,
      "step": 1869
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.6090292930603027,
      "learning_rate": 0.0011040070667526736,
      "loss": 2.4264,
      "step": 1870
    },
    {
      "epoch": 1.871,
      "grad_norm": 1.1276806592941284,
      "learning_rate": 0.0011032249184270886,
      "loss": 2.4129,
      "step": 1871
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.7655239701271057,
      "learning_rate": 0.0011024427062677027,
      "loss": 2.3108,
      "step": 1872
    },
    {
      "epoch": 1.873,
      "grad_norm": 0.8064021468162537,
      "learning_rate": 0.001101660430758232,
      "loss": 2.359,
      "step": 1873
    },
    {
      "epoch": 1.874,
      "grad_norm": 0.8715116381645203,
      "learning_rate": 0.0011008780923824327,
      "loss": 2.5082,
      "step": 1874
    },
    {
      "epoch": 1.875,
      "grad_norm": 1.0280039310455322,
      "learning_rate": 0.0011000956916240984,
      "loss": 2.517,
      "step": 1875
    },
    {
      "epoch": 1.876,
      "grad_norm": 1.1626144647598267,
      "learning_rate": 0.0010993132289670625,
      "loss": 2.4236,
      "step": 1876
    },
    {
      "epoch": 1.877,
      "grad_norm": 0.8517550230026245,
      "learning_rate": 0.001098530704895196,
      "loss": 2.652,
      "step": 1877
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 0.85512375831604,
      "learning_rate": 0.001097748119892408,
      "loss": 2.3814,
      "step": 1878
    },
    {
      "epoch": 1.879,
      "grad_norm": 0.7578036785125732,
      "learning_rate": 0.0010969654744426454,
      "loss": 2.3785,
      "step": 1879
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.8785529732704163,
      "learning_rate": 0.0010961827690298927,
      "loss": 2.3416,
      "step": 1880
    },
    {
      "epoch": 1.881,
      "grad_norm": 0.7848063707351685,
      "learning_rate": 0.001095400004138171,
      "loss": 2.4878,
      "step": 1881
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 1.0903570652008057,
      "learning_rate": 0.0010946171802515385,
      "loss": 2.3729,
      "step": 1882
    },
    {
      "epoch": 1.883,
      "grad_norm": 0.722445011138916,
      "learning_rate": 0.0010938342978540896,
      "loss": 2.3103,
      "step": 1883
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.8961209058761597,
      "learning_rate": 0.0010930513574299555,
      "loss": 2.421,
      "step": 1884
    },
    {
      "epoch": 1.885,
      "grad_norm": 1.4386197328567505,
      "learning_rate": 0.001092268359463302,
      "loss": 2.4885,
      "step": 1885
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 0.6810517311096191,
      "learning_rate": 0.0010914853044383323,
      "loss": 2.2813,
      "step": 1886
    },
    {
      "epoch": 1.887,
      "grad_norm": 1.1432725191116333,
      "learning_rate": 0.0010907021928392832,
      "loss": 2.4558,
      "step": 1887
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.0320746898651123,
      "learning_rate": 0.0010899190251504277,
      "loss": 2.3218,
      "step": 1888
    },
    {
      "epoch": 1.889,
      "grad_norm": 0.9494766592979431,
      "learning_rate": 0.0010891358018560728,
      "loss": 2.3184,
      "step": 1889
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.8259652853012085,
      "learning_rate": 0.0010883525234405603,
      "loss": 2.4067,
      "step": 1890
    },
    {
      "epoch": 1.891,
      "grad_norm": 0.9919520616531372,
      "learning_rate": 0.001087569190388265,
      "loss": 2.3806,
      "step": 1891
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.8958033323287964,
      "learning_rate": 0.0010867858031835976,
      "loss": 2.4804,
      "step": 1892
    },
    {
      "epoch": 1.893,
      "grad_norm": 0.9787759184837341,
      "learning_rate": 0.0010860023623110002,
      "loss": 2.3324,
      "step": 1893
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 1.0949326753616333,
      "learning_rate": 0.001085218868254949,
      "loss": 2.4169,
      "step": 1894
    },
    {
      "epoch": 1.895,
      "grad_norm": 0.5346352458000183,
      "learning_rate": 0.0010844353214999535,
      "loss": 2.235,
      "step": 1895
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.7763952612876892,
      "learning_rate": 0.0010836517225305546,
      "loss": 2.3808,
      "step": 1896
    },
    {
      "epoch": 1.897,
      "grad_norm": 0.638815701007843,
      "learning_rate": 0.0010828680718313268,
      "loss": 2.4188,
      "step": 1897
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 0.6805081367492676,
      "learning_rate": 0.0010820843698868758,
      "loss": 2.4341,
      "step": 1898
    },
    {
      "epoch": 1.899,
      "grad_norm": 0.5856301188468933,
      "learning_rate": 0.0010813006171818392,
      "loss": 2.4772,
      "step": 1899
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.7222910523414612,
      "learning_rate": 0.0010805168142008852,
      "loss": 2.3005,
      "step": 1900
    },
    {
      "epoch": 1.9,
      "eval_loss": 2.3757874965667725,
      "eval_runtime": 212.2671,
      "eval_samples_per_second": 0.471,
      "eval_steps_per_second": 0.471,
      "step": 1900
    },
    {
      "epoch": 1.901,
      "grad_norm": 1.0050989389419556,
      "learning_rate": 0.0010797329614287152,
      "loss": 2.1969,
      "step": 1901
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 0.596367359161377,
      "learning_rate": 0.0010789490593500592,
      "loss": 2.3012,
      "step": 1902
    },
    {
      "epoch": 1.903,
      "grad_norm": 0.7924681901931763,
      "learning_rate": 0.0010781651084496786,
      "loss": 2.3467,
      "step": 1903
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.5238382816314697,
      "learning_rate": 0.0010773811092123654,
      "loss": 2.2553,
      "step": 1904
    },
    {
      "epoch": 1.905,
      "grad_norm": 5.155019283294678,
      "learning_rate": 0.0010765970621229401,
      "loss": 2.8828,
      "step": 1905
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 0.82529217004776,
      "learning_rate": 0.0010758129676662544,
      "loss": 2.3589,
      "step": 1906
    },
    {
      "epoch": 1.907,
      "grad_norm": 0.7469539046287537,
      "learning_rate": 0.0010750288263271888,
      "loss": 2.2552,
      "step": 1907
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.9475672245025635,
      "learning_rate": 0.0010742446385906522,
      "loss": 2.4287,
      "step": 1908
    },
    {
      "epoch": 1.909,
      "grad_norm": 0.8552955985069275,
      "learning_rate": 0.001073460404941582,
      "loss": 2.3015,
      "step": 1909
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.7839760184288025,
      "learning_rate": 0.001072676125864946,
      "loss": 2.2905,
      "step": 1910
    },
    {
      "epoch": 1.911,
      "grad_norm": 0.6591631174087524,
      "learning_rate": 0.0010718918018457375,
      "loss": 2.3769,
      "step": 1911
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.7915618419647217,
      "learning_rate": 0.0010711074333689791,
      "loss": 2.5221,
      "step": 1912
    },
    {
      "epoch": 1.913,
      "grad_norm": 0.8362187147140503,
      "learning_rate": 0.0010703230209197206,
      "loss": 2.465,
      "step": 1913
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 0.9406256675720215,
      "learning_rate": 0.0010695385649830392,
      "loss": 2.2741,
      "step": 1914
    },
    {
      "epoch": 1.915,
      "grad_norm": 0.8270580768585205,
      "learning_rate": 0.001068754066044038,
      "loss": 2.4163,
      "step": 1915
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.6998143792152405,
      "learning_rate": 0.0010679695245878482,
      "loss": 2.2768,
      "step": 1916
    },
    {
      "epoch": 1.917,
      "grad_norm": 13.470837593078613,
      "learning_rate": 0.001067184941099626,
      "loss": 2.4483,
      "step": 1917
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 0.8451265692710876,
      "learning_rate": 0.0010664003160645546,
      "loss": 2.2901,
      "step": 1918
    },
    {
      "epoch": 1.919,
      "grad_norm": 4.349991798400879,
      "learning_rate": 0.001065615649967842,
      "loss": 2.4392,
      "step": 1919
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.5122525691986084,
      "learning_rate": 0.0010648309432947224,
      "loss": 2.4093,
      "step": 1920
    },
    {
      "epoch": 1.921,
      "grad_norm": 0.7429342865943909,
      "learning_rate": 0.001064046196530454,
      "loss": 2.3221,
      "step": 1921
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 1.9662038087844849,
      "learning_rate": 0.001063261410160321,
      "loss": 2.3715,
      "step": 1922
    },
    {
      "epoch": 1.923,
      "grad_norm": 0.7766202688217163,
      "learning_rate": 0.0010624765846696316,
      "loss": 2.438,
      "step": 1923
    },
    {
      "epoch": 1.924,
      "grad_norm": 1.535346508026123,
      "learning_rate": 0.001061691720543718,
      "loss": 2.2692,
      "step": 1924
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.8713352680206299,
      "learning_rate": 0.0010609068182679366,
      "loss": 2.4146,
      "step": 1925
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 2.061940908432007,
      "learning_rate": 0.0010601218783276671,
      "loss": 2.4703,
      "step": 1926
    },
    {
      "epoch": 1.927,
      "grad_norm": 1.795892596244812,
      "learning_rate": 0.0010593369012083128,
      "loss": 2.4004,
      "step": 1927
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.3032429218292236,
      "learning_rate": 0.0010585518873952996,
      "loss": 2.4142,
      "step": 1928
    },
    {
      "epoch": 1.929,
      "grad_norm": 1.1586289405822754,
      "learning_rate": 0.0010577668373740767,
      "loss": 2.4989,
      "step": 1929
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 1.1387147903442383,
      "learning_rate": 0.0010569817516301147,
      "loss": 2.3672,
      "step": 1930
    },
    {
      "epoch": 1.931,
      "grad_norm": 0.9712140560150146,
      "learning_rate": 0.0010561966306489076,
      "loss": 2.2424,
      "step": 1931
    },
    {
      "epoch": 1.932,
      "grad_norm": 1.5690264701843262,
      "learning_rate": 0.00105541147491597,
      "loss": 2.3176,
      "step": 1932
    },
    {
      "epoch": 1.933,
      "grad_norm": 1.1044085025787354,
      "learning_rate": 0.001054626284916839,
      "loss": 2.2108,
      "step": 1933
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 1.5865576267242432,
      "learning_rate": 0.0010538410611370718,
      "loss": 2.3363,
      "step": 1934
    },
    {
      "epoch": 1.935,
      "grad_norm": 1.3942457437515259,
      "learning_rate": 0.0010530558040622471,
      "loss": 2.3865,
      "step": 1935
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.0816459655761719,
      "learning_rate": 0.0010522705141779644,
      "loss": 2.3143,
      "step": 1936
    },
    {
      "epoch": 1.937,
      "grad_norm": 1.0929166078567505,
      "learning_rate": 0.0010514851919698437,
      "loss": 2.3874,
      "step": 1937
    },
    {
      "epoch": 1.938,
      "grad_norm": 0.9881373047828674,
      "learning_rate": 0.001050699837923524,
      "loss": 2.3931,
      "step": 1938
    },
    {
      "epoch": 1.939,
      "grad_norm": 0.9479817748069763,
      "learning_rate": 0.0010499144525246643,
      "loss": 2.3717,
      "step": 1939
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.1166679859161377,
      "learning_rate": 0.0010491290362589433,
      "loss": 2.3291,
      "step": 1940
    },
    {
      "epoch": 1.9409999999999998,
      "grad_norm": 0.6145399212837219,
      "learning_rate": 0.0010483435896120592,
      "loss": 2.3045,
      "step": 1941
    },
    {
      "epoch": 1.942,
      "grad_norm": 0.7624708414077759,
      "learning_rate": 0.0010475581130697284,
      "loss": 2.236,
      "step": 1942
    },
    {
      "epoch": 1.943,
      "grad_norm": 0.6152347922325134,
      "learning_rate": 0.0010467726071176854,
      "loss": 2.1866,
      "step": 1943
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.9304061532020569,
      "learning_rate": 0.0010459870722416833,
      "loss": 2.3122,
      "step": 1944
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 0.747649073600769,
      "learning_rate": 0.0010452015089274938,
      "loss": 2.1928,
      "step": 1945
    },
    {
      "epoch": 1.946,
      "grad_norm": 0.8150389194488525,
      "learning_rate": 0.0010444159176609053,
      "loss": 2.2858,
      "step": 1946
    },
    {
      "epoch": 1.947,
      "grad_norm": 0.7354493141174316,
      "learning_rate": 0.0010436302989277233,
      "loss": 2.2768,
      "step": 1947
    },
    {
      "epoch": 1.948,
      "grad_norm": 40.39358901977539,
      "learning_rate": 0.0010428446532137713,
      "loss": 2.6669,
      "step": 1948
    },
    {
      "epoch": 1.9489999999999998,
      "grad_norm": 0.7573434710502625,
      "learning_rate": 0.0010420589810048887,
      "loss": 2.2933,
      "step": 1949
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.8567703366279602,
      "learning_rate": 0.0010412732827869311,
      "loss": 2.4866,
      "step": 1950
    },
    {
      "epoch": 1.951,
      "grad_norm": 1.5223912000656128,
      "learning_rate": 0.001040487559045771,
      "loss": 2.4224,
      "step": 1951
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.9524375796318054,
      "learning_rate": 0.001039701810267296,
      "loss": 2.2213,
      "step": 1952
    },
    {
      "epoch": 1.9529999999999998,
      "grad_norm": 0.8266782760620117,
      "learning_rate": 0.0010389160369374095,
      "loss": 2.2366,
      "step": 1953
    },
    {
      "epoch": 1.954,
      "grad_norm": 0.7908279299736023,
      "learning_rate": 0.00103813023954203,
      "loss": 2.3042,
      "step": 1954
    },
    {
      "epoch": 1.955,
      "grad_norm": 1.0139590501785278,
      "learning_rate": 0.0010373444185670913,
      "loss": 2.4872,
      "step": 1955
    },
    {
      "epoch": 1.956,
      "grad_norm": 1.8256498575210571,
      "learning_rate": 0.0010365585744985405,
      "loss": 2.3185,
      "step": 1956
    },
    {
      "epoch": 1.9569999999999999,
      "grad_norm": 0.8673954010009766,
      "learning_rate": 0.0010357727078223404,
      "loss": 2.1896,
      "step": 1957
    },
    {
      "epoch": 1.958,
      "grad_norm": 0.7519951462745667,
      "learning_rate": 0.0010349868190244673,
      "loss": 2.2704,
      "step": 1958
    },
    {
      "epoch": 1.959,
      "grad_norm": 0.6602502465248108,
      "learning_rate": 0.001034200908590911,
      "loss": 2.2519,
      "step": 1959
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.7666191458702087,
      "learning_rate": 0.0010334149770076747,
      "loss": 2.2883,
      "step": 1960
    },
    {
      "epoch": 1.9609999999999999,
      "grad_norm": 0.722682535648346,
      "learning_rate": 0.0010326290247607746,
      "loss": 2.4087,
      "step": 1961
    },
    {
      "epoch": 1.962,
      "grad_norm": 0.7064039707183838,
      "learning_rate": 0.0010318430523362405,
      "loss": 2.1948,
      "step": 1962
    },
    {
      "epoch": 1.963,
      "grad_norm": 2.015214443206787,
      "learning_rate": 0.0010310570602201133,
      "loss": 2.3598,
      "step": 1963
    },
    {
      "epoch": 1.964,
      "grad_norm": 2.190575361251831,
      "learning_rate": 0.0010302710488984473,
      "loss": 2.3862,
      "step": 1964
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 1.3717267513275146,
      "learning_rate": 0.001029485018857308,
      "loss": 2.3596,
      "step": 1965
    },
    {
      "epoch": 1.966,
      "grad_norm": 1.1414411067962646,
      "learning_rate": 0.0010286989705827723,
      "loss": 2.3671,
      "step": 1966
    },
    {
      "epoch": 1.967,
      "grad_norm": 1.4929753541946411,
      "learning_rate": 0.0010279129045609294,
      "loss": 2.184,
      "step": 1967
    },
    {
      "epoch": 1.968,
      "grad_norm": 1.0788792371749878,
      "learning_rate": 0.0010271268212778786,
      "loss": 2.2789,
      "step": 1968
    },
    {
      "epoch": 1.9689999999999999,
      "grad_norm": 1.0667921304702759,
      "learning_rate": 0.0010263407212197295,
      "loss": 2.3495,
      "step": 1969
    },
    {
      "epoch": 1.97,
      "grad_norm": 1.8548195362091064,
      "learning_rate": 0.0010255546048726035,
      "loss": 2.3488,
      "step": 1970
    },
    {
      "epoch": 1.971,
      "grad_norm": 0.954060971736908,
      "learning_rate": 0.0010247684727226307,
      "loss": 2.3704,
      "step": 1971
    },
    {
      "epoch": 1.972,
      "grad_norm": 1.2137552499771118,
      "learning_rate": 0.0010239823252559516,
      "loss": 2.1826,
      "step": 1972
    },
    {
      "epoch": 1.9729999999999999,
      "grad_norm": 1.2667903900146484,
      "learning_rate": 0.0010231961629587163,
      "loss": 2.2634,
      "step": 1973
    },
    {
      "epoch": 1.974,
      "grad_norm": 0.9479231834411621,
      "learning_rate": 0.0010224099863170835,
      "loss": 2.1541,
      "step": 1974
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.7895773649215698,
      "learning_rate": 0.0010216237958172213,
      "loss": 2.3136,
      "step": 1975
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.8035208582878113,
      "learning_rate": 0.001020837591945306,
      "loss": 2.2816,
      "step": 1976
    },
    {
      "epoch": 1.9769999999999999,
      "grad_norm": 0.9704704284667969,
      "learning_rate": 0.0010200513751875227,
      "loss": 2.0496,
      "step": 1977
    },
    {
      "epoch": 1.978,
      "grad_norm": 1.1354899406433105,
      "learning_rate": 0.001019265146030064,
      "loss": 2.1333,
      "step": 1978
    },
    {
      "epoch": 1.979,
      "grad_norm": 1.7105129957199097,
      "learning_rate": 0.00101847890495913,
      "loss": 2.2493,
      "step": 1979
    },
    {
      "epoch": 1.98,
      "grad_norm": 4.236257076263428,
      "learning_rate": 0.001017692652460929,
      "loss": 2.3242,
      "step": 1980
    },
    {
      "epoch": 1.9809999999999999,
      "grad_norm": 1.2777091264724731,
      "learning_rate": 0.001016906389021675,
      "loss": 2.3351,
      "step": 1981
    },
    {
      "epoch": 1.982,
      "grad_norm": 0.9331059455871582,
      "learning_rate": 0.0010161201151275905,
      "loss": 2.1843,
      "step": 1982
    },
    {
      "epoch": 1.983,
      "grad_norm": 5.444386005401611,
      "learning_rate": 0.0010153338312649028,
      "loss": 2.4273,
      "step": 1983
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.7527839541435242,
      "learning_rate": 0.0010145475379198464,
      "loss": 2.2286,
      "step": 1984
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 1.5265655517578125,
      "learning_rate": 0.0010137612355786618,
      "loss": 2.3017,
      "step": 1985
    },
    {
      "epoch": 1.986,
      "grad_norm": 1.382240891456604,
      "learning_rate": 0.001012974924727594,
      "loss": 2.2143,
      "step": 1986
    },
    {
      "epoch": 1.987,
      "grad_norm": 0.6561806201934814,
      "learning_rate": 0.0010121886058528943,
      "loss": 2.1198,
      "step": 1987
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.5497470498085022,
      "learning_rate": 0.0010114022794408184,
      "loss": 2.1094,
      "step": 1988
    },
    {
      "epoch": 1.9889999999999999,
      "grad_norm": 1.1212087869644165,
      "learning_rate": 0.0010106159459776273,
      "loss": 2.1347,
      "step": 1989
    },
    {
      "epoch": 1.99,
      "grad_norm": 1.5005640983581543,
      "learning_rate": 0.0010098296059495848,
      "loss": 2.2062,
      "step": 1990
    },
    {
      "epoch": 1.991,
      "grad_norm": 1.021186113357544,
      "learning_rate": 0.0010090432598429608,
      "loss": 2.07,
      "step": 1991
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.7516790628433228,
      "learning_rate": 0.0010082569081440278,
      "loss": 2.0658,
      "step": 1992
    },
    {
      "epoch": 1.9929999999999999,
      "grad_norm": 1.4084712266921997,
      "learning_rate": 0.0010074705513390615,
      "loss": 2.2113,
      "step": 1993
    },
    {
      "epoch": 1.994,
      "grad_norm": 1.4005893468856812,
      "learning_rate": 0.0010066841899143424,
      "loss": 2.19,
      "step": 1994
    },
    {
      "epoch": 1.995,
      "grad_norm": 1.379130244255066,
      "learning_rate": 0.0010058978243561514,
      "loss": 2.0705,
      "step": 1995
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.9951066970825195,
      "learning_rate": 0.0010051114551507736,
      "loss": 2.2188,
      "step": 1996
    },
    {
      "epoch": 1.9969999999999999,
      "grad_norm": 0.8764761686325073,
      "learning_rate": 0.0010043250827844966,
      "loss": 1.9971,
      "step": 1997
    },
    {
      "epoch": 1.998,
      "grad_norm": 10.535087585449219,
      "learning_rate": 0.0010035387077436087,
      "loss": 3.3174,
      "step": 1998
    },
    {
      "epoch": 1.999,
      "grad_norm": 3.7914316654205322,
      "learning_rate": 0.0010027523305144003,
      "loss": 2.3955,
      "step": 1999
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.8111317157745361,
      "learning_rate": 0.0010019659515831643,
      "loss": 2.3357,
      "step": 2000
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.4285202026367188,
      "eval_runtime": 212.3626,
      "eval_samples_per_second": 0.471,
      "eval_steps_per_second": 0.471,
      "step": 2000
    },
    {
      "epoch": 2.001,
      "grad_norm": 1.2060117721557617,
      "learning_rate": 0.001001179571436193,
      "loss": 2.3867,
      "step": 2001
    },
    {
      "epoch": 2.002,
      "grad_norm": 2.207855463027954,
      "learning_rate": 0.00100039319055978,
      "loss": 2.4562,
      "step": 2002
    },
    {
      "epoch": 2.003,
      "grad_norm": 1.5912094116210938,
      "learning_rate": 0.00099960680944022,
      "loss": 2.7476,
      "step": 2003
    },
    {
      "epoch": 2.004,
      "grad_norm": 2.40547251701355,
      "learning_rate": 0.0009988204285638074,
      "loss": 2.568,
      "step": 2004
    },
    {
      "epoch": 2.005,
      "grad_norm": 1.8578987121582031,
      "learning_rate": 0.0009980340484168357,
      "loss": 2.4754,
      "step": 2005
    },
    {
      "epoch": 2.006,
      "grad_norm": 2.7166953086853027,
      "learning_rate": 0.0009972476694855997,
      "loss": 2.4058,
      "step": 2006
    },
    {
      "epoch": 2.007,
      "grad_norm": 2.4803457260131836,
      "learning_rate": 0.0009964612922563915,
      "loss": 2.4461,
      "step": 2007
    },
    {
      "epoch": 2.008,
      "grad_norm": 1.0640183687210083,
      "learning_rate": 0.0009956749172155036,
      "loss": 2.3275,
      "step": 2008
    },
    {
      "epoch": 2.009,
      "grad_norm": 1.3975309133529663,
      "learning_rate": 0.0009948885448492265,
      "loss": 2.3415,
      "step": 2009
    },
    {
      "epoch": 2.01,
      "grad_norm": 1.6553983688354492,
      "learning_rate": 0.0009941021756438488,
      "loss": 2.5454,
      "step": 2010
    },
    {
      "epoch": 2.011,
      "grad_norm": 1.188897728919983,
      "learning_rate": 0.000993315810085658,
      "loss": 2.3414,
      "step": 2011
    },
    {
      "epoch": 2.012,
      "grad_norm": 1.3145935535430908,
      "learning_rate": 0.0009925294486609383,
      "loss": 2.4071,
      "step": 2012
    },
    {
      "epoch": 2.013,
      "grad_norm": 0.957618236541748,
      "learning_rate": 0.0009917430918559723,
      "loss": 2.2699,
      "step": 2013
    },
    {
      "epoch": 2.014,
      "grad_norm": 1.1199216842651367,
      "learning_rate": 0.0009909567401570393,
      "loss": 2.1852,
      "step": 2014
    },
    {
      "epoch": 2.015,
      "grad_norm": 0.9899839758872986,
      "learning_rate": 0.0009901703940504154,
      "loss": 2.2537,
      "step": 2015
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.8983848690986633,
      "learning_rate": 0.000989384054022373,
      "loss": 2.3224,
      "step": 2016
    },
    {
      "epoch": 2.017,
      "grad_norm": 1.3650920391082764,
      "learning_rate": 0.0009885977205591817,
      "loss": 2.3194,
      "step": 2017
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.7397907972335815,
      "learning_rate": 0.000987811394147106,
      "loss": 2.3138,
      "step": 2018
    },
    {
      "epoch": 2.019,
      "grad_norm": 0.6847810745239258,
      "learning_rate": 0.0009870250752724062,
      "loss": 2.129,
      "step": 2019
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.5027427673339844,
      "learning_rate": 0.000986238764421338,
      "loss": 2.2081,
      "step": 2020
    },
    {
      "epoch": 2.021,
      "grad_norm": 0.8682758808135986,
      "learning_rate": 0.0009854524620801536,
      "loss": 2.323,
      "step": 2021
    },
    {
      "epoch": 2.022,
      "grad_norm": 0.7994541525840759,
      "learning_rate": 0.0009846661687350974,
      "loss": 2.2201,
      "step": 2022
    },
    {
      "epoch": 2.023,
      "grad_norm": 0.4929162561893463,
      "learning_rate": 0.0009838798848724097,
      "loss": 2.2021,
      "step": 2023
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.8029307723045349,
      "learning_rate": 0.0009830936109783252,
      "loss": 2.2396,
      "step": 2024
    },
    {
      "epoch": 2.025,
      "grad_norm": 1.3937559127807617,
      "learning_rate": 0.0009823073475390713,
      "loss": 2.0127,
      "step": 2025
    },
    {
      "epoch": 2.026,
      "grad_norm": 1.3816136121749878,
      "learning_rate": 0.0009815210950408703,
      "loss": 2.1186,
      "step": 2026
    },
    {
      "epoch": 2.027,
      "grad_norm": 0.7729529142379761,
      "learning_rate": 0.0009807348539699365,
      "loss": 2.1437,
      "step": 2027
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.616429328918457,
      "learning_rate": 0.0009799486248124774,
      "loss": 2.1379,
      "step": 2028
    },
    {
      "epoch": 2.029,
      "grad_norm": 0.8118811845779419,
      "learning_rate": 0.0009791624080546938,
      "loss": 2.0843,
      "step": 2029
    },
    {
      "epoch": 2.03,
      "grad_norm": 1.4632619619369507,
      "learning_rate": 0.0009783762041827787,
      "loss": 2.1606,
      "step": 2030
    },
    {
      "epoch": 2.031,
      "grad_norm": 3.2820632457733154,
      "learning_rate": 0.0009775900136829167,
      "loss": 2.181,
      "step": 2031
    },
    {
      "epoch": 2.032,
      "grad_norm": 1.628987431526184,
      "learning_rate": 0.0009768038370412838,
      "loss": 1.9819,
      "step": 2032
    },
    {
      "epoch": 2.033,
      "grad_norm": 2.7178096771240234,
      "learning_rate": 0.0009760176747440485,
      "loss": 2.4132,
      "step": 2033
    },
    {
      "epoch": 2.034,
      "grad_norm": 2.988645315170288,
      "learning_rate": 0.0009752315272773695,
      "loss": 2.1843,
      "step": 2034
    },
    {
      "epoch": 2.035,
      "grad_norm": 5.228445529937744,
      "learning_rate": 0.0009744453951273968,
      "loss": 2.2237,
      "step": 2035
    },
    {
      "epoch": 2.036,
      "grad_norm": 3.8609719276428223,
      "learning_rate": 0.0009736592787802709,
      "loss": 2.3155,
      "step": 2036
    },
    {
      "epoch": 2.037,
      "grad_norm": 1.5002135038375854,
      "learning_rate": 0.0009728731787221219,
      "loss": 2.3115,
      "step": 2037
    },
    {
      "epoch": 2.038,
      "grad_norm": 2.1026411056518555,
      "learning_rate": 0.0009720870954390704,
      "loss": 2.0623,
      "step": 2038
    },
    {
      "epoch": 2.039,
      "grad_norm": 1.8143348693847656,
      "learning_rate": 0.0009713010294172276,
      "loss": 2.0689,
      "step": 2039
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.8380879163742065,
      "learning_rate": 0.0009705149811426923,
      "loss": 2.0747,
      "step": 2040
    },
    {
      "epoch": 2.041,
      "grad_norm": 1.923916220664978,
      "learning_rate": 0.0009697289511015528,
      "loss": 2.2257,
      "step": 2041
    },
    {
      "epoch": 2.042,
      "grad_norm": 1.5315104722976685,
      "learning_rate": 0.0009689429397798869,
      "loss": 1.9936,
      "step": 2042
    },
    {
      "epoch": 2.043,
      "grad_norm": 2.706444501876831,
      "learning_rate": 0.0009681569476637596,
      "loss": 2.2793,
      "step": 2043
    },
    {
      "epoch": 2.044,
      "grad_norm": 2.44498348236084,
      "learning_rate": 0.0009673709752392255,
      "loss": 2.1377,
      "step": 2044
    },
    {
      "epoch": 2.045,
      "grad_norm": 1.5683927536010742,
      "learning_rate": 0.0009665850229923258,
      "loss": 2.1403,
      "step": 2045
    },
    {
      "epoch": 2.046,
      "grad_norm": 1.4960694313049316,
      "learning_rate": 0.0009657990914090893,
      "loss": 2.0394,
      "step": 2046
    },
    {
      "epoch": 2.047,
      "grad_norm": 1.4117988348007202,
      "learning_rate": 0.0009650131809755325,
      "loss": 2.0371,
      "step": 2047
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.076383113861084,
      "learning_rate": 0.0009642272921776596,
      "loss": 1.933,
      "step": 2048
    },
    {
      "epoch": 2.049,
      "grad_norm": 2.686004161834717,
      "learning_rate": 0.0009634414255014598,
      "loss": 2.1357,
      "step": 2049
    },
    {
      "epoch": 2.05,
      "grad_norm": 2.3094189167022705,
      "learning_rate": 0.000962655581432909,
      "loss": 2.2088,
      "step": 2050
    },
    {
      "epoch": 2.051,
      "grad_norm": 1.4690823554992676,
      "learning_rate": 0.00096186976045797,
      "loss": 2.2252,
      "step": 2051
    },
    {
      "epoch": 2.052,
      "grad_norm": 2.4187557697296143,
      "learning_rate": 0.0009610839630625905,
      "loss": 2.1131,
      "step": 2052
    },
    {
      "epoch": 2.053,
      "grad_norm": 1.855281949043274,
      "learning_rate": 0.0009602981897327042,
      "loss": 2.1106,
      "step": 2053
    },
    {
      "epoch": 2.054,
      "grad_norm": 1.4113342761993408,
      "learning_rate": 0.0009595124409542294,
      "loss": 2.0441,
      "step": 2054
    },
    {
      "epoch": 2.055,
      "grad_norm": 2.4305055141448975,
      "learning_rate": 0.0009587267172130689,
      "loss": 2.2213,
      "step": 2055
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.8580002784729004,
      "learning_rate": 0.0009579410189951114,
      "loss": 2.0561,
      "step": 2056
    },
    {
      "epoch": 2.057,
      "grad_norm": 1.0827749967575073,
      "learning_rate": 0.0009571553467862288,
      "loss": 2.1405,
      "step": 2057
    },
    {
      "epoch": 2.058,
      "grad_norm": 1.2148480415344238,
      "learning_rate": 0.0009563697010722768,
      "loss": 2.1061,
      "step": 2058
    },
    {
      "epoch": 2.059,
      "grad_norm": 1.1170985698699951,
      "learning_rate": 0.0009555840823390948,
      "loss": 2.139,
      "step": 2059
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.8951014876365662,
      "learning_rate": 0.0009547984910725064,
      "loss": 1.9038,
      "step": 2060
    },
    {
      "epoch": 2.061,
      "grad_norm": 1.384167194366455,
      "learning_rate": 0.0009540129277583167,
      "loss": 2.1237,
      "step": 2061
    },
    {
      "epoch": 2.062,
      "grad_norm": 1.2754040956497192,
      "learning_rate": 0.0009532273928823151,
      "loss": 2.0129,
      "step": 2062
    },
    {
      "epoch": 2.063,
      "grad_norm": 1.3254746198654175,
      "learning_rate": 0.0009524418869302722,
      "loss": 1.9322,
      "step": 2063
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.396362543106079,
      "learning_rate": 0.0009516564103879408,
      "loss": 1.9462,
      "step": 2064
    },
    {
      "epoch": 2.065,
      "grad_norm": 1.535260558128357,
      "learning_rate": 0.0009508709637410566,
      "loss": 1.9271,
      "step": 2065
    },
    {
      "epoch": 2.066,
      "grad_norm": 2.010939359664917,
      "learning_rate": 0.0009500855474753359,
      "loss": 1.9761,
      "step": 2066
    },
    {
      "epoch": 2.067,
      "grad_norm": 0.8820014595985413,
      "learning_rate": 0.0009493001620764763,
      "loss": 2.0026,
      "step": 2067
    },
    {
      "epoch": 2.068,
      "grad_norm": 1.055237889289856,
      "learning_rate": 0.0009485148080301565,
      "loss": 2.0077,
      "step": 2068
    },
    {
      "epoch": 2.069,
      "grad_norm": 0.6218322515487671,
      "learning_rate": 0.0009477294858220356,
      "loss": 1.8621,
      "step": 2069
    },
    {
      "epoch": 2.07,
      "grad_norm": 4.030369281768799,
      "learning_rate": 0.000946944195937753,
      "loss": 2.1279,
      "step": 2070
    },
    {
      "epoch": 2.071,
      "grad_norm": 2.444586992263794,
      "learning_rate": 0.0009461589388629287,
      "loss": 1.937,
      "step": 2071
    },
    {
      "epoch": 2.072,
      "grad_norm": 17.822574615478516,
      "learning_rate": 0.0009453737150831616,
      "loss": 2.4905,
      "step": 2072
    },
    {
      "epoch": 2.073,
      "grad_norm": 7.543455123901367,
      "learning_rate": 0.0009445885250840301,
      "loss": 2.417,
      "step": 2073
    },
    {
      "epoch": 2.074,
      "grad_norm": 6.103090286254883,
      "learning_rate": 0.0009438033693510925,
      "loss": 2.2202,
      "step": 2074
    },
    {
      "epoch": 2.075,
      "grad_norm": 2.4170989990234375,
      "learning_rate": 0.0009430182483698853,
      "loss": 2.1161,
      "step": 2075
    },
    {
      "epoch": 2.076,
      "grad_norm": 3.8726625442504883,
      "learning_rate": 0.0009422331626259235,
      "loss": 2.1835,
      "step": 2076
    },
    {
      "epoch": 2.077,
      "grad_norm": 2.1187264919281006,
      "learning_rate": 0.0009414481126047005,
      "loss": 2.0796,
      "step": 2077
    },
    {
      "epoch": 2.078,
      "grad_norm": 2.7178614139556885,
      "learning_rate": 0.0009406630987916875,
      "loss": 2.2704,
      "step": 2078
    },
    {
      "epoch": 2.079,
      "grad_norm": 1.8011538982391357,
      "learning_rate": 0.000939878121672333,
      "loss": 1.9265,
      "step": 2079
    },
    {
      "epoch": 2.08,
      "grad_norm": 2.361254930496216,
      "learning_rate": 0.0009390931817320638,
      "loss": 2.1086,
      "step": 2080
    },
    {
      "epoch": 2.081,
      "grad_norm": 1.5502747297286987,
      "learning_rate": 0.0009383082794562823,
      "loss": 1.9476,
      "step": 2081
    },
    {
      "epoch": 2.082,
      "grad_norm": 2.308220386505127,
      "learning_rate": 0.0009375234153303684,
      "loss": 2.0354,
      "step": 2082
    },
    {
      "epoch": 2.083,
      "grad_norm": 2.564983606338501,
      "learning_rate": 0.000936738589839679,
      "loss": 2.1267,
      "step": 2083
    },
    {
      "epoch": 2.084,
      "grad_norm": 2.618889808654785,
      "learning_rate": 0.0009359538034695462,
      "loss": 1.9728,
      "step": 2084
    },
    {
      "epoch": 2.085,
      "grad_norm": 1.3090004920959473,
      "learning_rate": 0.0009351690567052779,
      "loss": 1.9823,
      "step": 2085
    },
    {
      "epoch": 2.086,
      "grad_norm": 1.163045883178711,
      "learning_rate": 0.0009343843500321581,
      "loss": 2.0681,
      "step": 2086
    },
    {
      "epoch": 2.087,
      "grad_norm": 2.7365007400512695,
      "learning_rate": 0.0009335996839354457,
      "loss": 1.9498,
      "step": 2087
    },
    {
      "epoch": 2.088,
      "grad_norm": 3.3305201530456543,
      "learning_rate": 0.0009328150589003741,
      "loss": 2.6873,
      "step": 2088
    },
    {
      "epoch": 2.089,
      "grad_norm": 1.7320770025253296,
      "learning_rate": 0.0009320304754121522,
      "loss": 1.9393,
      "step": 2089
    },
    {
      "epoch": 2.09,
      "grad_norm": 1.2135611772537231,
      "learning_rate": 0.0009312459339559619,
      "loss": 1.9263,
      "step": 2090
    },
    {
      "epoch": 2.091,
      "grad_norm": 9.086320877075195,
      "learning_rate": 0.0009304614350169611,
      "loss": 2.0645,
      "step": 2091
    },
    {
      "epoch": 2.092,
      "grad_norm": 3.525212049484253,
      "learning_rate": 0.0009296769790802794,
      "loss": 2.1941,
      "step": 2092
    },
    {
      "epoch": 2.093,
      "grad_norm": 1.103467345237732,
      "learning_rate": 0.000928892566631021,
      "loss": 1.9122,
      "step": 2093
    },
    {
      "epoch": 2.094,
      "grad_norm": 6.154074192047119,
      "learning_rate": 0.0009281081981542626,
      "loss": 2.2015,
      "step": 2094
    },
    {
      "epoch": 2.095,
      "grad_norm": 38.862144470214844,
      "learning_rate": 0.0009273238741350542,
      "loss": 2.3683,
      "step": 2095
    },
    {
      "epoch": 2.096,
      "grad_norm": 667.9360961914062,
      "learning_rate": 0.000926539595058418,
      "loss": 3.001,
      "step": 2096
    },
    {
      "epoch": 2.097,
      "grad_norm": 2972.16943359375,
      "learning_rate": 0.0009257553614093481,
      "loss": 3.794,
      "step": 2097
    },
    {
      "epoch": 2.098,
      "grad_norm": 1096.5657958984375,
      "learning_rate": 0.0009249711736728115,
      "loss": 4.0624,
      "step": 2098
    },
    {
      "epoch": 2.099,
      "grad_norm": 1747.49560546875,
      "learning_rate": 0.0009241870323337453,
      "loss": 4.5817,
      "step": 2099
    },
    {
      "epoch": 2.1,
      "grad_norm": 761.3848876953125,
      "learning_rate": 0.0009234029378770599,
      "loss": 5.0615,
      "step": 2100
    },
    {
      "epoch": 2.1,
      "eval_loss": 5.492180347442627,
      "eval_runtime": 212.5742,
      "eval_samples_per_second": 0.47,
      "eval_steps_per_second": 0.47,
      "step": 2100
    },
    {
      "epoch": 2.101,
      "grad_norm": 4843.6181640625,
      "learning_rate": 0.0009226188907876348,
      "loss": 5.5064,
      "step": 2101
    },
    {
      "epoch": 2.102,
      "grad_norm": 566.3602294921875,
      "learning_rate": 0.0009218348915503214,
      "loss": 5.9992,
      "step": 2102
    },
    {
      "epoch": 2.103,
      "grad_norm": 22.723814010620117,
      "learning_rate": 0.0009210509406499407,
      "loss": 6.2767,
      "step": 2103
    },
    {
      "epoch": 2.104,
      "grad_norm": 40.52032470703125,
      "learning_rate": 0.000920267038571285,
      "loss": 5.1045,
      "step": 2104
    },
    {
      "epoch": 2.105,
      "grad_norm": 95.82376861572266,
      "learning_rate": 0.0009194831857991148,
      "loss": 3.566,
      "step": 2105
    },
    {
      "epoch": 2.106,
      "grad_norm": 7.031428813934326,
      "learning_rate": 0.0009186993828181612,
      "loss": 3.0045,
      "step": 2106
    },
    {
      "epoch": 2.107,
      "grad_norm": 8.232762336730957,
      "learning_rate": 0.0009179156301131246,
      "loss": 2.8063,
      "step": 2107
    },
    {
      "epoch": 2.108,
      "grad_norm": 4.857107639312744,
      "learning_rate": 0.0009171319281686732,
      "loss": 2.6981,
      "step": 2108
    },
    {
      "epoch": 2.109,
      "grad_norm": 4.782524585723877,
      "learning_rate": 0.0009163482774694454,
      "loss": 2.797,
      "step": 2109
    },
    {
      "epoch": 2.11,
      "grad_norm": 4.789472579956055,
      "learning_rate": 0.0009155646785000466,
      "loss": 2.5386,
      "step": 2110
    },
    {
      "epoch": 2.111,
      "grad_norm": 5.792885780334473,
      "learning_rate": 0.0009147811317450511,
      "loss": 2.2821,
      "step": 2111
    },
    {
      "epoch": 2.112,
      "grad_norm": 2.72969913482666,
      "learning_rate": 0.0009139976376889999,
      "loss": 2.2358,
      "step": 2112
    },
    {
      "epoch": 2.113,
      "grad_norm": 39.96775817871094,
      "learning_rate": 0.0009132141968164026,
      "loss": 2.3229,
      "step": 2113
    },
    {
      "epoch": 2.114,
      "grad_norm": 7.313253402709961,
      "learning_rate": 0.0009124308096117351,
      "loss": 2.7513,
      "step": 2114
    },
    {
      "epoch": 2.115,
      "grad_norm": 4.502941608428955,
      "learning_rate": 0.0009116474765594402,
      "loss": 2.3894,
      "step": 2115
    },
    {
      "epoch": 2.116,
      "grad_norm": 3.212543487548828,
      "learning_rate": 0.0009108641981439275,
      "loss": 2.2714,
      "step": 2116
    },
    {
      "epoch": 2.117,
      "grad_norm": 2.111278533935547,
      "learning_rate": 0.0009100809748495722,
      "loss": 2.1926,
      "step": 2117
    },
    {
      "epoch": 2.118,
      "grad_norm": 1.98982834815979,
      "learning_rate": 0.0009092978071607166,
      "loss": 2.2179,
      "step": 2118
    },
    {
      "epoch": 2.1189999999999998,
      "grad_norm": 1.2338768243789673,
      "learning_rate": 0.0009085146955616679,
      "loss": 2.0988,
      "step": 2119
    },
    {
      "epoch": 2.12,
      "grad_norm": 2.2740976810455322,
      "learning_rate": 0.0009077316405366981,
      "loss": 2.4268,
      "step": 2120
    },
    {
      "epoch": 2.121,
      "grad_norm": 2.4119699001312256,
      "learning_rate": 0.0009069486425700448,
      "loss": 2.2783,
      "step": 2121
    },
    {
      "epoch": 2.122,
      "grad_norm": 1.1901708841323853,
      "learning_rate": 0.0009061657021459106,
      "loss": 2.0455,
      "step": 2122
    },
    {
      "epoch": 2.123,
      "grad_norm": 1.2730039358139038,
      "learning_rate": 0.000905382819748462,
      "loss": 2.0187,
      "step": 2123
    },
    {
      "epoch": 2.124,
      "grad_norm": 1.8841735124588013,
      "learning_rate": 0.0009045999958618292,
      "loss": 1.983,
      "step": 2124
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.8789797425270081,
      "learning_rate": 0.0009038172309701072,
      "loss": 2.0132,
      "step": 2125
    },
    {
      "epoch": 2.126,
      "grad_norm": 0.9773375988006592,
      "learning_rate": 0.0009030345255573545,
      "loss": 1.9983,
      "step": 2126
    },
    {
      "epoch": 2.127,
      "grad_norm": 0.8646376729011536,
      "learning_rate": 0.000902251880107592,
      "loss": 2.1912,
      "step": 2127
    },
    {
      "epoch": 2.128,
      "grad_norm": 1.1332042217254639,
      "learning_rate": 0.0009014692951048042,
      "loss": 2.0217,
      "step": 2128
    },
    {
      "epoch": 2.129,
      "grad_norm": 1.0505952835083008,
      "learning_rate": 0.0009006867710329378,
      "loss": 1.9036,
      "step": 2129
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.9579782485961914,
      "learning_rate": 0.0008999043083759016,
      "loss": 1.9495,
      "step": 2130
    },
    {
      "epoch": 2.1310000000000002,
      "grad_norm": 1.4537264108657837,
      "learning_rate": 0.0008991219076175676,
      "loss": 2.1145,
      "step": 2131
    },
    {
      "epoch": 2.132,
      "grad_norm": 1.253145694732666,
      "learning_rate": 0.0008983395692417681,
      "loss": 1.9319,
      "step": 2132
    },
    {
      "epoch": 2.133,
      "grad_norm": 1.6018648147583008,
      "learning_rate": 0.0008975572937322976,
      "loss": 2.0502,
      "step": 2133
    },
    {
      "epoch": 2.134,
      "grad_norm": 1.6166273355484009,
      "learning_rate": 0.0008967750815729113,
      "loss": 2.0341,
      "step": 2134
    },
    {
      "epoch": 2.135,
      "grad_norm": 1.334792137145996,
      "learning_rate": 0.0008959929332473262,
      "loss": 2.158,
      "step": 2135
    },
    {
      "epoch": 2.136,
      "grad_norm": 1.377239465713501,
      "learning_rate": 0.0008952108492392186,
      "loss": 2.0739,
      "step": 2136
    },
    {
      "epoch": 2.137,
      "grad_norm": 0.6739513874053955,
      "learning_rate": 0.0008944288300322258,
      "loss": 1.9253,
      "step": 2137
    },
    {
      "epoch": 2.138,
      "grad_norm": 1.6739320755004883,
      "learning_rate": 0.0008936468761099448,
      "loss": 2.0868,
      "step": 2138
    },
    {
      "epoch": 2.1390000000000002,
      "grad_norm": 0.6562186479568481,
      "learning_rate": 0.0008928649879559317,
      "loss": 1.9055,
      "step": 2139
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.6670135855674744,
      "learning_rate": 0.000892083166053703,
      "loss": 1.9204,
      "step": 2140
    },
    {
      "epoch": 2.141,
      "grad_norm": 1.2103782892227173,
      "learning_rate": 0.0008913014108867335,
      "loss": 1.9352,
      "step": 2141
    },
    {
      "epoch": 2.142,
      "grad_norm": 0.6745867729187012,
      "learning_rate": 0.0008905197229384566,
      "loss": 1.8525,
      "step": 2142
    },
    {
      "epoch": 2.143,
      "grad_norm": 0.8793162703514099,
      "learning_rate": 0.0008897381026922645,
      "loss": 1.8515,
      "step": 2143
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.9028021097183228,
      "learning_rate": 0.000888956550631508,
      "loss": 2.1296,
      "step": 2144
    },
    {
      "epoch": 2.145,
      "grad_norm": 0.8182129859924316,
      "learning_rate": 0.0008881750672394946,
      "loss": 1.9732,
      "step": 2145
    },
    {
      "epoch": 2.146,
      "grad_norm": 0.6721987128257751,
      "learning_rate": 0.0008873936529994904,
      "loss": 1.9101,
      "step": 2146
    },
    {
      "epoch": 2.147,
      "grad_norm": 0.6283994317054749,
      "learning_rate": 0.000886612308394718,
      "loss": 1.8079,
      "step": 2147
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.5017772316932678,
      "learning_rate": 0.000885831033908357,
      "loss": 1.8783,
      "step": 2148
    },
    {
      "epoch": 2.149,
      "grad_norm": 0.6335386037826538,
      "learning_rate": 0.0008850498300235443,
      "loss": 1.9456,
      "step": 2149
    },
    {
      "epoch": 2.15,
      "grad_norm": 1.012019395828247,
      "learning_rate": 0.0008842686972233725,
      "loss": 1.8878,
      "step": 2150
    },
    {
      "epoch": 2.151,
      "grad_norm": 2.960698366165161,
      "learning_rate": 0.0008834876359908901,
      "loss": 1.8416,
      "step": 2151
    },
    {
      "epoch": 2.152,
      "grad_norm": 1.153370976448059,
      "learning_rate": 0.0008827066468091018,
      "loss": 2.1088,
      "step": 2152
    },
    {
      "epoch": 2.153,
      "grad_norm": 12.310924530029297,
      "learning_rate": 0.000881925730160968,
      "loss": 2.3836,
      "step": 2153
    },
    {
      "epoch": 2.154,
      "grad_norm": 5.810102939605713,
      "learning_rate": 0.0008811448865294035,
      "loss": 2.318,
      "step": 2154
    },
    {
      "epoch": 2.155,
      "grad_norm": 3.876805305480957,
      "learning_rate": 0.0008803641163972787,
      "loss": 2.1287,
      "step": 2155
    },
    {
      "epoch": 2.156,
      "grad_norm": 2.058581590652466,
      "learning_rate": 0.0008795834202474178,
      "loss": 1.9169,
      "step": 2156
    },
    {
      "epoch": 2.157,
      "grad_norm": 4.365992069244385,
      "learning_rate": 0.0008788027985625993,
      "loss": 2.1047,
      "step": 2157
    },
    {
      "epoch": 2.158,
      "grad_norm": 4.187187671661377,
      "learning_rate": 0.0008780222518255568,
      "loss": 2.0974,
      "step": 2158
    },
    {
      "epoch": 2.159,
      "grad_norm": 2.9387547969818115,
      "learning_rate": 0.000877241780518976,
      "loss": 2.0788,
      "step": 2159
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.9975872039794922,
      "learning_rate": 0.0008764613851254967,
      "loss": 1.9958,
      "step": 2160
    },
    {
      "epoch": 2.161,
      "grad_norm": 2.6746246814727783,
      "learning_rate": 0.000875681066127712,
      "loss": 2.1125,
      "step": 2161
    },
    {
      "epoch": 2.162,
      "grad_norm": 3.960432767868042,
      "learning_rate": 0.0008749008240081677,
      "loss": 2.1297,
      "step": 2162
    },
    {
      "epoch": 2.163,
      "grad_norm": 3.695030689239502,
      "learning_rate": 0.0008741206592493611,
      "loss": 2.0817,
      "step": 2163
    },
    {
      "epoch": 2.164,
      "grad_norm": 3.144973039627075,
      "learning_rate": 0.0008733405723337432,
      "loss": 1.9581,
      "step": 2164
    },
    {
      "epoch": 2.165,
      "grad_norm": 1.817711353302002,
      "learning_rate": 0.0008725605637437155,
      "loss": 2.0103,
      "step": 2165
    },
    {
      "epoch": 2.166,
      "grad_norm": 2.4245381355285645,
      "learning_rate": 0.0008717806339616316,
      "loss": 2.3082,
      "step": 2166
    },
    {
      "epoch": 2.167,
      "grad_norm": 3.446186065673828,
      "learning_rate": 0.000871000783469797,
      "loss": 2.1424,
      "step": 2167
    },
    {
      "epoch": 2.168,
      "grad_norm": 2.6594884395599365,
      "learning_rate": 0.0008702210127504667,
      "loss": 1.991,
      "step": 2168
    },
    {
      "epoch": 2.169,
      "grad_norm": 1.0519540309906006,
      "learning_rate": 0.0008694413222858477,
      "loss": 1.8249,
      "step": 2169
    },
    {
      "epoch": 2.17,
      "grad_norm": 1.5638679265975952,
      "learning_rate": 0.0008686617125580968,
      "loss": 2.0253,
      "step": 2170
    },
    {
      "epoch": 2.171,
      "grad_norm": 2.1576640605926514,
      "learning_rate": 0.0008678821840493212,
      "loss": 2.0059,
      "step": 2171
    },
    {
      "epoch": 2.172,
      "grad_norm": 1.2064672708511353,
      "learning_rate": 0.0008671027372415772,
      "loss": 2.1566,
      "step": 2172
    },
    {
      "epoch": 2.173,
      "grad_norm": 1.9698411226272583,
      "learning_rate": 0.0008663233726168715,
      "loss": 1.8828,
      "step": 2173
    },
    {
      "epoch": 2.174,
      "grad_norm": 2.2355427742004395,
      "learning_rate": 0.0008655440906571592,
      "loss": 2.2243,
      "step": 2174
    },
    {
      "epoch": 2.175,
      "grad_norm": 1.5156452655792236,
      "learning_rate": 0.0008647648918443444,
      "loss": 2.0624,
      "step": 2175
    },
    {
      "epoch": 2.176,
      "grad_norm": 1.3783293962478638,
      "learning_rate": 0.0008639857766602805,
      "loss": 1.8313,
      "step": 2176
    },
    {
      "epoch": 2.177,
      "grad_norm": 0.772955060005188,
      "learning_rate": 0.000863206745586768,
      "loss": 2.0627,
      "step": 2177
    },
    {
      "epoch": 2.178,
      "grad_norm": 2.158170461654663,
      "learning_rate": 0.0008624277991055562,
      "loss": 2.4143,
      "step": 2178
    },
    {
      "epoch": 2.179,
      "grad_norm": 3.05454158782959,
      "learning_rate": 0.0008616489376983424,
      "loss": 2.0915,
      "step": 2179
    },
    {
      "epoch": 2.18,
      "grad_norm": 3.5538129806518555,
      "learning_rate": 0.0008608701618467705,
      "loss": 2.1015,
      "step": 2180
    },
    {
      "epoch": 2.181,
      "grad_norm": 2.014427900314331,
      "learning_rate": 0.0008600914720324316,
      "loss": 2.0589,
      "step": 2181
    },
    {
      "epoch": 2.182,
      "grad_norm": 0.9460834264755249,
      "learning_rate": 0.0008593128687368641,
      "loss": 1.9855,
      "step": 2182
    },
    {
      "epoch": 2.183,
      "grad_norm": 1.1025744676589966,
      "learning_rate": 0.0008585343524415524,
      "loss": 2.0126,
      "step": 2183
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.9167166352272034,
      "learning_rate": 0.0008577559236279272,
      "loss": 2.2043,
      "step": 2184
    },
    {
      "epoch": 2.185,
      "grad_norm": 0.6145142912864685,
      "learning_rate": 0.0008569775827773655,
      "loss": 1.8202,
      "step": 2185
    },
    {
      "epoch": 2.186,
      "grad_norm": 0.7855167388916016,
      "learning_rate": 0.0008561993303711893,
      "loss": 1.9027,
      "step": 2186
    },
    {
      "epoch": 2.187,
      "grad_norm": 0.6846959590911865,
      "learning_rate": 0.000855421166890666,
      "loss": 1.8519,
      "step": 2187
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.6284233331680298,
      "learning_rate": 0.000854643092817009,
      "loss": 2.0032,
      "step": 2188
    },
    {
      "epoch": 2.189,
      "grad_norm": 0.8276501297950745,
      "learning_rate": 0.0008538651086313753,
      "loss": 1.8693,
      "step": 2189
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.8735954165458679,
      "learning_rate": 0.0008530872148148662,
      "loss": 2.0153,
      "step": 2190
    },
    {
      "epoch": 2.191,
      "grad_norm": 0.6996710300445557,
      "learning_rate": 0.0008523094118485279,
      "loss": 1.9142,
      "step": 2191
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.823249340057373,
      "learning_rate": 0.0008515317002133503,
      "loss": 1.7683,
      "step": 2192
    },
    {
      "epoch": 2.193,
      "grad_norm": 1.0813589096069336,
      "learning_rate": 0.0008507540803902661,
      "loss": 1.8759,
      "step": 2193
    },
    {
      "epoch": 2.194,
      "grad_norm": 0.5176334381103516,
      "learning_rate": 0.000849976552860152,
      "loss": 1.8671,
      "step": 2194
    },
    {
      "epoch": 2.195,
      "grad_norm": 0.6931856870651245,
      "learning_rate": 0.0008491991181038268,
      "loss": 1.887,
      "step": 2195
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.5484617352485657,
      "learning_rate": 0.0008484217766020532,
      "loss": 1.9418,
      "step": 2196
    },
    {
      "epoch": 2.197,
      "grad_norm": 0.776728093624115,
      "learning_rate": 0.0008476445288355356,
      "loss": 1.7922,
      "step": 2197
    },
    {
      "epoch": 2.198,
      "grad_norm": 0.6736178994178772,
      "learning_rate": 0.00084686737528492,
      "loss": 1.8797,
      "step": 2198
    },
    {
      "epoch": 2.199,
      "grad_norm": 1.4282879829406738,
      "learning_rate": 0.0008460903164307942,
      "loss": 2.3347,
      "step": 2199
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.3285527229309082,
      "learning_rate": 0.000845313352753688,
      "loss": 1.8127,
      "step": 2200
    },
    {
      "epoch": 2.2,
      "eval_loss": 1.9038925170898438,
      "eval_runtime": 212.5898,
      "eval_samples_per_second": 0.47,
      "eval_steps_per_second": 0.47,
      "step": 2200
    },
    {
      "epoch": 2.201,
      "grad_norm": 0.9708554148674011,
      "learning_rate": 0.0008445364847340724,
      "loss": 1.8119,
      "step": 2201
    },
    {
      "epoch": 2.202,
      "grad_norm": 0.5669171214103699,
      "learning_rate": 0.0008437597128523581,
      "loss": 1.7455,
      "step": 2202
    },
    {
      "epoch": 2.203,
      "grad_norm": 0.685161292552948,
      "learning_rate": 0.000842983037588898,
      "loss": 1.7974,
      "step": 2203
    },
    {
      "epoch": 2.204,
      "grad_norm": 1.801484227180481,
      "learning_rate": 0.0008422064594239833,
      "loss": 1.7988,
      "step": 2204
    },
    {
      "epoch": 2.205,
      "grad_norm": 0.5204541683197021,
      "learning_rate": 0.0008414299788378475,
      "loss": 1.8628,
      "step": 2205
    },
    {
      "epoch": 2.206,
      "grad_norm": 0.47892075777053833,
      "learning_rate": 0.000840653596310662,
      "loss": 1.7718,
      "step": 2206
    },
    {
      "epoch": 2.207,
      "grad_norm": 0.5318266153335571,
      "learning_rate": 0.0008398773123225382,
      "loss": 1.8889,
      "step": 2207
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.9137154221534729,
      "learning_rate": 0.0008391011273535259,
      "loss": 2.1095,
      "step": 2208
    },
    {
      "epoch": 2.209,
      "grad_norm": 0.8365164399147034,
      "learning_rate": 0.0008383250418836151,
      "loss": 1.8271,
      "step": 2209
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.717365562915802,
      "learning_rate": 0.0008375490563927328,
      "loss": 2.0858,
      "step": 2210
    },
    {
      "epoch": 2.211,
      "grad_norm": 1.4167500734329224,
      "learning_rate": 0.0008367731713607445,
      "loss": 1.816,
      "step": 2211
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.5360310077667236,
      "learning_rate": 0.0008359973872674544,
      "loss": 1.835,
      "step": 2212
    },
    {
      "epoch": 2.213,
      "grad_norm": 0.6406094431877136,
      "learning_rate": 0.0008352217045926033,
      "loss": 1.8672,
      "step": 2213
    },
    {
      "epoch": 2.214,
      "grad_norm": 0.7312414646148682,
      "learning_rate": 0.00083444612381587,
      "loss": 1.9656,
      "step": 2214
    },
    {
      "epoch": 2.215,
      "grad_norm": 0.7515009641647339,
      "learning_rate": 0.00083367064541687,
      "loss": 1.7889,
      "step": 2215
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.6123015880584717,
      "learning_rate": 0.0008328952698751554,
      "loss": 1.7235,
      "step": 2216
    },
    {
      "epoch": 2.217,
      "grad_norm": 10.15129566192627,
      "learning_rate": 0.0008321199976702145,
      "loss": 1.7126,
      "step": 2217
    },
    {
      "epoch": 2.218,
      "grad_norm": 18.2227840423584,
      "learning_rate": 0.0008313448292814723,
      "loss": 1.8109,
      "step": 2218
    },
    {
      "epoch": 2.219,
      "grad_norm": 4.283178806304932,
      "learning_rate": 0.000830569765188289,
      "loss": 1.9441,
      "step": 2219
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.572595238685608,
      "learning_rate": 0.0008297948058699608,
      "loss": 1.9197,
      "step": 2220
    },
    {
      "epoch": 2.221,
      "grad_norm": 4.010723114013672,
      "learning_rate": 0.0008290199518057188,
      "loss": 2.0079,
      "step": 2221
    },
    {
      "epoch": 2.222,
      "grad_norm": 3.093674421310425,
      "learning_rate": 0.0008282452034747287,
      "loss": 1.8579,
      "step": 2222
    },
    {
      "epoch": 2.223,
      "grad_norm": 13.743805885314941,
      "learning_rate": 0.0008274705613560918,
      "loss": 1.9307,
      "step": 2223
    },
    {
      "epoch": 2.224,
      "grad_norm": 14.357647895812988,
      "learning_rate": 0.000826696025928843,
      "loss": 2.2079,
      "step": 2224
    },
    {
      "epoch": 2.225,
      "grad_norm": 1663.48046875,
      "learning_rate": 0.0008259215976719511,
      "loss": 3.1072,
      "step": 2225
    },
    {
      "epoch": 2.226,
      "grad_norm": 483.5248107910156,
      "learning_rate": 0.0008251472770643189,
      "loss": 3.5408,
      "step": 2226
    },
    {
      "epoch": 2.227,
      "grad_norm": 456.84100341796875,
      "learning_rate": 0.0008243730645847828,
      "loss": 3.4747,
      "step": 2227
    },
    {
      "epoch": 2.228,
      "grad_norm": 1983.5616455078125,
      "learning_rate": 0.0008235989607121118,
      "loss": 3.5697,
      "step": 2228
    },
    {
      "epoch": 2.229,
      "grad_norm": 20.895235061645508,
      "learning_rate": 0.0008228249659250086,
      "loss": 3.5142,
      "step": 2229
    },
    {
      "epoch": 2.23,
      "grad_norm": 2518.85986328125,
      "learning_rate": 0.000822051080702107,
      "loss": 3.0113,
      "step": 2230
    },
    {
      "epoch": 2.231,
      "grad_norm": 19.681354522705078,
      "learning_rate": 0.0008212773055219752,
      "loss": 2.9255,
      "step": 2231
    },
    {
      "epoch": 2.232,
      "grad_norm": 322.8922424316406,
      "learning_rate": 0.0008205036408631109,
      "loss": 2.5766,
      "step": 2232
    },
    {
      "epoch": 2.233,
      "grad_norm": 28.738080978393555,
      "learning_rate": 0.0008197300872039456,
      "loss": 2.5437,
      "step": 2233
    },
    {
      "epoch": 2.234,
      "grad_norm": 68.02130889892578,
      "learning_rate": 0.0008189566450228408,
      "loss": 2.4485,
      "step": 2234
    },
    {
      "epoch": 2.235,
      "grad_norm": 466.4164733886719,
      "learning_rate": 0.0008181833147980894,
      "loss": 2.4663,
      "step": 2235
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 125.0902328491211,
      "learning_rate": 0.0008174100970079153,
      "loss": 2.8206,
      "step": 2236
    },
    {
      "epoch": 2.237,
      "grad_norm": 7.616527557373047,
      "learning_rate": 0.0008166369921304726,
      "loss": 3.9667,
      "step": 2237
    },
    {
      "epoch": 2.238,
      "grad_norm": 21.15932846069336,
      "learning_rate": 0.000815864000643846,
      "loss": 3.4668,
      "step": 2238
    },
    {
      "epoch": 2.239,
      "grad_norm": 7.9584455490112305,
      "learning_rate": 0.0008150911230260492,
      "loss": 3.2631,
      "step": 2239
    },
    {
      "epoch": 2.24,
      "grad_norm": 5.044869899749756,
      "learning_rate": 0.0008143183597550266,
      "loss": 2.9651,
      "step": 2240
    },
    {
      "epoch": 2.241,
      "grad_norm": 4.27548360824585,
      "learning_rate": 0.0008135457113086511,
      "loss": 2.7644,
      "step": 2241
    },
    {
      "epoch": 2.242,
      "grad_norm": 2.937363386154175,
      "learning_rate": 0.0008127731781647252,
      "loss": 2.6114,
      "step": 2242
    },
    {
      "epoch": 2.243,
      "grad_norm": 3.310533285140991,
      "learning_rate": 0.0008120007608009793,
      "loss": 2.4198,
      "step": 2243
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2.4355030059814453,
      "learning_rate": 0.0008112284596950727,
      "loss": 2.3181,
      "step": 2244
    },
    {
      "epoch": 2.245,
      "grad_norm": 1.8452281951904297,
      "learning_rate": 0.0008104562753245929,
      "loss": 2.2317,
      "step": 2245
    },
    {
      "epoch": 2.246,
      "grad_norm": 1.2157834768295288,
      "learning_rate": 0.0008096842081670547,
      "loss": 2.373,
      "step": 2246
    },
    {
      "epoch": 2.247,
      "grad_norm": 1.9890474081039429,
      "learning_rate": 0.0008089122586999015,
      "loss": 2.2367,
      "step": 2247
    },
    {
      "epoch": 2.248,
      "grad_norm": 2.0158910751342773,
      "learning_rate": 0.0008081404274005022,
      "loss": 2.2153,
      "step": 2248
    },
    {
      "epoch": 2.249,
      "grad_norm": 1.3839638233184814,
      "learning_rate": 0.0008073687147461547,
      "loss": 2.4108,
      "step": 2249
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.8298076391220093,
      "learning_rate": 0.0008065971212140817,
      "loss": 2.1376,
      "step": 2250
    },
    {
      "epoch": 2.251,
      "grad_norm": 0.9255645871162415,
      "learning_rate": 0.0008058256472814333,
      "loss": 2.2525,
      "step": 2251
    },
    {
      "epoch": 2.252,
      "grad_norm": 1.5745501518249512,
      "learning_rate": 0.0008050542934252854,
      "loss": 2.0644,
      "step": 2252
    },
    {
      "epoch": 2.253,
      "grad_norm": 0.9979752898216248,
      "learning_rate": 0.0008042830601226392,
      "loss": 2.0891,
      "step": 2253
    },
    {
      "epoch": 2.254,
      "grad_norm": 2.2063190937042236,
      "learning_rate": 0.000803511947850422,
      "loss": 2.0305,
      "step": 2254
    },
    {
      "epoch": 2.255,
      "grad_norm": 1.9136719703674316,
      "learning_rate": 0.0008027409570854858,
      "loss": 1.8997,
      "step": 2255
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 2.21846604347229,
      "learning_rate": 0.0008019700883046079,
      "loss": 2.2189,
      "step": 2256
    },
    {
      "epoch": 2.257,
      "grad_norm": 2.0522868633270264,
      "learning_rate": 0.0008011993419844895,
      "loss": 2.1117,
      "step": 2257
    },
    {
      "epoch": 2.258,
      "grad_norm": 1.4203275442123413,
      "learning_rate": 0.000800428718601757,
      "loss": 1.9517,
      "step": 2258
    },
    {
      "epoch": 2.259,
      "grad_norm": 2.3422322273254395,
      "learning_rate": 0.0007996582186329598,
      "loss": 2.1625,
      "step": 2259
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.6210464239120483,
      "learning_rate": 0.0007988878425545719,
      "loss": 2.0684,
      "step": 2260
    },
    {
      "epoch": 2.261,
      "grad_norm": 0.9793791174888611,
      "learning_rate": 0.00079811759084299,
      "loss": 1.9838,
      "step": 2261
    },
    {
      "epoch": 2.262,
      "grad_norm": 1.3480377197265625,
      "learning_rate": 0.0007973474639745337,
      "loss": 1.9006,
      "step": 2262
    },
    {
      "epoch": 2.263,
      "grad_norm": 1.0993598699569702,
      "learning_rate": 0.0007965774624254464,
      "loss": 1.8468,
      "step": 2263
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 4.800854682922363,
      "learning_rate": 0.000795807586671893,
      "loss": 2.1877,
      "step": 2264
    },
    {
      "epoch": 2.265,
      "grad_norm": 3.332902431488037,
      "learning_rate": 0.0007950378371899609,
      "loss": 2.1582,
      "step": 2265
    },
    {
      "epoch": 2.266,
      "grad_norm": 1.3927122354507446,
      "learning_rate": 0.0007942682144556604,
      "loss": 1.8804,
      "step": 2266
    },
    {
      "epoch": 2.267,
      "grad_norm": 1.292284607887268,
      "learning_rate": 0.0007934987189449218,
      "loss": 2.0409,
      "step": 2267
    },
    {
      "epoch": 2.268,
      "grad_norm": 1.8116388320922852,
      "learning_rate": 0.0007927293511335976,
      "loss": 2.1109,
      "step": 2268
    },
    {
      "epoch": 2.269,
      "grad_norm": 1.9046870470046997,
      "learning_rate": 0.0007919601114974614,
      "loss": 1.9384,
      "step": 2269
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.0673719644546509,
      "learning_rate": 0.0007911910005122073,
      "loss": 1.89,
      "step": 2270
    },
    {
      "epoch": 2.271,
      "grad_norm": 2.1333346366882324,
      "learning_rate": 0.0007904220186534494,
      "loss": 1.8668,
      "step": 2271
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.6488852500915527,
      "learning_rate": 0.0007896531663967232,
      "loss": 1.965,
      "step": 2272
    },
    {
      "epoch": 2.273,
      "grad_norm": 1.0068672895431519,
      "learning_rate": 0.0007888844442174829,
      "loss": 1.9388,
      "step": 2273
    },
    {
      "epoch": 2.274,
      "grad_norm": 0.9081913828849792,
      "learning_rate": 0.0007881158525911023,
      "loss": 1.7623,
      "step": 2274
    },
    {
      "epoch": 2.275,
      "grad_norm": 1.2902984619140625,
      "learning_rate": 0.0007873473919928757,
      "loss": 1.8705,
      "step": 2275
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.821154773235321,
      "learning_rate": 0.000786579062898015,
      "loss": 1.8401,
      "step": 2276
    },
    {
      "epoch": 2.277,
      "grad_norm": 1.1686893701553345,
      "learning_rate": 0.0007858108657816512,
      "loss": 2.0255,
      "step": 2277
    },
    {
      "epoch": 2.278,
      "grad_norm": 0.9847453236579895,
      "learning_rate": 0.0007850428011188339,
      "loss": 1.7018,
      "step": 2278
    },
    {
      "epoch": 2.279,
      "grad_norm": 1.2558104991912842,
      "learning_rate": 0.0007842748693845306,
      "loss": 1.8494,
      "step": 2279
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.856256365776062,
      "learning_rate": 0.0007835070710536267,
      "loss": 1.8096,
      "step": 2280
    },
    {
      "epoch": 2.281,
      "grad_norm": 1.526312232017517,
      "learning_rate": 0.000782739406600925,
      "loss": 1.7486,
      "step": 2281
    },
    {
      "epoch": 2.282,
      "grad_norm": 0.9505694508552551,
      "learning_rate": 0.0007819718765011454,
      "loss": 1.8406,
      "step": 2282
    },
    {
      "epoch": 2.283,
      "grad_norm": 2.5089354515075684,
      "learning_rate": 0.0007812044812289248,
      "loss": 2.1654,
      "step": 2283
    },
    {
      "epoch": 2.284,
      "grad_norm": 2.203871250152588,
      "learning_rate": 0.0007804372212588175,
      "loss": 1.8288,
      "step": 2284
    },
    {
      "epoch": 2.285,
      "grad_norm": 2.123441457748413,
      "learning_rate": 0.0007796700970652931,
      "loss": 1.8052,
      "step": 2285
    },
    {
      "epoch": 2.286,
      "grad_norm": 1.828514814376831,
      "learning_rate": 0.0007789031091227372,
      "loss": 1.764,
      "step": 2286
    },
    {
      "epoch": 2.287,
      "grad_norm": 1.145536184310913,
      "learning_rate": 0.0007781362579054519,
      "loss": 1.809,
      "step": 2287
    },
    {
      "epoch": 2.288,
      "grad_norm": 1.500354528427124,
      "learning_rate": 0.0007773695438876542,
      "loss": 2.261,
      "step": 2288
    },
    {
      "epoch": 2.289,
      "grad_norm": 0.4952744245529175,
      "learning_rate": 0.0007766029675434765,
      "loss": 1.7877,
      "step": 2289
    },
    {
      "epoch": 2.29,
      "grad_norm": 1.5625841617584229,
      "learning_rate": 0.0007758365293469659,
      "loss": 1.7846,
      "step": 2290
    },
    {
      "epoch": 2.291,
      "grad_norm": 0.6739016771316528,
      "learning_rate": 0.0007750702297720838,
      "loss": 1.7473,
      "step": 2291
    },
    {
      "epoch": 2.292,
      "grad_norm": 1.238861322402954,
      "learning_rate": 0.0007743040692927068,
      "loss": 1.7547,
      "step": 2292
    },
    {
      "epoch": 2.293,
      "grad_norm": 0.6020500063896179,
      "learning_rate": 0.0007735380483826251,
      "loss": 1.8027,
      "step": 2293
    },
    {
      "epoch": 2.294,
      "grad_norm": 1.000583529472351,
      "learning_rate": 0.0007727721675155419,
      "loss": 1.9095,
      "step": 2294
    },
    {
      "epoch": 2.295,
      "grad_norm": 0.6712951064109802,
      "learning_rate": 0.0007720064271650741,
      "loss": 1.7317,
      "step": 2295
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.9718048572540283,
      "learning_rate": 0.0007712408278047527,
      "loss": 1.6969,
      "step": 2296
    },
    {
      "epoch": 2.297,
      "grad_norm": 1.3074274063110352,
      "learning_rate": 0.0007704753699080198,
      "loss": 1.726,
      "step": 2297
    },
    {
      "epoch": 2.298,
      "grad_norm": 0.6406285762786865,
      "learning_rate": 0.0007697100539482318,
      "loss": 1.7877,
      "step": 2298
    },
    {
      "epoch": 2.299,
      "grad_norm": 0.9540613889694214,
      "learning_rate": 0.000768944880398656,
      "loss": 1.6873,
      "step": 2299
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.5434691905975342,
      "learning_rate": 0.0007681798497324717,
      "loss": 1.9175,
      "step": 2300
    },
    {
      "epoch": 2.3,
      "eval_loss": 1.8394583463668823,
      "eval_runtime": 212.5215,
      "eval_samples_per_second": 0.471,
      "eval_steps_per_second": 0.471,
      "step": 2300
    },
    {
      "epoch": 2.301,
      "grad_norm": 1.1958366632461548,
      "learning_rate": 0.0007674149624227709,
      "loss": 2.2708,
      "step": 2301
    },
    {
      "epoch": 2.302,
      "grad_norm": 0.46832919120788574,
      "learning_rate": 0.0007666502189425564,
      "loss": 1.7227,
      "step": 2302
    },
    {
      "epoch": 2.303,
      "grad_norm": 1.0734566450119019,
      "learning_rate": 0.0007658856197647416,
      "loss": 1.8786,
      "step": 2303
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.7905056476593018,
      "learning_rate": 0.0007651211653621508,
      "loss": 1.7823,
      "step": 2304
    },
    {
      "epoch": 2.305,
      "grad_norm": 0.4767358899116516,
      "learning_rate": 0.0007643568562075195,
      "loss": 1.7813,
      "step": 2305
    },
    {
      "epoch": 2.306,
      "grad_norm": 1.0053749084472656,
      "learning_rate": 0.0007635926927734924,
      "loss": 1.8574,
      "step": 2306
    },
    {
      "epoch": 2.307,
      "grad_norm": 0.5395399928092957,
      "learning_rate": 0.000762828675532625,
      "loss": 1.7471,
      "step": 2307
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.6303451657295227,
      "learning_rate": 0.0007620648049573815,
      "loss": 1.6554,
      "step": 2308
    },
    {
      "epoch": 2.309,
      "grad_norm": 0.6231064796447754,
      "learning_rate": 0.0007613010815201357,
      "loss": 1.8873,
      "step": 2309
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.4092135429382324,
      "learning_rate": 0.0007605375056931712,
      "loss": 1.803,
      "step": 2310
    },
    {
      "epoch": 2.311,
      "grad_norm": 0.5230421423912048,
      "learning_rate": 0.0007597740779486795,
      "loss": 1.886,
      "step": 2311
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.6184127330780029,
      "learning_rate": 0.0007590107987587607,
      "loss": 1.7396,
      "step": 2312
    },
    {
      "epoch": 2.313,
      "grad_norm": 0.5387259721755981,
      "learning_rate": 0.0007582476685954225,
      "loss": 1.7637,
      "step": 2313
    },
    {
      "epoch": 2.314,
      "grad_norm": 0.5781248807907104,
      "learning_rate": 0.0007574846879305816,
      "loss": 1.6341,
      "step": 2314
    },
    {
      "epoch": 2.315,
      "grad_norm": 0.6368367671966553,
      "learning_rate": 0.0007567218572360615,
      "loss": 1.7822,
      "step": 2315
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.7297372817993164,
      "learning_rate": 0.000755959176983593,
      "loss": 1.7592,
      "step": 2316
    },
    {
      "epoch": 2.317,
      "grad_norm": 0.3522447645664215,
      "learning_rate": 0.0007551966476448139,
      "loss": 1.7975,
      "step": 2317
    },
    {
      "epoch": 2.318,
      "grad_norm": 0.5847525596618652,
      "learning_rate": 0.0007544342696912685,
      "loss": 1.656,
      "step": 2318
    },
    {
      "epoch": 2.319,
      "grad_norm": 106.3251724243164,
      "learning_rate": 0.0007536720435944082,
      "loss": 1.7293,
      "step": 2319
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.6888189911842346,
      "learning_rate": 0.0007529099698255901,
      "loss": 1.7333,
      "step": 2320
    },
    {
      "epoch": 2.321,
      "grad_norm": 1.7502245903015137,
      "learning_rate": 0.0007521480488560766,
      "loss": 1.8444,
      "step": 2321
    },
    {
      "epoch": 2.322,
      "grad_norm": 1.446255087852478,
      "learning_rate": 0.0007513862811570362,
      "loss": 1.8556,
      "step": 2322
    },
    {
      "epoch": 2.323,
      "grad_norm": 12.290411949157715,
      "learning_rate": 0.0007506246671995423,
      "loss": 1.9488,
      "step": 2323
    },
    {
      "epoch": 2.324,
      "grad_norm": 5.628824710845947,
      "learning_rate": 0.0007498632074545735,
      "loss": 2.0895,
      "step": 2324
    },
    {
      "epoch": 2.325,
      "grad_norm": 1.1428558826446533,
      "learning_rate": 0.000749101902393013,
      "loss": 1.7023,
      "step": 2325
    },
    {
      "epoch": 2.326,
      "grad_norm": 1.1197969913482666,
      "learning_rate": 0.0007483407524856476,
      "loss": 1.7633,
      "step": 2326
    },
    {
      "epoch": 2.327,
      "grad_norm": 0.5923631191253662,
      "learning_rate": 0.0007475797582031698,
      "loss": 1.7131,
      "step": 2327
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.3619576692581177,
      "learning_rate": 0.0007468189200161741,
      "loss": 1.7188,
      "step": 2328
    },
    {
      "epoch": 2.329,
      "grad_norm": 1.4731457233428955,
      "learning_rate": 0.0007460582383951597,
      "loss": 2.1277,
      "step": 2329
    },
    {
      "epoch": 2.33,
      "grad_norm": 1.0264636278152466,
      "learning_rate": 0.0007452977138105283,
      "loss": 1.9256,
      "step": 2330
    },
    {
      "epoch": 2.331,
      "grad_norm": 4.379215717315674,
      "learning_rate": 0.0007445373467325847,
      "loss": 1.7882,
      "step": 2331
    },
    {
      "epoch": 2.332,
      "grad_norm": 9.50714111328125,
      "learning_rate": 0.0007437771376315364,
      "loss": 1.8012,
      "step": 2332
    },
    {
      "epoch": 2.333,
      "grad_norm": 4.389532089233398,
      "learning_rate": 0.0007430170869774928,
      "loss": 1.8237,
      "step": 2333
    },
    {
      "epoch": 2.334,
      "grad_norm": 1.796188473701477,
      "learning_rate": 0.0007422571952404662,
      "loss": 1.9708,
      "step": 2334
    },
    {
      "epoch": 2.335,
      "grad_norm": 2.7693698406219482,
      "learning_rate": 0.000741497462890369,
      "loss": 1.7704,
      "step": 2335
    },
    {
      "epoch": 2.336,
      "grad_norm": 7.239748001098633,
      "learning_rate": 0.0007407378903970174,
      "loss": 2.2526,
      "step": 2336
    },
    {
      "epoch": 2.337,
      "grad_norm": 7.655771732330322,
      "learning_rate": 0.0007399784782301267,
      "loss": 3.0507,
      "step": 2337
    },
    {
      "epoch": 2.338,
      "grad_norm": 3.4313106536865234,
      "learning_rate": 0.0007392192268593138,
      "loss": 2.9862,
      "step": 2338
    },
    {
      "epoch": 2.339,
      "grad_norm": 4.093647480010986,
      "learning_rate": 0.0007384601367540964,
      "loss": 2.8636,
      "step": 2339
    },
    {
      "epoch": 2.34,
      "grad_norm": 5.778218746185303,
      "learning_rate": 0.0007377012083838921,
      "loss": 2.9612,
      "step": 2340
    },
    {
      "epoch": 2.341,
      "grad_norm": 3.360590696334839,
      "learning_rate": 0.0007369424422180187,
      "loss": 2.9501,
      "step": 2341
    },
    {
      "epoch": 2.342,
      "grad_norm": 2.9420557022094727,
      "learning_rate": 0.0007361838387256932,
      "loss": 2.7794,
      "step": 2342
    },
    {
      "epoch": 2.343,
      "grad_norm": 3.6027793884277344,
      "learning_rate": 0.0007354253983760331,
      "loss": 2.6313,
      "step": 2343
    },
    {
      "epoch": 2.344,
      "grad_norm": 3.050718069076538,
      "learning_rate": 0.0007346671216380534,
      "loss": 2.4635,
      "step": 2344
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 2.4028427600860596,
      "learning_rate": 0.0007339090089806697,
      "loss": 2.5021,
      "step": 2345
    },
    {
      "epoch": 2.346,
      "grad_norm": 2.2777838706970215,
      "learning_rate": 0.0007331510608726949,
      "loss": 2.2357,
      "step": 2346
    },
    {
      "epoch": 2.347,
      "grad_norm": 4.0698018074035645,
      "learning_rate": 0.0007323932777828408,
      "loss": 2.5245,
      "step": 2347
    },
    {
      "epoch": 2.348,
      "grad_norm": 2.00020432472229,
      "learning_rate": 0.0007316356601797162,
      "loss": 2.3989,
      "step": 2348
    },
    {
      "epoch": 2.349,
      "grad_norm": 3.660659074783325,
      "learning_rate": 0.0007308782085318293,
      "loss": 2.4333,
      "step": 2349
    },
    {
      "epoch": 2.35,
      "grad_norm": 1.5071804523468018,
      "learning_rate": 0.0007301209233075837,
      "loss": 2.2054,
      "step": 2350
    },
    {
      "epoch": 2.351,
      "grad_norm": 5.876341819763184,
      "learning_rate": 0.0007293638049752812,
      "loss": 2.538,
      "step": 2351
    },
    {
      "epoch": 2.352,
      "grad_norm": 4.717275142669678,
      "learning_rate": 0.0007286068540031205,
      "loss": 2.5376,
      "step": 2352
    },
    {
      "epoch": 2.3529999999999998,
      "grad_norm": 2.4437499046325684,
      "learning_rate": 0.0007278500708591959,
      "loss": 2.4433,
      "step": 2353
    },
    {
      "epoch": 2.354,
      "grad_norm": 3.419747829437256,
      "learning_rate": 0.0007270934560114992,
      "loss": 2.5512,
      "step": 2354
    },
    {
      "epoch": 2.355,
      "grad_norm": 1.0970247983932495,
      "learning_rate": 0.0007263370099279172,
      "loss": 2.3308,
      "step": 2355
    },
    {
      "epoch": 2.356,
      "grad_norm": 1.3593897819519043,
      "learning_rate": 0.0007255807330762325,
      "loss": 2.182,
      "step": 2356
    },
    {
      "epoch": 2.357,
      "grad_norm": 3.0860791206359863,
      "learning_rate": 0.0007248246259241229,
      "loss": 2.3763,
      "step": 2357
    },
    {
      "epoch": 2.358,
      "grad_norm": 1.750659465789795,
      "learning_rate": 0.0007240686889391621,
      "loss": 2.1545,
      "step": 2358
    },
    {
      "epoch": 2.359,
      "grad_norm": 3.521756649017334,
      "learning_rate": 0.0007233129225888176,
      "loss": 2.3826,
      "step": 2359
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.796473264694214,
      "learning_rate": 0.0007225573273404513,
      "loss": 2.2987,
      "step": 2360
    },
    {
      "epoch": 2.3609999999999998,
      "grad_norm": 1.729978322982788,
      "learning_rate": 0.0007218019036613201,
      "loss": 2.1441,
      "step": 2361
    },
    {
      "epoch": 2.362,
      "grad_norm": 1.9375921487808228,
      "learning_rate": 0.0007210466520185748,
      "loss": 2.4091,
      "step": 2362
    },
    {
      "epoch": 2.363,
      "grad_norm": 1.7235023975372314,
      "learning_rate": 0.0007202915728792592,
      "loss": 2.1755,
      "step": 2363
    },
    {
      "epoch": 2.364,
      "grad_norm": 1.4626625776290894,
      "learning_rate": 0.0007195366667103102,
      "loss": 2.1481,
      "step": 2364
    },
    {
      "epoch": 2.365,
      "grad_norm": 2.755976915359497,
      "learning_rate": 0.0007187819339785588,
      "loss": 2.1881,
      "step": 2365
    },
    {
      "epoch": 2.366,
      "grad_norm": 2.1175827980041504,
      "learning_rate": 0.0007180273751507279,
      "loss": 2.1998,
      "step": 2366
    },
    {
      "epoch": 2.367,
      "grad_norm": 1.3707795143127441,
      "learning_rate": 0.0007172729906934332,
      "loss": 2.1883,
      "step": 2367
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.067706823348999,
      "learning_rate": 0.0007165187810731823,
      "loss": 1.9786,
      "step": 2368
    },
    {
      "epoch": 2.3689999999999998,
      "grad_norm": 2.2088112831115723,
      "learning_rate": 0.000715764746756375,
      "loss": 2.2181,
      "step": 2369
    },
    {
      "epoch": 2.37,
      "grad_norm": 1.931769847869873,
      "learning_rate": 0.0007150108882093021,
      "loss": 2.2375,
      "step": 2370
    },
    {
      "epoch": 2.371,
      "grad_norm": 1.2258671522140503,
      "learning_rate": 0.000714257205898147,
      "loss": 2.1197,
      "step": 2371
    },
    {
      "epoch": 2.372,
      "grad_norm": 1.7482943534851074,
      "learning_rate": 0.0007135037002889829,
      "loss": 2.2756,
      "step": 2372
    },
    {
      "epoch": 2.373,
      "grad_norm": 1.0365103483200073,
      "learning_rate": 0.0007127503718477738,
      "loss": 2.2235,
      "step": 2373
    },
    {
      "epoch": 2.374,
      "grad_norm": 0.6920243501663208,
      "learning_rate": 0.0007119972210403749,
      "loss": 2.0143,
      "step": 2374
    },
    {
      "epoch": 2.375,
      "grad_norm": 1.1129114627838135,
      "learning_rate": 0.0007112442483325306,
      "loss": 2.1677,
      "step": 2375
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.43773090839385986,
      "learning_rate": 0.0007104914541898763,
      "loss": 1.9644,
      "step": 2376
    },
    {
      "epoch": 2.377,
      "grad_norm": 1.5141507387161255,
      "learning_rate": 0.0007097388390779358,
      "loss": 2.0442,
      "step": 2377
    },
    {
      "epoch": 2.378,
      "grad_norm": 0.5941362380981445,
      "learning_rate": 0.0007089864034621228,
      "loss": 2.1827,
      "step": 2378
    },
    {
      "epoch": 2.379,
      "grad_norm": 1.447619080543518,
      "learning_rate": 0.0007082341478077396,
      "loss": 2.0621,
      "step": 2379
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.505939245223999,
      "learning_rate": 0.0007074820725799786,
      "loss": 1.9769,
      "step": 2380
    },
    {
      "epoch": 2.3810000000000002,
      "grad_norm": 1.7815039157867432,
      "learning_rate": 0.0007067301782439188,
      "loss": 2.1679,
      "step": 2381
    },
    {
      "epoch": 2.382,
      "grad_norm": 0.5918967127799988,
      "learning_rate": 0.000705978465264528,
      "loss": 1.9271,
      "step": 2382
    },
    {
      "epoch": 2.383,
      "grad_norm": 1.67282235622406,
      "learning_rate": 0.0007052269341066623,
      "loss": 2.0989,
      "step": 2383
    },
    {
      "epoch": 2.384,
      "grad_norm": 1.0950067043304443,
      "learning_rate": 0.0007044755852350648,
      "loss": 2.0318,
      "step": 2384
    },
    {
      "epoch": 2.385,
      "grad_norm": 2.1411197185516357,
      "learning_rate": 0.0007037244191143661,
      "loss": 2.0976,
      "step": 2385
    },
    {
      "epoch": 2.386,
      "grad_norm": 1.867233395576477,
      "learning_rate": 0.000702973436209084,
      "loss": 1.9584,
      "step": 2386
    },
    {
      "epoch": 2.387,
      "grad_norm": 0.9494896531105042,
      "learning_rate": 0.0007022226369836224,
      "loss": 2.0039,
      "step": 2387
    },
    {
      "epoch": 2.388,
      "grad_norm": 1.5255508422851562,
      "learning_rate": 0.0007014720219022718,
      "loss": 2.0889,
      "step": 2388
    },
    {
      "epoch": 2.3890000000000002,
      "grad_norm": 0.7226173877716064,
      "learning_rate": 0.00070072159142921,
      "loss": 1.9877,
      "step": 2389
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.0877798795700073,
      "learning_rate": 0.0006999713460284988,
      "loss": 2.032,
      "step": 2390
    },
    {
      "epoch": 2.391,
      "grad_norm": 0.6585405468940735,
      "learning_rate": 0.0006992212861640868,
      "loss": 1.8736,
      "step": 2391
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.9511442184448242,
      "learning_rate": 0.0006984714122998072,
      "loss": 1.8545,
      "step": 2392
    },
    {
      "epoch": 2.393,
      "grad_norm": 0.9648119211196899,
      "learning_rate": 0.0006977217248993785,
      "loss": 1.9945,
      "step": 2393
    },
    {
      "epoch": 2.394,
      "grad_norm": 0.7350901961326599,
      "learning_rate": 0.000696972224426404,
      "loss": 2.0622,
      "step": 2394
    },
    {
      "epoch": 2.395,
      "grad_norm": 0.7175577878952026,
      "learning_rate": 0.0006962229113443712,
      "loss": 2.0664,
      "step": 2395
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.6016458868980408,
      "learning_rate": 0.0006954737861166512,
      "loss": 1.947,
      "step": 2396
    },
    {
      "epoch": 2.3970000000000002,
      "grad_norm": 0.9557979702949524,
      "learning_rate": 0.0006947248492065003,
      "loss": 2.0089,
      "step": 2397
    },
    {
      "epoch": 2.398,
      "grad_norm": 0.6876890063285828,
      "learning_rate": 0.0006939761010770574,
      "loss": 1.9242,
      "step": 2398
    },
    {
      "epoch": 2.399,
      "grad_norm": 0.8950368165969849,
      "learning_rate": 0.0006932275421913446,
      "loss": 2.0076,
      "step": 2399
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.938229501247406,
      "learning_rate": 0.0006924791730122671,
      "loss": 1.9662,
      "step": 2400
    },
    {
      "epoch": 2.4,
      "eval_loss": 1.9377014636993408,
      "eval_runtime": 212.9041,
      "eval_samples_per_second": 0.47,
      "eval_steps_per_second": 0.47,
      "step": 2400
    },
    {
      "epoch": 2.401,
      "grad_norm": 0.8780391812324524,
      "learning_rate": 0.0006917309940026131,
      "loss": 1.8895,
      "step": 2401
    },
    {
      "epoch": 2.402,
      "grad_norm": 1.0222495794296265,
      "learning_rate": 0.0006909830056250527,
      "loss": 1.8431,
      "step": 2402
    },
    {
      "epoch": 2.403,
      "grad_norm": 2.0948426723480225,
      "learning_rate": 0.0006902352083421386,
      "loss": 2.1264,
      "step": 2403
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.7125944495201111,
      "learning_rate": 0.0006894876026163051,
      "loss": 1.8354,
      "step": 2404
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 1.0548762083053589,
      "learning_rate": 0.0006887401889098672,
      "loss": 1.7832,
      "step": 2405
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.9367396831512451,
      "learning_rate": 0.0006879929676850232,
      "loss": 1.9346,
      "step": 2406
    },
    {
      "epoch": 2.407,
      "grad_norm": 1.1813629865646362,
      "learning_rate": 0.0006872459394038508,
      "loss": 1.8462,
      "step": 2407
    },
    {
      "epoch": 2.408,
      "grad_norm": 1.2071199417114258,
      "learning_rate": 0.0006864991045283086,
      "loss": 2.1333,
      "step": 2408
    },
    {
      "epoch": 2.409,
      "grad_norm": 3.1273133754730225,
      "learning_rate": 0.000685752463520236,
      "loss": 1.849,
      "step": 2409
    },
    {
      "epoch": 2.41,
      "grad_norm": 3.278541088104248,
      "learning_rate": 0.0006850060168413518,
      "loss": 1.9552,
      "step": 2410
    },
    {
      "epoch": 2.411,
      "grad_norm": 1.925380825996399,
      "learning_rate": 0.0006842597649532552,
      "loss": 1.7514,
      "step": 2411
    },
    {
      "epoch": 2.412,
      "grad_norm": 3.4902989864349365,
      "learning_rate": 0.0006835137083174254,
      "loss": 2.3104,
      "step": 2412
    },
    {
      "epoch": 2.413,
      "grad_norm": 2.9348137378692627,
      "learning_rate": 0.0006827678473952197,
      "loss": 1.8017,
      "step": 2413
    },
    {
      "epoch": 2.414,
      "grad_norm": 1.8080841302871704,
      "learning_rate": 0.0006820221826478747,
      "loss": 2.0939,
      "step": 2414
    },
    {
      "epoch": 2.415,
      "grad_norm": 4.953780174255371,
      "learning_rate": 0.0006812767145365069,
      "loss": 1.8324,
      "step": 2415
    },
    {
      "epoch": 2.416,
      "grad_norm": 3.520529270172119,
      "learning_rate": 0.0006805314435221096,
      "loss": 2.1175,
      "step": 2416
    },
    {
      "epoch": 2.417,
      "grad_norm": 2.290590524673462,
      "learning_rate": 0.0006797863700655548,
      "loss": 1.8508,
      "step": 2417
    },
    {
      "epoch": 2.418,
      "grad_norm": 16.343032836914062,
      "learning_rate": 0.0006790414946275931,
      "loss": 2.0505,
      "step": 2418
    },
    {
      "epoch": 2.419,
      "grad_norm": 0.9054503440856934,
      "learning_rate": 0.0006782968176688514,
      "loss": 1.8215,
      "step": 2419
    },
    {
      "epoch": 2.42,
      "grad_norm": 2.339871883392334,
      "learning_rate": 0.000677552339649834,
      "loss": 1.9099,
      "step": 2420
    },
    {
      "epoch": 2.421,
      "grad_norm": 3.9064228534698486,
      "learning_rate": 0.0006768080610309234,
      "loss": 1.9324,
      "step": 2421
    },
    {
      "epoch": 2.422,
      "grad_norm": 1.0793594121932983,
      "learning_rate": 0.0006760639822723776,
      "loss": 1.8352,
      "step": 2422
    },
    {
      "epoch": 2.423,
      "grad_norm": 2.069563388824463,
      "learning_rate": 0.000675320103834331,
      "loss": 1.7912,
      "step": 2423
    },
    {
      "epoch": 2.424,
      "grad_norm": 2.0660533905029297,
      "learning_rate": 0.000674576426176795,
      "loss": 1.8947,
      "step": 2424
    },
    {
      "epoch": 2.425,
      "grad_norm": 1.7316720485687256,
      "learning_rate": 0.0006738329497596565,
      "loss": 1.8293,
      "step": 2425
    },
    {
      "epoch": 2.426,
      "grad_norm": 2.2813339233398438,
      "learning_rate": 0.0006730896750426773,
      "loss": 2.275,
      "step": 2426
    },
    {
      "epoch": 2.427,
      "grad_norm": 1.0232785940170288,
      "learning_rate": 0.0006723466024854955,
      "loss": 1.8347,
      "step": 2427
    },
    {
      "epoch": 2.428,
      "grad_norm": 1.5621846914291382,
      "learning_rate": 0.0006716037325476233,
      "loss": 1.9199,
      "step": 2428
    },
    {
      "epoch": 2.429,
      "grad_norm": 0.9158923625946045,
      "learning_rate": 0.0006708610656884478,
      "loss": 1.9343,
      "step": 2429
    },
    {
      "epoch": 2.43,
      "grad_norm": 1.1602914333343506,
      "learning_rate": 0.000670118602367231,
      "loss": 1.8063,
      "step": 2430
    },
    {
      "epoch": 2.431,
      "grad_norm": 0.5851469039916992,
      "learning_rate": 0.0006693763430431083,
      "loss": 1.8174,
      "step": 2431
    },
    {
      "epoch": 2.432,
      "grad_norm": 1.306472659111023,
      "learning_rate": 0.00066863428817509,
      "loss": 1.6649,
      "step": 2432
    },
    {
      "epoch": 2.433,
      "grad_norm": 0.8598076701164246,
      "learning_rate": 0.0006678924382220586,
      "loss": 1.6621,
      "step": 2433
    },
    {
      "epoch": 2.434,
      "grad_norm": 0.8817684054374695,
      "learning_rate": 0.0006671507936427713,
      "loss": 1.7808,
      "step": 2434
    },
    {
      "epoch": 2.435,
      "grad_norm": 0.7575269937515259,
      "learning_rate": 0.0006664093548958569,
      "loss": 1.8438,
      "step": 2435
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.8077273368835449,
      "learning_rate": 0.0006656681224398183,
      "loss": 1.7145,
      "step": 2436
    },
    {
      "epoch": 2.437,
      "grad_norm": 1.1334161758422852,
      "learning_rate": 0.0006649270967330296,
      "loss": 1.7318,
      "step": 2437
    },
    {
      "epoch": 2.438,
      "grad_norm": 0.868711531162262,
      "learning_rate": 0.0006641862782337378,
      "loss": 1.8181,
      "step": 2438
    },
    {
      "epoch": 2.439,
      "grad_norm": 1.9919604063034058,
      "learning_rate": 0.0006634456674000617,
      "loss": 1.934,
      "step": 2439
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.482144355773926,
      "learning_rate": 0.0006627052646899908,
      "loss": 1.9565,
      "step": 2440
    },
    {
      "epoch": 2.441,
      "grad_norm": 18.89666748046875,
      "learning_rate": 0.0006619650705613878,
      "loss": 1.7939,
      "step": 2441
    },
    {
      "epoch": 2.442,
      "grad_norm": 210.61386108398438,
      "learning_rate": 0.0006612250854719842,
      "loss": 1.823,
      "step": 2442
    },
    {
      "epoch": 2.443,
      "grad_norm": 16157.8203125,
      "learning_rate": 0.000660485309879384,
      "loss": 1.9028,
      "step": 2443
    },
    {
      "epoch": 2.444,
      "grad_norm": 408.64605712890625,
      "learning_rate": 0.0006597457442410605,
      "loss": 1.8029,
      "step": 2444
    },
    {
      "epoch": 2.445,
      "grad_norm": 35.20411682128906,
      "learning_rate": 0.0006590063890143578,
      "loss": 1.8618,
      "step": 2445
    },
    {
      "epoch": 2.446,
      "grad_norm": 396.8101806640625,
      "learning_rate": 0.0006582672446564898,
      "loss": 1.7341,
      "step": 2446
    },
    {
      "epoch": 2.447,
      "grad_norm": 445.02716064453125,
      "learning_rate": 0.0006575283116245394,
      "loss": 1.9156,
      "step": 2447
    },
    {
      "epoch": 2.448,
      "grad_norm": 6642.78662109375,
      "learning_rate": 0.0006567895903754597,
      "loss": 2.0687,
      "step": 2448
    },
    {
      "epoch": 2.449,
      "grad_norm": 821.037841796875,
      "learning_rate": 0.0006560510813660718,
      "loss": 2.2925,
      "step": 2449
    },
    {
      "epoch": 2.45,
      "grad_norm": 1287.9654541015625,
      "learning_rate": 0.000655312785053067,
      "loss": 2.5668,
      "step": 2450
    },
    {
      "epoch": 2.451,
      "grad_norm": 47.7873420715332,
      "learning_rate": 0.000654574701893004,
      "loss": 2.8827,
      "step": 2451
    },
    {
      "epoch": 2.452,
      "grad_norm": 70.02271270751953,
      "learning_rate": 0.0006538368323423099,
      "loss": 3.2849,
      "step": 2452
    },
    {
      "epoch": 2.453,
      "grad_norm": 208.11102294921875,
      "learning_rate": 0.0006530991768572793,
      "loss": 3.2212,
      "step": 2453
    },
    {
      "epoch": 2.454,
      "grad_norm": 26.10474395751953,
      "learning_rate": 0.0006523617358940757,
      "loss": 4.1423,
      "step": 2454
    },
    {
      "epoch": 2.455,
      "grad_norm": 13.8216552734375,
      "learning_rate": 0.0006516245099087286,
      "loss": 3.1628,
      "step": 2455
    },
    {
      "epoch": 2.456,
      "grad_norm": 14.652896881103516,
      "learning_rate": 0.0006508874993571348,
      "loss": 2.5111,
      "step": 2456
    },
    {
      "epoch": 2.457,
      "grad_norm": 11.614781379699707,
      "learning_rate": 0.0006501507046950585,
      "loss": 2.234,
      "step": 2457
    },
    {
      "epoch": 2.458,
      "grad_norm": 4.037275314331055,
      "learning_rate": 0.0006494141263781297,
      "loss": 2.03,
      "step": 2458
    },
    {
      "epoch": 2.459,
      "grad_norm": 2.410273790359497,
      "learning_rate": 0.0006486777648618453,
      "loss": 2.1677,
      "step": 2459
    },
    {
      "epoch": 2.46,
      "grad_norm": 171.1645050048828,
      "learning_rate": 0.0006479416206015678,
      "loss": 2.2088,
      "step": 2460
    },
    {
      "epoch": 2.461,
      "grad_norm": 3.6046769618988037,
      "learning_rate": 0.0006472056940525254,
      "loss": 2.1932,
      "step": 2461
    },
    {
      "epoch": 2.462,
      "grad_norm": 3.819661855697632,
      "learning_rate": 0.0006464699856698111,
      "loss": 2.3183,
      "step": 2462
    },
    {
      "epoch": 2.463,
      "grad_norm": 2.9848251342773438,
      "learning_rate": 0.0006457344959083839,
      "loss": 1.8603,
      "step": 2463
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.1641634702682495,
      "learning_rate": 0.0006449992252230672,
      "loss": 1.8271,
      "step": 2464
    },
    {
      "epoch": 2.465,
      "grad_norm": 2.6468453407287598,
      "learning_rate": 0.0006442641740685484,
      "loss": 1.9091,
      "step": 2465
    },
    {
      "epoch": 2.466,
      "grad_norm": 3.1123220920562744,
      "learning_rate": 0.0006435293428993798,
      "loss": 1.9657,
      "step": 2466
    },
    {
      "epoch": 2.467,
      "grad_norm": 1.9971134662628174,
      "learning_rate": 0.0006427947321699783,
      "loss": 1.9138,
      "step": 2467
    },
    {
      "epoch": 2.468,
      "grad_norm": 1.0212595462799072,
      "learning_rate": 0.000642060342334623,
      "loss": 1.7427,
      "step": 2468
    },
    {
      "epoch": 2.469,
      "grad_norm": 1.8742876052856445,
      "learning_rate": 0.0006413261738474572,
      "loss": 1.9615,
      "step": 2469
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 51.17414474487305,
      "learning_rate": 0.0006405922271624873,
      "loss": 2.0071,
      "step": 2470
    },
    {
      "epoch": 2.471,
      "grad_norm": 107.8422622680664,
      "learning_rate": 0.0006398585027335823,
      "loss": 2.1037,
      "step": 2471
    },
    {
      "epoch": 2.472,
      "grad_norm": 36.36034393310547,
      "learning_rate": 0.000639125001014474,
      "loss": 2.091,
      "step": 2472
    },
    {
      "epoch": 2.473,
      "grad_norm": 1091.5130615234375,
      "learning_rate": 0.0006383917224587566,
      "loss": 2.5635,
      "step": 2473
    },
    {
      "epoch": 2.474,
      "grad_norm": 614.8148193359375,
      "learning_rate": 0.0006376586675198857,
      "loss": 2.6868,
      "step": 2474
    },
    {
      "epoch": 2.475,
      "grad_norm": 143.27969360351562,
      "learning_rate": 0.0006369258366511789,
      "loss": 2.1304,
      "step": 2475
    },
    {
      "epoch": 2.476,
      "grad_norm": 53.61708068847656,
      "learning_rate": 0.000636193230305816,
      "loss": 2.187,
      "step": 2476
    },
    {
      "epoch": 2.477,
      "grad_norm": 3.2287633419036865,
      "learning_rate": 0.0006354608489368368,
      "loss": 1.8683,
      "step": 2477
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 1.5506411790847778,
      "learning_rate": 0.0006347286929971427,
      "loss": 1.9279,
      "step": 2478
    },
    {
      "epoch": 2.479,
      "grad_norm": 1.5880941152572632,
      "learning_rate": 0.0006339967629394956,
      "loss": 1.7712,
      "step": 2479
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.220764636993408,
      "learning_rate": 0.000633265059216517,
      "loss": 1.881,
      "step": 2480
    },
    {
      "epoch": 2.481,
      "grad_norm": 1.7477694749832153,
      "learning_rate": 0.0006325335822806895,
      "loss": 1.8683,
      "step": 2481
    },
    {
      "epoch": 2.482,
      "grad_norm": 1.2518452405929565,
      "learning_rate": 0.0006318023325843547,
      "loss": 1.6371,
      "step": 2482
    },
    {
      "epoch": 2.483,
      "grad_norm": 1.4890998601913452,
      "learning_rate": 0.0006310713105797144,
      "loss": 1.8583,
      "step": 2483
    },
    {
      "epoch": 2.484,
      "grad_norm": 2.0504567623138428,
      "learning_rate": 0.0006303405167188282,
      "loss": 2.0295,
      "step": 2484
    },
    {
      "epoch": 2.485,
      "grad_norm": 1.0920283794403076,
      "learning_rate": 0.0006296099514536167,
      "loss": 1.7917,
      "step": 2485
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 1.0460400581359863,
      "learning_rate": 0.0006288796152358573,
      "loss": 1.7941,
      "step": 2486
    },
    {
      "epoch": 2.487,
      "grad_norm": 1.4454172849655151,
      "learning_rate": 0.0006281495085171868,
      "loss": 1.6845,
      "step": 2487
    },
    {
      "epoch": 2.488,
      "grad_norm": 1.0059661865234375,
      "learning_rate": 0.0006274196317490997,
      "loss": 1.756,
      "step": 2488
    },
    {
      "epoch": 2.489,
      "grad_norm": 0.7086067795753479,
      "learning_rate": 0.0006266899853829479,
      "loss": 1.8325,
      "step": 2489
    },
    {
      "epoch": 2.49,
      "grad_norm": 1.0104947090148926,
      "learning_rate": 0.000625960569869942,
      "loss": 1.7245,
      "step": 2490
    },
    {
      "epoch": 2.491,
      "grad_norm": 1.0848133563995361,
      "learning_rate": 0.0006252313856611485,
      "loss": 1.6661,
      "step": 2491
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.6803668737411499,
      "learning_rate": 0.0006245024332074916,
      "loss": 1.7381,
      "step": 2492
    },
    {
      "epoch": 2.493,
      "grad_norm": 0.8660109043121338,
      "learning_rate": 0.0006237737129597518,
      "loss": 1.746,
      "step": 2493
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 0.6326789259910583,
      "learning_rate": 0.000623045225368567,
      "loss": 1.7726,
      "step": 2494
    },
    {
      "epoch": 2.495,
      "grad_norm": 0.6461012959480286,
      "learning_rate": 0.0006223169708844299,
      "loss": 1.799,
      "step": 2495
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.9024046659469604,
      "learning_rate": 0.0006215889499576897,
      "loss": 1.9108,
      "step": 2496
    },
    {
      "epoch": 2.497,
      "grad_norm": 0.6818236112594604,
      "learning_rate": 0.0006208611630385514,
      "loss": 1.7285,
      "step": 2497
    },
    {
      "epoch": 2.498,
      "grad_norm": 1.3104032278060913,
      "learning_rate": 0.0006201336105770745,
      "loss": 1.6927,
      "step": 2498
    },
    {
      "epoch": 2.499,
      "grad_norm": 1.120530605316162,
      "learning_rate": 0.0006194062930231743,
      "loss": 1.8594,
      "step": 2499
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.62704998254776,
      "learning_rate": 0.0006186792108266204,
      "loss": 1.6966,
      "step": 2500
    },
    {
      "epoch": 2.5,
      "eval_loss": 1.786892294883728,
      "eval_runtime": 212.9135,
      "eval_samples_per_second": 0.47,
      "eval_steps_per_second": 0.47,
      "step": 2500
    },
    {
      "epoch": 2.501,
      "grad_norm": 1.8306328058242798,
      "learning_rate": 0.0006179523644370366,
      "loss": 1.7806,
      "step": 2501
    },
    {
      "epoch": 2.502,
      "grad_norm": 1.0310947895050049,
      "learning_rate": 0.0006172257543039023,
      "loss": 1.8633,
      "step": 2502
    },
    {
      "epoch": 2.503,
      "grad_norm": 0.9595350623130798,
      "learning_rate": 0.0006164993808765492,
      "loss": 1.6896,
      "step": 2503
    },
    {
      "epoch": 2.504,
      "grad_norm": 1.6065303087234497,
      "learning_rate": 0.000615773244604163,
      "loss": 1.8586,
      "step": 2504
    },
    {
      "epoch": 2.505,
      "grad_norm": 1.1239553689956665,
      "learning_rate": 0.0006150473459357831,
      "loss": 1.8423,
      "step": 2505
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 0.9021286964416504,
      "learning_rate": 0.000614321685320302,
      "loss": 1.6807,
      "step": 2506
    },
    {
      "epoch": 2.507,
      "grad_norm": 8.73215389251709,
      "learning_rate": 0.0006135962632064643,
      "loss": 2.0504,
      "step": 2507
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.8476823568344116,
      "learning_rate": 0.000612871080042868,
      "loss": 1.877,
      "step": 2508
    },
    {
      "epoch": 2.509,
      "grad_norm": 3.500501871109009,
      "learning_rate": 0.0006121461362779628,
      "loss": 2.1884,
      "step": 2509
    },
    {
      "epoch": 2.51,
      "grad_norm": 2.414642095565796,
      "learning_rate": 0.0006114214323600503,
      "loss": 1.8139,
      "step": 2510
    },
    {
      "epoch": 2.511,
      "grad_norm": 1.8932068347930908,
      "learning_rate": 0.0006106969687372845,
      "loss": 1.7464,
      "step": 2511
    },
    {
      "epoch": 2.512,
      "grad_norm": 2.038740873336792,
      "learning_rate": 0.0006099727458576701,
      "loss": 2.0104,
      "step": 2512
    },
    {
      "epoch": 2.513,
      "grad_norm": 0.7700923085212708,
      "learning_rate": 0.0006092487641690625,
      "loss": 1.8122,
      "step": 2513
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 0.8154453039169312,
      "learning_rate": 0.0006085250241191694,
      "loss": 1.8299,
      "step": 2514
    },
    {
      "epoch": 2.515,
      "grad_norm": 1.0441522598266602,
      "learning_rate": 0.0006078015261555479,
      "loss": 1.8346,
      "step": 2515
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.6460794806480408,
      "learning_rate": 0.0006070782707256053,
      "loss": 1.7931,
      "step": 2516
    },
    {
      "epoch": 2.517,
      "grad_norm": 1.2244850397109985,
      "learning_rate": 0.0006063552582765999,
      "loss": 1.67,
      "step": 2517
    },
    {
      "epoch": 2.518,
      "grad_norm": 0.46004024147987366,
      "learning_rate": 0.0006056324892556388,
      "loss": 1.7143,
      "step": 2518
    },
    {
      "epoch": 2.519,
      "grad_norm": 0.6927555799484253,
      "learning_rate": 0.0006049099641096787,
      "loss": 1.7241,
      "step": 2519
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.5465037226676941,
      "learning_rate": 0.0006041876832855267,
      "loss": 1.7413,
      "step": 2520
    },
    {
      "epoch": 2.521,
      "grad_norm": 0.6164615750312805,
      "learning_rate": 0.0006034656472298373,
      "loss": 1.6863,
      "step": 2521
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 0.4862855076789856,
      "learning_rate": 0.0006027438563891139,
      "loss": 1.8059,
      "step": 2522
    },
    {
      "epoch": 2.523,
      "grad_norm": 0.7697430849075317,
      "learning_rate": 0.000602022311209709,
      "loss": 1.7459,
      "step": 2523
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.565418004989624,
      "learning_rate": 0.0006013010121378224,
      "loss": 1.6389,
      "step": 2524
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.684057354927063,
      "learning_rate": 0.0006005799596195021,
      "loss": 1.7427,
      "step": 2525
    },
    {
      "epoch": 2.526,
      "grad_norm": 0.5002802610397339,
      "learning_rate": 0.0005998591541006437,
      "loss": 1.6034,
      "step": 2526
    },
    {
      "epoch": 2.527,
      "grad_norm": 0.7707077264785767,
      "learning_rate": 0.0005991385960269896,
      "loss": 1.8101,
      "step": 2527
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.4242174029350281,
      "learning_rate": 0.0005984182858441295,
      "loss": 1.6917,
      "step": 2528
    },
    {
      "epoch": 2.529,
      "grad_norm": 0.6809511780738831,
      "learning_rate": 0.0005976982239975005,
      "loss": 1.6352,
      "step": 2529
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.5779212117195129,
      "learning_rate": 0.0005969784109323849,
      "loss": 1.6939,
      "step": 2530
    },
    {
      "epoch": 2.531,
      "grad_norm": 0.6794657707214355,
      "learning_rate": 0.0005962588470939116,
      "loss": 1.6462,
      "step": 2531
    },
    {
      "epoch": 2.532,
      "grad_norm": 1.0218620300292969,
      "learning_rate": 0.0005955395329270559,
      "loss": 1.86,
      "step": 2532
    },
    {
      "epoch": 2.533,
      "grad_norm": 1.4431378841400146,
      "learning_rate": 0.0005948204688766378,
      "loss": 1.8602,
      "step": 2533
    },
    {
      "epoch": 2.534,
      "grad_norm": 0.8638519644737244,
      "learning_rate": 0.0005941016553873232,
      "loss": 1.838,
      "step": 2534
    },
    {
      "epoch": 2.535,
      "grad_norm": 0.6568241119384766,
      "learning_rate": 0.0005933830929036232,
      "loss": 1.9099,
      "step": 2535
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.6855493783950806,
      "learning_rate": 0.0005926647818698931,
      "loss": 1.7448,
      "step": 2536
    },
    {
      "epoch": 2.537,
      "grad_norm": 0.5255064368247986,
      "learning_rate": 0.0005919467227303332,
      "loss": 1.7509,
      "step": 2537
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 0.7240579724311829,
      "learning_rate": 0.0005912289159289883,
      "loss": 1.6762,
      "step": 2538
    },
    {
      "epoch": 2.539,
      "grad_norm": 0.8368822336196899,
      "learning_rate": 0.0005905113619097461,
      "loss": 1.7329,
      "step": 2539
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.396426200866699,
      "learning_rate": 0.0005897940611163389,
      "loss": 1.9393,
      "step": 2540
    },
    {
      "epoch": 2.541,
      "grad_norm": 1.2174761295318604,
      "learning_rate": 0.0005890770139923422,
      "loss": 1.6488,
      "step": 2541
    },
    {
      "epoch": 2.542,
      "grad_norm": 0.9609884023666382,
      "learning_rate": 0.0005883602209811741,
      "loss": 1.6638,
      "step": 2542
    },
    {
      "epoch": 2.543,
      "grad_norm": 0.913882315158844,
      "learning_rate": 0.0005876436825260967,
      "loss": 1.7737,
      "step": 2543
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.5748111009597778,
      "learning_rate": 0.0005869273990702135,
      "loss": 1.672,
      "step": 2544
    },
    {
      "epoch": 2.545,
      "grad_norm": 0.9573638439178467,
      "learning_rate": 0.0005862113710564705,
      "loss": 1.6778,
      "step": 2545
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 1.0153495073318481,
      "learning_rate": 0.0005854955989276566,
      "loss": 1.6612,
      "step": 2546
    },
    {
      "epoch": 2.547,
      "grad_norm": 0.47210967540740967,
      "learning_rate": 0.0005847800831264018,
      "loss": 1.5895,
      "step": 2547
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.5544244647026062,
      "learning_rate": 0.0005840648240951778,
      "loss": 1.5983,
      "step": 2548
    },
    {
      "epoch": 2.549,
      "grad_norm": 0.4764573872089386,
      "learning_rate": 0.0005833498222762972,
      "loss": 1.566,
      "step": 2549
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.7153118848800659,
      "learning_rate": 0.0005826350781119134,
      "loss": 1.7382,
      "step": 2550
    },
    {
      "epoch": 2.551,
      "grad_norm": 0.4806666970252991,
      "learning_rate": 0.0005819205920440214,
      "loss": 1.6482,
      "step": 2551
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.7240769267082214,
      "learning_rate": 0.0005812063645144559,
      "loss": 1.9193,
      "step": 2552
    },
    {
      "epoch": 2.553,
      "grad_norm": 0.5380377769470215,
      "learning_rate": 0.0005804923959648916,
      "loss": 1.6619,
      "step": 2553
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 0.7007753252983093,
      "learning_rate": 0.000579778686836843,
      "loss": 1.6392,
      "step": 2554
    },
    {
      "epoch": 2.555,
      "grad_norm": 0.9262288808822632,
      "learning_rate": 0.0005790652375716652,
      "loss": 2.0872,
      "step": 2555
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.7102757692337036,
      "learning_rate": 0.0005783520486105509,
      "loss": 1.6296,
      "step": 2556
    },
    {
      "epoch": 2.557,
      "grad_norm": 0.8717906475067139,
      "learning_rate": 0.0005776391203945339,
      "loss": 1.7701,
      "step": 2557
    },
    {
      "epoch": 2.558,
      "grad_norm": 0.6545275449752808,
      "learning_rate": 0.0005769264533644849,
      "loss": 1.6617,
      "step": 2558
    },
    {
      "epoch": 2.559,
      "grad_norm": 0.7075095772743225,
      "learning_rate": 0.000576214047961114,
      "loss": 1.6575,
      "step": 2559
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.7214097380638123,
      "learning_rate": 0.0005755019046249694,
      "loss": 1.6351,
      "step": 2560
    },
    {
      "epoch": 2.561,
      "grad_norm": 0.4731607735157013,
      "learning_rate": 0.0005747900237964371,
      "loss": 1.6487,
      "step": 2561
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 0.7661054730415344,
      "learning_rate": 0.0005740784059157404,
      "loss": 1.6998,
      "step": 2562
    },
    {
      "epoch": 2.5629999999999997,
      "grad_norm": 0.7489938139915466,
      "learning_rate": 0.0005733670514229406,
      "loss": 1.7123,
      "step": 2563
    },
    {
      "epoch": 2.564,
      "grad_norm": 1.3013871908187866,
      "learning_rate": 0.0005726559607579368,
      "loss": 1.9452,
      "step": 2564
    },
    {
      "epoch": 2.565,
      "grad_norm": 0.5554901957511902,
      "learning_rate": 0.0005719451343604633,
      "loss": 1.7076,
      "step": 2565
    },
    {
      "epoch": 2.566,
      "grad_norm": 2.2012364864349365,
      "learning_rate": 0.0005712345726700921,
      "loss": 1.6892,
      "step": 2566
    },
    {
      "epoch": 2.567,
      "grad_norm": 0.9817360043525696,
      "learning_rate": 0.000570524276126231,
      "loss": 1.8714,
      "step": 2567
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.7058130502700806,
      "learning_rate": 0.0005698142451681237,
      "loss": 1.6244,
      "step": 2568
    },
    {
      "epoch": 2.569,
      "grad_norm": 2.2247602939605713,
      "learning_rate": 0.0005691044802348508,
      "loss": 2.0756,
      "step": 2569
    },
    {
      "epoch": 2.57,
      "grad_norm": 1.0024033784866333,
      "learning_rate": 0.000568394981765327,
      "loss": 1.7348,
      "step": 2570
    },
    {
      "epoch": 2.5709999999999997,
      "grad_norm": 0.7350995540618896,
      "learning_rate": 0.0005676857501983026,
      "loss": 1.6395,
      "step": 2571
    },
    {
      "epoch": 2.572,
      "grad_norm": 1.2576032876968384,
      "learning_rate": 0.0005669767859723635,
      "loss": 1.7152,
      "step": 2572
    },
    {
      "epoch": 2.573,
      "grad_norm": 0.6880805492401123,
      "learning_rate": 0.00056626808952593,
      "loss": 1.5891,
      "step": 2573
    },
    {
      "epoch": 2.574,
      "grad_norm": 0.957780659198761,
      "learning_rate": 0.0005655596612972558,
      "loss": 1.714,
      "step": 2574
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.7101672887802124,
      "learning_rate": 0.0005648515017244305,
      "loss": 1.7168,
      "step": 2575
    },
    {
      "epoch": 2.576,
      "grad_norm": 1.083708643913269,
      "learning_rate": 0.0005641436112453761,
      "loss": 1.8046,
      "step": 2576
    },
    {
      "epoch": 2.577,
      "grad_norm": 0.515053927898407,
      "learning_rate": 0.0005634359902978488,
      "loss": 1.6515,
      "step": 2577
    },
    {
      "epoch": 2.578,
      "grad_norm": 0.5923256278038025,
      "learning_rate": 0.0005627286393194382,
      "loss": 1.6122,
      "step": 2578
    },
    {
      "epoch": 2.5789999999999997,
      "grad_norm": 23.527576446533203,
      "learning_rate": 0.0005620215587475666,
      "loss": 1.7493,
      "step": 2579
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.9160093665122986,
      "learning_rate": 0.0005613147490194887,
      "loss": 1.9263,
      "step": 2580
    },
    {
      "epoch": 2.581,
      "grad_norm": 0.6945699453353882,
      "learning_rate": 0.0005606082105722932,
      "loss": 1.9439,
      "step": 2581
    },
    {
      "epoch": 2.582,
      "grad_norm": 0.7869582176208496,
      "learning_rate": 0.0005599019438429001,
      "loss": 1.9798,
      "step": 2582
    },
    {
      "epoch": 2.583,
      "grad_norm": 0.6511512398719788,
      "learning_rate": 0.0005591959492680613,
      "loss": 1.737,
      "step": 2583
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.7398090958595276,
      "learning_rate": 0.0005584902272843602,
      "loss": 1.738,
      "step": 2584
    },
    {
      "epoch": 2.585,
      "grad_norm": 0.7834665775299072,
      "learning_rate": 0.0005577847783282122,
      "loss": 1.85,
      "step": 2585
    },
    {
      "epoch": 2.586,
      "grad_norm": 1.2638270854949951,
      "learning_rate": 0.000557079602835863,
      "loss": 1.9294,
      "step": 2586
    },
    {
      "epoch": 2.5869999999999997,
      "grad_norm": 1.8547594547271729,
      "learning_rate": 0.0005563747012433907,
      "loss": 1.703,
      "step": 2587
    },
    {
      "epoch": 2.588,
      "grad_norm": 1.2856770753860474,
      "learning_rate": 0.0005556700739867027,
      "loss": 1.6521,
      "step": 2588
    },
    {
      "epoch": 2.589,
      "grad_norm": 0.6362088322639465,
      "learning_rate": 0.0005549657215015367,
      "loss": 1.5933,
      "step": 2589
    },
    {
      "epoch": 2.59,
      "grad_norm": 1.3786113262176514,
      "learning_rate": 0.0005542616442234618,
      "loss": 1.6357,
      "step": 2590
    },
    {
      "epoch": 2.591,
      "grad_norm": 1.1935216188430786,
      "learning_rate": 0.0005535578425878755,
      "loss": 1.6943,
      "step": 2591
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.7627520561218262,
      "learning_rate": 0.0005528543170300053,
      "loss": 1.6293,
      "step": 2592
    },
    {
      "epoch": 2.593,
      "grad_norm": 0.7389509081840515,
      "learning_rate": 0.0005521510679849087,
      "loss": 1.6863,
      "step": 2593
    },
    {
      "epoch": 2.594,
      "grad_norm": 1.0170669555664062,
      "learning_rate": 0.0005514480958874711,
      "loss": 1.789,
      "step": 2594
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 0.5492205023765564,
      "learning_rate": 0.0005507454011724072,
      "loss": 1.6853,
      "step": 2595
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.5737965106964111,
      "learning_rate": 0.0005500429842742602,
      "loss": 1.7427,
      "step": 2596
    },
    {
      "epoch": 2.597,
      "grad_norm": 0.728268563747406,
      "learning_rate": 0.0005493408456274007,
      "loss": 1.6715,
      "step": 2597
    },
    {
      "epoch": 2.598,
      "grad_norm": 0.793572187423706,
      "learning_rate": 0.000548638985666029,
      "loss": 1.6033,
      "step": 2598
    },
    {
      "epoch": 2.599,
      "grad_norm": 0.5729205012321472,
      "learning_rate": 0.000547937404824171,
      "loss": 1.6037,
      "step": 2599
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.6913429498672485,
      "learning_rate": 0.0005472361035356819,
      "loss": 1.6811,
      "step": 2600
    },
    {
      "epoch": 2.6,
      "eval_loss": 1.714665412902832,
      "eval_runtime": 211.1879,
      "eval_samples_per_second": 0.474,
      "eval_steps_per_second": 0.474,
      "step": 2600
    },
    {
      "epoch": 2.601,
      "grad_norm": 0.6165462136268616,
      "learning_rate": 0.0005465350822342425,
      "loss": 1.5545,
      "step": 2601
    },
    {
      "epoch": 2.602,
      "grad_norm": 0.5451176166534424,
      "learning_rate": 0.0005458343413533613,
      "loss": 1.652,
      "step": 2602
    },
    {
      "epoch": 2.6029999999999998,
      "grad_norm": 0.588034987449646,
      "learning_rate": 0.000545133881326373,
      "loss": 1.626,
      "step": 2603
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.44514888525009155,
      "learning_rate": 0.0005444337025864384,
      "loss": 1.6335,
      "step": 2604
    },
    {
      "epoch": 2.605,
      "grad_norm": 0.6051210165023804,
      "learning_rate": 0.0005437338055665455,
      "loss": 1.728,
      "step": 2605
    },
    {
      "epoch": 2.606,
      "grad_norm": 0.802769124507904,
      "learning_rate": 0.0005430341906995064,
      "loss": 1.6763,
      "step": 2606
    },
    {
      "epoch": 2.607,
      "grad_norm": 0.4954468905925751,
      "learning_rate": 0.0005423348584179607,
      "loss": 1.5872,
      "step": 2607
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.386615514755249,
      "learning_rate": 0.0005416358091543716,
      "loss": 1.6354,
      "step": 2608
    },
    {
      "epoch": 2.609,
      "grad_norm": 0.5581346750259399,
      "learning_rate": 0.0005409370433410277,
      "loss": 1.713,
      "step": 2609
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.5444179177284241,
      "learning_rate": 0.0005402385614100423,
      "loss": 1.7571,
      "step": 2610
    },
    {
      "epoch": 2.6109999999999998,
      "grad_norm": 0.6639494299888611,
      "learning_rate": 0.000539540363793354,
      "loss": 1.7783,
      "step": 2611
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.6099221110343933,
      "learning_rate": 0.0005388424509227248,
      "loss": 1.6137,
      "step": 2612
    },
    {
      "epoch": 2.613,
      "grad_norm": 0.6391711235046387,
      "learning_rate": 0.0005381448232297401,
      "loss": 1.691,
      "step": 2613
    },
    {
      "epoch": 2.614,
      "grad_norm": 0.4505997896194458,
      "learning_rate": 0.0005374474811458101,
      "loss": 1.6906,
      "step": 2614
    },
    {
      "epoch": 2.615,
      "grad_norm": 0.5902570486068726,
      "learning_rate": 0.0005367504251021673,
      "loss": 1.6485,
      "step": 2615
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.9148176908493042,
      "learning_rate": 0.0005360536555298682,
      "loss": 1.7056,
      "step": 2616
    },
    {
      "epoch": 2.617,
      "grad_norm": 0.3920457661151886,
      "learning_rate": 0.0005353571728597921,
      "loss": 1.6066,
      "step": 2617
    },
    {
      "epoch": 2.618,
      "grad_norm": 0.6054778099060059,
      "learning_rate": 0.0005346609775226406,
      "loss": 1.6643,
      "step": 2618
    },
    {
      "epoch": 2.6189999999999998,
      "grad_norm": 0.6647197604179382,
      "learning_rate": 0.000533965069948937,
      "loss": 1.5268,
      "step": 2619
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.4876647889614105,
      "learning_rate": 0.0005332694505690277,
      "loss": 1.5819,
      "step": 2620
    },
    {
      "epoch": 2.621,
      "grad_norm": 1.021680235862732,
      "learning_rate": 0.0005325741198130802,
      "loss": 1.9379,
      "step": 2621
    },
    {
      "epoch": 2.622,
      "grad_norm": 0.8382158875465393,
      "learning_rate": 0.0005318790781110836,
      "loss": 1.5663,
      "step": 2622
    },
    {
      "epoch": 2.623,
      "grad_norm": 0.6572638154029846,
      "learning_rate": 0.0005311843258928489,
      "loss": 1.6597,
      "step": 2623
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.7052785754203796,
      "learning_rate": 0.000530489863588007,
      "loss": 1.83,
      "step": 2624
    },
    {
      "epoch": 2.625,
      "grad_norm": 1.1207644939422607,
      "learning_rate": 0.0005297956916260108,
      "loss": 1.6567,
      "step": 2625
    },
    {
      "epoch": 2.626,
      "grad_norm": 1.4269109964370728,
      "learning_rate": 0.0005291018104361326,
      "loss": 1.8064,
      "step": 2626
    },
    {
      "epoch": 2.627,
      "grad_norm": 0.6816164255142212,
      "learning_rate": 0.0005284082204474651,
      "loss": 1.5841,
      "step": 2627
    },
    {
      "epoch": 2.628,
      "grad_norm": 4.067736625671387,
      "learning_rate": 0.0005277149220889209,
      "loss": 1.5267,
      "step": 2628
    },
    {
      "epoch": 2.629,
      "grad_norm": 0.49020063877105713,
      "learning_rate": 0.0005270219157892331,
      "loss": 1.6836,
      "step": 2629
    },
    {
      "epoch": 2.63,
      "grad_norm": 1.1302789449691772,
      "learning_rate": 0.0005263292019769531,
      "loss": 1.6464,
      "step": 2630
    },
    {
      "epoch": 2.6310000000000002,
      "grad_norm": 0.7153705954551697,
      "learning_rate": 0.0005256367810804518,
      "loss": 1.5958,
      "step": 2631
    },
    {
      "epoch": 2.632,
      "grad_norm": 1.0768030881881714,
      "learning_rate": 0.0005249446535279188,
      "loss": 1.6096,
      "step": 2632
    },
    {
      "epoch": 2.633,
      "grad_norm": 0.6340337991714478,
      "learning_rate": 0.0005242528197473631,
      "loss": 1.556,
      "step": 2633
    },
    {
      "epoch": 2.634,
      "grad_norm": 2.618925094604492,
      "learning_rate": 0.0005235612801666107,
      "loss": 2.0952,
      "step": 2634
    },
    {
      "epoch": 2.635,
      "grad_norm": 0.6729148626327515,
      "learning_rate": 0.0005228700352133071,
      "loss": 1.7312,
      "step": 2635
    },
    {
      "epoch": 2.636,
      "grad_norm": 1.022916316986084,
      "learning_rate": 0.0005221790853149147,
      "loss": 1.643,
      "step": 2636
    },
    {
      "epoch": 2.637,
      "grad_norm": 0.5486909747123718,
      "learning_rate": 0.0005214884308987136,
      "loss": 1.5614,
      "step": 2637
    },
    {
      "epoch": 2.638,
      "grad_norm": 1.0772607326507568,
      "learning_rate": 0.0005207980723918012,
      "loss": 1.8237,
      "step": 2638
    },
    {
      "epoch": 2.6390000000000002,
      "grad_norm": 2.0036652088165283,
      "learning_rate": 0.0005201080102210918,
      "loss": 1.9748,
      "step": 2639
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.0965181589126587,
      "learning_rate": 0.0005194182448133162,
      "loss": 1.627,
      "step": 2640
    },
    {
      "epoch": 2.641,
      "grad_norm": 1.0691677331924438,
      "learning_rate": 0.0005187287765950229,
      "loss": 1.5952,
      "step": 2641
    },
    {
      "epoch": 2.642,
      "grad_norm": 0.89686518907547,
      "learning_rate": 0.0005180396059925755,
      "loss": 1.5896,
      "step": 2642
    },
    {
      "epoch": 2.643,
      "grad_norm": 0.5281707644462585,
      "learning_rate": 0.0005173507334321538,
      "loss": 1.6226,
      "step": 2643
    },
    {
      "epoch": 2.644,
      "grad_norm": 1.72031569480896,
      "learning_rate": 0.0005166621593397535,
      "loss": 1.965,
      "step": 2644
    },
    {
      "epoch": 2.645,
      "grad_norm": 0.6937479376792908,
      "learning_rate": 0.0005159738841411852,
      "loss": 1.7338,
      "step": 2645
    },
    {
      "epoch": 2.646,
      "grad_norm": 0.8450900912284851,
      "learning_rate": 0.0005152859082620748,
      "loss": 1.8486,
      "step": 2646
    },
    {
      "epoch": 2.6470000000000002,
      "grad_norm": 1.1073660850524902,
      "learning_rate": 0.0005145982321278641,
      "loss": 1.6507,
      "step": 2647
    },
    {
      "epoch": 2.648,
      "grad_norm": 1.8003567457199097,
      "learning_rate": 0.0005139108561638084,
      "loss": 1.6763,
      "step": 2648
    },
    {
      "epoch": 2.649,
      "grad_norm": 0.5653976202011108,
      "learning_rate": 0.0005132237807949777,
      "loss": 1.6434,
      "step": 2649
    },
    {
      "epoch": 2.65,
      "grad_norm": 1.8363890647888184,
      "learning_rate": 0.0005125370064462558,
      "loss": 1.6055,
      "step": 2650
    },
    {
      "epoch": 2.651,
      "grad_norm": 1.0960880517959595,
      "learning_rate": 0.0005118505335423414,
      "loss": 1.7104,
      "step": 2651
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.7118647694587708,
      "learning_rate": 0.0005111643625077454,
      "loss": 1.6823,
      "step": 2652
    },
    {
      "epoch": 2.653,
      "grad_norm": 1.515892505645752,
      "learning_rate": 0.0005104784937667935,
      "loss": 1.6657,
      "step": 2653
    },
    {
      "epoch": 2.654,
      "grad_norm": 0.8080297112464905,
      "learning_rate": 0.0005097929277436233,
      "loss": 1.7263,
      "step": 2654
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 0.6272861361503601,
      "learning_rate": 0.0005091076648621856,
      "loss": 1.6687,
      "step": 2655
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.4161112308502197,
      "learning_rate": 0.0005084227055462435,
      "loss": 1.9254,
      "step": 2656
    },
    {
      "epoch": 2.657,
      "grad_norm": 0.7012234926223755,
      "learning_rate": 0.0005077380502193725,
      "loss": 1.679,
      "step": 2657
    },
    {
      "epoch": 2.658,
      "grad_norm": 0.5253830552101135,
      "learning_rate": 0.0005070536993049608,
      "loss": 1.5251,
      "step": 2658
    },
    {
      "epoch": 2.659,
      "grad_norm": 0.7805975079536438,
      "learning_rate": 0.0005063696532262071,
      "loss": 1.525,
      "step": 2659
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.8690061569213867,
      "learning_rate": 0.0005056859124061231,
      "loss": 1.6207,
      "step": 2660
    },
    {
      "epoch": 2.661,
      "grad_norm": 0.5905859470367432,
      "learning_rate": 0.0005050024772675301,
      "loss": 1.5671,
      "step": 2661
    },
    {
      "epoch": 2.662,
      "grad_norm": 0.6866260170936584,
      "learning_rate": 0.0005043193482330617,
      "loss": 1.5616,
      "step": 2662
    },
    {
      "epoch": 2.6630000000000003,
      "grad_norm": 0.687498152256012,
      "learning_rate": 0.0005036365257251613,
      "loss": 1.6139,
      "step": 2663
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.4296895265579224,
      "learning_rate": 0.0005029540101660829,
      "loss": 1.864,
      "step": 2664
    },
    {
      "epoch": 2.665,
      "grad_norm": 0.7041391134262085,
      "learning_rate": 0.0005022718019778915,
      "loss": 1.6665,
      "step": 2665
    },
    {
      "epoch": 2.666,
      "grad_norm": 0.9436611533164978,
      "learning_rate": 0.0005015899015824612,
      "loss": 1.6485,
      "step": 2666
    },
    {
      "epoch": 2.667,
      "grad_norm": 0.8914515972137451,
      "learning_rate": 0.0005009083094014755,
      "loss": 1.6149,
      "step": 2667
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.6392229199409485,
      "learning_rate": 0.0005002270258564284,
      "loss": 1.6366,
      "step": 2668
    },
    {
      "epoch": 2.669,
      "grad_norm": 0.8383467197418213,
      "learning_rate": 0.0004995460513686223,
      "loss": 1.6373,
      "step": 2669
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.9999741315841675,
      "learning_rate": 0.0004988653863591681,
      "loss": 1.5538,
      "step": 2670
    },
    {
      "epoch": 2.6710000000000003,
      "grad_norm": 0.5178250074386597,
      "learning_rate": 0.0004981850312489866,
      "loss": 1.6028,
      "step": 2671
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.6320394277572632,
      "learning_rate": 0.0004975049864588058,
      "loss": 1.6029,
      "step": 2672
    },
    {
      "epoch": 2.673,
      "grad_norm": 0.6849908828735352,
      "learning_rate": 0.0004968252524091621,
      "loss": 1.6393,
      "step": 2673
    },
    {
      "epoch": 2.674,
      "grad_norm": 0.7749836444854736,
      "learning_rate": 0.0004961458295203999,
      "loss": 1.5674,
      "step": 2674
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.6674734950065613,
      "learning_rate": 0.0004954667182126707,
      "loss": 1.7303,
      "step": 2675
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.6629909873008728,
      "learning_rate": 0.0004947879189059341,
      "loss": 1.7034,
      "step": 2676
    },
    {
      "epoch": 2.677,
      "grad_norm": 0.5895974040031433,
      "learning_rate": 0.0004941094320199568,
      "loss": 1.643,
      "step": 2677
    },
    {
      "epoch": 2.678,
      "grad_norm": 1.0804569721221924,
      "learning_rate": 0.0004934312579743113,
      "loss": 1.6328,
      "step": 2678
    },
    {
      "epoch": 2.6790000000000003,
      "grad_norm": 0.7077092528343201,
      "learning_rate": 0.0004927533971883775,
      "loss": 1.6484,
      "step": 2679
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.4881455898284912,
      "learning_rate": 0.0004920758500813412,
      "loss": 1.6355,
      "step": 2680
    },
    {
      "epoch": 2.681,
      "grad_norm": 0.8371907472610474,
      "learning_rate": 0.0004913986170721941,
      "loss": 1.6023,
      "step": 2681
    },
    {
      "epoch": 2.682,
      "grad_norm": 1.3651806116104126,
      "learning_rate": 0.0004907216985797338,
      "loss": 1.7417,
      "step": 2682
    },
    {
      "epoch": 2.683,
      "grad_norm": 0.778580904006958,
      "learning_rate": 0.0004900450950225642,
      "loss": 1.6156,
      "step": 2683
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.7120836973190308,
      "learning_rate": 0.0004893688068190932,
      "loss": 1.7019,
      "step": 2684
    },
    {
      "epoch": 2.685,
      "grad_norm": 2.0762710571289062,
      "learning_rate": 0.0004886928343875341,
      "loss": 1.6687,
      "step": 2685
    },
    {
      "epoch": 2.686,
      "grad_norm": 1.7178666591644287,
      "learning_rate": 0.0004880171781459055,
      "loss": 1.6388,
      "step": 2686
    },
    {
      "epoch": 2.6870000000000003,
      "grad_norm": 1.1252185106277466,
      "learning_rate": 0.00048734183851202996,
      "loss": 1.6503,
      "step": 2687
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 1.1257078647613525,
      "learning_rate": 0.00048666681590353366,
      "loss": 1.6562,
      "step": 2688
    },
    {
      "epoch": 2.689,
      "grad_norm": 2.315305709838867,
      "learning_rate": 0.00048599211073784823,
      "loss": 1.6026,
      "step": 2689
    },
    {
      "epoch": 2.69,
      "grad_norm": 2.942772150039673,
      "learning_rate": 0.00048531772343220783,
      "loss": 1.692,
      "step": 2690
    },
    {
      "epoch": 2.691,
      "grad_norm": 0.9468375444412231,
      "learning_rate": 0.00048464365440365045,
      "loss": 1.6072,
      "step": 2691
    },
    {
      "epoch": 2.692,
      "grad_norm": 2.454606056213379,
      "learning_rate": 0.00048396990406901686,
      "loss": 1.6446,
      "step": 2692
    },
    {
      "epoch": 2.693,
      "grad_norm": 1.7749698162078857,
      "learning_rate": 0.00048329647284495105,
      "loss": 1.5815,
      "step": 2693
    },
    {
      "epoch": 2.694,
      "grad_norm": 0.8689014315605164,
      "learning_rate": 0.00048262336114789995,
      "loss": 1.6063,
      "step": 2694
    },
    {
      "epoch": 2.695,
      "grad_norm": 1.3150792121887207,
      "learning_rate": 0.00048195056939411295,
      "loss": 1.5924,
      "step": 2695
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.7600401043891907,
      "learning_rate": 0.0004812780979996411,
      "loss": 1.5458,
      "step": 2696
    },
    {
      "epoch": 2.697,
      "grad_norm": 1.0346542596817017,
      "learning_rate": 0.00048060594738033737,
      "loss": 1.536,
      "step": 2697
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.5991209149360657,
      "learning_rate": 0.0004799341179518565,
      "loss": 1.6474,
      "step": 2698
    },
    {
      "epoch": 2.699,
      "grad_norm": 1.5697828531265259,
      "learning_rate": 0.00047926261012965464,
      "loss": 1.6749,
      "step": 2699
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.5510943531990051,
      "learning_rate": 0.00047859142432898883,
      "loss": 1.5175,
      "step": 2700
    },
    {
      "epoch": 2.7,
      "eval_loss": 1.6547294855117798,
      "eval_runtime": 211.0846,
      "eval_samples_per_second": 0.474,
      "eval_steps_per_second": 0.474,
      "step": 2700
    },
    {
      "epoch": 2.701,
      "grad_norm": 1.6184206008911133,
      "learning_rate": 0.00047792056096491775,
      "loss": 1.6671,
      "step": 2701
    },
    {
      "epoch": 2.702,
      "grad_norm": 1.0015337467193604,
      "learning_rate": 0.0004772500204522994,
      "loss": 1.8427,
      "step": 2702
    },
    {
      "epoch": 2.703,
      "grad_norm": 1.4045003652572632,
      "learning_rate": 0.00047657980320579374,
      "loss": 1.5787,
      "step": 2703
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.6076086759567261,
      "learning_rate": 0.0004759099096398595,
      "loss": 1.5715,
      "step": 2704
    },
    {
      "epoch": 2.705,
      "grad_norm": 0.9303048849105835,
      "learning_rate": 0.0004752403401687556,
      "loss": 1.4974,
      "step": 2705
    },
    {
      "epoch": 2.706,
      "grad_norm": 0.39984792470932007,
      "learning_rate": 0.0004745710952065404,
      "loss": 1.5377,
      "step": 2706
    },
    {
      "epoch": 2.707,
      "grad_norm": 0.8853598237037659,
      "learning_rate": 0.0004739021751670725,
      "loss": 1.5925,
      "step": 2707
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.8180899024009705,
      "learning_rate": 0.00047323358046400844,
      "loss": 1.5225,
      "step": 2708
    },
    {
      "epoch": 2.709,
      "grad_norm": 0.6517159938812256,
      "learning_rate": 0.00047256531151080417,
      "loss": 1.5936,
      "step": 2709
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.4584696590900421,
      "learning_rate": 0.000471897368720714,
      "loss": 1.5608,
      "step": 2710
    },
    {
      "epoch": 2.711,
      "grad_norm": 1.0241026878356934,
      "learning_rate": 0.00047122975250679,
      "loss": 1.765,
      "step": 2711
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.5657296776771545,
      "learning_rate": 0.00047056246328188356,
      "loss": 1.5343,
      "step": 2712
    },
    {
      "epoch": 2.713,
      "grad_norm": 0.4908584654331207,
      "learning_rate": 0.00046989550145864345,
      "loss": 1.6058,
      "step": 2713
    },
    {
      "epoch": 2.714,
      "grad_norm": 0.8137084245681763,
      "learning_rate": 0.0004692288674495152,
      "loss": 1.7229,
      "step": 2714
    },
    {
      "epoch": 2.715,
      "grad_norm": 0.7546272873878479,
      "learning_rate": 0.00046856256166674225,
      "loss": 1.5663,
      "step": 2715
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.5676807761192322,
      "learning_rate": 0.00046789658452236495,
      "loss": 1.576,
      "step": 2716
    },
    {
      "epoch": 2.717,
      "grad_norm": 0.4901192784309387,
      "learning_rate": 0.00046723093642822,
      "loss": 1.577,
      "step": 2717
    },
    {
      "epoch": 2.718,
      "grad_norm": 0.7134264707565308,
      "learning_rate": 0.00046656561779594174,
      "loss": 1.6745,
      "step": 2718
    },
    {
      "epoch": 2.719,
      "grad_norm": 0.6318200826644897,
      "learning_rate": 0.00046590062903695983,
      "loss": 1.5774,
      "step": 2719
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.7349672317504883,
      "learning_rate": 0.00046523597056249966,
      "loss": 1.762,
      "step": 2720
    },
    {
      "epoch": 2.721,
      "grad_norm": 0.6602698564529419,
      "learning_rate": 0.00046457164278358357,
      "loss": 1.5292,
      "step": 2721
    },
    {
      "epoch": 2.722,
      "grad_norm": 0.6272059082984924,
      "learning_rate": 0.00046390764611102856,
      "loss": 1.5392,
      "step": 2722
    },
    {
      "epoch": 2.723,
      "grad_norm": 0.7289316654205322,
      "learning_rate": 0.0004632439809554468,
      "loss": 1.6074,
      "step": 2723
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.55665522813797,
      "learning_rate": 0.0004625806477272455,
      "loss": 1.5754,
      "step": 2724
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.9119605422019958,
      "learning_rate": 0.0004619176468366274,
      "loss": 1.6243,
      "step": 2725
    },
    {
      "epoch": 2.726,
      "grad_norm": 0.3846370577812195,
      "learning_rate": 0.00046125497869358876,
      "loss": 1.5632,
      "step": 2726
    },
    {
      "epoch": 2.727,
      "grad_norm": 0.7189209461212158,
      "learning_rate": 0.00046059264370792053,
      "loss": 1.5446,
      "step": 2727
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.5822734832763672,
      "learning_rate": 0.00045993064228920733,
      "loss": 1.5582,
      "step": 2728
    },
    {
      "epoch": 2.729,
      "grad_norm": 0.7646465301513672,
      "learning_rate": 0.0004592689748468275,
      "loss": 1.5164,
      "step": 2729
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.46725308895111084,
      "learning_rate": 0.0004586076417899534,
      "loss": 1.4926,
      "step": 2730
    },
    {
      "epoch": 2.731,
      "grad_norm": 1.0159988403320312,
      "learning_rate": 0.00045794664352755057,
      "loss": 1.6759,
      "step": 2731
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.813540518283844,
      "learning_rate": 0.0004572859804683769,
      "loss": 1.6373,
      "step": 2732
    },
    {
      "epoch": 2.733,
      "grad_norm": 0.6046154499053955,
      "learning_rate": 0.00045662565302098326,
      "loss": 1.5805,
      "step": 2733
    },
    {
      "epoch": 2.734,
      "grad_norm": 0.5216584205627441,
      "learning_rate": 0.000455965661593713,
      "loss": 1.5585,
      "step": 2734
    },
    {
      "epoch": 2.735,
      "grad_norm": 1.3041568994522095,
      "learning_rate": 0.00045530600659470134,
      "loss": 1.7706,
      "step": 2735
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.7124929428100586,
      "learning_rate": 0.0004546466884318766,
      "loss": 1.6917,
      "step": 2736
    },
    {
      "epoch": 2.737,
      "grad_norm": 1.1152617931365967,
      "learning_rate": 0.00045398770751295717,
      "loss": 1.6325,
      "step": 2737
    },
    {
      "epoch": 2.738,
      "grad_norm": 0.5622147917747498,
      "learning_rate": 0.0004533290642454546,
      "loss": 1.4242,
      "step": 2738
    },
    {
      "epoch": 2.739,
      "grad_norm": 1.0355708599090576,
      "learning_rate": 0.00045267075903667023,
      "loss": 1.6276,
      "step": 2739
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.6938788294792175,
      "learning_rate": 0.0004520127922936971,
      "loss": 1.5407,
      "step": 2740
    },
    {
      "epoch": 2.741,
      "grad_norm": 1.1656748056411743,
      "learning_rate": 0.00045135516442341853,
      "loss": 1.7653,
      "step": 2741
    },
    {
      "epoch": 2.742,
      "grad_norm": 0.8580932021141052,
      "learning_rate": 0.0004506978758325081,
      "loss": 1.6485,
      "step": 2742
    },
    {
      "epoch": 2.743,
      "grad_norm": 0.7742586731910706,
      "learning_rate": 0.00045004092692743094,
      "loss": 1.6526,
      "step": 2743
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 1.327366590499878,
      "learning_rate": 0.0004493843181144406,
      "loss": 1.61,
      "step": 2744
    },
    {
      "epoch": 2.745,
      "grad_norm": 0.8768760561943054,
      "learning_rate": 0.0004487280497995808,
      "loss": 1.716,
      "step": 2745
    },
    {
      "epoch": 2.746,
      "grad_norm": 0.8420963287353516,
      "learning_rate": 0.00044807212238868457,
      "loss": 1.8137,
      "step": 2746
    },
    {
      "epoch": 2.747,
      "grad_norm": 0.6597977876663208,
      "learning_rate": 0.0004474165362873749,
      "loss": 1.493,
      "step": 2747
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.6789302825927734,
      "learning_rate": 0.0004467612919010624,
      "loss": 1.6171,
      "step": 2748
    },
    {
      "epoch": 2.749,
      "grad_norm": 0.7270874977111816,
      "learning_rate": 0.0004461063896349479,
      "loss": 1.5673,
      "step": 2749
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.6022965908050537,
      "learning_rate": 0.0004454518298940196,
      "loss": 1.5473,
      "step": 2750
    },
    {
      "epoch": 2.751,
      "grad_norm": 0.7870147228240967,
      "learning_rate": 0.00044479761308305386,
      "loss": 1.6631,
      "step": 2751
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.7767630219459534,
      "learning_rate": 0.00044414373960661546,
      "loss": 1.5846,
      "step": 2752
    },
    {
      "epoch": 2.753,
      "grad_norm": 0.7458609342575073,
      "learning_rate": 0.0004434902098690563,
      "loss": 1.7031,
      "step": 2753
    },
    {
      "epoch": 2.754,
      "grad_norm": 0.5235418081283569,
      "learning_rate": 0.00044283702427451677,
      "loss": 1.664,
      "step": 2754
    },
    {
      "epoch": 2.755,
      "grad_norm": 0.5253062844276428,
      "learning_rate": 0.00044218418322692287,
      "loss": 1.4476,
      "step": 2755
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.6673548221588135,
      "learning_rate": 0.00044153168712998946,
      "loss": 1.6009,
      "step": 2756
    },
    {
      "epoch": 2.757,
      "grad_norm": 0.5450385212898254,
      "learning_rate": 0.00044087953638721647,
      "loss": 1.5234,
      "step": 2757
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.533557116985321,
      "learning_rate": 0.0004402277314018908,
      "loss": 1.5103,
      "step": 2758
    },
    {
      "epoch": 2.759,
      "grad_norm": 1.226305603981018,
      "learning_rate": 0.0004395762725770852,
      "loss": 1.7293,
      "step": 2759
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.6753045916557312,
      "learning_rate": 0.00043892516031565957,
      "loss": 1.527,
      "step": 2760
    },
    {
      "epoch": 2.761,
      "grad_norm": 0.5285758972167969,
      "learning_rate": 0.00043827439502025826,
      "loss": 1.5635,
      "step": 2761
    },
    {
      "epoch": 2.762,
      "grad_norm": 0.8007608652114868,
      "learning_rate": 0.00043762397709331137,
      "loss": 1.5471,
      "step": 2762
    },
    {
      "epoch": 2.763,
      "grad_norm": 0.9237383604049683,
      "learning_rate": 0.0004369739069370343,
      "loss": 1.7647,
      "step": 2763
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 1.2391058206558228,
      "learning_rate": 0.00043632418495342696,
      "loss": 1.5435,
      "step": 2764
    },
    {
      "epoch": 2.765,
      "grad_norm": 0.8312241435050964,
      "learning_rate": 0.00043567481154427524,
      "loss": 1.6138,
      "step": 2765
    },
    {
      "epoch": 2.766,
      "grad_norm": 0.5228425860404968,
      "learning_rate": 0.0004350257871111476,
      "loss": 1.5294,
      "step": 2766
    },
    {
      "epoch": 2.767,
      "grad_norm": 1.4076159000396729,
      "learning_rate": 0.00043437711205539866,
      "loss": 1.633,
      "step": 2767
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.9707247614860535,
      "learning_rate": 0.0004337287867781654,
      "loss": 1.618,
      "step": 2768
    },
    {
      "epoch": 2.769,
      "grad_norm": 1.749049186706543,
      "learning_rate": 0.00043308081168036926,
      "loss": 1.7309,
      "step": 2769
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.8561179637908936,
      "learning_rate": 0.000432433187162715,
      "loss": 1.5507,
      "step": 2770
    },
    {
      "epoch": 2.771,
      "grad_norm": 1.5521392822265625,
      "learning_rate": 0.00043178591362569,
      "loss": 1.5628,
      "step": 2771
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.9469984173774719,
      "learning_rate": 0.0004311389914695657,
      "loss": 1.7136,
      "step": 2772
    },
    {
      "epoch": 2.773,
      "grad_norm": 0.941617488861084,
      "learning_rate": 0.000430492421094396,
      "loss": 1.5139,
      "step": 2773
    },
    {
      "epoch": 2.774,
      "grad_norm": 0.9064761400222778,
      "learning_rate": 0.0004298462029000165,
      "loss": 1.4714,
      "step": 2774
    },
    {
      "epoch": 2.775,
      "grad_norm": 1.6843352317810059,
      "learning_rate": 0.0004292003372860457,
      "loss": 2.0254,
      "step": 2775
    },
    {
      "epoch": 2.776,
      "grad_norm": 7.453846454620361,
      "learning_rate": 0.00042855482465188366,
      "loss": 1.803,
      "step": 2776
    },
    {
      "epoch": 2.777,
      "grad_norm": 3.987063407897949,
      "learning_rate": 0.00042790966539671193,
      "loss": 1.7599,
      "step": 2777
    },
    {
      "epoch": 2.778,
      "grad_norm": 600.1929931640625,
      "learning_rate": 0.00042726485991949483,
      "loss": 1.6821,
      "step": 2778
    },
    {
      "epoch": 2.779,
      "grad_norm": 9.745420455932617,
      "learning_rate": 0.00042662040861897655,
      "loss": 1.923,
      "step": 2779
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 3.6937382221221924,
      "learning_rate": 0.0004259763118936827,
      "loss": 1.7608,
      "step": 2780
    },
    {
      "epoch": 2.781,
      "grad_norm": 230.6990203857422,
      "learning_rate": 0.0004253325701419194,
      "loss": 2.123,
      "step": 2781
    },
    {
      "epoch": 2.782,
      "grad_norm": 1128998.25,
      "learning_rate": 0.00042468918376177415,
      "loss": 2.55,
      "step": 2782
    },
    {
      "epoch": 2.783,
      "grad_norm": 598.7612915039062,
      "learning_rate": 0.0004240461531511136,
      "loss": 2.1841,
      "step": 2783
    },
    {
      "epoch": 2.784,
      "grad_norm": 485.0113220214844,
      "learning_rate": 0.0004234034787075847,
      "loss": 1.8261,
      "step": 2784
    },
    {
      "epoch": 2.785,
      "grad_norm": 5.368681907653809,
      "learning_rate": 0.0004227611608286147,
      "loss": 1.7512,
      "step": 2785
    },
    {
      "epoch": 2.786,
      "grad_norm": 7.708611488342285,
      "learning_rate": 0.0004221191999114098,
      "loss": 1.6652,
      "step": 2786
    },
    {
      "epoch": 2.787,
      "grad_norm": 2.1130595207214355,
      "learning_rate": 0.0004214775963529555,
      "loss": 1.8122,
      "step": 2787
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.8842872381210327,
      "learning_rate": 0.00042083635055001643,
      "loss": 1.5306,
      "step": 2788
    },
    {
      "epoch": 2.789,
      "grad_norm": 1.1006648540496826,
      "learning_rate": 0.0004201954628991356,
      "loss": 1.7825,
      "step": 2789
    },
    {
      "epoch": 2.79,
      "grad_norm": 2.8651657104492188,
      "learning_rate": 0.0004195549337966352,
      "loss": 1.6093,
      "step": 2790
    },
    {
      "epoch": 2.791,
      "grad_norm": 2.062730550765991,
      "learning_rate": 0.00041891476363861593,
      "loss": 1.6649,
      "step": 2791
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.9403738975524902,
      "learning_rate": 0.0004182749528209555,
      "loss": 1.6235,
      "step": 2792
    },
    {
      "epoch": 2.793,
      "grad_norm": 1.4731743335723877,
      "learning_rate": 0.00041763550173930984,
      "loss": 1.5828,
      "step": 2793
    },
    {
      "epoch": 2.794,
      "grad_norm": 1.0100305080413818,
      "learning_rate": 0.00041699641078911257,
      "loss": 1.5544,
      "step": 2794
    },
    {
      "epoch": 2.795,
      "grad_norm": 1.4698182344436646,
      "learning_rate": 0.00041635768036557395,
      "loss": 1.6766,
      "step": 2795
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.8001377582550049,
      "learning_rate": 0.0004157193108636829,
      "loss": 1.6957,
      "step": 2796
    },
    {
      "epoch": 2.797,
      "grad_norm": 0.6368133425712585,
      "learning_rate": 0.00041508130267820344,
      "loss": 1.5627,
      "step": 2797
    },
    {
      "epoch": 2.798,
      "grad_norm": 1.0309510231018066,
      "learning_rate": 0.0004144436562036771,
      "loss": 1.5475,
      "step": 2798
    },
    {
      "epoch": 2.799,
      "grad_norm": 0.9413233399391174,
      "learning_rate": 0.0004138063718344207,
      "loss": 1.4812,
      "step": 2799
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.5314803123474121,
      "learning_rate": 0.000413169449964529,
      "loss": 1.6976,
      "step": 2800
    },
    {
      "epoch": 2.8,
      "eval_loss": 1.6028082370758057,
      "eval_runtime": 211.0935,
      "eval_samples_per_second": 0.474,
      "eval_steps_per_second": 0.474,
      "step": 2800
    },
    {
      "epoch": 2.801,
      "grad_norm": 0.44063320755958557,
      "learning_rate": 0.00041253289098787074,
      "loss": 1.6197,
      "step": 2801
    },
    {
      "epoch": 2.802,
      "grad_norm": 1.785651683807373,
      "learning_rate": 0.00041189669529809094,
      "loss": 1.9065,
      "step": 2802
    },
    {
      "epoch": 2.803,
      "grad_norm": 0.5230408906936646,
      "learning_rate": 0.00041126086328861045,
      "loss": 1.5838,
      "step": 2803
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.6135118007659912,
      "learning_rate": 0.0004106253953526247,
      "loss": 1.7733,
      "step": 2804
    },
    {
      "epoch": 2.805,
      "grad_norm": 0.944736897945404,
      "learning_rate": 0.0004099902918831038,
      "loss": 1.595,
      "step": 2805
    },
    {
      "epoch": 2.806,
      "grad_norm": 0.5661976337432861,
      "learning_rate": 0.00040935555327279273,
      "loss": 1.6477,
      "step": 2806
    },
    {
      "epoch": 2.807,
      "grad_norm": 0.5946155786514282,
      "learning_rate": 0.00040872117991421066,
      "loss": 1.5794,
      "step": 2807
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.8029278516769409,
      "learning_rate": 0.0004080871721996513,
      "loss": 1.6826,
      "step": 2808
    },
    {
      "epoch": 2.809,
      "grad_norm": 0.5915900468826294,
      "learning_rate": 0.0004074535305211825,
      "loss": 1.4827,
      "step": 2809
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.5614420175552368,
      "learning_rate": 0.0004068202552706448,
      "loss": 1.5324,
      "step": 2810
    },
    {
      "epoch": 2.811,
      "grad_norm": 0.697158932685852,
      "learning_rate": 0.0004061873468396526,
      "loss": 1.5495,
      "step": 2811
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.5645257830619812,
      "learning_rate": 0.0004055548056195937,
      "loss": 1.524,
      "step": 2812
    },
    {
      "epoch": 2.8129999999999997,
      "grad_norm": 0.4826032221317291,
      "learning_rate": 0.00040492263200162795,
      "loss": 1.6417,
      "step": 2813
    },
    {
      "epoch": 2.814,
      "grad_norm": 0.5260575413703918,
      "learning_rate": 0.0004042908263766893,
      "loss": 1.6384,
      "step": 2814
    },
    {
      "epoch": 2.815,
      "grad_norm": 0.5795539617538452,
      "learning_rate": 0.0004036593891354832,
      "loss": 1.4554,
      "step": 2815
    },
    {
      "epoch": 2.816,
      "grad_norm": 1.219848871231079,
      "learning_rate": 0.0004030283206684868,
      "loss": 1.7028,
      "step": 2816
    },
    {
      "epoch": 2.817,
      "grad_norm": 1.0341453552246094,
      "learning_rate": 0.0004023976213659508,
      "loss": 1.5539,
      "step": 2817
    },
    {
      "epoch": 2.818,
      "grad_norm": 1.0190085172653198,
      "learning_rate": 0.00040176729161789615,
      "loss": 1.5454,
      "step": 2818
    },
    {
      "epoch": 2.819,
      "grad_norm": 0.7321681380271912,
      "learning_rate": 0.0004011373318141156,
      "loss": 1.5333,
      "step": 2819
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.9203085899353027,
      "learning_rate": 0.00040050774234417385,
      "loss": 1.6443,
      "step": 2820
    },
    {
      "epoch": 2.8209999999999997,
      "grad_norm": 0.9211425185203552,
      "learning_rate": 0.0003998785235974058,
      "loss": 1.5425,
      "step": 2821
    },
    {
      "epoch": 2.822,
      "grad_norm": 0.7672539949417114,
      "learning_rate": 0.00039924967596291727,
      "loss": 1.462,
      "step": 2822
    },
    {
      "epoch": 2.823,
      "grad_norm": 1.0096848011016846,
      "learning_rate": 0.0003986211998295847,
      "loss": 1.5841,
      "step": 2823
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.8077565431594849,
      "learning_rate": 0.0003979930955860548,
      "loss": 1.54,
      "step": 2824
    },
    {
      "epoch": 2.825,
      "grad_norm": 1.5154625177383423,
      "learning_rate": 0.0003973653636207437,
      "loss": 1.5358,
      "step": 2825
    },
    {
      "epoch": 2.826,
      "grad_norm": 0.5569692850112915,
      "learning_rate": 0.0003967380043218385,
      "loss": 1.4745,
      "step": 2826
    },
    {
      "epoch": 2.827,
      "grad_norm": 1.2637677192687988,
      "learning_rate": 0.00039611101807729545,
      "loss": 1.7037,
      "step": 2827
    },
    {
      "epoch": 2.828,
      "grad_norm": 1.360514760017395,
      "learning_rate": 0.00039548440527483955,
      "loss": 1.5055,
      "step": 2828
    },
    {
      "epoch": 2.8289999999999997,
      "grad_norm": 0.7444736361503601,
      "learning_rate": 0.00039485816630196514,
      "loss": 1.5336,
      "step": 2829
    },
    {
      "epoch": 2.83,
      "grad_norm": 1.164390206336975,
      "learning_rate": 0.00039423230154593536,
      "loss": 1.7132,
      "step": 2830
    },
    {
      "epoch": 2.831,
      "grad_norm": 0.9504817724227905,
      "learning_rate": 0.0003936068113937819,
      "loss": 1.5185,
      "step": 2831
    },
    {
      "epoch": 2.832,
      "grad_norm": 1.1033931970596313,
      "learning_rate": 0.0003929816962323054,
      "loss": 1.5449,
      "step": 2832
    },
    {
      "epoch": 2.833,
      "grad_norm": 0.9390631914138794,
      "learning_rate": 0.0003923569564480736,
      "loss": 1.6146,
      "step": 2833
    },
    {
      "epoch": 2.834,
      "grad_norm": 1.4305680990219116,
      "learning_rate": 0.0003917325924274231,
      "loss": 1.7249,
      "step": 2834
    },
    {
      "epoch": 2.835,
      "grad_norm": 0.5559298396110535,
      "learning_rate": 0.00039110860455645745,
      "loss": 1.5658,
      "step": 2835
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.8339256048202515,
      "learning_rate": 0.0003904849932210478,
      "loss": 1.5179,
      "step": 2836
    },
    {
      "epoch": 2.8369999999999997,
      "grad_norm": 0.45181089639663696,
      "learning_rate": 0.00038986175880683196,
      "loss": 1.5498,
      "step": 2837
    },
    {
      "epoch": 2.838,
      "grad_norm": 1.2590218782424927,
      "learning_rate": 0.00038923890169921603,
      "loss": 1.6673,
      "step": 2838
    },
    {
      "epoch": 2.839,
      "grad_norm": 0.46835455298423767,
      "learning_rate": 0.00038861642228337146,
      "loss": 1.512,
      "step": 2839
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.895458459854126,
      "learning_rate": 0.0003879943209442366,
      "loss": 1.6891,
      "step": 2840
    },
    {
      "epoch": 2.841,
      "grad_norm": 0.5595178008079529,
      "learning_rate": 0.00038737259806651583,
      "loss": 1.6297,
      "step": 2841
    },
    {
      "epoch": 2.842,
      "grad_norm": 0.7449676394462585,
      "learning_rate": 0.0003867512540346795,
      "loss": 1.5824,
      "step": 2842
    },
    {
      "epoch": 2.843,
      "grad_norm": 0.5147786140441895,
      "learning_rate": 0.00038613028923296434,
      "loss": 1.4978,
      "step": 2843
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.6108516454696655,
      "learning_rate": 0.00038550970404537145,
      "loss": 1.4821,
      "step": 2844
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 0.7840749621391296,
      "learning_rate": 0.0003848894988556685,
      "loss": 1.6776,
      "step": 2845
    },
    {
      "epoch": 2.846,
      "grad_norm": 0.569681704044342,
      "learning_rate": 0.000384269674047387,
      "loss": 1.5768,
      "step": 2846
    },
    {
      "epoch": 2.847,
      "grad_norm": 0.5856366753578186,
      "learning_rate": 0.0003836502300038236,
      "loss": 1.525,
      "step": 2847
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.5847684741020203,
      "learning_rate": 0.00038303116710803974,
      "loss": 1.6438,
      "step": 2848
    },
    {
      "epoch": 2.849,
      "grad_norm": 0.6121484637260437,
      "learning_rate": 0.0003824124857428606,
      "loss": 1.4691,
      "step": 2849
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.5858379602432251,
      "learning_rate": 0.00038179418629087647,
      "loss": 1.5683,
      "step": 2850
    },
    {
      "epoch": 2.851,
      "grad_norm": 1.4785236120224,
      "learning_rate": 0.00038117626913444015,
      "loss": 1.9647,
      "step": 2851
    },
    {
      "epoch": 2.852,
      "grad_norm": 1.7787692546844482,
      "learning_rate": 0.0003805587346556693,
      "loss": 1.5772,
      "step": 2852
    },
    {
      "epoch": 2.8529999999999998,
      "grad_norm": 1.4837801456451416,
      "learning_rate": 0.000379941583236444,
      "loss": 1.6188,
      "step": 2853
    },
    {
      "epoch": 2.854,
      "grad_norm": 0.8370680809020996,
      "learning_rate": 0.00037932481525840777,
      "loss": 1.5888,
      "step": 2854
    },
    {
      "epoch": 2.855,
      "grad_norm": 1.394730567932129,
      "learning_rate": 0.0003787084311029666,
      "loss": 1.627,
      "step": 2855
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.9589259624481201,
      "learning_rate": 0.00037809243115129035,
      "loss": 1.5981,
      "step": 2856
    },
    {
      "epoch": 2.857,
      "grad_norm": 1.1478406190872192,
      "learning_rate": 0.00037747681578431013,
      "loss": 1.7518,
      "step": 2857
    },
    {
      "epoch": 2.858,
      "grad_norm": 1.190576195716858,
      "learning_rate": 0.00037686158538271944,
      "loss": 1.5916,
      "step": 2858
    },
    {
      "epoch": 2.859,
      "grad_norm": 0.6624794006347656,
      "learning_rate": 0.00037624674032697393,
      "loss": 1.598,
      "step": 2859
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.453573226928711,
      "learning_rate": 0.0003756322809972905,
      "loss": 1.4959,
      "step": 2860
    },
    {
      "epoch": 2.8609999999999998,
      "grad_norm": 2.123037815093994,
      "learning_rate": 0.00037501820777364857,
      "loss": 1.8102,
      "step": 2861
    },
    {
      "epoch": 2.862,
      "grad_norm": 0.7914379239082336,
      "learning_rate": 0.0003744045210357875,
      "loss": 1.5225,
      "step": 2862
    },
    {
      "epoch": 2.863,
      "grad_norm": 1.1952251195907593,
      "learning_rate": 0.00037379122116320884,
      "loss": 1.5114,
      "step": 2863
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.7349616289138794,
      "learning_rate": 0.0003731783085351741,
      "loss": 1.6322,
      "step": 2864
    },
    {
      "epoch": 2.865,
      "grad_norm": 1.1883488893508911,
      "learning_rate": 0.0003725657835307055,
      "loss": 1.8056,
      "step": 2865
    },
    {
      "epoch": 2.866,
      "grad_norm": 1.3156940937042236,
      "learning_rate": 0.00037195364652858545,
      "loss": 1.5882,
      "step": 2866
    },
    {
      "epoch": 2.867,
      "grad_norm": 1.418825626373291,
      "learning_rate": 0.0003713418979073565,
      "loss": 1.5863,
      "step": 2867
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.7351441383361816,
      "learning_rate": 0.0003707305380453213,
      "loss": 1.599,
      "step": 2868
    },
    {
      "epoch": 2.8689999999999998,
      "grad_norm": 6.90179967880249,
      "learning_rate": 0.0003701195673205422,
      "loss": 1.5361,
      "step": 2869
    },
    {
      "epoch": 2.87,
      "grad_norm": 1.8705881834030151,
      "learning_rate": 0.0003695089861108403,
      "loss": 1.5986,
      "step": 2870
    },
    {
      "epoch": 2.871,
      "grad_norm": 1.4769121408462524,
      "learning_rate": 0.0003688987947937962,
      "loss": 1.5602,
      "step": 2871
    },
    {
      "epoch": 2.872,
      "grad_norm": 1.3280106782913208,
      "learning_rate": 0.0003682889937467493,
      "loss": 1.6082,
      "step": 2872
    },
    {
      "epoch": 2.873,
      "grad_norm": 0.6102670431137085,
      "learning_rate": 0.0003676795833467972,
      "loss": 1.5752,
      "step": 2873
    },
    {
      "epoch": 2.874,
      "grad_norm": 1.0686936378479004,
      "learning_rate": 0.0003670705639707972,
      "loss": 1.5169,
      "step": 2874
    },
    {
      "epoch": 2.875,
      "grad_norm": 1.101523756980896,
      "learning_rate": 0.0003664619359953637,
      "loss": 1.4614,
      "step": 2875
    },
    {
      "epoch": 2.876,
      "grad_norm": 1.277363657951355,
      "learning_rate": 0.00036585369979686924,
      "loss": 1.5455,
      "step": 2876
    },
    {
      "epoch": 2.877,
      "grad_norm": 1.8309026956558228,
      "learning_rate": 0.000365245855751444,
      "loss": 1.5318,
      "step": 2877
    },
    {
      "epoch": 2.878,
      "grad_norm": 0.8296202421188354,
      "learning_rate": 0.0003646384042349764,
      "loss": 1.4783,
      "step": 2878
    },
    {
      "epoch": 2.879,
      "grad_norm": 0.6103249192237854,
      "learning_rate": 0.00036403134562311103,
      "loss": 1.4308,
      "step": 2879
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.8501794934272766,
      "learning_rate": 0.00036342468029125063,
      "loss": 1.5313,
      "step": 2880
    },
    {
      "epoch": 2.8810000000000002,
      "grad_norm": 1.9123623371124268,
      "learning_rate": 0.00036281840861455397,
      "loss": 1.5109,
      "step": 2881
    },
    {
      "epoch": 2.882,
      "grad_norm": 1.666219711303711,
      "learning_rate": 0.0003622125309679364,
      "loss": 1.6715,
      "step": 2882
    },
    {
      "epoch": 2.883,
      "grad_norm": 1.5296931266784668,
      "learning_rate": 0.00036160704772607,
      "loss": 1.9179,
      "step": 2883
    },
    {
      "epoch": 2.884,
      "grad_norm": 1.1420249938964844,
      "learning_rate": 0.00036100195926338255,
      "loss": 1.5484,
      "step": 2884
    },
    {
      "epoch": 2.885,
      "grad_norm": 3.0658061504364014,
      "learning_rate": 0.00036039726595405754,
      "loss": 1.6015,
      "step": 2885
    },
    {
      "epoch": 2.886,
      "grad_norm": 2.191812753677368,
      "learning_rate": 0.000359792968172035,
      "loss": 1.5431,
      "step": 2886
    },
    {
      "epoch": 2.887,
      "grad_norm": 1.135345458984375,
      "learning_rate": 0.00035918906629101,
      "loss": 1.5395,
      "step": 2887
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.9689342975616455,
      "learning_rate": 0.0003585855606844323,
      "loss": 1.658,
      "step": 2888
    },
    {
      "epoch": 2.8890000000000002,
      "grad_norm": 0.9669023156166077,
      "learning_rate": 0.0003579824517255068,
      "loss": 1.4926,
      "step": 2889
    },
    {
      "epoch": 2.89,
      "grad_norm": 1.519744873046875,
      "learning_rate": 0.0003573797397871934,
      "loss": 1.5838,
      "step": 2890
    },
    {
      "epoch": 2.891,
      "grad_norm": 1.2058261632919312,
      "learning_rate": 0.0003567774252422059,
      "loss": 1.6151,
      "step": 2891
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.7934817671775818,
      "learning_rate": 0.00035617550846301347,
      "loss": 1.5104,
      "step": 2892
    },
    {
      "epoch": 2.893,
      "grad_norm": 1.096342921257019,
      "learning_rate": 0.0003555739898218382,
      "loss": 1.6613,
      "step": 2893
    },
    {
      "epoch": 2.894,
      "grad_norm": 0.605451762676239,
      "learning_rate": 0.00035497286969065646,
      "loss": 1.4624,
      "step": 2894
    },
    {
      "epoch": 2.895,
      "grad_norm": 0.9115038514137268,
      "learning_rate": 0.0003543721484411976,
      "loss": 1.5306,
      "step": 2895
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.6945870518684387,
      "learning_rate": 0.00035377182644494564,
      "loss": 1.507,
      "step": 2896
    },
    {
      "epoch": 2.8970000000000002,
      "grad_norm": 1.5349260568618774,
      "learning_rate": 0.00035317190407313607,
      "loss": 1.6364,
      "step": 2897
    },
    {
      "epoch": 2.898,
      "grad_norm": 2.6412479877471924,
      "learning_rate": 0.0003525723816967588,
      "loss": 1.5903,
      "step": 2898
    },
    {
      "epoch": 2.899,
      "grad_norm": 1.3042734861373901,
      "learning_rate": 0.00035197325968655514,
      "loss": 1.4592,
      "step": 2899
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.4885167181491852,
      "learning_rate": 0.00035137453841301946,
      "loss": 1.4866,
      "step": 2900
    },
    {
      "epoch": 2.9,
      "eval_loss": 1.5979739427566528,
      "eval_runtime": 211.2403,
      "eval_samples_per_second": 0.473,
      "eval_steps_per_second": 0.473,
      "step": 2900
    },
    {
      "epoch": 2.901,
      "grad_norm": 1.2780747413635254,
      "learning_rate": 0.0003507762182463982,
      "loss": 1.5053,
      "step": 2901
    },
    {
      "epoch": 2.902,
      "grad_norm": 0.9620892405509949,
      "learning_rate": 0.00035017829955668944,
      "loss": 1.7234,
      "step": 2902
    },
    {
      "epoch": 2.903,
      "grad_norm": 0.9658979177474976,
      "learning_rate": 0.00034958078271364325,
      "loss": 1.5456,
      "step": 2903
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.9280832409858704,
      "learning_rate": 0.0003489836680867614,
      "loss": 1.5568,
      "step": 2904
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 0.9227556586265564,
      "learning_rate": 0.00034838695604529714,
      "loss": 1.4992,
      "step": 2905
    },
    {
      "epoch": 2.906,
      "grad_norm": 0.660529613494873,
      "learning_rate": 0.000347790646958254,
      "loss": 1.345,
      "step": 2906
    },
    {
      "epoch": 2.907,
      "grad_norm": 1.2550578117370605,
      "learning_rate": 0.0003471947411943867,
      "loss": 1.7409,
      "step": 2907
    },
    {
      "epoch": 2.908,
      "grad_norm": 3.848334312438965,
      "learning_rate": 0.0003465992391222006,
      "loss": 1.4789,
      "step": 2908
    },
    {
      "epoch": 2.909,
      "grad_norm": 2.2947535514831543,
      "learning_rate": 0.00034600414110995083,
      "loss": 1.5149,
      "step": 2909
    },
    {
      "epoch": 2.91,
      "grad_norm": 1.4690501689910889,
      "learning_rate": 0.00034540944752564406,
      "loss": 1.5252,
      "step": 2910
    },
    {
      "epoch": 2.911,
      "grad_norm": 0.81815105676651,
      "learning_rate": 0.0003448151587370356,
      "loss": 1.5654,
      "step": 2911
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.8675405383110046,
      "learning_rate": 0.0003442212751116305,
      "loss": 1.5954,
      "step": 2912
    },
    {
      "epoch": 2.9130000000000003,
      "grad_norm": 1.1869679689407349,
      "learning_rate": 0.0003436277970166841,
      "loss": 1.5341,
      "step": 2913
    },
    {
      "epoch": 2.914,
      "grad_norm": 1.0615267753601074,
      "learning_rate": 0.0003430347248192004,
      "loss": 1.4937,
      "step": 2914
    },
    {
      "epoch": 2.915,
      "grad_norm": 0.6119611859321594,
      "learning_rate": 0.0003424420588859318,
      "loss": 1.5615,
      "step": 2915
    },
    {
      "epoch": 2.916,
      "grad_norm": 1.8721697330474854,
      "learning_rate": 0.00034184979958338103,
      "loss": 1.8876,
      "step": 2916
    },
    {
      "epoch": 2.917,
      "grad_norm": 1.346787929534912,
      "learning_rate": 0.0003412579472777979,
      "loss": 1.495,
      "step": 2917
    },
    {
      "epoch": 2.918,
      "grad_norm": 0.5047949552536011,
      "learning_rate": 0.00034066650233518126,
      "loss": 1.5253,
      "step": 2918
    },
    {
      "epoch": 2.919,
      "grad_norm": 0.8430173993110657,
      "learning_rate": 0.0003400754651212776,
      "loss": 1.5388,
      "step": 2919
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.8219758868217468,
      "learning_rate": 0.0003394848360015815,
      "loss": 1.6827,
      "step": 2920
    },
    {
      "epoch": 2.9210000000000003,
      "grad_norm": 0.604712188243866,
      "learning_rate": 0.0003388946153413357,
      "loss": 1.5665,
      "step": 2921
    },
    {
      "epoch": 2.922,
      "grad_norm": 1.3831433057785034,
      "learning_rate": 0.0003383048035055293,
      "loss": 1.8223,
      "step": 2922
    },
    {
      "epoch": 2.923,
      "grad_norm": 0.42421481013298035,
      "learning_rate": 0.0003377154008588997,
      "loss": 1.5776,
      "step": 2923
    },
    {
      "epoch": 2.924,
      "grad_norm": 1.2358485460281372,
      "learning_rate": 0.0003371264077659304,
      "loss": 1.4881,
      "step": 2924
    },
    {
      "epoch": 2.925,
      "grad_norm": 1.0837093591690063,
      "learning_rate": 0.000336537824590852,
      "loss": 1.7042,
      "step": 2925
    },
    {
      "epoch": 2.926,
      "grad_norm": 0.6113150119781494,
      "learning_rate": 0.0003359496516976415,
      "loss": 1.6413,
      "step": 2926
    },
    {
      "epoch": 2.927,
      "grad_norm": 0.7063218951225281,
      "learning_rate": 0.0003353618894500219,
      "loss": 1.5107,
      "step": 2927
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.6724994778633118,
      "learning_rate": 0.00033477453821146333,
      "loss": 1.5294,
      "step": 2928
    },
    {
      "epoch": 2.9290000000000003,
      "grad_norm": 0.6678494811058044,
      "learning_rate": 0.00033418759834518054,
      "loss": 1.5735,
      "step": 2929
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.5903039574623108,
      "learning_rate": 0.0003336010702141341,
      "loss": 1.6847,
      "step": 2930
    },
    {
      "epoch": 2.931,
      "grad_norm": 0.571325957775116,
      "learning_rate": 0.00033301495418103103,
      "loss": 1.5657,
      "step": 2931
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.5779903531074524,
      "learning_rate": 0.0003324292506083221,
      "loss": 1.4431,
      "step": 2932
    },
    {
      "epoch": 2.933,
      "grad_norm": 1.0762821435928345,
      "learning_rate": 0.00033184395985820346,
      "loss": 1.5531,
      "step": 2933
    },
    {
      "epoch": 2.934,
      "grad_norm": 0.49560052156448364,
      "learning_rate": 0.00033125908229261693,
      "loss": 1.6354,
      "step": 2934
    },
    {
      "epoch": 2.935,
      "grad_norm": 1.7070142030715942,
      "learning_rate": 0.0003306746182732475,
      "loss": 1.7085,
      "step": 2935
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.3329634964466095,
      "learning_rate": 0.00033009056816152494,
      "loss": 1.4349,
      "step": 2936
    },
    {
      "epoch": 2.9370000000000003,
      "grad_norm": 1.2730454206466675,
      "learning_rate": 0.00032950693231862315,
      "loss": 1.5535,
      "step": 2937
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 0.7214822769165039,
      "learning_rate": 0.00032892371110545946,
      "loss": 1.453,
      "step": 2938
    },
    {
      "epoch": 2.939,
      "grad_norm": 1.4641995429992676,
      "learning_rate": 0.0003283409048826955,
      "loss": 1.7043,
      "step": 2939
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.4108821153640747,
      "learning_rate": 0.000327758514010736,
      "loss": 1.5035,
      "step": 2940
    },
    {
      "epoch": 2.941,
      "grad_norm": 0.8256182074546814,
      "learning_rate": 0.00032717653884972855,
      "loss": 1.5985,
      "step": 2941
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.7596215605735779,
      "learning_rate": 0.0003265949797595638,
      "loss": 1.4886,
      "step": 2942
    },
    {
      "epoch": 2.943,
      "grad_norm": 0.8877671360969543,
      "learning_rate": 0.0003260138370998751,
      "loss": 1.5217,
      "step": 2943
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.5965343117713928,
      "learning_rate": 0.0003254331112300384,
      "loss": 1.7081,
      "step": 2944
    },
    {
      "epoch": 2.945,
      "grad_norm": 0.5857459902763367,
      "learning_rate": 0.00032485280250917135,
      "loss": 1.5304,
      "step": 2945
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 0.8251914978027344,
      "learning_rate": 0.00032427291129613503,
      "loss": 1.6331,
      "step": 2946
    },
    {
      "epoch": 2.947,
      "grad_norm": 1.0627415180206299,
      "learning_rate": 0.00032369343794953043,
      "loss": 1.6998,
      "step": 2947
    },
    {
      "epoch": 2.948,
      "grad_norm": 1.350408911705017,
      "learning_rate": 0.0003231143828277021,
      "loss": 1.74,
      "step": 2948
    },
    {
      "epoch": 2.949,
      "grad_norm": 0.48449110984802246,
      "learning_rate": 0.0003225357462887344,
      "loss": 1.414,
      "step": 2949
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.6749898791313171,
      "learning_rate": 0.0003219575286904536,
      "loss": 1.5895,
      "step": 2950
    },
    {
      "epoch": 2.951,
      "grad_norm": 0.6302073001861572,
      "learning_rate": 0.00032137973039042636,
      "loss": 1.5376,
      "step": 2951
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.6557563543319702,
      "learning_rate": 0.00032080235174596094,
      "loss": 1.6034,
      "step": 2952
    },
    {
      "epoch": 2.953,
      "grad_norm": 0.7957144975662231,
      "learning_rate": 0.0003202253931141054,
      "loss": 1.6218,
      "step": 2953
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 0.8528870344161987,
      "learning_rate": 0.0003196488548516482,
      "loss": 1.479,
      "step": 2954
    },
    {
      "epoch": 2.955,
      "grad_norm": 0.9668441414833069,
      "learning_rate": 0.00031907273731511776,
      "loss": 1.5046,
      "step": 2955
    },
    {
      "epoch": 2.956,
      "grad_norm": 2.0891828536987305,
      "learning_rate": 0.00031849704086078226,
      "loss": 1.6615,
      "step": 2956
    },
    {
      "epoch": 2.957,
      "grad_norm": 1.8401247262954712,
      "learning_rate": 0.00031792176584464983,
      "loss": 1.4577,
      "step": 2957
    },
    {
      "epoch": 2.958,
      "grad_norm": 0.7530356049537659,
      "learning_rate": 0.0003173469126224682,
      "loss": 1.5539,
      "step": 2958
    },
    {
      "epoch": 2.959,
      "grad_norm": 4.1690673828125,
      "learning_rate": 0.0003167724815497237,
      "loss": 1.5119,
      "step": 2959
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.92154860496521,
      "learning_rate": 0.0003161984729816415,
      "loss": 1.6606,
      "step": 2960
    },
    {
      "epoch": 2.961,
      "grad_norm": 1.3151984214782715,
      "learning_rate": 0.00031562488727318605,
      "loss": 1.523,
      "step": 2961
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 3.4077985286712646,
      "learning_rate": 0.00031505172477905996,
      "loss": 1.6327,
      "step": 2962
    },
    {
      "epoch": 2.963,
      "grad_norm": 5.825990676879883,
      "learning_rate": 0.00031447898585370383,
      "loss": 2.0093,
      "step": 2963
    },
    {
      "epoch": 2.964,
      "grad_norm": 3.095324754714966,
      "learning_rate": 0.00031390667085129734,
      "loss": 1.5998,
      "step": 2964
    },
    {
      "epoch": 2.965,
      "grad_norm": 1.7271641492843628,
      "learning_rate": 0.00031333478012575677,
      "loss": 1.7738,
      "step": 2965
    },
    {
      "epoch": 2.966,
      "grad_norm": 2.8710803985595703,
      "learning_rate": 0.0003127633140307373,
      "loss": 1.6335,
      "step": 2966
    },
    {
      "epoch": 2.967,
      "grad_norm": 9.702322959899902,
      "learning_rate": 0.0003121922729196305,
      "loss": 1.5997,
      "step": 2967
    },
    {
      "epoch": 2.968,
      "grad_norm": 4.935318946838379,
      "learning_rate": 0.00031162165714556544,
      "loss": 1.6459,
      "step": 2968
    },
    {
      "epoch": 2.969,
      "grad_norm": 0.5957253575325012,
      "learning_rate": 0.00031105146706140796,
      "loss": 1.475,
      "step": 2969
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 1.2542451620101929,
      "learning_rate": 0.00031048170301976153,
      "loss": 1.4548,
      "step": 2970
    },
    {
      "epoch": 2.971,
      "grad_norm": 1.1514320373535156,
      "learning_rate": 0.0003099123653729653,
      "loss": 1.5921,
      "step": 2971
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.7890778183937073,
      "learning_rate": 0.00030934345447309485,
      "loss": 1.4933,
      "step": 2972
    },
    {
      "epoch": 2.973,
      "grad_norm": 1.0507763624191284,
      "learning_rate": 0.0003087749706719617,
      "loss": 1.5785,
      "step": 2973
    },
    {
      "epoch": 2.974,
      "grad_norm": 0.5347141027450562,
      "learning_rate": 0.00030820691432111426,
      "loss": 1.4713,
      "step": 2974
    },
    {
      "epoch": 2.975,
      "grad_norm": 0.8415813446044922,
      "learning_rate": 0.0003076392857718351,
      "loss": 1.4404,
      "step": 2975
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.8720101118087769,
      "learning_rate": 0.0003070720853751437,
      "loss": 1.6258,
      "step": 2976
    },
    {
      "epoch": 2.977,
      "grad_norm": 1.9982597827911377,
      "learning_rate": 0.0003065053134817939,
      "loss": 1.6375,
      "step": 2977
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 1.5704516172409058,
      "learning_rate": 0.0003059389704422747,
      "loss": 1.8027,
      "step": 2978
    },
    {
      "epoch": 2.979,
      "grad_norm": 0.9486679434776306,
      "learning_rate": 0.0003053730566068099,
      "loss": 1.5394,
      "step": 2979
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.8147966265678406,
      "learning_rate": 0.0003048075723253577,
      "loss": 1.5558,
      "step": 2980
    },
    {
      "epoch": 2.981,
      "grad_norm": 0.7358405590057373,
      "learning_rate": 0.0003042425179476115,
      "loss": 1.5864,
      "step": 2981
    },
    {
      "epoch": 2.982,
      "grad_norm": 0.7020919919013977,
      "learning_rate": 0.0003036778938229976,
      "loss": 1.5713,
      "step": 2982
    },
    {
      "epoch": 2.983,
      "grad_norm": 1.1224602460861206,
      "learning_rate": 0.0003031137003006775,
      "loss": 1.627,
      "step": 2983
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.949592649936676,
      "learning_rate": 0.00030254993772954554,
      "loss": 1.4641,
      "step": 2984
    },
    {
      "epoch": 2.985,
      "grad_norm": 1.1154228448867798,
      "learning_rate": 0.00030198660645822985,
      "loss": 1.6223,
      "step": 2985
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 0.9864811301231384,
      "learning_rate": 0.0003014237068350917,
      "loss": 1.5259,
      "step": 2986
    },
    {
      "epoch": 2.987,
      "grad_norm": 0.5933407545089722,
      "learning_rate": 0.00030086123920822525,
      "loss": 1.5339,
      "step": 2987
    },
    {
      "epoch": 2.988,
      "grad_norm": 1.4068275690078735,
      "learning_rate": 0.0003002992039254586,
      "loss": 1.5442,
      "step": 2988
    },
    {
      "epoch": 2.989,
      "grad_norm": 1.1263799667358398,
      "learning_rate": 0.00029973760133435135,
      "loss": 1.666,
      "step": 2989
    },
    {
      "epoch": 2.99,
      "grad_norm": 1.1282408237457275,
      "learning_rate": 0.0002991764317821958,
      "loss": 1.7058,
      "step": 2990
    },
    {
      "epoch": 2.991,
      "grad_norm": 0.612671971321106,
      "learning_rate": 0.0002986156956160162,
      "loss": 1.4001,
      "step": 2991
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.7594670057296753,
      "learning_rate": 0.0002980553931825698,
      "loss": 1.5032,
      "step": 2992
    },
    {
      "epoch": 2.993,
      "grad_norm": 0.7501254081726074,
      "learning_rate": 0.0002974955248283445,
      "loss": 1.4884,
      "step": 2993
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 0.9573844075202942,
      "learning_rate": 0.00029693609089956064,
      "loss": 1.6049,
      "step": 2994
    },
    {
      "epoch": 2.995,
      "grad_norm": 0.9519614577293396,
      "learning_rate": 0.00029637709174216943,
      "loss": 1.5917,
      "step": 2995
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.5815922021865845,
      "learning_rate": 0.000295818527701853,
      "loss": 1.5946,
      "step": 2996
    },
    {
      "epoch": 2.997,
      "grad_norm": 1.070947289466858,
      "learning_rate": 0.00029526039912402505,
      "loss": 1.5041,
      "step": 2997
    },
    {
      "epoch": 2.998,
      "grad_norm": 0.6692641377449036,
      "learning_rate": 0.000294702706353829,
      "loss": 1.5107,
      "step": 2998
    },
    {
      "epoch": 2.999,
      "grad_norm": 1.021701693534851,
      "learning_rate": 0.0002941454497361403,
      "loss": 1.5588,
      "step": 2999
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.545109212398529,
      "learning_rate": 0.0002935886296155631,
      "loss": 1.5041,
      "step": 3000
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.5527236461639404,
      "eval_runtime": 211.1501,
      "eval_samples_per_second": 0.474,
      "eval_steps_per_second": 0.474,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 4000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.610822464687636e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
