{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.0956664085388184,
      "learning_rate": 4e-05,
      "loss": 1.9448,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.2918241024017334,
      "learning_rate": 8e-05,
      "loss": 1.8955,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.376105308532715,
      "learning_rate": 0.00012,
      "loss": 1.9635,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.942995309829712,
      "learning_rate": 0.00016,
      "loss": 1.7703,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4782493114471436,
      "learning_rate": 0.0002,
      "loss": 1.558,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.251857042312622,
      "learning_rate": 0.00019993322203672788,
      "loss": 1.3528,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.6353691816329956,
      "learning_rate": 0.00019986644407345576,
      "loss": 1.0731,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.6602898836135864,
      "learning_rate": 0.00019979966611018366,
      "loss": 1.2105,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.6666288375854492,
      "learning_rate": 0.00019973288814691153,
      "loss": 0.8511,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 2.097238302230835,
      "learning_rate": 0.0001996661101836394,
      "loss": 0.9601,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.886622428894043,
      "learning_rate": 0.00019959933222036728,
      "loss": 0.6888,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8481279015541077,
      "learning_rate": 0.00019953255425709515,
      "loss": 0.4869,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.9076886177062988,
      "learning_rate": 0.00019946577629382305,
      "loss": 0.5155,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.4742388427257538,
      "learning_rate": 0.00019939899833055092,
      "loss": 0.6427,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.42061689496040344,
      "learning_rate": 0.00019933222036727882,
      "loss": 0.4551,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.49660366773605347,
      "learning_rate": 0.0001992654424040067,
      "loss": 0.6552,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.29572218656539917,
      "learning_rate": 0.00019919866444073457,
      "loss": 0.46,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.20609429478645325,
      "learning_rate": 0.00019913188647746244,
      "loss": 0.4393,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.2833709120750427,
      "learning_rate": 0.00019906510851419034,
      "loss": 0.4794,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3383139669895172,
      "learning_rate": 0.00019899833055091822,
      "loss": 0.5512,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2814450263977051,
      "learning_rate": 0.0001989315525876461,
      "loss": 0.5206,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.18887953460216522,
      "learning_rate": 0.00019886477462437396,
      "loss": 0.4972,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.1577294021844864,
      "learning_rate": 0.00019879799666110183,
      "loss": 0.3929,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.16395266354084015,
      "learning_rate": 0.00019873121869782974,
      "loss": 0.4311,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.19629164040088654,
      "learning_rate": 0.0001986644407345576,
      "loss": 0.4407,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.19796745479106903,
      "learning_rate": 0.00019859766277128548,
      "loss": 0.5199,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.15026581287384033,
      "learning_rate": 0.00019853088480801335,
      "loss": 0.444,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.15527544915676117,
      "learning_rate": 0.00019846410684474123,
      "loss": 0.47,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.12846054136753082,
      "learning_rate": 0.00019839732888146913,
      "loss": 0.3947,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15415337681770325,
      "learning_rate": 0.000198330550918197,
      "loss": 0.4066,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.16547812521457672,
      "learning_rate": 0.00019826377295492487,
      "loss": 0.4745,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10649402439594269,
      "learning_rate": 0.00019819699499165277,
      "loss": 0.4897,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.1004398912191391,
      "learning_rate": 0.00019813021702838065,
      "loss": 0.2804,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.1579684317111969,
      "learning_rate": 0.00019806343906510852,
      "loss": 0.4162,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.14333078265190125,
      "learning_rate": 0.00019799666110183642,
      "loss": 0.3944,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11239796131849289,
      "learning_rate": 0.0001979298831385643,
      "loss": 0.4197,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.11745484918355942,
      "learning_rate": 0.00019786310517529217,
      "loss": 0.3054,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.11401297152042389,
      "learning_rate": 0.00019779632721202004,
      "loss": 0.3476,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.11132606118917465,
      "learning_rate": 0.00019772954924874791,
      "loss": 0.3451,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.10694670677185059,
      "learning_rate": 0.00019766277128547581,
      "loss": 0.325,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.10581845790147781,
      "learning_rate": 0.0001975959933222037,
      "loss": 0.3798,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.09086530655622482,
      "learning_rate": 0.00019752921535893156,
      "loss": 0.3476,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.10241302847862244,
      "learning_rate": 0.00019746243739565943,
      "loss": 0.3672,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09187440574169159,
      "learning_rate": 0.0001973956594323873,
      "loss": 0.4196,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12214738875627518,
      "learning_rate": 0.0001973288814691152,
      "loss": 0.4203,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.11255165189504623,
      "learning_rate": 0.00019726210350584308,
      "loss": 0.3168,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.09649581462144852,
      "learning_rate": 0.00019719532554257095,
      "loss": 0.3994,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08454272150993347,
      "learning_rate": 0.00019712854757929883,
      "loss": 0.2907,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.09278556704521179,
      "learning_rate": 0.00019706176961602673,
      "loss": 0.3851,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.10700076818466187,
      "learning_rate": 0.0001969949916527546,
      "loss": 0.3579,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.08786964416503906,
      "learning_rate": 0.0001969282136894825,
      "loss": 0.3716,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.09527817368507385,
      "learning_rate": 0.00019686143572621037,
      "loss": 0.4149,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.06768197566270828,
      "learning_rate": 0.00019679465776293825,
      "loss": 0.3567,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.11477317661046982,
      "learning_rate": 0.00019672787979966612,
      "loss": 0.4293,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.084235779941082,
      "learning_rate": 0.000196661101836394,
      "loss": 0.3019,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.10784310102462769,
      "learning_rate": 0.0001965943238731219,
      "loss": 0.3491,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.10310035198926926,
      "learning_rate": 0.00019652754590984977,
      "loss": 0.3831,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.1375250220298767,
      "learning_rate": 0.00019646076794657764,
      "loss": 0.4089,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.07553678750991821,
      "learning_rate": 0.0001963939899833055,
      "loss": 0.3023,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0941384807229042,
      "learning_rate": 0.00019632721202003339,
      "loss": 0.3806,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.06856777518987656,
      "learning_rate": 0.00019626043405676129,
      "loss": 0.2628,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.07682006061077118,
      "learning_rate": 0.00019619365609348916,
      "loss": 0.3936,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06746046990156174,
      "learning_rate": 0.00019612687813021703,
      "loss": 0.3716,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07772789150476456,
      "learning_rate": 0.0001960601001669449,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.08953963220119476,
      "learning_rate": 0.00019599332220367278,
      "loss": 0.3442,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.0995715856552124,
      "learning_rate": 0.00019592654424040068,
      "loss": 0.3373,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.07466694712638855,
      "learning_rate": 0.00019585976627712855,
      "loss": 0.3045,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.08509878069162369,
      "learning_rate": 0.00019579298831385645,
      "loss": 0.2943,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.12085497379302979,
      "learning_rate": 0.00019572621035058433,
      "loss": 0.3952,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.10573051869869232,
      "learning_rate": 0.0001956594323873122,
      "loss": 0.3791,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.09017354995012283,
      "learning_rate": 0.00019559265442404007,
      "loss": 0.3193,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0738191232085228,
      "learning_rate": 0.00019552587646076797,
      "loss": 0.3396,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.06006060913205147,
      "learning_rate": 0.00019545909849749584,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.15513241291046143,
      "learning_rate": 0.00019539232053422372,
      "loss": 0.3864,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.07705923914909363,
      "learning_rate": 0.0001953255425709516,
      "loss": 0.3251,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17013779282569885,
      "learning_rate": 0.00019525876460767946,
      "loss": 0.3971,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.08929529041051865,
      "learning_rate": 0.00019519198664440736,
      "loss": 0.3465,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.08618784695863724,
      "learning_rate": 0.00019512520868113524,
      "loss": 0.3251,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.11940111219882965,
      "learning_rate": 0.0001950584307178631,
      "loss": 0.386,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08647278696298599,
      "learning_rate": 0.00019499165275459098,
      "loss": 0.3328,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.08629181981086731,
      "learning_rate": 0.00019492487479131886,
      "loss": 0.3497,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.09361159801483154,
      "learning_rate": 0.00019485809682804673,
      "loss": 0.3479,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.06511078774929047,
      "learning_rate": 0.00019479131886477463,
      "loss": 0.262,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0810856968164444,
      "learning_rate": 0.0001947245409015025,
      "loss": 0.375,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.06852146983146667,
      "learning_rate": 0.0001946577629382304,
      "loss": 0.3839,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.08113353699445724,
      "learning_rate": 0.00019459098497495828,
      "loss": 0.3473,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.07892511785030365,
      "learning_rate": 0.00019452420701168615,
      "loss": 0.2601,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08675333112478256,
      "learning_rate": 0.00019445742904841405,
      "loss": 0.3368,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.07467499375343323,
      "learning_rate": 0.00019439065108514192,
      "loss": 0.3206,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06793464720249176,
      "learning_rate": 0.0001943238731218698,
      "loss": 0.2437,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.16194306313991547,
      "learning_rate": 0.00019425709515859767,
      "loss": 0.3755,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08678337186574936,
      "learning_rate": 0.00019419031719532554,
      "loss": 0.364,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.09742604196071625,
      "learning_rate": 0.00019412353923205344,
      "loss": 0.336,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.12137395888566971,
      "learning_rate": 0.00019405676126878132,
      "loss": 0.3643,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.08253426104784012,
      "learning_rate": 0.0001939899833055092,
      "loss": 0.3209,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.07981440424919128,
      "learning_rate": 0.00019392320534223706,
      "loss": 0.3538,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.10532163828611374,
      "learning_rate": 0.00019385642737896494,
      "loss": 0.3208,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.06535062938928604,
      "learning_rate": 0.0001937896494156928,
      "loss": 0.3372,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.05981944501399994,
      "learning_rate": 0.0001937228714524207,
      "loss": 0.2412,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.09706909954547882,
      "learning_rate": 0.00019365609348914858,
      "loss": 0.4219,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.06982459872961044,
      "learning_rate": 0.00019358931552587646,
      "loss": 0.2833,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.08719934523105621,
      "learning_rate": 0.00019352253756260436,
      "loss": 0.353,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.08656898885965347,
      "learning_rate": 0.00019345575959933223,
      "loss": 0.346,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.06393106281757355,
      "learning_rate": 0.00019338898163606013,
      "loss": 0.2793,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.11401592195034027,
      "learning_rate": 0.000193322203672788,
      "loss": 0.4255,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.08084436506032944,
      "learning_rate": 0.00019325542570951588,
      "loss": 0.3427,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.07798241823911667,
      "learning_rate": 0.00019318864774624375,
      "loss": 0.4689,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.08108892291784286,
      "learning_rate": 0.00019312186978297162,
      "loss": 0.402,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.11977798491716385,
      "learning_rate": 0.00019305509181969952,
      "loss": 0.3084,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.058830082416534424,
      "learning_rate": 0.0001929883138564274,
      "loss": 0.3936,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.07111568003892899,
      "learning_rate": 0.00019292153589315527,
      "loss": 0.3664,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06430325657129288,
      "learning_rate": 0.00019285475792988314,
      "loss": 0.3048,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.06825023144483566,
      "learning_rate": 0.00019278797996661101,
      "loss": 0.3785,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.12661273777484894,
      "learning_rate": 0.0001927212020033389,
      "loss": 0.4042,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08384962379932404,
      "learning_rate": 0.0001926544240400668,
      "loss": 0.3674,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.12418660521507263,
      "learning_rate": 0.00019258764607679466,
      "loss": 0.3506,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.061764538288116455,
      "learning_rate": 0.00019252086811352253,
      "loss": 0.305,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.06392883509397507,
      "learning_rate": 0.0001924540901502504,
      "loss": 0.3497,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.09458301216363907,
      "learning_rate": 0.0001923873121869783,
      "loss": 0.3263,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05542557314038277,
      "learning_rate": 0.00019232053422370618,
      "loss": 0.2906,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.0728297159075737,
      "learning_rate": 0.00019225375626043408,
      "loss": 0.3746,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06852685660123825,
      "learning_rate": 0.00019218697829716195,
      "loss": 0.34,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05804547667503357,
      "learning_rate": 0.00019212020033388983,
      "loss": 0.2878,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06328660249710083,
      "learning_rate": 0.0001920534223706177,
      "loss": 0.3435,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.07714178413152695,
      "learning_rate": 0.0001919866444073456,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.07320096343755722,
      "learning_rate": 0.00019191986644407347,
      "loss": 0.3508,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.07564099133014679,
      "learning_rate": 0.00019185308848080135,
      "loss": 0.3074,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06308815628290176,
      "learning_rate": 0.00019178631051752922,
      "loss": 0.2524,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.07419486343860626,
      "learning_rate": 0.0001917195325542571,
      "loss": 0.3869,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.09122984856367111,
      "learning_rate": 0.00019165275459098497,
      "loss": 0.4091,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09594853967428207,
      "learning_rate": 0.00019158597662771287,
      "loss": 0.4443,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07332075387239456,
      "learning_rate": 0.00019151919866444074,
      "loss": 0.351,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.05169367790222168,
      "learning_rate": 0.0001914524207011686,
      "loss": 0.2951,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05943819880485535,
      "learning_rate": 0.0001913856427378965,
      "loss": 0.3033,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.0833563581109047,
      "learning_rate": 0.0001913188647746244,
      "loss": 0.3152,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05343350023031235,
      "learning_rate": 0.00019125208681135226,
      "loss": 0.2989,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.0843648836016655,
      "learning_rate": 0.00019118530884808016,
      "loss": 0.354,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.056954026222229004,
      "learning_rate": 0.00019111853088480803,
      "loss": 0.3748,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.07683058083057404,
      "learning_rate": 0.0001910517529215359,
      "loss": 0.3557,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.08052007853984833,
      "learning_rate": 0.00019098497495826378,
      "loss": 0.2724,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.07334864139556885,
      "learning_rate": 0.00019091819699499168,
      "loss": 0.3755,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.05416643247008324,
      "learning_rate": 0.00019085141903171955,
      "loss": 0.2737,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.0647912248969078,
      "learning_rate": 0.00019078464106844743,
      "loss": 0.3337,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.08493472635746002,
      "learning_rate": 0.0001907178631051753,
      "loss": 0.3992,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.06978499889373779,
      "learning_rate": 0.00019065108514190317,
      "loss": 0.3452,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.08354634791612625,
      "learning_rate": 0.00019058430717863107,
      "loss": 0.3801,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.08108527958393097,
      "learning_rate": 0.00019051752921535895,
      "loss": 0.3377,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06336899101734161,
      "learning_rate": 0.00019045075125208682,
      "loss": 0.3086,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.09381486475467682,
      "learning_rate": 0.0001903839732888147,
      "loss": 0.3614,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.0781172513961792,
      "learning_rate": 0.00019031719532554257,
      "loss": 0.3664,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.07172472029924393,
      "learning_rate": 0.00019025041736227044,
      "loss": 0.3111,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06654836237430573,
      "learning_rate": 0.00019018363939899834,
      "loss": 0.2931,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09867861866950989,
      "learning_rate": 0.0001901168614357262,
      "loss": 0.4418,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08223884552717209,
      "learning_rate": 0.0001900500834724541,
      "loss": 0.3609,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.0780235156416893,
      "learning_rate": 0.00018998330550918199,
      "loss": 0.3251,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06921236962080002,
      "learning_rate": 0.00018991652754590986,
      "loss": 0.3675,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.06987825036048889,
      "learning_rate": 0.00018984974958263776,
      "loss": 0.296,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.05670154094696045,
      "learning_rate": 0.00018978297161936563,
      "loss": 0.3226,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05686641484498978,
      "learning_rate": 0.0001897161936560935,
      "loss": 0.2558,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.08335665613412857,
      "learning_rate": 0.00018964941569282138,
      "loss": 0.3413,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.061617542058229446,
      "learning_rate": 0.00018958263772954925,
      "loss": 0.3585,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.07397318631410599,
      "learning_rate": 0.00018951585976627715,
      "loss": 0.2825,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.07524124532938004,
      "learning_rate": 0.00018944908180300502,
      "loss": 0.2172,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.09329230338335037,
      "learning_rate": 0.0001893823038397329,
      "loss": 0.365,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.0736825168132782,
      "learning_rate": 0.00018931552587646077,
      "loss": 0.3335,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.06876420229673386,
      "learning_rate": 0.00018924874791318864,
      "loss": 0.3162,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.06728898733854294,
      "learning_rate": 0.00018918196994991652,
      "loss": 0.3171,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06478077173233032,
      "learning_rate": 0.00018911519198664442,
      "loss": 0.2978,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.06718949228525162,
      "learning_rate": 0.0001890484140233723,
      "loss": 0.2701,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.06102819740772247,
      "learning_rate": 0.00018898163606010016,
      "loss": 0.3441,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.0668511912226677,
      "learning_rate": 0.00018891485809682806,
      "loss": 0.3145,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05413564294576645,
      "learning_rate": 0.00018884808013355594,
      "loss": 0.2821,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.051658570766448975,
      "learning_rate": 0.00018878130217028384,
      "loss": 0.2528,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.08169940859079361,
      "learning_rate": 0.0001887145242070117,
      "loss": 0.3672,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.08725954592227936,
      "learning_rate": 0.00018864774624373958,
      "loss": 0.3994,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.06593123078346252,
      "learning_rate": 0.00018858096828046746,
      "loss": 0.281,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.06933767348527908,
      "learning_rate": 0.00018851419031719533,
      "loss": 0.2368,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.10475962609052658,
      "learning_rate": 0.00018844741235392323,
      "loss": 0.3811,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.07346801459789276,
      "learning_rate": 0.0001883806343906511,
      "loss": 0.3234,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10945428907871246,
      "learning_rate": 0.00018831385642737898,
      "loss": 0.4434,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.0850963145494461,
      "learning_rate": 0.00018824707846410685,
      "loss": 0.3818,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06807483732700348,
      "learning_rate": 0.00018818030050083472,
      "loss": 0.3981,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.07143398374319077,
      "learning_rate": 0.0001881135225375626,
      "loss": 0.2976,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.060877878218889236,
      "learning_rate": 0.0001880467445742905,
      "loss": 0.3016,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.05298999696969986,
      "learning_rate": 0.00018797996661101837,
      "loss": 0.242,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.07313723862171173,
      "learning_rate": 0.00018791318864774624,
      "loss": 0.2949,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.06976813077926636,
      "learning_rate": 0.00018784641068447412,
      "loss": 0.3056,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.06937415897846222,
      "learning_rate": 0.00018777963272120202,
      "loss": 0.3172,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.07303337752819061,
      "learning_rate": 0.0001877128547579299,
      "loss": 0.3322,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.06589815765619278,
      "learning_rate": 0.0001876460767946578,
      "loss": 0.2966,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.041662562638521194,
      "learning_rate": 0.00018757929883138566,
      "loss": 0.2511,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05949961021542549,
      "learning_rate": 0.00018751252086811354,
      "loss": 0.3607,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.057940538972616196,
      "learning_rate": 0.0001874457429048414,
      "loss": 0.3539,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.049378618597984314,
      "learning_rate": 0.0001873789649415693,
      "loss": 0.2927,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.053056396543979645,
      "learning_rate": 0.00018731218697829718,
      "loss": 0.3114,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.09249860048294067,
      "learning_rate": 0.00018724540901502506,
      "loss": 0.3164,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.06798094511032104,
      "learning_rate": 0.00018717863105175293,
      "loss": 0.3167,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.0637553483247757,
      "learning_rate": 0.0001871118530884808,
      "loss": 0.39,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06718450784683228,
      "learning_rate": 0.00018704507512520868,
      "loss": 0.3436,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05805756896734238,
      "learning_rate": 0.00018697829716193658,
      "loss": 0.2615,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.066567063331604,
      "learning_rate": 0.00018691151919866445,
      "loss": 0.3377,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.0723097026348114,
      "learning_rate": 0.00018684474123539232,
      "loss": 0.3788,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.084930919110775,
      "learning_rate": 0.0001867779632721202,
      "loss": 0.3318,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.07866982370615005,
      "learning_rate": 0.00018671118530884807,
      "loss": 0.2717,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.07142424583435059,
      "learning_rate": 0.00018664440734557597,
      "loss": 0.2816,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.11116000264883041,
      "learning_rate": 0.00018657762938230384,
      "loss": 0.3934,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.06793380528688431,
      "learning_rate": 0.00018651085141903174,
      "loss": 0.3052,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.067479707300663,
      "learning_rate": 0.00018644407345575962,
      "loss": 0.2836,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.04016229510307312,
      "learning_rate": 0.0001863772954924875,
      "loss": 0.2509,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06527184695005417,
      "learning_rate": 0.0001863105175292154,
      "loss": 0.3379,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.08602338284254074,
      "learning_rate": 0.00018624373956594326,
      "loss": 0.3096,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.08282919973134995,
      "learning_rate": 0.00018617696160267113,
      "loss": 0.376,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.057017698884010315,
      "learning_rate": 0.000186110183639399,
      "loss": 0.254,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06612373888492584,
      "learning_rate": 0.00018604340567612688,
      "loss": 0.2664,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.08113586157560349,
      "learning_rate": 0.00018597662771285475,
      "loss": 0.3139,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.045813411474227905,
      "learning_rate": 0.00018590984974958265,
      "loss": 0.2541,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.06276820600032806,
      "learning_rate": 0.00018584307178631053,
      "loss": 0.3263,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.056134093552827835,
      "learning_rate": 0.0001857762938230384,
      "loss": 0.2932,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06455931812524796,
      "learning_rate": 0.00018570951585976627,
      "loss": 0.2713,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.0641329288482666,
      "learning_rate": 0.00018564273789649415,
      "loss": 0.3262,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.057115621864795685,
      "learning_rate": 0.00018557595993322205,
      "loss": 0.3218,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04558887332677841,
      "learning_rate": 0.00018550918196994992,
      "loss": 0.2999,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.05689835548400879,
      "learning_rate": 0.0001854424040066778,
      "loss": 0.334,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.058476708829402924,
      "learning_rate": 0.0001853756260434057,
      "loss": 0.3557,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.0640471801161766,
      "learning_rate": 0.00018530884808013357,
      "loss": 0.3499,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.06061849743127823,
      "learning_rate": 0.00018524207011686147,
      "loss": 0.2432,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.07924827188253403,
      "learning_rate": 0.00018517529215358934,
      "loss": 0.3285,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09070103615522385,
      "learning_rate": 0.00018510851419031721,
      "loss": 0.4282,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.07663854956626892,
      "learning_rate": 0.0001850417362270451,
      "loss": 0.3036,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.058672137558460236,
      "learning_rate": 0.00018497495826377296,
      "loss": 0.2984,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06622264534235,
      "learning_rate": 0.00018490818030050083,
      "loss": 0.28,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.07077071815729141,
      "learning_rate": 0.00018484140233722873,
      "loss": 0.3022,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.06734973192214966,
      "learning_rate": 0.0001847746243739566,
      "loss": 0.3449,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.08291982859373093,
      "learning_rate": 0.00018470784641068448,
      "loss": 0.4326,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.03683653473854065,
      "learning_rate": 0.00018464106844741235,
      "loss": 0.2626,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.061428993940353394,
      "learning_rate": 0.00018457429048414023,
      "loss": 0.3523,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.07222747057676315,
      "learning_rate": 0.00018450751252086813,
      "loss": 0.3464,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.050284553319215775,
      "learning_rate": 0.000184440734557596,
      "loss": 0.3032,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.05952095612883568,
      "learning_rate": 0.00018437395659432387,
      "loss": 0.277,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07080469280481339,
      "learning_rate": 0.00018430717863105175,
      "loss": 0.2825,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.11495279520750046,
      "learning_rate": 0.00018424040066777965,
      "loss": 0.4424,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06323540955781937,
      "learning_rate": 0.00018417362270450752,
      "loss": 0.3093,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.0691780149936676,
      "learning_rate": 0.00018410684474123542,
      "loss": 0.2963,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.07082061469554901,
      "learning_rate": 0.0001840400667779633,
      "loss": 0.3557,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.06467094272375107,
      "learning_rate": 0.00018397328881469117,
      "loss": 0.2971,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.09653520584106445,
      "learning_rate": 0.00018390651085141904,
      "loss": 0.4259,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.0660548061132431,
      "learning_rate": 0.0001838397328881469,
      "loss": 0.4072,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.05308215692639351,
      "learning_rate": 0.0001837729549248748,
      "loss": 0.3724,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.10216723382472992,
      "learning_rate": 0.00018370617696160269,
      "loss": 0.431,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.08200223743915558,
      "learning_rate": 0.00018363939899833056,
      "loss": 0.3823,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.06216123327612877,
      "learning_rate": 0.00018357262103505843,
      "loss": 0.2717,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.07566752284765244,
      "learning_rate": 0.0001835058430717863,
      "loss": 0.3584,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.09035226702690125,
      "learning_rate": 0.0001834390651085142,
      "loss": 0.3171,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.08981560170650482,
      "learning_rate": 0.00018337228714524208,
      "loss": 0.3525,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.08013089746236801,
      "learning_rate": 0.00018330550918196995,
      "loss": 0.3037,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.06599704176187515,
      "learning_rate": 0.00018323873121869782,
      "loss": 0.2963,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.06157409027218819,
      "learning_rate": 0.0001831719532554257,
      "loss": 0.3132,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.06951967626810074,
      "learning_rate": 0.0001831051752921536,
      "loss": 0.3272,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07502143085002899,
      "learning_rate": 0.00018303839732888147,
      "loss": 0.3704,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.0474037267267704,
      "learning_rate": 0.00018297161936560937,
      "loss": 0.2887,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.05565059185028076,
      "learning_rate": 0.00018290484140233724,
      "loss": 0.3066,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04033610224723816,
      "learning_rate": 0.00018283806343906512,
      "loss": 0.2671,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.08553161472082138,
      "learning_rate": 0.000182771285475793,
      "loss": 0.325,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.06281795352697372,
      "learning_rate": 0.0001827045075125209,
      "loss": 0.3459,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07509568333625793,
      "learning_rate": 0.00018263772954924876,
      "loss": 0.3709,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.061399880796670914,
      "learning_rate": 0.00018257095158597664,
      "loss": 0.39,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.04432804137468338,
      "learning_rate": 0.0001825041736227045,
      "loss": 0.3498,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.044908903539180756,
      "learning_rate": 0.00018243739565943238,
      "loss": 0.3071,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.052189409732818604,
      "learning_rate": 0.00018237061769616028,
      "loss": 0.2782,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.09120634198188782,
      "learning_rate": 0.00018230383973288816,
      "loss": 0.351,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.07506801187992096,
      "learning_rate": 0.00018223706176961603,
      "loss": 0.3482,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10021274536848068,
      "learning_rate": 0.0001821702838063439,
      "loss": 0.4434,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.05500519275665283,
      "learning_rate": 0.00018210350584307178,
      "loss": 0.3526,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.0683271586894989,
      "learning_rate": 0.00018203672787979968,
      "loss": 0.3372,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.10096199810504913,
      "learning_rate": 0.00018196994991652755,
      "loss": 0.3778,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.054836444556713104,
      "learning_rate": 0.00018190317195325542,
      "loss": 0.2848,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.06321289390325546,
      "learning_rate": 0.00018183639398998332,
      "loss": 0.3287,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.047697845846414566,
      "learning_rate": 0.0001817696160267112,
      "loss": 0.3336,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.0485498309135437,
      "learning_rate": 0.0001817028380634391,
      "loss": 0.3112,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06390071660280228,
      "learning_rate": 0.00018163606010016697,
      "loss": 0.2998,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.05406942218542099,
      "learning_rate": 0.00018156928213689484,
      "loss": 0.3118,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05003362149000168,
      "learning_rate": 0.00018150250417362272,
      "loss": 0.3149,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.06629174947738647,
      "learning_rate": 0.0001814357262103506,
      "loss": 0.3656,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05630950629711151,
      "learning_rate": 0.00018136894824707846,
      "loss": 0.3993,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.08491452783346176,
      "learning_rate": 0.00018130217028380636,
      "loss": 0.3745,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.1000320240855217,
      "learning_rate": 0.00018123539232053424,
      "loss": 0.3475,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.088816799223423,
      "learning_rate": 0.0001811686143572621,
      "loss": 0.3918,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.12104816734790802,
      "learning_rate": 0.00018110183639398998,
      "loss": 0.4646,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.059016842395067215,
      "learning_rate": 0.00018103505843071786,
      "loss": 0.3566,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.09891337156295776,
      "learning_rate": 0.00018096828046744576,
      "loss": 0.4484,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.07177264988422394,
      "learning_rate": 0.00018090150250417363,
      "loss": 0.3773,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.06040855124592781,
      "learning_rate": 0.0001808347245409015,
      "loss": 0.3401,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07831306010484695,
      "learning_rate": 0.00018076794657762938,
      "loss": 0.4055,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05135171487927437,
      "learning_rate": 0.00018070116861435728,
      "loss": 0.3338,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.09665145725011826,
      "learning_rate": 0.00018063439065108515,
      "loss": 0.4434,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.061043549329042435,
      "learning_rate": 0.00018056761268781305,
      "loss": 0.4006,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.05078258365392685,
      "learning_rate": 0.00018050083472454092,
      "loss": 0.2619,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.047669392079114914,
      "learning_rate": 0.0001804340567612688,
      "loss": 0.272,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.07371781021356583,
      "learning_rate": 0.00018036727879799667,
      "loss": 0.3549,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.045762427151203156,
      "learning_rate": 0.00018030050083472454,
      "loss": 0.2972,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.0923825204372406,
      "learning_rate": 0.00018023372287145244,
      "loss": 0.3934,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.05545240268111229,
      "learning_rate": 0.00018016694490818031,
      "loss": 0.2954,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.05194549635052681,
      "learning_rate": 0.0001801001669449082,
      "loss": 0.2697,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0787268877029419,
      "learning_rate": 0.00018003338898163606,
      "loss": 0.3735,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.07389024645090103,
      "learning_rate": 0.00017996661101836393,
      "loss": 0.4262,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05247769504785538,
      "learning_rate": 0.00017989983305509183,
      "loss": 0.3109,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.05217419192194939,
      "learning_rate": 0.0001798330550918197,
      "loss": 0.2739,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06178995221853256,
      "learning_rate": 0.00017976627712854758,
      "loss": 0.2783,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.0531550757586956,
      "learning_rate": 0.00017969949916527545,
      "loss": 0.3064,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.05511941760778427,
      "learning_rate": 0.00017963272120200333,
      "loss": 0.3669,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.0790976732969284,
      "learning_rate": 0.00017956594323873123,
      "loss": 0.3963,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09130754321813583,
      "learning_rate": 0.0001794991652754591,
      "loss": 0.3848,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.054158858954906464,
      "learning_rate": 0.000179432387312187,
      "loss": 0.3583,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.08371222764253616,
      "learning_rate": 0.00017936560934891487,
      "loss": 0.3896,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.06208382919430733,
      "learning_rate": 0.00017929883138564275,
      "loss": 0.2482,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07005839049816132,
      "learning_rate": 0.00017923205342237062,
      "loss": 0.412,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.05783788487315178,
      "learning_rate": 0.00017916527545909852,
      "loss": 0.3352,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06575559824705124,
      "learning_rate": 0.0001790984974958264,
      "loss": 0.3964,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.05575578659772873,
      "learning_rate": 0.00017903171953255427,
      "loss": 0.2551,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09290974587202072,
      "learning_rate": 0.00017896494156928214,
      "loss": 0.3649,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.06782729923725128,
      "learning_rate": 0.00017889816360601,
      "loss": 0.2901,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06380290538072586,
      "learning_rate": 0.0001788313856427379,
      "loss": 0.362,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04423273354768753,
      "learning_rate": 0.0001787646076794658,
      "loss": 0.2689,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07054946571588516,
      "learning_rate": 0.00017869782971619366,
      "loss": 0.3748,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.05055522546172142,
      "learning_rate": 0.00017863105175292153,
      "loss": 0.3167,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.05999542027711868,
      "learning_rate": 0.0001785642737896494,
      "loss": 0.3359,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.05833917483687401,
      "learning_rate": 0.0001784974958263773,
      "loss": 0.2835,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.058605678379535675,
      "learning_rate": 0.00017843071786310518,
      "loss": 0.3623,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.060921862721443176,
      "learning_rate": 0.00017836393989983305,
      "loss": 0.3618,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.034921158105134964,
      "learning_rate": 0.00017829716193656095,
      "loss": 0.2091,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.06082122027873993,
      "learning_rate": 0.00017823038397328883,
      "loss": 0.3362,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.04426771029829979,
      "learning_rate": 0.0001781636060100167,
      "loss": 0.3148,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.056465454399585724,
      "learning_rate": 0.0001780968280467446,
      "loss": 0.3649,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08533026278018951,
      "learning_rate": 0.00017803005008347247,
      "loss": 0.3809,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.05331623554229736,
      "learning_rate": 0.00017796327212020035,
      "loss": 0.3329,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.04352397844195366,
      "learning_rate": 0.00017789649415692822,
      "loss": 0.2272,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.04538191854953766,
      "learning_rate": 0.0001778297161936561,
      "loss": 0.3121,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.06431352347135544,
      "learning_rate": 0.000177762938230384,
      "loss": 0.3235,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.05296257883310318,
      "learning_rate": 0.00017769616026711187,
      "loss": 0.2699,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.04389478638768196,
      "learning_rate": 0.00017762938230383974,
      "loss": 0.307,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.0654267817735672,
      "learning_rate": 0.0001775626043405676,
      "loss": 0.3058,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07108623534440994,
      "learning_rate": 0.00017749582637729548,
      "loss": 0.3726,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.06231032684445381,
      "learning_rate": 0.00017742904841402339,
      "loss": 0.348,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.05152330547571182,
      "learning_rate": 0.00017736227045075126,
      "loss": 0.328,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.07389311492443085,
      "learning_rate": 0.00017729549248747913,
      "loss": 0.4102,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.05757777392864227,
      "learning_rate": 0.000177228714524207,
      "loss": 0.2844,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.04819808527827263,
      "learning_rate": 0.0001771619365609349,
      "loss": 0.3189,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11943501979112625,
      "learning_rate": 0.00017709515859766278,
      "loss": 0.3542,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06013776361942291,
      "learning_rate": 0.00017702838063439068,
      "loss": 0.3877,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.05257827043533325,
      "learning_rate": 0.00017696160267111855,
      "loss": 0.3227,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.053413026034832,
      "learning_rate": 0.00017689482470784642,
      "loss": 0.2781,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.04171682894229889,
      "learning_rate": 0.0001768280467445743,
      "loss": 0.2906,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.05988014116883278,
      "learning_rate": 0.00017676126878130217,
      "loss": 0.3826,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.04357519745826721,
      "learning_rate": 0.00017669449081803007,
      "loss": 0.2846,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.07369498163461685,
      "learning_rate": 0.00017662771285475794,
      "loss": 0.3901,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.056979950517416,
      "learning_rate": 0.00017656093489148582,
      "loss": 0.3105,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.04863990843296051,
      "learning_rate": 0.0001764941569282137,
      "loss": 0.2644,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.0584864467382431,
      "learning_rate": 0.00017642737896494156,
      "loss": 0.3412,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.06396812200546265,
      "learning_rate": 0.00017636060100166946,
      "loss": 0.3769,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.048790302127599716,
      "learning_rate": 0.00017629382303839734,
      "loss": 0.3412,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.05513220652937889,
      "learning_rate": 0.0001762270450751252,
      "loss": 0.3053,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.06738697737455368,
      "learning_rate": 0.00017616026711185308,
      "loss": 0.3689,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03774310275912285,
      "learning_rate": 0.00017609348914858096,
      "loss": 0.2866,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.0639033168554306,
      "learning_rate": 0.00017602671118530886,
      "loss": 0.2677,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.08802369236946106,
      "learning_rate": 0.00017595993322203673,
      "loss": 0.417,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.07504976540803909,
      "learning_rate": 0.00017589315525876463,
      "loss": 0.31,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.06366459280252457,
      "learning_rate": 0.0001758263772954925,
      "loss": 0.3457,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.06799014657735825,
      "learning_rate": 0.00017575959933222038,
      "loss": 0.3874,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.061081353574991226,
      "learning_rate": 0.00017569282136894825,
      "loss": 0.3125,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06835927814245224,
      "learning_rate": 0.00017562604340567615,
      "loss": 0.2911,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.06039321795105934,
      "learning_rate": 0.00017555926544240402,
      "loss": 0.3353,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06088876351714134,
      "learning_rate": 0.0001754924874791319,
      "loss": 0.3306,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.06041053682565689,
      "learning_rate": 0.00017542570951585977,
      "loss": 0.3883,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.049776844680309296,
      "learning_rate": 0.00017535893155258764,
      "loss": 0.3883,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.07191675901412964,
      "learning_rate": 0.00017529215358931554,
      "loss": 0.2709,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.08683455735445023,
      "learning_rate": 0.00017522537562604342,
      "loss": 0.3736,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.08970895409584045,
      "learning_rate": 0.0001751585976627713,
      "loss": 0.3928,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.047020550817251205,
      "learning_rate": 0.00017509181969949916,
      "loss": 0.2753,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.09434638172388077,
      "learning_rate": 0.00017502504173622704,
      "loss": 0.3704,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.060877081006765366,
      "learning_rate": 0.0001749582637729549,
      "loss": 0.3033,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.05946670472621918,
      "learning_rate": 0.0001748914858096828,
      "loss": 0.2807,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.05076393485069275,
      "learning_rate": 0.0001748247078464107,
      "loss": 0.2931,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.08851009607315063,
      "learning_rate": 0.00017475792988313858,
      "loss": 0.3605,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10143689811229706,
      "learning_rate": 0.00017469115191986646,
      "loss": 0.4418,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.07623142749071121,
      "learning_rate": 0.00017462437395659433,
      "loss": 0.3911,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.07023727893829346,
      "learning_rate": 0.00017455759599332223,
      "loss": 0.3663,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.06017600744962692,
      "learning_rate": 0.0001744908180300501,
      "loss": 0.3083,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05826139077544212,
      "learning_rate": 0.00017442404006677798,
      "loss": 0.2582,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.06767719238996506,
      "learning_rate": 0.00017435726210350585,
      "loss": 0.3188,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.05285611003637314,
      "learning_rate": 0.00017429048414023372,
      "loss": 0.2998,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.056411128491163254,
      "learning_rate": 0.00017422370617696162,
      "loss": 0.2921,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.061185091733932495,
      "learning_rate": 0.0001741569282136895,
      "loss": 0.2943,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.05641420558094978,
      "learning_rate": 0.00017409015025041737,
      "loss": 0.2759,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.06851478666067123,
      "learning_rate": 0.00017402337228714524,
      "loss": 0.4513,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.0594760999083519,
      "learning_rate": 0.00017395659432387311,
      "loss": 0.379,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06426342576742172,
      "learning_rate": 0.00017388981636060101,
      "loss": 0.3499,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.0700017437338829,
      "learning_rate": 0.0001738230383973289,
      "loss": 0.317,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.04312475770711899,
      "learning_rate": 0.00017375626043405676,
      "loss": 0.2678,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.06582977622747421,
      "learning_rate": 0.00017368948247078466,
      "loss": 0.3406,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.07006170600652695,
      "learning_rate": 0.00017362270450751253,
      "loss": 0.369,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.05010639503598213,
      "learning_rate": 0.0001735559265442404,
      "loss": 0.3638,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.07488889247179031,
      "learning_rate": 0.0001734891485809683,
      "loss": 0.3247,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.05184513330459595,
      "learning_rate": 0.00017342237061769618,
      "loss": 0.2603,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.06638722866773605,
      "learning_rate": 0.00017335559265442405,
      "loss": 0.2854,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.06103762611746788,
      "learning_rate": 0.00017328881469115193,
      "loss": 0.3253,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.05783310532569885,
      "learning_rate": 0.0001732220367278798,
      "loss": 0.289,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.08919013291597366,
      "learning_rate": 0.0001731552587646077,
      "loss": 0.4523,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.08311869204044342,
      "learning_rate": 0.00017308848080133557,
      "loss": 0.2787,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.0692351907491684,
      "learning_rate": 0.00017302170283806345,
      "loss": 0.3734,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.05560993030667305,
      "learning_rate": 0.00017295492487479132,
      "loss": 0.3845,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.04939797893166542,
      "learning_rate": 0.0001728881469115192,
      "loss": 0.2759,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.04753990098834038,
      "learning_rate": 0.0001728213689482471,
      "loss": 0.2949,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.0605107918381691,
      "learning_rate": 0.00017275459098497497,
      "loss": 0.332,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.05432650446891785,
      "learning_rate": 0.00017268781302170284,
      "loss": 0.334,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.0383317731320858,
      "learning_rate": 0.0001726210350584307,
      "loss": 0.1907,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.06635861098766327,
      "learning_rate": 0.0001725542570951586,
      "loss": 0.3594,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.07990048080682755,
      "learning_rate": 0.0001724874791318865,
      "loss": 0.34,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.06931383162736893,
      "learning_rate": 0.0001724207011686144,
      "loss": 0.3743,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.06240297481417656,
      "learning_rate": 0.00017235392320534226,
      "loss": 0.3097,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04172668978571892,
      "learning_rate": 0.00017228714524207013,
      "loss": 0.3016,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.05873313546180725,
      "learning_rate": 0.000172220367278798,
      "loss": 0.4202,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.06333927810192108,
      "learning_rate": 0.00017215358931552588,
      "loss": 0.3504,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.054410386830568314,
      "learning_rate": 0.00017208681135225378,
      "loss": 0.3442,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06687209010124207,
      "learning_rate": 0.00017202003338898165,
      "loss": 0.3851,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.04169590398669243,
      "learning_rate": 0.00017195325542570953,
      "loss": 0.3524,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.05493532493710518,
      "learning_rate": 0.0001718864774624374,
      "loss": 0.2999,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.04102494567632675,
      "learning_rate": 0.00017181969949916527,
      "loss": 0.2589,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.06786254793405533,
      "learning_rate": 0.00017175292153589317,
      "loss": 0.3247,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.05260216444730759,
      "learning_rate": 0.00017168614357262105,
      "loss": 0.3017,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.051911525428295135,
      "learning_rate": 0.00017161936560934892,
      "loss": 0.3347,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.04882310703396797,
      "learning_rate": 0.0001715525876460768,
      "loss": 0.3196,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06122424080967903,
      "learning_rate": 0.00017148580968280467,
      "loss": 0.3887,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.04538717120885849,
      "learning_rate": 0.00017141903171953257,
      "loss": 0.279,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.06093759089708328,
      "learning_rate": 0.00017135225375626044,
      "loss": 0.3215,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05240204557776451,
      "learning_rate": 0.00017128547579298834,
      "loss": 0.3036,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.05205075070261955,
      "learning_rate": 0.0001712186978297162,
      "loss": 0.311,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.05608535557985306,
      "learning_rate": 0.00017115191986644409,
      "loss": 0.331,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.047444626688957214,
      "learning_rate": 0.00017108514190317196,
      "loss": 0.3073,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.06831686943769455,
      "learning_rate": 0.00017101836393989986,
      "loss": 0.3452,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03715347498655319,
      "learning_rate": 0.00017095158597662773,
      "loss": 0.2624,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09170711785554886,
      "learning_rate": 0.0001708848080133556,
      "loss": 0.3888,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.05762215703725815,
      "learning_rate": 0.00017081803005008348,
      "loss": 0.3158,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06637157499790192,
      "learning_rate": 0.00017075125208681135,
      "loss": 0.2453,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04048594832420349,
      "learning_rate": 0.00017068447412353925,
      "loss": 0.2397,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.10212234407663345,
      "learning_rate": 0.00017061769616026712,
      "loss": 0.4159,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06937846541404724,
      "learning_rate": 0.000170550918196995,
      "loss": 0.3396,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.07474826276302338,
      "learning_rate": 0.00017048414023372287,
      "loss": 0.3326,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05824249982833862,
      "learning_rate": 0.00017041736227045074,
      "loss": 0.3379,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.04704830422997475,
      "learning_rate": 0.00017035058430717862,
      "loss": 0.2354,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.06278561055660248,
      "learning_rate": 0.00017028380634390652,
      "loss": 0.3221,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.08273832499980927,
      "learning_rate": 0.0001702170283806344,
      "loss": 0.366,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.0600234791636467,
      "learning_rate": 0.0001701502504173623,
      "loss": 0.2801,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.04539366066455841,
      "learning_rate": 0.00017008347245409016,
      "loss": 0.2435,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06901519000530243,
      "learning_rate": 0.00017001669449081804,
      "loss": 0.3805,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07664025574922562,
      "learning_rate": 0.00016994991652754594,
      "loss": 0.2939,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.046238843351602554,
      "learning_rate": 0.0001698831385642738,
      "loss": 0.2781,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.06035570055246353,
      "learning_rate": 0.00016981636060100168,
      "loss": 0.3907,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.06277912855148315,
      "learning_rate": 0.00016974958263772956,
      "loss": 0.3467,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.044353120028972626,
      "learning_rate": 0.00016968280467445743,
      "loss": 0.2857,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.04181146249175072,
      "learning_rate": 0.00016961602671118533,
      "loss": 0.2875,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.05250684544444084,
      "learning_rate": 0.0001695492487479132,
      "loss": 0.3587,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.05013364553451538,
      "learning_rate": 0.00016948247078464108,
      "loss": 0.2818,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.051002535969018936,
      "learning_rate": 0.00016941569282136895,
      "loss": 0.3078,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.06919902563095093,
      "learning_rate": 0.00016934891485809682,
      "loss": 0.4099,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06153741106390953,
      "learning_rate": 0.0001692821368948247,
      "loss": 0.3219,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.0433584563434124,
      "learning_rate": 0.0001692153589315526,
      "loss": 0.2554,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.041832782328128815,
      "learning_rate": 0.00016914858096828047,
      "loss": 0.2847,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.045837342739105225,
      "learning_rate": 0.00016908180300500834,
      "loss": 0.347,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.07606692612171173,
      "learning_rate": 0.00016901502504173624,
      "loss": 0.3495,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.04513280466198921,
      "learning_rate": 0.00016894824707846412,
      "loss": 0.303,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.039144162088632584,
      "learning_rate": 0.00016888146911519202,
      "loss": 0.2603,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04442967474460602,
      "learning_rate": 0.0001688146911519199,
      "loss": 0.2981,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.04966007545590401,
      "learning_rate": 0.00016874791318864776,
      "loss": 0.3159,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.05852694436907768,
      "learning_rate": 0.00016868113522537564,
      "loss": 0.3716,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.053059473633766174,
      "learning_rate": 0.0001686143572621035,
      "loss": 0.3792,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05099688097834587,
      "learning_rate": 0.0001685475792988314,
      "loss": 0.3137,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.10189834982156754,
      "learning_rate": 0.00016848080133555928,
      "loss": 0.3818,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.0526309497654438,
      "learning_rate": 0.00016841402337228716,
      "loss": 0.2817,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05295218899846077,
      "learning_rate": 0.00016834724540901503,
      "loss": 0.3401,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06662885844707489,
      "learning_rate": 0.0001682804674457429,
      "loss": 0.3991,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.06841499358415604,
      "learning_rate": 0.00016821368948247077,
      "loss": 0.4051,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.0365263968706131,
      "learning_rate": 0.00016814691151919868,
      "loss": 0.2376,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04485613852739334,
      "learning_rate": 0.00016808013355592655,
      "loss": 0.2694,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.06939379125833511,
      "learning_rate": 0.00016801335559265442,
      "loss": 0.3597,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.06019312143325806,
      "learning_rate": 0.0001679465776293823,
      "loss": 0.3509,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05627303943037987,
      "learning_rate": 0.0001678797996661102,
      "loss": 0.32,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.04382266476750374,
      "learning_rate": 0.00016781302170283807,
      "loss": 0.2564,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.056689437478780746,
      "learning_rate": 0.00016774624373956597,
      "loss": 0.2995,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06798779219388962,
      "learning_rate": 0.00016767946577629384,
      "loss": 0.3409,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.04012368619441986,
      "learning_rate": 0.00016761268781302171,
      "loss": 0.2714,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.046074435114860535,
      "learning_rate": 0.0001675459098497496,
      "loss": 0.2982,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.047727275639772415,
      "learning_rate": 0.0001674791318864775,
      "loss": 0.3133,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.04751616716384888,
      "learning_rate": 0.00016741235392320536,
      "loss": 0.2959,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.06545601040124893,
      "learning_rate": 0.00016734557595993323,
      "loss": 0.335,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.05398217961192131,
      "learning_rate": 0.0001672787979966611,
      "loss": 0.3406,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.049862947314977646,
      "learning_rate": 0.00016721202003338898,
      "loss": 0.2785,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.051326289772987366,
      "learning_rate": 0.00016714524207011685,
      "loss": 0.3111,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.06404118984937668,
      "learning_rate": 0.00016707846410684475,
      "loss": 0.3326,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04448550567030907,
      "learning_rate": 0.00016701168614357263,
      "loss": 0.2722,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.04964333772659302,
      "learning_rate": 0.0001669449081803005,
      "loss": 0.2919,
      "step": 500
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.05675473436713219,
      "learning_rate": 0.00016687813021702837,
      "loss": 0.293,
      "step": 501
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.059033967554569244,
      "learning_rate": 0.00016681135225375625,
      "loss": 0.3704,
      "step": 502
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.07478541135787964,
      "learning_rate": 0.00016674457429048415,
      "loss": 0.3317,
      "step": 503
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.05344531685113907,
      "learning_rate": 0.00016667779632721202,
      "loss": 0.2982,
      "step": 504
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.07415473461151123,
      "learning_rate": 0.00016661101836393992,
      "loss": 0.3207,
      "step": 505
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.09510181099176407,
      "learning_rate": 0.0001665442404006678,
      "loss": 0.3774,
      "step": 506
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.03938121721148491,
      "learning_rate": 0.00016647746243739567,
      "loss": 0.2367,
      "step": 507
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.09157609939575195,
      "learning_rate": 0.00016641068447412357,
      "loss": 0.4259,
      "step": 508
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.03961868956685066,
      "learning_rate": 0.00016634390651085144,
      "loss": 0.2935,
      "step": 509
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.04324951022863388,
      "learning_rate": 0.0001662771285475793,
      "loss": 0.2709,
      "step": 510
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 0.0446760393679142,
      "learning_rate": 0.00016621035058430719,
      "loss": 0.2964,
      "step": 511
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.03857719153165817,
      "learning_rate": 0.00016614357262103506,
      "loss": 0.2571,
      "step": 512
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.06435337662696838,
      "learning_rate": 0.00016607679465776293,
      "loss": 0.3262,
      "step": 513
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.06182745471596718,
      "learning_rate": 0.00016601001669449083,
      "loss": 0.3382,
      "step": 514
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.0427301824092865,
      "learning_rate": 0.0001659432387312187,
      "loss": 0.2971,
      "step": 515
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.05086801201105118,
      "learning_rate": 0.00016587646076794658,
      "loss": 0.3126,
      "step": 516
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.06715759634971619,
      "learning_rate": 0.00016580968280467445,
      "loss": 0.3453,
      "step": 517
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.07618245482444763,
      "learning_rate": 0.00016574290484140233,
      "loss": 0.3537,
      "step": 518
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.04840880632400513,
      "learning_rate": 0.00016567612687813023,
      "loss": 0.3,
      "step": 519
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04332226142287254,
      "learning_rate": 0.0001656093489148581,
      "loss": 0.2733,
      "step": 520
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 0.06146964803338051,
      "learning_rate": 0.00016554257095158597,
      "loss": 0.3326,
      "step": 521
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.04399601370096207,
      "learning_rate": 0.00016547579298831387,
      "loss": 0.2826,
      "step": 522
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 0.06812360137701035,
      "learning_rate": 0.00016540901502504175,
      "loss": 0.3761,
      "step": 523
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.05370856449007988,
      "learning_rate": 0.00016534223706176965,
      "loss": 0.3308,
      "step": 524
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.044260215014219284,
      "learning_rate": 0.00016527545909849752,
      "loss": 0.2686,
      "step": 525
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.05853787064552307,
      "learning_rate": 0.0001652086811352254,
      "loss": 0.3793,
      "step": 526
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.0512453131377697,
      "learning_rate": 0.00016514190317195327,
      "loss": 0.2853,
      "step": 527
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.07850892096757889,
      "learning_rate": 0.00016507512520868114,
      "loss": 0.4188,
      "step": 528
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 0.04495372250676155,
      "learning_rate": 0.00016500834724540904,
      "loss": 0.2759,
      "step": 529
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.06586789339780807,
      "learning_rate": 0.0001649415692821369,
      "loss": 0.4153,
      "step": 530
    },
    {
      "epoch": 0.885,
      "grad_norm": 0.056059613823890686,
      "learning_rate": 0.00016487479131886478,
      "loss": 0.3336,
      "step": 531
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.08161654323339462,
      "learning_rate": 0.00016480801335559266,
      "loss": 0.3384,
      "step": 532
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 0.03978919982910156,
      "learning_rate": 0.00016474123539232053,
      "loss": 0.2742,
      "step": 533
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.04851942136883736,
      "learning_rate": 0.0001646744574290484,
      "loss": 0.2634,
      "step": 534
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.05090683326125145,
      "learning_rate": 0.0001646076794657763,
      "loss": 0.3193,
      "step": 535
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.07465936988592148,
      "learning_rate": 0.00016454090150250418,
      "loss": 0.3146,
      "step": 536
    },
    {
      "epoch": 0.895,
      "grad_norm": 0.06789000332355499,
      "learning_rate": 0.00016447412353923205,
      "loss": 0.3492,
      "step": 537
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.10537659376859665,
      "learning_rate": 0.00016440734557595992,
      "loss": 0.426,
      "step": 538
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.053664591163396835,
      "learning_rate": 0.00016434056761268782,
      "loss": 0.3543,
      "step": 539
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.08032732456922531,
      "learning_rate": 0.0001642737896494157,
      "loss": 0.3495,
      "step": 540
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.07147842645645142,
      "learning_rate": 0.0001642070116861436,
      "loss": 0.3444,
      "step": 541
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.04289628565311432,
      "learning_rate": 0.00016414023372287147,
      "loss": 0.2781,
      "step": 542
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.030786065384745598,
      "learning_rate": 0.00016407345575959934,
      "loss": 0.2176,
      "step": 543
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.06521321833133698,
      "learning_rate": 0.00016400667779632722,
      "loss": 0.2598,
      "step": 544
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.055258918553590775,
      "learning_rate": 0.00016393989983305512,
      "loss": 0.2933,
      "step": 545
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.051709070801734924,
      "learning_rate": 0.000163873121869783,
      "loss": 0.2903,
      "step": 546
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 0.05646602064371109,
      "learning_rate": 0.00016380634390651086,
      "loss": 0.3041,
      "step": 547
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.0457528792321682,
      "learning_rate": 0.00016373956594323874,
      "loss": 0.3229,
      "step": 548
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.05021786317229271,
      "learning_rate": 0.0001636727879799666,
      "loss": 0.2456,
      "step": 549
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.06800035387277603,
      "learning_rate": 0.00016360601001669448,
      "loss": 0.3065,
      "step": 550
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.056317973881959915,
      "learning_rate": 0.00016353923205342238,
      "loss": 0.3476,
      "step": 551
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.05203660577535629,
      "learning_rate": 0.00016347245409015026,
      "loss": 0.2806,
      "step": 552
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 0.0463302880525589,
      "learning_rate": 0.00016340567612687813,
      "loss": 0.2319,
      "step": 553
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 0.049490366131067276,
      "learning_rate": 0.000163338898163606,
      "loss": 0.2979,
      "step": 554
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.03928279131650925,
      "learning_rate": 0.00016327212020033388,
      "loss": 0.2375,
      "step": 555
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04550032317638397,
      "learning_rate": 0.00016320534223706178,
      "loss": 0.2814,
      "step": 556
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.08014900237321854,
      "learning_rate": 0.00016313856427378965,
      "loss": 0.3969,
      "step": 557
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.047231823205947876,
      "learning_rate": 0.00016307178631051755,
      "loss": 0.2863,
      "step": 558
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 0.03892596811056137,
      "learning_rate": 0.00016300500834724542,
      "loss": 0.2684,
      "step": 559
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.07393954694271088,
      "learning_rate": 0.0001629382303839733,
      "loss": 0.332,
      "step": 560
    },
    {
      "epoch": 0.935,
      "grad_norm": 0.05307886749505997,
      "learning_rate": 0.0001628714524207012,
      "loss": 0.3283,
      "step": 561
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.04826061800122261,
      "learning_rate": 0.00016280467445742907,
      "loss": 0.2215,
      "step": 562
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.044075049459934235,
      "learning_rate": 0.00016273789649415694,
      "loss": 0.3149,
      "step": 563
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.09960328042507172,
      "learning_rate": 0.00016267111853088482,
      "loss": 0.3516,
      "step": 564
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.051804590970277786,
      "learning_rate": 0.0001626043405676127,
      "loss": 0.3135,
      "step": 565
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.05039425194263458,
      "learning_rate": 0.00016253756260434056,
      "loss": 0.2858,
      "step": 566
    },
    {
      "epoch": 0.945,
      "grad_norm": 0.05222024768590927,
      "learning_rate": 0.00016247078464106846,
      "loss": 0.3118,
      "step": 567
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.04534272849559784,
      "learning_rate": 0.00016240400667779634,
      "loss": 0.258,
      "step": 568
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 0.04662933945655823,
      "learning_rate": 0.0001623372287145242,
      "loss": 0.2613,
      "step": 569
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.0396605059504509,
      "learning_rate": 0.00016227045075125208,
      "loss": 0.277,
      "step": 570
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.06038658693432808,
      "learning_rate": 0.00016220367278797996,
      "loss": 0.3882,
      "step": 571
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.052110228687524796,
      "learning_rate": 0.00016213689482470786,
      "loss": 0.3054,
      "step": 572
    },
    {
      "epoch": 0.955,
      "grad_norm": 0.06867527216672897,
      "learning_rate": 0.00016207011686143573,
      "loss": 0.3896,
      "step": 573
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.03911750018596649,
      "learning_rate": 0.0001620033388981636,
      "loss": 0.2944,
      "step": 574
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.05596785619854927,
      "learning_rate": 0.0001619365609348915,
      "loss": 0.3148,
      "step": 575
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.05006161704659462,
      "learning_rate": 0.00016186978297161938,
      "loss": 0.3285,
      "step": 576
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.04391731321811676,
      "learning_rate": 0.00016180300500834728,
      "loss": 0.3006,
      "step": 577
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.039790429174900055,
      "learning_rate": 0.00016173622704507515,
      "loss": 0.3033,
      "step": 578
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.04407166317105293,
      "learning_rate": 0.00016166944908180302,
      "loss": 0.2928,
      "step": 579
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.05337825044989586,
      "learning_rate": 0.0001616026711185309,
      "loss": 0.269,
      "step": 580
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.04339572414755821,
      "learning_rate": 0.00016153589315525877,
      "loss": 0.3427,
      "step": 581
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.03984799608588219,
      "learning_rate": 0.00016146911519198664,
      "loss": 0.2478,
      "step": 582
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.04794854298233986,
      "learning_rate": 0.00016140233722871454,
      "loss": 0.2847,
      "step": 583
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.046956200152635574,
      "learning_rate": 0.00016133555926544241,
      "loss": 0.3028,
      "step": 584
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.04814032465219498,
      "learning_rate": 0.0001612687813021703,
      "loss": 0.3513,
      "step": 585
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.0567287839949131,
      "learning_rate": 0.00016120200333889816,
      "loss": 0.3163,
      "step": 586
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 0.05966684967279434,
      "learning_rate": 0.00016113522537562603,
      "loss": 0.3096,
      "step": 587
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.038798265159130096,
      "learning_rate": 0.00016106844741235393,
      "loss": 0.2708,
      "step": 588
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.05440191924571991,
      "learning_rate": 0.0001610016694490818,
      "loss": 0.3676,
      "step": 589
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.04472116008400917,
      "learning_rate": 0.00016093489148580968,
      "loss": 0.3371,
      "step": 590
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.03645741194486618,
      "learning_rate": 0.00016086811352253755,
      "loss": 0.2207,
      "step": 591
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08760751783847809,
      "learning_rate": 0.00016080133555926545,
      "loss": 0.3986,
      "step": 592
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.052007872611284256,
      "learning_rate": 0.00016073455759599333,
      "loss": 0.3025,
      "step": 593
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.0768626481294632,
      "learning_rate": 0.00016066777963272123,
      "loss": 0.2732,
      "step": 594
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.06116440147161484,
      "learning_rate": 0.0001606010016694491,
      "loss": 0.3004,
      "step": 595
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.0482056699693203,
      "learning_rate": 0.00016053422370617697,
      "loss": 0.3117,
      "step": 596
    },
    {
      "epoch": 0.995,
      "grad_norm": 0.0809585228562355,
      "learning_rate": 0.00016046744574290485,
      "loss": 0.3999,
      "step": 597
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.06876283884048462,
      "learning_rate": 0.00016040066777963272,
      "loss": 0.3137,
      "step": 598
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.06914673000574112,
      "learning_rate": 0.00016033388981636062,
      "loss": 0.3975,
      "step": 599
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.052446719259023666,
      "learning_rate": 0.0001602671118530885,
      "loss": 0.3224,
      "step": 600
    },
    {
      "epoch": 1.0016666666666667,
      "grad_norm": 0.038091398775577545,
      "learning_rate": 0.00016020033388981637,
      "loss": 0.2446,
      "step": 601
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.07895876467227936,
      "learning_rate": 0.00016013355592654424,
      "loss": 0.3709,
      "step": 602
    },
    {
      "epoch": 1.005,
      "grad_norm": 0.05968007072806358,
      "learning_rate": 0.0001600667779632721,
      "loss": 0.3438,
      "step": 603
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.040867894887924194,
      "learning_rate": 0.00016,
      "loss": 0.2978,
      "step": 604
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.04802035540342331,
      "learning_rate": 0.00015993322203672789,
      "loss": 0.2758,
      "step": 605
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.06557966023683548,
      "learning_rate": 0.00015986644407345576,
      "loss": 0.3414,
      "step": 606
    },
    {
      "epoch": 1.0116666666666667,
      "grad_norm": 0.0787474736571312,
      "learning_rate": 0.00015979966611018363,
      "loss": 0.4171,
      "step": 607
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.06755445897579193,
      "learning_rate": 0.0001597328881469115,
      "loss": 0.3587,
      "step": 608
    },
    {
      "epoch": 1.015,
      "grad_norm": 0.04977213591337204,
      "learning_rate": 0.0001596661101836394,
      "loss": 0.3133,
      "step": 609
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.04914654791355133,
      "learning_rate": 0.00015959933222036728,
      "loss": 0.2566,
      "step": 610
    },
    {
      "epoch": 1.0183333333333333,
      "grad_norm": 0.07955732196569443,
      "learning_rate": 0.00015953255425709518,
      "loss": 0.3409,
      "step": 611
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.05477627366781235,
      "learning_rate": 0.00015946577629382305,
      "loss": 0.3098,
      "step": 612
    },
    {
      "epoch": 1.0216666666666667,
      "grad_norm": 0.058321762830019,
      "learning_rate": 0.00015939899833055093,
      "loss": 0.3374,
      "step": 613
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.061733197420835495,
      "learning_rate": 0.0001593322203672788,
      "loss": 0.3389,
      "step": 614
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.06176535412669182,
      "learning_rate": 0.0001592654424040067,
      "loss": 0.3596,
      "step": 615
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.04223475605249405,
      "learning_rate": 0.00015919866444073457,
      "loss": 0.2665,
      "step": 616
    },
    {
      "epoch": 1.0283333333333333,
      "grad_norm": 0.057717952877283096,
      "learning_rate": 0.00015913188647746245,
      "loss": 0.3805,
      "step": 617
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.05137455463409424,
      "learning_rate": 0.00015906510851419032,
      "loss": 0.3496,
      "step": 618
    },
    {
      "epoch": 1.0316666666666667,
      "grad_norm": 0.059790827333927155,
      "learning_rate": 0.0001589983305509182,
      "loss": 0.3471,
      "step": 619
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.053921446204185486,
      "learning_rate": 0.0001589315525876461,
      "loss": 0.2848,
      "step": 620
    },
    {
      "epoch": 1.035,
      "grad_norm": 0.0980955958366394,
      "learning_rate": 0.00015886477462437397,
      "loss": 0.3987,
      "step": 621
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.04493315517902374,
      "learning_rate": 0.00015879799666110184,
      "loss": 0.2832,
      "step": 622
    },
    {
      "epoch": 1.0383333333333333,
      "grad_norm": 0.04871201887726784,
      "learning_rate": 0.0001587312186978297,
      "loss": 0.2994,
      "step": 623
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.07018174231052399,
      "learning_rate": 0.00015866444073455758,
      "loss": 0.3451,
      "step": 624
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.04685669019818306,
      "learning_rate": 0.00015859766277128548,
      "loss": 0.3145,
      "step": 625
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.05067994445562363,
      "learning_rate": 0.00015853088480801336,
      "loss": 0.2814,
      "step": 626
    },
    {
      "epoch": 1.045,
      "grad_norm": 0.047015778720378876,
      "learning_rate": 0.00015846410684474123,
      "loss": 0.2981,
      "step": 627
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.05327262356877327,
      "learning_rate": 0.00015839732888146913,
      "loss": 0.3616,
      "step": 628
    },
    {
      "epoch": 1.0483333333333333,
      "grad_norm": 0.04395569860935211,
      "learning_rate": 0.000158330550918197,
      "loss": 0.226,
      "step": 629
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.07606248557567596,
      "learning_rate": 0.00015826377295492488,
      "loss": 0.3494,
      "step": 630
    },
    {
      "epoch": 1.0516666666666667,
      "grad_norm": 0.04616096243262291,
      "learning_rate": 0.00015819699499165278,
      "loss": 0.2862,
      "step": 631
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.04835774004459381,
      "learning_rate": 0.00015813021702838065,
      "loss": 0.3222,
      "step": 632
    },
    {
      "epoch": 1.055,
      "grad_norm": 0.042904309928417206,
      "learning_rate": 0.00015806343906510852,
      "loss": 0.2884,
      "step": 633
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.04336833953857422,
      "learning_rate": 0.0001579966611018364,
      "loss": 0.2648,
      "step": 634
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.050038766115903854,
      "learning_rate": 0.00015792988313856427,
      "loss": 0.3104,
      "step": 635
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.034569013863801956,
      "learning_rate": 0.00015786310517529217,
      "loss": 0.2428,
      "step": 636
    },
    {
      "epoch": 1.0616666666666668,
      "grad_norm": 0.059664610773324966,
      "learning_rate": 0.00015779632721202004,
      "loss": 0.3434,
      "step": 637
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.05424325913190842,
      "learning_rate": 0.00015772954924874792,
      "loss": 0.3253,
      "step": 638
    },
    {
      "epoch": 1.065,
      "grad_norm": 0.05082716792821884,
      "learning_rate": 0.0001576627712854758,
      "loss": 0.2674,
      "step": 639
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.06522243469953537,
      "learning_rate": 0.00015759599332220366,
      "loss": 0.333,
      "step": 640
    },
    {
      "epoch": 1.0683333333333334,
      "grad_norm": 0.04958084598183632,
      "learning_rate": 0.00015752921535893156,
      "loss": 0.3349,
      "step": 641
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.047340359538793564,
      "learning_rate": 0.00015746243739565944,
      "loss": 0.2916,
      "step": 642
    },
    {
      "epoch": 1.0716666666666668,
      "grad_norm": 0.03549574688076973,
      "learning_rate": 0.0001573956594323873,
      "loss": 0.231,
      "step": 643
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.05005121976137161,
      "learning_rate": 0.0001573288814691152,
      "loss": 0.265,
      "step": 644
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.042196810245513916,
      "learning_rate": 0.00015726210350584308,
      "loss": 0.2566,
      "step": 645
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.04615529999136925,
      "learning_rate": 0.00015719532554257096,
      "loss": 0.3151,
      "step": 646
    },
    {
      "epoch": 1.0783333333333334,
      "grad_norm": 0.048391491174697876,
      "learning_rate": 0.00015712854757929886,
      "loss": 0.3205,
      "step": 647
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.04352809861302376,
      "learning_rate": 0.00015706176961602673,
      "loss": 0.3041,
      "step": 648
    },
    {
      "epoch": 1.0816666666666666,
      "grad_norm": 0.04534140229225159,
      "learning_rate": 0.0001569949916527546,
      "loss": 0.2781,
      "step": 649
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.057619065046310425,
      "learning_rate": 0.00015692821368948248,
      "loss": 0.3847,
      "step": 650
    },
    {
      "epoch": 1.085,
      "grad_norm": 0.045412901788949966,
      "learning_rate": 0.00015686143572621035,
      "loss": 0.3125,
      "step": 651
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.04086766391992569,
      "learning_rate": 0.00015679465776293825,
      "loss": 0.2781,
      "step": 652
    },
    {
      "epoch": 1.0883333333333334,
      "grad_norm": 0.042553484439849854,
      "learning_rate": 0.00015672787979966612,
      "loss": 0.31,
      "step": 653
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.07585950940847397,
      "learning_rate": 0.000156661101836394,
      "loss": 0.2926,
      "step": 654
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.050615083426237106,
      "learning_rate": 0.00015659432387312187,
      "loss": 0.285,
      "step": 655
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.049835097044706345,
      "learning_rate": 0.00015652754590984974,
      "loss": 0.2963,
      "step": 656
    },
    {
      "epoch": 1.095,
      "grad_norm": 0.06484062224626541,
      "learning_rate": 0.00015646076794657764,
      "loss": 0.3872,
      "step": 657
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.0384904108941555,
      "learning_rate": 0.00015639398998330552,
      "loss": 0.2242,
      "step": 658
    },
    {
      "epoch": 1.0983333333333334,
      "grad_norm": 0.04326695203781128,
      "learning_rate": 0.0001563272120200334,
      "loss": 0.3243,
      "step": 659
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.05285787582397461,
      "learning_rate": 0.00015626043405676126,
      "loss": 0.3104,
      "step": 660
    },
    {
      "epoch": 1.1016666666666666,
      "grad_norm": 0.04762762039899826,
      "learning_rate": 0.00015619365609348916,
      "loss": 0.2879,
      "step": 661
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.04077906161546707,
      "learning_rate": 0.00015612687813021704,
      "loss": 0.262,
      "step": 662
    },
    {
      "epoch": 1.105,
      "grad_norm": 0.05733682960271835,
      "learning_rate": 0.00015606010016694494,
      "loss": 0.351,
      "step": 663
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.04401100054383278,
      "learning_rate": 0.0001559933222036728,
      "loss": 0.3192,
      "step": 664
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.04534782096743584,
      "learning_rate": 0.00015592654424040068,
      "loss": 0.2772,
      "step": 665
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.04895781725645065,
      "learning_rate": 0.00015585976627712856,
      "loss": 0.313,
      "step": 666
    },
    {
      "epoch": 1.1116666666666666,
      "grad_norm": 0.062344253063201904,
      "learning_rate": 0.00015579298831385643,
      "loss": 0.3909,
      "step": 667
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.03933556750416756,
      "learning_rate": 0.00015572621035058433,
      "loss": 0.2868,
      "step": 668
    },
    {
      "epoch": 1.115,
      "grad_norm": 0.07237284630537033,
      "learning_rate": 0.0001556594323873122,
      "loss": 0.3845,
      "step": 669
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.06220970302820206,
      "learning_rate": 0.00015559265442404007,
      "loss": 0.386,
      "step": 670
    },
    {
      "epoch": 1.1183333333333334,
      "grad_norm": 0.044580187648534775,
      "learning_rate": 0.00015552587646076795,
      "loss": 0.2842,
      "step": 671
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.045256610959768295,
      "learning_rate": 0.00015545909849749582,
      "loss": 0.3183,
      "step": 672
    },
    {
      "epoch": 1.1216666666666666,
      "grad_norm": 0.04900319501757622,
      "learning_rate": 0.00015539232053422372,
      "loss": 0.3716,
      "step": 673
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.044318221509456635,
      "learning_rate": 0.0001553255425709516,
      "loss": 0.3414,
      "step": 674
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.05138834938406944,
      "learning_rate": 0.00015525876460767947,
      "loss": 0.2722,
      "step": 675
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.05407276377081871,
      "learning_rate": 0.00015519198664440734,
      "loss": 0.3588,
      "step": 676
    },
    {
      "epoch": 1.1283333333333334,
      "grad_norm": 0.03867404907941818,
      "learning_rate": 0.00015512520868113521,
      "loss": 0.2555,
      "step": 677
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.03920826315879822,
      "learning_rate": 0.00015505843071786311,
      "loss": 0.267,
      "step": 678
    },
    {
      "epoch": 1.1316666666666666,
      "grad_norm": 0.04791778698563576,
      "learning_rate": 0.000154991652754591,
      "loss": 0.3647,
      "step": 679
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.03733225166797638,
      "learning_rate": 0.0001549248747913189,
      "loss": 0.2255,
      "step": 680
    },
    {
      "epoch": 1.135,
      "grad_norm": 0.07614464312791824,
      "learning_rate": 0.00015485809682804676,
      "loss": 0.3266,
      "step": 681
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.04570731520652771,
      "learning_rate": 0.00015479131886477463,
      "loss": 0.2948,
      "step": 682
    },
    {
      "epoch": 1.1383333333333334,
      "grad_norm": 0.042178552597761154,
      "learning_rate": 0.0001547245409015025,
      "loss": 0.2722,
      "step": 683
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.05690876021981239,
      "learning_rate": 0.0001546577629382304,
      "loss": 0.3318,
      "step": 684
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.05208662152290344,
      "learning_rate": 0.00015459098497495828,
      "loss": 0.307,
      "step": 685
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 0.06061986833810806,
      "learning_rate": 0.00015452420701168615,
      "loss": 0.3622,
      "step": 686
    },
    {
      "epoch": 1.145,
      "grad_norm": 0.040480148047208786,
      "learning_rate": 0.00015445742904841403,
      "loss": 0.2865,
      "step": 687
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.04853244498372078,
      "learning_rate": 0.0001543906510851419,
      "loss": 0.3098,
      "step": 688
    },
    {
      "epoch": 1.1483333333333334,
      "grad_norm": 0.04154907166957855,
      "learning_rate": 0.0001543238731218698,
      "loss": 0.2638,
      "step": 689
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.06354950368404388,
      "learning_rate": 0.00015425709515859767,
      "loss": 0.3932,
      "step": 690
    },
    {
      "epoch": 1.1516666666666666,
      "grad_norm": 0.044019024819135666,
      "learning_rate": 0.00015419031719532555,
      "loss": 0.2719,
      "step": 691
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.0442047156393528,
      "learning_rate": 0.00015412353923205342,
      "loss": 0.268,
      "step": 692
    },
    {
      "epoch": 1.155,
      "grad_norm": 0.05394209176301956,
      "learning_rate": 0.0001540567612687813,
      "loss": 0.358,
      "step": 693
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.03976394608616829,
      "learning_rate": 0.0001539899833055092,
      "loss": 0.2444,
      "step": 694
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.041658367961645126,
      "learning_rate": 0.00015392320534223707,
      "loss": 0.2377,
      "step": 695
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.050059638917446136,
      "learning_rate": 0.00015385642737896494,
      "loss": 0.3108,
      "step": 696
    },
    {
      "epoch": 1.1616666666666666,
      "grad_norm": 0.049862418323755264,
      "learning_rate": 0.00015378964941569284,
      "loss": 0.3045,
      "step": 697
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.04192636162042618,
      "learning_rate": 0.0001537228714524207,
      "loss": 0.3207,
      "step": 698
    },
    {
      "epoch": 1.165,
      "grad_norm": 0.03869447484612465,
      "learning_rate": 0.00015365609348914859,
      "loss": 0.2999,
      "step": 699
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.05222214758396149,
      "learning_rate": 0.00015358931552587649,
      "loss": 0.2939,
      "step": 700
    },
    {
      "epoch": 1.1683333333333334,
      "grad_norm": 0.04712746664881706,
      "learning_rate": 0.00015352253756260436,
      "loss": 0.3363,
      "step": 701
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.04344676062464714,
      "learning_rate": 0.00015345575959933223,
      "loss": 0.2947,
      "step": 702
    },
    {
      "epoch": 1.1716666666666666,
      "grad_norm": 0.046429477632045746,
      "learning_rate": 0.0001533889816360601,
      "loss": 0.2927,
      "step": 703
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.044384684413671494,
      "learning_rate": 0.00015332220367278798,
      "loss": 0.2591,
      "step": 704
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.06318604201078415,
      "learning_rate": 0.00015325542570951588,
      "loss": 0.337,
      "step": 705
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.053392697125673294,
      "learning_rate": 0.00015318864774624375,
      "loss": 0.3509,
      "step": 706
    },
    {
      "epoch": 1.1783333333333332,
      "grad_norm": 0.03701367974281311,
      "learning_rate": 0.00015312186978297163,
      "loss": 0.2529,
      "step": 707
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.05953879654407501,
      "learning_rate": 0.0001530550918196995,
      "loss": 0.3747,
      "step": 708
    },
    {
      "epoch": 1.1816666666666666,
      "grad_norm": 0.058472611010074615,
      "learning_rate": 0.00015298831385642737,
      "loss": 0.3097,
      "step": 709
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.07187087833881378,
      "learning_rate": 0.00015292153589315527,
      "loss": 0.3592,
      "step": 710
    },
    {
      "epoch": 1.185,
      "grad_norm": 0.053101684898138046,
      "learning_rate": 0.00015285475792988315,
      "loss": 0.3345,
      "step": 711
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.050619129091501236,
      "learning_rate": 0.00015278797996661102,
      "loss": 0.3245,
      "step": 712
    },
    {
      "epoch": 1.1883333333333332,
      "grad_norm": 0.050112009048461914,
      "learning_rate": 0.0001527212020033389,
      "loss": 0.312,
      "step": 713
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.04445197060704231,
      "learning_rate": 0.0001526544240400668,
      "loss": 0.3328,
      "step": 714
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.04975873976945877,
      "learning_rate": 0.00015258764607679466,
      "loss": 0.3136,
      "step": 715
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.054833248257637024,
      "learning_rate": 0.00015252086811352257,
      "loss": 0.3345,
      "step": 716
    },
    {
      "epoch": 1.195,
      "grad_norm": 0.06073689088225365,
      "learning_rate": 0.00015245409015025044,
      "loss": 0.3557,
      "step": 717
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.048695940524339676,
      "learning_rate": 0.0001523873121869783,
      "loss": 0.3164,
      "step": 718
    },
    {
      "epoch": 1.1983333333333333,
      "grad_norm": 0.040197037160396576,
      "learning_rate": 0.00015232053422370618,
      "loss": 0.3443,
      "step": 719
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.0488058477640152,
      "learning_rate": 0.00015225375626043406,
      "loss": 0.3092,
      "step": 720
    },
    {
      "epoch": 1.2016666666666667,
      "grad_norm": 0.05434321239590645,
      "learning_rate": 0.00015218697829716196,
      "loss": 0.3358,
      "step": 721
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.057552069425582886,
      "learning_rate": 0.00015212020033388983,
      "loss": 0.3508,
      "step": 722
    },
    {
      "epoch": 1.205,
      "grad_norm": 0.06062345579266548,
      "learning_rate": 0.0001520534223706177,
      "loss": 0.3614,
      "step": 723
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.03837205469608307,
      "learning_rate": 0.00015198664440734558,
      "loss": 0.2514,
      "step": 724
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.05871514976024628,
      "learning_rate": 0.00015191986644407345,
      "loss": 0.3135,
      "step": 725
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.034936025738716125,
      "learning_rate": 0.00015185308848080135,
      "loss": 0.296,
      "step": 726
    },
    {
      "epoch": 1.2116666666666667,
      "grad_norm": 0.03742704913020134,
      "learning_rate": 0.00015178631051752922,
      "loss": 0.2351,
      "step": 727
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.07162391394376755,
      "learning_rate": 0.0001517195325542571,
      "loss": 0.3195,
      "step": 728
    },
    {
      "epoch": 1.215,
      "grad_norm": 0.043744172900915146,
      "learning_rate": 0.00015165275459098497,
      "loss": 0.2899,
      "step": 729
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.03992181643843651,
      "learning_rate": 0.00015158597662771284,
      "loss": 0.2675,
      "step": 730
    },
    {
      "epoch": 1.2183333333333333,
      "grad_norm": 0.05414244160056114,
      "learning_rate": 0.00015151919866444074,
      "loss": 0.3045,
      "step": 731
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.038938771933317184,
      "learning_rate": 0.00015145242070116862,
      "loss": 0.2826,
      "step": 732
    },
    {
      "epoch": 1.2216666666666667,
      "grad_norm": 0.06355774402618408,
      "learning_rate": 0.00015138564273789652,
      "loss": 0.3749,
      "step": 733
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.047413140535354614,
      "learning_rate": 0.0001513188647746244,
      "loss": 0.3002,
      "step": 734
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.04913008213043213,
      "learning_rate": 0.00015125208681135226,
      "loss": 0.3374,
      "step": 735
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.05188795179128647,
      "learning_rate": 0.00015118530884808014,
      "loss": 0.3244,
      "step": 736
    },
    {
      "epoch": 1.2283333333333333,
      "grad_norm": 0.06035169959068298,
      "learning_rate": 0.00015111853088480804,
      "loss": 0.3113,
      "step": 737
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.06492582708597183,
      "learning_rate": 0.0001510517529215359,
      "loss": 0.3551,
      "step": 738
    },
    {
      "epoch": 1.2316666666666667,
      "grad_norm": 0.0496777780354023,
      "learning_rate": 0.00015098497495826378,
      "loss": 0.2973,
      "step": 739
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.08312207460403442,
      "learning_rate": 0.00015091819699499166,
      "loss": 0.3171,
      "step": 740
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 0.055813297629356384,
      "learning_rate": 0.00015085141903171953,
      "loss": 0.3916,
      "step": 741
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 0.040186915546655655,
      "learning_rate": 0.00015078464106844743,
      "loss": 0.2706,
      "step": 742
    },
    {
      "epoch": 1.2383333333333333,
      "grad_norm": 0.05272645130753517,
      "learning_rate": 0.0001507178631051753,
      "loss": 0.3069,
      "step": 743
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.052250053733587265,
      "learning_rate": 0.00015065108514190318,
      "loss": 0.3607,
      "step": 744
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.059457927942276,
      "learning_rate": 0.00015058430717863105,
      "loss": 0.3299,
      "step": 745
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.04478736221790314,
      "learning_rate": 0.00015051752921535892,
      "loss": 0.317,
      "step": 746
    },
    {
      "epoch": 1.245,
      "grad_norm": 0.05439458787441254,
      "learning_rate": 0.0001504507512520868,
      "loss": 0.3492,
      "step": 747
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.04258972033858299,
      "learning_rate": 0.0001503839732888147,
      "loss": 0.2921,
      "step": 748
    },
    {
      "epoch": 1.2483333333333333,
      "grad_norm": 0.041337139904499054,
      "learning_rate": 0.00015031719532554257,
      "loss": 0.2635,
      "step": 749
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.08207235485315323,
      "learning_rate": 0.00015025041736227047,
      "loss": 0.4193,
      "step": 750
    },
    {
      "epoch": 1.2516666666666667,
      "grad_norm": 0.049493446946144104,
      "learning_rate": 0.00015018363939899834,
      "loss": 0.3559,
      "step": 751
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.04776306077837944,
      "learning_rate": 0.00015011686143572622,
      "loss": 0.3486,
      "step": 752
    },
    {
      "epoch": 1.255,
      "grad_norm": 0.04559142887592316,
      "learning_rate": 0.00015005008347245412,
      "loss": 0.2761,
      "step": 753
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.04893052950501442,
      "learning_rate": 0.000149983305509182,
      "loss": 0.3207,
      "step": 754
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.04281974956393242,
      "learning_rate": 0.00014991652754590986,
      "loss": 0.3255,
      "step": 755
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.04556006193161011,
      "learning_rate": 0.00014984974958263774,
      "loss": 0.2962,
      "step": 756
    },
    {
      "epoch": 1.2616666666666667,
      "grad_norm": 0.041101813316345215,
      "learning_rate": 0.0001497829716193656,
      "loss": 0.3404,
      "step": 757
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.052992042154073715,
      "learning_rate": 0.0001497161936560935,
      "loss": 0.3587,
      "step": 758
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 0.08488159626722336,
      "learning_rate": 0.00014964941569282138,
      "loss": 0.3768,
      "step": 759
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.05371224135160446,
      "learning_rate": 0.00014958263772954926,
      "loss": 0.3184,
      "step": 760
    },
    {
      "epoch": 1.2683333333333333,
      "grad_norm": 0.04836156219244003,
      "learning_rate": 0.00014951585976627713,
      "loss": 0.3076,
      "step": 761
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.04885965213179588,
      "learning_rate": 0.000149449081803005,
      "loss": 0.3324,
      "step": 762
    },
    {
      "epoch": 1.2716666666666667,
      "grad_norm": 0.04085482284426689,
      "learning_rate": 0.00014938230383973287,
      "loss": 0.3045,
      "step": 763
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.03918344900012016,
      "learning_rate": 0.00014931552587646077,
      "loss": 0.2725,
      "step": 764
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.053932107985019684,
      "learning_rate": 0.00014924874791318865,
      "loss": 0.3445,
      "step": 765
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.05748683959245682,
      "learning_rate": 0.00014918196994991652,
      "loss": 0.3655,
      "step": 766
    },
    {
      "epoch": 1.2783333333333333,
      "grad_norm": 0.03813568502664566,
      "learning_rate": 0.00014911519198664442,
      "loss": 0.2512,
      "step": 767
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.04042002931237221,
      "learning_rate": 0.0001490484140233723,
      "loss": 0.3028,
      "step": 768
    },
    {
      "epoch": 1.2816666666666667,
      "grad_norm": 0.036622658371925354,
      "learning_rate": 0.0001489816360601002,
      "loss": 0.2734,
      "step": 769
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.0436050221323967,
      "learning_rate": 0.00014891485809682807,
      "loss": 0.2971,
      "step": 770
    },
    {
      "epoch": 1.285,
      "grad_norm": 0.04095073044300079,
      "learning_rate": 0.00014884808013355594,
      "loss": 0.3093,
      "step": 771
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.054695144295692444,
      "learning_rate": 0.00014878130217028381,
      "loss": 0.3601,
      "step": 772
    },
    {
      "epoch": 1.2883333333333333,
      "grad_norm": 0.043455757200717926,
      "learning_rate": 0.0001487145242070117,
      "loss": 0.3178,
      "step": 773
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.04684720188379288,
      "learning_rate": 0.0001486477462437396,
      "loss": 0.2841,
      "step": 774
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.04243561998009682,
      "learning_rate": 0.00014858096828046746,
      "loss": 0.289,
      "step": 775
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.06325976550579071,
      "learning_rate": 0.00014851419031719533,
      "loss": 0.4244,
      "step": 776
    },
    {
      "epoch": 1.295,
      "grad_norm": 0.054326944053173065,
      "learning_rate": 0.0001484474123539232,
      "loss": 0.3071,
      "step": 777
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.04747593775391579,
      "learning_rate": 0.00014838063439065108,
      "loss": 0.366,
      "step": 778
    },
    {
      "epoch": 1.2983333333333333,
      "grad_norm": 0.04907405003905296,
      "learning_rate": 0.00014831385642737895,
      "loss": 0.3028,
      "step": 779
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.05792691558599472,
      "learning_rate": 0.00014824707846410685,
      "loss": 0.4048,
      "step": 780
    },
    {
      "epoch": 1.3016666666666667,
      "grad_norm": 0.05227367952466011,
      "learning_rate": 0.00014818030050083473,
      "loss": 0.3683,
      "step": 781
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.06667546182870865,
      "learning_rate": 0.0001481135225375626,
      "loss": 0.3757,
      "step": 782
    },
    {
      "epoch": 1.305,
      "grad_norm": 0.08497359603643417,
      "learning_rate": 0.00014804674457429047,
      "loss": 0.3357,
      "step": 783
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.046052612364292145,
      "learning_rate": 0.00014797996661101837,
      "loss": 0.231,
      "step": 784
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.05356629565358162,
      "learning_rate": 0.00014791318864774625,
      "loss": 0.3138,
      "step": 785
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.05236159265041351,
      "learning_rate": 0.00014784641068447415,
      "loss": 0.2895,
      "step": 786
    },
    {
      "epoch": 1.3116666666666665,
      "grad_norm": 0.04707653820514679,
      "learning_rate": 0.00014777963272120202,
      "loss": 0.2847,
      "step": 787
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.05783150717616081,
      "learning_rate": 0.0001477128547579299,
      "loss": 0.31,
      "step": 788
    },
    {
      "epoch": 1.315,
      "grad_norm": 0.05115664750337601,
      "learning_rate": 0.00014764607679465777,
      "loss": 0.3265,
      "step": 789
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.03781493380665779,
      "learning_rate": 0.00014757929883138567,
      "loss": 0.2625,
      "step": 790
    },
    {
      "epoch": 1.3183333333333334,
      "grad_norm": 0.06874945014715195,
      "learning_rate": 0.00014751252086811354,
      "loss": 0.3726,
      "step": 791
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.04975422844290733,
      "learning_rate": 0.0001474457429048414,
      "loss": 0.3299,
      "step": 792
    },
    {
      "epoch": 1.3216666666666668,
      "grad_norm": 0.03655277565121651,
      "learning_rate": 0.00014737896494156929,
      "loss": 0.2485,
      "step": 793
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.041052982211112976,
      "learning_rate": 0.00014731218697829716,
      "loss": 0.2951,
      "step": 794
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.08416272699832916,
      "learning_rate": 0.00014724540901502506,
      "loss": 0.3598,
      "step": 795
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.04242365062236786,
      "learning_rate": 0.00014717863105175293,
      "loss": 0.278,
      "step": 796
    },
    {
      "epoch": 1.3283333333333334,
      "grad_norm": 0.05480702593922615,
      "learning_rate": 0.0001471118530884808,
      "loss": 0.3561,
      "step": 797
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.07098352164030075,
      "learning_rate": 0.00014704507512520868,
      "loss": 0.3212,
      "step": 798
    },
    {
      "epoch": 1.3316666666666666,
      "grad_norm": 0.0520385205745697,
      "learning_rate": 0.00014697829716193655,
      "loss": 0.3883,
      "step": 799
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.038162343204021454,
      "learning_rate": 0.00014691151919866443,
      "loss": 0.2688,
      "step": 800
    },
    {
      "epoch": 1.335,
      "grad_norm": 0.05466436967253685,
      "learning_rate": 0.00014684474123539233,
      "loss": 0.3429,
      "step": 801
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.0466240718960762,
      "learning_rate": 0.0001467779632721202,
      "loss": 0.2699,
      "step": 802
    },
    {
      "epoch": 1.3383333333333334,
      "grad_norm": 0.04729887843132019,
      "learning_rate": 0.0001467111853088481,
      "loss": 0.3133,
      "step": 803
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.07128466665744781,
      "learning_rate": 0.00014664440734557597,
      "loss": 0.2994,
      "step": 804
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.04227706417441368,
      "learning_rate": 0.00014657762938230385,
      "loss": 0.2985,
      "step": 805
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.07957490533590317,
      "learning_rate": 0.00014651085141903175,
      "loss": 0.3257,
      "step": 806
    },
    {
      "epoch": 1.345,
      "grad_norm": 0.06762976944446564,
      "learning_rate": 0.00014644407345575962,
      "loss": 0.3764,
      "step": 807
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.061814580112695694,
      "learning_rate": 0.0001463772954924875,
      "loss": 0.3156,
      "step": 808
    },
    {
      "epoch": 1.3483333333333334,
      "grad_norm": 0.039647918194532394,
      "learning_rate": 0.00014631051752921536,
      "loss": 0.2927,
      "step": 809
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.042904578149318695,
      "learning_rate": 0.00014624373956594324,
      "loss": 0.3298,
      "step": 810
    },
    {
      "epoch": 1.3516666666666666,
      "grad_norm": 0.05725293606519699,
      "learning_rate": 0.00014617696160267114,
      "loss": 0.3172,
      "step": 811
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.05383605509996414,
      "learning_rate": 0.000146110183639399,
      "loss": 0.3196,
      "step": 812
    },
    {
      "epoch": 1.355,
      "grad_norm": 0.03727524355053902,
      "learning_rate": 0.00014604340567612688,
      "loss": 0.294,
      "step": 813
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 0.04082087054848671,
      "learning_rate": 0.00014597662771285476,
      "loss": 0.3092,
      "step": 814
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.053316593170166016,
      "learning_rate": 0.00014590984974958263,
      "loss": 0.3455,
      "step": 815
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.04215686023235321,
      "learning_rate": 0.0001458430717863105,
      "loss": 0.2891,
      "step": 816
    },
    {
      "epoch": 1.3616666666666668,
      "grad_norm": 0.04883541166782379,
      "learning_rate": 0.0001457762938230384,
      "loss": 0.3054,
      "step": 817
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.04452184960246086,
      "learning_rate": 0.00014570951585976628,
      "loss": 0.3251,
      "step": 818
    },
    {
      "epoch": 1.365,
      "grad_norm": 0.036817487329244614,
      "learning_rate": 0.00014564273789649415,
      "loss": 0.2761,
      "step": 819
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.05689091607928276,
      "learning_rate": 0.00014557595993322205,
      "loss": 0.3213,
      "step": 820
    },
    {
      "epoch": 1.3683333333333334,
      "grad_norm": 0.057630062103271484,
      "learning_rate": 0.00014550918196994992,
      "loss": 0.3112,
      "step": 821
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.04787573218345642,
      "learning_rate": 0.00014544240400667782,
      "loss": 0.3238,
      "step": 822
    },
    {
      "epoch": 1.3716666666666666,
      "grad_norm": 0.04322456568479538,
      "learning_rate": 0.0001453756260434057,
      "loss": 0.2784,
      "step": 823
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.050416603684425354,
      "learning_rate": 0.00014530884808013357,
      "loss": 0.317,
      "step": 824
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.07771429419517517,
      "learning_rate": 0.00014524207011686144,
      "loss": 0.3936,
      "step": 825
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.04893088340759277,
      "learning_rate": 0.00014517529215358932,
      "loss": 0.3228,
      "step": 826
    },
    {
      "epoch": 1.3783333333333334,
      "grad_norm": 0.0668465793132782,
      "learning_rate": 0.00014510851419031722,
      "loss": 0.3677,
      "step": 827
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.05439887195825577,
      "learning_rate": 0.0001450417362270451,
      "loss": 0.3399,
      "step": 828
    },
    {
      "epoch": 1.3816666666666666,
      "grad_norm": 0.04045936465263367,
      "learning_rate": 0.00014497495826377296,
      "loss": 0.2706,
      "step": 829
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.034796737134456635,
      "learning_rate": 0.00014490818030050084,
      "loss": 0.2297,
      "step": 830
    },
    {
      "epoch": 1.385,
      "grad_norm": 0.07324884831905365,
      "learning_rate": 0.0001448414023372287,
      "loss": 0.422,
      "step": 831
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.04674148187041283,
      "learning_rate": 0.00014477462437395658,
      "loss": 0.2802,
      "step": 832
    },
    {
      "epoch": 1.3883333333333332,
      "grad_norm": 0.03933430835604668,
      "learning_rate": 0.00014470784641068448,
      "loss": 0.2456,
      "step": 833
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.06884127855300903,
      "learning_rate": 0.00014464106844741236,
      "loss": 0.382,
      "step": 834
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.04725079983472824,
      "learning_rate": 0.00014457429048414023,
      "loss": 0.2561,
      "step": 835
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.039300739765167236,
      "learning_rate": 0.0001445075125208681,
      "loss": 0.2601,
      "step": 836
    },
    {
      "epoch": 1.395,
      "grad_norm": 0.07430903613567352,
      "learning_rate": 0.000144440734557596,
      "loss": 0.3424,
      "step": 837
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.0780021995306015,
      "learning_rate": 0.00014437395659432388,
      "loss": 0.3018,
      "step": 838
    },
    {
      "epoch": 1.3983333333333334,
      "grad_norm": 0.03458559140563011,
      "learning_rate": 0.00014430717863105178,
      "loss": 0.2922,
      "step": 839
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.03931190446019173,
      "learning_rate": 0.00014424040066777965,
      "loss": 0.2629,
      "step": 840
    },
    {
      "epoch": 1.4016666666666666,
      "grad_norm": 0.046334754675626755,
      "learning_rate": 0.00014417362270450752,
      "loss": 0.3368,
      "step": 841
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.06603558361530304,
      "learning_rate": 0.0001441068447412354,
      "loss": 0.3293,
      "step": 842
    },
    {
      "epoch": 1.405,
      "grad_norm": 0.042271047830581665,
      "learning_rate": 0.0001440400667779633,
      "loss": 0.3043,
      "step": 843
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.04367033392190933,
      "learning_rate": 0.00014397328881469117,
      "loss": 0.317,
      "step": 844
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.0427095927298069,
      "learning_rate": 0.00014390651085141904,
      "loss": 0.3325,
      "step": 845
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.04340110719203949,
      "learning_rate": 0.00014383973288814692,
      "loss": 0.3019,
      "step": 846
    },
    {
      "epoch": 1.4116666666666666,
      "grad_norm": 0.0474495030939579,
      "learning_rate": 0.0001437729549248748,
      "loss": 0.2916,
      "step": 847
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.057868655771017075,
      "learning_rate": 0.00014370617696160266,
      "loss": 0.2678,
      "step": 848
    },
    {
      "epoch": 1.415,
      "grad_norm": 0.05852421000599861,
      "learning_rate": 0.00014363939899833056,
      "loss": 0.346,
      "step": 849
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.03947846591472626,
      "learning_rate": 0.00014357262103505844,
      "loss": 0.2712,
      "step": 850
    },
    {
      "epoch": 1.4183333333333334,
      "grad_norm": 0.04878981411457062,
      "learning_rate": 0.0001435058430717863,
      "loss": 0.2994,
      "step": 851
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.040267035365104675,
      "learning_rate": 0.00014343906510851418,
      "loss": 0.3163,
      "step": 852
    },
    {
      "epoch": 1.4216666666666666,
      "grad_norm": 0.03372003138065338,
      "learning_rate": 0.00014337228714524205,
      "loss": 0.2364,
      "step": 853
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.046071574091911316,
      "learning_rate": 0.00014330550918196995,
      "loss": 0.3244,
      "step": 854
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.07180870324373245,
      "learning_rate": 0.00014323873121869783,
      "loss": 0.4465,
      "step": 855
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.04414349049329758,
      "learning_rate": 0.00014317195325542573,
      "loss": 0.3168,
      "step": 856
    },
    {
      "epoch": 1.4283333333333332,
      "grad_norm": 0.046280134469270706,
      "learning_rate": 0.0001431051752921536,
      "loss": 0.2685,
      "step": 857
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.06045612692832947,
      "learning_rate": 0.00014303839732888147,
      "loss": 0.3512,
      "step": 858
    },
    {
      "epoch": 1.4316666666666666,
      "grad_norm": 0.06843306124210358,
      "learning_rate": 0.00014297161936560937,
      "loss": 0.3599,
      "step": 859
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.04307536408305168,
      "learning_rate": 0.00014290484140233725,
      "loss": 0.2431,
      "step": 860
    },
    {
      "epoch": 1.435,
      "grad_norm": 0.04636266827583313,
      "learning_rate": 0.00014283806343906512,
      "loss": 0.3456,
      "step": 861
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.06852729618549347,
      "learning_rate": 0.000142771285475793,
      "loss": 0.3909,
      "step": 862
    },
    {
      "epoch": 1.4383333333333335,
      "grad_norm": 0.049584850668907166,
      "learning_rate": 0.00014270450751252087,
      "loss": 0.3395,
      "step": 863
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.04812886193394661,
      "learning_rate": 0.00014263772954924874,
      "loss": 0.3646,
      "step": 864
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.04151337966322899,
      "learning_rate": 0.00014257095158597664,
      "loss": 0.3284,
      "step": 865
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 0.03223811835050583,
      "learning_rate": 0.00014250417362270451,
      "loss": 0.2744,
      "step": 866
    },
    {
      "epoch": 1.445,
      "grad_norm": 0.04047216475009918,
      "learning_rate": 0.0001424373956594324,
      "loss": 0.2553,
      "step": 867
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.043886639177799225,
      "learning_rate": 0.00014237061769616026,
      "loss": 0.2993,
      "step": 868
    },
    {
      "epoch": 1.4483333333333333,
      "grad_norm": 0.05120173469185829,
      "learning_rate": 0.00014230383973288813,
      "loss": 0.3368,
      "step": 869
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.04859189689159393,
      "learning_rate": 0.00014223706176961603,
      "loss": 0.3288,
      "step": 870
    },
    {
      "epoch": 1.4516666666666667,
      "grad_norm": 0.04985019192099571,
      "learning_rate": 0.0001421702838063439,
      "loss": 0.3306,
      "step": 871
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.045380111783742905,
      "learning_rate": 0.00014210350584307178,
      "loss": 0.3145,
      "step": 872
    },
    {
      "epoch": 1.455,
      "grad_norm": 0.06482722610235214,
      "learning_rate": 0.00014203672787979968,
      "loss": 0.3764,
      "step": 873
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.07491414994001389,
      "learning_rate": 0.00014196994991652755,
      "loss": 0.313,
      "step": 874
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.046655621379613876,
      "learning_rate": 0.00014190317195325545,
      "loss": 0.3219,
      "step": 875
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.0647640973329544,
      "learning_rate": 0.00014183639398998333,
      "loss": 0.3129,
      "step": 876
    },
    {
      "epoch": 1.4616666666666667,
      "grad_norm": 0.0445483922958374,
      "learning_rate": 0.0001417696160267112,
      "loss": 0.3285,
      "step": 877
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.046954866498708725,
      "learning_rate": 0.00014170283806343907,
      "loss": 0.3179,
      "step": 878
    },
    {
      "epoch": 1.465,
      "grad_norm": 0.04769301414489746,
      "learning_rate": 0.00014163606010016695,
      "loss": 0.2987,
      "step": 879
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.045295119285583496,
      "learning_rate": 0.00014156928213689482,
      "loss": 0.287,
      "step": 880
    },
    {
      "epoch": 1.4683333333333333,
      "grad_norm": 0.04727170988917351,
      "learning_rate": 0.00014150250417362272,
      "loss": 0.3451,
      "step": 881
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.045966386795043945,
      "learning_rate": 0.0001414357262103506,
      "loss": 0.2792,
      "step": 882
    },
    {
      "epoch": 1.4716666666666667,
      "grad_norm": 0.05901378393173218,
      "learning_rate": 0.00014136894824707847,
      "loss": 0.3172,
      "step": 883
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.05820878967642784,
      "learning_rate": 0.00014130217028380634,
      "loss": 0.2884,
      "step": 884
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.07530195266008377,
      "learning_rate": 0.0001412353923205342,
      "loss": 0.4522,
      "step": 885
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.06534326076507568,
      "learning_rate": 0.0001411686143572621,
      "loss": 0.313,
      "step": 886
    },
    {
      "epoch": 1.4783333333333333,
      "grad_norm": 0.0510396845638752,
      "learning_rate": 0.00014110183639398999,
      "loss": 0.3392,
      "step": 887
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.03537813201546669,
      "learning_rate": 0.00014103505843071786,
      "loss": 0.2205,
      "step": 888
    },
    {
      "epoch": 1.4816666666666667,
      "grad_norm": 0.0505184605717659,
      "learning_rate": 0.00014096828046744576,
      "loss": 0.3194,
      "step": 889
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.04588684067130089,
      "learning_rate": 0.00014090150250417363,
      "loss": 0.3088,
      "step": 890
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 0.05029069259762764,
      "learning_rate": 0.00014083472454090153,
      "loss": 0.3679,
      "step": 891
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.042831696569919586,
      "learning_rate": 0.0001407679465776294,
      "loss": 0.3141,
      "step": 892
    },
    {
      "epoch": 1.4883333333333333,
      "grad_norm": 0.05055160075426102,
      "learning_rate": 0.00014070116861435728,
      "loss": 0.3077,
      "step": 893
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.0531141497194767,
      "learning_rate": 0.00014063439065108515,
      "loss": 0.2193,
      "step": 894
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.04216708242893219,
      "learning_rate": 0.00014056761268781303,
      "loss": 0.292,
      "step": 895
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.04327073320746422,
      "learning_rate": 0.0001405008347245409,
      "loss": 0.2168,
      "step": 896
    },
    {
      "epoch": 1.495,
      "grad_norm": 0.04799102246761322,
      "learning_rate": 0.0001404340567612688,
      "loss": 0.3308,
      "step": 897
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.06827569007873535,
      "learning_rate": 0.00014036727879799667,
      "loss": 0.3433,
      "step": 898
    },
    {
      "epoch": 1.4983333333333333,
      "grad_norm": 0.05772053450345993,
      "learning_rate": 0.00014030050083472454,
      "loss": 0.3023,
      "step": 899
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.032591111958026886,
      "learning_rate": 0.00014023372287145242,
      "loss": 0.228,
      "step": 900
    },
    {
      "epoch": 1.5016666666666667,
      "grad_norm": 0.08502249419689178,
      "learning_rate": 0.0001401669449081803,
      "loss": 0.3983,
      "step": 901
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.04612479731440544,
      "learning_rate": 0.0001401001669449082,
      "loss": 0.2727,
      "step": 902
    },
    {
      "epoch": 1.505,
      "grad_norm": 0.0466214157640934,
      "learning_rate": 0.00014003338898163606,
      "loss": 0.3075,
      "step": 903
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.06671636551618576,
      "learning_rate": 0.00013996661101836394,
      "loss": 0.415,
      "step": 904
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.05804523453116417,
      "learning_rate": 0.0001398998330550918,
      "loss": 0.295,
      "step": 905
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.050621435046195984,
      "learning_rate": 0.0001398330550918197,
      "loss": 0.355,
      "step": 906
    },
    {
      "epoch": 1.5116666666666667,
      "grad_norm": 0.05810292810201645,
      "learning_rate": 0.00013976627712854758,
      "loss": 0.3699,
      "step": 907
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.06873875856399536,
      "learning_rate": 0.00013969949916527548,
      "loss": 0.4122,
      "step": 908
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.03874743729829788,
      "learning_rate": 0.00013963272120200336,
      "loss": 0.2893,
      "step": 909
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.055813420563936234,
      "learning_rate": 0.00013956594323873123,
      "loss": 0.3964,
      "step": 910
    },
    {
      "epoch": 1.5183333333333333,
      "grad_norm": 0.07139451056718826,
      "learning_rate": 0.0001394991652754591,
      "loss": 0.2981,
      "step": 911
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.044913649559020996,
      "learning_rate": 0.00013943238731218698,
      "loss": 0.3106,
      "step": 912
    },
    {
      "epoch": 1.5216666666666665,
      "grad_norm": 0.04596996307373047,
      "learning_rate": 0.00013936560934891488,
      "loss": 0.3384,
      "step": 913
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.05136718600988388,
      "learning_rate": 0.00013929883138564275,
      "loss": 0.3476,
      "step": 914
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.0747559443116188,
      "learning_rate": 0.00013923205342237062,
      "loss": 0.3849,
      "step": 915
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.05316942185163498,
      "learning_rate": 0.0001391652754590985,
      "loss": 0.2951,
      "step": 916
    },
    {
      "epoch": 1.5283333333333333,
      "grad_norm": 0.061886005103588104,
      "learning_rate": 0.00013909849749582637,
      "loss": 0.3757,
      "step": 917
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.04856758564710617,
      "learning_rate": 0.00013903171953255427,
      "loss": 0.3222,
      "step": 918
    },
    {
      "epoch": 1.5316666666666667,
      "grad_norm": 0.07645608484745026,
      "learning_rate": 0.00013896494156928214,
      "loss": 0.3277,
      "step": 919
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.06772875785827637,
      "learning_rate": 0.00013889816360601002,
      "loss": 0.4009,
      "step": 920
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 0.0757509395480156,
      "learning_rate": 0.0001388313856427379,
      "loss": 0.4035,
      "step": 921
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.05501822009682655,
      "learning_rate": 0.00013876460767946576,
      "loss": 0.3308,
      "step": 922
    },
    {
      "epoch": 1.5383333333333333,
      "grad_norm": 0.043375641107559204,
      "learning_rate": 0.00013869782971619366,
      "loss": 0.3036,
      "step": 923
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.07038883119821548,
      "learning_rate": 0.00013863105175292154,
      "loss": 0.3607,
      "step": 924
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.058588892221450806,
      "learning_rate": 0.00013856427378964944,
      "loss": 0.338,
      "step": 925
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 0.05313384532928467,
      "learning_rate": 0.0001384974958263773,
      "loss": 0.3626,
      "step": 926
    },
    {
      "epoch": 1.545,
      "grad_norm": 0.055786944925785065,
      "learning_rate": 0.00013843071786310518,
      "loss": 0.3001,
      "step": 927
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.09075192362070084,
      "learning_rate": 0.00013836393989983308,
      "loss": 0.3753,
      "step": 928
    },
    {
      "epoch": 1.5483333333333333,
      "grad_norm": 0.04591156542301178,
      "learning_rate": 0.00013829716193656096,
      "loss": 0.3354,
      "step": 929
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.040177710354328156,
      "learning_rate": 0.00013823038397328883,
      "loss": 0.2631,
      "step": 930
    },
    {
      "epoch": 1.5516666666666667,
      "grad_norm": 0.05105746164917946,
      "learning_rate": 0.0001381636060100167,
      "loss": 0.3158,
      "step": 931
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.0577065646648407,
      "learning_rate": 0.00013809682804674458,
      "loss": 0.3233,
      "step": 932
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 0.04383842274546623,
      "learning_rate": 0.00013803005008347245,
      "loss": 0.2954,
      "step": 933
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.04711417853832245,
      "learning_rate": 0.00013796327212020035,
      "loss": 0.3059,
      "step": 934
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.0515235997736454,
      "learning_rate": 0.00013789649415692822,
      "loss": 0.3368,
      "step": 935
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05888603627681732,
      "learning_rate": 0.0001378297161936561,
      "loss": 0.3381,
      "step": 936
    },
    {
      "epoch": 1.5616666666666665,
      "grad_norm": 0.05350120738148689,
      "learning_rate": 0.00013776293823038397,
      "loss": 0.3531,
      "step": 937
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.04944382235407829,
      "learning_rate": 0.00013769616026711184,
      "loss": 0.3821,
      "step": 938
    },
    {
      "epoch": 1.565,
      "grad_norm": 0.04268326237797737,
      "learning_rate": 0.00013762938230383974,
      "loss": 0.3095,
      "step": 939
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.05493771657347679,
      "learning_rate": 0.00013756260434056762,
      "loss": 0.3839,
      "step": 940
    },
    {
      "epoch": 1.5683333333333334,
      "grad_norm": 0.07032311707735062,
      "learning_rate": 0.0001374958263772955,
      "loss": 0.3824,
      "step": 941
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.05139169842004776,
      "learning_rate": 0.0001374290484140234,
      "loss": 0.2681,
      "step": 942
    },
    {
      "epoch": 1.5716666666666668,
      "grad_norm": 0.05202993005514145,
      "learning_rate": 0.00013736227045075126,
      "loss": 0.3322,
      "step": 943
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.04159674420952797,
      "learning_rate": 0.00013729549248747916,
      "loss": 0.2975,
      "step": 944
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.06989212334156036,
      "learning_rate": 0.00013722871452420704,
      "loss": 0.3638,
      "step": 945
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.05473538860678673,
      "learning_rate": 0.0001371619365609349,
      "loss": 0.2648,
      "step": 946
    },
    {
      "epoch": 1.5783333333333334,
      "grad_norm": 0.0342022143304348,
      "learning_rate": 0.00013709515859766278,
      "loss": 0.2865,
      "step": 947
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.042526617646217346,
      "learning_rate": 0.00013702838063439065,
      "loss": 0.2357,
      "step": 948
    },
    {
      "epoch": 1.5816666666666666,
      "grad_norm": 0.049015387892723083,
      "learning_rate": 0.00013696160267111853,
      "loss": 0.3125,
      "step": 949
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.030944352969527245,
      "learning_rate": 0.00013689482470784643,
      "loss": 0.2528,
      "step": 950
    },
    {
      "epoch": 1.585,
      "grad_norm": 0.04372309148311615,
      "learning_rate": 0.0001368280467445743,
      "loss": 0.3103,
      "step": 951
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.04487812891602516,
      "learning_rate": 0.00013676126878130217,
      "loss": 0.3224,
      "step": 952
    },
    {
      "epoch": 1.5883333333333334,
      "grad_norm": 0.0446169339120388,
      "learning_rate": 0.00013669449081803005,
      "loss": 0.3361,
      "step": 953
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.048597756773233414,
      "learning_rate": 0.00013662771285475792,
      "loss": 0.3359,
      "step": 954
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.044693704694509506,
      "learning_rate": 0.00013656093489148582,
      "loss": 0.3239,
      "step": 955
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.041537608951330185,
      "learning_rate": 0.0001364941569282137,
      "loss": 0.2646,
      "step": 956
    },
    {
      "epoch": 1.595,
      "grad_norm": 0.042723800987005234,
      "learning_rate": 0.00013642737896494157,
      "loss": 0.3339,
      "step": 957
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.05692213773727417,
      "learning_rate": 0.00013636060100166944,
      "loss": 0.3254,
      "step": 958
    },
    {
      "epoch": 1.5983333333333334,
      "grad_norm": 0.04255099222064018,
      "learning_rate": 0.00013629382303839734,
      "loss": 0.3515,
      "step": 959
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.06511254608631134,
      "learning_rate": 0.00013622704507512521,
      "loss": 0.3308,
      "step": 960
    },
    {
      "epoch": 1.6016666666666666,
      "grad_norm": 0.09750804305076599,
      "learning_rate": 0.00013616026711185311,
      "loss": 0.3096,
      "step": 961
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.05046100914478302,
      "learning_rate": 0.000136093489148581,
      "loss": 0.3334,
      "step": 962
    },
    {
      "epoch": 1.605,
      "grad_norm": 0.04997367039322853,
      "learning_rate": 0.00013602671118530886,
      "loss": 0.3465,
      "step": 963
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.03947768732905388,
      "learning_rate": 0.00013595993322203673,
      "loss": 0.2854,
      "step": 964
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.05549127236008644,
      "learning_rate": 0.0001358931552587646,
      "loss": 0.3037,
      "step": 965
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.04163867607712746,
      "learning_rate": 0.0001358263772954925,
      "loss": 0.3209,
      "step": 966
    },
    {
      "epoch": 1.6116666666666668,
      "grad_norm": 0.05413821339607239,
      "learning_rate": 0.00013575959933222038,
      "loss": 0.3436,
      "step": 967
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.07838261872529984,
      "learning_rate": 0.00013569282136894825,
      "loss": 0.3963,
      "step": 968
    },
    {
      "epoch": 1.615,
      "grad_norm": 0.04193643108010292,
      "learning_rate": 0.00013562604340567613,
      "loss": 0.2631,
      "step": 969
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.048738691955804825,
      "learning_rate": 0.000135559265442404,
      "loss": 0.2688,
      "step": 970
    },
    {
      "epoch": 1.6183333333333332,
      "grad_norm": 0.0459895133972168,
      "learning_rate": 0.0001354924874791319,
      "loss": 0.3084,
      "step": 971
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.062209729105234146,
      "learning_rate": 0.00013542570951585977,
      "loss": 0.3037,
      "step": 972
    },
    {
      "epoch": 1.6216666666666666,
      "grad_norm": 0.04098627343773842,
      "learning_rate": 0.00013535893155258765,
      "loss": 0.2495,
      "step": 973
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.06052949279546738,
      "learning_rate": 0.00013529215358931552,
      "loss": 0.3883,
      "step": 974
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.058028850704431534,
      "learning_rate": 0.0001352253756260434,
      "loss": 0.3131,
      "step": 975
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.037967611104249954,
      "learning_rate": 0.0001351585976627713,
      "loss": 0.258,
      "step": 976
    },
    {
      "epoch": 1.6283333333333334,
      "grad_norm": 0.04722531884908676,
      "learning_rate": 0.00013509181969949917,
      "loss": 0.2778,
      "step": 977
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.044648781418800354,
      "learning_rate": 0.00013502504173622707,
      "loss": 0.3343,
      "step": 978
    },
    {
      "epoch": 1.6316666666666668,
      "grad_norm": 0.03678503260016441,
      "learning_rate": 0.00013495826377295494,
      "loss": 0.2565,
      "step": 979
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.039121415466070175,
      "learning_rate": 0.0001348914858096828,
      "loss": 0.2623,
      "step": 980
    },
    {
      "epoch": 1.635,
      "grad_norm": 0.04092656075954437,
      "learning_rate": 0.00013482470784641069,
      "loss": 0.3092,
      "step": 981
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.051251523196697235,
      "learning_rate": 0.00013475792988313859,
      "loss": 0.346,
      "step": 982
    },
    {
      "epoch": 1.6383333333333332,
      "grad_norm": 0.04310860484838486,
      "learning_rate": 0.00013469115191986646,
      "loss": 0.3074,
      "step": 983
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.047086965292692184,
      "learning_rate": 0.00013462437395659433,
      "loss": 0.3179,
      "step": 984
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.0584566593170166,
      "learning_rate": 0.0001345575959933222,
      "loss": 0.3475,
      "step": 985
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.05166751146316528,
      "learning_rate": 0.00013449081803005008,
      "loss": 0.35,
      "step": 986
    },
    {
      "epoch": 1.645,
      "grad_norm": 0.04174710810184479,
      "learning_rate": 0.00013442404006677798,
      "loss": 0.3101,
      "step": 987
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.06651616096496582,
      "learning_rate": 0.00013435726210350585,
      "loss": 0.3081,
      "step": 988
    },
    {
      "epoch": 1.6483333333333334,
      "grad_norm": 0.04170955717563629,
      "learning_rate": 0.00013429048414023373,
      "loss": 0.2392,
      "step": 989
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.04214433580636978,
      "learning_rate": 0.0001342237061769616,
      "loss": 0.2838,
      "step": 990
    },
    {
      "epoch": 1.6516666666666666,
      "grad_norm": 0.05202667787671089,
      "learning_rate": 0.00013415692821368947,
      "loss": 0.3577,
      "step": 991
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.04032661393284798,
      "learning_rate": 0.00013409015025041737,
      "loss": 0.3128,
      "step": 992
    },
    {
      "epoch": 1.655,
      "grad_norm": 0.051851995289325714,
      "learning_rate": 0.00013402337228714524,
      "loss": 0.3386,
      "step": 993
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.0496482290327549,
      "learning_rate": 0.00013395659432387312,
      "loss": 0.3024,
      "step": 994
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.0371108278632164,
      "learning_rate": 0.00013388981636060102,
      "loss": 0.2869,
      "step": 995
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.03606151416897774,
      "learning_rate": 0.0001338230383973289,
      "loss": 0.2459,
      "step": 996
    },
    {
      "epoch": 1.6616666666666666,
      "grad_norm": 0.07311677932739258,
      "learning_rate": 0.00013375626043405676,
      "loss": 0.3692,
      "step": 997
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.03846399486064911,
      "learning_rate": 0.00013368948247078466,
      "loss": 0.2719,
      "step": 998
    },
    {
      "epoch": 1.665,
      "grad_norm": 0.04948924109339714,
      "learning_rate": 0.00013362270450751254,
      "loss": 0.3157,
      "step": 999
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.037923675030469894,
      "learning_rate": 0.0001335559265442404,
      "loss": 0.276,
      "step": 1000
    },
    {
      "epoch": 1.6683333333333334,
      "grad_norm": 0.06729485839605331,
      "learning_rate": 0.00013348914858096828,
      "loss": 0.3583,
      "step": 1001
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.0628759115934372,
      "learning_rate": 0.00013342237061769616,
      "loss": 0.3913,
      "step": 1002
    },
    {
      "epoch": 1.6716666666666666,
      "grad_norm": 0.053583502769470215,
      "learning_rate": 0.00013335559265442406,
      "loss": 0.3705,
      "step": 1003
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.046253591775894165,
      "learning_rate": 0.00013328881469115193,
      "loss": 0.3445,
      "step": 1004
    },
    {
      "epoch": 1.675,
      "grad_norm": 0.08141642808914185,
      "learning_rate": 0.0001332220367278798,
      "loss": 0.3728,
      "step": 1005
    },
    {
      "epoch": 1.6766666666666667,
      "grad_norm": 0.07604476064443588,
      "learning_rate": 0.00013315525876460768,
      "loss": 0.3716,
      "step": 1006
    },
    {
      "epoch": 1.6783333333333332,
      "grad_norm": 0.03778963163495064,
      "learning_rate": 0.00013308848080133555,
      "loss": 0.2796,
      "step": 1007
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.042032450437545776,
      "learning_rate": 0.00013302170283806345,
      "loss": 0.2925,
      "step": 1008
    },
    {
      "epoch": 1.6816666666666666,
      "grad_norm": 0.04012667387723923,
      "learning_rate": 0.00013295492487479132,
      "loss": 0.2715,
      "step": 1009
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.05352889746427536,
      "learning_rate": 0.0001328881469115192,
      "loss": 0.3461,
      "step": 1010
    },
    {
      "epoch": 1.685,
      "grad_norm": 0.044235631823539734,
      "learning_rate": 0.00013282136894824707,
      "loss": 0.3187,
      "step": 1011
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.04060358554124832,
      "learning_rate": 0.00013275459098497497,
      "loss": 0.2852,
      "step": 1012
    },
    {
      "epoch": 1.6883333333333335,
      "grad_norm": 0.04608463495969772,
      "learning_rate": 0.00013268781302170284,
      "loss": 0.3632,
      "step": 1013
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.05349704623222351,
      "learning_rate": 0.00013262103505843074,
      "loss": 0.3759,
      "step": 1014
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.05008327215909958,
      "learning_rate": 0.00013255425709515862,
      "loss": 0.3212,
      "step": 1015
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.028942150995135307,
      "learning_rate": 0.0001324874791318865,
      "loss": 0.2125,
      "step": 1016
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 0.05627622455358505,
      "learning_rate": 0.00013242070116861436,
      "loss": 0.2966,
      "step": 1017
    },
    {
      "epoch": 1.6966666666666668,
      "grad_norm": 0.03976047784090042,
      "learning_rate": 0.00013235392320534224,
      "loss": 0.2623,
      "step": 1018
    },
    {
      "epoch": 1.6983333333333333,
      "grad_norm": 0.04030706360936165,
      "learning_rate": 0.00013228714524207014,
      "loss": 0.2414,
      "step": 1019
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.05688543990254402,
      "learning_rate": 0.000132220367278798,
      "loss": 0.3208,
      "step": 1020
    },
    {
      "epoch": 1.7016666666666667,
      "grad_norm": 0.047575611621141434,
      "learning_rate": 0.00013215358931552588,
      "loss": 0.2781,
      "step": 1021
    },
    {
      "epoch": 1.7033333333333334,
      "grad_norm": 0.05373598635196686,
      "learning_rate": 0.00013208681135225376,
      "loss": 0.3775,
      "step": 1022
    },
    {
      "epoch": 1.705,
      "grad_norm": 0.04273916780948639,
      "learning_rate": 0.00013202003338898163,
      "loss": 0.2865,
      "step": 1023
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.05813917517662048,
      "learning_rate": 0.00013195325542570953,
      "loss": 0.3427,
      "step": 1024
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.06394652277231216,
      "learning_rate": 0.0001318864774624374,
      "loss": 0.3542,
      "step": 1025
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.04026031866669655,
      "learning_rate": 0.00013181969949916528,
      "loss": 0.3074,
      "step": 1026
    },
    {
      "epoch": 1.7116666666666667,
      "grad_norm": 0.03589266166090965,
      "learning_rate": 0.00013175292153589315,
      "loss": 0.2771,
      "step": 1027
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.049929939210414886,
      "learning_rate": 0.00013168614357262102,
      "loss": 0.3866,
      "step": 1028
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 0.04077513515949249,
      "learning_rate": 0.00013161936560934892,
      "loss": 0.2904,
      "step": 1029
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.0371658131480217,
      "learning_rate": 0.0001315525876460768,
      "loss": 0.2015,
      "step": 1030
    },
    {
      "epoch": 1.7183333333333333,
      "grad_norm": 0.046947360038757324,
      "learning_rate": 0.0001314858096828047,
      "loss": 0.2857,
      "step": 1031
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.06437688320875168,
      "learning_rate": 0.00013141903171953257,
      "loss": 0.3645,
      "step": 1032
    },
    {
      "epoch": 1.7216666666666667,
      "grad_norm": 0.053786635398864746,
      "learning_rate": 0.00013135225375626044,
      "loss": 0.3295,
      "step": 1033
    },
    {
      "epoch": 1.7233333333333334,
      "grad_norm": 0.06229240447282791,
      "learning_rate": 0.00013128547579298832,
      "loss": 0.3459,
      "step": 1034
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.04503043740987778,
      "learning_rate": 0.00013121869782971622,
      "loss": 0.3144,
      "step": 1035
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.0631893202662468,
      "learning_rate": 0.0001311519198664441,
      "loss": 0.3412,
      "step": 1036
    },
    {
      "epoch": 1.7283333333333335,
      "grad_norm": 0.054712895303964615,
      "learning_rate": 0.00013108514190317196,
      "loss": 0.348,
      "step": 1037
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.05043491721153259,
      "learning_rate": 0.00013101836393989983,
      "loss": 0.2999,
      "step": 1038
    },
    {
      "epoch": 1.7316666666666667,
      "grad_norm": 0.0500413253903389,
      "learning_rate": 0.0001309515859766277,
      "loss": 0.3012,
      "step": 1039
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.08589386940002441,
      "learning_rate": 0.0001308848080133556,
      "loss": 0.393,
      "step": 1040
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 0.05146476998925209,
      "learning_rate": 0.00013081803005008348,
      "loss": 0.3549,
      "step": 1041
    },
    {
      "epoch": 1.7366666666666668,
      "grad_norm": 0.052840061485767365,
      "learning_rate": 0.00013075125208681135,
      "loss": 0.345,
      "step": 1042
    },
    {
      "epoch": 1.7383333333333333,
      "grad_norm": 0.05032000318169594,
      "learning_rate": 0.00013068447412353923,
      "loss": 0.2806,
      "step": 1043
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.03866669535636902,
      "learning_rate": 0.0001306176961602671,
      "loss": 0.2678,
      "step": 1044
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 0.04533272981643677,
      "learning_rate": 0.00013055091819699497,
      "loss": 0.2913,
      "step": 1045
    },
    {
      "epoch": 1.7433333333333332,
      "grad_norm": 0.03810100629925728,
      "learning_rate": 0.00013048414023372287,
      "loss": 0.3111,
      "step": 1046
    },
    {
      "epoch": 1.745,
      "grad_norm": 0.05846405401825905,
      "learning_rate": 0.00013041736227045075,
      "loss": 0.3332,
      "step": 1047
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.07434669882059097,
      "learning_rate": 0.00013035058430717865,
      "loss": 0.3787,
      "step": 1048
    },
    {
      "epoch": 1.7483333333333333,
      "grad_norm": 0.053505972027778625,
      "learning_rate": 0.00013028380634390652,
      "loss": 0.3141,
      "step": 1049
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.03644069656729698,
      "learning_rate": 0.0001302170283806344,
      "loss": 0.2824,
      "step": 1050
    },
    {
      "epoch": 1.7516666666666667,
      "grad_norm": 0.06271888315677643,
      "learning_rate": 0.0001301502504173623,
      "loss": 0.3049,
      "step": 1051
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.04181337729096413,
      "learning_rate": 0.00013008347245409017,
      "loss": 0.2881,
      "step": 1052
    },
    {
      "epoch": 1.755,
      "grad_norm": 0.037730623036623,
      "learning_rate": 0.00013001669449081804,
      "loss": 0.2675,
      "step": 1053
    },
    {
      "epoch": 1.7566666666666668,
      "grad_norm": 0.08262412995100021,
      "learning_rate": 0.00012994991652754591,
      "loss": 0.3231,
      "step": 1054
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.04140356555581093,
      "learning_rate": 0.0001298831385642738,
      "loss": 0.267,
      "step": 1055
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.05467144772410393,
      "learning_rate": 0.0001298163606010017,
      "loss": 0.3021,
      "step": 1056
    },
    {
      "epoch": 1.7616666666666667,
      "grad_norm": 0.05465537682175636,
      "learning_rate": 0.00012974958263772956,
      "loss": 0.2948,
      "step": 1057
    },
    {
      "epoch": 1.7633333333333332,
      "grad_norm": 0.05109381675720215,
      "learning_rate": 0.00012968280467445743,
      "loss": 0.2796,
      "step": 1058
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 0.04681461676955223,
      "learning_rate": 0.0001296160267111853,
      "loss": 0.3565,
      "step": 1059
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.050567008554935455,
      "learning_rate": 0.00012954924874791318,
      "loss": 0.3976,
      "step": 1060
    },
    {
      "epoch": 1.7683333333333333,
      "grad_norm": 0.058165788650512695,
      "learning_rate": 0.00012948247078464108,
      "loss": 0.3255,
      "step": 1061
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.039803605526685715,
      "learning_rate": 0.00012941569282136895,
      "loss": 0.317,
      "step": 1062
    },
    {
      "epoch": 1.7716666666666665,
      "grad_norm": 0.042155761271715164,
      "learning_rate": 0.00012934891485809683,
      "loss": 0.3175,
      "step": 1063
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.037175532430410385,
      "learning_rate": 0.0001292821368948247,
      "loss": 0.2673,
      "step": 1064
    },
    {
      "epoch": 1.775,
      "grad_norm": 0.07222988456487656,
      "learning_rate": 0.0001292153589315526,
      "loss": 0.3578,
      "step": 1065
    },
    {
      "epoch": 1.7766666666666666,
      "grad_norm": 0.0403539314866066,
      "learning_rate": 0.00012914858096828047,
      "loss": 0.2678,
      "step": 1066
    },
    {
      "epoch": 1.7783333333333333,
      "grad_norm": 0.05393616110086441,
      "learning_rate": 0.00012908180300500837,
      "loss": 0.3511,
      "step": 1067
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.046882618218660355,
      "learning_rate": 0.00012901502504173625,
      "loss": 0.33,
      "step": 1068
    },
    {
      "epoch": 1.7816666666666667,
      "grad_norm": 0.03750748932361603,
      "learning_rate": 0.00012894824707846412,
      "loss": 0.2687,
      "step": 1069
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.03617999330163002,
      "learning_rate": 0.000128881469115192,
      "loss": 0.2617,
      "step": 1070
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 0.03936118632555008,
      "learning_rate": 0.00012881469115191987,
      "loss": 0.2933,
      "step": 1071
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.04972649738192558,
      "learning_rate": 0.00012874791318864777,
      "loss": 0.3582,
      "step": 1072
    },
    {
      "epoch": 1.7883333333333333,
      "grad_norm": 0.062223099172115326,
      "learning_rate": 0.00012868113522537564,
      "loss": 0.3844,
      "step": 1073
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.060599926859140396,
      "learning_rate": 0.0001286143572621035,
      "loss": 0.4029,
      "step": 1074
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.051546867936849594,
      "learning_rate": 0.00012854757929883139,
      "loss": 0.2903,
      "step": 1075
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.06320391595363617,
      "learning_rate": 0.00012848080133555926,
      "loss": 0.3899,
      "step": 1076
    },
    {
      "epoch": 1.795,
      "grad_norm": 0.033998556435108185,
      "learning_rate": 0.00012841402337228716,
      "loss": 0.244,
      "step": 1077
    },
    {
      "epoch": 1.7966666666666666,
      "grad_norm": 0.04409806430339813,
      "learning_rate": 0.00012834724540901503,
      "loss": 0.3124,
      "step": 1078
    },
    {
      "epoch": 1.7983333333333333,
      "grad_norm": 0.05397401377558708,
      "learning_rate": 0.0001282804674457429,
      "loss": 0.3262,
      "step": 1079
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.04816342890262604,
      "learning_rate": 0.00012821368948247078,
      "loss": 0.309,
      "step": 1080
    },
    {
      "epoch": 1.8016666666666667,
      "grad_norm": 0.03964848443865776,
      "learning_rate": 0.00012814691151919865,
      "loss": 0.3079,
      "step": 1081
    },
    {
      "epoch": 1.8033333333333332,
      "grad_norm": 0.041097044944763184,
      "learning_rate": 0.00012808013355592655,
      "loss": 0.2459,
      "step": 1082
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 0.055256959050893784,
      "learning_rate": 0.00012801335559265442,
      "loss": 0.3729,
      "step": 1083
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.04444698989391327,
      "learning_rate": 0.00012794657762938233,
      "loss": 0.3352,
      "step": 1084
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 0.0473179928958416,
      "learning_rate": 0.0001278797996661102,
      "loss": 0.3565,
      "step": 1085
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.060201678425073624,
      "learning_rate": 0.00012781302170283807,
      "loss": 0.3616,
      "step": 1086
    },
    {
      "epoch": 1.8116666666666665,
      "grad_norm": 0.046176470816135406,
      "learning_rate": 0.00012774624373956594,
      "loss": 0.3273,
      "step": 1087
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.04041766747832298,
      "learning_rate": 0.00012767946577629384,
      "loss": 0.2688,
      "step": 1088
    },
    {
      "epoch": 1.815,
      "grad_norm": 0.04602207988500595,
      "learning_rate": 0.00012761268781302172,
      "loss": 0.302,
      "step": 1089
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.0424186997115612,
      "learning_rate": 0.0001275459098497496,
      "loss": 0.3271,
      "step": 1090
    },
    {
      "epoch": 1.8183333333333334,
      "grad_norm": 0.045363474637269974,
      "learning_rate": 0.00012747913188647746,
      "loss": 0.3105,
      "step": 1091
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.05695577710866928,
      "learning_rate": 0.00012741235392320534,
      "loss": 0.3415,
      "step": 1092
    },
    {
      "epoch": 1.8216666666666668,
      "grad_norm": 0.03469161316752434,
      "learning_rate": 0.00012734557595993324,
      "loss": 0.2925,
      "step": 1093
    },
    {
      "epoch": 1.8233333333333333,
      "grad_norm": 0.0432279035449028,
      "learning_rate": 0.0001272787979966611,
      "loss": 0.3286,
      "step": 1094
    },
    {
      "epoch": 1.825,
      "grad_norm": 0.04450453445315361,
      "learning_rate": 0.00012721202003338898,
      "loss": 0.2256,
      "step": 1095
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.05200564116239548,
      "learning_rate": 0.00012714524207011686,
      "loss": 0.2749,
      "step": 1096
    },
    {
      "epoch": 1.8283333333333334,
      "grad_norm": 0.051189400255680084,
      "learning_rate": 0.00012707846410684473,
      "loss": 0.3486,
      "step": 1097
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.036846961826086044,
      "learning_rate": 0.0001270116861435726,
      "loss": 0.2347,
      "step": 1098
    },
    {
      "epoch": 1.8316666666666666,
      "grad_norm": 0.03655676916241646,
      "learning_rate": 0.0001269449081803005,
      "loss": 0.2953,
      "step": 1099
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.04654020816087723,
      "learning_rate": 0.00012687813021702838,
      "loss": 0.3467,
      "step": 1100
    },
    {
      "epoch": 1.835,
      "grad_norm": 0.0403098464012146,
      "learning_rate": 0.00012681135225375628,
      "loss": 0.2688,
      "step": 1101
    },
    {
      "epoch": 1.8366666666666667,
      "grad_norm": 0.050502561032772064,
      "learning_rate": 0.00012674457429048415,
      "loss": 0.3155,
      "step": 1102
    },
    {
      "epoch": 1.8383333333333334,
      "grad_norm": 0.06417331099510193,
      "learning_rate": 0.00012667779632721202,
      "loss": 0.3282,
      "step": 1103
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.06599336117506027,
      "learning_rate": 0.00012661101836393992,
      "loss": 0.4203,
      "step": 1104
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 0.05034000426530838,
      "learning_rate": 0.0001265442404006678,
      "loss": 0.3062,
      "step": 1105
    },
    {
      "epoch": 1.8433333333333333,
      "grad_norm": 0.052412789314985275,
      "learning_rate": 0.00012647746243739567,
      "loss": 0.3026,
      "step": 1106
    },
    {
      "epoch": 1.845,
      "grad_norm": 0.058007050305604935,
      "learning_rate": 0.00012641068447412354,
      "loss": 0.2552,
      "step": 1107
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.03514641150832176,
      "learning_rate": 0.00012634390651085142,
      "loss": 0.2337,
      "step": 1108
    },
    {
      "epoch": 1.8483333333333334,
      "grad_norm": 0.06032254546880722,
      "learning_rate": 0.00012627712854757932,
      "loss": 0.402,
      "step": 1109
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.038448065519332886,
      "learning_rate": 0.0001262103505843072,
      "loss": 0.276,
      "step": 1110
    },
    {
      "epoch": 1.8516666666666666,
      "grad_norm": 0.05786294490098953,
      "learning_rate": 0.00012614357262103506,
      "loss": 0.3557,
      "step": 1111
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.05790349841117859,
      "learning_rate": 0.00012607679465776294,
      "loss": 0.3282,
      "step": 1112
    },
    {
      "epoch": 1.855,
      "grad_norm": 0.04294292628765106,
      "learning_rate": 0.0001260100166944908,
      "loss": 0.2792,
      "step": 1113
    },
    {
      "epoch": 1.8566666666666667,
      "grad_norm": 0.05003895238041878,
      "learning_rate": 0.00012594323873121868,
      "loss": 0.3719,
      "step": 1114
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.03888378664851189,
      "learning_rate": 0.00012587646076794658,
      "loss": 0.2378,
      "step": 1115
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.05043719336390495,
      "learning_rate": 0.00012580968280467446,
      "loss": 0.303,
      "step": 1116
    },
    {
      "epoch": 1.8616666666666668,
      "grad_norm": 0.03904039412736893,
      "learning_rate": 0.00012574290484140233,
      "loss": 0.2895,
      "step": 1117
    },
    {
      "epoch": 1.8633333333333333,
      "grad_norm": 0.04182562232017517,
      "learning_rate": 0.00012567612687813023,
      "loss": 0.2637,
      "step": 1118
    },
    {
      "epoch": 1.865,
      "grad_norm": 0.04074758663773537,
      "learning_rate": 0.0001256093489148581,
      "loss": 0.2843,
      "step": 1119
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.02902400493621826,
      "learning_rate": 0.000125542570951586,
      "loss": 0.2089,
      "step": 1120
    },
    {
      "epoch": 1.8683333333333332,
      "grad_norm": 0.07069988548755646,
      "learning_rate": 0.00012547579298831388,
      "loss": 0.3593,
      "step": 1121
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.056055884808301926,
      "learning_rate": 0.00012540901502504175,
      "loss": 0.3478,
      "step": 1122
    },
    {
      "epoch": 1.8716666666666666,
      "grad_norm": 0.046870555728673935,
      "learning_rate": 0.00012534223706176962,
      "loss": 0.3217,
      "step": 1123
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.03961705043911934,
      "learning_rate": 0.0001252754590984975,
      "loss": 0.272,
      "step": 1124
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.04048635810613632,
      "learning_rate": 0.0001252086811352254,
      "loss": 0.2811,
      "step": 1125
    },
    {
      "epoch": 1.8766666666666667,
      "grad_norm": 0.047017309814691544,
      "learning_rate": 0.00012514190317195327,
      "loss": 0.304,
      "step": 1126
    },
    {
      "epoch": 1.8783333333333334,
      "grad_norm": 0.05114813148975372,
      "learning_rate": 0.00012507512520868114,
      "loss": 0.3211,
      "step": 1127
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.03841552510857582,
      "learning_rate": 0.00012500834724540902,
      "loss": 0.3041,
      "step": 1128
    },
    {
      "epoch": 1.8816666666666668,
      "grad_norm": 0.03635908663272858,
      "learning_rate": 0.0001249415692821369,
      "loss": 0.2734,
      "step": 1129
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.04458603635430336,
      "learning_rate": 0.00012487479131886476,
      "loss": 0.3067,
      "step": 1130
    },
    {
      "epoch": 1.885,
      "grad_norm": 0.04333452507853508,
      "learning_rate": 0.00012480801335559266,
      "loss": 0.2842,
      "step": 1131
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.02883961983025074,
      "learning_rate": 0.00012474123539232053,
      "loss": 0.2504,
      "step": 1132
    },
    {
      "epoch": 1.8883333333333332,
      "grad_norm": 0.04479227960109711,
      "learning_rate": 0.0001246744574290484,
      "loss": 0.3086,
      "step": 1133
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.05305534228682518,
      "learning_rate": 0.0001246076794657763,
      "loss": 0.3315,
      "step": 1134
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.0718669518828392,
      "learning_rate": 0.00012454090150250418,
      "loss": 0.4454,
      "step": 1135
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.05058874934911728,
      "learning_rate": 0.00012447412353923208,
      "loss": 0.3696,
      "step": 1136
    },
    {
      "epoch": 1.895,
      "grad_norm": 0.05493483319878578,
      "learning_rate": 0.00012440734557595995,
      "loss": 0.3405,
      "step": 1137
    },
    {
      "epoch": 1.8966666666666665,
      "grad_norm": 0.047928810119628906,
      "learning_rate": 0.00012434056761268783,
      "loss": 0.3121,
      "step": 1138
    },
    {
      "epoch": 1.8983333333333334,
      "grad_norm": 0.04084394872188568,
      "learning_rate": 0.0001242737896494157,
      "loss": 0.2626,
      "step": 1139
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.033703576773405075,
      "learning_rate": 0.00012420701168614357,
      "loss": 0.2532,
      "step": 1140
    },
    {
      "epoch": 1.9016666666666666,
      "grad_norm": 0.054389838129282,
      "learning_rate": 0.00012414023372287147,
      "loss": 0.3116,
      "step": 1141
    },
    {
      "epoch": 1.9033333333333333,
      "grad_norm": 0.06424488872289658,
      "learning_rate": 0.00012407345575959935,
      "loss": 0.2854,
      "step": 1142
    },
    {
      "epoch": 1.905,
      "grad_norm": 0.05081469938158989,
      "learning_rate": 0.00012400667779632722,
      "loss": 0.2871,
      "step": 1143
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.048271387815475464,
      "learning_rate": 0.0001239398998330551,
      "loss": 0.2618,
      "step": 1144
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 0.0423254556953907,
      "learning_rate": 0.00012387312186978297,
      "loss": 0.2979,
      "step": 1145
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.04164205491542816,
      "learning_rate": 0.00012380634390651084,
      "loss": 0.2951,
      "step": 1146
    },
    {
      "epoch": 1.9116666666666666,
      "grad_norm": 0.059152450412511826,
      "learning_rate": 0.00012373956594323874,
      "loss": 0.3737,
      "step": 1147
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.05186520516872406,
      "learning_rate": 0.00012367278797996661,
      "loss": 0.3757,
      "step": 1148
    },
    {
      "epoch": 1.915,
      "grad_norm": 0.05937480553984642,
      "learning_rate": 0.0001236060100166945,
      "loss": 0.3671,
      "step": 1149
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.03685033693909645,
      "learning_rate": 0.00012353923205342236,
      "loss": 0.2617,
      "step": 1150
    },
    {
      "epoch": 1.9183333333333334,
      "grad_norm": 0.03587542846798897,
      "learning_rate": 0.00012347245409015026,
      "loss": 0.2756,
      "step": 1151
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.05459021404385567,
      "learning_rate": 0.00012340567612687813,
      "loss": 0.3597,
      "step": 1152
    },
    {
      "epoch": 1.9216666666666666,
      "grad_norm": 0.050952520221471786,
      "learning_rate": 0.00012333889816360603,
      "loss": 0.2844,
      "step": 1153
    },
    {
      "epoch": 1.9233333333333333,
      "grad_norm": 0.06575113534927368,
      "learning_rate": 0.0001232721202003339,
      "loss": 0.2916,
      "step": 1154
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.050047725439071655,
      "learning_rate": 0.00012320534223706178,
      "loss": 0.3743,
      "step": 1155
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.0698772743344307,
      "learning_rate": 0.00012313856427378965,
      "loss": 0.3497,
      "step": 1156
    },
    {
      "epoch": 1.9283333333333332,
      "grad_norm": 0.05200282111763954,
      "learning_rate": 0.00012307178631051755,
      "loss": 0.3345,
      "step": 1157
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.05544343218207359,
      "learning_rate": 0.00012300500834724543,
      "loss": 0.3249,
      "step": 1158
    },
    {
      "epoch": 1.9316666666666666,
      "grad_norm": 0.04666020721197128,
      "learning_rate": 0.0001229382303839733,
      "loss": 0.3174,
      "step": 1159
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.06507077813148499,
      "learning_rate": 0.00012287145242070117,
      "loss": 0.3865,
      "step": 1160
    },
    {
      "epoch": 1.935,
      "grad_norm": 0.04382528364658356,
      "learning_rate": 0.00012280467445742905,
      "loss": 0.3343,
      "step": 1161
    },
    {
      "epoch": 1.9366666666666665,
      "grad_norm": 0.041210055351257324,
      "learning_rate": 0.00012273789649415692,
      "loss": 0.3005,
      "step": 1162
    },
    {
      "epoch": 1.9383333333333335,
      "grad_norm": 0.046642765402793884,
      "learning_rate": 0.00012267111853088482,
      "loss": 0.3086,
      "step": 1163
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.05490682274103165,
      "learning_rate": 0.0001226043405676127,
      "loss": 0.4,
      "step": 1164
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 0.04329514503479004,
      "learning_rate": 0.00012253756260434057,
      "loss": 0.3348,
      "step": 1165
    },
    {
      "epoch": 1.9433333333333334,
      "grad_norm": 0.05037299543619156,
      "learning_rate": 0.00012247078464106844,
      "loss": 0.2956,
      "step": 1166
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 0.02435540407896042,
      "learning_rate": 0.0001224040066777963,
      "loss": 0.1945,
      "step": 1167
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.05931873992085457,
      "learning_rate": 0.0001223372287145242,
      "loss": 0.3262,
      "step": 1168
    },
    {
      "epoch": 1.9483333333333333,
      "grad_norm": 0.03564140945672989,
      "learning_rate": 0.00012227045075125209,
      "loss": 0.2655,
      "step": 1169
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.04192393273115158,
      "learning_rate": 0.00012220367278797999,
      "loss": 0.2884,
      "step": 1170
    },
    {
      "epoch": 1.9516666666666667,
      "grad_norm": 0.07567590475082397,
      "learning_rate": 0.00012213689482470786,
      "loss": 0.3339,
      "step": 1171
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.04032636433839798,
      "learning_rate": 0.00012207011686143572,
      "loss": 0.3242,
      "step": 1172
    },
    {
      "epoch": 1.955,
      "grad_norm": 0.03501078113913536,
      "learning_rate": 0.00012200333889816362,
      "loss": 0.2415,
      "step": 1173
    },
    {
      "epoch": 1.9566666666666666,
      "grad_norm": 0.03502175956964493,
      "learning_rate": 0.00012193656093489149,
      "loss": 0.2791,
      "step": 1174
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.04669326916337013,
      "learning_rate": 0.00012186978297161938,
      "loss": 0.3513,
      "step": 1175
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.03518512472510338,
      "learning_rate": 0.00012180300500834725,
      "loss": 0.2499,
      "step": 1176
    },
    {
      "epoch": 1.9616666666666667,
      "grad_norm": 0.03784247860312462,
      "learning_rate": 0.00012173622704507512,
      "loss": 0.2596,
      "step": 1177
    },
    {
      "epoch": 1.9633333333333334,
      "grad_norm": 0.07321161776781082,
      "learning_rate": 0.00012166944908180303,
      "loss": 0.4059,
      "step": 1178
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 0.07648066431283951,
      "learning_rate": 0.0001216026711185309,
      "loss": 0.3981,
      "step": 1179
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.04671837389469147,
      "learning_rate": 0.00012153589315525877,
      "loss": 0.3058,
      "step": 1180
    },
    {
      "epoch": 1.9683333333333333,
      "grad_norm": 0.05132994428277016,
      "learning_rate": 0.00012146911519198664,
      "loss": 0.3086,
      "step": 1181
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.039940882474184036,
      "learning_rate": 0.00012140233722871452,
      "loss": 0.2736,
      "step": 1182
    },
    {
      "epoch": 1.9716666666666667,
      "grad_norm": 0.04608076065778732,
      "learning_rate": 0.0001213355592654424,
      "loss": 0.342,
      "step": 1183
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.039566028863191605,
      "learning_rate": 0.00012126878130217029,
      "loss": 0.2808,
      "step": 1184
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.03335586562752724,
      "learning_rate": 0.00012120200333889818,
      "loss": 0.2124,
      "step": 1185
    },
    {
      "epoch": 1.9766666666666666,
      "grad_norm": 0.03265528008341789,
      "learning_rate": 0.00012113522537562605,
      "loss": 0.2382,
      "step": 1186
    },
    {
      "epoch": 1.9783333333333335,
      "grad_norm": 0.048312555998563766,
      "learning_rate": 0.00012106844741235392,
      "loss": 0.3313,
      "step": 1187
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.0679377019405365,
      "learning_rate": 0.0001210016694490818,
      "loss": 0.3322,
      "step": 1188
    },
    {
      "epoch": 1.9816666666666667,
      "grad_norm": 0.054133232682943344,
      "learning_rate": 0.0001209348914858097,
      "loss": 0.2452,
      "step": 1189
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.06654620170593262,
      "learning_rate": 0.00012086811352253757,
      "loss": 0.3705,
      "step": 1190
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 0.057237282395362854,
      "learning_rate": 0.00012080133555926544,
      "loss": 0.3936,
      "step": 1191
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.05323536694049835,
      "learning_rate": 0.00012073455759599333,
      "loss": 0.3878,
      "step": 1192
    },
    {
      "epoch": 1.9883333333333333,
      "grad_norm": 0.053447891026735306,
      "learning_rate": 0.0001206677796327212,
      "loss": 0.3099,
      "step": 1193
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.05408481881022453,
      "learning_rate": 0.0001206010016694491,
      "loss": 0.3395,
      "step": 1194
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 0.04290888085961342,
      "learning_rate": 0.00012053422370617698,
      "loss": 0.2569,
      "step": 1195
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.05431116744875908,
      "learning_rate": 0.00012046744574290485,
      "loss": 0.323,
      "step": 1196
    },
    {
      "epoch": 1.995,
      "grad_norm": 0.04820726811885834,
      "learning_rate": 0.00012040066777963272,
      "loss": 0.2842,
      "step": 1197
    },
    {
      "epoch": 1.9966666666666666,
      "grad_norm": 0.043807029724121094,
      "learning_rate": 0.0001203338898163606,
      "loss": 0.3166,
      "step": 1198
    },
    {
      "epoch": 1.9983333333333333,
      "grad_norm": 0.0364585816860199,
      "learning_rate": 0.00012026711185308848,
      "loss": 0.2895,
      "step": 1199
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.0497480109333992,
      "learning_rate": 0.00012020033388981637,
      "loss": 0.3132,
      "step": 1200
    },
    {
      "epoch": 2.0016666666666665,
      "grad_norm": 0.04301776364445686,
      "learning_rate": 0.00012013355592654426,
      "loss": 0.2962,
      "step": 1201
    },
    {
      "epoch": 2.0033333333333334,
      "grad_norm": 0.04695621132850647,
      "learning_rate": 0.00012006677796327213,
      "loss": 0.3224,
      "step": 1202
    },
    {
      "epoch": 2.005,
      "grad_norm": 0.03328459709882736,
      "learning_rate": 0.00012,
      "loss": 0.2763,
      "step": 1203
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.04892897605895996,
      "learning_rate": 0.00011993322203672788,
      "loss": 0.2917,
      "step": 1204
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.09201797097921371,
      "learning_rate": 0.00011986644407345578,
      "loss": 0.4178,
      "step": 1205
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.07487814873456955,
      "learning_rate": 0.00011979966611018365,
      "loss": 0.3638,
      "step": 1206
    },
    {
      "epoch": 2.0116666666666667,
      "grad_norm": 0.03581713140010834,
      "learning_rate": 0.00011973288814691152,
      "loss": 0.2592,
      "step": 1207
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.04913928732275963,
      "learning_rate": 0.0001196661101836394,
      "loss": 0.286,
      "step": 1208
    },
    {
      "epoch": 2.015,
      "grad_norm": 0.05776914954185486,
      "learning_rate": 0.00011959933222036728,
      "loss": 0.3143,
      "step": 1209
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.05267548933625221,
      "learning_rate": 0.00011953255425709517,
      "loss": 0.2611,
      "step": 1210
    },
    {
      "epoch": 2.0183333333333335,
      "grad_norm": 0.046605128794908524,
      "learning_rate": 0.00011946577629382306,
      "loss": 0.268,
      "step": 1211
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.04790177568793297,
      "learning_rate": 0.00011939899833055093,
      "loss": 0.2693,
      "step": 1212
    },
    {
      "epoch": 2.0216666666666665,
      "grad_norm": 0.06168558448553085,
      "learning_rate": 0.0001193322203672788,
      "loss": 0.3532,
      "step": 1213
    },
    {
      "epoch": 2.0233333333333334,
      "grad_norm": 0.04422501474618912,
      "learning_rate": 0.00011926544240400668,
      "loss": 0.2868,
      "step": 1214
    },
    {
      "epoch": 2.025,
      "grad_norm": 0.04268654063344002,
      "learning_rate": 0.00011919866444073455,
      "loss": 0.3197,
      "step": 1215
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.050761036574840546,
      "learning_rate": 0.00011913188647746245,
      "loss": 0.3228,
      "step": 1216
    },
    {
      "epoch": 2.0283333333333333,
      "grad_norm": 0.06652382016181946,
      "learning_rate": 0.00011906510851419032,
      "loss": 0.3488,
      "step": 1217
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.061333417892456055,
      "learning_rate": 0.00011899833055091821,
      "loss": 0.3277,
      "step": 1218
    },
    {
      "epoch": 2.0316666666666667,
      "grad_norm": 0.053160157054662704,
      "learning_rate": 0.00011893155258764608,
      "loss": 0.2835,
      "step": 1219
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.0709669291973114,
      "learning_rate": 0.00011886477462437396,
      "loss": 0.3788,
      "step": 1220
    },
    {
      "epoch": 2.035,
      "grad_norm": 0.05168885737657547,
      "learning_rate": 0.00011879799666110186,
      "loss": 0.2915,
      "step": 1221
    },
    {
      "epoch": 2.0366666666666666,
      "grad_norm": 0.05158694460988045,
      "learning_rate": 0.00011873121869782973,
      "loss": 0.2606,
      "step": 1222
    },
    {
      "epoch": 2.038333333333333,
      "grad_norm": 0.05717436969280243,
      "learning_rate": 0.0001186644407345576,
      "loss": 0.3099,
      "step": 1223
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.05401004105806351,
      "learning_rate": 0.00011859766277128547,
      "loss": 0.2868,
      "step": 1224
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 0.05693448707461357,
      "learning_rate": 0.00011853088480801335,
      "loss": 0.2986,
      "step": 1225
    },
    {
      "epoch": 2.0433333333333334,
      "grad_norm": 0.06464631855487823,
      "learning_rate": 0.00011846410684474125,
      "loss": 0.338,
      "step": 1226
    },
    {
      "epoch": 2.045,
      "grad_norm": 0.04655960947275162,
      "learning_rate": 0.00011839732888146912,
      "loss": 0.3065,
      "step": 1227
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.051158301532268524,
      "learning_rate": 0.00011833055091819701,
      "loss": 0.2891,
      "step": 1228
    },
    {
      "epoch": 2.0483333333333333,
      "grad_norm": 0.04498038813471794,
      "learning_rate": 0.00011826377295492488,
      "loss": 0.3505,
      "step": 1229
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.041308749467134476,
      "learning_rate": 0.00011819699499165275,
      "loss": 0.2738,
      "step": 1230
    },
    {
      "epoch": 2.0516666666666667,
      "grad_norm": 0.036096833646297455,
      "learning_rate": 0.00011813021702838063,
      "loss": 0.2898,
      "step": 1231
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.05366828665137291,
      "learning_rate": 0.00011806343906510853,
      "loss": 0.338,
      "step": 1232
    },
    {
      "epoch": 2.055,
      "grad_norm": 0.069493867456913,
      "learning_rate": 0.0001179966611018364,
      "loss": 0.4065,
      "step": 1233
    },
    {
      "epoch": 2.0566666666666666,
      "grad_norm": 0.06753870844841003,
      "learning_rate": 0.00011792988313856427,
      "loss": 0.3368,
      "step": 1234
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.0371033139526844,
      "learning_rate": 0.00011786310517529216,
      "loss": 0.2654,
      "step": 1235
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.03463153541088104,
      "learning_rate": 0.00011779632721202003,
      "loss": 0.264,
      "step": 1236
    },
    {
      "epoch": 2.0616666666666665,
      "grad_norm": 0.055887047201395035,
      "learning_rate": 0.00011772954924874793,
      "loss": 0.3996,
      "step": 1237
    },
    {
      "epoch": 2.0633333333333335,
      "grad_norm": 0.04600432515144348,
      "learning_rate": 0.00011766277128547581,
      "loss": 0.2363,
      "step": 1238
    },
    {
      "epoch": 2.065,
      "grad_norm": 0.04478434473276138,
      "learning_rate": 0.00011759599332220368,
      "loss": 0.2813,
      "step": 1239
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.07549832761287689,
      "learning_rate": 0.00011752921535893155,
      "loss": 0.3249,
      "step": 1240
    },
    {
      "epoch": 2.0683333333333334,
      "grad_norm": 0.060893427580595016,
      "learning_rate": 0.00011746243739565943,
      "loss": 0.3581,
      "step": 1241
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.03600490093231201,
      "learning_rate": 0.00011739565943238733,
      "loss": 0.2735,
      "step": 1242
    },
    {
      "epoch": 2.0716666666666668,
      "grad_norm": 0.06484731286764145,
      "learning_rate": 0.0001173288814691152,
      "loss": 0.3156,
      "step": 1243
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.047721635550260544,
      "learning_rate": 0.00011726210350584307,
      "loss": 0.2556,
      "step": 1244
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.04867493361234665,
      "learning_rate": 0.00011719532554257096,
      "loss": 0.2812,
      "step": 1245
    },
    {
      "epoch": 2.0766666666666667,
      "grad_norm": 0.043749306350946426,
      "learning_rate": 0.00011712854757929883,
      "loss": 0.2899,
      "step": 1246
    },
    {
      "epoch": 2.078333333333333,
      "grad_norm": 0.07606405764818192,
      "learning_rate": 0.0001170617696160267,
      "loss": 0.434,
      "step": 1247
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.03311292082071304,
      "learning_rate": 0.0001169949916527546,
      "loss": 0.2225,
      "step": 1248
    },
    {
      "epoch": 2.0816666666666666,
      "grad_norm": 0.04637237638235092,
      "learning_rate": 0.00011692821368948248,
      "loss": 0.2598,
      "step": 1249
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.07954106479883194,
      "learning_rate": 0.00011686143572621035,
      "loss": 0.2734,
      "step": 1250
    },
    {
      "epoch": 2.085,
      "grad_norm": 0.03784738853573799,
      "learning_rate": 0.00011679465776293823,
      "loss": 0.28,
      "step": 1251
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.04352603852748871,
      "learning_rate": 0.00011672787979966611,
      "loss": 0.2822,
      "step": 1252
    },
    {
      "epoch": 2.0883333333333334,
      "grad_norm": 0.04250387102365494,
      "learning_rate": 0.000116661101836394,
      "loss": 0.2837,
      "step": 1253
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.04520426690578461,
      "learning_rate": 0.00011659432387312189,
      "loss": 0.2885,
      "step": 1254
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 0.05342428386211395,
      "learning_rate": 0.00011652754590984976,
      "loss": 0.3319,
      "step": 1255
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.053849637508392334,
      "learning_rate": 0.00011646076794657763,
      "loss": 0.3392,
      "step": 1256
    },
    {
      "epoch": 2.095,
      "grad_norm": 0.05088827759027481,
      "learning_rate": 0.0001163939899833055,
      "loss": 0.3065,
      "step": 1257
    },
    {
      "epoch": 2.0966666666666667,
      "grad_norm": 0.036561690270900726,
      "learning_rate": 0.0001163272120200334,
      "loss": 0.2581,
      "step": 1258
    },
    {
      "epoch": 2.098333333333333,
      "grad_norm": 0.05425052344799042,
      "learning_rate": 0.00011626043405676128,
      "loss": 0.394,
      "step": 1259
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.05010334029793739,
      "learning_rate": 0.00011619365609348915,
      "loss": 0.3283,
      "step": 1260
    },
    {
      "epoch": 2.1016666666666666,
      "grad_norm": 0.04656018316745758,
      "learning_rate": 0.00011612687813021703,
      "loss": 0.3648,
      "step": 1261
    },
    {
      "epoch": 2.1033333333333335,
      "grad_norm": 0.04638374596834183,
      "learning_rate": 0.00011606010016694491,
      "loss": 0.3413,
      "step": 1262
    },
    {
      "epoch": 2.105,
      "grad_norm": 0.05373930558562279,
      "learning_rate": 0.00011599332220367279,
      "loss": 0.3145,
      "step": 1263
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.04815629869699478,
      "learning_rate": 0.00011592654424040069,
      "loss": 0.2571,
      "step": 1264
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.03963738679885864,
      "learning_rate": 0.00011585976627712856,
      "loss": 0.2162,
      "step": 1265
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.05108199641108513,
      "learning_rate": 0.00011579298831385643,
      "loss": 0.2871,
      "step": 1266
    },
    {
      "epoch": 2.111666666666667,
      "grad_norm": 0.045757319778203964,
      "learning_rate": 0.0001157262103505843,
      "loss": 0.294,
      "step": 1267
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.07438202202320099,
      "learning_rate": 0.00011565943238731218,
      "loss": 0.3619,
      "step": 1268
    },
    {
      "epoch": 2.115,
      "grad_norm": 0.03945685923099518,
      "learning_rate": 0.00011559265442404008,
      "loss": 0.2387,
      "step": 1269
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.043085549026727676,
      "learning_rate": 0.00011552587646076795,
      "loss": 0.3168,
      "step": 1270
    },
    {
      "epoch": 2.118333333333333,
      "grad_norm": 0.043303411453962326,
      "learning_rate": 0.00011545909849749584,
      "loss": 0.3265,
      "step": 1271
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.037291768938302994,
      "learning_rate": 0.00011539232053422371,
      "loss": 0.2302,
      "step": 1272
    },
    {
      "epoch": 2.1216666666666666,
      "grad_norm": 0.04229063168168068,
      "learning_rate": 0.00011532554257095158,
      "loss": 0.2627,
      "step": 1273
    },
    {
      "epoch": 2.1233333333333335,
      "grad_norm": 0.04173721373081207,
      "learning_rate": 0.00011525876460767948,
      "loss": 0.2962,
      "step": 1274
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.09908606112003326,
      "learning_rate": 0.00011519198664440736,
      "loss": 0.3874,
      "step": 1275
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.0525374710559845,
      "learning_rate": 0.00011512520868113523,
      "loss": 0.3574,
      "step": 1276
    },
    {
      "epoch": 2.1283333333333334,
      "grad_norm": 0.047327496111392975,
      "learning_rate": 0.0001150584307178631,
      "loss": 0.3074,
      "step": 1277
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.03906570002436638,
      "learning_rate": 0.00011499165275459098,
      "loss": 0.2517,
      "step": 1278
    },
    {
      "epoch": 2.131666666666667,
      "grad_norm": 0.05297704413533211,
      "learning_rate": 0.00011492487479131886,
      "loss": 0.3171,
      "step": 1279
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.05640590935945511,
      "learning_rate": 0.00011485809682804675,
      "loss": 0.3066,
      "step": 1280
    },
    {
      "epoch": 2.135,
      "grad_norm": 0.03975578024983406,
      "learning_rate": 0.00011479131886477464,
      "loss": 0.2516,
      "step": 1281
    },
    {
      "epoch": 2.1366666666666667,
      "grad_norm": 0.045066531747579575,
      "learning_rate": 0.00011472454090150251,
      "loss": 0.3042,
      "step": 1282
    },
    {
      "epoch": 2.138333333333333,
      "grad_norm": 0.04243871569633484,
      "learning_rate": 0.00011465776293823038,
      "loss": 0.2633,
      "step": 1283
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.03583915904164314,
      "learning_rate": 0.00011459098497495826,
      "loss": 0.2419,
      "step": 1284
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 0.03844531625509262,
      "learning_rate": 0.00011452420701168616,
      "loss": 0.293,
      "step": 1285
    },
    {
      "epoch": 2.1433333333333335,
      "grad_norm": 0.04643364995718002,
      "learning_rate": 0.00011445742904841403,
      "loss": 0.3394,
      "step": 1286
    },
    {
      "epoch": 2.145,
      "grad_norm": 0.06798799335956573,
      "learning_rate": 0.0001143906510851419,
      "loss": 0.3206,
      "step": 1287
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.04110465571284294,
      "learning_rate": 0.00011432387312186979,
      "loss": 0.2962,
      "step": 1288
    },
    {
      "epoch": 2.1483333333333334,
      "grad_norm": 0.059502556920051575,
      "learning_rate": 0.00011425709515859766,
      "loss": 0.3362,
      "step": 1289
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.05724635720252991,
      "learning_rate": 0.00011419031719532556,
      "loss": 0.3601,
      "step": 1290
    },
    {
      "epoch": 2.151666666666667,
      "grad_norm": 0.07895616441965103,
      "learning_rate": 0.00011412353923205344,
      "loss": 0.3575,
      "step": 1291
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.04843952879309654,
      "learning_rate": 0.00011405676126878131,
      "loss": 0.2698,
      "step": 1292
    },
    {
      "epoch": 2.155,
      "grad_norm": 0.04575173929333687,
      "learning_rate": 0.00011398998330550918,
      "loss": 0.2741,
      "step": 1293
    },
    {
      "epoch": 2.1566666666666667,
      "grad_norm": 0.03620077669620514,
      "learning_rate": 0.00011392320534223706,
      "loss": 0.3065,
      "step": 1294
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 0.04040117561817169,
      "learning_rate": 0.00011385642737896493,
      "loss": 0.3059,
      "step": 1295
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.041057880967855453,
      "learning_rate": 0.00011378964941569283,
      "loss": 0.2541,
      "step": 1296
    },
    {
      "epoch": 2.1616666666666666,
      "grad_norm": 0.041201863437891006,
      "learning_rate": 0.0001137228714524207,
      "loss": 0.2531,
      "step": 1297
    },
    {
      "epoch": 2.163333333333333,
      "grad_norm": 0.04538583755493164,
      "learning_rate": 0.00011365609348914859,
      "loss": 0.3106,
      "step": 1298
    },
    {
      "epoch": 2.165,
      "grad_norm": 0.048856984823942184,
      "learning_rate": 0.00011358931552587646,
      "loss": 0.3562,
      "step": 1299
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.03312812000513077,
      "learning_rate": 0.00011352253756260434,
      "loss": 0.2841,
      "step": 1300
    },
    {
      "epoch": 2.1683333333333334,
      "grad_norm": 0.03758879378437996,
      "learning_rate": 0.00011345575959933224,
      "loss": 0.285,
      "step": 1301
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.04096600413322449,
      "learning_rate": 0.00011338898163606011,
      "loss": 0.2328,
      "step": 1302
    },
    {
      "epoch": 2.171666666666667,
      "grad_norm": 0.04577379673719406,
      "learning_rate": 0.00011332220367278798,
      "loss": 0.3275,
      "step": 1303
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.05625300481915474,
      "learning_rate": 0.00011325542570951586,
      "loss": 0.31,
      "step": 1304
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.04204051196575165,
      "learning_rate": 0.00011318864774624374,
      "loss": 0.3193,
      "step": 1305
    },
    {
      "epoch": 2.1766666666666667,
      "grad_norm": 0.03669551759958267,
      "learning_rate": 0.00011312186978297163,
      "loss": 0.2619,
      "step": 1306
    },
    {
      "epoch": 2.1783333333333332,
      "grad_norm": 0.0501430407166481,
      "learning_rate": 0.00011305509181969952,
      "loss": 0.3343,
      "step": 1307
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.050863105803728104,
      "learning_rate": 0.00011298831385642739,
      "loss": 0.3702,
      "step": 1308
    },
    {
      "epoch": 2.1816666666666666,
      "grad_norm": 0.03904383257031441,
      "learning_rate": 0.00011292153589315526,
      "loss": 0.3204,
      "step": 1309
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.06822840869426727,
      "learning_rate": 0.00011285475792988314,
      "loss": 0.3521,
      "step": 1310
    },
    {
      "epoch": 2.185,
      "grad_norm": 0.0567891001701355,
      "learning_rate": 0.00011278797996661104,
      "loss": 0.3248,
      "step": 1311
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.04745042696595192,
      "learning_rate": 0.00011272120200333891,
      "loss": 0.3027,
      "step": 1312
    },
    {
      "epoch": 2.1883333333333335,
      "grad_norm": 0.025709282606840134,
      "learning_rate": 0.00011265442404006678,
      "loss": 0.2223,
      "step": 1313
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.06083907559514046,
      "learning_rate": 0.00011258764607679465,
      "loss": 0.3983,
      "step": 1314
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 0.06139684468507767,
      "learning_rate": 0.00011252086811352254,
      "loss": 0.4065,
      "step": 1315
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.06708298623561859,
      "learning_rate": 0.00011245409015025041,
      "loss": 0.3596,
      "step": 1316
    },
    {
      "epoch": 2.195,
      "grad_norm": 0.04532661661505699,
      "learning_rate": 0.00011238731218697832,
      "loss": 0.2637,
      "step": 1317
    },
    {
      "epoch": 2.1966666666666668,
      "grad_norm": 0.03970969840884209,
      "learning_rate": 0.00011232053422370619,
      "loss": 0.3214,
      "step": 1318
    },
    {
      "epoch": 2.1983333333333333,
      "grad_norm": 0.06071338430047035,
      "learning_rate": 0.00011225375626043406,
      "loss": 0.3532,
      "step": 1319
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.046101249754428864,
      "learning_rate": 0.00011218697829716193,
      "loss": 0.3271,
      "step": 1320
    },
    {
      "epoch": 2.2016666666666667,
      "grad_norm": 0.048069532960653305,
      "learning_rate": 0.00011212020033388981,
      "loss": 0.3596,
      "step": 1321
    },
    {
      "epoch": 2.203333333333333,
      "grad_norm": 0.037619784474372864,
      "learning_rate": 0.00011205342237061771,
      "loss": 0.301,
      "step": 1322
    },
    {
      "epoch": 2.205,
      "grad_norm": 0.052347343415021896,
      "learning_rate": 0.00011198664440734558,
      "loss": 0.3647,
      "step": 1323
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.0492757223546505,
      "learning_rate": 0.00011191986644407347,
      "loss": 0.3142,
      "step": 1324
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 0.06685268133878708,
      "learning_rate": 0.00011185308848080134,
      "loss": 0.4283,
      "step": 1325
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.0422087237238884,
      "learning_rate": 0.00011178631051752921,
      "loss": 0.3026,
      "step": 1326
    },
    {
      "epoch": 2.211666666666667,
      "grad_norm": 0.0540628507733345,
      "learning_rate": 0.00011171953255425711,
      "loss": 0.3743,
      "step": 1327
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.0715484470129013,
      "learning_rate": 0.00011165275459098499,
      "loss": 0.3723,
      "step": 1328
    },
    {
      "epoch": 2.215,
      "grad_norm": 0.04975099489092827,
      "learning_rate": 0.00011158597662771286,
      "loss": 0.3596,
      "step": 1329
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.07029929012060165,
      "learning_rate": 0.00011151919866444073,
      "loss": 0.3037,
      "step": 1330
    },
    {
      "epoch": 2.2183333333333333,
      "grad_norm": 0.05714070051908493,
      "learning_rate": 0.00011145242070116862,
      "loss": 0.3412,
      "step": 1331
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.032669927924871445,
      "learning_rate": 0.0001113856427378965,
      "loss": 0.2463,
      "step": 1332
    },
    {
      "epoch": 2.2216666666666667,
      "grad_norm": 0.04589973762631416,
      "learning_rate": 0.0001113188647746244,
      "loss": 0.2883,
      "step": 1333
    },
    {
      "epoch": 2.223333333333333,
      "grad_norm": 0.050850581377744675,
      "learning_rate": 0.00011125208681135227,
      "loss": 0.3393,
      "step": 1334
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.04126717895269394,
      "learning_rate": 0.00011118530884808014,
      "loss": 0.3097,
      "step": 1335
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.108832947909832,
      "learning_rate": 0.00011111853088480801,
      "loss": 0.4379,
      "step": 1336
    },
    {
      "epoch": 2.2283333333333335,
      "grad_norm": 0.05090248957276344,
      "learning_rate": 0.00011105175292153589,
      "loss": 0.3108,
      "step": 1337
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.04152049869298935,
      "learning_rate": 0.00011098497495826379,
      "loss": 0.3456,
      "step": 1338
    },
    {
      "epoch": 2.2316666666666665,
      "grad_norm": 0.052308954298496246,
      "learning_rate": 0.00011091819699499166,
      "loss": 0.3684,
      "step": 1339
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.04100535809993744,
      "learning_rate": 0.00011085141903171953,
      "loss": 0.2687,
      "step": 1340
    },
    {
      "epoch": 2.235,
      "grad_norm": 0.04624568670988083,
      "learning_rate": 0.00011078464106844742,
      "loss": 0.2804,
      "step": 1341
    },
    {
      "epoch": 2.236666666666667,
      "grad_norm": 0.05360019579529762,
      "learning_rate": 0.00011071786310517529,
      "loss": 0.3219,
      "step": 1342
    },
    {
      "epoch": 2.2383333333333333,
      "grad_norm": 0.06889837980270386,
      "learning_rate": 0.0001106510851419032,
      "loss": 0.343,
      "step": 1343
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.04372263699769974,
      "learning_rate": 0.00011058430717863107,
      "loss": 0.2988,
      "step": 1344
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 0.03760122135281563,
      "learning_rate": 0.00011051752921535894,
      "loss": 0.2645,
      "step": 1345
    },
    {
      "epoch": 2.243333333333333,
      "grad_norm": 0.043109696358442307,
      "learning_rate": 0.00011045075125208681,
      "loss": 0.3019,
      "step": 1346
    },
    {
      "epoch": 2.245,
      "grad_norm": 0.04281935095787048,
      "learning_rate": 0.00011038397328881469,
      "loss": 0.3023,
      "step": 1347
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.050939857959747314,
      "learning_rate": 0.00011031719532554257,
      "loss": 0.3137,
      "step": 1348
    },
    {
      "epoch": 2.2483333333333335,
      "grad_norm": 0.047550491988658905,
      "learning_rate": 0.00011025041736227046,
      "loss": 0.306,
      "step": 1349
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.03763650730252266,
      "learning_rate": 0.00011018363939899835,
      "loss": 0.273,
      "step": 1350
    },
    {
      "epoch": 2.2516666666666665,
      "grad_norm": 0.05680291727185249,
      "learning_rate": 0.00011011686143572622,
      "loss": 0.3371,
      "step": 1351
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.04880351573228836,
      "learning_rate": 0.00011005008347245409,
      "loss": 0.2968,
      "step": 1352
    },
    {
      "epoch": 2.255,
      "grad_norm": 0.041535984724760056,
      "learning_rate": 0.00010998330550918197,
      "loss": 0.2605,
      "step": 1353
    },
    {
      "epoch": 2.256666666666667,
      "grad_norm": 0.040757179260253906,
      "learning_rate": 0.00010991652754590987,
      "loss": 0.2877,
      "step": 1354
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.04649787396192551,
      "learning_rate": 0.00010984974958263774,
      "loss": 0.2928,
      "step": 1355
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.04432113841176033,
      "learning_rate": 0.00010978297161936561,
      "loss": 0.3453,
      "step": 1356
    },
    {
      "epoch": 2.2616666666666667,
      "grad_norm": 0.057204846292734146,
      "learning_rate": 0.00010971619365609349,
      "loss": 0.361,
      "step": 1357
    },
    {
      "epoch": 2.263333333333333,
      "grad_norm": 0.04873208701610565,
      "learning_rate": 0.00010964941569282137,
      "loss": 0.3309,
      "step": 1358
    },
    {
      "epoch": 2.265,
      "grad_norm": 0.0430399626493454,
      "learning_rate": 0.00010958263772954926,
      "loss": 0.3367,
      "step": 1359
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.04419350624084473,
      "learning_rate": 0.00010951585976627715,
      "loss": 0.3069,
      "step": 1360
    },
    {
      "epoch": 2.2683333333333335,
      "grad_norm": 0.04080675169825554,
      "learning_rate": 0.00010944908180300502,
      "loss": 0.3404,
      "step": 1361
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.029385805130004883,
      "learning_rate": 0.00010938230383973289,
      "loss": 0.2315,
      "step": 1362
    },
    {
      "epoch": 2.2716666666666665,
      "grad_norm": 0.05341643467545509,
      "learning_rate": 0.00010931552587646076,
      "loss": 0.347,
      "step": 1363
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.04181624576449394,
      "learning_rate": 0.00010924874791318864,
      "loss": 0.2952,
      "step": 1364
    },
    {
      "epoch": 2.275,
      "grad_norm": 0.055085428059101105,
      "learning_rate": 0.00010918196994991654,
      "loss": 0.3287,
      "step": 1365
    },
    {
      "epoch": 2.276666666666667,
      "grad_norm": 0.056234534829854965,
      "learning_rate": 0.00010911519198664441,
      "loss": 0.3361,
      "step": 1366
    },
    {
      "epoch": 2.2783333333333333,
      "grad_norm": 0.04931879788637161,
      "learning_rate": 0.0001090484140233723,
      "loss": 0.3286,
      "step": 1367
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.043716371059417725,
      "learning_rate": 0.00010898163606010017,
      "loss": 0.3393,
      "step": 1368
    },
    {
      "epoch": 2.2816666666666667,
      "grad_norm": 0.03245168551802635,
      "learning_rate": 0.00010891485809682804,
      "loss": 0.2302,
      "step": 1369
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.04572686180472374,
      "learning_rate": 0.00010884808013355594,
      "loss": 0.3107,
      "step": 1370
    },
    {
      "epoch": 2.285,
      "grad_norm": 0.04401659592986107,
      "learning_rate": 0.00010878130217028382,
      "loss": 0.3246,
      "step": 1371
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.043931297957897186,
      "learning_rate": 0.00010871452420701169,
      "loss": 0.3358,
      "step": 1372
    },
    {
      "epoch": 2.288333333333333,
      "grad_norm": 0.05000155046582222,
      "learning_rate": 0.00010864774624373956,
      "loss": 0.3867,
      "step": 1373
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.04099830612540245,
      "learning_rate": 0.00010858096828046744,
      "loss": 0.3148,
      "step": 1374
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.04383559897542,
      "learning_rate": 0.00010851419031719534,
      "loss": 0.3317,
      "step": 1375
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.04229459911584854,
      "learning_rate": 0.00010844741235392321,
      "loss": 0.327,
      "step": 1376
    },
    {
      "epoch": 2.295,
      "grad_norm": 0.03466473147273064,
      "learning_rate": 0.0001083806343906511,
      "loss": 0.2609,
      "step": 1377
    },
    {
      "epoch": 2.296666666666667,
      "grad_norm": 0.0394781120121479,
      "learning_rate": 0.00010831385642737897,
      "loss": 0.2967,
      "step": 1378
    },
    {
      "epoch": 2.2983333333333333,
      "grad_norm": 0.05149355158209801,
      "learning_rate": 0.00010824707846410684,
      "loss": 0.3836,
      "step": 1379
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.04335353523492813,
      "learning_rate": 0.00010818030050083472,
      "loss": 0.351,
      "step": 1380
    },
    {
      "epoch": 2.3016666666666667,
      "grad_norm": 0.04405440017580986,
      "learning_rate": 0.00010811352253756262,
      "loss": 0.3123,
      "step": 1381
    },
    {
      "epoch": 2.3033333333333332,
      "grad_norm": 0.038893867284059525,
      "learning_rate": 0.00010804674457429049,
      "loss": 0.289,
      "step": 1382
    },
    {
      "epoch": 2.305,
      "grad_norm": 0.08164285123348236,
      "learning_rate": 0.00010797996661101836,
      "loss": 0.3821,
      "step": 1383
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.05829804763197899,
      "learning_rate": 0.00010791318864774625,
      "loss": 0.3847,
      "step": 1384
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 0.04495077207684517,
      "learning_rate": 0.00010784641068447412,
      "loss": 0.379,
      "step": 1385
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.037379465997219086,
      "learning_rate": 0.00010777963272120202,
      "loss": 0.3035,
      "step": 1386
    },
    {
      "epoch": 2.3116666666666665,
      "grad_norm": 0.06868374347686768,
      "learning_rate": 0.0001077128547579299,
      "loss": 0.3282,
      "step": 1387
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.04402051866054535,
      "learning_rate": 0.00010764607679465777,
      "loss": 0.3685,
      "step": 1388
    },
    {
      "epoch": 2.315,
      "grad_norm": 0.045365747064352036,
      "learning_rate": 0.00010757929883138564,
      "loss": 0.3306,
      "step": 1389
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.05386698991060257,
      "learning_rate": 0.00010751252086811352,
      "loss": 0.295,
      "step": 1390
    },
    {
      "epoch": 2.3183333333333334,
      "grad_norm": 0.032515693455934525,
      "learning_rate": 0.00010744574290484142,
      "loss": 0.2196,
      "step": 1391
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.03688321262598038,
      "learning_rate": 0.00010737896494156929,
      "loss": 0.2676,
      "step": 1392
    },
    {
      "epoch": 2.3216666666666668,
      "grad_norm": 0.04910999536514282,
      "learning_rate": 0.00010731218697829716,
      "loss": 0.2799,
      "step": 1393
    },
    {
      "epoch": 2.3233333333333333,
      "grad_norm": 0.04248809069395065,
      "learning_rate": 0.00010724540901502505,
      "loss": 0.2644,
      "step": 1394
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.04116111993789673,
      "learning_rate": 0.00010717863105175292,
      "loss": 0.3013,
      "step": 1395
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.062009792774915695,
      "learning_rate": 0.0001071118530884808,
      "loss": 0.2973,
      "step": 1396
    },
    {
      "epoch": 2.328333333333333,
      "grad_norm": 0.036329109221696854,
      "learning_rate": 0.0001070450751252087,
      "loss": 0.2141,
      "step": 1397
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.0481041744351387,
      "learning_rate": 0.00010697829716193657,
      "loss": 0.3501,
      "step": 1398
    },
    {
      "epoch": 2.3316666666666666,
      "grad_norm": 0.03341355919837952,
      "learning_rate": 0.00010691151919866444,
      "loss": 0.25,
      "step": 1399
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.0625496357679367,
      "learning_rate": 0.00010684474123539232,
      "loss": 0.3125,
      "step": 1400
    },
    {
      "epoch": 2.335,
      "grad_norm": 0.04510776698589325,
      "learning_rate": 0.0001067779632721202,
      "loss": 0.2956,
      "step": 1401
    },
    {
      "epoch": 2.336666666666667,
      "grad_norm": 0.04404119774699211,
      "learning_rate": 0.00010671118530884809,
      "loss": 0.2968,
      "step": 1402
    },
    {
      "epoch": 2.3383333333333334,
      "grad_norm": 0.050552189350128174,
      "learning_rate": 0.00010664440734557598,
      "loss": 0.3081,
      "step": 1403
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.03290446847677231,
      "learning_rate": 0.00010657762938230385,
      "loss": 0.2827,
      "step": 1404
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.04212913289666176,
      "learning_rate": 0.00010651085141903172,
      "loss": 0.3014,
      "step": 1405
    },
    {
      "epoch": 2.3433333333333333,
      "grad_norm": 0.049999527633190155,
      "learning_rate": 0.0001064440734557596,
      "loss": 0.3589,
      "step": 1406
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 0.04403725266456604,
      "learning_rate": 0.0001063772954924875,
      "loss": 0.3754,
      "step": 1407
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.03893263638019562,
      "learning_rate": 0.00010631051752921537,
      "loss": 0.273,
      "step": 1408
    },
    {
      "epoch": 2.348333333333333,
      "grad_norm": 0.03134392946958542,
      "learning_rate": 0.00010624373956594324,
      "loss": 0.2499,
      "step": 1409
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.05040867626667023,
      "learning_rate": 0.00010617696160267111,
      "loss": 0.3533,
      "step": 1410
    },
    {
      "epoch": 2.3516666666666666,
      "grad_norm": 0.05000928416848183,
      "learning_rate": 0.000106110183639399,
      "loss": 0.3373,
      "step": 1411
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.061851173639297485,
      "learning_rate": 0.00010604340567612687,
      "loss": 0.3079,
      "step": 1412
    },
    {
      "epoch": 2.355,
      "grad_norm": 0.03486660495400429,
      "learning_rate": 0.00010597662771285477,
      "loss": 0.2064,
      "step": 1413
    },
    {
      "epoch": 2.3566666666666665,
      "grad_norm": 0.09169895201921463,
      "learning_rate": 0.00010590984974958265,
      "loss": 0.3719,
      "step": 1414
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 0.03266946226358414,
      "learning_rate": 0.00010584307178631052,
      "loss": 0.2807,
      "step": 1415
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.04826796427369118,
      "learning_rate": 0.0001057762938230384,
      "loss": 0.3029,
      "step": 1416
    },
    {
      "epoch": 2.361666666666667,
      "grad_norm": 0.03472161293029785,
      "learning_rate": 0.00010570951585976627,
      "loss": 0.2631,
      "step": 1417
    },
    {
      "epoch": 2.3633333333333333,
      "grad_norm": 0.04982312396168709,
      "learning_rate": 0.00010564273789649417,
      "loss": 0.3509,
      "step": 1418
    },
    {
      "epoch": 2.365,
      "grad_norm": 0.044585008174180984,
      "learning_rate": 0.00010557595993322204,
      "loss": 0.3208,
      "step": 1419
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.0367768369615078,
      "learning_rate": 0.00010550918196994993,
      "loss": 0.2658,
      "step": 1420
    },
    {
      "epoch": 2.368333333333333,
      "grad_norm": 0.04895355924963951,
      "learning_rate": 0.0001054424040066778,
      "loss": 0.2705,
      "step": 1421
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.03240306302905083,
      "learning_rate": 0.00010537562604340567,
      "loss": 0.2677,
      "step": 1422
    },
    {
      "epoch": 2.3716666666666666,
      "grad_norm": 0.0411730594933033,
      "learning_rate": 0.00010530884808013357,
      "loss": 0.2537,
      "step": 1423
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.044723402708768845,
      "learning_rate": 0.00010524207011686145,
      "loss": 0.2805,
      "step": 1424
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.039580296725034714,
      "learning_rate": 0.00010517529215358932,
      "loss": 0.2558,
      "step": 1425
    },
    {
      "epoch": 2.3766666666666665,
      "grad_norm": 0.0568104088306427,
      "learning_rate": 0.0001051085141903172,
      "loss": 0.3363,
      "step": 1426
    },
    {
      "epoch": 2.3783333333333334,
      "grad_norm": 0.0723712220788002,
      "learning_rate": 0.00010504173622704507,
      "loss": 0.3595,
      "step": 1427
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.0555865503847599,
      "learning_rate": 0.00010497495826377295,
      "loss": 0.3461,
      "step": 1428
    },
    {
      "epoch": 2.381666666666667,
      "grad_norm": 0.04186144843697548,
      "learning_rate": 0.00010490818030050084,
      "loss": 0.2923,
      "step": 1429
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.05412086099386215,
      "learning_rate": 0.00010484140233722873,
      "loss": 0.347,
      "step": 1430
    },
    {
      "epoch": 2.385,
      "grad_norm": 0.05489248037338257,
      "learning_rate": 0.0001047746243739566,
      "loss": 0.3451,
      "step": 1431
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.040147196501493454,
      "learning_rate": 0.00010470784641068447,
      "loss": 0.3164,
      "step": 1432
    },
    {
      "epoch": 2.388333333333333,
      "grad_norm": 0.04177836328744888,
      "learning_rate": 0.00010464106844741235,
      "loss": 0.2573,
      "step": 1433
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.08215593546628952,
      "learning_rate": 0.00010457429048414025,
      "loss": 0.334,
      "step": 1434
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.07419685274362564,
      "learning_rate": 0.00010450751252086812,
      "loss": 0.3534,
      "step": 1435
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.039621878415346146,
      "learning_rate": 0.00010444073455759599,
      "loss": 0.2554,
      "step": 1436
    },
    {
      "epoch": 2.395,
      "grad_norm": 0.05845452472567558,
      "learning_rate": 0.00010437395659432388,
      "loss": 0.3355,
      "step": 1437
    },
    {
      "epoch": 2.3966666666666665,
      "grad_norm": 0.038017723709344864,
      "learning_rate": 0.00010430717863105175,
      "loss": 0.2635,
      "step": 1438
    },
    {
      "epoch": 2.3983333333333334,
      "grad_norm": 0.034781862050294876,
      "learning_rate": 0.00010424040066777965,
      "loss": 0.2561,
      "step": 1439
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.04372202605009079,
      "learning_rate": 0.00010417362270450753,
      "loss": 0.3515,
      "step": 1440
    },
    {
      "epoch": 2.401666666666667,
      "grad_norm": 0.04032072052359581,
      "learning_rate": 0.0001041068447412354,
      "loss": 0.3116,
      "step": 1441
    },
    {
      "epoch": 2.4033333333333333,
      "grad_norm": 0.03662048652768135,
      "learning_rate": 0.00010404006677796327,
      "loss": 0.2558,
      "step": 1442
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 0.03352939710021019,
      "learning_rate": 0.00010397328881469115,
      "loss": 0.228,
      "step": 1443
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.037490297108888626,
      "learning_rate": 0.00010390651085141905,
      "loss": 0.2497,
      "step": 1444
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.03344340994954109,
      "learning_rate": 0.00010383973288814692,
      "loss": 0.256,
      "step": 1445
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.03682905063033104,
      "learning_rate": 0.0001037729549248748,
      "loss": 0.2844,
      "step": 1446
    },
    {
      "epoch": 2.4116666666666666,
      "grad_norm": 0.046089161187410355,
      "learning_rate": 0.00010370617696160268,
      "loss": 0.3211,
      "step": 1447
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.031198538839817047,
      "learning_rate": 0.00010363939899833055,
      "loss": 0.2192,
      "step": 1448
    },
    {
      "epoch": 2.415,
      "grad_norm": 0.037006739526987076,
      "learning_rate": 0.00010357262103505843,
      "loss": 0.2511,
      "step": 1449
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.027640122920274734,
      "learning_rate": 0.00010350584307178633,
      "loss": 0.1915,
      "step": 1450
    },
    {
      "epoch": 2.4183333333333334,
      "grad_norm": 0.036473535001277924,
      "learning_rate": 0.0001034390651085142,
      "loss": 0.2545,
      "step": 1451
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.07211489230394363,
      "learning_rate": 0.00010337228714524207,
      "loss": 0.3303,
      "step": 1452
    },
    {
      "epoch": 2.421666666666667,
      "grad_norm": 0.03753171116113663,
      "learning_rate": 0.00010330550918196994,
      "loss": 0.2867,
      "step": 1453
    },
    {
      "epoch": 2.4233333333333333,
      "grad_norm": 0.04831143096089363,
      "learning_rate": 0.00010323873121869783,
      "loss": 0.332,
      "step": 1454
    },
    {
      "epoch": 2.425,
      "grad_norm": 0.043362703174352646,
      "learning_rate": 0.00010317195325542572,
      "loss": 0.3117,
      "step": 1455
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.04973810166120529,
      "learning_rate": 0.0001031051752921536,
      "loss": 0.3035,
      "step": 1456
    },
    {
      "epoch": 2.4283333333333332,
      "grad_norm": 0.045074813067913055,
      "learning_rate": 0.00010303839732888148,
      "loss": 0.3453,
      "step": 1457
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.03210065886378288,
      "learning_rate": 0.00010297161936560935,
      "loss": 0.2499,
      "step": 1458
    },
    {
      "epoch": 2.4316666666666666,
      "grad_norm": 0.051591772586107254,
      "learning_rate": 0.00010290484140233722,
      "loss": 0.2978,
      "step": 1459
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.042086780071258545,
      "learning_rate": 0.00010283806343906512,
      "loss": 0.3373,
      "step": 1460
    },
    {
      "epoch": 2.435,
      "grad_norm": 0.05760788172483444,
      "learning_rate": 0.000102771285475793,
      "loss": 0.3067,
      "step": 1461
    },
    {
      "epoch": 2.4366666666666665,
      "grad_norm": 0.050000838935375214,
      "learning_rate": 0.00010270450751252087,
      "loss": 0.3388,
      "step": 1462
    },
    {
      "epoch": 2.4383333333333335,
      "grad_norm": 0.043961744755506516,
      "learning_rate": 0.00010263772954924876,
      "loss": 0.3272,
      "step": 1463
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.039009422063827515,
      "learning_rate": 0.00010257095158597663,
      "loss": 0.3006,
      "step": 1464
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 0.04189685359597206,
      "learning_rate": 0.0001025041736227045,
      "loss": 0.2712,
      "step": 1465
    },
    {
      "epoch": 2.4433333333333334,
      "grad_norm": 0.06212511286139488,
      "learning_rate": 0.0001024373956594324,
      "loss": 0.3555,
      "step": 1466
    },
    {
      "epoch": 2.445,
      "grad_norm": 0.05445551127195358,
      "learning_rate": 0.00010237061769616028,
      "loss": 0.3327,
      "step": 1467
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.04559718817472458,
      "learning_rate": 0.00010230383973288815,
      "loss": 0.2876,
      "step": 1468
    },
    {
      "epoch": 2.4483333333333333,
      "grad_norm": 0.04328901320695877,
      "learning_rate": 0.00010223706176961602,
      "loss": 0.3143,
      "step": 1469
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.0399332158267498,
      "learning_rate": 0.0001021702838063439,
      "loss": 0.2802,
      "step": 1470
    },
    {
      "epoch": 2.4516666666666667,
      "grad_norm": 0.053718775510787964,
      "learning_rate": 0.0001021035058430718,
      "loss": 0.353,
      "step": 1471
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.06225457414984703,
      "learning_rate": 0.00010203672787979967,
      "loss": 0.4134,
      "step": 1472
    },
    {
      "epoch": 2.455,
      "grad_norm": 0.055224280804395676,
      "learning_rate": 0.00010196994991652756,
      "loss": 0.3934,
      "step": 1473
    },
    {
      "epoch": 2.4566666666666666,
      "grad_norm": 0.0434880368411541,
      "learning_rate": 0.00010190317195325543,
      "loss": 0.3333,
      "step": 1474
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.04586184769868851,
      "learning_rate": 0.0001018363939899833,
      "loss": 0.3338,
      "step": 1475
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.05217510089278221,
      "learning_rate": 0.0001017696160267112,
      "loss": 0.3305,
      "step": 1476
    },
    {
      "epoch": 2.461666666666667,
      "grad_norm": 0.05205001309514046,
      "learning_rate": 0.00010170283806343908,
      "loss": 0.3079,
      "step": 1477
    },
    {
      "epoch": 2.4633333333333334,
      "grad_norm": 0.04915407672524452,
      "learning_rate": 0.00010163606010016695,
      "loss": 0.2451,
      "step": 1478
    },
    {
      "epoch": 2.465,
      "grad_norm": 0.0557168647646904,
      "learning_rate": 0.00010156928213689482,
      "loss": 0.3579,
      "step": 1479
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.048031434416770935,
      "learning_rate": 0.00010150250417362271,
      "loss": 0.3294,
      "step": 1480
    },
    {
      "epoch": 2.4683333333333333,
      "grad_norm": 0.036160025745630264,
      "learning_rate": 0.00010143572621035058,
      "loss": 0.2485,
      "step": 1481
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.04580971226096153,
      "learning_rate": 0.00010136894824707848,
      "loss": 0.3042,
      "step": 1482
    },
    {
      "epoch": 2.4716666666666667,
      "grad_norm": 0.04843103513121605,
      "learning_rate": 0.00010130217028380636,
      "loss": 0.3334,
      "step": 1483
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.04426259547472,
      "learning_rate": 0.00010123539232053423,
      "loss": 0.245,
      "step": 1484
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.05204618349671364,
      "learning_rate": 0.0001011686143572621,
      "loss": 0.3016,
      "step": 1485
    },
    {
      "epoch": 2.4766666666666666,
      "grad_norm": 0.04717372730374336,
      "learning_rate": 0.00010110183639398998,
      "loss": 0.3077,
      "step": 1486
    },
    {
      "epoch": 2.4783333333333335,
      "grad_norm": 0.04665202647447586,
      "learning_rate": 0.00010103505843071788,
      "loss": 0.3075,
      "step": 1487
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.05995183810591698,
      "learning_rate": 0.00010096828046744575,
      "loss": 0.371,
      "step": 1488
    },
    {
      "epoch": 2.4816666666666665,
      "grad_norm": 0.056211378425359726,
      "learning_rate": 0.00010090150250417362,
      "loss": 0.351,
      "step": 1489
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.04900602251291275,
      "learning_rate": 0.00010083472454090151,
      "loss": 0.3293,
      "step": 1490
    },
    {
      "epoch": 2.485,
      "grad_norm": 0.04777010902762413,
      "learning_rate": 0.00010076794657762938,
      "loss": 0.3365,
      "step": 1491
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.05941714346408844,
      "learning_rate": 0.00010070116861435728,
      "loss": 0.3405,
      "step": 1492
    },
    {
      "epoch": 2.4883333333333333,
      "grad_norm": 0.0757373794913292,
      "learning_rate": 0.00010063439065108516,
      "loss": 0.3159,
      "step": 1493
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.06918811798095703,
      "learning_rate": 0.00010056761268781303,
      "loss": 0.3212,
      "step": 1494
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 0.05530046671628952,
      "learning_rate": 0.0001005008347245409,
      "loss": 0.3859,
      "step": 1495
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.05100347101688385,
      "learning_rate": 0.00010043405676126878,
      "loss": 0.3435,
      "step": 1496
    },
    {
      "epoch": 2.495,
      "grad_norm": 0.04422896355390549,
      "learning_rate": 0.00010036727879799666,
      "loss": 0.2547,
      "step": 1497
    },
    {
      "epoch": 2.4966666666666666,
      "grad_norm": 0.07904937863349915,
      "learning_rate": 0.00010030050083472455,
      "loss": 0.4235,
      "step": 1498
    },
    {
      "epoch": 2.4983333333333335,
      "grad_norm": 0.046593986451625824,
      "learning_rate": 0.00010023372287145244,
      "loss": 0.3389,
      "step": 1499
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.05924468860030174,
      "learning_rate": 0.00010016694490818031,
      "loss": 0.3851,
      "step": 1500
    },
    {
      "epoch": 2.501666666666667,
      "grad_norm": 0.04860279709100723,
      "learning_rate": 0.00010010016694490818,
      "loss": 0.3279,
      "step": 1501
    },
    {
      "epoch": 2.5033333333333334,
      "grad_norm": 0.03980454429984093,
      "learning_rate": 0.00010003338898163605,
      "loss": 0.2832,
      "step": 1502
    },
    {
      "epoch": 2.505,
      "grad_norm": 0.042865097522735596,
      "learning_rate": 9.996661101836394e-05,
      "loss": 0.3493,
      "step": 1503
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.05201095715165138,
      "learning_rate": 9.989983305509183e-05,
      "loss": 0.3547,
      "step": 1504
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 0.04896574839949608,
      "learning_rate": 9.98330550918197e-05,
      "loss": 0.2747,
      "step": 1505
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.06433258205652237,
      "learning_rate": 9.976627712854757e-05,
      "loss": 0.304,
      "step": 1506
    },
    {
      "epoch": 2.5116666666666667,
      "grad_norm": 0.03255603462457657,
      "learning_rate": 9.969949916527546e-05,
      "loss": 0.2685,
      "step": 1507
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.046865493059158325,
      "learning_rate": 9.963272120200335e-05,
      "loss": 0.2831,
      "step": 1508
    },
    {
      "epoch": 2.515,
      "grad_norm": 0.040885619819164276,
      "learning_rate": 9.956594323873122e-05,
      "loss": 0.3055,
      "step": 1509
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.06424609571695328,
      "learning_rate": 9.949916527545911e-05,
      "loss": 0.32,
      "step": 1510
    },
    {
      "epoch": 2.5183333333333335,
      "grad_norm": 0.046194788068532944,
      "learning_rate": 9.943238731218698e-05,
      "loss": 0.3047,
      "step": 1511
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.04364015907049179,
      "learning_rate": 9.936560934891487e-05,
      "loss": 0.3097,
      "step": 1512
    },
    {
      "epoch": 2.5216666666666665,
      "grad_norm": 0.046550437808036804,
      "learning_rate": 9.929883138564274e-05,
      "loss": 0.2911,
      "step": 1513
    },
    {
      "epoch": 2.5233333333333334,
      "grad_norm": 0.05098520219326019,
      "learning_rate": 9.923205342237061e-05,
      "loss": 0.2666,
      "step": 1514
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.04539436474442482,
      "learning_rate": 9.91652754590985e-05,
      "loss": 0.3253,
      "step": 1515
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.04162094369530678,
      "learning_rate": 9.909849749582639e-05,
      "loss": 0.3263,
      "step": 1516
    },
    {
      "epoch": 2.5283333333333333,
      "grad_norm": 0.04882491007447243,
      "learning_rate": 9.903171953255426e-05,
      "loss": 0.3434,
      "step": 1517
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.0744718685746193,
      "learning_rate": 9.896494156928215e-05,
      "loss": 0.4152,
      "step": 1518
    },
    {
      "epoch": 2.5316666666666667,
      "grad_norm": 0.04255329817533493,
      "learning_rate": 9.889816360601002e-05,
      "loss": 0.3161,
      "step": 1519
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.04395616427063942,
      "learning_rate": 9.883138564273791e-05,
      "loss": 0.2929,
      "step": 1520
    },
    {
      "epoch": 2.535,
      "grad_norm": 0.058904558420181274,
      "learning_rate": 9.876460767946578e-05,
      "loss": 0.2938,
      "step": 1521
    },
    {
      "epoch": 2.5366666666666666,
      "grad_norm": 0.04642521217465401,
      "learning_rate": 9.869782971619365e-05,
      "loss": 0.325,
      "step": 1522
    },
    {
      "epoch": 2.538333333333333,
      "grad_norm": 0.04006343334913254,
      "learning_rate": 9.863105175292154e-05,
      "loss": 0.3015,
      "step": 1523
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.03922372683882713,
      "learning_rate": 9.856427378964941e-05,
      "loss": 0.2492,
      "step": 1524
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 0.057173602283000946,
      "learning_rate": 9.84974958263773e-05,
      "loss": 0.2785,
      "step": 1525
    },
    {
      "epoch": 2.5433333333333334,
      "grad_norm": 0.045919183641672134,
      "learning_rate": 9.843071786310519e-05,
      "loss": 0.2854,
      "step": 1526
    },
    {
      "epoch": 2.545,
      "grad_norm": 0.04644322767853737,
      "learning_rate": 9.836393989983306e-05,
      "loss": 0.3072,
      "step": 1527
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.05382341146469116,
      "learning_rate": 9.829716193656095e-05,
      "loss": 0.3675,
      "step": 1528
    },
    {
      "epoch": 2.5483333333333333,
      "grad_norm": 0.04132735729217529,
      "learning_rate": 9.823038397328882e-05,
      "loss": 0.2697,
      "step": 1529
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.04370049014687538,
      "learning_rate": 9.816360601001669e-05,
      "loss": 0.2886,
      "step": 1530
    },
    {
      "epoch": 2.5516666666666667,
      "grad_norm": 0.038106270134449005,
      "learning_rate": 9.809682804674458e-05,
      "loss": 0.2909,
      "step": 1531
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.05607146397233009,
      "learning_rate": 9.803005008347245e-05,
      "loss": 0.3127,
      "step": 1532
    },
    {
      "epoch": 2.555,
      "grad_norm": 0.046773411333560944,
      "learning_rate": 9.796327212020034e-05,
      "loss": 0.2524,
      "step": 1533
    },
    {
      "epoch": 2.5566666666666666,
      "grad_norm": 0.044901855289936066,
      "learning_rate": 9.789649415692823e-05,
      "loss": 0.2699,
      "step": 1534
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 0.04604604095220566,
      "learning_rate": 9.78297161936561e-05,
      "loss": 0.3067,
      "step": 1535
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.05064383149147034,
      "learning_rate": 9.776293823038399e-05,
      "loss": 0.3318,
      "step": 1536
    },
    {
      "epoch": 2.5616666666666665,
      "grad_norm": 0.05409511923789978,
      "learning_rate": 9.769616026711186e-05,
      "loss": 0.357,
      "step": 1537
    },
    {
      "epoch": 2.5633333333333335,
      "grad_norm": 0.051280319690704346,
      "learning_rate": 9.762938230383973e-05,
      "loss": 0.3435,
      "step": 1538
    },
    {
      "epoch": 2.565,
      "grad_norm": 0.07118725031614304,
      "learning_rate": 9.756260434056762e-05,
      "loss": 0.3336,
      "step": 1539
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.03861064091324806,
      "learning_rate": 9.749582637729549e-05,
      "loss": 0.2762,
      "step": 1540
    },
    {
      "epoch": 2.5683333333333334,
      "grad_norm": 0.044821541756391525,
      "learning_rate": 9.742904841402337e-05,
      "loss": 0.3216,
      "step": 1541
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.033218663185834885,
      "learning_rate": 9.736227045075125e-05,
      "loss": 0.2958,
      "step": 1542
    },
    {
      "epoch": 2.5716666666666668,
      "grad_norm": 0.04860520735383034,
      "learning_rate": 9.729549248747914e-05,
      "loss": 0.2623,
      "step": 1543
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.04269019886851311,
      "learning_rate": 9.722871452420703e-05,
      "loss": 0.3093,
      "step": 1544
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.038209863007068634,
      "learning_rate": 9.71619365609349e-05,
      "loss": 0.2042,
      "step": 1545
    },
    {
      "epoch": 2.5766666666666667,
      "grad_norm": 0.07220359146595001,
      "learning_rate": 9.709515859766277e-05,
      "loss": 0.3441,
      "step": 1546
    },
    {
      "epoch": 2.578333333333333,
      "grad_norm": 0.04453956335783005,
      "learning_rate": 9.702838063439066e-05,
      "loss": 0.2676,
      "step": 1547
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.0627213567495346,
      "learning_rate": 9.696160267111853e-05,
      "loss": 0.3716,
      "step": 1548
    },
    {
      "epoch": 2.5816666666666666,
      "grad_norm": 0.04248931258916855,
      "learning_rate": 9.68948247078464e-05,
      "loss": 0.3313,
      "step": 1549
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.04183764383196831,
      "learning_rate": 9.682804674457429e-05,
      "loss": 0.273,
      "step": 1550
    },
    {
      "epoch": 2.585,
      "grad_norm": 0.04628795012831688,
      "learning_rate": 9.676126878130218e-05,
      "loss": 0.3299,
      "step": 1551
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.051544878631830215,
      "learning_rate": 9.669449081803006e-05,
      "loss": 0.33,
      "step": 1552
    },
    {
      "epoch": 2.5883333333333334,
      "grad_norm": 0.03823621943593025,
      "learning_rate": 9.662771285475794e-05,
      "loss": 0.2559,
      "step": 1553
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.03482464700937271,
      "learning_rate": 9.656093489148581e-05,
      "loss": 0.2614,
      "step": 1554
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 0.05422293394804001,
      "learning_rate": 9.64941569282137e-05,
      "loss": 0.3567,
      "step": 1555
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.04622291773557663,
      "learning_rate": 9.642737896494157e-05,
      "loss": 0.3042,
      "step": 1556
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 0.05616637319326401,
      "learning_rate": 9.636060100166944e-05,
      "loss": 0.3198,
      "step": 1557
    },
    {
      "epoch": 2.5966666666666667,
      "grad_norm": 0.0397917777299881,
      "learning_rate": 9.629382303839733e-05,
      "loss": 0.289,
      "step": 1558
    },
    {
      "epoch": 2.5983333333333336,
      "grad_norm": 0.0488848052918911,
      "learning_rate": 9.62270450751252e-05,
      "loss": 0.3654,
      "step": 1559
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.04984961450099945,
      "learning_rate": 9.616026711185309e-05,
      "loss": 0.2809,
      "step": 1560
    },
    {
      "epoch": 2.6016666666666666,
      "grad_norm": 0.055315062403678894,
      "learning_rate": 9.609348914858098e-05,
      "loss": 0.3753,
      "step": 1561
    },
    {
      "epoch": 2.6033333333333335,
      "grad_norm": 0.036553479731082916,
      "learning_rate": 9.602671118530885e-05,
      "loss": 0.3146,
      "step": 1562
    },
    {
      "epoch": 2.605,
      "grad_norm": 0.030510088428854942,
      "learning_rate": 9.595993322203674e-05,
      "loss": 0.2703,
      "step": 1563
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.041884467005729675,
      "learning_rate": 9.589315525876461e-05,
      "loss": 0.2293,
      "step": 1564
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 0.04312128201127052,
      "learning_rate": 9.582637729549248e-05,
      "loss": 0.2191,
      "step": 1565
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.060316380113363266,
      "learning_rate": 9.575959933222037e-05,
      "loss": 0.4072,
      "step": 1566
    },
    {
      "epoch": 2.611666666666667,
      "grad_norm": 0.034005820751190186,
      "learning_rate": 9.569282136894824e-05,
      "loss": 0.2692,
      "step": 1567
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.05063597485423088,
      "learning_rate": 9.562604340567613e-05,
      "loss": 0.3426,
      "step": 1568
    },
    {
      "epoch": 2.615,
      "grad_norm": 0.04117279127240181,
      "learning_rate": 9.555926544240402e-05,
      "loss": 0.2609,
      "step": 1569
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.04434072971343994,
      "learning_rate": 9.549248747913189e-05,
      "loss": 0.3061,
      "step": 1570
    },
    {
      "epoch": 2.618333333333333,
      "grad_norm": 0.03798019513487816,
      "learning_rate": 9.542570951585978e-05,
      "loss": 0.3019,
      "step": 1571
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.04448610544204712,
      "learning_rate": 9.535893155258765e-05,
      "loss": 0.3137,
      "step": 1572
    },
    {
      "epoch": 2.6216666666666666,
      "grad_norm": 0.04178617149591446,
      "learning_rate": 9.529215358931554e-05,
      "loss": 0.3255,
      "step": 1573
    },
    {
      "epoch": 2.623333333333333,
      "grad_norm": 0.03211899474263191,
      "learning_rate": 9.522537562604341e-05,
      "loss": 0.2755,
      "step": 1574
    },
    {
      "epoch": 2.625,
      "grad_norm": 0.04368414729833603,
      "learning_rate": 9.515859766277128e-05,
      "loss": 0.3766,
      "step": 1575
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.03243180364370346,
      "learning_rate": 9.509181969949917e-05,
      "loss": 0.2588,
      "step": 1576
    },
    {
      "epoch": 2.6283333333333334,
      "grad_norm": 0.05255623906850815,
      "learning_rate": 9.502504173622706e-05,
      "loss": 0.3357,
      "step": 1577
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.04521565139293671,
      "learning_rate": 9.495826377295493e-05,
      "loss": 0.2781,
      "step": 1578
    },
    {
      "epoch": 2.631666666666667,
      "grad_norm": 0.060719382017850876,
      "learning_rate": 9.489148580968282e-05,
      "loss": 0.2695,
      "step": 1579
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.03958090767264366,
      "learning_rate": 9.482470784641069e-05,
      "loss": 0.2673,
      "step": 1580
    },
    {
      "epoch": 2.635,
      "grad_norm": 0.05443704500794411,
      "learning_rate": 9.475792988313858e-05,
      "loss": 0.349,
      "step": 1581
    },
    {
      "epoch": 2.6366666666666667,
      "grad_norm": 0.0599311962723732,
      "learning_rate": 9.469115191986645e-05,
      "loss": 0.3955,
      "step": 1582
    },
    {
      "epoch": 2.638333333333333,
      "grad_norm": 0.03956799954175949,
      "learning_rate": 9.462437395659432e-05,
      "loss": 0.2908,
      "step": 1583
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.046994637697935104,
      "learning_rate": 9.455759599332221e-05,
      "loss": 0.2848,
      "step": 1584
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 0.041222453117370605,
      "learning_rate": 9.449081803005008e-05,
      "loss": 0.2998,
      "step": 1585
    },
    {
      "epoch": 2.6433333333333335,
      "grad_norm": 0.042495451867580414,
      "learning_rate": 9.442404006677797e-05,
      "loss": 0.2949,
      "step": 1586
    },
    {
      "epoch": 2.645,
      "grad_norm": 0.034285880625247955,
      "learning_rate": 9.435726210350586e-05,
      "loss": 0.252,
      "step": 1587
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.06452673673629761,
      "learning_rate": 9.429048414023373e-05,
      "loss": 0.3969,
      "step": 1588
    },
    {
      "epoch": 2.6483333333333334,
      "grad_norm": 0.06244054064154625,
      "learning_rate": 9.422370617696162e-05,
      "loss": 0.4138,
      "step": 1589
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.03866592049598694,
      "learning_rate": 9.415692821368949e-05,
      "loss": 0.2497,
      "step": 1590
    },
    {
      "epoch": 2.6516666666666664,
      "grad_norm": 0.0330529659986496,
      "learning_rate": 9.409015025041736e-05,
      "loss": 0.2321,
      "step": 1591
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.04372876510024071,
      "learning_rate": 9.402337228714525e-05,
      "loss": 0.2892,
      "step": 1592
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 0.035646259784698486,
      "learning_rate": 9.395659432387312e-05,
      "loss": 0.2279,
      "step": 1593
    },
    {
      "epoch": 2.6566666666666667,
      "grad_norm": 0.045436251908540726,
      "learning_rate": 9.388981636060101e-05,
      "loss": 0.2637,
      "step": 1594
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 0.05178976058959961,
      "learning_rate": 9.38230383973289e-05,
      "loss": 0.3277,
      "step": 1595
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.04672107845544815,
      "learning_rate": 9.375626043405677e-05,
      "loss": 0.3213,
      "step": 1596
    },
    {
      "epoch": 2.6616666666666666,
      "grad_norm": 0.04229549691081047,
      "learning_rate": 9.368948247078465e-05,
      "loss": 0.3025,
      "step": 1597
    },
    {
      "epoch": 2.663333333333333,
      "grad_norm": 0.03946895897388458,
      "learning_rate": 9.362270450751253e-05,
      "loss": 0.3214,
      "step": 1598
    },
    {
      "epoch": 2.665,
      "grad_norm": 0.04942112788558006,
      "learning_rate": 9.35559265442404e-05,
      "loss": 0.3219,
      "step": 1599
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.041279736906290054,
      "learning_rate": 9.348914858096829e-05,
      "loss": 0.3202,
      "step": 1600
    },
    {
      "epoch": 2.6683333333333334,
      "grad_norm": 0.07345326989889145,
      "learning_rate": 9.342237061769616e-05,
      "loss": 0.3479,
      "step": 1601
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.035891275852918625,
      "learning_rate": 9.335559265442403e-05,
      "loss": 0.2839,
      "step": 1602
    },
    {
      "epoch": 2.671666666666667,
      "grad_norm": 0.034867335110902786,
      "learning_rate": 9.328881469115192e-05,
      "loss": 0.2773,
      "step": 1603
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.06647089123725891,
      "learning_rate": 9.322203672787981e-05,
      "loss": 0.38,
      "step": 1604
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.05582566559314728,
      "learning_rate": 9.31552587646077e-05,
      "loss": 0.3642,
      "step": 1605
    },
    {
      "epoch": 2.6766666666666667,
      "grad_norm": 0.05072063207626343,
      "learning_rate": 9.308848080133557e-05,
      "loss": 0.326,
      "step": 1606
    },
    {
      "epoch": 2.6783333333333332,
      "grad_norm": 0.044651731848716736,
      "learning_rate": 9.302170283806344e-05,
      "loss": 0.3458,
      "step": 1607
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.04370144009590149,
      "learning_rate": 9.295492487479133e-05,
      "loss": 0.3337,
      "step": 1608
    },
    {
      "epoch": 2.6816666666666666,
      "grad_norm": 0.04303613677620888,
      "learning_rate": 9.28881469115192e-05,
      "loss": 0.2946,
      "step": 1609
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.05299096927046776,
      "learning_rate": 9.282136894824707e-05,
      "loss": 0.361,
      "step": 1610
    },
    {
      "epoch": 2.685,
      "grad_norm": 0.04723408818244934,
      "learning_rate": 9.275459098497496e-05,
      "loss": 0.3377,
      "step": 1611
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.02858196385204792,
      "learning_rate": 9.268781302170285e-05,
      "loss": 0.2469,
      "step": 1612
    },
    {
      "epoch": 2.6883333333333335,
      "grad_norm": 0.04349818453192711,
      "learning_rate": 9.262103505843073e-05,
      "loss": 0.2925,
      "step": 1613
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.06297700107097626,
      "learning_rate": 9.255425709515861e-05,
      "loss": 0.2851,
      "step": 1614
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 0.053128376603126526,
      "learning_rate": 9.248747913188648e-05,
      "loss": 0.3748,
      "step": 1615
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.058875080198049545,
      "learning_rate": 9.242070116861437e-05,
      "loss": 0.3815,
      "step": 1616
    },
    {
      "epoch": 2.695,
      "grad_norm": 0.038760725408792496,
      "learning_rate": 9.235392320534224e-05,
      "loss": 0.2723,
      "step": 1617
    },
    {
      "epoch": 2.6966666666666668,
      "grad_norm": 0.0714057981967926,
      "learning_rate": 9.228714524207011e-05,
      "loss": 0.3152,
      "step": 1618
    },
    {
      "epoch": 2.6983333333333333,
      "grad_norm": 0.0540485605597496,
      "learning_rate": 9.2220367278798e-05,
      "loss": 0.2548,
      "step": 1619
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.061292435973882675,
      "learning_rate": 9.215358931552587e-05,
      "loss": 0.3041,
      "step": 1620
    },
    {
      "epoch": 2.7016666666666667,
      "grad_norm": 0.04033732786774635,
      "learning_rate": 9.208681135225376e-05,
      "loss": 0.2538,
      "step": 1621
    },
    {
      "epoch": 2.703333333333333,
      "grad_norm": 0.05250457301735878,
      "learning_rate": 9.202003338898165e-05,
      "loss": 0.3224,
      "step": 1622
    },
    {
      "epoch": 2.705,
      "grad_norm": 0.0576818473637104,
      "learning_rate": 9.195325542570952e-05,
      "loss": 0.3965,
      "step": 1623
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.055253662168979645,
      "learning_rate": 9.18864774624374e-05,
      "loss": 0.3434,
      "step": 1624
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 0.05126950517296791,
      "learning_rate": 9.181969949916528e-05,
      "loss": 0.3221,
      "step": 1625
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.04211192950606346,
      "learning_rate": 9.175292153589315e-05,
      "loss": 0.2552,
      "step": 1626
    },
    {
      "epoch": 2.711666666666667,
      "grad_norm": 0.052871424704790115,
      "learning_rate": 9.168614357262104e-05,
      "loss": 0.374,
      "step": 1627
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.046574316918849945,
      "learning_rate": 9.161936560934891e-05,
      "loss": 0.3223,
      "step": 1628
    },
    {
      "epoch": 2.715,
      "grad_norm": 0.04577081650495529,
      "learning_rate": 9.15525876460768e-05,
      "loss": 0.3171,
      "step": 1629
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.04292566701769829,
      "learning_rate": 9.148580968280469e-05,
      "loss": 0.327,
      "step": 1630
    },
    {
      "epoch": 2.7183333333333333,
      "grad_norm": 0.0694575384259224,
      "learning_rate": 9.141903171953256e-05,
      "loss": 0.3733,
      "step": 1631
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.08301645517349243,
      "learning_rate": 9.135225375626045e-05,
      "loss": 0.3759,
      "step": 1632
    },
    {
      "epoch": 2.7216666666666667,
      "grad_norm": 0.03549305349588394,
      "learning_rate": 9.128547579298832e-05,
      "loss": 0.2437,
      "step": 1633
    },
    {
      "epoch": 2.7233333333333336,
      "grad_norm": 0.04927077889442444,
      "learning_rate": 9.121869782971619e-05,
      "loss": 0.3222,
      "step": 1634
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.04516775906085968,
      "learning_rate": 9.115191986644408e-05,
      "loss": 0.3344,
      "step": 1635
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.06127592548727989,
      "learning_rate": 9.108514190317195e-05,
      "loss": 0.4096,
      "step": 1636
    },
    {
      "epoch": 2.7283333333333335,
      "grad_norm": 0.05938784033060074,
      "learning_rate": 9.101836393989984e-05,
      "loss": 0.3164,
      "step": 1637
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.04476083442568779,
      "learning_rate": 9.095158597662771e-05,
      "loss": 0.2722,
      "step": 1638
    },
    {
      "epoch": 2.7316666666666665,
      "grad_norm": 0.04067155718803406,
      "learning_rate": 9.08848080133556e-05,
      "loss": 0.2945,
      "step": 1639
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.04531254991889,
      "learning_rate": 9.081803005008348e-05,
      "loss": 0.3453,
      "step": 1640
    },
    {
      "epoch": 2.735,
      "grad_norm": 0.045985303819179535,
      "learning_rate": 9.075125208681136e-05,
      "loss": 0.322,
      "step": 1641
    },
    {
      "epoch": 2.736666666666667,
      "grad_norm": 0.03628100827336311,
      "learning_rate": 9.068447412353923e-05,
      "loss": 0.2737,
      "step": 1642
    },
    {
      "epoch": 2.7383333333333333,
      "grad_norm": 0.054699528962373734,
      "learning_rate": 9.061769616026712e-05,
      "loss": 0.2876,
      "step": 1643
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.06573758274316788,
      "learning_rate": 9.055091819699499e-05,
      "loss": 0.4059,
      "step": 1644
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 0.02731984667479992,
      "learning_rate": 9.048414023372288e-05,
      "loss": 0.1778,
      "step": 1645
    },
    {
      "epoch": 2.743333333333333,
      "grad_norm": 0.03958531469106674,
      "learning_rate": 9.041736227045075e-05,
      "loss": 0.3059,
      "step": 1646
    },
    {
      "epoch": 2.745,
      "grad_norm": 0.07306063920259476,
      "learning_rate": 9.035058430717864e-05,
      "loss": 0.2874,
      "step": 1647
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.032448045909404755,
      "learning_rate": 9.028380634390652e-05,
      "loss": 0.2178,
      "step": 1648
    },
    {
      "epoch": 2.748333333333333,
      "grad_norm": 0.09047801792621613,
      "learning_rate": 9.02170283806344e-05,
      "loss": 0.3987,
      "step": 1649
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.04923524707555771,
      "learning_rate": 9.015025041736227e-05,
      "loss": 0.3381,
      "step": 1650
    },
    {
      "epoch": 2.751666666666667,
      "grad_norm": 0.03262829780578613,
      "learning_rate": 9.008347245409016e-05,
      "loss": 0.2252,
      "step": 1651
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.03571411967277527,
      "learning_rate": 9.001669449081803e-05,
      "loss": 0.2323,
      "step": 1652
    },
    {
      "epoch": 2.755,
      "grad_norm": 0.059754423797130585,
      "learning_rate": 8.994991652754592e-05,
      "loss": 0.2835,
      "step": 1653
    },
    {
      "epoch": 2.756666666666667,
      "grad_norm": 0.03523053973913193,
      "learning_rate": 8.988313856427379e-05,
      "loss": 0.2376,
      "step": 1654
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 0.04821668565273285,
      "learning_rate": 8.981636060100166e-05,
      "loss": 0.3869,
      "step": 1655
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.04167263209819794,
      "learning_rate": 8.974958263772955e-05,
      "loss": 0.2559,
      "step": 1656
    },
    {
      "epoch": 2.7616666666666667,
      "grad_norm": 0.04648933559656143,
      "learning_rate": 8.968280467445744e-05,
      "loss": 0.3211,
      "step": 1657
    },
    {
      "epoch": 2.763333333333333,
      "grad_norm": 0.05502984672784805,
      "learning_rate": 8.961602671118531e-05,
      "loss": 0.3987,
      "step": 1658
    },
    {
      "epoch": 2.765,
      "grad_norm": 0.04678349569439888,
      "learning_rate": 8.95492487479132e-05,
      "loss": 0.2953,
      "step": 1659
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.04555510729551315,
      "learning_rate": 8.948247078464107e-05,
      "loss": 0.2564,
      "step": 1660
    },
    {
      "epoch": 2.7683333333333335,
      "grad_norm": 0.04498264566063881,
      "learning_rate": 8.941569282136896e-05,
      "loss": 0.3276,
      "step": 1661
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.06954893469810486,
      "learning_rate": 8.934891485809683e-05,
      "loss": 0.3483,
      "step": 1662
    },
    {
      "epoch": 2.7716666666666665,
      "grad_norm": 0.04065701737999916,
      "learning_rate": 8.92821368948247e-05,
      "loss": 0.2975,
      "step": 1663
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.05764603614807129,
      "learning_rate": 8.921535893155259e-05,
      "loss": 0.3684,
      "step": 1664
    },
    {
      "epoch": 2.775,
      "grad_norm": 0.039263349026441574,
      "learning_rate": 8.914858096828048e-05,
      "loss": 0.2915,
      "step": 1665
    },
    {
      "epoch": 2.7766666666666664,
      "grad_norm": 0.03536909818649292,
      "learning_rate": 8.908180300500835e-05,
      "loss": 0.2746,
      "step": 1666
    },
    {
      "epoch": 2.7783333333333333,
      "grad_norm": 0.04421917721629143,
      "learning_rate": 8.901502504173624e-05,
      "loss": 0.2854,
      "step": 1667
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.05380678176879883,
      "learning_rate": 8.894824707846411e-05,
      "loss": 0.3388,
      "step": 1668
    },
    {
      "epoch": 2.7816666666666667,
      "grad_norm": 0.04852959141135216,
      "learning_rate": 8.8881469115192e-05,
      "loss": 0.3108,
      "step": 1669
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.03634675592184067,
      "learning_rate": 8.881469115191987e-05,
      "loss": 0.2777,
      "step": 1670
    },
    {
      "epoch": 2.785,
      "grad_norm": 0.03994840011000633,
      "learning_rate": 8.874791318864774e-05,
      "loss": 0.2843,
      "step": 1671
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.052810005843639374,
      "learning_rate": 8.868113522537563e-05,
      "loss": 0.351,
      "step": 1672
    },
    {
      "epoch": 2.788333333333333,
      "grad_norm": 0.03771612420678139,
      "learning_rate": 8.86143572621035e-05,
      "loss": 0.2683,
      "step": 1673
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.048753317445516586,
      "learning_rate": 8.854757929883139e-05,
      "loss": 0.283,
      "step": 1674
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 0.0476420521736145,
      "learning_rate": 8.848080133555928e-05,
      "loss": 0.2877,
      "step": 1675
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.04404391720890999,
      "learning_rate": 8.841402337228715e-05,
      "loss": 0.3229,
      "step": 1676
    },
    {
      "epoch": 2.795,
      "grad_norm": 0.05693819001317024,
      "learning_rate": 8.834724540901504e-05,
      "loss": 0.3183,
      "step": 1677
    },
    {
      "epoch": 2.796666666666667,
      "grad_norm": 0.07419586181640625,
      "learning_rate": 8.828046744574291e-05,
      "loss": 0.3624,
      "step": 1678
    },
    {
      "epoch": 2.7983333333333333,
      "grad_norm": 0.057663727551698685,
      "learning_rate": 8.821368948247078e-05,
      "loss": 0.354,
      "step": 1679
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.04275108501315117,
      "learning_rate": 8.814691151919867e-05,
      "loss": 0.3108,
      "step": 1680
    },
    {
      "epoch": 2.8016666666666667,
      "grad_norm": 0.037055253982543945,
      "learning_rate": 8.808013355592654e-05,
      "loss": 0.2945,
      "step": 1681
    },
    {
      "epoch": 2.8033333333333332,
      "grad_norm": 0.048453010618686676,
      "learning_rate": 8.801335559265443e-05,
      "loss": 0.2973,
      "step": 1682
    },
    {
      "epoch": 2.805,
      "grad_norm": 0.048249486833810806,
      "learning_rate": 8.794657762938232e-05,
      "loss": 0.2872,
      "step": 1683
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.05002042278647423,
      "learning_rate": 8.787979966611019e-05,
      "loss": 0.3281,
      "step": 1684
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 0.05146430432796478,
      "learning_rate": 8.781302170283808e-05,
      "loss": 0.3286,
      "step": 1685
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.049254562705755234,
      "learning_rate": 8.774624373956595e-05,
      "loss": 0.3284,
      "step": 1686
    },
    {
      "epoch": 2.8116666666666665,
      "grad_norm": 0.04307960346341133,
      "learning_rate": 8.767946577629382e-05,
      "loss": 0.2929,
      "step": 1687
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.06477370858192444,
      "learning_rate": 8.761268781302171e-05,
      "loss": 0.3812,
      "step": 1688
    },
    {
      "epoch": 2.815,
      "grad_norm": 0.04973062500357628,
      "learning_rate": 8.754590984974958e-05,
      "loss": 0.3509,
      "step": 1689
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.034220196306705475,
      "learning_rate": 8.747913188647745e-05,
      "loss": 0.2196,
      "step": 1690
    },
    {
      "epoch": 2.8183333333333334,
      "grad_norm": 0.04354066029191017,
      "learning_rate": 8.741235392320535e-05,
      "loss": 0.3124,
      "step": 1691
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.03348344936966896,
      "learning_rate": 8.734557595993323e-05,
      "loss": 0.288,
      "step": 1692
    },
    {
      "epoch": 2.8216666666666668,
      "grad_norm": 0.040099214762449265,
      "learning_rate": 8.727879799666111e-05,
      "loss": 0.2757,
      "step": 1693
    },
    {
      "epoch": 2.8233333333333333,
      "grad_norm": 0.0380651131272316,
      "learning_rate": 8.721202003338899e-05,
      "loss": 0.2825,
      "step": 1694
    },
    {
      "epoch": 2.825,
      "grad_norm": 0.04457317665219307,
      "learning_rate": 8.714524207011686e-05,
      "loss": 0.2814,
      "step": 1695
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.05216367170214653,
      "learning_rate": 8.707846410684475e-05,
      "loss": 0.3297,
      "step": 1696
    },
    {
      "epoch": 2.828333333333333,
      "grad_norm": 0.03690806031227112,
      "learning_rate": 8.701168614357262e-05,
      "loss": 0.2924,
      "step": 1697
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.03283984586596489,
      "learning_rate": 8.694490818030051e-05,
      "loss": 0.2539,
      "step": 1698
    },
    {
      "epoch": 2.8316666666666666,
      "grad_norm": 0.04777517169713974,
      "learning_rate": 8.687813021702838e-05,
      "loss": 0.3179,
      "step": 1699
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.05054415017366409,
      "learning_rate": 8.681135225375627e-05,
      "loss": 0.3879,
      "step": 1700
    },
    {
      "epoch": 2.835,
      "grad_norm": 0.05280664190649986,
      "learning_rate": 8.674457429048415e-05,
      "loss": 0.3802,
      "step": 1701
    },
    {
      "epoch": 2.836666666666667,
      "grad_norm": 0.04612463712692261,
      "learning_rate": 8.667779632721203e-05,
      "loss": 0.2845,
      "step": 1702
    },
    {
      "epoch": 2.8383333333333334,
      "grad_norm": 0.04250083863735199,
      "learning_rate": 8.66110183639399e-05,
      "loss": 0.3297,
      "step": 1703
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.038179587572813034,
      "learning_rate": 8.654424040066779e-05,
      "loss": 0.2876,
      "step": 1704
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 0.04308481141924858,
      "learning_rate": 8.647746243739566e-05,
      "loss": 0.2928,
      "step": 1705
    },
    {
      "epoch": 2.8433333333333333,
      "grad_norm": 0.04304201155900955,
      "learning_rate": 8.641068447412355e-05,
      "loss": 0.32,
      "step": 1706
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 0.04328133165836334,
      "learning_rate": 8.634390651085142e-05,
      "loss": 0.3312,
      "step": 1707
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.06877147406339645,
      "learning_rate": 8.62771285475793e-05,
      "loss": 0.3094,
      "step": 1708
    },
    {
      "epoch": 2.8483333333333336,
      "grad_norm": 0.035699132829904556,
      "learning_rate": 8.62103505843072e-05,
      "loss": 0.281,
      "step": 1709
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.03639662265777588,
      "learning_rate": 8.614357262103507e-05,
      "loss": 0.3104,
      "step": 1710
    },
    {
      "epoch": 2.8516666666666666,
      "grad_norm": 0.052675940096378326,
      "learning_rate": 8.607679465776294e-05,
      "loss": 0.3565,
      "step": 1711
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.03498644754290581,
      "learning_rate": 8.601001669449083e-05,
      "loss": 0.2892,
      "step": 1712
    },
    {
      "epoch": 2.855,
      "grad_norm": 0.06445112079381943,
      "learning_rate": 8.59432387312187e-05,
      "loss": 0.3519,
      "step": 1713
    },
    {
      "epoch": 2.8566666666666665,
      "grad_norm": 0.06771146506071091,
      "learning_rate": 8.587646076794659e-05,
      "loss": 0.3812,
      "step": 1714
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.036643415689468384,
      "learning_rate": 8.580968280467446e-05,
      "loss": 0.319,
      "step": 1715
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.05764908716082573,
      "learning_rate": 8.574290484140233e-05,
      "loss": 0.3843,
      "step": 1716
    },
    {
      "epoch": 2.861666666666667,
      "grad_norm": 0.0437035970389843,
      "learning_rate": 8.567612687813022e-05,
      "loss": 0.3152,
      "step": 1717
    },
    {
      "epoch": 2.8633333333333333,
      "grad_norm": 0.0572824664413929,
      "learning_rate": 8.56093489148581e-05,
      "loss": 0.375,
      "step": 1718
    },
    {
      "epoch": 2.865,
      "grad_norm": 0.0432576984167099,
      "learning_rate": 8.554257095158598e-05,
      "loss": 0.3062,
      "step": 1719
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.05589733645319939,
      "learning_rate": 8.547579298831387e-05,
      "loss": 0.3952,
      "step": 1720
    },
    {
      "epoch": 2.868333333333333,
      "grad_norm": 0.03899713233113289,
      "learning_rate": 8.540901502504174e-05,
      "loss": 0.299,
      "step": 1721
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.045164573937654495,
      "learning_rate": 8.534223706176963e-05,
      "loss": 0.344,
      "step": 1722
    },
    {
      "epoch": 2.8716666666666666,
      "grad_norm": 0.034284889698028564,
      "learning_rate": 8.52754590984975e-05,
      "loss": 0.259,
      "step": 1723
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.03649614006280899,
      "learning_rate": 8.520868113522537e-05,
      "loss": 0.2798,
      "step": 1724
    },
    {
      "epoch": 2.875,
      "grad_norm": 0.07365485280752182,
      "learning_rate": 8.514190317195326e-05,
      "loss": 0.3779,
      "step": 1725
    },
    {
      "epoch": 2.876666666666667,
      "grad_norm": 0.04037002846598625,
      "learning_rate": 8.507512520868115e-05,
      "loss": 0.3013,
      "step": 1726
    },
    {
      "epoch": 2.8783333333333334,
      "grad_norm": 0.07766978442668915,
      "learning_rate": 8.500834724540902e-05,
      "loss": 0.382,
      "step": 1727
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.042778875678777695,
      "learning_rate": 8.49415692821369e-05,
      "loss": 0.2868,
      "step": 1728
    },
    {
      "epoch": 2.881666666666667,
      "grad_norm": 0.04713287949562073,
      "learning_rate": 8.487479131886478e-05,
      "loss": 0.3331,
      "step": 1729
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.038159728050231934,
      "learning_rate": 8.480801335559267e-05,
      "loss": 0.2741,
      "step": 1730
    },
    {
      "epoch": 2.885,
      "grad_norm": 0.04001330956816673,
      "learning_rate": 8.474123539232054e-05,
      "loss": 0.3012,
      "step": 1731
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.07030853629112244,
      "learning_rate": 8.467445742904841e-05,
      "loss": 0.3328,
      "step": 1732
    },
    {
      "epoch": 2.888333333333333,
      "grad_norm": 0.03826836869120598,
      "learning_rate": 8.46076794657763e-05,
      "loss": 0.2787,
      "step": 1733
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.039966873824596405,
      "learning_rate": 8.454090150250417e-05,
      "loss": 0.2491,
      "step": 1734
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 0.044166114181280136,
      "learning_rate": 8.447412353923206e-05,
      "loss": 0.3631,
      "step": 1735
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.05149497091770172,
      "learning_rate": 8.440734557595994e-05,
      "loss": 0.336,
      "step": 1736
    },
    {
      "epoch": 2.895,
      "grad_norm": 0.031434644013643265,
      "learning_rate": 8.434056761268782e-05,
      "loss": 0.2126,
      "step": 1737
    },
    {
      "epoch": 2.8966666666666665,
      "grad_norm": 0.04810778424143791,
      "learning_rate": 8.42737896494157e-05,
      "loss": 0.3083,
      "step": 1738
    },
    {
      "epoch": 2.8983333333333334,
      "grad_norm": 0.041820794343948364,
      "learning_rate": 8.420701168614358e-05,
      "loss": 0.2776,
      "step": 1739
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.04950530081987381,
      "learning_rate": 8.414023372287145e-05,
      "loss": 0.3464,
      "step": 1740
    },
    {
      "epoch": 2.9016666666666664,
      "grad_norm": 0.04157741740345955,
      "learning_rate": 8.407345575959934e-05,
      "loss": 0.3118,
      "step": 1741
    },
    {
      "epoch": 2.9033333333333333,
      "grad_norm": 0.03876209259033203,
      "learning_rate": 8.400667779632721e-05,
      "loss": 0.3357,
      "step": 1742
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 0.05652514472603798,
      "learning_rate": 8.39398998330551e-05,
      "loss": 0.3483,
      "step": 1743
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.041300103068351746,
      "learning_rate": 8.387312186978298e-05,
      "loss": 0.3272,
      "step": 1744
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 0.041619833558797836,
      "learning_rate": 8.380634390651086e-05,
      "loss": 0.2874,
      "step": 1745
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.05518817901611328,
      "learning_rate": 8.373956594323874e-05,
      "loss": 0.3453,
      "step": 1746
    },
    {
      "epoch": 2.9116666666666666,
      "grad_norm": 0.0516633503139019,
      "learning_rate": 8.367278797996662e-05,
      "loss": 0.3365,
      "step": 1747
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.05352460592985153,
      "learning_rate": 8.360601001669449e-05,
      "loss": 0.3252,
      "step": 1748
    },
    {
      "epoch": 2.915,
      "grad_norm": 0.060561489313840866,
      "learning_rate": 8.353923205342238e-05,
      "loss": 0.3342,
      "step": 1749
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.04378483071923256,
      "learning_rate": 8.347245409015025e-05,
      "loss": 0.3631,
      "step": 1750
    },
    {
      "epoch": 2.9183333333333334,
      "grad_norm": 0.05210136994719505,
      "learning_rate": 8.340567612687812e-05,
      "loss": 0.3417,
      "step": 1751
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.040732674300670624,
      "learning_rate": 8.333889816360601e-05,
      "loss": 0.3023,
      "step": 1752
    },
    {
      "epoch": 2.921666666666667,
      "grad_norm": 0.05952214449644089,
      "learning_rate": 8.32721202003339e-05,
      "loss": 0.3326,
      "step": 1753
    },
    {
      "epoch": 2.9233333333333333,
      "grad_norm": 0.03842893987894058,
      "learning_rate": 8.320534223706178e-05,
      "loss": 0.2792,
      "step": 1754
    },
    {
      "epoch": 2.925,
      "grad_norm": 0.040204744786024094,
      "learning_rate": 8.313856427378966e-05,
      "loss": 0.2297,
      "step": 1755
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.04274682700634003,
      "learning_rate": 8.307178631051753e-05,
      "loss": 0.3367,
      "step": 1756
    },
    {
      "epoch": 2.9283333333333332,
      "grad_norm": 0.053967397660017014,
      "learning_rate": 8.300500834724542e-05,
      "loss": 0.3102,
      "step": 1757
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.05331723392009735,
      "learning_rate": 8.293823038397329e-05,
      "loss": 0.3046,
      "step": 1758
    },
    {
      "epoch": 2.9316666666666666,
      "grad_norm": 0.042656060308218,
      "learning_rate": 8.287145242070116e-05,
      "loss": 0.2577,
      "step": 1759
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.04243306443095207,
      "learning_rate": 8.280467445742905e-05,
      "loss": 0.3295,
      "step": 1760
    },
    {
      "epoch": 2.935,
      "grad_norm": 0.036988988518714905,
      "learning_rate": 8.273789649415694e-05,
      "loss": 0.2866,
      "step": 1761
    },
    {
      "epoch": 2.9366666666666665,
      "grad_norm": 0.03810156509280205,
      "learning_rate": 8.267111853088482e-05,
      "loss": 0.3093,
      "step": 1762
    },
    {
      "epoch": 2.9383333333333335,
      "grad_norm": 0.031263768672943115,
      "learning_rate": 8.26043405676127e-05,
      "loss": 0.2352,
      "step": 1763
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.047868791967630386,
      "learning_rate": 8.253756260434057e-05,
      "loss": 0.3082,
      "step": 1764
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 0.035331618040800095,
      "learning_rate": 8.247078464106846e-05,
      "loss": 0.2885,
      "step": 1765
    },
    {
      "epoch": 2.9433333333333334,
      "grad_norm": 0.03622666373848915,
      "learning_rate": 8.240400667779633e-05,
      "loss": 0.2991,
      "step": 1766
    },
    {
      "epoch": 2.945,
      "grad_norm": 0.03794525936245918,
      "learning_rate": 8.23372287145242e-05,
      "loss": 0.3112,
      "step": 1767
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.05157243087887764,
      "learning_rate": 8.227045075125209e-05,
      "loss": 0.3045,
      "step": 1768
    },
    {
      "epoch": 2.9483333333333333,
      "grad_norm": 0.03859364241361618,
      "learning_rate": 8.220367278797996e-05,
      "loss": 0.2398,
      "step": 1769
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.03938182815909386,
      "learning_rate": 8.213689482470785e-05,
      "loss": 0.3298,
      "step": 1770
    },
    {
      "epoch": 2.9516666666666667,
      "grad_norm": 0.041797276586294174,
      "learning_rate": 8.207011686143574e-05,
      "loss": 0.3214,
      "step": 1771
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.045228779315948486,
      "learning_rate": 8.200333889816361e-05,
      "loss": 0.3121,
      "step": 1772
    },
    {
      "epoch": 2.955,
      "grad_norm": 0.04828165844082832,
      "learning_rate": 8.19365609348915e-05,
      "loss": 0.342,
      "step": 1773
    },
    {
      "epoch": 2.9566666666666666,
      "grad_norm": 0.0464993454515934,
      "learning_rate": 8.186978297161937e-05,
      "loss": 0.318,
      "step": 1774
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 0.04298202693462372,
      "learning_rate": 8.180300500834724e-05,
      "loss": 0.301,
      "step": 1775
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.06733006238937378,
      "learning_rate": 8.173622704507513e-05,
      "loss": 0.3102,
      "step": 1776
    },
    {
      "epoch": 2.961666666666667,
      "grad_norm": 0.06587858498096466,
      "learning_rate": 8.1669449081803e-05,
      "loss": 0.3423,
      "step": 1777
    },
    {
      "epoch": 2.9633333333333334,
      "grad_norm": 0.042274948209524155,
      "learning_rate": 8.160267111853089e-05,
      "loss": 0.2757,
      "step": 1778
    },
    {
      "epoch": 2.965,
      "grad_norm": 0.04579617828130722,
      "learning_rate": 8.153589315525877e-05,
      "loss": 0.3273,
      "step": 1779
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.049923937767744064,
      "learning_rate": 8.146911519198665e-05,
      "loss": 0.3379,
      "step": 1780
    },
    {
      "epoch": 2.9683333333333333,
      "grad_norm": 0.044286128133535385,
      "learning_rate": 8.140233722871453e-05,
      "loss": 0.3186,
      "step": 1781
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.027619585394859314,
      "learning_rate": 8.133555926544241e-05,
      "loss": 0.2405,
      "step": 1782
    },
    {
      "epoch": 2.9716666666666667,
      "grad_norm": 0.08982249349355698,
      "learning_rate": 8.126878130217028e-05,
      "loss": 0.2677,
      "step": 1783
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.0562853142619133,
      "learning_rate": 8.120200333889817e-05,
      "loss": 0.2726,
      "step": 1784
    },
    {
      "epoch": 2.975,
      "grad_norm": 0.036661453545093536,
      "learning_rate": 8.113522537562604e-05,
      "loss": 0.2634,
      "step": 1785
    },
    {
      "epoch": 2.9766666666666666,
      "grad_norm": 0.08005508780479431,
      "learning_rate": 8.106844741235393e-05,
      "loss": 0.3435,
      "step": 1786
    },
    {
      "epoch": 2.9783333333333335,
      "grad_norm": 0.04153050482273102,
      "learning_rate": 8.10016694490818e-05,
      "loss": 0.2513,
      "step": 1787
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.04377968609333038,
      "learning_rate": 8.093489148580969e-05,
      "loss": 0.3529,
      "step": 1788
    },
    {
      "epoch": 2.9816666666666665,
      "grad_norm": 0.052492160350084305,
      "learning_rate": 8.086811352253757e-05,
      "loss": 0.36,
      "step": 1789
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.06879901885986328,
      "learning_rate": 8.080133555926545e-05,
      "loss": 0.3221,
      "step": 1790
    },
    {
      "epoch": 2.985,
      "grad_norm": 0.04712841659784317,
      "learning_rate": 8.073455759599332e-05,
      "loss": 0.3526,
      "step": 1791
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.038678959012031555,
      "learning_rate": 8.066777963272121e-05,
      "loss": 0.2906,
      "step": 1792
    },
    {
      "epoch": 2.9883333333333333,
      "grad_norm": 0.05625537037849426,
      "learning_rate": 8.060100166944908e-05,
      "loss": 0.2988,
      "step": 1793
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.04601704329252243,
      "learning_rate": 8.053422370617697e-05,
      "loss": 0.295,
      "step": 1794
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 0.036263540387153625,
      "learning_rate": 8.046744574290484e-05,
      "loss": 0.2999,
      "step": 1795
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.057181768119335175,
      "learning_rate": 8.040066777963273e-05,
      "loss": 0.3838,
      "step": 1796
    },
    {
      "epoch": 2.995,
      "grad_norm": 0.06910249590873718,
      "learning_rate": 8.033388981636061e-05,
      "loss": 0.3164,
      "step": 1797
    },
    {
      "epoch": 2.9966666666666666,
      "grad_norm": 0.030871175229549408,
      "learning_rate": 8.026711185308849e-05,
      "loss": 0.2584,
      "step": 1798
    },
    {
      "epoch": 2.998333333333333,
      "grad_norm": 0.040770117193460464,
      "learning_rate": 8.020033388981636e-05,
      "loss": 0.3354,
      "step": 1799
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.0421888493001461,
      "learning_rate": 8.013355592654425e-05,
      "loss": 0.2723,
      "step": 1800
    },
    {
      "epoch": 3.0016666666666665,
      "grad_norm": 0.04541584104299545,
      "learning_rate": 8.006677796327212e-05,
      "loss": 0.3686,
      "step": 1801
    },
    {
      "epoch": 3.0033333333333334,
      "grad_norm": 0.03528640791773796,
      "learning_rate": 8e-05,
      "loss": 0.2751,
      "step": 1802
    },
    {
      "epoch": 3.005,
      "grad_norm": 0.04031870886683464,
      "learning_rate": 7.993322203672788e-05,
      "loss": 0.2954,
      "step": 1803
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.037244126200675964,
      "learning_rate": 7.986644407345575e-05,
      "loss": 0.2842,
      "step": 1804
    },
    {
      "epoch": 3.0083333333333333,
      "grad_norm": 0.049293097108602524,
      "learning_rate": 7.979966611018364e-05,
      "loss": 0.3172,
      "step": 1805
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.03121976926922798,
      "learning_rate": 7.973288814691153e-05,
      "loss": 0.2058,
      "step": 1806
    },
    {
      "epoch": 3.0116666666666667,
      "grad_norm": 0.040222346782684326,
      "learning_rate": 7.96661101836394e-05,
      "loss": 0.3107,
      "step": 1807
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.03151959553360939,
      "learning_rate": 7.959933222036729e-05,
      "loss": 0.2514,
      "step": 1808
    },
    {
      "epoch": 3.015,
      "grad_norm": 0.050796665251255035,
      "learning_rate": 7.953255425709516e-05,
      "loss": 0.303,
      "step": 1809
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 0.04309459030628204,
      "learning_rate": 7.946577629382305e-05,
      "loss": 0.3128,
      "step": 1810
    },
    {
      "epoch": 3.0183333333333335,
      "grad_norm": 0.04955955222249031,
      "learning_rate": 7.939899833055092e-05,
      "loss": 0.3231,
      "step": 1811
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.05283171311020851,
      "learning_rate": 7.933222036727879e-05,
      "loss": 0.3517,
      "step": 1812
    },
    {
      "epoch": 3.0216666666666665,
      "grad_norm": 0.04427511990070343,
      "learning_rate": 7.926544240400668e-05,
      "loss": 0.33,
      "step": 1813
    },
    {
      "epoch": 3.0233333333333334,
      "grad_norm": 0.054537151008844376,
      "learning_rate": 7.919866444073457e-05,
      "loss": 0.2533,
      "step": 1814
    },
    {
      "epoch": 3.025,
      "grad_norm": 0.031284116208553314,
      "learning_rate": 7.913188647746244e-05,
      "loss": 0.2362,
      "step": 1815
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.04416152834892273,
      "learning_rate": 7.906510851419033e-05,
      "loss": 0.2892,
      "step": 1816
    },
    {
      "epoch": 3.0283333333333333,
      "grad_norm": 0.05115003511309624,
      "learning_rate": 7.89983305509182e-05,
      "loss": 0.3221,
      "step": 1817
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.051557354629039764,
      "learning_rate": 7.893155258764609e-05,
      "loss": 0.324,
      "step": 1818
    },
    {
      "epoch": 3.0316666666666667,
      "grad_norm": 0.06388100236654282,
      "learning_rate": 7.886477462437396e-05,
      "loss": 0.3682,
      "step": 1819
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.04635249823331833,
      "learning_rate": 7.879799666110183e-05,
      "loss": 0.314,
      "step": 1820
    },
    {
      "epoch": 3.035,
      "grad_norm": 0.042842913419008255,
      "learning_rate": 7.873121869782972e-05,
      "loss": 0.3239,
      "step": 1821
    },
    {
      "epoch": 3.0366666666666666,
      "grad_norm": 0.051439277827739716,
      "learning_rate": 7.86644407345576e-05,
      "loss": 0.3074,
      "step": 1822
    },
    {
      "epoch": 3.038333333333333,
      "grad_norm": 0.0634741485118866,
      "learning_rate": 7.859766277128548e-05,
      "loss": 0.3309,
      "step": 1823
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.051695555448532104,
      "learning_rate": 7.853088480801337e-05,
      "loss": 0.2812,
      "step": 1824
    },
    {
      "epoch": 3.0416666666666665,
      "grad_norm": 0.04394450783729553,
      "learning_rate": 7.846410684474124e-05,
      "loss": 0.2964,
      "step": 1825
    },
    {
      "epoch": 3.0433333333333334,
      "grad_norm": 0.040015123784542084,
      "learning_rate": 7.839732888146912e-05,
      "loss": 0.2892,
      "step": 1826
    },
    {
      "epoch": 3.045,
      "grad_norm": 0.05006762966513634,
      "learning_rate": 7.8330550918197e-05,
      "loss": 0.2965,
      "step": 1827
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.0505928210914135,
      "learning_rate": 7.826377295492487e-05,
      "loss": 0.3236,
      "step": 1828
    },
    {
      "epoch": 3.0483333333333333,
      "grad_norm": 0.04971438646316528,
      "learning_rate": 7.819699499165276e-05,
      "loss": 0.3287,
      "step": 1829
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.03221292793750763,
      "learning_rate": 7.813021702838063e-05,
      "loss": 0.2524,
      "step": 1830
    },
    {
      "epoch": 3.0516666666666667,
      "grad_norm": 0.037283755838871,
      "learning_rate": 7.806343906510852e-05,
      "loss": 0.2493,
      "step": 1831
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.04357919096946716,
      "learning_rate": 7.79966611018364e-05,
      "loss": 0.3119,
      "step": 1832
    },
    {
      "epoch": 3.055,
      "grad_norm": 0.040828146040439606,
      "learning_rate": 7.792988313856428e-05,
      "loss": 0.314,
      "step": 1833
    },
    {
      "epoch": 3.0566666666666666,
      "grad_norm": 0.043523870408535004,
      "learning_rate": 7.786310517529216e-05,
      "loss": 0.3051,
      "step": 1834
    },
    {
      "epoch": 3.058333333333333,
      "grad_norm": 0.0597648024559021,
      "learning_rate": 7.779632721202004e-05,
      "loss": 0.3587,
      "step": 1835
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.03658333420753479,
      "learning_rate": 7.772954924874791e-05,
      "loss": 0.2758,
      "step": 1836
    },
    {
      "epoch": 3.0616666666666665,
      "grad_norm": 0.04200690612196922,
      "learning_rate": 7.76627712854758e-05,
      "loss": 0.3287,
      "step": 1837
    },
    {
      "epoch": 3.0633333333333335,
      "grad_norm": 0.03709191456437111,
      "learning_rate": 7.759599332220367e-05,
      "loss": 0.2802,
      "step": 1838
    },
    {
      "epoch": 3.065,
      "grad_norm": 0.05215577036142349,
      "learning_rate": 7.752921535893156e-05,
      "loss": 0.3031,
      "step": 1839
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.03720850497484207,
      "learning_rate": 7.746243739565944e-05,
      "loss": 0.2727,
      "step": 1840
    },
    {
      "epoch": 3.0683333333333334,
      "grad_norm": 0.04934772104024887,
      "learning_rate": 7.739565943238732e-05,
      "loss": 0.3307,
      "step": 1841
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.07080462574958801,
      "learning_rate": 7.73288814691152e-05,
      "loss": 0.3784,
      "step": 1842
    },
    {
      "epoch": 3.0716666666666668,
      "grad_norm": 0.06534583121538162,
      "learning_rate": 7.726210350584308e-05,
      "loss": 0.306,
      "step": 1843
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.03852129727602005,
      "learning_rate": 7.719532554257095e-05,
      "loss": 0.256,
      "step": 1844
    },
    {
      "epoch": 3.075,
      "grad_norm": 0.04578801989555359,
      "learning_rate": 7.712854757929884e-05,
      "loss": 0.2988,
      "step": 1845
    },
    {
      "epoch": 3.0766666666666667,
      "grad_norm": 0.05437009036540985,
      "learning_rate": 7.706176961602671e-05,
      "loss": 0.2685,
      "step": 1846
    },
    {
      "epoch": 3.078333333333333,
      "grad_norm": 0.04440987482666969,
      "learning_rate": 7.69949916527546e-05,
      "loss": 0.2748,
      "step": 1847
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.046628404408693314,
      "learning_rate": 7.692821368948247e-05,
      "loss": 0.2918,
      "step": 1848
    },
    {
      "epoch": 3.0816666666666666,
      "grad_norm": 0.05074763670563698,
      "learning_rate": 7.686143572621036e-05,
      "loss": 0.3102,
      "step": 1849
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 0.03968515619635582,
      "learning_rate": 7.679465776293824e-05,
      "loss": 0.3023,
      "step": 1850
    },
    {
      "epoch": 3.085,
      "grad_norm": 0.06132771074771881,
      "learning_rate": 7.672787979966612e-05,
      "loss": 0.3124,
      "step": 1851
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.08465270698070526,
      "learning_rate": 7.666110183639399e-05,
      "loss": 0.3258,
      "step": 1852
    },
    {
      "epoch": 3.0883333333333334,
      "grad_norm": 0.03618341684341431,
      "learning_rate": 7.659432387312188e-05,
      "loss": 0.2581,
      "step": 1853
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.05559894070029259,
      "learning_rate": 7.652754590984975e-05,
      "loss": 0.3796,
      "step": 1854
    },
    {
      "epoch": 3.091666666666667,
      "grad_norm": 0.049053747206926346,
      "learning_rate": 7.646076794657764e-05,
      "loss": 0.3,
      "step": 1855
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.04042108729481697,
      "learning_rate": 7.639398998330551e-05,
      "loss": 0.2373,
      "step": 1856
    },
    {
      "epoch": 3.095,
      "grad_norm": 0.03619570657610893,
      "learning_rate": 7.63272120200334e-05,
      "loss": 0.2385,
      "step": 1857
    },
    {
      "epoch": 3.0966666666666667,
      "grad_norm": 0.08667860180139542,
      "learning_rate": 7.626043405676128e-05,
      "loss": 0.3592,
      "step": 1858
    },
    {
      "epoch": 3.098333333333333,
      "grad_norm": 0.04978185519576073,
      "learning_rate": 7.619365609348916e-05,
      "loss": 0.3545,
      "step": 1859
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.06837813556194305,
      "learning_rate": 7.612687813021703e-05,
      "loss": 0.318,
      "step": 1860
    },
    {
      "epoch": 3.1016666666666666,
      "grad_norm": 0.04039027914404869,
      "learning_rate": 7.606010016694492e-05,
      "loss": 0.2626,
      "step": 1861
    },
    {
      "epoch": 3.1033333333333335,
      "grad_norm": 0.05216934531927109,
      "learning_rate": 7.599332220367279e-05,
      "loss": 0.3448,
      "step": 1862
    },
    {
      "epoch": 3.105,
      "grad_norm": 0.053990527987480164,
      "learning_rate": 7.592654424040068e-05,
      "loss": 0.2916,
      "step": 1863
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.04477456957101822,
      "learning_rate": 7.585976627712855e-05,
      "loss": 0.2884,
      "step": 1864
    },
    {
      "epoch": 3.1083333333333334,
      "grad_norm": 0.04697180539369583,
      "learning_rate": 7.579298831385642e-05,
      "loss": 0.3077,
      "step": 1865
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.04573246091604233,
      "learning_rate": 7.572621035058431e-05,
      "loss": 0.3236,
      "step": 1866
    },
    {
      "epoch": 3.111666666666667,
      "grad_norm": 0.045280877500772476,
      "learning_rate": 7.56594323873122e-05,
      "loss": 0.3196,
      "step": 1867
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.04241721332073212,
      "learning_rate": 7.559265442404007e-05,
      "loss": 0.2885,
      "step": 1868
    },
    {
      "epoch": 3.115,
      "grad_norm": 0.03630904480814934,
      "learning_rate": 7.552587646076796e-05,
      "loss": 0.2906,
      "step": 1869
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 0.04076362028717995,
      "learning_rate": 7.545909849749583e-05,
      "loss": 0.2576,
      "step": 1870
    },
    {
      "epoch": 3.118333333333333,
      "grad_norm": 0.05942670628428459,
      "learning_rate": 7.539232053422371e-05,
      "loss": 0.3588,
      "step": 1871
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.044322967529296875,
      "learning_rate": 7.532554257095159e-05,
      "loss": 0.2883,
      "step": 1872
    },
    {
      "epoch": 3.1216666666666666,
      "grad_norm": 0.06606008857488632,
      "learning_rate": 7.525876460767946e-05,
      "loss": 0.32,
      "step": 1873
    },
    {
      "epoch": 3.1233333333333335,
      "grad_norm": 0.05665998533368111,
      "learning_rate": 7.519198664440735e-05,
      "loss": 0.3275,
      "step": 1874
    },
    {
      "epoch": 3.125,
      "grad_norm": 0.04410020634531975,
      "learning_rate": 7.512520868113523e-05,
      "loss": 0.2898,
      "step": 1875
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.041388027369976044,
      "learning_rate": 7.505843071786311e-05,
      "loss": 0.2954,
      "step": 1876
    },
    {
      "epoch": 3.1283333333333334,
      "grad_norm": 0.03775274381041527,
      "learning_rate": 7.4991652754591e-05,
      "loss": 0.2868,
      "step": 1877
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.1165275126695633,
      "learning_rate": 7.492487479131887e-05,
      "loss": 0.3457,
      "step": 1878
    },
    {
      "epoch": 3.131666666666667,
      "grad_norm": 0.035384032875299454,
      "learning_rate": 7.485809682804675e-05,
      "loss": 0.275,
      "step": 1879
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.06762245297431946,
      "learning_rate": 7.479131886477463e-05,
      "loss": 0.3746,
      "step": 1880
    },
    {
      "epoch": 3.135,
      "grad_norm": 0.060104720294475555,
      "learning_rate": 7.47245409015025e-05,
      "loss": 0.3284,
      "step": 1881
    },
    {
      "epoch": 3.1366666666666667,
      "grad_norm": 0.06153131276369095,
      "learning_rate": 7.465776293823039e-05,
      "loss": 0.3147,
      "step": 1882
    },
    {
      "epoch": 3.138333333333333,
      "grad_norm": 0.03754366189241409,
      "learning_rate": 7.459098497495826e-05,
      "loss": 0.2597,
      "step": 1883
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.03717878460884094,
      "learning_rate": 7.452420701168615e-05,
      "loss": 0.2706,
      "step": 1884
    },
    {
      "epoch": 3.1416666666666666,
      "grad_norm": 0.046462349593639374,
      "learning_rate": 7.445742904841403e-05,
      "loss": 0.3003,
      "step": 1885
    },
    {
      "epoch": 3.1433333333333335,
      "grad_norm": 0.0731838271021843,
      "learning_rate": 7.439065108514191e-05,
      "loss": 0.4354,
      "step": 1886
    },
    {
      "epoch": 3.145,
      "grad_norm": 0.05333917587995529,
      "learning_rate": 7.43238731218698e-05,
      "loss": 0.3295,
      "step": 1887
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.045024894177913666,
      "learning_rate": 7.425709515859767e-05,
      "loss": 0.3021,
      "step": 1888
    },
    {
      "epoch": 3.1483333333333334,
      "grad_norm": 0.05566568300127983,
      "learning_rate": 7.419031719532554e-05,
      "loss": 0.3435,
      "step": 1889
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.04620678722858429,
      "learning_rate": 7.412353923205343e-05,
      "loss": 0.3218,
      "step": 1890
    },
    {
      "epoch": 3.151666666666667,
      "grad_norm": 0.046761784702539444,
      "learning_rate": 7.40567612687813e-05,
      "loss": 0.3107,
      "step": 1891
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.05286797508597374,
      "learning_rate": 7.398998330550919e-05,
      "loss": 0.3655,
      "step": 1892
    },
    {
      "epoch": 3.155,
      "grad_norm": 0.05345473438501358,
      "learning_rate": 7.392320534223707e-05,
      "loss": 0.3164,
      "step": 1893
    },
    {
      "epoch": 3.1566666666666667,
      "grad_norm": 0.051942773163318634,
      "learning_rate": 7.385642737896495e-05,
      "loss": 0.3385,
      "step": 1894
    },
    {
      "epoch": 3.158333333333333,
      "grad_norm": 0.05300989747047424,
      "learning_rate": 7.378964941569283e-05,
      "loss": 0.3258,
      "step": 1895
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.04389581084251404,
      "learning_rate": 7.37228714524207e-05,
      "loss": 0.3056,
      "step": 1896
    },
    {
      "epoch": 3.1616666666666666,
      "grad_norm": 0.041038185358047485,
      "learning_rate": 7.365609348914858e-05,
      "loss": 0.3449,
      "step": 1897
    },
    {
      "epoch": 3.163333333333333,
      "grad_norm": 0.04178354889154434,
      "learning_rate": 7.358931552587647e-05,
      "loss": 0.3239,
      "step": 1898
    },
    {
      "epoch": 3.165,
      "grad_norm": 0.0574641115963459,
      "learning_rate": 7.352253756260434e-05,
      "loss": 0.2695,
      "step": 1899
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.03517550602555275,
      "learning_rate": 7.345575959933221e-05,
      "loss": 0.2721,
      "step": 1900
    },
    {
      "epoch": 3.1683333333333334,
      "grad_norm": 0.036349765956401825,
      "learning_rate": 7.33889816360601e-05,
      "loss": 0.2595,
      "step": 1901
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.0342557355761528,
      "learning_rate": 7.332220367278799e-05,
      "loss": 0.2285,
      "step": 1902
    },
    {
      "epoch": 3.171666666666667,
      "grad_norm": 0.040422532707452774,
      "learning_rate": 7.325542570951587e-05,
      "loss": 0.2848,
      "step": 1903
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.06451321393251419,
      "learning_rate": 7.318864774624375e-05,
      "loss": 0.3648,
      "step": 1904
    },
    {
      "epoch": 3.175,
      "grad_norm": 0.03699067607522011,
      "learning_rate": 7.312186978297162e-05,
      "loss": 0.2868,
      "step": 1905
    },
    {
      "epoch": 3.1766666666666667,
      "grad_norm": 0.053216300904750824,
      "learning_rate": 7.30550918196995e-05,
      "loss": 0.312,
      "step": 1906
    },
    {
      "epoch": 3.1783333333333332,
      "grad_norm": 0.04039696604013443,
      "learning_rate": 7.298831385642738e-05,
      "loss": 0.2478,
      "step": 1907
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.06905537098646164,
      "learning_rate": 7.292153589315525e-05,
      "loss": 0.3975,
      "step": 1908
    },
    {
      "epoch": 3.1816666666666666,
      "grad_norm": 0.05084792524576187,
      "learning_rate": 7.285475792988314e-05,
      "loss": 0.3112,
      "step": 1909
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 0.03672866150736809,
      "learning_rate": 7.278797996661103e-05,
      "loss": 0.287,
      "step": 1910
    },
    {
      "epoch": 3.185,
      "grad_norm": 0.03428696468472481,
      "learning_rate": 7.272120200333891e-05,
      "loss": 0.2672,
      "step": 1911
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.040565263479948044,
      "learning_rate": 7.265442404006679e-05,
      "loss": 0.2901,
      "step": 1912
    },
    {
      "epoch": 3.1883333333333335,
      "grad_norm": 0.047184959053993225,
      "learning_rate": 7.258764607679466e-05,
      "loss": 0.3093,
      "step": 1913
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.05179686099290848,
      "learning_rate": 7.252086811352255e-05,
      "loss": 0.3309,
      "step": 1914
    },
    {
      "epoch": 3.191666666666667,
      "grad_norm": 0.058640990406274796,
      "learning_rate": 7.245409015025042e-05,
      "loss": 0.3297,
      "step": 1915
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.06425520777702332,
      "learning_rate": 7.238731218697829e-05,
      "loss": 0.3446,
      "step": 1916
    },
    {
      "epoch": 3.195,
      "grad_norm": 0.06259792298078537,
      "learning_rate": 7.232053422370618e-05,
      "loss": 0.3171,
      "step": 1917
    },
    {
      "epoch": 3.1966666666666668,
      "grad_norm": 0.04911472275853157,
      "learning_rate": 7.225375626043405e-05,
      "loss": 0.3453,
      "step": 1918
    },
    {
      "epoch": 3.1983333333333333,
      "grad_norm": 0.04847652092576027,
      "learning_rate": 7.218697829716194e-05,
      "loss": 0.3383,
      "step": 1919
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.03590187430381775,
      "learning_rate": 7.212020033388982e-05,
      "loss": 0.2521,
      "step": 1920
    },
    {
      "epoch": 3.2016666666666667,
      "grad_norm": 0.03862759470939636,
      "learning_rate": 7.20534223706177e-05,
      "loss": 0.2785,
      "step": 1921
    },
    {
      "epoch": 3.203333333333333,
      "grad_norm": 0.07242780923843384,
      "learning_rate": 7.198664440734558e-05,
      "loss": 0.3664,
      "step": 1922
    },
    {
      "epoch": 3.205,
      "grad_norm": 0.07777393609285355,
      "learning_rate": 7.191986644407346e-05,
      "loss": 0.3496,
      "step": 1923
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.05847702920436859,
      "learning_rate": 7.185308848080133e-05,
      "loss": 0.3235,
      "step": 1924
    },
    {
      "epoch": 3.2083333333333335,
      "grad_norm": 0.03996541351079941,
      "learning_rate": 7.178631051752922e-05,
      "loss": 0.2518,
      "step": 1925
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.05681852623820305,
      "learning_rate": 7.171953255425709e-05,
      "loss": 0.3514,
      "step": 1926
    },
    {
      "epoch": 3.211666666666667,
      "grad_norm": 0.034271273761987686,
      "learning_rate": 7.165275459098498e-05,
      "loss": 0.1942,
      "step": 1927
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.04583323374390602,
      "learning_rate": 7.158597662771286e-05,
      "loss": 0.3154,
      "step": 1928
    },
    {
      "epoch": 3.215,
      "grad_norm": 0.03237083554267883,
      "learning_rate": 7.151919866444074e-05,
      "loss": 0.2429,
      "step": 1929
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 0.06339070945978165,
      "learning_rate": 7.145242070116862e-05,
      "loss": 0.4,
      "step": 1930
    },
    {
      "epoch": 3.2183333333333333,
      "grad_norm": 0.04651374742388725,
      "learning_rate": 7.13856427378965e-05,
      "loss": 0.3492,
      "step": 1931
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.03765115514397621,
      "learning_rate": 7.131886477462437e-05,
      "loss": 0.2631,
      "step": 1932
    },
    {
      "epoch": 3.2216666666666667,
      "grad_norm": 0.05352940410375595,
      "learning_rate": 7.125208681135226e-05,
      "loss": 0.3211,
      "step": 1933
    },
    {
      "epoch": 3.223333333333333,
      "grad_norm": 0.07142828404903412,
      "learning_rate": 7.118530884808013e-05,
      "loss": 0.2891,
      "step": 1934
    },
    {
      "epoch": 3.225,
      "grad_norm": 0.05118471011519432,
      "learning_rate": 7.111853088480802e-05,
      "loss": 0.3583,
      "step": 1935
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.04598791524767876,
      "learning_rate": 7.105175292153589e-05,
      "loss": 0.2816,
      "step": 1936
    },
    {
      "epoch": 3.2283333333333335,
      "grad_norm": 0.04890603572130203,
      "learning_rate": 7.098497495826378e-05,
      "loss": 0.3235,
      "step": 1937
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.04858086630702019,
      "learning_rate": 7.091819699499166e-05,
      "loss": 0.3251,
      "step": 1938
    },
    {
      "epoch": 3.2316666666666665,
      "grad_norm": 0.05395907536149025,
      "learning_rate": 7.085141903171954e-05,
      "loss": 0.3135,
      "step": 1939
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.10882072895765305,
      "learning_rate": 7.078464106844741e-05,
      "loss": 0.3377,
      "step": 1940
    },
    {
      "epoch": 3.235,
      "grad_norm": 0.0583127923309803,
      "learning_rate": 7.07178631051753e-05,
      "loss": 0.3157,
      "step": 1941
    },
    {
      "epoch": 3.236666666666667,
      "grad_norm": 0.05990222468972206,
      "learning_rate": 7.065108514190317e-05,
      "loss": 0.3488,
      "step": 1942
    },
    {
      "epoch": 3.2383333333333333,
      "grad_norm": 0.058706413954496384,
      "learning_rate": 7.058430717863106e-05,
      "loss": 0.3299,
      "step": 1943
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.05227067694067955,
      "learning_rate": 7.051752921535893e-05,
      "loss": 0.3104,
      "step": 1944
    },
    {
      "epoch": 3.2416666666666667,
      "grad_norm": 0.03561064600944519,
      "learning_rate": 7.045075125208682e-05,
      "loss": 0.2842,
      "step": 1945
    },
    {
      "epoch": 3.243333333333333,
      "grad_norm": 0.08451181650161743,
      "learning_rate": 7.03839732888147e-05,
      "loss": 0.3672,
      "step": 1946
    },
    {
      "epoch": 3.245,
      "grad_norm": 0.05787156522274017,
      "learning_rate": 7.031719532554258e-05,
      "loss": 0.3301,
      "step": 1947
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.0470435731112957,
      "learning_rate": 7.025041736227045e-05,
      "loss": 0.3168,
      "step": 1948
    },
    {
      "epoch": 3.2483333333333335,
      "grad_norm": 0.0644371286034584,
      "learning_rate": 7.018363939899834e-05,
      "loss": 0.383,
      "step": 1949
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.05055207014083862,
      "learning_rate": 7.011686143572621e-05,
      "loss": 0.3718,
      "step": 1950
    },
    {
      "epoch": 3.2516666666666665,
      "grad_norm": 0.043880634009838104,
      "learning_rate": 7.00500834724541e-05,
      "loss": 0.3222,
      "step": 1951
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.0435078963637352,
      "learning_rate": 6.998330550918197e-05,
      "loss": 0.2776,
      "step": 1952
    },
    {
      "epoch": 3.255,
      "grad_norm": 0.039945222437381744,
      "learning_rate": 6.991652754590986e-05,
      "loss": 0.2542,
      "step": 1953
    },
    {
      "epoch": 3.256666666666667,
      "grad_norm": 0.05568145960569382,
      "learning_rate": 6.984974958263774e-05,
      "loss": 0.3403,
      "step": 1954
    },
    {
      "epoch": 3.2583333333333333,
      "grad_norm": 0.0379413478076458,
      "learning_rate": 6.978297161936562e-05,
      "loss": 0.2424,
      "step": 1955
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.04653671011328697,
      "learning_rate": 6.971619365609349e-05,
      "loss": 0.3308,
      "step": 1956
    },
    {
      "epoch": 3.2616666666666667,
      "grad_norm": 0.06158275902271271,
      "learning_rate": 6.964941569282138e-05,
      "loss": 0.3421,
      "step": 1957
    },
    {
      "epoch": 3.263333333333333,
      "grad_norm": 0.05091460421681404,
      "learning_rate": 6.958263772954925e-05,
      "loss": 0.3084,
      "step": 1958
    },
    {
      "epoch": 3.265,
      "grad_norm": 0.03178892657160759,
      "learning_rate": 6.951585976627714e-05,
      "loss": 0.2331,
      "step": 1959
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.0409359484910965,
      "learning_rate": 6.944908180300501e-05,
      "loss": 0.2804,
      "step": 1960
    },
    {
      "epoch": 3.2683333333333335,
      "grad_norm": 0.04044630751013756,
      "learning_rate": 6.938230383973288e-05,
      "loss": 0.2616,
      "step": 1961
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.054501939564943314,
      "learning_rate": 6.931552587646077e-05,
      "loss": 0.3403,
      "step": 1962
    },
    {
      "epoch": 3.2716666666666665,
      "grad_norm": 0.04256148263812065,
      "learning_rate": 6.924874791318865e-05,
      "loss": 0.2835,
      "step": 1963
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.043000444769859314,
      "learning_rate": 6.918196994991654e-05,
      "loss": 0.2508,
      "step": 1964
    },
    {
      "epoch": 3.275,
      "grad_norm": 0.09627416729927063,
      "learning_rate": 6.911519198664441e-05,
      "loss": 0.3444,
      "step": 1965
    },
    {
      "epoch": 3.276666666666667,
      "grad_norm": 0.04965662583708763,
      "learning_rate": 6.904841402337229e-05,
      "loss": 0.2798,
      "step": 1966
    },
    {
      "epoch": 3.2783333333333333,
      "grad_norm": 0.09230545908212662,
      "learning_rate": 6.898163606010017e-05,
      "loss": 0.3103,
      "step": 1967
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.0467187725007534,
      "learning_rate": 6.891485809682805e-05,
      "loss": 0.3178,
      "step": 1968
    },
    {
      "epoch": 3.2816666666666667,
      "grad_norm": 0.06639155745506287,
      "learning_rate": 6.884808013355592e-05,
      "loss": 0.3156,
      "step": 1969
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 0.06999440491199493,
      "learning_rate": 6.878130217028381e-05,
      "loss": 0.3448,
      "step": 1970
    },
    {
      "epoch": 3.285,
      "grad_norm": 0.042147792875766754,
      "learning_rate": 6.87145242070117e-05,
      "loss": 0.2762,
      "step": 1971
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.0306941419839859,
      "learning_rate": 6.864774624373958e-05,
      "loss": 0.2675,
      "step": 1972
    },
    {
      "epoch": 3.288333333333333,
      "grad_norm": 0.049194347113370895,
      "learning_rate": 6.858096828046745e-05,
      "loss": 0.3118,
      "step": 1973
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.03389156237244606,
      "learning_rate": 6.851419031719533e-05,
      "loss": 0.2119,
      "step": 1974
    },
    {
      "epoch": 3.2916666666666665,
      "grad_norm": 0.054473090916872025,
      "learning_rate": 6.844741235392321e-05,
      "loss": 0.3192,
      "step": 1975
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.04439837858080864,
      "learning_rate": 6.838063439065109e-05,
      "loss": 0.2996,
      "step": 1976
    },
    {
      "epoch": 3.295,
      "grad_norm": 0.03516266494989395,
      "learning_rate": 6.831385642737896e-05,
      "loss": 0.3126,
      "step": 1977
    },
    {
      "epoch": 3.296666666666667,
      "grad_norm": 0.04002642259001732,
      "learning_rate": 6.824707846410685e-05,
      "loss": 0.2803,
      "step": 1978
    },
    {
      "epoch": 3.2983333333333333,
      "grad_norm": 0.0641351267695427,
      "learning_rate": 6.818030050083472e-05,
      "loss": 0.294,
      "step": 1979
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.03961629420518875,
      "learning_rate": 6.811352253756261e-05,
      "loss": 0.2437,
      "step": 1980
    },
    {
      "epoch": 3.3016666666666667,
      "grad_norm": 0.048376213759183884,
      "learning_rate": 6.80467445742905e-05,
      "loss": 0.3243,
      "step": 1981
    },
    {
      "epoch": 3.3033333333333332,
      "grad_norm": 0.06028677895665169,
      "learning_rate": 6.797996661101837e-05,
      "loss": 0.3733,
      "step": 1982
    },
    {
      "epoch": 3.305,
      "grad_norm": 0.060222942382097244,
      "learning_rate": 6.791318864774625e-05,
      "loss": 0.349,
      "step": 1983
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.03805830702185631,
      "learning_rate": 6.784641068447413e-05,
      "loss": 0.2506,
      "step": 1984
    },
    {
      "epoch": 3.3083333333333336,
      "grad_norm": 0.04972353205084801,
      "learning_rate": 6.7779632721202e-05,
      "loss": 0.2944,
      "step": 1985
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.06024017557501793,
      "learning_rate": 6.771285475792989e-05,
      "loss": 0.333,
      "step": 1986
    },
    {
      "epoch": 3.3116666666666665,
      "grad_norm": 0.03592798486351967,
      "learning_rate": 6.764607679465776e-05,
      "loss": 0.2421,
      "step": 1987
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.03784001246094704,
      "learning_rate": 6.757929883138565e-05,
      "loss": 0.2799,
      "step": 1988
    },
    {
      "epoch": 3.315,
      "grad_norm": 0.03511564061045647,
      "learning_rate": 6.751252086811353e-05,
      "loss": 0.2629,
      "step": 1989
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 0.03491130471229553,
      "learning_rate": 6.74457429048414e-05,
      "loss": 0.2166,
      "step": 1990
    },
    {
      "epoch": 3.3183333333333334,
      "grad_norm": 0.09781550616025925,
      "learning_rate": 6.737896494156929e-05,
      "loss": 0.3694,
      "step": 1991
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.04425785318017006,
      "learning_rate": 6.731218697829717e-05,
      "loss": 0.3013,
      "step": 1992
    },
    {
      "epoch": 3.3216666666666668,
      "grad_norm": 0.05876626446843147,
      "learning_rate": 6.724540901502504e-05,
      "loss": 0.353,
      "step": 1993
    },
    {
      "epoch": 3.3233333333333333,
      "grad_norm": 0.061055734753608704,
      "learning_rate": 6.717863105175293e-05,
      "loss": 0.2969,
      "step": 1994
    },
    {
      "epoch": 3.325,
      "grad_norm": 0.04458479583263397,
      "learning_rate": 6.71118530884808e-05,
      "loss": 0.2799,
      "step": 1995
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.05633660405874252,
      "learning_rate": 6.704507512520869e-05,
      "loss": 0.327,
      "step": 1996
    },
    {
      "epoch": 3.328333333333333,
      "grad_norm": 0.051210302859544754,
      "learning_rate": 6.697829716193656e-05,
      "loss": 0.3428,
      "step": 1997
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.03859937563538551,
      "learning_rate": 6.691151919866445e-05,
      "loss": 0.2386,
      "step": 1998
    },
    {
      "epoch": 3.3316666666666666,
      "grad_norm": 0.09102495014667511,
      "learning_rate": 6.684474123539233e-05,
      "loss": 0.3397,
      "step": 1999
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.03891771286725998,
      "learning_rate": 6.67779632721202e-05,
      "loss": 0.2993,
      "step": 2000
    },
    {
      "epoch": 3.335,
      "grad_norm": 0.038485247641801834,
      "learning_rate": 6.671118530884808e-05,
      "loss": 0.2967,
      "step": 2001
    },
    {
      "epoch": 3.336666666666667,
      "grad_norm": 0.04617803916335106,
      "learning_rate": 6.664440734557597e-05,
      "loss": 0.2892,
      "step": 2002
    },
    {
      "epoch": 3.3383333333333334,
      "grad_norm": 0.05999235436320305,
      "learning_rate": 6.657762938230384e-05,
      "loss": 0.3144,
      "step": 2003
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.043443601578474045,
      "learning_rate": 6.651085141903173e-05,
      "loss": 0.2507,
      "step": 2004
    },
    {
      "epoch": 3.341666666666667,
      "grad_norm": 0.026348788291215897,
      "learning_rate": 6.64440734557596e-05,
      "loss": 0.211,
      "step": 2005
    },
    {
      "epoch": 3.3433333333333333,
      "grad_norm": 0.05688203126192093,
      "learning_rate": 6.637729549248749e-05,
      "loss": 0.3562,
      "step": 2006
    },
    {
      "epoch": 3.3449999999999998,
      "grad_norm": 0.0631614625453949,
      "learning_rate": 6.631051752921537e-05,
      "loss": 0.3239,
      "step": 2007
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.039779212325811386,
      "learning_rate": 6.624373956594325e-05,
      "loss": 0.2702,
      "step": 2008
    },
    {
      "epoch": 3.348333333333333,
      "grad_norm": 0.05668586492538452,
      "learning_rate": 6.617696160267112e-05,
      "loss": 0.3229,
      "step": 2009
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.03640012443065643,
      "learning_rate": 6.6110183639399e-05,
      "loss": 0.2666,
      "step": 2010
    },
    {
      "epoch": 3.3516666666666666,
      "grad_norm": 0.038975875824689865,
      "learning_rate": 6.604340567612688e-05,
      "loss": 0.3117,
      "step": 2011
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.07264374196529388,
      "learning_rate": 6.597662771285476e-05,
      "loss": 0.4354,
      "step": 2012
    },
    {
      "epoch": 3.355,
      "grad_norm": 0.03906364366412163,
      "learning_rate": 6.590984974958264e-05,
      "loss": 0.3123,
      "step": 2013
    },
    {
      "epoch": 3.3566666666666665,
      "grad_norm": 0.046639565378427505,
      "learning_rate": 6.584307178631051e-05,
      "loss": 0.3024,
      "step": 2014
    },
    {
      "epoch": 3.3583333333333334,
      "grad_norm": 0.04378839582204819,
      "learning_rate": 6.57762938230384e-05,
      "loss": 0.3131,
      "step": 2015
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.03679829090833664,
      "learning_rate": 6.570951585976628e-05,
      "loss": 0.2836,
      "step": 2016
    },
    {
      "epoch": 3.361666666666667,
      "grad_norm": 0.03456678241491318,
      "learning_rate": 6.564273789649416e-05,
      "loss": 0.2592,
      "step": 2017
    },
    {
      "epoch": 3.3633333333333333,
      "grad_norm": 0.042626772075891495,
      "learning_rate": 6.557595993322204e-05,
      "loss": 0.3051,
      "step": 2018
    },
    {
      "epoch": 3.365,
      "grad_norm": 0.043728966265916824,
      "learning_rate": 6.550918196994992e-05,
      "loss": 0.2935,
      "step": 2019
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.03445037081837654,
      "learning_rate": 6.54424040066778e-05,
      "loss": 0.2251,
      "step": 2020
    },
    {
      "epoch": 3.368333333333333,
      "grad_norm": 0.029067637398838997,
      "learning_rate": 6.537562604340568e-05,
      "loss": 0.2059,
      "step": 2021
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.06606494635343552,
      "learning_rate": 6.530884808013355e-05,
      "loss": 0.2965,
      "step": 2022
    },
    {
      "epoch": 3.3716666666666666,
      "grad_norm": 0.06976784020662308,
      "learning_rate": 6.524207011686144e-05,
      "loss": 0.387,
      "step": 2023
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.06282202154397964,
      "learning_rate": 6.517529215358932e-05,
      "loss": 0.2644,
      "step": 2024
    },
    {
      "epoch": 3.375,
      "grad_norm": 0.054343175143003464,
      "learning_rate": 6.51085141903172e-05,
      "loss": 0.3582,
      "step": 2025
    },
    {
      "epoch": 3.3766666666666665,
      "grad_norm": 0.03829209879040718,
      "learning_rate": 6.504173622704508e-05,
      "loss": 0.2744,
      "step": 2026
    },
    {
      "epoch": 3.3783333333333334,
      "grad_norm": 0.04381963983178139,
      "learning_rate": 6.497495826377296e-05,
      "loss": 0.3069,
      "step": 2027
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.04887743666768074,
      "learning_rate": 6.490818030050084e-05,
      "loss": 0.3336,
      "step": 2028
    },
    {
      "epoch": 3.381666666666667,
      "grad_norm": 0.04264998063445091,
      "learning_rate": 6.484140233722872e-05,
      "loss": 0.2905,
      "step": 2029
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 0.04823857918381691,
      "learning_rate": 6.477462437395659e-05,
      "loss": 0.2808,
      "step": 2030
    },
    {
      "epoch": 3.385,
      "grad_norm": 0.043894000351428986,
      "learning_rate": 6.470784641068448e-05,
      "loss": 0.2949,
      "step": 2031
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.03674204275012016,
      "learning_rate": 6.464106844741235e-05,
      "loss": 0.2467,
      "step": 2032
    },
    {
      "epoch": 3.388333333333333,
      "grad_norm": 0.04801836609840393,
      "learning_rate": 6.457429048414024e-05,
      "loss": 0.3085,
      "step": 2033
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.0995577722787857,
      "learning_rate": 6.450751252086812e-05,
      "loss": 0.3208,
      "step": 2034
    },
    {
      "epoch": 3.3916666666666666,
      "grad_norm": 0.04442519694566727,
      "learning_rate": 6.4440734557596e-05,
      "loss": 0.2585,
      "step": 2035
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.04428204521536827,
      "learning_rate": 6.437395659432388e-05,
      "loss": 0.2789,
      "step": 2036
    },
    {
      "epoch": 3.395,
      "grad_norm": 0.03864416480064392,
      "learning_rate": 6.430717863105176e-05,
      "loss": 0.2811,
      "step": 2037
    },
    {
      "epoch": 3.3966666666666665,
      "grad_norm": 0.0532698780298233,
      "learning_rate": 6.424040066777963e-05,
      "loss": 0.3105,
      "step": 2038
    },
    {
      "epoch": 3.3983333333333334,
      "grad_norm": 0.040752533823251724,
      "learning_rate": 6.417362270450752e-05,
      "loss": 0.3247,
      "step": 2039
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.04539632052183151,
      "learning_rate": 6.410684474123539e-05,
      "loss": 0.3119,
      "step": 2040
    },
    {
      "epoch": 3.401666666666667,
      "grad_norm": 0.04479105770587921,
      "learning_rate": 6.404006677796328e-05,
      "loss": 0.3232,
      "step": 2041
    },
    {
      "epoch": 3.4033333333333333,
      "grad_norm": 0.05587129667401314,
      "learning_rate": 6.397328881469116e-05,
      "loss": 0.2835,
      "step": 2042
    },
    {
      "epoch": 3.4050000000000002,
      "grad_norm": 0.11970072239637375,
      "learning_rate": 6.390651085141904e-05,
      "loss": 0.2441,
      "step": 2043
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.0490596666932106,
      "learning_rate": 6.383973288814692e-05,
      "loss": 0.3075,
      "step": 2044
    },
    {
      "epoch": 3.408333333333333,
      "grad_norm": 0.09917628020048141,
      "learning_rate": 6.37729549248748e-05,
      "loss": 0.3415,
      "step": 2045
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.0529356524348259,
      "learning_rate": 6.370617696160267e-05,
      "loss": 0.3545,
      "step": 2046
    },
    {
      "epoch": 3.4116666666666666,
      "grad_norm": 0.05328482761979103,
      "learning_rate": 6.363939899833056e-05,
      "loss": 0.3544,
      "step": 2047
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.0485142357647419,
      "learning_rate": 6.357262103505843e-05,
      "loss": 0.2936,
      "step": 2048
    },
    {
      "epoch": 3.415,
      "grad_norm": 0.09459465742111206,
      "learning_rate": 6.35058430717863e-05,
      "loss": 0.3715,
      "step": 2049
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 0.04866833612322807,
      "learning_rate": 6.343906510851419e-05,
      "loss": 0.3154,
      "step": 2050
    },
    {
      "epoch": 3.4183333333333334,
      "grad_norm": 0.04937712103128433,
      "learning_rate": 6.337228714524208e-05,
      "loss": 0.3132,
      "step": 2051
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.05401559919118881,
      "learning_rate": 6.330550918196996e-05,
      "loss": 0.2957,
      "step": 2052
    },
    {
      "epoch": 3.421666666666667,
      "grad_norm": 0.045784998685121536,
      "learning_rate": 6.323873121869784e-05,
      "loss": 0.2977,
      "step": 2053
    },
    {
      "epoch": 3.4233333333333333,
      "grad_norm": 0.03644782304763794,
      "learning_rate": 6.317195325542571e-05,
      "loss": 0.2293,
      "step": 2054
    },
    {
      "epoch": 3.425,
      "grad_norm": 0.04475553333759308,
      "learning_rate": 6.31051752921536e-05,
      "loss": 0.3478,
      "step": 2055
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.045833662152290344,
      "learning_rate": 6.303839732888147e-05,
      "loss": 0.3209,
      "step": 2056
    },
    {
      "epoch": 3.4283333333333332,
      "grad_norm": 0.05608954280614853,
      "learning_rate": 6.297161936560934e-05,
      "loss": 0.3502,
      "step": 2057
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.04619499668478966,
      "learning_rate": 6.290484140233723e-05,
      "loss": 0.3515,
      "step": 2058
    },
    {
      "epoch": 3.4316666666666666,
      "grad_norm": 0.0409436859190464,
      "learning_rate": 6.283806343906511e-05,
      "loss": 0.2839,
      "step": 2059
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.038404181599617004,
      "learning_rate": 6.2771285475793e-05,
      "loss": 0.2827,
      "step": 2060
    },
    {
      "epoch": 3.435,
      "grad_norm": 0.059138379991054535,
      "learning_rate": 6.270450751252087e-05,
      "loss": 0.3219,
      "step": 2061
    },
    {
      "epoch": 3.4366666666666665,
      "grad_norm": 0.041400592774152756,
      "learning_rate": 6.263772954924875e-05,
      "loss": 0.2749,
      "step": 2062
    },
    {
      "epoch": 3.4383333333333335,
      "grad_norm": 0.05634197220206261,
      "learning_rate": 6.257095158597663e-05,
      "loss": 0.3037,
      "step": 2063
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.04075832664966583,
      "learning_rate": 6.250417362270451e-05,
      "loss": 0.296,
      "step": 2064
    },
    {
      "epoch": 3.4416666666666664,
      "grad_norm": 0.06027395278215408,
      "learning_rate": 6.243739565943238e-05,
      "loss": 0.3707,
      "step": 2065
    },
    {
      "epoch": 3.4433333333333334,
      "grad_norm": 0.0553305484354496,
      "learning_rate": 6.237061769616027e-05,
      "loss": 0.3571,
      "step": 2066
    },
    {
      "epoch": 3.445,
      "grad_norm": 0.052620384842157364,
      "learning_rate": 6.230383973288815e-05,
      "loss": 0.3118,
      "step": 2067
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.05140170082449913,
      "learning_rate": 6.223706176961604e-05,
      "loss": 0.3231,
      "step": 2068
    },
    {
      "epoch": 3.4483333333333333,
      "grad_norm": 0.07995046675205231,
      "learning_rate": 6.217028380634391e-05,
      "loss": 0.3757,
      "step": 2069
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.0514252744615078,
      "learning_rate": 6.210350584307179e-05,
      "loss": 0.291,
      "step": 2070
    },
    {
      "epoch": 3.4516666666666667,
      "grad_norm": 0.03791176155209541,
      "learning_rate": 6.203672787979967e-05,
      "loss": 0.2363,
      "step": 2071
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.037770967930555344,
      "learning_rate": 6.196994991652755e-05,
      "loss": 0.3006,
      "step": 2072
    },
    {
      "epoch": 3.455,
      "grad_norm": 0.0508088618516922,
      "learning_rate": 6.190317195325542e-05,
      "loss": 0.3231,
      "step": 2073
    },
    {
      "epoch": 3.4566666666666666,
      "grad_norm": 0.0371355377137661,
      "learning_rate": 6.183639398998331e-05,
      "loss": 0.2442,
      "step": 2074
    },
    {
      "epoch": 3.4583333333333335,
      "grad_norm": 0.047762032598257065,
      "learning_rate": 6.176961602671118e-05,
      "loss": 0.2931,
      "step": 2075
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.04797909036278725,
      "learning_rate": 6.170283806343907e-05,
      "loss": 0.3103,
      "step": 2076
    },
    {
      "epoch": 3.461666666666667,
      "grad_norm": 0.04756325110793114,
      "learning_rate": 6.163606010016695e-05,
      "loss": 0.3205,
      "step": 2077
    },
    {
      "epoch": 3.4633333333333334,
      "grad_norm": 0.060840919613838196,
      "learning_rate": 6.156928213689483e-05,
      "loss": 0.3583,
      "step": 2078
    },
    {
      "epoch": 3.465,
      "grad_norm": 0.036075521260499954,
      "learning_rate": 6.150250417362271e-05,
      "loss": 0.2882,
      "step": 2079
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.030622612684965134,
      "learning_rate": 6.143572621035059e-05,
      "loss": 0.2492,
      "step": 2080
    },
    {
      "epoch": 3.4683333333333333,
      "grad_norm": 0.04135744273662567,
      "learning_rate": 6.136894824707846e-05,
      "loss": 0.2628,
      "step": 2081
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 0.049733683466911316,
      "learning_rate": 6.130217028380635e-05,
      "loss": 0.2435,
      "step": 2082
    },
    {
      "epoch": 3.4716666666666667,
      "grad_norm": 0.07373890280723572,
      "learning_rate": 6.123539232053422e-05,
      "loss": 0.2805,
      "step": 2083
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.09475231915712357,
      "learning_rate": 6.11686143572621e-05,
      "loss": 0.315,
      "step": 2084
    },
    {
      "epoch": 3.475,
      "grad_norm": 0.04136555269360542,
      "learning_rate": 6.110183639398999e-05,
      "loss": 0.2463,
      "step": 2085
    },
    {
      "epoch": 3.4766666666666666,
      "grad_norm": 0.034269366413354874,
      "learning_rate": 6.103505843071786e-05,
      "loss": 0.2743,
      "step": 2086
    },
    {
      "epoch": 3.4783333333333335,
      "grad_norm": 0.04330003261566162,
      "learning_rate": 6.0968280467445746e-05,
      "loss": 0.2881,
      "step": 2087
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.05072743818163872,
      "learning_rate": 6.0901502504173626e-05,
      "loss": 0.3061,
      "step": 2088
    },
    {
      "epoch": 3.4816666666666665,
      "grad_norm": 0.0670299306511879,
      "learning_rate": 6.083472454090151e-05,
      "loss": 0.2789,
      "step": 2089
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 0.03718339279294014,
      "learning_rate": 6.0767946577629386e-05,
      "loss": 0.2566,
      "step": 2090
    },
    {
      "epoch": 3.485,
      "grad_norm": 0.056562282145023346,
      "learning_rate": 6.070116861435726e-05,
      "loss": 0.391,
      "step": 2091
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.043511610478162766,
      "learning_rate": 6.0634390651085146e-05,
      "loss": 0.3038,
      "step": 2092
    },
    {
      "epoch": 3.4883333333333333,
      "grad_norm": 0.09519277513027191,
      "learning_rate": 6.0567612687813026e-05,
      "loss": 0.3102,
      "step": 2093
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.04536786302924156,
      "learning_rate": 6.05008347245409e-05,
      "loss": 0.3076,
      "step": 2094
    },
    {
      "epoch": 3.4916666666666667,
      "grad_norm": 0.0445106104016304,
      "learning_rate": 6.0434056761268785e-05,
      "loss": 0.2891,
      "step": 2095
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.04498714208602905,
      "learning_rate": 6.0367278797996665e-05,
      "loss": 0.2727,
      "step": 2096
    },
    {
      "epoch": 3.495,
      "grad_norm": 0.05923183262348175,
      "learning_rate": 6.030050083472455e-05,
      "loss": 0.3169,
      "step": 2097
    },
    {
      "epoch": 3.4966666666666666,
      "grad_norm": 0.053400181233882904,
      "learning_rate": 6.0233722871452425e-05,
      "loss": 0.3514,
      "step": 2098
    },
    {
      "epoch": 3.4983333333333335,
      "grad_norm": 0.04064300283789635,
      "learning_rate": 6.01669449081803e-05,
      "loss": 0.2773,
      "step": 2099
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.030713601037859917,
      "learning_rate": 6.0100166944908185e-05,
      "loss": 0.2579,
      "step": 2100
    },
    {
      "epoch": 3.501666666666667,
      "grad_norm": 0.051388345658779144,
      "learning_rate": 6.0033388981636065e-05,
      "loss": 0.3813,
      "step": 2101
    },
    {
      "epoch": 3.5033333333333334,
      "grad_norm": 0.043418481945991516,
      "learning_rate": 5.996661101836394e-05,
      "loss": 0.27,
      "step": 2102
    },
    {
      "epoch": 3.505,
      "grad_norm": 0.08386903256177902,
      "learning_rate": 5.9899833055091825e-05,
      "loss": 0.3263,
      "step": 2103
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.050936128944158554,
      "learning_rate": 5.98330550918197e-05,
      "loss": 0.3021,
      "step": 2104
    },
    {
      "epoch": 3.5083333333333333,
      "grad_norm": 0.041430793702602386,
      "learning_rate": 5.9766277128547585e-05,
      "loss": 0.3066,
      "step": 2105
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.04144631698727608,
      "learning_rate": 5.9699499165275465e-05,
      "loss": 0.2983,
      "step": 2106
    },
    {
      "epoch": 3.5116666666666667,
      "grad_norm": 0.04565873742103577,
      "learning_rate": 5.963272120200334e-05,
      "loss": 0.2578,
      "step": 2107
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.03668409213423729,
      "learning_rate": 5.9565943238731224e-05,
      "loss": 0.2648,
      "step": 2108
    },
    {
      "epoch": 3.515,
      "grad_norm": 0.043469708412885666,
      "learning_rate": 5.9499165275459104e-05,
      "loss": 0.2738,
      "step": 2109
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 0.046502504497766495,
      "learning_rate": 5.943238731218698e-05,
      "loss": 0.3277,
      "step": 2110
    },
    {
      "epoch": 3.5183333333333335,
      "grad_norm": 0.06096930801868439,
      "learning_rate": 5.9365609348914864e-05,
      "loss": 0.3812,
      "step": 2111
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.05932052060961723,
      "learning_rate": 5.929883138564274e-05,
      "loss": 0.3486,
      "step": 2112
    },
    {
      "epoch": 3.5216666666666665,
      "grad_norm": 0.04265471547842026,
      "learning_rate": 5.9232053422370624e-05,
      "loss": 0.2719,
      "step": 2113
    },
    {
      "epoch": 3.5233333333333334,
      "grad_norm": 0.052132006734609604,
      "learning_rate": 5.9165275459098504e-05,
      "loss": 0.3978,
      "step": 2114
    },
    {
      "epoch": 3.525,
      "grad_norm": 0.04467051476240158,
      "learning_rate": 5.909849749582638e-05,
      "loss": 0.348,
      "step": 2115
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.08242807537317276,
      "learning_rate": 5.9031719532554264e-05,
      "loss": 0.3745,
      "step": 2116
    },
    {
      "epoch": 3.5283333333333333,
      "grad_norm": 0.05021875724196434,
      "learning_rate": 5.896494156928214e-05,
      "loss": 0.3318,
      "step": 2117
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 0.06181914359331131,
      "learning_rate": 5.889816360601002e-05,
      "loss": 0.3475,
      "step": 2118
    },
    {
      "epoch": 3.5316666666666667,
      "grad_norm": 0.04824293032288551,
      "learning_rate": 5.8831385642737904e-05,
      "loss": 0.31,
      "step": 2119
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.05398060381412506,
      "learning_rate": 5.876460767946578e-05,
      "loss": 0.3864,
      "step": 2120
    },
    {
      "epoch": 3.535,
      "grad_norm": 0.037481386214494705,
      "learning_rate": 5.8697829716193664e-05,
      "loss": 0.2829,
      "step": 2121
    },
    {
      "epoch": 3.5366666666666666,
      "grad_norm": 0.07597435265779495,
      "learning_rate": 5.863105175292154e-05,
      "loss": 0.405,
      "step": 2122
    },
    {
      "epoch": 3.538333333333333,
      "grad_norm": 0.07065800577402115,
      "learning_rate": 5.856427378964942e-05,
      "loss": 0.3645,
      "step": 2123
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.0418495312333107,
      "learning_rate": 5.84974958263773e-05,
      "loss": 0.3106,
      "step": 2124
    },
    {
      "epoch": 3.5416666666666665,
      "grad_norm": 0.049410976469516754,
      "learning_rate": 5.8430717863105176e-05,
      "loss": 0.3114,
      "step": 2125
    },
    {
      "epoch": 3.5433333333333334,
      "grad_norm": 0.05970558524131775,
      "learning_rate": 5.8363939899833056e-05,
      "loss": 0.368,
      "step": 2126
    },
    {
      "epoch": 3.545,
      "grad_norm": 0.03928883373737335,
      "learning_rate": 5.829716193656094e-05,
      "loss": 0.3401,
      "step": 2127
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.03953804448246956,
      "learning_rate": 5.8230383973288816e-05,
      "loss": 0.2943,
      "step": 2128
    },
    {
      "epoch": 3.5483333333333333,
      "grad_norm": 0.03655675798654556,
      "learning_rate": 5.81636060100167e-05,
      "loss": 0.2753,
      "step": 2129
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.05468636751174927,
      "learning_rate": 5.8096828046744576e-05,
      "loss": 0.2575,
      "step": 2130
    },
    {
      "epoch": 3.5516666666666667,
      "grad_norm": 0.04502753168344498,
      "learning_rate": 5.8030050083472456e-05,
      "loss": 0.3372,
      "step": 2131
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.036204274743795395,
      "learning_rate": 5.796327212020034e-05,
      "loss": 0.2344,
      "step": 2132
    },
    {
      "epoch": 3.555,
      "grad_norm": 0.052106957882642746,
      "learning_rate": 5.7896494156928216e-05,
      "loss": 0.4049,
      "step": 2133
    },
    {
      "epoch": 3.5566666666666666,
      "grad_norm": 0.04682428762316704,
      "learning_rate": 5.782971619365609e-05,
      "loss": 0.3577,
      "step": 2134
    },
    {
      "epoch": 3.5583333333333336,
      "grad_norm": 0.05573548004031181,
      "learning_rate": 5.7762938230383976e-05,
      "loss": 0.3215,
      "step": 2135
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.04634620249271393,
      "learning_rate": 5.7696160267111856e-05,
      "loss": 0.2693,
      "step": 2136
    },
    {
      "epoch": 3.5616666666666665,
      "grad_norm": 0.05225541070103645,
      "learning_rate": 5.762938230383974e-05,
      "loss": 0.3002,
      "step": 2137
    },
    {
      "epoch": 3.5633333333333335,
      "grad_norm": 0.044238049536943436,
      "learning_rate": 5.7562604340567616e-05,
      "loss": 0.3178,
      "step": 2138
    },
    {
      "epoch": 3.565,
      "grad_norm": 0.03657504543662071,
      "learning_rate": 5.749582637729549e-05,
      "loss": 0.2666,
      "step": 2139
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.045749764889478683,
      "learning_rate": 5.7429048414023375e-05,
      "loss": 0.3056,
      "step": 2140
    },
    {
      "epoch": 3.5683333333333334,
      "grad_norm": 0.04016266390681267,
      "learning_rate": 5.7362270450751255e-05,
      "loss": 0.2645,
      "step": 2141
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.05695531517267227,
      "learning_rate": 5.729549248747913e-05,
      "loss": 0.3371,
      "step": 2142
    },
    {
      "epoch": 3.5716666666666668,
      "grad_norm": 0.0816175788640976,
      "learning_rate": 5.7228714524207015e-05,
      "loss": 0.303,
      "step": 2143
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.05023675411939621,
      "learning_rate": 5.7161936560934895e-05,
      "loss": 0.276,
      "step": 2144
    },
    {
      "epoch": 3.575,
      "grad_norm": 0.06959271430969238,
      "learning_rate": 5.709515859766278e-05,
      "loss": 0.4113,
      "step": 2145
    },
    {
      "epoch": 3.5766666666666667,
      "grad_norm": 0.039745837450027466,
      "learning_rate": 5.7028380634390655e-05,
      "loss": 0.2766,
      "step": 2146
    },
    {
      "epoch": 3.578333333333333,
      "grad_norm": 0.05220984295010567,
      "learning_rate": 5.696160267111853e-05,
      "loss": 0.2743,
      "step": 2147
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.03259306401014328,
      "learning_rate": 5.6894824707846415e-05,
      "loss": 0.2564,
      "step": 2148
    },
    {
      "epoch": 3.5816666666666666,
      "grad_norm": 0.051403552293777466,
      "learning_rate": 5.6828046744574295e-05,
      "loss": 0.285,
      "step": 2149
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 0.041519466787576675,
      "learning_rate": 5.676126878130217e-05,
      "loss": 0.2948,
      "step": 2150
    },
    {
      "epoch": 3.585,
      "grad_norm": 0.041232798248529434,
      "learning_rate": 5.6694490818030055e-05,
      "loss": 0.3063,
      "step": 2151
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.03517613559961319,
      "learning_rate": 5.662771285475793e-05,
      "loss": 0.2529,
      "step": 2152
    },
    {
      "epoch": 3.5883333333333334,
      "grad_norm": 0.047473929822444916,
      "learning_rate": 5.6560934891485815e-05,
      "loss": 0.3399,
      "step": 2153
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.07008601725101471,
      "learning_rate": 5.6494156928213694e-05,
      "loss": 0.2778,
      "step": 2154
    },
    {
      "epoch": 3.591666666666667,
      "grad_norm": 0.06740865111351013,
      "learning_rate": 5.642737896494157e-05,
      "loss": 0.3169,
      "step": 2155
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.05508756265044212,
      "learning_rate": 5.6360601001669454e-05,
      "loss": 0.3514,
      "step": 2156
    },
    {
      "epoch": 3.5949999999999998,
      "grad_norm": 0.03313438966870308,
      "learning_rate": 5.629382303839733e-05,
      "loss": 0.1979,
      "step": 2157
    },
    {
      "epoch": 3.5966666666666667,
      "grad_norm": 0.05486585944890976,
      "learning_rate": 5.622704507512521e-05,
      "loss": 0.3009,
      "step": 2158
    },
    {
      "epoch": 3.5983333333333336,
      "grad_norm": 0.07651910185813904,
      "learning_rate": 5.6160267111853094e-05,
      "loss": 0.3792,
      "step": 2159
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.04317765682935715,
      "learning_rate": 5.609348914858097e-05,
      "loss": 0.3089,
      "step": 2160
    },
    {
      "epoch": 3.6016666666666666,
      "grad_norm": 0.03822141885757446,
      "learning_rate": 5.6026711185308854e-05,
      "loss": 0.3034,
      "step": 2161
    },
    {
      "epoch": 3.6033333333333335,
      "grad_norm": 0.06039479747414589,
      "learning_rate": 5.5959933222036734e-05,
      "loss": 0.3444,
      "step": 2162
    },
    {
      "epoch": 3.605,
      "grad_norm": 0.04031137377023697,
      "learning_rate": 5.589315525876461e-05,
      "loss": 0.2628,
      "step": 2163
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.041085269302129745,
      "learning_rate": 5.5826377295492494e-05,
      "loss": 0.293,
      "step": 2164
    },
    {
      "epoch": 3.6083333333333334,
      "grad_norm": 0.08359452337026596,
      "learning_rate": 5.575959933222037e-05,
      "loss": 0.3455,
      "step": 2165
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.02727295085787773,
      "learning_rate": 5.569282136894825e-05,
      "loss": 0.2507,
      "step": 2166
    },
    {
      "epoch": 3.611666666666667,
      "grad_norm": 0.07600970566272736,
      "learning_rate": 5.5626043405676134e-05,
      "loss": 0.3457,
      "step": 2167
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.033365968614816666,
      "learning_rate": 5.555926544240401e-05,
      "loss": 0.2203,
      "step": 2168
    },
    {
      "epoch": 3.615,
      "grad_norm": 0.07142902165651321,
      "learning_rate": 5.5492487479131893e-05,
      "loss": 0.392,
      "step": 2169
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 0.04928964003920555,
      "learning_rate": 5.5425709515859767e-05,
      "loss": 0.2866,
      "step": 2170
    },
    {
      "epoch": 3.618333333333333,
      "grad_norm": 0.042940955609083176,
      "learning_rate": 5.5358931552587646e-05,
      "loss": 0.2857,
      "step": 2171
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.027947548776865005,
      "learning_rate": 5.529215358931553e-05,
      "loss": 0.2156,
      "step": 2172
    },
    {
      "epoch": 3.6216666666666666,
      "grad_norm": 0.03687729686498642,
      "learning_rate": 5.5225375626043406e-05,
      "loss": 0.279,
      "step": 2173
    },
    {
      "epoch": 3.623333333333333,
      "grad_norm": 0.06112594157457352,
      "learning_rate": 5.5158597662771286e-05,
      "loss": 0.3684,
      "step": 2174
    },
    {
      "epoch": 3.625,
      "grad_norm": 0.052963294088840485,
      "learning_rate": 5.509181969949917e-05,
      "loss": 0.2949,
      "step": 2175
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.08442001789808273,
      "learning_rate": 5.5025041736227046e-05,
      "loss": 0.3458,
      "step": 2176
    },
    {
      "epoch": 3.6283333333333334,
      "grad_norm": 0.033177632838487625,
      "learning_rate": 5.495826377295493e-05,
      "loss": 0.2205,
      "step": 2177
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.037988338619470596,
      "learning_rate": 5.4891485809682806e-05,
      "loss": 0.2849,
      "step": 2178
    },
    {
      "epoch": 3.631666666666667,
      "grad_norm": 0.041489243507385254,
      "learning_rate": 5.4824707846410686e-05,
      "loss": 0.2559,
      "step": 2179
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.05973999947309494,
      "learning_rate": 5.475792988313857e-05,
      "loss": 0.3039,
      "step": 2180
    },
    {
      "epoch": 3.635,
      "grad_norm": 0.04539039731025696,
      "learning_rate": 5.4691151919866446e-05,
      "loss": 0.2515,
      "step": 2181
    },
    {
      "epoch": 3.6366666666666667,
      "grad_norm": 0.05985662341117859,
      "learning_rate": 5.462437395659432e-05,
      "loss": 0.3542,
      "step": 2182
    },
    {
      "epoch": 3.638333333333333,
      "grad_norm": 0.037091873586177826,
      "learning_rate": 5.4557595993322206e-05,
      "loss": 0.2781,
      "step": 2183
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.03129229694604874,
      "learning_rate": 5.4490818030050086e-05,
      "loss": 0.2435,
      "step": 2184
    },
    {
      "epoch": 3.6416666666666666,
      "grad_norm": 0.03741559386253357,
      "learning_rate": 5.442404006677797e-05,
      "loss": 0.2766,
      "step": 2185
    },
    {
      "epoch": 3.6433333333333335,
      "grad_norm": 0.05551649630069733,
      "learning_rate": 5.4357262103505845e-05,
      "loss": 0.3235,
      "step": 2186
    },
    {
      "epoch": 3.645,
      "grad_norm": 0.04032178968191147,
      "learning_rate": 5.429048414023372e-05,
      "loss": 0.2797,
      "step": 2187
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.04858928918838501,
      "learning_rate": 5.4223706176961605e-05,
      "loss": 0.3012,
      "step": 2188
    },
    {
      "epoch": 3.6483333333333334,
      "grad_norm": 0.05200204998254776,
      "learning_rate": 5.4156928213689485e-05,
      "loss": 0.3032,
      "step": 2189
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.04631216451525688,
      "learning_rate": 5.409015025041736e-05,
      "loss": 0.332,
      "step": 2190
    },
    {
      "epoch": 3.6516666666666664,
      "grad_norm": 0.03935147449374199,
      "learning_rate": 5.4023372287145245e-05,
      "loss": 0.2455,
      "step": 2191
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.040932752192020416,
      "learning_rate": 5.3956594323873125e-05,
      "loss": 0.2493,
      "step": 2192
    },
    {
      "epoch": 3.6550000000000002,
      "grad_norm": 0.04903619736433029,
      "learning_rate": 5.388981636060101e-05,
      "loss": 0.2764,
      "step": 2193
    },
    {
      "epoch": 3.6566666666666667,
      "grad_norm": 0.04444397613406181,
      "learning_rate": 5.3823038397328885e-05,
      "loss": 0.2908,
      "step": 2194
    },
    {
      "epoch": 3.658333333333333,
      "grad_norm": 0.06973066926002502,
      "learning_rate": 5.375626043405676e-05,
      "loss": 0.3605,
      "step": 2195
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.07812836766242981,
      "learning_rate": 5.3689482470784645e-05,
      "loss": 0.3632,
      "step": 2196
    },
    {
      "epoch": 3.6616666666666666,
      "grad_norm": 0.03597690910100937,
      "learning_rate": 5.3622704507512525e-05,
      "loss": 0.2491,
      "step": 2197
    },
    {
      "epoch": 3.663333333333333,
      "grad_norm": 0.04885919392108917,
      "learning_rate": 5.35559265442404e-05,
      "loss": 0.3442,
      "step": 2198
    },
    {
      "epoch": 3.665,
      "grad_norm": 0.04427666962146759,
      "learning_rate": 5.3489148580968285e-05,
      "loss": 0.2865,
      "step": 2199
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.04566289857029915,
      "learning_rate": 5.342237061769616e-05,
      "loss": 0.2768,
      "step": 2200
    },
    {
      "epoch": 3.6683333333333334,
      "grad_norm": 0.058974504470825195,
      "learning_rate": 5.3355592654424044e-05,
      "loss": 0.2651,
      "step": 2201
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.03703690320253372,
      "learning_rate": 5.3288814691151924e-05,
      "loss": 0.2708,
      "step": 2202
    },
    {
      "epoch": 3.671666666666667,
      "grad_norm": 0.0379658006131649,
      "learning_rate": 5.32220367278798e-05,
      "loss": 0.2428,
      "step": 2203
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.07808391749858856,
      "learning_rate": 5.3155258764607684e-05,
      "loss": 0.3127,
      "step": 2204
    },
    {
      "epoch": 3.675,
      "grad_norm": 0.04623301327228546,
      "learning_rate": 5.308848080133556e-05,
      "loss": 0.2843,
      "step": 2205
    },
    {
      "epoch": 3.6766666666666667,
      "grad_norm": 0.05156264826655388,
      "learning_rate": 5.302170283806344e-05,
      "loss": 0.3127,
      "step": 2206
    },
    {
      "epoch": 3.6783333333333332,
      "grad_norm": 0.04490715637803078,
      "learning_rate": 5.2954924874791324e-05,
      "loss": 0.2762,
      "step": 2207
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.05401931703090668,
      "learning_rate": 5.28881469115192e-05,
      "loss": 0.3037,
      "step": 2208
    },
    {
      "epoch": 3.6816666666666666,
      "grad_norm": 0.039429180324077606,
      "learning_rate": 5.2821368948247084e-05,
      "loss": 0.2574,
      "step": 2209
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 0.04376679286360741,
      "learning_rate": 5.2754590984974964e-05,
      "loss": 0.2437,
      "step": 2210
    },
    {
      "epoch": 3.685,
      "grad_norm": 0.0706021711230278,
      "learning_rate": 5.268781302170284e-05,
      "loss": 0.3547,
      "step": 2211
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.030877159908413887,
      "learning_rate": 5.2621035058430724e-05,
      "loss": 0.2485,
      "step": 2212
    },
    {
      "epoch": 3.6883333333333335,
      "grad_norm": 0.03962983936071396,
      "learning_rate": 5.25542570951586e-05,
      "loss": 0.2948,
      "step": 2213
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.029541831463575363,
      "learning_rate": 5.248747913188648e-05,
      "loss": 0.2306,
      "step": 2214
    },
    {
      "epoch": 3.6916666666666664,
      "grad_norm": 0.03556875139474869,
      "learning_rate": 5.2420701168614363e-05,
      "loss": 0.2843,
      "step": 2215
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.04626573622226715,
      "learning_rate": 5.2353923205342237e-05,
      "loss": 0.3055,
      "step": 2216
    },
    {
      "epoch": 3.695,
      "grad_norm": 0.06917616724967957,
      "learning_rate": 5.228714524207012e-05,
      "loss": 0.3532,
      "step": 2217
    },
    {
      "epoch": 3.6966666666666668,
      "grad_norm": 0.04706976190209389,
      "learning_rate": 5.2220367278797996e-05,
      "loss": 0.2923,
      "step": 2218
    },
    {
      "epoch": 3.6983333333333333,
      "grad_norm": 0.044191792607307434,
      "learning_rate": 5.2153589315525876e-05,
      "loss": 0.309,
      "step": 2219
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.05214516818523407,
      "learning_rate": 5.208681135225376e-05,
      "loss": 0.3456,
      "step": 2220
    },
    {
      "epoch": 3.7016666666666667,
      "grad_norm": 0.03652738779783249,
      "learning_rate": 5.2020033388981636e-05,
      "loss": 0.2815,
      "step": 2221
    },
    {
      "epoch": 3.703333333333333,
      "grad_norm": 0.04918980598449707,
      "learning_rate": 5.195325542570952e-05,
      "loss": 0.2889,
      "step": 2222
    },
    {
      "epoch": 3.705,
      "grad_norm": 0.07549447566270828,
      "learning_rate": 5.18864774624374e-05,
      "loss": 0.3339,
      "step": 2223
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.047072090208530426,
      "learning_rate": 5.1819699499165276e-05,
      "loss": 0.3275,
      "step": 2224
    },
    {
      "epoch": 3.7083333333333335,
      "grad_norm": 0.036456912755966187,
      "learning_rate": 5.175292153589316e-05,
      "loss": 0.2697,
      "step": 2225
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.049919966608285904,
      "learning_rate": 5.1686143572621036e-05,
      "loss": 0.2816,
      "step": 2226
    },
    {
      "epoch": 3.711666666666667,
      "grad_norm": 0.043810661882162094,
      "learning_rate": 5.1619365609348916e-05,
      "loss": 0.277,
      "step": 2227
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.055726032704114914,
      "learning_rate": 5.15525876460768e-05,
      "loss": 0.3613,
      "step": 2228
    },
    {
      "epoch": 3.715,
      "grad_norm": 0.03758722171187401,
      "learning_rate": 5.1485809682804676e-05,
      "loss": 0.2715,
      "step": 2229
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 0.04562978073954582,
      "learning_rate": 5.141903171953256e-05,
      "loss": 0.2839,
      "step": 2230
    },
    {
      "epoch": 3.7183333333333333,
      "grad_norm": 0.039042118936777115,
      "learning_rate": 5.1352253756260436e-05,
      "loss": 0.3018,
      "step": 2231
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.050726260989904404,
      "learning_rate": 5.1285475792988315e-05,
      "loss": 0.2855,
      "step": 2232
    },
    {
      "epoch": 3.7216666666666667,
      "grad_norm": 0.04372188448905945,
      "learning_rate": 5.12186978297162e-05,
      "loss": 0.3005,
      "step": 2233
    },
    {
      "epoch": 3.7233333333333336,
      "grad_norm": 0.09707112610340118,
      "learning_rate": 5.1151919866444075e-05,
      "loss": 0.3656,
      "step": 2234
    },
    {
      "epoch": 3.725,
      "grad_norm": 0.055984415113925934,
      "learning_rate": 5.108514190317195e-05,
      "loss": 0.3251,
      "step": 2235
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.04265431687235832,
      "learning_rate": 5.1018363939899835e-05,
      "loss": 0.3105,
      "step": 2236
    },
    {
      "epoch": 3.7283333333333335,
      "grad_norm": 0.08544260263442993,
      "learning_rate": 5.0951585976627715e-05,
      "loss": 0.3211,
      "step": 2237
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.03608574718236923,
      "learning_rate": 5.08848080133556e-05,
      "loss": 0.2881,
      "step": 2238
    },
    {
      "epoch": 3.7316666666666665,
      "grad_norm": 0.059761010110378265,
      "learning_rate": 5.0818030050083475e-05,
      "loss": 0.3406,
      "step": 2239
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.05093911290168762,
      "learning_rate": 5.0751252086811355e-05,
      "loss": 0.3731,
      "step": 2240
    },
    {
      "epoch": 3.735,
      "grad_norm": 0.03408380225300789,
      "learning_rate": 5.068447412353924e-05,
      "loss": 0.2278,
      "step": 2241
    },
    {
      "epoch": 3.736666666666667,
      "grad_norm": 0.05256097763776779,
      "learning_rate": 5.0617696160267115e-05,
      "loss": 0.3145,
      "step": 2242
    },
    {
      "epoch": 3.7383333333333333,
      "grad_norm": 0.041055336594581604,
      "learning_rate": 5.055091819699499e-05,
      "loss": 0.3346,
      "step": 2243
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.049299802631139755,
      "learning_rate": 5.0484140233722875e-05,
      "loss": 0.2422,
      "step": 2244
    },
    {
      "epoch": 3.7416666666666667,
      "grad_norm": 0.047608859837055206,
      "learning_rate": 5.0417362270450755e-05,
      "loss": 0.3025,
      "step": 2245
    },
    {
      "epoch": 3.743333333333333,
      "grad_norm": 0.04479500651359558,
      "learning_rate": 5.035058430717864e-05,
      "loss": 0.3221,
      "step": 2246
    },
    {
      "epoch": 3.745,
      "grad_norm": 0.06393972039222717,
      "learning_rate": 5.0283806343906514e-05,
      "loss": 0.3525,
      "step": 2247
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.06390109658241272,
      "learning_rate": 5.021702838063439e-05,
      "loss": 0.341,
      "step": 2248
    },
    {
      "epoch": 3.748333333333333,
      "grad_norm": 0.03969492018222809,
      "learning_rate": 5.0150250417362274e-05,
      "loss": 0.2968,
      "step": 2249
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.05575576052069664,
      "learning_rate": 5.0083472454090154e-05,
      "loss": 0.3776,
      "step": 2250
    },
    {
      "epoch": 3.751666666666667,
      "grad_norm": 0.04547608643770218,
      "learning_rate": 5.001669449081803e-05,
      "loss": 0.3373,
      "step": 2251
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.0491897277534008,
      "learning_rate": 4.9949916527545914e-05,
      "loss": 0.3029,
      "step": 2252
    },
    {
      "epoch": 3.755,
      "grad_norm": 0.04900585860013962,
      "learning_rate": 4.988313856427379e-05,
      "loss": 0.3234,
      "step": 2253
    },
    {
      "epoch": 3.756666666666667,
      "grad_norm": 0.03796408697962761,
      "learning_rate": 4.9816360601001674e-05,
      "loss": 0.2824,
      "step": 2254
    },
    {
      "epoch": 3.7583333333333333,
      "grad_norm": 0.045203328132629395,
      "learning_rate": 4.9749582637729554e-05,
      "loss": 0.292,
      "step": 2255
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.05905674397945404,
      "learning_rate": 4.9682804674457434e-05,
      "loss": 0.3142,
      "step": 2256
    },
    {
      "epoch": 3.7616666666666667,
      "grad_norm": 0.06192672997713089,
      "learning_rate": 4.961602671118531e-05,
      "loss": 0.331,
      "step": 2257
    },
    {
      "epoch": 3.763333333333333,
      "grad_norm": 0.061643291264772415,
      "learning_rate": 4.9549248747913194e-05,
      "loss": 0.3196,
      "step": 2258
    },
    {
      "epoch": 3.765,
      "grad_norm": 0.046851977705955505,
      "learning_rate": 4.9482470784641074e-05,
      "loss": 0.3633,
      "step": 2259
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.0406402163207531,
      "learning_rate": 4.9415692821368953e-05,
      "loss": 0.2865,
      "step": 2260
    },
    {
      "epoch": 3.7683333333333335,
      "grad_norm": 0.0398743562400341,
      "learning_rate": 4.934891485809683e-05,
      "loss": 0.261,
      "step": 2261
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.05403910204768181,
      "learning_rate": 4.9282136894824707e-05,
      "loss": 0.3146,
      "step": 2262
    },
    {
      "epoch": 3.7716666666666665,
      "grad_norm": 0.07161390781402588,
      "learning_rate": 4.921535893155259e-05,
      "loss": 0.3279,
      "step": 2263
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.0631110891699791,
      "learning_rate": 4.914858096828047e-05,
      "loss": 0.3584,
      "step": 2264
    },
    {
      "epoch": 3.775,
      "grad_norm": 0.045052625238895416,
      "learning_rate": 4.9081803005008346e-05,
      "loss": 0.2692,
      "step": 2265
    },
    {
      "epoch": 3.7766666666666664,
      "grad_norm": 0.05073972046375275,
      "learning_rate": 4.9015025041736226e-05,
      "loss": 0.3358,
      "step": 2266
    },
    {
      "epoch": 3.7783333333333333,
      "grad_norm": 0.03439733386039734,
      "learning_rate": 4.894824707846411e-05,
      "loss": 0.239,
      "step": 2267
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.04179534688591957,
      "learning_rate": 4.888146911519199e-05,
      "loss": 0.3156,
      "step": 2268
    },
    {
      "epoch": 3.7816666666666667,
      "grad_norm": 0.042909495532512665,
      "learning_rate": 4.8814691151919866e-05,
      "loss": 0.3015,
      "step": 2269
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 0.0441095270216465,
      "learning_rate": 4.8747913188647746e-05,
      "loss": 0.2605,
      "step": 2270
    },
    {
      "epoch": 3.785,
      "grad_norm": 0.09778786450624466,
      "learning_rate": 4.8681135225375626e-05,
      "loss": 0.3285,
      "step": 2271
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.08273831009864807,
      "learning_rate": 4.861435726210351e-05,
      "loss": 0.3321,
      "step": 2272
    },
    {
      "epoch": 3.788333333333333,
      "grad_norm": 0.03355126827955246,
      "learning_rate": 4.8547579298831386e-05,
      "loss": 0.2688,
      "step": 2273
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.04577527195215225,
      "learning_rate": 4.8480801335559266e-05,
      "loss": 0.3076,
      "step": 2274
    },
    {
      "epoch": 3.7916666666666665,
      "grad_norm": 0.045582495629787445,
      "learning_rate": 4.8414023372287146e-05,
      "loss": 0.3233,
      "step": 2275
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.05001555755734444,
      "learning_rate": 4.834724540901503e-05,
      "loss": 0.3147,
      "step": 2276
    },
    {
      "epoch": 3.795,
      "grad_norm": 0.03978624567389488,
      "learning_rate": 4.8280467445742906e-05,
      "loss": 0.3079,
      "step": 2277
    },
    {
      "epoch": 3.796666666666667,
      "grad_norm": 0.06976000219583511,
      "learning_rate": 4.8213689482470785e-05,
      "loss": 0.3761,
      "step": 2278
    },
    {
      "epoch": 3.7983333333333333,
      "grad_norm": 0.03987511619925499,
      "learning_rate": 4.8146911519198665e-05,
      "loss": 0.2698,
      "step": 2279
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.03853457421064377,
      "learning_rate": 4.8080133555926545e-05,
      "loss": 0.2565,
      "step": 2280
    },
    {
      "epoch": 3.8016666666666667,
      "grad_norm": 0.047276075929403305,
      "learning_rate": 4.8013355592654425e-05,
      "loss": 0.3098,
      "step": 2281
    },
    {
      "epoch": 3.8033333333333332,
      "grad_norm": 0.07286152243614197,
      "learning_rate": 4.7946577629382305e-05,
      "loss": 0.2866,
      "step": 2282
    },
    {
      "epoch": 3.805,
      "grad_norm": 0.03284744173288345,
      "learning_rate": 4.7879799666110185e-05,
      "loss": 0.2637,
      "step": 2283
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.041273392736911774,
      "learning_rate": 4.7813021702838065e-05,
      "loss": 0.2914,
      "step": 2284
    },
    {
      "epoch": 3.8083333333333336,
      "grad_norm": 0.06893475353717804,
      "learning_rate": 4.7746243739565945e-05,
      "loss": 0.2672,
      "step": 2285
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.04721027985215187,
      "learning_rate": 4.7679465776293825e-05,
      "loss": 0.356,
      "step": 2286
    },
    {
      "epoch": 3.8116666666666665,
      "grad_norm": 0.04905925318598747,
      "learning_rate": 4.7612687813021705e-05,
      "loss": 0.3341,
      "step": 2287
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.04620695859193802,
      "learning_rate": 4.7545909849749585e-05,
      "loss": 0.3148,
      "step": 2288
    },
    {
      "epoch": 3.815,
      "grad_norm": 0.04177837446331978,
      "learning_rate": 4.7479131886477465e-05,
      "loss": 0.2746,
      "step": 2289
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 0.0627787634730339,
      "learning_rate": 4.7412353923205345e-05,
      "loss": 0.3769,
      "step": 2290
    },
    {
      "epoch": 3.8183333333333334,
      "grad_norm": 0.08533315360546112,
      "learning_rate": 4.7345575959933225e-05,
      "loss": 0.3308,
      "step": 2291
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.052805475890636444,
      "learning_rate": 4.7278797996661104e-05,
      "loss": 0.3141,
      "step": 2292
    },
    {
      "epoch": 3.8216666666666668,
      "grad_norm": 0.041355639696121216,
      "learning_rate": 4.7212020033388984e-05,
      "loss": 0.3148,
      "step": 2293
    },
    {
      "epoch": 3.8233333333333333,
      "grad_norm": 0.09669406712055206,
      "learning_rate": 4.7145242070116864e-05,
      "loss": 0.3972,
      "step": 2294
    },
    {
      "epoch": 3.825,
      "grad_norm": 0.06271034479141235,
      "learning_rate": 4.7078464106844744e-05,
      "loss": 0.3338,
      "step": 2295
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.07006921619176865,
      "learning_rate": 4.7011686143572624e-05,
      "loss": 0.336,
      "step": 2296
    },
    {
      "epoch": 3.828333333333333,
      "grad_norm": 0.07071804255247116,
      "learning_rate": 4.6944908180300504e-05,
      "loss": 0.2339,
      "step": 2297
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.058745551854372025,
      "learning_rate": 4.6878130217028384e-05,
      "loss": 0.3139,
      "step": 2298
    },
    {
      "epoch": 3.8316666666666666,
      "grad_norm": 0.0717434287071228,
      "learning_rate": 4.6811352253756264e-05,
      "loss": 0.3839,
      "step": 2299
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.03580072149634361,
      "learning_rate": 4.6744574290484144e-05,
      "loss": 0.2775,
      "step": 2300
    },
    {
      "epoch": 3.835,
      "grad_norm": 0.032919686287641525,
      "learning_rate": 4.667779632721202e-05,
      "loss": 0.2359,
      "step": 2301
    },
    {
      "epoch": 3.836666666666667,
      "grad_norm": 0.05487212911248207,
      "learning_rate": 4.6611018363939904e-05,
      "loss": 0.3401,
      "step": 2302
    },
    {
      "epoch": 3.8383333333333334,
      "grad_norm": 0.048409927636384964,
      "learning_rate": 4.6544240400667784e-05,
      "loss": 0.2745,
      "step": 2303
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.043662428855895996,
      "learning_rate": 4.6477462437395664e-05,
      "loss": 0.2665,
      "step": 2304
    },
    {
      "epoch": 3.841666666666667,
      "grad_norm": 0.055802542716264725,
      "learning_rate": 4.641068447412354e-05,
      "loss": 0.3909,
      "step": 2305
    },
    {
      "epoch": 3.8433333333333333,
      "grad_norm": 0.04583372548222542,
      "learning_rate": 4.6343906510851423e-05,
      "loss": 0.3004,
      "step": 2306
    },
    {
      "epoch": 3.8449999999999998,
      "grad_norm": 0.06197332218289375,
      "learning_rate": 4.6277128547579303e-05,
      "loss": 0.2652,
      "step": 2307
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.06811054795980453,
      "learning_rate": 4.621035058430718e-05,
      "loss": 0.3203,
      "step": 2308
    },
    {
      "epoch": 3.8483333333333336,
      "grad_norm": 0.03475429490208626,
      "learning_rate": 4.6143572621035056e-05,
      "loss": 0.2778,
      "step": 2309
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.08098166435956955,
      "learning_rate": 4.6076794657762936e-05,
      "loss": 0.3411,
      "step": 2310
    },
    {
      "epoch": 3.8516666666666666,
      "grad_norm": 0.04008112847805023,
      "learning_rate": 4.601001669449082e-05,
      "loss": 0.297,
      "step": 2311
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.0524994432926178,
      "learning_rate": 4.59432387312187e-05,
      "loss": 0.3234,
      "step": 2312
    },
    {
      "epoch": 3.855,
      "grad_norm": 0.05131039023399353,
      "learning_rate": 4.5876460767946576e-05,
      "loss": 0.3285,
      "step": 2313
    },
    {
      "epoch": 3.8566666666666665,
      "grad_norm": 0.05893629416823387,
      "learning_rate": 4.5809682804674456e-05,
      "loss": 0.3216,
      "step": 2314
    },
    {
      "epoch": 3.8583333333333334,
      "grad_norm": 0.08207676559686661,
      "learning_rate": 4.574290484140234e-05,
      "loss": 0.399,
      "step": 2315
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.04657166823744774,
      "learning_rate": 4.567612687813022e-05,
      "loss": 0.3632,
      "step": 2316
    },
    {
      "epoch": 3.861666666666667,
      "grad_norm": 0.0332380048930645,
      "learning_rate": 4.5609348914858096e-05,
      "loss": 0.2474,
      "step": 2317
    },
    {
      "epoch": 3.8633333333333333,
      "grad_norm": 0.043884992599487305,
      "learning_rate": 4.5542570951585976e-05,
      "loss": 0.3039,
      "step": 2318
    },
    {
      "epoch": 3.865,
      "grad_norm": 0.04678218439221382,
      "learning_rate": 4.5475792988313856e-05,
      "loss": 0.3519,
      "step": 2319
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.041687071323394775,
      "learning_rate": 4.540901502504174e-05,
      "loss": 0.2773,
      "step": 2320
    },
    {
      "epoch": 3.868333333333333,
      "grad_norm": 0.04243272915482521,
      "learning_rate": 4.5342237061769616e-05,
      "loss": 0.2713,
      "step": 2321
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.03999955207109451,
      "learning_rate": 4.5275459098497496e-05,
      "loss": 0.2791,
      "step": 2322
    },
    {
      "epoch": 3.8716666666666666,
      "grad_norm": 0.037708114832639694,
      "learning_rate": 4.5208681135225376e-05,
      "loss": 0.2689,
      "step": 2323
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.040024664252996445,
      "learning_rate": 4.514190317195326e-05,
      "loss": 0.3122,
      "step": 2324
    },
    {
      "epoch": 3.875,
      "grad_norm": 0.04993494227528572,
      "learning_rate": 4.5075125208681135e-05,
      "loss": 0.2607,
      "step": 2325
    },
    {
      "epoch": 3.876666666666667,
      "grad_norm": 0.03762444853782654,
      "learning_rate": 4.5008347245409015e-05,
      "loss": 0.282,
      "step": 2326
    },
    {
      "epoch": 3.8783333333333334,
      "grad_norm": 0.05172199383378029,
      "learning_rate": 4.4941569282136895e-05,
      "loss": 0.2868,
      "step": 2327
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.046904318034648895,
      "learning_rate": 4.4874791318864775e-05,
      "loss": 0.2973,
      "step": 2328
    },
    {
      "epoch": 3.881666666666667,
      "grad_norm": 0.04556960240006447,
      "learning_rate": 4.4808013355592655e-05,
      "loss": 0.2751,
      "step": 2329
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 0.049598485231399536,
      "learning_rate": 4.4741235392320535e-05,
      "loss": 0.3514,
      "step": 2330
    },
    {
      "epoch": 3.885,
      "grad_norm": 0.04886253550648689,
      "learning_rate": 4.4674457429048415e-05,
      "loss": 0.3203,
      "step": 2331
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.05291158705949783,
      "learning_rate": 4.4607679465776295e-05,
      "loss": 0.3057,
      "step": 2332
    },
    {
      "epoch": 3.888333333333333,
      "grad_norm": 0.053746018558740616,
      "learning_rate": 4.4540901502504175e-05,
      "loss": 0.3042,
      "step": 2333
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.043827153742313385,
      "learning_rate": 4.4474123539232055e-05,
      "loss": 0.2733,
      "step": 2334
    },
    {
      "epoch": 3.8916666666666666,
      "grad_norm": 0.04716042801737785,
      "learning_rate": 4.4407345575959935e-05,
      "loss": 0.2988,
      "step": 2335
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.0733037143945694,
      "learning_rate": 4.4340567612687815e-05,
      "loss": 0.429,
      "step": 2336
    },
    {
      "epoch": 3.895,
      "grad_norm": 0.054058678448200226,
      "learning_rate": 4.4273789649415695e-05,
      "loss": 0.3338,
      "step": 2337
    },
    {
      "epoch": 3.8966666666666665,
      "grad_norm": 0.06411673873662949,
      "learning_rate": 4.4207011686143574e-05,
      "loss": 0.3251,
      "step": 2338
    },
    {
      "epoch": 3.8983333333333334,
      "grad_norm": 0.05633579194545746,
      "learning_rate": 4.4140233722871454e-05,
      "loss": 0.3623,
      "step": 2339
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.03515937924385071,
      "learning_rate": 4.4073455759599334e-05,
      "loss": 0.2704,
      "step": 2340
    },
    {
      "epoch": 3.9016666666666664,
      "grad_norm": 0.06234006583690643,
      "learning_rate": 4.4006677796327214e-05,
      "loss": 0.4366,
      "step": 2341
    },
    {
      "epoch": 3.9033333333333333,
      "grad_norm": 0.10498373210430145,
      "learning_rate": 4.3939899833055094e-05,
      "loss": 0.3463,
      "step": 2342
    },
    {
      "epoch": 3.9050000000000002,
      "grad_norm": 0.04254730045795441,
      "learning_rate": 4.3873121869782974e-05,
      "loss": 0.3197,
      "step": 2343
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.07321332395076752,
      "learning_rate": 4.3806343906510854e-05,
      "loss": 0.3436,
      "step": 2344
    },
    {
      "epoch": 3.908333333333333,
      "grad_norm": 0.041004523634910583,
      "learning_rate": 4.373956594323873e-05,
      "loss": 0.2892,
      "step": 2345
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.0425226166844368,
      "learning_rate": 4.3672787979966614e-05,
      "loss": 0.2976,
      "step": 2346
    },
    {
      "epoch": 3.9116666666666666,
      "grad_norm": 0.04414508491754532,
      "learning_rate": 4.3606010016694494e-05,
      "loss": 0.2864,
      "step": 2347
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.05345797911286354,
      "learning_rate": 4.3539232053422374e-05,
      "loss": 0.3614,
      "step": 2348
    },
    {
      "epoch": 3.915,
      "grad_norm": 0.08206330239772797,
      "learning_rate": 4.3472454090150254e-05,
      "loss": 0.4113,
      "step": 2349
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 0.05221524462103844,
      "learning_rate": 4.3405676126878134e-05,
      "loss": 0.3406,
      "step": 2350
    },
    {
      "epoch": 3.9183333333333334,
      "grad_norm": 0.04745186120271683,
      "learning_rate": 4.3338898163606014e-05,
      "loss": 0.3333,
      "step": 2351
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.055711448192596436,
      "learning_rate": 4.3272120200333893e-05,
      "loss": 0.3494,
      "step": 2352
    },
    {
      "epoch": 3.921666666666667,
      "grad_norm": 0.041494034230709076,
      "learning_rate": 4.3205342237061773e-05,
      "loss": 0.3046,
      "step": 2353
    },
    {
      "epoch": 3.9233333333333333,
      "grad_norm": 0.03756599500775337,
      "learning_rate": 4.313856427378965e-05,
      "loss": 0.2502,
      "step": 2354
    },
    {
      "epoch": 3.925,
      "grad_norm": 0.0662330612540245,
      "learning_rate": 4.307178631051753e-05,
      "loss": 0.374,
      "step": 2355
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.09095367044210434,
      "learning_rate": 4.300500834724541e-05,
      "loss": 0.3743,
      "step": 2356
    },
    {
      "epoch": 3.9283333333333332,
      "grad_norm": 0.043696340173482895,
      "learning_rate": 4.293823038397329e-05,
      "loss": 0.2575,
      "step": 2357
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.0525212362408638,
      "learning_rate": 4.2871452420701166e-05,
      "loss": 0.3439,
      "step": 2358
    },
    {
      "epoch": 3.9316666666666666,
      "grad_norm": 0.06201520934700966,
      "learning_rate": 4.280467445742905e-05,
      "loss": 0.3117,
      "step": 2359
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.0761747881770134,
      "learning_rate": 4.273789649415693e-05,
      "loss": 0.3337,
      "step": 2360
    },
    {
      "epoch": 3.935,
      "grad_norm": 0.04278711974620819,
      "learning_rate": 4.267111853088481e-05,
      "loss": 0.2493,
      "step": 2361
    },
    {
      "epoch": 3.9366666666666665,
      "grad_norm": 0.04268822446465492,
      "learning_rate": 4.2604340567612686e-05,
      "loss": 0.3039,
      "step": 2362
    },
    {
      "epoch": 3.9383333333333335,
      "grad_norm": 0.054084841161966324,
      "learning_rate": 4.253756260434057e-05,
      "loss": 0.3883,
      "step": 2363
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.05019110068678856,
      "learning_rate": 4.247078464106845e-05,
      "loss": 0.3253,
      "step": 2364
    },
    {
      "epoch": 3.9416666666666664,
      "grad_norm": 0.045176636427640915,
      "learning_rate": 4.240400667779633e-05,
      "loss": 0.24,
      "step": 2365
    },
    {
      "epoch": 3.9433333333333334,
      "grad_norm": 0.04279222711920738,
      "learning_rate": 4.2337228714524206e-05,
      "loss": 0.2782,
      "step": 2366
    },
    {
      "epoch": 3.945,
      "grad_norm": 0.06285958737134933,
      "learning_rate": 4.2270450751252086e-05,
      "loss": 0.285,
      "step": 2367
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.04119238629937172,
      "learning_rate": 4.220367278797997e-05,
      "loss": 0.2457,
      "step": 2368
    },
    {
      "epoch": 3.9483333333333333,
      "grad_norm": 0.04008001461625099,
      "learning_rate": 4.213689482470785e-05,
      "loss": 0.2609,
      "step": 2369
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.03805127739906311,
      "learning_rate": 4.2070116861435725e-05,
      "loss": 0.2494,
      "step": 2370
    },
    {
      "epoch": 3.9516666666666667,
      "grad_norm": 0.032111261039972305,
      "learning_rate": 4.2003338898163605e-05,
      "loss": 0.286,
      "step": 2371
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.03954695165157318,
      "learning_rate": 4.193656093489149e-05,
      "loss": 0.3042,
      "step": 2372
    },
    {
      "epoch": 3.955,
      "grad_norm": 0.04237766191363335,
      "learning_rate": 4.186978297161937e-05,
      "loss": 0.2981,
      "step": 2373
    },
    {
      "epoch": 3.9566666666666666,
      "grad_norm": 0.04288220405578613,
      "learning_rate": 4.1803005008347245e-05,
      "loss": 0.2646,
      "step": 2374
    },
    {
      "epoch": 3.9583333333333335,
      "grad_norm": 0.0444675050675869,
      "learning_rate": 4.1736227045075125e-05,
      "loss": 0.2808,
      "step": 2375
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.03974517062306404,
      "learning_rate": 4.1669449081803005e-05,
      "loss": 0.2659,
      "step": 2376
    },
    {
      "epoch": 3.961666666666667,
      "grad_norm": 0.045515626668930054,
      "learning_rate": 4.160267111853089e-05,
      "loss": 0.3135,
      "step": 2377
    },
    {
      "epoch": 3.9633333333333334,
      "grad_norm": 0.06815577298402786,
      "learning_rate": 4.1535893155258765e-05,
      "loss": 0.3494,
      "step": 2378
    },
    {
      "epoch": 3.965,
      "grad_norm": 0.0344059132039547,
      "learning_rate": 4.1469115191986645e-05,
      "loss": 0.2317,
      "step": 2379
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.03781523555517197,
      "learning_rate": 4.1402337228714525e-05,
      "loss": 0.2815,
      "step": 2380
    },
    {
      "epoch": 3.9683333333333333,
      "grad_norm": 0.06556397676467896,
      "learning_rate": 4.133555926544241e-05,
      "loss": 0.3116,
      "step": 2381
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 0.04270011931657791,
      "learning_rate": 4.1268781302170285e-05,
      "loss": 0.288,
      "step": 2382
    },
    {
      "epoch": 3.9716666666666667,
      "grad_norm": 0.05917954444885254,
      "learning_rate": 4.1202003338898165e-05,
      "loss": 0.3628,
      "step": 2383
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.04837238788604736,
      "learning_rate": 4.1135225375626044e-05,
      "loss": 0.3035,
      "step": 2384
    },
    {
      "epoch": 3.975,
      "grad_norm": 0.08503120392560959,
      "learning_rate": 4.1068447412353924e-05,
      "loss": 0.2932,
      "step": 2385
    },
    {
      "epoch": 3.9766666666666666,
      "grad_norm": 0.047676052898168564,
      "learning_rate": 4.1001669449081804e-05,
      "loss": 0.3283,
      "step": 2386
    },
    {
      "epoch": 3.9783333333333335,
      "grad_norm": 0.033103346824645996,
      "learning_rate": 4.0934891485809684e-05,
      "loss": 0.229,
      "step": 2387
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.10246075689792633,
      "learning_rate": 4.0868113522537564e-05,
      "loss": 0.375,
      "step": 2388
    },
    {
      "epoch": 3.9816666666666665,
      "grad_norm": 0.04297503083944321,
      "learning_rate": 4.0801335559265444e-05,
      "loss": 0.3124,
      "step": 2389
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 0.03325897455215454,
      "learning_rate": 4.0734557595993324e-05,
      "loss": 0.2562,
      "step": 2390
    },
    {
      "epoch": 3.985,
      "grad_norm": 0.041968896985054016,
      "learning_rate": 4.0667779632721204e-05,
      "loss": 0.2851,
      "step": 2391
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.04116244241595268,
      "learning_rate": 4.0601001669449084e-05,
      "loss": 0.2824,
      "step": 2392
    },
    {
      "epoch": 3.9883333333333333,
      "grad_norm": 0.032439712435007095,
      "learning_rate": 4.0534223706176964e-05,
      "loss": 0.2796,
      "step": 2393
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.05249546468257904,
      "learning_rate": 4.0467445742904844e-05,
      "loss": 0.3374,
      "step": 2394
    },
    {
      "epoch": 3.9916666666666667,
      "grad_norm": 0.033579520881175995,
      "learning_rate": 4.0400667779632724e-05,
      "loss": 0.267,
      "step": 2395
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.03208526596426964,
      "learning_rate": 4.0333889816360604e-05,
      "loss": 0.2765,
      "step": 2396
    },
    {
      "epoch": 3.995,
      "grad_norm": 0.1003008633852005,
      "learning_rate": 4.0267111853088484e-05,
      "loss": 0.3672,
      "step": 2397
    },
    {
      "epoch": 3.9966666666666666,
      "grad_norm": 0.0398680716753006,
      "learning_rate": 4.0200333889816363e-05,
      "loss": 0.2957,
      "step": 2398
    },
    {
      "epoch": 3.998333333333333,
      "grad_norm": 0.05385678634047508,
      "learning_rate": 4.0133555926544243e-05,
      "loss": 0.3371,
      "step": 2399
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.05757718160748482,
      "learning_rate": 4.006677796327212e-05,
      "loss": 0.2768,
      "step": 2400
    },
    {
      "epoch": 4.001666666666667,
      "grad_norm": 0.042632147669792175,
      "learning_rate": 4e-05,
      "loss": 0.2843,
      "step": 2401
    },
    {
      "epoch": 4.003333333333333,
      "grad_norm": 0.07289852201938629,
      "learning_rate": 3.9933222036727876e-05,
      "loss": 0.2681,
      "step": 2402
    },
    {
      "epoch": 4.005,
      "grad_norm": 0.1199968159198761,
      "learning_rate": 3.986644407345576e-05,
      "loss": 0.3592,
      "step": 2403
    },
    {
      "epoch": 4.006666666666667,
      "grad_norm": 0.06005864962935448,
      "learning_rate": 3.979966611018364e-05,
      "loss": 0.3395,
      "step": 2404
    },
    {
      "epoch": 4.008333333333334,
      "grad_norm": 0.058161091059446335,
      "learning_rate": 3.973288814691152e-05,
      "loss": 0.3489,
      "step": 2405
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.050109200179576874,
      "learning_rate": 3.9666110183639396e-05,
      "loss": 0.3216,
      "step": 2406
    },
    {
      "epoch": 4.011666666666667,
      "grad_norm": 0.057557038962841034,
      "learning_rate": 3.959933222036728e-05,
      "loss": 0.2819,
      "step": 2407
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.04099245369434357,
      "learning_rate": 3.953255425709516e-05,
      "loss": 0.266,
      "step": 2408
    },
    {
      "epoch": 4.015,
      "grad_norm": 0.1582806408405304,
      "learning_rate": 3.946577629382304e-05,
      "loss": 0.2654,
      "step": 2409
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 0.04695403203368187,
      "learning_rate": 3.9398998330550916e-05,
      "loss": 0.3076,
      "step": 2410
    },
    {
      "epoch": 4.0183333333333335,
      "grad_norm": 0.07520873844623566,
      "learning_rate": 3.93322203672788e-05,
      "loss": 0.3129,
      "step": 2411
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.08493182063102722,
      "learning_rate": 3.926544240400668e-05,
      "loss": 0.2975,
      "step": 2412
    },
    {
      "epoch": 4.0216666666666665,
      "grad_norm": 0.06020873039960861,
      "learning_rate": 3.919866444073456e-05,
      "loss": 0.3539,
      "step": 2413
    },
    {
      "epoch": 4.023333333333333,
      "grad_norm": 0.03551341965794563,
      "learning_rate": 3.9131886477462436e-05,
      "loss": 0.2511,
      "step": 2414
    },
    {
      "epoch": 4.025,
      "grad_norm": 0.044321827590465546,
      "learning_rate": 3.9065108514190316e-05,
      "loss": 0.2667,
      "step": 2415
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.03595517575740814,
      "learning_rate": 3.89983305509182e-05,
      "loss": 0.247,
      "step": 2416
    },
    {
      "epoch": 4.028333333333333,
      "grad_norm": 0.053797945380210876,
      "learning_rate": 3.893155258764608e-05,
      "loss": 0.3109,
      "step": 2417
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.05803889408707619,
      "learning_rate": 3.8864774624373955e-05,
      "loss": 0.2792,
      "step": 2418
    },
    {
      "epoch": 4.031666666666666,
      "grad_norm": 0.047526903450489044,
      "learning_rate": 3.8797996661101835e-05,
      "loss": 0.2904,
      "step": 2419
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.06881172955036163,
      "learning_rate": 3.873121869782972e-05,
      "loss": 0.3457,
      "step": 2420
    },
    {
      "epoch": 4.035,
      "grad_norm": 0.03555629402399063,
      "learning_rate": 3.86644407345576e-05,
      "loss": 0.186,
      "step": 2421
    },
    {
      "epoch": 4.036666666666667,
      "grad_norm": 0.04160255566239357,
      "learning_rate": 3.8597662771285475e-05,
      "loss": 0.2958,
      "step": 2422
    },
    {
      "epoch": 4.038333333333333,
      "grad_norm": 0.04215022549033165,
      "learning_rate": 3.8530884808013355e-05,
      "loss": 0.2322,
      "step": 2423
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.04109271615743637,
      "learning_rate": 3.8464106844741235e-05,
      "loss": 0.2179,
      "step": 2424
    },
    {
      "epoch": 4.041666666666667,
      "grad_norm": 0.03665188327431679,
      "learning_rate": 3.839732888146912e-05,
      "loss": 0.2518,
      "step": 2425
    },
    {
      "epoch": 4.043333333333333,
      "grad_norm": 0.12523527443408966,
      "learning_rate": 3.8330550918196995e-05,
      "loss": 0.359,
      "step": 2426
    },
    {
      "epoch": 4.045,
      "grad_norm": 0.03840736672282219,
      "learning_rate": 3.8263772954924875e-05,
      "loss": 0.2665,
      "step": 2427
    },
    {
      "epoch": 4.046666666666667,
      "grad_norm": 0.07230963557958603,
      "learning_rate": 3.8196994991652755e-05,
      "loss": 0.3939,
      "step": 2428
    },
    {
      "epoch": 4.048333333333333,
      "grad_norm": 0.06032205745577812,
      "learning_rate": 3.813021702838064e-05,
      "loss": 0.3117,
      "step": 2429
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.05423600971698761,
      "learning_rate": 3.8063439065108514e-05,
      "loss": 0.3025,
      "step": 2430
    },
    {
      "epoch": 4.051666666666667,
      "grad_norm": 0.0452219657599926,
      "learning_rate": 3.7996661101836394e-05,
      "loss": 0.2553,
      "step": 2431
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.0625486820936203,
      "learning_rate": 3.7929883138564274e-05,
      "loss": 0.3377,
      "step": 2432
    },
    {
      "epoch": 4.055,
      "grad_norm": 0.04200071468949318,
      "learning_rate": 3.7863105175292154e-05,
      "loss": 0.2463,
      "step": 2433
    },
    {
      "epoch": 4.056666666666667,
      "grad_norm": 0.04818307235836983,
      "learning_rate": 3.7796327212020034e-05,
      "loss": 0.2772,
      "step": 2434
    },
    {
      "epoch": 4.058333333333334,
      "grad_norm": 0.04974489286541939,
      "learning_rate": 3.7729549248747914e-05,
      "loss": 0.3024,
      "step": 2435
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.03490771725773811,
      "learning_rate": 3.7662771285475794e-05,
      "loss": 0.2532,
      "step": 2436
    },
    {
      "epoch": 4.0616666666666665,
      "grad_norm": 0.04164567217230797,
      "learning_rate": 3.7595993322203674e-05,
      "loss": 0.2415,
      "step": 2437
    },
    {
      "epoch": 4.0633333333333335,
      "grad_norm": 0.03503647819161415,
      "learning_rate": 3.7529215358931554e-05,
      "loss": 0.2599,
      "step": 2438
    },
    {
      "epoch": 4.065,
      "grad_norm": 0.04092158004641533,
      "learning_rate": 3.7462437395659434e-05,
      "loss": 0.2634,
      "step": 2439
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.06079591438174248,
      "learning_rate": 3.7395659432387314e-05,
      "loss": 0.2926,
      "step": 2440
    },
    {
      "epoch": 4.068333333333333,
      "grad_norm": 0.03713510185480118,
      "learning_rate": 3.7328881469115194e-05,
      "loss": 0.2502,
      "step": 2441
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.07474702596664429,
      "learning_rate": 3.7262103505843074e-05,
      "loss": 0.2782,
      "step": 2442
    },
    {
      "epoch": 4.071666666666666,
      "grad_norm": 0.04118949919939041,
      "learning_rate": 3.7195325542570954e-05,
      "loss": 0.3324,
      "step": 2443
    },
    {
      "epoch": 4.073333333333333,
      "grad_norm": 0.05641225352883339,
      "learning_rate": 3.7128547579298833e-05,
      "loss": 0.3174,
      "step": 2444
    },
    {
      "epoch": 4.075,
      "grad_norm": 0.0742374137043953,
      "learning_rate": 3.7061769616026713e-05,
      "loss": 0.2907,
      "step": 2445
    },
    {
      "epoch": 4.076666666666666,
      "grad_norm": 0.12167669087648392,
      "learning_rate": 3.699499165275459e-05,
      "loss": 0.2956,
      "step": 2446
    },
    {
      "epoch": 4.078333333333333,
      "grad_norm": 0.0525716133415699,
      "learning_rate": 3.692821368948247e-05,
      "loss": 0.2924,
      "step": 2447
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.05301329120993614,
      "learning_rate": 3.686143572621035e-05,
      "loss": 0.3302,
      "step": 2448
    },
    {
      "epoch": 4.081666666666667,
      "grad_norm": 0.04604674130678177,
      "learning_rate": 3.679465776293823e-05,
      "loss": 0.2798,
      "step": 2449
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 0.04994615912437439,
      "learning_rate": 3.6727879799666106e-05,
      "loss": 0.2836,
      "step": 2450
    },
    {
      "epoch": 4.085,
      "grad_norm": 0.05867433175444603,
      "learning_rate": 3.666110183639399e-05,
      "loss": 0.3646,
      "step": 2451
    },
    {
      "epoch": 4.086666666666667,
      "grad_norm": 0.05572754144668579,
      "learning_rate": 3.659432387312187e-05,
      "loss": 0.3221,
      "step": 2452
    },
    {
      "epoch": 4.088333333333333,
      "grad_norm": 0.08994381129741669,
      "learning_rate": 3.652754590984975e-05,
      "loss": 0.3574,
      "step": 2453
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.03924897685647011,
      "learning_rate": 3.6460767946577626e-05,
      "loss": 0.2511,
      "step": 2454
    },
    {
      "epoch": 4.091666666666667,
      "grad_norm": 0.05199775844812393,
      "learning_rate": 3.639398998330551e-05,
      "loss": 0.3195,
      "step": 2455
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.07234898209571838,
      "learning_rate": 3.632721202003339e-05,
      "loss": 0.2697,
      "step": 2456
    },
    {
      "epoch": 4.095,
      "grad_norm": 0.06769519299268723,
      "learning_rate": 3.626043405676127e-05,
      "loss": 0.2674,
      "step": 2457
    },
    {
      "epoch": 4.096666666666667,
      "grad_norm": 0.043366000056266785,
      "learning_rate": 3.6193656093489146e-05,
      "loss": 0.256,
      "step": 2458
    },
    {
      "epoch": 4.098333333333334,
      "grad_norm": 0.06884930282831192,
      "learning_rate": 3.6126878130217026e-05,
      "loss": 0.326,
      "step": 2459
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.042821671813726425,
      "learning_rate": 3.606010016694491e-05,
      "loss": 0.233,
      "step": 2460
    },
    {
      "epoch": 4.101666666666667,
      "grad_norm": 0.0707429051399231,
      "learning_rate": 3.599332220367279e-05,
      "loss": 0.2963,
      "step": 2461
    },
    {
      "epoch": 4.1033333333333335,
      "grad_norm": 0.049943435937166214,
      "learning_rate": 3.5926544240400665e-05,
      "loss": 0.2938,
      "step": 2462
    },
    {
      "epoch": 4.105,
      "grad_norm": 0.043600767850875854,
      "learning_rate": 3.5859766277128545e-05,
      "loss": 0.2742,
      "step": 2463
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.06156275421380997,
      "learning_rate": 3.579298831385643e-05,
      "loss": 0.32,
      "step": 2464
    },
    {
      "epoch": 4.108333333333333,
      "grad_norm": 0.043459076434373856,
      "learning_rate": 3.572621035058431e-05,
      "loss": 0.3081,
      "step": 2465
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.0659215897321701,
      "learning_rate": 3.5659432387312185e-05,
      "loss": 0.2414,
      "step": 2466
    },
    {
      "epoch": 4.111666666666666,
      "grad_norm": 0.06923731416463852,
      "learning_rate": 3.5592654424040065e-05,
      "loss": 0.3475,
      "step": 2467
    },
    {
      "epoch": 4.113333333333333,
      "grad_norm": 0.06262554228305817,
      "learning_rate": 3.5525876460767945e-05,
      "loss": 0.3665,
      "step": 2468
    },
    {
      "epoch": 4.115,
      "grad_norm": 0.0923684611916542,
      "learning_rate": 3.545909849749583e-05,
      "loss": 0.3838,
      "step": 2469
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 0.08122648298740387,
      "learning_rate": 3.5392320534223705e-05,
      "loss": 0.3123,
      "step": 2470
    },
    {
      "epoch": 4.118333333333333,
      "grad_norm": 0.03670548275113106,
      "learning_rate": 3.5325542570951585e-05,
      "loss": 0.2649,
      "step": 2471
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.06951224058866501,
      "learning_rate": 3.5258764607679465e-05,
      "loss": 0.3492,
      "step": 2472
    },
    {
      "epoch": 4.121666666666667,
      "grad_norm": 0.06802479177713394,
      "learning_rate": 3.519198664440735e-05,
      "loss": 0.3195,
      "step": 2473
    },
    {
      "epoch": 4.123333333333333,
      "grad_norm": 0.05160733684897423,
      "learning_rate": 3.5125208681135225e-05,
      "loss": 0.2816,
      "step": 2474
    },
    {
      "epoch": 4.125,
      "grad_norm": 0.06604316830635071,
      "learning_rate": 3.5058430717863105e-05,
      "loss": 0.2973,
      "step": 2475
    },
    {
      "epoch": 4.126666666666667,
      "grad_norm": 0.05636732652783394,
      "learning_rate": 3.4991652754590984e-05,
      "loss": 0.2769,
      "step": 2476
    },
    {
      "epoch": 4.128333333333333,
      "grad_norm": 0.039384905248880386,
      "learning_rate": 3.492487479131887e-05,
      "loss": 0.2757,
      "step": 2477
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.058618880808353424,
      "learning_rate": 3.4858096828046744e-05,
      "loss": 0.3281,
      "step": 2478
    },
    {
      "epoch": 4.131666666666667,
      "grad_norm": 0.04351762682199478,
      "learning_rate": 3.4791318864774624e-05,
      "loss": 0.3078,
      "step": 2479
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.040588896721601486,
      "learning_rate": 3.4724540901502504e-05,
      "loss": 0.3158,
      "step": 2480
    },
    {
      "epoch": 4.135,
      "grad_norm": 0.07188652455806732,
      "learning_rate": 3.4657762938230384e-05,
      "loss": 0.3644,
      "step": 2481
    },
    {
      "epoch": 4.136666666666667,
      "grad_norm": 0.06659849733114243,
      "learning_rate": 3.459098497495827e-05,
      "loss": 0.3659,
      "step": 2482
    },
    {
      "epoch": 4.138333333333334,
      "grad_norm": 0.05530848726630211,
      "learning_rate": 3.4524207011686144e-05,
      "loss": 0.3038,
      "step": 2483
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.05401857942342758,
      "learning_rate": 3.4457429048414024e-05,
      "loss": 0.3558,
      "step": 2484
    },
    {
      "epoch": 4.141666666666667,
      "grad_norm": 0.04150303825736046,
      "learning_rate": 3.4390651085141904e-05,
      "loss": 0.2847,
      "step": 2485
    },
    {
      "epoch": 4.1433333333333335,
      "grad_norm": 0.04948662593960762,
      "learning_rate": 3.432387312186979e-05,
      "loss": 0.3468,
      "step": 2486
    },
    {
      "epoch": 4.145,
      "grad_norm": 0.04566934332251549,
      "learning_rate": 3.4257095158597664e-05,
      "loss": 0.307,
      "step": 2487
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.04021117836236954,
      "learning_rate": 3.4190317195325544e-05,
      "loss": 0.2785,
      "step": 2488
    },
    {
      "epoch": 4.148333333333333,
      "grad_norm": 0.046001456677913666,
      "learning_rate": 3.4123539232053424e-05,
      "loss": 0.3083,
      "step": 2489
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.046577516943216324,
      "learning_rate": 3.4056761268781303e-05,
      "loss": 0.2908,
      "step": 2490
    },
    {
      "epoch": 4.151666666666666,
      "grad_norm": 0.04515524208545685,
      "learning_rate": 3.3989983305509183e-05,
      "loss": 0.2543,
      "step": 2491
    },
    {
      "epoch": 4.153333333333333,
      "grad_norm": 0.04183337837457657,
      "learning_rate": 3.392320534223706e-05,
      "loss": 0.2607,
      "step": 2492
    },
    {
      "epoch": 4.155,
      "grad_norm": 0.08218061178922653,
      "learning_rate": 3.385642737896494e-05,
      "loss": 0.28,
      "step": 2493
    },
    {
      "epoch": 4.156666666666666,
      "grad_norm": 0.053191591054201126,
      "learning_rate": 3.378964941569282e-05,
      "loss": 0.3253,
      "step": 2494
    },
    {
      "epoch": 4.158333333333333,
      "grad_norm": 0.04077981039881706,
      "learning_rate": 3.37228714524207e-05,
      "loss": 0.2613,
      "step": 2495
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.09897181391716003,
      "learning_rate": 3.365609348914858e-05,
      "loss": 0.3355,
      "step": 2496
    },
    {
      "epoch": 4.161666666666667,
      "grad_norm": 0.044291988015174866,
      "learning_rate": 3.358931552587646e-05,
      "loss": 0.2956,
      "step": 2497
    },
    {
      "epoch": 4.163333333333333,
      "grad_norm": 0.06392454355955124,
      "learning_rate": 3.352253756260434e-05,
      "loss": 0.3654,
      "step": 2498
    },
    {
      "epoch": 4.165,
      "grad_norm": 0.03228370472788811,
      "learning_rate": 3.345575959933222e-05,
      "loss": 0.2435,
      "step": 2499
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.04455592483282089,
      "learning_rate": 3.33889816360601e-05,
      "loss": 0.2601,
      "step": 2500
    },
    {
      "epoch": 4.168333333333333,
      "grad_norm": 0.04950655624270439,
      "learning_rate": 3.332220367278798e-05,
      "loss": 0.314,
      "step": 2501
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.0405222550034523,
      "learning_rate": 3.325542570951586e-05,
      "loss": 0.2413,
      "step": 2502
    },
    {
      "epoch": 4.171666666666667,
      "grad_norm": 0.06269162148237228,
      "learning_rate": 3.318864774624374e-05,
      "loss": 0.3804,
      "step": 2503
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.03359552472829819,
      "learning_rate": 3.312186978297162e-05,
      "loss": 0.2506,
      "step": 2504
    },
    {
      "epoch": 4.175,
      "grad_norm": 0.10824306309223175,
      "learning_rate": 3.30550918196995e-05,
      "loss": 0.3617,
      "step": 2505
    },
    {
      "epoch": 4.176666666666667,
      "grad_norm": 0.04502870887517929,
      "learning_rate": 3.298831385642738e-05,
      "loss": 0.3172,
      "step": 2506
    },
    {
      "epoch": 4.178333333333334,
      "grad_norm": 0.04950929805636406,
      "learning_rate": 3.2921535893155256e-05,
      "loss": 0.298,
      "step": 2507
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.03932636231184006,
      "learning_rate": 3.285475792988314e-05,
      "loss": 0.3001,
      "step": 2508
    },
    {
      "epoch": 4.181666666666667,
      "grad_norm": 0.08116426318883896,
      "learning_rate": 3.278797996661102e-05,
      "loss": 0.2883,
      "step": 2509
    },
    {
      "epoch": 4.183333333333334,
      "grad_norm": 0.05559760332107544,
      "learning_rate": 3.27212020033389e-05,
      "loss": 0.3014,
      "step": 2510
    },
    {
      "epoch": 4.185,
      "grad_norm": 0.05221611261367798,
      "learning_rate": 3.2654424040066775e-05,
      "loss": 0.3546,
      "step": 2511
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.08857633918523788,
      "learning_rate": 3.258764607679466e-05,
      "loss": 0.3268,
      "step": 2512
    },
    {
      "epoch": 4.1883333333333335,
      "grad_norm": 0.06152050569653511,
      "learning_rate": 3.252086811352254e-05,
      "loss": 0.2835,
      "step": 2513
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.06825856119394302,
      "learning_rate": 3.245409015025042e-05,
      "loss": 0.3505,
      "step": 2514
    },
    {
      "epoch": 4.191666666666666,
      "grad_norm": 0.05348846688866615,
      "learning_rate": 3.2387312186978295e-05,
      "loss": 0.3004,
      "step": 2515
    },
    {
      "epoch": 4.193333333333333,
      "grad_norm": 0.050401121377944946,
      "learning_rate": 3.2320534223706175e-05,
      "loss": 0.3002,
      "step": 2516
    },
    {
      "epoch": 4.195,
      "grad_norm": 0.07038283348083496,
      "learning_rate": 3.225375626043406e-05,
      "loss": 0.2968,
      "step": 2517
    },
    {
      "epoch": 4.196666666666666,
      "grad_norm": 0.047227032482624054,
      "learning_rate": 3.218697829716194e-05,
      "loss": 0.278,
      "step": 2518
    },
    {
      "epoch": 4.198333333333333,
      "grad_norm": 0.04669943451881409,
      "learning_rate": 3.2120200333889815e-05,
      "loss": 0.3213,
      "step": 2519
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.048380136489868164,
      "learning_rate": 3.2053422370617695e-05,
      "loss": 0.2863,
      "step": 2520
    },
    {
      "epoch": 4.201666666666666,
      "grad_norm": 0.07220529764890671,
      "learning_rate": 3.198664440734558e-05,
      "loss": 0.3221,
      "step": 2521
    },
    {
      "epoch": 4.203333333333333,
      "grad_norm": 0.11691394448280334,
      "learning_rate": 3.191986644407346e-05,
      "loss": 0.3506,
      "step": 2522
    },
    {
      "epoch": 4.205,
      "grad_norm": 0.04357609525322914,
      "learning_rate": 3.1853088480801334e-05,
      "loss": 0.2421,
      "step": 2523
    },
    {
      "epoch": 4.206666666666667,
      "grad_norm": 0.056057531386613846,
      "learning_rate": 3.1786310517529214e-05,
      "loss": 0.3825,
      "step": 2524
    },
    {
      "epoch": 4.208333333333333,
      "grad_norm": 0.06129267439246178,
      "learning_rate": 3.1719532554257094e-05,
      "loss": 0.3398,
      "step": 2525
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.05890761315822601,
      "learning_rate": 3.165275459098498e-05,
      "loss": 0.2722,
      "step": 2526
    },
    {
      "epoch": 4.211666666666667,
      "grad_norm": 0.03727583959698677,
      "learning_rate": 3.1585976627712854e-05,
      "loss": 0.2712,
      "step": 2527
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.03459892421960831,
      "learning_rate": 3.1519198664440734e-05,
      "loss": 0.2548,
      "step": 2528
    },
    {
      "epoch": 4.215,
      "grad_norm": 0.056371770799160004,
      "learning_rate": 3.1452420701168614e-05,
      "loss": 0.2859,
      "step": 2529
    },
    {
      "epoch": 4.216666666666667,
      "grad_norm": 0.058950748294591904,
      "learning_rate": 3.13856427378965e-05,
      "loss": 0.3093,
      "step": 2530
    },
    {
      "epoch": 4.218333333333334,
      "grad_norm": 0.032083675265312195,
      "learning_rate": 3.1318864774624374e-05,
      "loss": 0.2677,
      "step": 2531
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.04290849715471268,
      "learning_rate": 3.1252086811352254e-05,
      "loss": 0.2991,
      "step": 2532
    },
    {
      "epoch": 4.221666666666667,
      "grad_norm": 0.040342919528484344,
      "learning_rate": 3.1185308848080134e-05,
      "loss": 0.2998,
      "step": 2533
    },
    {
      "epoch": 4.223333333333334,
      "grad_norm": 0.05009816586971283,
      "learning_rate": 3.111853088480802e-05,
      "loss": 0.2211,
      "step": 2534
    },
    {
      "epoch": 4.225,
      "grad_norm": 0.04994930699467659,
      "learning_rate": 3.1051752921535894e-05,
      "loss": 0.3237,
      "step": 2535
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.04030104726552963,
      "learning_rate": 3.0984974958263773e-05,
      "loss": 0.2934,
      "step": 2536
    },
    {
      "epoch": 4.2283333333333335,
      "grad_norm": 0.029641736298799515,
      "learning_rate": 3.0918196994991653e-05,
      "loss": 0.2326,
      "step": 2537
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.04718010127544403,
      "learning_rate": 3.085141903171953e-05,
      "loss": 0.253,
      "step": 2538
    },
    {
      "epoch": 4.2316666666666665,
      "grad_norm": 0.04363427311182022,
      "learning_rate": 3.078464106844741e-05,
      "loss": 0.2687,
      "step": 2539
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 0.05486704036593437,
      "learning_rate": 3.071786310517529e-05,
      "loss": 0.3464,
      "step": 2540
    },
    {
      "epoch": 4.235,
      "grad_norm": 0.04572334885597229,
      "learning_rate": 3.065108514190317e-05,
      "loss": 0.308,
      "step": 2541
    },
    {
      "epoch": 4.236666666666666,
      "grad_norm": 0.052726034075021744,
      "learning_rate": 3.058430717863105e-05,
      "loss": 0.3095,
      "step": 2542
    },
    {
      "epoch": 4.238333333333333,
      "grad_norm": 0.05840439349412918,
      "learning_rate": 3.051752921535893e-05,
      "loss": 0.3408,
      "step": 2543
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.049482621252536774,
      "learning_rate": 3.0450751252086813e-05,
      "loss": 0.2736,
      "step": 2544
    },
    {
      "epoch": 4.241666666666666,
      "grad_norm": 0.0345662496984005,
      "learning_rate": 3.0383973288814693e-05,
      "loss": 0.237,
      "step": 2545
    },
    {
      "epoch": 4.243333333333333,
      "grad_norm": 0.054374996572732925,
      "learning_rate": 3.0317195325542573e-05,
      "loss": 0.3391,
      "step": 2546
    },
    {
      "epoch": 4.245,
      "grad_norm": 0.04829787462949753,
      "learning_rate": 3.025041736227045e-05,
      "loss": 0.2918,
      "step": 2547
    },
    {
      "epoch": 4.246666666666667,
      "grad_norm": 0.055972106754779816,
      "learning_rate": 3.0183639398998333e-05,
      "loss": 0.3269,
      "step": 2548
    },
    {
      "epoch": 4.248333333333333,
      "grad_norm": 0.0755239725112915,
      "learning_rate": 3.0116861435726213e-05,
      "loss": 0.3341,
      "step": 2549
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.11003386974334717,
      "learning_rate": 3.0050083472454093e-05,
      "loss": 0.3725,
      "step": 2550
    },
    {
      "epoch": 4.251666666666667,
      "grad_norm": 0.03766980767250061,
      "learning_rate": 2.998330550918197e-05,
      "loss": 0.261,
      "step": 2551
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.036990366876125336,
      "learning_rate": 2.991652754590985e-05,
      "loss": 0.2653,
      "step": 2552
    },
    {
      "epoch": 4.255,
      "grad_norm": 0.044007085263729095,
      "learning_rate": 2.9849749582637732e-05,
      "loss": 0.2821,
      "step": 2553
    },
    {
      "epoch": 4.256666666666667,
      "grad_norm": 0.07448176294565201,
      "learning_rate": 2.9782971619365612e-05,
      "loss": 0.3554,
      "step": 2554
    },
    {
      "epoch": 4.258333333333334,
      "grad_norm": 0.058756571263074875,
      "learning_rate": 2.971619365609349e-05,
      "loss": 0.3351,
      "step": 2555
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.05875828489661217,
      "learning_rate": 2.964941569282137e-05,
      "loss": 0.2846,
      "step": 2556
    },
    {
      "epoch": 4.261666666666667,
      "grad_norm": 0.056139543652534485,
      "learning_rate": 2.9582637729549252e-05,
      "loss": 0.3347,
      "step": 2557
    },
    {
      "epoch": 4.263333333333334,
      "grad_norm": 0.036534495651721954,
      "learning_rate": 2.9515859766277132e-05,
      "loss": 0.2492,
      "step": 2558
    },
    {
      "epoch": 4.265,
      "grad_norm": 0.06464437395334244,
      "learning_rate": 2.944908180300501e-05,
      "loss": 0.3189,
      "step": 2559
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.05223771557211876,
      "learning_rate": 2.938230383973289e-05,
      "loss": 0.3376,
      "step": 2560
    },
    {
      "epoch": 4.2683333333333335,
      "grad_norm": 0.06916167587041855,
      "learning_rate": 2.931552587646077e-05,
      "loss": 0.3463,
      "step": 2561
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.04995094612240791,
      "learning_rate": 2.924874791318865e-05,
      "loss": 0.3352,
      "step": 2562
    },
    {
      "epoch": 4.2716666666666665,
      "grad_norm": 0.04135150834918022,
      "learning_rate": 2.9181969949916528e-05,
      "loss": 0.3004,
      "step": 2563
    },
    {
      "epoch": 4.273333333333333,
      "grad_norm": 0.04171762242913246,
      "learning_rate": 2.9115191986644408e-05,
      "loss": 0.2763,
      "step": 2564
    },
    {
      "epoch": 4.275,
      "grad_norm": 0.05470849201083183,
      "learning_rate": 2.9048414023372288e-05,
      "loss": 0.3831,
      "step": 2565
    },
    {
      "epoch": 4.276666666666666,
      "grad_norm": 0.057729750871658325,
      "learning_rate": 2.898163606010017e-05,
      "loss": 0.3111,
      "step": 2566
    },
    {
      "epoch": 4.278333333333333,
      "grad_norm": 0.10040348023176193,
      "learning_rate": 2.8914858096828045e-05,
      "loss": 0.3807,
      "step": 2567
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.04623483121395111,
      "learning_rate": 2.8848080133555928e-05,
      "loss": 0.2843,
      "step": 2568
    },
    {
      "epoch": 4.281666666666666,
      "grad_norm": 0.04580945149064064,
      "learning_rate": 2.8781302170283808e-05,
      "loss": 0.309,
      "step": 2569
    },
    {
      "epoch": 4.283333333333333,
      "grad_norm": 0.06345660984516144,
      "learning_rate": 2.8714524207011688e-05,
      "loss": 0.3283,
      "step": 2570
    },
    {
      "epoch": 4.285,
      "grad_norm": 0.040647462010383606,
      "learning_rate": 2.8647746243739564e-05,
      "loss": 0.256,
      "step": 2571
    },
    {
      "epoch": 4.286666666666667,
      "grad_norm": 0.06961789727210999,
      "learning_rate": 2.8580968280467448e-05,
      "loss": 0.3322,
      "step": 2572
    },
    {
      "epoch": 4.288333333333333,
      "grad_norm": 0.04622265696525574,
      "learning_rate": 2.8514190317195328e-05,
      "loss": 0.3254,
      "step": 2573
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.04448201134800911,
      "learning_rate": 2.8447412353923207e-05,
      "loss": 0.2446,
      "step": 2574
    },
    {
      "epoch": 4.291666666666667,
      "grad_norm": 0.06108305975794792,
      "learning_rate": 2.8380634390651084e-05,
      "loss": 0.256,
      "step": 2575
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.051152411848306656,
      "learning_rate": 2.8313856427378964e-05,
      "loss": 0.309,
      "step": 2576
    },
    {
      "epoch": 4.295,
      "grad_norm": 0.035635076463222504,
      "learning_rate": 2.8247078464106847e-05,
      "loss": 0.2565,
      "step": 2577
    },
    {
      "epoch": 4.296666666666667,
      "grad_norm": 0.044845473021268845,
      "learning_rate": 2.8180300500834727e-05,
      "loss": 0.2629,
      "step": 2578
    },
    {
      "epoch": 4.298333333333334,
      "grad_norm": 0.10333165526390076,
      "learning_rate": 2.8113522537562604e-05,
      "loss": 0.3667,
      "step": 2579
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.06825540214776993,
      "learning_rate": 2.8046744574290484e-05,
      "loss": 0.265,
      "step": 2580
    },
    {
      "epoch": 4.301666666666667,
      "grad_norm": 0.047053221613168716,
      "learning_rate": 2.7979966611018367e-05,
      "loss": 0.2675,
      "step": 2581
    },
    {
      "epoch": 4.303333333333334,
      "grad_norm": 0.0717955157160759,
      "learning_rate": 2.7913188647746247e-05,
      "loss": 0.3239,
      "step": 2582
    },
    {
      "epoch": 4.305,
      "grad_norm": 0.07852278649806976,
      "learning_rate": 2.7846410684474123e-05,
      "loss": 0.3077,
      "step": 2583
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.061252642422914505,
      "learning_rate": 2.7779632721202003e-05,
      "loss": 0.3207,
      "step": 2584
    },
    {
      "epoch": 4.308333333333334,
      "grad_norm": 0.09566570818424225,
      "learning_rate": 2.7712854757929883e-05,
      "loss": 0.2995,
      "step": 2585
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.06018539145588875,
      "learning_rate": 2.7646076794657767e-05,
      "loss": 0.3529,
      "step": 2586
    },
    {
      "epoch": 4.3116666666666665,
      "grad_norm": 0.053931426256895065,
      "learning_rate": 2.7579298831385643e-05,
      "loss": 0.3193,
      "step": 2587
    },
    {
      "epoch": 4.3133333333333335,
      "grad_norm": 0.08737054467201233,
      "learning_rate": 2.7512520868113523e-05,
      "loss": 0.25,
      "step": 2588
    },
    {
      "epoch": 4.315,
      "grad_norm": 0.034999243915081024,
      "learning_rate": 2.7445742904841403e-05,
      "loss": 0.2329,
      "step": 2589
    },
    {
      "epoch": 4.316666666666666,
      "grad_norm": 0.0465102419257164,
      "learning_rate": 2.7378964941569286e-05,
      "loss": 0.3176,
      "step": 2590
    },
    {
      "epoch": 4.318333333333333,
      "grad_norm": 0.10191872715950012,
      "learning_rate": 2.731218697829716e-05,
      "loss": 0.3133,
      "step": 2591
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.05729416757822037,
      "learning_rate": 2.7245409015025043e-05,
      "loss": 0.3373,
      "step": 2592
    },
    {
      "epoch": 4.321666666666666,
      "grad_norm": 0.04809652268886566,
      "learning_rate": 2.7178631051752923e-05,
      "loss": 0.2438,
      "step": 2593
    },
    {
      "epoch": 4.323333333333333,
      "grad_norm": 0.0818868950009346,
      "learning_rate": 2.7111853088480803e-05,
      "loss": 0.3382,
      "step": 2594
    },
    {
      "epoch": 4.325,
      "grad_norm": 0.07674728333950043,
      "learning_rate": 2.704507512520868e-05,
      "loss": 0.3383,
      "step": 2595
    },
    {
      "epoch": 4.326666666666666,
      "grad_norm": 0.061916809529066086,
      "learning_rate": 2.6978297161936563e-05,
      "loss": 0.3208,
      "step": 2596
    },
    {
      "epoch": 4.328333333333333,
      "grad_norm": 0.04776381701231003,
      "learning_rate": 2.6911519198664442e-05,
      "loss": 0.2879,
      "step": 2597
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.05471456050872803,
      "learning_rate": 2.6844741235392322e-05,
      "loss": 0.3292,
      "step": 2598
    },
    {
      "epoch": 4.331666666666667,
      "grad_norm": 0.03634297475218773,
      "learning_rate": 2.67779632721202e-05,
      "loss": 0.2094,
      "step": 2599
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.0820794627070427,
      "learning_rate": 2.671118530884808e-05,
      "loss": 0.3573,
      "step": 2600
    },
    {
      "epoch": 4.335,
      "grad_norm": 0.03548863157629967,
      "learning_rate": 2.6644407345575962e-05,
      "loss": 0.2406,
      "step": 2601
    },
    {
      "epoch": 4.336666666666667,
      "grad_norm": 0.046100616455078125,
      "learning_rate": 2.6577629382303842e-05,
      "loss": 0.3044,
      "step": 2602
    },
    {
      "epoch": 4.338333333333333,
      "grad_norm": 0.11928866803646088,
      "learning_rate": 2.651085141903172e-05,
      "loss": 0.2822,
      "step": 2603
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.05632886290550232,
      "learning_rate": 2.64440734557596e-05,
      "loss": 0.3482,
      "step": 2604
    },
    {
      "epoch": 4.341666666666667,
      "grad_norm": 0.06943139433860779,
      "learning_rate": 2.6377295492487482e-05,
      "loss": 0.3089,
      "step": 2605
    },
    {
      "epoch": 4.343333333333334,
      "grad_norm": 0.05126681178808212,
      "learning_rate": 2.6310517529215362e-05,
      "loss": 0.233,
      "step": 2606
    },
    {
      "epoch": 4.345,
      "grad_norm": 0.05285421758890152,
      "learning_rate": 2.624373956594324e-05,
      "loss": 0.3391,
      "step": 2607
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.09051513671875,
      "learning_rate": 2.6176961602671118e-05,
      "loss": 0.3125,
      "step": 2608
    },
    {
      "epoch": 4.348333333333334,
      "grad_norm": 0.04185972735285759,
      "learning_rate": 2.6110183639398998e-05,
      "loss": 0.2471,
      "step": 2609
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.034619808197021484,
      "learning_rate": 2.604340567612688e-05,
      "loss": 0.2605,
      "step": 2610
    },
    {
      "epoch": 4.351666666666667,
      "grad_norm": 0.05973046272993088,
      "learning_rate": 2.597662771285476e-05,
      "loss": 0.3445,
      "step": 2611
    },
    {
      "epoch": 4.3533333333333335,
      "grad_norm": 0.07242212444543839,
      "learning_rate": 2.5909849749582638e-05,
      "loss": 0.3581,
      "step": 2612
    },
    {
      "epoch": 4.355,
      "grad_norm": 0.035689979791641235,
      "learning_rate": 2.5843071786310518e-05,
      "loss": 0.2091,
      "step": 2613
    },
    {
      "epoch": 4.3566666666666665,
      "grad_norm": 0.04199907183647156,
      "learning_rate": 2.57762938230384e-05,
      "loss": 0.248,
      "step": 2614
    },
    {
      "epoch": 4.358333333333333,
      "grad_norm": 0.07896091789007187,
      "learning_rate": 2.570951585976628e-05,
      "loss": 0.3635,
      "step": 2615
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.0489177480340004,
      "learning_rate": 2.5642737896494158e-05,
      "loss": 0.2333,
      "step": 2616
    },
    {
      "epoch": 4.361666666666666,
      "grad_norm": 0.06277226656675339,
      "learning_rate": 2.5575959933222038e-05,
      "loss": 0.3429,
      "step": 2617
    },
    {
      "epoch": 4.363333333333333,
      "grad_norm": 0.0376591831445694,
      "learning_rate": 2.5509181969949918e-05,
      "loss": 0.2578,
      "step": 2618
    },
    {
      "epoch": 4.365,
      "grad_norm": 0.0431097038090229,
      "learning_rate": 2.54424040066778e-05,
      "loss": 0.2657,
      "step": 2619
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 0.03855057433247566,
      "learning_rate": 2.5375626043405677e-05,
      "loss": 0.2282,
      "step": 2620
    },
    {
      "epoch": 4.368333333333333,
      "grad_norm": 0.0398845449090004,
      "learning_rate": 2.5308848080133557e-05,
      "loss": 0.3082,
      "step": 2621
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.08220727741718292,
      "learning_rate": 2.5242070116861437e-05,
      "loss": 0.3128,
      "step": 2622
    },
    {
      "epoch": 4.371666666666667,
      "grad_norm": 0.04736354202032089,
      "learning_rate": 2.517529215358932e-05,
      "loss": 0.3281,
      "step": 2623
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.11251157522201538,
      "learning_rate": 2.5108514190317194e-05,
      "loss": 0.2451,
      "step": 2624
    },
    {
      "epoch": 4.375,
      "grad_norm": 0.047582417726516724,
      "learning_rate": 2.5041736227045077e-05,
      "loss": 0.2924,
      "step": 2625
    },
    {
      "epoch": 4.376666666666667,
      "grad_norm": 0.034507930278778076,
      "learning_rate": 2.4974958263772957e-05,
      "loss": 0.2395,
      "step": 2626
    },
    {
      "epoch": 4.378333333333333,
      "grad_norm": 0.06373557448387146,
      "learning_rate": 2.4908180300500837e-05,
      "loss": 0.3615,
      "step": 2627
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.06386108696460724,
      "learning_rate": 2.4841402337228717e-05,
      "loss": 0.3205,
      "step": 2628
    },
    {
      "epoch": 4.381666666666667,
      "grad_norm": 0.03841439634561539,
      "learning_rate": 2.4774624373956597e-05,
      "loss": 0.2607,
      "step": 2629
    },
    {
      "epoch": 4.383333333333334,
      "grad_norm": 0.0467105507850647,
      "learning_rate": 2.4707846410684477e-05,
      "loss": 0.3038,
      "step": 2630
    },
    {
      "epoch": 4.385,
      "grad_norm": 0.03938751295208931,
      "learning_rate": 2.4641068447412353e-05,
      "loss": 0.2572,
      "step": 2631
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.09664253145456314,
      "learning_rate": 2.4574290484140237e-05,
      "loss": 0.2961,
      "step": 2632
    },
    {
      "epoch": 4.388333333333334,
      "grad_norm": 0.047580916434526443,
      "learning_rate": 2.4507512520868113e-05,
      "loss": 0.3099,
      "step": 2633
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.04526331648230553,
      "learning_rate": 2.4440734557595996e-05,
      "loss": 0.2647,
      "step": 2634
    },
    {
      "epoch": 4.391666666666667,
      "grad_norm": 0.054259952157735825,
      "learning_rate": 2.4373956594323873e-05,
      "loss": 0.286,
      "step": 2635
    },
    {
      "epoch": 4.3933333333333335,
      "grad_norm": 0.057123925536870956,
      "learning_rate": 2.4307178631051756e-05,
      "loss": 0.3538,
      "step": 2636
    },
    {
      "epoch": 4.395,
      "grad_norm": 0.04680921137332916,
      "learning_rate": 2.4240400667779633e-05,
      "loss": 0.3327,
      "step": 2637
    },
    {
      "epoch": 4.3966666666666665,
      "grad_norm": 0.055616408586502075,
      "learning_rate": 2.4173622704507516e-05,
      "loss": 0.3147,
      "step": 2638
    },
    {
      "epoch": 4.398333333333333,
      "grad_norm": 0.04548594728112221,
      "learning_rate": 2.4106844741235393e-05,
      "loss": 0.2396,
      "step": 2639
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.0563538484275341,
      "learning_rate": 2.4040066777963273e-05,
      "loss": 0.3458,
      "step": 2640
    },
    {
      "epoch": 4.401666666666666,
      "grad_norm": 0.050818707793951035,
      "learning_rate": 2.3973288814691153e-05,
      "loss": 0.2287,
      "step": 2641
    },
    {
      "epoch": 4.403333333333333,
      "grad_norm": 0.050778571516275406,
      "learning_rate": 2.3906510851419033e-05,
      "loss": 0.3335,
      "step": 2642
    },
    {
      "epoch": 4.405,
      "grad_norm": 0.07384174317121506,
      "learning_rate": 2.3839732888146912e-05,
      "loss": 0.3964,
      "step": 2643
    },
    {
      "epoch": 4.406666666666666,
      "grad_norm": 0.06684073060750961,
      "learning_rate": 2.3772954924874792e-05,
      "loss": 0.2508,
      "step": 2644
    },
    {
      "epoch": 4.408333333333333,
      "grad_norm": 0.06501787900924683,
      "learning_rate": 2.3706176961602672e-05,
      "loss": 0.3554,
      "step": 2645
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.09221694618463516,
      "learning_rate": 2.3639398998330552e-05,
      "loss": 0.3278,
      "step": 2646
    },
    {
      "epoch": 4.411666666666667,
      "grad_norm": 0.04598364606499672,
      "learning_rate": 2.3572621035058432e-05,
      "loss": 0.2481,
      "step": 2647
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.050829555839300156,
      "learning_rate": 2.3505843071786312e-05,
      "loss": 0.2653,
      "step": 2648
    },
    {
      "epoch": 4.415,
      "grad_norm": 0.052021849900484085,
      "learning_rate": 2.3439065108514192e-05,
      "loss": 0.2981,
      "step": 2649
    },
    {
      "epoch": 4.416666666666667,
      "grad_norm": 0.13042674958705902,
      "learning_rate": 2.3372287145242072e-05,
      "loss": 0.3562,
      "step": 2650
    },
    {
      "epoch": 4.418333333333333,
      "grad_norm": 0.07181274890899658,
      "learning_rate": 2.3305509181969952e-05,
      "loss": 0.2673,
      "step": 2651
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.05280003324151039,
      "learning_rate": 2.3238731218697832e-05,
      "loss": 0.2594,
      "step": 2652
    },
    {
      "epoch": 4.421666666666667,
      "grad_norm": 0.11961933970451355,
      "learning_rate": 2.3171953255425712e-05,
      "loss": 0.2686,
      "step": 2653
    },
    {
      "epoch": 4.423333333333334,
      "grad_norm": 0.039567701518535614,
      "learning_rate": 2.310517529215359e-05,
      "loss": 0.317,
      "step": 2654
    },
    {
      "epoch": 4.425,
      "grad_norm": 0.04887618497014046,
      "learning_rate": 2.3038397328881468e-05,
      "loss": 0.3021,
      "step": 2655
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.06502016633749008,
      "learning_rate": 2.297161936560935e-05,
      "loss": 0.3663,
      "step": 2656
    },
    {
      "epoch": 4.428333333333334,
      "grad_norm": 0.07130604237318039,
      "learning_rate": 2.2904841402337228e-05,
      "loss": 0.3083,
      "step": 2657
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.035889267921447754,
      "learning_rate": 2.283806343906511e-05,
      "loss": 0.25,
      "step": 2658
    },
    {
      "epoch": 4.431666666666667,
      "grad_norm": 0.06648524105548859,
      "learning_rate": 2.2771285475792988e-05,
      "loss": 0.2885,
      "step": 2659
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 0.04535192251205444,
      "learning_rate": 2.270450751252087e-05,
      "loss": 0.2492,
      "step": 2660
    },
    {
      "epoch": 4.435,
      "grad_norm": 0.048863232135772705,
      "learning_rate": 2.2637729549248748e-05,
      "loss": 0.211,
      "step": 2661
    },
    {
      "epoch": 4.4366666666666665,
      "grad_norm": 0.06364914774894714,
      "learning_rate": 2.257095158597663e-05,
      "loss": 0.305,
      "step": 2662
    },
    {
      "epoch": 4.4383333333333335,
      "grad_norm": 0.049301810562610626,
      "learning_rate": 2.2504173622704508e-05,
      "loss": 0.3213,
      "step": 2663
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.13312943279743195,
      "learning_rate": 2.2437395659432388e-05,
      "loss": 0.3426,
      "step": 2664
    },
    {
      "epoch": 4.441666666666666,
      "grad_norm": 0.035659223794937134,
      "learning_rate": 2.2370617696160268e-05,
      "loss": 0.2514,
      "step": 2665
    },
    {
      "epoch": 4.443333333333333,
      "grad_norm": 0.0734514519572258,
      "learning_rate": 2.2303839732888147e-05,
      "loss": 0.3353,
      "step": 2666
    },
    {
      "epoch": 4.445,
      "grad_norm": 0.03861021250486374,
      "learning_rate": 2.2237061769616027e-05,
      "loss": 0.2664,
      "step": 2667
    },
    {
      "epoch": 4.446666666666666,
      "grad_norm": 0.04275742545723915,
      "learning_rate": 2.2170283806343907e-05,
      "loss": 0.2699,
      "step": 2668
    },
    {
      "epoch": 4.448333333333333,
      "grad_norm": 0.041196417063474655,
      "learning_rate": 2.2103505843071787e-05,
      "loss": 0.2869,
      "step": 2669
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.17665788531303406,
      "learning_rate": 2.2036727879799667e-05,
      "loss": 0.3336,
      "step": 2670
    },
    {
      "epoch": 4.451666666666666,
      "grad_norm": 0.0818871557712555,
      "learning_rate": 2.1969949916527547e-05,
      "loss": 0.3091,
      "step": 2671
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.04896293580532074,
      "learning_rate": 2.1903171953255427e-05,
      "loss": 0.2477,
      "step": 2672
    },
    {
      "epoch": 4.455,
      "grad_norm": 0.0883425772190094,
      "learning_rate": 2.1836393989983307e-05,
      "loss": 0.3506,
      "step": 2673
    },
    {
      "epoch": 4.456666666666667,
      "grad_norm": 0.03321492299437523,
      "learning_rate": 2.1769616026711187e-05,
      "loss": 0.2407,
      "step": 2674
    },
    {
      "epoch": 4.458333333333333,
      "grad_norm": 0.0838158056139946,
      "learning_rate": 2.1702838063439067e-05,
      "loss": 0.3211,
      "step": 2675
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.03500605374574661,
      "learning_rate": 2.1636060100166947e-05,
      "loss": 0.2455,
      "step": 2676
    },
    {
      "epoch": 4.461666666666667,
      "grad_norm": 0.038702014833688736,
      "learning_rate": 2.1569282136894827e-05,
      "loss": 0.297,
      "step": 2677
    },
    {
      "epoch": 4.463333333333333,
      "grad_norm": 0.04499758780002594,
      "learning_rate": 2.1502504173622707e-05,
      "loss": 0.273,
      "step": 2678
    },
    {
      "epoch": 4.465,
      "grad_norm": 0.03910122066736221,
      "learning_rate": 2.1435726210350583e-05,
      "loss": 0.2897,
      "step": 2679
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.04476914182305336,
      "learning_rate": 2.1368948247078466e-05,
      "loss": 0.3105,
      "step": 2680
    },
    {
      "epoch": 4.468333333333334,
      "grad_norm": 0.04880363866686821,
      "learning_rate": 2.1302170283806343e-05,
      "loss": 0.3417,
      "step": 2681
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.030641430988907814,
      "learning_rate": 2.1235392320534226e-05,
      "loss": 0.2263,
      "step": 2682
    },
    {
      "epoch": 4.471666666666667,
      "grad_norm": 0.0486392043530941,
      "learning_rate": 2.1168614357262103e-05,
      "loss": 0.2765,
      "step": 2683
    },
    {
      "epoch": 4.473333333333334,
      "grad_norm": 0.04759145528078079,
      "learning_rate": 2.1101836393989986e-05,
      "loss": 0.3258,
      "step": 2684
    },
    {
      "epoch": 4.475,
      "grad_norm": 0.07489033788442612,
      "learning_rate": 2.1035058430717863e-05,
      "loss": 0.3453,
      "step": 2685
    },
    {
      "epoch": 4.476666666666667,
      "grad_norm": 0.055716898292303085,
      "learning_rate": 2.0968280467445746e-05,
      "loss": 0.3247,
      "step": 2686
    },
    {
      "epoch": 4.4783333333333335,
      "grad_norm": 0.07196290791034698,
      "learning_rate": 2.0901502504173623e-05,
      "loss": 0.3636,
      "step": 2687
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.03732656314969063,
      "learning_rate": 2.0834724540901503e-05,
      "loss": 0.2841,
      "step": 2688
    },
    {
      "epoch": 4.4816666666666665,
      "grad_norm": 0.036479637026786804,
      "learning_rate": 2.0767946577629382e-05,
      "loss": 0.224,
      "step": 2689
    },
    {
      "epoch": 4.483333333333333,
      "grad_norm": 0.036050960421562195,
      "learning_rate": 2.0701168614357262e-05,
      "loss": 0.2331,
      "step": 2690
    },
    {
      "epoch": 4.485,
      "grad_norm": 0.04583069309592247,
      "learning_rate": 2.0634390651085142e-05,
      "loss": 0.2843,
      "step": 2691
    },
    {
      "epoch": 4.486666666666666,
      "grad_norm": 0.03804969787597656,
      "learning_rate": 2.0567612687813022e-05,
      "loss": 0.2597,
      "step": 2692
    },
    {
      "epoch": 4.488333333333333,
      "grad_norm": 0.04616210237145424,
      "learning_rate": 2.0500834724540902e-05,
      "loss": 0.2828,
      "step": 2693
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.03939870372414589,
      "learning_rate": 2.0434056761268782e-05,
      "loss": 0.2638,
      "step": 2694
    },
    {
      "epoch": 4.491666666666666,
      "grad_norm": 0.037784673273563385,
      "learning_rate": 2.0367278797996662e-05,
      "loss": 0.2561,
      "step": 2695
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.05030560865998268,
      "learning_rate": 2.0300500834724542e-05,
      "loss": 0.2683,
      "step": 2696
    },
    {
      "epoch": 4.495,
      "grad_norm": 0.05075063556432724,
      "learning_rate": 2.0233722871452422e-05,
      "loss": 0.2967,
      "step": 2697
    },
    {
      "epoch": 4.496666666666667,
      "grad_norm": 0.0435015968978405,
      "learning_rate": 2.0166944908180302e-05,
      "loss": 0.2749,
      "step": 2698
    },
    {
      "epoch": 4.498333333333333,
      "grad_norm": 0.059844981878995895,
      "learning_rate": 2.0100166944908182e-05,
      "loss": 0.3614,
      "step": 2699
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.07699571549892426,
      "learning_rate": 2.003338898163606e-05,
      "loss": 0.3992,
      "step": 2700
    },
    {
      "epoch": 4.501666666666667,
      "grad_norm": 0.0554332472383976,
      "learning_rate": 1.9966611018363938e-05,
      "loss": 0.3187,
      "step": 2701
    },
    {
      "epoch": 4.503333333333333,
      "grad_norm": 0.058731649070978165,
      "learning_rate": 1.989983305509182e-05,
      "loss": 0.3252,
      "step": 2702
    },
    {
      "epoch": 4.505,
      "grad_norm": 0.04309955611824989,
      "learning_rate": 1.9833055091819698e-05,
      "loss": 0.2469,
      "step": 2703
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.07157505303621292,
      "learning_rate": 1.976627712854758e-05,
      "loss": 0.3591,
      "step": 2704
    },
    {
      "epoch": 4.508333333333333,
      "grad_norm": 0.03910694271326065,
      "learning_rate": 1.9699499165275458e-05,
      "loss": 0.2851,
      "step": 2705
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.10019135475158691,
      "learning_rate": 1.963272120200334e-05,
      "loss": 0.3295,
      "step": 2706
    },
    {
      "epoch": 4.511666666666667,
      "grad_norm": 0.10680773109197617,
      "learning_rate": 1.9565943238731218e-05,
      "loss": 0.3805,
      "step": 2707
    },
    {
      "epoch": 4.513333333333334,
      "grad_norm": 0.048560041934251785,
      "learning_rate": 1.94991652754591e-05,
      "loss": 0.3013,
      "step": 2708
    },
    {
      "epoch": 4.515,
      "grad_norm": 0.05781670659780502,
      "learning_rate": 1.9432387312186978e-05,
      "loss": 0.3272,
      "step": 2709
    },
    {
      "epoch": 4.516666666666667,
      "grad_norm": 0.05313660204410553,
      "learning_rate": 1.936560934891486e-05,
      "loss": 0.3088,
      "step": 2710
    },
    {
      "epoch": 4.5183333333333335,
      "grad_norm": 0.11948142200708389,
      "learning_rate": 1.9298831385642738e-05,
      "loss": 0.2928,
      "step": 2711
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.0639624297618866,
      "learning_rate": 1.9232053422370617e-05,
      "loss": 0.3321,
      "step": 2712
    },
    {
      "epoch": 4.5216666666666665,
      "grad_norm": 0.05885301157832146,
      "learning_rate": 1.9165275459098497e-05,
      "loss": 0.3136,
      "step": 2713
    },
    {
      "epoch": 4.523333333333333,
      "grad_norm": 0.07494687288999557,
      "learning_rate": 1.9098497495826377e-05,
      "loss": 0.2997,
      "step": 2714
    },
    {
      "epoch": 4.525,
      "grad_norm": 0.052646078169345856,
      "learning_rate": 1.9031719532554257e-05,
      "loss": 0.2513,
      "step": 2715
    },
    {
      "epoch": 4.526666666666666,
      "grad_norm": 0.050765231251716614,
      "learning_rate": 1.8964941569282137e-05,
      "loss": 0.2902,
      "step": 2716
    },
    {
      "epoch": 4.528333333333333,
      "grad_norm": 0.052270568907260895,
      "learning_rate": 1.8898163606010017e-05,
      "loss": 0.3151,
      "step": 2717
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.048800960183143616,
      "learning_rate": 1.8831385642737897e-05,
      "loss": 0.3334,
      "step": 2718
    },
    {
      "epoch": 4.531666666666666,
      "grad_norm": 0.05171078070998192,
      "learning_rate": 1.8764607679465777e-05,
      "loss": 0.2694,
      "step": 2719
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.047119174152612686,
      "learning_rate": 1.8697829716193657e-05,
      "loss": 0.3134,
      "step": 2720
    },
    {
      "epoch": 4.535,
      "grad_norm": 0.03846320882439613,
      "learning_rate": 1.8631051752921537e-05,
      "loss": 0.2266,
      "step": 2721
    },
    {
      "epoch": 4.536666666666667,
      "grad_norm": 0.08530262857675552,
      "learning_rate": 1.8564273789649417e-05,
      "loss": 0.394,
      "step": 2722
    },
    {
      "epoch": 4.538333333333333,
      "grad_norm": 0.049323175102472305,
      "learning_rate": 1.8497495826377297e-05,
      "loss": 0.2667,
      "step": 2723
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.05037526786327362,
      "learning_rate": 1.8430717863105177e-05,
      "loss": 0.3264,
      "step": 2724
    },
    {
      "epoch": 4.541666666666667,
      "grad_norm": 0.04779855161905289,
      "learning_rate": 1.8363939899833053e-05,
      "loss": 0.2882,
      "step": 2725
    },
    {
      "epoch": 4.543333333333333,
      "grad_norm": 0.04460882022976875,
      "learning_rate": 1.8297161936560936e-05,
      "loss": 0.2727,
      "step": 2726
    },
    {
      "epoch": 4.545,
      "grad_norm": 0.05443637818098068,
      "learning_rate": 1.8230383973288813e-05,
      "loss": 0.2869,
      "step": 2727
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.05456630140542984,
      "learning_rate": 1.8163606010016696e-05,
      "loss": 0.3177,
      "step": 2728
    },
    {
      "epoch": 4.548333333333334,
      "grad_norm": 0.040951211005449295,
      "learning_rate": 1.8096828046744573e-05,
      "loss": 0.2416,
      "step": 2729
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.06912383437156677,
      "learning_rate": 1.8030050083472456e-05,
      "loss": 0.296,
      "step": 2730
    },
    {
      "epoch": 4.551666666666667,
      "grad_norm": 0.06618314981460571,
      "learning_rate": 1.7963272120200333e-05,
      "loss": 0.3265,
      "step": 2731
    },
    {
      "epoch": 4.553333333333334,
      "grad_norm": 0.04689953476190567,
      "learning_rate": 1.7896494156928216e-05,
      "loss": 0.3087,
      "step": 2732
    },
    {
      "epoch": 4.555,
      "grad_norm": 0.05263284966349602,
      "learning_rate": 1.7829716193656093e-05,
      "loss": 0.2994,
      "step": 2733
    },
    {
      "epoch": 4.556666666666667,
      "grad_norm": 0.043119121342897415,
      "learning_rate": 1.7762938230383973e-05,
      "loss": 0.297,
      "step": 2734
    },
    {
      "epoch": 4.558333333333334,
      "grad_norm": 0.05125643312931061,
      "learning_rate": 1.7696160267111852e-05,
      "loss": 0.3093,
      "step": 2735
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.11482519656419754,
      "learning_rate": 1.7629382303839732e-05,
      "loss": 0.3511,
      "step": 2736
    },
    {
      "epoch": 4.5616666666666665,
      "grad_norm": 0.04130219668149948,
      "learning_rate": 1.7562604340567612e-05,
      "loss": 0.2629,
      "step": 2737
    },
    {
      "epoch": 4.5633333333333335,
      "grad_norm": 0.03760569542646408,
      "learning_rate": 1.7495826377295492e-05,
      "loss": 0.2838,
      "step": 2738
    },
    {
      "epoch": 4.5649999999999995,
      "grad_norm": 0.03653554618358612,
      "learning_rate": 1.7429048414023372e-05,
      "loss": 0.2259,
      "step": 2739
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 0.07028178870677948,
      "learning_rate": 1.7362270450751252e-05,
      "loss": 0.3109,
      "step": 2740
    },
    {
      "epoch": 4.568333333333333,
      "grad_norm": 0.0373249426484108,
      "learning_rate": 1.7295492487479135e-05,
      "loss": 0.2407,
      "step": 2741
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.06620839238166809,
      "learning_rate": 1.7228714524207012e-05,
      "loss": 0.2887,
      "step": 2742
    },
    {
      "epoch": 4.571666666666666,
      "grad_norm": 0.03607608750462532,
      "learning_rate": 1.7161936560934895e-05,
      "loss": 0.2294,
      "step": 2743
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.059523679316043854,
      "learning_rate": 1.7095158597662772e-05,
      "loss": 0.3446,
      "step": 2744
    },
    {
      "epoch": 4.575,
      "grad_norm": 0.04938521608710289,
      "learning_rate": 1.7028380634390652e-05,
      "loss": 0.321,
      "step": 2745
    },
    {
      "epoch": 4.576666666666666,
      "grad_norm": 0.06042911857366562,
      "learning_rate": 1.696160267111853e-05,
      "loss": 0.2065,
      "step": 2746
    },
    {
      "epoch": 4.578333333333333,
      "grad_norm": 0.06672200560569763,
      "learning_rate": 1.689482470784641e-05,
      "loss": 0.2927,
      "step": 2747
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.06009630486369133,
      "learning_rate": 1.682804674457429e-05,
      "loss": 0.285,
      "step": 2748
    },
    {
      "epoch": 4.581666666666667,
      "grad_norm": 0.058195870369672775,
      "learning_rate": 1.676126878130217e-05,
      "loss": 0.3133,
      "step": 2749
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 0.18828050792217255,
      "learning_rate": 1.669449081803005e-05,
      "loss": 0.3054,
      "step": 2750
    },
    {
      "epoch": 4.585,
      "grad_norm": 0.060826774686574936,
      "learning_rate": 1.662771285475793e-05,
      "loss": 0.315,
      "step": 2751
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.06767841428518295,
      "learning_rate": 1.656093489148581e-05,
      "loss": 0.3073,
      "step": 2752
    },
    {
      "epoch": 4.588333333333333,
      "grad_norm": 0.04216107353568077,
      "learning_rate": 1.649415692821369e-05,
      "loss": 0.2704,
      "step": 2753
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.06547300517559052,
      "learning_rate": 1.642737896494157e-05,
      "loss": 0.307,
      "step": 2754
    },
    {
      "epoch": 4.591666666666667,
      "grad_norm": 0.0839301347732544,
      "learning_rate": 1.636060100166945e-05,
      "loss": 0.3056,
      "step": 2755
    },
    {
      "epoch": 4.593333333333334,
      "grad_norm": 0.03603212907910347,
      "learning_rate": 1.629382303839733e-05,
      "loss": 0.2561,
      "step": 2756
    },
    {
      "epoch": 4.595,
      "grad_norm": 0.046933937817811966,
      "learning_rate": 1.622704507512521e-05,
      "loss": 0.3115,
      "step": 2757
    },
    {
      "epoch": 4.596666666666667,
      "grad_norm": 0.04107724875211716,
      "learning_rate": 1.6160267111853087e-05,
      "loss": 0.2809,
      "step": 2758
    },
    {
      "epoch": 4.598333333333334,
      "grad_norm": 0.05222691595554352,
      "learning_rate": 1.609348914858097e-05,
      "loss": 0.2732,
      "step": 2759
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.08252743631601334,
      "learning_rate": 1.6026711185308847e-05,
      "loss": 0.3574,
      "step": 2760
    },
    {
      "epoch": 4.601666666666667,
      "grad_norm": 0.04514109343290329,
      "learning_rate": 1.595993322203673e-05,
      "loss": 0.2714,
      "step": 2761
    },
    {
      "epoch": 4.6033333333333335,
      "grad_norm": 0.0589495487511158,
      "learning_rate": 1.5893155258764607e-05,
      "loss": 0.3092,
      "step": 2762
    },
    {
      "epoch": 4.605,
      "grad_norm": 0.11742767691612244,
      "learning_rate": 1.582637729549249e-05,
      "loss": 0.3089,
      "step": 2763
    },
    {
      "epoch": 4.6066666666666665,
      "grad_norm": 0.11392446607351303,
      "learning_rate": 1.5759599332220367e-05,
      "loss": 0.3058,
      "step": 2764
    },
    {
      "epoch": 4.608333333333333,
      "grad_norm": 0.10514522343873978,
      "learning_rate": 1.569282136894825e-05,
      "loss": 0.3618,
      "step": 2765
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.07087709754705429,
      "learning_rate": 1.5626043405676127e-05,
      "loss": 0.292,
      "step": 2766
    },
    {
      "epoch": 4.611666666666666,
      "grad_norm": 0.060921017080545425,
      "learning_rate": 1.555926544240401e-05,
      "loss": 0.3344,
      "step": 2767
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.07651757448911667,
      "learning_rate": 1.5492487479131887e-05,
      "loss": 0.3732,
      "step": 2768
    },
    {
      "epoch": 4.615,
      "grad_norm": 0.05626954138278961,
      "learning_rate": 1.5425709515859767e-05,
      "loss": 0.2892,
      "step": 2769
    },
    {
      "epoch": 4.616666666666667,
      "grad_norm": 0.04298160597681999,
      "learning_rate": 1.5358931552587647e-05,
      "loss": 0.2505,
      "step": 2770
    },
    {
      "epoch": 4.618333333333333,
      "grad_norm": 0.08236737549304962,
      "learning_rate": 1.5292153589315527e-05,
      "loss": 0.4266,
      "step": 2771
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.05047152191400528,
      "learning_rate": 1.5225375626043406e-05,
      "loss": 0.3084,
      "step": 2772
    },
    {
      "epoch": 4.621666666666667,
      "grad_norm": 0.04008699208498001,
      "learning_rate": 1.5158597662771286e-05,
      "loss": 0.2607,
      "step": 2773
    },
    {
      "epoch": 4.623333333333333,
      "grad_norm": 0.043882668018341064,
      "learning_rate": 1.5091819699499166e-05,
      "loss": 0.2925,
      "step": 2774
    },
    {
      "epoch": 4.625,
      "grad_norm": 0.05502691864967346,
      "learning_rate": 1.5025041736227046e-05,
      "loss": 0.3348,
      "step": 2775
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.039271581918001175,
      "learning_rate": 1.4958263772954924e-05,
      "loss": 0.2474,
      "step": 2776
    },
    {
      "epoch": 4.628333333333333,
      "grad_norm": 0.05136914923787117,
      "learning_rate": 1.4891485809682806e-05,
      "loss": 0.3072,
      "step": 2777
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.07263332605361938,
      "learning_rate": 1.4824707846410684e-05,
      "loss": 0.3261,
      "step": 2778
    },
    {
      "epoch": 4.631666666666667,
      "grad_norm": 0.04765884205698967,
      "learning_rate": 1.4757929883138566e-05,
      "loss": 0.2871,
      "step": 2779
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 0.0566401481628418,
      "learning_rate": 1.4691151919866444e-05,
      "loss": 0.297,
      "step": 2780
    },
    {
      "epoch": 4.635,
      "grad_norm": 0.053932808339595795,
      "learning_rate": 1.4624373956594326e-05,
      "loss": 0.2862,
      "step": 2781
    },
    {
      "epoch": 4.636666666666667,
      "grad_norm": 0.04478994384407997,
      "learning_rate": 1.4557595993322204e-05,
      "loss": 0.2584,
      "step": 2782
    },
    {
      "epoch": 4.638333333333334,
      "grad_norm": 0.03910481557250023,
      "learning_rate": 1.4490818030050086e-05,
      "loss": 0.2596,
      "step": 2783
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.04069698974490166,
      "learning_rate": 1.4424040066777964e-05,
      "loss": 0.3236,
      "step": 2784
    },
    {
      "epoch": 4.641666666666667,
      "grad_norm": 0.043116915971040726,
      "learning_rate": 1.4357262103505844e-05,
      "loss": 0.2608,
      "step": 2785
    },
    {
      "epoch": 4.6433333333333335,
      "grad_norm": 0.04176327958703041,
      "learning_rate": 1.4290484140233724e-05,
      "loss": 0.2739,
      "step": 2786
    },
    {
      "epoch": 4.645,
      "grad_norm": 0.057520050555467606,
      "learning_rate": 1.4223706176961604e-05,
      "loss": 0.3084,
      "step": 2787
    },
    {
      "epoch": 4.6466666666666665,
      "grad_norm": 0.048808299005031586,
      "learning_rate": 1.4156928213689482e-05,
      "loss": 0.3279,
      "step": 2788
    },
    {
      "epoch": 4.648333333333333,
      "grad_norm": 0.045986998826265335,
      "learning_rate": 1.4090150250417364e-05,
      "loss": 0.308,
      "step": 2789
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.03796902671456337,
      "learning_rate": 1.4023372287145242e-05,
      "loss": 0.2742,
      "step": 2790
    },
    {
      "epoch": 4.651666666666666,
      "grad_norm": 0.03278317302465439,
      "learning_rate": 1.3956594323873123e-05,
      "loss": 0.2477,
      "step": 2791
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.05665230005979538,
      "learning_rate": 1.3889816360601002e-05,
      "loss": 0.3316,
      "step": 2792
    },
    {
      "epoch": 4.655,
      "grad_norm": 0.06340686231851578,
      "learning_rate": 1.3823038397328883e-05,
      "loss": 0.2683,
      "step": 2793
    },
    {
      "epoch": 4.656666666666666,
      "grad_norm": 0.06964089721441269,
      "learning_rate": 1.3756260434056762e-05,
      "loss": 0.3765,
      "step": 2794
    },
    {
      "epoch": 4.658333333333333,
      "grad_norm": 0.09485696256160736,
      "learning_rate": 1.3689482470784643e-05,
      "loss": 0.2968,
      "step": 2795
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.045188821852207184,
      "learning_rate": 1.3622704507512521e-05,
      "loss": 0.3193,
      "step": 2796
    },
    {
      "epoch": 4.661666666666667,
      "grad_norm": 0.03776607662439346,
      "learning_rate": 1.3555926544240401e-05,
      "loss": 0.2431,
      "step": 2797
    },
    {
      "epoch": 4.663333333333333,
      "grad_norm": 0.0401783362030983,
      "learning_rate": 1.3489148580968281e-05,
      "loss": 0.2949,
      "step": 2798
    },
    {
      "epoch": 4.665,
      "grad_norm": 0.17376616597175598,
      "learning_rate": 1.3422370617696161e-05,
      "loss": 0.3105,
      "step": 2799
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.09729651361703873,
      "learning_rate": 1.335559265442404e-05,
      "loss": 0.3348,
      "step": 2800
    },
    {
      "epoch": 4.668333333333333,
      "grad_norm": 0.049955815076828,
      "learning_rate": 1.3288814691151921e-05,
      "loss": 0.3163,
      "step": 2801
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.07599448412656784,
      "learning_rate": 1.32220367278798e-05,
      "loss": 0.3317,
      "step": 2802
    },
    {
      "epoch": 4.671666666666667,
      "grad_norm": 0.04339319095015526,
      "learning_rate": 1.3155258764607681e-05,
      "loss": 0.3034,
      "step": 2803
    },
    {
      "epoch": 4.673333333333334,
      "grad_norm": 0.044802580028772354,
      "learning_rate": 1.3088480801335559e-05,
      "loss": 0.292,
      "step": 2804
    },
    {
      "epoch": 4.675,
      "grad_norm": 0.04526403546333313,
      "learning_rate": 1.302170283806344e-05,
      "loss": 0.295,
      "step": 2805
    },
    {
      "epoch": 4.676666666666667,
      "grad_norm": 0.04292969033122063,
      "learning_rate": 1.2954924874791319e-05,
      "loss": 0.2654,
      "step": 2806
    },
    {
      "epoch": 4.678333333333334,
      "grad_norm": 0.04267469048500061,
      "learning_rate": 1.28881469115192e-05,
      "loss": 0.2757,
      "step": 2807
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.037364695221185684,
      "learning_rate": 1.2821368948247079e-05,
      "loss": 0.2423,
      "step": 2808
    },
    {
      "epoch": 4.681666666666667,
      "grad_norm": 0.05093476548790932,
      "learning_rate": 1.2754590984974959e-05,
      "loss": 0.2982,
      "step": 2809
    },
    {
      "epoch": 4.683333333333334,
      "grad_norm": 0.05574287474155426,
      "learning_rate": 1.2687813021702839e-05,
      "loss": 0.368,
      "step": 2810
    },
    {
      "epoch": 4.6850000000000005,
      "grad_norm": 0.047679293900728226,
      "learning_rate": 1.2621035058430719e-05,
      "loss": 0.3062,
      "step": 2811
    },
    {
      "epoch": 4.6866666666666665,
      "grad_norm": 0.06359750777482986,
      "learning_rate": 1.2554257095158597e-05,
      "loss": 0.3154,
      "step": 2812
    },
    {
      "epoch": 4.6883333333333335,
      "grad_norm": 0.05875086411833763,
      "learning_rate": 1.2487479131886479e-05,
      "loss": 0.3694,
      "step": 2813
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 0.05680186301469803,
      "learning_rate": 1.2420701168614358e-05,
      "loss": 0.2738,
      "step": 2814
    },
    {
      "epoch": 4.691666666666666,
      "grad_norm": 0.08865170180797577,
      "learning_rate": 1.2353923205342238e-05,
      "loss": 0.3175,
      "step": 2815
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.04685264825820923,
      "learning_rate": 1.2287145242070118e-05,
      "loss": 0.3143,
      "step": 2816
    },
    {
      "epoch": 4.695,
      "grad_norm": 0.0489225760102272,
      "learning_rate": 1.2220367278797998e-05,
      "loss": 0.314,
      "step": 2817
    },
    {
      "epoch": 4.696666666666666,
      "grad_norm": 0.09295837581157684,
      "learning_rate": 1.2153589315525878e-05,
      "loss": 0.308,
      "step": 2818
    },
    {
      "epoch": 4.698333333333333,
      "grad_norm": 0.0630779042840004,
      "learning_rate": 1.2086811352253758e-05,
      "loss": 0.3918,
      "step": 2819
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.08784627169370651,
      "learning_rate": 1.2020033388981636e-05,
      "loss": 0.3522,
      "step": 2820
    },
    {
      "epoch": 4.701666666666666,
      "grad_norm": 0.03797462582588196,
      "learning_rate": 1.1953255425709516e-05,
      "loss": 0.248,
      "step": 2821
    },
    {
      "epoch": 4.703333333333333,
      "grad_norm": 0.06971275806427002,
      "learning_rate": 1.1886477462437396e-05,
      "loss": 0.3661,
      "step": 2822
    },
    {
      "epoch": 4.705,
      "grad_norm": 0.11962156742811203,
      "learning_rate": 1.1819699499165276e-05,
      "loss": 0.294,
      "step": 2823
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.0585893951356411,
      "learning_rate": 1.1752921535893156e-05,
      "loss": 0.3228,
      "step": 2824
    },
    {
      "epoch": 4.708333333333333,
      "grad_norm": 0.07481531798839569,
      "learning_rate": 1.1686143572621036e-05,
      "loss": 0.4087,
      "step": 2825
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.08132367581129074,
      "learning_rate": 1.1619365609348916e-05,
      "loss": 0.2978,
      "step": 2826
    },
    {
      "epoch": 4.711666666666667,
      "grad_norm": 0.08370528370141983,
      "learning_rate": 1.1552587646076796e-05,
      "loss": 0.3257,
      "step": 2827
    },
    {
      "epoch": 4.713333333333333,
      "grad_norm": 0.04940981790423393,
      "learning_rate": 1.1485809682804676e-05,
      "loss": 0.3194,
      "step": 2828
    },
    {
      "epoch": 4.715,
      "grad_norm": 0.05138635262846947,
      "learning_rate": 1.1419031719532556e-05,
      "loss": 0.2911,
      "step": 2829
    },
    {
      "epoch": 4.716666666666667,
      "grad_norm": 0.09723591059446335,
      "learning_rate": 1.1352253756260436e-05,
      "loss": 0.3218,
      "step": 2830
    },
    {
      "epoch": 4.718333333333334,
      "grad_norm": 0.04312112182378769,
      "learning_rate": 1.1285475792988316e-05,
      "loss": 0.2508,
      "step": 2831
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.06004071608185768,
      "learning_rate": 1.1218697829716194e-05,
      "loss": 0.3381,
      "step": 2832
    },
    {
      "epoch": 4.721666666666667,
      "grad_norm": 0.04867580533027649,
      "learning_rate": 1.1151919866444074e-05,
      "loss": 0.2982,
      "step": 2833
    },
    {
      "epoch": 4.723333333333334,
      "grad_norm": 0.08496968448162079,
      "learning_rate": 1.1085141903171954e-05,
      "loss": 0.3487,
      "step": 2834
    },
    {
      "epoch": 4.725,
      "grad_norm": 0.05046732351183891,
      "learning_rate": 1.1018363939899834e-05,
      "loss": 0.3066,
      "step": 2835
    },
    {
      "epoch": 4.726666666666667,
      "grad_norm": 0.05172625929117203,
      "learning_rate": 1.0951585976627714e-05,
      "loss": 0.2984,
      "step": 2836
    },
    {
      "epoch": 4.7283333333333335,
      "grad_norm": 0.11749596893787384,
      "learning_rate": 1.0884808013355593e-05,
      "loss": 0.3467,
      "step": 2837
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.045746318995952606,
      "learning_rate": 1.0818030050083473e-05,
      "loss": 0.311,
      "step": 2838
    },
    {
      "epoch": 4.7316666666666665,
      "grad_norm": 0.05171678960323334,
      "learning_rate": 1.0751252086811353e-05,
      "loss": 0.3315,
      "step": 2839
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.09218733012676239,
      "learning_rate": 1.0684474123539233e-05,
      "loss": 0.3369,
      "step": 2840
    },
    {
      "epoch": 4.735,
      "grad_norm": 0.03638022020459175,
      "learning_rate": 1.0617696160267113e-05,
      "loss": 0.2669,
      "step": 2841
    },
    {
      "epoch": 4.736666666666666,
      "grad_norm": 0.098743736743927,
      "learning_rate": 1.0550918196994993e-05,
      "loss": 0.2463,
      "step": 2842
    },
    {
      "epoch": 4.738333333333333,
      "grad_norm": 0.042458996176719666,
      "learning_rate": 1.0484140233722873e-05,
      "loss": 0.3291,
      "step": 2843
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.05145614966750145,
      "learning_rate": 1.0417362270450751e-05,
      "loss": 0.2604,
      "step": 2844
    },
    {
      "epoch": 4.741666666666667,
      "grad_norm": 0.041993316262960434,
      "learning_rate": 1.0350584307178631e-05,
      "loss": 0.255,
      "step": 2845
    },
    {
      "epoch": 4.743333333333333,
      "grad_norm": 0.11201407015323639,
      "learning_rate": 1.0283806343906511e-05,
      "loss": 0.3461,
      "step": 2846
    },
    {
      "epoch": 4.745,
      "grad_norm": 0.05169035494327545,
      "learning_rate": 1.0217028380634391e-05,
      "loss": 0.2857,
      "step": 2847
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.04829785227775574,
      "learning_rate": 1.0150250417362271e-05,
      "loss": 0.279,
      "step": 2848
    },
    {
      "epoch": 4.748333333333333,
      "grad_norm": 0.06006386876106262,
      "learning_rate": 1.0083472454090151e-05,
      "loss": 0.2625,
      "step": 2849
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.07926870882511139,
      "learning_rate": 1.001669449081803e-05,
      "loss": 0.3298,
      "step": 2850
    },
    {
      "epoch": 4.751666666666667,
      "grad_norm": 0.05768539384007454,
      "learning_rate": 9.94991652754591e-06,
      "loss": 0.3155,
      "step": 2851
    },
    {
      "epoch": 4.753333333333333,
      "grad_norm": 0.05010871961712837,
      "learning_rate": 9.88313856427379e-06,
      "loss": 0.3316,
      "step": 2852
    },
    {
      "epoch": 4.755,
      "grad_norm": 0.1779758185148239,
      "learning_rate": 9.81636060100167e-06,
      "loss": 0.3632,
      "step": 2853
    },
    {
      "epoch": 4.756666666666667,
      "grad_norm": 0.046219225972890854,
      "learning_rate": 9.74958263772955e-06,
      "loss": 0.3001,
      "step": 2854
    },
    {
      "epoch": 4.758333333333333,
      "grad_norm": 0.044280264526605606,
      "learning_rate": 9.68280467445743e-06,
      "loss": 0.2813,
      "step": 2855
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.06378781050443649,
      "learning_rate": 9.616026711185309e-06,
      "loss": 0.3016,
      "step": 2856
    },
    {
      "epoch": 4.761666666666667,
      "grad_norm": 0.1101803258061409,
      "learning_rate": 9.549248747913189e-06,
      "loss": 0.3079,
      "step": 2857
    },
    {
      "epoch": 4.763333333333334,
      "grad_norm": 0.03778514266014099,
      "learning_rate": 9.482470784641069e-06,
      "loss": 0.2475,
      "step": 2858
    },
    {
      "epoch": 4.765,
      "grad_norm": 0.038008999079465866,
      "learning_rate": 9.415692821368949e-06,
      "loss": 0.2514,
      "step": 2859
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 0.060605768114328384,
      "learning_rate": 9.348914858096828e-06,
      "loss": 0.3796,
      "step": 2860
    },
    {
      "epoch": 4.7683333333333335,
      "grad_norm": 0.037052497267723083,
      "learning_rate": 9.282136894824708e-06,
      "loss": 0.2089,
      "step": 2861
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.06696859747171402,
      "learning_rate": 9.215358931552588e-06,
      "loss": 0.321,
      "step": 2862
    },
    {
      "epoch": 4.7716666666666665,
      "grad_norm": 0.03501233085989952,
      "learning_rate": 9.148580968280468e-06,
      "loss": 0.2313,
      "step": 2863
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.04017378389835358,
      "learning_rate": 9.081803005008348e-06,
      "loss": 0.2851,
      "step": 2864
    },
    {
      "epoch": 4.775,
      "grad_norm": 0.06491553038358688,
      "learning_rate": 9.015025041736228e-06,
      "loss": 0.3017,
      "step": 2865
    },
    {
      "epoch": 4.776666666666666,
      "grad_norm": 0.111688032746315,
      "learning_rate": 8.948247078464108e-06,
      "loss": 0.2976,
      "step": 2866
    },
    {
      "epoch": 4.778333333333333,
      "grad_norm": 0.05002143979072571,
      "learning_rate": 8.881469115191986e-06,
      "loss": 0.3527,
      "step": 2867
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.05150340497493744,
      "learning_rate": 8.814691151919866e-06,
      "loss": 0.3386,
      "step": 2868
    },
    {
      "epoch": 4.781666666666666,
      "grad_norm": 0.06361814588308334,
      "learning_rate": 8.747913188647746e-06,
      "loss": 0.303,
      "step": 2869
    },
    {
      "epoch": 4.783333333333333,
      "grad_norm": 0.06928613781929016,
      "learning_rate": 8.681135225375626e-06,
      "loss": 0.3192,
      "step": 2870
    },
    {
      "epoch": 4.785,
      "grad_norm": 0.04311870411038399,
      "learning_rate": 8.614357262103506e-06,
      "loss": 0.2724,
      "step": 2871
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.047294918447732925,
      "learning_rate": 8.547579298831386e-06,
      "loss": 0.2997,
      "step": 2872
    },
    {
      "epoch": 4.788333333333333,
      "grad_norm": 0.05143645405769348,
      "learning_rate": 8.480801335559266e-06,
      "loss": 0.3252,
      "step": 2873
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.1056194007396698,
      "learning_rate": 8.414023372287146e-06,
      "loss": 0.2886,
      "step": 2874
    },
    {
      "epoch": 4.791666666666667,
      "grad_norm": 0.048644740134477615,
      "learning_rate": 8.347245409015026e-06,
      "loss": 0.2965,
      "step": 2875
    },
    {
      "epoch": 4.793333333333333,
      "grad_norm": 0.048148151487112045,
      "learning_rate": 8.280467445742906e-06,
      "loss": 0.3191,
      "step": 2876
    },
    {
      "epoch": 4.795,
      "grad_norm": 0.08712326735258102,
      "learning_rate": 8.213689482470786e-06,
      "loss": 0.3176,
      "step": 2877
    },
    {
      "epoch": 4.796666666666667,
      "grad_norm": 0.06344903260469437,
      "learning_rate": 8.146911519198665e-06,
      "loss": 0.3792,
      "step": 2878
    },
    {
      "epoch": 4.798333333333334,
      "grad_norm": 0.07547010481357574,
      "learning_rate": 8.080133555926544e-06,
      "loss": 0.3427,
      "step": 2879
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.035894833505153656,
      "learning_rate": 8.013355592654424e-06,
      "loss": 0.2656,
      "step": 2880
    },
    {
      "epoch": 4.801666666666667,
      "grad_norm": 0.0488261878490448,
      "learning_rate": 7.946577629382304e-06,
      "loss": 0.3231,
      "step": 2881
    },
    {
      "epoch": 4.803333333333334,
      "grad_norm": 0.039631567895412445,
      "learning_rate": 7.879799666110184e-06,
      "loss": 0.2802,
      "step": 2882
    },
    {
      "epoch": 4.805,
      "grad_norm": 0.0469258613884449,
      "learning_rate": 7.813021702838063e-06,
      "loss": 0.2465,
      "step": 2883
    },
    {
      "epoch": 4.806666666666667,
      "grad_norm": 0.06178179755806923,
      "learning_rate": 7.746243739565943e-06,
      "loss": 0.3167,
      "step": 2884
    },
    {
      "epoch": 4.808333333333334,
      "grad_norm": 0.05171976238489151,
      "learning_rate": 7.679465776293823e-06,
      "loss": 0.3244,
      "step": 2885
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 0.06210137531161308,
      "learning_rate": 7.612687813021703e-06,
      "loss": 0.3328,
      "step": 2886
    },
    {
      "epoch": 4.8116666666666665,
      "grad_norm": 0.07277411967515945,
      "learning_rate": 7.545909849749583e-06,
      "loss": 0.4329,
      "step": 2887
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.03760965168476105,
      "learning_rate": 7.479131886477462e-06,
      "loss": 0.2763,
      "step": 2888
    },
    {
      "epoch": 4.8149999999999995,
      "grad_norm": 0.03560929372906685,
      "learning_rate": 7.412353923205342e-06,
      "loss": 0.247,
      "step": 2889
    },
    {
      "epoch": 4.816666666666666,
      "grad_norm": 0.055542223155498505,
      "learning_rate": 7.345575959933222e-06,
      "loss": 0.2922,
      "step": 2890
    },
    {
      "epoch": 4.818333333333333,
      "grad_norm": 0.055821482092142105,
      "learning_rate": 7.278797996661102e-06,
      "loss": 0.3418,
      "step": 2891
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.06136682257056236,
      "learning_rate": 7.212020033388982e-06,
      "loss": 0.3498,
      "step": 2892
    },
    {
      "epoch": 4.821666666666666,
      "grad_norm": 0.04201347380876541,
      "learning_rate": 7.145242070116862e-06,
      "loss": 0.3006,
      "step": 2893
    },
    {
      "epoch": 4.823333333333333,
      "grad_norm": 0.04390571266412735,
      "learning_rate": 7.078464106844741e-06,
      "loss": 0.2905,
      "step": 2894
    },
    {
      "epoch": 4.825,
      "grad_norm": 0.041546404361724854,
      "learning_rate": 7.011686143572621e-06,
      "loss": 0.2879,
      "step": 2895
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.05157903954386711,
      "learning_rate": 6.944908180300501e-06,
      "loss": 0.3452,
      "step": 2896
    },
    {
      "epoch": 4.828333333333333,
      "grad_norm": 0.052729371935129166,
      "learning_rate": 6.878130217028381e-06,
      "loss": 0.3038,
      "step": 2897
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.04429250955581665,
      "learning_rate": 6.811352253756261e-06,
      "loss": 0.269,
      "step": 2898
    },
    {
      "epoch": 4.831666666666667,
      "grad_norm": 0.03566594794392586,
      "learning_rate": 6.744574290484141e-06,
      "loss": 0.2984,
      "step": 2899
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.04397253692150116,
      "learning_rate": 6.67779632721202e-06,
      "loss": 0.2612,
      "step": 2900
    },
    {
      "epoch": 4.835,
      "grad_norm": 0.058069273829460144,
      "learning_rate": 6.6110183639399e-06,
      "loss": 0.3227,
      "step": 2901
    },
    {
      "epoch": 4.836666666666667,
      "grad_norm": 0.06743717938661575,
      "learning_rate": 6.5442404006677796e-06,
      "loss": 0.2862,
      "step": 2902
    },
    {
      "epoch": 4.838333333333333,
      "grad_norm": 0.05384013056755066,
      "learning_rate": 6.4774624373956595e-06,
      "loss": 0.3074,
      "step": 2903
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.055049218237400055,
      "learning_rate": 6.4106844741235394e-06,
      "loss": 0.2873,
      "step": 2904
    },
    {
      "epoch": 4.841666666666667,
      "grad_norm": 0.1044217050075531,
      "learning_rate": 6.343906510851419e-06,
      "loss": 0.3109,
      "step": 2905
    },
    {
      "epoch": 4.843333333333334,
      "grad_norm": 0.06759396940469742,
      "learning_rate": 6.2771285475792984e-06,
      "loss": 0.3077,
      "step": 2906
    },
    {
      "epoch": 4.845,
      "grad_norm": 0.03856892138719559,
      "learning_rate": 6.210350584307179e-06,
      "loss": 0.2377,
      "step": 2907
    },
    {
      "epoch": 4.846666666666667,
      "grad_norm": 0.03012957237660885,
      "learning_rate": 6.143572621035059e-06,
      "loss": 0.2454,
      "step": 2908
    },
    {
      "epoch": 4.848333333333334,
      "grad_norm": 0.04493341222405434,
      "learning_rate": 6.076794657762939e-06,
      "loss": 0.2232,
      "step": 2909
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.04956131428480148,
      "learning_rate": 6.010016694490818e-06,
      "loss": 0.2882,
      "step": 2910
    },
    {
      "epoch": 4.851666666666667,
      "grad_norm": 0.041122790426015854,
      "learning_rate": 5.943238731218698e-06,
      "loss": 0.3067,
      "step": 2911
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.07327968627214432,
      "learning_rate": 5.876460767946578e-06,
      "loss": 0.2761,
      "step": 2912
    },
    {
      "epoch": 4.855,
      "grad_norm": 0.05136526748538017,
      "learning_rate": 5.809682804674458e-06,
      "loss": 0.2944,
      "step": 2913
    },
    {
      "epoch": 4.8566666666666665,
      "grad_norm": 0.03711089864373207,
      "learning_rate": 5.742904841402338e-06,
      "loss": 0.2934,
      "step": 2914
    },
    {
      "epoch": 4.858333333333333,
      "grad_norm": 0.09686552733182907,
      "learning_rate": 5.676126878130218e-06,
      "loss": 0.2962,
      "step": 2915
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.04686233773827553,
      "learning_rate": 5.609348914858097e-06,
      "loss": 0.2945,
      "step": 2916
    },
    {
      "epoch": 4.861666666666666,
      "grad_norm": 0.051393505185842514,
      "learning_rate": 5.542570951585977e-06,
      "loss": 0.2989,
      "step": 2917
    },
    {
      "epoch": 4.863333333333333,
      "grad_norm": 0.07118653506040573,
      "learning_rate": 5.475792988313857e-06,
      "loss": 0.3763,
      "step": 2918
    },
    {
      "epoch": 4.865,
      "grad_norm": 0.08768180757761002,
      "learning_rate": 5.409015025041737e-06,
      "loss": 0.3457,
      "step": 2919
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.05394367873668671,
      "learning_rate": 5.342237061769617e-06,
      "loss": 0.3193,
      "step": 2920
    },
    {
      "epoch": 4.868333333333333,
      "grad_norm": 0.05821389704942703,
      "learning_rate": 5.2754590984974965e-06,
      "loss": 0.2981,
      "step": 2921
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.045163560658693314,
      "learning_rate": 5.208681135225376e-06,
      "loss": 0.2763,
      "step": 2922
    },
    {
      "epoch": 4.871666666666667,
      "grad_norm": 0.04965350776910782,
      "learning_rate": 5.1419031719532556e-06,
      "loss": 0.3135,
      "step": 2923
    },
    {
      "epoch": 4.873333333333333,
      "grad_norm": 0.04712221026420593,
      "learning_rate": 5.0751252086811355e-06,
      "loss": 0.2641,
      "step": 2924
    },
    {
      "epoch": 4.875,
      "grad_norm": 0.06241146847605705,
      "learning_rate": 5.008347245409015e-06,
      "loss": 0.3315,
      "step": 2925
    },
    {
      "epoch": 4.876666666666667,
      "grad_norm": 0.05089443922042847,
      "learning_rate": 4.941569282136895e-06,
      "loss": 0.2775,
      "step": 2926
    },
    {
      "epoch": 4.878333333333333,
      "grad_norm": 0.04693557322025299,
      "learning_rate": 4.874791318864775e-06,
      "loss": 0.3204,
      "step": 2927
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.04268188029527664,
      "learning_rate": 4.808013355592654e-06,
      "loss": 0.2715,
      "step": 2928
    },
    {
      "epoch": 4.881666666666667,
      "grad_norm": 0.05273814871907234,
      "learning_rate": 4.741235392320534e-06,
      "loss": 0.3012,
      "step": 2929
    },
    {
      "epoch": 4.883333333333333,
      "grad_norm": 0.061315104365348816,
      "learning_rate": 4.674457429048414e-06,
      "loss": 0.2368,
      "step": 2930
    },
    {
      "epoch": 4.885,
      "grad_norm": 0.04423832520842552,
      "learning_rate": 4.607679465776294e-06,
      "loss": 0.2379,
      "step": 2931
    },
    {
      "epoch": 4.886666666666667,
      "grad_norm": 0.05249783396720886,
      "learning_rate": 4.540901502504174e-06,
      "loss": 0.3048,
      "step": 2932
    },
    {
      "epoch": 4.888333333333334,
      "grad_norm": 0.11749839782714844,
      "learning_rate": 4.474123539232054e-06,
      "loss": 0.2844,
      "step": 2933
    },
    {
      "epoch": 4.89,
      "grad_norm": 0.05687497928738594,
      "learning_rate": 4.407345575959933e-06,
      "loss": 0.3537,
      "step": 2934
    },
    {
      "epoch": 4.891666666666667,
      "grad_norm": 0.0774293839931488,
      "learning_rate": 4.340567612687813e-06,
      "loss": 0.3385,
      "step": 2935
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.12082691490650177,
      "learning_rate": 4.273789649415693e-06,
      "loss": 0.3432,
      "step": 2936
    },
    {
      "epoch": 4.895,
      "grad_norm": 0.04970834404230118,
      "learning_rate": 4.207011686143573e-06,
      "loss": 0.2966,
      "step": 2937
    },
    {
      "epoch": 4.8966666666666665,
      "grad_norm": 0.041372399777173996,
      "learning_rate": 4.140233722871453e-06,
      "loss": 0.2828,
      "step": 2938
    },
    {
      "epoch": 4.898333333333333,
      "grad_norm": 0.05386120080947876,
      "learning_rate": 4.073455759599333e-06,
      "loss": 0.2781,
      "step": 2939
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.030960358679294586,
      "learning_rate": 4.006677796327212e-06,
      "loss": 0.242,
      "step": 2940
    },
    {
      "epoch": 4.901666666666666,
      "grad_norm": 0.0496402308344841,
      "learning_rate": 3.939899833055092e-06,
      "loss": 0.2936,
      "step": 2941
    },
    {
      "epoch": 4.903333333333333,
      "grad_norm": 0.05163928493857384,
      "learning_rate": 3.873121869782972e-06,
      "loss": 0.3151,
      "step": 2942
    },
    {
      "epoch": 4.905,
      "grad_norm": 0.043743789196014404,
      "learning_rate": 3.8063439065108516e-06,
      "loss": 0.2916,
      "step": 2943
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.16336604952812195,
      "learning_rate": 3.739565943238731e-06,
      "loss": 0.3465,
      "step": 2944
    },
    {
      "epoch": 4.908333333333333,
      "grad_norm": 0.046820368617773056,
      "learning_rate": 3.672787979966611e-06,
      "loss": 0.2771,
      "step": 2945
    },
    {
      "epoch": 4.91,
      "grad_norm": 0.06874115020036697,
      "learning_rate": 3.606010016694491e-06,
      "loss": 0.2358,
      "step": 2946
    },
    {
      "epoch": 4.911666666666667,
      "grad_norm": 0.05912685766816139,
      "learning_rate": 3.5392320534223705e-06,
      "loss": 0.3691,
      "step": 2947
    },
    {
      "epoch": 4.913333333333333,
      "grad_norm": 0.1168377622961998,
      "learning_rate": 3.4724540901502504e-06,
      "loss": 0.3152,
      "step": 2948
    },
    {
      "epoch": 4.915,
      "grad_norm": 0.06615614145994186,
      "learning_rate": 3.4056761268781303e-06,
      "loss": 0.3786,
      "step": 2949
    },
    {
      "epoch": 4.916666666666667,
      "grad_norm": 0.061199288815259933,
      "learning_rate": 3.33889816360601e-06,
      "loss": 0.3041,
      "step": 2950
    },
    {
      "epoch": 4.918333333333333,
      "grad_norm": 0.0730506107211113,
      "learning_rate": 3.2721202003338898e-06,
      "loss": 0.2981,
      "step": 2951
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.11486700177192688,
      "learning_rate": 3.2053422370617697e-06,
      "loss": 0.3497,
      "step": 2952
    },
    {
      "epoch": 4.921666666666667,
      "grad_norm": 0.0641394630074501,
      "learning_rate": 3.1385642737896492e-06,
      "loss": 0.3413,
      "step": 2953
    },
    {
      "epoch": 4.923333333333334,
      "grad_norm": 0.0492401123046875,
      "learning_rate": 3.0717863105175296e-06,
      "loss": 0.3131,
      "step": 2954
    },
    {
      "epoch": 4.925,
      "grad_norm": 0.07001165300607681,
      "learning_rate": 3.005008347245409e-06,
      "loss": 0.3567,
      "step": 2955
    },
    {
      "epoch": 4.926666666666667,
      "grad_norm": 0.10307307541370392,
      "learning_rate": 2.938230383973289e-06,
      "loss": 0.2604,
      "step": 2956
    },
    {
      "epoch": 4.928333333333334,
      "grad_norm": 0.030416106805205345,
      "learning_rate": 2.871452420701169e-06,
      "loss": 0.2537,
      "step": 2957
    },
    {
      "epoch": 4.93,
      "grad_norm": 0.03648540750145912,
      "learning_rate": 2.8046744574290484e-06,
      "loss": 0.258,
      "step": 2958
    },
    {
      "epoch": 4.931666666666667,
      "grad_norm": 0.07984159141778946,
      "learning_rate": 2.7378964941569284e-06,
      "loss": 0.2992,
      "step": 2959
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.04972909763455391,
      "learning_rate": 2.6711185308848083e-06,
      "loss": 0.2522,
      "step": 2960
    },
    {
      "epoch": 4.9350000000000005,
      "grad_norm": 0.07324350625276566,
      "learning_rate": 2.604340567612688e-06,
      "loss": 0.3204,
      "step": 2961
    },
    {
      "epoch": 4.9366666666666665,
      "grad_norm": 0.08114171773195267,
      "learning_rate": 2.5375626043405677e-06,
      "loss": 0.3258,
      "step": 2962
    },
    {
      "epoch": 4.9383333333333335,
      "grad_norm": 0.05727304890751839,
      "learning_rate": 2.4707846410684477e-06,
      "loss": 0.3241,
      "step": 2963
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.0593571774661541,
      "learning_rate": 2.404006677796327e-06,
      "loss": 0.325,
      "step": 2964
    },
    {
      "epoch": 4.941666666666666,
      "grad_norm": 0.047914907336235046,
      "learning_rate": 2.337228714524207e-06,
      "loss": 0.3158,
      "step": 2965
    },
    {
      "epoch": 4.943333333333333,
      "grad_norm": 0.06729251891374588,
      "learning_rate": 2.270450751252087e-06,
      "loss": 0.2269,
      "step": 2966
    },
    {
      "epoch": 4.945,
      "grad_norm": 0.035683512687683105,
      "learning_rate": 2.2036727879799665e-06,
      "loss": 0.2645,
      "step": 2967
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.03595855459570885,
      "learning_rate": 2.1368948247078465e-06,
      "loss": 0.2477,
      "step": 2968
    },
    {
      "epoch": 4.948333333333333,
      "grad_norm": 0.03774641081690788,
      "learning_rate": 2.0701168614357264e-06,
      "loss": 0.2512,
      "step": 2969
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.06565535813570023,
      "learning_rate": 2.003338898163606e-06,
      "loss": 0.3978,
      "step": 2970
    },
    {
      "epoch": 4.951666666666666,
      "grad_norm": 0.05897967144846916,
      "learning_rate": 1.936560934891486e-06,
      "loss": 0.2982,
      "step": 2971
    },
    {
      "epoch": 4.953333333333333,
      "grad_norm": 0.09031958878040314,
      "learning_rate": 1.8697829716193656e-06,
      "loss": 0.3048,
      "step": 2972
    },
    {
      "epoch": 4.955,
      "grad_norm": 0.06202599033713341,
      "learning_rate": 1.8030050083472455e-06,
      "loss": 0.313,
      "step": 2973
    },
    {
      "epoch": 4.956666666666667,
      "grad_norm": 0.15521405637264252,
      "learning_rate": 1.7362270450751252e-06,
      "loss": 0.2804,
      "step": 2974
    },
    {
      "epoch": 4.958333333333333,
      "grad_norm": 0.04205981642007828,
      "learning_rate": 1.669449081803005e-06,
      "loss": 0.2892,
      "step": 2975
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.04241781309247017,
      "learning_rate": 1.6026711185308849e-06,
      "loss": 0.2794,
      "step": 2976
    },
    {
      "epoch": 4.961666666666667,
      "grad_norm": 0.060542892664670944,
      "learning_rate": 1.5358931552587648e-06,
      "loss": 0.2548,
      "step": 2977
    },
    {
      "epoch": 4.963333333333333,
      "grad_norm": 0.06432651728391647,
      "learning_rate": 1.4691151919866445e-06,
      "loss": 0.2467,
      "step": 2978
    },
    {
      "epoch": 4.965,
      "grad_norm": 0.1993604302406311,
      "learning_rate": 1.4023372287145242e-06,
      "loss": 0.2494,
      "step": 2979
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 0.061908602714538574,
      "learning_rate": 1.3355592654424042e-06,
      "loss": 0.3398,
      "step": 2980
    },
    {
      "epoch": 4.968333333333334,
      "grad_norm": 0.06075524911284447,
      "learning_rate": 1.2687813021702839e-06,
      "loss": 0.3218,
      "step": 2981
    },
    {
      "epoch": 4.97,
      "grad_norm": 0.05253901705145836,
      "learning_rate": 1.2020033388981636e-06,
      "loss": 0.2888,
      "step": 2982
    },
    {
      "epoch": 4.971666666666667,
      "grad_norm": 0.044449567794799805,
      "learning_rate": 1.1352253756260435e-06,
      "loss": 0.2807,
      "step": 2983
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.06361185759305954,
      "learning_rate": 1.0684474123539232e-06,
      "loss": 0.3544,
      "step": 2984
    },
    {
      "epoch": 4.975,
      "grad_norm": 0.03138159215450287,
      "learning_rate": 1.001669449081803e-06,
      "loss": 0.2588,
      "step": 2985
    },
    {
      "epoch": 4.976666666666667,
      "grad_norm": 0.05530906841158867,
      "learning_rate": 9.348914858096828e-07,
      "loss": 0.3283,
      "step": 2986
    },
    {
      "epoch": 4.9783333333333335,
      "grad_norm": 0.05306041240692139,
      "learning_rate": 8.681135225375626e-07,
      "loss": 0.3044,
      "step": 2987
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.03887154161930084,
      "learning_rate": 8.013355592654424e-07,
      "loss": 0.2751,
      "step": 2988
    },
    {
      "epoch": 4.9816666666666665,
      "grad_norm": 0.04533722996711731,
      "learning_rate": 7.345575959933223e-07,
      "loss": 0.304,
      "step": 2989
    },
    {
      "epoch": 4.983333333333333,
      "grad_norm": 0.15664030611515045,
      "learning_rate": 6.677796327212021e-07,
      "loss": 0.3592,
      "step": 2990
    },
    {
      "epoch": 4.985,
      "grad_norm": 0.04750180244445801,
      "learning_rate": 6.010016694490818e-07,
      "loss": 0.2928,
      "step": 2991
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.049128808081150055,
      "learning_rate": 5.342237061769616e-07,
      "loss": 0.2842,
      "step": 2992
    },
    {
      "epoch": 4.988333333333333,
      "grad_norm": 0.046757735311985016,
      "learning_rate": 4.674457429048414e-07,
      "loss": 0.2777,
      "step": 2993
    },
    {
      "epoch": 4.99,
      "grad_norm": 0.0599355548620224,
      "learning_rate": 4.006677796327212e-07,
      "loss": 0.331,
      "step": 2994
    },
    {
      "epoch": 4.991666666666667,
      "grad_norm": 0.08268024027347565,
      "learning_rate": 3.3388981636060104e-07,
      "loss": 0.3574,
      "step": 2995
    },
    {
      "epoch": 4.993333333333333,
      "grad_norm": 0.09297409653663635,
      "learning_rate": 2.671118530884808e-07,
      "loss": 0.3272,
      "step": 2996
    },
    {
      "epoch": 4.995,
      "grad_norm": 0.08653255552053452,
      "learning_rate": 2.003338898163606e-07,
      "loss": 0.3446,
      "step": 2997
    },
    {
      "epoch": 4.996666666666667,
      "grad_norm": 0.0552184134721756,
      "learning_rate": 1.335559265442404e-07,
      "loss": 0.3367,
      "step": 2998
    },
    {
      "epoch": 4.998333333333333,
      "grad_norm": 0.051409099251031876,
      "learning_rate": 6.67779632721202e-08,
      "loss": 0.3181,
      "step": 2999
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.05119987577199936,
      "learning_rate": 0.0,
      "loss": 0.3104,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.869254445524255e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
