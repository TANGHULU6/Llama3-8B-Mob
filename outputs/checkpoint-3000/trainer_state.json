{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8823529411764706,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002941176470588235,
      "grad_norm": 2.1835074424743652,
      "learning_rate": 4e-05,
      "loss": 2.0034,
      "step": 1
    },
    {
      "epoch": 0.000588235294117647,
      "grad_norm": 2.1066360473632812,
      "learning_rate": 8e-05,
      "loss": 2.1112,
      "step": 2
    },
    {
      "epoch": 0.0008823529411764706,
      "grad_norm": 2.020095109939575,
      "learning_rate": 0.00012,
      "loss": 2.0412,
      "step": 3
    },
    {
      "epoch": 0.001176470588235294,
      "grad_norm": 1.8769174814224243,
      "learning_rate": 0.00016,
      "loss": 2.0435,
      "step": 4
    },
    {
      "epoch": 0.0014705882352941176,
      "grad_norm": 1.5894464254379272,
      "learning_rate": 0.0002,
      "loss": 1.6745,
      "step": 5
    },
    {
      "epoch": 0.0017647058823529412,
      "grad_norm": 2.2829883098602295,
      "learning_rate": 0.00019994108983799707,
      "loss": 1.2758,
      "step": 6
    },
    {
      "epoch": 0.002058823529411765,
      "grad_norm": 1.3070324659347534,
      "learning_rate": 0.00019988217967599413,
      "loss": 1.2102,
      "step": 7
    },
    {
      "epoch": 0.002352941176470588,
      "grad_norm": 1.2940396070480347,
      "learning_rate": 0.00019982326951399116,
      "loss": 1.198,
      "step": 8
    },
    {
      "epoch": 0.0026470588235294116,
      "grad_norm": 1.2325717210769653,
      "learning_rate": 0.00019976435935198822,
      "loss": 1.0959,
      "step": 9
    },
    {
      "epoch": 0.0029411764705882353,
      "grad_norm": 1.2202820777893066,
      "learning_rate": 0.00019970544918998528,
      "loss": 0.998,
      "step": 10
    },
    {
      "epoch": 0.003235294117647059,
      "grad_norm": 2.3062727451324463,
      "learning_rate": 0.00019964653902798234,
      "loss": 0.7899,
      "step": 11
    },
    {
      "epoch": 0.0035294117647058825,
      "grad_norm": 1.056572437286377,
      "learning_rate": 0.0001995876288659794,
      "loss": 0.7102,
      "step": 12
    },
    {
      "epoch": 0.003823529411764706,
      "grad_norm": 0.9499183297157288,
      "learning_rate": 0.00019952871870397644,
      "loss": 0.8086,
      "step": 13
    },
    {
      "epoch": 0.00411764705882353,
      "grad_norm": 0.5703122615814209,
      "learning_rate": 0.0001994698085419735,
      "loss": 0.5728,
      "step": 14
    },
    {
      "epoch": 0.004411764705882353,
      "grad_norm": 0.4960967004299164,
      "learning_rate": 0.00019941089837997056,
      "loss": 0.5901,
      "step": 15
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 0.4866180121898651,
      "learning_rate": 0.00019935198821796762,
      "loss": 0.63,
      "step": 16
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.471566766500473,
      "learning_rate": 0.00019929307805596468,
      "loss": 0.6949,
      "step": 17
    },
    {
      "epoch": 0.005294117647058823,
      "grad_norm": 0.3700290620326996,
      "learning_rate": 0.0001992341678939617,
      "loss": 0.6886,
      "step": 18
    },
    {
      "epoch": 0.005588235294117647,
      "grad_norm": 0.252212792634964,
      "learning_rate": 0.00019917525773195877,
      "loss": 0.4542,
      "step": 19
    },
    {
      "epoch": 0.0058823529411764705,
      "grad_norm": 0.2795506417751312,
      "learning_rate": 0.00019911634756995583,
      "loss": 0.5234,
      "step": 20
    },
    {
      "epoch": 0.006176470588235294,
      "grad_norm": 0.23773442208766937,
      "learning_rate": 0.0001990574374079529,
      "loss": 0.5363,
      "step": 21
    },
    {
      "epoch": 0.006470588235294118,
      "grad_norm": 0.17992019653320312,
      "learning_rate": 0.00019899852724594995,
      "loss": 0.44,
      "step": 22
    },
    {
      "epoch": 0.006764705882352941,
      "grad_norm": 0.23571519553661346,
      "learning_rate": 0.00019893961708394698,
      "loss": 0.5706,
      "step": 23
    },
    {
      "epoch": 0.007058823529411765,
      "grad_norm": 0.14562827348709106,
      "learning_rate": 0.00019888070692194404,
      "loss": 0.4329,
      "step": 24
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 0.29210659861564636,
      "learning_rate": 0.0001988217967599411,
      "loss": 0.5207,
      "step": 25
    },
    {
      "epoch": 0.007647058823529412,
      "grad_norm": 0.2006864696741104,
      "learning_rate": 0.00019876288659793816,
      "loss": 0.5589,
      "step": 26
    },
    {
      "epoch": 0.007941176470588234,
      "grad_norm": 0.12910480797290802,
      "learning_rate": 0.00019870397643593522,
      "loss": 0.4613,
      "step": 27
    },
    {
      "epoch": 0.00823529411764706,
      "grad_norm": 0.18537572026252747,
      "learning_rate": 0.00019864506627393226,
      "loss": 0.4548,
      "step": 28
    },
    {
      "epoch": 0.008529411764705883,
      "grad_norm": 0.18503913283348083,
      "learning_rate": 0.00019858615611192932,
      "loss": 0.4766,
      "step": 29
    },
    {
      "epoch": 0.008823529411764706,
      "grad_norm": 0.17886650562286377,
      "learning_rate": 0.00019852724594992638,
      "loss": 0.3675,
      "step": 30
    },
    {
      "epoch": 0.009117647058823529,
      "grad_norm": 0.1789274662733078,
      "learning_rate": 0.00019846833578792344,
      "loss": 0.3992,
      "step": 31
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 0.16663184762001038,
      "learning_rate": 0.0001984094256259205,
      "loss": 0.4142,
      "step": 32
    },
    {
      "epoch": 0.009705882352941177,
      "grad_norm": 0.19921065866947174,
      "learning_rate": 0.00019835051546391753,
      "loss": 0.5053,
      "step": 33
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1876063495874405,
      "learning_rate": 0.0001982916053019146,
      "loss": 0.477,
      "step": 34
    },
    {
      "epoch": 0.010294117647058823,
      "grad_norm": 0.18909965455532074,
      "learning_rate": 0.00019823269513991165,
      "loss": 0.3618,
      "step": 35
    },
    {
      "epoch": 0.010588235294117647,
      "grad_norm": 0.12914979457855225,
      "learning_rate": 0.0001981737849779087,
      "loss": 0.4915,
      "step": 36
    },
    {
      "epoch": 0.01088235294117647,
      "grad_norm": 0.14478152990341187,
      "learning_rate": 0.00019811487481590577,
      "loss": 0.3616,
      "step": 37
    },
    {
      "epoch": 0.011176470588235295,
      "grad_norm": 0.174483984708786,
      "learning_rate": 0.0001980559646539028,
      "loss": 0.4842,
      "step": 38
    },
    {
      "epoch": 0.011470588235294118,
      "grad_norm": 0.09472692012786865,
      "learning_rate": 0.00019799705449189987,
      "loss": 0.2295,
      "step": 39
    },
    {
      "epoch": 0.011764705882352941,
      "grad_norm": 0.11488152295351028,
      "learning_rate": 0.00019793814432989693,
      "loss": 0.3744,
      "step": 40
    },
    {
      "epoch": 0.012058823529411764,
      "grad_norm": 0.13654370605945587,
      "learning_rate": 0.00019787923416789399,
      "loss": 0.4129,
      "step": 41
    },
    {
      "epoch": 0.012352941176470587,
      "grad_norm": 0.08820246160030365,
      "learning_rate": 0.00019782032400589105,
      "loss": 0.3549,
      "step": 42
    },
    {
      "epoch": 0.012647058823529412,
      "grad_norm": 0.15094943344593048,
      "learning_rate": 0.00019776141384388808,
      "loss": 0.4397,
      "step": 43
    },
    {
      "epoch": 0.012941176470588235,
      "grad_norm": 0.10602625459432602,
      "learning_rate": 0.00019770250368188514,
      "loss": 0.3604,
      "step": 44
    },
    {
      "epoch": 0.013235294117647059,
      "grad_norm": 0.0990748330950737,
      "learning_rate": 0.0001976435935198822,
      "loss": 0.4773,
      "step": 45
    },
    {
      "epoch": 0.013529411764705882,
      "grad_norm": 0.10455132275819778,
      "learning_rate": 0.00019758468335787926,
      "loss": 0.3782,
      "step": 46
    },
    {
      "epoch": 0.013823529411764707,
      "grad_norm": 0.10086803883314133,
      "learning_rate": 0.00019752577319587632,
      "loss": 0.4278,
      "step": 47
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 0.11410357058048248,
      "learning_rate": 0.00019746686303387335,
      "loss": 0.4953,
      "step": 48
    },
    {
      "epoch": 0.014411764705882353,
      "grad_norm": 0.08383175730705261,
      "learning_rate": 0.0001974079528718704,
      "loss": 0.4163,
      "step": 49
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 0.09381704777479172,
      "learning_rate": 0.00019734904270986747,
      "loss": 0.4508,
      "step": 50
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.08147246390581131,
      "learning_rate": 0.00019729013254786453,
      "loss": 0.3236,
      "step": 51
    },
    {
      "epoch": 0.015294117647058824,
      "grad_norm": 0.10762112587690353,
      "learning_rate": 0.0001972312223858616,
      "loss": 0.3602,
      "step": 52
    },
    {
      "epoch": 0.015588235294117648,
      "grad_norm": 0.10376492887735367,
      "learning_rate": 0.00019717231222385863,
      "loss": 0.4145,
      "step": 53
    },
    {
      "epoch": 0.01588235294117647,
      "grad_norm": 0.07615689188241959,
      "learning_rate": 0.0001971134020618557,
      "loss": 0.337,
      "step": 54
    },
    {
      "epoch": 0.016176470588235296,
      "grad_norm": 0.09692107886075974,
      "learning_rate": 0.00019705449189985275,
      "loss": 0.4248,
      "step": 55
    },
    {
      "epoch": 0.01647058823529412,
      "grad_norm": 0.0790897086262703,
      "learning_rate": 0.0001969955817378498,
      "loss": 0.3692,
      "step": 56
    },
    {
      "epoch": 0.016764705882352942,
      "grad_norm": 0.09807276725769043,
      "learning_rate": 0.00019693667157584687,
      "loss": 0.4863,
      "step": 57
    },
    {
      "epoch": 0.017058823529411765,
      "grad_norm": 0.0984250158071518,
      "learning_rate": 0.0001968777614138439,
      "loss": 0.3647,
      "step": 58
    },
    {
      "epoch": 0.01735294117647059,
      "grad_norm": 0.0653030276298523,
      "learning_rate": 0.00019681885125184093,
      "loss": 0.3701,
      "step": 59
    },
    {
      "epoch": 0.01764705882352941,
      "grad_norm": 0.09172077476978302,
      "learning_rate": 0.000196759941089838,
      "loss": 0.4507,
      "step": 60
    },
    {
      "epoch": 0.017941176470588235,
      "grad_norm": 0.07702208310365677,
      "learning_rate": 0.00019670103092783505,
      "loss": 0.3775,
      "step": 61
    },
    {
      "epoch": 0.018235294117647058,
      "grad_norm": 0.08453193306922913,
      "learning_rate": 0.00019664212076583211,
      "loss": 0.3371,
      "step": 62
    },
    {
      "epoch": 0.01852941176470588,
      "grad_norm": 0.07407738268375397,
      "learning_rate": 0.00019658321060382915,
      "loss": 0.3505,
      "step": 63
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 0.07670024037361145,
      "learning_rate": 0.0001965243004418262,
      "loss": 0.3849,
      "step": 64
    },
    {
      "epoch": 0.01911764705882353,
      "grad_norm": 0.07443025708198547,
      "learning_rate": 0.00019646539027982327,
      "loss": 0.3448,
      "step": 65
    },
    {
      "epoch": 0.019411764705882354,
      "grad_norm": 0.09393195062875748,
      "learning_rate": 0.00019640648011782033,
      "loss": 0.4421,
      "step": 66
    },
    {
      "epoch": 0.019705882352941177,
      "grad_norm": 0.08082257211208344,
      "learning_rate": 0.0001963475699558174,
      "loss": 0.2845,
      "step": 67
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.0882955864071846,
      "learning_rate": 0.00019628865979381442,
      "loss": 0.3601,
      "step": 68
    },
    {
      "epoch": 0.020294117647058824,
      "grad_norm": 0.06807062774896622,
      "learning_rate": 0.00019622974963181148,
      "loss": 0.3227,
      "step": 69
    },
    {
      "epoch": 0.020588235294117647,
      "grad_norm": 0.08838649839162827,
      "learning_rate": 0.00019617083946980854,
      "loss": 0.2737,
      "step": 70
    },
    {
      "epoch": 0.02088235294117647,
      "grad_norm": 0.12643128633499146,
      "learning_rate": 0.0001961119293078056,
      "loss": 0.4371,
      "step": 71
    },
    {
      "epoch": 0.021176470588235293,
      "grad_norm": 0.07378862053155899,
      "learning_rate": 0.00019605301914580266,
      "loss": 0.314,
      "step": 72
    },
    {
      "epoch": 0.021470588235294116,
      "grad_norm": 0.148187518119812,
      "learning_rate": 0.0001959941089837997,
      "loss": 0.4053,
      "step": 73
    },
    {
      "epoch": 0.02176470588235294,
      "grad_norm": 0.09213563799858093,
      "learning_rate": 0.00019593519882179675,
      "loss": 0.4561,
      "step": 74
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 0.07311000674962997,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.4227,
      "step": 75
    },
    {
      "epoch": 0.02235294117647059,
      "grad_norm": 0.10356037318706512,
      "learning_rate": 0.00019581737849779087,
      "loss": 0.3563,
      "step": 76
    },
    {
      "epoch": 0.022647058823529412,
      "grad_norm": 0.09420822560787201,
      "learning_rate": 0.00019575846833578793,
      "loss": 0.3436,
      "step": 77
    },
    {
      "epoch": 0.022941176470588236,
      "grad_norm": 0.06972385942935944,
      "learning_rate": 0.00019569955817378497,
      "loss": 0.3347,
      "step": 78
    },
    {
      "epoch": 0.02323529411764706,
      "grad_norm": 0.07533767819404602,
      "learning_rate": 0.00019564064801178203,
      "loss": 0.3263,
      "step": 79
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 0.12221214920282364,
      "learning_rate": 0.0001955817378497791,
      "loss": 0.4261,
      "step": 80
    },
    {
      "epoch": 0.023823529411764705,
      "grad_norm": 0.09389033168554306,
      "learning_rate": 0.00019552282768777615,
      "loss": 0.3815,
      "step": 81
    },
    {
      "epoch": 0.02411764705882353,
      "grad_norm": 0.11122535914182663,
      "learning_rate": 0.0001954639175257732,
      "loss": 0.43,
      "step": 82
    },
    {
      "epoch": 0.02441176470588235,
      "grad_norm": 0.15265360474586487,
      "learning_rate": 0.00019540500736377024,
      "loss": 0.498,
      "step": 83
    },
    {
      "epoch": 0.024705882352941175,
      "grad_norm": 0.09435665607452393,
      "learning_rate": 0.0001953460972017673,
      "loss": 0.4346,
      "step": 84
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.0835433080792427,
      "learning_rate": 0.00019528718703976436,
      "loss": 0.2738,
      "step": 85
    },
    {
      "epoch": 0.025294117647058825,
      "grad_norm": 0.12333851307630539,
      "learning_rate": 0.00019522827687776142,
      "loss": 0.3724,
      "step": 86
    },
    {
      "epoch": 0.025588235294117648,
      "grad_norm": 0.11394748836755753,
      "learning_rate": 0.00019516936671575848,
      "loss": 0.4179,
      "step": 87
    },
    {
      "epoch": 0.02588235294117647,
      "grad_norm": 0.0668557807803154,
      "learning_rate": 0.00019511045655375552,
      "loss": 0.2998,
      "step": 88
    },
    {
      "epoch": 0.026176470588235294,
      "grad_norm": 0.0722629576921463,
      "learning_rate": 0.00019505154639175258,
      "loss": 0.3479,
      "step": 89
    },
    {
      "epoch": 0.026470588235294117,
      "grad_norm": 0.08276353776454926,
      "learning_rate": 0.00019499263622974964,
      "loss": 0.4187,
      "step": 90
    },
    {
      "epoch": 0.02676470588235294,
      "grad_norm": 0.09815466403961182,
      "learning_rate": 0.0001949337260677467,
      "loss": 0.453,
      "step": 91
    },
    {
      "epoch": 0.027058823529411764,
      "grad_norm": 0.1115587130188942,
      "learning_rate": 0.00019487481590574376,
      "loss": 0.391,
      "step": 92
    },
    {
      "epoch": 0.027352941176470587,
      "grad_norm": 0.08895131945610046,
      "learning_rate": 0.0001948159057437408,
      "loss": 0.3613,
      "step": 93
    },
    {
      "epoch": 0.027647058823529413,
      "grad_norm": 0.07907140254974365,
      "learning_rate": 0.00019475699558173785,
      "loss": 0.3996,
      "step": 94
    },
    {
      "epoch": 0.027941176470588237,
      "grad_norm": 0.060466546565294266,
      "learning_rate": 0.0001946980854197349,
      "loss": 0.3514,
      "step": 95
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 0.08669271320104599,
      "learning_rate": 0.00019463917525773197,
      "loss": 0.3257,
      "step": 96
    },
    {
      "epoch": 0.028529411764705883,
      "grad_norm": 0.10394621640443802,
      "learning_rate": 0.00019458026509572903,
      "loss": 0.4313,
      "step": 97
    },
    {
      "epoch": 0.028823529411764706,
      "grad_norm": 0.08653022348880768,
      "learning_rate": 0.00019452135493372606,
      "loss": 0.4557,
      "step": 98
    },
    {
      "epoch": 0.02911764705882353,
      "grad_norm": 0.05869050323963165,
      "learning_rate": 0.00019446244477172312,
      "loss": 0.3059,
      "step": 99
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.09673353284597397,
      "learning_rate": 0.00019440353460972018,
      "loss": 0.5006,
      "step": 100
    },
    {
      "epoch": 0.029705882352941176,
      "grad_norm": 0.07488714158535004,
      "learning_rate": 0.00019434462444771724,
      "loss": 0.3754,
      "step": 101
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.06179310753941536,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.3578,
      "step": 102
    },
    {
      "epoch": 0.030294117647058822,
      "grad_norm": 0.07002272456884384,
      "learning_rate": 0.00019422680412371134,
      "loss": 0.3498,
      "step": 103
    },
    {
      "epoch": 0.03058823529411765,
      "grad_norm": 0.08683742582798004,
      "learning_rate": 0.0001941678939617084,
      "loss": 0.4209,
      "step": 104
    },
    {
      "epoch": 0.030882352941176472,
      "grad_norm": 0.09129704535007477,
      "learning_rate": 0.00019410898379970546,
      "loss": 0.38,
      "step": 105
    },
    {
      "epoch": 0.031176470588235295,
      "grad_norm": 0.0711800754070282,
      "learning_rate": 0.00019405007363770252,
      "loss": 0.4083,
      "step": 106
    },
    {
      "epoch": 0.03147058823529412,
      "grad_norm": 0.11384622752666473,
      "learning_rate": 0.00019399116347569958,
      "loss": 0.4224,
      "step": 107
    },
    {
      "epoch": 0.03176470588235294,
      "grad_norm": 0.0907897874712944,
      "learning_rate": 0.0001939322533136966,
      "loss": 0.3922,
      "step": 108
    },
    {
      "epoch": 0.032058823529411765,
      "grad_norm": 0.06573566794395447,
      "learning_rate": 0.00019387334315169367,
      "loss": 0.2844,
      "step": 109
    },
    {
      "epoch": 0.03235294117647059,
      "grad_norm": 0.10348077118396759,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.3964,
      "step": 110
    },
    {
      "epoch": 0.03264705882352941,
      "grad_norm": 0.0762903019785881,
      "learning_rate": 0.0001937555228276878,
      "loss": 0.3731,
      "step": 111
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 0.0904083326458931,
      "learning_rate": 0.00019369661266568485,
      "loss": 0.4141,
      "step": 112
    },
    {
      "epoch": 0.03323529411764706,
      "grad_norm": 0.1139480322599411,
      "learning_rate": 0.00019363770250368188,
      "loss": 0.3646,
      "step": 113
    },
    {
      "epoch": 0.033529411764705884,
      "grad_norm": 0.08844795823097229,
      "learning_rate": 0.00019357879234167894,
      "loss": 0.3768,
      "step": 114
    },
    {
      "epoch": 0.033823529411764704,
      "grad_norm": 0.08932732790708542,
      "learning_rate": 0.000193519882179676,
      "loss": 0.415,
      "step": 115
    },
    {
      "epoch": 0.03411764705882353,
      "grad_norm": 0.1254056990146637,
      "learning_rate": 0.00019346097201767306,
      "loss": 0.3819,
      "step": 116
    },
    {
      "epoch": 0.03441176470588235,
      "grad_norm": 0.09746512025594711,
      "learning_rate": 0.00019340206185567012,
      "loss": 0.425,
      "step": 117
    },
    {
      "epoch": 0.03470588235294118,
      "grad_norm": 0.08445445448160172,
      "learning_rate": 0.00019334315169366716,
      "loss": 0.4351,
      "step": 118
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.09242141991853714,
      "learning_rate": 0.00019328424153166422,
      "loss": 0.4325,
      "step": 119
    },
    {
      "epoch": 0.03529411764705882,
      "grad_norm": 0.07406548410654068,
      "learning_rate": 0.00019322533136966128,
      "loss": 0.4032,
      "step": 120
    },
    {
      "epoch": 0.03558823529411765,
      "grad_norm": 0.10077258199453354,
      "learning_rate": 0.00019316642120765834,
      "loss": 0.3409,
      "step": 121
    },
    {
      "epoch": 0.03588235294117647,
      "grad_norm": 0.10805519670248032,
      "learning_rate": 0.0001931075110456554,
      "loss": 0.489,
      "step": 122
    },
    {
      "epoch": 0.036176470588235296,
      "grad_norm": 0.08050020039081573,
      "learning_rate": 0.00019304860088365243,
      "loss": 0.3938,
      "step": 123
    },
    {
      "epoch": 0.036470588235294116,
      "grad_norm": 0.08660823851823807,
      "learning_rate": 0.0001929896907216495,
      "loss": 0.4373,
      "step": 124
    },
    {
      "epoch": 0.03676470588235294,
      "grad_norm": 0.0706796795129776,
      "learning_rate": 0.00019293078055964655,
      "loss": 0.3082,
      "step": 125
    },
    {
      "epoch": 0.03705882352941176,
      "grad_norm": 0.09656774252653122,
      "learning_rate": 0.0001928718703976436,
      "loss": 0.4031,
      "step": 126
    },
    {
      "epoch": 0.03735294117647059,
      "grad_norm": 0.07571390271186829,
      "learning_rate": 0.00019281296023564067,
      "loss": 0.3605,
      "step": 127
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 0.0586065910756588,
      "learning_rate": 0.0001927540500736377,
      "loss": 0.3305,
      "step": 128
    },
    {
      "epoch": 0.037941176470588235,
      "grad_norm": 0.07895198464393616,
      "learning_rate": 0.00019269513991163477,
      "loss": 0.4031,
      "step": 129
    },
    {
      "epoch": 0.03823529411764706,
      "grad_norm": 0.07876359671354294,
      "learning_rate": 0.00019263622974963183,
      "loss": 0.3708,
      "step": 130
    },
    {
      "epoch": 0.03852941176470588,
      "grad_norm": 0.06287344545125961,
      "learning_rate": 0.00019257731958762889,
      "loss": 0.3241,
      "step": 131
    },
    {
      "epoch": 0.03882352941176471,
      "grad_norm": 0.09601379185914993,
      "learning_rate": 0.00019251840942562595,
      "loss": 0.4325,
      "step": 132
    },
    {
      "epoch": 0.03911764705882353,
      "grad_norm": 0.07955898344516754,
      "learning_rate": 0.00019245949926362298,
      "loss": 0.2913,
      "step": 133
    },
    {
      "epoch": 0.039411764705882354,
      "grad_norm": 0.0715600773692131,
      "learning_rate": 0.00019240058910162004,
      "loss": 0.4114,
      "step": 134
    },
    {
      "epoch": 0.039705882352941174,
      "grad_norm": 0.08470238000154495,
      "learning_rate": 0.0001923416789396171,
      "loss": 0.4022,
      "step": 135
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.09018309414386749,
      "learning_rate": 0.00019228276877761416,
      "loss": 0.4207,
      "step": 136
    },
    {
      "epoch": 0.04029411764705882,
      "grad_norm": 0.08381707221269608,
      "learning_rate": 0.00019222385861561122,
      "loss": 0.3539,
      "step": 137
    },
    {
      "epoch": 0.04058823529411765,
      "grad_norm": 0.07111643254756927,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.2911,
      "step": 138
    },
    {
      "epoch": 0.040882352941176474,
      "grad_norm": 0.09789180010557175,
      "learning_rate": 0.0001921060382916053,
      "loss": 0.4222,
      "step": 139
    },
    {
      "epoch": 0.041176470588235294,
      "grad_norm": 0.06659652292728424,
      "learning_rate": 0.00019204712812960237,
      "loss": 0.3623,
      "step": 140
    },
    {
      "epoch": 0.04147058823529412,
      "grad_norm": 0.09430304914712906,
      "learning_rate": 0.00019198821796759943,
      "loss": 0.4324,
      "step": 141
    },
    {
      "epoch": 0.04176470588235294,
      "grad_norm": 0.079689621925354,
      "learning_rate": 0.0001919293078055965,
      "loss": 0.425,
      "step": 142
    },
    {
      "epoch": 0.04205882352941177,
      "grad_norm": 0.07443461567163467,
      "learning_rate": 0.00019187039764359353,
      "loss": 0.3895,
      "step": 143
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 0.09401646256446838,
      "learning_rate": 0.0001918114874815906,
      "loss": 0.4119,
      "step": 144
    },
    {
      "epoch": 0.04264705882352941,
      "grad_norm": 0.0865236446261406,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.3633,
      "step": 145
    },
    {
      "epoch": 0.04294117647058823,
      "grad_norm": 0.099636971950531,
      "learning_rate": 0.0001916936671575847,
      "loss": 0.4135,
      "step": 146
    },
    {
      "epoch": 0.04323529411764706,
      "grad_norm": 0.06416217237710953,
      "learning_rate": 0.00019163475699558177,
      "loss": 0.3523,
      "step": 147
    },
    {
      "epoch": 0.04352941176470588,
      "grad_norm": 0.0789947584271431,
      "learning_rate": 0.0001915758468335788,
      "loss": 0.3704,
      "step": 148
    },
    {
      "epoch": 0.043823529411764706,
      "grad_norm": 0.09879098832607269,
      "learning_rate": 0.00019151693667157586,
      "loss": 0.4428,
      "step": 149
    },
    {
      "epoch": 0.04411764705882353,
      "grad_norm": 0.11308111250400543,
      "learning_rate": 0.00019145802650957292,
      "loss": 0.3952,
      "step": 150
    },
    {
      "epoch": 0.04441176470588235,
      "grad_norm": 0.057182349264621735,
      "learning_rate": 0.00019139911634756998,
      "loss": 0.3241,
      "step": 151
    },
    {
      "epoch": 0.04470588235294118,
      "grad_norm": 0.07456338405609131,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.3198,
      "step": 152
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.08957011252641678,
      "learning_rate": 0.00019128129602356407,
      "loss": 0.421,
      "step": 153
    },
    {
      "epoch": 0.045294117647058825,
      "grad_norm": 0.08492514491081238,
      "learning_rate": 0.00019122238586156113,
      "loss": 0.4126,
      "step": 154
    },
    {
      "epoch": 0.045588235294117645,
      "grad_norm": 0.07395157963037491,
      "learning_rate": 0.0001911634756995582,
      "loss": 0.37,
      "step": 155
    },
    {
      "epoch": 0.04588235294117647,
      "grad_norm": 0.052728813141584396,
      "learning_rate": 0.00019110456553755525,
      "loss": 0.3022,
      "step": 156
    },
    {
      "epoch": 0.04617647058823529,
      "grad_norm": 0.09481725841760635,
      "learning_rate": 0.00019104565537555231,
      "loss": 0.472,
      "step": 157
    },
    {
      "epoch": 0.04647058823529412,
      "grad_norm": 0.07792573422193527,
      "learning_rate": 0.00019098674521354935,
      "loss": 0.3787,
      "step": 158
    },
    {
      "epoch": 0.046764705882352944,
      "grad_norm": 0.10479330271482468,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.4218,
      "step": 159
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 0.0658794417977333,
      "learning_rate": 0.00019086892488954347,
      "loss": 0.2732,
      "step": 160
    },
    {
      "epoch": 0.04735294117647059,
      "grad_norm": 0.09008630365133286,
      "learning_rate": 0.00019081001472754053,
      "loss": 0.3636,
      "step": 161
    },
    {
      "epoch": 0.04764705882352941,
      "grad_norm": 0.08040700852870941,
      "learning_rate": 0.0001907511045655376,
      "loss": 0.3693,
      "step": 162
    },
    {
      "epoch": 0.04794117647058824,
      "grad_norm": 0.09139419347047806,
      "learning_rate": 0.00019069219440353462,
      "loss": 0.3655,
      "step": 163
    },
    {
      "epoch": 0.04823529411764706,
      "grad_norm": 0.09087029099464417,
      "learning_rate": 0.00019063328424153168,
      "loss": 0.3532,
      "step": 164
    },
    {
      "epoch": 0.04852941176470588,
      "grad_norm": 0.08937810361385345,
      "learning_rate": 0.00019057437407952871,
      "loss": 0.3484,
      "step": 165
    },
    {
      "epoch": 0.0488235294117647,
      "grad_norm": 0.10684864968061447,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.4195,
      "step": 166
    },
    {
      "epoch": 0.04911764705882353,
      "grad_norm": 0.07988648861646652,
      "learning_rate": 0.00019045655375552283,
      "loss": 0.3038,
      "step": 167
    },
    {
      "epoch": 0.04941176470588235,
      "grad_norm": 0.07970356941223145,
      "learning_rate": 0.00019039764359351987,
      "loss": 0.3355,
      "step": 168
    },
    {
      "epoch": 0.049705882352941176,
      "grad_norm": 0.07659484446048737,
      "learning_rate": 0.00019033873343151693,
      "loss": 0.3742,
      "step": 169
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.07002292573451996,
      "learning_rate": 0.000190279823269514,
      "loss": 0.3269,
      "step": 170
    },
    {
      "epoch": 0.05029411764705882,
      "grad_norm": 0.07644231617450714,
      "learning_rate": 0.00019022091310751105,
      "loss": 0.4137,
      "step": 171
    },
    {
      "epoch": 0.05058823529411765,
      "grad_norm": 0.08522560447454453,
      "learning_rate": 0.0001901620029455081,
      "loss": 0.3864,
      "step": 172
    },
    {
      "epoch": 0.05088235294117647,
      "grad_norm": 0.07500582933425903,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.3319,
      "step": 173
    },
    {
      "epoch": 0.051176470588235295,
      "grad_norm": 0.08164885640144348,
      "learning_rate": 0.0001900441826215022,
      "loss": 0.3337,
      "step": 174
    },
    {
      "epoch": 0.051470588235294115,
      "grad_norm": 0.09285160154104233,
      "learning_rate": 0.00018998527245949926,
      "loss": 0.4254,
      "step": 175
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 0.0819367915391922,
      "learning_rate": 0.00018992636229749632,
      "loss": 0.3257,
      "step": 176
    },
    {
      "epoch": 0.05205882352941176,
      "grad_norm": 0.12490145862102509,
      "learning_rate": 0.00018986745213549338,
      "loss": 0.3524,
      "step": 177
    },
    {
      "epoch": 0.05235294117647059,
      "grad_norm": 0.0866042822599411,
      "learning_rate": 0.00018980854197349042,
      "loss": 0.3386,
      "step": 178
    },
    {
      "epoch": 0.052647058823529415,
      "grad_norm": 0.07165395468473434,
      "learning_rate": 0.00018974963181148748,
      "loss": 0.3618,
      "step": 179
    },
    {
      "epoch": 0.052941176470588235,
      "grad_norm": 0.10758268088102341,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.3723,
      "step": 180
    },
    {
      "epoch": 0.05323529411764706,
      "grad_norm": 0.08238274604082108,
      "learning_rate": 0.0001896318114874816,
      "loss": 0.4296,
      "step": 181
    },
    {
      "epoch": 0.05352941176470588,
      "grad_norm": 0.07832297682762146,
      "learning_rate": 0.00018957290132547866,
      "loss": 0.3288,
      "step": 182
    },
    {
      "epoch": 0.05382352941176471,
      "grad_norm": 0.058846212923526764,
      "learning_rate": 0.0001895139911634757,
      "loss": 0.2637,
      "step": 183
    },
    {
      "epoch": 0.05411764705882353,
      "grad_norm": 0.07765717804431915,
      "learning_rate": 0.00018945508100147275,
      "loss": 0.3672,
      "step": 184
    },
    {
      "epoch": 0.054411764705882354,
      "grad_norm": 0.07941259443759918,
      "learning_rate": 0.0001893961708394698,
      "loss": 0.3371,
      "step": 185
    },
    {
      "epoch": 0.054705882352941174,
      "grad_norm": 0.08304019272327423,
      "learning_rate": 0.00018933726067746687,
      "loss": 0.3507,
      "step": 186
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.11205962300300598,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.3935,
      "step": 187
    },
    {
      "epoch": 0.05529411764705883,
      "grad_norm": 0.05942642316222191,
      "learning_rate": 0.00018921944035346096,
      "loss": 0.3451,
      "step": 188
    },
    {
      "epoch": 0.05558823529411765,
      "grad_norm": 0.06165124475955963,
      "learning_rate": 0.00018916053019145802,
      "loss": 0.2979,
      "step": 189
    },
    {
      "epoch": 0.05588235294117647,
      "grad_norm": 0.08855830132961273,
      "learning_rate": 0.00018910162002945508,
      "loss": 0.415,
      "step": 190
    },
    {
      "epoch": 0.05617647058823529,
      "grad_norm": 0.05826304852962494,
      "learning_rate": 0.00018904270986745214,
      "loss": 0.3196,
      "step": 191
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 0.06903307884931564,
      "learning_rate": 0.0001889837997054492,
      "loss": 0.4254,
      "step": 192
    },
    {
      "epoch": 0.05676470588235294,
      "grad_norm": 0.06660143285989761,
      "learning_rate": 0.00018892488954344624,
      "loss": 0.3819,
      "step": 193
    },
    {
      "epoch": 0.057058823529411766,
      "grad_norm": 0.06879907101392746,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.3708,
      "step": 194
    },
    {
      "epoch": 0.057352941176470586,
      "grad_norm": 0.0858893170952797,
      "learning_rate": 0.00018880706921944036,
      "loss": 0.488,
      "step": 195
    },
    {
      "epoch": 0.05764705882352941,
      "grad_norm": 0.08478183299303055,
      "learning_rate": 0.00018874815905743742,
      "loss": 0.4276,
      "step": 196
    },
    {
      "epoch": 0.05794117647058823,
      "grad_norm": 0.06848134100437164,
      "learning_rate": 0.00018868924889543448,
      "loss": 0.2908,
      "step": 197
    },
    {
      "epoch": 0.05823529411764706,
      "grad_norm": 0.0728461891412735,
      "learning_rate": 0.0001886303387334315,
      "loss": 0.372,
      "step": 198
    },
    {
      "epoch": 0.058529411764705885,
      "grad_norm": 0.06572681665420532,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3229,
      "step": 199
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.06704764813184738,
      "learning_rate": 0.00018851251840942563,
      "loss": 0.3441,
      "step": 200
    },
    {
      "epoch": 0.05911764705882353,
      "grad_norm": 0.08072269707918167,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.3864,
      "step": 201
    },
    {
      "epoch": 0.05941176470588235,
      "grad_norm": 0.06302585452795029,
      "learning_rate": 0.00018839469808541975,
      "loss": 0.3172,
      "step": 202
    },
    {
      "epoch": 0.05970588235294118,
      "grad_norm": 0.0621432401239872,
      "learning_rate": 0.00018833578792341678,
      "loss": 0.3834,
      "step": 203
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.052341435104608536,
      "learning_rate": 0.00018827687776141384,
      "loss": 0.2979,
      "step": 204
    },
    {
      "epoch": 0.060294117647058824,
      "grad_norm": 0.09799778461456299,
      "learning_rate": 0.0001882179675994109,
      "loss": 0.3965,
      "step": 205
    },
    {
      "epoch": 0.060588235294117644,
      "grad_norm": 0.054619040340185165,
      "learning_rate": 0.00018815905743740796,
      "loss": 0.3463,
      "step": 206
    },
    {
      "epoch": 0.06088235294117647,
      "grad_norm": 0.06466399878263474,
      "learning_rate": 0.00018810014727540502,
      "loss": 0.3271,
      "step": 207
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 0.0610072985291481,
      "learning_rate": 0.00018804123711340206,
      "loss": 0.3431,
      "step": 208
    },
    {
      "epoch": 0.06147058823529412,
      "grad_norm": 0.084291972219944,
      "learning_rate": 0.00018798232695139912,
      "loss": 0.3796,
      "step": 209
    },
    {
      "epoch": 0.061764705882352944,
      "grad_norm": 0.06195719167590141,
      "learning_rate": 0.00018792341678939618,
      "loss": 0.3384,
      "step": 210
    },
    {
      "epoch": 0.062058823529411763,
      "grad_norm": 0.08425376564264297,
      "learning_rate": 0.00018786450662739324,
      "loss": 0.4044,
      "step": 211
    },
    {
      "epoch": 0.06235294117647059,
      "grad_norm": 0.06337657570838928,
      "learning_rate": 0.0001878055964653903,
      "loss": 0.354,
      "step": 212
    },
    {
      "epoch": 0.06264705882352942,
      "grad_norm": 0.06619176268577576,
      "learning_rate": 0.00018774668630338733,
      "loss": 0.3842,
      "step": 213
    },
    {
      "epoch": 0.06294117647058824,
      "grad_norm": 0.0671001449227333,
      "learning_rate": 0.0001876877761413844,
      "loss": 0.3074,
      "step": 214
    },
    {
      "epoch": 0.06323529411764706,
      "grad_norm": 0.07938998937606812,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.3713,
      "step": 215
    },
    {
      "epoch": 0.06352941176470588,
      "grad_norm": 0.05267553776502609,
      "learning_rate": 0.0001875699558173785,
      "loss": 0.3206,
      "step": 216
    },
    {
      "epoch": 0.06382352941176471,
      "grad_norm": 0.06867070496082306,
      "learning_rate": 0.00018751104565537557,
      "loss": 0.2797,
      "step": 217
    },
    {
      "epoch": 0.06411764705882353,
      "grad_norm": 0.0672331377863884,
      "learning_rate": 0.0001874521354933726,
      "loss": 0.3487,
      "step": 218
    },
    {
      "epoch": 0.06441176470588235,
      "grad_norm": 0.0979737862944603,
      "learning_rate": 0.00018739322533136967,
      "loss": 0.3366,
      "step": 219
    },
    {
      "epoch": 0.06470588235294118,
      "grad_norm": 0.09713222831487656,
      "learning_rate": 0.00018733431516936673,
      "loss": 0.4014,
      "step": 220
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.09628748893737793,
      "learning_rate": 0.00018727540500736379,
      "loss": 0.3651,
      "step": 221
    },
    {
      "epoch": 0.06529411764705882,
      "grad_norm": 0.07174662500619888,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.3295,
      "step": 222
    },
    {
      "epoch": 0.06558823529411764,
      "grad_norm": 0.09166757762432098,
      "learning_rate": 0.00018715758468335788,
      "loss": 0.335,
      "step": 223
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 0.08839033544063568,
      "learning_rate": 0.00018709867452135494,
      "loss": 0.3159,
      "step": 224
    },
    {
      "epoch": 0.0661764705882353,
      "grad_norm": 0.08679421991109848,
      "learning_rate": 0.000187039764359352,
      "loss": 0.3906,
      "step": 225
    },
    {
      "epoch": 0.06647058823529411,
      "grad_norm": 0.07508322596549988,
      "learning_rate": 0.00018698085419734906,
      "loss": 0.3996,
      "step": 226
    },
    {
      "epoch": 0.06676470588235293,
      "grad_norm": 0.056456372141838074,
      "learning_rate": 0.00018692194403534612,
      "loss": 0.3694,
      "step": 227
    },
    {
      "epoch": 0.06705882352941177,
      "grad_norm": 0.05808485671877861,
      "learning_rate": 0.00018686303387334315,
      "loss": 0.3419,
      "step": 228
    },
    {
      "epoch": 0.06735294117647059,
      "grad_norm": 0.06492408365011215,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.2908,
      "step": 229
    },
    {
      "epoch": 0.06764705882352941,
      "grad_norm": 0.07471184432506561,
      "learning_rate": 0.00018674521354933727,
      "loss": 0.3712,
      "step": 230
    },
    {
      "epoch": 0.06794117647058824,
      "grad_norm": 0.08175772428512573,
      "learning_rate": 0.00018668630338733433,
      "loss": 0.3676,
      "step": 231
    },
    {
      "epoch": 0.06823529411764706,
      "grad_norm": 0.07734062522649765,
      "learning_rate": 0.0001866273932253314,
      "loss": 0.432,
      "step": 232
    },
    {
      "epoch": 0.06852941176470588,
      "grad_norm": 0.11309321969747543,
      "learning_rate": 0.00018656848306332843,
      "loss": 0.3868,
      "step": 233
    },
    {
      "epoch": 0.0688235294117647,
      "grad_norm": 0.07719901204109192,
      "learning_rate": 0.0001865095729013255,
      "loss": 0.3841,
      "step": 234
    },
    {
      "epoch": 0.06911764705882353,
      "grad_norm": 0.052105437964200974,
      "learning_rate": 0.00018645066273932255,
      "loss": 0.3381,
      "step": 235
    },
    {
      "epoch": 0.06941176470588235,
      "grad_norm": 0.0661512091755867,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.3807,
      "step": 236
    },
    {
      "epoch": 0.06970588235294117,
      "grad_norm": 0.07502448558807373,
      "learning_rate": 0.00018633284241531667,
      "loss": 0.3497,
      "step": 237
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.08837555348873138,
      "learning_rate": 0.0001862739322533137,
      "loss": 0.4959,
      "step": 238
    },
    {
      "epoch": 0.07029411764705883,
      "grad_norm": 0.08164649456739426,
      "learning_rate": 0.00018621502209131076,
      "loss": 0.372,
      "step": 239
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 0.07557537406682968,
      "learning_rate": 0.00018615611192930782,
      "loss": 0.4156,
      "step": 240
    },
    {
      "epoch": 0.07088235294117647,
      "grad_norm": 0.0747814029455185,
      "learning_rate": 0.00018609720176730488,
      "loss": 0.316,
      "step": 241
    },
    {
      "epoch": 0.0711764705882353,
      "grad_norm": 0.06791973859071732,
      "learning_rate": 0.00018603829160530194,
      "loss": 0.3607,
      "step": 242
    },
    {
      "epoch": 0.07147058823529412,
      "grad_norm": 0.08322137594223022,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.3926,
      "step": 243
    },
    {
      "epoch": 0.07176470588235294,
      "grad_norm": 0.067705437541008,
      "learning_rate": 0.00018592047128129603,
      "loss": 0.394,
      "step": 244
    },
    {
      "epoch": 0.07205882352941176,
      "grad_norm": 0.08154399693012238,
      "learning_rate": 0.0001858615611192931,
      "loss": 0.3632,
      "step": 245
    },
    {
      "epoch": 0.07235294117647059,
      "grad_norm": 0.10853835195302963,
      "learning_rate": 0.00018580265095729015,
      "loss": 0.4394,
      "step": 246
    },
    {
      "epoch": 0.07264705882352941,
      "grad_norm": 0.09938590973615646,
      "learning_rate": 0.00018574374079528721,
      "loss": 0.4557,
      "step": 247
    },
    {
      "epoch": 0.07294117647058823,
      "grad_norm": 0.06192362681031227,
      "learning_rate": 0.00018568483063328425,
      "loss": 0.3553,
      "step": 248
    },
    {
      "epoch": 0.07323529411764707,
      "grad_norm": 0.09583700448274612,
      "learning_rate": 0.0001856259204712813,
      "loss": 0.3844,
      "step": 249
    },
    {
      "epoch": 0.07352941176470588,
      "grad_norm": 0.10403881222009659,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.4756,
      "step": 250
    },
    {
      "epoch": 0.0738235294117647,
      "grad_norm": 0.05601095035672188,
      "learning_rate": 0.00018550810014727543,
      "loss": 0.2973,
      "step": 251
    },
    {
      "epoch": 0.07411764705882352,
      "grad_norm": 0.06452985852956772,
      "learning_rate": 0.0001854491899852725,
      "loss": 0.3872,
      "step": 252
    },
    {
      "epoch": 0.07441176470588236,
      "grad_norm": 0.06883833557367325,
      "learning_rate": 0.00018539027982326952,
      "loss": 0.3115,
      "step": 253
    },
    {
      "epoch": 0.07470588235294118,
      "grad_norm": 0.06155490130186081,
      "learning_rate": 0.00018533136966126658,
      "loss": 0.3156,
      "step": 254
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.08306905627250671,
      "learning_rate": 0.00018527245949926364,
      "loss": 0.4543,
      "step": 255
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 0.06061279773712158,
      "learning_rate": 0.0001852135493372607,
      "loss": 0.3221,
      "step": 256
    },
    {
      "epoch": 0.07558823529411765,
      "grad_norm": 0.04823104664683342,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.2969,
      "step": 257
    },
    {
      "epoch": 0.07588235294117647,
      "grad_norm": 0.07310818135738373,
      "learning_rate": 0.0001850957290132548,
      "loss": 0.365,
      "step": 258
    },
    {
      "epoch": 0.07617647058823529,
      "grad_norm": 0.08841561526060104,
      "learning_rate": 0.00018503681885125186,
      "loss": 0.381,
      "step": 259
    },
    {
      "epoch": 0.07647058823529412,
      "grad_norm": 0.06944283097982407,
      "learning_rate": 0.00018497790868924892,
      "loss": 0.4261,
      "step": 260
    },
    {
      "epoch": 0.07676470588235294,
      "grad_norm": 0.07298435270786285,
      "learning_rate": 0.00018491899852724598,
      "loss": 0.4459,
      "step": 261
    },
    {
      "epoch": 0.07705882352941176,
      "grad_norm": 0.09518639743328094,
      "learning_rate": 0.00018486008836524304,
      "loss": 0.4657,
      "step": 262
    },
    {
      "epoch": 0.07735294117647058,
      "grad_norm": 0.05629313364624977,
      "learning_rate": 0.00018480117820324007,
      "loss": 0.3351,
      "step": 263
    },
    {
      "epoch": 0.07764705882352942,
      "grad_norm": 0.07238731533288956,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.3908,
      "step": 264
    },
    {
      "epoch": 0.07794117647058824,
      "grad_norm": 0.08914436399936676,
      "learning_rate": 0.0001846833578792342,
      "loss": 0.3806,
      "step": 265
    },
    {
      "epoch": 0.07823529411764706,
      "grad_norm": 0.0776228979229927,
      "learning_rate": 0.00018462444771723125,
      "loss": 0.4126,
      "step": 266
    },
    {
      "epoch": 0.07852941176470589,
      "grad_norm": 0.09864117205142975,
      "learning_rate": 0.0001845655375552283,
      "loss": 0.4262,
      "step": 267
    },
    {
      "epoch": 0.07882352941176471,
      "grad_norm": 0.08066605776548386,
      "learning_rate": 0.00018450662739322534,
      "loss": 0.3962,
      "step": 268
    },
    {
      "epoch": 0.07911764705882353,
      "grad_norm": 0.07323722541332245,
      "learning_rate": 0.0001844477172312224,
      "loss": 0.3747,
      "step": 269
    },
    {
      "epoch": 0.07941176470588235,
      "grad_norm": 0.09612224251031876,
      "learning_rate": 0.00018438880706921946,
      "loss": 0.382,
      "step": 270
    },
    {
      "epoch": 0.07970588235294118,
      "grad_norm": 0.08757799118757248,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.4167,
      "step": 271
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.06094774603843689,
      "learning_rate": 0.00018427098674521356,
      "loss": 0.2908,
      "step": 272
    },
    {
      "epoch": 0.08029411764705882,
      "grad_norm": 0.07896409183740616,
      "learning_rate": 0.0001842120765832106,
      "loss": 0.3249,
      "step": 273
    },
    {
      "epoch": 0.08058823529411764,
      "grad_norm": 0.09363578259944916,
      "learning_rate": 0.00018415316642120765,
      "loss": 0.3697,
      "step": 274
    },
    {
      "epoch": 0.08088235294117647,
      "grad_norm": 0.07927625626325607,
      "learning_rate": 0.0001840942562592047,
      "loss": 0.3971,
      "step": 275
    },
    {
      "epoch": 0.0811764705882353,
      "grad_norm": 0.08030544966459274,
      "learning_rate": 0.00018403534609720177,
      "loss": 0.3374,
      "step": 276
    },
    {
      "epoch": 0.08147058823529411,
      "grad_norm": 0.07672452181577682,
      "learning_rate": 0.00018397643593519883,
      "loss": 0.3978,
      "step": 277
    },
    {
      "epoch": 0.08176470588235295,
      "grad_norm": 0.06783337891101837,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.3899,
      "step": 278
    },
    {
      "epoch": 0.08205882352941177,
      "grad_norm": 0.07583273202180862,
      "learning_rate": 0.00018385861561119292,
      "loss": 0.4009,
      "step": 279
    },
    {
      "epoch": 0.08235294117647059,
      "grad_norm": 0.08617544919252396,
      "learning_rate": 0.00018379970544918998,
      "loss": 0.4353,
      "step": 280
    },
    {
      "epoch": 0.0826470588235294,
      "grad_norm": 0.09015573561191559,
      "learning_rate": 0.00018374079528718704,
      "loss": 0.4554,
      "step": 281
    },
    {
      "epoch": 0.08294117647058824,
      "grad_norm": 0.09381821006536484,
      "learning_rate": 0.0001836818851251841,
      "loss": 0.4542,
      "step": 282
    },
    {
      "epoch": 0.08323529411764706,
      "grad_norm": 0.059960540384054184,
      "learning_rate": 0.00018362297496318114,
      "loss": 0.394,
      "step": 283
    },
    {
      "epoch": 0.08352941176470588,
      "grad_norm": 0.05992317572236061,
      "learning_rate": 0.0001835640648011782,
      "loss": 0.3182,
      "step": 284
    },
    {
      "epoch": 0.0838235294117647,
      "grad_norm": 0.06755014508962631,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.3384,
      "step": 285
    },
    {
      "epoch": 0.08411764705882353,
      "grad_norm": 0.05742734670639038,
      "learning_rate": 0.00018344624447717232,
      "loss": 0.328,
      "step": 286
    },
    {
      "epoch": 0.08441176470588235,
      "grad_norm": 0.07704940438270569,
      "learning_rate": 0.00018338733431516938,
      "loss": 0.4231,
      "step": 287
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 0.06226291507482529,
      "learning_rate": 0.0001833284241531664,
      "loss": 0.3478,
      "step": 288
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.06497034430503845,
      "learning_rate": 0.00018326951399116347,
      "loss": 0.4152,
      "step": 289
    },
    {
      "epoch": 0.08529411764705883,
      "grad_norm": 0.0818169042468071,
      "learning_rate": 0.00018321060382916053,
      "loss": 0.4429,
      "step": 290
    },
    {
      "epoch": 0.08558823529411765,
      "grad_norm": 0.053118690848350525,
      "learning_rate": 0.0001831516936671576,
      "loss": 0.2825,
      "step": 291
    },
    {
      "epoch": 0.08588235294117647,
      "grad_norm": 0.10339044779539108,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.4807,
      "step": 292
    },
    {
      "epoch": 0.0861764705882353,
      "grad_norm": 0.06001389026641846,
      "learning_rate": 0.00018303387334315168,
      "loss": 0.3856,
      "step": 293
    },
    {
      "epoch": 0.08647058823529412,
      "grad_norm": 0.0635848417878151,
      "learning_rate": 0.00018297496318114874,
      "loss": 0.3795,
      "step": 294
    },
    {
      "epoch": 0.08676470588235294,
      "grad_norm": 0.06537123769521713,
      "learning_rate": 0.0001829160530191458,
      "loss": 0.3689,
      "step": 295
    },
    {
      "epoch": 0.08705882352941176,
      "grad_norm": 0.06368153542280197,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.4154,
      "step": 296
    },
    {
      "epoch": 0.08735294117647059,
      "grad_norm": 0.06571908295154572,
      "learning_rate": 0.00018279823269513992,
      "loss": 0.3549,
      "step": 297
    },
    {
      "epoch": 0.08764705882352941,
      "grad_norm": 0.07359504699707031,
      "learning_rate": 0.00018273932253313696,
      "loss": 0.3488,
      "step": 298
    },
    {
      "epoch": 0.08794117647058823,
      "grad_norm": 0.06670806556940079,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.3614,
      "step": 299
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.06990577280521393,
      "learning_rate": 0.00018262150220913108,
      "loss": 0.4047,
      "step": 300
    },
    {
      "epoch": 0.08852941176470588,
      "grad_norm": 0.05055892840027809,
      "learning_rate": 0.00018256259204712814,
      "loss": 0.294,
      "step": 301
    },
    {
      "epoch": 0.0888235294117647,
      "grad_norm": 0.06855972856283188,
      "learning_rate": 0.0001825036818851252,
      "loss": 0.3903,
      "step": 302
    },
    {
      "epoch": 0.08911764705882352,
      "grad_norm": 0.07813625782728195,
      "learning_rate": 0.00018244477172312223,
      "loss": 0.3862,
      "step": 303
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 0.0653594508767128,
      "learning_rate": 0.0001823858615611193,
      "loss": 0.3646,
      "step": 304
    },
    {
      "epoch": 0.08970588235294118,
      "grad_norm": 0.06089194864034653,
      "learning_rate": 0.00018232695139911635,
      "loss": 0.3923,
      "step": 305
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.053997382521629333,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.3208,
      "step": 306
    },
    {
      "epoch": 0.09029411764705883,
      "grad_norm": 0.07740602642297745,
      "learning_rate": 0.00018220913107511047,
      "loss": 0.3631,
      "step": 307
    },
    {
      "epoch": 0.09058823529411765,
      "grad_norm": 0.07099450379610062,
      "learning_rate": 0.0001821502209131075,
      "loss": 0.3599,
      "step": 308
    },
    {
      "epoch": 0.09088235294117647,
      "grad_norm": 0.07136023789644241,
      "learning_rate": 0.00018209131075110457,
      "loss": 0.3771,
      "step": 309
    },
    {
      "epoch": 0.09117647058823529,
      "grad_norm": 0.05170866474509239,
      "learning_rate": 0.00018203240058910163,
      "loss": 0.3335,
      "step": 310
    },
    {
      "epoch": 0.09147058823529412,
      "grad_norm": 0.07615814357995987,
      "learning_rate": 0.00018197349042709869,
      "loss": 0.4079,
      "step": 311
    },
    {
      "epoch": 0.09176470588235294,
      "grad_norm": 0.06258656084537506,
      "learning_rate": 0.00018191458026509575,
      "loss": 0.3571,
      "step": 312
    },
    {
      "epoch": 0.09205882352941176,
      "grad_norm": 0.07436079531908035,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.402,
      "step": 313
    },
    {
      "epoch": 0.09235294117647058,
      "grad_norm": 0.053074728697538376,
      "learning_rate": 0.00018179675994108984,
      "loss": 0.3659,
      "step": 314
    },
    {
      "epoch": 0.09264705882352942,
      "grad_norm": 0.06488879024982452,
      "learning_rate": 0.0001817378497790869,
      "loss": 0.345,
      "step": 315
    },
    {
      "epoch": 0.09294117647058824,
      "grad_norm": 0.060727816075086594,
      "learning_rate": 0.00018167893961708396,
      "loss": 0.3886,
      "step": 316
    },
    {
      "epoch": 0.09323529411764706,
      "grad_norm": 0.08009360730648041,
      "learning_rate": 0.00018162002945508102,
      "loss": 0.2971,
      "step": 317
    },
    {
      "epoch": 0.09352941176470589,
      "grad_norm": 0.05685359612107277,
      "learning_rate": 0.00018156111929307805,
      "loss": 0.3106,
      "step": 318
    },
    {
      "epoch": 0.09382352941176471,
      "grad_norm": 0.08681752532720566,
      "learning_rate": 0.0001815022091310751,
      "loss": 0.4174,
      "step": 319
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.05234995111823082,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.3066,
      "step": 320
    },
    {
      "epoch": 0.09441176470588235,
      "grad_norm": 0.06165998801589012,
      "learning_rate": 0.00018138438880706923,
      "loss": 0.3306,
      "step": 321
    },
    {
      "epoch": 0.09470588235294118,
      "grad_norm": 0.09200310707092285,
      "learning_rate": 0.0001813254786450663,
      "loss": 0.4439,
      "step": 322
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06489979475736618,
      "learning_rate": 0.00018126656848306333,
      "loss": 0.3442,
      "step": 323
    },
    {
      "epoch": 0.09529411764705882,
      "grad_norm": 0.05849449336528778,
      "learning_rate": 0.00018120765832106039,
      "loss": 0.348,
      "step": 324
    },
    {
      "epoch": 0.09558823529411764,
      "grad_norm": 0.07316594570875168,
      "learning_rate": 0.00018114874815905745,
      "loss": 0.3457,
      "step": 325
    },
    {
      "epoch": 0.09588235294117647,
      "grad_norm": 0.08434084802865982,
      "learning_rate": 0.0001810898379970545,
      "loss": 0.4089,
      "step": 326
    },
    {
      "epoch": 0.0961764705882353,
      "grad_norm": 0.061086054891347885,
      "learning_rate": 0.00018103092783505157,
      "loss": 0.3714,
      "step": 327
    },
    {
      "epoch": 0.09647058823529411,
      "grad_norm": 0.0624801442027092,
      "learning_rate": 0.0001809720176730486,
      "loss": 0.3219,
      "step": 328
    },
    {
      "epoch": 0.09676470588235295,
      "grad_norm": 0.06434611231088638,
      "learning_rate": 0.00018091310751104566,
      "loss": 0.3222,
      "step": 329
    },
    {
      "epoch": 0.09705882352941177,
      "grad_norm": 0.06464701145887375,
      "learning_rate": 0.00018085419734904272,
      "loss": 0.3103,
      "step": 330
    },
    {
      "epoch": 0.09735294117647059,
      "grad_norm": 0.06848642975091934,
      "learning_rate": 0.00018079528718703978,
      "loss": 0.3865,
      "step": 331
    },
    {
      "epoch": 0.0976470588235294,
      "grad_norm": 0.06561613827943802,
      "learning_rate": 0.00018073637702503684,
      "loss": 0.3578,
      "step": 332
    },
    {
      "epoch": 0.09794117647058824,
      "grad_norm": 0.05055467411875725,
      "learning_rate": 0.00018067746686303387,
      "loss": 0.3225,
      "step": 333
    },
    {
      "epoch": 0.09823529411764706,
      "grad_norm": 0.07373936474323273,
      "learning_rate": 0.00018061855670103093,
      "loss": 0.3922,
      "step": 334
    },
    {
      "epoch": 0.09852941176470588,
      "grad_norm": 0.06367476284503937,
      "learning_rate": 0.000180559646539028,
      "loss": 0.3685,
      "step": 335
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 0.058183781802654266,
      "learning_rate": 0.00018050073637702505,
      "loss": 0.3361,
      "step": 336
    },
    {
      "epoch": 0.09911764705882353,
      "grad_norm": 0.061347659677267075,
      "learning_rate": 0.00018044182621502211,
      "loss": 0.4039,
      "step": 337
    },
    {
      "epoch": 0.09941176470588235,
      "grad_norm": 0.06506085395812988,
      "learning_rate": 0.00018038291605301915,
      "loss": 0.3489,
      "step": 338
    },
    {
      "epoch": 0.09970588235294117,
      "grad_norm": 0.04899275302886963,
      "learning_rate": 0.0001803240058910162,
      "loss": 0.3151,
      "step": 339
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.07107077538967133,
      "learning_rate": 0.00018026509572901327,
      "loss": 0.4117,
      "step": 340
    },
    {
      "epoch": 0.10029411764705883,
      "grad_norm": 0.06344206631183624,
      "learning_rate": 0.00018020618556701033,
      "loss": 0.3668,
      "step": 341
    },
    {
      "epoch": 0.10058823529411764,
      "grad_norm": 0.07463090121746063,
      "learning_rate": 0.0001801472754050074,
      "loss": 0.3728,
      "step": 342
    },
    {
      "epoch": 0.10088235294117646,
      "grad_norm": 0.06196259707212448,
      "learning_rate": 0.00018008836524300442,
      "loss": 0.3402,
      "step": 343
    },
    {
      "epoch": 0.1011764705882353,
      "grad_norm": 0.07231795787811279,
      "learning_rate": 0.00018002945508100148,
      "loss": 0.4009,
      "step": 344
    },
    {
      "epoch": 0.10147058823529412,
      "grad_norm": 0.07656300067901611,
      "learning_rate": 0.00017997054491899854,
      "loss": 0.3672,
      "step": 345
    },
    {
      "epoch": 0.10176470588235294,
      "grad_norm": 0.0582057349383831,
      "learning_rate": 0.0001799116347569956,
      "loss": 0.4122,
      "step": 346
    },
    {
      "epoch": 0.10205882352941177,
      "grad_norm": 0.06444188952445984,
      "learning_rate": 0.00017985272459499266,
      "loss": 0.3125,
      "step": 347
    },
    {
      "epoch": 0.10235294117647059,
      "grad_norm": 0.07201991975307465,
      "learning_rate": 0.0001797938144329897,
      "loss": 0.3101,
      "step": 348
    },
    {
      "epoch": 0.10264705882352941,
      "grad_norm": 0.06943051517009735,
      "learning_rate": 0.00017973490427098675,
      "loss": 0.3428,
      "step": 349
    },
    {
      "epoch": 0.10294117647058823,
      "grad_norm": 0.05349080264568329,
      "learning_rate": 0.00017967599410898382,
      "loss": 0.3251,
      "step": 350
    },
    {
      "epoch": 0.10323529411764706,
      "grad_norm": 0.05618695169687271,
      "learning_rate": 0.00017961708394698088,
      "loss": 0.317,
      "step": 351
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 0.05786282941699028,
      "learning_rate": 0.00017955817378497794,
      "loss": 0.3418,
      "step": 352
    },
    {
      "epoch": 0.1038235294117647,
      "grad_norm": 0.07092132419347763,
      "learning_rate": 0.00017949926362297497,
      "loss": 0.3658,
      "step": 353
    },
    {
      "epoch": 0.10411764705882352,
      "grad_norm": 0.07410195469856262,
      "learning_rate": 0.00017944035346097203,
      "loss": 0.4483,
      "step": 354
    },
    {
      "epoch": 0.10441176470588236,
      "grad_norm": 0.07642927020788193,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4236,
      "step": 355
    },
    {
      "epoch": 0.10470588235294118,
      "grad_norm": 0.09109827876091003,
      "learning_rate": 0.00017932253313696615,
      "loss": 0.4819,
      "step": 356
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06083108112215996,
      "learning_rate": 0.0001792636229749632,
      "loss": 0.3593,
      "step": 357
    },
    {
      "epoch": 0.10529411764705883,
      "grad_norm": 0.05710162967443466,
      "learning_rate": 0.00017920471281296024,
      "loss": 0.3548,
      "step": 358
    },
    {
      "epoch": 0.10558823529411765,
      "grad_norm": 0.06898236274719238,
      "learning_rate": 0.0001791458026509573,
      "loss": 0.4279,
      "step": 359
    },
    {
      "epoch": 0.10588235294117647,
      "grad_norm": 0.06828377395868301,
      "learning_rate": 0.00017908689248895436,
      "loss": 0.4319,
      "step": 360
    },
    {
      "epoch": 0.10617647058823529,
      "grad_norm": 0.07263288646936417,
      "learning_rate": 0.00017902798232695142,
      "loss": 0.4207,
      "step": 361
    },
    {
      "epoch": 0.10647058823529412,
      "grad_norm": 0.07407941669225693,
      "learning_rate": 0.00017896907216494848,
      "loss": 0.4381,
      "step": 362
    },
    {
      "epoch": 0.10676470588235294,
      "grad_norm": 0.07479868084192276,
      "learning_rate": 0.00017891016200294552,
      "loss": 0.3792,
      "step": 363
    },
    {
      "epoch": 0.10705882352941176,
      "grad_norm": 0.06474834680557251,
      "learning_rate": 0.00017885125184094258,
      "loss": 0.3294,
      "step": 364
    },
    {
      "epoch": 0.10735294117647058,
      "grad_norm": 0.05806325748562813,
      "learning_rate": 0.00017879234167893964,
      "loss": 0.3455,
      "step": 365
    },
    {
      "epoch": 0.10764705882352942,
      "grad_norm": 0.08173426985740662,
      "learning_rate": 0.0001787334315169367,
      "loss": 0.3826,
      "step": 366
    },
    {
      "epoch": 0.10794117647058823,
      "grad_norm": 0.06613181531429291,
      "learning_rate": 0.00017867452135493376,
      "loss": 0.3342,
      "step": 367
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 0.08808574080467224,
      "learning_rate": 0.0001786156111929308,
      "loss": 0.4492,
      "step": 368
    },
    {
      "epoch": 0.10852941176470589,
      "grad_norm": 0.06901829689741135,
      "learning_rate": 0.00017855670103092785,
      "loss": 0.4216,
      "step": 369
    },
    {
      "epoch": 0.10882352941176471,
      "grad_norm": 0.07953732460737228,
      "learning_rate": 0.0001784977908689249,
      "loss": 0.4006,
      "step": 370
    },
    {
      "epoch": 0.10911764705882353,
      "grad_norm": 0.07308214157819748,
      "learning_rate": 0.00017843888070692197,
      "loss": 0.3644,
      "step": 371
    },
    {
      "epoch": 0.10941176470588235,
      "grad_norm": 0.0722666010260582,
      "learning_rate": 0.00017837997054491903,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.10970588235294118,
      "grad_norm": 0.06988727301359177,
      "learning_rate": 0.00017832106038291606,
      "loss": 0.3816,
      "step": 373
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.062480855733156204,
      "learning_rate": 0.00017826215022091312,
      "loss": 0.3408,
      "step": 374
    },
    {
      "epoch": 0.11029411764705882,
      "grad_norm": 0.07396873086690903,
      "learning_rate": 0.00017820324005891018,
      "loss": 0.3718,
      "step": 375
    },
    {
      "epoch": 0.11058823529411765,
      "grad_norm": 0.045270275324583054,
      "learning_rate": 0.00017814432989690724,
      "loss": 0.2581,
      "step": 376
    },
    {
      "epoch": 0.11088235294117647,
      "grad_norm": 0.05783374607563019,
      "learning_rate": 0.00017808541973490428,
      "loss": 0.3824,
      "step": 377
    },
    {
      "epoch": 0.1111764705882353,
      "grad_norm": 0.09262153506278992,
      "learning_rate": 0.0001780265095729013,
      "loss": 0.3936,
      "step": 378
    },
    {
      "epoch": 0.11147058823529411,
      "grad_norm": 0.07748464494943619,
      "learning_rate": 0.00017796759941089837,
      "loss": 0.3862,
      "step": 379
    },
    {
      "epoch": 0.11176470588235295,
      "grad_norm": 0.047957275062799454,
      "learning_rate": 0.00017790868924889543,
      "loss": 0.3173,
      "step": 380
    },
    {
      "epoch": 0.11205882352941177,
      "grad_norm": 0.05351791903376579,
      "learning_rate": 0.0001778497790868925,
      "loss": 0.3197,
      "step": 381
    },
    {
      "epoch": 0.11235294117647059,
      "grad_norm": 0.06672481447458267,
      "learning_rate": 0.00017779086892488955,
      "loss": 0.367,
      "step": 382
    },
    {
      "epoch": 0.1126470588235294,
      "grad_norm": 0.0817033052444458,
      "learning_rate": 0.00017773195876288658,
      "loss": 0.3801,
      "step": 383
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.05897071585059166,
      "learning_rate": 0.00017767304860088364,
      "loss": 0.3172,
      "step": 384
    },
    {
      "epoch": 0.11323529411764706,
      "grad_norm": 0.06713476777076721,
      "learning_rate": 0.0001776141384388807,
      "loss": 0.4544,
      "step": 385
    },
    {
      "epoch": 0.11352941176470588,
      "grad_norm": 0.06568536907434464,
      "learning_rate": 0.00017755522827687776,
      "loss": 0.321,
      "step": 386
    },
    {
      "epoch": 0.11382352941176471,
      "grad_norm": 0.05591333657503128,
      "learning_rate": 0.00017749631811487482,
      "loss": 0.3195,
      "step": 387
    },
    {
      "epoch": 0.11411764705882353,
      "grad_norm": 0.06133725121617317,
      "learning_rate": 0.00017743740795287186,
      "loss": 0.3934,
      "step": 388
    },
    {
      "epoch": 0.11441176470588235,
      "grad_norm": 0.05600662901997566,
      "learning_rate": 0.00017737849779086892,
      "loss": 0.3227,
      "step": 389
    },
    {
      "epoch": 0.11470588235294117,
      "grad_norm": 0.052532583475112915,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.3207,
      "step": 390
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.0512576550245285,
      "learning_rate": 0.00017726067746686304,
      "loss": 0.3087,
      "step": 391
    },
    {
      "epoch": 0.11529411764705882,
      "grad_norm": 0.05669146031141281,
      "learning_rate": 0.0001772017673048601,
      "loss": 0.3183,
      "step": 392
    },
    {
      "epoch": 0.11558823529411764,
      "grad_norm": 0.06597939878702164,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3809,
      "step": 393
    },
    {
      "epoch": 0.11588235294117646,
      "grad_norm": 0.059711527079343796,
      "learning_rate": 0.0001770839469808542,
      "loss": 0.3867,
      "step": 394
    },
    {
      "epoch": 0.1161764705882353,
      "grad_norm": 0.08614617586135864,
      "learning_rate": 0.00017702503681885125,
      "loss": 0.4372,
      "step": 395
    },
    {
      "epoch": 0.11647058823529412,
      "grad_norm": 0.08026904612779617,
      "learning_rate": 0.0001769661266568483,
      "loss": 0.4646,
      "step": 396
    },
    {
      "epoch": 0.11676470588235294,
      "grad_norm": 0.05460386350750923,
      "learning_rate": 0.00017690721649484537,
      "loss": 0.3232,
      "step": 397
    },
    {
      "epoch": 0.11705882352941177,
      "grad_norm": 0.06595972180366516,
      "learning_rate": 0.0001768483063328424,
      "loss": 0.3641,
      "step": 398
    },
    {
      "epoch": 0.11735294117647059,
      "grad_norm": 0.0833452045917511,
      "learning_rate": 0.00017678939617083947,
      "loss": 0.3866,
      "step": 399
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.062220629304647446,
      "learning_rate": 0.00017673048600883653,
      "loss": 0.3576,
      "step": 400
    },
    {
      "epoch": 0.11794117647058823,
      "grad_norm": 0.08257558196783066,
      "learning_rate": 0.00017667157584683359,
      "loss": 0.4615,
      "step": 401
    },
    {
      "epoch": 0.11823529411764706,
      "grad_norm": 0.07315663993358612,
      "learning_rate": 0.00017661266568483065,
      "loss": 0.4741,
      "step": 402
    },
    {
      "epoch": 0.11852941176470588,
      "grad_norm": 0.07738703489303589,
      "learning_rate": 0.00017655375552282768,
      "loss": 0.3537,
      "step": 403
    },
    {
      "epoch": 0.1188235294117647,
      "grad_norm": 0.06754371523857117,
      "learning_rate": 0.00017649484536082474,
      "loss": 0.336,
      "step": 404
    },
    {
      "epoch": 0.11911764705882352,
      "grad_norm": 0.06247233599424362,
      "learning_rate": 0.0001764359351988218,
      "loss": 0.3823,
      "step": 405
    },
    {
      "epoch": 0.11941176470588236,
      "grad_norm": 0.077308289706707,
      "learning_rate": 0.00017637702503681886,
      "loss": 0.3849,
      "step": 406
    },
    {
      "epoch": 0.11970588235294118,
      "grad_norm": 0.05560912936925888,
      "learning_rate": 0.00017631811487481592,
      "loss": 0.331,
      "step": 407
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07612839341163635,
      "learning_rate": 0.00017625920471281295,
      "loss": 0.4348,
      "step": 408
    },
    {
      "epoch": 0.12029411764705883,
      "grad_norm": 0.0655772015452385,
      "learning_rate": 0.00017620029455081,
      "loss": 0.3628,
      "step": 409
    },
    {
      "epoch": 0.12058823529411765,
      "grad_norm": 0.0807056576013565,
      "learning_rate": 0.00017614138438880707,
      "loss": 0.4177,
      "step": 410
    },
    {
      "epoch": 0.12088235294117647,
      "grad_norm": 0.06848520785570145,
      "learning_rate": 0.00017608247422680413,
      "loss": 0.39,
      "step": 411
    },
    {
      "epoch": 0.12117647058823529,
      "grad_norm": 0.05728285014629364,
      "learning_rate": 0.0001760235640648012,
      "loss": 0.3723,
      "step": 412
    },
    {
      "epoch": 0.12147058823529412,
      "grad_norm": 0.05980508774518967,
      "learning_rate": 0.00017596465390279823,
      "loss": 0.4077,
      "step": 413
    },
    {
      "epoch": 0.12176470588235294,
      "grad_norm": 0.04307478293776512,
      "learning_rate": 0.00017590574374079529,
      "loss": 0.3021,
      "step": 414
    },
    {
      "epoch": 0.12205882352941176,
      "grad_norm": 0.07224758714437485,
      "learning_rate": 0.00017584683357879235,
      "loss": 0.3899,
      "step": 415
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 0.060470569878816605,
      "learning_rate": 0.0001757879234167894,
      "loss": 0.313,
      "step": 416
    },
    {
      "epoch": 0.12264705882352941,
      "grad_norm": 0.08347459137439728,
      "learning_rate": 0.00017572901325478647,
      "loss": 0.4183,
      "step": 417
    },
    {
      "epoch": 0.12294117647058823,
      "grad_norm": 0.05439579114317894,
      "learning_rate": 0.0001756701030927835,
      "loss": 0.3214,
      "step": 418
    },
    {
      "epoch": 0.12323529411764705,
      "grad_norm": 0.056708924472332,
      "learning_rate": 0.00017561119293078056,
      "loss": 0.3282,
      "step": 419
    },
    {
      "epoch": 0.12352941176470589,
      "grad_norm": 0.0986727923154831,
      "learning_rate": 0.00017555228276877762,
      "loss": 0.3043,
      "step": 420
    },
    {
      "epoch": 0.12382352941176471,
      "grad_norm": 0.0913943499326706,
      "learning_rate": 0.00017549337260677468,
      "loss": 0.4343,
      "step": 421
    },
    {
      "epoch": 0.12411764705882353,
      "grad_norm": 0.05140068382024765,
      "learning_rate": 0.00017543446244477174,
      "loss": 0.3159,
      "step": 422
    },
    {
      "epoch": 0.12441176470588235,
      "grad_norm": 0.08153297007083893,
      "learning_rate": 0.00017537555228276877,
      "loss": 0.3605,
      "step": 423
    },
    {
      "epoch": 0.12470588235294118,
      "grad_norm": 0.08590778708457947,
      "learning_rate": 0.00017531664212076583,
      "loss": 0.4171,
      "step": 424
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06797812134027481,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4094,
      "step": 425
    },
    {
      "epoch": 0.12529411764705883,
      "grad_norm": 0.07034395635128021,
      "learning_rate": 0.00017519882179675995,
      "loss": 0.4094,
      "step": 426
    },
    {
      "epoch": 0.12558823529411764,
      "grad_norm": 0.06863990426063538,
      "learning_rate": 0.00017513991163475701,
      "loss": 0.3671,
      "step": 427
    },
    {
      "epoch": 0.12588235294117647,
      "grad_norm": 0.05765393748879433,
      "learning_rate": 0.00017508100147275405,
      "loss": 0.3131,
      "step": 428
    },
    {
      "epoch": 0.1261764705882353,
      "grad_norm": 0.05362609773874283,
      "learning_rate": 0.0001750220913107511,
      "loss": 0.2999,
      "step": 429
    },
    {
      "epoch": 0.1264705882352941,
      "grad_norm": 0.08066284656524658,
      "learning_rate": 0.00017496318114874817,
      "loss": 0.3869,
      "step": 430
    },
    {
      "epoch": 0.12676470588235295,
      "grad_norm": 0.06270390748977661,
      "learning_rate": 0.00017490427098674523,
      "loss": 0.3278,
      "step": 431
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 0.07776700705289841,
      "learning_rate": 0.0001748453608247423,
      "loss": 0.4157,
      "step": 432
    },
    {
      "epoch": 0.12735294117647059,
      "grad_norm": 0.06161027401685715,
      "learning_rate": 0.00017478645066273932,
      "loss": 0.3204,
      "step": 433
    },
    {
      "epoch": 0.12764705882352942,
      "grad_norm": 0.054306965321302414,
      "learning_rate": 0.00017472754050073638,
      "loss": 0.3386,
      "step": 434
    },
    {
      "epoch": 0.12794117647058822,
      "grad_norm": 0.05765855684876442,
      "learning_rate": 0.00017466863033873344,
      "loss": 0.3537,
      "step": 435
    },
    {
      "epoch": 0.12823529411764706,
      "grad_norm": 0.08346139639616013,
      "learning_rate": 0.0001746097201767305,
      "loss": 0.4039,
      "step": 436
    },
    {
      "epoch": 0.1285294117647059,
      "grad_norm": 0.056448858231306076,
      "learning_rate": 0.00017455081001472756,
      "loss": 0.3729,
      "step": 437
    },
    {
      "epoch": 0.1288235294117647,
      "grad_norm": 0.07123912870883942,
      "learning_rate": 0.0001744918998527246,
      "loss": 0.4258,
      "step": 438
    },
    {
      "epoch": 0.12911764705882353,
      "grad_norm": 0.06362074613571167,
      "learning_rate": 0.00017443298969072165,
      "loss": 0.3943,
      "step": 439
    },
    {
      "epoch": 0.12941176470588237,
      "grad_norm": 0.06075656786561012,
      "learning_rate": 0.00017437407952871872,
      "loss": 0.371,
      "step": 440
    },
    {
      "epoch": 0.12970588235294117,
      "grad_norm": 0.06329082697629929,
      "learning_rate": 0.00017431516936671578,
      "loss": 0.347,
      "step": 441
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.06610801815986633,
      "learning_rate": 0.00017425625920471284,
      "loss": 0.3524,
      "step": 442
    },
    {
      "epoch": 0.1302941176470588,
      "grad_norm": 0.07422363013029099,
      "learning_rate": 0.00017419734904270987,
      "loss": 0.3686,
      "step": 443
    },
    {
      "epoch": 0.13058823529411764,
      "grad_norm": 0.0583348348736763,
      "learning_rate": 0.00017413843888070693,
      "loss": 0.37,
      "step": 444
    },
    {
      "epoch": 0.13088235294117648,
      "grad_norm": 0.06199157238006592,
      "learning_rate": 0.000174079528718704,
      "loss": 0.3247,
      "step": 445
    },
    {
      "epoch": 0.13117647058823528,
      "grad_norm": 0.07494378089904785,
      "learning_rate": 0.00017402061855670105,
      "loss": 0.4019,
      "step": 446
    },
    {
      "epoch": 0.13147058823529412,
      "grad_norm": 0.05664242058992386,
      "learning_rate": 0.0001739617083946981,
      "loss": 0.3218,
      "step": 447
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 0.05308847874403,
      "learning_rate": 0.00017390279823269514,
      "loss": 0.3145,
      "step": 448
    },
    {
      "epoch": 0.13205882352941176,
      "grad_norm": 0.07317924499511719,
      "learning_rate": 0.0001738438880706922,
      "loss": 0.3673,
      "step": 449
    },
    {
      "epoch": 0.1323529411764706,
      "grad_norm": 0.06949937343597412,
      "learning_rate": 0.00017378497790868926,
      "loss": 0.3324,
      "step": 450
    },
    {
      "epoch": 0.13264705882352942,
      "grad_norm": 0.05919482558965683,
      "learning_rate": 0.00017372606774668632,
      "loss": 0.4061,
      "step": 451
    },
    {
      "epoch": 0.13294117647058823,
      "grad_norm": 0.04733148589730263,
      "learning_rate": 0.00017366715758468338,
      "loss": 0.2809,
      "step": 452
    },
    {
      "epoch": 0.13323529411764706,
      "grad_norm": 0.06390117108821869,
      "learning_rate": 0.00017360824742268042,
      "loss": 0.3655,
      "step": 453
    },
    {
      "epoch": 0.13352941176470587,
      "grad_norm": 0.05269750580191612,
      "learning_rate": 0.00017354933726067748,
      "loss": 0.3068,
      "step": 454
    },
    {
      "epoch": 0.1338235294117647,
      "grad_norm": 0.05385202169418335,
      "learning_rate": 0.00017349042709867454,
      "loss": 0.3273,
      "step": 455
    },
    {
      "epoch": 0.13411764705882354,
      "grad_norm": 0.061493147164583206,
      "learning_rate": 0.0001734315169366716,
      "loss": 0.3199,
      "step": 456
    },
    {
      "epoch": 0.13441176470588234,
      "grad_norm": 0.05856568366289139,
      "learning_rate": 0.00017337260677466866,
      "loss": 0.3036,
      "step": 457
    },
    {
      "epoch": 0.13470588235294118,
      "grad_norm": 0.06415963172912598,
      "learning_rate": 0.0001733136966126657,
      "loss": 0.3535,
      "step": 458
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.05263753607869148,
      "learning_rate": 0.00017325478645066275,
      "loss": 0.354,
      "step": 459
    },
    {
      "epoch": 0.13529411764705881,
      "grad_norm": 0.052405644208192825,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.2826,
      "step": 460
    },
    {
      "epoch": 0.13558823529411765,
      "grad_norm": 0.0717768445611,
      "learning_rate": 0.00017313696612665687,
      "loss": 0.3819,
      "step": 461
    },
    {
      "epoch": 0.13588235294117648,
      "grad_norm": 0.05164726451039314,
      "learning_rate": 0.00017307805596465393,
      "loss": 0.3202,
      "step": 462
    },
    {
      "epoch": 0.1361764705882353,
      "grad_norm": 0.06342334300279617,
      "learning_rate": 0.00017301914580265096,
      "loss": 0.3069,
      "step": 463
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 0.0639723390340805,
      "learning_rate": 0.00017296023564064802,
      "loss": 0.34,
      "step": 464
    },
    {
      "epoch": 0.13676470588235295,
      "grad_norm": 0.057234782725572586,
      "learning_rate": 0.00017290132547864508,
      "loss": 0.3596,
      "step": 465
    },
    {
      "epoch": 0.13705882352941176,
      "grad_norm": 0.0710613876581192,
      "learning_rate": 0.00017284241531664214,
      "loss": 0.4529,
      "step": 466
    },
    {
      "epoch": 0.1373529411764706,
      "grad_norm": 0.05788923054933548,
      "learning_rate": 0.0001727835051546392,
      "loss": 0.3876,
      "step": 467
    },
    {
      "epoch": 0.1376470588235294,
      "grad_norm": 0.05585173889994621,
      "learning_rate": 0.00017272459499263624,
      "loss": 0.3995,
      "step": 468
    },
    {
      "epoch": 0.13794117647058823,
      "grad_norm": 0.0703703761100769,
      "learning_rate": 0.0001726656848306333,
      "loss": 0.3501,
      "step": 469
    },
    {
      "epoch": 0.13823529411764707,
      "grad_norm": 0.05603313073515892,
      "learning_rate": 0.00017260677466863036,
      "loss": 0.3252,
      "step": 470
    },
    {
      "epoch": 0.13852941176470587,
      "grad_norm": 0.049182724207639694,
      "learning_rate": 0.00017254786450662742,
      "loss": 0.3075,
      "step": 471
    },
    {
      "epoch": 0.1388235294117647,
      "grad_norm": 0.07177457213401794,
      "learning_rate": 0.00017248895434462448,
      "loss": 0.3536,
      "step": 472
    },
    {
      "epoch": 0.13911764705882354,
      "grad_norm": 0.05876198038458824,
      "learning_rate": 0.0001724300441826215,
      "loss": 0.2987,
      "step": 473
    },
    {
      "epoch": 0.13941176470588235,
      "grad_norm": 0.0633619874715805,
      "learning_rate": 0.00017237113402061857,
      "loss": 0.3818,
      "step": 474
    },
    {
      "epoch": 0.13970588235294118,
      "grad_norm": 0.055080026388168335,
      "learning_rate": 0.00017231222385861563,
      "loss": 0.314,
      "step": 475
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.04072139039635658,
      "learning_rate": 0.0001722533136966127,
      "loss": 0.2703,
      "step": 476
    },
    {
      "epoch": 0.14029411764705882,
      "grad_norm": 0.05345906317234039,
      "learning_rate": 0.00017219440353460975,
      "loss": 0.4003,
      "step": 477
    },
    {
      "epoch": 0.14058823529411765,
      "grad_norm": 0.0584883950650692,
      "learning_rate": 0.00017213549337260678,
      "loss": 0.3767,
      "step": 478
    },
    {
      "epoch": 0.14088235294117646,
      "grad_norm": 0.06285300105810165,
      "learning_rate": 0.00017207658321060384,
      "loss": 0.3324,
      "step": 479
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 0.049121711403131485,
      "learning_rate": 0.0001720176730486009,
      "loss": 0.3386,
      "step": 480
    },
    {
      "epoch": 0.14147058823529413,
      "grad_norm": 0.07132626324892044,
      "learning_rate": 0.00017195876288659796,
      "loss": 0.3565,
      "step": 481
    },
    {
      "epoch": 0.14176470588235293,
      "grad_norm": 0.06294410675764084,
      "learning_rate": 0.00017189985272459503,
      "loss": 0.3789,
      "step": 482
    },
    {
      "epoch": 0.14205882352941177,
      "grad_norm": 0.07501737773418427,
      "learning_rate": 0.00017184094256259203,
      "loss": 0.316,
      "step": 483
    },
    {
      "epoch": 0.1423529411764706,
      "grad_norm": 0.058059290051460266,
      "learning_rate": 0.0001717820324005891,
      "loss": 0.3649,
      "step": 484
    },
    {
      "epoch": 0.1426470588235294,
      "grad_norm": 0.051437921822071075,
      "learning_rate": 0.00017172312223858615,
      "loss": 0.3134,
      "step": 485
    },
    {
      "epoch": 0.14294117647058824,
      "grad_norm": 0.06578783690929413,
      "learning_rate": 0.0001716642120765832,
      "loss": 0.408,
      "step": 486
    },
    {
      "epoch": 0.14323529411764707,
      "grad_norm": 0.054990254342556,
      "learning_rate": 0.00017160530191458027,
      "loss": 0.3665,
      "step": 487
    },
    {
      "epoch": 0.14352941176470588,
      "grad_norm": 0.05266501009464264,
      "learning_rate": 0.0001715463917525773,
      "loss": 0.2937,
      "step": 488
    },
    {
      "epoch": 0.1438235294117647,
      "grad_norm": 0.06444081664085388,
      "learning_rate": 0.00017148748159057437,
      "loss": 0.3852,
      "step": 489
    },
    {
      "epoch": 0.14411764705882352,
      "grad_norm": 0.07051171362400055,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4163,
      "step": 490
    },
    {
      "epoch": 0.14441176470588235,
      "grad_norm": 0.04941039904952049,
      "learning_rate": 0.00017136966126656849,
      "loss": 0.3275,
      "step": 491
    },
    {
      "epoch": 0.14470588235294118,
      "grad_norm": 0.07058579474687576,
      "learning_rate": 0.00017131075110456555,
      "loss": 0.3945,
      "step": 492
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05393172800540924,
      "learning_rate": 0.00017125184094256258,
      "loss": 0.3068,
      "step": 493
    },
    {
      "epoch": 0.14529411764705882,
      "grad_norm": 0.05968550592660904,
      "learning_rate": 0.00017119293078055964,
      "loss": 0.4008,
      "step": 494
    },
    {
      "epoch": 0.14558823529411766,
      "grad_norm": 0.06070926785469055,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.3796,
      "step": 495
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 0.04709979146718979,
      "learning_rate": 0.00017107511045655376,
      "loss": 0.2895,
      "step": 496
    },
    {
      "epoch": 0.1461764705882353,
      "grad_norm": 0.050172340124845505,
      "learning_rate": 0.00017101620029455082,
      "loss": 0.3294,
      "step": 497
    },
    {
      "epoch": 0.14647058823529413,
      "grad_norm": 0.07811614871025085,
      "learning_rate": 0.00017095729013254785,
      "loss": 0.3634,
      "step": 498
    },
    {
      "epoch": 0.14676470588235294,
      "grad_norm": 0.052022531628608704,
      "learning_rate": 0.0001708983799705449,
      "loss": 0.3388,
      "step": 499
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.057600993663072586,
      "learning_rate": 0.00017083946980854197,
      "loss": 0.3581,
      "step": 500
    },
    {
      "epoch": 0.14735294117647058,
      "grad_norm": 0.06193580478429794,
      "learning_rate": 0.00017078055964653903,
      "loss": 0.3617,
      "step": 501
    },
    {
      "epoch": 0.1476470588235294,
      "grad_norm": 0.07039792090654373,
      "learning_rate": 0.0001707216494845361,
      "loss": 0.4368,
      "step": 502
    },
    {
      "epoch": 0.14794117647058824,
      "grad_norm": 0.05649368837475777,
      "learning_rate": 0.00017066273932253313,
      "loss": 0.2813,
      "step": 503
    },
    {
      "epoch": 0.14823529411764705,
      "grad_norm": 0.07238007336854935,
      "learning_rate": 0.00017060382916053019,
      "loss": 0.3657,
      "step": 504
    },
    {
      "epoch": 0.14852941176470588,
      "grad_norm": 0.06057054549455643,
      "learning_rate": 0.00017054491899852725,
      "loss": 0.3245,
      "step": 505
    },
    {
      "epoch": 0.14882352941176472,
      "grad_norm": 0.06513822078704834,
      "learning_rate": 0.0001704860088365243,
      "loss": 0.4206,
      "step": 506
    },
    {
      "epoch": 0.14911764705882352,
      "grad_norm": 0.05669257044792175,
      "learning_rate": 0.00017042709867452137,
      "loss": 0.3914,
      "step": 507
    },
    {
      "epoch": 0.14941176470588236,
      "grad_norm": 0.06805407255887985,
      "learning_rate": 0.0001703681885125184,
      "loss": 0.4091,
      "step": 508
    },
    {
      "epoch": 0.1497058823529412,
      "grad_norm": 0.06307753920555115,
      "learning_rate": 0.00017030927835051546,
      "loss": 0.3465,
      "step": 509
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06840942800045013,
      "learning_rate": 0.00017025036818851252,
      "loss": 0.3658,
      "step": 510
    },
    {
      "epoch": 0.15029411764705883,
      "grad_norm": 0.053714569658041,
      "learning_rate": 0.00017019145802650958,
      "loss": 0.3499,
      "step": 511
    },
    {
      "epoch": 0.15058823529411763,
      "grad_norm": 0.040315892547369,
      "learning_rate": 0.00017013254786450664,
      "loss": 0.2971,
      "step": 512
    },
    {
      "epoch": 0.15088235294117647,
      "grad_norm": 0.05545332655310631,
      "learning_rate": 0.00017007363770250367,
      "loss": 0.3312,
      "step": 513
    },
    {
      "epoch": 0.1511764705882353,
      "grad_norm": 0.05611592158675194,
      "learning_rate": 0.00017001472754050073,
      "loss": 0.3433,
      "step": 514
    },
    {
      "epoch": 0.1514705882352941,
      "grad_norm": 0.05403720587491989,
      "learning_rate": 0.0001699558173784978,
      "loss": 0.3296,
      "step": 515
    },
    {
      "epoch": 0.15176470588235294,
      "grad_norm": 0.06694722175598145,
      "learning_rate": 0.00016989690721649485,
      "loss": 0.3829,
      "step": 516
    },
    {
      "epoch": 0.15205882352941177,
      "grad_norm": 0.06600271910429001,
      "learning_rate": 0.00016983799705449191,
      "loss": 0.3239,
      "step": 517
    },
    {
      "epoch": 0.15235294117647058,
      "grad_norm": 0.07736946642398834,
      "learning_rate": 0.00016977908689248895,
      "loss": 0.3627,
      "step": 518
    },
    {
      "epoch": 0.1526470588235294,
      "grad_norm": 0.05466568470001221,
      "learning_rate": 0.000169720176730486,
      "loss": 0.2987,
      "step": 519
    },
    {
      "epoch": 0.15294117647058825,
      "grad_norm": 0.0472368523478508,
      "learning_rate": 0.00016966126656848307,
      "loss": 0.2589,
      "step": 520
    },
    {
      "epoch": 0.15323529411764705,
      "grad_norm": 0.08170114457607269,
      "learning_rate": 0.00016960235640648013,
      "loss": 0.3619,
      "step": 521
    },
    {
      "epoch": 0.1535294117647059,
      "grad_norm": 0.07004242390394211,
      "learning_rate": 0.0001695434462444772,
      "loss": 0.397,
      "step": 522
    },
    {
      "epoch": 0.1538235294117647,
      "grad_norm": 0.04679474979639053,
      "learning_rate": 0.00016948453608247422,
      "loss": 0.2954,
      "step": 523
    },
    {
      "epoch": 0.15411764705882353,
      "grad_norm": 0.04191538691520691,
      "learning_rate": 0.00016942562592047128,
      "loss": 0.3184,
      "step": 524
    },
    {
      "epoch": 0.15441176470588236,
      "grad_norm": 0.05352380871772766,
      "learning_rate": 0.00016936671575846834,
      "loss": 0.3021,
      "step": 525
    },
    {
      "epoch": 0.15470588235294117,
      "grad_norm": 0.06191958487033844,
      "learning_rate": 0.0001693078055964654,
      "loss": 0.2907,
      "step": 526
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.06200692802667618,
      "learning_rate": 0.00016924889543446246,
      "loss": 0.3711,
      "step": 527
    },
    {
      "epoch": 0.15529411764705883,
      "grad_norm": 0.05208279937505722,
      "learning_rate": 0.0001691899852724595,
      "loss": 0.3267,
      "step": 528
    },
    {
      "epoch": 0.15558823529411764,
      "grad_norm": 0.07539403438568115,
      "learning_rate": 0.00016913107511045655,
      "loss": 0.4333,
      "step": 529
    },
    {
      "epoch": 0.15588235294117647,
      "grad_norm": 0.05377708747982979,
      "learning_rate": 0.00016907216494845361,
      "loss": 0.2867,
      "step": 530
    },
    {
      "epoch": 0.1561764705882353,
      "grad_norm": 0.051238998770713806,
      "learning_rate": 0.00016901325478645068,
      "loss": 0.3736,
      "step": 531
    },
    {
      "epoch": 0.1564705882352941,
      "grad_norm": 0.05820276588201523,
      "learning_rate": 0.00016895434462444774,
      "loss": 0.3972,
      "step": 532
    },
    {
      "epoch": 0.15676470588235294,
      "grad_norm": 0.047747302800416946,
      "learning_rate": 0.00016889543446244477,
      "loss": 0.3116,
      "step": 533
    },
    {
      "epoch": 0.15705882352941178,
      "grad_norm": 0.07083246856927872,
      "learning_rate": 0.00016883652430044183,
      "loss": 0.3625,
      "step": 534
    },
    {
      "epoch": 0.15735294117647058,
      "grad_norm": 0.058713871985673904,
      "learning_rate": 0.0001687776141384389,
      "loss": 0.3754,
      "step": 535
    },
    {
      "epoch": 0.15764705882352942,
      "grad_norm": 0.05031213164329529,
      "learning_rate": 0.00016871870397643595,
      "loss": 0.3209,
      "step": 536
    },
    {
      "epoch": 0.15794117647058822,
      "grad_norm": 0.05241355672478676,
      "learning_rate": 0.000168659793814433,
      "loss": 0.356,
      "step": 537
    },
    {
      "epoch": 0.15823529411764706,
      "grad_norm": 0.049185171723365784,
      "learning_rate": 0.00016860088365243004,
      "loss": 0.3436,
      "step": 538
    },
    {
      "epoch": 0.1585294117647059,
      "grad_norm": 0.05663933604955673,
      "learning_rate": 0.0001685419734904271,
      "loss": 0.3484,
      "step": 539
    },
    {
      "epoch": 0.1588235294117647,
      "grad_norm": 0.054598014801740646,
      "learning_rate": 0.00016848306332842416,
      "loss": 0.3277,
      "step": 540
    },
    {
      "epoch": 0.15911764705882353,
      "grad_norm": 0.07492633163928986,
      "learning_rate": 0.00016842415316642122,
      "loss": 0.3594,
      "step": 541
    },
    {
      "epoch": 0.15941176470588236,
      "grad_norm": 0.06319060176610947,
      "learning_rate": 0.00016836524300441828,
      "loss": 0.271,
      "step": 542
    },
    {
      "epoch": 0.15970588235294117,
      "grad_norm": 0.06293650716543198,
      "learning_rate": 0.00016830633284241532,
      "loss": 0.4016,
      "step": 543
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.060195211321115494,
      "learning_rate": 0.00016824742268041238,
      "loss": 0.3704,
      "step": 544
    },
    {
      "epoch": 0.16029411764705884,
      "grad_norm": 0.04301571846008301,
      "learning_rate": 0.00016818851251840944,
      "loss": 0.2679,
      "step": 545
    },
    {
      "epoch": 0.16058823529411764,
      "grad_norm": 0.06011172756552696,
      "learning_rate": 0.0001681296023564065,
      "loss": 0.3569,
      "step": 546
    },
    {
      "epoch": 0.16088235294117648,
      "grad_norm": 0.045250967144966125,
      "learning_rate": 0.00016807069219440356,
      "loss": 0.2533,
      "step": 547
    },
    {
      "epoch": 0.16117647058823528,
      "grad_norm": 0.056782059371471405,
      "learning_rate": 0.0001680117820324006,
      "loss": 0.2964,
      "step": 548
    },
    {
      "epoch": 0.16147058823529412,
      "grad_norm": 0.058335352689027786,
      "learning_rate": 0.00016795287187039765,
      "loss": 0.3577,
      "step": 549
    },
    {
      "epoch": 0.16176470588235295,
      "grad_norm": 0.05302882567048073,
      "learning_rate": 0.0001678939617083947,
      "loss": 0.3866,
      "step": 550
    },
    {
      "epoch": 0.16205882352941176,
      "grad_norm": 0.045930467545986176,
      "learning_rate": 0.00016783505154639177,
      "loss": 0.341,
      "step": 551
    },
    {
      "epoch": 0.1623529411764706,
      "grad_norm": 0.04835527762770653,
      "learning_rate": 0.00016777614138438883,
      "loss": 0.3106,
      "step": 552
    },
    {
      "epoch": 0.16264705882352942,
      "grad_norm": 0.06390727311372757,
      "learning_rate": 0.00016771723122238586,
      "loss": 0.268,
      "step": 553
    },
    {
      "epoch": 0.16294117647058823,
      "grad_norm": 0.0707143023610115,
      "learning_rate": 0.00016765832106038292,
      "loss": 0.4432,
      "step": 554
    },
    {
      "epoch": 0.16323529411764706,
      "grad_norm": 0.07104025781154633,
      "learning_rate": 0.00016759941089837998,
      "loss": 0.4227,
      "step": 555
    },
    {
      "epoch": 0.1635294117647059,
      "grad_norm": 0.05053001269698143,
      "learning_rate": 0.00016754050073637704,
      "loss": 0.3172,
      "step": 556
    },
    {
      "epoch": 0.1638235294117647,
      "grad_norm": 0.06678225100040436,
      "learning_rate": 0.0001674815905743741,
      "loss": 0.3237,
      "step": 557
    },
    {
      "epoch": 0.16411764705882353,
      "grad_norm": 0.06146559491753578,
      "learning_rate": 0.00016742268041237114,
      "loss": 0.3779,
      "step": 558
    },
    {
      "epoch": 0.16441176470588234,
      "grad_norm": 0.06342355161905289,
      "learning_rate": 0.0001673637702503682,
      "loss": 0.3702,
      "step": 559
    },
    {
      "epoch": 0.16470588235294117,
      "grad_norm": 0.06218024343252182,
      "learning_rate": 0.00016730486008836526,
      "loss": 0.4087,
      "step": 560
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.06960968673229218,
      "learning_rate": 0.00016724594992636232,
      "loss": 0.3562,
      "step": 561
    },
    {
      "epoch": 0.1652941176470588,
      "grad_norm": 0.07093697786331177,
      "learning_rate": 0.00016718703976435938,
      "loss": 0.3583,
      "step": 562
    },
    {
      "epoch": 0.16558823529411765,
      "grad_norm": 0.0660216435790062,
      "learning_rate": 0.0001671281296023564,
      "loss": 0.4231,
      "step": 563
    },
    {
      "epoch": 0.16588235294117648,
      "grad_norm": 0.0505535863339901,
      "learning_rate": 0.00016706921944035347,
      "loss": 0.3371,
      "step": 564
    },
    {
      "epoch": 0.1661764705882353,
      "grad_norm": 0.052712347358465195,
      "learning_rate": 0.00016701030927835053,
      "loss": 0.3381,
      "step": 565
    },
    {
      "epoch": 0.16647058823529412,
      "grad_norm": 0.06701494753360748,
      "learning_rate": 0.0001669513991163476,
      "loss": 0.4006,
      "step": 566
    },
    {
      "epoch": 0.16676470588235295,
      "grad_norm": 0.06080479919910431,
      "learning_rate": 0.00016689248895434465,
      "loss": 0.3621,
      "step": 567
    },
    {
      "epoch": 0.16705882352941176,
      "grad_norm": 0.05409641191363335,
      "learning_rate": 0.00016683357879234168,
      "loss": 0.3243,
      "step": 568
    },
    {
      "epoch": 0.1673529411764706,
      "grad_norm": 0.07134998589754105,
      "learning_rate": 0.00016677466863033874,
      "loss": 0.3008,
      "step": 569
    },
    {
      "epoch": 0.1676470588235294,
      "grad_norm": 0.07482819259166718,
      "learning_rate": 0.0001667157584683358,
      "loss": 0.3207,
      "step": 570
    },
    {
      "epoch": 0.16794117647058823,
      "grad_norm": 0.06479887664318085,
      "learning_rate": 0.00016665684830633286,
      "loss": 0.412,
      "step": 571
    },
    {
      "epoch": 0.16823529411764707,
      "grad_norm": 0.061265431344509125,
      "learning_rate": 0.00016659793814432993,
      "loss": 0.3652,
      "step": 572
    },
    {
      "epoch": 0.16852941176470587,
      "grad_norm": 0.0703388974070549,
      "learning_rate": 0.00016653902798232696,
      "loss": 0.3805,
      "step": 573
    },
    {
      "epoch": 0.1688235294117647,
      "grad_norm": 0.052610062062740326,
      "learning_rate": 0.00016648011782032402,
      "loss": 0.3312,
      "step": 574
    },
    {
      "epoch": 0.16911764705882354,
      "grad_norm": 0.06696663051843643,
      "learning_rate": 0.00016642120765832108,
      "loss": 0.4088,
      "step": 575
    },
    {
      "epoch": 0.16941176470588235,
      "grad_norm": 0.06395287066698074,
      "learning_rate": 0.00016636229749631814,
      "loss": 0.3971,
      "step": 576
    },
    {
      "epoch": 0.16970588235294118,
      "grad_norm": 0.062129609286785126,
      "learning_rate": 0.0001663033873343152,
      "loss": 0.4399,
      "step": 577
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.04644171521067619,
      "learning_rate": 0.00016624447717231223,
      "loss": 0.27,
      "step": 578
    },
    {
      "epoch": 0.17029411764705882,
      "grad_norm": 0.05524536594748497,
      "learning_rate": 0.0001661855670103093,
      "loss": 0.4055,
      "step": 579
    },
    {
      "epoch": 0.17058823529411765,
      "grad_norm": 0.05079932138323784,
      "learning_rate": 0.00016612665684830635,
      "loss": 0.3562,
      "step": 580
    },
    {
      "epoch": 0.17088235294117646,
      "grad_norm": 0.04984373226761818,
      "learning_rate": 0.0001660677466863034,
      "loss": 0.2906,
      "step": 581
    },
    {
      "epoch": 0.1711764705882353,
      "grad_norm": 0.04655125364661217,
      "learning_rate": 0.00016600883652430047,
      "loss": 0.3388,
      "step": 582
    },
    {
      "epoch": 0.17147058823529412,
      "grad_norm": 0.056734804064035416,
      "learning_rate": 0.0001659499263622975,
      "loss": 0.352,
      "step": 583
    },
    {
      "epoch": 0.17176470588235293,
      "grad_norm": 0.061462558805942535,
      "learning_rate": 0.00016589101620029457,
      "loss": 0.3824,
      "step": 584
    },
    {
      "epoch": 0.17205882352941176,
      "grad_norm": 0.05228470265865326,
      "learning_rate": 0.00016583210603829163,
      "loss": 0.3607,
      "step": 585
    },
    {
      "epoch": 0.1723529411764706,
      "grad_norm": 0.058265455067157745,
      "learning_rate": 0.00016577319587628869,
      "loss": 0.4351,
      "step": 586
    },
    {
      "epoch": 0.1726470588235294,
      "grad_norm": 0.06106986850500107,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.39,
      "step": 587
    },
    {
      "epoch": 0.17294117647058824,
      "grad_norm": 0.038869597017765045,
      "learning_rate": 0.00016565537555228278,
      "loss": 0.2904,
      "step": 588
    },
    {
      "epoch": 0.17323529411764707,
      "grad_norm": 0.05712001025676727,
      "learning_rate": 0.0001655964653902798,
      "loss": 0.4209,
      "step": 589
    },
    {
      "epoch": 0.17352941176470588,
      "grad_norm": 0.047472815960645676,
      "learning_rate": 0.00016553755522827687,
      "loss": 0.2935,
      "step": 590
    },
    {
      "epoch": 0.1738235294117647,
      "grad_norm": 0.04458276182413101,
      "learning_rate": 0.00016547864506627393,
      "loss": 0.2885,
      "step": 591
    },
    {
      "epoch": 0.17411764705882352,
      "grad_norm": 0.049312155693769455,
      "learning_rate": 0.000165419734904271,
      "loss": 0.3604,
      "step": 592
    },
    {
      "epoch": 0.17441176470588235,
      "grad_norm": 0.05668745934963226,
      "learning_rate": 0.00016536082474226803,
      "loss": 0.367,
      "step": 593
    },
    {
      "epoch": 0.17470588235294118,
      "grad_norm": 0.06275175511837006,
      "learning_rate": 0.00016530191458026509,
      "loss": 0.3404,
      "step": 594
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.050478558987379074,
      "learning_rate": 0.00016524300441826215,
      "loss": 0.4078,
      "step": 595
    },
    {
      "epoch": 0.17529411764705882,
      "grad_norm": 0.06471184641122818,
      "learning_rate": 0.0001651840942562592,
      "loss": 0.3823,
      "step": 596
    },
    {
      "epoch": 0.17558823529411766,
      "grad_norm": 0.058347951620817184,
      "learning_rate": 0.00016512518409425627,
      "loss": 0.3815,
      "step": 597
    },
    {
      "epoch": 0.17588235294117646,
      "grad_norm": 0.06391522288322449,
      "learning_rate": 0.0001650662739322533,
      "loss": 0.3636,
      "step": 598
    },
    {
      "epoch": 0.1761764705882353,
      "grad_norm": 0.06048767268657684,
      "learning_rate": 0.00016500736377025036,
      "loss": 0.3604,
      "step": 599
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.07474303245544434,
      "learning_rate": 0.00016494845360824742,
      "loss": 0.3756,
      "step": 600
    },
    {
      "epoch": 0.17676470588235293,
      "grad_norm": 0.06107667461037636,
      "learning_rate": 0.00016488954344624448,
      "loss": 0.4202,
      "step": 601
    },
    {
      "epoch": 0.17705882352941177,
      "grad_norm": 0.07629653811454773,
      "learning_rate": 0.00016483063328424154,
      "loss": 0.4249,
      "step": 602
    },
    {
      "epoch": 0.1773529411764706,
      "grad_norm": 0.05330305173993111,
      "learning_rate": 0.00016477172312223857,
      "loss": 0.3425,
      "step": 603
    },
    {
      "epoch": 0.1776470588235294,
      "grad_norm": 0.05535074695944786,
      "learning_rate": 0.00016471281296023563,
      "loss": 0.3534,
      "step": 604
    },
    {
      "epoch": 0.17794117647058824,
      "grad_norm": 0.061677485704422,
      "learning_rate": 0.0001646539027982327,
      "loss": 0.3499,
      "step": 605
    },
    {
      "epoch": 0.17823529411764705,
      "grad_norm": 0.06373181194067001,
      "learning_rate": 0.00016459499263622975,
      "loss": 0.4169,
      "step": 606
    },
    {
      "epoch": 0.17852941176470588,
      "grad_norm": 0.052286386489868164,
      "learning_rate": 0.00016453608247422681,
      "loss": 0.4158,
      "step": 607
    },
    {
      "epoch": 0.17882352941176471,
      "grad_norm": 0.06523198634386063,
      "learning_rate": 0.00016447717231222385,
      "loss": 0.3998,
      "step": 608
    },
    {
      "epoch": 0.17911764705882352,
      "grad_norm": 0.04262848198413849,
      "learning_rate": 0.0001644182621502209,
      "loss": 0.2542,
      "step": 609
    },
    {
      "epoch": 0.17941176470588235,
      "grad_norm": 0.0697278305888176,
      "learning_rate": 0.00016435935198821797,
      "loss": 0.3778,
      "step": 610
    },
    {
      "epoch": 0.1797058823529412,
      "grad_norm": 0.05354809761047363,
      "learning_rate": 0.00016430044182621503,
      "loss": 0.2937,
      "step": 611
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.05750763788819313,
      "learning_rate": 0.0001642415316642121,
      "loss": 0.3341,
      "step": 612
    },
    {
      "epoch": 0.18029411764705883,
      "grad_norm": 0.07634463906288147,
      "learning_rate": 0.00016418262150220912,
      "loss": 0.4056,
      "step": 613
    },
    {
      "epoch": 0.18058823529411766,
      "grad_norm": 0.05245326831936836,
      "learning_rate": 0.00016412371134020618,
      "loss": 0.3809,
      "step": 614
    },
    {
      "epoch": 0.18088235294117647,
      "grad_norm": 0.062018051743507385,
      "learning_rate": 0.00016406480117820324,
      "loss": 0.4161,
      "step": 615
    },
    {
      "epoch": 0.1811764705882353,
      "grad_norm": 0.071085624396801,
      "learning_rate": 0.0001640058910162003,
      "loss": 0.3459,
      "step": 616
    },
    {
      "epoch": 0.1814705882352941,
      "grad_norm": 0.06508353352546692,
      "learning_rate": 0.00016394698085419736,
      "loss": 0.3872,
      "step": 617
    },
    {
      "epoch": 0.18176470588235294,
      "grad_norm": 0.07106375694274902,
      "learning_rate": 0.0001638880706921944,
      "loss": 0.3966,
      "step": 618
    },
    {
      "epoch": 0.18205882352941177,
      "grad_norm": 0.048792023211717606,
      "learning_rate": 0.00016382916053019145,
      "loss": 0.3052,
      "step": 619
    },
    {
      "epoch": 0.18235294117647058,
      "grad_norm": 0.058426886796951294,
      "learning_rate": 0.00016377025036818851,
      "loss": 0.379,
      "step": 620
    },
    {
      "epoch": 0.1826470588235294,
      "grad_norm": 0.04340818524360657,
      "learning_rate": 0.00016371134020618558,
      "loss": 0.3102,
      "step": 621
    },
    {
      "epoch": 0.18294117647058825,
      "grad_norm": 0.05881083011627197,
      "learning_rate": 0.00016365243004418264,
      "loss": 0.3973,
      "step": 622
    },
    {
      "epoch": 0.18323529411764705,
      "grad_norm": 0.0563063845038414,
      "learning_rate": 0.00016359351988217967,
      "loss": 0.3502,
      "step": 623
    },
    {
      "epoch": 0.18352941176470589,
      "grad_norm": 0.06756271421909332,
      "learning_rate": 0.00016353460972017673,
      "loss": 0.372,
      "step": 624
    },
    {
      "epoch": 0.18382352941176472,
      "grad_norm": 0.04933758080005646,
      "learning_rate": 0.0001634756995581738,
      "loss": 0.3729,
      "step": 625
    },
    {
      "epoch": 0.18411764705882352,
      "grad_norm": 0.06471831351518631,
      "learning_rate": 0.00016341678939617085,
      "loss": 0.3797,
      "step": 626
    },
    {
      "epoch": 0.18441176470588236,
      "grad_norm": 0.07643914222717285,
      "learning_rate": 0.0001633578792341679,
      "loss": 0.3482,
      "step": 627
    },
    {
      "epoch": 0.18470588235294116,
      "grad_norm": 0.05957276001572609,
      "learning_rate": 0.00016329896907216494,
      "loss": 0.3729,
      "step": 628
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06735341250896454,
      "learning_rate": 0.000163240058910162,
      "loss": 0.3757,
      "step": 629
    },
    {
      "epoch": 0.18529411764705883,
      "grad_norm": 0.06439851969480515,
      "learning_rate": 0.00016318114874815906,
      "loss": 0.416,
      "step": 630
    },
    {
      "epoch": 0.18558823529411764,
      "grad_norm": 0.04577559977769852,
      "learning_rate": 0.00016312223858615612,
      "loss": 0.2814,
      "step": 631
    },
    {
      "epoch": 0.18588235294117647,
      "grad_norm": 0.04966415837407112,
      "learning_rate": 0.00016306332842415318,
      "loss": 0.3306,
      "step": 632
    },
    {
      "epoch": 0.1861764705882353,
      "grad_norm": 0.06001998111605644,
      "learning_rate": 0.00016300441826215022,
      "loss": 0.3347,
      "step": 633
    },
    {
      "epoch": 0.1864705882352941,
      "grad_norm": 0.04580844193696976,
      "learning_rate": 0.00016294550810014728,
      "loss": 0.3085,
      "step": 634
    },
    {
      "epoch": 0.18676470588235294,
      "grad_norm": 0.04229709878563881,
      "learning_rate": 0.00016288659793814434,
      "loss": 0.2584,
      "step": 635
    },
    {
      "epoch": 0.18705882352941178,
      "grad_norm": 0.06391051411628723,
      "learning_rate": 0.0001628276877761414,
      "loss": 0.3692,
      "step": 636
    },
    {
      "epoch": 0.18735294117647058,
      "grad_norm": 0.09518606960773468,
      "learning_rate": 0.00016276877761413846,
      "loss": 0.4263,
      "step": 637
    },
    {
      "epoch": 0.18764705882352942,
      "grad_norm": 0.05876845866441727,
      "learning_rate": 0.0001627098674521355,
      "loss": 0.3343,
      "step": 638
    },
    {
      "epoch": 0.18794117647058822,
      "grad_norm": 0.05391281843185425,
      "learning_rate": 0.00016265095729013255,
      "loss": 0.2743,
      "step": 639
    },
    {
      "epoch": 0.18823529411764706,
      "grad_norm": 0.07848921418190002,
      "learning_rate": 0.0001625920471281296,
      "loss": 0.3967,
      "step": 640
    },
    {
      "epoch": 0.1885294117647059,
      "grad_norm": 0.06307537108659744,
      "learning_rate": 0.00016253313696612667,
      "loss": 0.3708,
      "step": 641
    },
    {
      "epoch": 0.1888235294117647,
      "grad_norm": 0.06231262534856796,
      "learning_rate": 0.00016247422680412373,
      "loss": 0.4179,
      "step": 642
    },
    {
      "epoch": 0.18911764705882353,
      "grad_norm": 0.08004824817180634,
      "learning_rate": 0.00016241531664212076,
      "loss": 0.3985,
      "step": 643
    },
    {
      "epoch": 0.18941176470588236,
      "grad_norm": 0.05552724748849869,
      "learning_rate": 0.00016235640648011782,
      "loss": 0.3166,
      "step": 644
    },
    {
      "epoch": 0.18970588235294117,
      "grad_norm": 0.08370701223611832,
      "learning_rate": 0.00016229749631811488,
      "loss": 0.3741,
      "step": 645
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.060701947659254074,
      "learning_rate": 0.00016223858615611194,
      "loss": 0.361,
      "step": 646
    },
    {
      "epoch": 0.19029411764705884,
      "grad_norm": 0.07230162620544434,
      "learning_rate": 0.000162179675994109,
      "loss": 0.4247,
      "step": 647
    },
    {
      "epoch": 0.19058823529411764,
      "grad_norm": 0.07973267883062363,
      "learning_rate": 0.00016212076583210604,
      "loss": 0.3993,
      "step": 648
    },
    {
      "epoch": 0.19088235294117648,
      "grad_norm": 0.06663484871387482,
      "learning_rate": 0.0001620618556701031,
      "loss": 0.3584,
      "step": 649
    },
    {
      "epoch": 0.19117647058823528,
      "grad_norm": 0.05127593129873276,
      "learning_rate": 0.00016200294550810016,
      "loss": 0.3668,
      "step": 650
    },
    {
      "epoch": 0.19147058823529411,
      "grad_norm": 0.0807151049375534,
      "learning_rate": 0.00016194403534609722,
      "loss": 0.3889,
      "step": 651
    },
    {
      "epoch": 0.19176470588235295,
      "grad_norm": 0.07703541964292526,
      "learning_rate": 0.00016188512518409428,
      "loss": 0.3761,
      "step": 652
    },
    {
      "epoch": 0.19205882352941175,
      "grad_norm": 0.06200851500034332,
      "learning_rate": 0.0001618262150220913,
      "loss": 0.3669,
      "step": 653
    },
    {
      "epoch": 0.1923529411764706,
      "grad_norm": 0.06126273795962334,
      "learning_rate": 0.00016176730486008837,
      "loss": 0.3658,
      "step": 654
    },
    {
      "epoch": 0.19264705882352942,
      "grad_norm": 0.05351157486438751,
      "learning_rate": 0.00016170839469808543,
      "loss": 0.3293,
      "step": 655
    },
    {
      "epoch": 0.19294117647058823,
      "grad_norm": 0.03253978118300438,
      "learning_rate": 0.0001616494845360825,
      "loss": 0.1934,
      "step": 656
    },
    {
      "epoch": 0.19323529411764706,
      "grad_norm": 0.06235553324222565,
      "learning_rate": 0.00016159057437407955,
      "loss": 0.3295,
      "step": 657
    },
    {
      "epoch": 0.1935294117647059,
      "grad_norm": 0.0877184271812439,
      "learning_rate": 0.00016153166421207658,
      "loss": 0.3787,
      "step": 658
    },
    {
      "epoch": 0.1938235294117647,
      "grad_norm": 0.06164029613137245,
      "learning_rate": 0.00016147275405007364,
      "loss": 0.3811,
      "step": 659
    },
    {
      "epoch": 0.19411764705882353,
      "grad_norm": 0.055699754506349564,
      "learning_rate": 0.0001614138438880707,
      "loss": 0.3545,
      "step": 660
    },
    {
      "epoch": 0.19441176470588234,
      "grad_norm": 0.07298396527767181,
      "learning_rate": 0.00016135493372606776,
      "loss": 0.3992,
      "step": 661
    },
    {
      "epoch": 0.19470588235294117,
      "grad_norm": 0.06951436400413513,
      "learning_rate": 0.00016129602356406482,
      "loss": 0.2868,
      "step": 662
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.07633385807275772,
      "learning_rate": 0.00016123711340206186,
      "loss": 0.3997,
      "step": 663
    },
    {
      "epoch": 0.1952941176470588,
      "grad_norm": 0.0616174153983593,
      "learning_rate": 0.00016117820324005892,
      "loss": 0.3655,
      "step": 664
    },
    {
      "epoch": 0.19558823529411765,
      "grad_norm": 0.052812717854976654,
      "learning_rate": 0.00016111929307805598,
      "loss": 0.3561,
      "step": 665
    },
    {
      "epoch": 0.19588235294117648,
      "grad_norm": 0.06318505108356476,
      "learning_rate": 0.00016106038291605304,
      "loss": 0.3282,
      "step": 666
    },
    {
      "epoch": 0.19617647058823529,
      "grad_norm": 0.0728445053100586,
      "learning_rate": 0.0001610014727540501,
      "loss": 0.3631,
      "step": 667
    },
    {
      "epoch": 0.19647058823529412,
      "grad_norm": 0.07882007211446762,
      "learning_rate": 0.00016094256259204713,
      "loss": 0.3988,
      "step": 668
    },
    {
      "epoch": 0.19676470588235295,
      "grad_norm": 0.06059527024626732,
      "learning_rate": 0.0001608836524300442,
      "loss": 0.3657,
      "step": 669
    },
    {
      "epoch": 0.19705882352941176,
      "grad_norm": 0.042961303144693375,
      "learning_rate": 0.00016082474226804125,
      "loss": 0.3083,
      "step": 670
    },
    {
      "epoch": 0.1973529411764706,
      "grad_norm": 0.056660596281290054,
      "learning_rate": 0.0001607658321060383,
      "loss": 0.3595,
      "step": 671
    },
    {
      "epoch": 0.1976470588235294,
      "grad_norm": 0.07983566075563431,
      "learning_rate": 0.00016070692194403537,
      "loss": 0.3338,
      "step": 672
    },
    {
      "epoch": 0.19794117647058823,
      "grad_norm": 0.057897549122571945,
      "learning_rate": 0.0001606480117820324,
      "loss": 0.3452,
      "step": 673
    },
    {
      "epoch": 0.19823529411764707,
      "grad_norm": 0.04862534627318382,
      "learning_rate": 0.00016058910162002947,
      "loss": 0.3764,
      "step": 674
    },
    {
      "epoch": 0.19852941176470587,
      "grad_norm": 0.06794068217277527,
      "learning_rate": 0.00016053019145802653,
      "loss": 0.412,
      "step": 675
    },
    {
      "epoch": 0.1988235294117647,
      "grad_norm": 0.07110040634870529,
      "learning_rate": 0.00016047128129602359,
      "loss": 0.4105,
      "step": 676
    },
    {
      "epoch": 0.19911764705882354,
      "grad_norm": 0.04872092232108116,
      "learning_rate": 0.00016041237113402065,
      "loss": 0.3397,
      "step": 677
    },
    {
      "epoch": 0.19941176470588234,
      "grad_norm": 0.04470030963420868,
      "learning_rate": 0.00016035346097201768,
      "loss": 0.3192,
      "step": 678
    },
    {
      "epoch": 0.19970588235294118,
      "grad_norm": 0.05858094245195389,
      "learning_rate": 0.00016029455081001474,
      "loss": 0.4129,
      "step": 679
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.06117147207260132,
      "learning_rate": 0.0001602356406480118,
      "loss": 0.3452,
      "step": 680
    },
    {
      "epoch": 0.20029411764705882,
      "grad_norm": 0.07692009210586548,
      "learning_rate": 0.00016017673048600886,
      "loss": 0.4101,
      "step": 681
    },
    {
      "epoch": 0.20058823529411765,
      "grad_norm": 0.04911952465772629,
      "learning_rate": 0.00016011782032400592,
      "loss": 0.3679,
      "step": 682
    },
    {
      "epoch": 0.20088235294117648,
      "grad_norm": 0.04291140288114548,
      "learning_rate": 0.00016005891016200295,
      "loss": 0.3574,
      "step": 683
    },
    {
      "epoch": 0.2011764705882353,
      "grad_norm": 0.053893450647592545,
      "learning_rate": 0.00016,
      "loss": 0.2949,
      "step": 684
    },
    {
      "epoch": 0.20147058823529412,
      "grad_norm": 0.05680907145142555,
      "learning_rate": 0.00015994108983799707,
      "loss": 0.2984,
      "step": 685
    },
    {
      "epoch": 0.20176470588235293,
      "grad_norm": 0.060923878103494644,
      "learning_rate": 0.00015988217967599413,
      "loss": 0.3785,
      "step": 686
    },
    {
      "epoch": 0.20205882352941176,
      "grad_norm": 0.040785662829875946,
      "learning_rate": 0.0001598232695139912,
      "loss": 0.325,
      "step": 687
    },
    {
      "epoch": 0.2023529411764706,
      "grad_norm": 0.0461050346493721,
      "learning_rate": 0.00015976435935198823,
      "loss": 0.2981,
      "step": 688
    },
    {
      "epoch": 0.2026470588235294,
      "grad_norm": 0.04446212947368622,
      "learning_rate": 0.0001597054491899853,
      "loss": 0.3564,
      "step": 689
    },
    {
      "epoch": 0.20294117647058824,
      "grad_norm": 0.04444172978401184,
      "learning_rate": 0.00015964653902798235,
      "loss": 0.2802,
      "step": 690
    },
    {
      "epoch": 0.20323529411764707,
      "grad_norm": 0.048090510070323944,
      "learning_rate": 0.0001595876288659794,
      "loss": 0.2993,
      "step": 691
    },
    {
      "epoch": 0.20352941176470588,
      "grad_norm": 0.06313694268465042,
      "learning_rate": 0.00015952871870397647,
      "loss": 0.3867,
      "step": 692
    },
    {
      "epoch": 0.2038235294117647,
      "grad_norm": 0.059502724558115005,
      "learning_rate": 0.0001594698085419735,
      "loss": 0.4048,
      "step": 693
    },
    {
      "epoch": 0.20411764705882354,
      "grad_norm": 0.06442804634571075,
      "learning_rate": 0.00015941089837997056,
      "loss": 0.3585,
      "step": 694
    },
    {
      "epoch": 0.20441176470588235,
      "grad_norm": 0.050439152866601944,
      "learning_rate": 0.0001593519882179676,
      "loss": 0.3517,
      "step": 695
    },
    {
      "epoch": 0.20470588235294118,
      "grad_norm": 0.07076825946569443,
      "learning_rate": 0.00015929307805596465,
      "loss": 0.3776,
      "step": 696
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.06291080266237259,
      "learning_rate": 0.00015923416789396171,
      "loss": 0.4041,
      "step": 697
    },
    {
      "epoch": 0.20529411764705882,
      "grad_norm": 0.04894523322582245,
      "learning_rate": 0.00015917525773195875,
      "loss": 0.339,
      "step": 698
    },
    {
      "epoch": 0.20558823529411765,
      "grad_norm": 0.05032701790332794,
      "learning_rate": 0.0001591163475699558,
      "loss": 0.3302,
      "step": 699
    },
    {
      "epoch": 0.20588235294117646,
      "grad_norm": 0.056529004126787186,
      "learning_rate": 0.00015905743740795287,
      "loss": 0.3743,
      "step": 700
    },
    {
      "epoch": 0.2061764705882353,
      "grad_norm": 0.058404359966516495,
      "learning_rate": 0.00015899852724594993,
      "loss": 0.3456,
      "step": 701
    },
    {
      "epoch": 0.20647058823529413,
      "grad_norm": 0.05290975049138069,
      "learning_rate": 0.000158939617083947,
      "loss": 0.348,
      "step": 702
    },
    {
      "epoch": 0.20676470588235293,
      "grad_norm": 0.061785612255334854,
      "learning_rate": 0.00015888070692194402,
      "loss": 0.3425,
      "step": 703
    },
    {
      "epoch": 0.20705882352941177,
      "grad_norm": 0.053690794855356216,
      "learning_rate": 0.00015882179675994108,
      "loss": 0.3287,
      "step": 704
    },
    {
      "epoch": 0.2073529411764706,
      "grad_norm": 0.07636330276727676,
      "learning_rate": 0.00015876288659793814,
      "loss": 0.4783,
      "step": 705
    },
    {
      "epoch": 0.2076470588235294,
      "grad_norm": 0.06201598420739174,
      "learning_rate": 0.0001587039764359352,
      "loss": 0.3889,
      "step": 706
    },
    {
      "epoch": 0.20794117647058824,
      "grad_norm": 0.055612195283174515,
      "learning_rate": 0.00015864506627393226,
      "loss": 0.3632,
      "step": 707
    },
    {
      "epoch": 0.20823529411764705,
      "grad_norm": 0.04832616448402405,
      "learning_rate": 0.0001585861561119293,
      "loss": 0.2707,
      "step": 708
    },
    {
      "epoch": 0.20852941176470588,
      "grad_norm": 0.06464774906635284,
      "learning_rate": 0.00015852724594992635,
      "loss": 0.4143,
      "step": 709
    },
    {
      "epoch": 0.2088235294117647,
      "grad_norm": 0.04137435927987099,
      "learning_rate": 0.00015846833578792341,
      "loss": 0.2947,
      "step": 710
    },
    {
      "epoch": 0.20911764705882352,
      "grad_norm": 0.0571475476026535,
      "learning_rate": 0.00015840942562592047,
      "loss": 0.3184,
      "step": 711
    },
    {
      "epoch": 0.20941176470588235,
      "grad_norm": 0.06978932023048401,
      "learning_rate": 0.00015835051546391754,
      "loss": 0.3849,
      "step": 712
    },
    {
      "epoch": 0.2097058823529412,
      "grad_norm": 0.04857069253921509,
      "learning_rate": 0.00015829160530191457,
      "loss": 0.3517,
      "step": 713
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.04477962478995323,
      "learning_rate": 0.00015823269513991163,
      "loss": 0.331,
      "step": 714
    },
    {
      "epoch": 0.21029411764705883,
      "grad_norm": 0.05800754204392433,
      "learning_rate": 0.0001581737849779087,
      "loss": 0.307,
      "step": 715
    },
    {
      "epoch": 0.21058823529411766,
      "grad_norm": 0.058337271213531494,
      "learning_rate": 0.00015811487481590575,
      "loss": 0.3521,
      "step": 716
    },
    {
      "epoch": 0.21088235294117647,
      "grad_norm": 0.0661049485206604,
      "learning_rate": 0.0001580559646539028,
      "loss": 0.3996,
      "step": 717
    },
    {
      "epoch": 0.2111764705882353,
      "grad_norm": 0.05764153599739075,
      "learning_rate": 0.00015799705449189984,
      "loss": 0.4067,
      "step": 718
    },
    {
      "epoch": 0.2114705882352941,
      "grad_norm": 0.04720641300082207,
      "learning_rate": 0.0001579381443298969,
      "loss": 0.3394,
      "step": 719
    },
    {
      "epoch": 0.21176470588235294,
      "grad_norm": 0.060476917773485184,
      "learning_rate": 0.00015787923416789396,
      "loss": 0.3574,
      "step": 720
    },
    {
      "epoch": 0.21205882352941177,
      "grad_norm": 0.04659063741564751,
      "learning_rate": 0.00015782032400589102,
      "loss": 0.2939,
      "step": 721
    },
    {
      "epoch": 0.21235294117647058,
      "grad_norm": 0.0629427507519722,
      "learning_rate": 0.00015776141384388808,
      "loss": 0.35,
      "step": 722
    },
    {
      "epoch": 0.2126470588235294,
      "grad_norm": 0.06532291322946548,
      "learning_rate": 0.00015770250368188512,
      "loss": 0.3916,
      "step": 723
    },
    {
      "epoch": 0.21294117647058824,
      "grad_norm": 0.05802358686923981,
      "learning_rate": 0.00015764359351988218,
      "loss": 0.3201,
      "step": 724
    },
    {
      "epoch": 0.21323529411764705,
      "grad_norm": 0.047698717564344406,
      "learning_rate": 0.00015758468335787924,
      "loss": 0.3007,
      "step": 725
    },
    {
      "epoch": 0.21352941176470588,
      "grad_norm": 0.06573429703712463,
      "learning_rate": 0.0001575257731958763,
      "loss": 0.2949,
      "step": 726
    },
    {
      "epoch": 0.21382352941176472,
      "grad_norm": 0.06530513614416122,
      "learning_rate": 0.00015746686303387336,
      "loss": 0.3977,
      "step": 727
    },
    {
      "epoch": 0.21411764705882352,
      "grad_norm": 0.05426618084311485,
      "learning_rate": 0.0001574079528718704,
      "loss": 0.3868,
      "step": 728
    },
    {
      "epoch": 0.21441176470588236,
      "grad_norm": 0.06537091732025146,
      "learning_rate": 0.00015734904270986745,
      "loss": 0.4129,
      "step": 729
    },
    {
      "epoch": 0.21470588235294116,
      "grad_norm": 0.06139171123504639,
      "learning_rate": 0.0001572901325478645,
      "loss": 0.4059,
      "step": 730
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.0537787601351738,
      "learning_rate": 0.00015723122238586157,
      "loss": 0.3616,
      "step": 731
    },
    {
      "epoch": 0.21529411764705883,
      "grad_norm": 0.0435417965054512,
      "learning_rate": 0.00015717231222385863,
      "loss": 0.2964,
      "step": 732
    },
    {
      "epoch": 0.21558823529411764,
      "grad_norm": 0.07378406077623367,
      "learning_rate": 0.00015711340206185566,
      "loss": 0.4187,
      "step": 733
    },
    {
      "epoch": 0.21588235294117647,
      "grad_norm": 0.0531664676964283,
      "learning_rate": 0.00015705449189985272,
      "loss": 0.3912,
      "step": 734
    },
    {
      "epoch": 0.2161764705882353,
      "grad_norm": 0.047783706337213516,
      "learning_rate": 0.00015699558173784978,
      "loss": 0.3298,
      "step": 735
    },
    {
      "epoch": 0.2164705882352941,
      "grad_norm": 0.05372120440006256,
      "learning_rate": 0.00015693667157584684,
      "loss": 0.354,
      "step": 736
    },
    {
      "epoch": 0.21676470588235294,
      "grad_norm": 0.05178256705403328,
      "learning_rate": 0.0001568777614138439,
      "loss": 0.3821,
      "step": 737
    },
    {
      "epoch": 0.21705882352941178,
      "grad_norm": 0.05051824077963829,
      "learning_rate": 0.00015681885125184094,
      "loss": 0.3936,
      "step": 738
    },
    {
      "epoch": 0.21735294117647058,
      "grad_norm": 0.04216437041759491,
      "learning_rate": 0.000156759941089838,
      "loss": 0.3298,
      "step": 739
    },
    {
      "epoch": 0.21764705882352942,
      "grad_norm": 0.038164228200912476,
      "learning_rate": 0.00015670103092783506,
      "loss": 0.2802,
      "step": 740
    },
    {
      "epoch": 0.21794117647058822,
      "grad_norm": 0.031239116564393044,
      "learning_rate": 0.00015664212076583212,
      "loss": 0.2477,
      "step": 741
    },
    {
      "epoch": 0.21823529411764706,
      "grad_norm": 0.06569403409957886,
      "learning_rate": 0.00015658321060382918,
      "loss": 0.4356,
      "step": 742
    },
    {
      "epoch": 0.2185294117647059,
      "grad_norm": 0.03764916956424713,
      "learning_rate": 0.0001565243004418262,
      "loss": 0.2876,
      "step": 743
    },
    {
      "epoch": 0.2188235294117647,
      "grad_norm": 0.05332646891474724,
      "learning_rate": 0.00015646539027982327,
      "loss": 0.3227,
      "step": 744
    },
    {
      "epoch": 0.21911764705882353,
      "grad_norm": 0.0500241294503212,
      "learning_rate": 0.00015640648011782033,
      "loss": 0.3591,
      "step": 745
    },
    {
      "epoch": 0.21941176470588236,
      "grad_norm": 0.059934645891189575,
      "learning_rate": 0.0001563475699558174,
      "loss": 0.4106,
      "step": 746
    },
    {
      "epoch": 0.21970588235294117,
      "grad_norm": 0.04889747127890587,
      "learning_rate": 0.00015628865979381445,
      "loss": 0.3166,
      "step": 747
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.048640813678503036,
      "learning_rate": 0.00015622974963181148,
      "loss": 0.3461,
      "step": 748
    },
    {
      "epoch": 0.22029411764705883,
      "grad_norm": 0.039814915508031845,
      "learning_rate": 0.00015617083946980854,
      "loss": 0.2832,
      "step": 749
    },
    {
      "epoch": 0.22058823529411764,
      "grad_norm": 0.059623461216688156,
      "learning_rate": 0.0001561119293078056,
      "loss": 0.3801,
      "step": 750
    },
    {
      "epoch": 0.22088235294117647,
      "grad_norm": 0.044037219136953354,
      "learning_rate": 0.00015605301914580266,
      "loss": 0.3507,
      "step": 751
    },
    {
      "epoch": 0.2211764705882353,
      "grad_norm": 0.078094482421875,
      "learning_rate": 0.00015599410898379972,
      "loss": 0.4416,
      "step": 752
    },
    {
      "epoch": 0.2214705882352941,
      "grad_norm": 0.05183517932891846,
      "learning_rate": 0.00015593519882179676,
      "loss": 0.3962,
      "step": 753
    },
    {
      "epoch": 0.22176470588235295,
      "grad_norm": 0.049515582621097565,
      "learning_rate": 0.00015587628865979382,
      "loss": 0.3595,
      "step": 754
    },
    {
      "epoch": 0.22205882352941175,
      "grad_norm": 0.05179493501782417,
      "learning_rate": 0.00015581737849779088,
      "loss": 0.3304,
      "step": 755
    },
    {
      "epoch": 0.2223529411764706,
      "grad_norm": 0.04011007770895958,
      "learning_rate": 0.00015575846833578794,
      "loss": 0.3337,
      "step": 756
    },
    {
      "epoch": 0.22264705882352942,
      "grad_norm": 0.07643067836761475,
      "learning_rate": 0.000155699558173785,
      "loss": 0.3598,
      "step": 757
    },
    {
      "epoch": 0.22294117647058823,
      "grad_norm": 0.05183534696698189,
      "learning_rate": 0.00015564064801178203,
      "loss": 0.3348,
      "step": 758
    },
    {
      "epoch": 0.22323529411764706,
      "grad_norm": 0.05867390334606171,
      "learning_rate": 0.0001555817378497791,
      "loss": 0.3426,
      "step": 759
    },
    {
      "epoch": 0.2235294117647059,
      "grad_norm": 0.045140914618968964,
      "learning_rate": 0.00015552282768777615,
      "loss": 0.3419,
      "step": 760
    },
    {
      "epoch": 0.2238235294117647,
      "grad_norm": 0.06231553107500076,
      "learning_rate": 0.0001554639175257732,
      "loss": 0.3605,
      "step": 761
    },
    {
      "epoch": 0.22411764705882353,
      "grad_norm": 0.041879162192344666,
      "learning_rate": 0.00015540500736377027,
      "loss": 0.3444,
      "step": 762
    },
    {
      "epoch": 0.22441176470588237,
      "grad_norm": 0.0464022234082222,
      "learning_rate": 0.0001553460972017673,
      "loss": 0.3409,
      "step": 763
    },
    {
      "epoch": 0.22470588235294117,
      "grad_norm": 0.05099009349942207,
      "learning_rate": 0.00015528718703976437,
      "loss": 0.3948,
      "step": 764
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.04478596895933151,
      "learning_rate": 0.00015522827687776143,
      "loss": 0.3557,
      "step": 765
    },
    {
      "epoch": 0.2252941176470588,
      "grad_norm": 0.048449981957674026,
      "learning_rate": 0.00015516936671575849,
      "loss": 0.3608,
      "step": 766
    },
    {
      "epoch": 0.22558823529411764,
      "grad_norm": 0.08497659862041473,
      "learning_rate": 0.00015511045655375555,
      "loss": 0.4206,
      "step": 767
    },
    {
      "epoch": 0.22588235294117648,
      "grad_norm": 0.061473604291677475,
      "learning_rate": 0.00015505154639175258,
      "loss": 0.3059,
      "step": 768
    },
    {
      "epoch": 0.22617647058823528,
      "grad_norm": 0.05752651020884514,
      "learning_rate": 0.00015499263622974964,
      "loss": 0.3629,
      "step": 769
    },
    {
      "epoch": 0.22647058823529412,
      "grad_norm": 0.06633611023426056,
      "learning_rate": 0.0001549337260677467,
      "loss": 0.3843,
      "step": 770
    },
    {
      "epoch": 0.22676470588235295,
      "grad_norm": 0.05157380923628807,
      "learning_rate": 0.00015487481590574376,
      "loss": 0.3584,
      "step": 771
    },
    {
      "epoch": 0.22705882352941176,
      "grad_norm": 0.051166702061891556,
      "learning_rate": 0.00015481590574374082,
      "loss": 0.301,
      "step": 772
    },
    {
      "epoch": 0.2273529411764706,
      "grad_norm": 0.042449697852134705,
      "learning_rate": 0.00015475699558173785,
      "loss": 0.2995,
      "step": 773
    },
    {
      "epoch": 0.22764705882352942,
      "grad_norm": 0.04250197485089302,
      "learning_rate": 0.0001546980854197349,
      "loss": 0.3692,
      "step": 774
    },
    {
      "epoch": 0.22794117647058823,
      "grad_norm": 0.053548119962215424,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.4043,
      "step": 775
    },
    {
      "epoch": 0.22823529411764706,
      "grad_norm": 0.04838399589061737,
      "learning_rate": 0.00015458026509572903,
      "loss": 0.3087,
      "step": 776
    },
    {
      "epoch": 0.22852941176470587,
      "grad_norm": 0.0504065677523613,
      "learning_rate": 0.0001545213549337261,
      "loss": 0.3544,
      "step": 777
    },
    {
      "epoch": 0.2288235294117647,
      "grad_norm": 0.04909535124897957,
      "learning_rate": 0.00015446244477172313,
      "loss": 0.2677,
      "step": 778
    },
    {
      "epoch": 0.22911764705882354,
      "grad_norm": 0.0523778572678566,
      "learning_rate": 0.0001544035346097202,
      "loss": 0.3244,
      "step": 779
    },
    {
      "epoch": 0.22941176470588234,
      "grad_norm": 0.04403742030262947,
      "learning_rate": 0.00015434462444771725,
      "loss": 0.265,
      "step": 780
    },
    {
      "epoch": 0.22970588235294118,
      "grad_norm": 0.04736926779150963,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3697,
      "step": 781
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.05434809625148773,
      "learning_rate": 0.00015422680412371137,
      "loss": 0.3761,
      "step": 782
    },
    {
      "epoch": 0.23029411764705882,
      "grad_norm": 0.05977502837777138,
      "learning_rate": 0.0001541678939617084,
      "loss": 0.4036,
      "step": 783
    },
    {
      "epoch": 0.23058823529411765,
      "grad_norm": 0.051600657403469086,
      "learning_rate": 0.00015410898379970546,
      "loss": 0.3338,
      "step": 784
    },
    {
      "epoch": 0.23088235294117648,
      "grad_norm": 0.05443185567855835,
      "learning_rate": 0.00015405007363770252,
      "loss": 0.367,
      "step": 785
    },
    {
      "epoch": 0.2311764705882353,
      "grad_norm": 0.04886242002248764,
      "learning_rate": 0.00015399116347569958,
      "loss": 0.3254,
      "step": 786
    },
    {
      "epoch": 0.23147058823529412,
      "grad_norm": 0.0525943823158741,
      "learning_rate": 0.00015393225331369664,
      "loss": 0.3555,
      "step": 787
    },
    {
      "epoch": 0.23176470588235293,
      "grad_norm": 0.06001102179288864,
      "learning_rate": 0.00015387334315169367,
      "loss": 0.3846,
      "step": 788
    },
    {
      "epoch": 0.23205882352941176,
      "grad_norm": 0.05627163127064705,
      "learning_rate": 0.00015381443298969073,
      "loss": 0.4,
      "step": 789
    },
    {
      "epoch": 0.2323529411764706,
      "grad_norm": 0.05016671121120453,
      "learning_rate": 0.0001537555228276878,
      "loss": 0.4136,
      "step": 790
    },
    {
      "epoch": 0.2326470588235294,
      "grad_norm": 0.0548783503472805,
      "learning_rate": 0.00015369661266568485,
      "loss": 0.3869,
      "step": 791
    },
    {
      "epoch": 0.23294117647058823,
      "grad_norm": 0.05256401002407074,
      "learning_rate": 0.00015363770250368191,
      "loss": 0.3168,
      "step": 792
    },
    {
      "epoch": 0.23323529411764707,
      "grad_norm": 0.0504455491900444,
      "learning_rate": 0.00015357879234167895,
      "loss": 0.387,
      "step": 793
    },
    {
      "epoch": 0.23352941176470587,
      "grad_norm": 0.056336719542741776,
      "learning_rate": 0.000153519882179676,
      "loss": 0.3525,
      "step": 794
    },
    {
      "epoch": 0.2338235294117647,
      "grad_norm": 0.052811261266469955,
      "learning_rate": 0.00015346097201767307,
      "loss": 0.349,
      "step": 795
    },
    {
      "epoch": 0.23411764705882354,
      "grad_norm": 0.06716068089008331,
      "learning_rate": 0.00015340206185567013,
      "loss": 0.3651,
      "step": 796
    },
    {
      "epoch": 0.23441176470588235,
      "grad_norm": 0.05453069880604744,
      "learning_rate": 0.0001533431516936672,
      "loss": 0.3116,
      "step": 797
    },
    {
      "epoch": 0.23470588235294118,
      "grad_norm": 0.03846844285726547,
      "learning_rate": 0.00015328424153166422,
      "loss": 0.2707,
      "step": 798
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.05943136662244797,
      "learning_rate": 0.00015322533136966128,
      "loss": 0.3408,
      "step": 799
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.04608723521232605,
      "learning_rate": 0.00015316642120765834,
      "loss": 0.3243,
      "step": 800
    },
    {
      "epoch": 0.23558823529411765,
      "grad_norm": 0.07181533426046371,
      "learning_rate": 0.00015310751104565537,
      "loss": 0.3715,
      "step": 801
    },
    {
      "epoch": 0.23588235294117646,
      "grad_norm": 0.04271401837468147,
      "learning_rate": 0.00015304860088365243,
      "loss": 0.3159,
      "step": 802
    },
    {
      "epoch": 0.2361764705882353,
      "grad_norm": 0.04295480623841286,
      "learning_rate": 0.00015298969072164947,
      "loss": 0.3343,
      "step": 803
    },
    {
      "epoch": 0.23647058823529413,
      "grad_norm": 0.08767161518335342,
      "learning_rate": 0.00015293078055964653,
      "loss": 0.4023,
      "step": 804
    },
    {
      "epoch": 0.23676470588235293,
      "grad_norm": 0.05766723304986954,
      "learning_rate": 0.0001528718703976436,
      "loss": 0.38,
      "step": 805
    },
    {
      "epoch": 0.23705882352941177,
      "grad_norm": 0.06087141111493111,
      "learning_rate": 0.00015281296023564065,
      "loss": 0.3855,
      "step": 806
    },
    {
      "epoch": 0.2373529411764706,
      "grad_norm": 0.07504145801067352,
      "learning_rate": 0.0001527540500736377,
      "loss": 0.4296,
      "step": 807
    },
    {
      "epoch": 0.2376470588235294,
      "grad_norm": 0.059098973870277405,
      "learning_rate": 0.00015269513991163474,
      "loss": 0.3334,
      "step": 808
    },
    {
      "epoch": 0.23794117647058824,
      "grad_norm": 0.054836634546518326,
      "learning_rate": 0.0001526362297496318,
      "loss": 0.3732,
      "step": 809
    },
    {
      "epoch": 0.23823529411764705,
      "grad_norm": 0.054932720959186554,
      "learning_rate": 0.00015257731958762886,
      "loss": 0.3643,
      "step": 810
    },
    {
      "epoch": 0.23852941176470588,
      "grad_norm": 0.056128401309251785,
      "learning_rate": 0.00015251840942562592,
      "loss": 0.4055,
      "step": 811
    },
    {
      "epoch": 0.2388235294117647,
      "grad_norm": 0.06307770311832428,
      "learning_rate": 0.00015245949926362298,
      "loss": 0.3228,
      "step": 812
    },
    {
      "epoch": 0.23911764705882352,
      "grad_norm": 0.054991524666547775,
      "learning_rate": 0.00015240058910162002,
      "loss": 0.3676,
      "step": 813
    },
    {
      "epoch": 0.23941176470588235,
      "grad_norm": 0.04872496798634529,
      "learning_rate": 0.00015234167893961708,
      "loss": 0.3724,
      "step": 814
    },
    {
      "epoch": 0.23970588235294119,
      "grad_norm": 0.04811226204037666,
      "learning_rate": 0.00015228276877761414,
      "loss": 0.3281,
      "step": 815
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.05121970549225807,
      "learning_rate": 0.0001522238586156112,
      "loss": 0.3566,
      "step": 816
    },
    {
      "epoch": 0.24029411764705882,
      "grad_norm": 0.057808730751276016,
      "learning_rate": 0.00015216494845360826,
      "loss": 0.3152,
      "step": 817
    },
    {
      "epoch": 0.24058823529411766,
      "grad_norm": 0.06310531497001648,
      "learning_rate": 0.0001521060382916053,
      "loss": 0.3562,
      "step": 818
    },
    {
      "epoch": 0.24088235294117646,
      "grad_norm": 0.04868234694004059,
      "learning_rate": 0.00015204712812960235,
      "loss": 0.3329,
      "step": 819
    },
    {
      "epoch": 0.2411764705882353,
      "grad_norm": 0.06056853011250496,
      "learning_rate": 0.0001519882179675994,
      "loss": 0.3859,
      "step": 820
    },
    {
      "epoch": 0.24147058823529413,
      "grad_norm": 0.06528452038764954,
      "learning_rate": 0.00015192930780559647,
      "loss": 0.3617,
      "step": 821
    },
    {
      "epoch": 0.24176470588235294,
      "grad_norm": 0.05230215564370155,
      "learning_rate": 0.00015187039764359353,
      "loss": 0.2949,
      "step": 822
    },
    {
      "epoch": 0.24205882352941177,
      "grad_norm": 0.06731203198432922,
      "learning_rate": 0.00015181148748159056,
      "loss": 0.3657,
      "step": 823
    },
    {
      "epoch": 0.24235294117647058,
      "grad_norm": 0.056145813316106796,
      "learning_rate": 0.00015175257731958762,
      "loss": 0.3566,
      "step": 824
    },
    {
      "epoch": 0.2426470588235294,
      "grad_norm": 0.06409990042448044,
      "learning_rate": 0.00015169366715758468,
      "loss": 0.3839,
      "step": 825
    },
    {
      "epoch": 0.24294117647058824,
      "grad_norm": 0.06945058703422546,
      "learning_rate": 0.00015163475699558174,
      "loss": 0.4816,
      "step": 826
    },
    {
      "epoch": 0.24323529411764705,
      "grad_norm": 0.06122128292918205,
      "learning_rate": 0.0001515758468335788,
      "loss": 0.3622,
      "step": 827
    },
    {
      "epoch": 0.24352941176470588,
      "grad_norm": 0.05165794491767883,
      "learning_rate": 0.00015151693667157584,
      "loss": 0.3853,
      "step": 828
    },
    {
      "epoch": 0.24382352941176472,
      "grad_norm": 0.04984120652079582,
      "learning_rate": 0.0001514580265095729,
      "loss": 0.3838,
      "step": 829
    },
    {
      "epoch": 0.24411764705882352,
      "grad_norm": 0.04035786911845207,
      "learning_rate": 0.00015139911634756996,
      "loss": 0.2741,
      "step": 830
    },
    {
      "epoch": 0.24441176470588236,
      "grad_norm": 0.0698532834649086,
      "learning_rate": 0.00015134020618556702,
      "loss": 0.4192,
      "step": 831
    },
    {
      "epoch": 0.2447058823529412,
      "grad_norm": 0.05118882283568382,
      "learning_rate": 0.00015128129602356408,
      "loss": 0.3658,
      "step": 832
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.07093042880296707,
      "learning_rate": 0.0001512223858615611,
      "loss": 0.3019,
      "step": 833
    },
    {
      "epoch": 0.24529411764705883,
      "grad_norm": 0.05597876384854317,
      "learning_rate": 0.00015116347569955817,
      "loss": 0.3496,
      "step": 834
    },
    {
      "epoch": 0.24558823529411763,
      "grad_norm": 0.06814400851726532,
      "learning_rate": 0.00015110456553755523,
      "loss": 0.3652,
      "step": 835
    },
    {
      "epoch": 0.24588235294117647,
      "grad_norm": 0.06781720370054245,
      "learning_rate": 0.0001510456553755523,
      "loss": 0.3442,
      "step": 836
    },
    {
      "epoch": 0.2461764705882353,
      "grad_norm": 0.042514536529779434,
      "learning_rate": 0.00015098674521354935,
      "loss": 0.3874,
      "step": 837
    },
    {
      "epoch": 0.2464705882352941,
      "grad_norm": 0.057626936584711075,
      "learning_rate": 0.00015092783505154638,
      "loss": 0.3738,
      "step": 838
    },
    {
      "epoch": 0.24676470588235294,
      "grad_norm": 0.054062873125076294,
      "learning_rate": 0.00015086892488954344,
      "loss": 0.3188,
      "step": 839
    },
    {
      "epoch": 0.24705882352941178,
      "grad_norm": 0.059099771082401276,
      "learning_rate": 0.0001508100147275405,
      "loss": 0.3226,
      "step": 840
    },
    {
      "epoch": 0.24735294117647058,
      "grad_norm": 0.063271664083004,
      "learning_rate": 0.00015075110456553756,
      "loss": 0.4139,
      "step": 841
    },
    {
      "epoch": 0.24764705882352941,
      "grad_norm": 0.06892070919275284,
      "learning_rate": 0.00015069219440353462,
      "loss": 0.3332,
      "step": 842
    },
    {
      "epoch": 0.24794117647058825,
      "grad_norm": 0.05576075240969658,
      "learning_rate": 0.00015063328424153166,
      "loss": 0.3454,
      "step": 843
    },
    {
      "epoch": 0.24823529411764705,
      "grad_norm": 0.05657346546649933,
      "learning_rate": 0.00015057437407952872,
      "loss": 0.2995,
      "step": 844
    },
    {
      "epoch": 0.2485294117647059,
      "grad_norm": 0.06410253047943115,
      "learning_rate": 0.00015051546391752578,
      "loss": 0.3424,
      "step": 845
    },
    {
      "epoch": 0.2488235294117647,
      "grad_norm": 0.06051955372095108,
      "learning_rate": 0.00015045655375552284,
      "loss": 0.335,
      "step": 846
    },
    {
      "epoch": 0.24911764705882353,
      "grad_norm": 0.05353469029068947,
      "learning_rate": 0.0001503976435935199,
      "loss": 0.3578,
      "step": 847
    },
    {
      "epoch": 0.24941176470588236,
      "grad_norm": 0.06305938959121704,
      "learning_rate": 0.00015033873343151693,
      "loss": 0.402,
      "step": 848
    },
    {
      "epoch": 0.24970588235294117,
      "grad_norm": 0.053310081362724304,
      "learning_rate": 0.000150279823269514,
      "loss": 0.3336,
      "step": 849
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.0641208216547966,
      "learning_rate": 0.00015022091310751105,
      "loss": 0.4387,
      "step": 850
    },
    {
      "epoch": 0.25029411764705883,
      "grad_norm": 0.05950336158275604,
      "learning_rate": 0.0001501620029455081,
      "loss": 0.3822,
      "step": 851
    },
    {
      "epoch": 0.25058823529411767,
      "grad_norm": 0.060848936438560486,
      "learning_rate": 0.00015010309278350517,
      "loss": 0.389,
      "step": 852
    },
    {
      "epoch": 0.25088235294117645,
      "grad_norm": 0.04849682003259659,
      "learning_rate": 0.0001500441826215022,
      "loss": 0.3936,
      "step": 853
    },
    {
      "epoch": 0.2511764705882353,
      "grad_norm": 0.05510273575782776,
      "learning_rate": 0.00014998527245949927,
      "loss": 0.3608,
      "step": 854
    },
    {
      "epoch": 0.2514705882352941,
      "grad_norm": 0.06138743832707405,
      "learning_rate": 0.00014992636229749633,
      "loss": 0.3604,
      "step": 855
    },
    {
      "epoch": 0.25176470588235295,
      "grad_norm": 0.05221571773290634,
      "learning_rate": 0.00014986745213549339,
      "loss": 0.3548,
      "step": 856
    },
    {
      "epoch": 0.2520588235294118,
      "grad_norm": 0.06734069436788559,
      "learning_rate": 0.00014980854197349045,
      "loss": 0.4444,
      "step": 857
    },
    {
      "epoch": 0.2523529411764706,
      "grad_norm": 0.06240047886967659,
      "learning_rate": 0.00014974963181148748,
      "loss": 0.3968,
      "step": 858
    },
    {
      "epoch": 0.2526470588235294,
      "grad_norm": 0.05789473280310631,
      "learning_rate": 0.00014969072164948454,
      "loss": 0.383,
      "step": 859
    },
    {
      "epoch": 0.2529411764705882,
      "grad_norm": 0.05539305880665779,
      "learning_rate": 0.0001496318114874816,
      "loss": 0.3355,
      "step": 860
    },
    {
      "epoch": 0.25323529411764706,
      "grad_norm": 0.04203849658370018,
      "learning_rate": 0.00014957290132547866,
      "loss": 0.2664,
      "step": 861
    },
    {
      "epoch": 0.2535294117647059,
      "grad_norm": 0.054852575063705444,
      "learning_rate": 0.00014951399116347572,
      "loss": 0.3417,
      "step": 862
    },
    {
      "epoch": 0.2538235294117647,
      "grad_norm": 0.05410647764801979,
      "learning_rate": 0.00014945508100147275,
      "loss": 0.3605,
      "step": 863
    },
    {
      "epoch": 0.2541176470588235,
      "grad_norm": 0.043885912746191025,
      "learning_rate": 0.0001493961708394698,
      "loss": 0.3569,
      "step": 864
    },
    {
      "epoch": 0.25441176470588234,
      "grad_norm": 0.05074778571724892,
      "learning_rate": 0.00014933726067746687,
      "loss": 0.4279,
      "step": 865
    },
    {
      "epoch": 0.25470588235294117,
      "grad_norm": 0.03978065773844719,
      "learning_rate": 0.00014927835051546393,
      "loss": 0.3081,
      "step": 866
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.05362002179026604,
      "learning_rate": 0.000149219440353461,
      "loss": 0.3496,
      "step": 867
    },
    {
      "epoch": 0.25529411764705884,
      "grad_norm": 0.058729711920022964,
      "learning_rate": 0.00014916053019145803,
      "loss": 0.4101,
      "step": 868
    },
    {
      "epoch": 0.25558823529411767,
      "grad_norm": 0.052416399121284485,
      "learning_rate": 0.0001491016200294551,
      "loss": 0.3301,
      "step": 869
    },
    {
      "epoch": 0.25588235294117645,
      "grad_norm": 0.05703870579600334,
      "learning_rate": 0.00014904270986745215,
      "loss": 0.3518,
      "step": 870
    },
    {
      "epoch": 0.2561764705882353,
      "grad_norm": 0.055231329053640366,
      "learning_rate": 0.0001489837997054492,
      "loss": 0.3545,
      "step": 871
    },
    {
      "epoch": 0.2564705882352941,
      "grad_norm": 0.06382600218057632,
      "learning_rate": 0.00014892488954344627,
      "loss": 0.3608,
      "step": 872
    },
    {
      "epoch": 0.25676470588235295,
      "grad_norm": 0.06119081750512123,
      "learning_rate": 0.0001488659793814433,
      "loss": 0.36,
      "step": 873
    },
    {
      "epoch": 0.2570588235294118,
      "grad_norm": 0.08137867599725723,
      "learning_rate": 0.00014880706921944036,
      "loss": 0.4034,
      "step": 874
    },
    {
      "epoch": 0.25735294117647056,
      "grad_norm": 0.07120561599731445,
      "learning_rate": 0.00014874815905743742,
      "loss": 0.4204,
      "step": 875
    },
    {
      "epoch": 0.2576470588235294,
      "grad_norm": 0.055597659200429916,
      "learning_rate": 0.00014868924889543448,
      "loss": 0.3608,
      "step": 876
    },
    {
      "epoch": 0.25794117647058823,
      "grad_norm": 0.046641796827316284,
      "learning_rate": 0.00014863033873343154,
      "loss": 0.3481,
      "step": 877
    },
    {
      "epoch": 0.25823529411764706,
      "grad_norm": 0.041125234216451645,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3112,
      "step": 878
    },
    {
      "epoch": 0.2585294117647059,
      "grad_norm": 0.036888379603624344,
      "learning_rate": 0.00014851251840942563,
      "loss": 0.2504,
      "step": 879
    },
    {
      "epoch": 0.25882352941176473,
      "grad_norm": 0.05571471527218819,
      "learning_rate": 0.0001484536082474227,
      "loss": 0.3481,
      "step": 880
    },
    {
      "epoch": 0.2591176470588235,
      "grad_norm": 0.04781299829483032,
      "learning_rate": 0.00014839469808541975,
      "loss": 0.3333,
      "step": 881
    },
    {
      "epoch": 0.25941176470588234,
      "grad_norm": 0.059225380420684814,
      "learning_rate": 0.00014833578792341681,
      "loss": 0.3813,
      "step": 882
    },
    {
      "epoch": 0.2597058823529412,
      "grad_norm": 0.06043194234371185,
      "learning_rate": 0.00014827687776141385,
      "loss": 0.4207,
      "step": 883
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.05455294996500015,
      "learning_rate": 0.0001482179675994109,
      "loss": 0.3446,
      "step": 884
    },
    {
      "epoch": 0.26029411764705884,
      "grad_norm": 0.053884029388427734,
      "learning_rate": 0.00014815905743740797,
      "loss": 0.3898,
      "step": 885
    },
    {
      "epoch": 0.2605882352941176,
      "grad_norm": 0.0551350973546505,
      "learning_rate": 0.00014810014727540503,
      "loss": 0.376,
      "step": 886
    },
    {
      "epoch": 0.26088235294117645,
      "grad_norm": 0.061660684645175934,
      "learning_rate": 0.0001480412371134021,
      "loss": 0.4514,
      "step": 887
    },
    {
      "epoch": 0.2611764705882353,
      "grad_norm": 0.06455861032009125,
      "learning_rate": 0.00014798232695139912,
      "loss": 0.3512,
      "step": 888
    },
    {
      "epoch": 0.2614705882352941,
      "grad_norm": 0.05554740130901337,
      "learning_rate": 0.00014792341678939618,
      "loss": 0.3482,
      "step": 889
    },
    {
      "epoch": 0.26176470588235295,
      "grad_norm": 0.054420437663793564,
      "learning_rate": 0.00014786450662739324,
      "loss": 0.2929,
      "step": 890
    },
    {
      "epoch": 0.2620588235294118,
      "grad_norm": 0.06252085417509079,
      "learning_rate": 0.0001478055964653903,
      "loss": 0.3738,
      "step": 891
    },
    {
      "epoch": 0.26235294117647057,
      "grad_norm": 0.05034801736474037,
      "learning_rate": 0.00014774668630338736,
      "loss": 0.297,
      "step": 892
    },
    {
      "epoch": 0.2626470588235294,
      "grad_norm": 0.060217149555683136,
      "learning_rate": 0.0001476877761413844,
      "loss": 0.3661,
      "step": 893
    },
    {
      "epoch": 0.26294117647058823,
      "grad_norm": 0.06295526027679443,
      "learning_rate": 0.00014762886597938146,
      "loss": 0.3382,
      "step": 894
    },
    {
      "epoch": 0.26323529411764707,
      "grad_norm": 0.050394248217344284,
      "learning_rate": 0.00014756995581737852,
      "loss": 0.3734,
      "step": 895
    },
    {
      "epoch": 0.2635294117647059,
      "grad_norm": 0.05018913373351097,
      "learning_rate": 0.00014751104565537558,
      "loss": 0.3047,
      "step": 896
    },
    {
      "epoch": 0.2638235294117647,
      "grad_norm": 0.05402011796832085,
      "learning_rate": 0.00014745213549337264,
      "loss": 0.3515,
      "step": 897
    },
    {
      "epoch": 0.2641176470588235,
      "grad_norm": 0.047407541424036026,
      "learning_rate": 0.00014739322533136967,
      "loss": 0.3666,
      "step": 898
    },
    {
      "epoch": 0.26441176470588235,
      "grad_norm": 0.05710144340991974,
      "learning_rate": 0.00014733431516936673,
      "loss": 0.3625,
      "step": 899
    },
    {
      "epoch": 0.2647058823529412,
      "grad_norm": 0.06017865613102913,
      "learning_rate": 0.0001472754050073638,
      "loss": 0.3585,
      "step": 900
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05810526758432388,
      "learning_rate": 0.00014721649484536085,
      "loss": 0.3747,
      "step": 901
    },
    {
      "epoch": 0.26529411764705885,
      "grad_norm": 0.047773025929927826,
      "learning_rate": 0.0001471575846833579,
      "loss": 0.3426,
      "step": 902
    },
    {
      "epoch": 0.2655882352941176,
      "grad_norm": 0.08043684810400009,
      "learning_rate": 0.00014709867452135494,
      "loss": 0.3977,
      "step": 903
    },
    {
      "epoch": 0.26588235294117646,
      "grad_norm": 0.04550379887223244,
      "learning_rate": 0.000147039764359352,
      "loss": 0.3415,
      "step": 904
    },
    {
      "epoch": 0.2661764705882353,
      "grad_norm": 0.052217159420251846,
      "learning_rate": 0.00014698085419734906,
      "loss": 0.3295,
      "step": 905
    },
    {
      "epoch": 0.2664705882352941,
      "grad_norm": 0.05069698765873909,
      "learning_rate": 0.00014692194403534612,
      "loss": 0.4378,
      "step": 906
    },
    {
      "epoch": 0.26676470588235296,
      "grad_norm": 0.052270691841840744,
      "learning_rate": 0.00014686303387334316,
      "loss": 0.3739,
      "step": 907
    },
    {
      "epoch": 0.26705882352941174,
      "grad_norm": 0.05404457077383995,
      "learning_rate": 0.0001468041237113402,
      "loss": 0.375,
      "step": 908
    },
    {
      "epoch": 0.26735294117647057,
      "grad_norm": 0.06109911575913429,
      "learning_rate": 0.00014674521354933725,
      "loss": 0.4206,
      "step": 909
    },
    {
      "epoch": 0.2676470588235294,
      "grad_norm": 0.05253685638308525,
      "learning_rate": 0.0001466863033873343,
      "loss": 0.3255,
      "step": 910
    },
    {
      "epoch": 0.26794117647058824,
      "grad_norm": 0.047936927527189255,
      "learning_rate": 0.00014662739322533137,
      "loss": 0.3056,
      "step": 911
    },
    {
      "epoch": 0.26823529411764707,
      "grad_norm": 0.04639742523431778,
      "learning_rate": 0.00014656848306332843,
      "loss": 0.305,
      "step": 912
    },
    {
      "epoch": 0.2685294117647059,
      "grad_norm": 0.057180773466825485,
      "learning_rate": 0.00014650957290132546,
      "loss": 0.328,
      "step": 913
    },
    {
      "epoch": 0.2688235294117647,
      "grad_norm": 0.08240971714258194,
      "learning_rate": 0.00014645066273932252,
      "loss": 0.3702,
      "step": 914
    },
    {
      "epoch": 0.2691176470588235,
      "grad_norm": 0.05134551599621773,
      "learning_rate": 0.00014639175257731958,
      "loss": 0.3378,
      "step": 915
    },
    {
      "epoch": 0.26941176470588235,
      "grad_norm": 0.054945845156908035,
      "learning_rate": 0.00014633284241531664,
      "loss": 0.3412,
      "step": 916
    },
    {
      "epoch": 0.2697058823529412,
      "grad_norm": 0.0538768470287323,
      "learning_rate": 0.0001462739322533137,
      "loss": 0.3156,
      "step": 917
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.047760844230651855,
      "learning_rate": 0.00014621502209131074,
      "loss": 0.3007,
      "step": 918
    },
    {
      "epoch": 0.2702941176470588,
      "grad_norm": 0.0563683845102787,
      "learning_rate": 0.0001461561119293078,
      "loss": 0.3462,
      "step": 919
    },
    {
      "epoch": 0.27058823529411763,
      "grad_norm": 0.05150555446743965,
      "learning_rate": 0.00014609720176730486,
      "loss": 0.3679,
      "step": 920
    },
    {
      "epoch": 0.27088235294117646,
      "grad_norm": 0.043114855885505676,
      "learning_rate": 0.00014603829160530192,
      "loss": 0.3599,
      "step": 921
    },
    {
      "epoch": 0.2711764705882353,
      "grad_norm": 0.0499865859746933,
      "learning_rate": 0.00014597938144329898,
      "loss": 0.3404,
      "step": 922
    },
    {
      "epoch": 0.27147058823529413,
      "grad_norm": 0.05895785242319107,
      "learning_rate": 0.000145920471281296,
      "loss": 0.3441,
      "step": 923
    },
    {
      "epoch": 0.27176470588235296,
      "grad_norm": 0.05856414884328842,
      "learning_rate": 0.00014586156111929307,
      "loss": 0.4088,
      "step": 924
    },
    {
      "epoch": 0.27205882352941174,
      "grad_norm": 0.04538234323263168,
      "learning_rate": 0.00014580265095729013,
      "loss": 0.3354,
      "step": 925
    },
    {
      "epoch": 0.2723529411764706,
      "grad_norm": 0.06002802029252052,
      "learning_rate": 0.0001457437407952872,
      "loss": 0.4296,
      "step": 926
    },
    {
      "epoch": 0.2726470588235294,
      "grad_norm": 0.06993532180786133,
      "learning_rate": 0.00014568483063328425,
      "loss": 0.3951,
      "step": 927
    },
    {
      "epoch": 0.27294117647058824,
      "grad_norm": 0.054085832089185715,
      "learning_rate": 0.00014562592047128128,
      "loss": 0.3192,
      "step": 928
    },
    {
      "epoch": 0.2732352941176471,
      "grad_norm": 0.05395392328500748,
      "learning_rate": 0.00014556701030927834,
      "loss": 0.3681,
      "step": 929
    },
    {
      "epoch": 0.2735294117647059,
      "grad_norm": 0.05485706031322479,
      "learning_rate": 0.0001455081001472754,
      "loss": 0.3372,
      "step": 930
    },
    {
      "epoch": 0.2738235294117647,
      "grad_norm": 0.04228930547833443,
      "learning_rate": 0.00014544918998527246,
      "loss": 0.3249,
      "step": 931
    },
    {
      "epoch": 0.2741176470588235,
      "grad_norm": 0.05523638054728508,
      "learning_rate": 0.00014539027982326952,
      "loss": 0.3669,
      "step": 932
    },
    {
      "epoch": 0.27441176470588236,
      "grad_norm": 0.059681788086891174,
      "learning_rate": 0.00014533136966126656,
      "loss": 0.3452,
      "step": 933
    },
    {
      "epoch": 0.2747058823529412,
      "grad_norm": 0.05565102398395538,
      "learning_rate": 0.00014527245949926362,
      "loss": 0.3277,
      "step": 934
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.05444546416401863,
      "learning_rate": 0.00014521354933726068,
      "loss": 0.3573,
      "step": 935
    },
    {
      "epoch": 0.2752941176470588,
      "grad_norm": 0.0540597066283226,
      "learning_rate": 0.00014515463917525774,
      "loss": 0.4115,
      "step": 936
    },
    {
      "epoch": 0.27558823529411763,
      "grad_norm": 0.04078114777803421,
      "learning_rate": 0.0001450957290132548,
      "loss": 0.3223,
      "step": 937
    },
    {
      "epoch": 0.27588235294117647,
      "grad_norm": 0.055918723344802856,
      "learning_rate": 0.00014503681885125183,
      "loss": 0.3346,
      "step": 938
    },
    {
      "epoch": 0.2761764705882353,
      "grad_norm": 0.04926730692386627,
      "learning_rate": 0.0001449779086892489,
      "loss": 0.398,
      "step": 939
    },
    {
      "epoch": 0.27647058823529413,
      "grad_norm": 0.055656615644693375,
      "learning_rate": 0.00014491899852724595,
      "loss": 0.3776,
      "step": 940
    },
    {
      "epoch": 0.27676470588235297,
      "grad_norm": 0.04632708057761192,
      "learning_rate": 0.000144860088365243,
      "loss": 0.3798,
      "step": 941
    },
    {
      "epoch": 0.27705882352941175,
      "grad_norm": 0.05229625850915909,
      "learning_rate": 0.00014480117820324007,
      "loss": 0.3391,
      "step": 942
    },
    {
      "epoch": 0.2773529411764706,
      "grad_norm": 0.0403810478746891,
      "learning_rate": 0.0001447422680412371,
      "loss": 0.2661,
      "step": 943
    },
    {
      "epoch": 0.2776470588235294,
      "grad_norm": 0.062443945556879044,
      "learning_rate": 0.00014468335787923417,
      "loss": 0.4191,
      "step": 944
    },
    {
      "epoch": 0.27794117647058825,
      "grad_norm": 0.037968434393405914,
      "learning_rate": 0.00014462444771723123,
      "loss": 0.2812,
      "step": 945
    },
    {
      "epoch": 0.2782352941176471,
      "grad_norm": 0.05579185485839844,
      "learning_rate": 0.00014456553755522829,
      "loss": 0.3569,
      "step": 946
    },
    {
      "epoch": 0.27852941176470586,
      "grad_norm": 0.06327848881483078,
      "learning_rate": 0.00014450662739322535,
      "loss": 0.371,
      "step": 947
    },
    {
      "epoch": 0.2788235294117647,
      "grad_norm": 0.07567940652370453,
      "learning_rate": 0.00014444771723122238,
      "loss": 0.4381,
      "step": 948
    },
    {
      "epoch": 0.2791176470588235,
      "grad_norm": 0.050390057265758514,
      "learning_rate": 0.00014438880706921944,
      "loss": 0.3432,
      "step": 949
    },
    {
      "epoch": 0.27941176470588236,
      "grad_norm": 0.0642416924238205,
      "learning_rate": 0.0001443298969072165,
      "loss": 0.3706,
      "step": 950
    },
    {
      "epoch": 0.2797058823529412,
      "grad_norm": 0.06598974764347076,
      "learning_rate": 0.00014427098674521356,
      "loss": 0.4069,
      "step": 951
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06499747186899185,
      "learning_rate": 0.00014421207658321062,
      "loss": 0.4388,
      "step": 952
    },
    {
      "epoch": 0.2802941176470588,
      "grad_norm": 0.047016117721796036,
      "learning_rate": 0.00014415316642120765,
      "loss": 0.3563,
      "step": 953
    },
    {
      "epoch": 0.28058823529411764,
      "grad_norm": 0.06675603985786438,
      "learning_rate": 0.0001440942562592047,
      "loss": 0.4261,
      "step": 954
    },
    {
      "epoch": 0.28088235294117647,
      "grad_norm": 0.04983808845281601,
      "learning_rate": 0.00014403534609720177,
      "loss": 0.3007,
      "step": 955
    },
    {
      "epoch": 0.2811764705882353,
      "grad_norm": 0.043522849678993225,
      "learning_rate": 0.00014397643593519883,
      "loss": 0.4003,
      "step": 956
    },
    {
      "epoch": 0.28147058823529414,
      "grad_norm": 0.04677477851510048,
      "learning_rate": 0.0001439175257731959,
      "loss": 0.3196,
      "step": 957
    },
    {
      "epoch": 0.2817647058823529,
      "grad_norm": 0.04128250852227211,
      "learning_rate": 0.00014385861561119293,
      "loss": 0.2781,
      "step": 958
    },
    {
      "epoch": 0.28205882352941175,
      "grad_norm": 0.06443094462156296,
      "learning_rate": 0.00014379970544919,
      "loss": 0.3697,
      "step": 959
    },
    {
      "epoch": 0.2823529411764706,
      "grad_norm": 0.049895379692316055,
      "learning_rate": 0.00014374079528718705,
      "loss": 0.4171,
      "step": 960
    },
    {
      "epoch": 0.2826470588235294,
      "grad_norm": 0.04852476343512535,
      "learning_rate": 0.0001436818851251841,
      "loss": 0.3555,
      "step": 961
    },
    {
      "epoch": 0.28294117647058825,
      "grad_norm": 0.055776163935661316,
      "learning_rate": 0.00014362297496318117,
      "loss": 0.401,
      "step": 962
    },
    {
      "epoch": 0.2832352941176471,
      "grad_norm": 0.043341152369976044,
      "learning_rate": 0.0001435640648011782,
      "loss": 0.3427,
      "step": 963
    },
    {
      "epoch": 0.28352941176470586,
      "grad_norm": 0.0474889799952507,
      "learning_rate": 0.00014350515463917526,
      "loss": 0.3752,
      "step": 964
    },
    {
      "epoch": 0.2838235294117647,
      "grad_norm": 0.05166321247816086,
      "learning_rate": 0.00014344624447717232,
      "loss": 0.3732,
      "step": 965
    },
    {
      "epoch": 0.28411764705882353,
      "grad_norm": 0.04808064550161362,
      "learning_rate": 0.00014338733431516938,
      "loss": 0.3496,
      "step": 966
    },
    {
      "epoch": 0.28441176470588236,
      "grad_norm": 0.07296455651521683,
      "learning_rate": 0.00014332842415316644,
      "loss": 0.4219,
      "step": 967
    },
    {
      "epoch": 0.2847058823529412,
      "grad_norm": 0.04337478056550026,
      "learning_rate": 0.00014326951399116347,
      "loss": 0.2493,
      "step": 968
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.05178951099514961,
      "learning_rate": 0.00014321060382916053,
      "loss": 0.3127,
      "step": 969
    },
    {
      "epoch": 0.2852941176470588,
      "grad_norm": 0.057643283158540726,
      "learning_rate": 0.0001431516936671576,
      "loss": 0.3866,
      "step": 970
    },
    {
      "epoch": 0.28558823529411764,
      "grad_norm": 0.07185952365398407,
      "learning_rate": 0.00014309278350515465,
      "loss": 0.4367,
      "step": 971
    },
    {
      "epoch": 0.2858823529411765,
      "grad_norm": 0.06244342401623726,
      "learning_rate": 0.00014303387334315171,
      "loss": 0.4253,
      "step": 972
    },
    {
      "epoch": 0.2861764705882353,
      "grad_norm": 0.06102823466062546,
      "learning_rate": 0.00014297496318114875,
      "loss": 0.3888,
      "step": 973
    },
    {
      "epoch": 0.28647058823529414,
      "grad_norm": 0.04761918634176254,
      "learning_rate": 0.0001429160530191458,
      "loss": 0.3148,
      "step": 974
    },
    {
      "epoch": 0.2867647058823529,
      "grad_norm": 0.061437785625457764,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3687,
      "step": 975
    },
    {
      "epoch": 0.28705882352941176,
      "grad_norm": 0.05294808745384216,
      "learning_rate": 0.00014279823269513993,
      "loss": 0.3527,
      "step": 976
    },
    {
      "epoch": 0.2873529411764706,
      "grad_norm": 0.0563492514193058,
      "learning_rate": 0.000142739322533137,
      "loss": 0.4083,
      "step": 977
    },
    {
      "epoch": 0.2876470588235294,
      "grad_norm": 0.04850958287715912,
      "learning_rate": 0.00014268041237113402,
      "loss": 0.3412,
      "step": 978
    },
    {
      "epoch": 0.28794117647058826,
      "grad_norm": 0.06221327558159828,
      "learning_rate": 0.00014262150220913108,
      "loss": 0.3826,
      "step": 979
    },
    {
      "epoch": 0.28823529411764703,
      "grad_norm": 0.05060657486319542,
      "learning_rate": 0.00014256259204712814,
      "loss": 0.3531,
      "step": 980
    },
    {
      "epoch": 0.28852941176470587,
      "grad_norm": 0.06314856559038162,
      "learning_rate": 0.0001425036818851252,
      "loss": 0.3446,
      "step": 981
    },
    {
      "epoch": 0.2888235294117647,
      "grad_norm": 0.05264134705066681,
      "learning_rate": 0.00014244477172312226,
      "loss": 0.4282,
      "step": 982
    },
    {
      "epoch": 0.28911764705882353,
      "grad_norm": 0.05170394852757454,
      "learning_rate": 0.0001423858615611193,
      "loss": 0.3716,
      "step": 983
    },
    {
      "epoch": 0.28941176470588237,
      "grad_norm": 0.0440501905977726,
      "learning_rate": 0.00014232695139911636,
      "loss": 0.3877,
      "step": 984
    },
    {
      "epoch": 0.2897058823529412,
      "grad_norm": 0.06964879482984543,
      "learning_rate": 0.00014226804123711342,
      "loss": 0.4354,
      "step": 985
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.0427592471241951,
      "learning_rate": 0.00014220913107511048,
      "loss": 0.318,
      "step": 986
    },
    {
      "epoch": 0.2902941176470588,
      "grad_norm": 0.04687510058283806,
      "learning_rate": 0.00014215022091310754,
      "loss": 0.3432,
      "step": 987
    },
    {
      "epoch": 0.29058823529411765,
      "grad_norm": 0.06756463646888733,
      "learning_rate": 0.00014209131075110457,
      "loss": 0.3905,
      "step": 988
    },
    {
      "epoch": 0.2908823529411765,
      "grad_norm": 0.04211856424808502,
      "learning_rate": 0.00014203240058910163,
      "loss": 0.3422,
      "step": 989
    },
    {
      "epoch": 0.2911764705882353,
      "grad_norm": 0.04699687659740448,
      "learning_rate": 0.0001419734904270987,
      "loss": 0.3223,
      "step": 990
    },
    {
      "epoch": 0.2914705882352941,
      "grad_norm": 0.0592491552233696,
      "learning_rate": 0.00014191458026509575,
      "loss": 0.4158,
      "step": 991
    },
    {
      "epoch": 0.2917647058823529,
      "grad_norm": 0.04505949094891548,
      "learning_rate": 0.0001418556701030928,
      "loss": 0.3708,
      "step": 992
    },
    {
      "epoch": 0.29205882352941176,
      "grad_norm": 0.04811772331595421,
      "learning_rate": 0.00014179675994108984,
      "loss": 0.2678,
      "step": 993
    },
    {
      "epoch": 0.2923529411764706,
      "grad_norm": 0.048653293401002884,
      "learning_rate": 0.0001417378497790869,
      "loss": 0.3853,
      "step": 994
    },
    {
      "epoch": 0.2926470588235294,
      "grad_norm": 0.05546145513653755,
      "learning_rate": 0.00014167893961708396,
      "loss": 0.3563,
      "step": 995
    },
    {
      "epoch": 0.29294117647058826,
      "grad_norm": 0.04412589594721794,
      "learning_rate": 0.00014162002945508102,
      "loss": 0.3256,
      "step": 996
    },
    {
      "epoch": 0.29323529411764704,
      "grad_norm": 0.04751283675432205,
      "learning_rate": 0.00014156111929307808,
      "loss": 0.3829,
      "step": 997
    },
    {
      "epoch": 0.29352941176470587,
      "grad_norm": 0.04826300963759422,
      "learning_rate": 0.00014150220913107512,
      "loss": 0.331,
      "step": 998
    },
    {
      "epoch": 0.2938235294117647,
      "grad_norm": 0.05251488462090492,
      "learning_rate": 0.00014144329896907218,
      "loss": 0.3192,
      "step": 999
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.042043812572956085,
      "learning_rate": 0.00014138438880706924,
      "loss": 0.3131,
      "step": 1000
    },
    {
      "epoch": 0.2944117647058824,
      "grad_norm": 0.051341280341148376,
      "learning_rate": 0.0001413254786450663,
      "loss": 0.3874,
      "step": 1001
    },
    {
      "epoch": 0.29470588235294115,
      "grad_norm": 0.05732183903455734,
      "learning_rate": 0.00014126656848306336,
      "loss": 0.4081,
      "step": 1002
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.052939433604478836,
      "learning_rate": 0.0001412076583210604,
      "loss": 0.293,
      "step": 1003
    },
    {
      "epoch": 0.2952941176470588,
      "grad_norm": 0.048597078770399094,
      "learning_rate": 0.00014114874815905745,
      "loss": 0.3731,
      "step": 1004
    },
    {
      "epoch": 0.29558823529411765,
      "grad_norm": 0.05382390320301056,
      "learning_rate": 0.0001410898379970545,
      "loss": 0.339,
      "step": 1005
    },
    {
      "epoch": 0.2958823529411765,
      "grad_norm": 0.0557575523853302,
      "learning_rate": 0.00014103092783505157,
      "loss": 0.3071,
      "step": 1006
    },
    {
      "epoch": 0.2961764705882353,
      "grad_norm": 0.04721474274992943,
      "learning_rate": 0.00014097201767304863,
      "loss": 0.3034,
      "step": 1007
    },
    {
      "epoch": 0.2964705882352941,
      "grad_norm": 0.0453617237508297,
      "learning_rate": 0.00014091310751104566,
      "loss": 0.2703,
      "step": 1008
    },
    {
      "epoch": 0.29676470588235293,
      "grad_norm": 0.06340935826301575,
      "learning_rate": 0.00014085419734904272,
      "loss": 0.3545,
      "step": 1009
    },
    {
      "epoch": 0.29705882352941176,
      "grad_norm": 0.0745542123913765,
      "learning_rate": 0.00014079528718703978,
      "loss": 0.4097,
      "step": 1010
    },
    {
      "epoch": 0.2973529411764706,
      "grad_norm": 0.07125820219516754,
      "learning_rate": 0.00014073637702503684,
      "loss": 0.3752,
      "step": 1011
    },
    {
      "epoch": 0.29764705882352943,
      "grad_norm": 0.04367093741893768,
      "learning_rate": 0.0001406774668630339,
      "loss": 0.3059,
      "step": 1012
    },
    {
      "epoch": 0.2979411764705882,
      "grad_norm": 0.05852298066020012,
      "learning_rate": 0.0001406185567010309,
      "loss": 0.3381,
      "step": 1013
    },
    {
      "epoch": 0.29823529411764704,
      "grad_norm": 0.04852944239974022,
      "learning_rate": 0.00014055964653902797,
      "loss": 0.362,
      "step": 1014
    },
    {
      "epoch": 0.2985294117647059,
      "grad_norm": 0.05690501257777214,
      "learning_rate": 0.00014050073637702503,
      "loss": 0.3834,
      "step": 1015
    },
    {
      "epoch": 0.2988235294117647,
      "grad_norm": 0.04620125889778137,
      "learning_rate": 0.0001404418262150221,
      "loss": 0.3322,
      "step": 1016
    },
    {
      "epoch": 0.29911764705882354,
      "grad_norm": 0.0536544993519783,
      "learning_rate": 0.00014038291605301915,
      "loss": 0.3707,
      "step": 1017
    },
    {
      "epoch": 0.2994117647058824,
      "grad_norm": 0.04953986778855324,
      "learning_rate": 0.00014032400589101618,
      "loss": 0.3647,
      "step": 1018
    },
    {
      "epoch": 0.29970588235294116,
      "grad_norm": 0.06503067910671234,
      "learning_rate": 0.00014026509572901324,
      "loss": 0.3413,
      "step": 1019
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.052080508321523666,
      "learning_rate": 0.0001402061855670103,
      "loss": 0.3581,
      "step": 1020
    },
    {
      "epoch": 0.3002941176470588,
      "grad_norm": 0.05582733079791069,
      "learning_rate": 0.00014014727540500736,
      "loss": 0.3339,
      "step": 1021
    },
    {
      "epoch": 0.30058823529411766,
      "grad_norm": 0.05468344688415527,
      "learning_rate": 0.00014008836524300442,
      "loss": 0.3562,
      "step": 1022
    },
    {
      "epoch": 0.3008823529411765,
      "grad_norm": 0.050474394112825394,
      "learning_rate": 0.00014002945508100146,
      "loss": 0.3662,
      "step": 1023
    },
    {
      "epoch": 0.30117647058823527,
      "grad_norm": 0.0433373898267746,
      "learning_rate": 0.00013997054491899852,
      "loss": 0.2897,
      "step": 1024
    },
    {
      "epoch": 0.3014705882352941,
      "grad_norm": 0.04819948226213455,
      "learning_rate": 0.00013991163475699558,
      "loss": 0.4062,
      "step": 1025
    },
    {
      "epoch": 0.30176470588235293,
      "grad_norm": 0.05419018119573593,
      "learning_rate": 0.00013985272459499264,
      "loss": 0.3724,
      "step": 1026
    },
    {
      "epoch": 0.30205882352941177,
      "grad_norm": 0.05859893187880516,
      "learning_rate": 0.0001397938144329897,
      "loss": 0.3657,
      "step": 1027
    },
    {
      "epoch": 0.3023529411764706,
      "grad_norm": 0.06144881621003151,
      "learning_rate": 0.00013973490427098673,
      "loss": 0.3932,
      "step": 1028
    },
    {
      "epoch": 0.30264705882352944,
      "grad_norm": 0.05366535484790802,
      "learning_rate": 0.0001396759941089838,
      "loss": 0.4074,
      "step": 1029
    },
    {
      "epoch": 0.3029411764705882,
      "grad_norm": 0.06277600675821304,
      "learning_rate": 0.00013961708394698085,
      "loss": 0.3787,
      "step": 1030
    },
    {
      "epoch": 0.30323529411764705,
      "grad_norm": 0.03793596476316452,
      "learning_rate": 0.0001395581737849779,
      "loss": 0.2742,
      "step": 1031
    },
    {
      "epoch": 0.3035294117647059,
      "grad_norm": 0.04548404738306999,
      "learning_rate": 0.00013949926362297497,
      "loss": 0.2931,
      "step": 1032
    },
    {
      "epoch": 0.3038235294117647,
      "grad_norm": 0.06761139631271362,
      "learning_rate": 0.000139440353460972,
      "loss": 0.3707,
      "step": 1033
    },
    {
      "epoch": 0.30411764705882355,
      "grad_norm": 0.05042809620499611,
      "learning_rate": 0.00013938144329896907,
      "loss": 0.328,
      "step": 1034
    },
    {
      "epoch": 0.3044117647058823,
      "grad_norm": 0.049906518310308456,
      "learning_rate": 0.00013932253313696613,
      "loss": 0.3673,
      "step": 1035
    },
    {
      "epoch": 0.30470588235294116,
      "grad_norm": 0.06521006673574448,
      "learning_rate": 0.00013926362297496319,
      "loss": 0.3747,
      "step": 1036
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.04304023087024689,
      "learning_rate": 0.00013920471281296025,
      "loss": 0.2898,
      "step": 1037
    },
    {
      "epoch": 0.3052941176470588,
      "grad_norm": 0.031785622239112854,
      "learning_rate": 0.00013914580265095728,
      "loss": 0.2269,
      "step": 1038
    },
    {
      "epoch": 0.30558823529411766,
      "grad_norm": 0.051136963069438934,
      "learning_rate": 0.00013908689248895434,
      "loss": 0.3903,
      "step": 1039
    },
    {
      "epoch": 0.3058823529411765,
      "grad_norm": 0.04732788726687431,
      "learning_rate": 0.0001390279823269514,
      "loss": 0.3354,
      "step": 1040
    },
    {
      "epoch": 0.30617647058823527,
      "grad_norm": 0.0638386458158493,
      "learning_rate": 0.00013896907216494846,
      "loss": 0.4203,
      "step": 1041
    },
    {
      "epoch": 0.3064705882352941,
      "grad_norm": 0.049837905913591385,
      "learning_rate": 0.00013891016200294552,
      "loss": 0.3673,
      "step": 1042
    },
    {
      "epoch": 0.30676470588235294,
      "grad_norm": 0.044602878391742706,
      "learning_rate": 0.00013885125184094255,
      "loss": 0.2887,
      "step": 1043
    },
    {
      "epoch": 0.3070588235294118,
      "grad_norm": 0.048335302621126175,
      "learning_rate": 0.0001387923416789396,
      "loss": 0.3258,
      "step": 1044
    },
    {
      "epoch": 0.3073529411764706,
      "grad_norm": 0.06201092526316643,
      "learning_rate": 0.00013873343151693667,
      "loss": 0.4375,
      "step": 1045
    },
    {
      "epoch": 0.3076470588235294,
      "grad_norm": 0.05788855254650116,
      "learning_rate": 0.00013867452135493373,
      "loss": 0.3867,
      "step": 1046
    },
    {
      "epoch": 0.3079411764705882,
      "grad_norm": 0.0427359901368618,
      "learning_rate": 0.0001386156111929308,
      "loss": 0.3694,
      "step": 1047
    },
    {
      "epoch": 0.30823529411764705,
      "grad_norm": 0.07275224477052689,
      "learning_rate": 0.00013855670103092783,
      "loss": 0.4088,
      "step": 1048
    },
    {
      "epoch": 0.3085294117647059,
      "grad_norm": 0.06267794221639633,
      "learning_rate": 0.00013849779086892489,
      "loss": 0.3288,
      "step": 1049
    },
    {
      "epoch": 0.3088235294117647,
      "grad_norm": 0.05001003295183182,
      "learning_rate": 0.00013843888070692195,
      "loss": 0.3324,
      "step": 1050
    },
    {
      "epoch": 0.30911764705882355,
      "grad_norm": 0.045970428735017776,
      "learning_rate": 0.000138379970544919,
      "loss": 0.2827,
      "step": 1051
    },
    {
      "epoch": 0.30941176470588233,
      "grad_norm": 0.05254971608519554,
      "learning_rate": 0.00013832106038291607,
      "loss": 0.3431,
      "step": 1052
    },
    {
      "epoch": 0.30970588235294116,
      "grad_norm": 0.0602114200592041,
      "learning_rate": 0.0001382621502209131,
      "loss": 0.352,
      "step": 1053
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0577702671289444,
      "learning_rate": 0.00013820324005891016,
      "loss": 0.4206,
      "step": 1054
    },
    {
      "epoch": 0.31029411764705883,
      "grad_norm": 0.0564039908349514,
      "learning_rate": 0.00013814432989690722,
      "loss": 0.3579,
      "step": 1055
    },
    {
      "epoch": 0.31058823529411766,
      "grad_norm": 0.05527668073773384,
      "learning_rate": 0.00013808541973490428,
      "loss": 0.4295,
      "step": 1056
    },
    {
      "epoch": 0.31088235294117644,
      "grad_norm": 0.05100282281637192,
      "learning_rate": 0.00013802650957290134,
      "loss": 0.384,
      "step": 1057
    },
    {
      "epoch": 0.3111764705882353,
      "grad_norm": 0.05363363027572632,
      "learning_rate": 0.00013796759941089837,
      "loss": 0.38,
      "step": 1058
    },
    {
      "epoch": 0.3114705882352941,
      "grad_norm": 0.0552857480943203,
      "learning_rate": 0.00013790868924889543,
      "loss": 0.3679,
      "step": 1059
    },
    {
      "epoch": 0.31176470588235294,
      "grad_norm": 0.05524974688887596,
      "learning_rate": 0.0001378497790868925,
      "loss": 0.3647,
      "step": 1060
    },
    {
      "epoch": 0.3120588235294118,
      "grad_norm": 0.048499077558517456,
      "learning_rate": 0.00013779086892488955,
      "loss": 0.3439,
      "step": 1061
    },
    {
      "epoch": 0.3123529411764706,
      "grad_norm": 0.046894680708646774,
      "learning_rate": 0.00013773195876288661,
      "loss": 0.3264,
      "step": 1062
    },
    {
      "epoch": 0.3126470588235294,
      "grad_norm": 0.04811553657054901,
      "learning_rate": 0.00013767304860088365,
      "loss": 0.3262,
      "step": 1063
    },
    {
      "epoch": 0.3129411764705882,
      "grad_norm": 0.04047423601150513,
      "learning_rate": 0.0001376141384388807,
      "loss": 0.3056,
      "step": 1064
    },
    {
      "epoch": 0.31323529411764706,
      "grad_norm": 0.051817793399095535,
      "learning_rate": 0.00013755522827687777,
      "loss": 0.3703,
      "step": 1065
    },
    {
      "epoch": 0.3135294117647059,
      "grad_norm": 0.059142518788576126,
      "learning_rate": 0.00013749631811487483,
      "loss": 0.3725,
      "step": 1066
    },
    {
      "epoch": 0.3138235294117647,
      "grad_norm": 0.04984457418322563,
      "learning_rate": 0.0001374374079528719,
      "loss": 0.3744,
      "step": 1067
    },
    {
      "epoch": 0.31411764705882356,
      "grad_norm": 0.04663741588592529,
      "learning_rate": 0.00013737849779086892,
      "loss": 0.3308,
      "step": 1068
    },
    {
      "epoch": 0.31441176470588234,
      "grad_norm": 0.044646967202425,
      "learning_rate": 0.00013731958762886598,
      "loss": 0.3521,
      "step": 1069
    },
    {
      "epoch": 0.31470588235294117,
      "grad_norm": 0.041974812746047974,
      "learning_rate": 0.00013726067746686304,
      "loss": 0.2823,
      "step": 1070
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.04352371767163277,
      "learning_rate": 0.0001372017673048601,
      "loss": 0.3461,
      "step": 1071
    },
    {
      "epoch": 0.31529411764705884,
      "grad_norm": 0.055350739508867264,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.3612,
      "step": 1072
    },
    {
      "epoch": 0.31558823529411767,
      "grad_norm": 0.06913478672504425,
      "learning_rate": 0.0001370839469808542,
      "loss": 0.3735,
      "step": 1073
    },
    {
      "epoch": 0.31588235294117645,
      "grad_norm": 0.041391290724277496,
      "learning_rate": 0.00013702503681885126,
      "loss": 0.3416,
      "step": 1074
    },
    {
      "epoch": 0.3161764705882353,
      "grad_norm": 0.05816817283630371,
      "learning_rate": 0.00013696612665684832,
      "loss": 0.3962,
      "step": 1075
    },
    {
      "epoch": 0.3164705882352941,
      "grad_norm": 0.048165369778871536,
      "learning_rate": 0.00013690721649484538,
      "loss": 0.3569,
      "step": 1076
    },
    {
      "epoch": 0.31676470588235295,
      "grad_norm": 0.05057896301150322,
      "learning_rate": 0.00013684830633284244,
      "loss": 0.3312,
      "step": 1077
    },
    {
      "epoch": 0.3170588235294118,
      "grad_norm": 0.05162261426448822,
      "learning_rate": 0.00013678939617083947,
      "loss": 0.3552,
      "step": 1078
    },
    {
      "epoch": 0.3173529411764706,
      "grad_norm": 0.05308175086975098,
      "learning_rate": 0.00013673048600883653,
      "loss": 0.2831,
      "step": 1079
    },
    {
      "epoch": 0.3176470588235294,
      "grad_norm": 0.05311012640595436,
      "learning_rate": 0.0001366715758468336,
      "loss": 0.4058,
      "step": 1080
    },
    {
      "epoch": 0.3179411764705882,
      "grad_norm": 0.0663798376917839,
      "learning_rate": 0.00013661266568483065,
      "loss": 0.391,
      "step": 1081
    },
    {
      "epoch": 0.31823529411764706,
      "grad_norm": 0.05489886552095413,
      "learning_rate": 0.0001365537555228277,
      "loss": 0.2937,
      "step": 1082
    },
    {
      "epoch": 0.3185294117647059,
      "grad_norm": 0.048546139150857925,
      "learning_rate": 0.00013649484536082474,
      "loss": 0.3674,
      "step": 1083
    },
    {
      "epoch": 0.31882352941176473,
      "grad_norm": 0.055234961211681366,
      "learning_rate": 0.0001364359351988218,
      "loss": 0.3731,
      "step": 1084
    },
    {
      "epoch": 0.3191176470588235,
      "grad_norm": 0.04562889039516449,
      "learning_rate": 0.00013637702503681886,
      "loss": 0.2734,
      "step": 1085
    },
    {
      "epoch": 0.31941176470588234,
      "grad_norm": 0.05898446589708328,
      "learning_rate": 0.00013631811487481592,
      "loss": 0.3617,
      "step": 1086
    },
    {
      "epoch": 0.3197058823529412,
      "grad_norm": 0.046660155057907104,
      "learning_rate": 0.00013625920471281298,
      "loss": 0.2847,
      "step": 1087
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.046587660908699036,
      "learning_rate": 0.00013620029455081002,
      "loss": 0.3655,
      "step": 1088
    },
    {
      "epoch": 0.32029411764705884,
      "grad_norm": 0.05368533357977867,
      "learning_rate": 0.00013614138438880708,
      "loss": 0.3793,
      "step": 1089
    },
    {
      "epoch": 0.3205882352941177,
      "grad_norm": 0.05707770213484764,
      "learning_rate": 0.00013608247422680414,
      "loss": 0.3495,
      "step": 1090
    },
    {
      "epoch": 0.32088235294117645,
      "grad_norm": 0.04955393821001053,
      "learning_rate": 0.0001360235640648012,
      "loss": 0.3648,
      "step": 1091
    },
    {
      "epoch": 0.3211764705882353,
      "grad_norm": 0.04677427187561989,
      "learning_rate": 0.00013596465390279826,
      "loss": 0.3031,
      "step": 1092
    },
    {
      "epoch": 0.3214705882352941,
      "grad_norm": 0.05691395699977875,
      "learning_rate": 0.0001359057437407953,
      "loss": 0.3468,
      "step": 1093
    },
    {
      "epoch": 0.32176470588235295,
      "grad_norm": 0.05948138236999512,
      "learning_rate": 0.00013584683357879235,
      "loss": 0.4187,
      "step": 1094
    },
    {
      "epoch": 0.3220588235294118,
      "grad_norm": 0.06208048015832901,
      "learning_rate": 0.0001357879234167894,
      "loss": 0.3769,
      "step": 1095
    },
    {
      "epoch": 0.32235294117647056,
      "grad_norm": 0.05681874230504036,
      "learning_rate": 0.00013572901325478647,
      "loss": 0.4021,
      "step": 1096
    },
    {
      "epoch": 0.3226470588235294,
      "grad_norm": 0.056921616196632385,
      "learning_rate": 0.00013567010309278353,
      "loss": 0.3689,
      "step": 1097
    },
    {
      "epoch": 0.32294117647058823,
      "grad_norm": 0.05650492012500763,
      "learning_rate": 0.00013561119293078056,
      "loss": 0.3981,
      "step": 1098
    },
    {
      "epoch": 0.32323529411764707,
      "grad_norm": 0.04958132654428482,
      "learning_rate": 0.00013555228276877762,
      "loss": 0.3189,
      "step": 1099
    },
    {
      "epoch": 0.3235294117647059,
      "grad_norm": 0.05411536991596222,
      "learning_rate": 0.00013549337260677468,
      "loss": 0.3653,
      "step": 1100
    },
    {
      "epoch": 0.32382352941176473,
      "grad_norm": 0.060905225574970245,
      "learning_rate": 0.00013543446244477174,
      "loss": 0.2597,
      "step": 1101
    },
    {
      "epoch": 0.3241176470588235,
      "grad_norm": 0.058901071548461914,
      "learning_rate": 0.0001353755522827688,
      "loss": 0.293,
      "step": 1102
    },
    {
      "epoch": 0.32441176470588234,
      "grad_norm": 0.05032173916697502,
      "learning_rate": 0.00013531664212076584,
      "loss": 0.3299,
      "step": 1103
    },
    {
      "epoch": 0.3247058823529412,
      "grad_norm": 0.05952264368534088,
      "learning_rate": 0.0001352577319587629,
      "loss": 0.3491,
      "step": 1104
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.041966840624809265,
      "learning_rate": 0.00013519882179675996,
      "loss": 0.2822,
      "step": 1105
    },
    {
      "epoch": 0.32529411764705884,
      "grad_norm": 0.054049789905548096,
      "learning_rate": 0.00013513991163475702,
      "loss": 0.3297,
      "step": 1106
    },
    {
      "epoch": 0.3255882352941176,
      "grad_norm": 0.05360212177038193,
      "learning_rate": 0.00013508100147275408,
      "loss": 0.3517,
      "step": 1107
    },
    {
      "epoch": 0.32588235294117646,
      "grad_norm": 0.04166169464588165,
      "learning_rate": 0.0001350220913107511,
      "loss": 0.3206,
      "step": 1108
    },
    {
      "epoch": 0.3261764705882353,
      "grad_norm": 0.05082499608397484,
      "learning_rate": 0.00013496318114874817,
      "loss": 0.3961,
      "step": 1109
    },
    {
      "epoch": 0.3264705882352941,
      "grad_norm": 0.0545523576438427,
      "learning_rate": 0.00013490427098674523,
      "loss": 0.3646,
      "step": 1110
    },
    {
      "epoch": 0.32676470588235296,
      "grad_norm": 0.06231502443552017,
      "learning_rate": 0.0001348453608247423,
      "loss": 0.3833,
      "step": 1111
    },
    {
      "epoch": 0.3270588235294118,
      "grad_norm": 0.05166138708591461,
      "learning_rate": 0.00013478645066273935,
      "loss": 0.365,
      "step": 1112
    },
    {
      "epoch": 0.32735294117647057,
      "grad_norm": 0.04634623974561691,
      "learning_rate": 0.00013472754050073638,
      "loss": 0.3292,
      "step": 1113
    },
    {
      "epoch": 0.3276470588235294,
      "grad_norm": 0.05647117644548416,
      "learning_rate": 0.00013466863033873344,
      "loss": 0.3138,
      "step": 1114
    },
    {
      "epoch": 0.32794117647058824,
      "grad_norm": 0.08821556717157364,
      "learning_rate": 0.0001346097201767305,
      "loss": 0.4855,
      "step": 1115
    },
    {
      "epoch": 0.32823529411764707,
      "grad_norm": 0.05965130776166916,
      "learning_rate": 0.00013455081001472757,
      "loss": 0.3736,
      "step": 1116
    },
    {
      "epoch": 0.3285294117647059,
      "grad_norm": 0.053009383380413055,
      "learning_rate": 0.00013449189985272463,
      "loss": 0.3389,
      "step": 1117
    },
    {
      "epoch": 0.3288235294117647,
      "grad_norm": 0.05358622223138809,
      "learning_rate": 0.00013443298969072166,
      "loss": 0.3801,
      "step": 1118
    },
    {
      "epoch": 0.3291176470588235,
      "grad_norm": 0.06522226333618164,
      "learning_rate": 0.0001343740795287187,
      "loss": 0.334,
      "step": 1119
    },
    {
      "epoch": 0.32941176470588235,
      "grad_norm": 0.0511314794421196,
      "learning_rate": 0.00013431516936671575,
      "loss": 0.3786,
      "step": 1120
    },
    {
      "epoch": 0.3297058823529412,
      "grad_norm": 0.050858501344919205,
      "learning_rate": 0.0001342562592047128,
      "loss": 0.311,
      "step": 1121
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.056546714156866074,
      "learning_rate": 0.00013419734904270987,
      "loss": 0.3584,
      "step": 1122
    },
    {
      "epoch": 0.33029411764705885,
      "grad_norm": 0.056931186467409134,
      "learning_rate": 0.0001341384388807069,
      "loss": 0.3409,
      "step": 1123
    },
    {
      "epoch": 0.3305882352941176,
      "grad_norm": 0.047702353447675705,
      "learning_rate": 0.00013407952871870397,
      "loss": 0.3111,
      "step": 1124
    },
    {
      "epoch": 0.33088235294117646,
      "grad_norm": 0.04180747643113136,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.32,
      "step": 1125
    },
    {
      "epoch": 0.3311764705882353,
      "grad_norm": 0.03771497681736946,
      "learning_rate": 0.00013396170839469809,
      "loss": 0.3429,
      "step": 1126
    },
    {
      "epoch": 0.33147058823529413,
      "grad_norm": 0.06501225382089615,
      "learning_rate": 0.00013390279823269515,
      "loss": 0.4667,
      "step": 1127
    },
    {
      "epoch": 0.33176470588235296,
      "grad_norm": 0.04210295528173447,
      "learning_rate": 0.00013384388807069218,
      "loss": 0.2947,
      "step": 1128
    },
    {
      "epoch": 0.33205882352941174,
      "grad_norm": 0.0507059320807457,
      "learning_rate": 0.00013378497790868924,
      "loss": 0.3017,
      "step": 1129
    },
    {
      "epoch": 0.3323529411764706,
      "grad_norm": 0.046720776706933975,
      "learning_rate": 0.0001337260677466863,
      "loss": 0.3464,
      "step": 1130
    },
    {
      "epoch": 0.3326470588235294,
      "grad_norm": 0.06956089287996292,
      "learning_rate": 0.00013366715758468336,
      "loss": 0.4392,
      "step": 1131
    },
    {
      "epoch": 0.33294117647058824,
      "grad_norm": 0.0410098172724247,
      "learning_rate": 0.00013360824742268042,
      "loss": 0.3748,
      "step": 1132
    },
    {
      "epoch": 0.3332352941176471,
      "grad_norm": 0.04849325865507126,
      "learning_rate": 0.00013354933726067745,
      "loss": 0.3575,
      "step": 1133
    },
    {
      "epoch": 0.3335294117647059,
      "grad_norm": 0.03790104389190674,
      "learning_rate": 0.0001334904270986745,
      "loss": 0.3219,
      "step": 1134
    },
    {
      "epoch": 0.3338235294117647,
      "grad_norm": 0.05562128871679306,
      "learning_rate": 0.00013343151693667157,
      "loss": 0.3955,
      "step": 1135
    },
    {
      "epoch": 0.3341176470588235,
      "grad_norm": 0.049508705735206604,
      "learning_rate": 0.00013337260677466863,
      "loss": 0.3246,
      "step": 1136
    },
    {
      "epoch": 0.33441176470588235,
      "grad_norm": 0.05327296629548073,
      "learning_rate": 0.0001333136966126657,
      "loss": 0.3621,
      "step": 1137
    },
    {
      "epoch": 0.3347058823529412,
      "grad_norm": 0.04058036208152771,
      "learning_rate": 0.00013325478645066273,
      "loss": 0.2758,
      "step": 1138
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.05807316675782204,
      "learning_rate": 0.00013319587628865979,
      "loss": 0.4075,
      "step": 1139
    },
    {
      "epoch": 0.3352941176470588,
      "grad_norm": 0.06362071633338928,
      "learning_rate": 0.00013313696612665685,
      "loss": 0.4215,
      "step": 1140
    },
    {
      "epoch": 0.33558823529411763,
      "grad_norm": 0.05364774912595749,
      "learning_rate": 0.0001330780559646539,
      "loss": 0.3759,
      "step": 1141
    },
    {
      "epoch": 0.33588235294117647,
      "grad_norm": 0.053076982498168945,
      "learning_rate": 0.00013301914580265097,
      "loss": 0.3309,
      "step": 1142
    },
    {
      "epoch": 0.3361764705882353,
      "grad_norm": 0.042229343205690384,
      "learning_rate": 0.000132960235640648,
      "loss": 0.337,
      "step": 1143
    },
    {
      "epoch": 0.33647058823529413,
      "grad_norm": 0.0443028099834919,
      "learning_rate": 0.00013290132547864506,
      "loss": 0.3441,
      "step": 1144
    },
    {
      "epoch": 0.33676470588235297,
      "grad_norm": 0.048171836882829666,
      "learning_rate": 0.00013284241531664212,
      "loss": 0.3796,
      "step": 1145
    },
    {
      "epoch": 0.33705882352941174,
      "grad_norm": 0.04862277954816818,
      "learning_rate": 0.00013278350515463918,
      "loss": 0.3878,
      "step": 1146
    },
    {
      "epoch": 0.3373529411764706,
      "grad_norm": 0.04794380068778992,
      "learning_rate": 0.00013272459499263624,
      "loss": 0.2884,
      "step": 1147
    },
    {
      "epoch": 0.3376470588235294,
      "grad_norm": 0.045239683240652084,
      "learning_rate": 0.00013266568483063327,
      "loss": 0.3435,
      "step": 1148
    },
    {
      "epoch": 0.33794117647058824,
      "grad_norm": 0.054516080766916275,
      "learning_rate": 0.00013260677466863033,
      "loss": 0.3278,
      "step": 1149
    },
    {
      "epoch": 0.3382352941176471,
      "grad_norm": 0.06573491543531418,
      "learning_rate": 0.0001325478645066274,
      "loss": 0.358,
      "step": 1150
    },
    {
      "epoch": 0.33852941176470586,
      "grad_norm": 0.05373070389032364,
      "learning_rate": 0.00013248895434462445,
      "loss": 0.3672,
      "step": 1151
    },
    {
      "epoch": 0.3388235294117647,
      "grad_norm": 0.06397560983896255,
      "learning_rate": 0.00013243004418262151,
      "loss": 0.3491,
      "step": 1152
    },
    {
      "epoch": 0.3391176470588235,
      "grad_norm": 0.051888830959796906,
      "learning_rate": 0.00013237113402061855,
      "loss": 0.3513,
      "step": 1153
    },
    {
      "epoch": 0.33941176470588236,
      "grad_norm": 0.07812928408384323,
      "learning_rate": 0.0001323122238586156,
      "loss": 0.3966,
      "step": 1154
    },
    {
      "epoch": 0.3397058823529412,
      "grad_norm": 0.05271434411406517,
      "learning_rate": 0.00013225331369661267,
      "loss": 0.3744,
      "step": 1155
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.03349430114030838,
      "learning_rate": 0.00013219440353460973,
      "loss": 0.2378,
      "step": 1156
    },
    {
      "epoch": 0.3402941176470588,
      "grad_norm": 0.04840904101729393,
      "learning_rate": 0.0001321354933726068,
      "loss": 0.3039,
      "step": 1157
    },
    {
      "epoch": 0.34058823529411764,
      "grad_norm": 0.051185380667448044,
      "learning_rate": 0.00013207658321060382,
      "loss": 0.3322,
      "step": 1158
    },
    {
      "epoch": 0.34088235294117647,
      "grad_norm": 0.06270311772823334,
      "learning_rate": 0.00013201767304860088,
      "loss": 0.4098,
      "step": 1159
    },
    {
      "epoch": 0.3411764705882353,
      "grad_norm": 0.062127821147441864,
      "learning_rate": 0.00013195876288659794,
      "loss": 0.3761,
      "step": 1160
    },
    {
      "epoch": 0.34147058823529414,
      "grad_norm": 0.0544779971241951,
      "learning_rate": 0.000131899852724595,
      "loss": 0.3686,
      "step": 1161
    },
    {
      "epoch": 0.3417647058823529,
      "grad_norm": 0.04909919202327728,
      "learning_rate": 0.00013184094256259206,
      "loss": 0.3304,
      "step": 1162
    },
    {
      "epoch": 0.34205882352941175,
      "grad_norm": 0.0571029856801033,
      "learning_rate": 0.0001317820324005891,
      "loss": 0.3775,
      "step": 1163
    },
    {
      "epoch": 0.3423529411764706,
      "grad_norm": 0.0465298593044281,
      "learning_rate": 0.00013172312223858615,
      "loss": 0.3383,
      "step": 1164
    },
    {
      "epoch": 0.3426470588235294,
      "grad_norm": 0.046719081699848175,
      "learning_rate": 0.00013166421207658322,
      "loss": 0.3436,
      "step": 1165
    },
    {
      "epoch": 0.34294117647058825,
      "grad_norm": 0.038665104657411575,
      "learning_rate": 0.00013160530191458028,
      "loss": 0.2566,
      "step": 1166
    },
    {
      "epoch": 0.3432352941176471,
      "grad_norm": 0.05060292035341263,
      "learning_rate": 0.00013154639175257734,
      "loss": 0.3664,
      "step": 1167
    },
    {
      "epoch": 0.34352941176470586,
      "grad_norm": 0.04308950528502464,
      "learning_rate": 0.00013148748159057437,
      "loss": 0.3255,
      "step": 1168
    },
    {
      "epoch": 0.3438235294117647,
      "grad_norm": 0.045976053923368454,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.3549,
      "step": 1169
    },
    {
      "epoch": 0.34411764705882353,
      "grad_norm": 0.0441671758890152,
      "learning_rate": 0.0001313696612665685,
      "loss": 0.3246,
      "step": 1170
    },
    {
      "epoch": 0.34441176470588236,
      "grad_norm": 0.047823529690504074,
      "learning_rate": 0.00013131075110456555,
      "loss": 0.3289,
      "step": 1171
    },
    {
      "epoch": 0.3447058823529412,
      "grad_norm": 0.04683883488178253,
      "learning_rate": 0.0001312518409425626,
      "loss": 0.3269,
      "step": 1172
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.050591904670000076,
      "learning_rate": 0.00013119293078055964,
      "loss": 0.3581,
      "step": 1173
    },
    {
      "epoch": 0.3452941176470588,
      "grad_norm": 0.050069548189640045,
      "learning_rate": 0.0001311340206185567,
      "loss": 0.3422,
      "step": 1174
    },
    {
      "epoch": 0.34558823529411764,
      "grad_norm": 0.0477614589035511,
      "learning_rate": 0.00013107511045655376,
      "loss": 0.3696,
      "step": 1175
    },
    {
      "epoch": 0.3458823529411765,
      "grad_norm": 0.06405249238014221,
      "learning_rate": 0.00013101620029455082,
      "loss": 0.3718,
      "step": 1176
    },
    {
      "epoch": 0.3461764705882353,
      "grad_norm": 0.03844161331653595,
      "learning_rate": 0.00013095729013254788,
      "loss": 0.2802,
      "step": 1177
    },
    {
      "epoch": 0.34647058823529414,
      "grad_norm": 0.0393957756459713,
      "learning_rate": 0.00013089837997054492,
      "loss": 0.2905,
      "step": 1178
    },
    {
      "epoch": 0.3467647058823529,
      "grad_norm": 0.06549356132745743,
      "learning_rate": 0.00013083946980854198,
      "loss": 0.3622,
      "step": 1179
    },
    {
      "epoch": 0.34705882352941175,
      "grad_norm": 0.067227803170681,
      "learning_rate": 0.00013078055964653904,
      "loss": 0.3598,
      "step": 1180
    },
    {
      "epoch": 0.3473529411764706,
      "grad_norm": 0.050884101539850235,
      "learning_rate": 0.0001307216494845361,
      "loss": 0.3306,
      "step": 1181
    },
    {
      "epoch": 0.3476470588235294,
      "grad_norm": 0.054385289549827576,
      "learning_rate": 0.00013066273932253316,
      "loss": 0.4175,
      "step": 1182
    },
    {
      "epoch": 0.34794117647058825,
      "grad_norm": 0.04653196781873703,
      "learning_rate": 0.0001306038291605302,
      "loss": 0.3671,
      "step": 1183
    },
    {
      "epoch": 0.34823529411764703,
      "grad_norm": 0.053195320069789886,
      "learning_rate": 0.00013054491899852725,
      "loss": 0.3586,
      "step": 1184
    },
    {
      "epoch": 0.34852941176470587,
      "grad_norm": 0.05439235270023346,
      "learning_rate": 0.0001304860088365243,
      "loss": 0.377,
      "step": 1185
    },
    {
      "epoch": 0.3488235294117647,
      "grad_norm": 0.056144874542951584,
      "learning_rate": 0.00013042709867452137,
      "loss": 0.3729,
      "step": 1186
    },
    {
      "epoch": 0.34911764705882353,
      "grad_norm": 0.06505059450864792,
      "learning_rate": 0.00013036818851251843,
      "loss": 0.4422,
      "step": 1187
    },
    {
      "epoch": 0.34941176470588237,
      "grad_norm": 0.048418764024972916,
      "learning_rate": 0.00013030927835051546,
      "loss": 0.3321,
      "step": 1188
    },
    {
      "epoch": 0.3497058823529412,
      "grad_norm": 0.04322925582528114,
      "learning_rate": 0.00013025036818851252,
      "loss": 0.2927,
      "step": 1189
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06102242320775986,
      "learning_rate": 0.00013019145802650958,
      "loss": 0.3811,
      "step": 1190
    },
    {
      "epoch": 0.3502941176470588,
      "grad_norm": 0.04398060962557793,
      "learning_rate": 0.00013013254786450664,
      "loss": 0.3183,
      "step": 1191
    },
    {
      "epoch": 0.35058823529411764,
      "grad_norm": 0.05748866870999336,
      "learning_rate": 0.0001300736377025037,
      "loss": 0.3801,
      "step": 1192
    },
    {
      "epoch": 0.3508823529411765,
      "grad_norm": 0.06515807658433914,
      "learning_rate": 0.00013001472754050074,
      "loss": 0.4093,
      "step": 1193
    },
    {
      "epoch": 0.3511764705882353,
      "grad_norm": 0.053015001118183136,
      "learning_rate": 0.0001299558173784978,
      "loss": 0.3875,
      "step": 1194
    },
    {
      "epoch": 0.3514705882352941,
      "grad_norm": 0.04905546084046364,
      "learning_rate": 0.00012989690721649486,
      "loss": 0.3678,
      "step": 1195
    },
    {
      "epoch": 0.3517647058823529,
      "grad_norm": 0.059127405285835266,
      "learning_rate": 0.00012983799705449192,
      "loss": 0.3067,
      "step": 1196
    },
    {
      "epoch": 0.35205882352941176,
      "grad_norm": 0.047860611230134964,
      "learning_rate": 0.00012977908689248898,
      "loss": 0.3325,
      "step": 1197
    },
    {
      "epoch": 0.3523529411764706,
      "grad_norm": 0.053043536841869354,
      "learning_rate": 0.000129720176730486,
      "loss": 0.3653,
      "step": 1198
    },
    {
      "epoch": 0.3526470588235294,
      "grad_norm": 0.05277305468916893,
      "learning_rate": 0.00012966126656848307,
      "loss": 0.3189,
      "step": 1199
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.05675249546766281,
      "learning_rate": 0.00012960235640648013,
      "loss": 0.387,
      "step": 1200
    },
    {
      "epoch": 0.35323529411764704,
      "grad_norm": 0.05718355253338814,
      "learning_rate": 0.0001295434462444772,
      "loss": 0.3462,
      "step": 1201
    },
    {
      "epoch": 0.35352941176470587,
      "grad_norm": 0.05512603372335434,
      "learning_rate": 0.00012948453608247425,
      "loss": 0.3875,
      "step": 1202
    },
    {
      "epoch": 0.3538235294117647,
      "grad_norm": 0.05182097107172012,
      "learning_rate": 0.00012942562592047128,
      "loss": 0.3338,
      "step": 1203
    },
    {
      "epoch": 0.35411764705882354,
      "grad_norm": 0.057482410222291946,
      "learning_rate": 0.00012936671575846834,
      "loss": 0.3243,
      "step": 1204
    },
    {
      "epoch": 0.35441176470588237,
      "grad_norm": 0.038084618747234344,
      "learning_rate": 0.0001293078055964654,
      "loss": 0.2387,
      "step": 1205
    },
    {
      "epoch": 0.3547058823529412,
      "grad_norm": 0.05351848527789116,
      "learning_rate": 0.00012924889543446247,
      "loss": 0.3644,
      "step": 1206
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.04926329478621483,
      "learning_rate": 0.00012918998527245953,
      "loss": 0.3252,
      "step": 1207
    },
    {
      "epoch": 0.3552941176470588,
      "grad_norm": 0.05077064037322998,
      "learning_rate": 0.00012913107511045656,
      "loss": 0.367,
      "step": 1208
    },
    {
      "epoch": 0.35558823529411765,
      "grad_norm": 0.051585786044597626,
      "learning_rate": 0.00012907216494845362,
      "loss": 0.3343,
      "step": 1209
    },
    {
      "epoch": 0.3558823529411765,
      "grad_norm": 0.055382419377565384,
      "learning_rate": 0.00012901325478645068,
      "loss": 0.4434,
      "step": 1210
    },
    {
      "epoch": 0.3561764705882353,
      "grad_norm": 0.06065521761775017,
      "learning_rate": 0.00012895434462444774,
      "loss": 0.3631,
      "step": 1211
    },
    {
      "epoch": 0.3564705882352941,
      "grad_norm": 0.04082208499312401,
      "learning_rate": 0.0001288954344624448,
      "loss": 0.2923,
      "step": 1212
    },
    {
      "epoch": 0.35676470588235293,
      "grad_norm": 0.06574767082929611,
      "learning_rate": 0.00012883652430044183,
      "loss": 0.3859,
      "step": 1213
    },
    {
      "epoch": 0.35705882352941176,
      "grad_norm": 0.05407517030835152,
      "learning_rate": 0.0001287776141384389,
      "loss": 0.3507,
      "step": 1214
    },
    {
      "epoch": 0.3573529411764706,
      "grad_norm": 0.07608309388160706,
      "learning_rate": 0.00012871870397643595,
      "loss": 0.4222,
      "step": 1215
    },
    {
      "epoch": 0.35764705882352943,
      "grad_norm": 0.052025239914655685,
      "learning_rate": 0.000128659793814433,
      "loss": 0.3405,
      "step": 1216
    },
    {
      "epoch": 0.35794117647058826,
      "grad_norm": 0.04819073528051376,
      "learning_rate": 0.00012860088365243007,
      "loss": 0.3266,
      "step": 1217
    },
    {
      "epoch": 0.35823529411764704,
      "grad_norm": 0.0502382293343544,
      "learning_rate": 0.0001285419734904271,
      "loss": 0.3672,
      "step": 1218
    },
    {
      "epoch": 0.3585294117647059,
      "grad_norm": 0.06028193235397339,
      "learning_rate": 0.00012848306332842417,
      "loss": 0.3983,
      "step": 1219
    },
    {
      "epoch": 0.3588235294117647,
      "grad_norm": 0.05038086324930191,
      "learning_rate": 0.00012842415316642123,
      "loss": 0.3409,
      "step": 1220
    },
    {
      "epoch": 0.35911764705882354,
      "grad_norm": 0.05125132575631142,
      "learning_rate": 0.00012836524300441829,
      "loss": 0.4019,
      "step": 1221
    },
    {
      "epoch": 0.3594117647058824,
      "grad_norm": 0.04419030621647835,
      "learning_rate": 0.00012830633284241535,
      "loss": 0.3667,
      "step": 1222
    },
    {
      "epoch": 0.35970588235294115,
      "grad_norm": 0.057859521359205246,
      "learning_rate": 0.00012824742268041238,
      "loss": 0.3581,
      "step": 1223
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.04242267087101936,
      "learning_rate": 0.00012818851251840944,
      "loss": 0.3345,
      "step": 1224
    },
    {
      "epoch": 0.3602941176470588,
      "grad_norm": 0.0687156543135643,
      "learning_rate": 0.0001281296023564065,
      "loss": 0.3811,
      "step": 1225
    },
    {
      "epoch": 0.36058823529411765,
      "grad_norm": 0.040801309049129486,
      "learning_rate": 0.00012807069219440353,
      "loss": 0.2779,
      "step": 1226
    },
    {
      "epoch": 0.3608823529411765,
      "grad_norm": 0.05003528296947479,
      "learning_rate": 0.0001280117820324006,
      "loss": 0.3619,
      "step": 1227
    },
    {
      "epoch": 0.3611764705882353,
      "grad_norm": 0.05927104875445366,
      "learning_rate": 0.00012795287187039763,
      "loss": 0.3638,
      "step": 1228
    },
    {
      "epoch": 0.3614705882352941,
      "grad_norm": 0.04665049910545349,
      "learning_rate": 0.00012789396170839469,
      "loss": 0.3266,
      "step": 1229
    },
    {
      "epoch": 0.36176470588235293,
      "grad_norm": 0.053482670336961746,
      "learning_rate": 0.00012783505154639175,
      "loss": 0.3751,
      "step": 1230
    },
    {
      "epoch": 0.36205882352941177,
      "grad_norm": 0.042950551956892014,
      "learning_rate": 0.0001277761413843888,
      "loss": 0.3171,
      "step": 1231
    },
    {
      "epoch": 0.3623529411764706,
      "grad_norm": 0.03970734030008316,
      "learning_rate": 0.00012771723122238587,
      "loss": 0.3433,
      "step": 1232
    },
    {
      "epoch": 0.36264705882352943,
      "grad_norm": 0.04591294750571251,
      "learning_rate": 0.0001276583210603829,
      "loss": 0.2926,
      "step": 1233
    },
    {
      "epoch": 0.3629411764705882,
      "grad_norm": 0.06312287598848343,
      "learning_rate": 0.00012759941089837996,
      "loss": 0.4164,
      "step": 1234
    },
    {
      "epoch": 0.36323529411764705,
      "grad_norm": 0.042269881814718246,
      "learning_rate": 0.00012754050073637702,
      "loss": 0.3201,
      "step": 1235
    },
    {
      "epoch": 0.3635294117647059,
      "grad_norm": 0.04923565313220024,
      "learning_rate": 0.00012748159057437408,
      "loss": 0.4111,
      "step": 1236
    },
    {
      "epoch": 0.3638235294117647,
      "grad_norm": 0.05416258051991463,
      "learning_rate": 0.00012742268041237114,
      "loss": 0.3528,
      "step": 1237
    },
    {
      "epoch": 0.36411764705882355,
      "grad_norm": 0.041663143783807755,
      "learning_rate": 0.00012736377025036817,
      "loss": 0.2794,
      "step": 1238
    },
    {
      "epoch": 0.3644117647058824,
      "grad_norm": 0.05009074509143829,
      "learning_rate": 0.00012730486008836523,
      "loss": 0.3265,
      "step": 1239
    },
    {
      "epoch": 0.36470588235294116,
      "grad_norm": 0.050338003784418106,
      "learning_rate": 0.0001272459499263623,
      "loss": 0.3747,
      "step": 1240
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06102761998772621,
      "learning_rate": 0.00012718703976435935,
      "loss": 0.4242,
      "step": 1241
    },
    {
      "epoch": 0.3652941176470588,
      "grad_norm": 0.047139983624219894,
      "learning_rate": 0.00012712812960235641,
      "loss": 0.3892,
      "step": 1242
    },
    {
      "epoch": 0.36558823529411766,
      "grad_norm": 0.04694826155900955,
      "learning_rate": 0.00012706921944035345,
      "loss": 0.303,
      "step": 1243
    },
    {
      "epoch": 0.3658823529411765,
      "grad_norm": 0.06911609321832657,
      "learning_rate": 0.0001270103092783505,
      "loss": 0.3601,
      "step": 1244
    },
    {
      "epoch": 0.36617647058823527,
      "grad_norm": 0.062218792736530304,
      "learning_rate": 0.00012695139911634757,
      "loss": 0.3421,
      "step": 1245
    },
    {
      "epoch": 0.3664705882352941,
      "grad_norm": 0.05655054375529289,
      "learning_rate": 0.00012689248895434463,
      "loss": 0.3996,
      "step": 1246
    },
    {
      "epoch": 0.36676470588235294,
      "grad_norm": 0.07099892199039459,
      "learning_rate": 0.0001268335787923417,
      "loss": 0.4487,
      "step": 1247
    },
    {
      "epoch": 0.36705882352941177,
      "grad_norm": 0.07421479374170303,
      "learning_rate": 0.00012677466863033872,
      "loss": 0.4268,
      "step": 1248
    },
    {
      "epoch": 0.3673529411764706,
      "grad_norm": 0.06206342205405235,
      "learning_rate": 0.00012671575846833578,
      "loss": 0.374,
      "step": 1249
    },
    {
      "epoch": 0.36764705882352944,
      "grad_norm": 0.052368368953466415,
      "learning_rate": 0.00012665684830633284,
      "loss": 0.3717,
      "step": 1250
    },
    {
      "epoch": 0.3679411764705882,
      "grad_norm": 0.06473471224308014,
      "learning_rate": 0.0001265979381443299,
      "loss": 0.3914,
      "step": 1251
    },
    {
      "epoch": 0.36823529411764705,
      "grad_norm": 0.047993823885917664,
      "learning_rate": 0.00012653902798232696,
      "loss": 0.3249,
      "step": 1252
    },
    {
      "epoch": 0.3685294117647059,
      "grad_norm": 0.04610491916537285,
      "learning_rate": 0.000126480117820324,
      "loss": 0.2667,
      "step": 1253
    },
    {
      "epoch": 0.3688235294117647,
      "grad_norm": 0.05358102545142174,
      "learning_rate": 0.00012642120765832105,
      "loss": 0.3577,
      "step": 1254
    },
    {
      "epoch": 0.36911764705882355,
      "grad_norm": 0.05526606738567352,
      "learning_rate": 0.00012636229749631812,
      "loss": 0.3335,
      "step": 1255
    },
    {
      "epoch": 0.36941176470588233,
      "grad_norm": 0.04813884198665619,
      "learning_rate": 0.00012630338733431518,
      "loss": 0.3455,
      "step": 1256
    },
    {
      "epoch": 0.36970588235294116,
      "grad_norm": 0.05519367754459381,
      "learning_rate": 0.00012624447717231224,
      "loss": 0.3557,
      "step": 1257
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04282229393720627,
      "learning_rate": 0.00012618556701030927,
      "loss": 0.2827,
      "step": 1258
    },
    {
      "epoch": 0.37029411764705883,
      "grad_norm": 0.04396507889032364,
      "learning_rate": 0.00012612665684830633,
      "loss": 0.2994,
      "step": 1259
    },
    {
      "epoch": 0.37058823529411766,
      "grad_norm": 0.05511938035488129,
      "learning_rate": 0.0001260677466863034,
      "loss": 0.3551,
      "step": 1260
    },
    {
      "epoch": 0.3708823529411765,
      "grad_norm": 0.0768919438123703,
      "learning_rate": 0.00012600883652430045,
      "loss": 0.4302,
      "step": 1261
    },
    {
      "epoch": 0.3711764705882353,
      "grad_norm": 0.07389368861913681,
      "learning_rate": 0.0001259499263622975,
      "loss": 0.3785,
      "step": 1262
    },
    {
      "epoch": 0.3714705882352941,
      "grad_norm": 0.04722702130675316,
      "learning_rate": 0.00012589101620029454,
      "loss": 0.335,
      "step": 1263
    },
    {
      "epoch": 0.37176470588235294,
      "grad_norm": 0.06630875170230865,
      "learning_rate": 0.0001258321060382916,
      "loss": 0.348,
      "step": 1264
    },
    {
      "epoch": 0.3720588235294118,
      "grad_norm": 0.04876803606748581,
      "learning_rate": 0.00012577319587628866,
      "loss": 0.3731,
      "step": 1265
    },
    {
      "epoch": 0.3723529411764706,
      "grad_norm": 0.05568288639187813,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.3978,
      "step": 1266
    },
    {
      "epoch": 0.3726470588235294,
      "grad_norm": 0.06753812730312347,
      "learning_rate": 0.00012565537555228278,
      "loss": 0.4195,
      "step": 1267
    },
    {
      "epoch": 0.3729411764705882,
      "grad_norm": 0.04759494587779045,
      "learning_rate": 0.00012559646539027982,
      "loss": 0.3162,
      "step": 1268
    },
    {
      "epoch": 0.37323529411764705,
      "grad_norm": 0.049286358058452606,
      "learning_rate": 0.00012553755522827688,
      "loss": 0.3572,
      "step": 1269
    },
    {
      "epoch": 0.3735294117647059,
      "grad_norm": 0.05771895498037338,
      "learning_rate": 0.00012547864506627394,
      "loss": 0.3607,
      "step": 1270
    },
    {
      "epoch": 0.3738235294117647,
      "grad_norm": 0.055867891758680344,
      "learning_rate": 0.000125419734904271,
      "loss": 0.3535,
      "step": 1271
    },
    {
      "epoch": 0.37411764705882355,
      "grad_norm": 0.05273571237921715,
      "learning_rate": 0.00012536082474226806,
      "loss": 0.3678,
      "step": 1272
    },
    {
      "epoch": 0.37441176470588233,
      "grad_norm": 0.04210754483938217,
      "learning_rate": 0.0001253019145802651,
      "loss": 0.3302,
      "step": 1273
    },
    {
      "epoch": 0.37470588235294117,
      "grad_norm": 0.04759346321225166,
      "learning_rate": 0.00012524300441826215,
      "loss": 0.3719,
      "step": 1274
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.044636648148298264,
      "learning_rate": 0.0001251840942562592,
      "loss": 0.3158,
      "step": 1275
    },
    {
      "epoch": 0.37529411764705883,
      "grad_norm": 0.060443103313446045,
      "learning_rate": 0.00012512518409425627,
      "loss": 0.4525,
      "step": 1276
    },
    {
      "epoch": 0.37558823529411767,
      "grad_norm": 0.04425424709916115,
      "learning_rate": 0.00012506627393225333,
      "loss": 0.3295,
      "step": 1277
    },
    {
      "epoch": 0.37588235294117645,
      "grad_norm": 0.04747970402240753,
      "learning_rate": 0.00012500736377025036,
      "loss": 0.3899,
      "step": 1278
    },
    {
      "epoch": 0.3761764705882353,
      "grad_norm": 0.04468274116516113,
      "learning_rate": 0.00012494845360824742,
      "loss": 0.3071,
      "step": 1279
    },
    {
      "epoch": 0.3764705882352941,
      "grad_norm": 0.05490390583872795,
      "learning_rate": 0.00012488954344624448,
      "loss": 0.3782,
      "step": 1280
    },
    {
      "epoch": 0.37676470588235295,
      "grad_norm": 0.05540602281689644,
      "learning_rate": 0.00012483063328424154,
      "loss": 0.3591,
      "step": 1281
    },
    {
      "epoch": 0.3770588235294118,
      "grad_norm": 0.04729980230331421,
      "learning_rate": 0.0001247717231222386,
      "loss": 0.3363,
      "step": 1282
    },
    {
      "epoch": 0.3773529411764706,
      "grad_norm": 0.04645152390003204,
      "learning_rate": 0.00012471281296023564,
      "loss": 0.3546,
      "step": 1283
    },
    {
      "epoch": 0.3776470588235294,
      "grad_norm": 0.05511489510536194,
      "learning_rate": 0.0001246539027982327,
      "loss": 0.4294,
      "step": 1284
    },
    {
      "epoch": 0.3779411764705882,
      "grad_norm": 0.04356254264712334,
      "learning_rate": 0.00012459499263622976,
      "loss": 0.3119,
      "step": 1285
    },
    {
      "epoch": 0.37823529411764706,
      "grad_norm": 0.064662866294384,
      "learning_rate": 0.00012453608247422682,
      "loss": 0.3804,
      "step": 1286
    },
    {
      "epoch": 0.3785294117647059,
      "grad_norm": 0.03797849267721176,
      "learning_rate": 0.00012447717231222388,
      "loss": 0.2851,
      "step": 1287
    },
    {
      "epoch": 0.3788235294117647,
      "grad_norm": 0.059722378849983215,
      "learning_rate": 0.0001244182621502209,
      "loss": 0.3578,
      "step": 1288
    },
    {
      "epoch": 0.3791176470588235,
      "grad_norm": 0.05306950956583023,
      "learning_rate": 0.00012435935198821797,
      "loss": 0.3818,
      "step": 1289
    },
    {
      "epoch": 0.37941176470588234,
      "grad_norm": 0.0426027774810791,
      "learning_rate": 0.00012430044182621503,
      "loss": 0.3543,
      "step": 1290
    },
    {
      "epoch": 0.37970588235294117,
      "grad_norm": 0.04577934741973877,
      "learning_rate": 0.0001242415316642121,
      "loss": 0.3139,
      "step": 1291
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.050159208476543427,
      "learning_rate": 0.00012418262150220915,
      "loss": 0.3748,
      "step": 1292
    },
    {
      "epoch": 0.38029411764705884,
      "grad_norm": 0.058271512389183044,
      "learning_rate": 0.00012412371134020618,
      "loss": 0.4219,
      "step": 1293
    },
    {
      "epoch": 0.38058823529411767,
      "grad_norm": 0.04483126848936081,
      "learning_rate": 0.00012406480117820324,
      "loss": 0.324,
      "step": 1294
    },
    {
      "epoch": 0.38088235294117645,
      "grad_norm": 0.04657711461186409,
      "learning_rate": 0.0001240058910162003,
      "loss": 0.3071,
      "step": 1295
    },
    {
      "epoch": 0.3811764705882353,
      "grad_norm": 0.03605169057846069,
      "learning_rate": 0.00012394698085419736,
      "loss": 0.2929,
      "step": 1296
    },
    {
      "epoch": 0.3814705882352941,
      "grad_norm": 0.042276110500097275,
      "learning_rate": 0.00012388807069219443,
      "loss": 0.3455,
      "step": 1297
    },
    {
      "epoch": 0.38176470588235295,
      "grad_norm": 0.0625295639038086,
      "learning_rate": 0.00012382916053019146,
      "loss": 0.3889,
      "step": 1298
    },
    {
      "epoch": 0.3820588235294118,
      "grad_norm": 0.03907632455229759,
      "learning_rate": 0.00012377025036818852,
      "loss": 0.3408,
      "step": 1299
    },
    {
      "epoch": 0.38235294117647056,
      "grad_norm": 0.041001755744218826,
      "learning_rate": 0.00012371134020618558,
      "loss": 0.3384,
      "step": 1300
    },
    {
      "epoch": 0.3826470588235294,
      "grad_norm": 0.053751252591609955,
      "learning_rate": 0.00012365243004418264,
      "loss": 0.3946,
      "step": 1301
    },
    {
      "epoch": 0.38294117647058823,
      "grad_norm": 0.054638054221868515,
      "learning_rate": 0.0001235935198821797,
      "loss": 0.3628,
      "step": 1302
    },
    {
      "epoch": 0.38323529411764706,
      "grad_norm": 0.05607336387038231,
      "learning_rate": 0.00012353460972017673,
      "loss": 0.344,
      "step": 1303
    },
    {
      "epoch": 0.3835294117647059,
      "grad_norm": 0.053780119866132736,
      "learning_rate": 0.0001234756995581738,
      "loss": 0.3947,
      "step": 1304
    },
    {
      "epoch": 0.38382352941176473,
      "grad_norm": 0.06745556741952896,
      "learning_rate": 0.00012341678939617085,
      "loss": 0.4397,
      "step": 1305
    },
    {
      "epoch": 0.3841176470588235,
      "grad_norm": 0.061755623668432236,
      "learning_rate": 0.0001233578792341679,
      "loss": 0.3992,
      "step": 1306
    },
    {
      "epoch": 0.38441176470588234,
      "grad_norm": 0.03516651317477226,
      "learning_rate": 0.00012329896907216497,
      "loss": 0.2784,
      "step": 1307
    },
    {
      "epoch": 0.3847058823529412,
      "grad_norm": 0.039529815316200256,
      "learning_rate": 0.000123240058910162,
      "loss": 0.2914,
      "step": 1308
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.058236125856637955,
      "learning_rate": 0.00012318114874815907,
      "loss": 0.3438,
      "step": 1309
    },
    {
      "epoch": 0.38529411764705884,
      "grad_norm": 0.04956669360399246,
      "learning_rate": 0.00012312223858615613,
      "loss": 0.3844,
      "step": 1310
    },
    {
      "epoch": 0.3855882352941176,
      "grad_norm": 0.03252106159925461,
      "learning_rate": 0.00012306332842415319,
      "loss": 0.258,
      "step": 1311
    },
    {
      "epoch": 0.38588235294117645,
      "grad_norm": 0.05745253339409828,
      "learning_rate": 0.00012300441826215025,
      "loss": 0.3595,
      "step": 1312
    },
    {
      "epoch": 0.3861764705882353,
      "grad_norm": 0.046317920088768005,
      "learning_rate": 0.00012294550810014728,
      "loss": 0.3206,
      "step": 1313
    },
    {
      "epoch": 0.3864705882352941,
      "grad_norm": 0.04663233831524849,
      "learning_rate": 0.00012288659793814434,
      "loss": 0.3575,
      "step": 1314
    },
    {
      "epoch": 0.38676470588235295,
      "grad_norm": 0.05239938199520111,
      "learning_rate": 0.0001228276877761414,
      "loss": 0.3665,
      "step": 1315
    },
    {
      "epoch": 0.3870588235294118,
      "grad_norm": 0.047448012977838516,
      "learning_rate": 0.00012276877761413846,
      "loss": 0.3681,
      "step": 1316
    },
    {
      "epoch": 0.38735294117647057,
      "grad_norm": 0.05752386897802353,
      "learning_rate": 0.00012270986745213552,
      "loss": 0.4177,
      "step": 1317
    },
    {
      "epoch": 0.3876470588235294,
      "grad_norm": 0.04953908175230026,
      "learning_rate": 0.00012265095729013255,
      "loss": 0.3596,
      "step": 1318
    },
    {
      "epoch": 0.38794117647058823,
      "grad_norm": 0.047495074570178986,
      "learning_rate": 0.0001225920471281296,
      "loss": 0.3751,
      "step": 1319
    },
    {
      "epoch": 0.38823529411764707,
      "grad_norm": 0.05075526610016823,
      "learning_rate": 0.00012253313696612667,
      "loss": 0.3756,
      "step": 1320
    },
    {
      "epoch": 0.3885294117647059,
      "grad_norm": 0.055516067892313004,
      "learning_rate": 0.00012247422680412373,
      "loss": 0.3861,
      "step": 1321
    },
    {
      "epoch": 0.3888235294117647,
      "grad_norm": 0.06265439838171005,
      "learning_rate": 0.0001224153166421208,
      "loss": 0.3668,
      "step": 1322
    },
    {
      "epoch": 0.3891176470588235,
      "grad_norm": 0.04859693720936775,
      "learning_rate": 0.00012235640648011783,
      "loss": 0.3575,
      "step": 1323
    },
    {
      "epoch": 0.38941176470588235,
      "grad_norm": 0.0557103231549263,
      "learning_rate": 0.0001222974963181149,
      "loss": 0.363,
      "step": 1324
    },
    {
      "epoch": 0.3897058823529412,
      "grad_norm": 0.06495865434408188,
      "learning_rate": 0.00012223858615611195,
      "loss": 0.3823,
      "step": 1325
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.043825916945934296,
      "learning_rate": 0.000122179675994109,
      "loss": 0.3139,
      "step": 1326
    },
    {
      "epoch": 0.39029411764705885,
      "grad_norm": 0.0545281358063221,
      "learning_rate": 0.00012212076583210607,
      "loss": 0.4027,
      "step": 1327
    },
    {
      "epoch": 0.3905882352941176,
      "grad_norm": 0.04920879378914833,
      "learning_rate": 0.00012206185567010311,
      "loss": 0.3857,
      "step": 1328
    },
    {
      "epoch": 0.39088235294117646,
      "grad_norm": 0.050975970923900604,
      "learning_rate": 0.00012200294550810016,
      "loss": 0.3449,
      "step": 1329
    },
    {
      "epoch": 0.3911764705882353,
      "grad_norm": 0.07928720116615295,
      "learning_rate": 0.00012194403534609722,
      "loss": 0.4392,
      "step": 1330
    },
    {
      "epoch": 0.3914705882352941,
      "grad_norm": 0.04282214120030403,
      "learning_rate": 0.00012188512518409427,
      "loss": 0.3444,
      "step": 1331
    },
    {
      "epoch": 0.39176470588235296,
      "grad_norm": 0.05043970048427582,
      "learning_rate": 0.0001218262150220913,
      "loss": 0.3694,
      "step": 1332
    },
    {
      "epoch": 0.39205882352941174,
      "grad_norm": 0.051021769642829895,
      "learning_rate": 0.00012176730486008836,
      "loss": 0.3612,
      "step": 1333
    },
    {
      "epoch": 0.39235294117647057,
      "grad_norm": 0.049550630152225494,
      "learning_rate": 0.00012170839469808542,
      "loss": 0.3859,
      "step": 1334
    },
    {
      "epoch": 0.3926470588235294,
      "grad_norm": 0.051163844764232635,
      "learning_rate": 0.00012164948453608247,
      "loss": 0.4093,
      "step": 1335
    },
    {
      "epoch": 0.39294117647058824,
      "grad_norm": 0.0495951883494854,
      "learning_rate": 0.00012159057437407953,
      "loss": 0.3864,
      "step": 1336
    },
    {
      "epoch": 0.39323529411764707,
      "grad_norm": 0.04082044959068298,
      "learning_rate": 0.00012153166421207657,
      "loss": 0.3191,
      "step": 1337
    },
    {
      "epoch": 0.3935294117647059,
      "grad_norm": 0.06964438408613205,
      "learning_rate": 0.00012147275405007363,
      "loss": 0.4417,
      "step": 1338
    },
    {
      "epoch": 0.3938235294117647,
      "grad_norm": 0.06520950794219971,
      "learning_rate": 0.0001214138438880707,
      "loss": 0.4494,
      "step": 1339
    },
    {
      "epoch": 0.3941176470588235,
      "grad_norm": 0.04573240876197815,
      "learning_rate": 0.00012135493372606774,
      "loss": 0.33,
      "step": 1340
    },
    {
      "epoch": 0.39441176470588235,
      "grad_norm": 0.03914204239845276,
      "learning_rate": 0.0001212960235640648,
      "loss": 0.2753,
      "step": 1341
    },
    {
      "epoch": 0.3947058823529412,
      "grad_norm": 0.06401056051254272,
      "learning_rate": 0.00012123711340206185,
      "loss": 0.3679,
      "step": 1342
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.049247968941926956,
      "learning_rate": 0.00012117820324005891,
      "loss": 0.3236,
      "step": 1343
    },
    {
      "epoch": 0.3952941176470588,
      "grad_norm": 0.058798640966415405,
      "learning_rate": 0.00012111929307805597,
      "loss": 0.3597,
      "step": 1344
    },
    {
      "epoch": 0.39558823529411763,
      "grad_norm": 0.06224774941802025,
      "learning_rate": 0.00012106038291605301,
      "loss": 0.3943,
      "step": 1345
    },
    {
      "epoch": 0.39588235294117646,
      "grad_norm": 0.03800506889820099,
      "learning_rate": 0.00012100147275405008,
      "loss": 0.2893,
      "step": 1346
    },
    {
      "epoch": 0.3961764705882353,
      "grad_norm": 0.06172817572951317,
      "learning_rate": 0.00012094256259204712,
      "loss": 0.4119,
      "step": 1347
    },
    {
      "epoch": 0.39647058823529413,
      "grad_norm": 0.0794820785522461,
      "learning_rate": 0.00012088365243004418,
      "loss": 0.3972,
      "step": 1348
    },
    {
      "epoch": 0.39676470588235296,
      "grad_norm": 0.050745267421007156,
      "learning_rate": 0.00012082474226804124,
      "loss": 0.3603,
      "step": 1349
    },
    {
      "epoch": 0.39705882352941174,
      "grad_norm": 0.046682968735694885,
      "learning_rate": 0.00012076583210603829,
      "loss": 0.4045,
      "step": 1350
    },
    {
      "epoch": 0.3973529411764706,
      "grad_norm": 0.04742817580699921,
      "learning_rate": 0.00012070692194403535,
      "loss": 0.3202,
      "step": 1351
    },
    {
      "epoch": 0.3976470588235294,
      "grad_norm": 0.042382266372442245,
      "learning_rate": 0.0001206480117820324,
      "loss": 0.3541,
      "step": 1352
    },
    {
      "epoch": 0.39794117647058824,
      "grad_norm": 0.04189004376530647,
      "learning_rate": 0.00012058910162002946,
      "loss": 0.2823,
      "step": 1353
    },
    {
      "epoch": 0.3982352941176471,
      "grad_norm": 0.042059335857629776,
      "learning_rate": 0.00012053019145802652,
      "loss": 0.3408,
      "step": 1354
    },
    {
      "epoch": 0.3985294117647059,
      "grad_norm": 0.04467776045203209,
      "learning_rate": 0.00012047128129602356,
      "loss": 0.2704,
      "step": 1355
    },
    {
      "epoch": 0.3988235294117647,
      "grad_norm": 0.05947007238864899,
      "learning_rate": 0.00012041237113402062,
      "loss": 0.4138,
      "step": 1356
    },
    {
      "epoch": 0.3991176470588235,
      "grad_norm": 0.05069107562303543,
      "learning_rate": 0.00012035346097201767,
      "loss": 0.291,
      "step": 1357
    },
    {
      "epoch": 0.39941176470588236,
      "grad_norm": 0.051622193306684494,
      "learning_rate": 0.00012029455081001473,
      "loss": 0.3358,
      "step": 1358
    },
    {
      "epoch": 0.3997058823529412,
      "grad_norm": 0.04652887582778931,
      "learning_rate": 0.00012023564064801178,
      "loss": 0.3386,
      "step": 1359
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.05218986049294472,
      "learning_rate": 0.00012017673048600884,
      "loss": 0.3683,
      "step": 1360
    },
    {
      "epoch": 0.4002941176470588,
      "grad_norm": 0.04599303752183914,
      "learning_rate": 0.0001201178203240059,
      "loss": 0.3488,
      "step": 1361
    },
    {
      "epoch": 0.40058823529411763,
      "grad_norm": 0.051949143409729004,
      "learning_rate": 0.00012005891016200294,
      "loss": 0.4233,
      "step": 1362
    },
    {
      "epoch": 0.40088235294117647,
      "grad_norm": 0.06342252343893051,
      "learning_rate": 0.00012,
      "loss": 0.4162,
      "step": 1363
    },
    {
      "epoch": 0.4011764705882353,
      "grad_norm": 0.04974079132080078,
      "learning_rate": 0.00011994108983799705,
      "loss": 0.2892,
      "step": 1364
    },
    {
      "epoch": 0.40147058823529413,
      "grad_norm": 0.054079294204711914,
      "learning_rate": 0.00011988217967599411,
      "loss": 0.3676,
      "step": 1365
    },
    {
      "epoch": 0.40176470588235297,
      "grad_norm": 0.05098779499530792,
      "learning_rate": 0.00011982326951399117,
      "loss": 0.3998,
      "step": 1366
    },
    {
      "epoch": 0.40205882352941175,
      "grad_norm": 0.04928348958492279,
      "learning_rate": 0.00011976435935198822,
      "loss": 0.3154,
      "step": 1367
    },
    {
      "epoch": 0.4023529411764706,
      "grad_norm": 0.04938960820436478,
      "learning_rate": 0.00011970544918998528,
      "loss": 0.3316,
      "step": 1368
    },
    {
      "epoch": 0.4026470588235294,
      "grad_norm": 0.0444144569337368,
      "learning_rate": 0.00011964653902798232,
      "loss": 0.324,
      "step": 1369
    },
    {
      "epoch": 0.40294117647058825,
      "grad_norm": 0.05739104747772217,
      "learning_rate": 0.00011958762886597938,
      "loss": 0.398,
      "step": 1370
    },
    {
      "epoch": 0.4032352941176471,
      "grad_norm": 0.041217319667339325,
      "learning_rate": 0.00011952871870397644,
      "loss": 0.3219,
      "step": 1371
    },
    {
      "epoch": 0.40352941176470586,
      "grad_norm": 0.03784308210015297,
      "learning_rate": 0.00011946980854197349,
      "loss": 0.3319,
      "step": 1372
    },
    {
      "epoch": 0.4038235294117647,
      "grad_norm": 0.06552661210298538,
      "learning_rate": 0.00011941089837997055,
      "loss": 0.4132,
      "step": 1373
    },
    {
      "epoch": 0.4041176470588235,
      "grad_norm": 0.06006975099444389,
      "learning_rate": 0.0001193519882179676,
      "loss": 0.3654,
      "step": 1374
    },
    {
      "epoch": 0.40441176470588236,
      "grad_norm": 0.06632746756076813,
      "learning_rate": 0.00011929307805596466,
      "loss": 0.4114,
      "step": 1375
    },
    {
      "epoch": 0.4047058823529412,
      "grad_norm": 0.053175587207078934,
      "learning_rate": 0.00011923416789396172,
      "loss": 0.3593,
      "step": 1376
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.0450337752699852,
      "learning_rate": 0.00011917525773195876,
      "loss": 0.3261,
      "step": 1377
    },
    {
      "epoch": 0.4052941176470588,
      "grad_norm": 0.04300384223461151,
      "learning_rate": 0.00011911634756995582,
      "loss": 0.3361,
      "step": 1378
    },
    {
      "epoch": 0.40558823529411764,
      "grad_norm": 0.043306466192007065,
      "learning_rate": 0.00011905743740795287,
      "loss": 0.316,
      "step": 1379
    },
    {
      "epoch": 0.40588235294117647,
      "grad_norm": 0.06758464127779007,
      "learning_rate": 0.00011899852724594993,
      "loss": 0.4155,
      "step": 1380
    },
    {
      "epoch": 0.4061764705882353,
      "grad_norm": 0.0488818921148777,
      "learning_rate": 0.00011893961708394699,
      "loss": 0.3613,
      "step": 1381
    },
    {
      "epoch": 0.40647058823529414,
      "grad_norm": 0.04967258870601654,
      "learning_rate": 0.00011888070692194404,
      "loss": 0.3739,
      "step": 1382
    },
    {
      "epoch": 0.4067647058823529,
      "grad_norm": 0.04391469061374664,
      "learning_rate": 0.0001188217967599411,
      "loss": 0.3287,
      "step": 1383
    },
    {
      "epoch": 0.40705882352941175,
      "grad_norm": 0.06117546185851097,
      "learning_rate": 0.00011876288659793814,
      "loss": 0.4001,
      "step": 1384
    },
    {
      "epoch": 0.4073529411764706,
      "grad_norm": 0.05095063894987106,
      "learning_rate": 0.0001187039764359352,
      "loss": 0.3601,
      "step": 1385
    },
    {
      "epoch": 0.4076470588235294,
      "grad_norm": 0.04802936315536499,
      "learning_rate": 0.00011864506627393226,
      "loss": 0.3265,
      "step": 1386
    },
    {
      "epoch": 0.40794117647058825,
      "grad_norm": 0.04463087394833565,
      "learning_rate": 0.00011858615611192931,
      "loss": 0.3093,
      "step": 1387
    },
    {
      "epoch": 0.4082352941176471,
      "grad_norm": 0.055778104811906815,
      "learning_rate": 0.00011852724594992637,
      "loss": 0.3636,
      "step": 1388
    },
    {
      "epoch": 0.40852941176470586,
      "grad_norm": 0.044072072952985764,
      "learning_rate": 0.00011846833578792342,
      "loss": 0.3297,
      "step": 1389
    },
    {
      "epoch": 0.4088235294117647,
      "grad_norm": 0.051623936742544174,
      "learning_rate": 0.00011840942562592048,
      "loss": 0.3949,
      "step": 1390
    },
    {
      "epoch": 0.40911764705882353,
      "grad_norm": 0.05663737654685974,
      "learning_rate": 0.00011835051546391754,
      "loss": 0.369,
      "step": 1391
    },
    {
      "epoch": 0.40941176470588236,
      "grad_norm": 0.05134459584951401,
      "learning_rate": 0.00011829160530191459,
      "loss": 0.3253,
      "step": 1392
    },
    {
      "epoch": 0.4097058823529412,
      "grad_norm": 0.05714389681816101,
      "learning_rate": 0.00011823269513991165,
      "loss": 0.4311,
      "step": 1393
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.04258742555975914,
      "learning_rate": 0.00011817378497790869,
      "loss": 0.3503,
      "step": 1394
    },
    {
      "epoch": 0.4102941176470588,
      "grad_norm": 0.056830912828445435,
      "learning_rate": 0.00011811487481590575,
      "loss": 0.3877,
      "step": 1395
    },
    {
      "epoch": 0.41058823529411764,
      "grad_norm": 0.05259817838668823,
      "learning_rate": 0.00011805596465390281,
      "loss": 0.3977,
      "step": 1396
    },
    {
      "epoch": 0.4108823529411765,
      "grad_norm": 0.050232723355293274,
      "learning_rate": 0.00011799705449189986,
      "loss": 0.3841,
      "step": 1397
    },
    {
      "epoch": 0.4111764705882353,
      "grad_norm": 0.04241369664669037,
      "learning_rate": 0.00011793814432989692,
      "loss": 0.3039,
      "step": 1398
    },
    {
      "epoch": 0.41147058823529414,
      "grad_norm": 0.03655943274497986,
      "learning_rate": 0.00011787923416789397,
      "loss": 0.2639,
      "step": 1399
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.03909563273191452,
      "learning_rate": 0.00011782032400589103,
      "loss": 0.274,
      "step": 1400
    },
    {
      "epoch": 0.41205882352941176,
      "grad_norm": 0.07460065931081772,
      "learning_rate": 0.00011776141384388809,
      "loss": 0.3828,
      "step": 1401
    },
    {
      "epoch": 0.4123529411764706,
      "grad_norm": 0.04392150044441223,
      "learning_rate": 0.00011770250368188513,
      "loss": 0.3196,
      "step": 1402
    },
    {
      "epoch": 0.4126470588235294,
      "grad_norm": 0.053774643689394,
      "learning_rate": 0.00011764359351988219,
      "loss": 0.3753,
      "step": 1403
    },
    {
      "epoch": 0.41294117647058826,
      "grad_norm": 0.05775910243391991,
      "learning_rate": 0.00011758468335787924,
      "loss": 0.3431,
      "step": 1404
    },
    {
      "epoch": 0.41323529411764703,
      "grad_norm": 0.057249050587415695,
      "learning_rate": 0.0001175257731958763,
      "loss": 0.323,
      "step": 1405
    },
    {
      "epoch": 0.41352941176470587,
      "grad_norm": 0.05422135815024376,
      "learning_rate": 0.00011746686303387336,
      "loss": 0.3726,
      "step": 1406
    },
    {
      "epoch": 0.4138235294117647,
      "grad_norm": 0.068003349006176,
      "learning_rate": 0.0001174079528718704,
      "loss": 0.4008,
      "step": 1407
    },
    {
      "epoch": 0.41411764705882353,
      "grad_norm": 0.06354010105133057,
      "learning_rate": 0.00011734904270986747,
      "loss": 0.3895,
      "step": 1408
    },
    {
      "epoch": 0.41441176470588237,
      "grad_norm": 0.048743490129709244,
      "learning_rate": 0.00011729013254786451,
      "loss": 0.3584,
      "step": 1409
    },
    {
      "epoch": 0.4147058823529412,
      "grad_norm": 0.03658454492688179,
      "learning_rate": 0.00011723122238586157,
      "loss": 0.3123,
      "step": 1410
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.04793798550963402,
      "learning_rate": 0.00011717231222385863,
      "loss": 0.3576,
      "step": 1411
    },
    {
      "epoch": 0.4152941176470588,
      "grad_norm": 0.03904734551906586,
      "learning_rate": 0.00011711340206185568,
      "loss": 0.2857,
      "step": 1412
    },
    {
      "epoch": 0.41558823529411765,
      "grad_norm": 0.052268434315919876,
      "learning_rate": 0.00011705449189985274,
      "loss": 0.3849,
      "step": 1413
    },
    {
      "epoch": 0.4158823529411765,
      "grad_norm": 0.05369556322693825,
      "learning_rate": 0.00011699558173784979,
      "loss": 0.4046,
      "step": 1414
    },
    {
      "epoch": 0.4161764705882353,
      "grad_norm": 0.04974417760968208,
      "learning_rate": 0.00011693667157584685,
      "loss": 0.3595,
      "step": 1415
    },
    {
      "epoch": 0.4164705882352941,
      "grad_norm": 0.05012394115328789,
      "learning_rate": 0.0001168777614138439,
      "loss": 0.3995,
      "step": 1416
    },
    {
      "epoch": 0.4167647058823529,
      "grad_norm": 0.055952947586774826,
      "learning_rate": 0.00011681885125184095,
      "loss": 0.3384,
      "step": 1417
    },
    {
      "epoch": 0.41705882352941176,
      "grad_norm": 0.04726039990782738,
      "learning_rate": 0.00011675994108983801,
      "loss": 0.3713,
      "step": 1418
    },
    {
      "epoch": 0.4173529411764706,
      "grad_norm": 0.048330146819353104,
      "learning_rate": 0.00011670103092783506,
      "loss": 0.3836,
      "step": 1419
    },
    {
      "epoch": 0.4176470588235294,
      "grad_norm": 0.04266269877552986,
      "learning_rate": 0.00011664212076583212,
      "loss": 0.3235,
      "step": 1420
    },
    {
      "epoch": 0.41794117647058826,
      "grad_norm": 0.05757750943303108,
      "learning_rate": 0.00011658321060382917,
      "loss": 0.4101,
      "step": 1421
    },
    {
      "epoch": 0.41823529411764704,
      "grad_norm": 0.045642271637916565,
      "learning_rate": 0.00011652430044182623,
      "loss": 0.3659,
      "step": 1422
    },
    {
      "epoch": 0.41852941176470587,
      "grad_norm": 0.04070141166448593,
      "learning_rate": 0.00011646539027982329,
      "loss": 0.3492,
      "step": 1423
    },
    {
      "epoch": 0.4188235294117647,
      "grad_norm": 0.042854733765125275,
      "learning_rate": 0.00011640648011782033,
      "loss": 0.2532,
      "step": 1424
    },
    {
      "epoch": 0.41911764705882354,
      "grad_norm": 0.051709409803152084,
      "learning_rate": 0.0001163475699558174,
      "loss": 0.4325,
      "step": 1425
    },
    {
      "epoch": 0.4194117647058824,
      "grad_norm": 0.05406966432929039,
      "learning_rate": 0.00011628865979381444,
      "loss": 0.4008,
      "step": 1426
    },
    {
      "epoch": 0.41970588235294115,
      "grad_norm": 0.04792850837111473,
      "learning_rate": 0.0001162297496318115,
      "loss": 0.3322,
      "step": 1427
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.04542122036218643,
      "learning_rate": 0.00011617083946980856,
      "loss": 0.3124,
      "step": 1428
    },
    {
      "epoch": 0.4202941176470588,
      "grad_norm": 0.06228867545723915,
      "learning_rate": 0.00011611192930780561,
      "loss": 0.3382,
      "step": 1429
    },
    {
      "epoch": 0.42058823529411765,
      "grad_norm": 0.04599594324827194,
      "learning_rate": 0.00011605301914580267,
      "loss": 0.3762,
      "step": 1430
    },
    {
      "epoch": 0.4208823529411765,
      "grad_norm": 0.044481392949819565,
      "learning_rate": 0.00011599410898379971,
      "loss": 0.3516,
      "step": 1431
    },
    {
      "epoch": 0.4211764705882353,
      "grad_norm": 0.043788935989141464,
      "learning_rate": 0.00011593519882179677,
      "loss": 0.2687,
      "step": 1432
    },
    {
      "epoch": 0.4214705882352941,
      "grad_norm": 0.05554361641407013,
      "learning_rate": 0.00011587628865979384,
      "loss": 0.3385,
      "step": 1433
    },
    {
      "epoch": 0.42176470588235293,
      "grad_norm": 0.04629557952284813,
      "learning_rate": 0.00011581737849779088,
      "loss": 0.3375,
      "step": 1434
    },
    {
      "epoch": 0.42205882352941176,
      "grad_norm": 0.0548151396214962,
      "learning_rate": 0.00011575846833578794,
      "loss": 0.3856,
      "step": 1435
    },
    {
      "epoch": 0.4223529411764706,
      "grad_norm": 0.05201254412531853,
      "learning_rate": 0.00011569955817378499,
      "loss": 0.3599,
      "step": 1436
    },
    {
      "epoch": 0.42264705882352943,
      "grad_norm": 0.049818918108940125,
      "learning_rate": 0.00011564064801178205,
      "loss": 0.2902,
      "step": 1437
    },
    {
      "epoch": 0.4229411764705882,
      "grad_norm": 0.04683271795511246,
      "learning_rate": 0.00011558173784977908,
      "loss": 0.3647,
      "step": 1438
    },
    {
      "epoch": 0.42323529411764704,
      "grad_norm": 0.04776962101459503,
      "learning_rate": 0.00011552282768777614,
      "loss": 0.3513,
      "step": 1439
    },
    {
      "epoch": 0.4235294117647059,
      "grad_norm": 0.04836948588490486,
      "learning_rate": 0.00011546391752577319,
      "loss": 0.3291,
      "step": 1440
    },
    {
      "epoch": 0.4238235294117647,
      "grad_norm": 0.04423702135682106,
      "learning_rate": 0.00011540500736377025,
      "loss": 0.358,
      "step": 1441
    },
    {
      "epoch": 0.42411764705882354,
      "grad_norm": 0.03998124971985817,
      "learning_rate": 0.0001153460972017673,
      "loss": 0.2979,
      "step": 1442
    },
    {
      "epoch": 0.4244117647058824,
      "grad_norm": 0.0550394281744957,
      "learning_rate": 0.00011528718703976436,
      "loss": 0.4217,
      "step": 1443
    },
    {
      "epoch": 0.42470588235294116,
      "grad_norm": 0.05399560183286667,
      "learning_rate": 0.00011522827687776142,
      "loss": 0.3838,
      "step": 1444
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.0430891215801239,
      "learning_rate": 0.00011516936671575846,
      "loss": 0.3429,
      "step": 1445
    },
    {
      "epoch": 0.4252941176470588,
      "grad_norm": 0.035114020109176636,
      "learning_rate": 0.00011511045655375552,
      "loss": 0.274,
      "step": 1446
    },
    {
      "epoch": 0.42558823529411766,
      "grad_norm": 0.04181864485144615,
      "learning_rate": 0.00011505154639175257,
      "loss": 0.2732,
      "step": 1447
    },
    {
      "epoch": 0.4258823529411765,
      "grad_norm": 0.03943372517824173,
      "learning_rate": 0.00011499263622974963,
      "loss": 0.3427,
      "step": 1448
    },
    {
      "epoch": 0.42617647058823527,
      "grad_norm": 0.041472163051366806,
      "learning_rate": 0.00011493372606774669,
      "loss": 0.3155,
      "step": 1449
    },
    {
      "epoch": 0.4264705882352941,
      "grad_norm": 0.05455680191516876,
      "learning_rate": 0.00011487481590574374,
      "loss": 0.3761,
      "step": 1450
    },
    {
      "epoch": 0.42676470588235293,
      "grad_norm": 0.03510666266083717,
      "learning_rate": 0.0001148159057437408,
      "loss": 0.2656,
      "step": 1451
    },
    {
      "epoch": 0.42705882352941177,
      "grad_norm": 0.0478341169655323,
      "learning_rate": 0.00011475699558173784,
      "loss": 0.3046,
      "step": 1452
    },
    {
      "epoch": 0.4273529411764706,
      "grad_norm": 0.05802810192108154,
      "learning_rate": 0.0001146980854197349,
      "loss": 0.3269,
      "step": 1453
    },
    {
      "epoch": 0.42764705882352944,
      "grad_norm": 0.0640670657157898,
      "learning_rate": 0.00011463917525773196,
      "loss": 0.3757,
      "step": 1454
    },
    {
      "epoch": 0.4279411764705882,
      "grad_norm": 0.05871502682566643,
      "learning_rate": 0.00011458026509572901,
      "loss": 0.4113,
      "step": 1455
    },
    {
      "epoch": 0.42823529411764705,
      "grad_norm": 0.0450851172208786,
      "learning_rate": 0.00011452135493372607,
      "loss": 0.3557,
      "step": 1456
    },
    {
      "epoch": 0.4285294117647059,
      "grad_norm": 0.04450637102127075,
      "learning_rate": 0.00011446244477172312,
      "loss": 0.303,
      "step": 1457
    },
    {
      "epoch": 0.4288235294117647,
      "grad_norm": 0.06504804641008377,
      "learning_rate": 0.00011440353460972018,
      "loss": 0.3945,
      "step": 1458
    },
    {
      "epoch": 0.42911764705882355,
      "grad_norm": 0.05218217894434929,
      "learning_rate": 0.00011434462444771724,
      "loss": 0.3469,
      "step": 1459
    },
    {
      "epoch": 0.4294117647058823,
      "grad_norm": 0.05035050958395004,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.3679,
      "step": 1460
    },
    {
      "epoch": 0.42970588235294116,
      "grad_norm": 0.050837110728025436,
      "learning_rate": 0.00011422680412371134,
      "loss": 0.4008,
      "step": 1461
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.0503595694899559,
      "learning_rate": 0.00011416789396170839,
      "loss": 0.3999,
      "step": 1462
    },
    {
      "epoch": 0.4302941176470588,
      "grad_norm": 0.05921599268913269,
      "learning_rate": 0.00011410898379970545,
      "loss": 0.389,
      "step": 1463
    },
    {
      "epoch": 0.43058823529411766,
      "grad_norm": 0.05030768737196922,
      "learning_rate": 0.0001140500736377025,
      "loss": 0.3489,
      "step": 1464
    },
    {
      "epoch": 0.4308823529411765,
      "grad_norm": 0.04163459688425064,
      "learning_rate": 0.00011399116347569956,
      "loss": 0.2671,
      "step": 1465
    },
    {
      "epoch": 0.43117647058823527,
      "grad_norm": 0.038342759013175964,
      "learning_rate": 0.00011393225331369662,
      "loss": 0.3334,
      "step": 1466
    },
    {
      "epoch": 0.4314705882352941,
      "grad_norm": 0.04202922061085701,
      "learning_rate": 0.00011387334315169366,
      "loss": 0.3569,
      "step": 1467
    },
    {
      "epoch": 0.43176470588235294,
      "grad_norm": 0.04468262940645218,
      "learning_rate": 0.00011381443298969072,
      "loss": 0.3695,
      "step": 1468
    },
    {
      "epoch": 0.4320588235294118,
      "grad_norm": 0.03938781097531319,
      "learning_rate": 0.00011375552282768777,
      "loss": 0.2995,
      "step": 1469
    },
    {
      "epoch": 0.4323529411764706,
      "grad_norm": 0.04766286909580231,
      "learning_rate": 0.00011369661266568483,
      "loss": 0.3381,
      "step": 1470
    },
    {
      "epoch": 0.4326470588235294,
      "grad_norm": 0.05229419469833374,
      "learning_rate": 0.00011363770250368189,
      "loss": 0.4105,
      "step": 1471
    },
    {
      "epoch": 0.4329411764705882,
      "grad_norm": 0.04983612149953842,
      "learning_rate": 0.00011357879234167894,
      "loss": 0.3589,
      "step": 1472
    },
    {
      "epoch": 0.43323529411764705,
      "grad_norm": 0.043541260063648224,
      "learning_rate": 0.000113519882179676,
      "loss": 0.2984,
      "step": 1473
    },
    {
      "epoch": 0.4335294117647059,
      "grad_norm": 0.0470004603266716,
      "learning_rate": 0.00011346097201767304,
      "loss": 0.3411,
      "step": 1474
    },
    {
      "epoch": 0.4338235294117647,
      "grad_norm": 0.0457620844244957,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.3593,
      "step": 1475
    },
    {
      "epoch": 0.43411764705882355,
      "grad_norm": 0.04757905378937721,
      "learning_rate": 0.00011334315169366716,
      "loss": 0.3644,
      "step": 1476
    },
    {
      "epoch": 0.43441176470588233,
      "grad_norm": 0.05284321680665016,
      "learning_rate": 0.00011328424153166421,
      "loss": 0.3303,
      "step": 1477
    },
    {
      "epoch": 0.43470588235294116,
      "grad_norm": 0.04926764592528343,
      "learning_rate": 0.00011322533136966127,
      "loss": 0.3788,
      "step": 1478
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.039164453744888306,
      "learning_rate": 0.00011316642120765832,
      "loss": 0.2723,
      "step": 1479
    },
    {
      "epoch": 0.43529411764705883,
      "grad_norm": 0.04571625217795372,
      "learning_rate": 0.00011310751104565538,
      "loss": 0.3715,
      "step": 1480
    },
    {
      "epoch": 0.43558823529411766,
      "grad_norm": 0.04187869653105736,
      "learning_rate": 0.00011304860088365244,
      "loss": 0.3268,
      "step": 1481
    },
    {
      "epoch": 0.43588235294117644,
      "grad_norm": 0.04321970045566559,
      "learning_rate": 0.00011298969072164949,
      "loss": 0.3474,
      "step": 1482
    },
    {
      "epoch": 0.4361764705882353,
      "grad_norm": 0.04268364608287811,
      "learning_rate": 0.00011293078055964655,
      "loss": 0.3419,
      "step": 1483
    },
    {
      "epoch": 0.4364705882352941,
      "grad_norm": 0.038879282772541046,
      "learning_rate": 0.00011287187039764359,
      "loss": 0.3315,
      "step": 1484
    },
    {
      "epoch": 0.43676470588235294,
      "grad_norm": 0.05065784975886345,
      "learning_rate": 0.00011281296023564065,
      "loss": 0.3524,
      "step": 1485
    },
    {
      "epoch": 0.4370588235294118,
      "grad_norm": 0.046091243624687195,
      "learning_rate": 0.00011275405007363771,
      "loss": 0.4012,
      "step": 1486
    },
    {
      "epoch": 0.4373529411764706,
      "grad_norm": 0.06165005639195442,
      "learning_rate": 0.00011269513991163476,
      "loss": 0.4577,
      "step": 1487
    },
    {
      "epoch": 0.4376470588235294,
      "grad_norm": 0.04834842309355736,
      "learning_rate": 0.00011263622974963182,
      "loss": 0.3805,
      "step": 1488
    },
    {
      "epoch": 0.4379411764705882,
      "grad_norm": 0.04201754182577133,
      "learning_rate": 0.00011257731958762887,
      "loss": 0.3225,
      "step": 1489
    },
    {
      "epoch": 0.43823529411764706,
      "grad_norm": 0.05125832557678223,
      "learning_rate": 0.00011251840942562593,
      "loss": 0.3816,
      "step": 1490
    },
    {
      "epoch": 0.4385294117647059,
      "grad_norm": 0.048723846673965454,
      "learning_rate": 0.00011245949926362299,
      "loss": 0.3476,
      "step": 1491
    },
    {
      "epoch": 0.4388235294117647,
      "grad_norm": 0.04789407551288605,
      "learning_rate": 0.00011240058910162003,
      "loss": 0.375,
      "step": 1492
    },
    {
      "epoch": 0.43911764705882356,
      "grad_norm": 0.058249033987522125,
      "learning_rate": 0.00011234167893961709,
      "loss": 0.3821,
      "step": 1493
    },
    {
      "epoch": 0.43941176470588234,
      "grad_norm": 0.03490755334496498,
      "learning_rate": 0.00011228276877761414,
      "loss": 0.3257,
      "step": 1494
    },
    {
      "epoch": 0.43970588235294117,
      "grad_norm": 0.04989989846944809,
      "learning_rate": 0.0001122238586156112,
      "loss": 0.4336,
      "step": 1495
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.04563220962882042,
      "learning_rate": 0.00011216494845360826,
      "loss": 0.3639,
      "step": 1496
    },
    {
      "epoch": 0.44029411764705884,
      "grad_norm": 0.0425448939204216,
      "learning_rate": 0.0001121060382916053,
      "loss": 0.291,
      "step": 1497
    },
    {
      "epoch": 0.44058823529411767,
      "grad_norm": 0.05256984010338783,
      "learning_rate": 0.00011204712812960237,
      "loss": 0.3574,
      "step": 1498
    },
    {
      "epoch": 0.44088235294117645,
      "grad_norm": 0.04514993354678154,
      "learning_rate": 0.00011198821796759941,
      "loss": 0.354,
      "step": 1499
    },
    {
      "epoch": 0.4411764705882353,
      "grad_norm": 0.04611453041434288,
      "learning_rate": 0.00011192930780559647,
      "loss": 0.3127,
      "step": 1500
    },
    {
      "epoch": 0.4414705882352941,
      "grad_norm": 0.04996565356850624,
      "learning_rate": 0.00011187039764359353,
      "loss": 0.3416,
      "step": 1501
    },
    {
      "epoch": 0.44176470588235295,
      "grad_norm": 0.0396297387778759,
      "learning_rate": 0.00011181148748159058,
      "loss": 0.2972,
      "step": 1502
    },
    {
      "epoch": 0.4420588235294118,
      "grad_norm": 0.05946362018585205,
      "learning_rate": 0.00011175257731958764,
      "loss": 0.3408,
      "step": 1503
    },
    {
      "epoch": 0.4423529411764706,
      "grad_norm": 0.04681534320116043,
      "learning_rate": 0.00011169366715758469,
      "loss": 0.3626,
      "step": 1504
    },
    {
      "epoch": 0.4426470588235294,
      "grad_norm": 0.04412347823381424,
      "learning_rate": 0.00011163475699558175,
      "loss": 0.3661,
      "step": 1505
    },
    {
      "epoch": 0.4429411764705882,
      "grad_norm": 0.04627593234181404,
      "learning_rate": 0.00011157584683357881,
      "loss": 0.3047,
      "step": 1506
    },
    {
      "epoch": 0.44323529411764706,
      "grad_norm": 0.04517799988389015,
      "learning_rate": 0.00011151693667157585,
      "loss": 0.355,
      "step": 1507
    },
    {
      "epoch": 0.4435294117647059,
      "grad_norm": 0.0797954872250557,
      "learning_rate": 0.00011145802650957291,
      "loss": 0.3821,
      "step": 1508
    },
    {
      "epoch": 0.44382352941176473,
      "grad_norm": 0.03964851424098015,
      "learning_rate": 0.00011139911634756996,
      "loss": 0.3286,
      "step": 1509
    },
    {
      "epoch": 0.4441176470588235,
      "grad_norm": 0.05145232006907463,
      "learning_rate": 0.00011134020618556702,
      "loss": 0.3449,
      "step": 1510
    },
    {
      "epoch": 0.44441176470588234,
      "grad_norm": 0.03949221596121788,
      "learning_rate": 0.00011128129602356408,
      "loss": 0.3225,
      "step": 1511
    },
    {
      "epoch": 0.4447058823529412,
      "grad_norm": 0.050057414919137955,
      "learning_rate": 0.00011122238586156113,
      "loss": 0.3753,
      "step": 1512
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.042849645018577576,
      "learning_rate": 0.00011116347569955819,
      "loss": 0.3113,
      "step": 1513
    },
    {
      "epoch": 0.44529411764705884,
      "grad_norm": 0.05185086652636528,
      "learning_rate": 0.00011110456553755523,
      "loss": 0.4107,
      "step": 1514
    },
    {
      "epoch": 0.4455882352941177,
      "grad_norm": 0.0417131744325161,
      "learning_rate": 0.0001110456553755523,
      "loss": 0.3366,
      "step": 1515
    },
    {
      "epoch": 0.44588235294117645,
      "grad_norm": 0.047112494707107544,
      "learning_rate": 0.00011098674521354935,
      "loss": 0.3212,
      "step": 1516
    },
    {
      "epoch": 0.4461764705882353,
      "grad_norm": 0.05989829823374748,
      "learning_rate": 0.0001109278350515464,
      "loss": 0.3868,
      "step": 1517
    },
    {
      "epoch": 0.4464705882352941,
      "grad_norm": 0.0547897070646286,
      "learning_rate": 0.00011086892488954346,
      "loss": 0.3546,
      "step": 1518
    },
    {
      "epoch": 0.44676470588235295,
      "grad_norm": 0.040732551366090775,
      "learning_rate": 0.00011081001472754051,
      "loss": 0.3202,
      "step": 1519
    },
    {
      "epoch": 0.4470588235294118,
      "grad_norm": 0.05577116459608078,
      "learning_rate": 0.00011075110456553757,
      "loss": 0.3714,
      "step": 1520
    },
    {
      "epoch": 0.44735294117647056,
      "grad_norm": 0.06133227422833443,
      "learning_rate": 0.00011069219440353461,
      "loss": 0.4011,
      "step": 1521
    },
    {
      "epoch": 0.4476470588235294,
      "grad_norm": 0.05878865718841553,
      "learning_rate": 0.00011063328424153167,
      "loss": 0.3646,
      "step": 1522
    },
    {
      "epoch": 0.44794117647058823,
      "grad_norm": 0.040557611733675,
      "learning_rate": 0.00011057437407952874,
      "loss": 0.4106,
      "step": 1523
    },
    {
      "epoch": 0.44823529411764707,
      "grad_norm": 0.03936440497636795,
      "learning_rate": 0.00011051546391752578,
      "loss": 0.3244,
      "step": 1524
    },
    {
      "epoch": 0.4485294117647059,
      "grad_norm": 0.05551707744598389,
      "learning_rate": 0.00011045655375552284,
      "loss": 0.4082,
      "step": 1525
    },
    {
      "epoch": 0.44882352941176473,
      "grad_norm": 0.06387176364660263,
      "learning_rate": 0.00011039764359351989,
      "loss": 0.3444,
      "step": 1526
    },
    {
      "epoch": 0.4491176470588235,
      "grad_norm": 0.06836609542369843,
      "learning_rate": 0.00011033873343151695,
      "loss": 0.4276,
      "step": 1527
    },
    {
      "epoch": 0.44941176470588234,
      "grad_norm": 0.05119181424379349,
      "learning_rate": 0.00011027982326951401,
      "loss": 0.3486,
      "step": 1528
    },
    {
      "epoch": 0.4497058823529412,
      "grad_norm": 0.04968676343560219,
      "learning_rate": 0.00011022091310751106,
      "loss": 0.3978,
      "step": 1529
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.06076176464557648,
      "learning_rate": 0.00011016200294550812,
      "loss": 0.4234,
      "step": 1530
    },
    {
      "epoch": 0.45029411764705884,
      "grad_norm": 0.07032450288534164,
      "learning_rate": 0.00011010309278350516,
      "loss": 0.3637,
      "step": 1531
    },
    {
      "epoch": 0.4505882352941176,
      "grad_norm": 0.05875327065587044,
      "learning_rate": 0.00011004418262150222,
      "loss": 0.3602,
      "step": 1532
    },
    {
      "epoch": 0.45088235294117646,
      "grad_norm": 0.05390306934714317,
      "learning_rate": 0.00010998527245949928,
      "loss": 0.3559,
      "step": 1533
    },
    {
      "epoch": 0.4511764705882353,
      "grad_norm": 0.049704477190971375,
      "learning_rate": 0.00010992636229749633,
      "loss": 0.3434,
      "step": 1534
    },
    {
      "epoch": 0.4514705882352941,
      "grad_norm": 0.03768438100814819,
      "learning_rate": 0.00010986745213549339,
      "loss": 0.2522,
      "step": 1535
    },
    {
      "epoch": 0.45176470588235296,
      "grad_norm": 0.06631748378276825,
      "learning_rate": 0.00010980854197349044,
      "loss": 0.3777,
      "step": 1536
    },
    {
      "epoch": 0.4520588235294118,
      "grad_norm": 0.04501518979668617,
      "learning_rate": 0.0001097496318114875,
      "loss": 0.3346,
      "step": 1537
    },
    {
      "epoch": 0.45235294117647057,
      "grad_norm": 0.060075923800468445,
      "learning_rate": 0.00010969072164948456,
      "loss": 0.3668,
      "step": 1538
    },
    {
      "epoch": 0.4526470588235294,
      "grad_norm": 0.04932805895805359,
      "learning_rate": 0.0001096318114874816,
      "loss": 0.3259,
      "step": 1539
    },
    {
      "epoch": 0.45294117647058824,
      "grad_norm": 0.05072769522666931,
      "learning_rate": 0.00010957290132547866,
      "loss": 0.3384,
      "step": 1540
    },
    {
      "epoch": 0.45323529411764707,
      "grad_norm": 0.060001954436302185,
      "learning_rate": 0.00010951399116347571,
      "loss": 0.4018,
      "step": 1541
    },
    {
      "epoch": 0.4535294117647059,
      "grad_norm": 0.04950806871056557,
      "learning_rate": 0.00010945508100147277,
      "loss": 0.328,
      "step": 1542
    },
    {
      "epoch": 0.4538235294117647,
      "grad_norm": 0.04368221014738083,
      "learning_rate": 0.00010939617083946983,
      "loss": 0.3249,
      "step": 1543
    },
    {
      "epoch": 0.4541176470588235,
      "grad_norm": 0.044423338025808334,
      "learning_rate": 0.00010933726067746686,
      "loss": 0.2859,
      "step": 1544
    },
    {
      "epoch": 0.45441176470588235,
      "grad_norm": 0.057619359344244,
      "learning_rate": 0.00010927835051546391,
      "loss": 0.3582,
      "step": 1545
    },
    {
      "epoch": 0.4547058823529412,
      "grad_norm": 0.041177861392498016,
      "learning_rate": 0.00010921944035346097,
      "loss": 0.3315,
      "step": 1546
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.04666743054986,
      "learning_rate": 0.00010916053019145802,
      "loss": 0.3107,
      "step": 1547
    },
    {
      "epoch": 0.45529411764705885,
      "grad_norm": 0.03269438445568085,
      "learning_rate": 0.00010910162002945508,
      "loss": 0.2652,
      "step": 1548
    },
    {
      "epoch": 0.4555882352941176,
      "grad_norm": 0.044344011694192886,
      "learning_rate": 0.00010904270986745214,
      "loss": 0.3135,
      "step": 1549
    },
    {
      "epoch": 0.45588235294117646,
      "grad_norm": 0.03731999546289444,
      "learning_rate": 0.00010898379970544918,
      "loss": 0.3155,
      "step": 1550
    },
    {
      "epoch": 0.4561764705882353,
      "grad_norm": 0.04207676649093628,
      "learning_rate": 0.00010892488954344624,
      "loss": 0.3112,
      "step": 1551
    },
    {
      "epoch": 0.45647058823529413,
      "grad_norm": 0.054549384862184525,
      "learning_rate": 0.00010886597938144329,
      "loss": 0.339,
      "step": 1552
    },
    {
      "epoch": 0.45676470588235296,
      "grad_norm": 0.05396872013807297,
      "learning_rate": 0.00010880706921944035,
      "loss": 0.3637,
      "step": 1553
    },
    {
      "epoch": 0.45705882352941174,
      "grad_norm": 0.05206937715411186,
      "learning_rate": 0.00010874815905743741,
      "loss": 0.3742,
      "step": 1554
    },
    {
      "epoch": 0.4573529411764706,
      "grad_norm": 0.0385734923183918,
      "learning_rate": 0.00010868924889543446,
      "loss": 0.273,
      "step": 1555
    },
    {
      "epoch": 0.4576470588235294,
      "grad_norm": 0.04951557144522667,
      "learning_rate": 0.00010863033873343152,
      "loss": 0.3577,
      "step": 1556
    },
    {
      "epoch": 0.45794117647058824,
      "grad_norm": 0.03804982453584671,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.3132,
      "step": 1557
    },
    {
      "epoch": 0.4582352941176471,
      "grad_norm": 0.04393552988767624,
      "learning_rate": 0.00010851251840942562,
      "loss": 0.3539,
      "step": 1558
    },
    {
      "epoch": 0.4585294117647059,
      "grad_norm": 0.08167626708745956,
      "learning_rate": 0.00010845360824742268,
      "loss": 0.4455,
      "step": 1559
    },
    {
      "epoch": 0.4588235294117647,
      "grad_norm": 0.066561259329319,
      "learning_rate": 0.00010839469808541973,
      "loss": 0.4179,
      "step": 1560
    },
    {
      "epoch": 0.4591176470588235,
      "grad_norm": 0.06217210367321968,
      "learning_rate": 0.00010833578792341679,
      "loss": 0.3174,
      "step": 1561
    },
    {
      "epoch": 0.45941176470588235,
      "grad_norm": 0.0501830019056797,
      "learning_rate": 0.00010827687776141384,
      "loss": 0.3743,
      "step": 1562
    },
    {
      "epoch": 0.4597058823529412,
      "grad_norm": 0.034816205501556396,
      "learning_rate": 0.0001082179675994109,
      "loss": 0.2685,
      "step": 1563
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.049973201006650925,
      "learning_rate": 0.00010815905743740796,
      "loss": 0.3514,
      "step": 1564
    },
    {
      "epoch": 0.4602941176470588,
      "grad_norm": 0.05310817062854767,
      "learning_rate": 0.000108100147275405,
      "loss": 0.3218,
      "step": 1565
    },
    {
      "epoch": 0.46058823529411763,
      "grad_norm": 0.06790368258953094,
      "learning_rate": 0.00010804123711340206,
      "loss": 0.4033,
      "step": 1566
    },
    {
      "epoch": 0.46088235294117647,
      "grad_norm": 0.045013148337602615,
      "learning_rate": 0.00010798232695139911,
      "loss": 0.3073,
      "step": 1567
    },
    {
      "epoch": 0.4611764705882353,
      "grad_norm": 0.04164276272058487,
      "learning_rate": 0.00010792341678939617,
      "loss": 0.2708,
      "step": 1568
    },
    {
      "epoch": 0.46147058823529413,
      "grad_norm": 0.057754673063755035,
      "learning_rate": 0.00010786450662739322,
      "loss": 0.3833,
      "step": 1569
    },
    {
      "epoch": 0.46176470588235297,
      "grad_norm": 0.06374802440404892,
      "learning_rate": 0.00010780559646539028,
      "loss": 0.3947,
      "step": 1570
    },
    {
      "epoch": 0.46205882352941174,
      "grad_norm": 0.05687975510954857,
      "learning_rate": 0.00010774668630338734,
      "loss": 0.3128,
      "step": 1571
    },
    {
      "epoch": 0.4623529411764706,
      "grad_norm": 0.061306390911340714,
      "learning_rate": 0.00010768777614138439,
      "loss": 0.3978,
      "step": 1572
    },
    {
      "epoch": 0.4626470588235294,
      "grad_norm": 0.0564378947019577,
      "learning_rate": 0.00010762886597938145,
      "loss": 0.3885,
      "step": 1573
    },
    {
      "epoch": 0.46294117647058824,
      "grad_norm": 0.04113654047250748,
      "learning_rate": 0.00010756995581737849,
      "loss": 0.3452,
      "step": 1574
    },
    {
      "epoch": 0.4632352941176471,
      "grad_norm": 0.04063607379794121,
      "learning_rate": 0.00010751104565537555,
      "loss": 0.3229,
      "step": 1575
    },
    {
      "epoch": 0.46352941176470586,
      "grad_norm": 0.03111562877893448,
      "learning_rate": 0.00010745213549337261,
      "loss": 0.2592,
      "step": 1576
    },
    {
      "epoch": 0.4638235294117647,
      "grad_norm": 0.05138027295470238,
      "learning_rate": 0.00010739322533136966,
      "loss": 0.3993,
      "step": 1577
    },
    {
      "epoch": 0.4641176470588235,
      "grad_norm": 0.045711223036050797,
      "learning_rate": 0.00010733431516936672,
      "loss": 0.3289,
      "step": 1578
    },
    {
      "epoch": 0.46441176470588236,
      "grad_norm": 0.04100920259952545,
      "learning_rate": 0.00010727540500736377,
      "loss": 0.3317,
      "step": 1579
    },
    {
      "epoch": 0.4647058823529412,
      "grad_norm": 0.0554165281355381,
      "learning_rate": 0.00010721649484536083,
      "loss": 0.4069,
      "step": 1580
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.04923376068472862,
      "learning_rate": 0.00010715758468335789,
      "loss": 0.3721,
      "step": 1581
    },
    {
      "epoch": 0.4652941176470588,
      "grad_norm": 0.04735457897186279,
      "learning_rate": 0.00010709867452135493,
      "loss": 0.3695,
      "step": 1582
    },
    {
      "epoch": 0.46558823529411764,
      "grad_norm": 0.05385046824812889,
      "learning_rate": 0.00010703976435935199,
      "loss": 0.3565,
      "step": 1583
    },
    {
      "epoch": 0.46588235294117647,
      "grad_norm": 0.0658460259437561,
      "learning_rate": 0.00010698085419734904,
      "loss": 0.3787,
      "step": 1584
    },
    {
      "epoch": 0.4661764705882353,
      "grad_norm": 0.05313729867339134,
      "learning_rate": 0.0001069219440353461,
      "loss": 0.3463,
      "step": 1585
    },
    {
      "epoch": 0.46647058823529414,
      "grad_norm": 0.05310601741075516,
      "learning_rate": 0.00010686303387334316,
      "loss": 0.4157,
      "step": 1586
    },
    {
      "epoch": 0.4667647058823529,
      "grad_norm": 0.07327543944120407,
      "learning_rate": 0.0001068041237113402,
      "loss": 0.4518,
      "step": 1587
    },
    {
      "epoch": 0.46705882352941175,
      "grad_norm": 0.04460148885846138,
      "learning_rate": 0.00010674521354933727,
      "loss": 0.3187,
      "step": 1588
    },
    {
      "epoch": 0.4673529411764706,
      "grad_norm": 0.04321657493710518,
      "learning_rate": 0.00010668630338733431,
      "loss": 0.2724,
      "step": 1589
    },
    {
      "epoch": 0.4676470588235294,
      "grad_norm": 0.0513748824596405,
      "learning_rate": 0.00010662739322533137,
      "loss": 0.3782,
      "step": 1590
    },
    {
      "epoch": 0.46794117647058825,
      "grad_norm": 0.04104898124933243,
      "learning_rate": 0.00010656848306332843,
      "loss": 0.2716,
      "step": 1591
    },
    {
      "epoch": 0.4682352941176471,
      "grad_norm": 0.04468146711587906,
      "learning_rate": 0.00010650957290132548,
      "loss": 0.3336,
      "step": 1592
    },
    {
      "epoch": 0.46852941176470586,
      "grad_norm": 0.04445319622755051,
      "learning_rate": 0.00010645066273932254,
      "loss": 0.3455,
      "step": 1593
    },
    {
      "epoch": 0.4688235294117647,
      "grad_norm": 0.046247731894254684,
      "learning_rate": 0.00010639175257731959,
      "loss": 0.344,
      "step": 1594
    },
    {
      "epoch": 0.46911764705882353,
      "grad_norm": 0.055153872817754745,
      "learning_rate": 0.00010633284241531665,
      "loss": 0.352,
      "step": 1595
    },
    {
      "epoch": 0.46941176470588236,
      "grad_norm": 0.06185879558324814,
      "learning_rate": 0.00010627393225331371,
      "loss": 0.3772,
      "step": 1596
    },
    {
      "epoch": 0.4697058823529412,
      "grad_norm": 0.037161439657211304,
      "learning_rate": 0.00010621502209131075,
      "loss": 0.3167,
      "step": 1597
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05698985233902931,
      "learning_rate": 0.00010615611192930781,
      "loss": 0.2988,
      "step": 1598
    },
    {
      "epoch": 0.4702941176470588,
      "grad_norm": 0.05649687722325325,
      "learning_rate": 0.00010609720176730486,
      "loss": 0.3669,
      "step": 1599
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.04116698354482651,
      "learning_rate": 0.00010603829160530192,
      "loss": 0.3329,
      "step": 1600
    },
    {
      "epoch": 0.4708823529411765,
      "grad_norm": 0.04815853759646416,
      "learning_rate": 0.00010597938144329898,
      "loss": 0.2632,
      "step": 1601
    },
    {
      "epoch": 0.4711764705882353,
      "grad_norm": 0.046050313860177994,
      "learning_rate": 0.00010592047128129603,
      "loss": 0.3412,
      "step": 1602
    },
    {
      "epoch": 0.47147058823529414,
      "grad_norm": 0.0577034130692482,
      "learning_rate": 0.00010586156111929309,
      "loss": 0.4092,
      "step": 1603
    },
    {
      "epoch": 0.4717647058823529,
      "grad_norm": 0.047326162457466125,
      "learning_rate": 0.00010580265095729013,
      "loss": 0.3622,
      "step": 1604
    },
    {
      "epoch": 0.47205882352941175,
      "grad_norm": 0.0484573133289814,
      "learning_rate": 0.0001057437407952872,
      "loss": 0.3924,
      "step": 1605
    },
    {
      "epoch": 0.4723529411764706,
      "grad_norm": 0.04488509148359299,
      "learning_rate": 0.00010568483063328425,
      "loss": 0.3252,
      "step": 1606
    },
    {
      "epoch": 0.4726470588235294,
      "grad_norm": 0.05537515878677368,
      "learning_rate": 0.0001056259204712813,
      "loss": 0.3995,
      "step": 1607
    },
    {
      "epoch": 0.47294117647058825,
      "grad_norm": 0.039306167513132095,
      "learning_rate": 0.00010556701030927836,
      "loss": 0.3468,
      "step": 1608
    },
    {
      "epoch": 0.47323529411764703,
      "grad_norm": 0.04201760143041611,
      "learning_rate": 0.00010550810014727541,
      "loss": 0.3216,
      "step": 1609
    },
    {
      "epoch": 0.47352941176470587,
      "grad_norm": 0.049283742904663086,
      "learning_rate": 0.00010544918998527247,
      "loss": 0.3066,
      "step": 1610
    },
    {
      "epoch": 0.4738235294117647,
      "grad_norm": 0.05364955589175224,
      "learning_rate": 0.00010539027982326953,
      "loss": 0.4098,
      "step": 1611
    },
    {
      "epoch": 0.47411764705882353,
      "grad_norm": 0.054694660007953644,
      "learning_rate": 0.00010533136966126657,
      "loss": 0.3485,
      "step": 1612
    },
    {
      "epoch": 0.47441176470588237,
      "grad_norm": 0.054295122623443604,
      "learning_rate": 0.00010527245949926363,
      "loss": 0.383,
      "step": 1613
    },
    {
      "epoch": 0.4747058823529412,
      "grad_norm": 0.043671876192092896,
      "learning_rate": 0.00010521354933726068,
      "loss": 0.3126,
      "step": 1614
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.04859592393040657,
      "learning_rate": 0.00010515463917525774,
      "loss": 0.344,
      "step": 1615
    },
    {
      "epoch": 0.4752941176470588,
      "grad_norm": 0.05386274680495262,
      "learning_rate": 0.0001050957290132548,
      "loss": 0.3402,
      "step": 1616
    },
    {
      "epoch": 0.47558823529411764,
      "grad_norm": 0.05467946082353592,
      "learning_rate": 0.00010503681885125185,
      "loss": 0.372,
      "step": 1617
    },
    {
      "epoch": 0.4758823529411765,
      "grad_norm": 0.0416824147105217,
      "learning_rate": 0.00010497790868924891,
      "loss": 0.3271,
      "step": 1618
    },
    {
      "epoch": 0.4761764705882353,
      "grad_norm": 0.04188081994652748,
      "learning_rate": 0.00010491899852724596,
      "loss": 0.3026,
      "step": 1619
    },
    {
      "epoch": 0.4764705882352941,
      "grad_norm": 0.04249352589249611,
      "learning_rate": 0.00010486008836524302,
      "loss": 0.3428,
      "step": 1620
    },
    {
      "epoch": 0.4767647058823529,
      "grad_norm": 0.060749731957912445,
      "learning_rate": 0.00010480117820324008,
      "loss": 0.3879,
      "step": 1621
    },
    {
      "epoch": 0.47705882352941176,
      "grad_norm": 0.061519522219896317,
      "learning_rate": 0.00010474226804123712,
      "loss": 0.3397,
      "step": 1622
    },
    {
      "epoch": 0.4773529411764706,
      "grad_norm": 0.03687308728694916,
      "learning_rate": 0.00010468335787923418,
      "loss": 0.3298,
      "step": 1623
    },
    {
      "epoch": 0.4776470588235294,
      "grad_norm": 0.05751847103238106,
      "learning_rate": 0.00010462444771723123,
      "loss": 0.365,
      "step": 1624
    },
    {
      "epoch": 0.47794117647058826,
      "grad_norm": 0.04388488829135895,
      "learning_rate": 0.00010456553755522829,
      "loss": 0.3395,
      "step": 1625
    },
    {
      "epoch": 0.47823529411764704,
      "grad_norm": 0.05367289111018181,
      "learning_rate": 0.00010450662739322534,
      "loss": 0.378,
      "step": 1626
    },
    {
      "epoch": 0.47852941176470587,
      "grad_norm": 0.04846613109111786,
      "learning_rate": 0.0001044477172312224,
      "loss": 0.3774,
      "step": 1627
    },
    {
      "epoch": 0.4788235294117647,
      "grad_norm": 0.05722597986459732,
      "learning_rate": 0.00010438880706921946,
      "loss": 0.3417,
      "step": 1628
    },
    {
      "epoch": 0.47911764705882354,
      "grad_norm": 0.04576248303055763,
      "learning_rate": 0.0001043298969072165,
      "loss": 0.3074,
      "step": 1629
    },
    {
      "epoch": 0.47941176470588237,
      "grad_norm": 0.034391649067401886,
      "learning_rate": 0.00010427098674521356,
      "loss": 0.2601,
      "step": 1630
    },
    {
      "epoch": 0.4797058823529412,
      "grad_norm": 0.043281491845846176,
      "learning_rate": 0.00010421207658321061,
      "loss": 0.2928,
      "step": 1631
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.04104922339320183,
      "learning_rate": 0.00010415316642120767,
      "loss": 0.3389,
      "step": 1632
    },
    {
      "epoch": 0.4802941176470588,
      "grad_norm": 0.058150529861450195,
      "learning_rate": 0.00010409425625920473,
      "loss": 0.3681,
      "step": 1633
    },
    {
      "epoch": 0.48058823529411765,
      "grad_norm": 0.05129276216030121,
      "learning_rate": 0.00010403534609720178,
      "loss": 0.4018,
      "step": 1634
    },
    {
      "epoch": 0.4808823529411765,
      "grad_norm": 0.03901097550988197,
      "learning_rate": 0.00010397643593519884,
      "loss": 0.3053,
      "step": 1635
    },
    {
      "epoch": 0.4811764705882353,
      "grad_norm": 0.04767318069934845,
      "learning_rate": 0.00010391752577319588,
      "loss": 0.3482,
      "step": 1636
    },
    {
      "epoch": 0.4814705882352941,
      "grad_norm": 0.03906404227018356,
      "learning_rate": 0.00010385861561119294,
      "loss": 0.2932,
      "step": 1637
    },
    {
      "epoch": 0.48176470588235293,
      "grad_norm": 0.043772608041763306,
      "learning_rate": 0.00010379970544919,
      "loss": 0.2893,
      "step": 1638
    },
    {
      "epoch": 0.48205882352941176,
      "grad_norm": 0.050422027707099915,
      "learning_rate": 0.00010374079528718705,
      "loss": 0.3334,
      "step": 1639
    },
    {
      "epoch": 0.4823529411764706,
      "grad_norm": 0.05499884486198425,
      "learning_rate": 0.00010368188512518411,
      "loss": 0.3936,
      "step": 1640
    },
    {
      "epoch": 0.48264705882352943,
      "grad_norm": 0.05134790763258934,
      "learning_rate": 0.00010362297496318116,
      "loss": 0.3437,
      "step": 1641
    },
    {
      "epoch": 0.48294117647058826,
      "grad_norm": 0.03839237615466118,
      "learning_rate": 0.00010356406480117822,
      "loss": 0.3472,
      "step": 1642
    },
    {
      "epoch": 0.48323529411764704,
      "grad_norm": 0.05395989120006561,
      "learning_rate": 0.00010350515463917528,
      "loss": 0.3526,
      "step": 1643
    },
    {
      "epoch": 0.4835294117647059,
      "grad_norm": 0.040760960429906845,
      "learning_rate": 0.00010344624447717232,
      "loss": 0.2818,
      "step": 1644
    },
    {
      "epoch": 0.4838235294117647,
      "grad_norm": 0.05515395104885101,
      "learning_rate": 0.00010338733431516938,
      "loss": 0.4233,
      "step": 1645
    },
    {
      "epoch": 0.48411764705882354,
      "grad_norm": 0.048836689442396164,
      "learning_rate": 0.00010332842415316643,
      "loss": 0.3019,
      "step": 1646
    },
    {
      "epoch": 0.4844117647058824,
      "grad_norm": 0.04313739389181137,
      "learning_rate": 0.00010326951399116349,
      "loss": 0.3171,
      "step": 1647
    },
    {
      "epoch": 0.48470588235294115,
      "grad_norm": 0.04980864375829697,
      "learning_rate": 0.00010321060382916055,
      "loss": 0.3917,
      "step": 1648
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.050440553575754166,
      "learning_rate": 0.0001031516936671576,
      "loss": 0.3241,
      "step": 1649
    },
    {
      "epoch": 0.4852941176470588,
      "grad_norm": 0.05427825078368187,
      "learning_rate": 0.00010309278350515463,
      "loss": 0.3317,
      "step": 1650
    },
    {
      "epoch": 0.48558823529411765,
      "grad_norm": 0.05027943104505539,
      "learning_rate": 0.00010303387334315169,
      "loss": 0.3793,
      "step": 1651
    },
    {
      "epoch": 0.4858823529411765,
      "grad_norm": 0.043264325708150864,
      "learning_rate": 0.00010297496318114874,
      "loss": 0.3448,
      "step": 1652
    },
    {
      "epoch": 0.4861764705882353,
      "grad_norm": 0.049301717430353165,
      "learning_rate": 0.0001029160530191458,
      "loss": 0.3599,
      "step": 1653
    },
    {
      "epoch": 0.4864705882352941,
      "grad_norm": 0.04631444811820984,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.3075,
      "step": 1654
    },
    {
      "epoch": 0.48676470588235293,
      "grad_norm": 0.05508012697100639,
      "learning_rate": 0.0001027982326951399,
      "loss": 0.4279,
      "step": 1655
    },
    {
      "epoch": 0.48705882352941177,
      "grad_norm": 0.035190921276807785,
      "learning_rate": 0.00010273932253313696,
      "loss": 0.312,
      "step": 1656
    },
    {
      "epoch": 0.4873529411764706,
      "grad_norm": 0.0549871027469635,
      "learning_rate": 0.00010268041237113401,
      "loss": 0.3851,
      "step": 1657
    },
    {
      "epoch": 0.48764705882352943,
      "grad_norm": 0.04408010467886925,
      "learning_rate": 0.00010262150220913107,
      "loss": 0.3596,
      "step": 1658
    },
    {
      "epoch": 0.4879411764705882,
      "grad_norm": 0.05020835995674133,
      "learning_rate": 0.00010256259204712813,
      "loss": 0.3498,
      "step": 1659
    },
    {
      "epoch": 0.48823529411764705,
      "grad_norm": 0.04549717530608177,
      "learning_rate": 0.00010250368188512518,
      "loss": 0.3721,
      "step": 1660
    },
    {
      "epoch": 0.4885294117647059,
      "grad_norm": 0.05551078915596008,
      "learning_rate": 0.00010244477172312224,
      "loss": 0.4319,
      "step": 1661
    },
    {
      "epoch": 0.4888235294117647,
      "grad_norm": 0.04931827634572983,
      "learning_rate": 0.00010238586156111928,
      "loss": 0.2932,
      "step": 1662
    },
    {
      "epoch": 0.48911764705882355,
      "grad_norm": 0.04819115623831749,
      "learning_rate": 0.00010232695139911635,
      "loss": 0.3568,
      "step": 1663
    },
    {
      "epoch": 0.4894117647058824,
      "grad_norm": 0.05260131508111954,
      "learning_rate": 0.0001022680412371134,
      "loss": 0.3467,
      "step": 1664
    },
    {
      "epoch": 0.48970588235294116,
      "grad_norm": 0.05996706336736679,
      "learning_rate": 0.00010220913107511045,
      "loss": 0.3795,
      "step": 1665
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.0503837950527668,
      "learning_rate": 0.00010215022091310751,
      "loss": 0.338,
      "step": 1666
    },
    {
      "epoch": 0.4902941176470588,
      "grad_norm": 0.0438215509057045,
      "learning_rate": 0.00010209131075110456,
      "loss": 0.3595,
      "step": 1667
    },
    {
      "epoch": 0.49058823529411766,
      "grad_norm": 0.05210927128791809,
      "learning_rate": 0.00010203240058910162,
      "loss": 0.3734,
      "step": 1668
    },
    {
      "epoch": 0.4908823529411765,
      "grad_norm": 0.04223896563053131,
      "learning_rate": 0.00010197349042709868,
      "loss": 0.2933,
      "step": 1669
    },
    {
      "epoch": 0.49117647058823527,
      "grad_norm": 0.049610719084739685,
      "learning_rate": 0.00010191458026509573,
      "loss": 0.4069,
      "step": 1670
    },
    {
      "epoch": 0.4914705882352941,
      "grad_norm": 0.04502226039767265,
      "learning_rate": 0.00010185567010309279,
      "loss": 0.3204,
      "step": 1671
    },
    {
      "epoch": 0.49176470588235294,
      "grad_norm": 0.052173908799886703,
      "learning_rate": 0.00010179675994108983,
      "loss": 0.3264,
      "step": 1672
    },
    {
      "epoch": 0.49205882352941177,
      "grad_norm": 0.05802050605416298,
      "learning_rate": 0.00010173784977908689,
      "loss": 0.3826,
      "step": 1673
    },
    {
      "epoch": 0.4923529411764706,
      "grad_norm": 0.04176749661564827,
      "learning_rate": 0.00010167893961708394,
      "loss": 0.3272,
      "step": 1674
    },
    {
      "epoch": 0.49264705882352944,
      "grad_norm": 0.05213237926363945,
      "learning_rate": 0.000101620029455081,
      "loss": 0.3542,
      "step": 1675
    },
    {
      "epoch": 0.4929411764705882,
      "grad_norm": 0.05315670743584633,
      "learning_rate": 0.00010156111929307806,
      "loss": 0.3859,
      "step": 1676
    },
    {
      "epoch": 0.49323529411764705,
      "grad_norm": 0.052568159997463226,
      "learning_rate": 0.0001015022091310751,
      "loss": 0.3498,
      "step": 1677
    },
    {
      "epoch": 0.4935294117647059,
      "grad_norm": 0.03914724662899971,
      "learning_rate": 0.00010144329896907217,
      "loss": 0.3168,
      "step": 1678
    },
    {
      "epoch": 0.4938235294117647,
      "grad_norm": 0.0526774562895298,
      "learning_rate": 0.00010138438880706921,
      "loss": 0.3699,
      "step": 1679
    },
    {
      "epoch": 0.49411764705882355,
      "grad_norm": 0.05429819971323013,
      "learning_rate": 0.00010132547864506627,
      "loss": 0.3411,
      "step": 1680
    },
    {
      "epoch": 0.49441176470588233,
      "grad_norm": 0.049580857157707214,
      "learning_rate": 0.00010126656848306333,
      "loss": 0.3398,
      "step": 1681
    },
    {
      "epoch": 0.49470588235294116,
      "grad_norm": 0.05388356000185013,
      "learning_rate": 0.00010120765832106038,
      "loss": 0.4033,
      "step": 1682
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.06032685190439224,
      "learning_rate": 0.00010114874815905744,
      "loss": 0.4151,
      "step": 1683
    },
    {
      "epoch": 0.49529411764705883,
      "grad_norm": 0.03444807231426239,
      "learning_rate": 0.00010108983799705449,
      "loss": 0.2746,
      "step": 1684
    },
    {
      "epoch": 0.49558823529411766,
      "grad_norm": 0.05871571972966194,
      "learning_rate": 0.00010103092783505155,
      "loss": 0.3488,
      "step": 1685
    },
    {
      "epoch": 0.4958823529411765,
      "grad_norm": 0.047315813601017,
      "learning_rate": 0.00010097201767304861,
      "loss": 0.3782,
      "step": 1686
    },
    {
      "epoch": 0.4961764705882353,
      "grad_norm": 0.04772000014781952,
      "learning_rate": 0.00010091310751104565,
      "loss": 0.3528,
      "step": 1687
    },
    {
      "epoch": 0.4964705882352941,
      "grad_norm": 0.05624046176671982,
      "learning_rate": 0.00010085419734904271,
      "loss": 0.3656,
      "step": 1688
    },
    {
      "epoch": 0.49676470588235294,
      "grad_norm": 0.0543564073741436,
      "learning_rate": 0.00010079528718703976,
      "loss": 0.3865,
      "step": 1689
    },
    {
      "epoch": 0.4970588235294118,
      "grad_norm": 0.05994848906993866,
      "learning_rate": 0.00010073637702503682,
      "loss": 0.4037,
      "step": 1690
    },
    {
      "epoch": 0.4973529411764706,
      "grad_norm": 0.038097694516181946,
      "learning_rate": 0.00010067746686303388,
      "loss": 0.3199,
      "step": 1691
    },
    {
      "epoch": 0.4976470588235294,
      "grad_norm": 0.05198708176612854,
      "learning_rate": 0.00010061855670103093,
      "loss": 0.3721,
      "step": 1692
    },
    {
      "epoch": 0.4979411764705882,
      "grad_norm": 0.05876998230814934,
      "learning_rate": 0.00010055964653902799,
      "loss": 0.3433,
      "step": 1693
    },
    {
      "epoch": 0.49823529411764705,
      "grad_norm": 0.06102240830659866,
      "learning_rate": 0.00010050073637702503,
      "loss": 0.3954,
      "step": 1694
    },
    {
      "epoch": 0.4985294117647059,
      "grad_norm": 0.04415270313620567,
      "learning_rate": 0.0001004418262150221,
      "loss": 0.3091,
      "step": 1695
    },
    {
      "epoch": 0.4988235294117647,
      "grad_norm": 0.054650258272886276,
      "learning_rate": 0.00010038291605301915,
      "loss": 0.3461,
      "step": 1696
    },
    {
      "epoch": 0.49911764705882355,
      "grad_norm": 0.043888840824365616,
      "learning_rate": 0.0001003240058910162,
      "loss": 0.326,
      "step": 1697
    },
    {
      "epoch": 0.49941176470588233,
      "grad_norm": 0.06897685676813126,
      "learning_rate": 0.00010026509572901326,
      "loss": 0.3919,
      "step": 1698
    },
    {
      "epoch": 0.49970588235294117,
      "grad_norm": 0.05583410710096359,
      "learning_rate": 0.00010020618556701031,
      "loss": 0.3737,
      "step": 1699
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.044986799359321594,
      "learning_rate": 0.00010014727540500737,
      "loss": 0.3284,
      "step": 1700
    },
    {
      "epoch": 0.5002941176470588,
      "grad_norm": 0.04314029589295387,
      "learning_rate": 0.00010008836524300443,
      "loss": 0.3189,
      "step": 1701
    },
    {
      "epoch": 0.5005882352941177,
      "grad_norm": 0.05714030563831329,
      "learning_rate": 0.00010002945508100147,
      "loss": 0.3413,
      "step": 1702
    },
    {
      "epoch": 0.5008823529411764,
      "grad_norm": 0.04229305684566498,
      "learning_rate": 9.997054491899853e-05,
      "loss": 0.2874,
      "step": 1703
    },
    {
      "epoch": 0.5011764705882353,
      "grad_norm": 0.04807736724615097,
      "learning_rate": 9.991163475699558e-05,
      "loss": 0.3267,
      "step": 1704
    },
    {
      "epoch": 0.5014705882352941,
      "grad_norm": 0.045692529529333115,
      "learning_rate": 9.985272459499264e-05,
      "loss": 0.3514,
      "step": 1705
    },
    {
      "epoch": 0.5017647058823529,
      "grad_norm": 0.04166034609079361,
      "learning_rate": 9.97938144329897e-05,
      "loss": 0.3236,
      "step": 1706
    },
    {
      "epoch": 0.5020588235294118,
      "grad_norm": 0.06644153594970703,
      "learning_rate": 9.973490427098675e-05,
      "loss": 0.3642,
      "step": 1707
    },
    {
      "epoch": 0.5023529411764706,
      "grad_norm": 0.04737411439418793,
      "learning_rate": 9.967599410898381e-05,
      "loss": 0.2849,
      "step": 1708
    },
    {
      "epoch": 0.5026470588235294,
      "grad_norm": 0.040606915950775146,
      "learning_rate": 9.961708394698086e-05,
      "loss": 0.2729,
      "step": 1709
    },
    {
      "epoch": 0.5029411764705882,
      "grad_norm": 0.05366084724664688,
      "learning_rate": 9.955817378497792e-05,
      "loss": 0.344,
      "step": 1710
    },
    {
      "epoch": 0.5032352941176471,
      "grad_norm": 0.05963437631726265,
      "learning_rate": 9.949926362297498e-05,
      "loss": 0.4064,
      "step": 1711
    },
    {
      "epoch": 0.5035294117647059,
      "grad_norm": 0.04335085302591324,
      "learning_rate": 9.944035346097202e-05,
      "loss": 0.3395,
      "step": 1712
    },
    {
      "epoch": 0.5038235294117647,
      "grad_norm": 0.05161083862185478,
      "learning_rate": 9.938144329896908e-05,
      "loss": 0.3517,
      "step": 1713
    },
    {
      "epoch": 0.5041176470588236,
      "grad_norm": 0.03938307985663414,
      "learning_rate": 9.932253313696613e-05,
      "loss": 0.2898,
      "step": 1714
    },
    {
      "epoch": 0.5044117647058823,
      "grad_norm": 0.05173768848180771,
      "learning_rate": 9.926362297496319e-05,
      "loss": 0.3225,
      "step": 1715
    },
    {
      "epoch": 0.5047058823529412,
      "grad_norm": 0.043331652879714966,
      "learning_rate": 9.920471281296025e-05,
      "loss": 0.3485,
      "step": 1716
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.04942404478788376,
      "learning_rate": 9.91458026509573e-05,
      "loss": 0.3595,
      "step": 1717
    },
    {
      "epoch": 0.5052941176470588,
      "grad_norm": 0.050931137055158615,
      "learning_rate": 9.908689248895436e-05,
      "loss": 0.3413,
      "step": 1718
    },
    {
      "epoch": 0.5055882352941177,
      "grad_norm": 0.05510043352842331,
      "learning_rate": 9.90279823269514e-05,
      "loss": 0.3776,
      "step": 1719
    },
    {
      "epoch": 0.5058823529411764,
      "grad_norm": 0.051406893879175186,
      "learning_rate": 9.896907216494846e-05,
      "loss": 0.4179,
      "step": 1720
    },
    {
      "epoch": 0.5061764705882353,
      "grad_norm": 0.048609327524900436,
      "learning_rate": 9.891016200294552e-05,
      "loss": 0.3615,
      "step": 1721
    },
    {
      "epoch": 0.5064705882352941,
      "grad_norm": 0.041330400854349136,
      "learning_rate": 9.885125184094257e-05,
      "loss": 0.3378,
      "step": 1722
    },
    {
      "epoch": 0.5067647058823529,
      "grad_norm": 0.03717472404241562,
      "learning_rate": 9.879234167893963e-05,
      "loss": 0.3245,
      "step": 1723
    },
    {
      "epoch": 0.5070588235294118,
      "grad_norm": 0.05721132829785347,
      "learning_rate": 9.873343151693668e-05,
      "loss": 0.4139,
      "step": 1724
    },
    {
      "epoch": 0.5073529411764706,
      "grad_norm": 0.05043575540184975,
      "learning_rate": 9.867452135493374e-05,
      "loss": 0.3803,
      "step": 1725
    },
    {
      "epoch": 0.5076470588235295,
      "grad_norm": 0.03458159416913986,
      "learning_rate": 9.86156111929308e-05,
      "loss": 0.2757,
      "step": 1726
    },
    {
      "epoch": 0.5079411764705882,
      "grad_norm": 0.03070598654448986,
      "learning_rate": 9.855670103092784e-05,
      "loss": 0.2587,
      "step": 1727
    },
    {
      "epoch": 0.508235294117647,
      "grad_norm": 0.04429348185658455,
      "learning_rate": 9.84977908689249e-05,
      "loss": 0.3741,
      "step": 1728
    },
    {
      "epoch": 0.5085294117647059,
      "grad_norm": 0.05178232863545418,
      "learning_rate": 9.843888070692195e-05,
      "loss": 0.4181,
      "step": 1729
    },
    {
      "epoch": 0.5088235294117647,
      "grad_norm": 0.041066039353609085,
      "learning_rate": 9.8379970544919e-05,
      "loss": 0.3489,
      "step": 1730
    },
    {
      "epoch": 0.5091176470588236,
      "grad_norm": 0.043598711490631104,
      "learning_rate": 9.832106038291606e-05,
      "loss": 0.3595,
      "step": 1731
    },
    {
      "epoch": 0.5094117647058823,
      "grad_norm": 0.04233913496136665,
      "learning_rate": 9.82621502209131e-05,
      "loss": 0.3142,
      "step": 1732
    },
    {
      "epoch": 0.5097058823529412,
      "grad_norm": 0.03843590244650841,
      "learning_rate": 9.820324005891016e-05,
      "loss": 0.3766,
      "step": 1733
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.04705873504281044,
      "learning_rate": 9.814432989690721e-05,
      "loss": 0.3203,
      "step": 1734
    },
    {
      "epoch": 0.5102941176470588,
      "grad_norm": 0.03240329027175903,
      "learning_rate": 9.808541973490427e-05,
      "loss": 0.279,
      "step": 1735
    },
    {
      "epoch": 0.5105882352941177,
      "grad_norm": 0.051266901195049286,
      "learning_rate": 9.802650957290133e-05,
      "loss": 0.3536,
      "step": 1736
    },
    {
      "epoch": 0.5108823529411765,
      "grad_norm": 0.0476740300655365,
      "learning_rate": 9.796759941089838e-05,
      "loss": 0.3439,
      "step": 1737
    },
    {
      "epoch": 0.5111764705882353,
      "grad_norm": 0.054506778717041016,
      "learning_rate": 9.790868924889544e-05,
      "loss": 0.3653,
      "step": 1738
    },
    {
      "epoch": 0.5114705882352941,
      "grad_norm": 0.03795924782752991,
      "learning_rate": 9.784977908689248e-05,
      "loss": 0.3192,
      "step": 1739
    },
    {
      "epoch": 0.5117647058823529,
      "grad_norm": 0.041091252118349075,
      "learning_rate": 9.779086892488954e-05,
      "loss": 0.3122,
      "step": 1740
    },
    {
      "epoch": 0.5120588235294118,
      "grad_norm": 0.04838965833187103,
      "learning_rate": 9.77319587628866e-05,
      "loss": 0.3354,
      "step": 1741
    },
    {
      "epoch": 0.5123529411764706,
      "grad_norm": 0.05340604856610298,
      "learning_rate": 9.767304860088365e-05,
      "loss": 0.3298,
      "step": 1742
    },
    {
      "epoch": 0.5126470588235295,
      "grad_norm": 0.06375912576913834,
      "learning_rate": 9.761413843888071e-05,
      "loss": 0.4136,
      "step": 1743
    },
    {
      "epoch": 0.5129411764705882,
      "grad_norm": 0.058586206287145615,
      "learning_rate": 9.755522827687776e-05,
      "loss": 0.4237,
      "step": 1744
    },
    {
      "epoch": 0.513235294117647,
      "grad_norm": 0.05427192896604538,
      "learning_rate": 9.749631811487482e-05,
      "loss": 0.3621,
      "step": 1745
    },
    {
      "epoch": 0.5135294117647059,
      "grad_norm": 0.04835103452205658,
      "learning_rate": 9.743740795287188e-05,
      "loss": 0.3174,
      "step": 1746
    },
    {
      "epoch": 0.5138235294117647,
      "grad_norm": 0.04915889352560043,
      "learning_rate": 9.737849779086892e-05,
      "loss": 0.3484,
      "step": 1747
    },
    {
      "epoch": 0.5141176470588236,
      "grad_norm": 0.04535088688135147,
      "learning_rate": 9.731958762886598e-05,
      "loss": 0.3452,
      "step": 1748
    },
    {
      "epoch": 0.5144117647058823,
      "grad_norm": 0.036650434136390686,
      "learning_rate": 9.726067746686303e-05,
      "loss": 0.3139,
      "step": 1749
    },
    {
      "epoch": 0.5147058823529411,
      "grad_norm": 0.049402810633182526,
      "learning_rate": 9.720176730486009e-05,
      "loss": 0.3524,
      "step": 1750
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.04154234379529953,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.333,
      "step": 1751
    },
    {
      "epoch": 0.5152941176470588,
      "grad_norm": 0.04651952534914017,
      "learning_rate": 9.70839469808542e-05,
      "loss": 0.3121,
      "step": 1752
    },
    {
      "epoch": 0.5155882352941177,
      "grad_norm": 0.05712904408574104,
      "learning_rate": 9.702503681885126e-05,
      "loss": 0.3184,
      "step": 1753
    },
    {
      "epoch": 0.5158823529411765,
      "grad_norm": 0.04704102873802185,
      "learning_rate": 9.69661266568483e-05,
      "loss": 0.3963,
      "step": 1754
    },
    {
      "epoch": 0.5161764705882353,
      "grad_norm": 0.04659250006079674,
      "learning_rate": 9.690721649484537e-05,
      "loss": 0.3113,
      "step": 1755
    },
    {
      "epoch": 0.5164705882352941,
      "grad_norm": 0.05459751561284065,
      "learning_rate": 9.684830633284243e-05,
      "loss": 0.3909,
      "step": 1756
    },
    {
      "epoch": 0.5167647058823529,
      "grad_norm": 0.04374907910823822,
      "learning_rate": 9.678939617083947e-05,
      "loss": 0.32,
      "step": 1757
    },
    {
      "epoch": 0.5170588235294118,
      "grad_norm": 0.05395831540226936,
      "learning_rate": 9.673048600883653e-05,
      "loss": 0.4007,
      "step": 1758
    },
    {
      "epoch": 0.5173529411764706,
      "grad_norm": 0.053716257214546204,
      "learning_rate": 9.667157584683358e-05,
      "loss": 0.3514,
      "step": 1759
    },
    {
      "epoch": 0.5176470588235295,
      "grad_norm": 0.06334272027015686,
      "learning_rate": 9.661266568483064e-05,
      "loss": 0.4463,
      "step": 1760
    },
    {
      "epoch": 0.5179411764705882,
      "grad_norm": 0.055094145238399506,
      "learning_rate": 9.65537555228277e-05,
      "loss": 0.2956,
      "step": 1761
    },
    {
      "epoch": 0.518235294117647,
      "grad_norm": 0.04488834738731384,
      "learning_rate": 9.649484536082475e-05,
      "loss": 0.3276,
      "step": 1762
    },
    {
      "epoch": 0.5185294117647059,
      "grad_norm": 0.051017001271247864,
      "learning_rate": 9.64359351988218e-05,
      "loss": 0.3413,
      "step": 1763
    },
    {
      "epoch": 0.5188235294117647,
      "grad_norm": 0.052900031208992004,
      "learning_rate": 9.637702503681885e-05,
      "loss": 0.3443,
      "step": 1764
    },
    {
      "epoch": 0.5191176470588236,
      "grad_norm": 0.07021323591470718,
      "learning_rate": 9.631811487481591e-05,
      "loss": 0.4267,
      "step": 1765
    },
    {
      "epoch": 0.5194117647058824,
      "grad_norm": 0.04007675498723984,
      "learning_rate": 9.625920471281297e-05,
      "loss": 0.3001,
      "step": 1766
    },
    {
      "epoch": 0.5197058823529411,
      "grad_norm": 0.04947793856263161,
      "learning_rate": 9.620029455081002e-05,
      "loss": 0.3426,
      "step": 1767
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.04989256709814072,
      "learning_rate": 9.614138438880708e-05,
      "loss": 0.3093,
      "step": 1768
    },
    {
      "epoch": 0.5202941176470588,
      "grad_norm": 0.04785575345158577,
      "learning_rate": 9.608247422680413e-05,
      "loss": 0.3348,
      "step": 1769
    },
    {
      "epoch": 0.5205882352941177,
      "grad_norm": 0.049703653901815414,
      "learning_rate": 9.602356406480119e-05,
      "loss": 0.3157,
      "step": 1770
    },
    {
      "epoch": 0.5208823529411765,
      "grad_norm": 0.050546661019325256,
      "learning_rate": 9.596465390279825e-05,
      "loss": 0.3018,
      "step": 1771
    },
    {
      "epoch": 0.5211764705882352,
      "grad_norm": 0.04619484022259712,
      "learning_rate": 9.59057437407953e-05,
      "loss": 0.3182,
      "step": 1772
    },
    {
      "epoch": 0.5214705882352941,
      "grad_norm": 0.05117855593562126,
      "learning_rate": 9.584683357879235e-05,
      "loss": 0.4098,
      "step": 1773
    },
    {
      "epoch": 0.5217647058823529,
      "grad_norm": 0.06353809684515,
      "learning_rate": 9.57879234167894e-05,
      "loss": 0.3996,
      "step": 1774
    },
    {
      "epoch": 0.5220588235294118,
      "grad_norm": 0.04273150488734245,
      "learning_rate": 9.572901325478646e-05,
      "loss": 0.3402,
      "step": 1775
    },
    {
      "epoch": 0.5223529411764706,
      "grad_norm": 0.04604823514819145,
      "learning_rate": 9.567010309278352e-05,
      "loss": 0.348,
      "step": 1776
    },
    {
      "epoch": 0.5226470588235295,
      "grad_norm": 0.048253390938043594,
      "learning_rate": 9.561119293078057e-05,
      "loss": 0.2884,
      "step": 1777
    },
    {
      "epoch": 0.5229411764705882,
      "grad_norm": 0.043408870697021484,
      "learning_rate": 9.555228276877763e-05,
      "loss": 0.2893,
      "step": 1778
    },
    {
      "epoch": 0.523235294117647,
      "grad_norm": 0.05219200253486633,
      "learning_rate": 9.549337260677467e-05,
      "loss": 0.379,
      "step": 1779
    },
    {
      "epoch": 0.5235294117647059,
      "grad_norm": 0.03573836758732796,
      "learning_rate": 9.543446244477173e-05,
      "loss": 0.2919,
      "step": 1780
    },
    {
      "epoch": 0.5238235294117647,
      "grad_norm": 0.04882768169045448,
      "learning_rate": 9.53755522827688e-05,
      "loss": 0.396,
      "step": 1781
    },
    {
      "epoch": 0.5241176470588236,
      "grad_norm": 0.0480903796851635,
      "learning_rate": 9.531664212076584e-05,
      "loss": 0.3369,
      "step": 1782
    },
    {
      "epoch": 0.5244117647058824,
      "grad_norm": 0.04885858669877052,
      "learning_rate": 9.525773195876289e-05,
      "loss": 0.2973,
      "step": 1783
    },
    {
      "epoch": 0.5247058823529411,
      "grad_norm": 0.05399562790989876,
      "learning_rate": 9.519882179675993e-05,
      "loss": 0.3947,
      "step": 1784
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.044782157987356186,
      "learning_rate": 9.5139911634757e-05,
      "loss": 0.4063,
      "step": 1785
    },
    {
      "epoch": 0.5252941176470588,
      "grad_norm": 0.04685145616531372,
      "learning_rate": 9.508100147275405e-05,
      "loss": 0.3693,
      "step": 1786
    },
    {
      "epoch": 0.5255882352941177,
      "grad_norm": 0.0559387244284153,
      "learning_rate": 9.50220913107511e-05,
      "loss": 0.3642,
      "step": 1787
    },
    {
      "epoch": 0.5258823529411765,
      "grad_norm": 0.04516026750206947,
      "learning_rate": 9.496318114874816e-05,
      "loss": 0.3089,
      "step": 1788
    },
    {
      "epoch": 0.5261764705882352,
      "grad_norm": 0.041087307035923004,
      "learning_rate": 9.490427098674521e-05,
      "loss": 0.3003,
      "step": 1789
    },
    {
      "epoch": 0.5264705882352941,
      "grad_norm": 0.04301367700099945,
      "learning_rate": 9.484536082474227e-05,
      "loss": 0.3245,
      "step": 1790
    },
    {
      "epoch": 0.5267647058823529,
      "grad_norm": 0.04176270216703415,
      "learning_rate": 9.478645066273933e-05,
      "loss": 0.3562,
      "step": 1791
    },
    {
      "epoch": 0.5270588235294118,
      "grad_norm": 0.03706453740596771,
      "learning_rate": 9.472754050073637e-05,
      "loss": 0.2432,
      "step": 1792
    },
    {
      "epoch": 0.5273529411764706,
      "grad_norm": 0.04435214400291443,
      "learning_rate": 9.466863033873343e-05,
      "loss": 0.3096,
      "step": 1793
    },
    {
      "epoch": 0.5276470588235294,
      "grad_norm": 0.037736229598522186,
      "learning_rate": 9.460972017673048e-05,
      "loss": 0.249,
      "step": 1794
    },
    {
      "epoch": 0.5279411764705882,
      "grad_norm": 0.05446421727538109,
      "learning_rate": 9.455081001472754e-05,
      "loss": 0.3592,
      "step": 1795
    },
    {
      "epoch": 0.528235294117647,
      "grad_norm": 0.0384519100189209,
      "learning_rate": 9.44918998527246e-05,
      "loss": 0.3303,
      "step": 1796
    },
    {
      "epoch": 0.5285294117647059,
      "grad_norm": 0.04328423738479614,
      "learning_rate": 9.443298969072165e-05,
      "loss": 0.2855,
      "step": 1797
    },
    {
      "epoch": 0.5288235294117647,
      "grad_norm": 0.051416054368019104,
      "learning_rate": 9.437407952871871e-05,
      "loss": 0.3707,
      "step": 1798
    },
    {
      "epoch": 0.5291176470588236,
      "grad_norm": 0.04992001876235008,
      "learning_rate": 9.431516936671576e-05,
      "loss": 0.391,
      "step": 1799
    },
    {
      "epoch": 0.5294117647058824,
      "grad_norm": 0.038775309920310974,
      "learning_rate": 9.425625920471282e-05,
      "loss": 0.3196,
      "step": 1800
    },
    {
      "epoch": 0.5297058823529411,
      "grad_norm": 0.045641958713531494,
      "learning_rate": 9.419734904270988e-05,
      "loss": 0.3482,
      "step": 1801
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.04661744087934494,
      "learning_rate": 9.413843888070692e-05,
      "loss": 0.4019,
      "step": 1802
    },
    {
      "epoch": 0.5302941176470588,
      "grad_norm": 0.05736302584409714,
      "learning_rate": 9.407952871870398e-05,
      "loss": 0.3452,
      "step": 1803
    },
    {
      "epoch": 0.5305882352941177,
      "grad_norm": 0.04555872455239296,
      "learning_rate": 9.402061855670103e-05,
      "loss": 0.3009,
      "step": 1804
    },
    {
      "epoch": 0.5308823529411765,
      "grad_norm": 0.04901941493153572,
      "learning_rate": 9.396170839469809e-05,
      "loss": 0.3297,
      "step": 1805
    },
    {
      "epoch": 0.5311764705882352,
      "grad_norm": 0.06484629213809967,
      "learning_rate": 9.390279823269515e-05,
      "loss": 0.4278,
      "step": 1806
    },
    {
      "epoch": 0.5314705882352941,
      "grad_norm": 0.05929072946310043,
      "learning_rate": 9.38438880706922e-05,
      "loss": 0.4246,
      "step": 1807
    },
    {
      "epoch": 0.5317647058823529,
      "grad_norm": 0.054920002818107605,
      "learning_rate": 9.378497790868926e-05,
      "loss": 0.3841,
      "step": 1808
    },
    {
      "epoch": 0.5320588235294118,
      "grad_norm": 0.03491223230957985,
      "learning_rate": 9.37260677466863e-05,
      "loss": 0.2364,
      "step": 1809
    },
    {
      "epoch": 0.5323529411764706,
      "grad_norm": 0.04458010569214821,
      "learning_rate": 9.366715758468336e-05,
      "loss": 0.3474,
      "step": 1810
    },
    {
      "epoch": 0.5326470588235294,
      "grad_norm": 0.037657447159290314,
      "learning_rate": 9.360824742268042e-05,
      "loss": 0.3308,
      "step": 1811
    },
    {
      "epoch": 0.5329411764705883,
      "grad_norm": 0.04052026942372322,
      "learning_rate": 9.354933726067747e-05,
      "loss": 0.396,
      "step": 1812
    },
    {
      "epoch": 0.533235294117647,
      "grad_norm": 0.0498795360326767,
      "learning_rate": 9.349042709867453e-05,
      "loss": 0.344,
      "step": 1813
    },
    {
      "epoch": 0.5335294117647059,
      "grad_norm": 0.04045773670077324,
      "learning_rate": 9.343151693667158e-05,
      "loss": 0.2732,
      "step": 1814
    },
    {
      "epoch": 0.5338235294117647,
      "grad_norm": 0.044679854065179825,
      "learning_rate": 9.337260677466864e-05,
      "loss": 0.3667,
      "step": 1815
    },
    {
      "epoch": 0.5341176470588235,
      "grad_norm": 0.04649074375629425,
      "learning_rate": 9.33136966126657e-05,
      "loss": 0.3296,
      "step": 1816
    },
    {
      "epoch": 0.5344117647058824,
      "grad_norm": 0.04295928031206131,
      "learning_rate": 9.325478645066274e-05,
      "loss": 0.3424,
      "step": 1817
    },
    {
      "epoch": 0.5347058823529411,
      "grad_norm": 0.04023902490735054,
      "learning_rate": 9.31958762886598e-05,
      "loss": 0.3312,
      "step": 1818
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.044913873076438904,
      "learning_rate": 9.313696612665685e-05,
      "loss": 0.3377,
      "step": 1819
    },
    {
      "epoch": 0.5352941176470588,
      "grad_norm": 0.06871085613965988,
      "learning_rate": 9.307805596465391e-05,
      "loss": 0.3913,
      "step": 1820
    },
    {
      "epoch": 0.5355882352941177,
      "grad_norm": 0.051148463040590286,
      "learning_rate": 9.301914580265097e-05,
      "loss": 0.3084,
      "step": 1821
    },
    {
      "epoch": 0.5358823529411765,
      "grad_norm": 0.04445590078830719,
      "learning_rate": 9.296023564064802e-05,
      "loss": 0.3121,
      "step": 1822
    },
    {
      "epoch": 0.5361764705882353,
      "grad_norm": 0.05190468579530716,
      "learning_rate": 9.290132547864508e-05,
      "loss": 0.3617,
      "step": 1823
    },
    {
      "epoch": 0.5364705882352941,
      "grad_norm": 0.04546792432665825,
      "learning_rate": 9.284241531664212e-05,
      "loss": 0.3617,
      "step": 1824
    },
    {
      "epoch": 0.5367647058823529,
      "grad_norm": 0.04321611672639847,
      "learning_rate": 9.278350515463918e-05,
      "loss": 0.3577,
      "step": 1825
    },
    {
      "epoch": 0.5370588235294118,
      "grad_norm": 0.04895901679992676,
      "learning_rate": 9.272459499263624e-05,
      "loss": 0.3618,
      "step": 1826
    },
    {
      "epoch": 0.5373529411764706,
      "grad_norm": 0.043483976274728775,
      "learning_rate": 9.266568483063329e-05,
      "loss": 0.3189,
      "step": 1827
    },
    {
      "epoch": 0.5376470588235294,
      "grad_norm": 0.045404158532619476,
      "learning_rate": 9.260677466863035e-05,
      "loss": 0.3309,
      "step": 1828
    },
    {
      "epoch": 0.5379411764705883,
      "grad_norm": 0.05100367218255997,
      "learning_rate": 9.25478645066274e-05,
      "loss": 0.2801,
      "step": 1829
    },
    {
      "epoch": 0.538235294117647,
      "grad_norm": 0.03449404984712601,
      "learning_rate": 9.248895434462446e-05,
      "loss": 0.273,
      "step": 1830
    },
    {
      "epoch": 0.5385294117647059,
      "grad_norm": 0.05476994812488556,
      "learning_rate": 9.243004418262152e-05,
      "loss": 0.2967,
      "step": 1831
    },
    {
      "epoch": 0.5388235294117647,
      "grad_norm": 0.06019939109683037,
      "learning_rate": 9.237113402061856e-05,
      "loss": 0.4024,
      "step": 1832
    },
    {
      "epoch": 0.5391176470588235,
      "grad_norm": 0.051418546587228775,
      "learning_rate": 9.231222385861562e-05,
      "loss": 0.3675,
      "step": 1833
    },
    {
      "epoch": 0.5394117647058824,
      "grad_norm": 0.05283453315496445,
      "learning_rate": 9.225331369661267e-05,
      "loss": 0.3447,
      "step": 1834
    },
    {
      "epoch": 0.5397058823529411,
      "grad_norm": 0.04236438870429993,
      "learning_rate": 9.219440353460973e-05,
      "loss": 0.344,
      "step": 1835
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.04832961782813072,
      "learning_rate": 9.213549337260678e-05,
      "loss": 0.316,
      "step": 1836
    },
    {
      "epoch": 0.5402941176470588,
      "grad_norm": 0.04837290570139885,
      "learning_rate": 9.207658321060382e-05,
      "loss": 0.3461,
      "step": 1837
    },
    {
      "epoch": 0.5405882352941176,
      "grad_norm": 0.04113490879535675,
      "learning_rate": 9.201767304860088e-05,
      "loss": 0.3071,
      "step": 1838
    },
    {
      "epoch": 0.5408823529411765,
      "grad_norm": 0.04157930612564087,
      "learning_rate": 9.195876288659793e-05,
      "loss": 0.2878,
      "step": 1839
    },
    {
      "epoch": 0.5411764705882353,
      "grad_norm": 0.042680710554122925,
      "learning_rate": 9.189985272459499e-05,
      "loss": 0.3028,
      "step": 1840
    },
    {
      "epoch": 0.5414705882352941,
      "grad_norm": 0.04654286429286003,
      "learning_rate": 9.184094256259205e-05,
      "loss": 0.3387,
      "step": 1841
    },
    {
      "epoch": 0.5417647058823529,
      "grad_norm": 0.061227209866046906,
      "learning_rate": 9.17820324005891e-05,
      "loss": 0.4314,
      "step": 1842
    },
    {
      "epoch": 0.5420588235294118,
      "grad_norm": 0.04747416079044342,
      "learning_rate": 9.172312223858616e-05,
      "loss": 0.2932,
      "step": 1843
    },
    {
      "epoch": 0.5423529411764706,
      "grad_norm": 0.044712577015161514,
      "learning_rate": 9.16642120765832e-05,
      "loss": 0.3628,
      "step": 1844
    },
    {
      "epoch": 0.5426470588235294,
      "grad_norm": 0.04033961892127991,
      "learning_rate": 9.160530191458027e-05,
      "loss": 0.3008,
      "step": 1845
    },
    {
      "epoch": 0.5429411764705883,
      "grad_norm": 0.03762767091393471,
      "learning_rate": 9.154639175257733e-05,
      "loss": 0.2543,
      "step": 1846
    },
    {
      "epoch": 0.543235294117647,
      "grad_norm": 0.04270537942647934,
      "learning_rate": 9.148748159057437e-05,
      "loss": 0.3853,
      "step": 1847
    },
    {
      "epoch": 0.5435294117647059,
      "grad_norm": 0.04909765347838402,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.3872,
      "step": 1848
    },
    {
      "epoch": 0.5438235294117647,
      "grad_norm": 0.05174367129802704,
      "learning_rate": 9.136966126656848e-05,
      "loss": 0.3875,
      "step": 1849
    },
    {
      "epoch": 0.5441176470588235,
      "grad_norm": 0.04752833768725395,
      "learning_rate": 9.131075110456554e-05,
      "loss": 0.348,
      "step": 1850
    },
    {
      "epoch": 0.5444117647058824,
      "grad_norm": 0.06018294394016266,
      "learning_rate": 9.12518409425626e-05,
      "loss": 0.3336,
      "step": 1851
    },
    {
      "epoch": 0.5447058823529412,
      "grad_norm": 0.061131637543439865,
      "learning_rate": 9.119293078055965e-05,
      "loss": 0.3621,
      "step": 1852
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.04405287280678749,
      "learning_rate": 9.11340206185567e-05,
      "loss": 0.3422,
      "step": 1853
    },
    {
      "epoch": 0.5452941176470588,
      "grad_norm": 0.05011977255344391,
      "learning_rate": 9.107511045655375e-05,
      "loss": 0.3375,
      "step": 1854
    },
    {
      "epoch": 0.5455882352941176,
      "grad_norm": 0.05423375219106674,
      "learning_rate": 9.101620029455081e-05,
      "loss": 0.3115,
      "step": 1855
    },
    {
      "epoch": 0.5458823529411765,
      "grad_norm": 0.042326051741838455,
      "learning_rate": 9.095729013254787e-05,
      "loss": 0.3266,
      "step": 1856
    },
    {
      "epoch": 0.5461764705882353,
      "grad_norm": 0.05603432282805443,
      "learning_rate": 9.089837997054492e-05,
      "loss": 0.3302,
      "step": 1857
    },
    {
      "epoch": 0.5464705882352942,
      "grad_norm": 0.03929820656776428,
      "learning_rate": 9.083946980854198e-05,
      "loss": 0.2708,
      "step": 1858
    },
    {
      "epoch": 0.5467647058823529,
      "grad_norm": 0.050273261964321136,
      "learning_rate": 9.078055964653903e-05,
      "loss": 0.3704,
      "step": 1859
    },
    {
      "epoch": 0.5470588235294118,
      "grad_norm": 0.06715530157089233,
      "learning_rate": 9.072164948453609e-05,
      "loss": 0.4818,
      "step": 1860
    },
    {
      "epoch": 0.5473529411764706,
      "grad_norm": 0.053527962416410446,
      "learning_rate": 9.066273932253315e-05,
      "loss": 0.3576,
      "step": 1861
    },
    {
      "epoch": 0.5476470588235294,
      "grad_norm": 0.05625772103667259,
      "learning_rate": 9.060382916053019e-05,
      "loss": 0.3763,
      "step": 1862
    },
    {
      "epoch": 0.5479411764705883,
      "grad_norm": 0.04277811571955681,
      "learning_rate": 9.054491899852725e-05,
      "loss": 0.3317,
      "step": 1863
    },
    {
      "epoch": 0.548235294117647,
      "grad_norm": 0.05503545701503754,
      "learning_rate": 9.04860088365243e-05,
      "loss": 0.4212,
      "step": 1864
    },
    {
      "epoch": 0.5485294117647059,
      "grad_norm": 0.044441577047109604,
      "learning_rate": 9.042709867452136e-05,
      "loss": 0.351,
      "step": 1865
    },
    {
      "epoch": 0.5488235294117647,
      "grad_norm": 0.046410929411649704,
      "learning_rate": 9.036818851251842e-05,
      "loss": 0.3678,
      "step": 1866
    },
    {
      "epoch": 0.5491176470588235,
      "grad_norm": 0.0543406680226326,
      "learning_rate": 9.030927835051547e-05,
      "loss": 0.4072,
      "step": 1867
    },
    {
      "epoch": 0.5494117647058824,
      "grad_norm": 0.04547872766852379,
      "learning_rate": 9.025036818851253e-05,
      "loss": 0.3331,
      "step": 1868
    },
    {
      "epoch": 0.5497058823529412,
      "grad_norm": 0.05041125789284706,
      "learning_rate": 9.019145802650957e-05,
      "loss": 0.3622,
      "step": 1869
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.040180061012506485,
      "learning_rate": 9.013254786450663e-05,
      "loss": 0.331,
      "step": 1870
    },
    {
      "epoch": 0.5502941176470588,
      "grad_norm": 0.05022214353084564,
      "learning_rate": 9.00736377025037e-05,
      "loss": 0.3388,
      "step": 1871
    },
    {
      "epoch": 0.5505882352941176,
      "grad_norm": 0.04845084995031357,
      "learning_rate": 9.001472754050074e-05,
      "loss": 0.4085,
      "step": 1872
    },
    {
      "epoch": 0.5508823529411765,
      "grad_norm": 0.04441871494054794,
      "learning_rate": 8.99558173784978e-05,
      "loss": 0.3412,
      "step": 1873
    },
    {
      "epoch": 0.5511764705882353,
      "grad_norm": 0.03926204517483711,
      "learning_rate": 8.989690721649485e-05,
      "loss": 0.3427,
      "step": 1874
    },
    {
      "epoch": 0.5514705882352942,
      "grad_norm": 0.0482245571911335,
      "learning_rate": 8.983799705449191e-05,
      "loss": 0.3368,
      "step": 1875
    },
    {
      "epoch": 0.5517647058823529,
      "grad_norm": 0.047287940979003906,
      "learning_rate": 8.977908689248897e-05,
      "loss": 0.3398,
      "step": 1876
    },
    {
      "epoch": 0.5520588235294117,
      "grad_norm": 0.036139532923698425,
      "learning_rate": 8.972017673048601e-05,
      "loss": 0.2991,
      "step": 1877
    },
    {
      "epoch": 0.5523529411764706,
      "grad_norm": 0.05056361109018326,
      "learning_rate": 8.966126656848307e-05,
      "loss": 0.3531,
      "step": 1878
    },
    {
      "epoch": 0.5526470588235294,
      "grad_norm": 0.056019142270088196,
      "learning_rate": 8.960235640648012e-05,
      "loss": 0.4254,
      "step": 1879
    },
    {
      "epoch": 0.5529411764705883,
      "grad_norm": 0.045749466866254807,
      "learning_rate": 8.954344624447718e-05,
      "loss": 0.3702,
      "step": 1880
    },
    {
      "epoch": 0.553235294117647,
      "grad_norm": 0.05128956958651543,
      "learning_rate": 8.948453608247424e-05,
      "loss": 0.3783,
      "step": 1881
    },
    {
      "epoch": 0.5535294117647059,
      "grad_norm": 0.054449282586574554,
      "learning_rate": 8.942562592047129e-05,
      "loss": 0.392,
      "step": 1882
    },
    {
      "epoch": 0.5538235294117647,
      "grad_norm": 0.040967218577861786,
      "learning_rate": 8.936671575846835e-05,
      "loss": 0.3568,
      "step": 1883
    },
    {
      "epoch": 0.5541176470588235,
      "grad_norm": 0.043015964329242706,
      "learning_rate": 8.93078055964654e-05,
      "loss": 0.3582,
      "step": 1884
    },
    {
      "epoch": 0.5544117647058824,
      "grad_norm": 0.0573016032576561,
      "learning_rate": 8.924889543446246e-05,
      "loss": 0.3809,
      "step": 1885
    },
    {
      "epoch": 0.5547058823529412,
      "grad_norm": 0.042188338935375214,
      "learning_rate": 8.918998527245952e-05,
      "loss": 0.2744,
      "step": 1886
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.04881438612937927,
      "learning_rate": 8.913107511045656e-05,
      "loss": 0.2731,
      "step": 1887
    },
    {
      "epoch": 0.5552941176470588,
      "grad_norm": 0.04903298616409302,
      "learning_rate": 8.907216494845362e-05,
      "loss": 0.3417,
      "step": 1888
    },
    {
      "epoch": 0.5555882352941176,
      "grad_norm": 0.06089577451348305,
      "learning_rate": 8.901325478645066e-05,
      "loss": 0.4095,
      "step": 1889
    },
    {
      "epoch": 0.5558823529411765,
      "grad_norm": 0.0626143366098404,
      "learning_rate": 8.895434462444772e-05,
      "loss": 0.4254,
      "step": 1890
    },
    {
      "epoch": 0.5561764705882353,
      "grad_norm": 0.04690704494714737,
      "learning_rate": 8.889543446244478e-05,
      "loss": 0.3566,
      "step": 1891
    },
    {
      "epoch": 0.5564705882352942,
      "grad_norm": 0.04312502592802048,
      "learning_rate": 8.883652430044182e-05,
      "loss": 0.2938,
      "step": 1892
    },
    {
      "epoch": 0.5567647058823529,
      "grad_norm": 0.06090954691171646,
      "learning_rate": 8.877761413843888e-05,
      "loss": 0.4023,
      "step": 1893
    },
    {
      "epoch": 0.5570588235294117,
      "grad_norm": 0.03922247886657715,
      "learning_rate": 8.871870397643593e-05,
      "loss": 0.2752,
      "step": 1894
    },
    {
      "epoch": 0.5573529411764706,
      "grad_norm": 0.0501248836517334,
      "learning_rate": 8.865979381443299e-05,
      "loss": 0.3384,
      "step": 1895
    },
    {
      "epoch": 0.5576470588235294,
      "grad_norm": 0.03975377976894379,
      "learning_rate": 8.860088365243005e-05,
      "loss": 0.273,
      "step": 1896
    },
    {
      "epoch": 0.5579411764705883,
      "grad_norm": 0.05189146101474762,
      "learning_rate": 8.85419734904271e-05,
      "loss": 0.4033,
      "step": 1897
    },
    {
      "epoch": 0.558235294117647,
      "grad_norm": 0.04359805956482887,
      "learning_rate": 8.848306332842416e-05,
      "loss": 0.3075,
      "step": 1898
    },
    {
      "epoch": 0.5585294117647058,
      "grad_norm": 0.053385987877845764,
      "learning_rate": 8.84241531664212e-05,
      "loss": 0.3999,
      "step": 1899
    },
    {
      "epoch": 0.5588235294117647,
      "grad_norm": 0.06324176490306854,
      "learning_rate": 8.836524300441826e-05,
      "loss": 0.4013,
      "step": 1900
    },
    {
      "epoch": 0.5591176470588235,
      "grad_norm": 0.04991088807582855,
      "learning_rate": 8.830633284241532e-05,
      "loss": 0.4115,
      "step": 1901
    },
    {
      "epoch": 0.5594117647058824,
      "grad_norm": 0.049365390092134476,
      "learning_rate": 8.824742268041237e-05,
      "loss": 0.3434,
      "step": 1902
    },
    {
      "epoch": 0.5597058823529412,
      "grad_norm": 0.05062161758542061,
      "learning_rate": 8.818851251840943e-05,
      "loss": 0.2918,
      "step": 1903
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.04806137457489967,
      "learning_rate": 8.812960235640648e-05,
      "loss": 0.34,
      "step": 1904
    },
    {
      "epoch": 0.5602941176470588,
      "grad_norm": 0.0482453852891922,
      "learning_rate": 8.807069219440354e-05,
      "loss": 0.3461,
      "step": 1905
    },
    {
      "epoch": 0.5605882352941176,
      "grad_norm": 0.05163322016596794,
      "learning_rate": 8.80117820324006e-05,
      "loss": 0.4391,
      "step": 1906
    },
    {
      "epoch": 0.5608823529411765,
      "grad_norm": 0.047586485743522644,
      "learning_rate": 8.795287187039764e-05,
      "loss": 0.3305,
      "step": 1907
    },
    {
      "epoch": 0.5611764705882353,
      "grad_norm": 0.04456404969096184,
      "learning_rate": 8.78939617083947e-05,
      "loss": 0.3298,
      "step": 1908
    },
    {
      "epoch": 0.5614705882352942,
      "grad_norm": 0.061330024152994156,
      "learning_rate": 8.783505154639175e-05,
      "loss": 0.41,
      "step": 1909
    },
    {
      "epoch": 0.5617647058823529,
      "grad_norm": 0.04331842437386513,
      "learning_rate": 8.777614138438881e-05,
      "loss": 0.397,
      "step": 1910
    },
    {
      "epoch": 0.5620588235294117,
      "grad_norm": 0.04770917072892189,
      "learning_rate": 8.771723122238587e-05,
      "loss": 0.3948,
      "step": 1911
    },
    {
      "epoch": 0.5623529411764706,
      "grad_norm": 0.04839891940355301,
      "learning_rate": 8.765832106038292e-05,
      "loss": 0.3809,
      "step": 1912
    },
    {
      "epoch": 0.5626470588235294,
      "grad_norm": 0.04041104391217232,
      "learning_rate": 8.759941089837998e-05,
      "loss": 0.3148,
      "step": 1913
    },
    {
      "epoch": 0.5629411764705883,
      "grad_norm": 0.04938764125108719,
      "learning_rate": 8.754050073637702e-05,
      "loss": 0.353,
      "step": 1914
    },
    {
      "epoch": 0.5632352941176471,
      "grad_norm": 0.04358674958348274,
      "learning_rate": 8.748159057437408e-05,
      "loss": 0.392,
      "step": 1915
    },
    {
      "epoch": 0.5635294117647058,
      "grad_norm": 0.04530709981918335,
      "learning_rate": 8.742268041237114e-05,
      "loss": 0.3558,
      "step": 1916
    },
    {
      "epoch": 0.5638235294117647,
      "grad_norm": 0.0603218749165535,
      "learning_rate": 8.736377025036819e-05,
      "loss": 0.3841,
      "step": 1917
    },
    {
      "epoch": 0.5641176470588235,
      "grad_norm": 0.06122058257460594,
      "learning_rate": 8.730486008836525e-05,
      "loss": 0.4329,
      "step": 1918
    },
    {
      "epoch": 0.5644117647058824,
      "grad_norm": 0.057791609317064285,
      "learning_rate": 8.72459499263623e-05,
      "loss": 0.4258,
      "step": 1919
    },
    {
      "epoch": 0.5647058823529412,
      "grad_norm": 0.04668904095888138,
      "learning_rate": 8.718703976435936e-05,
      "loss": 0.3086,
      "step": 1920
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.040429048240184784,
      "learning_rate": 8.712812960235642e-05,
      "loss": 0.28,
      "step": 1921
    },
    {
      "epoch": 0.5652941176470588,
      "grad_norm": 0.05434780195355415,
      "learning_rate": 8.706921944035346e-05,
      "loss": 0.3817,
      "step": 1922
    },
    {
      "epoch": 0.5655882352941176,
      "grad_norm": 0.04890887439250946,
      "learning_rate": 8.701030927835052e-05,
      "loss": 0.3056,
      "step": 1923
    },
    {
      "epoch": 0.5658823529411765,
      "grad_norm": 0.055483151227235794,
      "learning_rate": 8.695139911634757e-05,
      "loss": 0.3976,
      "step": 1924
    },
    {
      "epoch": 0.5661764705882353,
      "grad_norm": 0.03608332946896553,
      "learning_rate": 8.689248895434463e-05,
      "loss": 0.3342,
      "step": 1925
    },
    {
      "epoch": 0.5664705882352942,
      "grad_norm": 0.055924054235219955,
      "learning_rate": 8.683357879234169e-05,
      "loss": 0.4038,
      "step": 1926
    },
    {
      "epoch": 0.566764705882353,
      "grad_norm": 0.04935825243592262,
      "learning_rate": 8.677466863033874e-05,
      "loss": 0.3646,
      "step": 1927
    },
    {
      "epoch": 0.5670588235294117,
      "grad_norm": 0.06263311952352524,
      "learning_rate": 8.67157584683358e-05,
      "loss": 0.4338,
      "step": 1928
    },
    {
      "epoch": 0.5673529411764706,
      "grad_norm": 0.03838044032454491,
      "learning_rate": 8.665684830633284e-05,
      "loss": 0.3181,
      "step": 1929
    },
    {
      "epoch": 0.5676470588235294,
      "grad_norm": 0.04221194609999657,
      "learning_rate": 8.65979381443299e-05,
      "loss": 0.3767,
      "step": 1930
    },
    {
      "epoch": 0.5679411764705883,
      "grad_norm": 0.04936078563332558,
      "learning_rate": 8.653902798232697e-05,
      "loss": 0.3559,
      "step": 1931
    },
    {
      "epoch": 0.5682352941176471,
      "grad_norm": 0.035055842250585556,
      "learning_rate": 8.648011782032401e-05,
      "loss": 0.3139,
      "step": 1932
    },
    {
      "epoch": 0.5685294117647058,
      "grad_norm": 0.06115572899580002,
      "learning_rate": 8.642120765832107e-05,
      "loss": 0.392,
      "step": 1933
    },
    {
      "epoch": 0.5688235294117647,
      "grad_norm": 0.03951466828584671,
      "learning_rate": 8.636229749631812e-05,
      "loss": 0.2942,
      "step": 1934
    },
    {
      "epoch": 0.5691176470588235,
      "grad_norm": 0.05689695104956627,
      "learning_rate": 8.630338733431518e-05,
      "loss": 0.4243,
      "step": 1935
    },
    {
      "epoch": 0.5694117647058824,
      "grad_norm": 0.05247476324439049,
      "learning_rate": 8.624447717231224e-05,
      "loss": 0.4031,
      "step": 1936
    },
    {
      "epoch": 0.5697058823529412,
      "grad_norm": 0.0506211593747139,
      "learning_rate": 8.618556701030929e-05,
      "loss": 0.3393,
      "step": 1937
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.04916169121861458,
      "learning_rate": 8.612665684830635e-05,
      "loss": 0.4159,
      "step": 1938
    },
    {
      "epoch": 0.5702941176470588,
      "grad_norm": 0.05872855708003044,
      "learning_rate": 8.606774668630339e-05,
      "loss": 0.3387,
      "step": 1939
    },
    {
      "epoch": 0.5705882352941176,
      "grad_norm": 0.05482875183224678,
      "learning_rate": 8.600883652430045e-05,
      "loss": 0.3801,
      "step": 1940
    },
    {
      "epoch": 0.5708823529411765,
      "grad_norm": 0.03473929315805435,
      "learning_rate": 8.594992636229751e-05,
      "loss": 0.2574,
      "step": 1941
    },
    {
      "epoch": 0.5711764705882353,
      "grad_norm": 0.0453232005238533,
      "learning_rate": 8.589101620029455e-05,
      "loss": 0.3089,
      "step": 1942
    },
    {
      "epoch": 0.5714705882352941,
      "grad_norm": 0.05138956755399704,
      "learning_rate": 8.58321060382916e-05,
      "loss": 0.3502,
      "step": 1943
    },
    {
      "epoch": 0.571764705882353,
      "grad_norm": 0.04110918194055557,
      "learning_rate": 8.577319587628865e-05,
      "loss": 0.2742,
      "step": 1944
    },
    {
      "epoch": 0.5720588235294117,
      "grad_norm": 0.05299581214785576,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.3845,
      "step": 1945
    },
    {
      "epoch": 0.5723529411764706,
      "grad_norm": 0.04682376608252525,
      "learning_rate": 8.565537555228277e-05,
      "loss": 0.3398,
      "step": 1946
    },
    {
      "epoch": 0.5726470588235294,
      "grad_norm": 0.05454351380467415,
      "learning_rate": 8.559646539027982e-05,
      "loss": 0.3826,
      "step": 1947
    },
    {
      "epoch": 0.5729411764705883,
      "grad_norm": 0.0436612144112587,
      "learning_rate": 8.553755522827688e-05,
      "loss": 0.342,
      "step": 1948
    },
    {
      "epoch": 0.5732352941176471,
      "grad_norm": 0.0523659773170948,
      "learning_rate": 8.547864506627393e-05,
      "loss": 0.3261,
      "step": 1949
    },
    {
      "epoch": 0.5735294117647058,
      "grad_norm": 0.05130484327673912,
      "learning_rate": 8.541973490427099e-05,
      "loss": 0.2832,
      "step": 1950
    },
    {
      "epoch": 0.5738235294117647,
      "grad_norm": 0.04333888366818428,
      "learning_rate": 8.536082474226805e-05,
      "loss": 0.3106,
      "step": 1951
    },
    {
      "epoch": 0.5741176470588235,
      "grad_norm": 0.05693695321679115,
      "learning_rate": 8.530191458026509e-05,
      "loss": 0.3878,
      "step": 1952
    },
    {
      "epoch": 0.5744117647058824,
      "grad_norm": 0.045601736754179,
      "learning_rate": 8.524300441826215e-05,
      "loss": 0.3578,
      "step": 1953
    },
    {
      "epoch": 0.5747058823529412,
      "grad_norm": 0.06073611229658127,
      "learning_rate": 8.51840942562592e-05,
      "loss": 0.38,
      "step": 1954
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.04110075533390045,
      "learning_rate": 8.512518409425626e-05,
      "loss": 0.3214,
      "step": 1955
    },
    {
      "epoch": 0.5752941176470588,
      "grad_norm": 0.044550709426403046,
      "learning_rate": 8.506627393225332e-05,
      "loss": 0.2843,
      "step": 1956
    },
    {
      "epoch": 0.5755882352941176,
      "grad_norm": 0.038090068846940994,
      "learning_rate": 8.500736377025037e-05,
      "loss": 0.2873,
      "step": 1957
    },
    {
      "epoch": 0.5758823529411765,
      "grad_norm": 0.03994321823120117,
      "learning_rate": 8.494845360824743e-05,
      "loss": 0.2624,
      "step": 1958
    },
    {
      "epoch": 0.5761764705882353,
      "grad_norm": 0.057197827845811844,
      "learning_rate": 8.488954344624447e-05,
      "loss": 0.3533,
      "step": 1959
    },
    {
      "epoch": 0.5764705882352941,
      "grad_norm": 0.045863375067710876,
      "learning_rate": 8.483063328424153e-05,
      "loss": 0.337,
      "step": 1960
    },
    {
      "epoch": 0.576764705882353,
      "grad_norm": 0.05365129932761192,
      "learning_rate": 8.47717231222386e-05,
      "loss": 0.3767,
      "step": 1961
    },
    {
      "epoch": 0.5770588235294117,
      "grad_norm": 0.05532209575176239,
      "learning_rate": 8.471281296023564e-05,
      "loss": 0.3434,
      "step": 1962
    },
    {
      "epoch": 0.5773529411764706,
      "grad_norm": 0.04320697486400604,
      "learning_rate": 8.46539027982327e-05,
      "loss": 0.3372,
      "step": 1963
    },
    {
      "epoch": 0.5776470588235294,
      "grad_norm": 0.042121756821870804,
      "learning_rate": 8.459499263622975e-05,
      "loss": 0.3284,
      "step": 1964
    },
    {
      "epoch": 0.5779411764705882,
      "grad_norm": 0.0426524318754673,
      "learning_rate": 8.453608247422681e-05,
      "loss": 0.3117,
      "step": 1965
    },
    {
      "epoch": 0.5782352941176471,
      "grad_norm": 0.0497121624648571,
      "learning_rate": 8.447717231222387e-05,
      "loss": 0.3632,
      "step": 1966
    },
    {
      "epoch": 0.5785294117647058,
      "grad_norm": 0.05511480197310448,
      "learning_rate": 8.441826215022091e-05,
      "loss": 0.2721,
      "step": 1967
    },
    {
      "epoch": 0.5788235294117647,
      "grad_norm": 0.03793850913643837,
      "learning_rate": 8.435935198821797e-05,
      "loss": 0.3005,
      "step": 1968
    },
    {
      "epoch": 0.5791176470588235,
      "grad_norm": 0.04649626836180687,
      "learning_rate": 8.430044182621502e-05,
      "loss": 0.3447,
      "step": 1969
    },
    {
      "epoch": 0.5794117647058824,
      "grad_norm": 0.04333297535777092,
      "learning_rate": 8.424153166421208e-05,
      "loss": 0.3403,
      "step": 1970
    },
    {
      "epoch": 0.5797058823529412,
      "grad_norm": 0.04057876393198967,
      "learning_rate": 8.418262150220914e-05,
      "loss": 0.3498,
      "step": 1971
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0570792518556118,
      "learning_rate": 8.412371134020619e-05,
      "loss": 0.376,
      "step": 1972
    },
    {
      "epoch": 0.5802941176470588,
      "grad_norm": 0.03682044893503189,
      "learning_rate": 8.406480117820325e-05,
      "loss": 0.305,
      "step": 1973
    },
    {
      "epoch": 0.5805882352941176,
      "grad_norm": 0.05369585379958153,
      "learning_rate": 8.40058910162003e-05,
      "loss": 0.3927,
      "step": 1974
    },
    {
      "epoch": 0.5808823529411765,
      "grad_norm": 0.05063173919916153,
      "learning_rate": 8.394698085419735e-05,
      "loss": 0.382,
      "step": 1975
    },
    {
      "epoch": 0.5811764705882353,
      "grad_norm": 0.030188195407390594,
      "learning_rate": 8.388807069219442e-05,
      "loss": 0.2812,
      "step": 1976
    },
    {
      "epoch": 0.5814705882352941,
      "grad_norm": 0.03634750843048096,
      "learning_rate": 8.382916053019146e-05,
      "loss": 0.3078,
      "step": 1977
    },
    {
      "epoch": 0.581764705882353,
      "grad_norm": 0.034519631415605545,
      "learning_rate": 8.377025036818852e-05,
      "loss": 0.3062,
      "step": 1978
    },
    {
      "epoch": 0.5820588235294117,
      "grad_norm": 0.057246893644332886,
      "learning_rate": 8.371134020618557e-05,
      "loss": 0.3597,
      "step": 1979
    },
    {
      "epoch": 0.5823529411764706,
      "grad_norm": 0.04884710535407066,
      "learning_rate": 8.365243004418263e-05,
      "loss": 0.3527,
      "step": 1980
    },
    {
      "epoch": 0.5826470588235294,
      "grad_norm": 0.04735523462295532,
      "learning_rate": 8.359351988217969e-05,
      "loss": 0.4071,
      "step": 1981
    },
    {
      "epoch": 0.5829411764705882,
      "grad_norm": 0.04910470172762871,
      "learning_rate": 8.353460972017674e-05,
      "loss": 0.3446,
      "step": 1982
    },
    {
      "epoch": 0.5832352941176471,
      "grad_norm": 0.04478530213236809,
      "learning_rate": 8.34756995581738e-05,
      "loss": 0.3291,
      "step": 1983
    },
    {
      "epoch": 0.5835294117647059,
      "grad_norm": 0.06400790065526962,
      "learning_rate": 8.341678939617084e-05,
      "loss": 0.3982,
      "step": 1984
    },
    {
      "epoch": 0.5838235294117647,
      "grad_norm": 0.04608780890703201,
      "learning_rate": 8.33578792341679e-05,
      "loss": 0.3891,
      "step": 1985
    },
    {
      "epoch": 0.5841176470588235,
      "grad_norm": 0.04977082833647728,
      "learning_rate": 8.329896907216496e-05,
      "loss": 0.2633,
      "step": 1986
    },
    {
      "epoch": 0.5844117647058824,
      "grad_norm": 0.043789464980363846,
      "learning_rate": 8.324005891016201e-05,
      "loss": 0.3972,
      "step": 1987
    },
    {
      "epoch": 0.5847058823529412,
      "grad_norm": 0.059054356068372726,
      "learning_rate": 8.318114874815907e-05,
      "loss": 0.401,
      "step": 1988
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.05372563377022743,
      "learning_rate": 8.312223858615612e-05,
      "loss": 0.4159,
      "step": 1989
    },
    {
      "epoch": 0.5852941176470589,
      "grad_norm": 0.04786352068185806,
      "learning_rate": 8.306332842415318e-05,
      "loss": 0.3269,
      "step": 1990
    },
    {
      "epoch": 0.5855882352941176,
      "grad_norm": 0.04620801657438278,
      "learning_rate": 8.300441826215024e-05,
      "loss": 0.3619,
      "step": 1991
    },
    {
      "epoch": 0.5858823529411765,
      "grad_norm": 0.055389873683452606,
      "learning_rate": 8.294550810014728e-05,
      "loss": 0.4037,
      "step": 1992
    },
    {
      "epoch": 0.5861764705882353,
      "grad_norm": 0.0462874174118042,
      "learning_rate": 8.288659793814434e-05,
      "loss": 0.3409,
      "step": 1993
    },
    {
      "epoch": 0.5864705882352941,
      "grad_norm": 0.03694258630275726,
      "learning_rate": 8.282768777614139e-05,
      "loss": 0.301,
      "step": 1994
    },
    {
      "epoch": 0.586764705882353,
      "grad_norm": 0.034240078181028366,
      "learning_rate": 8.276877761413844e-05,
      "loss": 0.2963,
      "step": 1995
    },
    {
      "epoch": 0.5870588235294117,
      "grad_norm": 0.052373722195625305,
      "learning_rate": 8.27098674521355e-05,
      "loss": 0.3142,
      "step": 1996
    },
    {
      "epoch": 0.5873529411764706,
      "grad_norm": 0.05202869698405266,
      "learning_rate": 8.265095729013254e-05,
      "loss": 0.3932,
      "step": 1997
    },
    {
      "epoch": 0.5876470588235294,
      "grad_norm": 0.04942825064063072,
      "learning_rate": 8.25920471281296e-05,
      "loss": 0.3614,
      "step": 1998
    },
    {
      "epoch": 0.5879411764705882,
      "grad_norm": 0.049552906304597855,
      "learning_rate": 8.253313696612665e-05,
      "loss": 0.3754,
      "step": 1999
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.039579376578330994,
      "learning_rate": 8.247422680412371e-05,
      "loss": 0.342,
      "step": 2000
    },
    {
      "epoch": 0.5885294117647059,
      "grad_norm": 0.07056751847267151,
      "learning_rate": 8.241531664212077e-05,
      "loss": 0.3161,
      "step": 2001
    },
    {
      "epoch": 0.5888235294117647,
      "grad_norm": 0.049371376633644104,
      "learning_rate": 8.235640648011782e-05,
      "loss": 0.4001,
      "step": 2002
    },
    {
      "epoch": 0.5891176470588235,
      "grad_norm": 0.05252690240740776,
      "learning_rate": 8.229749631811488e-05,
      "loss": 0.3441,
      "step": 2003
    },
    {
      "epoch": 0.5894117647058823,
      "grad_norm": 0.05045747756958008,
      "learning_rate": 8.223858615611192e-05,
      "loss": 0.3716,
      "step": 2004
    },
    {
      "epoch": 0.5897058823529412,
      "grad_norm": 0.0497693233191967,
      "learning_rate": 8.217967599410898e-05,
      "loss": 0.3764,
      "step": 2005
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.05079316347837448,
      "learning_rate": 8.212076583210604e-05,
      "loss": 0.3589,
      "step": 2006
    },
    {
      "epoch": 0.5902941176470589,
      "grad_norm": 0.04655410721898079,
      "learning_rate": 8.206185567010309e-05,
      "loss": 0.3452,
      "step": 2007
    },
    {
      "epoch": 0.5905882352941176,
      "grad_norm": 0.04033109173178673,
      "learning_rate": 8.200294550810015e-05,
      "loss": 0.2693,
      "step": 2008
    },
    {
      "epoch": 0.5908823529411765,
      "grad_norm": 0.042946260422468185,
      "learning_rate": 8.19440353460972e-05,
      "loss": 0.243,
      "step": 2009
    },
    {
      "epoch": 0.5911764705882353,
      "grad_norm": 0.06162109971046448,
      "learning_rate": 8.188512518409426e-05,
      "loss": 0.3533,
      "step": 2010
    },
    {
      "epoch": 0.5914705882352941,
      "grad_norm": 0.04448312893509865,
      "learning_rate": 8.182621502209132e-05,
      "loss": 0.2685,
      "step": 2011
    },
    {
      "epoch": 0.591764705882353,
      "grad_norm": 0.0458676852285862,
      "learning_rate": 8.176730486008836e-05,
      "loss": 0.2981,
      "step": 2012
    },
    {
      "epoch": 0.5920588235294117,
      "grad_norm": 0.04915335029363632,
      "learning_rate": 8.170839469808542e-05,
      "loss": 0.3488,
      "step": 2013
    },
    {
      "epoch": 0.5923529411764706,
      "grad_norm": 0.06377781927585602,
      "learning_rate": 8.164948453608247e-05,
      "loss": 0.411,
      "step": 2014
    },
    {
      "epoch": 0.5926470588235294,
      "grad_norm": 0.04750128090381622,
      "learning_rate": 8.159057437407953e-05,
      "loss": 0.3795,
      "step": 2015
    },
    {
      "epoch": 0.5929411764705882,
      "grad_norm": 0.06166286766529083,
      "learning_rate": 8.153166421207659e-05,
      "loss": 0.3881,
      "step": 2016
    },
    {
      "epoch": 0.5932352941176471,
      "grad_norm": 0.05853146314620972,
      "learning_rate": 8.147275405007364e-05,
      "loss": 0.3629,
      "step": 2017
    },
    {
      "epoch": 0.5935294117647059,
      "grad_norm": 0.050591710954904556,
      "learning_rate": 8.14138438880707e-05,
      "loss": 0.3441,
      "step": 2018
    },
    {
      "epoch": 0.5938235294117648,
      "grad_norm": 0.05789291858673096,
      "learning_rate": 8.135493372606774e-05,
      "loss": 0.4044,
      "step": 2019
    },
    {
      "epoch": 0.5941176470588235,
      "grad_norm": 0.0626937597990036,
      "learning_rate": 8.12960235640648e-05,
      "loss": 0.3617,
      "step": 2020
    },
    {
      "epoch": 0.5944117647058823,
      "grad_norm": 0.05817635357379913,
      "learning_rate": 8.123711340206187e-05,
      "loss": 0.4366,
      "step": 2021
    },
    {
      "epoch": 0.5947058823529412,
      "grad_norm": 0.03756875544786453,
      "learning_rate": 8.117820324005891e-05,
      "loss": 0.2586,
      "step": 2022
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.05333438888192177,
      "learning_rate": 8.111929307805597e-05,
      "loss": 0.3694,
      "step": 2023
    },
    {
      "epoch": 0.5952941176470589,
      "grad_norm": 0.03825882449746132,
      "learning_rate": 8.106038291605302e-05,
      "loss": 0.3502,
      "step": 2024
    },
    {
      "epoch": 0.5955882352941176,
      "grad_norm": 0.044621922075748444,
      "learning_rate": 8.100147275405008e-05,
      "loss": 0.4065,
      "step": 2025
    },
    {
      "epoch": 0.5958823529411764,
      "grad_norm": 0.058223847299814224,
      "learning_rate": 8.094256259204714e-05,
      "loss": 0.4465,
      "step": 2026
    },
    {
      "epoch": 0.5961764705882353,
      "grad_norm": 0.04139634594321251,
      "learning_rate": 8.088365243004419e-05,
      "loss": 0.2746,
      "step": 2027
    },
    {
      "epoch": 0.5964705882352941,
      "grad_norm": 0.05824800953269005,
      "learning_rate": 8.082474226804125e-05,
      "loss": 0.4049,
      "step": 2028
    },
    {
      "epoch": 0.596764705882353,
      "grad_norm": 0.06637313961982727,
      "learning_rate": 8.076583210603829e-05,
      "loss": 0.3564,
      "step": 2029
    },
    {
      "epoch": 0.5970588235294118,
      "grad_norm": 0.0509616918861866,
      "learning_rate": 8.070692194403535e-05,
      "loss": 0.341,
      "step": 2030
    },
    {
      "epoch": 0.5973529411764706,
      "grad_norm": 0.0504922978579998,
      "learning_rate": 8.064801178203241e-05,
      "loss": 0.332,
      "step": 2031
    },
    {
      "epoch": 0.5976470588235294,
      "grad_norm": 0.05140898749232292,
      "learning_rate": 8.058910162002946e-05,
      "loss": 0.3849,
      "step": 2032
    },
    {
      "epoch": 0.5979411764705882,
      "grad_norm": 0.059855133295059204,
      "learning_rate": 8.053019145802652e-05,
      "loss": 0.3627,
      "step": 2033
    },
    {
      "epoch": 0.5982352941176471,
      "grad_norm": 0.04769385978579521,
      "learning_rate": 8.047128129602357e-05,
      "loss": 0.2832,
      "step": 2034
    },
    {
      "epoch": 0.5985294117647059,
      "grad_norm": 0.0451643280684948,
      "learning_rate": 8.041237113402063e-05,
      "loss": 0.3384,
      "step": 2035
    },
    {
      "epoch": 0.5988235294117648,
      "grad_norm": 0.05263255536556244,
      "learning_rate": 8.035346097201769e-05,
      "loss": 0.3626,
      "step": 2036
    },
    {
      "epoch": 0.5991176470588235,
      "grad_norm": 0.055807508528232574,
      "learning_rate": 8.029455081001473e-05,
      "loss": 0.3397,
      "step": 2037
    },
    {
      "epoch": 0.5994117647058823,
      "grad_norm": 0.05044310912489891,
      "learning_rate": 8.023564064801179e-05,
      "loss": 0.358,
      "step": 2038
    },
    {
      "epoch": 0.5997058823529412,
      "grad_norm": 0.04114565998315811,
      "learning_rate": 8.017673048600884e-05,
      "loss": 0.3525,
      "step": 2039
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04422594979405403,
      "learning_rate": 8.01178203240059e-05,
      "loss": 0.3238,
      "step": 2040
    },
    {
      "epoch": 0.6002941176470589,
      "grad_norm": 0.04799537733197212,
      "learning_rate": 8.005891016200296e-05,
      "loss": 0.3336,
      "step": 2041
    },
    {
      "epoch": 0.6005882352941176,
      "grad_norm": 0.052346765995025635,
      "learning_rate": 8e-05,
      "loss": 0.3334,
      "step": 2042
    },
    {
      "epoch": 0.6008823529411764,
      "grad_norm": 0.0660063624382019,
      "learning_rate": 7.994108983799707e-05,
      "loss": 0.3922,
      "step": 2043
    },
    {
      "epoch": 0.6011764705882353,
      "grad_norm": 0.04510908201336861,
      "learning_rate": 7.988217967599411e-05,
      "loss": 0.3013,
      "step": 2044
    },
    {
      "epoch": 0.6014705882352941,
      "grad_norm": 0.05910590663552284,
      "learning_rate": 7.982326951399117e-05,
      "loss": 0.3777,
      "step": 2045
    },
    {
      "epoch": 0.601764705882353,
      "grad_norm": 0.04961925372481346,
      "learning_rate": 7.976435935198823e-05,
      "loss": 0.3593,
      "step": 2046
    },
    {
      "epoch": 0.6020588235294118,
      "grad_norm": 0.04827817156910896,
      "learning_rate": 7.970544918998528e-05,
      "loss": 0.3147,
      "step": 2047
    },
    {
      "epoch": 0.6023529411764705,
      "grad_norm": 0.060500357300043106,
      "learning_rate": 7.964653902798233e-05,
      "loss": 0.3728,
      "step": 2048
    },
    {
      "epoch": 0.6026470588235294,
      "grad_norm": 0.04252849519252777,
      "learning_rate": 7.958762886597937e-05,
      "loss": 0.2603,
      "step": 2049
    },
    {
      "epoch": 0.6029411764705882,
      "grad_norm": 0.058983758091926575,
      "learning_rate": 7.952871870397643e-05,
      "loss": 0.3711,
      "step": 2050
    },
    {
      "epoch": 0.6032352941176471,
      "grad_norm": 0.049397848546504974,
      "learning_rate": 7.94698085419735e-05,
      "loss": 0.3354,
      "step": 2051
    },
    {
      "epoch": 0.6035294117647059,
      "grad_norm": 0.044279370456933975,
      "learning_rate": 7.941089837997054e-05,
      "loss": 0.3128,
      "step": 2052
    },
    {
      "epoch": 0.6038235294117648,
      "grad_norm": 0.058200638741254807,
      "learning_rate": 7.93519882179676e-05,
      "loss": 0.3963,
      "step": 2053
    },
    {
      "epoch": 0.6041176470588235,
      "grad_norm": 0.0482264868915081,
      "learning_rate": 7.929307805596465e-05,
      "loss": 0.3743,
      "step": 2054
    },
    {
      "epoch": 0.6044117647058823,
      "grad_norm": 0.04680157080292702,
      "learning_rate": 7.923416789396171e-05,
      "loss": 0.3429,
      "step": 2055
    },
    {
      "epoch": 0.6047058823529412,
      "grad_norm": 0.061536405235528946,
      "learning_rate": 7.917525773195877e-05,
      "loss": 0.3732,
      "step": 2056
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.061270006000995636,
      "learning_rate": 7.911634756995581e-05,
      "loss": 0.4149,
      "step": 2057
    },
    {
      "epoch": 0.6052941176470589,
      "grad_norm": 0.04340337589383125,
      "learning_rate": 7.905743740795287e-05,
      "loss": 0.3358,
      "step": 2058
    },
    {
      "epoch": 0.6055882352941176,
      "grad_norm": 0.05888912454247475,
      "learning_rate": 7.899852724594992e-05,
      "loss": 0.3402,
      "step": 2059
    },
    {
      "epoch": 0.6058823529411764,
      "grad_norm": 0.04732930660247803,
      "learning_rate": 7.893961708394698e-05,
      "loss": 0.2735,
      "step": 2060
    },
    {
      "epoch": 0.6061764705882353,
      "grad_norm": 0.047161977738142014,
      "learning_rate": 7.888070692194404e-05,
      "loss": 0.3039,
      "step": 2061
    },
    {
      "epoch": 0.6064705882352941,
      "grad_norm": 0.032684575766325,
      "learning_rate": 7.882179675994109e-05,
      "loss": 0.2259,
      "step": 2062
    },
    {
      "epoch": 0.606764705882353,
      "grad_norm": 0.03554006665945053,
      "learning_rate": 7.876288659793815e-05,
      "loss": 0.2649,
      "step": 2063
    },
    {
      "epoch": 0.6070588235294118,
      "grad_norm": 0.050823431462049484,
      "learning_rate": 7.87039764359352e-05,
      "loss": 0.2743,
      "step": 2064
    },
    {
      "epoch": 0.6073529411764705,
      "grad_norm": 0.05714256316423416,
      "learning_rate": 7.864506627393225e-05,
      "loss": 0.3325,
      "step": 2065
    },
    {
      "epoch": 0.6076470588235294,
      "grad_norm": 0.059716373682022095,
      "learning_rate": 7.858615611192931e-05,
      "loss": 0.3676,
      "step": 2066
    },
    {
      "epoch": 0.6079411764705882,
      "grad_norm": 0.05823332816362381,
      "learning_rate": 7.852724594992636e-05,
      "loss": 0.4092,
      "step": 2067
    },
    {
      "epoch": 0.6082352941176471,
      "grad_norm": 0.05077841505408287,
      "learning_rate": 7.846833578792342e-05,
      "loss": 0.3406,
      "step": 2068
    },
    {
      "epoch": 0.6085294117647059,
      "grad_norm": 0.04094169661402702,
      "learning_rate": 7.840942562592047e-05,
      "loss": 0.3686,
      "step": 2069
    },
    {
      "epoch": 0.6088235294117647,
      "grad_norm": 0.05271606519818306,
      "learning_rate": 7.835051546391753e-05,
      "loss": 0.3869,
      "step": 2070
    },
    {
      "epoch": 0.6091176470588235,
      "grad_norm": 0.05005795136094093,
      "learning_rate": 7.829160530191459e-05,
      "loss": 0.361,
      "step": 2071
    },
    {
      "epoch": 0.6094117647058823,
      "grad_norm": 0.058408014476299286,
      "learning_rate": 7.823269513991164e-05,
      "loss": 0.3655,
      "step": 2072
    },
    {
      "epoch": 0.6097058823529412,
      "grad_norm": 0.04653017967939377,
      "learning_rate": 7.81737849779087e-05,
      "loss": 0.3171,
      "step": 2073
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.045083221048116684,
      "learning_rate": 7.811487481590574e-05,
      "loss": 0.3511,
      "step": 2074
    },
    {
      "epoch": 0.6102941176470589,
      "grad_norm": 0.04826666787266731,
      "learning_rate": 7.80559646539028e-05,
      "loss": 0.3701,
      "step": 2075
    },
    {
      "epoch": 0.6105882352941177,
      "grad_norm": 0.040295135229825974,
      "learning_rate": 7.799705449189986e-05,
      "loss": 0.3019,
      "step": 2076
    },
    {
      "epoch": 0.6108823529411764,
      "grad_norm": 0.04354424774646759,
      "learning_rate": 7.793814432989691e-05,
      "loss": 0.3324,
      "step": 2077
    },
    {
      "epoch": 0.6111764705882353,
      "grad_norm": 0.04708963632583618,
      "learning_rate": 7.787923416789397e-05,
      "loss": 0.2683,
      "step": 2078
    },
    {
      "epoch": 0.6114705882352941,
      "grad_norm": 0.0431140661239624,
      "learning_rate": 7.782032400589102e-05,
      "loss": 0.3025,
      "step": 2079
    },
    {
      "epoch": 0.611764705882353,
      "grad_norm": 0.047712747007608414,
      "learning_rate": 7.776141384388808e-05,
      "loss": 0.3232,
      "step": 2080
    },
    {
      "epoch": 0.6120588235294118,
      "grad_norm": 0.0650450810790062,
      "learning_rate": 7.770250368188514e-05,
      "loss": 0.3663,
      "step": 2081
    },
    {
      "epoch": 0.6123529411764705,
      "grad_norm": 0.05819523334503174,
      "learning_rate": 7.764359351988218e-05,
      "loss": 0.4088,
      "step": 2082
    },
    {
      "epoch": 0.6126470588235294,
      "grad_norm": 0.0439702607691288,
      "learning_rate": 7.758468335787924e-05,
      "loss": 0.3548,
      "step": 2083
    },
    {
      "epoch": 0.6129411764705882,
      "grad_norm": 0.04252992942929268,
      "learning_rate": 7.752577319587629e-05,
      "loss": 0.3351,
      "step": 2084
    },
    {
      "epoch": 0.6132352941176471,
      "grad_norm": 0.0587465837597847,
      "learning_rate": 7.746686303387335e-05,
      "loss": 0.3334,
      "step": 2085
    },
    {
      "epoch": 0.6135294117647059,
      "grad_norm": 0.06527865678071976,
      "learning_rate": 7.740795287187041e-05,
      "loss": 0.3281,
      "step": 2086
    },
    {
      "epoch": 0.6138235294117647,
      "grad_norm": 0.0462467186152935,
      "learning_rate": 7.734904270986746e-05,
      "loss": 0.3337,
      "step": 2087
    },
    {
      "epoch": 0.6141176470588235,
      "grad_norm": 0.03539568558335304,
      "learning_rate": 7.729013254786452e-05,
      "loss": 0.3153,
      "step": 2088
    },
    {
      "epoch": 0.6144117647058823,
      "grad_norm": 0.04049060121178627,
      "learning_rate": 7.723122238586156e-05,
      "loss": 0.2992,
      "step": 2089
    },
    {
      "epoch": 0.6147058823529412,
      "grad_norm": 0.059833865612745285,
      "learning_rate": 7.717231222385862e-05,
      "loss": 0.424,
      "step": 2090
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.041253287345170975,
      "learning_rate": 7.711340206185568e-05,
      "loss": 0.3157,
      "step": 2091
    },
    {
      "epoch": 0.6152941176470588,
      "grad_norm": 0.05854453146457672,
      "learning_rate": 7.705449189985273e-05,
      "loss": 0.4061,
      "step": 2092
    },
    {
      "epoch": 0.6155882352941177,
      "grad_norm": 0.05069074034690857,
      "learning_rate": 7.699558173784979e-05,
      "loss": 0.3383,
      "step": 2093
    },
    {
      "epoch": 0.6158823529411764,
      "grad_norm": 0.041421450674533844,
      "learning_rate": 7.693667157584684e-05,
      "loss": 0.3429,
      "step": 2094
    },
    {
      "epoch": 0.6161764705882353,
      "grad_norm": 0.0574091337621212,
      "learning_rate": 7.68777614138439e-05,
      "loss": 0.3309,
      "step": 2095
    },
    {
      "epoch": 0.6164705882352941,
      "grad_norm": 0.05367534980177879,
      "learning_rate": 7.681885125184096e-05,
      "loss": 0.3894,
      "step": 2096
    },
    {
      "epoch": 0.616764705882353,
      "grad_norm": 0.053532909601926804,
      "learning_rate": 7.6759941089838e-05,
      "loss": 0.352,
      "step": 2097
    },
    {
      "epoch": 0.6170588235294118,
      "grad_norm": 0.057054225355386734,
      "learning_rate": 7.670103092783506e-05,
      "loss": 0.3729,
      "step": 2098
    },
    {
      "epoch": 0.6173529411764705,
      "grad_norm": 0.04864414036273956,
      "learning_rate": 7.664212076583211e-05,
      "loss": 0.3858,
      "step": 2099
    },
    {
      "epoch": 0.6176470588235294,
      "grad_norm": 0.04215078055858612,
      "learning_rate": 7.658321060382917e-05,
      "loss": 0.3353,
      "step": 2100
    },
    {
      "epoch": 0.6179411764705882,
      "grad_norm": 0.04095003008842468,
      "learning_rate": 7.652430044182622e-05,
      "loss": 0.3399,
      "step": 2101
    },
    {
      "epoch": 0.6182352941176471,
      "grad_norm": 0.043805256485939026,
      "learning_rate": 7.646539027982326e-05,
      "loss": 0.3196,
      "step": 2102
    },
    {
      "epoch": 0.6185294117647059,
      "grad_norm": 0.045547325164079666,
      "learning_rate": 7.640648011782032e-05,
      "loss": 0.2897,
      "step": 2103
    },
    {
      "epoch": 0.6188235294117647,
      "grad_norm": 0.05376164987683296,
      "learning_rate": 7.634756995581737e-05,
      "loss": 0.399,
      "step": 2104
    },
    {
      "epoch": 0.6191176470588236,
      "grad_norm": 0.03679865971207619,
      "learning_rate": 7.628865979381443e-05,
      "loss": 0.3072,
      "step": 2105
    },
    {
      "epoch": 0.6194117647058823,
      "grad_norm": 0.060676209628582,
      "learning_rate": 7.622974963181149e-05,
      "loss": 0.3886,
      "step": 2106
    },
    {
      "epoch": 0.6197058823529412,
      "grad_norm": 0.04073013365268707,
      "learning_rate": 7.617083946980854e-05,
      "loss": 0.3171,
      "step": 2107
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.03440430015325546,
      "learning_rate": 7.61119293078056e-05,
      "loss": 0.2823,
      "step": 2108
    },
    {
      "epoch": 0.6202941176470588,
      "grad_norm": 0.04224596917629242,
      "learning_rate": 7.605301914580264e-05,
      "loss": 0.3552,
      "step": 2109
    },
    {
      "epoch": 0.6205882352941177,
      "grad_norm": 0.054143160581588745,
      "learning_rate": 7.59941089837997e-05,
      "loss": 0.4037,
      "step": 2110
    },
    {
      "epoch": 0.6208823529411764,
      "grad_norm": 0.042456842958927155,
      "learning_rate": 7.593519882179676e-05,
      "loss": 0.3339,
      "step": 2111
    },
    {
      "epoch": 0.6211764705882353,
      "grad_norm": 0.041688546538352966,
      "learning_rate": 7.587628865979381e-05,
      "loss": 0.3177,
      "step": 2112
    },
    {
      "epoch": 0.6214705882352941,
      "grad_norm": 0.03720372915267944,
      "learning_rate": 7.581737849779087e-05,
      "loss": 0.2683,
      "step": 2113
    },
    {
      "epoch": 0.6217647058823529,
      "grad_norm": 0.04178701341152191,
      "learning_rate": 7.575846833578792e-05,
      "loss": 0.3209,
      "step": 2114
    },
    {
      "epoch": 0.6220588235294118,
      "grad_norm": 0.04856502637267113,
      "learning_rate": 7.569955817378498e-05,
      "loss": 0.3225,
      "step": 2115
    },
    {
      "epoch": 0.6223529411764706,
      "grad_norm": 0.05051272362470627,
      "learning_rate": 7.564064801178204e-05,
      "loss": 0.3809,
      "step": 2116
    },
    {
      "epoch": 0.6226470588235294,
      "grad_norm": 0.05461616814136505,
      "learning_rate": 7.558173784977909e-05,
      "loss": 0.3906,
      "step": 2117
    },
    {
      "epoch": 0.6229411764705882,
      "grad_norm": 0.043325114995241165,
      "learning_rate": 7.552282768777615e-05,
      "loss": 0.2705,
      "step": 2118
    },
    {
      "epoch": 0.6232352941176471,
      "grad_norm": 0.04911251366138458,
      "learning_rate": 7.546391752577319e-05,
      "loss": 0.4161,
      "step": 2119
    },
    {
      "epoch": 0.6235294117647059,
      "grad_norm": 0.04063773527741432,
      "learning_rate": 7.540500736377025e-05,
      "loss": 0.3597,
      "step": 2120
    },
    {
      "epoch": 0.6238235294117647,
      "grad_norm": 0.06252336502075195,
      "learning_rate": 7.534609720176731e-05,
      "loss": 0.3458,
      "step": 2121
    },
    {
      "epoch": 0.6241176470588236,
      "grad_norm": 0.048632871359586716,
      "learning_rate": 7.528718703976436e-05,
      "loss": 0.3503,
      "step": 2122
    },
    {
      "epoch": 0.6244117647058823,
      "grad_norm": 0.04571393504738808,
      "learning_rate": 7.522827687776142e-05,
      "loss": 0.3135,
      "step": 2123
    },
    {
      "epoch": 0.6247058823529412,
      "grad_norm": 0.05013566091656685,
      "learning_rate": 7.516936671575847e-05,
      "loss": 0.3825,
      "step": 2124
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.0416688434779644,
      "learning_rate": 7.511045655375553e-05,
      "loss": 0.3063,
      "step": 2125
    },
    {
      "epoch": 0.6252941176470588,
      "grad_norm": 0.04685951769351959,
      "learning_rate": 7.505154639175259e-05,
      "loss": 0.2974,
      "step": 2126
    },
    {
      "epoch": 0.6255882352941177,
      "grad_norm": 0.05656694620847702,
      "learning_rate": 7.499263622974963e-05,
      "loss": 0.3697,
      "step": 2127
    },
    {
      "epoch": 0.6258823529411764,
      "grad_norm": 0.03874140605330467,
      "learning_rate": 7.493372606774669e-05,
      "loss": 0.2913,
      "step": 2128
    },
    {
      "epoch": 0.6261764705882353,
      "grad_norm": 0.05958382785320282,
      "learning_rate": 7.487481590574374e-05,
      "loss": 0.3865,
      "step": 2129
    },
    {
      "epoch": 0.6264705882352941,
      "grad_norm": 0.050443120300769806,
      "learning_rate": 7.48159057437408e-05,
      "loss": 0.3428,
      "step": 2130
    },
    {
      "epoch": 0.6267647058823529,
      "grad_norm": 0.03707334026694298,
      "learning_rate": 7.475699558173786e-05,
      "loss": 0.2993,
      "step": 2131
    },
    {
      "epoch": 0.6270588235294118,
      "grad_norm": 0.044916003942489624,
      "learning_rate": 7.46980854197349e-05,
      "loss": 0.323,
      "step": 2132
    },
    {
      "epoch": 0.6273529411764706,
      "grad_norm": 0.038452908396720886,
      "learning_rate": 7.463917525773197e-05,
      "loss": 0.3272,
      "step": 2133
    },
    {
      "epoch": 0.6276470588235294,
      "grad_norm": 0.06942971795797348,
      "learning_rate": 7.458026509572901e-05,
      "loss": 0.4071,
      "step": 2134
    },
    {
      "epoch": 0.6279411764705882,
      "grad_norm": 0.04895513504743576,
      "learning_rate": 7.452135493372607e-05,
      "loss": 0.3671,
      "step": 2135
    },
    {
      "epoch": 0.6282352941176471,
      "grad_norm": 0.04175768047571182,
      "learning_rate": 7.446244477172313e-05,
      "loss": 0.3492,
      "step": 2136
    },
    {
      "epoch": 0.6285294117647059,
      "grad_norm": 0.053107861429452896,
      "learning_rate": 7.440353460972018e-05,
      "loss": 0.4053,
      "step": 2137
    },
    {
      "epoch": 0.6288235294117647,
      "grad_norm": 0.049631472676992416,
      "learning_rate": 7.434462444771724e-05,
      "loss": 0.3373,
      "step": 2138
    },
    {
      "epoch": 0.6291176470588236,
      "grad_norm": 0.03858492523431778,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.3,
      "step": 2139
    },
    {
      "epoch": 0.6294117647058823,
      "grad_norm": 0.04772866889834404,
      "learning_rate": 7.422680412371135e-05,
      "loss": 0.4072,
      "step": 2140
    },
    {
      "epoch": 0.6297058823529412,
      "grad_norm": 0.04160059615969658,
      "learning_rate": 7.416789396170841e-05,
      "loss": 0.3363,
      "step": 2141
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.0404479093849659,
      "learning_rate": 7.410898379970545e-05,
      "loss": 0.3852,
      "step": 2142
    },
    {
      "epoch": 0.6302941176470588,
      "grad_norm": 0.04355965927243233,
      "learning_rate": 7.405007363770251e-05,
      "loss": 0.3025,
      "step": 2143
    },
    {
      "epoch": 0.6305882352941177,
      "grad_norm": 0.04907221347093582,
      "learning_rate": 7.399116347569956e-05,
      "loss": 0.3691,
      "step": 2144
    },
    {
      "epoch": 0.6308823529411764,
      "grad_norm": 0.04373473674058914,
      "learning_rate": 7.393225331369662e-05,
      "loss": 0.2935,
      "step": 2145
    },
    {
      "epoch": 0.6311764705882353,
      "grad_norm": 0.04324820637702942,
      "learning_rate": 7.387334315169368e-05,
      "loss": 0.3156,
      "step": 2146
    },
    {
      "epoch": 0.6314705882352941,
      "grad_norm": 0.042206358164548874,
      "learning_rate": 7.381443298969073e-05,
      "loss": 0.3361,
      "step": 2147
    },
    {
      "epoch": 0.6317647058823529,
      "grad_norm": 0.0353575199842453,
      "learning_rate": 7.375552282768779e-05,
      "loss": 0.2509,
      "step": 2148
    },
    {
      "epoch": 0.6320588235294118,
      "grad_norm": 0.0458044707775116,
      "learning_rate": 7.369661266568483e-05,
      "loss": 0.3581,
      "step": 2149
    },
    {
      "epoch": 0.6323529411764706,
      "grad_norm": 0.04085717722773552,
      "learning_rate": 7.36377025036819e-05,
      "loss": 0.3533,
      "step": 2150
    },
    {
      "epoch": 0.6326470588235295,
      "grad_norm": 0.040318138897418976,
      "learning_rate": 7.357879234167895e-05,
      "loss": 0.3145,
      "step": 2151
    },
    {
      "epoch": 0.6329411764705882,
      "grad_norm": 0.04981398582458496,
      "learning_rate": 7.3519882179676e-05,
      "loss": 0.3658,
      "step": 2152
    },
    {
      "epoch": 0.633235294117647,
      "grad_norm": 0.05428260937333107,
      "learning_rate": 7.346097201767306e-05,
      "loss": 0.4002,
      "step": 2153
    },
    {
      "epoch": 0.6335294117647059,
      "grad_norm": 0.060306183993816376,
      "learning_rate": 7.34020618556701e-05,
      "loss": 0.3293,
      "step": 2154
    },
    {
      "epoch": 0.6338235294117647,
      "grad_norm": 0.04359021410346031,
      "learning_rate": 7.334315169366715e-05,
      "loss": 0.3064,
      "step": 2155
    },
    {
      "epoch": 0.6341176470588236,
      "grad_norm": 0.05521870404481888,
      "learning_rate": 7.328424153166421e-05,
      "loss": 0.4018,
      "step": 2156
    },
    {
      "epoch": 0.6344117647058823,
      "grad_norm": 0.056304991245269775,
      "learning_rate": 7.322533136966126e-05,
      "loss": 0.3438,
      "step": 2157
    },
    {
      "epoch": 0.6347058823529412,
      "grad_norm": 0.03452163189649582,
      "learning_rate": 7.316642120765832e-05,
      "loss": 0.2713,
      "step": 2158
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.04383242875337601,
      "learning_rate": 7.310751104565537e-05,
      "loss": 0.4085,
      "step": 2159
    },
    {
      "epoch": 0.6352941176470588,
      "grad_norm": 0.041975926607847214,
      "learning_rate": 7.304860088365243e-05,
      "loss": 0.3308,
      "step": 2160
    },
    {
      "epoch": 0.6355882352941177,
      "grad_norm": 0.035833682864904404,
      "learning_rate": 7.298969072164949e-05,
      "loss": 0.3048,
      "step": 2161
    },
    {
      "epoch": 0.6358823529411765,
      "grad_norm": 0.05351791903376579,
      "learning_rate": 7.293078055964654e-05,
      "loss": 0.3594,
      "step": 2162
    },
    {
      "epoch": 0.6361764705882353,
      "grad_norm": 0.05280531570315361,
      "learning_rate": 7.28718703976436e-05,
      "loss": 0.4067,
      "step": 2163
    },
    {
      "epoch": 0.6364705882352941,
      "grad_norm": 0.041301511228084564,
      "learning_rate": 7.281296023564064e-05,
      "loss": 0.3034,
      "step": 2164
    },
    {
      "epoch": 0.6367647058823529,
      "grad_norm": 0.040484718978405,
      "learning_rate": 7.27540500736377e-05,
      "loss": 0.3554,
      "step": 2165
    },
    {
      "epoch": 0.6370588235294118,
      "grad_norm": 0.060925524681806564,
      "learning_rate": 7.269513991163476e-05,
      "loss": 0.3928,
      "step": 2166
    },
    {
      "epoch": 0.6373529411764706,
      "grad_norm": 0.033312708139419556,
      "learning_rate": 7.263622974963181e-05,
      "loss": 0.2326,
      "step": 2167
    },
    {
      "epoch": 0.6376470588235295,
      "grad_norm": 0.03846779465675354,
      "learning_rate": 7.257731958762887e-05,
      "loss": 0.3522,
      "step": 2168
    },
    {
      "epoch": 0.6379411764705882,
      "grad_norm": 0.047232430428266525,
      "learning_rate": 7.251840942562592e-05,
      "loss": 0.3946,
      "step": 2169
    },
    {
      "epoch": 0.638235294117647,
      "grad_norm": 0.05111080780625343,
      "learning_rate": 7.245949926362298e-05,
      "loss": 0.3099,
      "step": 2170
    },
    {
      "epoch": 0.6385294117647059,
      "grad_norm": 0.05361463874578476,
      "learning_rate": 7.240058910162004e-05,
      "loss": 0.3825,
      "step": 2171
    },
    {
      "epoch": 0.6388235294117647,
      "grad_norm": 0.04113186150789261,
      "learning_rate": 7.234167893961708e-05,
      "loss": 0.303,
      "step": 2172
    },
    {
      "epoch": 0.6391176470588236,
      "grad_norm": 0.057009343057870865,
      "learning_rate": 7.228276877761414e-05,
      "loss": 0.369,
      "step": 2173
    },
    {
      "epoch": 0.6394117647058823,
      "grad_norm": 0.04292616620659828,
      "learning_rate": 7.222385861561119e-05,
      "loss": 0.3731,
      "step": 2174
    },
    {
      "epoch": 0.6397058823529411,
      "grad_norm": 0.038144659250974655,
      "learning_rate": 7.216494845360825e-05,
      "loss": 0.3083,
      "step": 2175
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.04849313199520111,
      "learning_rate": 7.210603829160531e-05,
      "loss": 0.3543,
      "step": 2176
    },
    {
      "epoch": 0.6402941176470588,
      "grad_norm": 0.050636786967515945,
      "learning_rate": 7.204712812960236e-05,
      "loss": 0.3501,
      "step": 2177
    },
    {
      "epoch": 0.6405882352941177,
      "grad_norm": 0.050235457718372345,
      "learning_rate": 7.198821796759942e-05,
      "loss": 0.4095,
      "step": 2178
    },
    {
      "epoch": 0.6408823529411765,
      "grad_norm": 0.05225961655378342,
      "learning_rate": 7.192930780559646e-05,
      "loss": 0.3391,
      "step": 2179
    },
    {
      "epoch": 0.6411764705882353,
      "grad_norm": 0.04203161597251892,
      "learning_rate": 7.187039764359352e-05,
      "loss": 0.2767,
      "step": 2180
    },
    {
      "epoch": 0.6414705882352941,
      "grad_norm": 0.04949195310473442,
      "learning_rate": 7.181148748159058e-05,
      "loss": 0.393,
      "step": 2181
    },
    {
      "epoch": 0.6417647058823529,
      "grad_norm": 0.054564788937568665,
      "learning_rate": 7.175257731958763e-05,
      "loss": 0.4082,
      "step": 2182
    },
    {
      "epoch": 0.6420588235294118,
      "grad_norm": 0.04095477983355522,
      "learning_rate": 7.169366715758469e-05,
      "loss": 0.3269,
      "step": 2183
    },
    {
      "epoch": 0.6423529411764706,
      "grad_norm": 0.036458615213632584,
      "learning_rate": 7.163475699558174e-05,
      "loss": 0.3011,
      "step": 2184
    },
    {
      "epoch": 0.6426470588235295,
      "grad_norm": 0.04074671119451523,
      "learning_rate": 7.15758468335788e-05,
      "loss": 0.3775,
      "step": 2185
    },
    {
      "epoch": 0.6429411764705882,
      "grad_norm": 0.04424959793686867,
      "learning_rate": 7.151693667157586e-05,
      "loss": 0.3509,
      "step": 2186
    },
    {
      "epoch": 0.643235294117647,
      "grad_norm": 0.03764531761407852,
      "learning_rate": 7.14580265095729e-05,
      "loss": 0.2857,
      "step": 2187
    },
    {
      "epoch": 0.6435294117647059,
      "grad_norm": 0.0524974949657917,
      "learning_rate": 7.139911634756996e-05,
      "loss": 0.3913,
      "step": 2188
    },
    {
      "epoch": 0.6438235294117647,
      "grad_norm": 0.04850577563047409,
      "learning_rate": 7.134020618556701e-05,
      "loss": 0.3793,
      "step": 2189
    },
    {
      "epoch": 0.6441176470588236,
      "grad_norm": 0.03890348970890045,
      "learning_rate": 7.128129602356407e-05,
      "loss": 0.3036,
      "step": 2190
    },
    {
      "epoch": 0.6444117647058824,
      "grad_norm": 0.038739219307899475,
      "learning_rate": 7.122238586156113e-05,
      "loss": 0.303,
      "step": 2191
    },
    {
      "epoch": 0.6447058823529411,
      "grad_norm": 0.04788711294531822,
      "learning_rate": 7.116347569955818e-05,
      "loss": 0.3665,
      "step": 2192
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.05236014351248741,
      "learning_rate": 7.110456553755524e-05,
      "loss": 0.3652,
      "step": 2193
    },
    {
      "epoch": 0.6452941176470588,
      "grad_norm": 0.048781901597976685,
      "learning_rate": 7.104565537555228e-05,
      "loss": 0.3317,
      "step": 2194
    },
    {
      "epoch": 0.6455882352941177,
      "grad_norm": 0.04119415581226349,
      "learning_rate": 7.098674521354934e-05,
      "loss": 0.31,
      "step": 2195
    },
    {
      "epoch": 0.6458823529411765,
      "grad_norm": 0.04253610596060753,
      "learning_rate": 7.09278350515464e-05,
      "loss": 0.36,
      "step": 2196
    },
    {
      "epoch": 0.6461764705882352,
      "grad_norm": 0.05680859461426735,
      "learning_rate": 7.086892488954345e-05,
      "loss": 0.3901,
      "step": 2197
    },
    {
      "epoch": 0.6464705882352941,
      "grad_norm": 0.06969771534204483,
      "learning_rate": 7.081001472754051e-05,
      "loss": 0.4039,
      "step": 2198
    },
    {
      "epoch": 0.6467647058823529,
      "grad_norm": 0.04569918289780617,
      "learning_rate": 7.075110456553756e-05,
      "loss": 0.3471,
      "step": 2199
    },
    {
      "epoch": 0.6470588235294118,
      "grad_norm": 0.04682646319270134,
      "learning_rate": 7.069219440353462e-05,
      "loss": 0.3676,
      "step": 2200
    },
    {
      "epoch": 0.6473529411764706,
      "grad_norm": 0.04738574102520943,
      "learning_rate": 7.063328424153168e-05,
      "loss": 0.3694,
      "step": 2201
    },
    {
      "epoch": 0.6476470588235295,
      "grad_norm": 0.04825076833367348,
      "learning_rate": 7.057437407952873e-05,
      "loss": 0.4145,
      "step": 2202
    },
    {
      "epoch": 0.6479411764705882,
      "grad_norm": 0.046834830194711685,
      "learning_rate": 7.051546391752579e-05,
      "loss": 0.3631,
      "step": 2203
    },
    {
      "epoch": 0.648235294117647,
      "grad_norm": 0.06408782303333282,
      "learning_rate": 7.045655375552283e-05,
      "loss": 0.4804,
      "step": 2204
    },
    {
      "epoch": 0.6485294117647059,
      "grad_norm": 0.05442212522029877,
      "learning_rate": 7.039764359351989e-05,
      "loss": 0.4001,
      "step": 2205
    },
    {
      "epoch": 0.6488235294117647,
      "grad_norm": 0.04834597185254097,
      "learning_rate": 7.033873343151695e-05,
      "loss": 0.3364,
      "step": 2206
    },
    {
      "epoch": 0.6491176470588236,
      "grad_norm": 0.052880022674798965,
      "learning_rate": 7.027982326951399e-05,
      "loss": 0.3723,
      "step": 2207
    },
    {
      "epoch": 0.6494117647058824,
      "grad_norm": 0.049329545348882675,
      "learning_rate": 7.022091310751105e-05,
      "loss": 0.3391,
      "step": 2208
    },
    {
      "epoch": 0.6497058823529411,
      "grad_norm": 0.049422528594732285,
      "learning_rate": 7.016200294550809e-05,
      "loss": 0.286,
      "step": 2209
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.045695677399635315,
      "learning_rate": 7.010309278350515e-05,
      "loss": 0.3015,
      "step": 2210
    },
    {
      "epoch": 0.6502941176470588,
      "grad_norm": 0.05491280555725098,
      "learning_rate": 7.004418262150221e-05,
      "loss": 0.3831,
      "step": 2211
    },
    {
      "epoch": 0.6505882352941177,
      "grad_norm": 0.0459819920361042,
      "learning_rate": 6.998527245949926e-05,
      "loss": 0.3126,
      "step": 2212
    },
    {
      "epoch": 0.6508823529411765,
      "grad_norm": 0.05441996827721596,
      "learning_rate": 6.992636229749632e-05,
      "loss": 0.3888,
      "step": 2213
    },
    {
      "epoch": 0.6511764705882352,
      "grad_norm": 0.04172290489077568,
      "learning_rate": 6.986745213549337e-05,
      "loss": 0.3419,
      "step": 2214
    },
    {
      "epoch": 0.6514705882352941,
      "grad_norm": 0.047594860196113586,
      "learning_rate": 6.980854197349043e-05,
      "loss": 0.3775,
      "step": 2215
    },
    {
      "epoch": 0.6517647058823529,
      "grad_norm": 0.034154247492551804,
      "learning_rate": 6.974963181148749e-05,
      "loss": 0.2757,
      "step": 2216
    },
    {
      "epoch": 0.6520588235294118,
      "grad_norm": 0.03893875703215599,
      "learning_rate": 6.969072164948453e-05,
      "loss": 0.3048,
      "step": 2217
    },
    {
      "epoch": 0.6523529411764706,
      "grad_norm": 0.028835929930210114,
      "learning_rate": 6.963181148748159e-05,
      "loss": 0.2247,
      "step": 2218
    },
    {
      "epoch": 0.6526470588235294,
      "grad_norm": 0.048821330070495605,
      "learning_rate": 6.957290132547864e-05,
      "loss": 0.4116,
      "step": 2219
    },
    {
      "epoch": 0.6529411764705882,
      "grad_norm": 0.05292560160160065,
      "learning_rate": 6.95139911634757e-05,
      "loss": 0.3734,
      "step": 2220
    },
    {
      "epoch": 0.653235294117647,
      "grad_norm": 0.06115587428212166,
      "learning_rate": 6.945508100147276e-05,
      "loss": 0.3737,
      "step": 2221
    },
    {
      "epoch": 0.6535294117647059,
      "grad_norm": 0.04480207338929176,
      "learning_rate": 6.93961708394698e-05,
      "loss": 0.3856,
      "step": 2222
    },
    {
      "epoch": 0.6538235294117647,
      "grad_norm": 0.03986913710832596,
      "learning_rate": 6.933726067746687e-05,
      "loss": 0.3598,
      "step": 2223
    },
    {
      "epoch": 0.6541176470588236,
      "grad_norm": 0.04760652780532837,
      "learning_rate": 6.927835051546391e-05,
      "loss": 0.383,
      "step": 2224
    },
    {
      "epoch": 0.6544117647058824,
      "grad_norm": 0.04260959103703499,
      "learning_rate": 6.921944035346097e-05,
      "loss": 0.3299,
      "step": 2225
    },
    {
      "epoch": 0.6547058823529411,
      "grad_norm": 0.04339681193232536,
      "learning_rate": 6.916053019145803e-05,
      "loss": 0.3368,
      "step": 2226
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.06066949665546417,
      "learning_rate": 6.910162002945508e-05,
      "loss": 0.4406,
      "step": 2227
    },
    {
      "epoch": 0.6552941176470588,
      "grad_norm": 0.04734734818339348,
      "learning_rate": 6.904270986745214e-05,
      "loss": 0.378,
      "step": 2228
    },
    {
      "epoch": 0.6555882352941177,
      "grad_norm": 0.04576979950070381,
      "learning_rate": 6.898379970544919e-05,
      "loss": 0.4081,
      "step": 2229
    },
    {
      "epoch": 0.6558823529411765,
      "grad_norm": 0.032953985035419464,
      "learning_rate": 6.892488954344625e-05,
      "loss": 0.3079,
      "step": 2230
    },
    {
      "epoch": 0.6561764705882352,
      "grad_norm": 0.04657362401485443,
      "learning_rate": 6.886597938144331e-05,
      "loss": 0.3801,
      "step": 2231
    },
    {
      "epoch": 0.6564705882352941,
      "grad_norm": 0.048387110233306885,
      "learning_rate": 6.880706921944035e-05,
      "loss": 0.3286,
      "step": 2232
    },
    {
      "epoch": 0.6567647058823529,
      "grad_norm": 0.0447160005569458,
      "learning_rate": 6.874815905743741e-05,
      "loss": 0.3705,
      "step": 2233
    },
    {
      "epoch": 0.6570588235294118,
      "grad_norm": 0.040296219289302826,
      "learning_rate": 6.868924889543446e-05,
      "loss": 0.2963,
      "step": 2234
    },
    {
      "epoch": 0.6573529411764706,
      "grad_norm": 0.0415634885430336,
      "learning_rate": 6.863033873343152e-05,
      "loss": 0.3098,
      "step": 2235
    },
    {
      "epoch": 0.6576470588235294,
      "grad_norm": 0.05981420353055,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.3962,
      "step": 2236
    },
    {
      "epoch": 0.6579411764705883,
      "grad_norm": 0.0573529414832592,
      "learning_rate": 6.851251840942563e-05,
      "loss": 0.3861,
      "step": 2237
    },
    {
      "epoch": 0.658235294117647,
      "grad_norm": 0.039172179996967316,
      "learning_rate": 6.845360824742269e-05,
      "loss": 0.279,
      "step": 2238
    },
    {
      "epoch": 0.6585294117647059,
      "grad_norm": 0.04957537725567818,
      "learning_rate": 6.839469808541973e-05,
      "loss": 0.4602,
      "step": 2239
    },
    {
      "epoch": 0.6588235294117647,
      "grad_norm": 0.05606165900826454,
      "learning_rate": 6.83357879234168e-05,
      "loss": 0.3521,
      "step": 2240
    },
    {
      "epoch": 0.6591176470588235,
      "grad_norm": 0.06649676710367203,
      "learning_rate": 6.827687776141385e-05,
      "loss": 0.4439,
      "step": 2241
    },
    {
      "epoch": 0.6594117647058824,
      "grad_norm": 0.04549049586057663,
      "learning_rate": 6.82179675994109e-05,
      "loss": 0.3459,
      "step": 2242
    },
    {
      "epoch": 0.6597058823529411,
      "grad_norm": 0.04952708259224892,
      "learning_rate": 6.815905743740796e-05,
      "loss": 0.3616,
      "step": 2243
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.03840707242488861,
      "learning_rate": 6.810014727540501e-05,
      "loss": 0.2885,
      "step": 2244
    },
    {
      "epoch": 0.6602941176470588,
      "grad_norm": 0.0364496111869812,
      "learning_rate": 6.804123711340207e-05,
      "loss": 0.317,
      "step": 2245
    },
    {
      "epoch": 0.6605882352941177,
      "grad_norm": 0.05562179908156395,
      "learning_rate": 6.798232695139913e-05,
      "loss": 0.3827,
      "step": 2246
    },
    {
      "epoch": 0.6608823529411765,
      "grad_norm": 0.038867007941007614,
      "learning_rate": 6.792341678939617e-05,
      "loss": 0.2795,
      "step": 2247
    },
    {
      "epoch": 0.6611764705882353,
      "grad_norm": 0.048590175807476044,
      "learning_rate": 6.786450662739324e-05,
      "loss": 0.3766,
      "step": 2248
    },
    {
      "epoch": 0.6614705882352941,
      "grad_norm": 0.046917811036109924,
      "learning_rate": 6.780559646539028e-05,
      "loss": 0.3742,
      "step": 2249
    },
    {
      "epoch": 0.6617647058823529,
      "grad_norm": 0.04268498718738556,
      "learning_rate": 6.774668630338734e-05,
      "loss": 0.2708,
      "step": 2250
    },
    {
      "epoch": 0.6620588235294118,
      "grad_norm": 0.04579256474971771,
      "learning_rate": 6.76877761413844e-05,
      "loss": 0.3686,
      "step": 2251
    },
    {
      "epoch": 0.6623529411764706,
      "grad_norm": 0.04920322075486183,
      "learning_rate": 6.762886597938145e-05,
      "loss": 0.3567,
      "step": 2252
    },
    {
      "epoch": 0.6626470588235294,
      "grad_norm": 0.03553129360079765,
      "learning_rate": 6.756995581737851e-05,
      "loss": 0.2495,
      "step": 2253
    },
    {
      "epoch": 0.6629411764705883,
      "grad_norm": 0.04399888217449188,
      "learning_rate": 6.751104565537556e-05,
      "loss": 0.3856,
      "step": 2254
    },
    {
      "epoch": 0.663235294117647,
      "grad_norm": 0.050932712852954865,
      "learning_rate": 6.745213549337262e-05,
      "loss": 0.3194,
      "step": 2255
    },
    {
      "epoch": 0.6635294117647059,
      "grad_norm": 0.043192021548748016,
      "learning_rate": 6.739322533136968e-05,
      "loss": 0.3247,
      "step": 2256
    },
    {
      "epoch": 0.6638235294117647,
      "grad_norm": 0.04562203213572502,
      "learning_rate": 6.733431516936672e-05,
      "loss": 0.3477,
      "step": 2257
    },
    {
      "epoch": 0.6641176470588235,
      "grad_norm": 0.06173831596970558,
      "learning_rate": 6.727540500736378e-05,
      "loss": 0.3571,
      "step": 2258
    },
    {
      "epoch": 0.6644117647058824,
      "grad_norm": 0.04510433226823807,
      "learning_rate": 6.721649484536083e-05,
      "loss": 0.2881,
      "step": 2259
    },
    {
      "epoch": 0.6647058823529411,
      "grad_norm": 0.053753335028886795,
      "learning_rate": 6.715758468335788e-05,
      "loss": 0.3672,
      "step": 2260
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.03818830847740173,
      "learning_rate": 6.709867452135494e-05,
      "loss": 0.3044,
      "step": 2261
    },
    {
      "epoch": 0.6652941176470588,
      "grad_norm": 0.03532753139734268,
      "learning_rate": 6.703976435935198e-05,
      "loss": 0.3047,
      "step": 2262
    },
    {
      "epoch": 0.6655882352941176,
      "grad_norm": 0.04885559156537056,
      "learning_rate": 6.698085419734904e-05,
      "loss": 0.3512,
      "step": 2263
    },
    {
      "epoch": 0.6658823529411765,
      "grad_norm": 0.0507655069231987,
      "learning_rate": 6.692194403534609e-05,
      "loss": 0.3322,
      "step": 2264
    },
    {
      "epoch": 0.6661764705882353,
      "grad_norm": 0.03809826076030731,
      "learning_rate": 6.686303387334315e-05,
      "loss": 0.331,
      "step": 2265
    },
    {
      "epoch": 0.6664705882352941,
      "grad_norm": 0.06184301897883415,
      "learning_rate": 6.680412371134021e-05,
      "loss": 0.4019,
      "step": 2266
    },
    {
      "epoch": 0.6667647058823529,
      "grad_norm": 0.04973474144935608,
      "learning_rate": 6.674521354933726e-05,
      "loss": 0.33,
      "step": 2267
    },
    {
      "epoch": 0.6670588235294118,
      "grad_norm": 0.05633837357163429,
      "learning_rate": 6.668630338733432e-05,
      "loss": 0.3752,
      "step": 2268
    },
    {
      "epoch": 0.6673529411764706,
      "grad_norm": 0.06594084948301315,
      "learning_rate": 6.662739322533136e-05,
      "loss": 0.4695,
      "step": 2269
    },
    {
      "epoch": 0.6676470588235294,
      "grad_norm": 0.03862633928656578,
      "learning_rate": 6.656848306332842e-05,
      "loss": 0.2968,
      "step": 2270
    },
    {
      "epoch": 0.6679411764705883,
      "grad_norm": 0.07687505334615707,
      "learning_rate": 6.650957290132548e-05,
      "loss": 0.4626,
      "step": 2271
    },
    {
      "epoch": 0.668235294117647,
      "grad_norm": 0.03391765058040619,
      "learning_rate": 6.645066273932253e-05,
      "loss": 0.2678,
      "step": 2272
    },
    {
      "epoch": 0.6685294117647059,
      "grad_norm": 0.038430970162153244,
      "learning_rate": 6.639175257731959e-05,
      "loss": 0.3599,
      "step": 2273
    },
    {
      "epoch": 0.6688235294117647,
      "grad_norm": 0.05139806494116783,
      "learning_rate": 6.633284241531664e-05,
      "loss": 0.312,
      "step": 2274
    },
    {
      "epoch": 0.6691176470588235,
      "grad_norm": 0.04405580461025238,
      "learning_rate": 6.62739322533137e-05,
      "loss": 0.3292,
      "step": 2275
    },
    {
      "epoch": 0.6694117647058824,
      "grad_norm": 0.05784444138407707,
      "learning_rate": 6.621502209131076e-05,
      "loss": 0.3365,
      "step": 2276
    },
    {
      "epoch": 0.6697058823529412,
      "grad_norm": 0.03843054920434952,
      "learning_rate": 6.61561119293078e-05,
      "loss": 0.3466,
      "step": 2277
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.04907367378473282,
      "learning_rate": 6.609720176730486e-05,
      "loss": 0.3077,
      "step": 2278
    },
    {
      "epoch": 0.6702941176470588,
      "grad_norm": 0.053056929260492325,
      "learning_rate": 6.603829160530191e-05,
      "loss": 0.3417,
      "step": 2279
    },
    {
      "epoch": 0.6705882352941176,
      "grad_norm": 0.051893677562475204,
      "learning_rate": 6.597938144329897e-05,
      "loss": 0.4127,
      "step": 2280
    },
    {
      "epoch": 0.6708823529411765,
      "grad_norm": 0.04868094623088837,
      "learning_rate": 6.592047128129603e-05,
      "loss": 0.3146,
      "step": 2281
    },
    {
      "epoch": 0.6711764705882353,
      "grad_norm": 0.0553760752081871,
      "learning_rate": 6.586156111929308e-05,
      "loss": 0.3415,
      "step": 2282
    },
    {
      "epoch": 0.6714705882352942,
      "grad_norm": 0.039330147206783295,
      "learning_rate": 6.580265095729014e-05,
      "loss": 0.2871,
      "step": 2283
    },
    {
      "epoch": 0.6717647058823529,
      "grad_norm": 0.04735511168837547,
      "learning_rate": 6.574374079528718e-05,
      "loss": 0.3704,
      "step": 2284
    },
    {
      "epoch": 0.6720588235294118,
      "grad_norm": 0.052121106535196304,
      "learning_rate": 6.568483063328424e-05,
      "loss": 0.3499,
      "step": 2285
    },
    {
      "epoch": 0.6723529411764706,
      "grad_norm": 0.04294152930378914,
      "learning_rate": 6.56259204712813e-05,
      "loss": 0.3735,
      "step": 2286
    },
    {
      "epoch": 0.6726470588235294,
      "grad_norm": 0.04196895658969879,
      "learning_rate": 6.556701030927835e-05,
      "loss": 0.3088,
      "step": 2287
    },
    {
      "epoch": 0.6729411764705883,
      "grad_norm": 0.053609222173690796,
      "learning_rate": 6.550810014727541e-05,
      "loss": 0.3573,
      "step": 2288
    },
    {
      "epoch": 0.673235294117647,
      "grad_norm": 0.03842151537537575,
      "learning_rate": 6.544918998527246e-05,
      "loss": 0.3099,
      "step": 2289
    },
    {
      "epoch": 0.6735294117647059,
      "grad_norm": 0.0590580552816391,
      "learning_rate": 6.539027982326952e-05,
      "loss": 0.3987,
      "step": 2290
    },
    {
      "epoch": 0.6738235294117647,
      "grad_norm": 0.036782220005989075,
      "learning_rate": 6.533136966126658e-05,
      "loss": 0.3234,
      "step": 2291
    },
    {
      "epoch": 0.6741176470588235,
      "grad_norm": 0.05229595676064491,
      "learning_rate": 6.527245949926362e-05,
      "loss": 0.4105,
      "step": 2292
    },
    {
      "epoch": 0.6744117647058824,
      "grad_norm": 0.04691304638981819,
      "learning_rate": 6.521354933726069e-05,
      "loss": 0.3482,
      "step": 2293
    },
    {
      "epoch": 0.6747058823529412,
      "grad_norm": 0.04115607589483261,
      "learning_rate": 6.515463917525773e-05,
      "loss": 0.2753,
      "step": 2294
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.03722958639264107,
      "learning_rate": 6.509572901325479e-05,
      "loss": 0.2658,
      "step": 2295
    },
    {
      "epoch": 0.6752941176470588,
      "grad_norm": 0.047147296369075775,
      "learning_rate": 6.503681885125185e-05,
      "loss": 0.3086,
      "step": 2296
    },
    {
      "epoch": 0.6755882352941176,
      "grad_norm": 0.04445239156484604,
      "learning_rate": 6.49779086892489e-05,
      "loss": 0.3114,
      "step": 2297
    },
    {
      "epoch": 0.6758823529411765,
      "grad_norm": 0.03904902935028076,
      "learning_rate": 6.491899852724596e-05,
      "loss": 0.293,
      "step": 2298
    },
    {
      "epoch": 0.6761764705882353,
      "grad_norm": 0.04315778985619545,
      "learning_rate": 6.4860088365243e-05,
      "loss": 0.3272,
      "step": 2299
    },
    {
      "epoch": 0.6764705882352942,
      "grad_norm": 0.05673079565167427,
      "learning_rate": 6.480117820324007e-05,
      "loss": 0.4071,
      "step": 2300
    },
    {
      "epoch": 0.6767647058823529,
      "grad_norm": 0.05413137376308441,
      "learning_rate": 6.474226804123713e-05,
      "loss": 0.395,
      "step": 2301
    },
    {
      "epoch": 0.6770588235294117,
      "grad_norm": 0.05576680228114128,
      "learning_rate": 6.468335787923417e-05,
      "loss": 0.4296,
      "step": 2302
    },
    {
      "epoch": 0.6773529411764706,
      "grad_norm": 0.05676041543483734,
      "learning_rate": 6.462444771723123e-05,
      "loss": 0.3291,
      "step": 2303
    },
    {
      "epoch": 0.6776470588235294,
      "grad_norm": 0.053087327629327774,
      "learning_rate": 6.456553755522828e-05,
      "loss": 0.3732,
      "step": 2304
    },
    {
      "epoch": 0.6779411764705883,
      "grad_norm": 0.04427069425582886,
      "learning_rate": 6.450662739322534e-05,
      "loss": 0.3705,
      "step": 2305
    },
    {
      "epoch": 0.678235294117647,
      "grad_norm": 0.05099189653992653,
      "learning_rate": 6.44477172312224e-05,
      "loss": 0.3809,
      "step": 2306
    },
    {
      "epoch": 0.6785294117647059,
      "grad_norm": 0.03889943286776543,
      "learning_rate": 6.438880706921945e-05,
      "loss": 0.3483,
      "step": 2307
    },
    {
      "epoch": 0.6788235294117647,
      "grad_norm": 0.0633288249373436,
      "learning_rate": 6.43298969072165e-05,
      "loss": 0.4041,
      "step": 2308
    },
    {
      "epoch": 0.6791176470588235,
      "grad_norm": 0.03785555437207222,
      "learning_rate": 6.427098674521355e-05,
      "loss": 0.2975,
      "step": 2309
    },
    {
      "epoch": 0.6794117647058824,
      "grad_norm": 0.041693028062582016,
      "learning_rate": 6.421207658321061e-05,
      "loss": 0.3001,
      "step": 2310
    },
    {
      "epoch": 0.6797058823529412,
      "grad_norm": 0.04556373879313469,
      "learning_rate": 6.415316642120767e-05,
      "loss": 0.385,
      "step": 2311
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.05370691418647766,
      "learning_rate": 6.409425625920472e-05,
      "loss": 0.4184,
      "step": 2312
    },
    {
      "epoch": 0.6802941176470588,
      "grad_norm": 0.046437520533800125,
      "learning_rate": 6.403534609720177e-05,
      "loss": 0.3451,
      "step": 2313
    },
    {
      "epoch": 0.6805882352941176,
      "grad_norm": 0.04158703610301018,
      "learning_rate": 6.397643593519881e-05,
      "loss": 0.3023,
      "step": 2314
    },
    {
      "epoch": 0.6808823529411765,
      "grad_norm": 0.05313799902796745,
      "learning_rate": 6.391752577319587e-05,
      "loss": 0.3708,
      "step": 2315
    },
    {
      "epoch": 0.6811764705882353,
      "grad_norm": 0.03953718766570091,
      "learning_rate": 6.385861561119293e-05,
      "loss": 0.3212,
      "step": 2316
    },
    {
      "epoch": 0.6814705882352942,
      "grad_norm": 0.05626191943883896,
      "learning_rate": 6.379970544918998e-05,
      "loss": 0.3751,
      "step": 2317
    },
    {
      "epoch": 0.6817647058823529,
      "grad_norm": 0.0464593768119812,
      "learning_rate": 6.374079528718704e-05,
      "loss": 0.3591,
      "step": 2318
    },
    {
      "epoch": 0.6820588235294117,
      "grad_norm": 0.04394601657986641,
      "learning_rate": 6.368188512518409e-05,
      "loss": 0.2945,
      "step": 2319
    },
    {
      "epoch": 0.6823529411764706,
      "grad_norm": 0.044584017246961594,
      "learning_rate": 6.362297496318115e-05,
      "loss": 0.3577,
      "step": 2320
    },
    {
      "epoch": 0.6826470588235294,
      "grad_norm": 0.05000860244035721,
      "learning_rate": 6.356406480117821e-05,
      "loss": 0.3945,
      "step": 2321
    },
    {
      "epoch": 0.6829411764705883,
      "grad_norm": 0.05670865997672081,
      "learning_rate": 6.350515463917525e-05,
      "loss": 0.3381,
      "step": 2322
    },
    {
      "epoch": 0.683235294117647,
      "grad_norm": 0.042193107306957245,
      "learning_rate": 6.344624447717231e-05,
      "loss": 0.2661,
      "step": 2323
    },
    {
      "epoch": 0.6835294117647058,
      "grad_norm": 0.05842498317360878,
      "learning_rate": 6.338733431516936e-05,
      "loss": 0.3968,
      "step": 2324
    },
    {
      "epoch": 0.6838235294117647,
      "grad_norm": 0.046069659292697906,
      "learning_rate": 6.332842415316642e-05,
      "loss": 0.3436,
      "step": 2325
    },
    {
      "epoch": 0.6841176470588235,
      "grad_norm": 0.034174591302871704,
      "learning_rate": 6.326951399116348e-05,
      "loss": 0.2911,
      "step": 2326
    },
    {
      "epoch": 0.6844117647058824,
      "grad_norm": 0.061209529638290405,
      "learning_rate": 6.321060382916053e-05,
      "loss": 0.3663,
      "step": 2327
    },
    {
      "epoch": 0.6847058823529412,
      "grad_norm": 0.04360613599419594,
      "learning_rate": 6.315169366715759e-05,
      "loss": 0.3006,
      "step": 2328
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.04109768569469452,
      "learning_rate": 6.309278350515463e-05,
      "loss": 0.3228,
      "step": 2329
    },
    {
      "epoch": 0.6852941176470588,
      "grad_norm": 0.04106299951672554,
      "learning_rate": 6.30338733431517e-05,
      "loss": 0.2977,
      "step": 2330
    },
    {
      "epoch": 0.6855882352941176,
      "grad_norm": 0.051979709416627884,
      "learning_rate": 6.297496318114875e-05,
      "loss": 0.2798,
      "step": 2331
    },
    {
      "epoch": 0.6858823529411765,
      "grad_norm": 0.048644497990608215,
      "learning_rate": 6.29160530191458e-05,
      "loss": 0.3433,
      "step": 2332
    },
    {
      "epoch": 0.6861764705882353,
      "grad_norm": 0.059174515306949615,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.3975,
      "step": 2333
    },
    {
      "epoch": 0.6864705882352942,
      "grad_norm": 0.05103588476777077,
      "learning_rate": 6.279823269513991e-05,
      "loss": 0.4051,
      "step": 2334
    },
    {
      "epoch": 0.6867647058823529,
      "grad_norm": 0.046287838369607925,
      "learning_rate": 6.273932253313697e-05,
      "loss": 0.3547,
      "step": 2335
    },
    {
      "epoch": 0.6870588235294117,
      "grad_norm": 0.0645025447010994,
      "learning_rate": 6.268041237113403e-05,
      "loss": 0.419,
      "step": 2336
    },
    {
      "epoch": 0.6873529411764706,
      "grad_norm": 0.0507858581840992,
      "learning_rate": 6.262150220913107e-05,
      "loss": 0.3775,
      "step": 2337
    },
    {
      "epoch": 0.6876470588235294,
      "grad_norm": 0.046967778354883194,
      "learning_rate": 6.256259204712814e-05,
      "loss": 0.2883,
      "step": 2338
    },
    {
      "epoch": 0.6879411764705883,
      "grad_norm": 0.03161153197288513,
      "learning_rate": 6.250368188512518e-05,
      "loss": 0.1993,
      "step": 2339
    },
    {
      "epoch": 0.6882352941176471,
      "grad_norm": 0.056099146604537964,
      "learning_rate": 6.244477172312224e-05,
      "loss": 0.4068,
      "step": 2340
    },
    {
      "epoch": 0.6885294117647058,
      "grad_norm": 0.04975591227412224,
      "learning_rate": 6.23858615611193e-05,
      "loss": 0.4011,
      "step": 2341
    },
    {
      "epoch": 0.6888235294117647,
      "grad_norm": 0.045653700828552246,
      "learning_rate": 6.232695139911635e-05,
      "loss": 0.3476,
      "step": 2342
    },
    {
      "epoch": 0.6891176470588235,
      "grad_norm": 0.0544707328081131,
      "learning_rate": 6.226804123711341e-05,
      "loss": 0.3661,
      "step": 2343
    },
    {
      "epoch": 0.6894117647058824,
      "grad_norm": 0.04306691512465477,
      "learning_rate": 6.220913107511046e-05,
      "loss": 0.3151,
      "step": 2344
    },
    {
      "epoch": 0.6897058823529412,
      "grad_norm": 0.046091482043266296,
      "learning_rate": 6.215022091310752e-05,
      "loss": 0.3855,
      "step": 2345
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.04660331830382347,
      "learning_rate": 6.209131075110458e-05,
      "loss": 0.3905,
      "step": 2346
    },
    {
      "epoch": 0.6902941176470588,
      "grad_norm": 0.05369610711932182,
      "learning_rate": 6.203240058910162e-05,
      "loss": 0.3881,
      "step": 2347
    },
    {
      "epoch": 0.6905882352941176,
      "grad_norm": 0.0438065342605114,
      "learning_rate": 6.197349042709868e-05,
      "loss": 0.2661,
      "step": 2348
    },
    {
      "epoch": 0.6908823529411765,
      "grad_norm": 0.04576045274734497,
      "learning_rate": 6.191458026509573e-05,
      "loss": 0.3054,
      "step": 2349
    },
    {
      "epoch": 0.6911764705882353,
      "grad_norm": 0.05288995802402496,
      "learning_rate": 6.185567010309279e-05,
      "loss": 0.3312,
      "step": 2350
    },
    {
      "epoch": 0.6914705882352942,
      "grad_norm": 0.04070980101823807,
      "learning_rate": 6.179675994108985e-05,
      "loss": 0.3429,
      "step": 2351
    },
    {
      "epoch": 0.691764705882353,
      "grad_norm": 0.049375247210264206,
      "learning_rate": 6.17378497790869e-05,
      "loss": 0.3553,
      "step": 2352
    },
    {
      "epoch": 0.6920588235294117,
      "grad_norm": 0.03784918040037155,
      "learning_rate": 6.167893961708396e-05,
      "loss": 0.3384,
      "step": 2353
    },
    {
      "epoch": 0.6923529411764706,
      "grad_norm": 0.036860015243291855,
      "learning_rate": 6.1620029455081e-05,
      "loss": 0.3032,
      "step": 2354
    },
    {
      "epoch": 0.6926470588235294,
      "grad_norm": 0.037697453051805496,
      "learning_rate": 6.156111929307806e-05,
      "loss": 0.3141,
      "step": 2355
    },
    {
      "epoch": 0.6929411764705883,
      "grad_norm": 0.04364199936389923,
      "learning_rate": 6.150220913107512e-05,
      "loss": 0.3638,
      "step": 2356
    },
    {
      "epoch": 0.6932352941176471,
      "grad_norm": 0.03766687959432602,
      "learning_rate": 6.144329896907217e-05,
      "loss": 0.3227,
      "step": 2357
    },
    {
      "epoch": 0.6935294117647058,
      "grad_norm": 0.048196371644735336,
      "learning_rate": 6.138438880706923e-05,
      "loss": 0.3903,
      "step": 2358
    },
    {
      "epoch": 0.6938235294117647,
      "grad_norm": 0.04439029097557068,
      "learning_rate": 6.132547864506628e-05,
      "loss": 0.3434,
      "step": 2359
    },
    {
      "epoch": 0.6941176470588235,
      "grad_norm": 0.058961980044841766,
      "learning_rate": 6.126656848306334e-05,
      "loss": 0.39,
      "step": 2360
    },
    {
      "epoch": 0.6944117647058824,
      "grad_norm": 0.03878915309906006,
      "learning_rate": 6.12076583210604e-05,
      "loss": 0.3363,
      "step": 2361
    },
    {
      "epoch": 0.6947058823529412,
      "grad_norm": 0.04494589567184448,
      "learning_rate": 6.114874815905744e-05,
      "loss": 0.3814,
      "step": 2362
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.04802444949746132,
      "learning_rate": 6.10898379970545e-05,
      "loss": 0.2752,
      "step": 2363
    },
    {
      "epoch": 0.6952941176470588,
      "grad_norm": 0.03743802011013031,
      "learning_rate": 6.103092783505156e-05,
      "loss": 0.3032,
      "step": 2364
    },
    {
      "epoch": 0.6955882352941176,
      "grad_norm": 0.04589886963367462,
      "learning_rate": 6.097201767304861e-05,
      "loss": 0.3576,
      "step": 2365
    },
    {
      "epoch": 0.6958823529411765,
      "grad_norm": 0.06673473119735718,
      "learning_rate": 6.091310751104565e-05,
      "loss": 0.3635,
      "step": 2366
    },
    {
      "epoch": 0.6961764705882353,
      "grad_norm": 0.04117496311664581,
      "learning_rate": 6.085419734904271e-05,
      "loss": 0.3636,
      "step": 2367
    },
    {
      "epoch": 0.6964705882352941,
      "grad_norm": 0.04226019233465195,
      "learning_rate": 6.0795287187039764e-05,
      "loss": 0.3457,
      "step": 2368
    },
    {
      "epoch": 0.696764705882353,
      "grad_norm": 0.04297720640897751,
      "learning_rate": 6.073637702503682e-05,
      "loss": 0.3064,
      "step": 2369
    },
    {
      "epoch": 0.6970588235294117,
      "grad_norm": 0.062323249876499176,
      "learning_rate": 6.067746686303387e-05,
      "loss": 0.387,
      "step": 2370
    },
    {
      "epoch": 0.6973529411764706,
      "grad_norm": 0.06237587705254555,
      "learning_rate": 6.0618556701030924e-05,
      "loss": 0.4365,
      "step": 2371
    },
    {
      "epoch": 0.6976470588235294,
      "grad_norm": 0.05049051716923714,
      "learning_rate": 6.0559646539027984e-05,
      "loss": 0.2831,
      "step": 2372
    },
    {
      "epoch": 0.6979411764705883,
      "grad_norm": 0.04006141796708107,
      "learning_rate": 6.050073637702504e-05,
      "loss": 0.3229,
      "step": 2373
    },
    {
      "epoch": 0.6982352941176471,
      "grad_norm": 0.049293115735054016,
      "learning_rate": 6.044182621502209e-05,
      "loss": 0.319,
      "step": 2374
    },
    {
      "epoch": 0.6985294117647058,
      "grad_norm": 0.05624872446060181,
      "learning_rate": 6.0382916053019144e-05,
      "loss": 0.3999,
      "step": 2375
    },
    {
      "epoch": 0.6988235294117647,
      "grad_norm": 0.0511152446269989,
      "learning_rate": 6.03240058910162e-05,
      "loss": 0.3371,
      "step": 2376
    },
    {
      "epoch": 0.6991176470588235,
      "grad_norm": 0.05412766709923744,
      "learning_rate": 6.026509572901326e-05,
      "loss": 0.367,
      "step": 2377
    },
    {
      "epoch": 0.6994117647058824,
      "grad_norm": 0.04362644627690315,
      "learning_rate": 6.020618556701031e-05,
      "loss": 0.3354,
      "step": 2378
    },
    {
      "epoch": 0.6997058823529412,
      "grad_norm": 0.04027038440108299,
      "learning_rate": 6.0147275405007365e-05,
      "loss": 0.3305,
      "step": 2379
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.06162538006901741,
      "learning_rate": 6.008836524300442e-05,
      "loss": 0.4464,
      "step": 2380
    },
    {
      "epoch": 0.7002941176470588,
      "grad_norm": 0.048826225101947784,
      "learning_rate": 6.002945508100147e-05,
      "loss": 0.3855,
      "step": 2381
    },
    {
      "epoch": 0.7005882352941176,
      "grad_norm": 0.0497182160615921,
      "learning_rate": 5.9970544918998525e-05,
      "loss": 0.4152,
      "step": 2382
    },
    {
      "epoch": 0.7008823529411765,
      "grad_norm": 0.03771822527050972,
      "learning_rate": 5.9911634756995585e-05,
      "loss": 0.3368,
      "step": 2383
    },
    {
      "epoch": 0.7011764705882353,
      "grad_norm": 0.03511820361018181,
      "learning_rate": 5.985272459499264e-05,
      "loss": 0.3058,
      "step": 2384
    },
    {
      "epoch": 0.7014705882352941,
      "grad_norm": 0.047175098210573196,
      "learning_rate": 5.979381443298969e-05,
      "loss": 0.3317,
      "step": 2385
    },
    {
      "epoch": 0.701764705882353,
      "grad_norm": 0.059390485286712646,
      "learning_rate": 5.9734904270986745e-05,
      "loss": 0.4057,
      "step": 2386
    },
    {
      "epoch": 0.7020588235294117,
      "grad_norm": 0.04016943275928497,
      "learning_rate": 5.96759941089838e-05,
      "loss": 0.33,
      "step": 2387
    },
    {
      "epoch": 0.7023529411764706,
      "grad_norm": 0.06207844242453575,
      "learning_rate": 5.961708394698086e-05,
      "loss": 0.3772,
      "step": 2388
    },
    {
      "epoch": 0.7026470588235294,
      "grad_norm": 0.04080841317772865,
      "learning_rate": 5.955817378497791e-05,
      "loss": 0.2946,
      "step": 2389
    },
    {
      "epoch": 0.7029411764705882,
      "grad_norm": 0.06300457566976547,
      "learning_rate": 5.9499263622974965e-05,
      "loss": 0.3511,
      "step": 2390
    },
    {
      "epoch": 0.7032352941176471,
      "grad_norm": 0.0425124317407608,
      "learning_rate": 5.944035346097202e-05,
      "loss": 0.3162,
      "step": 2391
    },
    {
      "epoch": 0.7035294117647058,
      "grad_norm": 0.04151857644319534,
      "learning_rate": 5.938144329896907e-05,
      "loss": 0.2766,
      "step": 2392
    },
    {
      "epoch": 0.7038235294117647,
      "grad_norm": 0.056678734719753265,
      "learning_rate": 5.932253313696613e-05,
      "loss": 0.3877,
      "step": 2393
    },
    {
      "epoch": 0.7041176470588235,
      "grad_norm": 0.05333526432514191,
      "learning_rate": 5.9263622974963186e-05,
      "loss": 0.3426,
      "step": 2394
    },
    {
      "epoch": 0.7044117647058824,
      "grad_norm": 0.04449830949306488,
      "learning_rate": 5.920471281296024e-05,
      "loss": 0.345,
      "step": 2395
    },
    {
      "epoch": 0.7047058823529412,
      "grad_norm": 0.04679960384964943,
      "learning_rate": 5.914580265095729e-05,
      "loss": 0.3568,
      "step": 2396
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.051450006663799286,
      "learning_rate": 5.9086892488954346e-05,
      "loss": 0.4074,
      "step": 2397
    },
    {
      "epoch": 0.7052941176470588,
      "grad_norm": 0.04128715395927429,
      "learning_rate": 5.9027982326951406e-05,
      "loss": 0.3167,
      "step": 2398
    },
    {
      "epoch": 0.7055882352941176,
      "grad_norm": 0.04115470126271248,
      "learning_rate": 5.896907216494846e-05,
      "loss": 0.3399,
      "step": 2399
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.04746091738343239,
      "learning_rate": 5.891016200294551e-05,
      "loss": 0.3032,
      "step": 2400
    },
    {
      "epoch": 0.7061764705882353,
      "grad_norm": 0.04456372559070587,
      "learning_rate": 5.8851251840942566e-05,
      "loss": 0.3375,
      "step": 2401
    },
    {
      "epoch": 0.7064705882352941,
      "grad_norm": 0.08263371139764786,
      "learning_rate": 5.879234167893962e-05,
      "loss": 0.4223,
      "step": 2402
    },
    {
      "epoch": 0.706764705882353,
      "grad_norm": 0.034187979996204376,
      "learning_rate": 5.873343151693668e-05,
      "loss": 0.297,
      "step": 2403
    },
    {
      "epoch": 0.7070588235294117,
      "grad_norm": 0.04027097299695015,
      "learning_rate": 5.867452135493373e-05,
      "loss": 0.3196,
      "step": 2404
    },
    {
      "epoch": 0.7073529411764706,
      "grad_norm": 0.04785626009106636,
      "learning_rate": 5.861561119293079e-05,
      "loss": 0.3582,
      "step": 2405
    },
    {
      "epoch": 0.7076470588235294,
      "grad_norm": 0.03911935165524483,
      "learning_rate": 5.855670103092784e-05,
      "loss": 0.3548,
      "step": 2406
    },
    {
      "epoch": 0.7079411764705882,
      "grad_norm": 0.05682135373353958,
      "learning_rate": 5.8497790868924893e-05,
      "loss": 0.4222,
      "step": 2407
    },
    {
      "epoch": 0.7082352941176471,
      "grad_norm": 0.04133635759353638,
      "learning_rate": 5.843888070692195e-05,
      "loss": 0.3019,
      "step": 2408
    },
    {
      "epoch": 0.7085294117647059,
      "grad_norm": 0.03952626511454582,
      "learning_rate": 5.837997054491901e-05,
      "loss": 0.3638,
      "step": 2409
    },
    {
      "epoch": 0.7088235294117647,
      "grad_norm": 0.039331454783678055,
      "learning_rate": 5.832106038291606e-05,
      "loss": 0.3069,
      "step": 2410
    },
    {
      "epoch": 0.7091176470588235,
      "grad_norm": 0.04316426068544388,
      "learning_rate": 5.8262150220913114e-05,
      "loss": 0.3448,
      "step": 2411
    },
    {
      "epoch": 0.7094117647058824,
      "grad_norm": 0.04119736701250076,
      "learning_rate": 5.820324005891017e-05,
      "loss": 0.3428,
      "step": 2412
    },
    {
      "epoch": 0.7097058823529412,
      "grad_norm": 0.04296604171395302,
      "learning_rate": 5.814432989690722e-05,
      "loss": 0.358,
      "step": 2413
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.04363442584872246,
      "learning_rate": 5.808541973490428e-05,
      "loss": 0.3123,
      "step": 2414
    },
    {
      "epoch": 0.7102941176470589,
      "grad_norm": 0.03806767612695694,
      "learning_rate": 5.8026509572901334e-05,
      "loss": 0.3348,
      "step": 2415
    },
    {
      "epoch": 0.7105882352941176,
      "grad_norm": 0.04096746817231178,
      "learning_rate": 5.796759941089839e-05,
      "loss": 0.3403,
      "step": 2416
    },
    {
      "epoch": 0.7108823529411765,
      "grad_norm": 0.040437325835227966,
      "learning_rate": 5.790868924889544e-05,
      "loss": 0.2835,
      "step": 2417
    },
    {
      "epoch": 0.7111764705882353,
      "grad_norm": 0.04783535376191139,
      "learning_rate": 5.7849779086892494e-05,
      "loss": 0.3509,
      "step": 2418
    },
    {
      "epoch": 0.7114705882352941,
      "grad_norm": 0.053744107484817505,
      "learning_rate": 5.779086892488954e-05,
      "loss": 0.3724,
      "step": 2419
    },
    {
      "epoch": 0.711764705882353,
      "grad_norm": 0.042224083095788956,
      "learning_rate": 5.7731958762886594e-05,
      "loss": 0.337,
      "step": 2420
    },
    {
      "epoch": 0.7120588235294117,
      "grad_norm": 0.05160171166062355,
      "learning_rate": 5.767304860088365e-05,
      "loss": 0.3155,
      "step": 2421
    },
    {
      "epoch": 0.7123529411764706,
      "grad_norm": 0.03771234676241875,
      "learning_rate": 5.761413843888071e-05,
      "loss": 0.3442,
      "step": 2422
    },
    {
      "epoch": 0.7126470588235294,
      "grad_norm": 0.035682562738657,
      "learning_rate": 5.755522827687776e-05,
      "loss": 0.297,
      "step": 2423
    },
    {
      "epoch": 0.7129411764705882,
      "grad_norm": 0.04329853132367134,
      "learning_rate": 5.7496318114874815e-05,
      "loss": 0.3426,
      "step": 2424
    },
    {
      "epoch": 0.7132352941176471,
      "grad_norm": 0.05340070277452469,
      "learning_rate": 5.743740795287187e-05,
      "loss": 0.4024,
      "step": 2425
    },
    {
      "epoch": 0.7135294117647059,
      "grad_norm": 0.047105949372053146,
      "learning_rate": 5.737849779086892e-05,
      "loss": 0.3908,
      "step": 2426
    },
    {
      "epoch": 0.7138235294117647,
      "grad_norm": 0.0461711585521698,
      "learning_rate": 5.731958762886598e-05,
      "loss": 0.2987,
      "step": 2427
    },
    {
      "epoch": 0.7141176470588235,
      "grad_norm": 0.04355302080512047,
      "learning_rate": 5.7260677466863035e-05,
      "loss": 0.2964,
      "step": 2428
    },
    {
      "epoch": 0.7144117647058823,
      "grad_norm": 0.058165933936834335,
      "learning_rate": 5.720176730486009e-05,
      "loss": 0.4025,
      "step": 2429
    },
    {
      "epoch": 0.7147058823529412,
      "grad_norm": 0.047819796949625015,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.322,
      "step": 2430
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.04811520501971245,
      "learning_rate": 5.7083946980854195e-05,
      "loss": 0.2926,
      "step": 2431
    },
    {
      "epoch": 0.7152941176470589,
      "grad_norm": 0.04835719242691994,
      "learning_rate": 5.702503681885125e-05,
      "loss": 0.3471,
      "step": 2432
    },
    {
      "epoch": 0.7155882352941176,
      "grad_norm": 0.041959498077631,
      "learning_rate": 5.696612665684831e-05,
      "loss": 0.2817,
      "step": 2433
    },
    {
      "epoch": 0.7158823529411765,
      "grad_norm": 0.04759318381547928,
      "learning_rate": 5.690721649484536e-05,
      "loss": 0.3708,
      "step": 2434
    },
    {
      "epoch": 0.7161764705882353,
      "grad_norm": 0.050654493272304535,
      "learning_rate": 5.6848306332842415e-05,
      "loss": 0.3332,
      "step": 2435
    },
    {
      "epoch": 0.7164705882352941,
      "grad_norm": 0.04946960508823395,
      "learning_rate": 5.678939617083947e-05,
      "loss": 0.3411,
      "step": 2436
    },
    {
      "epoch": 0.716764705882353,
      "grad_norm": 0.03735903650522232,
      "learning_rate": 5.673048600883652e-05,
      "loss": 0.3073,
      "step": 2437
    },
    {
      "epoch": 0.7170588235294117,
      "grad_norm": 0.05105089768767357,
      "learning_rate": 5.667157584683358e-05,
      "loss": 0.4237,
      "step": 2438
    },
    {
      "epoch": 0.7173529411764706,
      "grad_norm": 0.052849672734737396,
      "learning_rate": 5.6612665684830636e-05,
      "loss": 0.381,
      "step": 2439
    },
    {
      "epoch": 0.7176470588235294,
      "grad_norm": 0.04888220131397247,
      "learning_rate": 5.655375552282769e-05,
      "loss": 0.3662,
      "step": 2440
    },
    {
      "epoch": 0.7179411764705882,
      "grad_norm": 0.044581279158592224,
      "learning_rate": 5.649484536082474e-05,
      "loss": 0.3222,
      "step": 2441
    },
    {
      "epoch": 0.7182352941176471,
      "grad_norm": 0.04000798612833023,
      "learning_rate": 5.6435935198821796e-05,
      "loss": 0.3123,
      "step": 2442
    },
    {
      "epoch": 0.7185294117647059,
      "grad_norm": 0.05315117910504341,
      "learning_rate": 5.6377025036818856e-05,
      "loss": 0.3701,
      "step": 2443
    },
    {
      "epoch": 0.7188235294117648,
      "grad_norm": 0.04062443599104881,
      "learning_rate": 5.631811487481591e-05,
      "loss": 0.3447,
      "step": 2444
    },
    {
      "epoch": 0.7191176470588235,
      "grad_norm": 0.0491904653608799,
      "learning_rate": 5.625920471281296e-05,
      "loss": 0.4266,
      "step": 2445
    },
    {
      "epoch": 0.7194117647058823,
      "grad_norm": 0.05714544653892517,
      "learning_rate": 5.6200294550810016e-05,
      "loss": 0.3497,
      "step": 2446
    },
    {
      "epoch": 0.7197058823529412,
      "grad_norm": 0.05635753273963928,
      "learning_rate": 5.614138438880707e-05,
      "loss": 0.4704,
      "step": 2447
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.03587173670530319,
      "learning_rate": 5.608247422680413e-05,
      "loss": 0.2754,
      "step": 2448
    },
    {
      "epoch": 0.7202941176470589,
      "grad_norm": 0.04375645890831947,
      "learning_rate": 5.602356406480118e-05,
      "loss": 0.3247,
      "step": 2449
    },
    {
      "epoch": 0.7205882352941176,
      "grad_norm": 0.0331563763320446,
      "learning_rate": 5.596465390279824e-05,
      "loss": 0.2494,
      "step": 2450
    },
    {
      "epoch": 0.7208823529411764,
      "grad_norm": 0.04708685725927353,
      "learning_rate": 5.590574374079529e-05,
      "loss": 0.3281,
      "step": 2451
    },
    {
      "epoch": 0.7211764705882353,
      "grad_norm": 0.04588239639997482,
      "learning_rate": 5.5846833578792343e-05,
      "loss": 0.3523,
      "step": 2452
    },
    {
      "epoch": 0.7214705882352941,
      "grad_norm": 0.04321247339248657,
      "learning_rate": 5.5787923416789404e-05,
      "loss": 0.3636,
      "step": 2453
    },
    {
      "epoch": 0.721764705882353,
      "grad_norm": 0.047603994607925415,
      "learning_rate": 5.572901325478646e-05,
      "loss": 0.3488,
      "step": 2454
    },
    {
      "epoch": 0.7220588235294118,
      "grad_norm": 0.048792753368616104,
      "learning_rate": 5.567010309278351e-05,
      "loss": 0.4172,
      "step": 2455
    },
    {
      "epoch": 0.7223529411764706,
      "grad_norm": 0.03851915895938873,
      "learning_rate": 5.5611192930780564e-05,
      "loss": 0.3146,
      "step": 2456
    },
    {
      "epoch": 0.7226470588235294,
      "grad_norm": 0.06347581744194031,
      "learning_rate": 5.555228276877762e-05,
      "loss": 0.3829,
      "step": 2457
    },
    {
      "epoch": 0.7229411764705882,
      "grad_norm": 0.04544646292924881,
      "learning_rate": 5.549337260677468e-05,
      "loss": 0.3212,
      "step": 2458
    },
    {
      "epoch": 0.7232352941176471,
      "grad_norm": 0.04757208004593849,
      "learning_rate": 5.543446244477173e-05,
      "loss": 0.3573,
      "step": 2459
    },
    {
      "epoch": 0.7235294117647059,
      "grad_norm": 0.05062094330787659,
      "learning_rate": 5.5375552282768784e-05,
      "loss": 0.3611,
      "step": 2460
    },
    {
      "epoch": 0.7238235294117648,
      "grad_norm": 0.03749044984579086,
      "learning_rate": 5.531664212076584e-05,
      "loss": 0.346,
      "step": 2461
    },
    {
      "epoch": 0.7241176470588235,
      "grad_norm": 0.048356812447309494,
      "learning_rate": 5.525773195876289e-05,
      "loss": 0.3343,
      "step": 2462
    },
    {
      "epoch": 0.7244117647058823,
      "grad_norm": 0.04298416152596474,
      "learning_rate": 5.5198821796759944e-05,
      "loss": 0.346,
      "step": 2463
    },
    {
      "epoch": 0.7247058823529412,
      "grad_norm": 0.04269329085946083,
      "learning_rate": 5.5139911634757004e-05,
      "loss": 0.2981,
      "step": 2464
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05174672231078148,
      "learning_rate": 5.508100147275406e-05,
      "loss": 0.4002,
      "step": 2465
    },
    {
      "epoch": 0.7252941176470589,
      "grad_norm": 0.05138156935572624,
      "learning_rate": 5.502209131075111e-05,
      "loss": 0.3373,
      "step": 2466
    },
    {
      "epoch": 0.7255882352941176,
      "grad_norm": 0.04854300245642662,
      "learning_rate": 5.4963181148748165e-05,
      "loss": 0.4209,
      "step": 2467
    },
    {
      "epoch": 0.7258823529411764,
      "grad_norm": 0.049500204622745514,
      "learning_rate": 5.490427098674522e-05,
      "loss": 0.415,
      "step": 2468
    },
    {
      "epoch": 0.7261764705882353,
      "grad_norm": 0.04415062442421913,
      "learning_rate": 5.484536082474228e-05,
      "loss": 0.3906,
      "step": 2469
    },
    {
      "epoch": 0.7264705882352941,
      "grad_norm": 0.0336029976606369,
      "learning_rate": 5.478645066273933e-05,
      "loss": 0.2419,
      "step": 2470
    },
    {
      "epoch": 0.726764705882353,
      "grad_norm": 0.04345954954624176,
      "learning_rate": 5.4727540500736385e-05,
      "loss": 0.3758,
      "step": 2471
    },
    {
      "epoch": 0.7270588235294118,
      "grad_norm": 0.03892780840396881,
      "learning_rate": 5.466863033873343e-05,
      "loss": 0.3292,
      "step": 2472
    },
    {
      "epoch": 0.7273529411764705,
      "grad_norm": 0.05680282041430473,
      "learning_rate": 5.4609720176730485e-05,
      "loss": 0.3664,
      "step": 2473
    },
    {
      "epoch": 0.7276470588235294,
      "grad_norm": 0.0587124340236187,
      "learning_rate": 5.455081001472754e-05,
      "loss": 0.3659,
      "step": 2474
    },
    {
      "epoch": 0.7279411764705882,
      "grad_norm": 0.04700548201799393,
      "learning_rate": 5.449189985272459e-05,
      "loss": 0.3325,
      "step": 2475
    },
    {
      "epoch": 0.7282352941176471,
      "grad_norm": 0.04327377304434776,
      "learning_rate": 5.4432989690721645e-05,
      "loss": 0.304,
      "step": 2476
    },
    {
      "epoch": 0.7285294117647059,
      "grad_norm": 0.0505366250872612,
      "learning_rate": 5.4374079528718705e-05,
      "loss": 0.364,
      "step": 2477
    },
    {
      "epoch": 0.7288235294117648,
      "grad_norm": 0.048610568046569824,
      "learning_rate": 5.431516936671576e-05,
      "loss": 0.3423,
      "step": 2478
    },
    {
      "epoch": 0.7291176470588235,
      "grad_norm": 0.043820519000291824,
      "learning_rate": 5.425625920471281e-05,
      "loss": 0.3261,
      "step": 2479
    },
    {
      "epoch": 0.7294117647058823,
      "grad_norm": 0.04111974686384201,
      "learning_rate": 5.4197349042709865e-05,
      "loss": 0.2809,
      "step": 2480
    },
    {
      "epoch": 0.7297058823529412,
      "grad_norm": 0.06091553717851639,
      "learning_rate": 5.413843888070692e-05,
      "loss": 0.323,
      "step": 2481
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.06078139692544937,
      "learning_rate": 5.407952871870398e-05,
      "loss": 0.4211,
      "step": 2482
    },
    {
      "epoch": 0.7302941176470589,
      "grad_norm": 0.061560504138469696,
      "learning_rate": 5.402061855670103e-05,
      "loss": 0.3377,
      "step": 2483
    },
    {
      "epoch": 0.7305882352941176,
      "grad_norm": 0.03936246410012245,
      "learning_rate": 5.3961708394698086e-05,
      "loss": 0.3055,
      "step": 2484
    },
    {
      "epoch": 0.7308823529411764,
      "grad_norm": 0.0652296394109726,
      "learning_rate": 5.390279823269514e-05,
      "loss": 0.4049,
      "step": 2485
    },
    {
      "epoch": 0.7311764705882353,
      "grad_norm": 0.04751827195286751,
      "learning_rate": 5.384388807069219e-05,
      "loss": 0.3186,
      "step": 2486
    },
    {
      "epoch": 0.7314705882352941,
      "grad_norm": 0.05418948456645012,
      "learning_rate": 5.3784977908689246e-05,
      "loss": 0.3835,
      "step": 2487
    },
    {
      "epoch": 0.731764705882353,
      "grad_norm": 0.041628897190093994,
      "learning_rate": 5.3726067746686306e-05,
      "loss": 0.3199,
      "step": 2488
    },
    {
      "epoch": 0.7320588235294118,
      "grad_norm": 0.04790958762168884,
      "learning_rate": 5.366715758468336e-05,
      "loss": 0.3305,
      "step": 2489
    },
    {
      "epoch": 0.7323529411764705,
      "grad_norm": 0.04726563021540642,
      "learning_rate": 5.360824742268041e-05,
      "loss": 0.3597,
      "step": 2490
    },
    {
      "epoch": 0.7326470588235294,
      "grad_norm": 0.0424964614212513,
      "learning_rate": 5.3549337260677466e-05,
      "loss": 0.285,
      "step": 2491
    },
    {
      "epoch": 0.7329411764705882,
      "grad_norm": 0.05753543600440025,
      "learning_rate": 5.349042709867452e-05,
      "loss": 0.3683,
      "step": 2492
    },
    {
      "epoch": 0.7332352941176471,
      "grad_norm": 0.03975369781255722,
      "learning_rate": 5.343151693667158e-05,
      "loss": 0.308,
      "step": 2493
    },
    {
      "epoch": 0.7335294117647059,
      "grad_norm": 0.05558691546320915,
      "learning_rate": 5.337260677466863e-05,
      "loss": 0.4475,
      "step": 2494
    },
    {
      "epoch": 0.7338235294117647,
      "grad_norm": 0.04198598116636276,
      "learning_rate": 5.3313696612665687e-05,
      "loss": 0.2755,
      "step": 2495
    },
    {
      "epoch": 0.7341176470588235,
      "grad_norm": 0.049913786351680756,
      "learning_rate": 5.325478645066274e-05,
      "loss": 0.3703,
      "step": 2496
    },
    {
      "epoch": 0.7344117647058823,
      "grad_norm": 0.04936375096440315,
      "learning_rate": 5.319587628865979e-05,
      "loss": 0.4134,
      "step": 2497
    },
    {
      "epoch": 0.7347058823529412,
      "grad_norm": 0.047192808240652084,
      "learning_rate": 5.3136966126656854e-05,
      "loss": 0.2689,
      "step": 2498
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.048000410199165344,
      "learning_rate": 5.307805596465391e-05,
      "loss": 0.3026,
      "step": 2499
    },
    {
      "epoch": 0.7352941176470589,
      "grad_norm": 0.05424845591187477,
      "learning_rate": 5.301914580265096e-05,
      "loss": 0.3893,
      "step": 2500
    },
    {
      "epoch": 0.7355882352941177,
      "grad_norm": 0.05587872862815857,
      "learning_rate": 5.2960235640648014e-05,
      "loss": 0.3446,
      "step": 2501
    },
    {
      "epoch": 0.7358823529411764,
      "grad_norm": 0.04845311492681503,
      "learning_rate": 5.290132547864507e-05,
      "loss": 0.3912,
      "step": 2502
    },
    {
      "epoch": 0.7361764705882353,
      "grad_norm": 0.04429493099451065,
      "learning_rate": 5.284241531664213e-05,
      "loss": 0.3205,
      "step": 2503
    },
    {
      "epoch": 0.7364705882352941,
      "grad_norm": 0.045089490711688995,
      "learning_rate": 5.278350515463918e-05,
      "loss": 0.2904,
      "step": 2504
    },
    {
      "epoch": 0.736764705882353,
      "grad_norm": 0.05890662595629692,
      "learning_rate": 5.2724594992636234e-05,
      "loss": 0.3788,
      "step": 2505
    },
    {
      "epoch": 0.7370588235294118,
      "grad_norm": 0.03847791254520416,
      "learning_rate": 5.266568483063329e-05,
      "loss": 0.2801,
      "step": 2506
    },
    {
      "epoch": 0.7373529411764705,
      "grad_norm": 0.05090545117855072,
      "learning_rate": 5.260677466863034e-05,
      "loss": 0.3601,
      "step": 2507
    },
    {
      "epoch": 0.7376470588235294,
      "grad_norm": 0.04818500578403473,
      "learning_rate": 5.25478645066274e-05,
      "loss": 0.3006,
      "step": 2508
    },
    {
      "epoch": 0.7379411764705882,
      "grad_norm": 0.05065444856882095,
      "learning_rate": 5.2488954344624454e-05,
      "loss": 0.3605,
      "step": 2509
    },
    {
      "epoch": 0.7382352941176471,
      "grad_norm": 0.04130394011735916,
      "learning_rate": 5.243004418262151e-05,
      "loss": 0.3581,
      "step": 2510
    },
    {
      "epoch": 0.7385294117647059,
      "grad_norm": 0.05018962174654007,
      "learning_rate": 5.237113402061856e-05,
      "loss": 0.3724,
      "step": 2511
    },
    {
      "epoch": 0.7388235294117647,
      "grad_norm": 0.04156948998570442,
      "learning_rate": 5.2312223858615615e-05,
      "loss": 0.2831,
      "step": 2512
    },
    {
      "epoch": 0.7391176470588235,
      "grad_norm": 0.046901214867830276,
      "learning_rate": 5.225331369661267e-05,
      "loss": 0.338,
      "step": 2513
    },
    {
      "epoch": 0.7394117647058823,
      "grad_norm": 0.044793400913476944,
      "learning_rate": 5.219440353460973e-05,
      "loss": 0.3637,
      "step": 2514
    },
    {
      "epoch": 0.7397058823529412,
      "grad_norm": 0.04723132401704788,
      "learning_rate": 5.213549337260678e-05,
      "loss": 0.313,
      "step": 2515
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.047208692878484726,
      "learning_rate": 5.2076583210603835e-05,
      "loss": 0.3526,
      "step": 2516
    },
    {
      "epoch": 0.7402941176470588,
      "grad_norm": 0.04576174542307854,
      "learning_rate": 5.201767304860089e-05,
      "loss": 0.3527,
      "step": 2517
    },
    {
      "epoch": 0.7405882352941177,
      "grad_norm": 0.05542440712451935,
      "learning_rate": 5.195876288659794e-05,
      "loss": 0.3965,
      "step": 2518
    },
    {
      "epoch": 0.7408823529411764,
      "grad_norm": 0.04141741618514061,
      "learning_rate": 5.1899852724595e-05,
      "loss": 0.2832,
      "step": 2519
    },
    {
      "epoch": 0.7411764705882353,
      "grad_norm": 0.04460849612951279,
      "learning_rate": 5.1840942562592055e-05,
      "loss": 0.3431,
      "step": 2520
    },
    {
      "epoch": 0.7414705882352941,
      "grad_norm": 0.05289970710873604,
      "learning_rate": 5.178203240058911e-05,
      "loss": 0.374,
      "step": 2521
    },
    {
      "epoch": 0.741764705882353,
      "grad_norm": 0.04643529653549194,
      "learning_rate": 5.172312223858616e-05,
      "loss": 0.327,
      "step": 2522
    },
    {
      "epoch": 0.7420588235294118,
      "grad_norm": 0.04615145921707153,
      "learning_rate": 5.1664212076583215e-05,
      "loss": 0.3644,
      "step": 2523
    },
    {
      "epoch": 0.7423529411764705,
      "grad_norm": 0.04407631233334541,
      "learning_rate": 5.1605301914580276e-05,
      "loss": 0.3417,
      "step": 2524
    },
    {
      "epoch": 0.7426470588235294,
      "grad_norm": 0.05599062889814377,
      "learning_rate": 5.1546391752577315e-05,
      "loss": 0.3791,
      "step": 2525
    },
    {
      "epoch": 0.7429411764705882,
      "grad_norm": 0.03010452724993229,
      "learning_rate": 5.148748159057437e-05,
      "loss": 0.2556,
      "step": 2526
    },
    {
      "epoch": 0.7432352941176471,
      "grad_norm": 0.04289814829826355,
      "learning_rate": 5.142857142857143e-05,
      "loss": 0.3179,
      "step": 2527
    },
    {
      "epoch": 0.7435294117647059,
      "grad_norm": 0.045094359666109085,
      "learning_rate": 5.136966126656848e-05,
      "loss": 0.3615,
      "step": 2528
    },
    {
      "epoch": 0.7438235294117647,
      "grad_norm": 0.03758858144283295,
      "learning_rate": 5.1310751104565536e-05,
      "loss": 0.3176,
      "step": 2529
    },
    {
      "epoch": 0.7441176470588236,
      "grad_norm": 0.047279439866542816,
      "learning_rate": 5.125184094256259e-05,
      "loss": 0.3431,
      "step": 2530
    },
    {
      "epoch": 0.7444117647058823,
      "grad_norm": 0.03997904807329178,
      "learning_rate": 5.119293078055964e-05,
      "loss": 0.2539,
      "step": 2531
    },
    {
      "epoch": 0.7447058823529412,
      "grad_norm": 0.03683249652385712,
      "learning_rate": 5.11340206185567e-05,
      "loss": 0.3178,
      "step": 2532
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.036747757345438004,
      "learning_rate": 5.1075110456553756e-05,
      "loss": 0.2836,
      "step": 2533
    },
    {
      "epoch": 0.7452941176470588,
      "grad_norm": 0.045821741223335266,
      "learning_rate": 5.101620029455081e-05,
      "loss": 0.3199,
      "step": 2534
    },
    {
      "epoch": 0.7455882352941177,
      "grad_norm": 0.03250744566321373,
      "learning_rate": 5.095729013254786e-05,
      "loss": 0.2809,
      "step": 2535
    },
    {
      "epoch": 0.7458823529411764,
      "grad_norm": 0.047759346663951874,
      "learning_rate": 5.0898379970544916e-05,
      "loss": 0.3755,
      "step": 2536
    },
    {
      "epoch": 0.7461764705882353,
      "grad_norm": 0.03645145148038864,
      "learning_rate": 5.083946980854197e-05,
      "loss": 0.3014,
      "step": 2537
    },
    {
      "epoch": 0.7464705882352941,
      "grad_norm": 0.04270980507135391,
      "learning_rate": 5.078055964653903e-05,
      "loss": 0.3338,
      "step": 2538
    },
    {
      "epoch": 0.7467647058823529,
      "grad_norm": 0.055150117725133896,
      "learning_rate": 5.072164948453608e-05,
      "loss": 0.3892,
      "step": 2539
    },
    {
      "epoch": 0.7470588235294118,
      "grad_norm": 0.042753756046295166,
      "learning_rate": 5.0662739322533137e-05,
      "loss": 0.3489,
      "step": 2540
    },
    {
      "epoch": 0.7473529411764706,
      "grad_norm": 0.054048698395490646,
      "learning_rate": 5.060382916053019e-05,
      "loss": 0.386,
      "step": 2541
    },
    {
      "epoch": 0.7476470588235294,
      "grad_norm": 0.04979846626520157,
      "learning_rate": 5.054491899852724e-05,
      "loss": 0.319,
      "step": 2542
    },
    {
      "epoch": 0.7479411764705882,
      "grad_norm": 0.05345318466424942,
      "learning_rate": 5.0486008836524303e-05,
      "loss": 0.3612,
      "step": 2543
    },
    {
      "epoch": 0.7482352941176471,
      "grad_norm": 0.04303016513586044,
      "learning_rate": 5.042709867452136e-05,
      "loss": 0.3694,
      "step": 2544
    },
    {
      "epoch": 0.7485294117647059,
      "grad_norm": 0.03719528764486313,
      "learning_rate": 5.036818851251841e-05,
      "loss": 0.2818,
      "step": 2545
    },
    {
      "epoch": 0.7488235294117647,
      "grad_norm": 0.044462982565164566,
      "learning_rate": 5.0309278350515464e-05,
      "loss": 0.2915,
      "step": 2546
    },
    {
      "epoch": 0.7491176470588236,
      "grad_norm": 0.05048308148980141,
      "learning_rate": 5.025036818851252e-05,
      "loss": 0.3656,
      "step": 2547
    },
    {
      "epoch": 0.7494117647058823,
      "grad_norm": 0.04908723756670952,
      "learning_rate": 5.019145802650958e-05,
      "loss": 0.2707,
      "step": 2548
    },
    {
      "epoch": 0.7497058823529412,
      "grad_norm": 0.04425140470266342,
      "learning_rate": 5.013254786450663e-05,
      "loss": 0.3245,
      "step": 2549
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.04941601678729057,
      "learning_rate": 5.0073637702503684e-05,
      "loss": 0.3149,
      "step": 2550
    },
    {
      "epoch": 0.7502941176470588,
      "grad_norm": 0.04844007268548012,
      "learning_rate": 5.001472754050074e-05,
      "loss": 0.2962,
      "step": 2551
    },
    {
      "epoch": 0.7505882352941177,
      "grad_norm": 0.047782398760318756,
      "learning_rate": 4.995581737849779e-05,
      "loss": 0.3977,
      "step": 2552
    },
    {
      "epoch": 0.7508823529411764,
      "grad_norm": 0.054748889058828354,
      "learning_rate": 4.989690721649485e-05,
      "loss": 0.4034,
      "step": 2553
    },
    {
      "epoch": 0.7511764705882353,
      "grad_norm": 0.05224071443080902,
      "learning_rate": 4.9837997054491904e-05,
      "loss": 0.3975,
      "step": 2554
    },
    {
      "epoch": 0.7514705882352941,
      "grad_norm": 0.0674571692943573,
      "learning_rate": 4.977908689248896e-05,
      "loss": 0.3905,
      "step": 2555
    },
    {
      "epoch": 0.7517647058823529,
      "grad_norm": 0.05794999375939369,
      "learning_rate": 4.972017673048601e-05,
      "loss": 0.3585,
      "step": 2556
    },
    {
      "epoch": 0.7520588235294118,
      "grad_norm": 0.04942350834608078,
      "learning_rate": 4.9661266568483064e-05,
      "loss": 0.3571,
      "step": 2557
    },
    {
      "epoch": 0.7523529411764706,
      "grad_norm": 0.03975812718272209,
      "learning_rate": 4.9602356406480125e-05,
      "loss": 0.3047,
      "step": 2558
    },
    {
      "epoch": 0.7526470588235294,
      "grad_norm": 0.05839823931455612,
      "learning_rate": 4.954344624447718e-05,
      "loss": 0.3514,
      "step": 2559
    },
    {
      "epoch": 0.7529411764705882,
      "grad_norm": 0.052046846598386765,
      "learning_rate": 4.948453608247423e-05,
      "loss": 0.3483,
      "step": 2560
    },
    {
      "epoch": 0.7532352941176471,
      "grad_norm": 0.04902418702840805,
      "learning_rate": 4.9425625920471285e-05,
      "loss": 0.3526,
      "step": 2561
    },
    {
      "epoch": 0.7535294117647059,
      "grad_norm": 0.049435101449489594,
      "learning_rate": 4.936671575846834e-05,
      "loss": 0.3241,
      "step": 2562
    },
    {
      "epoch": 0.7538235294117647,
      "grad_norm": 0.048858340829610825,
      "learning_rate": 4.93078055964654e-05,
      "loss": 0.3775,
      "step": 2563
    },
    {
      "epoch": 0.7541176470588236,
      "grad_norm": 0.07894866168498993,
      "learning_rate": 4.924889543446245e-05,
      "loss": 0.3474,
      "step": 2564
    },
    {
      "epoch": 0.7544117647058823,
      "grad_norm": 0.051249176263809204,
      "learning_rate": 4.91899852724595e-05,
      "loss": 0.366,
      "step": 2565
    },
    {
      "epoch": 0.7547058823529412,
      "grad_norm": 0.04331908002495766,
      "learning_rate": 4.913107511045655e-05,
      "loss": 0.3018,
      "step": 2566
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.06636124104261398,
      "learning_rate": 4.9072164948453605e-05,
      "loss": 0.428,
      "step": 2567
    },
    {
      "epoch": 0.7552941176470588,
      "grad_norm": 0.05029081925749779,
      "learning_rate": 4.9013254786450665e-05,
      "loss": 0.4071,
      "step": 2568
    },
    {
      "epoch": 0.7555882352941177,
      "grad_norm": 0.05168577656149864,
      "learning_rate": 4.895434462444772e-05,
      "loss": 0.3752,
      "step": 2569
    },
    {
      "epoch": 0.7558823529411764,
      "grad_norm": 0.04006514698266983,
      "learning_rate": 4.889543446244477e-05,
      "loss": 0.3746,
      "step": 2570
    },
    {
      "epoch": 0.7561764705882353,
      "grad_norm": 0.04837057366967201,
      "learning_rate": 4.8836524300441825e-05,
      "loss": 0.4127,
      "step": 2571
    },
    {
      "epoch": 0.7564705882352941,
      "grad_norm": 0.04172879084944725,
      "learning_rate": 4.877761413843888e-05,
      "loss": 0.3201,
      "step": 2572
    },
    {
      "epoch": 0.7567647058823529,
      "grad_norm": 0.05307726562023163,
      "learning_rate": 4.871870397643594e-05,
      "loss": 0.4337,
      "step": 2573
    },
    {
      "epoch": 0.7570588235294118,
      "grad_norm": 0.05140412971377373,
      "learning_rate": 4.865979381443299e-05,
      "loss": 0.3648,
      "step": 2574
    },
    {
      "epoch": 0.7573529411764706,
      "grad_norm": 0.0470607653260231,
      "learning_rate": 4.8600883652430046e-05,
      "loss": 0.3426,
      "step": 2575
    },
    {
      "epoch": 0.7576470588235295,
      "grad_norm": 0.04251071810722351,
      "learning_rate": 4.85419734904271e-05,
      "loss": 0.3591,
      "step": 2576
    },
    {
      "epoch": 0.7579411764705882,
      "grad_norm": 0.0678701177239418,
      "learning_rate": 4.848306332842415e-05,
      "loss": 0.4373,
      "step": 2577
    },
    {
      "epoch": 0.758235294117647,
      "grad_norm": 0.040411580353975296,
      "learning_rate": 4.842415316642121e-05,
      "loss": 0.295,
      "step": 2578
    },
    {
      "epoch": 0.7585294117647059,
      "grad_norm": 0.048331234604120255,
      "learning_rate": 4.8365243004418266e-05,
      "loss": 0.3372,
      "step": 2579
    },
    {
      "epoch": 0.7588235294117647,
      "grad_norm": 0.03969718888401985,
      "learning_rate": 4.830633284241532e-05,
      "loss": 0.2562,
      "step": 2580
    },
    {
      "epoch": 0.7591176470588236,
      "grad_norm": 0.04532845690846443,
      "learning_rate": 4.824742268041237e-05,
      "loss": 0.3468,
      "step": 2581
    },
    {
      "epoch": 0.7594117647058823,
      "grad_norm": 0.048502422869205475,
      "learning_rate": 4.8188512518409426e-05,
      "loss": 0.3232,
      "step": 2582
    },
    {
      "epoch": 0.7597058823529412,
      "grad_norm": 0.05107083544135094,
      "learning_rate": 4.8129602356406486e-05,
      "loss": 0.3487,
      "step": 2583
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.035192277282476425,
      "learning_rate": 4.807069219440354e-05,
      "loss": 0.2533,
      "step": 2584
    },
    {
      "epoch": 0.7602941176470588,
      "grad_norm": 0.048578936606645584,
      "learning_rate": 4.801178203240059e-05,
      "loss": 0.3504,
      "step": 2585
    },
    {
      "epoch": 0.7605882352941177,
      "grad_norm": 0.04568125680088997,
      "learning_rate": 4.795287187039765e-05,
      "loss": 0.3554,
      "step": 2586
    },
    {
      "epoch": 0.7608823529411765,
      "grad_norm": 0.048546452075242996,
      "learning_rate": 4.78939617083947e-05,
      "loss": 0.3584,
      "step": 2587
    },
    {
      "epoch": 0.7611764705882353,
      "grad_norm": 0.04341030865907669,
      "learning_rate": 4.783505154639176e-05,
      "loss": 0.3157,
      "step": 2588
    },
    {
      "epoch": 0.7614705882352941,
      "grad_norm": 0.04145682975649834,
      "learning_rate": 4.7776141384388814e-05,
      "loss": 0.3696,
      "step": 2589
    },
    {
      "epoch": 0.7617647058823529,
      "grad_norm": 0.04858282580971718,
      "learning_rate": 4.771723122238587e-05,
      "loss": 0.3627,
      "step": 2590
    },
    {
      "epoch": 0.7620588235294118,
      "grad_norm": 0.040191736072301865,
      "learning_rate": 4.765832106038292e-05,
      "loss": 0.2788,
      "step": 2591
    },
    {
      "epoch": 0.7623529411764706,
      "grad_norm": 0.07140229642391205,
      "learning_rate": 4.759941089837997e-05,
      "loss": 0.3654,
      "step": 2592
    },
    {
      "epoch": 0.7626470588235295,
      "grad_norm": 0.03830021992325783,
      "learning_rate": 4.754050073637703e-05,
      "loss": 0.3307,
      "step": 2593
    },
    {
      "epoch": 0.7629411764705882,
      "grad_norm": 0.05417953059077263,
      "learning_rate": 4.748159057437408e-05,
      "loss": 0.3417,
      "step": 2594
    },
    {
      "epoch": 0.763235294117647,
      "grad_norm": 0.054652098566293716,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 0.3301,
      "step": 2595
    },
    {
      "epoch": 0.7635294117647059,
      "grad_norm": 0.04263975843787193,
      "learning_rate": 4.736377025036819e-05,
      "loss": 0.3264,
      "step": 2596
    },
    {
      "epoch": 0.7638235294117647,
      "grad_norm": 0.0509808175265789,
      "learning_rate": 4.730486008836524e-05,
      "loss": 0.4081,
      "step": 2597
    },
    {
      "epoch": 0.7641176470588236,
      "grad_norm": 0.038311414420604706,
      "learning_rate": 4.72459499263623e-05,
      "loss": 0.2677,
      "step": 2598
    },
    {
      "epoch": 0.7644117647058823,
      "grad_norm": 0.03855719789862633,
      "learning_rate": 4.7187039764359354e-05,
      "loss": 0.3014,
      "step": 2599
    },
    {
      "epoch": 0.7647058823529411,
      "grad_norm": 0.0463298074901104,
      "learning_rate": 4.712812960235641e-05,
      "loss": 0.3537,
      "step": 2600
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.03915520757436752,
      "learning_rate": 4.706921944035346e-05,
      "loss": 0.2792,
      "step": 2601
    },
    {
      "epoch": 0.7652941176470588,
      "grad_norm": 0.05130762606859207,
      "learning_rate": 4.7010309278350514e-05,
      "loss": 0.364,
      "step": 2602
    },
    {
      "epoch": 0.7655882352941177,
      "grad_norm": 0.05412411689758301,
      "learning_rate": 4.6951399116347575e-05,
      "loss": 0.369,
      "step": 2603
    },
    {
      "epoch": 0.7658823529411765,
      "grad_norm": 0.05277238041162491,
      "learning_rate": 4.689248895434463e-05,
      "loss": 0.3659,
      "step": 2604
    },
    {
      "epoch": 0.7661764705882353,
      "grad_norm": 0.045078568160533905,
      "learning_rate": 4.683357879234168e-05,
      "loss": 0.3812,
      "step": 2605
    },
    {
      "epoch": 0.7664705882352941,
      "grad_norm": 0.05633995309472084,
      "learning_rate": 4.6774668630338735e-05,
      "loss": 0.3921,
      "step": 2606
    },
    {
      "epoch": 0.7667647058823529,
      "grad_norm": 0.048639312386512756,
      "learning_rate": 4.671575846833579e-05,
      "loss": 0.3365,
      "step": 2607
    },
    {
      "epoch": 0.7670588235294118,
      "grad_norm": 0.050339311361312866,
      "learning_rate": 4.665684830633285e-05,
      "loss": 0.3493,
      "step": 2608
    },
    {
      "epoch": 0.7673529411764706,
      "grad_norm": 0.054237768054008484,
      "learning_rate": 4.65979381443299e-05,
      "loss": 0.3578,
      "step": 2609
    },
    {
      "epoch": 0.7676470588235295,
      "grad_norm": 0.048582322895526886,
      "learning_rate": 4.6539027982326955e-05,
      "loss": 0.3355,
      "step": 2610
    },
    {
      "epoch": 0.7679411764705882,
      "grad_norm": 0.04701707512140274,
      "learning_rate": 4.648011782032401e-05,
      "loss": 0.4044,
      "step": 2611
    },
    {
      "epoch": 0.768235294117647,
      "grad_norm": 0.046714164316654205,
      "learning_rate": 4.642120765832106e-05,
      "loss": 0.3394,
      "step": 2612
    },
    {
      "epoch": 0.7685294117647059,
      "grad_norm": 0.048783861100673676,
      "learning_rate": 4.636229749631812e-05,
      "loss": 0.3176,
      "step": 2613
    },
    {
      "epoch": 0.7688235294117647,
      "grad_norm": 0.03505256772041321,
      "learning_rate": 4.6303387334315175e-05,
      "loss": 0.3027,
      "step": 2614
    },
    {
      "epoch": 0.7691176470588236,
      "grad_norm": 0.04352796822786331,
      "learning_rate": 4.624447717231223e-05,
      "loss": 0.3275,
      "step": 2615
    },
    {
      "epoch": 0.7694117647058824,
      "grad_norm": 0.03863512724637985,
      "learning_rate": 4.618556701030928e-05,
      "loss": 0.348,
      "step": 2616
    },
    {
      "epoch": 0.7697058823529411,
      "grad_norm": 0.040543217211961746,
      "learning_rate": 4.6126656848306336e-05,
      "loss": 0.3188,
      "step": 2617
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.04934222251176834,
      "learning_rate": 4.606774668630339e-05,
      "loss": 0.3994,
      "step": 2618
    },
    {
      "epoch": 0.7702941176470588,
      "grad_norm": 0.051786329597234726,
      "learning_rate": 4.600883652430044e-05,
      "loss": 0.391,
      "step": 2619
    },
    {
      "epoch": 0.7705882352941177,
      "grad_norm": 0.04208257794380188,
      "learning_rate": 4.5949926362297496e-05,
      "loss": 0.3208,
      "step": 2620
    },
    {
      "epoch": 0.7708823529411765,
      "grad_norm": 0.04524150490760803,
      "learning_rate": 4.589101620029455e-05,
      "loss": 0.3146,
      "step": 2621
    },
    {
      "epoch": 0.7711764705882352,
      "grad_norm": 0.04323277622461319,
      "learning_rate": 4.58321060382916e-05,
      "loss": 0.3219,
      "step": 2622
    },
    {
      "epoch": 0.7714705882352941,
      "grad_norm": 0.04202895611524582,
      "learning_rate": 4.577319587628866e-05,
      "loss": 0.3739,
      "step": 2623
    },
    {
      "epoch": 0.7717647058823529,
      "grad_norm": 0.04279544949531555,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.3226,
      "step": 2624
    },
    {
      "epoch": 0.7720588235294118,
      "grad_norm": 0.04863948002457619,
      "learning_rate": 4.565537555228277e-05,
      "loss": 0.339,
      "step": 2625
    },
    {
      "epoch": 0.7723529411764706,
      "grad_norm": 0.055367425084114075,
      "learning_rate": 4.559646539027982e-05,
      "loss": 0.3786,
      "step": 2626
    },
    {
      "epoch": 0.7726470588235295,
      "grad_norm": 0.043951041996479034,
      "learning_rate": 4.5537555228276876e-05,
      "loss": 0.3073,
      "step": 2627
    },
    {
      "epoch": 0.7729411764705882,
      "grad_norm": 0.059190668165683746,
      "learning_rate": 4.5478645066273936e-05,
      "loss": 0.3961,
      "step": 2628
    },
    {
      "epoch": 0.773235294117647,
      "grad_norm": 0.04917582869529724,
      "learning_rate": 4.541973490427099e-05,
      "loss": 0.2955,
      "step": 2629
    },
    {
      "epoch": 0.7735294117647059,
      "grad_norm": 0.04059746116399765,
      "learning_rate": 4.536082474226804e-05,
      "loss": 0.3109,
      "step": 2630
    },
    {
      "epoch": 0.7738235294117647,
      "grad_norm": 0.05447935685515404,
      "learning_rate": 4.5301914580265097e-05,
      "loss": 0.4043,
      "step": 2631
    },
    {
      "epoch": 0.7741176470588236,
      "grad_norm": 0.056423768401145935,
      "learning_rate": 4.524300441826215e-05,
      "loss": 0.4066,
      "step": 2632
    },
    {
      "epoch": 0.7744117647058824,
      "grad_norm": 0.0505974218249321,
      "learning_rate": 4.518409425625921e-05,
      "loss": 0.3455,
      "step": 2633
    },
    {
      "epoch": 0.7747058823529411,
      "grad_norm": 0.04323481023311615,
      "learning_rate": 4.5125184094256264e-05,
      "loss": 0.3042,
      "step": 2634
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.05095796659588814,
      "learning_rate": 4.506627393225332e-05,
      "loss": 0.3415,
      "step": 2635
    },
    {
      "epoch": 0.7752941176470588,
      "grad_norm": 0.06111704558134079,
      "learning_rate": 4.500736377025037e-05,
      "loss": 0.3699,
      "step": 2636
    },
    {
      "epoch": 0.7755882352941177,
      "grad_norm": 0.04401826485991478,
      "learning_rate": 4.4948453608247424e-05,
      "loss": 0.3546,
      "step": 2637
    },
    {
      "epoch": 0.7758823529411765,
      "grad_norm": 0.046690717339515686,
      "learning_rate": 4.4889543446244484e-05,
      "loss": 0.3393,
      "step": 2638
    },
    {
      "epoch": 0.7761764705882352,
      "grad_norm": 0.06326885521411896,
      "learning_rate": 4.483063328424154e-05,
      "loss": 0.4054,
      "step": 2639
    },
    {
      "epoch": 0.7764705882352941,
      "grad_norm": 0.05116421356797218,
      "learning_rate": 4.477172312223859e-05,
      "loss": 0.3376,
      "step": 2640
    },
    {
      "epoch": 0.7767647058823529,
      "grad_norm": 0.04314364492893219,
      "learning_rate": 4.4712812960235644e-05,
      "loss": 0.3562,
      "step": 2641
    },
    {
      "epoch": 0.7770588235294118,
      "grad_norm": 0.04450109973549843,
      "learning_rate": 4.46539027982327e-05,
      "loss": 0.341,
      "step": 2642
    },
    {
      "epoch": 0.7773529411764706,
      "grad_norm": 0.034504830837249756,
      "learning_rate": 4.459499263622976e-05,
      "loss": 0.2746,
      "step": 2643
    },
    {
      "epoch": 0.7776470588235294,
      "grad_norm": 0.036202553659677505,
      "learning_rate": 4.453608247422681e-05,
      "loss": 0.2824,
      "step": 2644
    },
    {
      "epoch": 0.7779411764705882,
      "grad_norm": 0.059026893228292465,
      "learning_rate": 4.447717231222386e-05,
      "loss": 0.3665,
      "step": 2645
    },
    {
      "epoch": 0.778235294117647,
      "grad_norm": 0.057083360850811005,
      "learning_rate": 4.441826215022091e-05,
      "loss": 0.3841,
      "step": 2646
    },
    {
      "epoch": 0.7785294117647059,
      "grad_norm": 0.04430018737912178,
      "learning_rate": 4.4359351988217964e-05,
      "loss": 0.3264,
      "step": 2647
    },
    {
      "epoch": 0.7788235294117647,
      "grad_norm": 0.04143967479467392,
      "learning_rate": 4.4300441826215025e-05,
      "loss": 0.2908,
      "step": 2648
    },
    {
      "epoch": 0.7791176470588236,
      "grad_norm": 0.04132485017180443,
      "learning_rate": 4.424153166421208e-05,
      "loss": 0.3256,
      "step": 2649
    },
    {
      "epoch": 0.7794117647058824,
      "grad_norm": 0.04716360569000244,
      "learning_rate": 4.418262150220913e-05,
      "loss": 0.3657,
      "step": 2650
    },
    {
      "epoch": 0.7797058823529411,
      "grad_norm": 0.06754927337169647,
      "learning_rate": 4.4123711340206185e-05,
      "loss": 0.344,
      "step": 2651
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.053403109312057495,
      "learning_rate": 4.406480117820324e-05,
      "loss": 0.3884,
      "step": 2652
    },
    {
      "epoch": 0.7802941176470588,
      "grad_norm": 0.05068771168589592,
      "learning_rate": 4.40058910162003e-05,
      "loss": 0.3417,
      "step": 2653
    },
    {
      "epoch": 0.7805882352941177,
      "grad_norm": 0.03827829286456108,
      "learning_rate": 4.394698085419735e-05,
      "loss": 0.2888,
      "step": 2654
    },
    {
      "epoch": 0.7808823529411765,
      "grad_norm": 0.051492005586624146,
      "learning_rate": 4.3888070692194405e-05,
      "loss": 0.3424,
      "step": 2655
    },
    {
      "epoch": 0.7811764705882352,
      "grad_norm": 0.05018363893032074,
      "learning_rate": 4.382916053019146e-05,
      "loss": 0.3654,
      "step": 2656
    },
    {
      "epoch": 0.7814705882352941,
      "grad_norm": 0.047232288867235184,
      "learning_rate": 4.377025036818851e-05,
      "loss": 0.3492,
      "step": 2657
    },
    {
      "epoch": 0.7817647058823529,
      "grad_norm": 0.04660807177424431,
      "learning_rate": 4.371134020618557e-05,
      "loss": 0.3395,
      "step": 2658
    },
    {
      "epoch": 0.7820588235294118,
      "grad_norm": 0.04286261647939682,
      "learning_rate": 4.3652430044182625e-05,
      "loss": 0.3279,
      "step": 2659
    },
    {
      "epoch": 0.7823529411764706,
      "grad_norm": 0.04965722933411598,
      "learning_rate": 4.359351988217968e-05,
      "loss": 0.4291,
      "step": 2660
    },
    {
      "epoch": 0.7826470588235294,
      "grad_norm": 0.04121331125497818,
      "learning_rate": 4.353460972017673e-05,
      "loss": 0.3013,
      "step": 2661
    },
    {
      "epoch": 0.7829411764705883,
      "grad_norm": 0.051565781235694885,
      "learning_rate": 4.3475699558173786e-05,
      "loss": 0.3656,
      "step": 2662
    },
    {
      "epoch": 0.783235294117647,
      "grad_norm": 0.04977770522236824,
      "learning_rate": 4.3416789396170846e-05,
      "loss": 0.3525,
      "step": 2663
    },
    {
      "epoch": 0.7835294117647059,
      "grad_norm": 0.038232434540987015,
      "learning_rate": 4.33578792341679e-05,
      "loss": 0.3245,
      "step": 2664
    },
    {
      "epoch": 0.7838235294117647,
      "grad_norm": 0.06326175481081009,
      "learning_rate": 4.329896907216495e-05,
      "loss": 0.4238,
      "step": 2665
    },
    {
      "epoch": 0.7841176470588235,
      "grad_norm": 0.0461139976978302,
      "learning_rate": 4.3240058910162006e-05,
      "loss": 0.3978,
      "step": 2666
    },
    {
      "epoch": 0.7844117647058824,
      "grad_norm": 0.05400262773036957,
      "learning_rate": 4.318114874815906e-05,
      "loss": 0.3711,
      "step": 2667
    },
    {
      "epoch": 0.7847058823529411,
      "grad_norm": 0.043356724083423615,
      "learning_rate": 4.312223858615612e-05,
      "loss": 0.2676,
      "step": 2668
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.04452478140592575,
      "learning_rate": 4.306332842415317e-05,
      "loss": 0.3343,
      "step": 2669
    },
    {
      "epoch": 0.7852941176470588,
      "grad_norm": 0.044388145208358765,
      "learning_rate": 4.3004418262150226e-05,
      "loss": 0.3834,
      "step": 2670
    },
    {
      "epoch": 0.7855882352941177,
      "grad_norm": 0.057274188846349716,
      "learning_rate": 4.294550810014727e-05,
      "loss": 0.3477,
      "step": 2671
    },
    {
      "epoch": 0.7858823529411765,
      "grad_norm": 0.04559427872300148,
      "learning_rate": 4.2886597938144326e-05,
      "loss": 0.3117,
      "step": 2672
    },
    {
      "epoch": 0.7861764705882353,
      "grad_norm": 0.04271022602915764,
      "learning_rate": 4.2827687776141386e-05,
      "loss": 0.2951,
      "step": 2673
    },
    {
      "epoch": 0.7864705882352941,
      "grad_norm": 0.043319739401340485,
      "learning_rate": 4.276877761413844e-05,
      "loss": 0.3317,
      "step": 2674
    },
    {
      "epoch": 0.7867647058823529,
      "grad_norm": 0.04798142984509468,
      "learning_rate": 4.270986745213549e-05,
      "loss": 0.2605,
      "step": 2675
    },
    {
      "epoch": 0.7870588235294118,
      "grad_norm": 0.05725071206688881,
      "learning_rate": 4.2650957290132547e-05,
      "loss": 0.3592,
      "step": 2676
    },
    {
      "epoch": 0.7873529411764706,
      "grad_norm": 0.049052465707063675,
      "learning_rate": 4.25920471281296e-05,
      "loss": 0.3759,
      "step": 2677
    },
    {
      "epoch": 0.7876470588235294,
      "grad_norm": 0.04848988354206085,
      "learning_rate": 4.253313696612666e-05,
      "loss": 0.3247,
      "step": 2678
    },
    {
      "epoch": 0.7879411764705883,
      "grad_norm": 0.05414851754903793,
      "learning_rate": 4.2474226804123713e-05,
      "loss": 0.3569,
      "step": 2679
    },
    {
      "epoch": 0.788235294117647,
      "grad_norm": 0.047721147537231445,
      "learning_rate": 4.241531664212077e-05,
      "loss": 0.3369,
      "step": 2680
    },
    {
      "epoch": 0.7885294117647059,
      "grad_norm": 0.05292191356420517,
      "learning_rate": 4.235640648011782e-05,
      "loss": 0.4019,
      "step": 2681
    },
    {
      "epoch": 0.7888235294117647,
      "grad_norm": 0.04958708584308624,
      "learning_rate": 4.2297496318114874e-05,
      "loss": 0.3608,
      "step": 2682
    },
    {
      "epoch": 0.7891176470588235,
      "grad_norm": 0.03975209221243858,
      "learning_rate": 4.2238586156111934e-05,
      "loss": 0.3219,
      "step": 2683
    },
    {
      "epoch": 0.7894117647058824,
      "grad_norm": 0.05098946765065193,
      "learning_rate": 4.217967599410899e-05,
      "loss": 0.3466,
      "step": 2684
    },
    {
      "epoch": 0.7897058823529411,
      "grad_norm": 0.03361056372523308,
      "learning_rate": 4.212076583210604e-05,
      "loss": 0.2486,
      "step": 2685
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.05331331491470337,
      "learning_rate": 4.2061855670103094e-05,
      "loss": 0.3765,
      "step": 2686
    },
    {
      "epoch": 0.7902941176470588,
      "grad_norm": 0.053456202149391174,
      "learning_rate": 4.200294550810015e-05,
      "loss": 0.3166,
      "step": 2687
    },
    {
      "epoch": 0.7905882352941176,
      "grad_norm": 0.04205091670155525,
      "learning_rate": 4.194403534609721e-05,
      "loss": 0.3475,
      "step": 2688
    },
    {
      "epoch": 0.7908823529411765,
      "grad_norm": 0.04488130286335945,
      "learning_rate": 4.188512518409426e-05,
      "loss": 0.3373,
      "step": 2689
    },
    {
      "epoch": 0.7911764705882353,
      "grad_norm": 0.0429602786898613,
      "learning_rate": 4.1826215022091314e-05,
      "loss": 0.2904,
      "step": 2690
    },
    {
      "epoch": 0.7914705882352941,
      "grad_norm": 0.05099548399448395,
      "learning_rate": 4.176730486008837e-05,
      "loss": 0.3829,
      "step": 2691
    },
    {
      "epoch": 0.7917647058823529,
      "grad_norm": 0.06410739570856094,
      "learning_rate": 4.170839469808542e-05,
      "loss": 0.4177,
      "step": 2692
    },
    {
      "epoch": 0.7920588235294118,
      "grad_norm": 0.05001905560493469,
      "learning_rate": 4.164948453608248e-05,
      "loss": 0.3117,
      "step": 2693
    },
    {
      "epoch": 0.7923529411764706,
      "grad_norm": 0.048483557999134064,
      "learning_rate": 4.1590574374079535e-05,
      "loss": 0.399,
      "step": 2694
    },
    {
      "epoch": 0.7926470588235294,
      "grad_norm": 0.05027348920702934,
      "learning_rate": 4.153166421207659e-05,
      "loss": 0.3055,
      "step": 2695
    },
    {
      "epoch": 0.7929411764705883,
      "grad_norm": 0.0480203703045845,
      "learning_rate": 4.147275405007364e-05,
      "loss": 0.3454,
      "step": 2696
    },
    {
      "epoch": 0.793235294117647,
      "grad_norm": 0.039168354123830795,
      "learning_rate": 4.1413843888070695e-05,
      "loss": 0.2449,
      "step": 2697
    },
    {
      "epoch": 0.7935294117647059,
      "grad_norm": 0.04636773094534874,
      "learning_rate": 4.135493372606775e-05,
      "loss": 0.2978,
      "step": 2698
    },
    {
      "epoch": 0.7938235294117647,
      "grad_norm": 0.04129250720143318,
      "learning_rate": 4.12960235640648e-05,
      "loss": 0.2921,
      "step": 2699
    },
    {
      "epoch": 0.7941176470588235,
      "grad_norm": 0.04227719455957413,
      "learning_rate": 4.1237113402061855e-05,
      "loss": 0.3187,
      "step": 2700
    },
    {
      "epoch": 0.7944117647058824,
      "grad_norm": 0.06290746480226517,
      "learning_rate": 4.117820324005891e-05,
      "loss": 0.4073,
      "step": 2701
    },
    {
      "epoch": 0.7947058823529412,
      "grad_norm": 0.04986351355910301,
      "learning_rate": 4.111929307805596e-05,
      "loss": 0.3315,
      "step": 2702
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.05034739524126053,
      "learning_rate": 4.106038291605302e-05,
      "loss": 0.3411,
      "step": 2703
    },
    {
      "epoch": 0.7952941176470588,
      "grad_norm": 0.04172324016690254,
      "learning_rate": 4.1001472754050075e-05,
      "loss": 0.2996,
      "step": 2704
    },
    {
      "epoch": 0.7955882352941176,
      "grad_norm": 0.04357687011361122,
      "learning_rate": 4.094256259204713e-05,
      "loss": 0.3824,
      "step": 2705
    },
    {
      "epoch": 0.7958823529411765,
      "grad_norm": 0.052455414086580276,
      "learning_rate": 4.088365243004418e-05,
      "loss": 0.3521,
      "step": 2706
    },
    {
      "epoch": 0.7961764705882353,
      "grad_norm": 0.054602522403001785,
      "learning_rate": 4.0824742268041235e-05,
      "loss": 0.3782,
      "step": 2707
    },
    {
      "epoch": 0.7964705882352942,
      "grad_norm": 0.043731946498155594,
      "learning_rate": 4.0765832106038296e-05,
      "loss": 0.3326,
      "step": 2708
    },
    {
      "epoch": 0.7967647058823529,
      "grad_norm": 0.04848922789096832,
      "learning_rate": 4.070692194403535e-05,
      "loss": 0.3305,
      "step": 2709
    },
    {
      "epoch": 0.7970588235294118,
      "grad_norm": 0.040510859340429306,
      "learning_rate": 4.06480117820324e-05,
      "loss": 0.302,
      "step": 2710
    },
    {
      "epoch": 0.7973529411764706,
      "grad_norm": 0.052050597965717316,
      "learning_rate": 4.0589101620029456e-05,
      "loss": 0.3225,
      "step": 2711
    },
    {
      "epoch": 0.7976470588235294,
      "grad_norm": 0.0338161326944828,
      "learning_rate": 4.053019145802651e-05,
      "loss": 0.2933,
      "step": 2712
    },
    {
      "epoch": 0.7979411764705883,
      "grad_norm": 0.05418168380856514,
      "learning_rate": 4.047128129602357e-05,
      "loss": 0.3879,
      "step": 2713
    },
    {
      "epoch": 0.798235294117647,
      "grad_norm": 0.04967905208468437,
      "learning_rate": 4.041237113402062e-05,
      "loss": 0.3462,
      "step": 2714
    },
    {
      "epoch": 0.7985294117647059,
      "grad_norm": 0.048429224640131,
      "learning_rate": 4.0353460972017676e-05,
      "loss": 0.3197,
      "step": 2715
    },
    {
      "epoch": 0.7988235294117647,
      "grad_norm": 0.05064113810658455,
      "learning_rate": 4.029455081001473e-05,
      "loss": 0.3505,
      "step": 2716
    },
    {
      "epoch": 0.7991176470588235,
      "grad_norm": 0.05375028774142265,
      "learning_rate": 4.023564064801178e-05,
      "loss": 0.3581,
      "step": 2717
    },
    {
      "epoch": 0.7994117647058824,
      "grad_norm": 0.058624375611543655,
      "learning_rate": 4.017673048600884e-05,
      "loss": 0.3068,
      "step": 2718
    },
    {
      "epoch": 0.7997058823529412,
      "grad_norm": 0.04926517978310585,
      "learning_rate": 4.0117820324005896e-05,
      "loss": 0.4318,
      "step": 2719
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06409692764282227,
      "learning_rate": 4.005891016200295e-05,
      "loss": 0.3836,
      "step": 2720
    },
    {
      "epoch": 0.8002941176470588,
      "grad_norm": 0.043360576033592224,
      "learning_rate": 4e-05,
      "loss": 0.2907,
      "step": 2721
    },
    {
      "epoch": 0.8005882352941176,
      "grad_norm": 0.04411977902054787,
      "learning_rate": 3.994108983799706e-05,
      "loss": 0.2665,
      "step": 2722
    },
    {
      "epoch": 0.8008823529411765,
      "grad_norm": 0.06458032876253128,
      "learning_rate": 3.988217967599412e-05,
      "loss": 0.4035,
      "step": 2723
    },
    {
      "epoch": 0.8011764705882353,
      "grad_norm": 0.05214209854602814,
      "learning_rate": 3.9823269513991163e-05,
      "loss": 0.3598,
      "step": 2724
    },
    {
      "epoch": 0.8014705882352942,
      "grad_norm": 0.04924268648028374,
      "learning_rate": 3.976435935198822e-05,
      "loss": 0.341,
      "step": 2725
    },
    {
      "epoch": 0.8017647058823529,
      "grad_norm": 0.04732133448123932,
      "learning_rate": 3.970544918998527e-05,
      "loss": 0.3084,
      "step": 2726
    },
    {
      "epoch": 0.8020588235294117,
      "grad_norm": 0.053274642676115036,
      "learning_rate": 3.9646539027982324e-05,
      "loss": 0.3694,
      "step": 2727
    },
    {
      "epoch": 0.8023529411764706,
      "grad_norm": 0.053677838295698166,
      "learning_rate": 3.9587628865979384e-05,
      "loss": 0.3053,
      "step": 2728
    },
    {
      "epoch": 0.8026470588235294,
      "grad_norm": 0.055276207625865936,
      "learning_rate": 3.952871870397644e-05,
      "loss": 0.3865,
      "step": 2729
    },
    {
      "epoch": 0.8029411764705883,
      "grad_norm": 0.050216857343912125,
      "learning_rate": 3.946980854197349e-05,
      "loss": 0.332,
      "step": 2730
    },
    {
      "epoch": 0.803235294117647,
      "grad_norm": 0.03689140826463699,
      "learning_rate": 3.9410898379970544e-05,
      "loss": 0.2793,
      "step": 2731
    },
    {
      "epoch": 0.8035294117647059,
      "grad_norm": 0.05070654675364494,
      "learning_rate": 3.93519882179676e-05,
      "loss": 0.4143,
      "step": 2732
    },
    {
      "epoch": 0.8038235294117647,
      "grad_norm": 0.05084521695971489,
      "learning_rate": 3.929307805596466e-05,
      "loss": 0.3759,
      "step": 2733
    },
    {
      "epoch": 0.8041176470588235,
      "grad_norm": 0.0494927279651165,
      "learning_rate": 3.923416789396171e-05,
      "loss": 0.346,
      "step": 2734
    },
    {
      "epoch": 0.8044117647058824,
      "grad_norm": 0.045482996851205826,
      "learning_rate": 3.9175257731958764e-05,
      "loss": 0.3295,
      "step": 2735
    },
    {
      "epoch": 0.8047058823529412,
      "grad_norm": 0.04459574446082115,
      "learning_rate": 3.911634756995582e-05,
      "loss": 0.3825,
      "step": 2736
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04802308976650238,
      "learning_rate": 3.905743740795287e-05,
      "loss": 0.3405,
      "step": 2737
    },
    {
      "epoch": 0.8052941176470588,
      "grad_norm": 0.04522540420293808,
      "learning_rate": 3.899852724594993e-05,
      "loss": 0.3779,
      "step": 2738
    },
    {
      "epoch": 0.8055882352941176,
      "grad_norm": 0.038214102387428284,
      "learning_rate": 3.8939617083946985e-05,
      "loss": 0.2804,
      "step": 2739
    },
    {
      "epoch": 0.8058823529411765,
      "grad_norm": 0.06651432812213898,
      "learning_rate": 3.888070692194404e-05,
      "loss": 0.3909,
      "step": 2740
    },
    {
      "epoch": 0.8061764705882353,
      "grad_norm": 0.04393036663532257,
      "learning_rate": 3.882179675994109e-05,
      "loss": 0.3218,
      "step": 2741
    },
    {
      "epoch": 0.8064705882352942,
      "grad_norm": 0.04543817415833473,
      "learning_rate": 3.8762886597938145e-05,
      "loss": 0.3737,
      "step": 2742
    },
    {
      "epoch": 0.8067647058823529,
      "grad_norm": 0.04516463726758957,
      "learning_rate": 3.8703976435935205e-05,
      "loss": 0.3698,
      "step": 2743
    },
    {
      "epoch": 0.8070588235294117,
      "grad_norm": 0.0499793142080307,
      "learning_rate": 3.864506627393226e-05,
      "loss": 0.3605,
      "step": 2744
    },
    {
      "epoch": 0.8073529411764706,
      "grad_norm": 0.048702701926231384,
      "learning_rate": 3.858615611192931e-05,
      "loss": 0.3648,
      "step": 2745
    },
    {
      "epoch": 0.8076470588235294,
      "grad_norm": 0.038533151149749756,
      "learning_rate": 3.8527245949926365e-05,
      "loss": 0.3231,
      "step": 2746
    },
    {
      "epoch": 0.8079411764705883,
      "grad_norm": 0.03608211874961853,
      "learning_rate": 3.846833578792342e-05,
      "loss": 0.2931,
      "step": 2747
    },
    {
      "epoch": 0.808235294117647,
      "grad_norm": 0.05533391609787941,
      "learning_rate": 3.840942562592048e-05,
      "loss": 0.3634,
      "step": 2748
    },
    {
      "epoch": 0.8085294117647058,
      "grad_norm": 0.05428295582532883,
      "learning_rate": 3.835051546391753e-05,
      "loss": 0.4001,
      "step": 2749
    },
    {
      "epoch": 0.8088235294117647,
      "grad_norm": 0.047772545367479324,
      "learning_rate": 3.8291605301914585e-05,
      "loss": 0.3785,
      "step": 2750
    },
    {
      "epoch": 0.8091176470588235,
      "grad_norm": 0.046697914600372314,
      "learning_rate": 3.823269513991163e-05,
      "loss": 0.291,
      "step": 2751
    },
    {
      "epoch": 0.8094117647058824,
      "grad_norm": 0.04964215308427811,
      "learning_rate": 3.8173784977908685e-05,
      "loss": 0.3835,
      "step": 2752
    },
    {
      "epoch": 0.8097058823529412,
      "grad_norm": 0.07445170730352402,
      "learning_rate": 3.8114874815905746e-05,
      "loss": 0.5142,
      "step": 2753
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05408201366662979,
      "learning_rate": 3.80559646539028e-05,
      "loss": 0.353,
      "step": 2754
    },
    {
      "epoch": 0.8102941176470588,
      "grad_norm": 0.049975745379924774,
      "learning_rate": 3.799705449189985e-05,
      "loss": 0.2828,
      "step": 2755
    },
    {
      "epoch": 0.8105882352941176,
      "grad_norm": 0.055002182722091675,
      "learning_rate": 3.7938144329896906e-05,
      "loss": 0.4266,
      "step": 2756
    },
    {
      "epoch": 0.8108823529411765,
      "grad_norm": 0.04946159943938255,
      "learning_rate": 3.787923416789396e-05,
      "loss": 0.3458,
      "step": 2757
    },
    {
      "epoch": 0.8111764705882353,
      "grad_norm": 0.048453085124492645,
      "learning_rate": 3.782032400589102e-05,
      "loss": 0.3533,
      "step": 2758
    },
    {
      "epoch": 0.8114705882352942,
      "grad_norm": 0.045329850167036057,
      "learning_rate": 3.776141384388807e-05,
      "loss": 0.3107,
      "step": 2759
    },
    {
      "epoch": 0.8117647058823529,
      "grad_norm": 0.04980900138616562,
      "learning_rate": 3.7702503681885126e-05,
      "loss": 0.3372,
      "step": 2760
    },
    {
      "epoch": 0.8120588235294117,
      "grad_norm": 0.04479609802365303,
      "learning_rate": 3.764359351988218e-05,
      "loss": 0.3336,
      "step": 2761
    },
    {
      "epoch": 0.8123529411764706,
      "grad_norm": 0.04628079757094383,
      "learning_rate": 3.758468335787923e-05,
      "loss": 0.3061,
      "step": 2762
    },
    {
      "epoch": 0.8126470588235294,
      "grad_norm": 0.052313052117824554,
      "learning_rate": 3.752577319587629e-05,
      "loss": 0.3574,
      "step": 2763
    },
    {
      "epoch": 0.8129411764705883,
      "grad_norm": 0.04227640479803085,
      "learning_rate": 3.7466863033873346e-05,
      "loss": 0.3458,
      "step": 2764
    },
    {
      "epoch": 0.8132352941176471,
      "grad_norm": 0.06016654521226883,
      "learning_rate": 3.74079528718704e-05,
      "loss": 0.3778,
      "step": 2765
    },
    {
      "epoch": 0.8135294117647058,
      "grad_norm": 0.058240488171577454,
      "learning_rate": 3.734904270986745e-05,
      "loss": 0.3815,
      "step": 2766
    },
    {
      "epoch": 0.8138235294117647,
      "grad_norm": 0.061017319560050964,
      "learning_rate": 3.729013254786451e-05,
      "loss": 0.3744,
      "step": 2767
    },
    {
      "epoch": 0.8141176470588235,
      "grad_norm": 0.05616682022809982,
      "learning_rate": 3.723122238586157e-05,
      "loss": 0.3594,
      "step": 2768
    },
    {
      "epoch": 0.8144117647058824,
      "grad_norm": 0.04840107634663582,
      "learning_rate": 3.717231222385862e-05,
      "loss": 0.3369,
      "step": 2769
    },
    {
      "epoch": 0.8147058823529412,
      "grad_norm": 0.052199412137269974,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 0.3208,
      "step": 2770
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06585270911455154,
      "learning_rate": 3.705449189985273e-05,
      "loss": 0.3816,
      "step": 2771
    },
    {
      "epoch": 0.8152941176470588,
      "grad_norm": 0.04047274962067604,
      "learning_rate": 3.699558173784978e-05,
      "loss": 0.3255,
      "step": 2772
    },
    {
      "epoch": 0.8155882352941176,
      "grad_norm": 0.046827711164951324,
      "learning_rate": 3.693667157584684e-05,
      "loss": 0.3436,
      "step": 2773
    },
    {
      "epoch": 0.8158823529411765,
      "grad_norm": 0.04838196560740471,
      "learning_rate": 3.6877761413843894e-05,
      "loss": 0.3748,
      "step": 2774
    },
    {
      "epoch": 0.8161764705882353,
      "grad_norm": 0.0550498440861702,
      "learning_rate": 3.681885125184095e-05,
      "loss": 0.3718,
      "step": 2775
    },
    {
      "epoch": 0.8164705882352942,
      "grad_norm": 0.060999978333711624,
      "learning_rate": 3.6759941089838e-05,
      "loss": 0.4431,
      "step": 2776
    },
    {
      "epoch": 0.816764705882353,
      "grad_norm": 0.057094722986221313,
      "learning_rate": 3.670103092783505e-05,
      "loss": 0.4041,
      "step": 2777
    },
    {
      "epoch": 0.8170588235294117,
      "grad_norm": 0.04865676537156105,
      "learning_rate": 3.664212076583211e-05,
      "loss": 0.3329,
      "step": 2778
    },
    {
      "epoch": 0.8173529411764706,
      "grad_norm": 0.04940241575241089,
      "learning_rate": 3.658321060382916e-05,
      "loss": 0.3358,
      "step": 2779
    },
    {
      "epoch": 0.8176470588235294,
      "grad_norm": 0.04951189085841179,
      "learning_rate": 3.6524300441826214e-05,
      "loss": 0.3342,
      "step": 2780
    },
    {
      "epoch": 0.8179411764705883,
      "grad_norm": 0.0564563050866127,
      "learning_rate": 3.646539027982327e-05,
      "loss": 0.3856,
      "step": 2781
    },
    {
      "epoch": 0.8182352941176471,
      "grad_norm": 0.05103897675871849,
      "learning_rate": 3.640648011782032e-05,
      "loss": 0.3573,
      "step": 2782
    },
    {
      "epoch": 0.8185294117647058,
      "grad_norm": 0.05993961542844772,
      "learning_rate": 3.634756995581738e-05,
      "loss": 0.3995,
      "step": 2783
    },
    {
      "epoch": 0.8188235294117647,
      "grad_norm": 0.04802112653851509,
      "learning_rate": 3.6288659793814435e-05,
      "loss": 0.3436,
      "step": 2784
    },
    {
      "epoch": 0.8191176470588235,
      "grad_norm": 0.03792214021086693,
      "learning_rate": 3.622974963181149e-05,
      "loss": 0.2588,
      "step": 2785
    },
    {
      "epoch": 0.8194117647058824,
      "grad_norm": 0.04608818143606186,
      "learning_rate": 3.617083946980854e-05,
      "loss": 0.3262,
      "step": 2786
    },
    {
      "epoch": 0.8197058823529412,
      "grad_norm": 0.0645453929901123,
      "learning_rate": 3.6111929307805595e-05,
      "loss": 0.4411,
      "step": 2787
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.061316587030887604,
      "learning_rate": 3.6053019145802655e-05,
      "loss": 0.3837,
      "step": 2788
    },
    {
      "epoch": 0.8202941176470588,
      "grad_norm": 0.04097369685769081,
      "learning_rate": 3.599410898379971e-05,
      "loss": 0.3213,
      "step": 2789
    },
    {
      "epoch": 0.8205882352941176,
      "grad_norm": 0.0458800308406353,
      "learning_rate": 3.593519882179676e-05,
      "loss": 0.3716,
      "step": 2790
    },
    {
      "epoch": 0.8208823529411765,
      "grad_norm": 0.051114920526742935,
      "learning_rate": 3.5876288659793815e-05,
      "loss": 0.3135,
      "step": 2791
    },
    {
      "epoch": 0.8211764705882353,
      "grad_norm": 0.0562237985432148,
      "learning_rate": 3.581737849779087e-05,
      "loss": 0.3946,
      "step": 2792
    },
    {
      "epoch": 0.8214705882352941,
      "grad_norm": 0.048140574246644974,
      "learning_rate": 3.575846833578793e-05,
      "loss": 0.376,
      "step": 2793
    },
    {
      "epoch": 0.821764705882353,
      "grad_norm": 0.059897251427173615,
      "learning_rate": 3.569955817378498e-05,
      "loss": 0.4189,
      "step": 2794
    },
    {
      "epoch": 0.8220588235294117,
      "grad_norm": 0.05568714812397957,
      "learning_rate": 3.5640648011782035e-05,
      "loss": 0.3696,
      "step": 2795
    },
    {
      "epoch": 0.8223529411764706,
      "grad_norm": 0.04653725400567055,
      "learning_rate": 3.558173784977909e-05,
      "loss": 0.3918,
      "step": 2796
    },
    {
      "epoch": 0.8226470588235294,
      "grad_norm": 0.05169306695461273,
      "learning_rate": 3.552282768777614e-05,
      "loss": 0.4061,
      "step": 2797
    },
    {
      "epoch": 0.8229411764705883,
      "grad_norm": 0.05127384886145592,
      "learning_rate": 3.54639175257732e-05,
      "loss": 0.3216,
      "step": 2798
    },
    {
      "epoch": 0.8232352941176471,
      "grad_norm": 0.04099204018712044,
      "learning_rate": 3.5405007363770256e-05,
      "loss": 0.2762,
      "step": 2799
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 0.05830424651503563,
      "learning_rate": 3.534609720176731e-05,
      "loss": 0.343,
      "step": 2800
    },
    {
      "epoch": 0.8238235294117647,
      "grad_norm": 0.041503310203552246,
      "learning_rate": 3.528718703976436e-05,
      "loss": 0.3758,
      "step": 2801
    },
    {
      "epoch": 0.8241176470588235,
      "grad_norm": 0.0445149727165699,
      "learning_rate": 3.5228276877761416e-05,
      "loss": 0.3079,
      "step": 2802
    },
    {
      "epoch": 0.8244117647058824,
      "grad_norm": 0.05369013547897339,
      "learning_rate": 3.5169366715758476e-05,
      "loss": 0.3536,
      "step": 2803
    },
    {
      "epoch": 0.8247058823529412,
      "grad_norm": 0.053984634578228,
      "learning_rate": 3.511045655375552e-05,
      "loss": 0.3447,
      "step": 2804
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.04739730805158615,
      "learning_rate": 3.5051546391752576e-05,
      "loss": 0.3487,
      "step": 2805
    },
    {
      "epoch": 0.8252941176470588,
      "grad_norm": 0.04151952266693115,
      "learning_rate": 3.499263622974963e-05,
      "loss": 0.4014,
      "step": 2806
    },
    {
      "epoch": 0.8255882352941176,
      "grad_norm": 0.03796389698982239,
      "learning_rate": 3.493372606774668e-05,
      "loss": 0.2992,
      "step": 2807
    },
    {
      "epoch": 0.8258823529411765,
      "grad_norm": 0.04262728989124298,
      "learning_rate": 3.487481590574374e-05,
      "loss": 0.3589,
      "step": 2808
    },
    {
      "epoch": 0.8261764705882353,
      "grad_norm": 0.0553639754652977,
      "learning_rate": 3.4815905743740796e-05,
      "loss": 0.3507,
      "step": 2809
    },
    {
      "epoch": 0.8264705882352941,
      "grad_norm": 0.03560487553477287,
      "learning_rate": 3.475699558173785e-05,
      "loss": 0.3307,
      "step": 2810
    },
    {
      "epoch": 0.826764705882353,
      "grad_norm": 0.03463674709200859,
      "learning_rate": 3.46980854197349e-05,
      "loss": 0.2739,
      "step": 2811
    },
    {
      "epoch": 0.8270588235294117,
      "grad_norm": 0.05361563339829445,
      "learning_rate": 3.4639175257731957e-05,
      "loss": 0.3402,
      "step": 2812
    },
    {
      "epoch": 0.8273529411764706,
      "grad_norm": 0.05343262478709221,
      "learning_rate": 3.458026509572902e-05,
      "loss": 0.3971,
      "step": 2813
    },
    {
      "epoch": 0.8276470588235294,
      "grad_norm": 0.05263229086995125,
      "learning_rate": 3.452135493372607e-05,
      "loss": 0.37,
      "step": 2814
    },
    {
      "epoch": 0.8279411764705882,
      "grad_norm": 0.03978682681918144,
      "learning_rate": 3.4462444771723124e-05,
      "loss": 0.3306,
      "step": 2815
    },
    {
      "epoch": 0.8282352941176471,
      "grad_norm": 0.03867819532752037,
      "learning_rate": 3.440353460972018e-05,
      "loss": 0.3166,
      "step": 2816
    },
    {
      "epoch": 0.8285294117647058,
      "grad_norm": 0.04166262596845627,
      "learning_rate": 3.434462444771723e-05,
      "loss": 0.3351,
      "step": 2817
    },
    {
      "epoch": 0.8288235294117647,
      "grad_norm": 0.05759808421134949,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.377,
      "step": 2818
    },
    {
      "epoch": 0.8291176470588235,
      "grad_norm": 0.07115340232849121,
      "learning_rate": 3.4226804123711344e-05,
      "loss": 0.3767,
      "step": 2819
    },
    {
      "epoch": 0.8294117647058824,
      "grad_norm": 0.03827952221035957,
      "learning_rate": 3.41678939617084e-05,
      "loss": 0.2934,
      "step": 2820
    },
    {
      "epoch": 0.8297058823529412,
      "grad_norm": 0.05288894847035408,
      "learning_rate": 3.410898379970545e-05,
      "loss": 0.3299,
      "step": 2821
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.05342038348317146,
      "learning_rate": 3.4050073637702504e-05,
      "loss": 0.372,
      "step": 2822
    },
    {
      "epoch": 0.8302941176470588,
      "grad_norm": 0.05277183651924133,
      "learning_rate": 3.3991163475699564e-05,
      "loss": 0.4002,
      "step": 2823
    },
    {
      "epoch": 0.8305882352941176,
      "grad_norm": 0.04315470531582832,
      "learning_rate": 3.393225331369662e-05,
      "loss": 0.3226,
      "step": 2824
    },
    {
      "epoch": 0.8308823529411765,
      "grad_norm": 0.055317364633083344,
      "learning_rate": 3.387334315169367e-05,
      "loss": 0.4287,
      "step": 2825
    },
    {
      "epoch": 0.8311764705882353,
      "grad_norm": 0.05312599986791611,
      "learning_rate": 3.3814432989690724e-05,
      "loss": 0.3514,
      "step": 2826
    },
    {
      "epoch": 0.8314705882352941,
      "grad_norm": 0.04017631337046623,
      "learning_rate": 3.375552282768778e-05,
      "loss": 0.3041,
      "step": 2827
    },
    {
      "epoch": 0.831764705882353,
      "grad_norm": 0.038324687629938126,
      "learning_rate": 3.369661266568484e-05,
      "loss": 0.2667,
      "step": 2828
    },
    {
      "epoch": 0.8320588235294117,
      "grad_norm": 0.05086252838373184,
      "learning_rate": 3.363770250368189e-05,
      "loss": 0.3568,
      "step": 2829
    },
    {
      "epoch": 0.8323529411764706,
      "grad_norm": 0.04857271537184715,
      "learning_rate": 3.357879234167894e-05,
      "loss": 0.3435,
      "step": 2830
    },
    {
      "epoch": 0.8326470588235294,
      "grad_norm": 0.04104543477296829,
      "learning_rate": 3.351988217967599e-05,
      "loss": 0.2801,
      "step": 2831
    },
    {
      "epoch": 0.8329411764705882,
      "grad_norm": 0.042275410145521164,
      "learning_rate": 3.3460972017673045e-05,
      "loss": 0.342,
      "step": 2832
    },
    {
      "epoch": 0.8332352941176471,
      "grad_norm": 0.04045342653989792,
      "learning_rate": 3.3402061855670105e-05,
      "loss": 0.257,
      "step": 2833
    },
    {
      "epoch": 0.8335294117647059,
      "grad_norm": 0.040513284504413605,
      "learning_rate": 3.334315169366716e-05,
      "loss": 0.2616,
      "step": 2834
    },
    {
      "epoch": 0.8338235294117647,
      "grad_norm": 0.04856212064623833,
      "learning_rate": 3.328424153166421e-05,
      "loss": 0.3456,
      "step": 2835
    },
    {
      "epoch": 0.8341176470588235,
      "grad_norm": 0.040209122002124786,
      "learning_rate": 3.3225331369661265e-05,
      "loss": 0.3241,
      "step": 2836
    },
    {
      "epoch": 0.8344117647058824,
      "grad_norm": 0.04634251818060875,
      "learning_rate": 3.316642120765832e-05,
      "loss": 0.3857,
      "step": 2837
    },
    {
      "epoch": 0.8347058823529412,
      "grad_norm": 0.05856012925505638,
      "learning_rate": 3.310751104565538e-05,
      "loss": 0.421,
      "step": 2838
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.06527136266231537,
      "learning_rate": 3.304860088365243e-05,
      "loss": 0.3788,
      "step": 2839
    },
    {
      "epoch": 0.8352941176470589,
      "grad_norm": 0.06230945885181427,
      "learning_rate": 3.2989690721649485e-05,
      "loss": 0.3602,
      "step": 2840
    },
    {
      "epoch": 0.8355882352941176,
      "grad_norm": 0.04676260054111481,
      "learning_rate": 3.293078055964654e-05,
      "loss": 0.3918,
      "step": 2841
    },
    {
      "epoch": 0.8358823529411765,
      "grad_norm": 0.04608722776174545,
      "learning_rate": 3.287187039764359e-05,
      "loss": 0.319,
      "step": 2842
    },
    {
      "epoch": 0.8361764705882353,
      "grad_norm": 0.05909409373998642,
      "learning_rate": 3.281296023564065e-05,
      "loss": 0.387,
      "step": 2843
    },
    {
      "epoch": 0.8364705882352941,
      "grad_norm": 0.04287169128656387,
      "learning_rate": 3.2754050073637706e-05,
      "loss": 0.3333,
      "step": 2844
    },
    {
      "epoch": 0.836764705882353,
      "grad_norm": 0.05017166584730148,
      "learning_rate": 3.269513991163476e-05,
      "loss": 0.3557,
      "step": 2845
    },
    {
      "epoch": 0.8370588235294117,
      "grad_norm": 0.04767812043428421,
      "learning_rate": 3.263622974963181e-05,
      "loss": 0.3619,
      "step": 2846
    },
    {
      "epoch": 0.8373529411764706,
      "grad_norm": 0.06252284348011017,
      "learning_rate": 3.2577319587628866e-05,
      "loss": 0.4248,
      "step": 2847
    },
    {
      "epoch": 0.8376470588235294,
      "grad_norm": 0.05717552453279495,
      "learning_rate": 3.2518409425625926e-05,
      "loss": 0.386,
      "step": 2848
    },
    {
      "epoch": 0.8379411764705882,
      "grad_norm": 0.04774545878171921,
      "learning_rate": 3.245949926362298e-05,
      "loss": 0.3528,
      "step": 2849
    },
    {
      "epoch": 0.8382352941176471,
      "grad_norm": 0.04838574677705765,
      "learning_rate": 3.240058910162003e-05,
      "loss": 0.3579,
      "step": 2850
    },
    {
      "epoch": 0.8385294117647059,
      "grad_norm": 0.05710386484861374,
      "learning_rate": 3.2341678939617086e-05,
      "loss": 0.3489,
      "step": 2851
    },
    {
      "epoch": 0.8388235294117647,
      "grad_norm": 0.06314649432897568,
      "learning_rate": 3.228276877761414e-05,
      "loss": 0.4468,
      "step": 2852
    },
    {
      "epoch": 0.8391176470588235,
      "grad_norm": 0.0440404936671257,
      "learning_rate": 3.22238586156112e-05,
      "loss": 0.3591,
      "step": 2853
    },
    {
      "epoch": 0.8394117647058823,
      "grad_norm": 0.052133314311504364,
      "learning_rate": 3.216494845360825e-05,
      "loss": 0.3797,
      "step": 2854
    },
    {
      "epoch": 0.8397058823529412,
      "grad_norm": 0.0492379330098629,
      "learning_rate": 3.2106038291605307e-05,
      "loss": 0.3953,
      "step": 2855
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.05858398601412773,
      "learning_rate": 3.204712812960236e-05,
      "loss": 0.3565,
      "step": 2856
    },
    {
      "epoch": 0.8402941176470589,
      "grad_norm": 0.044241633266210556,
      "learning_rate": 3.1988217967599407e-05,
      "loss": 0.3553,
      "step": 2857
    },
    {
      "epoch": 0.8405882352941176,
      "grad_norm": 0.05383797362446785,
      "learning_rate": 3.192930780559647e-05,
      "loss": 0.4185,
      "step": 2858
    },
    {
      "epoch": 0.8408823529411765,
      "grad_norm": 0.04965043440461159,
      "learning_rate": 3.187039764359352e-05,
      "loss": 0.3469,
      "step": 2859
    },
    {
      "epoch": 0.8411764705882353,
      "grad_norm": 0.054218094795942307,
      "learning_rate": 3.1811487481590573e-05,
      "loss": 0.3574,
      "step": 2860
    },
    {
      "epoch": 0.8414705882352941,
      "grad_norm": 0.05171547457575798,
      "learning_rate": 3.175257731958763e-05,
      "loss": 0.3289,
      "step": 2861
    },
    {
      "epoch": 0.841764705882353,
      "grad_norm": 0.05857101082801819,
      "learning_rate": 3.169366715758468e-05,
      "loss": 0.387,
      "step": 2862
    },
    {
      "epoch": 0.8420588235294117,
      "grad_norm": 0.0466507263481617,
      "learning_rate": 3.163475699558174e-05,
      "loss": 0.3731,
      "step": 2863
    },
    {
      "epoch": 0.8423529411764706,
      "grad_norm": 0.0575953833758831,
      "learning_rate": 3.1575846833578794e-05,
      "loss": 0.3856,
      "step": 2864
    },
    {
      "epoch": 0.8426470588235294,
      "grad_norm": 0.04673353582620621,
      "learning_rate": 3.151693667157585e-05,
      "loss": 0.3468,
      "step": 2865
    },
    {
      "epoch": 0.8429411764705882,
      "grad_norm": 0.05318289250135422,
      "learning_rate": 3.14580265095729e-05,
      "loss": 0.3874,
      "step": 2866
    },
    {
      "epoch": 0.8432352941176471,
      "grad_norm": 0.048787809908390045,
      "learning_rate": 3.1399116347569954e-05,
      "loss": 0.3652,
      "step": 2867
    },
    {
      "epoch": 0.8435294117647059,
      "grad_norm": 0.060411639511585236,
      "learning_rate": 3.1340206185567014e-05,
      "loss": 0.361,
      "step": 2868
    },
    {
      "epoch": 0.8438235294117648,
      "grad_norm": 0.03720911592245102,
      "learning_rate": 3.128129602356407e-05,
      "loss": 0.2963,
      "step": 2869
    },
    {
      "epoch": 0.8441176470588235,
      "grad_norm": 0.0491618774831295,
      "learning_rate": 3.122238586156112e-05,
      "loss": 0.3738,
      "step": 2870
    },
    {
      "epoch": 0.8444117647058823,
      "grad_norm": 0.05656241625547409,
      "learning_rate": 3.1163475699558174e-05,
      "loss": 0.3444,
      "step": 2871
    },
    {
      "epoch": 0.8447058823529412,
      "grad_norm": 0.0564546175301075,
      "learning_rate": 3.110456553755523e-05,
      "loss": 0.413,
      "step": 2872
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.0412025973200798,
      "learning_rate": 3.104565537555229e-05,
      "loss": 0.2641,
      "step": 2873
    },
    {
      "epoch": 0.8452941176470589,
      "grad_norm": 0.036519475281238556,
      "learning_rate": 3.098674521354934e-05,
      "loss": 0.2622,
      "step": 2874
    },
    {
      "epoch": 0.8455882352941176,
      "grad_norm": 0.05029819533228874,
      "learning_rate": 3.0927835051546395e-05,
      "loss": 0.3128,
      "step": 2875
    },
    {
      "epoch": 0.8458823529411764,
      "grad_norm": 0.03448224067687988,
      "learning_rate": 3.086892488954345e-05,
      "loss": 0.253,
      "step": 2876
    },
    {
      "epoch": 0.8461764705882353,
      "grad_norm": 0.05537790805101395,
      "learning_rate": 3.08100147275405e-05,
      "loss": 0.3879,
      "step": 2877
    },
    {
      "epoch": 0.8464705882352941,
      "grad_norm": 0.06043592840433121,
      "learning_rate": 3.075110456553756e-05,
      "loss": 0.3667,
      "step": 2878
    },
    {
      "epoch": 0.846764705882353,
      "grad_norm": 0.04668039828538895,
      "learning_rate": 3.0692194403534615e-05,
      "loss": 0.3606,
      "step": 2879
    },
    {
      "epoch": 0.8470588235294118,
      "grad_norm": 0.05118879675865173,
      "learning_rate": 3.063328424153167e-05,
      "loss": 0.3334,
      "step": 2880
    },
    {
      "epoch": 0.8473529411764706,
      "grad_norm": 0.04890558123588562,
      "learning_rate": 3.057437407952872e-05,
      "loss": 0.291,
      "step": 2881
    },
    {
      "epoch": 0.8476470588235294,
      "grad_norm": 0.04442369565367699,
      "learning_rate": 3.051546391752578e-05,
      "loss": 0.3094,
      "step": 2882
    },
    {
      "epoch": 0.8479411764705882,
      "grad_norm": 0.05615631118416786,
      "learning_rate": 3.0456553755522825e-05,
      "loss": 0.3852,
      "step": 2883
    },
    {
      "epoch": 0.8482352941176471,
      "grad_norm": 0.04870303347706795,
      "learning_rate": 3.0397643593519882e-05,
      "loss": 0.3469,
      "step": 2884
    },
    {
      "epoch": 0.8485294117647059,
      "grad_norm": 0.050383612513542175,
      "learning_rate": 3.0338733431516935e-05,
      "loss": 0.4573,
      "step": 2885
    },
    {
      "epoch": 0.8488235294117648,
      "grad_norm": 0.06001172214746475,
      "learning_rate": 3.0279823269513992e-05,
      "loss": 0.3809,
      "step": 2886
    },
    {
      "epoch": 0.8491176470588235,
      "grad_norm": 0.04871842637658119,
      "learning_rate": 3.0220913107511045e-05,
      "loss": 0.3206,
      "step": 2887
    },
    {
      "epoch": 0.8494117647058823,
      "grad_norm": 0.04912487044930458,
      "learning_rate": 3.01620029455081e-05,
      "loss": 0.3067,
      "step": 2888
    },
    {
      "epoch": 0.8497058823529412,
      "grad_norm": 0.04097698628902435,
      "learning_rate": 3.0103092783505156e-05,
      "loss": 0.3269,
      "step": 2889
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.04757946729660034,
      "learning_rate": 3.004418262150221e-05,
      "loss": 0.3578,
      "step": 2890
    },
    {
      "epoch": 0.8502941176470589,
      "grad_norm": 0.04308750480413437,
      "learning_rate": 2.9985272459499262e-05,
      "loss": 0.3314,
      "step": 2891
    },
    {
      "epoch": 0.8505882352941176,
      "grad_norm": 0.04286596551537514,
      "learning_rate": 2.992636229749632e-05,
      "loss": 0.318,
      "step": 2892
    },
    {
      "epoch": 0.8508823529411764,
      "grad_norm": 0.04075123742222786,
      "learning_rate": 2.9867452135493373e-05,
      "loss": 0.3216,
      "step": 2893
    },
    {
      "epoch": 0.8511764705882353,
      "grad_norm": 0.05012349411845207,
      "learning_rate": 2.980854197349043e-05,
      "loss": 0.3917,
      "step": 2894
    },
    {
      "epoch": 0.8514705882352941,
      "grad_norm": 0.045586396008729935,
      "learning_rate": 2.9749631811487483e-05,
      "loss": 0.3439,
      "step": 2895
    },
    {
      "epoch": 0.851764705882353,
      "grad_norm": 0.05322425439953804,
      "learning_rate": 2.9690721649484536e-05,
      "loss": 0.3556,
      "step": 2896
    },
    {
      "epoch": 0.8520588235294118,
      "grad_norm": 0.04789445921778679,
      "learning_rate": 2.9631811487481593e-05,
      "loss": 0.3648,
      "step": 2897
    },
    {
      "epoch": 0.8523529411764705,
      "grad_norm": 0.03528520464897156,
      "learning_rate": 2.9572901325478646e-05,
      "loss": 0.2706,
      "step": 2898
    },
    {
      "epoch": 0.8526470588235294,
      "grad_norm": 0.032584454864263535,
      "learning_rate": 2.9513991163475703e-05,
      "loss": 0.2721,
      "step": 2899
    },
    {
      "epoch": 0.8529411764705882,
      "grad_norm": 0.04542496055364609,
      "learning_rate": 2.9455081001472756e-05,
      "loss": 0.3366,
      "step": 2900
    },
    {
      "epoch": 0.8532352941176471,
      "grad_norm": 0.046282991766929626,
      "learning_rate": 2.939617083946981e-05,
      "loss": 0.3304,
      "step": 2901
    },
    {
      "epoch": 0.8535294117647059,
      "grad_norm": 0.061427850276231766,
      "learning_rate": 2.9337260677466867e-05,
      "loss": 0.3604,
      "step": 2902
    },
    {
      "epoch": 0.8538235294117648,
      "grad_norm": 0.0462501160800457,
      "learning_rate": 2.927835051546392e-05,
      "loss": 0.3983,
      "step": 2903
    },
    {
      "epoch": 0.8541176470588235,
      "grad_norm": 0.06410367041826248,
      "learning_rate": 2.9219440353460973e-05,
      "loss": 0.3906,
      "step": 2904
    },
    {
      "epoch": 0.8544117647058823,
      "grad_norm": 0.04901789873838425,
      "learning_rate": 2.916053019145803e-05,
      "loss": 0.3738,
      "step": 2905
    },
    {
      "epoch": 0.8547058823529412,
      "grad_norm": 0.04982336238026619,
      "learning_rate": 2.9101620029455084e-05,
      "loss": 0.3312,
      "step": 2906
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.0518890880048275,
      "learning_rate": 2.904270986745214e-05,
      "loss": 0.3539,
      "step": 2907
    },
    {
      "epoch": 0.8552941176470589,
      "grad_norm": 0.06771092116832733,
      "learning_rate": 2.8983799705449194e-05,
      "loss": 0.3061,
      "step": 2908
    },
    {
      "epoch": 0.8555882352941176,
      "grad_norm": 0.03933561220765114,
      "learning_rate": 2.8924889543446247e-05,
      "loss": 0.323,
      "step": 2909
    },
    {
      "epoch": 0.8558823529411764,
      "grad_norm": 0.057433899492025375,
      "learning_rate": 2.8865979381443297e-05,
      "loss": 0.3631,
      "step": 2910
    },
    {
      "epoch": 0.8561764705882353,
      "grad_norm": 0.03687005490064621,
      "learning_rate": 2.8807069219440354e-05,
      "loss": 0.2861,
      "step": 2911
    },
    {
      "epoch": 0.8564705882352941,
      "grad_norm": 0.05365350469946861,
      "learning_rate": 2.8748159057437407e-05,
      "loss": 0.37,
      "step": 2912
    },
    {
      "epoch": 0.856764705882353,
      "grad_norm": 0.047709111124277115,
      "learning_rate": 2.868924889543446e-05,
      "loss": 0.3394,
      "step": 2913
    },
    {
      "epoch": 0.8570588235294118,
      "grad_norm": 0.04034966602921486,
      "learning_rate": 2.8630338733431517e-05,
      "loss": 0.29,
      "step": 2914
    },
    {
      "epoch": 0.8573529411764705,
      "grad_norm": 0.04667127877473831,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.3328,
      "step": 2915
    },
    {
      "epoch": 0.8576470588235294,
      "grad_norm": 0.0530979186296463,
      "learning_rate": 2.8512518409425624e-05,
      "loss": 0.4035,
      "step": 2916
    },
    {
      "epoch": 0.8579411764705882,
      "grad_norm": 0.054946910589933395,
      "learning_rate": 2.845360824742268e-05,
      "loss": 0.3226,
      "step": 2917
    },
    {
      "epoch": 0.8582352941176471,
      "grad_norm": 0.059641849249601364,
      "learning_rate": 2.8394698085419734e-05,
      "loss": 0.4313,
      "step": 2918
    },
    {
      "epoch": 0.8585294117647059,
      "grad_norm": 0.045903898775577545,
      "learning_rate": 2.833578792341679e-05,
      "loss": 0.3335,
      "step": 2919
    },
    {
      "epoch": 0.8588235294117647,
      "grad_norm": 0.04622720181941986,
      "learning_rate": 2.8276877761413845e-05,
      "loss": 0.3706,
      "step": 2920
    },
    {
      "epoch": 0.8591176470588235,
      "grad_norm": 0.044697992503643036,
      "learning_rate": 2.8217967599410898e-05,
      "loss": 0.3137,
      "step": 2921
    },
    {
      "epoch": 0.8594117647058823,
      "grad_norm": 0.05136140063405037,
      "learning_rate": 2.8159057437407955e-05,
      "loss": 0.3657,
      "step": 2922
    },
    {
      "epoch": 0.8597058823529412,
      "grad_norm": 0.036645229905843735,
      "learning_rate": 2.8100147275405008e-05,
      "loss": 0.2746,
      "step": 2923
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.03904839977622032,
      "learning_rate": 2.8041237113402065e-05,
      "loss": 0.3108,
      "step": 2924
    },
    {
      "epoch": 0.8602941176470589,
      "grad_norm": 0.047981053590774536,
      "learning_rate": 2.798232695139912e-05,
      "loss": 0.3131,
      "step": 2925
    },
    {
      "epoch": 0.8605882352941177,
      "grad_norm": 0.04272201284766197,
      "learning_rate": 2.7923416789396172e-05,
      "loss": 0.3104,
      "step": 2926
    },
    {
      "epoch": 0.8608823529411764,
      "grad_norm": 0.04266549274325371,
      "learning_rate": 2.786450662739323e-05,
      "loss": 0.3258,
      "step": 2927
    },
    {
      "epoch": 0.8611764705882353,
      "grad_norm": 0.06382425874471664,
      "learning_rate": 2.7805596465390282e-05,
      "loss": 0.3979,
      "step": 2928
    },
    {
      "epoch": 0.8614705882352941,
      "grad_norm": 0.04887080192565918,
      "learning_rate": 2.774668630338734e-05,
      "loss": 0.2577,
      "step": 2929
    },
    {
      "epoch": 0.861764705882353,
      "grad_norm": 0.05238350108265877,
      "learning_rate": 2.7687776141384392e-05,
      "loss": 0.3365,
      "step": 2930
    },
    {
      "epoch": 0.8620588235294118,
      "grad_norm": 0.03941403701901436,
      "learning_rate": 2.7628865979381445e-05,
      "loss": 0.3011,
      "step": 2931
    },
    {
      "epoch": 0.8623529411764705,
      "grad_norm": 0.050649065524339676,
      "learning_rate": 2.7569955817378502e-05,
      "loss": 0.3498,
      "step": 2932
    },
    {
      "epoch": 0.8626470588235294,
      "grad_norm": 0.04431651532649994,
      "learning_rate": 2.7511045655375556e-05,
      "loss": 0.3363,
      "step": 2933
    },
    {
      "epoch": 0.8629411764705882,
      "grad_norm": 0.04962174966931343,
      "learning_rate": 2.745213549337261e-05,
      "loss": 0.387,
      "step": 2934
    },
    {
      "epoch": 0.8632352941176471,
      "grad_norm": 0.046640872955322266,
      "learning_rate": 2.7393225331369666e-05,
      "loss": 0.3707,
      "step": 2935
    },
    {
      "epoch": 0.8635294117647059,
      "grad_norm": 0.04805915057659149,
      "learning_rate": 2.7334315169366716e-05,
      "loss": 0.3567,
      "step": 2936
    },
    {
      "epoch": 0.8638235294117647,
      "grad_norm": 0.04055362567305565,
      "learning_rate": 2.727540500736377e-05,
      "loss": 0.3346,
      "step": 2937
    },
    {
      "epoch": 0.8641176470588235,
      "grad_norm": 0.056987229734659195,
      "learning_rate": 2.7216494845360823e-05,
      "loss": 0.4282,
      "step": 2938
    },
    {
      "epoch": 0.8644117647058823,
      "grad_norm": 0.045160580426454544,
      "learning_rate": 2.715758468335788e-05,
      "loss": 0.3029,
      "step": 2939
    },
    {
      "epoch": 0.8647058823529412,
      "grad_norm": 0.041883304715156555,
      "learning_rate": 2.7098674521354933e-05,
      "loss": 0.3593,
      "step": 2940
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.04741509631276131,
      "learning_rate": 2.703976435935199e-05,
      "loss": 0.3126,
      "step": 2941
    },
    {
      "epoch": 0.8652941176470588,
      "grad_norm": 0.046829719096422195,
      "learning_rate": 2.6980854197349043e-05,
      "loss": 0.3202,
      "step": 2942
    },
    {
      "epoch": 0.8655882352941177,
      "grad_norm": 0.05288582295179367,
      "learning_rate": 2.6921944035346096e-05,
      "loss": 0.4259,
      "step": 2943
    },
    {
      "epoch": 0.8658823529411764,
      "grad_norm": 0.03944062441587448,
      "learning_rate": 2.6863033873343153e-05,
      "loss": 0.3232,
      "step": 2944
    },
    {
      "epoch": 0.8661764705882353,
      "grad_norm": 0.0394354872405529,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.265,
      "step": 2945
    },
    {
      "epoch": 0.8664705882352941,
      "grad_norm": 0.05304459482431412,
      "learning_rate": 2.674521354933726e-05,
      "loss": 0.3376,
      "step": 2946
    },
    {
      "epoch": 0.866764705882353,
      "grad_norm": 0.05497744306921959,
      "learning_rate": 2.6686303387334317e-05,
      "loss": 0.3759,
      "step": 2947
    },
    {
      "epoch": 0.8670588235294118,
      "grad_norm": 0.05649145692586899,
      "learning_rate": 2.662739322533137e-05,
      "loss": 0.3414,
      "step": 2948
    },
    {
      "epoch": 0.8673529411764705,
      "grad_norm": 0.048805370926856995,
      "learning_rate": 2.6568483063328427e-05,
      "loss": 0.364,
      "step": 2949
    },
    {
      "epoch": 0.8676470588235294,
      "grad_norm": 0.06224207952618599,
      "learning_rate": 2.650957290132548e-05,
      "loss": 0.345,
      "step": 2950
    },
    {
      "epoch": 0.8679411764705882,
      "grad_norm": 0.03404300659894943,
      "learning_rate": 2.6450662739322534e-05,
      "loss": 0.3063,
      "step": 2951
    },
    {
      "epoch": 0.8682352941176471,
      "grad_norm": 0.037977226078510284,
      "learning_rate": 2.639175257731959e-05,
      "loss": 0.284,
      "step": 2952
    },
    {
      "epoch": 0.8685294117647059,
      "grad_norm": 0.051012806594371796,
      "learning_rate": 2.6332842415316644e-05,
      "loss": 0.3823,
      "step": 2953
    },
    {
      "epoch": 0.8688235294117647,
      "grad_norm": 0.03691298887133598,
      "learning_rate": 2.62739322533137e-05,
      "loss": 0.2969,
      "step": 2954
    },
    {
      "epoch": 0.8691176470588236,
      "grad_norm": 0.04254189133644104,
      "learning_rate": 2.6215022091310754e-05,
      "loss": 0.3082,
      "step": 2955
    },
    {
      "epoch": 0.8694117647058823,
      "grad_norm": 0.04394366219639778,
      "learning_rate": 2.6156111929307807e-05,
      "loss": 0.352,
      "step": 2956
    },
    {
      "epoch": 0.8697058823529412,
      "grad_norm": 0.03993500769138336,
      "learning_rate": 2.6097201767304864e-05,
      "loss": 0.3057,
      "step": 2957
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.040066562592983246,
      "learning_rate": 2.6038291605301917e-05,
      "loss": 0.2911,
      "step": 2958
    },
    {
      "epoch": 0.8702941176470588,
      "grad_norm": 0.06485316157341003,
      "learning_rate": 2.597938144329897e-05,
      "loss": 0.4262,
      "step": 2959
    },
    {
      "epoch": 0.8705882352941177,
      "grad_norm": 0.05326550081372261,
      "learning_rate": 2.5920471281296028e-05,
      "loss": 0.3768,
      "step": 2960
    },
    {
      "epoch": 0.8708823529411764,
      "grad_norm": 0.05343211069703102,
      "learning_rate": 2.586156111929308e-05,
      "loss": 0.4152,
      "step": 2961
    },
    {
      "epoch": 0.8711764705882353,
      "grad_norm": 0.04497485235333443,
      "learning_rate": 2.5802650957290138e-05,
      "loss": 0.3457,
      "step": 2962
    },
    {
      "epoch": 0.8714705882352941,
      "grad_norm": 0.046200115233659744,
      "learning_rate": 2.5743740795287184e-05,
      "loss": 0.3521,
      "step": 2963
    },
    {
      "epoch": 0.8717647058823529,
      "grad_norm": 0.04776199907064438,
      "learning_rate": 2.568483063328424e-05,
      "loss": 0.3555,
      "step": 2964
    },
    {
      "epoch": 0.8720588235294118,
      "grad_norm": 0.04488035663962364,
      "learning_rate": 2.5625920471281295e-05,
      "loss": 0.3262,
      "step": 2965
    },
    {
      "epoch": 0.8723529411764706,
      "grad_norm": 0.04824154078960419,
      "learning_rate": 2.556701030927835e-05,
      "loss": 0.3339,
      "step": 2966
    },
    {
      "epoch": 0.8726470588235294,
      "grad_norm": 0.05880394205451012,
      "learning_rate": 2.5508100147275405e-05,
      "loss": 0.3941,
      "step": 2967
    },
    {
      "epoch": 0.8729411764705882,
      "grad_norm": 0.03981185331940651,
      "learning_rate": 2.5449189985272458e-05,
      "loss": 0.3214,
      "step": 2968
    },
    {
      "epoch": 0.8732352941176471,
      "grad_norm": 0.05087551474571228,
      "learning_rate": 2.5390279823269515e-05,
      "loss": 0.4088,
      "step": 2969
    },
    {
      "epoch": 0.8735294117647059,
      "grad_norm": 0.05052465945482254,
      "learning_rate": 2.5331369661266568e-05,
      "loss": 0.335,
      "step": 2970
    },
    {
      "epoch": 0.8738235294117647,
      "grad_norm": 0.057176098227500916,
      "learning_rate": 2.527245949926362e-05,
      "loss": 0.397,
      "step": 2971
    },
    {
      "epoch": 0.8741176470588236,
      "grad_norm": 0.04021574556827545,
      "learning_rate": 2.521354933726068e-05,
      "loss": 0.3271,
      "step": 2972
    },
    {
      "epoch": 0.8744117647058823,
      "grad_norm": 0.0454811230301857,
      "learning_rate": 2.5154639175257732e-05,
      "loss": 0.2923,
      "step": 2973
    },
    {
      "epoch": 0.8747058823529412,
      "grad_norm": 0.03558078035712242,
      "learning_rate": 2.509572901325479e-05,
      "loss": 0.2538,
      "step": 2974
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.07130084186792374,
      "learning_rate": 2.5036818851251842e-05,
      "loss": 0.4126,
      "step": 2975
    },
    {
      "epoch": 0.8752941176470588,
      "grad_norm": 0.04527901113033295,
      "learning_rate": 2.4977908689248895e-05,
      "loss": 0.3656,
      "step": 2976
    },
    {
      "epoch": 0.8755882352941177,
      "grad_norm": 0.04527207463979721,
      "learning_rate": 2.4918998527245952e-05,
      "loss": 0.2987,
      "step": 2977
    },
    {
      "epoch": 0.8758823529411764,
      "grad_norm": 0.04047997668385506,
      "learning_rate": 2.4860088365243006e-05,
      "loss": 0.2979,
      "step": 2978
    },
    {
      "epoch": 0.8761764705882353,
      "grad_norm": 0.0514560230076313,
      "learning_rate": 2.4801178203240062e-05,
      "loss": 0.3863,
      "step": 2979
    },
    {
      "epoch": 0.8764705882352941,
      "grad_norm": 0.056580979377031326,
      "learning_rate": 2.4742268041237116e-05,
      "loss": 0.3971,
      "step": 2980
    },
    {
      "epoch": 0.8767647058823529,
      "grad_norm": 0.041805025190114975,
      "learning_rate": 2.468335787923417e-05,
      "loss": 0.3132,
      "step": 2981
    },
    {
      "epoch": 0.8770588235294118,
      "grad_norm": 0.05691147968173027,
      "learning_rate": 2.4624447717231226e-05,
      "loss": 0.3804,
      "step": 2982
    },
    {
      "epoch": 0.8773529411764706,
      "grad_norm": 0.04147947207093239,
      "learning_rate": 2.4565537555228276e-05,
      "loss": 0.2978,
      "step": 2983
    },
    {
      "epoch": 0.8776470588235294,
      "grad_norm": 0.041146159172058105,
      "learning_rate": 2.4506627393225333e-05,
      "loss": 0.2613,
      "step": 2984
    },
    {
      "epoch": 0.8779411764705882,
      "grad_norm": 0.06457290798425674,
      "learning_rate": 2.4447717231222386e-05,
      "loss": 0.4203,
      "step": 2985
    },
    {
      "epoch": 0.8782352941176471,
      "grad_norm": 0.04858910292387009,
      "learning_rate": 2.438880706921944e-05,
      "loss": 0.325,
      "step": 2986
    },
    {
      "epoch": 0.8785294117647059,
      "grad_norm": 0.036946263164281845,
      "learning_rate": 2.4329896907216496e-05,
      "loss": 0.3155,
      "step": 2987
    },
    {
      "epoch": 0.8788235294117647,
      "grad_norm": 0.0351654514670372,
      "learning_rate": 2.427098674521355e-05,
      "loss": 0.3008,
      "step": 2988
    },
    {
      "epoch": 0.8791176470588236,
      "grad_norm": 0.04775172844529152,
      "learning_rate": 2.4212076583210606e-05,
      "loss": 0.3347,
      "step": 2989
    },
    {
      "epoch": 0.8794117647058823,
      "grad_norm": 0.053632427006959915,
      "learning_rate": 2.415316642120766e-05,
      "loss": 0.343,
      "step": 2990
    },
    {
      "epoch": 0.8797058823529412,
      "grad_norm": 0.040240030735731125,
      "learning_rate": 2.4094256259204713e-05,
      "loss": 0.3395,
      "step": 2991
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.045951347798109055,
      "learning_rate": 2.403534609720177e-05,
      "loss": 0.3693,
      "step": 2992
    },
    {
      "epoch": 0.8802941176470588,
      "grad_norm": 0.048568032681941986,
      "learning_rate": 2.3976435935198823e-05,
      "loss": 0.323,
      "step": 2993
    },
    {
      "epoch": 0.8805882352941177,
      "grad_norm": 0.04786134511232376,
      "learning_rate": 2.391752577319588e-05,
      "loss": 0.3444,
      "step": 2994
    },
    {
      "epoch": 0.8808823529411764,
      "grad_norm": 0.042313460260629654,
      "learning_rate": 2.3858615611192933e-05,
      "loss": 0.2855,
      "step": 2995
    },
    {
      "epoch": 0.8811764705882353,
      "grad_norm": 0.05352091044187546,
      "learning_rate": 2.3799705449189983e-05,
      "loss": 0.3424,
      "step": 2996
    },
    {
      "epoch": 0.8814705882352941,
      "grad_norm": 0.05087542533874512,
      "learning_rate": 2.374079528718704e-05,
      "loss": 0.3825,
      "step": 2997
    },
    {
      "epoch": 0.8817647058823529,
      "grad_norm": 0.049534961581230164,
      "learning_rate": 2.3681885125184094e-05,
      "loss": 0.353,
      "step": 2998
    },
    {
      "epoch": 0.8820588235294118,
      "grad_norm": 0.05630623176693916,
      "learning_rate": 2.362297496318115e-05,
      "loss": 0.3457,
      "step": 2999
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 0.05331028997898102,
      "learning_rate": 2.3564064801178204e-05,
      "loss": 0.4,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 3400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.584629098107535e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
