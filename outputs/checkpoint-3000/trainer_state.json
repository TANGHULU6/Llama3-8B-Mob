{
  "best_metric": 0.3207613229751587,
  "best_model_checkpoint": "outputs/checkpoint-2000",
  "epoch": 1.0909090909090908,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 2.335993528366089,
      "learning_rate": 4e-05,
      "loss": 2.1044,
      "step": 1
    },
    {
      "epoch": 0.008,
      "grad_norm": 2.0764105319976807,
      "learning_rate": 8e-05,
      "loss": 2.0487,
      "step": 2
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.237320899963379,
      "learning_rate": 0.00012,
      "loss": 1.7674,
      "step": 3
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.9673866033554077,
      "learning_rate": 0.00016,
      "loss": 1.9002,
      "step": 4
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.7386751174926758,
      "learning_rate": 0.0002,
      "loss": 1.4927,
      "step": 5
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.9869507551193237,
      "learning_rate": 0.00019983935742971887,
      "loss": 1.5623,
      "step": 6
    },
    {
      "epoch": 0.028,
      "grad_norm": 1.6267954111099243,
      "learning_rate": 0.00019967871485943777,
      "loss": 1.3588,
      "step": 7
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.545114517211914,
      "learning_rate": 0.00019951807228915663,
      "loss": 1.011,
      "step": 8
    },
    {
      "epoch": 0.036,
      "grad_norm": 1.9192930459976196,
      "learning_rate": 0.00019935742971887552,
      "loss": 0.7881,
      "step": 9
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.575687289237976,
      "learning_rate": 0.0001991967871485944,
      "loss": 0.986,
      "step": 10
    },
    {
      "epoch": 0.044,
      "grad_norm": 1.5517014265060425,
      "learning_rate": 0.00019903614457831325,
      "loss": 0.7018,
      "step": 11
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.7378866672515869,
      "learning_rate": 0.00019887550200803214,
      "loss": 0.7582,
      "step": 12
    },
    {
      "epoch": 0.052,
      "grad_norm": 1.377540946006775,
      "learning_rate": 0.000198714859437751,
      "loss": 0.6486,
      "step": 13
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.4343770444393158,
      "learning_rate": 0.0001985542168674699,
      "loss": 0.5785,
      "step": 14
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.45771166682243347,
      "learning_rate": 0.00019839357429718877,
      "loss": 0.6001,
      "step": 15
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.4625537097454071,
      "learning_rate": 0.00019823293172690763,
      "loss": 0.6308,
      "step": 16
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.22737477719783783,
      "learning_rate": 0.00019807228915662652,
      "loss": 0.5656,
      "step": 17
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.2755506932735443,
      "learning_rate": 0.0001979116465863454,
      "loss": 0.5628,
      "step": 18
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.25809144973754883,
      "learning_rate": 0.00019775100401606425,
      "loss": 0.5148,
      "step": 19
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.26768651604652405,
      "learning_rate": 0.00019759036144578314,
      "loss": 0.6071,
      "step": 20
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.2362787276506424,
      "learning_rate": 0.000197429718875502,
      "loss": 0.5066,
      "step": 21
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.22010239958763123,
      "learning_rate": 0.0001972690763052209,
      "loss": 0.4615,
      "step": 22
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.16027754545211792,
      "learning_rate": 0.00019710843373493977,
      "loss": 0.5312,
      "step": 23
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.2091284692287445,
      "learning_rate": 0.00019694779116465866,
      "loss": 0.5346,
      "step": 24
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.16975794732570648,
      "learning_rate": 0.00019678714859437752,
      "loss": 0.414,
      "step": 25
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.1686217188835144,
      "learning_rate": 0.00019662650602409642,
      "loss": 0.3801,
      "step": 26
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.17285840213298798,
      "learning_rate": 0.00019646586345381528,
      "loss": 0.4267,
      "step": 27
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.1359291970729828,
      "learning_rate": 0.00019630522088353415,
      "loss": 0.4488,
      "step": 28
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.1422470659017563,
      "learning_rate": 0.000196144578313253,
      "loss": 0.4601,
      "step": 29
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.12334515154361725,
      "learning_rate": 0.0001959839357429719,
      "loss": 0.4322,
      "step": 30
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.1435297727584839,
      "learning_rate": 0.00019582329317269077,
      "loss": 0.3381,
      "step": 31
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.17335285246372223,
      "learning_rate": 0.00019566265060240966,
      "loss": 0.455,
      "step": 32
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.16958647966384888,
      "learning_rate": 0.00019550200803212852,
      "loss": 0.4504,
      "step": 33
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.12330922484397888,
      "learning_rate": 0.00019534136546184742,
      "loss": 0.4526,
      "step": 34
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.18745723366737366,
      "learning_rate": 0.00019518072289156628,
      "loss": 0.4559,
      "step": 35
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.1568511426448822,
      "learning_rate": 0.00019502008032128517,
      "loss": 0.3822,
      "step": 36
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.12996335327625275,
      "learning_rate": 0.000194859437751004,
      "loss": 0.3018,
      "step": 37
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.1483553797006607,
      "learning_rate": 0.0001946987951807229,
      "loss": 0.387,
      "step": 38
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.10578163713216782,
      "learning_rate": 0.00019453815261044177,
      "loss": 0.4175,
      "step": 39
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.11619345098733902,
      "learning_rate": 0.00019437751004016066,
      "loss": 0.4452,
      "step": 40
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.13259772956371307,
      "learning_rate": 0.00019421686746987952,
      "loss": 0.3646,
      "step": 41
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.11811677366495132,
      "learning_rate": 0.00019405622489959842,
      "loss": 0.3602,
      "step": 42
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.13755086064338684,
      "learning_rate": 0.00019389558232931728,
      "loss": 0.4618,
      "step": 43
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.09911821782588959,
      "learning_rate": 0.00019373493975903617,
      "loss": 0.3583,
      "step": 44
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.12044307589530945,
      "learning_rate": 0.00019357429718875504,
      "loss": 0.3332,
      "step": 45
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.1471354067325592,
      "learning_rate": 0.0001934136546184739,
      "loss": 0.384,
      "step": 46
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.08774939924478531,
      "learning_rate": 0.00019325301204819277,
      "loss": 0.3009,
      "step": 47
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.18413765728473663,
      "learning_rate": 0.00019309236947791166,
      "loss": 0.4787,
      "step": 48
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.14196979999542236,
      "learning_rate": 0.00019293172690763052,
      "loss": 0.3943,
      "step": 49
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.09552393108606339,
      "learning_rate": 0.00019277108433734942,
      "loss": 0.4077,
      "step": 50
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.06973276287317276,
      "learning_rate": 0.00019261044176706828,
      "loss": 0.4221,
      "step": 51
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.1023520901799202,
      "learning_rate": 0.00019244979919678717,
      "loss": 0.3369,
      "step": 52
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.12484478205442429,
      "learning_rate": 0.00019228915662650604,
      "loss": 0.4303,
      "step": 53
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.07435164600610733,
      "learning_rate": 0.00019212851405622493,
      "loss": 0.3116,
      "step": 54
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.09656102955341339,
      "learning_rate": 0.00019196787148594377,
      "loss": 0.3461,
      "step": 55
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.10590583831071854,
      "learning_rate": 0.00019180722891566266,
      "loss": 0.3895,
      "step": 56
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.09179916232824326,
      "learning_rate": 0.00019164658634538152,
      "loss": 0.3836,
      "step": 57
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.0764811784029007,
      "learning_rate": 0.00019148594377510042,
      "loss": 0.3577,
      "step": 58
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.07078667730093002,
      "learning_rate": 0.00019132530120481928,
      "loss": 0.3768,
      "step": 59
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.07151555269956589,
      "learning_rate": 0.00019116465863453817,
      "loss": 0.3165,
      "step": 60
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.06467336416244507,
      "learning_rate": 0.00019100401606425704,
      "loss": 0.3248,
      "step": 61
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.08873247355222702,
      "learning_rate": 0.00019084337349397593,
      "loss": 0.3983,
      "step": 62
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.07367058843374252,
      "learning_rate": 0.0001906827309236948,
      "loss": 0.3514,
      "step": 63
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.10028500854969025,
      "learning_rate": 0.00019052208835341369,
      "loss": 0.3982,
      "step": 64
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.09366507828235626,
      "learning_rate": 0.00019036144578313252,
      "loss": 0.4446,
      "step": 65
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.09211641550064087,
      "learning_rate": 0.00019020080321285142,
      "loss": 0.409,
      "step": 66
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.0891832783818245,
      "learning_rate": 0.00019004016064257028,
      "loss": 0.3825,
      "step": 67
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.09754790365695953,
      "learning_rate": 0.00018987951807228917,
      "loss": 0.3873,
      "step": 68
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.08898184448480606,
      "learning_rate": 0.00018971887550200804,
      "loss": 0.332,
      "step": 69
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.09052803367376328,
      "learning_rate": 0.00018955823293172693,
      "loss": 0.3241,
      "step": 70
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.09529199451208115,
      "learning_rate": 0.0001893975903614458,
      "loss": 0.3876,
      "step": 71
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.07163433730602264,
      "learning_rate": 0.00018923694779116469,
      "loss": 0.3512,
      "step": 72
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.08099471032619476,
      "learning_rate": 0.00018907630522088355,
      "loss": 0.3955,
      "step": 73
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.07369812577962875,
      "learning_rate": 0.00018891566265060242,
      "loss": 0.4445,
      "step": 74
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.09180382639169693,
      "learning_rate": 0.00018875502008032128,
      "loss": 0.2843,
      "step": 75
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.07807604968547821,
      "learning_rate": 0.00018859437751004017,
      "loss": 0.3374,
      "step": 76
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.07779300957918167,
      "learning_rate": 0.00018843373493975904,
      "loss": 0.3527,
      "step": 77
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.07766057550907135,
      "learning_rate": 0.00018827309236947793,
      "loss": 0.3607,
      "step": 78
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.06747744977474213,
      "learning_rate": 0.0001881124497991968,
      "loss": 0.2857,
      "step": 79
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.08666116744279861,
      "learning_rate": 0.00018795180722891569,
      "loss": 0.4099,
      "step": 80
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.06930151581764221,
      "learning_rate": 0.00018779116465863455,
      "loss": 0.3761,
      "step": 81
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.09900833666324615,
      "learning_rate": 0.00018763052208835344,
      "loss": 0.4676,
      "step": 82
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.06293882429599762,
      "learning_rate": 0.00018746987951807228,
      "loss": 0.327,
      "step": 83
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.07585560530424118,
      "learning_rate": 0.00018730923694779117,
      "loss": 0.3957,
      "step": 84
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.06081603094935417,
      "learning_rate": 0.00018714859437751004,
      "loss": 0.3235,
      "step": 85
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.07200583815574646,
      "learning_rate": 0.00018698795180722893,
      "loss": 0.3336,
      "step": 86
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.08296167850494385,
      "learning_rate": 0.0001868273092369478,
      "loss": 0.3309,
      "step": 87
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.08123473823070526,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.3498,
      "step": 88
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.07755216211080551,
      "learning_rate": 0.00018650602409638555,
      "loss": 0.3879,
      "step": 89
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.07337453961372375,
      "learning_rate": 0.00018634538152610444,
      "loss": 0.3695,
      "step": 90
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.07253798842430115,
      "learning_rate": 0.0001861847389558233,
      "loss": 0.3469,
      "step": 91
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.07289763540029526,
      "learning_rate": 0.00018602409638554217,
      "loss": 0.4025,
      "step": 92
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.07015921920537949,
      "learning_rate": 0.00018586345381526104,
      "loss": 0.3644,
      "step": 93
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.08791476488113403,
      "learning_rate": 0.00018570281124497993,
      "loss": 0.4197,
      "step": 94
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.07034123688936234,
      "learning_rate": 0.0001855421686746988,
      "loss": 0.4748,
      "step": 95
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.07798592001199722,
      "learning_rate": 0.0001853815261044177,
      "loss": 0.3519,
      "step": 96
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.1311381757259369,
      "learning_rate": 0.00018522088353413655,
      "loss": 0.4467,
      "step": 97
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.10356705635786057,
      "learning_rate": 0.00018506024096385544,
      "loss": 0.441,
      "step": 98
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.08293379843235016,
      "learning_rate": 0.0001848995983935743,
      "loss": 0.3453,
      "step": 99
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.1095842570066452,
      "learning_rate": 0.0001847389558232932,
      "loss": 0.3544,
      "step": 100
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.10393920540809631,
      "learning_rate": 0.00018457831325301204,
      "loss": 0.381,
      "step": 101
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.1124354749917984,
      "learning_rate": 0.00018441767068273093,
      "loss": 0.3562,
      "step": 102
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.11954368650913239,
      "learning_rate": 0.0001842570281124498,
      "loss": 0.3883,
      "step": 103
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.07915162295103073,
      "learning_rate": 0.0001840963855421687,
      "loss": 0.3348,
      "step": 104
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.06640903651714325,
      "learning_rate": 0.00018393574297188755,
      "loss": 0.2751,
      "step": 105
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.08325211703777313,
      "learning_rate": 0.00018377510040160644,
      "loss": 0.4079,
      "step": 106
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.10817031562328339,
      "learning_rate": 0.0001836144578313253,
      "loss": 0.4021,
      "step": 107
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.10752814263105392,
      "learning_rate": 0.0001834538152610442,
      "loss": 0.3674,
      "step": 108
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.0876161977648735,
      "learning_rate": 0.00018329317269076307,
      "loss": 0.3236,
      "step": 109
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.0885641872882843,
      "learning_rate": 0.00018313253012048193,
      "loss": 0.3748,
      "step": 110
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.0758696049451828,
      "learning_rate": 0.0001829718875502008,
      "loss": 0.3411,
      "step": 111
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.09491542726755142,
      "learning_rate": 0.0001828112449799197,
      "loss": 0.412,
      "step": 112
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.059162773191928864,
      "learning_rate": 0.00018265060240963855,
      "loss": 0.3147,
      "step": 113
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.0785461813211441,
      "learning_rate": 0.00018248995983935744,
      "loss": 0.3908,
      "step": 114
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.07412897795438766,
      "learning_rate": 0.0001823293172690763,
      "loss": 0.3639,
      "step": 115
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.09699475020170212,
      "learning_rate": 0.0001821686746987952,
      "loss": 0.374,
      "step": 116
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.0687938928604126,
      "learning_rate": 0.00018200803212851407,
      "loss": 0.3108,
      "step": 117
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.06590307503938675,
      "learning_rate": 0.00018184738955823296,
      "loss": 0.3258,
      "step": 118
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.08049190789461136,
      "learning_rate": 0.0001816867469879518,
      "loss": 0.4126,
      "step": 119
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.055834926664829254,
      "learning_rate": 0.0001815261044176707,
      "loss": 0.2752,
      "step": 120
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.06517331302165985,
      "learning_rate": 0.00018136546184738955,
      "loss": 0.3431,
      "step": 121
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.06454561650753021,
      "learning_rate": 0.00018120481927710844,
      "loss": 0.3132,
      "step": 122
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.08515077829360962,
      "learning_rate": 0.0001810441767068273,
      "loss": 0.3555,
      "step": 123
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.08610805124044418,
      "learning_rate": 0.0001808835341365462,
      "loss": 0.4093,
      "step": 124
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.07305427640676498,
      "learning_rate": 0.00018072289156626507,
      "loss": 0.398,
      "step": 125
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.0955563485622406,
      "learning_rate": 0.00018056224899598396,
      "loss": 0.3588,
      "step": 126
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.08587243407964706,
      "learning_rate": 0.00018040160642570282,
      "loss": 0.2839,
      "step": 127
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.054443467408418655,
      "learning_rate": 0.0001802409638554217,
      "loss": 0.277,
      "step": 128
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.07162444293498993,
      "learning_rate": 0.00018008032128514055,
      "loss": 0.3489,
      "step": 129
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.06221877411007881,
      "learning_rate": 0.00017991967871485944,
      "loss": 0.3312,
      "step": 130
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.07439780235290527,
      "learning_rate": 0.0001797590361445783,
      "loss": 0.3367,
      "step": 131
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.061645716428756714,
      "learning_rate": 0.0001795983935742972,
      "loss": 0.3791,
      "step": 132
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.061701029539108276,
      "learning_rate": 0.00017943775100401607,
      "loss": 0.3811,
      "step": 133
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.06690779328346252,
      "learning_rate": 0.00017927710843373496,
      "loss": 0.3455,
      "step": 134
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07746878266334534,
      "learning_rate": 0.00017911646586345382,
      "loss": 0.4215,
      "step": 135
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.07369588315486908,
      "learning_rate": 0.00017895582329317271,
      "loss": 0.4002,
      "step": 136
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.08413571119308472,
      "learning_rate": 0.00017879518072289155,
      "loss": 0.373,
      "step": 137
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.06716182082891464,
      "learning_rate": 0.00017863453815261044,
      "loss": 0.2714,
      "step": 138
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.06655866652727127,
      "learning_rate": 0.0001784738955823293,
      "loss": 0.3781,
      "step": 139
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.06919930875301361,
      "learning_rate": 0.0001783132530120482,
      "loss": 0.3294,
      "step": 140
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.06431005895137787,
      "learning_rate": 0.00017815261044176707,
      "loss": 0.338,
      "step": 141
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.08051612973213196,
      "learning_rate": 0.00017799196787148596,
      "loss": 0.3701,
      "step": 142
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.07067444175481796,
      "learning_rate": 0.00017783132530120482,
      "loss": 0.2364,
      "step": 143
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.07966411858797073,
      "learning_rate": 0.00017767068273092371,
      "loss": 0.3311,
      "step": 144
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0810563862323761,
      "learning_rate": 0.00017751004016064258,
      "loss": 0.3939,
      "step": 145
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.07686350494623184,
      "learning_rate": 0.00017734939759036144,
      "loss": 0.3207,
      "step": 146
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.07982712239027023,
      "learning_rate": 0.0001771887550200803,
      "loss": 0.3626,
      "step": 147
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.06971494108438492,
      "learning_rate": 0.0001770281124497992,
      "loss": 0.3391,
      "step": 148
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.08965300768613815,
      "learning_rate": 0.00017686746987951807,
      "loss": 0.4032,
      "step": 149
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.08622463792562485,
      "learning_rate": 0.00017670682730923696,
      "loss": 0.3394,
      "step": 150
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.08759962022304535,
      "learning_rate": 0.00017654618473895582,
      "loss": 0.3615,
      "step": 151
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.07205653190612793,
      "learning_rate": 0.00017638554216867471,
      "loss": 0.362,
      "step": 152
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.06996813416481018,
      "learning_rate": 0.00017622489959839358,
      "loss": 0.34,
      "step": 153
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.06293423473834991,
      "learning_rate": 0.00017606425702811247,
      "loss": 0.3187,
      "step": 154
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06999028474092484,
      "learning_rate": 0.00017590361445783134,
      "loss": 0.3619,
      "step": 155
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.05771356076002121,
      "learning_rate": 0.0001757429718875502,
      "loss": 0.3452,
      "step": 156
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.06877999007701874,
      "learning_rate": 0.00017558232931726907,
      "loss": 0.3305,
      "step": 157
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.08761386573314667,
      "learning_rate": 0.00017542168674698796,
      "loss": 0.4431,
      "step": 158
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.07388655096292496,
      "learning_rate": 0.00017526104417670682,
      "loss": 0.3475,
      "step": 159
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.07229222357273102,
      "learning_rate": 0.00017510040160642571,
      "loss": 0.358,
      "step": 160
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.06303266435861588,
      "learning_rate": 0.00017493975903614458,
      "loss": 0.3167,
      "step": 161
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.07816348969936371,
      "learning_rate": 0.00017477911646586347,
      "loss": 0.4034,
      "step": 162
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.0892229676246643,
      "learning_rate": 0.00017461847389558234,
      "loss": 0.3956,
      "step": 163
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.08024004846811295,
      "learning_rate": 0.00017445783132530123,
      "loss": 0.3121,
      "step": 164
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.1255861222743988,
      "learning_rate": 0.0001742971887550201,
      "loss": 0.3656,
      "step": 165
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.1040395051240921,
      "learning_rate": 0.00017413654618473896,
      "loss": 0.3903,
      "step": 166
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.0626545324921608,
      "learning_rate": 0.00017397590361445782,
      "loss": 0.3353,
      "step": 167
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.10490436851978302,
      "learning_rate": 0.00017381526104417671,
      "loss": 0.4114,
      "step": 168
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.07469852268695831,
      "learning_rate": 0.00017365461847389558,
      "loss": 0.3456,
      "step": 169
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.11949927359819412,
      "learning_rate": 0.00017349397590361447,
      "loss": 0.4294,
      "step": 170
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.10944344103336334,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4077,
      "step": 171
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.08204059302806854,
      "learning_rate": 0.00017317269076305223,
      "loss": 0.3674,
      "step": 172
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.09916073828935623,
      "learning_rate": 0.0001730120481927711,
      "loss": 0.4261,
      "step": 173
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.09841838479042053,
      "learning_rate": 0.00017285140562248996,
      "loss": 0.4026,
      "step": 174
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.06968950480222702,
      "learning_rate": 0.00017269076305220885,
      "loss": 0.344,
      "step": 175
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.06344326585531235,
      "learning_rate": 0.00017253012048192771,
      "loss": 0.4312,
      "step": 176
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.0671236515045166,
      "learning_rate": 0.00017236947791164658,
      "loss": 0.3303,
      "step": 177
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.08015843480825424,
      "learning_rate": 0.00017220883534136547,
      "loss": 0.3147,
      "step": 178
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.07488393038511276,
      "learning_rate": 0.00017204819277108434,
      "loss": 0.3574,
      "step": 179
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06061435118317604,
      "learning_rate": 0.00017188755020080323,
      "loss": 0.2953,
      "step": 180
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.070791095495224,
      "learning_rate": 0.0001717269076305221,
      "loss": 0.3448,
      "step": 181
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.06472242623567581,
      "learning_rate": 0.00017156626506024099,
      "loss": 0.3519,
      "step": 182
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.07848669588565826,
      "learning_rate": 0.00017140562248995985,
      "loss": 0.4092,
      "step": 183
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.06604913622140884,
      "learning_rate": 0.00017124497991967871,
      "loss": 0.3529,
      "step": 184
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.061453040689229965,
      "learning_rate": 0.0001710843373493976,
      "loss": 0.3624,
      "step": 185
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.05750114098191261,
      "learning_rate": 0.00017092369477911647,
      "loss": 0.3066,
      "step": 186
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.05608813464641571,
      "learning_rate": 0.00017076305220883536,
      "loss": 0.2696,
      "step": 187
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.07880958914756775,
      "learning_rate": 0.00017060240963855423,
      "loss": 0.373,
      "step": 188
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.0631590187549591,
      "learning_rate": 0.0001704417670682731,
      "loss": 0.3457,
      "step": 189
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.07894622534513474,
      "learning_rate": 0.00017028112449799199,
      "loss": 0.4256,
      "step": 190
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.061956290155649185,
      "learning_rate": 0.00017012048192771085,
      "loss": 0.3374,
      "step": 191
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.05901569128036499,
      "learning_rate": 0.00016995983935742971,
      "loss": 0.2921,
      "step": 192
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.07318167388439178,
      "learning_rate": 0.0001697991967871486,
      "loss": 0.3847,
      "step": 193
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.059468816965818405,
      "learning_rate": 0.00016963855421686747,
      "loss": 0.3339,
      "step": 194
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.07377639412879944,
      "learning_rate": 0.00016947791164658636,
      "loss": 0.3926,
      "step": 195
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.055249638855457306,
      "learning_rate": 0.00016931726907630523,
      "loss": 0.33,
      "step": 196
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.08036017417907715,
      "learning_rate": 0.00016915662650602412,
      "loss": 0.4066,
      "step": 197
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.06219193711876869,
      "learning_rate": 0.00016899598393574299,
      "loss": 0.3378,
      "step": 198
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.06654492765665054,
      "learning_rate": 0.00016883534136546185,
      "loss": 0.3968,
      "step": 199
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.07095576077699661,
      "learning_rate": 0.00016867469879518074,
      "loss": 0.4038,
      "step": 200
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.06736365705728531,
      "learning_rate": 0.0001685140562248996,
      "loss": 0.3412,
      "step": 201
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.08883492648601532,
      "learning_rate": 0.00016835341365461847,
      "loss": 0.347,
      "step": 202
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.0840194821357727,
      "learning_rate": 0.00016819277108433736,
      "loss": 0.3516,
      "step": 203
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.06955178081989288,
      "learning_rate": 0.00016803212851405623,
      "loss": 0.3325,
      "step": 204
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.07094302773475647,
      "learning_rate": 0.00016787148594377512,
      "loss": 0.3636,
      "step": 205
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.09593607485294342,
      "learning_rate": 0.00016771084337349399,
      "loss": 0.3592,
      "step": 206
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.06644279509782791,
      "learning_rate": 0.00016755020080321288,
      "loss": 0.3806,
      "step": 207
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.06576841324567795,
      "learning_rate": 0.00016738955823293174,
      "loss": 0.347,
      "step": 208
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.06923682987689972,
      "learning_rate": 0.0001672289156626506,
      "loss": 0.2953,
      "step": 209
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.06872890889644623,
      "learning_rate": 0.00016706827309236947,
      "loss": 0.3702,
      "step": 210
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.05236044526100159,
      "learning_rate": 0.00016690763052208836,
      "loss": 0.2673,
      "step": 211
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.07570517808198929,
      "learning_rate": 0.00016674698795180723,
      "loss": 0.336,
      "step": 212
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.08200521022081375,
      "learning_rate": 0.00016658634538152612,
      "loss": 0.445,
      "step": 213
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.05843085050582886,
      "learning_rate": 0.00016642570281124499,
      "loss": 0.3116,
      "step": 214
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.0784873217344284,
      "learning_rate": 0.00016626506024096388,
      "loss": 0.3977,
      "step": 215
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.08429329097270966,
      "learning_rate": 0.00016610441767068274,
      "loss": 0.341,
      "step": 216
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.06723389774560928,
      "learning_rate": 0.00016594377510040163,
      "loss": 0.3981,
      "step": 217
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.053809914737939835,
      "learning_rate": 0.0001657831325301205,
      "loss": 0.3266,
      "step": 218
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.07245859503746033,
      "learning_rate": 0.00016562248995983936,
      "loss": 0.4244,
      "step": 219
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.09080168604850769,
      "learning_rate": 0.00016546184738955823,
      "loss": 0.364,
      "step": 220
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.07945255935192108,
      "learning_rate": 0.00016530120481927712,
      "loss": 0.3989,
      "step": 221
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.06590015441179276,
      "learning_rate": 0.00016514056224899599,
      "loss": 0.4175,
      "step": 222
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.07693785429000854,
      "learning_rate": 0.00016497991967871488,
      "loss": 0.3823,
      "step": 223
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.09281361103057861,
      "learning_rate": 0.00016481927710843374,
      "loss": 0.3511,
      "step": 224
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.06478194147348404,
      "learning_rate": 0.00016465863453815263,
      "loss": 0.2972,
      "step": 225
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.058284956961870193,
      "learning_rate": 0.0001644979919678715,
      "loss": 0.356,
      "step": 226
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.06803490221500397,
      "learning_rate": 0.0001643373493975904,
      "loss": 0.3542,
      "step": 227
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.09583438187837601,
      "learning_rate": 0.00016417670682730923,
      "loss": 0.3945,
      "step": 228
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.06071958318352699,
      "learning_rate": 0.00016401606425702812,
      "loss": 0.327,
      "step": 229
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.07146310806274414,
      "learning_rate": 0.00016385542168674699,
      "loss": 0.3992,
      "step": 230
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.07429759949445724,
      "learning_rate": 0.00016369477911646588,
      "loss": 0.3296,
      "step": 231
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.06697545200586319,
      "learning_rate": 0.00016353413654618474,
      "loss": 0.3349,
      "step": 232
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.051754191517829895,
      "learning_rate": 0.00016337349397590363,
      "loss": 0.3249,
      "step": 233
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.056257762014865875,
      "learning_rate": 0.0001632128514056225,
      "loss": 0.2442,
      "step": 234
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.057602159678936005,
      "learning_rate": 0.0001630522088353414,
      "loss": 0.3051,
      "step": 235
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.06567410379648209,
      "learning_rate": 0.00016289156626506026,
      "loss": 0.293,
      "step": 236
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.060780271887779236,
      "learning_rate": 0.00016273092369477912,
      "loss": 0.2917,
      "step": 237
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.07810168713331223,
      "learning_rate": 0.00016257028112449799,
      "loss": 0.3813,
      "step": 238
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.08475077152252197,
      "learning_rate": 0.00016240963855421688,
      "loss": 0.3991,
      "step": 239
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06024686247110367,
      "learning_rate": 0.00016224899598393574,
      "loss": 0.3483,
      "step": 240
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.06779856234788895,
      "learning_rate": 0.00016208835341365463,
      "loss": 0.3596,
      "step": 241
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.06725198030471802,
      "learning_rate": 0.0001619277108433735,
      "loss": 0.3426,
      "step": 242
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.0619363971054554,
      "learning_rate": 0.0001617670682730924,
      "loss": 0.3552,
      "step": 243
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.06151701137423515,
      "learning_rate": 0.00016160642570281126,
      "loss": 0.4036,
      "step": 244
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.06504974514245987,
      "learning_rate": 0.00016144578313253015,
      "loss": 0.3599,
      "step": 245
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.058046501129865646,
      "learning_rate": 0.00016128514056224899,
      "loss": 0.3057,
      "step": 246
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.07275928556919098,
      "learning_rate": 0.00016112449799196788,
      "loss": 0.441,
      "step": 247
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.05370446294546127,
      "learning_rate": 0.00016096385542168674,
      "loss": 0.3622,
      "step": 248
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.0528927743434906,
      "learning_rate": 0.00016080321285140563,
      "loss": 0.3417,
      "step": 249
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06459882110357285,
      "learning_rate": 0.0001606425702811245,
      "loss": 0.4024,
      "step": 250
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.05694146081805229,
      "learning_rate": 0.0001604819277108434,
      "loss": 0.3239,
      "step": 251
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.052814971655607224,
      "learning_rate": 0.00016032128514056226,
      "loss": 0.2924,
      "step": 252
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.0602634958922863,
      "learning_rate": 0.00016016064257028115,
      "loss": 0.3156,
      "step": 253
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.08341137319803238,
      "learning_rate": 0.00016,
      "loss": 0.4277,
      "step": 254
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.0671900063753128,
      "learning_rate": 0.00015983935742971888,
      "loss": 0.4155,
      "step": 255
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.07289593666791916,
      "learning_rate": 0.00015967871485943774,
      "loss": 0.3468,
      "step": 256
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.08262821286916733,
      "learning_rate": 0.00015951807228915663,
      "loss": 0.3703,
      "step": 257
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.06663036346435547,
      "learning_rate": 0.0001593574297188755,
      "loss": 0.3566,
      "step": 258
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.08363865315914154,
      "learning_rate": 0.0001591967871485944,
      "loss": 0.3789,
      "step": 259
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.07572802156209946,
      "learning_rate": 0.00015903614457831326,
      "loss": 0.3779,
      "step": 260
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.05516692250967026,
      "learning_rate": 0.00015887550200803215,
      "loss": 0.3354,
      "step": 261
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.06751298904418945,
      "learning_rate": 0.000158714859437751,
      "loss": 0.3784,
      "step": 262
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.067877858877182,
      "learning_rate": 0.0001585542168674699,
      "loss": 0.3421,
      "step": 263
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.049092669039964676,
      "learning_rate": 0.00015839357429718874,
      "loss": 0.2879,
      "step": 264
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.06602587550878525,
      "learning_rate": 0.00015823293172690763,
      "loss": 0.3464,
      "step": 265
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.09919986873865128,
      "learning_rate": 0.0001580722891566265,
      "loss": 0.3853,
      "step": 266
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.07069980353116989,
      "learning_rate": 0.0001579116465863454,
      "loss": 0.3628,
      "step": 267
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.07314503937959671,
      "learning_rate": 0.00015775100401606426,
      "loss": 0.4107,
      "step": 268
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.055256351828575134,
      "learning_rate": 0.00015759036144578315,
      "loss": 0.3522,
      "step": 269
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.08744784444570541,
      "learning_rate": 0.000157429718875502,
      "loss": 0.4255,
      "step": 270
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.0754099041223526,
      "learning_rate": 0.0001572690763052209,
      "loss": 0.3107,
      "step": 271
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.09834086894989014,
      "learning_rate": 0.00015710843373493977,
      "loss": 0.4183,
      "step": 272
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.06723307073116302,
      "learning_rate": 0.00015694779116465866,
      "loss": 0.3162,
      "step": 273
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.08567360788583755,
      "learning_rate": 0.0001567871485943775,
      "loss": 0.3772,
      "step": 274
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.0799490436911583,
      "learning_rate": 0.0001566265060240964,
      "loss": 0.4218,
      "step": 275
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.08629798144102097,
      "learning_rate": 0.00015646586345381526,
      "loss": 0.3811,
      "step": 276
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.07569904625415802,
      "learning_rate": 0.00015630522088353415,
      "loss": 0.2752,
      "step": 277
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.07915880531072617,
      "learning_rate": 0.000156144578313253,
      "loss": 0.3641,
      "step": 278
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.06226642429828644,
      "learning_rate": 0.0001559839357429719,
      "loss": 0.3176,
      "step": 279
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.07813778519630432,
      "learning_rate": 0.00015582329317269077,
      "loss": 0.3488,
      "step": 280
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.09409919381141663,
      "learning_rate": 0.00015566265060240966,
      "loss": 0.3239,
      "step": 281
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.05827879160642624,
      "learning_rate": 0.00015550200803212853,
      "loss": 0.3125,
      "step": 282
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.058646511286497116,
      "learning_rate": 0.0001553413654618474,
      "loss": 0.3374,
      "step": 283
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.06671176105737686,
      "learning_rate": 0.00015518072289156626,
      "loss": 0.3279,
      "step": 284
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.061283767223358154,
      "learning_rate": 0.00015502008032128515,
      "loss": 0.3447,
      "step": 285
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.08076661825180054,
      "learning_rate": 0.000154859437751004,
      "loss": 0.361,
      "step": 286
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.0608193464577198,
      "learning_rate": 0.0001546987951807229,
      "loss": 0.2849,
      "step": 287
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.08166457712650299,
      "learning_rate": 0.00015453815261044177,
      "loss": 0.4369,
      "step": 288
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.06410278379917145,
      "learning_rate": 0.00015437751004016066,
      "loss": 0.3736,
      "step": 289
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.0636240616440773,
      "learning_rate": 0.00015421686746987953,
      "loss": 0.3278,
      "step": 290
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.05957586318254471,
      "learning_rate": 0.00015405622489959842,
      "loss": 0.2925,
      "step": 291
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.056080400943756104,
      "learning_rate": 0.00015389558232931726,
      "loss": 0.3226,
      "step": 292
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.0644913986325264,
      "learning_rate": 0.00015373493975903615,
      "loss": 0.3878,
      "step": 293
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.09171632677316666,
      "learning_rate": 0.00015357429718875501,
      "loss": 0.3462,
      "step": 294
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.05310825631022453,
      "learning_rate": 0.0001534136546184739,
      "loss": 0.3016,
      "step": 295
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.06027137488126755,
      "learning_rate": 0.00015325301204819277,
      "loss": 0.3833,
      "step": 296
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.06841080635786057,
      "learning_rate": 0.00015309236947791166,
      "loss": 0.3867,
      "step": 297
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.0633535385131836,
      "learning_rate": 0.00015293172690763053,
      "loss": 0.3505,
      "step": 298
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.0656474158167839,
      "learning_rate": 0.00015277108433734942,
      "loss": 0.3606,
      "step": 299
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.06487196683883667,
      "learning_rate": 0.00015261044176706828,
      "loss": 0.3376,
      "step": 300
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.07483614981174469,
      "learning_rate": 0.00015244979919678715,
      "loss": 0.4073,
      "step": 301
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.05843934416770935,
      "learning_rate": 0.00015228915662650601,
      "loss": 0.3911,
      "step": 302
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.07266239821910858,
      "learning_rate": 0.0001521285140562249,
      "loss": 0.4023,
      "step": 303
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.06044679135084152,
      "learning_rate": 0.00015196787148594377,
      "loss": 0.3381,
      "step": 304
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.06958542764186859,
      "learning_rate": 0.00015180722891566266,
      "loss": 0.3439,
      "step": 305
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.0557260625064373,
      "learning_rate": 0.00015164658634538153,
      "loss": 0.3503,
      "step": 306
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.08595304191112518,
      "learning_rate": 0.00015148594377510042,
      "loss": 0.3699,
      "step": 307
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.06100659444928169,
      "learning_rate": 0.00015132530120481928,
      "loss": 0.348,
      "step": 308
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.06639671325683594,
      "learning_rate": 0.00015116465863453818,
      "loss": 0.3804,
      "step": 309
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.07296732068061829,
      "learning_rate": 0.00015100401606425701,
      "loss": 0.3552,
      "step": 310
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.07658863067626953,
      "learning_rate": 0.0001508433734939759,
      "loss": 0.3849,
      "step": 311
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.08187424391508102,
      "learning_rate": 0.00015068273092369477,
      "loss": 0.3045,
      "step": 312
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.054695986211299896,
      "learning_rate": 0.00015052208835341366,
      "loss": 0.2914,
      "step": 313
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.07878047227859497,
      "learning_rate": 0.00015036144578313253,
      "loss": 0.3202,
      "step": 314
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.05848023667931557,
      "learning_rate": 0.00015020080321285142,
      "loss": 0.2703,
      "step": 315
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.07939372956752777,
      "learning_rate": 0.00015004016064257028,
      "loss": 0.3402,
      "step": 316
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.08254089951515198,
      "learning_rate": 0.00014987951807228918,
      "loss": 0.3723,
      "step": 317
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.05142083391547203,
      "learning_rate": 0.00014971887550200804,
      "loss": 0.2588,
      "step": 318
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.08336351066827774,
      "learning_rate": 0.0001495582329317269,
      "loss": 0.3993,
      "step": 319
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.08617806434631348,
      "learning_rate": 0.00014939759036144577,
      "loss": 0.3895,
      "step": 320
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.05436963215470314,
      "learning_rate": 0.00014923694779116466,
      "loss": 0.3036,
      "step": 321
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.06492237746715546,
      "learning_rate": 0.00014907630522088353,
      "loss": 0.3331,
      "step": 322
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.06815395504236221,
      "learning_rate": 0.00014891566265060242,
      "loss": 0.3081,
      "step": 323
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.07209860533475876,
      "learning_rate": 0.00014875502008032128,
      "loss": 0.3465,
      "step": 324
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.0524505153298378,
      "learning_rate": 0.00014859437751004018,
      "loss": 0.312,
      "step": 325
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.056617751717567444,
      "learning_rate": 0.00014843373493975904,
      "loss": 0.3368,
      "step": 326
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.06496227532625198,
      "learning_rate": 0.00014827309236947793,
      "loss": 0.328,
      "step": 327
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.06006057560443878,
      "learning_rate": 0.00014811244979919677,
      "loss": 0.2888,
      "step": 328
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.060342662036418915,
      "learning_rate": 0.00014795180722891566,
      "loss": 0.3124,
      "step": 329
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.07837870717048645,
      "learning_rate": 0.00014779116465863453,
      "loss": 0.3481,
      "step": 330
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.061768848448991776,
      "learning_rate": 0.00014763052208835342,
      "loss": 0.3193,
      "step": 331
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.07427514344453812,
      "learning_rate": 0.00014746987951807228,
      "loss": 0.4524,
      "step": 332
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.05896192044019699,
      "learning_rate": 0.00014730923694779118,
      "loss": 0.3636,
      "step": 333
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.06696806102991104,
      "learning_rate": 0.00014714859437751004,
      "loss": 0.3641,
      "step": 334
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.0650644451379776,
      "learning_rate": 0.00014698795180722893,
      "loss": 0.3223,
      "step": 335
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.06293586641550064,
      "learning_rate": 0.0001468273092369478,
      "loss": 0.3445,
      "step": 336
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.0640149638056755,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.3811,
      "step": 337
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.054649729281663895,
      "learning_rate": 0.00014650602409638555,
      "loss": 0.3175,
      "step": 338
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.07102999091148376,
      "learning_rate": 0.00014634538152610442,
      "loss": 0.3178,
      "step": 339
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.08287917822599411,
      "learning_rate": 0.00014618473895582328,
      "loss": 0.3797,
      "step": 340
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.07051320374011993,
      "learning_rate": 0.00014602409638554218,
      "loss": 0.3476,
      "step": 341
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.056444812566041946,
      "learning_rate": 0.00014586345381526104,
      "loss": 0.3053,
      "step": 342
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.07634054869413376,
      "learning_rate": 0.00014570281124497993,
      "loss": 0.3565,
      "step": 343
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.05936524271965027,
      "learning_rate": 0.0001455421686746988,
      "loss": 0.3073,
      "step": 344
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.07029277086257935,
      "learning_rate": 0.0001453815261044177,
      "loss": 0.41,
      "step": 345
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.07487163692712784,
      "learning_rate": 0.00014522088353413655,
      "loss": 0.2838,
      "step": 346
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.05611513555049896,
      "learning_rate": 0.00014506024096385542,
      "loss": 0.3929,
      "step": 347
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.062323667109012604,
      "learning_rate": 0.0001448995983935743,
      "loss": 0.3378,
      "step": 348
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.06772825121879578,
      "learning_rate": 0.00014473895582329318,
      "loss": 0.3344,
      "step": 349
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.06531104445457458,
      "learning_rate": 0.00014457831325301204,
      "loss": 0.3927,
      "step": 350
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.0679556280374527,
      "learning_rate": 0.00014441767068273093,
      "loss": 0.372,
      "step": 351
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.050222743302583694,
      "learning_rate": 0.0001442570281124498,
      "loss": 0.3286,
      "step": 352
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.062492139637470245,
      "learning_rate": 0.0001440963855421687,
      "loss": 0.3553,
      "step": 353
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.06843584030866623,
      "learning_rate": 0.00014393574297188756,
      "loss": 0.3963,
      "step": 354
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.07956761866807938,
      "learning_rate": 0.00014377510040160642,
      "loss": 0.4109,
      "step": 355
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.06414541602134705,
      "learning_rate": 0.0001436144578313253,
      "loss": 0.3187,
      "step": 356
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.07679648697376251,
      "learning_rate": 0.00014345381526104418,
      "loss": 0.4499,
      "step": 357
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.06024763733148575,
      "learning_rate": 0.00014329317269076307,
      "loss": 0.2708,
      "step": 358
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.06396373361349106,
      "learning_rate": 0.00014313253012048193,
      "loss": 0.3622,
      "step": 359
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.05741715058684349,
      "learning_rate": 0.0001429718875502008,
      "loss": 0.3304,
      "step": 360
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.06880754232406616,
      "learning_rate": 0.0001428112449799197,
      "loss": 0.3792,
      "step": 361
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.0671331137418747,
      "learning_rate": 0.00014265060240963856,
      "loss": 0.3078,
      "step": 362
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.05137662962079048,
      "learning_rate": 0.00014248995983935745,
      "loss": 0.3237,
      "step": 363
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.048981547355651855,
      "learning_rate": 0.0001423293172690763,
      "loss": 0.3334,
      "step": 364
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.057533781975507736,
      "learning_rate": 0.00014216867469879518,
      "loss": 0.3242,
      "step": 365
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.0870923399925232,
      "learning_rate": 0.00014200803212851407,
      "loss": 0.3538,
      "step": 366
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.07887953519821167,
      "learning_rate": 0.00014184738955823293,
      "loss": 0.3622,
      "step": 367
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.07234906405210495,
      "learning_rate": 0.00014168674698795183,
      "loss": 0.3133,
      "step": 368
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.06823708117008209,
      "learning_rate": 0.0001415261044176707,
      "loss": 0.3414,
      "step": 369
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.04216761514544487,
      "learning_rate": 0.00014136546184738956,
      "loss": 0.2962,
      "step": 370
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.0656755119562149,
      "learning_rate": 0.00014120481927710845,
      "loss": 0.3893,
      "step": 371
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.05478699132800102,
      "learning_rate": 0.0001410441767068273,
      "loss": 0.3127,
      "step": 372
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.08511320501565933,
      "learning_rate": 0.0001408835341365462,
      "loss": 0.3642,
      "step": 373
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.06668339669704437,
      "learning_rate": 0.00014072289156626507,
      "loss": 0.3729,
      "step": 374
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.05444904789328575,
      "learning_rate": 0.00014056224899598393,
      "loss": 0.3775,
      "step": 375
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.06002403795719147,
      "learning_rate": 0.00014040160642570283,
      "loss": 0.3169,
      "step": 376
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.06433095037937164,
      "learning_rate": 0.0001402409638554217,
      "loss": 0.3722,
      "step": 377
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.06275834143161774,
      "learning_rate": 0.00014008032128514058,
      "loss": 0.2992,
      "step": 378
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.06934570521116257,
      "learning_rate": 0.00013991967871485945,
      "loss": 0.3294,
      "step": 379
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.0770656168460846,
      "learning_rate": 0.00013975903614457834,
      "loss": 0.4519,
      "step": 380
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.04543275013566017,
      "learning_rate": 0.0001395983935742972,
      "loss": 0.2634,
      "step": 381
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.05905156582593918,
      "learning_rate": 0.00013943775100401607,
      "loss": 0.3331,
      "step": 382
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.06380102038383484,
      "learning_rate": 0.00013927710843373493,
      "loss": 0.3557,
      "step": 383
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.055626995861530304,
      "learning_rate": 0.00013911646586345383,
      "loss": 0.3012,
      "step": 384
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.04658617451786995,
      "learning_rate": 0.0001389558232931727,
      "loss": 0.2507,
      "step": 385
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.05973340943455696,
      "learning_rate": 0.00013879518072289158,
      "loss": 0.4102,
      "step": 386
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.06271574646234512,
      "learning_rate": 0.00013863453815261045,
      "loss": 0.262,
      "step": 387
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.07494782656431198,
      "learning_rate": 0.00013847389558232934,
      "loss": 0.4246,
      "step": 388
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.057505398988723755,
      "learning_rate": 0.0001383132530120482,
      "loss": 0.3433,
      "step": 389
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.08697894960641861,
      "learning_rate": 0.0001381526104417671,
      "loss": 0.3339,
      "step": 390
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.06824053078889847,
      "learning_rate": 0.00013799196787148596,
      "loss": 0.4237,
      "step": 391
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.05841195955872536,
      "learning_rate": 0.00013783132530120483,
      "loss": 0.3389,
      "step": 392
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.07292234897613525,
      "learning_rate": 0.0001376706827309237,
      "loss": 0.3483,
      "step": 393
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.07554447650909424,
      "learning_rate": 0.00013751004016064258,
      "loss": 0.3536,
      "step": 394
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.08072918653488159,
      "learning_rate": 0.00013734939759036145,
      "loss": 0.3924,
      "step": 395
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.05952663719654083,
      "learning_rate": 0.00013718875502008034,
      "loss": 0.2671,
      "step": 396
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.062004029750823975,
      "learning_rate": 0.0001370281124497992,
      "loss": 0.3471,
      "step": 397
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.05111677944660187,
      "learning_rate": 0.0001368674698795181,
      "loss": 0.3461,
      "step": 398
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.06390495598316193,
      "learning_rate": 0.00013670682730923696,
      "loss": 0.3126,
      "step": 399
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.10742776095867157,
      "learning_rate": 0.00013654618473895585,
      "loss": 0.4099,
      "step": 400
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.05186112970113754,
      "learning_rate": 0.0001363855421686747,
      "loss": 0.3015,
      "step": 401
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.058421120047569275,
      "learning_rate": 0.00013622489959839358,
      "loss": 0.3438,
      "step": 402
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.07301514595746994,
      "learning_rate": 0.00013606425702811245,
      "loss": 0.4,
      "step": 403
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.05842231959104538,
      "learning_rate": 0.00013590361445783134,
      "loss": 0.3682,
      "step": 404
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.05466844141483307,
      "learning_rate": 0.0001357429718875502,
      "loss": 0.3584,
      "step": 405
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.06547771394252777,
      "learning_rate": 0.0001355823293172691,
      "loss": 0.4307,
      "step": 406
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.06842739880084991,
      "learning_rate": 0.00013542168674698796,
      "loss": 0.3946,
      "step": 407
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.055751364678144455,
      "learning_rate": 0.00013526104417670685,
      "loss": 0.3435,
      "step": 408
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.06769607216119766,
      "learning_rate": 0.00013510040160642572,
      "loss": 0.3322,
      "step": 409
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.050946980714797974,
      "learning_rate": 0.00013493975903614458,
      "loss": 0.3128,
      "step": 410
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.08187974244356155,
      "learning_rate": 0.00013477911646586345,
      "loss": 0.386,
      "step": 411
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.059112466871738434,
      "learning_rate": 0.00013461847389558234,
      "loss": 0.3063,
      "step": 412
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.09134822338819504,
      "learning_rate": 0.0001344578313253012,
      "loss": 0.4101,
      "step": 413
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.0735611543059349,
      "learning_rate": 0.0001342971887550201,
      "loss": 0.3685,
      "step": 414
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.05567735806107521,
      "learning_rate": 0.00013413654618473896,
      "loss": 0.369,
      "step": 415
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.0744597464799881,
      "learning_rate": 0.00013397590361445785,
      "loss": 0.3272,
      "step": 416
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.07096382975578308,
      "learning_rate": 0.00013381526104417672,
      "loss": 0.3756,
      "step": 417
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.06880462914705276,
      "learning_rate": 0.0001336546184738956,
      "loss": 0.3668,
      "step": 418
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.05321045219898224,
      "learning_rate": 0.00013349397590361445,
      "loss": 0.3133,
      "step": 419
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.07057032734155655,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.3746,
      "step": 420
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.06684423983097076,
      "learning_rate": 0.0001331726907630522,
      "loss": 0.3178,
      "step": 421
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.06373964995145798,
      "learning_rate": 0.0001330120481927711,
      "loss": 0.3951,
      "step": 422
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.08410476893186569,
      "learning_rate": 0.00013285140562248996,
      "loss": 0.4107,
      "step": 423
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.06285464018583298,
      "learning_rate": 0.00013269076305220885,
      "loss": 0.3483,
      "step": 424
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.09642153233289719,
      "learning_rate": 0.00013253012048192772,
      "loss": 0.4496,
      "step": 425
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.05279170349240303,
      "learning_rate": 0.0001323694779116466,
      "loss": 0.276,
      "step": 426
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.061423420906066895,
      "learning_rate": 0.00013220883534136547,
      "loss": 0.2914,
      "step": 427
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.0811285600066185,
      "learning_rate": 0.00013204819277108434,
      "loss": 0.3513,
      "step": 428
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.04665320739150047,
      "learning_rate": 0.0001318875502008032,
      "loss": 0.2817,
      "step": 429
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.06472908705472946,
      "learning_rate": 0.0001317269076305221,
      "loss": 0.3985,
      "step": 430
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.07561035454273224,
      "learning_rate": 0.00013156626506024096,
      "loss": 0.3673,
      "step": 431
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.08141298592090607,
      "learning_rate": 0.00013140562248995985,
      "loss": 0.4384,
      "step": 432
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.05684703215956688,
      "learning_rate": 0.00013124497991967872,
      "loss": 0.2688,
      "step": 433
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.07057306915521622,
      "learning_rate": 0.0001310843373493976,
      "loss": 0.3571,
      "step": 434
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.06435080617666245,
      "learning_rate": 0.00013092369477911648,
      "loss": 0.3257,
      "step": 435
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.05148976296186447,
      "learning_rate": 0.00013076305220883537,
      "loss": 0.3112,
      "step": 436
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.0629786029458046,
      "learning_rate": 0.0001306024096385542,
      "loss": 0.4097,
      "step": 437
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.04584609344601631,
      "learning_rate": 0.0001304417670682731,
      "loss": 0.2812,
      "step": 438
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.04421386122703552,
      "learning_rate": 0.00013028112449799196,
      "loss": 0.2564,
      "step": 439
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.054488200694322586,
      "learning_rate": 0.00013012048192771085,
      "loss": 0.2832,
      "step": 440
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.07859399169683456,
      "learning_rate": 0.00012995983935742972,
      "loss": 0.4182,
      "step": 441
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.0666842833161354,
      "learning_rate": 0.0001297991967871486,
      "loss": 0.3526,
      "step": 442
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.0540032684803009,
      "learning_rate": 0.00012963855421686748,
      "loss": 0.3454,
      "step": 443
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.07004120200872421,
      "learning_rate": 0.00012947791164658637,
      "loss": 0.4235,
      "step": 444
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.08790916949510574,
      "learning_rate": 0.00012931726907630523,
      "loss": 0.3941,
      "step": 445
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.04883920028805733,
      "learning_rate": 0.0001291566265060241,
      "loss": 0.3172,
      "step": 446
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.08013439923524857,
      "learning_rate": 0.00012899598393574296,
      "loss": 0.3427,
      "step": 447
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.07826225459575653,
      "learning_rate": 0.00012883534136546185,
      "loss": 0.31,
      "step": 448
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.060293689370155334,
      "learning_rate": 0.00012867469879518072,
      "loss": 0.3258,
      "step": 449
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.05818617716431618,
      "learning_rate": 0.0001285140562248996,
      "loss": 0.397,
      "step": 450
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.0640728622674942,
      "learning_rate": 0.00012835341365461848,
      "loss": 0.322,
      "step": 451
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.06293360888957977,
      "learning_rate": 0.00012819277108433737,
      "loss": 0.3168,
      "step": 452
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.062387946993112564,
      "learning_rate": 0.00012803212851405623,
      "loss": 0.3307,
      "step": 453
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.052799709141254425,
      "learning_rate": 0.00012787148594377512,
      "loss": 0.3528,
      "step": 454
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.07839447259902954,
      "learning_rate": 0.00012771084337349396,
      "loss": 0.417,
      "step": 455
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.06793881952762604,
      "learning_rate": 0.00012755020080321285,
      "loss": 0.3713,
      "step": 456
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.05840355157852173,
      "learning_rate": 0.00012738955823293172,
      "loss": 0.3543,
      "step": 457
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.04605015367269516,
      "learning_rate": 0.0001272289156626506,
      "loss": 0.248,
      "step": 458
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.06799069046974182,
      "learning_rate": 0.00012706827309236948,
      "loss": 0.3863,
      "step": 459
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.06314552575349808,
      "learning_rate": 0.00012690763052208837,
      "loss": 0.3199,
      "step": 460
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.08095140755176544,
      "learning_rate": 0.00012674698795180723,
      "loss": 0.4221,
      "step": 461
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.05364196002483368,
      "learning_rate": 0.00012658634538152612,
      "loss": 0.3802,
      "step": 462
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.07214777916669846,
      "learning_rate": 0.000126425702811245,
      "loss": 0.3586,
      "step": 463
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.08265020698308945,
      "learning_rate": 0.00012626506024096385,
      "loss": 0.4133,
      "step": 464
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.06965584307909012,
      "learning_rate": 0.00012610441767068272,
      "loss": 0.3595,
      "step": 465
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.06290312856435776,
      "learning_rate": 0.0001259437751004016,
      "loss": 0.341,
      "step": 466
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.05940334498882294,
      "learning_rate": 0.00012578313253012048,
      "loss": 0.2993,
      "step": 467
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.06343154609203339,
      "learning_rate": 0.00012562248995983937,
      "loss": 0.3568,
      "step": 468
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.06140105426311493,
      "learning_rate": 0.00012546184738955823,
      "loss": 0.2582,
      "step": 469
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.07051917165517807,
      "learning_rate": 0.00012530120481927712,
      "loss": 0.3678,
      "step": 470
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.0739557221531868,
      "learning_rate": 0.000125140562248996,
      "loss": 0.3865,
      "step": 471
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.0739377811551094,
      "learning_rate": 0.00012497991967871488,
      "loss": 0.3635,
      "step": 472
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.05477158725261688,
      "learning_rate": 0.00012481927710843375,
      "loss": 0.3516,
      "step": 473
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.05563298240303993,
      "learning_rate": 0.0001246586345381526,
      "loss": 0.2941,
      "step": 474
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.076485775411129,
      "learning_rate": 0.00012449799196787148,
      "loss": 0.406,
      "step": 475
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.07891671359539032,
      "learning_rate": 0.00012433734939759037,
      "loss": 0.3989,
      "step": 476
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.07034915685653687,
      "learning_rate": 0.00012417670682730923,
      "loss": 0.3909,
      "step": 477
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.078117236495018,
      "learning_rate": 0.00012401606425702812,
      "loss": 0.4012,
      "step": 478
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.07159332185983658,
      "learning_rate": 0.000123855421686747,
      "loss": 0.2831,
      "step": 479
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.06709907203912735,
      "learning_rate": 0.00012369477911646588,
      "loss": 0.364,
      "step": 480
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.073427215218544,
      "learning_rate": 0.00012353413654618475,
      "loss": 0.3225,
      "step": 481
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.0798756331205368,
      "learning_rate": 0.00012337349397590364,
      "loss": 0.4014,
      "step": 482
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.06695383787155151,
      "learning_rate": 0.00012321285140562248,
      "loss": 0.4046,
      "step": 483
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.04132775962352753,
      "learning_rate": 0.00012305220883534137,
      "loss": 0.2848,
      "step": 484
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.06046376749873161,
      "learning_rate": 0.00012289156626506023,
      "loss": 0.2746,
      "step": 485
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.0717458575963974,
      "learning_rate": 0.00012273092369477912,
      "loss": 0.3226,
      "step": 486
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.059205252677202225,
      "learning_rate": 0.000122570281124498,
      "loss": 0.3267,
      "step": 487
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.043504565954208374,
      "learning_rate": 0.00012240963855421688,
      "loss": 0.2729,
      "step": 488
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.06624353677034378,
      "learning_rate": 0.00012224899598393575,
      "loss": 0.4015,
      "step": 489
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.08172548562288284,
      "learning_rate": 0.00012208835341365464,
      "loss": 0.3318,
      "step": 490
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.059720031917095184,
      "learning_rate": 0.00012192771084337352,
      "loss": 0.3744,
      "step": 491
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.06723903864622116,
      "learning_rate": 0.00012176706827309237,
      "loss": 0.3807,
      "step": 492
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.05478193983435631,
      "learning_rate": 0.00012160642570281125,
      "loss": 0.3462,
      "step": 493
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.06529470533132553,
      "learning_rate": 0.00012144578313253012,
      "loss": 0.3408,
      "step": 494
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.06673943996429443,
      "learning_rate": 0.000121285140562249,
      "loss": 0.3542,
      "step": 495
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.05105610191822052,
      "learning_rate": 0.00012112449799196788,
      "loss": 0.3524,
      "step": 496
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.044642169028520584,
      "learning_rate": 0.00012096385542168676,
      "loss": 0.2489,
      "step": 497
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.05824999511241913,
      "learning_rate": 0.00012080321285140564,
      "loss": 0.3408,
      "step": 498
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.06392057240009308,
      "learning_rate": 0.00012064257028112452,
      "loss": 0.3512,
      "step": 499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.07307913154363632,
      "learning_rate": 0.0001204819277108434,
      "loss": 0.4072,
      "step": 500
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.055405471473932266,
      "learning_rate": 0.00012032128514056225,
      "loss": 0.3152,
      "step": 501
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.06649139523506165,
      "learning_rate": 0.00012016064257028112,
      "loss": 0.4284,
      "step": 502
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.059203796088695526,
      "learning_rate": 0.00012,
      "loss": 0.3336,
      "step": 503
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.06327962875366211,
      "learning_rate": 0.00011983935742971888,
      "loss": 0.4096,
      "step": 504
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.0759531706571579,
      "learning_rate": 0.00011967871485943776,
      "loss": 0.4174,
      "step": 505
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.05792231857776642,
      "learning_rate": 0.00011951807228915664,
      "loss": 0.3665,
      "step": 506
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.07508721202611923,
      "learning_rate": 0.00011935742971887552,
      "loss": 0.4285,
      "step": 507
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.06456132233142853,
      "learning_rate": 0.0001191967871485944,
      "loss": 0.3748,
      "step": 508
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.05225694924592972,
      "learning_rate": 0.00011903614457831327,
      "loss": 0.2816,
      "step": 509
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.05805736035108566,
      "learning_rate": 0.00011887550200803212,
      "loss": 0.3627,
      "step": 510
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.0608530156314373,
      "learning_rate": 0.000118714859437751,
      "loss": 0.2572,
      "step": 511
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.06565160304307938,
      "learning_rate": 0.00011855421686746988,
      "loss": 0.3707,
      "step": 512
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.057280633598566055,
      "learning_rate": 0.00011839357429718876,
      "loss": 0.3667,
      "step": 513
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.06733991950750351,
      "learning_rate": 0.00011823293172690764,
      "loss": 0.4012,
      "step": 514
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.055476900190114975,
      "learning_rate": 0.00011807228915662652,
      "loss": 0.2885,
      "step": 515
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.06762271374464035,
      "learning_rate": 0.0001179116465863454,
      "loss": 0.3587,
      "step": 516
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.06388411670923233,
      "learning_rate": 0.00011775100401606427,
      "loss": 0.3084,
      "step": 517
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.05875794216990471,
      "learning_rate": 0.00011759036144578315,
      "loss": 0.2787,
      "step": 518
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.06348701566457748,
      "learning_rate": 0.000117429718875502,
      "loss": 0.3835,
      "step": 519
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.06531812995672226,
      "learning_rate": 0.00011726907630522088,
      "loss": 0.3807,
      "step": 520
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.056963302195072174,
      "learning_rate": 0.00011710843373493976,
      "loss": 0.3458,
      "step": 521
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.04476078972220421,
      "learning_rate": 0.00011694779116465864,
      "loss": 0.2507,
      "step": 522
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.060027506202459335,
      "learning_rate": 0.00011678714859437752,
      "loss": 0.3396,
      "step": 523
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.0651395246386528,
      "learning_rate": 0.0001166265060240964,
      "loss": 0.3553,
      "step": 524
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.0519057959318161,
      "learning_rate": 0.00011646586345381527,
      "loss": 0.2662,
      "step": 525
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.06216929480433464,
      "learning_rate": 0.00011630522088353415,
      "loss": 0.3389,
      "step": 526
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.062046825885772705,
      "learning_rate": 0.00011614457831325303,
      "loss": 0.3127,
      "step": 527
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.06794798374176025,
      "learning_rate": 0.00011598393574297188,
      "loss": 0.3434,
      "step": 528
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.0629834234714508,
      "learning_rate": 0.00011582329317269076,
      "loss": 0.3714,
      "step": 529
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.060365013778209686,
      "learning_rate": 0.00011566265060240964,
      "loss": 0.3266,
      "step": 530
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.05298301950097084,
      "learning_rate": 0.00011550200803212852,
      "loss": 0.3362,
      "step": 531
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.0628928542137146,
      "learning_rate": 0.0001153413654618474,
      "loss": 0.3043,
      "step": 532
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.04337809979915619,
      "learning_rate": 0.00011518072289156627,
      "loss": 0.2272,
      "step": 533
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.05928153172135353,
      "learning_rate": 0.00011502008032128515,
      "loss": 0.3368,
      "step": 534
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.08310858905315399,
      "learning_rate": 0.00011485943775100403,
      "loss": 0.4477,
      "step": 535
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.07302271574735641,
      "learning_rate": 0.00011469879518072291,
      "loss": 0.352,
      "step": 536
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.06697707623243332,
      "learning_rate": 0.00011453815261044176,
      "loss": 0.3517,
      "step": 537
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.07374100387096405,
      "learning_rate": 0.00011437751004016064,
      "loss": 0.3433,
      "step": 538
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.06807583570480347,
      "learning_rate": 0.00011421686746987952,
      "loss": 0.3142,
      "step": 539
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.049346841871738434,
      "learning_rate": 0.0001140562248995984,
      "loss": 0.2837,
      "step": 540
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.10214679688215256,
      "learning_rate": 0.00011389558232931727,
      "loss": 0.4569,
      "step": 541
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.05804583430290222,
      "learning_rate": 0.00011373493975903615,
      "loss": 0.3059,
      "step": 542
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.06731875240802765,
      "learning_rate": 0.00011357429718875503,
      "loss": 0.386,
      "step": 543
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.09399156272411346,
      "learning_rate": 0.00011341365461847391,
      "loss": 0.3992,
      "step": 544
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.07207625359296799,
      "learning_rate": 0.00011325301204819279,
      "loss": 0.3433,
      "step": 545
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.055079150944948196,
      "learning_rate": 0.00011309236947791164,
      "loss": 0.2937,
      "step": 546
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.06304877996444702,
      "learning_rate": 0.00011293172690763052,
      "loss": 0.3029,
      "step": 547
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.07973883301019669,
      "learning_rate": 0.0001127710843373494,
      "loss": 0.3467,
      "step": 548
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.08874926716089249,
      "learning_rate": 0.00011261044176706827,
      "loss": 0.4199,
      "step": 549
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.051859576255083084,
      "learning_rate": 0.00011244979919678715,
      "loss": 0.3044,
      "step": 550
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.08072362840175629,
      "learning_rate": 0.00011228915662650603,
      "loss": 0.3845,
      "step": 551
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.08989651501178741,
      "learning_rate": 0.00011212851405622491,
      "loss": 0.4638,
      "step": 552
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.07602068036794662,
      "learning_rate": 0.00011196787148594379,
      "loss": 0.4062,
      "step": 553
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.0704541727900505,
      "learning_rate": 0.00011180722891566267,
      "loss": 0.3109,
      "step": 554
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.0643598809838295,
      "learning_rate": 0.00011164658634538152,
      "loss": 0.3394,
      "step": 555
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.06563718616962433,
      "learning_rate": 0.0001114859437751004,
      "loss": 0.3969,
      "step": 556
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.06879755854606628,
      "learning_rate": 0.00011132530120481927,
      "loss": 0.3652,
      "step": 557
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.055445555597543716,
      "learning_rate": 0.00011116465863453815,
      "loss": 0.2987,
      "step": 558
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.06609378755092621,
      "learning_rate": 0.00011100401606425703,
      "loss": 0.3754,
      "step": 559
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.061825674027204514,
      "learning_rate": 0.00011084337349397591,
      "loss": 0.3138,
      "step": 560
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.05663657560944557,
      "learning_rate": 0.00011068273092369479,
      "loss": 0.3377,
      "step": 561
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.06473535299301147,
      "learning_rate": 0.00011052208835341367,
      "loss": 0.3319,
      "step": 562
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.058231256902217865,
      "learning_rate": 0.00011036144578313254,
      "loss": 0.3586,
      "step": 563
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.07010842859745026,
      "learning_rate": 0.0001102008032128514,
      "loss": 0.3739,
      "step": 564
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.06500428169965744,
      "learning_rate": 0.00011004016064257027,
      "loss": 0.3561,
      "step": 565
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.07704146206378937,
      "learning_rate": 0.00010987951807228915,
      "loss": 0.3547,
      "step": 566
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.06808086484670639,
      "learning_rate": 0.00010971887550200803,
      "loss": 0.2892,
      "step": 567
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.06329892575740814,
      "learning_rate": 0.00010955823293172691,
      "loss": 0.36,
      "step": 568
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.07368778437376022,
      "learning_rate": 0.00010939759036144579,
      "loss": 0.3185,
      "step": 569
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.06914938241243362,
      "learning_rate": 0.00010923694779116467,
      "loss": 0.3212,
      "step": 570
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.07905329763889313,
      "learning_rate": 0.00010907630522088354,
      "loss": 0.3338,
      "step": 571
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.07625938206911087,
      "learning_rate": 0.00010891566265060242,
      "loss": 0.3825,
      "step": 572
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.048118483275175095,
      "learning_rate": 0.00010875502008032127,
      "loss": 0.2699,
      "step": 573
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.061509810388088226,
      "learning_rate": 0.00010859437751004015,
      "loss": 0.3248,
      "step": 574
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.06734441965818405,
      "learning_rate": 0.00010843373493975903,
      "loss": 0.3715,
      "step": 575
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.06595247983932495,
      "learning_rate": 0.00010827309236947791,
      "loss": 0.3299,
      "step": 576
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.06982835382223129,
      "learning_rate": 0.00010811244979919679,
      "loss": 0.3339,
      "step": 577
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.0674685388803482,
      "learning_rate": 0.00010795180722891567,
      "loss": 0.3931,
      "step": 578
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.07563046365976334,
      "learning_rate": 0.00010779116465863454,
      "loss": 0.3329,
      "step": 579
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.06894244998693466,
      "learning_rate": 0.00010763052208835342,
      "loss": 0.3745,
      "step": 580
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.07127521932125092,
      "learning_rate": 0.0001074698795180723,
      "loss": 0.3789,
      "step": 581
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.054480671882629395,
      "learning_rate": 0.00010730923694779118,
      "loss": 0.2815,
      "step": 582
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.07069193571805954,
      "learning_rate": 0.00010714859437751003,
      "loss": 0.294,
      "step": 583
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.060547493398189545,
      "learning_rate": 0.00010698795180722891,
      "loss": 0.3325,
      "step": 584
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.07301385700702667,
      "learning_rate": 0.00010682730923694779,
      "loss": 0.3671,
      "step": 585
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.058200154453516006,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.3499,
      "step": 586
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.05767529830336571,
      "learning_rate": 0.00010650602409638554,
      "loss": 0.3117,
      "step": 587
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.06692565977573395,
      "learning_rate": 0.00010634538152610442,
      "loss": 0.2841,
      "step": 588
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.05908389762043953,
      "learning_rate": 0.0001061847389558233,
      "loss": 0.3403,
      "step": 589
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.06080919876694679,
      "learning_rate": 0.00010602409638554218,
      "loss": 0.284,
      "step": 590
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.05906993895769119,
      "learning_rate": 0.00010586345381526106,
      "loss": 0.3419,
      "step": 591
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.06341567635536194,
      "learning_rate": 0.00010570281124497991,
      "loss": 0.3563,
      "step": 592
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.05414258688688278,
      "learning_rate": 0.00010554216867469879,
      "loss": 0.3125,
      "step": 593
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.05058576911687851,
      "learning_rate": 0.00010538152610441767,
      "loss": 0.2632,
      "step": 594
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.0635431557893753,
      "learning_rate": 0.00010522088353413654,
      "loss": 0.3128,
      "step": 595
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.05724791809916496,
      "learning_rate": 0.00010506024096385542,
      "loss": 0.3195,
      "step": 596
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.05529351904988289,
      "learning_rate": 0.0001048995983935743,
      "loss": 0.2951,
      "step": 597
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.07328370213508606,
      "learning_rate": 0.00010473895582329318,
      "loss": 0.4148,
      "step": 598
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.05996965616941452,
      "learning_rate": 0.00010457831325301206,
      "loss": 0.3687,
      "step": 599
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.06598532944917679,
      "learning_rate": 0.00010441767068273094,
      "loss": 0.3319,
      "step": 600
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.07335791736841202,
      "learning_rate": 0.0001042570281124498,
      "loss": 0.414,
      "step": 601
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.05746404826641083,
      "learning_rate": 0.00010409638554216867,
      "loss": 0.3841,
      "step": 602
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.05675050988793373,
      "learning_rate": 0.00010393574297188754,
      "loss": 0.2982,
      "step": 603
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.07604406028985977,
      "learning_rate": 0.00010377510040160642,
      "loss": 0.3936,
      "step": 604
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.06502636522054672,
      "learning_rate": 0.0001036144578313253,
      "loss": 0.3988,
      "step": 605
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.06589922308921814,
      "learning_rate": 0.00010345381526104418,
      "loss": 0.3865,
      "step": 606
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.06779200583696365,
      "learning_rate": 0.00010329317269076306,
      "loss": 0.3784,
      "step": 607
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.06073305755853653,
      "learning_rate": 0.00010313253012048194,
      "loss": 0.2842,
      "step": 608
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.05451224744319916,
      "learning_rate": 0.00010297188755020082,
      "loss": 0.3457,
      "step": 609
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.06519901007413864,
      "learning_rate": 0.00010281124497991968,
      "loss": 0.334,
      "step": 610
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.06272966414690018,
      "learning_rate": 0.00010265060240963856,
      "loss": 0.3298,
      "step": 611
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.05813700333237648,
      "learning_rate": 0.00010248995983935742,
      "loss": 0.2917,
      "step": 612
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.05543652921915054,
      "learning_rate": 0.0001023293172690763,
      "loss": 0.3084,
      "step": 613
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.07068846374750137,
      "learning_rate": 0.00010216867469879518,
      "loss": 0.3501,
      "step": 614
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.07762624323368073,
      "learning_rate": 0.00010200803212851406,
      "loss": 0.3736,
      "step": 615
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.052042409777641296,
      "learning_rate": 0.00010184738955823294,
      "loss": 0.2914,
      "step": 616
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.050495922565460205,
      "learning_rate": 0.00010168674698795182,
      "loss": 0.2996,
      "step": 617
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.05474431440234184,
      "learning_rate": 0.0001015261044176707,
      "loss": 0.3102,
      "step": 618
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.050446365028619766,
      "learning_rate": 0.00010136546184738956,
      "loss": 0.302,
      "step": 619
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.05460984259843826,
      "learning_rate": 0.00010120481927710844,
      "loss": 0.2515,
      "step": 620
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.06691516935825348,
      "learning_rate": 0.00010104417670682732,
      "loss": 0.3644,
      "step": 621
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.04104073718190193,
      "learning_rate": 0.0001008835341365462,
      "loss": 0.255,
      "step": 622
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.06972356885671616,
      "learning_rate": 0.00010072289156626506,
      "loss": 0.2978,
      "step": 623
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.06232043728232384,
      "learning_rate": 0.00010056224899598394,
      "loss": 0.3762,
      "step": 624
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.06501064449548721,
      "learning_rate": 0.00010040160642570282,
      "loss": 0.3653,
      "step": 625
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.07496889680624008,
      "learning_rate": 0.0001002409638554217,
      "loss": 0.3997,
      "step": 626
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.05388135835528374,
      "learning_rate": 0.00010008032128514057,
      "loss": 0.334,
      "step": 627
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.07505439221858978,
      "learning_rate": 9.991967871485944e-05,
      "loss": 0.382,
      "step": 628
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.05444902926683426,
      "learning_rate": 9.975903614457832e-05,
      "loss": 0.3391,
      "step": 629
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.061701174825429916,
      "learning_rate": 9.95983935742972e-05,
      "loss": 0.293,
      "step": 630
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.0580899752676487,
      "learning_rate": 9.943775100401607e-05,
      "loss": 0.3518,
      "step": 631
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.06712877750396729,
      "learning_rate": 9.927710843373495e-05,
      "loss": 0.3401,
      "step": 632
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.0747351422905922,
      "learning_rate": 9.911646586345382e-05,
      "loss": 0.3542,
      "step": 633
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.054940078407526016,
      "learning_rate": 9.89558232931727e-05,
      "loss": 0.2927,
      "step": 634
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.06732618063688278,
      "learning_rate": 9.879518072289157e-05,
      "loss": 0.3216,
      "step": 635
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.0647784024477005,
      "learning_rate": 9.863453815261045e-05,
      "loss": 0.3372,
      "step": 636
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.06565595418214798,
      "learning_rate": 9.847389558232933e-05,
      "loss": 0.3967,
      "step": 637
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.07509265094995499,
      "learning_rate": 9.831325301204821e-05,
      "loss": 0.3699,
      "step": 638
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.0734381154179573,
      "learning_rate": 9.815261044176707e-05,
      "loss": 0.3928,
      "step": 639
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.08888192474842072,
      "learning_rate": 9.799196787148595e-05,
      "loss": 0.3329,
      "step": 640
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.06879717856645584,
      "learning_rate": 9.783132530120483e-05,
      "loss": 0.3843,
      "step": 641
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.04971776530146599,
      "learning_rate": 9.767068273092371e-05,
      "loss": 0.2313,
      "step": 642
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.088130883872509,
      "learning_rate": 9.751004016064259e-05,
      "loss": 0.3728,
      "step": 643
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.06959286332130432,
      "learning_rate": 9.734939759036145e-05,
      "loss": 0.3003,
      "step": 644
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.05551495403051376,
      "learning_rate": 9.718875502008033e-05,
      "loss": 0.2726,
      "step": 645
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.06217409297823906,
      "learning_rate": 9.702811244979921e-05,
      "loss": 0.3727,
      "step": 646
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.061373092234134674,
      "learning_rate": 9.686746987951809e-05,
      "loss": 0.3388,
      "step": 647
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.0696963220834732,
      "learning_rate": 9.670682730923695e-05,
      "loss": 0.3501,
      "step": 648
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.07098784297704697,
      "learning_rate": 9.654618473895583e-05,
      "loss": 0.3515,
      "step": 649
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.08324383199214935,
      "learning_rate": 9.638554216867471e-05,
      "loss": 0.396,
      "step": 650
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.07100134342908859,
      "learning_rate": 9.622489959839359e-05,
      "loss": 0.322,
      "step": 651
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.062463440001010895,
      "learning_rate": 9.606425702811246e-05,
      "loss": 0.3157,
      "step": 652
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.07759964466094971,
      "learning_rate": 9.590361445783133e-05,
      "loss": 0.3951,
      "step": 653
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.05050428956747055,
      "learning_rate": 9.574297188755021e-05,
      "loss": 0.2981,
      "step": 654
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.059949375689029694,
      "learning_rate": 9.558232931726909e-05,
      "loss": 0.359,
      "step": 655
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.07678517699241638,
      "learning_rate": 9.542168674698796e-05,
      "loss": 0.3625,
      "step": 656
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.0681803971529007,
      "learning_rate": 9.526104417670684e-05,
      "loss": 0.321,
      "step": 657
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.07053149491548538,
      "learning_rate": 9.510040160642571e-05,
      "loss": 0.356,
      "step": 658
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.04623858630657196,
      "learning_rate": 9.493975903614459e-05,
      "loss": 0.3022,
      "step": 659
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.06396803259849548,
      "learning_rate": 9.477911646586346e-05,
      "loss": 0.3593,
      "step": 660
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.06764748692512512,
      "learning_rate": 9.461847389558234e-05,
      "loss": 0.3776,
      "step": 661
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.06690395623445511,
      "learning_rate": 9.445783132530121e-05,
      "loss": 0.3308,
      "step": 662
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.07635217905044556,
      "learning_rate": 9.429718875502009e-05,
      "loss": 0.4256,
      "step": 663
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.06111736595630646,
      "learning_rate": 9.413654618473896e-05,
      "loss": 0.3168,
      "step": 664
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.06100204586982727,
      "learning_rate": 9.397590361445784e-05,
      "loss": 0.2841,
      "step": 665
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.059569984674453735,
      "learning_rate": 9.381526104417672e-05,
      "loss": 0.3256,
      "step": 666
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.058353327214717865,
      "learning_rate": 9.365461847389559e-05,
      "loss": 0.3652,
      "step": 667
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.06474019587039948,
      "learning_rate": 9.349397590361446e-05,
      "loss": 0.3764,
      "step": 668
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.060683105140924454,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.3659,
      "step": 669
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.08275843411684036,
      "learning_rate": 9.317269076305222e-05,
      "loss": 0.3836,
      "step": 670
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.0530012808740139,
      "learning_rate": 9.301204819277109e-05,
      "loss": 0.3137,
      "step": 671
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.07025498151779175,
      "learning_rate": 9.285140562248996e-05,
      "loss": 0.4003,
      "step": 672
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.07092162221670151,
      "learning_rate": 9.269076305220884e-05,
      "loss": 0.315,
      "step": 673
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.06941451132297516,
      "learning_rate": 9.253012048192772e-05,
      "loss": 0.2913,
      "step": 674
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.06042703613638878,
      "learning_rate": 9.23694779116466e-05,
      "loss": 0.339,
      "step": 675
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.06751547753810883,
      "learning_rate": 9.220883534136546e-05,
      "loss": 0.3317,
      "step": 676
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.05197446793317795,
      "learning_rate": 9.204819277108434e-05,
      "loss": 0.3504,
      "step": 677
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.05260242894291878,
      "learning_rate": 9.188755020080322e-05,
      "loss": 0.2957,
      "step": 678
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.04754674434661865,
      "learning_rate": 9.17269076305221e-05,
      "loss": 0.2624,
      "step": 679
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.05637321621179581,
      "learning_rate": 9.156626506024096e-05,
      "loss": 0.2964,
      "step": 680
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.05382164567708969,
      "learning_rate": 9.140562248995984e-05,
      "loss": 0.303,
      "step": 681
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.06555651873350143,
      "learning_rate": 9.124497991967872e-05,
      "loss": 0.3711,
      "step": 682
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.05630161985754967,
      "learning_rate": 9.10843373493976e-05,
      "loss": 0.3141,
      "step": 683
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.04758686572313309,
      "learning_rate": 9.092369477911648e-05,
      "loss": 0.3211,
      "step": 684
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.06941699236631393,
      "learning_rate": 9.076305220883534e-05,
      "loss": 0.4181,
      "step": 685
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.07677900046110153,
      "learning_rate": 9.060240963855422e-05,
      "loss": 0.411,
      "step": 686
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.0701543316245079,
      "learning_rate": 9.04417670682731e-05,
      "loss": 0.427,
      "step": 687
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.07042483985424042,
      "learning_rate": 9.028112449799198e-05,
      "loss": 0.365,
      "step": 688
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.0624089390039444,
      "learning_rate": 9.012048192771084e-05,
      "loss": 0.3248,
      "step": 689
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.07342611253261566,
      "learning_rate": 8.995983935742972e-05,
      "loss": 0.3961,
      "step": 690
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.067500539124012,
      "learning_rate": 8.97991967871486e-05,
      "loss": 0.3088,
      "step": 691
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.06204976513981819,
      "learning_rate": 8.963855421686748e-05,
      "loss": 0.3296,
      "step": 692
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.06406392902135849,
      "learning_rate": 8.947791164658636e-05,
      "loss": 0.3554,
      "step": 693
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.06824392825365067,
      "learning_rate": 8.931726907630522e-05,
      "loss": 0.4331,
      "step": 694
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.06836428493261337,
      "learning_rate": 8.91566265060241e-05,
      "loss": 0.3993,
      "step": 695
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.06958330422639847,
      "learning_rate": 8.899598393574298e-05,
      "loss": 0.4124,
      "step": 696
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.07536298036575317,
      "learning_rate": 8.883534136546186e-05,
      "loss": 0.3375,
      "step": 697
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.06698978692293167,
      "learning_rate": 8.867469879518072e-05,
      "loss": 0.3148,
      "step": 698
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.05479678139090538,
      "learning_rate": 8.85140562248996e-05,
      "loss": 0.2946,
      "step": 699
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.05650778114795685,
      "learning_rate": 8.835341365461848e-05,
      "loss": 0.3324,
      "step": 700
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.06264760345220566,
      "learning_rate": 8.819277108433736e-05,
      "loss": 0.3516,
      "step": 701
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.04631378501653671,
      "learning_rate": 8.803212851405624e-05,
      "loss": 0.3074,
      "step": 702
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.07527635246515274,
      "learning_rate": 8.78714859437751e-05,
      "loss": 0.3999,
      "step": 703
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.06622155755758286,
      "learning_rate": 8.771084337349398e-05,
      "loss": 0.3852,
      "step": 704
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.06164142116904259,
      "learning_rate": 8.755020080321286e-05,
      "loss": 0.3561,
      "step": 705
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.06995919346809387,
      "learning_rate": 8.738955823293174e-05,
      "loss": 0.3334,
      "step": 706
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.06369038671255112,
      "learning_rate": 8.722891566265061e-05,
      "loss": 0.3553,
      "step": 707
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.06237335130572319,
      "learning_rate": 8.706827309236948e-05,
      "loss": 0.3636,
      "step": 708
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.059221796691417694,
      "learning_rate": 8.690763052208836e-05,
      "loss": 0.3427,
      "step": 709
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.05940323323011398,
      "learning_rate": 8.674698795180724e-05,
      "loss": 0.3578,
      "step": 710
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.0697200670838356,
      "learning_rate": 8.658634538152611e-05,
      "loss": 0.415,
      "step": 711
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.04892057925462723,
      "learning_rate": 8.642570281124498e-05,
      "loss": 0.2844,
      "step": 712
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.06754235923290253,
      "learning_rate": 8.626506024096386e-05,
      "loss": 0.3717,
      "step": 713
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.07916872948408127,
      "learning_rate": 8.610441767068274e-05,
      "loss": 0.3271,
      "step": 714
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.055646251887083054,
      "learning_rate": 8.594377510040161e-05,
      "loss": 0.371,
      "step": 715
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.06692821532487869,
      "learning_rate": 8.578313253012049e-05,
      "loss": 0.3782,
      "step": 716
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.07036356627941132,
      "learning_rate": 8.562248995983936e-05,
      "loss": 0.4307,
      "step": 717
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.052394840866327286,
      "learning_rate": 8.546184738955824e-05,
      "loss": 0.3459,
      "step": 718
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.0548989363014698,
      "learning_rate": 8.530120481927711e-05,
      "loss": 0.3456,
      "step": 719
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.06297802180051804,
      "learning_rate": 8.514056224899599e-05,
      "loss": 0.3475,
      "step": 720
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.06020796671509743,
      "learning_rate": 8.497991967871486e-05,
      "loss": 0.3492,
      "step": 721
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.07616507261991501,
      "learning_rate": 8.481927710843374e-05,
      "loss": 0.3988,
      "step": 722
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.0653141662478447,
      "learning_rate": 8.465863453815261e-05,
      "loss": 0.3425,
      "step": 723
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.05484617128968239,
      "learning_rate": 8.449799196787149e-05,
      "loss": 0.3076,
      "step": 724
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.07821284979581833,
      "learning_rate": 8.433734939759037e-05,
      "loss": 0.3934,
      "step": 725
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.06438212841749191,
      "learning_rate": 8.417670682730924e-05,
      "loss": 0.3965,
      "step": 726
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.06164877489209175,
      "learning_rate": 8.401606425702811e-05,
      "loss": 0.3135,
      "step": 727
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.07923221588134766,
      "learning_rate": 8.385542168674699e-05,
      "loss": 0.4168,
      "step": 728
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.05760684236884117,
      "learning_rate": 8.369477911646587e-05,
      "loss": 0.2959,
      "step": 729
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.06142643094062805,
      "learning_rate": 8.353413654618474e-05,
      "loss": 0.2242,
      "step": 730
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.0661158412694931,
      "learning_rate": 8.337349397590361e-05,
      "loss": 0.2715,
      "step": 731
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.05614747107028961,
      "learning_rate": 8.321285140562249e-05,
      "loss": 0.299,
      "step": 732
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.0581224262714386,
      "learning_rate": 8.305220883534137e-05,
      "loss": 0.3285,
      "step": 733
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.0792611762881279,
      "learning_rate": 8.289156626506025e-05,
      "loss": 0.3715,
      "step": 734
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.06930011510848999,
      "learning_rate": 8.273092369477911e-05,
      "loss": 0.3351,
      "step": 735
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.051409684121608734,
      "learning_rate": 8.257028112449799e-05,
      "loss": 0.2843,
      "step": 736
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.05640774965286255,
      "learning_rate": 8.240963855421687e-05,
      "loss": 0.3319,
      "step": 737
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.06869612634181976,
      "learning_rate": 8.224899598393575e-05,
      "loss": 0.3738,
      "step": 738
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.06058045104146004,
      "learning_rate": 8.208835341365461e-05,
      "loss": 0.3164,
      "step": 739
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.05693518742918968,
      "learning_rate": 8.192771084337349e-05,
      "loss": 0.3008,
      "step": 740
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.07786459475755692,
      "learning_rate": 8.176706827309237e-05,
      "loss": 0.3987,
      "step": 741
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.04226836934685707,
      "learning_rate": 8.160642570281125e-05,
      "loss": 0.2837,
      "step": 742
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.0608837865293026,
      "learning_rate": 8.144578313253013e-05,
      "loss": 0.3666,
      "step": 743
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.05701594054698944,
      "learning_rate": 8.128514056224899e-05,
      "loss": 0.3085,
      "step": 744
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.056706078350543976,
      "learning_rate": 8.112449799196787e-05,
      "loss": 0.3634,
      "step": 745
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.05391532555222511,
      "learning_rate": 8.096385542168675e-05,
      "loss": 0.3253,
      "step": 746
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.06930788606405258,
      "learning_rate": 8.080321285140563e-05,
      "loss": 0.3847,
      "step": 747
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.05402973294258118,
      "learning_rate": 8.064257028112449e-05,
      "loss": 0.2224,
      "step": 748
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.04367247596383095,
      "learning_rate": 8.048192771084337e-05,
      "loss": 0.2493,
      "step": 749
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.05629059299826622,
      "learning_rate": 8.032128514056225e-05,
      "loss": 0.3259,
      "step": 750
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.06913460046052933,
      "learning_rate": 8.016064257028113e-05,
      "loss": 0.325,
      "step": 751
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.09685324132442474,
      "learning_rate": 8e-05,
      "loss": 0.412,
      "step": 752
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.058769021183252335,
      "learning_rate": 7.983935742971887e-05,
      "loss": 0.3166,
      "step": 753
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.06336981803178787,
      "learning_rate": 7.967871485943775e-05,
      "loss": 0.3478,
      "step": 754
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.07857558876276016,
      "learning_rate": 7.951807228915663e-05,
      "loss": 0.3866,
      "step": 755
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.07662692666053772,
      "learning_rate": 7.93574297188755e-05,
      "loss": 0.2993,
      "step": 756
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.08136207610368729,
      "learning_rate": 7.919678714859437e-05,
      "loss": 0.3674,
      "step": 757
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.05897491052746773,
      "learning_rate": 7.903614457831325e-05,
      "loss": 0.3512,
      "step": 758
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.06769218295812607,
      "learning_rate": 7.887550200803213e-05,
      "loss": 0.3354,
      "step": 759
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.09639851003885269,
      "learning_rate": 7.8714859437751e-05,
      "loss": 0.3951,
      "step": 760
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.06762901693582535,
      "learning_rate": 7.855421686746989e-05,
      "loss": 0.3276,
      "step": 761
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.07127787917852402,
      "learning_rate": 7.839357429718875e-05,
      "loss": 0.338,
      "step": 762
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.06763633340597153,
      "learning_rate": 7.823293172690763e-05,
      "loss": 0.3922,
      "step": 763
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.06606677919626236,
      "learning_rate": 7.80722891566265e-05,
      "loss": 0.3433,
      "step": 764
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.05927329882979393,
      "learning_rate": 7.791164658634539e-05,
      "loss": 0.3154,
      "step": 765
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.10767359286546707,
      "learning_rate": 7.775100401606426e-05,
      "loss": 0.3461,
      "step": 766
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.06738024204969406,
      "learning_rate": 7.759036144578313e-05,
      "loss": 0.3466,
      "step": 767
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.07033147662878036,
      "learning_rate": 7.7429718875502e-05,
      "loss": 0.3234,
      "step": 768
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.06247854232788086,
      "learning_rate": 7.726907630522089e-05,
      "loss": 0.2867,
      "step": 769
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.06394550949335098,
      "learning_rate": 7.710843373493976e-05,
      "loss": 0.3052,
      "step": 770
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.0793142020702362,
      "learning_rate": 7.694779116465863e-05,
      "loss": 0.3461,
      "step": 771
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.0626949667930603,
      "learning_rate": 7.678714859437751e-05,
      "loss": 0.316,
      "step": 772
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.06731603294610977,
      "learning_rate": 7.662650602409639e-05,
      "loss": 0.3225,
      "step": 773
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.0630626380443573,
      "learning_rate": 7.646586345381526e-05,
      "loss": 0.3059,
      "step": 774
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.06702890247106552,
      "learning_rate": 7.630522088353414e-05,
      "loss": 0.3234,
      "step": 775
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.06852971017360687,
      "learning_rate": 7.614457831325301e-05,
      "loss": 0.3075,
      "step": 776
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.070630744099617,
      "learning_rate": 7.598393574297189e-05,
      "loss": 0.3223,
      "step": 777
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.07377469539642334,
      "learning_rate": 7.582329317269076e-05,
      "loss": 0.3399,
      "step": 778
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.05884328484535217,
      "learning_rate": 7.566265060240964e-05,
      "loss": 0.2909,
      "step": 779
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.060448747128248215,
      "learning_rate": 7.550200803212851e-05,
      "loss": 0.3194,
      "step": 780
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.07086186110973358,
      "learning_rate": 7.534136546184739e-05,
      "loss": 0.36,
      "step": 781
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.0797935351729393,
      "learning_rate": 7.518072289156626e-05,
      "loss": 0.362,
      "step": 782
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.08758911490440369,
      "learning_rate": 7.502008032128514e-05,
      "loss": 0.3911,
      "step": 783
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.06299483776092529,
      "learning_rate": 7.485943775100402e-05,
      "loss": 0.3245,
      "step": 784
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.07413064688444138,
      "learning_rate": 7.469879518072289e-05,
      "loss": 0.3643,
      "step": 785
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.05972236767411232,
      "learning_rate": 7.453815261044176e-05,
      "loss": 0.3083,
      "step": 786
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.08040235191583633,
      "learning_rate": 7.437751004016064e-05,
      "loss": 0.3668,
      "step": 787
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.08502376079559326,
      "learning_rate": 7.421686746987952e-05,
      "loss": 0.302,
      "step": 788
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.06079428642988205,
      "learning_rate": 7.405622489959839e-05,
      "loss": 0.3174,
      "step": 789
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.06185637414455414,
      "learning_rate": 7.389558232931726e-05,
      "loss": 0.3144,
      "step": 790
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.0818169042468071,
      "learning_rate": 7.373493975903614e-05,
      "loss": 0.4196,
      "step": 791
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.06853940337896347,
      "learning_rate": 7.357429718875502e-05,
      "loss": 0.3459,
      "step": 792
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.07001756876707077,
      "learning_rate": 7.34136546184739e-05,
      "loss": 0.397,
      "step": 793
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.061555106192827225,
      "learning_rate": 7.325301204819278e-05,
      "loss": 0.3117,
      "step": 794
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.06159190461039543,
      "learning_rate": 7.309236947791164e-05,
      "loss": 0.3129,
      "step": 795
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.06964801996946335,
      "learning_rate": 7.293172690763052e-05,
      "loss": 0.3088,
      "step": 796
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.08794831484556198,
      "learning_rate": 7.27710843373494e-05,
      "loss": 0.4381,
      "step": 797
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.06179235130548477,
      "learning_rate": 7.261044176706828e-05,
      "loss": 0.3074,
      "step": 798
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.06699154525995255,
      "learning_rate": 7.244979919678716e-05,
      "loss": 0.3156,
      "step": 799
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.05596529692411423,
      "learning_rate": 7.228915662650602e-05,
      "loss": 0.3188,
      "step": 800
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.06793969869613647,
      "learning_rate": 7.21285140562249e-05,
      "loss": 0.3415,
      "step": 801
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.052193399518728256,
      "learning_rate": 7.196787148594378e-05,
      "loss": 0.2731,
      "step": 802
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.06743379682302475,
      "learning_rate": 7.180722891566266e-05,
      "loss": 0.3453,
      "step": 803
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.0719335526227951,
      "learning_rate": 7.164658634538153e-05,
      "loss": 0.3492,
      "step": 804
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.05035535991191864,
      "learning_rate": 7.14859437751004e-05,
      "loss": 0.2675,
      "step": 805
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.09651588648557663,
      "learning_rate": 7.132530120481928e-05,
      "loss": 0.4286,
      "step": 806
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.07611177861690521,
      "learning_rate": 7.116465863453816e-05,
      "loss": 0.3222,
      "step": 807
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.05719692260026932,
      "learning_rate": 7.100401606425703e-05,
      "loss": 0.2242,
      "step": 808
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.05904106795787811,
      "learning_rate": 7.084337349397591e-05,
      "loss": 0.3345,
      "step": 809
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.05010358244180679,
      "learning_rate": 7.068273092369478e-05,
      "loss": 0.2688,
      "step": 810
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.06359312683343887,
      "learning_rate": 7.052208835341366e-05,
      "loss": 0.3305,
      "step": 811
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.06419109553098679,
      "learning_rate": 7.036144578313253e-05,
      "loss": 0.3375,
      "step": 812
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.060938913375139236,
      "learning_rate": 7.020080321285141e-05,
      "loss": 0.3781,
      "step": 813
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.05595514178276062,
      "learning_rate": 7.004016064257029e-05,
      "loss": 0.332,
      "step": 814
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.0642617717385292,
      "learning_rate": 6.987951807228917e-05,
      "loss": 0.3926,
      "step": 815
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.07238379120826721,
      "learning_rate": 6.971887550200803e-05,
      "loss": 0.3487,
      "step": 816
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.06234511360526085,
      "learning_rate": 6.955823293172691e-05,
      "loss": 0.3656,
      "step": 817
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.09680210053920746,
      "learning_rate": 6.939759036144579e-05,
      "loss": 0.4089,
      "step": 818
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.0693131685256958,
      "learning_rate": 6.923694779116467e-05,
      "loss": 0.348,
      "step": 819
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.06078977510333061,
      "learning_rate": 6.907630522088355e-05,
      "loss": 0.3712,
      "step": 820
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.06521117687225342,
      "learning_rate": 6.891566265060241e-05,
      "loss": 0.3575,
      "step": 821
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.08043354004621506,
      "learning_rate": 6.875502008032129e-05,
      "loss": 0.3844,
      "step": 822
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.06076139584183693,
      "learning_rate": 6.859437751004017e-05,
      "loss": 0.3619,
      "step": 823
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.06166011467576027,
      "learning_rate": 6.843373493975905e-05,
      "loss": 0.3237,
      "step": 824
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.06582176685333252,
      "learning_rate": 6.827309236947793e-05,
      "loss": 0.3791,
      "step": 825
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.05226768180727959,
      "learning_rate": 6.811244979919679e-05,
      "loss": 0.2579,
      "step": 826
    },
    {
      "epoch": 3.308,
      "grad_norm": 0.05718173831701279,
      "learning_rate": 6.795180722891567e-05,
      "loss": 0.2933,
      "step": 827
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.06015141308307648,
      "learning_rate": 6.779116465863455e-05,
      "loss": 0.3207,
      "step": 828
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.06119931489229202,
      "learning_rate": 6.763052208835343e-05,
      "loss": 0.3267,
      "step": 829
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.062433578073978424,
      "learning_rate": 6.746987951807229e-05,
      "loss": 0.3539,
      "step": 830
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.05884864181280136,
      "learning_rate": 6.730923694779117e-05,
      "loss": 0.3224,
      "step": 831
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.03820882365107536,
      "learning_rate": 6.714859437751005e-05,
      "loss": 0.2517,
      "step": 832
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.061039358377456665,
      "learning_rate": 6.698795180722893e-05,
      "loss": 0.3079,
      "step": 833
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.07376720756292343,
      "learning_rate": 6.68273092369478e-05,
      "loss": 0.3459,
      "step": 834
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.0742548480629921,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.3665,
      "step": 835
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.06704956293106079,
      "learning_rate": 6.650602409638555e-05,
      "loss": 0.3403,
      "step": 836
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.07260894775390625,
      "learning_rate": 6.634538152610443e-05,
      "loss": 0.3836,
      "step": 837
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.06285098940134048,
      "learning_rate": 6.61847389558233e-05,
      "loss": 0.3278,
      "step": 838
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.0621136873960495,
      "learning_rate": 6.602409638554217e-05,
      "loss": 0.3044,
      "step": 839
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.0731024518609047,
      "learning_rate": 6.586345381526105e-05,
      "loss": 0.3289,
      "step": 840
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.05381637439131737,
      "learning_rate": 6.570281124497993e-05,
      "loss": 0.3167,
      "step": 841
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.06778277456760406,
      "learning_rate": 6.55421686746988e-05,
      "loss": 0.3227,
      "step": 842
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.05645051226019859,
      "learning_rate": 6.538152610441768e-05,
      "loss": 0.3458,
      "step": 843
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.07776034623384476,
      "learning_rate": 6.522088353413655e-05,
      "loss": 0.3224,
      "step": 844
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.06675401329994202,
      "learning_rate": 6.506024096385543e-05,
      "loss": 0.3405,
      "step": 845
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.06670773774385452,
      "learning_rate": 6.48995983935743e-05,
      "loss": 0.3536,
      "step": 846
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.06397136300802231,
      "learning_rate": 6.473895582329318e-05,
      "loss": 0.3487,
      "step": 847
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.06806420534849167,
      "learning_rate": 6.457831325301205e-05,
      "loss": 0.3528,
      "step": 848
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.07221612334251404,
      "learning_rate": 6.441767068273093e-05,
      "loss": 0.3237,
      "step": 849
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.058261267840862274,
      "learning_rate": 6.42570281124498e-05,
      "loss": 0.3263,
      "step": 850
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.058360081166028976,
      "learning_rate": 6.409638554216868e-05,
      "loss": 0.3279,
      "step": 851
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.05699371546506882,
      "learning_rate": 6.393574297188756e-05,
      "loss": 0.3426,
      "step": 852
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.05163797363638878,
      "learning_rate": 6.377510040160643e-05,
      "loss": 0.3005,
      "step": 853
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.07014431059360504,
      "learning_rate": 6.36144578313253e-05,
      "loss": 0.3627,
      "step": 854
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.060335516929626465,
      "learning_rate": 6.345381526104418e-05,
      "loss": 0.3856,
      "step": 855
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.08026950806379318,
      "learning_rate": 6.329317269076306e-05,
      "loss": 0.4107,
      "step": 856
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.07349849492311478,
      "learning_rate": 6.313253012048193e-05,
      "loss": 0.3465,
      "step": 857
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.06333954632282257,
      "learning_rate": 6.29718875502008e-05,
      "loss": 0.3557,
      "step": 858
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.05129498243331909,
      "learning_rate": 6.281124497991968e-05,
      "loss": 0.2538,
      "step": 859
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.0566449873149395,
      "learning_rate": 6.265060240963856e-05,
      "loss": 0.3034,
      "step": 860
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.06684695929288864,
      "learning_rate": 6.248995983935744e-05,
      "loss": 0.3498,
      "step": 861
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.05534299090504646,
      "learning_rate": 6.23293172690763e-05,
      "loss": 0.2901,
      "step": 862
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.06711357831954956,
      "learning_rate": 6.216867469879518e-05,
      "loss": 0.3661,
      "step": 863
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.060379836708307266,
      "learning_rate": 6.200803212851406e-05,
      "loss": 0.3115,
      "step": 864
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.07028962671756744,
      "learning_rate": 6.184738955823294e-05,
      "loss": 0.3282,
      "step": 865
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.06967539340257645,
      "learning_rate": 6.168674698795182e-05,
      "loss": 0.3892,
      "step": 866
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.06262396275997162,
      "learning_rate": 6.152610441767068e-05,
      "loss": 0.3699,
      "step": 867
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.06312043964862823,
      "learning_rate": 6.136546184738956e-05,
      "loss": 0.3483,
      "step": 868
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.06583817303180695,
      "learning_rate": 6.120481927710844e-05,
      "loss": 0.3508,
      "step": 869
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.05885249376296997,
      "learning_rate": 6.104417670682732e-05,
      "loss": 0.3506,
      "step": 870
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.06734023243188858,
      "learning_rate": 6.0883534136546184e-05,
      "loss": 0.3579,
      "step": 871
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.04721849039196968,
      "learning_rate": 6.072289156626506e-05,
      "loss": 0.31,
      "step": 872
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.07011149823665619,
      "learning_rate": 6.056224899598394e-05,
      "loss": 0.3604,
      "step": 873
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.0631944015622139,
      "learning_rate": 6.040160642570282e-05,
      "loss": 0.3677,
      "step": 874
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.05438712611794472,
      "learning_rate": 6.02409638554217e-05,
      "loss": 0.2788,
      "step": 875
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.044133737683296204,
      "learning_rate": 6.008032128514056e-05,
      "loss": 0.2845,
      "step": 876
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.059960611164569855,
      "learning_rate": 5.991967871485944e-05,
      "loss": 0.3133,
      "step": 877
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.05373760312795639,
      "learning_rate": 5.975903614457832e-05,
      "loss": 0.3306,
      "step": 878
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.06495556980371475,
      "learning_rate": 5.95983935742972e-05,
      "loss": 0.3843,
      "step": 879
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.08227512240409851,
      "learning_rate": 5.943775100401606e-05,
      "loss": 0.4089,
      "step": 880
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.05174765735864639,
      "learning_rate": 5.927710843373494e-05,
      "loss": 0.2823,
      "step": 881
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.057233598083257675,
      "learning_rate": 5.911646586345382e-05,
      "loss": 0.3324,
      "step": 882
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.06746894866228104,
      "learning_rate": 5.89558232931727e-05,
      "loss": 0.4,
      "step": 883
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.06282807886600494,
      "learning_rate": 5.8795180722891576e-05,
      "loss": 0.3544,
      "step": 884
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.05396772921085358,
      "learning_rate": 5.863453815261044e-05,
      "loss": 0.2981,
      "step": 885
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.057435423135757446,
      "learning_rate": 5.847389558232932e-05,
      "loss": 0.3237,
      "step": 886
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.05106205493211746,
      "learning_rate": 5.83132530120482e-05,
      "loss": 0.3197,
      "step": 887
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.06008034944534302,
      "learning_rate": 5.8152610441767076e-05,
      "loss": 0.3153,
      "step": 888
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.05237945541739464,
      "learning_rate": 5.799196787148594e-05,
      "loss": 0.3147,
      "step": 889
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.07601819932460785,
      "learning_rate": 5.783132530120482e-05,
      "loss": 0.3709,
      "step": 890
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.05103881284594536,
      "learning_rate": 5.76706827309237e-05,
      "loss": 0.3167,
      "step": 891
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.07412531226873398,
      "learning_rate": 5.7510040160642576e-05,
      "loss": 0.3841,
      "step": 892
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.07783621549606323,
      "learning_rate": 5.7349397590361454e-05,
      "loss": 0.327,
      "step": 893
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.05853015184402466,
      "learning_rate": 5.718875502008032e-05,
      "loss": 0.2904,
      "step": 894
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.06916490942239761,
      "learning_rate": 5.70281124497992e-05,
      "loss": 0.3565,
      "step": 895
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.053890060633420944,
      "learning_rate": 5.6867469879518076e-05,
      "loss": 0.3409,
      "step": 896
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.07679902017116547,
      "learning_rate": 5.6706827309236955e-05,
      "loss": 0.3485,
      "step": 897
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.07165731489658356,
      "learning_rate": 5.654618473895582e-05,
      "loss": 0.3456,
      "step": 898
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.07552235573530197,
      "learning_rate": 5.63855421686747e-05,
      "loss": 0.334,
      "step": 899
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.07578155398368835,
      "learning_rate": 5.6224899598393576e-05,
      "loss": 0.3333,
      "step": 900
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.06934592127799988,
      "learning_rate": 5.6064257028112455e-05,
      "loss": 0.3105,
      "step": 901
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.05572669953107834,
      "learning_rate": 5.590361445783133e-05,
      "loss": 0.2887,
      "step": 902
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.05328349396586418,
      "learning_rate": 5.57429718875502e-05,
      "loss": 0.2676,
      "step": 903
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.0644388422369957,
      "learning_rate": 5.5582329317269076e-05,
      "loss": 0.3148,
      "step": 904
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.06936785578727722,
      "learning_rate": 5.5421686746987955e-05,
      "loss": 0.3414,
      "step": 905
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.07503733038902283,
      "learning_rate": 5.526104417670683e-05,
      "loss": 0.3923,
      "step": 906
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.08331728726625443,
      "learning_rate": 5.51004016064257e-05,
      "loss": 0.4431,
      "step": 907
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.065740667283535,
      "learning_rate": 5.4939759036144576e-05,
      "loss": 0.3911,
      "step": 908
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.06147792935371399,
      "learning_rate": 5.4779116465863455e-05,
      "loss": 0.2752,
      "step": 909
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.06978560239076614,
      "learning_rate": 5.461847389558233e-05,
      "loss": 0.3295,
      "step": 910
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.06735303997993469,
      "learning_rate": 5.445783132530121e-05,
      "loss": 0.3009,
      "step": 911
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.06828147172927856,
      "learning_rate": 5.4297188755020076e-05,
      "loss": 0.3013,
      "step": 912
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.0634736493229866,
      "learning_rate": 5.4136546184738955e-05,
      "loss": 0.3127,
      "step": 913
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.07089237868785858,
      "learning_rate": 5.397590361445783e-05,
      "loss": 0.3122,
      "step": 914
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.06627516448497772,
      "learning_rate": 5.381526104417671e-05,
      "loss": 0.3108,
      "step": 915
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.07153508812189102,
      "learning_rate": 5.365461847389559e-05,
      "loss": 0.3648,
      "step": 916
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.05430467426776886,
      "learning_rate": 5.3493975903614455e-05,
      "loss": 0.2598,
      "step": 917
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.08301027864217758,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.4209,
      "step": 918
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.076593779027462,
      "learning_rate": 5.317269076305221e-05,
      "loss": 0.3588,
      "step": 919
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.06062394753098488,
      "learning_rate": 5.301204819277109e-05,
      "loss": 0.3424,
      "step": 920
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.0689755529165268,
      "learning_rate": 5.2851405622489955e-05,
      "loss": 0.3315,
      "step": 921
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.079436294734478,
      "learning_rate": 5.269076305220883e-05,
      "loss": 0.3275,
      "step": 922
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.07812658697366714,
      "learning_rate": 5.253012048192771e-05,
      "loss": 0.3644,
      "step": 923
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.058558546006679535,
      "learning_rate": 5.236947791164659e-05,
      "loss": 0.3116,
      "step": 924
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.0711672455072403,
      "learning_rate": 5.220883534136547e-05,
      "loss": 0.373,
      "step": 925
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.052782755345106125,
      "learning_rate": 5.204819277108433e-05,
      "loss": 0.3105,
      "step": 926
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.059886958450078964,
      "learning_rate": 5.188755020080321e-05,
      "loss": 0.3365,
      "step": 927
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.05507194995880127,
      "learning_rate": 5.172690763052209e-05,
      "loss": 0.276,
      "step": 928
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.0792672336101532,
      "learning_rate": 5.156626506024097e-05,
      "loss": 0.3649,
      "step": 929
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.06948582828044891,
      "learning_rate": 5.140562248995984e-05,
      "loss": 0.3447,
      "step": 930
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.05947618559002876,
      "learning_rate": 5.124497991967871e-05,
      "loss": 0.3414,
      "step": 931
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.07751172035932541,
      "learning_rate": 5.108433734939759e-05,
      "loss": 0.4159,
      "step": 932
    },
    {
      "epoch": 3.732,
      "grad_norm": 0.0861482098698616,
      "learning_rate": 5.092369477911647e-05,
      "loss": 0.3619,
      "step": 933
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.06493233144283295,
      "learning_rate": 5.076305220883535e-05,
      "loss": 0.3125,
      "step": 934
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.07322734594345093,
      "learning_rate": 5.060240963855422e-05,
      "loss": 0.3484,
      "step": 935
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.046704184263944626,
      "learning_rate": 5.04417670682731e-05,
      "loss": 0.255,
      "step": 936
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.0818551555275917,
      "learning_rate": 5.028112449799197e-05,
      "loss": 0.3699,
      "step": 937
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.05762128904461861,
      "learning_rate": 5.012048192771085e-05,
      "loss": 0.2649,
      "step": 938
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.06269076466560364,
      "learning_rate": 4.995983935742972e-05,
      "loss": 0.3335,
      "step": 939
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.06280229240655899,
      "learning_rate": 4.97991967871486e-05,
      "loss": 0.3341,
      "step": 940
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.0791536495089531,
      "learning_rate": 4.9638554216867475e-05,
      "loss": 0.3875,
      "step": 941
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.06359503418207169,
      "learning_rate": 4.947791164658635e-05,
      "loss": 0.3112,
      "step": 942
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.06605634838342667,
      "learning_rate": 4.9317269076305225e-05,
      "loss": 0.2986,
      "step": 943
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.07439744472503662,
      "learning_rate": 4.9156626506024104e-05,
      "loss": 0.3417,
      "step": 944
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.06638965755701065,
      "learning_rate": 4.8995983935742975e-05,
      "loss": 0.3459,
      "step": 945
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.05974597483873367,
      "learning_rate": 4.8835341365461854e-05,
      "loss": 0.3064,
      "step": 946
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.05864253267645836,
      "learning_rate": 4.8674698795180725e-05,
      "loss": 0.3442,
      "step": 947
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.07494492828845978,
      "learning_rate": 4.8514056224899604e-05,
      "loss": 0.3576,
      "step": 948
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.07719811797142029,
      "learning_rate": 4.8353413654618476e-05,
      "loss": 0.3658,
      "step": 949
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.059547703713178635,
      "learning_rate": 4.8192771084337354e-05,
      "loss": 0.33,
      "step": 950
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.0643593892455101,
      "learning_rate": 4.803212851405623e-05,
      "loss": 0.3054,
      "step": 951
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.053952284157276154,
      "learning_rate": 4.7871485943775104e-05,
      "loss": 0.3229,
      "step": 952
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 0.05609554424881935,
      "learning_rate": 4.771084337349398e-05,
      "loss": 0.2999,
      "step": 953
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.06336549669504166,
      "learning_rate": 4.7550200803212854e-05,
      "loss": 0.3297,
      "step": 954
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.060110412538051605,
      "learning_rate": 4.738955823293173e-05,
      "loss": 0.3258,
      "step": 955
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.10128485411405563,
      "learning_rate": 4.7228915662650604e-05,
      "loss": 0.4365,
      "step": 956
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.055415909737348557,
      "learning_rate": 4.706827309236948e-05,
      "loss": 0.2764,
      "step": 957
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.06552392989397049,
      "learning_rate": 4.690763052208836e-05,
      "loss": 0.3495,
      "step": 958
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.051926400512456894,
      "learning_rate": 4.674698795180723e-05,
      "loss": 0.2908,
      "step": 959
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.0885678082704544,
      "learning_rate": 4.658634538152611e-05,
      "loss": 0.4186,
      "step": 960
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.054709941148757935,
      "learning_rate": 4.642570281124498e-05,
      "loss": 0.2492,
      "step": 961
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.09979026764631271,
      "learning_rate": 4.626506024096386e-05,
      "loss": 0.3792,
      "step": 962
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.0620107427239418,
      "learning_rate": 4.610441767068273e-05,
      "loss": 0.3338,
      "step": 963
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.05976016819477081,
      "learning_rate": 4.594377510040161e-05,
      "loss": 0.3034,
      "step": 964
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.05392027646303177,
      "learning_rate": 4.578313253012048e-05,
      "loss": 0.2942,
      "step": 965
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.07095538079738617,
      "learning_rate": 4.562248995983936e-05,
      "loss": 0.3216,
      "step": 966
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.07403124868869781,
      "learning_rate": 4.546184738955824e-05,
      "loss": 0.3534,
      "step": 967
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.10078319162130356,
      "learning_rate": 4.530120481927711e-05,
      "loss": 0.4105,
      "step": 968
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.06834717094898224,
      "learning_rate": 4.514056224899599e-05,
      "loss": 0.3532,
      "step": 969
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.06663398444652557,
      "learning_rate": 4.497991967871486e-05,
      "loss": 0.3742,
      "step": 970
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.061885908246040344,
      "learning_rate": 4.481927710843374e-05,
      "loss": 0.3564,
      "step": 971
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.07040361315011978,
      "learning_rate": 4.465863453815261e-05,
      "loss": 0.3663,
      "step": 972
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.06400886178016663,
      "learning_rate": 4.449799196787149e-05,
      "loss": 0.3203,
      "step": 973
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.09045376628637314,
      "learning_rate": 4.433734939759036e-05,
      "loss": 0.3546,
      "step": 974
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.07340817898511887,
      "learning_rate": 4.417670682730924e-05,
      "loss": 0.3551,
      "step": 975
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.07468769699335098,
      "learning_rate": 4.401606425702812e-05,
      "loss": 0.3794,
      "step": 976
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.07087695598602295,
      "learning_rate": 4.385542168674699e-05,
      "loss": 0.3744,
      "step": 977
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.06122931092977524,
      "learning_rate": 4.369477911646587e-05,
      "loss": 0.3409,
      "step": 978
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.08882322162389755,
      "learning_rate": 4.353413654618474e-05,
      "loss": 0.4122,
      "step": 979
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.06084328144788742,
      "learning_rate": 4.337349397590362e-05,
      "loss": 0.3169,
      "step": 980
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.06039370968937874,
      "learning_rate": 4.321285140562249e-05,
      "loss": 0.3358,
      "step": 981
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.058218322694301605,
      "learning_rate": 4.305220883534137e-05,
      "loss": 0.296,
      "step": 982
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.04372991994023323,
      "learning_rate": 4.2891566265060246e-05,
      "loss": 0.2244,
      "step": 983
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.04712040722370148,
      "learning_rate": 4.273092369477912e-05,
      "loss": 0.2649,
      "step": 984
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.07850801944732666,
      "learning_rate": 4.2570281124497996e-05,
      "loss": 0.3852,
      "step": 985
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.07005621492862701,
      "learning_rate": 4.240963855421687e-05,
      "loss": 0.359,
      "step": 986
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.05998857691884041,
      "learning_rate": 4.2248995983935746e-05,
      "loss": 0.3494,
      "step": 987
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.08518651127815247,
      "learning_rate": 4.208835341365462e-05,
      "loss": 0.402,
      "step": 988
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.04897886514663696,
      "learning_rate": 4.1927710843373496e-05,
      "loss": 0.309,
      "step": 989
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.05251602083444595,
      "learning_rate": 4.176706827309237e-05,
      "loss": 0.2413,
      "step": 990
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.08505392074584961,
      "learning_rate": 4.1606425702811246e-05,
      "loss": 0.4225,
      "step": 991
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.06644245982170105,
      "learning_rate": 4.1445783132530125e-05,
      "loss": 0.3368,
      "step": 992
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.0680888444185257,
      "learning_rate": 4.1285140562248996e-05,
      "loss": 0.3743,
      "step": 993
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.09354901313781738,
      "learning_rate": 4.1124497991967875e-05,
      "loss": 0.3844,
      "step": 994
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.05107380077242851,
      "learning_rate": 4.0963855421686746e-05,
      "loss": 0.2765,
      "step": 995
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.07434029132127762,
      "learning_rate": 4.0803212851405625e-05,
      "loss": 0.3946,
      "step": 996
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.05985929071903229,
      "learning_rate": 4.0642570281124496e-05,
      "loss": 0.3593,
      "step": 997
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.07385115325450897,
      "learning_rate": 4.0481927710843375e-05,
      "loss": 0.3106,
      "step": 998
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.06520047038793564,
      "learning_rate": 4.0321285140562246e-05,
      "loss": 0.3321,
      "step": 999
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.07027820497751236,
      "learning_rate": 4.0160642570281125e-05,
      "loss": 0.346,
      "step": 1000
    },
    {
      "epoch": 4.004,
      "grad_norm": 0.07608012855052948,
      "learning_rate": 4e-05,
      "loss": 0.3995,
      "step": 1001
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.09271929413080215,
      "learning_rate": 3.9839357429718875e-05,
      "loss": 0.3757,
      "step": 1002
    },
    {
      "epoch": 4.012,
      "grad_norm": 0.06731030344963074,
      "learning_rate": 3.967871485943775e-05,
      "loss": 0.3617,
      "step": 1003
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.07755541056394577,
      "learning_rate": 3.9518072289156625e-05,
      "loss": 0.3639,
      "step": 1004
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.0803503468632698,
      "learning_rate": 3.93574297188755e-05,
      "loss": 0.3551,
      "step": 1005
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.09401915222406387,
      "learning_rate": 3.9196787148594375e-05,
      "loss": 0.3912,
      "step": 1006
    },
    {
      "epoch": 4.028,
      "grad_norm": 0.076294906437397,
      "learning_rate": 3.903614457831325e-05,
      "loss": 0.3268,
      "step": 1007
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.09266646951436996,
      "learning_rate": 3.887550200803213e-05,
      "loss": 0.3826,
      "step": 1008
    },
    {
      "epoch": 4.036,
      "grad_norm": 0.08985619992017746,
      "learning_rate": 3.8714859437751e-05,
      "loss": 0.3745,
      "step": 1009
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.07860277593135834,
      "learning_rate": 3.855421686746988e-05,
      "loss": 0.3679,
      "step": 1010
    },
    {
      "epoch": 4.044,
      "grad_norm": 0.061350565403699875,
      "learning_rate": 3.8393574297188753e-05,
      "loss": 0.3255,
      "step": 1011
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.06638193875551224,
      "learning_rate": 3.823293172690763e-05,
      "loss": 0.3643,
      "step": 1012
    },
    {
      "epoch": 4.052,
      "grad_norm": 0.06802335381507874,
      "learning_rate": 3.8072289156626503e-05,
      "loss": 0.362,
      "step": 1013
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.05811541900038719,
      "learning_rate": 3.791164658634538e-05,
      "loss": 0.325,
      "step": 1014
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.06647974252700806,
      "learning_rate": 3.7751004016064253e-05,
      "loss": 0.3142,
      "step": 1015
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.0454450286924839,
      "learning_rate": 3.759036144578313e-05,
      "loss": 0.253,
      "step": 1016
    },
    {
      "epoch": 4.068,
      "grad_norm": 0.044951386749744415,
      "learning_rate": 3.742971887550201e-05,
      "loss": 0.2744,
      "step": 1017
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.05554918572306633,
      "learning_rate": 3.726907630522088e-05,
      "loss": 0.2957,
      "step": 1018
    },
    {
      "epoch": 4.076,
      "grad_norm": 0.06081872433423996,
      "learning_rate": 3.710843373493976e-05,
      "loss": 0.3066,
      "step": 1019
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.07556692510843277,
      "learning_rate": 3.694779116465863e-05,
      "loss": 0.3527,
      "step": 1020
    },
    {
      "epoch": 4.084,
      "grad_norm": 0.07004257291555405,
      "learning_rate": 3.678714859437751e-05,
      "loss": 0.295,
      "step": 1021
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.07028688490390778,
      "learning_rate": 3.662650602409639e-05,
      "loss": 0.3389,
      "step": 1022
    },
    {
      "epoch": 4.092,
      "grad_norm": 0.09687897562980652,
      "learning_rate": 3.646586345381526e-05,
      "loss": 0.3922,
      "step": 1023
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.08196250349283218,
      "learning_rate": 3.630522088353414e-05,
      "loss": 0.3616,
      "step": 1024
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.07593652606010437,
      "learning_rate": 3.614457831325301e-05,
      "loss": 0.3437,
      "step": 1025
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.05524716526269913,
      "learning_rate": 3.598393574297189e-05,
      "loss": 0.2819,
      "step": 1026
    },
    {
      "epoch": 4.108,
      "grad_norm": 0.07505825906991959,
      "learning_rate": 3.582329317269077e-05,
      "loss": 0.3744,
      "step": 1027
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.07515105605125427,
      "learning_rate": 3.566265060240964e-05,
      "loss": 0.3306,
      "step": 1028
    },
    {
      "epoch": 4.116,
      "grad_norm": 0.06460859626531601,
      "learning_rate": 3.550200803212852e-05,
      "loss": 0.3213,
      "step": 1029
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.07369744032621384,
      "learning_rate": 3.534136546184739e-05,
      "loss": 0.355,
      "step": 1030
    },
    {
      "epoch": 4.124,
      "grad_norm": 0.07153717428445816,
      "learning_rate": 3.518072289156627e-05,
      "loss": 0.3755,
      "step": 1031
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.05942421779036522,
      "learning_rate": 3.5020080321285146e-05,
      "loss": 0.2788,
      "step": 1032
    },
    {
      "epoch": 4.132,
      "grad_norm": 0.07183287292718887,
      "learning_rate": 3.485943775100402e-05,
      "loss": 0.3018,
      "step": 1033
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.06851597875356674,
      "learning_rate": 3.4698795180722896e-05,
      "loss": 0.3098,
      "step": 1034
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.07925186306238174,
      "learning_rate": 3.4538152610441774e-05,
      "loss": 0.358,
      "step": 1035
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.07057134062051773,
      "learning_rate": 3.4377510040160646e-05,
      "loss": 0.3186,
      "step": 1036
    },
    {
      "epoch": 4.148,
      "grad_norm": 0.08185148984193802,
      "learning_rate": 3.4216867469879524e-05,
      "loss": 0.3633,
      "step": 1037
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.059664636850357056,
      "learning_rate": 3.4056224899598396e-05,
      "loss": 0.3004,
      "step": 1038
    },
    {
      "epoch": 4.156,
      "grad_norm": 0.07077894359827042,
      "learning_rate": 3.3895582329317274e-05,
      "loss": 0.3161,
      "step": 1039
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.07773805409669876,
      "learning_rate": 3.3734939759036146e-05,
      "loss": 0.3595,
      "step": 1040
    },
    {
      "epoch": 4.164,
      "grad_norm": 0.06219542771577835,
      "learning_rate": 3.3574297188755024e-05,
      "loss": 0.2517,
      "step": 1041
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.06854451447725296,
      "learning_rate": 3.34136546184739e-05,
      "loss": 0.3665,
      "step": 1042
    },
    {
      "epoch": 4.172,
      "grad_norm": 0.07815856486558914,
      "learning_rate": 3.3253012048192774e-05,
      "loss": 0.3699,
      "step": 1043
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.08612026274204254,
      "learning_rate": 3.309236947791165e-05,
      "loss": 0.3921,
      "step": 1044
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.06157894805073738,
      "learning_rate": 3.2931726907630524e-05,
      "loss": 0.2874,
      "step": 1045
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.05303103104233742,
      "learning_rate": 3.27710843373494e-05,
      "loss": 0.3009,
      "step": 1046
    },
    {
      "epoch": 4.188,
      "grad_norm": 0.08802121877670288,
      "learning_rate": 3.2610441767068274e-05,
      "loss": 0.3717,
      "step": 1047
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.05261576175689697,
      "learning_rate": 3.244979919678715e-05,
      "loss": 0.2911,
      "step": 1048
    },
    {
      "epoch": 4.196,
      "grad_norm": 0.058156538754701614,
      "learning_rate": 3.2289156626506024e-05,
      "loss": 0.2978,
      "step": 1049
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.0680796205997467,
      "learning_rate": 3.21285140562249e-05,
      "loss": 0.3416,
      "step": 1050
    },
    {
      "epoch": 4.204,
      "grad_norm": 0.05404488369822502,
      "learning_rate": 3.196787148594378e-05,
      "loss": 0.3028,
      "step": 1051
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.0628397986292839,
      "learning_rate": 3.180722891566265e-05,
      "loss": 0.3235,
      "step": 1052
    },
    {
      "epoch": 4.212,
      "grad_norm": 0.07324899733066559,
      "learning_rate": 3.164658634538153e-05,
      "loss": 0.3481,
      "step": 1053
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.07115763425827026,
      "learning_rate": 3.14859437751004e-05,
      "loss": 0.3938,
      "step": 1054
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.08001764863729477,
      "learning_rate": 3.132530120481928e-05,
      "loss": 0.4188,
      "step": 1055
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.0745464637875557,
      "learning_rate": 3.116465863453815e-05,
      "loss": 0.3282,
      "step": 1056
    },
    {
      "epoch": 4.228,
      "grad_norm": 0.06785126030445099,
      "learning_rate": 3.100401606425703e-05,
      "loss": 0.3401,
      "step": 1057
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.05942755192518234,
      "learning_rate": 3.084337349397591e-05,
      "loss": 0.3204,
      "step": 1058
    },
    {
      "epoch": 4.236,
      "grad_norm": 0.062345586717128754,
      "learning_rate": 3.068273092369478e-05,
      "loss": 0.3225,
      "step": 1059
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.07828346639871597,
      "learning_rate": 3.052208835341366e-05,
      "loss": 0.3742,
      "step": 1060
    },
    {
      "epoch": 4.244,
      "grad_norm": 0.061647575348615646,
      "learning_rate": 3.036144578313253e-05,
      "loss": 0.2977,
      "step": 1061
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.07958120852708817,
      "learning_rate": 3.020080321285141e-05,
      "loss": 0.3628,
      "step": 1062
    },
    {
      "epoch": 4.252,
      "grad_norm": 0.05743737891316414,
      "learning_rate": 3.004016064257028e-05,
      "loss": 0.2756,
      "step": 1063
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.06096562370657921,
      "learning_rate": 2.987951807228916e-05,
      "loss": 0.3126,
      "step": 1064
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.07691416889429092,
      "learning_rate": 2.971887550200803e-05,
      "loss": 0.3481,
      "step": 1065
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.09276284277439117,
      "learning_rate": 2.955823293172691e-05,
      "loss": 0.3648,
      "step": 1066
    },
    {
      "epoch": 4.268,
      "grad_norm": 0.060458842664957047,
      "learning_rate": 2.9397590361445788e-05,
      "loss": 0.3323,
      "step": 1067
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.058832742273807526,
      "learning_rate": 2.923694779116466e-05,
      "loss": 0.2956,
      "step": 1068
    },
    {
      "epoch": 4.276,
      "grad_norm": 0.07718738168478012,
      "learning_rate": 2.9076305220883538e-05,
      "loss": 0.3725,
      "step": 1069
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.062084734439849854,
      "learning_rate": 2.891566265060241e-05,
      "loss": 0.3089,
      "step": 1070
    },
    {
      "epoch": 4.284,
      "grad_norm": 0.059228722006082535,
      "learning_rate": 2.8755020080321288e-05,
      "loss": 0.2766,
      "step": 1071
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.059782687574625015,
      "learning_rate": 2.859437751004016e-05,
      "loss": 0.3354,
      "step": 1072
    },
    {
      "epoch": 4.292,
      "grad_norm": 0.07271529734134674,
      "learning_rate": 2.8433734939759038e-05,
      "loss": 0.404,
      "step": 1073
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.07451731711626053,
      "learning_rate": 2.827309236947791e-05,
      "loss": 0.3307,
      "step": 1074
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.07738983631134033,
      "learning_rate": 2.8112449799196788e-05,
      "loss": 0.3619,
      "step": 1075
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.0777519941329956,
      "learning_rate": 2.7951807228915666e-05,
      "loss": 0.3455,
      "step": 1076
    },
    {
      "epoch": 4.308,
      "grad_norm": 0.06534682959318161,
      "learning_rate": 2.7791164658634538e-05,
      "loss": 0.3397,
      "step": 1077
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.08272064477205276,
      "learning_rate": 2.7630522088353417e-05,
      "loss": 0.3886,
      "step": 1078
    },
    {
      "epoch": 4.316,
      "grad_norm": 0.06622425466775894,
      "learning_rate": 2.7469879518072288e-05,
      "loss": 0.3559,
      "step": 1079
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.07086004316806793,
      "learning_rate": 2.7309236947791167e-05,
      "loss": 0.3409,
      "step": 1080
    },
    {
      "epoch": 4.324,
      "grad_norm": 0.06112837791442871,
      "learning_rate": 2.7148594377510038e-05,
      "loss": 0.2963,
      "step": 1081
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.05938316881656647,
      "learning_rate": 2.6987951807228917e-05,
      "loss": 0.3096,
      "step": 1082
    },
    {
      "epoch": 4.332,
      "grad_norm": 0.07500853389501572,
      "learning_rate": 2.6827309236947795e-05,
      "loss": 0.3438,
      "step": 1083
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.06414971500635147,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.2839,
      "step": 1084
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.06769344955682755,
      "learning_rate": 2.6506024096385545e-05,
      "loss": 0.369,
      "step": 1085
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.07170149683952332,
      "learning_rate": 2.6345381526104417e-05,
      "loss": 0.3438,
      "step": 1086
    },
    {
      "epoch": 4.348,
      "grad_norm": 0.06773057579994202,
      "learning_rate": 2.6184738955823295e-05,
      "loss": 0.322,
      "step": 1087
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.06340667605400085,
      "learning_rate": 2.6024096385542167e-05,
      "loss": 0.2827,
      "step": 1088
    },
    {
      "epoch": 4.356,
      "grad_norm": 0.078917495906353,
      "learning_rate": 2.5863453815261045e-05,
      "loss": 0.3314,
      "step": 1089
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.07365758717060089,
      "learning_rate": 2.570281124497992e-05,
      "loss": 0.3609,
      "step": 1090
    },
    {
      "epoch": 4.364,
      "grad_norm": 0.06851284950971603,
      "learning_rate": 2.5542168674698795e-05,
      "loss": 0.3455,
      "step": 1091
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.08298452943563461,
      "learning_rate": 2.5381526104417673e-05,
      "loss": 0.3272,
      "step": 1092
    },
    {
      "epoch": 4.372,
      "grad_norm": 0.06114572659134865,
      "learning_rate": 2.522088353413655e-05,
      "loss": 0.2883,
      "step": 1093
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.06693938374519348,
      "learning_rate": 2.5060240963855423e-05,
      "loss": 0.3443,
      "step": 1094
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.04520536586642265,
      "learning_rate": 2.48995983935743e-05,
      "loss": 0.2521,
      "step": 1095
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.06298142671585083,
      "learning_rate": 2.4738955823293174e-05,
      "loss": 0.3213,
      "step": 1096
    },
    {
      "epoch": 4.388,
      "grad_norm": 0.07504967600107193,
      "learning_rate": 2.4578313253012052e-05,
      "loss": 0.3691,
      "step": 1097
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.06023120880126953,
      "learning_rate": 2.4417670682730927e-05,
      "loss": 0.3118,
      "step": 1098
    },
    {
      "epoch": 4.396,
      "grad_norm": 0.07957451790571213,
      "learning_rate": 2.4257028112449802e-05,
      "loss": 0.3639,
      "step": 1099
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.07660965621471405,
      "learning_rate": 2.4096385542168677e-05,
      "loss": 0.3601,
      "step": 1100
    },
    {
      "epoch": 4.404,
      "grad_norm": 0.07941675186157227,
      "learning_rate": 2.3935742971887552e-05,
      "loss": 0.3517,
      "step": 1101
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.09232799708843231,
      "learning_rate": 2.3775100401606427e-05,
      "loss": 0.4396,
      "step": 1102
    },
    {
      "epoch": 4.412,
      "grad_norm": 0.062493324279785156,
      "learning_rate": 2.3614457831325302e-05,
      "loss": 0.3135,
      "step": 1103
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.06895635277032852,
      "learning_rate": 2.345381526104418e-05,
      "loss": 0.3193,
      "step": 1104
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.07838252931833267,
      "learning_rate": 2.3293172690763055e-05,
      "loss": 0.3639,
      "step": 1105
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.06699391454458237,
      "learning_rate": 2.313253012048193e-05,
      "loss": 0.2891,
      "step": 1106
    },
    {
      "epoch": 4.428,
      "grad_norm": 0.07849083840847015,
      "learning_rate": 2.2971887550200805e-05,
      "loss": 0.3429,
      "step": 1107
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.07914416491985321,
      "learning_rate": 2.281124497991968e-05,
      "loss": 0.3597,
      "step": 1108
    },
    {
      "epoch": 4.436,
      "grad_norm": 0.07062007486820221,
      "learning_rate": 2.2650602409638555e-05,
      "loss": 0.3002,
      "step": 1109
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.0621710941195488,
      "learning_rate": 2.248995983935743e-05,
      "loss": 0.3039,
      "step": 1110
    },
    {
      "epoch": 4.444,
      "grad_norm": 0.07352610677480698,
      "learning_rate": 2.2329317269076305e-05,
      "loss": 0.3642,
      "step": 1111
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.07409007102251053,
      "learning_rate": 2.216867469879518e-05,
      "loss": 0.3466,
      "step": 1112
    },
    {
      "epoch": 4.452,
      "grad_norm": 0.06355248391628265,
      "learning_rate": 2.200803212851406e-05,
      "loss": 0.3081,
      "step": 1113
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.052487265318632126,
      "learning_rate": 2.1847389558232934e-05,
      "loss": 0.2394,
      "step": 1114
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.0736808180809021,
      "learning_rate": 2.168674698795181e-05,
      "loss": 0.338,
      "step": 1115
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.07699672877788544,
      "learning_rate": 2.1526104417670684e-05,
      "loss": 0.3949,
      "step": 1116
    },
    {
      "epoch": 4.468,
      "grad_norm": 0.0603024959564209,
      "learning_rate": 2.136546184738956e-05,
      "loss": 0.2945,
      "step": 1117
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.06909388303756714,
      "learning_rate": 2.1204819277108434e-05,
      "loss": 0.3453,
      "step": 1118
    },
    {
      "epoch": 4.476,
      "grad_norm": 0.06960465013980865,
      "learning_rate": 2.104417670682731e-05,
      "loss": 0.3792,
      "step": 1119
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.06499943882226944,
      "learning_rate": 2.0883534136546184e-05,
      "loss": 0.2974,
      "step": 1120
    },
    {
      "epoch": 4.484,
      "grad_norm": 0.08350331336259842,
      "learning_rate": 2.0722891566265062e-05,
      "loss": 0.3473,
      "step": 1121
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.05762599781155586,
      "learning_rate": 2.0562248995983937e-05,
      "loss": 0.3206,
      "step": 1122
    },
    {
      "epoch": 4.492,
      "grad_norm": 0.06353278458118439,
      "learning_rate": 2.0401606425702812e-05,
      "loss": 0.3586,
      "step": 1123
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.068761445581913,
      "learning_rate": 2.0240963855421687e-05,
      "loss": 0.3256,
      "step": 1124
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.06051427125930786,
      "learning_rate": 2.0080321285140562e-05,
      "loss": 0.3162,
      "step": 1125
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.08290122449398041,
      "learning_rate": 1.9919678714859437e-05,
      "loss": 0.3776,
      "step": 1126
    },
    {
      "epoch": 4.508,
      "grad_norm": 0.06915472447872162,
      "learning_rate": 1.9759036144578312e-05,
      "loss": 0.306,
      "step": 1127
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.08660311996936798,
      "learning_rate": 1.9598393574297187e-05,
      "loss": 0.3909,
      "step": 1128
    },
    {
      "epoch": 4.516,
      "grad_norm": 0.06357161700725555,
      "learning_rate": 1.9437751004016066e-05,
      "loss": 0.3408,
      "step": 1129
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.10949745029211044,
      "learning_rate": 1.927710843373494e-05,
      "loss": 0.4444,
      "step": 1130
    },
    {
      "epoch": 4.524,
      "grad_norm": 0.08262456208467484,
      "learning_rate": 1.9116465863453816e-05,
      "loss": 0.3714,
      "step": 1131
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.06365198642015457,
      "learning_rate": 1.895582329317269e-05,
      "loss": 0.332,
      "step": 1132
    },
    {
      "epoch": 4.532,
      "grad_norm": 0.06634843349456787,
      "learning_rate": 1.8795180722891566e-05,
      "loss": 0.3115,
      "step": 1133
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.07054116576910019,
      "learning_rate": 1.863453815261044e-05,
      "loss": 0.32,
      "step": 1134
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.07502342760562897,
      "learning_rate": 1.8473895582329316e-05,
      "loss": 0.3299,
      "step": 1135
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.06951558589935303,
      "learning_rate": 1.8313253012048194e-05,
      "loss": 0.3658,
      "step": 1136
    },
    {
      "epoch": 4.548,
      "grad_norm": 0.08559897541999817,
      "learning_rate": 1.815261044176707e-05,
      "loss": 0.3662,
      "step": 1137
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.08306363224983215,
      "learning_rate": 1.7991967871485944e-05,
      "loss": 0.3379,
      "step": 1138
    },
    {
      "epoch": 4.556,
      "grad_norm": 0.06848087161779404,
      "learning_rate": 1.783132530120482e-05,
      "loss": 0.37,
      "step": 1139
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.06949596107006073,
      "learning_rate": 1.7670682730923694e-05,
      "loss": 0.3299,
      "step": 1140
    },
    {
      "epoch": 4.564,
      "grad_norm": 0.0663071870803833,
      "learning_rate": 1.7510040160642573e-05,
      "loss": 0.3137,
      "step": 1141
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.062303606420755386,
      "learning_rate": 1.7349397590361448e-05,
      "loss": 0.2966,
      "step": 1142
    },
    {
      "epoch": 4.572,
      "grad_norm": 0.07519499957561493,
      "learning_rate": 1.7188755020080323e-05,
      "loss": 0.3387,
      "step": 1143
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.0764947310090065,
      "learning_rate": 1.7028112449799198e-05,
      "loss": 0.3584,
      "step": 1144
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.07488086819648743,
      "learning_rate": 1.6867469879518073e-05,
      "loss": 0.3316,
      "step": 1145
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.08305372297763824,
      "learning_rate": 1.670682730923695e-05,
      "loss": 0.3533,
      "step": 1146
    },
    {
      "epoch": 4.588,
      "grad_norm": 0.06676270067691803,
      "learning_rate": 1.6546184738955826e-05,
      "loss": 0.3374,
      "step": 1147
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.07240105420351028,
      "learning_rate": 1.63855421686747e-05,
      "loss": 0.3465,
      "step": 1148
    },
    {
      "epoch": 4.596,
      "grad_norm": 0.05313049629330635,
      "learning_rate": 1.6224899598393576e-05,
      "loss": 0.2915,
      "step": 1149
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.051914092153310776,
      "learning_rate": 1.606425702811245e-05,
      "loss": 0.2446,
      "step": 1150
    },
    {
      "epoch": 4.604,
      "grad_norm": 0.07292510569095612,
      "learning_rate": 1.5903614457831326e-05,
      "loss": 0.2984,
      "step": 1151
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.09178420901298523,
      "learning_rate": 1.57429718875502e-05,
      "loss": 0.368,
      "step": 1152
    },
    {
      "epoch": 4.612,
      "grad_norm": 0.07227212935686111,
      "learning_rate": 1.5582329317269076e-05,
      "loss": 0.3852,
      "step": 1153
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.07278136909008026,
      "learning_rate": 1.5421686746987955e-05,
      "loss": 0.3433,
      "step": 1154
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.06780106574296951,
      "learning_rate": 1.526104417670683e-05,
      "loss": 0.3078,
      "step": 1155
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.05912883207201958,
      "learning_rate": 1.5100401606425705e-05,
      "loss": 0.3155,
      "step": 1156
    },
    {
      "epoch": 4.628,
      "grad_norm": 0.07201611995697021,
      "learning_rate": 1.493975903614458e-05,
      "loss": 0.3289,
      "step": 1157
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.0634196326136589,
      "learning_rate": 1.4779116465863455e-05,
      "loss": 0.3225,
      "step": 1158
    },
    {
      "epoch": 4.636,
      "grad_norm": 0.06011909991502762,
      "learning_rate": 1.461847389558233e-05,
      "loss": 0.2753,
      "step": 1159
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.07146833837032318,
      "learning_rate": 1.4457831325301205e-05,
      "loss": 0.3195,
      "step": 1160
    },
    {
      "epoch": 4.644,
      "grad_norm": 0.0801546648144722,
      "learning_rate": 1.429718875502008e-05,
      "loss": 0.3194,
      "step": 1161
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.06589925289154053,
      "learning_rate": 1.4136546184738955e-05,
      "loss": 0.3468,
      "step": 1162
    },
    {
      "epoch": 4.652,
      "grad_norm": 0.08740968257188797,
      "learning_rate": 1.3975903614457833e-05,
      "loss": 0.3903,
      "step": 1163
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.06709902733564377,
      "learning_rate": 1.3815261044176708e-05,
      "loss": 0.3527,
      "step": 1164
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.0779719203710556,
      "learning_rate": 1.3654618473895583e-05,
      "loss": 0.3488,
      "step": 1165
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.054753657430410385,
      "learning_rate": 1.3493975903614458e-05,
      "loss": 0.2899,
      "step": 1166
    },
    {
      "epoch": 4.668,
      "grad_norm": 0.06346718221902847,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.3014,
      "step": 1167
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.06232336908578873,
      "learning_rate": 1.3172690763052208e-05,
      "loss": 0.3367,
      "step": 1168
    },
    {
      "epoch": 4.676,
      "grad_norm": 0.06083338335156441,
      "learning_rate": 1.3012048192771083e-05,
      "loss": 0.3336,
      "step": 1169
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.0630914643406868,
      "learning_rate": 1.285140562248996e-05,
      "loss": 0.289,
      "step": 1170
    },
    {
      "epoch": 4.684,
      "grad_norm": 0.06684181839227676,
      "learning_rate": 1.2690763052208837e-05,
      "loss": 0.3096,
      "step": 1171
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.06495542824268341,
      "learning_rate": 1.2530120481927712e-05,
      "loss": 0.2961,
      "step": 1172
    },
    {
      "epoch": 4.692,
      "grad_norm": 0.0715293139219284,
      "learning_rate": 1.2369477911646587e-05,
      "loss": 0.3564,
      "step": 1173
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.07694558799266815,
      "learning_rate": 1.2208835341365463e-05,
      "loss": 0.3742,
      "step": 1174
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.05711516737937927,
      "learning_rate": 1.2048192771084338e-05,
      "loss": 0.2713,
      "step": 1175
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.07788804918527603,
      "learning_rate": 1.1887550200803213e-05,
      "loss": 0.3378,
      "step": 1176
    },
    {
      "epoch": 4.708,
      "grad_norm": 0.06384312361478806,
      "learning_rate": 1.172690763052209e-05,
      "loss": 0.3329,
      "step": 1177
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.06816587597131729,
      "learning_rate": 1.1566265060240965e-05,
      "loss": 0.3285,
      "step": 1178
    },
    {
      "epoch": 4.716,
      "grad_norm": 0.08180928230285645,
      "learning_rate": 1.140562248995984e-05,
      "loss": 0.3688,
      "step": 1179
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.07392611354589462,
      "learning_rate": 1.1244979919678715e-05,
      "loss": 0.3384,
      "step": 1180
    },
    {
      "epoch": 4.724,
      "grad_norm": 0.06477408856153488,
      "learning_rate": 1.108433734939759e-05,
      "loss": 0.2963,
      "step": 1181
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.07954934984445572,
      "learning_rate": 1.0923694779116467e-05,
      "loss": 0.3634,
      "step": 1182
    },
    {
      "epoch": 4.732,
      "grad_norm": 0.07828480005264282,
      "learning_rate": 1.0763052208835342e-05,
      "loss": 0.3984,
      "step": 1183
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.068211130797863,
      "learning_rate": 1.0602409638554217e-05,
      "loss": 0.3401,
      "step": 1184
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.05466817319393158,
      "learning_rate": 1.0441767068273092e-05,
      "loss": 0.2802,
      "step": 1185
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.06301664561033249,
      "learning_rate": 1.0281124497991969e-05,
      "loss": 0.271,
      "step": 1186
    },
    {
      "epoch": 4.748,
      "grad_norm": 0.07355868816375732,
      "learning_rate": 1.0120481927710844e-05,
      "loss": 0.3322,
      "step": 1187
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.07548423856496811,
      "learning_rate": 9.959839357429719e-06,
      "loss": 0.3278,
      "step": 1188
    },
    {
      "epoch": 4.756,
      "grad_norm": 0.05453181266784668,
      "learning_rate": 9.799196787148594e-06,
      "loss": 0.2555,
      "step": 1189
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.05887797847390175,
      "learning_rate": 9.63855421686747e-06,
      "loss": 0.284,
      "step": 1190
    },
    {
      "epoch": 4.764,
      "grad_norm": 0.060243356972932816,
      "learning_rate": 9.477911646586345e-06,
      "loss": 0.2906,
      "step": 1191
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.07042047381401062,
      "learning_rate": 9.31726907630522e-06,
      "loss": 0.3148,
      "step": 1192
    },
    {
      "epoch": 4.772,
      "grad_norm": 0.06338249146938324,
      "learning_rate": 9.156626506024097e-06,
      "loss": 0.3228,
      "step": 1193
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.05677317827939987,
      "learning_rate": 8.995983935742972e-06,
      "loss": 0.2711,
      "step": 1194
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.06340236961841583,
      "learning_rate": 8.835341365461847e-06,
      "loss": 0.3599,
      "step": 1195
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.06619885563850403,
      "learning_rate": 8.674698795180724e-06,
      "loss": 0.3157,
      "step": 1196
    },
    {
      "epoch": 4.788,
      "grad_norm": 0.056730203330516815,
      "learning_rate": 8.514056224899599e-06,
      "loss": 0.2804,
      "step": 1197
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.07598240673542023,
      "learning_rate": 8.353413654618476e-06,
      "loss": 0.3347,
      "step": 1198
    },
    {
      "epoch": 4.796,
      "grad_norm": 0.05751001089811325,
      "learning_rate": 8.19277108433735e-06,
      "loss": 0.2874,
      "step": 1199
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.06600160896778107,
      "learning_rate": 8.032128514056226e-06,
      "loss": 0.2985,
      "step": 1200
    },
    {
      "epoch": 4.804,
      "grad_norm": 0.06841401755809784,
      "learning_rate": 7.8714859437751e-06,
      "loss": 0.3314,
      "step": 1201
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.06334950774908066,
      "learning_rate": 7.710843373493977e-06,
      "loss": 0.3266,
      "step": 1202
    },
    {
      "epoch": 4.812,
      "grad_norm": 0.06221854314208031,
      "learning_rate": 7.550200803212852e-06,
      "loss": 0.2772,
      "step": 1203
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.07020041346549988,
      "learning_rate": 7.389558232931727e-06,
      "loss": 0.3386,
      "step": 1204
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.060706641525030136,
      "learning_rate": 7.228915662650602e-06,
      "loss": 0.3372,
      "step": 1205
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.07390038669109344,
      "learning_rate": 7.068273092369477e-06,
      "loss": 0.3415,
      "step": 1206
    },
    {
      "epoch": 4.828,
      "grad_norm": 0.061017319560050964,
      "learning_rate": 6.907630522088354e-06,
      "loss": 0.3279,
      "step": 1207
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.06723271310329437,
      "learning_rate": 6.746987951807229e-06,
      "loss": 0.3353,
      "step": 1208
    },
    {
      "epoch": 4.836,
      "grad_norm": 0.059503521770238876,
      "learning_rate": 6.586345381526104e-06,
      "loss": 0.2979,
      "step": 1209
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.06834717094898224,
      "learning_rate": 6.42570281124498e-06,
      "loss": 0.3736,
      "step": 1210
    },
    {
      "epoch": 4.844,
      "grad_norm": 0.05755678191781044,
      "learning_rate": 6.265060240963856e-06,
      "loss": 0.3269,
      "step": 1211
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.05759752169251442,
      "learning_rate": 6.104417670682732e-06,
      "loss": 0.2951,
      "step": 1212
    },
    {
      "epoch": 4.852,
      "grad_norm": 0.07001538574695587,
      "learning_rate": 5.943775100401607e-06,
      "loss": 0.306,
      "step": 1213
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.05887836590409279,
      "learning_rate": 5.783132530120483e-06,
      "loss": 0.2829,
      "step": 1214
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.07959688454866409,
      "learning_rate": 5.622489959839358e-06,
      "loss": 0.3763,
      "step": 1215
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.07755152881145477,
      "learning_rate": 5.4618473895582335e-06,
      "loss": 0.3799,
      "step": 1216
    },
    {
      "epoch": 4.868,
      "grad_norm": 0.07103525847196579,
      "learning_rate": 5.3012048192771085e-06,
      "loss": 0.3013,
      "step": 1217
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.058027081191539764,
      "learning_rate": 5.140562248995984e-06,
      "loss": 0.3224,
      "step": 1218
    },
    {
      "epoch": 4.876,
      "grad_norm": 0.08914875984191895,
      "learning_rate": 4.979919678714859e-06,
      "loss": 0.391,
      "step": 1219
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.0716656818985939,
      "learning_rate": 4.819277108433735e-06,
      "loss": 0.3594,
      "step": 1220
    },
    {
      "epoch": 4.884,
      "grad_norm": 0.07463620603084564,
      "learning_rate": 4.65863453815261e-06,
      "loss": 0.3395,
      "step": 1221
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.062320590019226074,
      "learning_rate": 4.497991967871486e-06,
      "loss": 0.3472,
      "step": 1222
    },
    {
      "epoch": 4.892,
      "grad_norm": 0.0568612664937973,
      "learning_rate": 4.337349397590362e-06,
      "loss": 0.3059,
      "step": 1223
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.047476015985012054,
      "learning_rate": 4.176706827309238e-06,
      "loss": 0.2608,
      "step": 1224
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.06592810899019241,
      "learning_rate": 4.016064257028113e-06,
      "loss": 0.3107,
      "step": 1225
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.08105463534593582,
      "learning_rate": 3.855421686746989e-06,
      "loss": 0.4104,
      "step": 1226
    },
    {
      "epoch": 4.908,
      "grad_norm": 0.06516280025243759,
      "learning_rate": 3.6947791164658637e-06,
      "loss": 0.2717,
      "step": 1227
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.07825153321027756,
      "learning_rate": 3.5341365461847387e-06,
      "loss": 0.3673,
      "step": 1228
    },
    {
      "epoch": 4.916,
      "grad_norm": 0.06939127296209335,
      "learning_rate": 3.3734939759036146e-06,
      "loss": 0.3436,
      "step": 1229
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.06955701857805252,
      "learning_rate": 3.21285140562249e-06,
      "loss": 0.3014,
      "step": 1230
    },
    {
      "epoch": 4.924,
      "grad_norm": 0.07513226568698883,
      "learning_rate": 3.052208835341366e-06,
      "loss": 0.3245,
      "step": 1231
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.07424689829349518,
      "learning_rate": 2.8915662650602413e-06,
      "loss": 0.3273,
      "step": 1232
    },
    {
      "epoch": 4.932,
      "grad_norm": 0.08061236888170242,
      "learning_rate": 2.7309236947791167e-06,
      "loss": 0.3679,
      "step": 1233
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.0634324699640274,
      "learning_rate": 2.570281124497992e-06,
      "loss": 0.3029,
      "step": 1234
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.08879425376653671,
      "learning_rate": 2.4096385542168676e-06,
      "loss": 0.3846,
      "step": 1235
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.05772038549184799,
      "learning_rate": 2.248995983935743e-06,
      "loss": 0.2733,
      "step": 1236
    },
    {
      "epoch": 4.948,
      "grad_norm": 0.06587055325508118,
      "learning_rate": 2.088353413654619e-06,
      "loss": 0.3065,
      "step": 1237
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.07301998138427734,
      "learning_rate": 1.9277108433734943e-06,
      "loss": 0.2983,
      "step": 1238
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 0.06438156217336655,
      "learning_rate": 1.7670682730923694e-06,
      "loss": 0.331,
      "step": 1239
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.07870415598154068,
      "learning_rate": 1.606425702811245e-06,
      "loss": 0.3419,
      "step": 1240
    },
    {
      "epoch": 4.964,
      "grad_norm": 0.05877768248319626,
      "learning_rate": 1.4457831325301207e-06,
      "loss": 0.267,
      "step": 1241
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.06263543665409088,
      "learning_rate": 1.285140562248996e-06,
      "loss": 0.3239,
      "step": 1242
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 0.07143975794315338,
      "learning_rate": 1.1244979919678715e-06,
      "loss": 0.3783,
      "step": 1243
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.06552960723638535,
      "learning_rate": 9.638554216867472e-07,
      "loss": 0.3501,
      "step": 1244
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.06626873463392258,
      "learning_rate": 8.032128514056225e-07,
      "loss": 0.3605,
      "step": 1245
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.07259176671504974,
      "learning_rate": 6.42570281124498e-07,
      "loss": 0.3387,
      "step": 1246
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 0.07060140371322632,
      "learning_rate": 4.819277108433736e-07,
      "loss": 0.3804,
      "step": 1247
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.08749932795763016,
      "learning_rate": 3.21285140562249e-07,
      "loss": 0.3783,
      "step": 1248
    },
    {
      "epoch": 4.996,
      "grad_norm": 0.06887216120958328,
      "learning_rate": 1.606425702811245e-07,
      "loss": 0.3782,
      "step": 1249
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.0690314769744873,
      "learning_rate": 0.0,
      "loss": 0.3304,
      "step": 1250
    },
    {
      "epoch": 5.004,
      "grad_norm": 0.10286430269479752,
      "learning_rate": 6.185899362790339e-05,
      "loss": 0.4311,
      "step": 1251
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.09482920914888382,
      "learning_rate": 6.171347170830062e-05,
      "loss": 0.3872,
      "step": 1252
    },
    {
      "epoch": 5.012,
      "grad_norm": 0.08839413523674011,
      "learning_rate": 6.156804473101851e-05,
      "loss": 0.3787,
      "step": 1253
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.08035352826118469,
      "learning_rate": 6.142271305668463e-05,
      "loss": 0.3324,
      "step": 1254
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.09486547112464905,
      "learning_rate": 6.127747704569016e-05,
      "loss": 0.3691,
      "step": 1255
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.08288881182670593,
      "learning_rate": 6.113233705818897e-05,
      "loss": 0.3847,
      "step": 1256
    },
    {
      "epoch": 5.028,
      "grad_norm": 0.06331303715705872,
      "learning_rate": 6.098729345409693e-05,
      "loss": 0.3122,
      "step": 1257
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.0706353634595871,
      "learning_rate": 6.084234659309088e-05,
      "loss": 0.3646,
      "step": 1258
    },
    {
      "epoch": 5.036,
      "grad_norm": 0.08754336088895798,
      "learning_rate": 6.069749683460765e-05,
      "loss": 0.385,
      "step": 1259
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.06988023221492767,
      "learning_rate": 6.0552744537843385e-05,
      "loss": 0.3525,
      "step": 1260
    },
    {
      "epoch": 5.044,
      "grad_norm": 0.0716741532087326,
      "learning_rate": 6.040809006175253e-05,
      "loss": 0.3488,
      "step": 1261
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.06429117918014526,
      "learning_rate": 6.0263533765046975e-05,
      "loss": 0.3057,
      "step": 1262
    },
    {
      "epoch": 5.052,
      "grad_norm": 0.07288918644189835,
      "learning_rate": 6.011907600619504e-05,
      "loss": 0.3872,
      "step": 1263
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.07025146484375,
      "learning_rate": 5.997471714342079e-05,
      "loss": 0.3592,
      "step": 1264
    },
    {
      "epoch": 5.06,
      "grad_norm": 0.07117949426174164,
      "learning_rate": 5.983045753470308e-05,
      "loss": 0.3353,
      "step": 1265
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.0906367152929306,
      "learning_rate": 5.968629753777449e-05,
      "loss": 0.3529,
      "step": 1266
    },
    {
      "epoch": 5.068,
      "grad_norm": 0.07299376279115677,
      "learning_rate": 5.9542237510120736e-05,
      "loss": 0.3624,
      "step": 1267
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.07763403654098511,
      "learning_rate": 5.93982778089796e-05,
      "loss": 0.3242,
      "step": 1268
    },
    {
      "epoch": 5.076,
      "grad_norm": 0.06660070270299911,
      "learning_rate": 5.925441879133995e-05,
      "loss": 0.2862,
      "step": 1269
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.12569665908813477,
      "learning_rate": 5.911066081394113e-05,
      "loss": 0.4394,
      "step": 1270
    },
    {
      "epoch": 5.084,
      "grad_norm": 0.08243861049413681,
      "learning_rate": 5.896700423327188e-05,
      "loss": 0.3637,
      "step": 1271
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.07917941361665726,
      "learning_rate": 5.8823449405569516e-05,
      "loss": 0.4066,
      "step": 1272
    },
    {
      "epoch": 5.092,
      "grad_norm": 0.05881417170166969,
      "learning_rate": 5.867999668681896e-05,
      "loss": 0.3003,
      "step": 1273
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.08146513253450394,
      "learning_rate": 5.8536646432751996e-05,
      "loss": 0.3531,
      "step": 1274
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.12974731624126434,
      "learning_rate": 5.839339899884628e-05,
      "loss": 0.383,
      "step": 1275
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.07230187207460403,
      "learning_rate": 5.8250254740324503e-05,
      "loss": 0.3858,
      "step": 1276
    },
    {
      "epoch": 5.108,
      "grad_norm": 0.09172128140926361,
      "learning_rate": 5.810721401215353e-05,
      "loss": 0.4027,
      "step": 1277
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.0661420151591301,
      "learning_rate": 5.796427716904347e-05,
      "loss": 0.3584,
      "step": 1278
    },
    {
      "epoch": 5.116,
      "grad_norm": 0.06597074866294861,
      "learning_rate": 5.78214445654468e-05,
      "loss": 0.3095,
      "step": 1279
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.07421502470970154,
      "learning_rate": 5.767871655555751e-05,
      "loss": 0.3453,
      "step": 1280
    },
    {
      "epoch": 5.124,
      "grad_norm": 0.07372186332941055,
      "learning_rate": 5.7536093493310295e-05,
      "loss": 0.3776,
      "step": 1281
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.08331416547298431,
      "learning_rate": 5.739357573237951e-05,
      "loss": 0.3621,
      "step": 1282
    },
    {
      "epoch": 5.132,
      "grad_norm": 0.06860313564538956,
      "learning_rate": 5.7251163626178394e-05,
      "loss": 0.3818,
      "step": 1283
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.05275067687034607,
      "learning_rate": 5.7108857527858275e-05,
      "loss": 0.2562,
      "step": 1284
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.07705974578857422,
      "learning_rate": 5.69666577903075e-05,
      "loss": 0.3831,
      "step": 1285
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.08097223192453384,
      "learning_rate": 5.6824564766150726e-05,
      "loss": 0.4262,
      "step": 1286
    },
    {
      "epoch": 5.148,
      "grad_norm": 0.0675705075263977,
      "learning_rate": 5.6682578807747896e-05,
      "loss": 0.4022,
      "step": 1287
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.06342275440692902,
      "learning_rate": 5.654070026719365e-05,
      "loss": 0.2758,
      "step": 1288
    },
    {
      "epoch": 5.156,
      "grad_norm": 0.0717979222536087,
      "learning_rate": 5.6398929496315966e-05,
      "loss": 0.3518,
      "step": 1289
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.06832722574472427,
      "learning_rate": 5.625726684667586e-05,
      "loss": 0.4021,
      "step": 1290
    },
    {
      "epoch": 5.164,
      "grad_norm": 0.05331718176603317,
      "learning_rate": 5.6115712669566014e-05,
      "loss": 0.2937,
      "step": 1291
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.05233843997120857,
      "learning_rate": 5.597426731601034e-05,
      "loss": 0.2747,
      "step": 1292
    },
    {
      "epoch": 5.172,
      "grad_norm": 0.08103061467409134,
      "learning_rate": 5.58329311367626e-05,
      "loss": 0.4257,
      "step": 1293
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.07886604219675064,
      "learning_rate": 5.569170448230613e-05,
      "loss": 0.3662,
      "step": 1294
    },
    {
      "epoch": 5.18,
      "grad_norm": 0.07676172256469727,
      "learning_rate": 5.5550587702852463e-05,
      "loss": 0.2947,
      "step": 1295
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.06428788602352142,
      "learning_rate": 5.540958114834074e-05,
      "loss": 0.3334,
      "step": 1296
    },
    {
      "epoch": 5.188,
      "grad_norm": 0.06946373730897903,
      "learning_rate": 5.526868516843673e-05,
      "loss": 0.3707,
      "step": 1297
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.0651889219880104,
      "learning_rate": 5.5127900112532106e-05,
      "loss": 0.3485,
      "step": 1298
    },
    {
      "epoch": 5.196,
      "grad_norm": 0.07478291541337967,
      "learning_rate": 5.498722632974336e-05,
      "loss": 0.334,
      "step": 1299
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.08829541504383087,
      "learning_rate": 5.484666416891109e-05,
      "loss": 0.4081,
      "step": 1300
    },
    {
      "epoch": 5.204,
      "grad_norm": 0.06271594017744064,
      "learning_rate": 5.4706213978599055e-05,
      "loss": 0.3114,
      "step": 1301
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.07065393775701523,
      "learning_rate": 5.456587610709345e-05,
      "loss": 0.3301,
      "step": 1302
    },
    {
      "epoch": 5.212,
      "grad_norm": 0.09169621765613556,
      "learning_rate": 5.4425650902401885e-05,
      "loss": 0.3806,
      "step": 1303
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.06823237240314484,
      "learning_rate": 5.4285538712252514e-05,
      "loss": 0.3874,
      "step": 1304
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.07490356266498566,
      "learning_rate": 5.4145539884093436e-05,
      "loss": 0.3755,
      "step": 1305
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.06253264844417572,
      "learning_rate": 5.4005654765091344e-05,
      "loss": 0.3445,
      "step": 1306
    },
    {
      "epoch": 5.228,
      "grad_norm": 0.07971736043691635,
      "learning_rate": 5.386588370213124e-05,
      "loss": 0.3424,
      "step": 1307
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.07296908646821976,
      "learning_rate": 5.372622704181511e-05,
      "loss": 0.3367,
      "step": 1308
    },
    {
      "epoch": 5.236,
      "grad_norm": 0.0770367681980133,
      "learning_rate": 5.358668513046135e-05,
      "loss": 0.3935,
      "step": 1309
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.06112075597047806,
      "learning_rate": 5.344725831410369e-05,
      "loss": 0.3365,
      "step": 1310
    },
    {
      "epoch": 5.244,
      "grad_norm": 0.048552192747592926,
      "learning_rate": 5.3307946938490595e-05,
      "loss": 0.2664,
      "step": 1311
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.06470392644405365,
      "learning_rate": 5.3168751349084176e-05,
      "loss": 0.3333,
      "step": 1312
    },
    {
      "epoch": 5.252,
      "grad_norm": 0.07307211309671402,
      "learning_rate": 5.302967189105941e-05,
      "loss": 0.338,
      "step": 1313
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.07799877226352692,
      "learning_rate": 5.289070890930328e-05,
      "loss": 0.3663,
      "step": 1314
    },
    {
      "epoch": 5.26,
      "grad_norm": 0.07361683249473572,
      "learning_rate": 5.275186274841404e-05,
      "loss": 0.3637,
      "step": 1315
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.062167126685380936,
      "learning_rate": 5.261313375270014e-05,
      "loss": 0.3369,
      "step": 1316
    },
    {
      "epoch": 5.268,
      "grad_norm": 0.07327660173177719,
      "learning_rate": 5.2474522266179535e-05,
      "loss": 0.3641,
      "step": 1317
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.07341920584440231,
      "learning_rate": 5.233602863257876e-05,
      "loss": 0.3842,
      "step": 1318
    },
    {
      "epoch": 5.276,
      "grad_norm": 0.07374273985624313,
      "learning_rate": 5.2197653195332094e-05,
      "loss": 0.4135,
      "step": 1319
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.05586450919508934,
      "learning_rate": 5.205939629758079e-05,
      "loss": 0.3417,
      "step": 1320
    },
    {
      "epoch": 5.284,
      "grad_norm": 0.06745682656764984,
      "learning_rate": 5.1921258282172024e-05,
      "loss": 0.3643,
      "step": 1321
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.07655279338359833,
      "learning_rate": 5.1783239491658366e-05,
      "loss": 0.2999,
      "step": 1322
    },
    {
      "epoch": 5.292,
      "grad_norm": 0.07438227534294128,
      "learning_rate": 5.164534026829643e-05,
      "loss": 0.4099,
      "step": 1323
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.05990713834762573,
      "learning_rate": 5.150756095404663e-05,
      "loss": 0.2369,
      "step": 1324
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.08332576602697372,
      "learning_rate": 5.136990189057187e-05,
      "loss": 0.3801,
      "step": 1325
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.07590457797050476,
      "learning_rate": 5.12323634192369e-05,
      "loss": 0.3551,
      "step": 1326
    },
    {
      "epoch": 5.308,
      "grad_norm": 0.06229591369628906,
      "learning_rate": 5.109494588110737e-05,
      "loss": 0.3071,
      "step": 1327
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.06844282895326614,
      "learning_rate": 5.095764961694922e-05,
      "loss": 0.3178,
      "step": 1328
    },
    {
      "epoch": 5.316,
      "grad_norm": 0.0647655725479126,
      "learning_rate": 5.082047496722737e-05,
      "loss": 0.3609,
      "step": 1329
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.06497456878423691,
      "learning_rate": 5.0683422272105455e-05,
      "loss": 0.3411,
      "step": 1330
    },
    {
      "epoch": 5.324,
      "grad_norm": 0.0708564966917038,
      "learning_rate": 5.0546491871444454e-05,
      "loss": 0.3726,
      "step": 1331
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.07480520755052567,
      "learning_rate": 5.0409684104802325e-05,
      "loss": 0.2927,
      "step": 1332
    },
    {
      "epoch": 5.332,
      "grad_norm": 0.07077789306640625,
      "learning_rate": 5.027299931143261e-05,
      "loss": 0.3661,
      "step": 1333
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.0698363184928894,
      "learning_rate": 5.013643783028419e-05,
      "loss": 0.3346,
      "step": 1334
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.07652999460697174,
      "learning_rate": 5.000000000000002e-05,
      "loss": 0.3511,
      "step": 1335
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.07182027399539948,
      "learning_rate": 4.98636861589164e-05,
      "loss": 0.3582,
      "step": 1336
    },
    {
      "epoch": 5.348,
      "grad_norm": 0.06475584208965302,
      "learning_rate": 4.972749664506229e-05,
      "loss": 0.3475,
      "step": 1337
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.056258078664541245,
      "learning_rate": 4.959143179615823e-05,
      "loss": 0.2956,
      "step": 1338
    },
    {
      "epoch": 5.356,
      "grad_norm": 0.052652232348918915,
      "learning_rate": 4.945549194961566e-05,
      "loss": 0.2348,
      "step": 1339
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.06400461494922638,
      "learning_rate": 4.931967744253601e-05,
      "loss": 0.3297,
      "step": 1340
    },
    {
      "epoch": 5.364,
      "grad_norm": 0.06967831403017044,
      "learning_rate": 4.918398861170997e-05,
      "loss": 0.3871,
      "step": 1341
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.07687326520681381,
      "learning_rate": 4.904842579361653e-05,
      "loss": 0.3731,
      "step": 1342
    },
    {
      "epoch": 5.372,
      "grad_norm": 0.09075138717889786,
      "learning_rate": 4.8912989324422164e-05,
      "loss": 0.4107,
      "step": 1343
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.07036430388689041,
      "learning_rate": 4.8777679539980056e-05,
      "loss": 0.3873,
      "step": 1344
    },
    {
      "epoch": 5.38,
      "grad_norm": 0.07135169953107834,
      "learning_rate": 4.864249677582935e-05,
      "loss": 0.3155,
      "step": 1345
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.07942651212215424,
      "learning_rate": 4.8507441367193954e-05,
      "loss": 0.3574,
      "step": 1346
    },
    {
      "epoch": 5.388,
      "grad_norm": 0.07965966314077377,
      "learning_rate": 4.837251364898221e-05,
      "loss": 0.3707,
      "step": 1347
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.08352436870336533,
      "learning_rate": 4.823771395578569e-05,
      "loss": 0.409,
      "step": 1348
    },
    {
      "epoch": 5.396,
      "grad_norm": 0.08098716288805008,
      "learning_rate": 4.810304262187852e-05,
      "loss": 0.3147,
      "step": 1349
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.07290949672460556,
      "learning_rate": 4.7968499981216475e-05,
      "loss": 0.3447,
      "step": 1350
    },
    {
      "epoch": 5.404,
      "grad_norm": 0.07721389830112457,
      "learning_rate": 4.7834086367436325e-05,
      "loss": 0.3865,
      "step": 1351
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.0753265917301178,
      "learning_rate": 4.7699802113854766e-05,
      "loss": 0.3539,
      "step": 1352
    },
    {
      "epoch": 5.412,
      "grad_norm": 0.07317423075437546,
      "learning_rate": 4.756564755346773e-05,
      "loss": 0.3786,
      "step": 1353
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.0685834139585495,
      "learning_rate": 4.743162301894952e-05,
      "loss": 0.3782,
      "step": 1354
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.07063393294811249,
      "learning_rate": 4.729772884265212e-05,
      "loss": 0.3253,
      "step": 1355
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.07078594714403152,
      "learning_rate": 4.7163965356604125e-05,
      "loss": 0.3707,
      "step": 1356
    },
    {
      "epoch": 5.428,
      "grad_norm": 0.05097377300262451,
      "learning_rate": 4.703033289251004e-05,
      "loss": 0.278,
      "step": 1357
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.054955169558525085,
      "learning_rate": 4.689683178174964e-05,
      "loss": 0.2889,
      "step": 1358
    },
    {
      "epoch": 5.436,
      "grad_norm": 0.06361274421215057,
      "learning_rate": 4.676346235537669e-05,
      "loss": 0.2738,
      "step": 1359
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.0684351846575737,
      "learning_rate": 4.663022494411866e-05,
      "loss": 0.3863,
      "step": 1360
    },
    {
      "epoch": 5.444,
      "grad_norm": 0.06626681238412857,
      "learning_rate": 4.6497119878375495e-05,
      "loss": 0.3467,
      "step": 1361
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.07839856296777725,
      "learning_rate": 4.6364147488219126e-05,
      "loss": 0.4122,
      "step": 1362
    },
    {
      "epoch": 5.452,
      "grad_norm": 0.05455663055181503,
      "learning_rate": 4.623130810339219e-05,
      "loss": 0.292,
      "step": 1363
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.06618224829435349,
      "learning_rate": 4.609860205330778e-05,
      "loss": 0.3051,
      "step": 1364
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.07473935931921005,
      "learning_rate": 4.596602966704823e-05,
      "loss": 0.4076,
      "step": 1365
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.07702738046646118,
      "learning_rate": 4.5833591273364396e-05,
      "loss": 0.3952,
      "step": 1366
    },
    {
      "epoch": 5.468,
      "grad_norm": 0.06555194407701492,
      "learning_rate": 4.570128720067487e-05,
      "loss": 0.3271,
      "step": 1367
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.06199859082698822,
      "learning_rate": 4.556911777706524e-05,
      "loss": 0.3568,
      "step": 1368
    },
    {
      "epoch": 5.476,
      "grad_norm": 0.09016108512878418,
      "learning_rate": 4.543708333028709e-05,
      "loss": 0.3872,
      "step": 1369
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.0670170709490776,
      "learning_rate": 4.530518418775733e-05,
      "loss": 0.3506,
      "step": 1370
    },
    {
      "epoch": 5.484,
      "grad_norm": 0.05801727622747421,
      "learning_rate": 4.517342067655732e-05,
      "loss": 0.2804,
      "step": 1371
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.0818270593881607,
      "learning_rate": 4.5041793123432176e-05,
      "loss": 0.386,
      "step": 1372
    },
    {
      "epoch": 5.492,
      "grad_norm": 0.07578010112047195,
      "learning_rate": 4.491030185478976e-05,
      "loss": 0.3415,
      "step": 1373
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.07953044772148132,
      "learning_rate": 4.4778947196699984e-05,
      "loss": 0.3727,
      "step": 1374
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.06297533214092255,
      "learning_rate": 4.4647729474894127e-05,
      "loss": 0.3775,
      "step": 1375
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.09137982875108719,
      "learning_rate": 4.451664901476367e-05,
      "loss": 0.4129,
      "step": 1376
    },
    {
      "epoch": 5.508,
      "grad_norm": 0.06647052615880966,
      "learning_rate": 4.438570614135994e-05,
      "loss": 0.388,
      "step": 1377
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.06445204466581345,
      "learning_rate": 4.425490117939295e-05,
      "loss": 0.3693,
      "step": 1378
    },
    {
      "epoch": 5.516,
      "grad_norm": 0.06347966194152832,
      "learning_rate": 4.412423445323075e-05,
      "loss": 0.3119,
      "step": 1379
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.07091045379638672,
      "learning_rate": 4.399370628689857e-05,
      "loss": 0.3377,
      "step": 1380
    },
    {
      "epoch": 5.524,
      "grad_norm": 0.0694519430398941,
      "learning_rate": 4.386331700407813e-05,
      "loss": 0.3534,
      "step": 1381
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.07070666551589966,
      "learning_rate": 4.373306692810666e-05,
      "loss": 0.334,
      "step": 1382
    },
    {
      "epoch": 5.532,
      "grad_norm": 0.049921631813049316,
      "learning_rate": 4.360295638197621e-05,
      "loss": 0.3047,
      "step": 1383
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.06781832873821259,
      "learning_rate": 4.3472985688332815e-05,
      "loss": 0.3385,
      "step": 1384
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.07965169101953506,
      "learning_rate": 4.33431551694758e-05,
      "loss": 0.4225,
      "step": 1385
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.06616738438606262,
      "learning_rate": 4.321346514735669e-05,
      "loss": 0.3523,
      "step": 1386
    },
    {
      "epoch": 5.548,
      "grad_norm": 0.06555170565843582,
      "learning_rate": 4.3083915943578823e-05,
      "loss": 0.379,
      "step": 1387
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.07192256301641464,
      "learning_rate": 4.295450787939622e-05,
      "loss": 0.3582,
      "step": 1388
    },
    {
      "epoch": 5.556,
      "grad_norm": 0.07419484108686447,
      "learning_rate": 4.2825241275712894e-05,
      "loss": 0.3098,
      "step": 1389
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.0682104155421257,
      "learning_rate": 4.269611645308215e-05,
      "loss": 0.3551,
      "step": 1390
    },
    {
      "epoch": 5.564,
      "grad_norm": 0.05566307529807091,
      "learning_rate": 4.256713373170564e-05,
      "loss": 0.346,
      "step": 1391
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.04573927819728851,
      "learning_rate": 4.2438293431432665e-05,
      "loss": 0.2627,
      "step": 1392
    },
    {
      "epoch": 5.572,
      "grad_norm": 0.059581123292446136,
      "learning_rate": 4.230959587175929e-05,
      "loss": 0.3017,
      "step": 1393
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.059864431619644165,
      "learning_rate": 4.2181041371827745e-05,
      "loss": 0.3082,
      "step": 1394
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.07570411264896393,
      "learning_rate": 4.2052630250425386e-05,
      "loss": 0.4135,
      "step": 1395
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.07503623515367508,
      "learning_rate": 4.1924362825984054e-05,
      "loss": 0.4028,
      "step": 1396
    },
    {
      "epoch": 5.588,
      "grad_norm": 0.0531986728310585,
      "learning_rate": 4.179623941657922e-05,
      "loss": 0.3395,
      "step": 1397
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.0705685019493103,
      "learning_rate": 4.1668260339929385e-05,
      "loss": 0.3975,
      "step": 1398
    },
    {
      "epoch": 5.596,
      "grad_norm": 0.06362846493721008,
      "learning_rate": 4.154042591339486e-05,
      "loss": 0.3665,
      "step": 1399
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.06491789221763611,
      "learning_rate": 4.141273645397754e-05,
      "loss": 0.3308,
      "step": 1400
    },
    {
      "epoch": 5.604,
      "grad_norm": 0.08867054432630539,
      "learning_rate": 4.128519227831962e-05,
      "loss": 0.4485,
      "step": 1401
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.07392854988574982,
      "learning_rate": 4.1157793702703204e-05,
      "loss": 0.2982,
      "step": 1402
    },
    {
      "epoch": 5.612,
      "grad_norm": 0.059145476669073105,
      "learning_rate": 4.103054104304912e-05,
      "loss": 0.3035,
      "step": 1403
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.06679452210664749,
      "learning_rate": 4.090343461491657e-05,
      "loss": 0.3414,
      "step": 1404
    },
    {
      "epoch": 5.62,
      "grad_norm": 0.055576175451278687,
      "learning_rate": 4.077647473350201e-05,
      "loss": 0.2712,
      "step": 1405
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.05578647553920746,
      "learning_rate": 4.0649661713638544e-05,
      "loss": 0.3099,
      "step": 1406
    },
    {
      "epoch": 5.628,
      "grad_norm": 0.06621626019477844,
      "learning_rate": 4.052299586979501e-05,
      "loss": 0.2789,
      "step": 1407
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.06541191041469574,
      "learning_rate": 4.0396477516075425e-05,
      "loss": 0.3566,
      "step": 1408
    },
    {
      "epoch": 5.636,
      "grad_norm": 0.08049258589744568,
      "learning_rate": 4.0270106966217946e-05,
      "loss": 0.4265,
      "step": 1409
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.056329164654016495,
      "learning_rate": 4.01438845335942e-05,
      "loss": 0.3275,
      "step": 1410
    },
    {
      "epoch": 5.644,
      "grad_norm": 0.06805781275033951,
      "learning_rate": 4.001781053120863e-05,
      "loss": 0.3788,
      "step": 1411
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.05706524848937988,
      "learning_rate": 3.9891885271697496e-05,
      "loss": 0.289,
      "step": 1412
    },
    {
      "epoch": 5.652,
      "grad_norm": 0.08890432119369507,
      "learning_rate": 3.976610906732825e-05,
      "loss": 0.3667,
      "step": 1413
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.06699011474847794,
      "learning_rate": 3.964048222999866e-05,
      "loss": 0.3863,
      "step": 1414
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.06419165432453156,
      "learning_rate": 3.9515005071236274e-05,
      "loss": 0.2954,
      "step": 1415
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.09363912791013718,
      "learning_rate": 3.93896779021972e-05,
      "loss": 0.3801,
      "step": 1416
    },
    {
      "epoch": 5.668,
      "grad_norm": 0.0723518431186676,
      "learning_rate": 3.9264501033665846e-05,
      "loss": 0.3586,
      "step": 1417
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.07713578641414642,
      "learning_rate": 3.913947477605378e-05,
      "loss": 0.3412,
      "step": 1418
    },
    {
      "epoch": 5.676,
      "grad_norm": 0.07737288624048233,
      "learning_rate": 3.90145994393991e-05,
      "loss": 0.3741,
      "step": 1419
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.058811068534851074,
      "learning_rate": 3.8889875333365666e-05,
      "loss": 0.3008,
      "step": 1420
    },
    {
      "epoch": 5.684,
      "grad_norm": 0.06416481733322144,
      "learning_rate": 3.876530276724236e-05,
      "loss": 0.3067,
      "step": 1421
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.06621170043945312,
      "learning_rate": 3.864088204994223e-05,
      "loss": 0.3334,
      "step": 1422
    },
    {
      "epoch": 5.692,
      "grad_norm": 0.057311590760946274,
      "learning_rate": 3.851661349000176e-05,
      "loss": 0.2866,
      "step": 1423
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.05528675764799118,
      "learning_rate": 3.839249739558013e-05,
      "loss": 0.2672,
      "step": 1424
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.0803154781460762,
      "learning_rate": 3.8268534074458486e-05,
      "loss": 0.4088,
      "step": 1425
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.06815856695175171,
      "learning_rate": 3.814472383403907e-05,
      "loss": 0.3839,
      "step": 1426
    },
    {
      "epoch": 5.708,
      "grad_norm": 0.05472904443740845,
      "learning_rate": 3.8021066981344496e-05,
      "loss": 0.297,
      "step": 1427
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.06851674616336823,
      "learning_rate": 3.789756382301718e-05,
      "loss": 0.3582,
      "step": 1428
    },
    {
      "epoch": 5.716,
      "grad_norm": 0.06472031772136688,
      "learning_rate": 3.7774214665318095e-05,
      "loss": 0.3562,
      "step": 1429
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.06826998293399811,
      "learning_rate": 3.7651019814126654e-05,
      "loss": 0.3512,
      "step": 1430
    },
    {
      "epoch": 5.724,
      "grad_norm": 0.053979162126779556,
      "learning_rate": 3.75279795749394e-05,
      "loss": 0.2749,
      "step": 1431
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.05856079235672951,
      "learning_rate": 3.7405094252869634e-05,
      "loss": 0.3027,
      "step": 1432
    },
    {
      "epoch": 5.732,
      "grad_norm": 0.07197418063879013,
      "learning_rate": 3.7282364152646297e-05,
      "loss": 0.3791,
      "step": 1433
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.05946287140250206,
      "learning_rate": 3.715978957861363e-05,
      "loss": 0.3456,
      "step": 1434
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.0675501674413681,
      "learning_rate": 3.7037370834730055e-05,
      "loss": 0.3523,
      "step": 1435
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.05708341300487518,
      "learning_rate": 3.691510822456764e-05,
      "loss": 0.285,
      "step": 1436
    },
    {
      "epoch": 5.748,
      "grad_norm": 0.07394290715456009,
      "learning_rate": 3.6793002051311186e-05,
      "loss": 0.3714,
      "step": 1437
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.06611339002847672,
      "learning_rate": 3.667105261775775e-05,
      "loss": 0.3528,
      "step": 1438
    },
    {
      "epoch": 5.756,
      "grad_norm": 0.08343213051557541,
      "learning_rate": 3.654926022631545e-05,
      "loss": 0.4136,
      "step": 1439
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.06028338894248009,
      "learning_rate": 3.642762517900322e-05,
      "loss": 0.3232,
      "step": 1440
    },
    {
      "epoch": 5.764,
      "grad_norm": 0.0700080469250679,
      "learning_rate": 3.6306147777449614e-05,
      "loss": 0.3123,
      "step": 1441
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.05367661640048027,
      "learning_rate": 3.618482832289245e-05,
      "loss": 0.3278,
      "step": 1442
    },
    {
      "epoch": 5.772,
      "grad_norm": 0.0694417655467987,
      "learning_rate": 3.606366711617771e-05,
      "loss": 0.3062,
      "step": 1443
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.06953588873147964,
      "learning_rate": 3.594266445775903e-05,
      "loss": 0.3035,
      "step": 1444
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.06993982195854187,
      "learning_rate": 3.5821820647696867e-05,
      "loss": 0.353,
      "step": 1445
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.0663439929485321,
      "learning_rate": 3.570113598565773e-05,
      "loss": 0.354,
      "step": 1446
    },
    {
      "epoch": 5.788,
      "grad_norm": 0.07089172303676605,
      "learning_rate": 3.558061077091359e-05,
      "loss": 0.3207,
      "step": 1447
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.07295609265565872,
      "learning_rate": 3.546024530234091e-05,
      "loss": 0.349,
      "step": 1448
    },
    {
      "epoch": 5.796,
      "grad_norm": 0.06256895512342453,
      "learning_rate": 3.534003987842005e-05,
      "loss": 0.3051,
      "step": 1449
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.0633658841252327,
      "learning_rate": 3.521999479723448e-05,
      "loss": 0.3572,
      "step": 1450
    },
    {
      "epoch": 5.804,
      "grad_norm": 0.08907626569271088,
      "learning_rate": 3.5100110356470116e-05,
      "loss": 0.3387,
      "step": 1451
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.09027552604675293,
      "learning_rate": 3.498038685341447e-05,
      "loss": 0.386,
      "step": 1452
    },
    {
      "epoch": 5.812,
      "grad_norm": 0.05855569615960121,
      "learning_rate": 3.4860824584955955e-05,
      "loss": 0.3304,
      "step": 1453
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.07259529829025269,
      "learning_rate": 3.4741423847583134e-05,
      "loss": 0.364,
      "step": 1454
    },
    {
      "epoch": 5.82,
      "grad_norm": 0.07771603018045425,
      "learning_rate": 3.462218493738416e-05,
      "loss": 0.4155,
      "step": 1455
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.07394455373287201,
      "learning_rate": 3.450310815004564e-05,
      "loss": 0.3771,
      "step": 1456
    },
    {
      "epoch": 5.828,
      "grad_norm": 0.06213834136724472,
      "learning_rate": 3.438419378085238e-05,
      "loss": 0.3116,
      "step": 1457
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.06321006268262863,
      "learning_rate": 3.4265442124686306e-05,
      "loss": 0.3202,
      "step": 1458
    },
    {
      "epoch": 5.836,
      "grad_norm": 0.08083482086658478,
      "learning_rate": 3.414685347602588e-05,
      "loss": 0.3647,
      "step": 1459
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.054998189210891724,
      "learning_rate": 3.402842812894529e-05,
      "loss": 0.261,
      "step": 1460
    },
    {
      "epoch": 5.844,
      "grad_norm": 0.08063041418790817,
      "learning_rate": 3.3910166377113894e-05,
      "loss": 0.3844,
      "step": 1461
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.05450510233640671,
      "learning_rate": 3.379206851379525e-05,
      "loss": 0.3119,
      "step": 1462
    },
    {
      "epoch": 5.852,
      "grad_norm": 0.06813904643058777,
      "learning_rate": 3.367413483184654e-05,
      "loss": 0.3737,
      "step": 1463
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.04988278076052666,
      "learning_rate": 3.355636562371787e-05,
      "loss": 0.3064,
      "step": 1464
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.05979011580348015,
      "learning_rate": 3.3438761181451414e-05,
      "loss": 0.3027,
      "step": 1465
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.07311499863862991,
      "learning_rate": 3.332132179668078e-05,
      "loss": 0.3908,
      "step": 1466
    },
    {
      "epoch": 5.868,
      "grad_norm": 0.077669657766819,
      "learning_rate": 3.320404776063025e-05,
      "loss": 0.416,
      "step": 1467
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.07861768454313278,
      "learning_rate": 3.308693936411421e-05,
      "loss": 0.3557,
      "step": 1468
    },
    {
      "epoch": 5.876,
      "grad_norm": 0.06965794414281845,
      "learning_rate": 3.296999689753604e-05,
      "loss": 0.3872,
      "step": 1469
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.0742010697722435,
      "learning_rate": 3.285322065088792e-05,
      "loss": 0.4164,
      "step": 1470
    },
    {
      "epoch": 5.884,
      "grad_norm": 0.0589270181953907,
      "learning_rate": 3.273661091374962e-05,
      "loss": 0.2804,
      "step": 1471
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.061630476266145706,
      "learning_rate": 3.262016797528824e-05,
      "loss": 0.3424,
      "step": 1472
    },
    {
      "epoch": 5.892,
      "grad_norm": 0.07137200981378555,
      "learning_rate": 3.2503892124256966e-05,
      "loss": 0.3406,
      "step": 1473
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.07087653130292892,
      "learning_rate": 3.238778364899487e-05,
      "loss": 0.3849,
      "step": 1474
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.06615737080574036,
      "learning_rate": 3.227184283742591e-05,
      "loss": 0.346,
      "step": 1475
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.06773682683706284,
      "learning_rate": 3.215606997705822e-05,
      "loss": 0.3689,
      "step": 1476
    },
    {
      "epoch": 5.908,
      "grad_norm": 0.0625658705830574,
      "learning_rate": 3.204046535498346e-05,
      "loss": 0.3518,
      "step": 1477
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.0683465301990509,
      "learning_rate": 3.19250292578762e-05,
      "loss": 0.3584,
      "step": 1478
    },
    {
      "epoch": 5.916,
      "grad_norm": 0.05550326406955719,
      "learning_rate": 3.1809761971993e-05,
      "loss": 0.3331,
      "step": 1479
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.06319141387939453,
      "learning_rate": 3.169466378317177e-05,
      "loss": 0.3135,
      "step": 1480
    },
    {
      "epoch": 5.924,
      "grad_norm": 0.051002420485019684,
      "learning_rate": 3.1579734976831265e-05,
      "loss": 0.2802,
      "step": 1481
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.07325625419616699,
      "learning_rate": 3.1464975837970036e-05,
      "loss": 0.326,
      "step": 1482
    },
    {
      "epoch": 5.932,
      "grad_norm": 0.06349228322505951,
      "learning_rate": 3.135038665116596e-05,
      "loss": 0.3252,
      "step": 1483
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.06629408895969391,
      "learning_rate": 3.1235967700575476e-05,
      "loss": 0.346,
      "step": 1484
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 0.05678519234061241,
      "learning_rate": 3.1121719269932916e-05,
      "loss": 0.327,
      "step": 1485
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.06831565499305725,
      "learning_rate": 3.100764164254962e-05,
      "loss": 0.4061,
      "step": 1486
    },
    {
      "epoch": 5.948,
      "grad_norm": 0.07953806966543198,
      "learning_rate": 3.089373510131354e-05,
      "loss": 0.3392,
      "step": 1487
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.05298736318945885,
      "learning_rate": 3.0779999928688276e-05,
      "loss": 0.279,
      "step": 1488
    },
    {
      "epoch": 5.9559999999999995,
      "grad_norm": 0.06474721431732178,
      "learning_rate": 3.0666436406712485e-05,
      "loss": 0.3262,
      "step": 1489
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.05851787328720093,
      "learning_rate": 3.0553044816999135e-05,
      "loss": 0.3222,
      "step": 1490
    },
    {
      "epoch": 5.964,
      "grad_norm": 0.06887523829936981,
      "learning_rate": 3.0439825440734947e-05,
      "loss": 0.3788,
      "step": 1491
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.06421826034784317,
      "learning_rate": 3.0326778558679492e-05,
      "loss": 0.3406,
      "step": 1492
    },
    {
      "epoch": 5.9719999999999995,
      "grad_norm": 0.06386883556842804,
      "learning_rate": 3.021390445116462e-05,
      "loss": 0.3305,
      "step": 1493
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.0610087513923645,
      "learning_rate": 3.0101203398093702e-05,
      "loss": 0.3032,
      "step": 1494
    },
    {
      "epoch": 5.98,
      "grad_norm": 0.052572447806596756,
      "learning_rate": 2.998867567894108e-05,
      "loss": 0.2951,
      "step": 1495
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.05696314573287964,
      "learning_rate": 2.9876321572751144e-05,
      "loss": 0.3499,
      "step": 1496
    },
    {
      "epoch": 5.9879999999999995,
      "grad_norm": 0.06487584114074707,
      "learning_rate": 2.976414135813782e-05,
      "loss": 0.3462,
      "step": 1497
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.0769282728433609,
      "learning_rate": 2.9652135313283825e-05,
      "loss": 0.3542,
      "step": 1498
    },
    {
      "epoch": 5.996,
      "grad_norm": 0.06616387516260147,
      "learning_rate": 2.9540303715939898e-05,
      "loss": 0.2768,
      "step": 1499
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.06416326016187668,
      "learning_rate": 2.9428646843424323e-05,
      "loss": 0.3187,
      "step": 1500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.3223506510257721,
      "eval_runtime": 361.7734,
      "eval_samples_per_second": 0.276,
      "eval_steps_per_second": 0.276,
      "step": 1500
    },
    {
      "epoch": 6.004,
      "grad_norm": 0.06834211200475693,
      "learning_rate": 2.931716497262198e-05,
      "loss": 0.3379,
      "step": 1501
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.07678252458572388,
      "learning_rate": 2.9205858379983896e-05,
      "loss": 0.359,
      "step": 1502
    },
    {
      "epoch": 6.012,
      "grad_norm": 0.05428697168827057,
      "learning_rate": 2.9094727341526275e-05,
      "loss": 0.2856,
      "step": 1503
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.06556776165962219,
      "learning_rate": 2.8983772132830167e-05,
      "loss": 0.3625,
      "step": 1504
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.061612319201231,
      "learning_rate": 2.8872993029040508e-05,
      "loss": 0.3784,
      "step": 1505
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.05048124119639397,
      "learning_rate": 2.876239030486554e-05,
      "loss": 0.2834,
      "step": 1506
    },
    {
      "epoch": 6.028,
      "grad_norm": 0.048362135887145996,
      "learning_rate": 2.86519642345761e-05,
      "loss": 0.3022,
      "step": 1507
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.0515739880502224,
      "learning_rate": 2.8541715092005094e-05,
      "loss": 0.26,
      "step": 1508
    },
    {
      "epoch": 6.036,
      "grad_norm": 0.06418711692094803,
      "learning_rate": 2.843164315054644e-05,
      "loss": 0.3459,
      "step": 1509
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.0668971911072731,
      "learning_rate": 2.8321748683154893e-05,
      "loss": 0.3177,
      "step": 1510
    },
    {
      "epoch": 6.044,
      "grad_norm": 0.0728256106376648,
      "learning_rate": 2.821203196234492e-05,
      "loss": 0.3959,
      "step": 1511
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.08186802268028259,
      "learning_rate": 2.8102493260190386e-05,
      "loss": 0.4215,
      "step": 1512
    },
    {
      "epoch": 6.052,
      "grad_norm": 0.08128944784402847,
      "learning_rate": 2.799313284832349e-05,
      "loss": 0.3787,
      "step": 1513
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.06873710453510284,
      "learning_rate": 2.7883950997934526e-05,
      "loss": 0.3334,
      "step": 1514
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.05838138237595558,
      "learning_rate": 2.7774947979770883e-05,
      "loss": 0.2945,
      "step": 1515
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.06437106430530548,
      "learning_rate": 2.7666124064136455e-05,
      "loss": 0.3653,
      "step": 1516
    },
    {
      "epoch": 6.068,
      "grad_norm": 0.0673665702342987,
      "learning_rate": 2.7557479520891104e-05,
      "loss": 0.3232,
      "step": 1517
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.049847133457660675,
      "learning_rate": 2.744901461944982e-05,
      "loss": 0.2615,
      "step": 1518
    },
    {
      "epoch": 6.076,
      "grad_norm": 0.06598155200481415,
      "learning_rate": 2.7340729628782125e-05,
      "loss": 0.3214,
      "step": 1519
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.07354263216257095,
      "learning_rate": 2.7232624817411378e-05,
      "loss": 0.37,
      "step": 1520
    },
    {
      "epoch": 6.084,
      "grad_norm": 0.05388859659433365,
      "learning_rate": 2.712470045341421e-05,
      "loss": 0.2867,
      "step": 1521
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.07660053670406342,
      "learning_rate": 2.7016956804419714e-05,
      "loss": 0.3621,
      "step": 1522
    },
    {
      "epoch": 6.092,
      "grad_norm": 0.06088152527809143,
      "learning_rate": 2.6909394137608868e-05,
      "loss": 0.3338,
      "step": 1523
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.07148559391498566,
      "learning_rate": 2.680201271971383e-05,
      "loss": 0.3456,
      "step": 1524
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.06811141222715378,
      "learning_rate": 2.669481281701739e-05,
      "loss": 0.3893,
      "step": 1525
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.08867043256759644,
      "learning_rate": 2.6587794695352064e-05,
      "loss": 0.3871,
      "step": 1526
    },
    {
      "epoch": 6.108,
      "grad_norm": 0.09078586846590042,
      "learning_rate": 2.6480958620099737e-05,
      "loss": 0.4173,
      "step": 1527
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.058210331946611404,
      "learning_rate": 2.63743048561908e-05,
      "loss": 0.2944,
      "step": 1528
    },
    {
      "epoch": 6.116,
      "grad_norm": 0.07828005403280258,
      "learning_rate": 2.6267833668103535e-05,
      "loss": 0.3952,
      "step": 1529
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.061640504747629166,
      "learning_rate": 2.6161545319863456e-05,
      "loss": 0.3265,
      "step": 1530
    },
    {
      "epoch": 6.124,
      "grad_norm": 0.06492935121059418,
      "learning_rate": 2.6055440075042793e-05,
      "loss": 0.3528,
      "step": 1531
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.07990214228630066,
      "learning_rate": 2.5949518196759583e-05,
      "loss": 0.373,
      "step": 1532
    },
    {
      "epoch": 6.132,
      "grad_norm": 0.06499257683753967,
      "learning_rate": 2.5843779947677183e-05,
      "loss": 0.346,
      "step": 1533
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.07580700516700745,
      "learning_rate": 2.573822559000367e-05,
      "loss": 0.3574,
      "step": 1534
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.07059193402528763,
      "learning_rate": 2.563285538549104e-05,
      "loss": 0.3786,
      "step": 1535
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.06445319205522537,
      "learning_rate": 2.5527669595434622e-05,
      "loss": 0.3099,
      "step": 1536
    },
    {
      "epoch": 6.148,
      "grad_norm": 0.07512721419334412,
      "learning_rate": 2.542266848067243e-05,
      "loss": 0.4234,
      "step": 1537
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.05570266395807266,
      "learning_rate": 2.5317852301584643e-05,
      "loss": 0.3244,
      "step": 1538
    },
    {
      "epoch": 6.156,
      "grad_norm": 0.06006212159991264,
      "learning_rate": 2.5213221318092627e-05,
      "loss": 0.2984,
      "step": 1539
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.06175018474459648,
      "learning_rate": 2.51087757896587e-05,
      "loss": 0.3351,
      "step": 1540
    },
    {
      "epoch": 6.164,
      "grad_norm": 0.06432738155126572,
      "learning_rate": 2.5004515975285183e-05,
      "loss": 0.2868,
      "step": 1541
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.07189884781837463,
      "learning_rate": 2.4900442133513946e-05,
      "loss": 0.3932,
      "step": 1542
    },
    {
      "epoch": 6.172,
      "grad_norm": 0.06963254511356354,
      "learning_rate": 2.479655452242555e-05,
      "loss": 0.3546,
      "step": 1543
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.06186135485768318,
      "learning_rate": 2.4692853399638917e-05,
      "loss": 0.3296,
      "step": 1544
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.07065204530954361,
      "learning_rate": 2.4589339022310386e-05,
      "loss": 0.397,
      "step": 1545
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.060639429837465286,
      "learning_rate": 2.448601164713328e-05,
      "loss": 0.3136,
      "step": 1546
    },
    {
      "epoch": 6.188,
      "grad_norm": 0.06817039847373962,
      "learning_rate": 2.4382871530337114e-05,
      "loss": 0.318,
      "step": 1547
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.06765474379062653,
      "learning_rate": 2.4279918927687184e-05,
      "loss": 0.3109,
      "step": 1548
    },
    {
      "epoch": 6.196,
      "grad_norm": 0.06859865039587021,
      "learning_rate": 2.4177154094483666e-05,
      "loss": 0.3211,
      "step": 1549
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.08339592814445496,
      "learning_rate": 2.407457728556115e-05,
      "loss": 0.3952,
      "step": 1550
    },
    {
      "epoch": 6.204,
      "grad_norm": 0.06933092325925827,
      "learning_rate": 2.397218875528795e-05,
      "loss": 0.3117,
      "step": 1551
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.06159750744700432,
      "learning_rate": 2.3869988757565543e-05,
      "loss": 0.3325,
      "step": 1552
    },
    {
      "epoch": 6.212,
      "grad_norm": 0.0784892588853836,
      "learning_rate": 2.3767977545827845e-05,
      "loss": 0.3759,
      "step": 1553
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.08632590621709824,
      "learning_rate": 2.3666155373040578e-05,
      "loss": 0.3604,
      "step": 1554
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.08326347172260284,
      "learning_rate": 2.3564522491700835e-05,
      "loss": 0.4147,
      "step": 1555
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.0698813945055008,
      "learning_rate": 2.34630791538361e-05,
      "loss": 0.3444,
      "step": 1556
    },
    {
      "epoch": 6.228,
      "grad_norm": 0.061834629625082016,
      "learning_rate": 2.3361825611004017e-05,
      "loss": 0.315,
      "step": 1557
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.08070134371519089,
      "learning_rate": 2.326076211429147e-05,
      "loss": 0.34,
      "step": 1558
    },
    {
      "epoch": 6.236,
      "grad_norm": 0.07019302994012833,
      "learning_rate": 2.315988891431412e-05,
      "loss": 0.3775,
      "step": 1559
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.055203236639499664,
      "learning_rate": 2.305920626121567e-05,
      "loss": 0.2789,
      "step": 1560
    },
    {
      "epoch": 6.244,
      "grad_norm": 0.07085897773504257,
      "learning_rate": 2.2958714404667415e-05,
      "loss": 0.3151,
      "step": 1561
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.0708853155374527,
      "learning_rate": 2.2858413593867434e-05,
      "loss": 0.3518,
      "step": 1562
    },
    {
      "epoch": 6.252,
      "grad_norm": 0.06934941560029984,
      "learning_rate": 2.275830407754006e-05,
      "loss": 0.3434,
      "step": 1563
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.07150150090456009,
      "learning_rate": 2.2658386103935257e-05,
      "loss": 0.3573,
      "step": 1564
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.07300613820552826,
      "learning_rate": 2.2558659920828097e-05,
      "loss": 0.3633,
      "step": 1565
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.06360750645399094,
      "learning_rate": 2.2459125775517852e-05,
      "loss": 0.3247,
      "step": 1566
    },
    {
      "epoch": 6.268,
      "grad_norm": 0.07971139252185822,
      "learning_rate": 2.2359783914827813e-05,
      "loss": 0.3387,
      "step": 1567
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.06357252597808838,
      "learning_rate": 2.226063458510428e-05,
      "loss": 0.3168,
      "step": 1568
    },
    {
      "epoch": 6.276,
      "grad_norm": 0.054927706718444824,
      "learning_rate": 2.216167803221617e-05,
      "loss": 0.3046,
      "step": 1569
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.06144815683364868,
      "learning_rate": 2.2062914501554412e-05,
      "loss": 0.2943,
      "step": 1570
    },
    {
      "epoch": 6.284,
      "grad_norm": 0.08160805702209473,
      "learning_rate": 2.1964344238031186e-05,
      "loss": 0.3313,
      "step": 1571
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.056418273597955704,
      "learning_rate": 2.1865967486079476e-05,
      "loss": 0.2932,
      "step": 1572
    },
    {
      "epoch": 6.292,
      "grad_norm": 0.0610978938639164,
      "learning_rate": 2.1767784489652343e-05,
      "loss": 0.3102,
      "step": 1573
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.07407183200120926,
      "learning_rate": 2.1669795492222467e-05,
      "loss": 0.3371,
      "step": 1574
    },
    {
      "epoch": 6.3,
      "grad_norm": 0.07553558051586151,
      "learning_rate": 2.1572000736781373e-05,
      "loss": 0.3493,
      "step": 1575
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.07774172723293304,
      "learning_rate": 2.1474400465838928e-05,
      "loss": 0.375,
      "step": 1576
    },
    {
      "epoch": 6.308,
      "grad_norm": 0.05388942360877991,
      "learning_rate": 2.137699492142269e-05,
      "loss": 0.27,
      "step": 1577
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.062295980751514435,
      "learning_rate": 2.127978434507747e-05,
      "loss": 0.2916,
      "step": 1578
    },
    {
      "epoch": 6.316,
      "grad_norm": 0.07141172140836716,
      "learning_rate": 2.1182768977864375e-05,
      "loss": 0.3973,
      "step": 1579
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.07251191884279251,
      "learning_rate": 2.1085949060360654e-05,
      "loss": 0.3722,
      "step": 1580
    },
    {
      "epoch": 6.324,
      "grad_norm": 0.06098160892724991,
      "learning_rate": 2.098932483265873e-05,
      "loss": 0.3488,
      "step": 1581
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.0810917317867279,
      "learning_rate": 2.0892896534365904e-05,
      "loss": 0.293,
      "step": 1582
    },
    {
      "epoch": 6.332,
      "grad_norm": 0.08034728467464447,
      "learning_rate": 2.0796664404603416e-05,
      "loss": 0.3342,
      "step": 1583
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.06465353816747665,
      "learning_rate": 2.070062868200624e-05,
      "loss": 0.3461,
      "step": 1584
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.06269951164722443,
      "learning_rate": 2.0604789604722208e-05,
      "loss": 0.2603,
      "step": 1585
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.06369026750326157,
      "learning_rate": 2.05091474104115e-05,
      "loss": 0.3213,
      "step": 1586
    },
    {
      "epoch": 6.348,
      "grad_norm": 0.08072788268327713,
      "learning_rate": 2.0413702336246154e-05,
      "loss": 0.3363,
      "step": 1587
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.05637127906084061,
      "learning_rate": 2.0318454618909322e-05,
      "loss": 0.2717,
      "step": 1588
    },
    {
      "epoch": 6.356,
      "grad_norm": 0.06689298897981644,
      "learning_rate": 2.0223404494594743e-05,
      "loss": 0.3411,
      "step": 1589
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.07675068825483322,
      "learning_rate": 2.01285521990062e-05,
      "loss": 0.3721,
      "step": 1590
    },
    {
      "epoch": 6.364,
      "grad_norm": 0.07539574056863785,
      "learning_rate": 2.0033897967356942e-05,
      "loss": 0.3671,
      "step": 1591
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.0838676393032074,
      "learning_rate": 1.993944203436898e-05,
      "loss": 0.3881,
      "step": 1592
    },
    {
      "epoch": 6.372,
      "grad_norm": 0.05498814582824707,
      "learning_rate": 1.984518463427264e-05,
      "loss": 0.3083,
      "step": 1593
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.06718313694000244,
      "learning_rate": 1.9751126000805897e-05,
      "loss": 0.3084,
      "step": 1594
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.06628042459487915,
      "learning_rate": 1.96572663672139e-05,
      "loss": 0.381,
      "step": 1595
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.06474746763706207,
      "learning_rate": 1.956360596624819e-05,
      "loss": 0.3484,
      "step": 1596
    },
    {
      "epoch": 6.388,
      "grad_norm": 0.06282491981983185,
      "learning_rate": 1.9470145030166385e-05,
      "loss": 0.3887,
      "step": 1597
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.06860149651765823,
      "learning_rate": 1.9376883790731414e-05,
      "loss": 0.3195,
      "step": 1598
    },
    {
      "epoch": 6.396,
      "grad_norm": 0.06845808029174805,
      "learning_rate": 1.9283822479210987e-05,
      "loss": 0.3788,
      "step": 1599
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.07803408056497574,
      "learning_rate": 1.9190961326377054e-05,
      "loss": 0.3577,
      "step": 1600
    },
    {
      "epoch": 6.404,
      "grad_norm": 0.06761171668767929,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 0.3484,
      "step": 1601
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.07675305008888245,
      "learning_rate": 1.9005840417374265e-05,
      "loss": 0.343,
      "step": 1602
    },
    {
      "epoch": 6.412,
      "grad_norm": 0.07515837252140045,
      "learning_rate": 1.8913581120265234e-05,
      "loss": 0.3688,
      "step": 1603
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.06318706274032593,
      "learning_rate": 1.8821522899961308e-05,
      "loss": 0.3004,
      "step": 1604
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.05988398566842079,
      "learning_rate": 1.8729665984747e-05,
      "loss": 0.3318,
      "step": 1605
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.06585785746574402,
      "learning_rate": 1.863801060240764e-05,
      "loss": 0.3087,
      "step": 1606
    },
    {
      "epoch": 6.428,
      "grad_norm": 0.07867848873138428,
      "learning_rate": 1.8546556980228715e-05,
      "loss": 0.3189,
      "step": 1607
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.07922521233558655,
      "learning_rate": 1.8455305344995523e-05,
      "loss": 0.357,
      "step": 1608
    },
    {
      "epoch": 6.436,
      "grad_norm": 0.07018020004034042,
      "learning_rate": 1.8364255922992392e-05,
      "loss": 0.3727,
      "step": 1609
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.06986752897500992,
      "learning_rate": 1.82734089400022e-05,
      "loss": 0.3221,
      "step": 1610
    },
    {
      "epoch": 6.444,
      "grad_norm": 0.059008173644542694,
      "learning_rate": 1.818276462130585e-05,
      "loss": 0.3317,
      "step": 1611
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.07571770995855331,
      "learning_rate": 1.8092323191681715e-05,
      "loss": 0.3418,
      "step": 1612
    },
    {
      "epoch": 6.452,
      "grad_norm": 0.05427943542599678,
      "learning_rate": 1.8002084875404934e-05,
      "loss": 0.2947,
      "step": 1613
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.06651146709918976,
      "learning_rate": 1.7912049896247098e-05,
      "loss": 0.3538,
      "step": 1614
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.06253816187381744,
      "learning_rate": 1.7822218477475494e-05,
      "loss": 0.3433,
      "step": 1615
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.07133420556783676,
      "learning_rate": 1.7732590841852636e-05,
      "loss": 0.3056,
      "step": 1616
    },
    {
      "epoch": 6.468,
      "grad_norm": 0.05695674195885658,
      "learning_rate": 1.7643167211635668e-05,
      "loss": 0.3422,
      "step": 1617
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.06581369042396545,
      "learning_rate": 1.7553947808575944e-05,
      "loss": 0.3497,
      "step": 1618
    },
    {
      "epoch": 6.476,
      "grad_norm": 0.06971593201160431,
      "learning_rate": 1.746493285391827e-05,
      "loss": 0.3614,
      "step": 1619
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.06565846502780914,
      "learning_rate": 1.7376122568400532e-05,
      "loss": 0.3221,
      "step": 1620
    },
    {
      "epoch": 6.484,
      "grad_norm": 0.07580750435590744,
      "learning_rate": 1.7287517172253e-05,
      "loss": 0.3866,
      "step": 1621
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.08425440639257431,
      "learning_rate": 1.7199116885197995e-05,
      "loss": 0.3956,
      "step": 1622
    },
    {
      "epoch": 6.492,
      "grad_norm": 0.061618704348802567,
      "learning_rate": 1.7110921926449096e-05,
      "loss": 0.3231,
      "step": 1623
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.08008591085672379,
      "learning_rate": 1.7022932514710765e-05,
      "loss": 0.4201,
      "step": 1624
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.05159040540456772,
      "learning_rate": 1.693514886817772e-05,
      "loss": 0.2918,
      "step": 1625
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.06473664939403534,
      "learning_rate": 1.684757120453444e-05,
      "loss": 0.3153,
      "step": 1626
    },
    {
      "epoch": 6.508,
      "grad_norm": 0.10873451083898544,
      "learning_rate": 1.676019974095464e-05,
      "loss": 0.3139,
      "step": 1627
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.06348952651023865,
      "learning_rate": 1.6673034694100655e-05,
      "loss": 0.318,
      "step": 1628
    },
    {
      "epoch": 6.516,
      "grad_norm": 0.07484299689531326,
      "learning_rate": 1.658607628012303e-05,
      "loss": 0.3863,
      "step": 1629
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.08195864409208298,
      "learning_rate": 1.649932471465976e-05,
      "loss": 0.3694,
      "step": 1630
    },
    {
      "epoch": 6.524,
      "grad_norm": 0.08166933059692383,
      "learning_rate": 1.6412780212836054e-05,
      "loss": 0.4534,
      "step": 1631
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.06810278445482254,
      "learning_rate": 1.6326442989263567e-05,
      "loss": 0.3589,
      "step": 1632
    },
    {
      "epoch": 6.532,
      "grad_norm": 0.0673985406756401,
      "learning_rate": 1.6240313258039975e-05,
      "loss": 0.3645,
      "step": 1633
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.06983423233032227,
      "learning_rate": 1.6154391232748368e-05,
      "loss": 0.3176,
      "step": 1634
    },
    {
      "epoch": 6.54,
      "grad_norm": 0.08191224932670593,
      "learning_rate": 1.60686771264569e-05,
      "loss": 0.3368,
      "step": 1635
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.08323264867067337,
      "learning_rate": 1.5983171151717923e-05,
      "loss": 0.3836,
      "step": 1636
    },
    {
      "epoch": 6.548,
      "grad_norm": 0.07223682105541229,
      "learning_rate": 1.5897873520567874e-05,
      "loss": 0.365,
      "step": 1637
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.0680270865559578,
      "learning_rate": 1.5812784444526396e-05,
      "loss": 0.3768,
      "step": 1638
    },
    {
      "epoch": 6.556,
      "grad_norm": 0.06786799430847168,
      "learning_rate": 1.5727904134596083e-05,
      "loss": 0.3261,
      "step": 1639
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.06063956022262573,
      "learning_rate": 1.564323280126173e-05,
      "loss": 0.3523,
      "step": 1640
    },
    {
      "epoch": 6.564,
      "grad_norm": 0.06728655099868774,
      "learning_rate": 1.5558770654489973e-05,
      "loss": 0.3585,
      "step": 1641
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.06619132310152054,
      "learning_rate": 1.547451790372868e-05,
      "loss": 0.3063,
      "step": 1642
    },
    {
      "epoch": 6.572,
      "grad_norm": 0.08243689686059952,
      "learning_rate": 1.5390474757906446e-05,
      "loss": 0.3984,
      "step": 1643
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.07628393918275833,
      "learning_rate": 1.530664142543219e-05,
      "loss": 0.3552,
      "step": 1644
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.0770251527428627,
      "learning_rate": 1.5223018114194421e-05,
      "loss": 0.3782,
      "step": 1645
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.0853295847773552,
      "learning_rate": 1.513960503156091e-05,
      "loss": 0.4113,
      "step": 1646
    },
    {
      "epoch": 6.588,
      "grad_norm": 0.07045392692089081,
      "learning_rate": 1.5056402384378055e-05,
      "loss": 0.421,
      "step": 1647
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.057165130972862244,
      "learning_rate": 1.4973410378970488e-05,
      "loss": 0.2881,
      "step": 1648
    },
    {
      "epoch": 6.596,
      "grad_norm": 0.0702192410826683,
      "learning_rate": 1.489062922114044e-05,
      "loss": 0.3404,
      "step": 1649
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.0711183026432991,
      "learning_rate": 1.4808059116167305e-05,
      "loss": 0.405,
      "step": 1650
    },
    {
      "epoch": 6.604,
      "grad_norm": 0.07092752307653427,
      "learning_rate": 1.4725700268807085e-05,
      "loss": 0.3414,
      "step": 1651
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.07099132239818573,
      "learning_rate": 1.4643552883292e-05,
      "loss": 0.3805,
      "step": 1652
    },
    {
      "epoch": 6.612,
      "grad_norm": 0.061583779752254486,
      "learning_rate": 1.4561617163329732e-05,
      "loss": 0.3311,
      "step": 1653
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.07112471014261246,
      "learning_rate": 1.4479893312103243e-05,
      "loss": 0.3878,
      "step": 1654
    },
    {
      "epoch": 6.62,
      "grad_norm": 0.07899638265371323,
      "learning_rate": 1.439838153227e-05,
      "loss": 0.3598,
      "step": 1655
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.07646069675683975,
      "learning_rate": 1.4317082025961626e-05,
      "loss": 0.3976,
      "step": 1656
    },
    {
      "epoch": 6.628,
      "grad_norm": 0.0713951513171196,
      "learning_rate": 1.4235994994783297e-05,
      "loss": 0.3869,
      "step": 1657
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.059976961463689804,
      "learning_rate": 1.415512063981339e-05,
      "loss": 0.317,
      "step": 1658
    },
    {
      "epoch": 6.636,
      "grad_norm": 0.07263367623090744,
      "learning_rate": 1.4074459161602826e-05,
      "loss": 0.3635,
      "step": 1659
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.06870348006486893,
      "learning_rate": 1.3994010760174603e-05,
      "loss": 0.3682,
      "step": 1660
    },
    {
      "epoch": 6.644,
      "grad_norm": 0.06620118021965027,
      "learning_rate": 1.391377563502344e-05,
      "loss": 0.3346,
      "step": 1661
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.07713780552148819,
      "learning_rate": 1.3833753985115106e-05,
      "loss": 0.3186,
      "step": 1662
    },
    {
      "epoch": 6.652,
      "grad_norm": 0.06906948238611221,
      "learning_rate": 1.3753946008885977e-05,
      "loss": 0.3645,
      "step": 1663
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.07395844906568527,
      "learning_rate": 1.3674351904242611e-05,
      "loss": 0.3613,
      "step": 1664
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.061829451471567154,
      "learning_rate": 1.3594971868561235e-05,
      "loss": 0.3275,
      "step": 1665
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.05523969233036041,
      "learning_rate": 1.351580609868711e-05,
      "loss": 0.2824,
      "step": 1666
    },
    {
      "epoch": 6.668,
      "grad_norm": 0.07989276200532913,
      "learning_rate": 1.3436854790934327e-05,
      "loss": 0.3513,
      "step": 1667
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.07453078776597977,
      "learning_rate": 1.3358118141085019e-05,
      "loss": 0.4046,
      "step": 1668
    },
    {
      "epoch": 6.676,
      "grad_norm": 0.06544866412878036,
      "learning_rate": 1.3279596344389134e-05,
      "loss": 0.2885,
      "step": 1669
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.05968371406197548,
      "learning_rate": 1.3201289595563693e-05,
      "loss": 0.2856,
      "step": 1670
    },
    {
      "epoch": 6.684,
      "grad_norm": 0.07181845605373383,
      "learning_rate": 1.3123198088792576e-05,
      "loss": 0.3518,
      "step": 1671
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.060635730624198914,
      "learning_rate": 1.3045322017725837e-05,
      "loss": 0.3205,
      "step": 1672
    },
    {
      "epoch": 6.692,
      "grad_norm": 0.06530008465051651,
      "learning_rate": 1.2967661575479317e-05,
      "loss": 0.31,
      "step": 1673
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.06731990724802017,
      "learning_rate": 1.2890216954634093e-05,
      "loss": 0.3066,
      "step": 1674
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.061968930065631866,
      "learning_rate": 1.2812988347236166e-05,
      "loss": 0.3129,
      "step": 1675
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.07315780222415924,
      "learning_rate": 1.2735975944795775e-05,
      "loss": 0.319,
      "step": 1676
    },
    {
      "epoch": 6.708,
      "grad_norm": 0.07938916236162186,
      "learning_rate": 1.2659179938287035e-05,
      "loss": 0.3513,
      "step": 1677
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.08315595984458923,
      "learning_rate": 1.2582600518147447e-05,
      "loss": 0.4268,
      "step": 1678
    },
    {
      "epoch": 6.716,
      "grad_norm": 0.06271794438362122,
      "learning_rate": 1.250623787427746e-05,
      "loss": 0.3332,
      "step": 1679
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.05773932859301567,
      "learning_rate": 1.2430092196039932e-05,
      "loss": 0.3132,
      "step": 1680
    },
    {
      "epoch": 6.724,
      "grad_norm": 0.0686379000544548,
      "learning_rate": 1.2354163672259667e-05,
      "loss": 0.3136,
      "step": 1681
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.07931342720985413,
      "learning_rate": 1.227845249122307e-05,
      "loss": 0.4109,
      "step": 1682
    },
    {
      "epoch": 6.732,
      "grad_norm": 0.07872749119997025,
      "learning_rate": 1.2202958840677425e-05,
      "loss": 0.3572,
      "step": 1683
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.07981351763010025,
      "learning_rate": 1.2127682907830761e-05,
      "loss": 0.3356,
      "step": 1684
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.07075639814138412,
      "learning_rate": 1.2052624879351104e-05,
      "loss": 0.2927,
      "step": 1685
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.0625603199005127,
      "learning_rate": 1.1977784941366143e-05,
      "loss": 0.2931,
      "step": 1686
    },
    {
      "epoch": 6.748,
      "grad_norm": 0.07119811326265335,
      "learning_rate": 1.1903163279462748e-05,
      "loss": 0.3303,
      "step": 1687
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.06568252295255661,
      "learning_rate": 1.1828760078686562e-05,
      "loss": 0.3436,
      "step": 1688
    },
    {
      "epoch": 6.756,
      "grad_norm": 0.07682748138904572,
      "learning_rate": 1.1754575523541433e-05,
      "loss": 0.389,
      "step": 1689
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.06321481615304947,
      "learning_rate": 1.168060979798904e-05,
      "loss": 0.3332,
      "step": 1690
    },
    {
      "epoch": 6.764,
      "grad_norm": 0.07390200346708298,
      "learning_rate": 1.1606863085448383e-05,
      "loss": 0.3401,
      "step": 1691
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.055608294904232025,
      "learning_rate": 1.1533335568795412e-05,
      "loss": 0.2558,
      "step": 1692
    },
    {
      "epoch": 6.772,
      "grad_norm": 0.08194738626480103,
      "learning_rate": 1.1460027430362474e-05,
      "loss": 0.3995,
      "step": 1693
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.08594003319740295,
      "learning_rate": 1.1386938851937924e-05,
      "loss": 0.3832,
      "step": 1694
    },
    {
      "epoch": 6.78,
      "grad_norm": 0.09175901859998703,
      "learning_rate": 1.1314070014765643e-05,
      "loss": 0.4272,
      "step": 1695
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.057208724319934845,
      "learning_rate": 1.124142109954459e-05,
      "loss": 0.2989,
      "step": 1696
    },
    {
      "epoch": 6.788,
      "grad_norm": 0.0652952566742897,
      "learning_rate": 1.1168992286428425e-05,
      "loss": 0.3098,
      "step": 1697
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.07192903012037277,
      "learning_rate": 1.1096783755024942e-05,
      "loss": 0.3258,
      "step": 1698
    },
    {
      "epoch": 6.796,
      "grad_norm": 0.0574200265109539,
      "learning_rate": 1.1024795684395694e-05,
      "loss": 0.3111,
      "step": 1699
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.07114102691411972,
      "learning_rate": 1.0953028253055542e-05,
      "loss": 0.3291,
      "step": 1700
    },
    {
      "epoch": 6.804,
      "grad_norm": 0.07077038288116455,
      "learning_rate": 1.0881481638972247e-05,
      "loss": 0.3335,
      "step": 1701
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.06756005436182022,
      "learning_rate": 1.081015601956593e-05,
      "loss": 0.3577,
      "step": 1702
    },
    {
      "epoch": 6.812,
      "grad_norm": 0.08042052388191223,
      "learning_rate": 1.0739051571708736e-05,
      "loss": 0.3228,
      "step": 1703
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.06991647928953171,
      "learning_rate": 1.0668168471724315e-05,
      "loss": 0.3651,
      "step": 1704
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.07002831995487213,
      "learning_rate": 1.05975068953875e-05,
      "loss": 0.3928,
      "step": 1705
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.05871078372001648,
      "learning_rate": 1.0527067017923654e-05,
      "loss": 0.3084,
      "step": 1706
    },
    {
      "epoch": 6.828,
      "grad_norm": 0.06260944157838821,
      "learning_rate": 1.045684901400853e-05,
      "loss": 0.3137,
      "step": 1707
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.06697069108486176,
      "learning_rate": 1.0386853057767576e-05,
      "loss": 0.3569,
      "step": 1708
    },
    {
      "epoch": 6.836,
      "grad_norm": 0.0876997858285904,
      "learning_rate": 1.031707932277568e-05,
      "loss": 0.4042,
      "step": 1709
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.04953070729970932,
      "learning_rate": 1.0247527982056582e-05,
      "loss": 0.2814,
      "step": 1710
    },
    {
      "epoch": 6.844,
      "grad_norm": 0.0716179609298706,
      "learning_rate": 1.0178199208082628e-05,
      "loss": 0.3384,
      "step": 1711
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.06051650270819664,
      "learning_rate": 1.0109093172774186e-05,
      "loss": 0.2942,
      "step": 1712
    },
    {
      "epoch": 6.852,
      "grad_norm": 0.0853307768702507,
      "learning_rate": 1.0040210047499288e-05,
      "loss": 0.3837,
      "step": 1713
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.07565628737211227,
      "learning_rate": 9.971550003073238e-06,
      "loss": 0.3811,
      "step": 1714
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.07399039715528488,
      "learning_rate": 9.903113209758096e-06,
      "loss": 0.332,
      "step": 1715
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.07338201254606247,
      "learning_rate": 9.834899837262324e-06,
      "loss": 0.3338,
      "step": 1716
    },
    {
      "epoch": 6.868,
      "grad_norm": 0.06950452923774719,
      "learning_rate": 9.766910054740341e-06,
      "loss": 0.3569,
      "step": 1717
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.06662455946207047,
      "learning_rate": 9.699144030792162e-06,
      "loss": 0.336,
      "step": 1718
    },
    {
      "epoch": 6.876,
      "grad_norm": 0.06443554908037186,
      "learning_rate": 9.631601933462863e-06,
      "loss": 0.3034,
      "step": 1719
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.061981480568647385,
      "learning_rate": 9.564283930242257e-06,
      "loss": 0.32,
      "step": 1720
    },
    {
      "epoch": 6.884,
      "grad_norm": 0.0610513761639595,
      "learning_rate": 9.497190188064442e-06,
      "loss": 0.3426,
      "step": 1721
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.07061169296503067,
      "learning_rate": 9.430320873307464e-06,
      "loss": 0.3567,
      "step": 1722
    },
    {
      "epoch": 6.892,
      "grad_norm": 0.07533406466245651,
      "learning_rate": 9.36367615179269e-06,
      "loss": 0.3655,
      "step": 1723
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.05891163647174835,
      "learning_rate": 9.297256188784709e-06,
      "loss": 0.2964,
      "step": 1724
    },
    {
      "epoch": 6.9,
      "grad_norm": 0.06087759509682655,
      "learning_rate": 9.23106114899065e-06,
      "loss": 0.3294,
      "step": 1725
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.07095373421907425,
      "learning_rate": 9.165091196559928e-06,
      "loss": 0.3599,
      "step": 1726
    },
    {
      "epoch": 6.908,
      "grad_norm": 0.04633491113781929,
      "learning_rate": 9.09934649508375e-06,
      "loss": 0.2373,
      "step": 1727
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.07184664160013199,
      "learning_rate": 9.033827207594814e-06,
      "loss": 0.3624,
      "step": 1728
    },
    {
      "epoch": 6.916,
      "grad_norm": 0.05205054208636284,
      "learning_rate": 8.968533496566778e-06,
      "loss": 0.2713,
      "step": 1729
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.08065509796142578,
      "learning_rate": 8.903465523913957e-06,
      "loss": 0.3824,
      "step": 1730
    },
    {
      "epoch": 6.924,
      "grad_norm": 0.07127881795167923,
      "learning_rate": 8.838623450990834e-06,
      "loss": 0.3534,
      "step": 1731
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.08578307926654816,
      "learning_rate": 8.774007438591792e-06,
      "loss": 0.3771,
      "step": 1732
    },
    {
      "epoch": 6.932,
      "grad_norm": 0.06616287678480148,
      "learning_rate": 8.709617646950564e-06,
      "loss": 0.3164,
      "step": 1733
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.06640265136957169,
      "learning_rate": 8.645454235739903e-06,
      "loss": 0.3051,
      "step": 1734
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 0.0575602687895298,
      "learning_rate": 8.581517364071268e-06,
      "loss": 0.2843,
      "step": 1735
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.06579773873090744,
      "learning_rate": 8.517807190494231e-06,
      "loss": 0.3106,
      "step": 1736
    },
    {
      "epoch": 6.948,
      "grad_norm": 0.055733583867549896,
      "learning_rate": 8.454323872996295e-06,
      "loss": 0.2756,
      "step": 1737
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.08045383542776108,
      "learning_rate": 8.391067569002353e-06,
      "loss": 0.4263,
      "step": 1738
    },
    {
      "epoch": 6.9559999999999995,
      "grad_norm": 0.06913798302412033,
      "learning_rate": 8.32803843537443e-06,
      "loss": 0.3619,
      "step": 1739
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.10413046926259995,
      "learning_rate": 8.265236628411087e-06,
      "loss": 0.3995,
      "step": 1740
    },
    {
      "epoch": 6.964,
      "grad_norm": 0.08461292833089828,
      "learning_rate": 8.202662303847298e-06,
      "loss": 0.3641,
      "step": 1741
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.07167329639196396,
      "learning_rate": 8.140315616853855e-06,
      "loss": 0.3877,
      "step": 1742
    },
    {
      "epoch": 6.9719999999999995,
      "grad_norm": 0.06499399244785309,
      "learning_rate": 8.078196722037068e-06,
      "loss": 0.2843,
      "step": 1743
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.06496383249759674,
      "learning_rate": 8.016305773438371e-06,
      "loss": 0.317,
      "step": 1744
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.0639159083366394,
      "learning_rate": 7.954642924533995e-06,
      "loss": 0.3294,
      "step": 1745
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.06774436682462692,
      "learning_rate": 7.893208328234425e-06,
      "loss": 0.3509,
      "step": 1746
    },
    {
      "epoch": 6.9879999999999995,
      "grad_norm": 0.09106981754302979,
      "learning_rate": 7.832002136884232e-06,
      "loss": 0.4168,
      "step": 1747
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.07773931324481964,
      "learning_rate": 7.771024502261526e-06,
      "loss": 0.3421,
      "step": 1748
    },
    {
      "epoch": 6.996,
      "grad_norm": 0.06181430444121361,
      "learning_rate": 7.710275575577697e-06,
      "loss": 0.343,
      "step": 1749
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.07491794973611832,
      "learning_rate": 7.649755507476952e-06,
      "loss": 0.3913,
      "step": 1750
    },
    {
      "epoch": 7.004,
      "grad_norm": 0.07115694880485535,
      "learning_rate": 7.589464448035988e-06,
      "loss": 0.365,
      "step": 1751
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.05755060538649559,
      "learning_rate": 7.529402546763598e-06,
      "loss": 0.2918,
      "step": 1752
    },
    {
      "epoch": 7.012,
      "grad_norm": 0.05148196965456009,
      "learning_rate": 7.46956995260033e-06,
      "loss": 0.3132,
      "step": 1753
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.05203291401267052,
      "learning_rate": 7.409966813918101e-06,
      "loss": 0.2684,
      "step": 1754
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.06587203592061996,
      "learning_rate": 7.350593278519824e-06,
      "loss": 0.3341,
      "step": 1755
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.07544936239719391,
      "learning_rate": 7.291449493639047e-06,
      "loss": 0.4051,
      "step": 1756
    },
    {
      "epoch": 7.028,
      "grad_norm": 0.08630243688821793,
      "learning_rate": 7.232535605939539e-06,
      "loss": 0.3693,
      "step": 1757
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.060416143387556076,
      "learning_rate": 7.173851761515082e-06,
      "loss": 0.337,
      "step": 1758
    },
    {
      "epoch": 7.036,
      "grad_norm": 0.06680666655302048,
      "learning_rate": 7.115398105888893e-06,
      "loss": 0.3501,
      "step": 1759
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.07807392627000809,
      "learning_rate": 7.057174784013431e-06,
      "loss": 0.3696,
      "step": 1760
    },
    {
      "epoch": 7.044,
      "grad_norm": 0.07530663907527924,
      "learning_rate": 6.999181940269927e-06,
      "loss": 0.3404,
      "step": 1761
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.06049760431051254,
      "learning_rate": 6.941419718468168e-06,
      "loss": 0.281,
      "step": 1762
    },
    {
      "epoch": 7.052,
      "grad_norm": 0.0771552324295044,
      "learning_rate": 6.8838882618459165e-06,
      "loss": 0.379,
      "step": 1763
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.051942259073257446,
      "learning_rate": 6.82658771306881e-06,
      "loss": 0.2804,
      "step": 1764
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.07366617769002914,
      "learning_rate": 6.76951821422982e-06,
      "loss": 0.3558,
      "step": 1765
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.06637141853570938,
      "learning_rate": 6.712679906848962e-06,
      "loss": 0.3559,
      "step": 1766
    },
    {
      "epoch": 7.068,
      "grad_norm": 0.06425582617521286,
      "learning_rate": 6.656072931872981e-06,
      "loss": 0.3643,
      "step": 1767
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.07481441646814346,
      "learning_rate": 6.599697429674945e-06,
      "loss": 0.4213,
      "step": 1768
    },
    {
      "epoch": 7.076,
      "grad_norm": 0.06854267418384552,
      "learning_rate": 6.543553540053926e-06,
      "loss": 0.377,
      "step": 1769
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.08277282118797302,
      "learning_rate": 6.4876414022346124e-06,
      "loss": 0.3637,
      "step": 1770
    },
    {
      "epoch": 7.084,
      "grad_norm": 0.06522608548402786,
      "learning_rate": 6.431961154867072e-06,
      "loss": 0.3452,
      "step": 1771
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.05797931179404259,
      "learning_rate": 6.37651293602628e-06,
      "loss": 0.3329,
      "step": 1772
    },
    {
      "epoch": 7.092,
      "grad_norm": 0.06654022634029388,
      "learning_rate": 6.321296883211836e-06,
      "loss": 0.3116,
      "step": 1773
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.05961538478732109,
      "learning_rate": 6.266313133347612e-06,
      "loss": 0.2449,
      "step": 1774
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.06706257164478302,
      "learning_rate": 6.211561822781476e-06,
      "loss": 0.3402,
      "step": 1775
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.07071281224489212,
      "learning_rate": 6.157043087284798e-06,
      "loss": 0.3268,
      "step": 1776
    },
    {
      "epoch": 7.108,
      "grad_norm": 0.08386856317520142,
      "learning_rate": 6.1027570620523e-06,
      "loss": 0.4074,
      "step": 1777
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.06056421250104904,
      "learning_rate": 6.048703881701578e-06,
      "loss": 0.3072,
      "step": 1778
    },
    {
      "epoch": 7.116,
      "grad_norm": 0.05476652830839157,
      "learning_rate": 5.9948836802728895e-06,
      "loss": 0.2761,
      "step": 1779
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.07631777226924896,
      "learning_rate": 5.94129659122864e-06,
      "loss": 0.3655,
      "step": 1780
    },
    {
      "epoch": 7.124,
      "grad_norm": 0.06251320242881775,
      "learning_rate": 5.887942747453301e-06,
      "loss": 0.351,
      "step": 1781
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.07377558946609497,
      "learning_rate": 5.83482228125285e-06,
      "loss": 0.3727,
      "step": 1782
    },
    {
      "epoch": 7.132,
      "grad_norm": 0.07212752848863602,
      "learning_rate": 5.78193532435457e-06,
      "loss": 0.346,
      "step": 1783
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.07325083762407303,
      "learning_rate": 5.729282007906678e-06,
      "loss": 0.3376,
      "step": 1784
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.04564272239804268,
      "learning_rate": 5.6768624624780605e-06,
      "loss": 0.237,
      "step": 1785
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.08803283423185349,
      "learning_rate": 5.624676818057861e-06,
      "loss": 0.4225,
      "step": 1786
    },
    {
      "epoch": 7.148,
      "grad_norm": 0.08994084596633911,
      "learning_rate": 5.572725204055174e-06,
      "loss": 0.4247,
      "step": 1787
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.06256765127182007,
      "learning_rate": 5.521007749298812e-06,
      "loss": 0.3232,
      "step": 1788
    },
    {
      "epoch": 7.156,
      "grad_norm": 0.07852304726839066,
      "learning_rate": 5.469524582036889e-06,
      "loss": 0.3563,
      "step": 1789
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.07447364926338196,
      "learning_rate": 5.418275829936537e-06,
      "loss": 0.3589,
      "step": 1790
    },
    {
      "epoch": 7.164,
      "grad_norm": 0.08206014335155487,
      "learning_rate": 5.367261620083575e-06,
      "loss": 0.3923,
      "step": 1791
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.08789362758398056,
      "learning_rate": 5.316482078982265e-06,
      "loss": 0.463,
      "step": 1792
    },
    {
      "epoch": 7.172,
      "grad_norm": 0.057730626314878464,
      "learning_rate": 5.265937332554849e-06,
      "loss": 0.2639,
      "step": 1793
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.06546614319086075,
      "learning_rate": 5.215627506141429e-06,
      "loss": 0.3314,
      "step": 1794
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.07147084176540375,
      "learning_rate": 5.1655527244994786e-06,
      "loss": 0.3205,
      "step": 1795
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.062464598566293716,
      "learning_rate": 5.115713111803655e-06,
      "loss": 0.3281,
      "step": 1796
    },
    {
      "epoch": 7.188,
      "grad_norm": 0.07677552103996277,
      "learning_rate": 5.066108791645408e-06,
      "loss": 0.3745,
      "step": 1797
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.07207349687814713,
      "learning_rate": 5.016739887032773e-06,
      "loss": 0.4037,
      "step": 1798
    },
    {
      "epoch": 7.196,
      "grad_norm": 0.0747726708650589,
      "learning_rate": 4.967606520389956e-06,
      "loss": 0.3575,
      "step": 1799
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.062127500772476196,
      "learning_rate": 4.918708813557093e-06,
      "loss": 0.3102,
      "step": 1800
    },
    {
      "epoch": 7.204,
      "grad_norm": 0.07861756533384323,
      "learning_rate": 4.8700468877899255e-06,
      "loss": 0.3533,
      "step": 1801
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.067652128636837,
      "learning_rate": 4.821620863759535e-06,
      "loss": 0.372,
      "step": 1802
    },
    {
      "epoch": 7.212,
      "grad_norm": 0.07845635712146759,
      "learning_rate": 4.773430861551998e-06,
      "loss": 0.3896,
      "step": 1803
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.06399240344762802,
      "learning_rate": 4.72547700066811e-06,
      "loss": 0.3329,
      "step": 1804
    },
    {
      "epoch": 7.22,
      "grad_norm": 0.08145828545093536,
      "learning_rate": 4.6777594000230855e-06,
      "loss": 0.4126,
      "step": 1805
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.07619790732860565,
      "learning_rate": 4.630278177946256e-06,
      "loss": 0.3692,
      "step": 1806
    },
    {
      "epoch": 7.228,
      "grad_norm": 0.0791427344083786,
      "learning_rate": 4.5830334521808185e-06,
      "loss": 0.3706,
      "step": 1807
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.059781625866889954,
      "learning_rate": 4.5360253398834765e-06,
      "loss": 0.2848,
      "step": 1808
    },
    {
      "epoch": 7.236,
      "grad_norm": 0.09802533686161041,
      "learning_rate": 4.489253957624218e-06,
      "loss": 0.4061,
      "step": 1809
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.0715552419424057,
      "learning_rate": 4.442719421385922e-06,
      "loss": 0.3632,
      "step": 1810
    },
    {
      "epoch": 7.244,
      "grad_norm": 0.06634201109409332,
      "learning_rate": 4.3964218465642355e-06,
      "loss": 0.338,
      "step": 1811
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.08145808428525925,
      "learning_rate": 4.350361347967125e-06,
      "loss": 0.3252,
      "step": 1812
    },
    {
      "epoch": 7.252,
      "grad_norm": 0.06900899857282639,
      "learning_rate": 4.304538039814676e-06,
      "loss": 0.3619,
      "step": 1813
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.07723107933998108,
      "learning_rate": 4.258952035738784e-06,
      "loss": 0.322,
      "step": 1814
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.07132524996995926,
      "learning_rate": 4.2136034487829325e-06,
      "loss": 0.3571,
      "step": 1815
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.06079550459980965,
      "learning_rate": 4.168492391401768e-06,
      "loss": 0.3381,
      "step": 1816
    },
    {
      "epoch": 7.268,
      "grad_norm": 0.06596238166093826,
      "learning_rate": 4.1236189754610075e-06,
      "loss": 0.3079,
      "step": 1817
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.05965283513069153,
      "learning_rate": 4.078983312237017e-06,
      "loss": 0.315,
      "step": 1818
    },
    {
      "epoch": 7.276,
      "grad_norm": 0.07164861261844635,
      "learning_rate": 4.034585512416611e-06,
      "loss": 0.3253,
      "step": 1819
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.07751236855983734,
      "learning_rate": 3.990425686096744e-06,
      "loss": 0.3675,
      "step": 1820
    },
    {
      "epoch": 7.284,
      "grad_norm": 0.06993449479341507,
      "learning_rate": 3.94650394278423e-06,
      "loss": 0.3845,
      "step": 1821
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.06768534332513809,
      "learning_rate": 3.902820391395523e-06,
      "loss": 0.3376,
      "step": 1822
    },
    {
      "epoch": 7.292,
      "grad_norm": 0.06748611479997635,
      "learning_rate": 3.859375140256371e-06,
      "loss": 0.3359,
      "step": 1823
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.05313417688012123,
      "learning_rate": 3.816168297101641e-06,
      "loss": 0.2779,
      "step": 1824
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.07111960649490356,
      "learning_rate": 3.7731999690749585e-06,
      "loss": 0.3601,
      "step": 1825
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.06752369552850723,
      "learning_rate": 3.7304702627285136e-06,
      "loss": 0.3132,
      "step": 1826
    },
    {
      "epoch": 7.308,
      "grad_norm": 0.06720174103975296,
      "learning_rate": 3.6879792840227135e-06,
      "loss": 0.3146,
      "step": 1827
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.08172621577978134,
      "learning_rate": 3.6457271383260383e-06,
      "loss": 0.3904,
      "step": 1828
    },
    {
      "epoch": 7.316,
      "grad_norm": 0.062441565096378326,
      "learning_rate": 3.6037139304146762e-06,
      "loss": 0.3292,
      "step": 1829
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.08191082626581192,
      "learning_rate": 3.5619397644722996e-06,
      "loss": 0.3847,
      "step": 1830
    },
    {
      "epoch": 7.324,
      "grad_norm": 0.06062719598412514,
      "learning_rate": 3.5204047440898224e-06,
      "loss": 0.3075,
      "step": 1831
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.07620499283075333,
      "learning_rate": 3.4791089722651436e-06,
      "loss": 0.3493,
      "step": 1832
    },
    {
      "epoch": 7.332,
      "grad_norm": 0.07255496084690094,
      "learning_rate": 3.438052551402815e-06,
      "loss": 0.3414,
      "step": 1833
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.06985880434513092,
      "learning_rate": 3.397235583313929e-06,
      "loss": 0.3372,
      "step": 1834
    },
    {
      "epoch": 7.34,
      "grad_norm": 0.0566701665520668,
      "learning_rate": 3.356658169215743e-06,
      "loss": 0.3169,
      "step": 1835
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.08010200411081314,
      "learning_rate": 3.316320409731477e-06,
      "loss": 0.3754,
      "step": 1836
    },
    {
      "epoch": 7.348,
      "grad_norm": 0.06793341040611267,
      "learning_rate": 3.2762224048900394e-06,
      "loss": 0.28,
      "step": 1837
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.05893740803003311,
      "learning_rate": 3.2363642541258676e-06,
      "loss": 0.3045,
      "step": 1838
    },
    {
      "epoch": 7.356,
      "grad_norm": 0.06669848412275314,
      "learning_rate": 3.1967460562785324e-06,
      "loss": 0.3495,
      "step": 1839
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.08187303692102432,
      "learning_rate": 3.157367909592601e-06,
      "loss": 0.3771,
      "step": 1840
    },
    {
      "epoch": 7.364,
      "grad_norm": 0.060829345136880875,
      "learning_rate": 3.1182299117174075e-06,
      "loss": 0.3036,
      "step": 1841
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.07106009125709534,
      "learning_rate": 3.0793321597067158e-06,
      "loss": 0.3568,
      "step": 1842
    },
    {
      "epoch": 7.372,
      "grad_norm": 0.06225648149847984,
      "learning_rate": 3.0406747500185353e-06,
      "loss": 0.3284,
      "step": 1843
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.081173837184906,
      "learning_rate": 3.0022577785149054e-06,
      "loss": 0.362,
      "step": 1844
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.07483567297458649,
      "learning_rate": 2.964081340461633e-06,
      "loss": 0.3524,
      "step": 1845
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.06228918209671974,
      "learning_rate": 2.926145530528002e-06,
      "loss": 0.333,
      "step": 1846
    },
    {
      "epoch": 7.388,
      "grad_norm": 0.06740439683198929,
      "learning_rate": 2.88845044278665e-06,
      "loss": 0.3743,
      "step": 1847
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.06567607074975967,
      "learning_rate": 2.8509961707132494e-06,
      "loss": 0.3351,
      "step": 1848
    },
    {
      "epoch": 7.396,
      "grad_norm": 0.08061040192842484,
      "learning_rate": 2.813782807186327e-06,
      "loss": 0.4088,
      "step": 1849
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.0595996230840683,
      "learning_rate": 2.7768104444869436e-06,
      "loss": 0.3512,
      "step": 1850
    },
    {
      "epoch": 7.404,
      "grad_norm": 0.0651741772890091,
      "learning_rate": 2.7400791742986044e-06,
      "loss": 0.325,
      "step": 1851
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.07464338839054108,
      "learning_rate": 2.703589087706926e-06,
      "loss": 0.3615,
      "step": 1852
    },
    {
      "epoch": 7.412,
      "grad_norm": 0.08318338543176651,
      "learning_rate": 2.667340275199426e-06,
      "loss": 0.3453,
      "step": 1853
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.0817970409989357,
      "learning_rate": 2.6313328266653228e-06,
      "loss": 0.3682,
      "step": 1854
    },
    {
      "epoch": 7.42,
      "grad_norm": 0.06139272078871727,
      "learning_rate": 2.595566831395346e-06,
      "loss": 0.3051,
      "step": 1855
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.057693805545568466,
      "learning_rate": 2.5600423780814175e-06,
      "loss": 0.3026,
      "step": 1856
    },
    {
      "epoch": 7.428,
      "grad_norm": 0.07005029916763306,
      "learning_rate": 2.5247595548165025e-06,
      "loss": 0.3103,
      "step": 1857
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.07066329568624496,
      "learning_rate": 2.48971844909438e-06,
      "loss": 0.3552,
      "step": 1858
    },
    {
      "epoch": 7.436,
      "grad_norm": 0.06327655166387558,
      "learning_rate": 2.4549191478094536e-06,
      "loss": 0.3099,
      "step": 1859
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.061146486550569534,
      "learning_rate": 2.420361737256438e-06,
      "loss": 0.2941,
      "step": 1860
    },
    {
      "epoch": 7.444,
      "grad_norm": 0.061484258621931076,
      "learning_rate": 2.3860463031302627e-06,
      "loss": 0.291,
      "step": 1861
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.0758381187915802,
      "learning_rate": 2.3519729305258143e-06,
      "loss": 0.3727,
      "step": 1862
    },
    {
      "epoch": 7.452,
      "grad_norm": 0.06665965169668198,
      "learning_rate": 2.3181417039376486e-06,
      "loss": 0.3535,
      "step": 1863
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.06582595407962799,
      "learning_rate": 2.284552707259946e-06,
      "loss": 0.3461,
      "step": 1864
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.06546159088611603,
      "learning_rate": 2.2512060237861453e-06,
      "loss": 0.3453,
      "step": 1865
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.06596999615430832,
      "learning_rate": 2.218101736208811e-06,
      "loss": 0.3656,
      "step": 1866
    },
    {
      "epoch": 7.468,
      "grad_norm": 0.06697241216897964,
      "learning_rate": 2.1852399266194314e-06,
      "loss": 0.2955,
      "step": 1867
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.06568899750709534,
      "learning_rate": 2.15262067650821e-06,
      "loss": 0.3346,
      "step": 1868
    },
    {
      "epoch": 7.476,
      "grad_norm": 0.055133722722530365,
      "learning_rate": 2.1202440667638323e-06,
      "loss": 0.2767,
      "step": 1869
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.07846606522798538,
      "learning_rate": 2.088110177673297e-06,
      "loss": 0.3723,
      "step": 1870
    },
    {
      "epoch": 7.484,
      "grad_norm": 0.07006467878818512,
      "learning_rate": 2.0562190889216846e-06,
      "loss": 0.3537,
      "step": 1871
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.07164550572633743,
      "learning_rate": 2.0245708795920582e-06,
      "loss": 0.3446,
      "step": 1872
    },
    {
      "epoch": 7.492,
      "grad_norm": 0.06200874596834183,
      "learning_rate": 1.993165628165106e-06,
      "loss": 0.259,
      "step": 1873
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.07168109714984894,
      "learning_rate": 1.9620034125190644e-06,
      "loss": 0.3707,
      "step": 1874
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.07329561561346054,
      "learning_rate": 1.9310843099295206e-06,
      "loss": 0.3223,
      "step": 1875
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.07544166594743729,
      "learning_rate": 1.9004083970691534e-06,
      "loss": 0.3702,
      "step": 1876
    },
    {
      "epoch": 7.508,
      "grad_norm": 0.06854665279388428,
      "learning_rate": 1.8699757500076131e-06,
      "loss": 0.3221,
      "step": 1877
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.06076663359999657,
      "learning_rate": 1.8397864442112666e-06,
      "loss": 0.2612,
      "step": 1878
    },
    {
      "epoch": 7.516,
      "grad_norm": 0.051650065928697586,
      "learning_rate": 1.8098405545431185e-06,
      "loss": 0.2621,
      "step": 1879
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.07537589222192764,
      "learning_rate": 1.7801381552624563e-06,
      "loss": 0.3205,
      "step": 1880
    },
    {
      "epoch": 7.524,
      "grad_norm": 0.0754336267709732,
      "learning_rate": 1.7506793200248506e-06,
      "loss": 0.3722,
      "step": 1881
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.07410530745983124,
      "learning_rate": 1.7214641218818328e-06,
      "loss": 0.38,
      "step": 1882
    },
    {
      "epoch": 7.532,
      "grad_norm": 0.0812942236661911,
      "learning_rate": 1.6924926332807956e-06,
      "loss": 0.3801,
      "step": 1883
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.06107766181230545,
      "learning_rate": 1.6637649260647481e-06,
      "loss": 0.3243,
      "step": 1884
    },
    {
      "epoch": 7.54,
      "grad_norm": 0.06728611886501312,
      "learning_rate": 1.635281071472239e-06,
      "loss": 0.3197,
      "step": 1885
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.06409554183483124,
      "learning_rate": 1.6070411401370334e-06,
      "loss": 0.3051,
      "step": 1886
    },
    {
      "epoch": 7.548,
      "grad_norm": 0.05423951521515846,
      "learning_rate": 1.579045202088092e-06,
      "loss": 0.2759,
      "step": 1887
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.07022702693939209,
      "learning_rate": 1.5512933267492813e-06,
      "loss": 0.3255,
      "step": 1888
    },
    {
      "epoch": 7.556,
      "grad_norm": 0.06552917510271072,
      "learning_rate": 1.5237855829392743e-06,
      "loss": 0.3627,
      "step": 1889
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.07801121473312378,
      "learning_rate": 1.4965220388712952e-06,
      "loss": 0.392,
      "step": 1890
    },
    {
      "epoch": 7.564,
      "grad_norm": 0.06923173367977142,
      "learning_rate": 1.4695027621530855e-06,
      "loss": 0.3387,
      "step": 1891
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.06187652051448822,
      "learning_rate": 1.4427278197866045e-06,
      "loss": 0.2925,
      "step": 1892
    },
    {
      "epoch": 7.572,
      "grad_norm": 0.07016947865486145,
      "learning_rate": 1.4161972781679077e-06,
      "loss": 0.3468,
      "step": 1893
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.054087795317173004,
      "learning_rate": 1.3899112030870353e-06,
      "loss": 0.2475,
      "step": 1894
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.06490921229124069,
      "learning_rate": 1.3638696597277679e-06,
      "loss": 0.3313,
      "step": 1895
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.061681877821683884,
      "learning_rate": 1.338072712667493e-06,
      "loss": 0.311,
      "step": 1896
    },
    {
      "epoch": 7.588,
      "grad_norm": 0.05346602946519852,
      "learning_rate": 1.312520425877073e-06,
      "loss": 0.2687,
      "step": 1897
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.06825944036245346,
      "learning_rate": 1.287212862720677e-06,
      "loss": 0.2772,
      "step": 1898
    },
    {
      "epoch": 7.596,
      "grad_norm": 0.06519757956266403,
      "learning_rate": 1.2621500859555824e-06,
      "loss": 0.3668,
      "step": 1899
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.07117590308189392,
      "learning_rate": 1.237332157732063e-06,
      "loss": 0.3137,
      "step": 1900
    },
    {
      "epoch": 7.604,
      "grad_norm": 0.07750191539525986,
      "learning_rate": 1.2127591395932114e-06,
      "loss": 0.3992,
      "step": 1901
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.07349538803100586,
      "learning_rate": 1.18843109247484e-06,
      "loss": 0.3584,
      "step": 1902
    },
    {
      "epoch": 7.612,
      "grad_norm": 0.06833881139755249,
      "learning_rate": 1.1643480767052017e-06,
      "loss": 0.3399,
      "step": 1903
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.0903027132153511,
      "learning_rate": 1.1405101520050031e-06,
      "loss": 0.3848,
      "step": 1904
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.07938597351312637,
      "learning_rate": 1.1169173774871478e-06,
      "loss": 0.3695,
      "step": 1905
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.07481728494167328,
      "learning_rate": 1.093569811656614e-06,
      "loss": 0.2976,
      "step": 1906
    },
    {
      "epoch": 7.628,
      "grad_norm": 0.06375423818826675,
      "learning_rate": 1.0704675124103225e-06,
      "loss": 0.3431,
      "step": 1907
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.08366649597883224,
      "learning_rate": 1.0476105370370026e-06,
      "loss": 0.3283,
      "step": 1908
    },
    {
      "epoch": 7.636,
      "grad_norm": 0.06796419620513916,
      "learning_rate": 1.0249989422169926e-06,
      "loss": 0.3901,
      "step": 1909
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.08193787187337875,
      "learning_rate": 1.0026327840221728e-06,
      "loss": 0.3843,
      "step": 1910
    },
    {
      "epoch": 7.644,
      "grad_norm": 0.0683007463812828,
      "learning_rate": 9.805121179157773e-07,
      "loss": 0.3384,
      "step": 1911
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.07236582040786743,
      "learning_rate": 9.586369987522936e-07,
      "loss": 0.3309,
      "step": 1912
    },
    {
      "epoch": 7.652,
      "grad_norm": 0.05704088881611824,
      "learning_rate": 9.370074807772966e-07,
      "loss": 0.3284,
      "step": 1913
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.07402947545051575,
      "learning_rate": 9.156236176272926e-07,
      "loss": 0.3667,
      "step": 1914
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.081264927983284,
      "learning_rate": 8.94485462329675e-07,
      "loss": 0.3742,
      "step": 1915
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.06359032541513443,
      "learning_rate": 8.735930673024806e-07,
      "loss": 0.3185,
      "step": 1916
    },
    {
      "epoch": 7.668,
      "grad_norm": 0.053948692977428436,
      "learning_rate": 8.529464843543445e-07,
      "loss": 0.2633,
      "step": 1917
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.06578071415424347,
      "learning_rate": 8.325457646843338e-07,
      "loss": 0.3513,
      "step": 1918
    },
    {
      "epoch": 7.676,
      "grad_norm": 0.0792125016450882,
      "learning_rate": 8.123909588818368e-07,
      "loss": 0.416,
      "step": 1919
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.07267042994499207,
      "learning_rate": 7.924821169263963e-07,
      "loss": 0.3407,
      "step": 1920
    },
    {
      "epoch": 7.684,
      "grad_norm": 0.06015534698963165,
      "learning_rate": 7.728192881876539e-07,
      "loss": 0.2744,
      "step": 1921
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.08377975225448608,
      "learning_rate": 7.534025214251839e-07,
      "loss": 0.3906,
      "step": 1922
    },
    {
      "epoch": 7.692,
      "grad_norm": 0.06472513824701309,
      "learning_rate": 7.342318647883595e-07,
      "loss": 0.3365,
      "step": 1923
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.06815971434116364,
      "learning_rate": 7.153073658162646e-07,
      "loss": 0.3528,
      "step": 1924
    },
    {
      "epoch": 7.7,
      "grad_norm": 0.06883665174245834,
      "learning_rate": 6.966290714375933e-07,
      "loss": 0.34,
      "step": 1925
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.07027025520801544,
      "learning_rate": 6.78197027970473e-07,
      "loss": 0.3554,
      "step": 1926
    },
    {
      "epoch": 7.708,
      "grad_norm": 0.0836653932929039,
      "learning_rate": 6.600112811223746e-07,
      "loss": 0.3606,
      "step": 1927
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.06948678195476532,
      "learning_rate": 6.420718759900357e-07,
      "loss": 0.3119,
      "step": 1928
    },
    {
      "epoch": 7.716,
      "grad_norm": 0.07325448095798492,
      "learning_rate": 6.243788570593046e-07,
      "loss": 0.3684,
      "step": 1929
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.07235638797283173,
      "learning_rate": 6.069322682050516e-07,
      "loss": 0.3151,
      "step": 1930
    },
    {
      "epoch": 7.724,
      "grad_norm": 0.05840388312935829,
      "learning_rate": 5.897321526910359e-07,
      "loss": 0.3009,
      "step": 1931
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.07496359944343567,
      "learning_rate": 5.727785531698504e-07,
      "loss": 0.3423,
      "step": 1932
    },
    {
      "epoch": 7.732,
      "grad_norm": 0.07188687473535538,
      "learning_rate": 5.56071511682732e-07,
      "loss": 0.3398,
      "step": 1933
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.06605696678161621,
      "learning_rate": 5.396110696595513e-07,
      "loss": 0.2562,
      "step": 1934
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.06665078550577164,
      "learning_rate": 5.233972679186461e-07,
      "loss": 0.3569,
      "step": 1935
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.08354675024747849,
      "learning_rate": 5.07430146666743e-07,
      "loss": 0.3631,
      "step": 1936
    },
    {
      "epoch": 7.748,
      "grad_norm": 0.0734817385673523,
      "learning_rate": 4.917097454988584e-07,
      "loss": 0.3811,
      "step": 1937
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.06457681208848953,
      "learning_rate": 4.7623610339818657e-07,
      "loss": 0.3143,
      "step": 1938
    },
    {
      "epoch": 7.756,
      "grad_norm": 0.07811103761196136,
      "learning_rate": 4.6100925873602264e-07,
      "loss": 0.4145,
      "step": 1939
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.0860779881477356,
      "learning_rate": 4.4602924927165115e-07,
      "loss": 0.3681,
      "step": 1940
    },
    {
      "epoch": 7.764,
      "grad_norm": 0.08122915029525757,
      "learning_rate": 4.312961121522463e-07,
      "loss": 0.3709,
      "step": 1941
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.08113864809274673,
      "learning_rate": 4.168098839128276e-07,
      "loss": 0.3695,
      "step": 1942
    },
    {
      "epoch": 7.772,
      "grad_norm": 0.06838919222354889,
      "learning_rate": 4.025706004760932e-07,
      "loss": 0.3312,
      "step": 1943
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.06717263162136078,
      "learning_rate": 3.885782971524088e-07,
      "loss": 0.315,
      "step": 1944
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.06548846513032913,
      "learning_rate": 3.748330086396523e-07,
      "loss": 0.3392,
      "step": 1945
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.062049347907304764,
      "learning_rate": 3.6133476902318055e-07,
      "loss": 0.2816,
      "step": 1946
    },
    {
      "epoch": 7.788,
      "grad_norm": 0.07655414938926697,
      "learning_rate": 3.4808361177569583e-07,
      "loss": 0.3868,
      "step": 1947
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.0578605979681015,
      "learning_rate": 3.3507956975721286e-07,
      "loss": 0.2786,
      "step": 1948
    },
    {
      "epoch": 7.796,
      "grad_norm": 0.054905738681554794,
      "learning_rate": 3.2232267521495886e-07,
      "loss": 0.2765,
      "step": 1949
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.0773906260728836,
      "learning_rate": 3.098129597832622e-07,
      "loss": 0.3748,
      "step": 1950
    },
    {
      "epoch": 7.804,
      "grad_norm": 0.08393339812755585,
      "learning_rate": 2.9755045448351946e-07,
      "loss": 0.3887,
      "step": 1951
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.07995673269033432,
      "learning_rate": 2.855351897241065e-07,
      "loss": 0.3981,
      "step": 1952
    },
    {
      "epoch": 7.812,
      "grad_norm": 0.07307682931423187,
      "learning_rate": 2.737671953002674e-07,
      "loss": 0.4091,
      "step": 1953
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.07312943041324615,
      "learning_rate": 2.6224650039409216e-07,
      "loss": 0.3759,
      "step": 1954
    },
    {
      "epoch": 7.82,
      "grad_norm": 0.06602239608764648,
      "learning_rate": 2.509731335744281e-07,
      "loss": 0.3272,
      "step": 1955
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.06818355619907379,
      "learning_rate": 2.3994712279676865e-07,
      "loss": 0.359,
      "step": 1956
    },
    {
      "epoch": 7.828,
      "grad_norm": 0.08324744552373886,
      "learning_rate": 2.291684954032536e-07,
      "loss": 0.4233,
      "step": 1957
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.0775933489203453,
      "learning_rate": 2.1863727812254653e-07,
      "loss": 0.3996,
      "step": 1958
    },
    {
      "epoch": 7.836,
      "grad_norm": 0.07101733982563019,
      "learning_rate": 2.083534970697909e-07,
      "loss": 0.3323,
      "step": 1959
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.06929664313793182,
      "learning_rate": 1.9831717774654312e-07,
      "loss": 0.3189,
      "step": 1960
    },
    {
      "epoch": 7.844,
      "grad_norm": 0.07972182333469391,
      "learning_rate": 1.8852834504071715e-07,
      "loss": 0.3993,
      "step": 1961
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.0705813616514206,
      "learning_rate": 1.7898702322648453e-07,
      "loss": 0.3484,
      "step": 1962
    },
    {
      "epoch": 7.852,
      "grad_norm": 0.08430343121290207,
      "learning_rate": 1.6969323596427445e-07,
      "loss": 0.3761,
      "step": 1963
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.059567105025053024,
      "learning_rate": 1.6064700630067376e-07,
      "loss": 0.3383,
      "step": 1964
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.06536858528852463,
      "learning_rate": 1.518483566683826e-07,
      "loss": 0.2963,
      "step": 1965
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.07003776729106903,
      "learning_rate": 1.432973088861367e-07,
      "loss": 0.2817,
      "step": 1966
    },
    {
      "epoch": 7.868,
      "grad_norm": 0.06928195804357529,
      "learning_rate": 1.3499388415868509e-07,
      "loss": 0.3517,
      "step": 1967
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.06621057540178299,
      "learning_rate": 1.2693810307674582e-07,
      "loss": 0.2873,
      "step": 1968
    },
    {
      "epoch": 7.876,
      "grad_norm": 0.06362056732177734,
      "learning_rate": 1.1912998561691701e-07,
      "loss": 0.307,
      "step": 1969
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.07245282828807831,
      "learning_rate": 1.1156955114162149e-07,
      "loss": 0.3456,
      "step": 1970
    },
    {
      "epoch": 7.884,
      "grad_norm": 0.0819115936756134,
      "learning_rate": 1.0425681839911772e-07,
      "loss": 0.4337,
      "step": 1971
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.06448531150817871,
      "learning_rate": 9.719180552341111e-08,
      "loss": 0.3296,
      "step": 1972
    },
    {
      "epoch": 7.892,
      "grad_norm": 0.09332754462957382,
      "learning_rate": 9.037453003418739e-08,
      "loss": 0.3934,
      "step": 1973
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.0596560537815094,
      "learning_rate": 8.380500883683473e-08,
      "loss": 0.3314,
      "step": 1974
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.05420389771461487,
      "learning_rate": 7.748325822234393e-08,
      "loss": 0.3084,
      "step": 1975
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.04742974042892456,
      "learning_rate": 7.140929386728612e-08,
      "loss": 0.2518,
      "step": 1976
    },
    {
      "epoch": 7.908,
      "grad_norm": 0.06001864746212959,
      "learning_rate": 6.558313083376843e-08,
      "loss": 0.3057,
      "step": 1977
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.055377520620822906,
      "learning_rate": 6.000478356944505e-08,
      "loss": 0.2728,
      "step": 1978
    },
    {
      "epoch": 7.916,
      "grad_norm": 0.06606284528970718,
      "learning_rate": 5.467426590739511e-08,
      "loss": 0.323,
      "step": 1979
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.06676243990659714,
      "learning_rate": 4.9591591066155963e-08,
      "loss": 0.3281,
      "step": 1980
    },
    {
      "epoch": 7.924,
      "grad_norm": 0.04986375942826271,
      "learning_rate": 4.475677164966774e-08,
      "loss": 0.2783,
      "step": 1981
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.07946966588497162,
      "learning_rate": 4.0169819647217776e-08,
      "loss": 0.3875,
      "step": 1982
    },
    {
      "epoch": 7.932,
      "grad_norm": 0.06895236670970917,
      "learning_rate": 3.583074643348505e-08,
      "loss": 0.3311,
      "step": 1983
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.06033845245838165,
      "learning_rate": 3.173956276840695e-08,
      "loss": 0.3159,
      "step": 1984
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 0.07175701856613159,
      "learning_rate": 2.789627879725698e-08,
      "loss": 0.3234,
      "step": 1985
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.07080655544996262,
      "learning_rate": 2.430090405054486e-08,
      "loss": 0.3645,
      "step": 1986
    },
    {
      "epoch": 7.948,
      "grad_norm": 0.07174613326787949,
      "learning_rate": 2.0953447444005403e-08,
      "loss": 0.3425,
      "step": 1987
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.0664987713098526,
      "learning_rate": 1.7853917278631838e-08,
      "loss": 0.3362,
      "step": 1988
    },
    {
      "epoch": 7.9559999999999995,
      "grad_norm": 0.06315221637487411,
      "learning_rate": 1.500232124057588e-08,
      "loss": 0.3435,
      "step": 1989
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.07629799097776413,
      "learning_rate": 1.2398666401181035e-08,
      "loss": 0.3548,
      "step": 1990
    },
    {
      "epoch": 7.964,
      "grad_norm": 0.07738031446933746,
      "learning_rate": 1.004295921694931e-08,
      "loss": 0.37,
      "step": 1991
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.07129336893558502,
      "learning_rate": 7.93520552954119e-09,
      "loss": 0.3358,
      "step": 1992
    },
    {
      "epoch": 7.9719999999999995,
      "grad_norm": 0.07827188819646835,
      "learning_rate": 6.075410565697937e-09,
      "loss": 0.3975,
      "step": 1993
    },
    {
      "epoch": 7.976,
      "grad_norm": 0.06525571644306183,
      "learning_rate": 4.463578937341506e-09,
      "loss": 0.322,
      "step": 1994
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.05965040996670723,
      "learning_rate": 3.0997146414524227e-09,
      "loss": 0.2944,
      "step": 1995
    },
    {
      "epoch": 7.984,
      "grad_norm": 0.06831296533346176,
      "learning_rate": 1.983821060114188e-09,
      "loss": 0.3274,
      "step": 1996
    },
    {
      "epoch": 7.9879999999999995,
      "grad_norm": 0.05743174999952316,
      "learning_rate": 1.1159009605132831e-09,
      "loss": 0.3558,
      "step": 1997
    },
    {
      "epoch": 7.992,
      "grad_norm": 0.07157175987958908,
      "learning_rate": 4.95956494894756e-10,
      "loss": 0.296,
      "step": 1998
    },
    {
      "epoch": 7.996,
      "grad_norm": 0.06333671510219574,
      "learning_rate": 1.2398920058442898e-10,
      "loss": 0.2787,
      "step": 1999
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.06210995465517044,
      "learning_rate": 0.0,
      "loss": 0.3361,
      "step": 2000
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.3207613229751587,
      "eval_runtime": 353.5579,
      "eval_samples_per_second": 0.283,
      "eval_steps_per_second": 0.283,
      "step": 2000
    },
    {
      "epoch": 0.7276363636363636,
      "grad_norm": 0.07399830222129822,
      "learning_rate": 0.0001968063259194223,
      "loss": 0.3148,
      "step": 2001
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.06877943873405457,
      "learning_rate": 0.0001968031422081398,
      "loss": 0.316,
      "step": 2002
    },
    {
      "epoch": 0.7283636363636363,
      "grad_norm": 0.08815081417560577,
      "learning_rate": 0.00019679995693653514,
      "loss": 0.3441,
      "step": 2003
    },
    {
      "epoch": 0.7287272727272728,
      "grad_norm": 0.09002398699522018,
      "learning_rate": 0.00019679677010465963,
      "loss": 0.2915,
      "step": 2004
    },
    {
      "epoch": 0.7290909090909091,
      "grad_norm": 0.09011247009038925,
      "learning_rate": 0.00019679358171256463,
      "loss": 0.3103,
      "step": 2005
    },
    {
      "epoch": 0.7294545454545455,
      "grad_norm": 0.08878981322050095,
      "learning_rate": 0.00019679039176030157,
      "loss": 0.3475,
      "step": 2006
    },
    {
      "epoch": 0.7298181818181818,
      "grad_norm": 0.08858906477689743,
      "learning_rate": 0.00019678720024792184,
      "loss": 0.3164,
      "step": 2007
    },
    {
      "epoch": 0.7301818181818182,
      "grad_norm": 0.08672398328781128,
      "learning_rate": 0.00019678400717547688,
      "loss": 0.37,
      "step": 2008
    },
    {
      "epoch": 0.7305454545454545,
      "grad_norm": 0.089852474629879,
      "learning_rate": 0.00019678081254301816,
      "loss": 0.3504,
      "step": 2009
    },
    {
      "epoch": 0.730909090909091,
      "grad_norm": 0.0875483974814415,
      "learning_rate": 0.0001967776163505972,
      "loss": 0.3824,
      "step": 2010
    },
    {
      "epoch": 0.7312727272727273,
      "grad_norm": 0.11072579026222229,
      "learning_rate": 0.00019677441859826546,
      "loss": 0.4068,
      "step": 2011
    },
    {
      "epoch": 0.7316363636363636,
      "grad_norm": 0.08814435452222824,
      "learning_rate": 0.00019677121928607453,
      "loss": 0.3381,
      "step": 2012
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.08495417982339859,
      "learning_rate": 0.00019676801841407598,
      "loss": 0.3071,
      "step": 2013
    },
    {
      "epoch": 0.7323636363636363,
      "grad_norm": 0.08962200582027435,
      "learning_rate": 0.00019676481598232139,
      "loss": 0.3757,
      "step": 2014
    },
    {
      "epoch": 0.7327272727272728,
      "grad_norm": 0.09510818868875504,
      "learning_rate": 0.00019676161199086236,
      "loss": 0.4063,
      "step": 2015
    },
    {
      "epoch": 0.7330909090909091,
      "grad_norm": 0.07222741097211838,
      "learning_rate": 0.00019675840643975056,
      "loss": 0.3224,
      "step": 2016
    },
    {
      "epoch": 0.7334545454545455,
      "grad_norm": 0.08765123039484024,
      "learning_rate": 0.00019675519932903766,
      "loss": 0.382,
      "step": 2017
    },
    {
      "epoch": 0.7338181818181818,
      "grad_norm": 0.07621904462575912,
      "learning_rate": 0.0001967519906587753,
      "loss": 0.3111,
      "step": 2018
    },
    {
      "epoch": 0.7341818181818182,
      "grad_norm": 0.07470082491636276,
      "learning_rate": 0.0001967487804290153,
      "loss": 0.39,
      "step": 2019
    },
    {
      "epoch": 0.7345454545454545,
      "grad_norm": 0.06854107230901718,
      "learning_rate": 0.00019674556863980933,
      "loss": 0.3808,
      "step": 2020
    },
    {
      "epoch": 0.734909090909091,
      "grad_norm": 0.08661975711584091,
      "learning_rate": 0.00019674235529120918,
      "loss": 0.3539,
      "step": 2021
    },
    {
      "epoch": 0.7352727272727273,
      "grad_norm": 0.07389165461063385,
      "learning_rate": 0.00019673914038326662,
      "loss": 0.2923,
      "step": 2022
    },
    {
      "epoch": 0.7356363636363636,
      "grad_norm": 0.06926336139440536,
      "learning_rate": 0.00019673592391603352,
      "loss": 0.3304,
      "step": 2023
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.07801894098520279,
      "learning_rate": 0.00019673270588956165,
      "loss": 0.351,
      "step": 2024
    },
    {
      "epoch": 0.7363636363636363,
      "grad_norm": 0.07612810283899307,
      "learning_rate": 0.00019672948630390294,
      "loss": 0.3183,
      "step": 2025
    },
    {
      "epoch": 0.7367272727272727,
      "grad_norm": 0.0784170925617218,
      "learning_rate": 0.0001967262651591093,
      "loss": 0.3252,
      "step": 2026
    },
    {
      "epoch": 0.7370909090909091,
      "grad_norm": 0.08152862638235092,
      "learning_rate": 0.00019672304245523257,
      "loss": 0.326,
      "step": 2027
    },
    {
      "epoch": 0.7374545454545455,
      "grad_norm": 0.10527444630861282,
      "learning_rate": 0.0001967198181923248,
      "loss": 0.3574,
      "step": 2028
    },
    {
      "epoch": 0.7378181818181818,
      "grad_norm": 0.07630555331707001,
      "learning_rate": 0.00019671659237043788,
      "loss": 0.3212,
      "step": 2029
    },
    {
      "epoch": 0.7381818181818182,
      "grad_norm": 0.07657580077648163,
      "learning_rate": 0.0001967133649896238,
      "loss": 0.3031,
      "step": 2030
    },
    {
      "epoch": 0.7385454545454545,
      "grad_norm": 0.079401396214962,
      "learning_rate": 0.00019671013604993464,
      "loss": 0.3456,
      "step": 2031
    },
    {
      "epoch": 0.738909090909091,
      "grad_norm": 0.07881339639425278,
      "learning_rate": 0.00019670690555142245,
      "loss": 0.2998,
      "step": 2032
    },
    {
      "epoch": 0.7392727272727273,
      "grad_norm": 0.0816805511713028,
      "learning_rate": 0.0001967036734941392,
      "loss": 0.3654,
      "step": 2033
    },
    {
      "epoch": 0.7396363636363636,
      "grad_norm": 0.0802745446562767,
      "learning_rate": 0.0001967004398781371,
      "loss": 0.4149,
      "step": 2034
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.07405239343643188,
      "learning_rate": 0.0001966972047034682,
      "loss": 0.3647,
      "step": 2035
    },
    {
      "epoch": 0.7403636363636363,
      "grad_norm": 0.09022506326436996,
      "learning_rate": 0.0001966939679701847,
      "loss": 0.4069,
      "step": 2036
    },
    {
      "epoch": 0.7407272727272727,
      "grad_norm": 0.0871080607175827,
      "learning_rate": 0.00019669072967833868,
      "loss": 0.3513,
      "step": 2037
    },
    {
      "epoch": 0.7410909090909091,
      "grad_norm": 0.0663779228925705,
      "learning_rate": 0.00019668748982798245,
      "loss": 0.328,
      "step": 2038
    },
    {
      "epoch": 0.7414545454545455,
      "grad_norm": 0.10409333556890488,
      "learning_rate": 0.00019668424841916816,
      "loss": 0.3067,
      "step": 2039
    },
    {
      "epoch": 0.7418181818181818,
      "grad_norm": 0.0822017639875412,
      "learning_rate": 0.0001966810054519481,
      "loss": 0.3679,
      "step": 2040
    },
    {
      "epoch": 0.7421818181818182,
      "grad_norm": 0.0935727208852768,
      "learning_rate": 0.00019667776092637448,
      "loss": 0.4094,
      "step": 2041
    },
    {
      "epoch": 0.7425454545454545,
      "grad_norm": 0.07750237733125687,
      "learning_rate": 0.00019667451484249967,
      "loss": 0.3227,
      "step": 2042
    },
    {
      "epoch": 0.742909090909091,
      "grad_norm": 0.10356579720973969,
      "learning_rate": 0.00019667126720037596,
      "loss": 0.3511,
      "step": 2043
    },
    {
      "epoch": 0.7432727272727273,
      "grad_norm": 0.10536635667085648,
      "learning_rate": 0.00019666801800005568,
      "loss": 0.4038,
      "step": 2044
    },
    {
      "epoch": 0.7436363636363637,
      "grad_norm": 0.09724612534046173,
      "learning_rate": 0.0001966647672415912,
      "loss": 0.3731,
      "step": 2045
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.0997684895992279,
      "learning_rate": 0.00019666151492503495,
      "loss": 0.3792,
      "step": 2046
    },
    {
      "epoch": 0.7443636363636363,
      "grad_norm": 0.07543099671602249,
      "learning_rate": 0.00019665826105043938,
      "loss": 0.3544,
      "step": 2047
    },
    {
      "epoch": 0.7447272727272727,
      "grad_norm": 0.07697667926549911,
      "learning_rate": 0.00019665500561785683,
      "loss": 0.2918,
      "step": 2048
    },
    {
      "epoch": 0.7450909090909091,
      "grad_norm": 0.08058688044548035,
      "learning_rate": 0.00019665174862733986,
      "loss": 0.3761,
      "step": 2049
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.10589607805013657,
      "learning_rate": 0.00019664849007894092,
      "loss": 0.3859,
      "step": 2050
    },
    {
      "epoch": 0.7458181818181818,
      "grad_norm": 0.08040358871221542,
      "learning_rate": 0.0001966452299727126,
      "loss": 0.4138,
      "step": 2051
    },
    {
      "epoch": 0.7461818181818182,
      "grad_norm": 0.07431122660636902,
      "learning_rate": 0.0001966419683087074,
      "loss": 0.3089,
      "step": 2052
    },
    {
      "epoch": 0.7465454545454545,
      "grad_norm": 0.09903336316347122,
      "learning_rate": 0.00019663870508697788,
      "loss": 0.3409,
      "step": 2053
    },
    {
      "epoch": 0.7469090909090909,
      "grad_norm": 0.0772448256611824,
      "learning_rate": 0.00019663544030757666,
      "loss": 0.2609,
      "step": 2054
    },
    {
      "epoch": 0.7472727272727273,
      "grad_norm": 0.11322501301765442,
      "learning_rate": 0.00019663217397055633,
      "loss": 0.4018,
      "step": 2055
    },
    {
      "epoch": 0.7476363636363637,
      "grad_norm": 0.06464797258377075,
      "learning_rate": 0.0001966289060759696,
      "loss": 0.3416,
      "step": 2056
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.07861793786287308,
      "learning_rate": 0.00019662563662386915,
      "loss": 0.3556,
      "step": 2057
    },
    {
      "epoch": 0.7483636363636363,
      "grad_norm": 0.11823953688144684,
      "learning_rate": 0.00019662236561430758,
      "loss": 0.3077,
      "step": 2058
    },
    {
      "epoch": 0.7487272727272727,
      "grad_norm": 0.11063271760940552,
      "learning_rate": 0.00019661909304733772,
      "loss": 0.3702,
      "step": 2059
    },
    {
      "epoch": 0.7490909090909091,
      "grad_norm": 0.05965341255068779,
      "learning_rate": 0.00019661581892301225,
      "loss": 0.3253,
      "step": 2060
    },
    {
      "epoch": 0.7494545454545455,
      "grad_norm": 0.08086973428726196,
      "learning_rate": 0.00019661254324138396,
      "loss": 0.3322,
      "step": 2061
    },
    {
      "epoch": 0.7498181818181818,
      "grad_norm": 0.09951900690793991,
      "learning_rate": 0.00019660926600250565,
      "loss": 0.4021,
      "step": 2062
    },
    {
      "epoch": 0.7501818181818182,
      "grad_norm": 0.06967166066169739,
      "learning_rate": 0.0001966059872064302,
      "loss": 0.3529,
      "step": 2063
    },
    {
      "epoch": 0.7505454545454545,
      "grad_norm": 0.10061757266521454,
      "learning_rate": 0.00019660270685321036,
      "loss": 0.4032,
      "step": 2064
    },
    {
      "epoch": 0.7509090909090909,
      "grad_norm": 0.08282122015953064,
      "learning_rate": 0.00019659942494289906,
      "loss": 0.3327,
      "step": 2065
    },
    {
      "epoch": 0.7512727272727273,
      "grad_norm": 0.08180582523345947,
      "learning_rate": 0.00019659614147554924,
      "loss": 0.3363,
      "step": 2066
    },
    {
      "epoch": 0.7516363636363637,
      "grad_norm": 0.07017682492733002,
      "learning_rate": 0.00019659285645121376,
      "loss": 0.3937,
      "step": 2067
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.07883808016777039,
      "learning_rate": 0.00019658956986994556,
      "loss": 0.353,
      "step": 2068
    },
    {
      "epoch": 0.7523636363636363,
      "grad_norm": 0.0866013690829277,
      "learning_rate": 0.00019658628173179765,
      "loss": 0.4013,
      "step": 2069
    },
    {
      "epoch": 0.7527272727272727,
      "grad_norm": 0.11631206423044205,
      "learning_rate": 0.00019658299203682305,
      "loss": 0.304,
      "step": 2070
    },
    {
      "epoch": 0.7530909090909091,
      "grad_norm": 0.07074842602014542,
      "learning_rate": 0.00019657970078507472,
      "loss": 0.3775,
      "step": 2071
    },
    {
      "epoch": 0.7534545454545455,
      "grad_norm": 0.07833711057901382,
      "learning_rate": 0.0001965764079766058,
      "loss": 0.271,
      "step": 2072
    },
    {
      "epoch": 0.7538181818181818,
      "grad_norm": 0.05952753871679306,
      "learning_rate": 0.00019657311361146926,
      "loss": 0.3108,
      "step": 2073
    },
    {
      "epoch": 0.7541818181818182,
      "grad_norm": 0.06720634549856186,
      "learning_rate": 0.00019656981768971827,
      "loss": 0.2878,
      "step": 2074
    },
    {
      "epoch": 0.7545454545454545,
      "grad_norm": 0.09381549060344696,
      "learning_rate": 0.00019656652021140592,
      "loss": 0.4115,
      "step": 2075
    },
    {
      "epoch": 0.7549090909090909,
      "grad_norm": 0.07144463062286377,
      "learning_rate": 0.00019656322117658542,
      "loss": 0.3323,
      "step": 2076
    },
    {
      "epoch": 0.7552727272727273,
      "grad_norm": 0.07878205180168152,
      "learning_rate": 0.00019655992058530993,
      "loss": 0.3589,
      "step": 2077
    },
    {
      "epoch": 0.7556363636363637,
      "grad_norm": 0.08315595984458923,
      "learning_rate": 0.00019655661843763259,
      "loss": 0.4065,
      "step": 2078
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.07347273826599121,
      "learning_rate": 0.00019655331473360664,
      "loss": 0.364,
      "step": 2079
    },
    {
      "epoch": 0.7563636363636363,
      "grad_norm": 0.07726313173770905,
      "learning_rate": 0.00019655000947328536,
      "loss": 0.3215,
      "step": 2080
    },
    {
      "epoch": 0.7567272727272727,
      "grad_norm": 0.07304426282644272,
      "learning_rate": 0.00019654670265672203,
      "loss": 0.3517,
      "step": 2081
    },
    {
      "epoch": 0.757090909090909,
      "grad_norm": 0.0733560249209404,
      "learning_rate": 0.00019654339428396993,
      "loss": 0.3549,
      "step": 2082
    },
    {
      "epoch": 0.7574545454545455,
      "grad_norm": 0.05931509658694267,
      "learning_rate": 0.0001965400843550824,
      "loss": 0.2793,
      "step": 2083
    },
    {
      "epoch": 0.7578181818181818,
      "grad_norm": 0.08049013465642929,
      "learning_rate": 0.00019653677287011277,
      "loss": 0.3803,
      "step": 2084
    },
    {
      "epoch": 0.7581818181818182,
      "grad_norm": 0.05248662456870079,
      "learning_rate": 0.00019653345982911447,
      "loss": 0.2893,
      "step": 2085
    },
    {
      "epoch": 0.7585454545454545,
      "grad_norm": 0.0730370283126831,
      "learning_rate": 0.00019653014523214084,
      "loss": 0.3071,
      "step": 2086
    },
    {
      "epoch": 0.7589090909090909,
      "grad_norm": 0.07657887041568756,
      "learning_rate": 0.00019652682907924534,
      "loss": 0.304,
      "step": 2087
    },
    {
      "epoch": 0.7592727272727273,
      "grad_norm": 0.0839502215385437,
      "learning_rate": 0.0001965235113704814,
      "loss": 0.3322,
      "step": 2088
    },
    {
      "epoch": 0.7596363636363637,
      "grad_norm": 0.08010748028755188,
      "learning_rate": 0.00019652019210590252,
      "loss": 0.3711,
      "step": 2089
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.06977430731058121,
      "learning_rate": 0.00019651687128556216,
      "loss": 0.3305,
      "step": 2090
    },
    {
      "epoch": 0.7603636363636364,
      "grad_norm": 0.058970097452402115,
      "learning_rate": 0.0001965135489095139,
      "loss": 0.3533,
      "step": 2091
    },
    {
      "epoch": 0.7607272727272727,
      "grad_norm": 0.07204708456993103,
      "learning_rate": 0.00019651022497781126,
      "loss": 0.361,
      "step": 2092
    },
    {
      "epoch": 0.761090909090909,
      "grad_norm": 0.08747602254152298,
      "learning_rate": 0.00019650689949050784,
      "loss": 0.4037,
      "step": 2093
    },
    {
      "epoch": 0.7614545454545455,
      "grad_norm": 0.06819164752960205,
      "learning_rate": 0.0001965035724476572,
      "loss": 0.3337,
      "step": 2094
    },
    {
      "epoch": 0.7618181818181818,
      "grad_norm": 0.07020024955272675,
      "learning_rate": 0.000196500243849313,
      "loss": 0.3988,
      "step": 2095
    },
    {
      "epoch": 0.7621818181818182,
      "grad_norm": 0.06970777362585068,
      "learning_rate": 0.00019649691369552893,
      "loss": 0.3665,
      "step": 2096
    },
    {
      "epoch": 0.7625454545454545,
      "grad_norm": 0.05783240869641304,
      "learning_rate": 0.00019649358198635858,
      "loss": 0.3282,
      "step": 2097
    },
    {
      "epoch": 0.7629090909090909,
      "grad_norm": 0.10922913253307343,
      "learning_rate": 0.0001964902487218557,
      "loss": 0.4191,
      "step": 2098
    },
    {
      "epoch": 0.7632727272727273,
      "grad_norm": 0.06304187327623367,
      "learning_rate": 0.000196486913902074,
      "loss": 0.3777,
      "step": 2099
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.09455297142267227,
      "learning_rate": 0.00019648357752706725,
      "loss": 0.4128,
      "step": 2100
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.08170881867408752,
      "learning_rate": 0.00019648023959688925,
      "loss": 0.3734,
      "step": 2101
    },
    {
      "epoch": 0.7643636363636364,
      "grad_norm": 0.08283978700637817,
      "learning_rate": 0.00019647690011159377,
      "loss": 0.3328,
      "step": 2102
    },
    {
      "epoch": 0.7647272727272727,
      "grad_norm": 0.0756402239203453,
      "learning_rate": 0.00019647355907123458,
      "loss": 0.3239,
      "step": 2103
    },
    {
      "epoch": 0.765090909090909,
      "grad_norm": 0.059711068868637085,
      "learning_rate": 0.00019647021647586566,
      "loss": 0.3496,
      "step": 2104
    },
    {
      "epoch": 0.7654545454545455,
      "grad_norm": 0.07986515015363693,
      "learning_rate": 0.0001964668723255408,
      "loss": 0.4042,
      "step": 2105
    },
    {
      "epoch": 0.7658181818181818,
      "grad_norm": 0.07364403456449509,
      "learning_rate": 0.00019646352662031392,
      "loss": 0.3208,
      "step": 2106
    },
    {
      "epoch": 0.7661818181818182,
      "grad_norm": 0.07723409682512283,
      "learning_rate": 0.00019646017936023898,
      "loss": 0.3459,
      "step": 2107
    },
    {
      "epoch": 0.7665454545454545,
      "grad_norm": 0.061411384493112564,
      "learning_rate": 0.00019645683054536988,
      "loss": 0.3009,
      "step": 2108
    },
    {
      "epoch": 0.7669090909090909,
      "grad_norm": 0.04839388281106949,
      "learning_rate": 0.00019645348017576061,
      "loss": 0.2843,
      "step": 2109
    },
    {
      "epoch": 0.7672727272727272,
      "grad_norm": 0.07888980209827423,
      "learning_rate": 0.00019645012825146523,
      "loss": 0.3334,
      "step": 2110
    },
    {
      "epoch": 0.7676363636363637,
      "grad_norm": 0.06697916984558105,
      "learning_rate": 0.00019644677477253768,
      "loss": 0.3681,
      "step": 2111
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.058469727635383606,
      "learning_rate": 0.00019644341973903208,
      "loss": 0.3353,
      "step": 2112
    },
    {
      "epoch": 0.7683636363636364,
      "grad_norm": 0.07795576751232147,
      "learning_rate": 0.00019644006315100248,
      "loss": 0.3463,
      "step": 2113
    },
    {
      "epoch": 0.7687272727272727,
      "grad_norm": 0.060105275362730026,
      "learning_rate": 0.000196436705008503,
      "loss": 0.3322,
      "step": 2114
    },
    {
      "epoch": 0.769090909090909,
      "grad_norm": 0.06529811769723892,
      "learning_rate": 0.00019643334531158774,
      "loss": 0.3393,
      "step": 2115
    },
    {
      "epoch": 0.7694545454545455,
      "grad_norm": 0.058866024017333984,
      "learning_rate": 0.00019642998406031088,
      "loss": 0.3754,
      "step": 2116
    },
    {
      "epoch": 0.7698181818181818,
      "grad_norm": 0.0735384151339531,
      "learning_rate": 0.0001964266212547266,
      "loss": 0.3689,
      "step": 2117
    },
    {
      "epoch": 0.7701818181818182,
      "grad_norm": 0.056511033326387405,
      "learning_rate": 0.00019642325689488908,
      "loss": 0.3424,
      "step": 2118
    },
    {
      "epoch": 0.7705454545454545,
      "grad_norm": 0.05852539837360382,
      "learning_rate": 0.00019641989098085255,
      "loss": 0.2847,
      "step": 2119
    },
    {
      "epoch": 0.7709090909090909,
      "grad_norm": 0.05499756708741188,
      "learning_rate": 0.00019641652351267127,
      "loss": 0.2425,
      "step": 2120
    },
    {
      "epoch": 0.7712727272727272,
      "grad_norm": 0.07433655112981796,
      "learning_rate": 0.00019641315449039955,
      "loss": 0.3185,
      "step": 2121
    },
    {
      "epoch": 0.7716363636363637,
      "grad_norm": 0.06705346703529358,
      "learning_rate": 0.0001964097839140916,
      "loss": 0.3717,
      "step": 2122
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.06729872524738312,
      "learning_rate": 0.0001964064117838019,
      "loss": 0.3702,
      "step": 2123
    },
    {
      "epoch": 0.7723636363636364,
      "grad_norm": 0.055652108043432236,
      "learning_rate": 0.00019640303809958468,
      "loss": 0.3649,
      "step": 2124
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.06705078482627869,
      "learning_rate": 0.00019639966286149436,
      "loss": 0.3541,
      "step": 2125
    },
    {
      "epoch": 0.773090909090909,
      "grad_norm": 0.06707964837551117,
      "learning_rate": 0.00019639628606958533,
      "loss": 0.3148,
      "step": 2126
    },
    {
      "epoch": 0.7734545454545455,
      "grad_norm": 0.07125569880008698,
      "learning_rate": 0.00019639290772391203,
      "loss": 0.3289,
      "step": 2127
    },
    {
      "epoch": 0.7738181818181818,
      "grad_norm": 0.07009004801511765,
      "learning_rate": 0.00019638952782452895,
      "loss": 0.396,
      "step": 2128
    },
    {
      "epoch": 0.7741818181818182,
      "grad_norm": 0.0615709125995636,
      "learning_rate": 0.00019638614637149048,
      "loss": 0.3713,
      "step": 2129
    },
    {
      "epoch": 0.7745454545454545,
      "grad_norm": 0.07389448583126068,
      "learning_rate": 0.00019638276336485122,
      "loss": 0.3913,
      "step": 2130
    },
    {
      "epoch": 0.7749090909090909,
      "grad_norm": 0.0695880800485611,
      "learning_rate": 0.00019637937880466564,
      "loss": 0.3371,
      "step": 2131
    },
    {
      "epoch": 0.7752727272727272,
      "grad_norm": 0.060403067618608475,
      "learning_rate": 0.00019637599269098832,
      "loss": 0.3201,
      "step": 2132
    },
    {
      "epoch": 0.7756363636363637,
      "grad_norm": 0.0604904443025589,
      "learning_rate": 0.0001963726050238738,
      "loss": 0.2989,
      "step": 2133
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.07215750217437744,
      "learning_rate": 0.00019636921580337675,
      "loss": 0.3768,
      "step": 2134
    },
    {
      "epoch": 0.7763636363636364,
      "grad_norm": 0.04422083869576454,
      "learning_rate": 0.00019636582502955175,
      "loss": 0.2437,
      "step": 2135
    },
    {
      "epoch": 0.7767272727272727,
      "grad_norm": 0.061924055218696594,
      "learning_rate": 0.00019636243270245346,
      "loss": 0.3537,
      "step": 2136
    },
    {
      "epoch": 0.777090909090909,
      "grad_norm": 0.07423325628042221,
      "learning_rate": 0.00019635903882213654,
      "loss": 0.399,
      "step": 2137
    },
    {
      "epoch": 0.7774545454545455,
      "grad_norm": 0.046116407960653305,
      "learning_rate": 0.00019635564338865575,
      "loss": 0.2806,
      "step": 2138
    },
    {
      "epoch": 0.7778181818181819,
      "grad_norm": 0.05681091919541359,
      "learning_rate": 0.00019635224640206578,
      "loss": 0.3658,
      "step": 2139
    },
    {
      "epoch": 0.7781818181818182,
      "grad_norm": 0.05673617124557495,
      "learning_rate": 0.0001963488478624214,
      "loss": 0.3396,
      "step": 2140
    },
    {
      "epoch": 0.7785454545454545,
      "grad_norm": 0.06891507655382156,
      "learning_rate": 0.00019634544776977737,
      "loss": 0.3983,
      "step": 2141
    },
    {
      "epoch": 0.7789090909090909,
      "grad_norm": 0.05174204707145691,
      "learning_rate": 0.00019634204612418852,
      "loss": 0.3416,
      "step": 2142
    },
    {
      "epoch": 0.7792727272727272,
      "grad_norm": 0.06273078918457031,
      "learning_rate": 0.00019633864292570964,
      "loss": 0.3889,
      "step": 2143
    },
    {
      "epoch": 0.7796363636363637,
      "grad_norm": 0.05568807199597359,
      "learning_rate": 0.00019633523817439566,
      "loss": 0.3114,
      "step": 2144
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.059781163930892944,
      "learning_rate": 0.0001963318318703014,
      "loss": 0.347,
      "step": 2145
    },
    {
      "epoch": 0.7803636363636364,
      "grad_norm": 0.06216650828719139,
      "learning_rate": 0.00019632842401348175,
      "loss": 0.3778,
      "step": 2146
    },
    {
      "epoch": 0.7807272727272727,
      "grad_norm": 0.06197948008775711,
      "learning_rate": 0.00019632501460399167,
      "loss": 0.3532,
      "step": 2147
    },
    {
      "epoch": 0.7810909090909091,
      "grad_norm": 0.06935616582632065,
      "learning_rate": 0.0001963216036418861,
      "loss": 0.3516,
      "step": 2148
    },
    {
      "epoch": 0.7814545454545454,
      "grad_norm": 0.05716662481427193,
      "learning_rate": 0.00019631819112722005,
      "loss": 0.348,
      "step": 2149
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.06725907325744629,
      "learning_rate": 0.0001963147770600485,
      "loss": 0.3784,
      "step": 2150
    },
    {
      "epoch": 0.7821818181818182,
      "grad_norm": 0.06734048575162888,
      "learning_rate": 0.00019631136144042648,
      "loss": 0.3352,
      "step": 2151
    },
    {
      "epoch": 0.7825454545454545,
      "grad_norm": 0.07667196542024612,
      "learning_rate": 0.00019630794426840907,
      "loss": 0.3091,
      "step": 2152
    },
    {
      "epoch": 0.7829090909090909,
      "grad_norm": 0.06370650976896286,
      "learning_rate": 0.0001963045255440513,
      "loss": 0.3417,
      "step": 2153
    },
    {
      "epoch": 0.7832727272727272,
      "grad_norm": 0.07653328031301498,
      "learning_rate": 0.0001963011052674083,
      "loss": 0.3688,
      "step": 2154
    },
    {
      "epoch": 0.7836363636363637,
      "grad_norm": 0.06303509324789047,
      "learning_rate": 0.00019629768343853524,
      "loss": 0.3008,
      "step": 2155
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.07986542582511902,
      "learning_rate": 0.00019629426005748717,
      "loss": 0.358,
      "step": 2156
    },
    {
      "epoch": 0.7843636363636364,
      "grad_norm": 0.06180393323302269,
      "learning_rate": 0.0001962908351243194,
      "loss": 0.3314,
      "step": 2157
    },
    {
      "epoch": 0.7847272727272727,
      "grad_norm": 0.06761078536510468,
      "learning_rate": 0.00019628740863908705,
      "loss": 0.3151,
      "step": 2158
    },
    {
      "epoch": 0.7850909090909091,
      "grad_norm": 0.07749444991350174,
      "learning_rate": 0.00019628398060184536,
      "loss": 0.3809,
      "step": 2159
    },
    {
      "epoch": 0.7854545454545454,
      "grad_norm": 0.07274197041988373,
      "learning_rate": 0.00019628055101264958,
      "loss": 0.2788,
      "step": 2160
    },
    {
      "epoch": 0.7858181818181819,
      "grad_norm": 0.08107294142246246,
      "learning_rate": 0.00019627711987155503,
      "loss": 0.3721,
      "step": 2161
    },
    {
      "epoch": 0.7861818181818182,
      "grad_norm": 0.09150898456573486,
      "learning_rate": 0.000196273687178617,
      "loss": 0.4524,
      "step": 2162
    },
    {
      "epoch": 0.7865454545454545,
      "grad_norm": 0.07116086035966873,
      "learning_rate": 0.0001962702529338908,
      "loss": 0.3538,
      "step": 2163
    },
    {
      "epoch": 0.7869090909090909,
      "grad_norm": 0.06489534676074982,
      "learning_rate": 0.00019626681713743184,
      "loss": 0.3721,
      "step": 2164
    },
    {
      "epoch": 0.7872727272727272,
      "grad_norm": 0.07010653614997864,
      "learning_rate": 0.00019626337978929538,
      "loss": 0.3074,
      "step": 2165
    },
    {
      "epoch": 0.7876363636363637,
      "grad_norm": 0.06212884560227394,
      "learning_rate": 0.00019625994088953694,
      "loss": 0.3108,
      "step": 2166
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.05391598120331764,
      "learning_rate": 0.0001962565004382119,
      "loss": 0.3083,
      "step": 2167
    },
    {
      "epoch": 0.7883636363636364,
      "grad_norm": 0.06683868169784546,
      "learning_rate": 0.00019625305843537573,
      "loss": 0.3563,
      "step": 2168
    },
    {
      "epoch": 0.7887272727272727,
      "grad_norm": 0.05955002084374428,
      "learning_rate": 0.0001962496148810839,
      "loss": 0.3156,
      "step": 2169
    },
    {
      "epoch": 0.7890909090909091,
      "grad_norm": 0.06366628408432007,
      "learning_rate": 0.00019624616977539195,
      "loss": 0.3313,
      "step": 2170
    },
    {
      "epoch": 0.7894545454545454,
      "grad_norm": 0.05989795923233032,
      "learning_rate": 0.00019624272311835535,
      "loss": 0.3437,
      "step": 2171
    },
    {
      "epoch": 0.7898181818181819,
      "grad_norm": 0.07700763642787933,
      "learning_rate": 0.00019623927491002968,
      "loss": 0.3829,
      "step": 2172
    },
    {
      "epoch": 0.7901818181818182,
      "grad_norm": 0.06730985641479492,
      "learning_rate": 0.00019623582515047056,
      "loss": 0.3902,
      "step": 2173
    },
    {
      "epoch": 0.7905454545454546,
      "grad_norm": 0.05710722878575325,
      "learning_rate": 0.00019623237383973354,
      "loss": 0.31,
      "step": 2174
    },
    {
      "epoch": 0.7909090909090909,
      "grad_norm": 0.05880909785628319,
      "learning_rate": 0.00019622892097787426,
      "loss": 0.325,
      "step": 2175
    },
    {
      "epoch": 0.7912727272727272,
      "grad_norm": 0.08388952165842056,
      "learning_rate": 0.0001962254665649484,
      "loss": 0.3692,
      "step": 2176
    },
    {
      "epoch": 0.7916363636363636,
      "grad_norm": 0.05851719155907631,
      "learning_rate": 0.00019622201060101162,
      "loss": 0.3383,
      "step": 2177
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.07057981193065643,
      "learning_rate": 0.00019621855308611964,
      "loss": 0.3957,
      "step": 2178
    },
    {
      "epoch": 0.7923636363636364,
      "grad_norm": 0.05107759311795235,
      "learning_rate": 0.00019621509402032816,
      "loss": 0.2692,
      "step": 2179
    },
    {
      "epoch": 0.7927272727272727,
      "grad_norm": 0.06063396856188774,
      "learning_rate": 0.00019621163340369298,
      "loss": 0.303,
      "step": 2180
    },
    {
      "epoch": 0.7930909090909091,
      "grad_norm": 0.061931636184453964,
      "learning_rate": 0.0001962081712362698,
      "loss": 0.3594,
      "step": 2181
    },
    {
      "epoch": 0.7934545454545454,
      "grad_norm": 0.08302628248929977,
      "learning_rate": 0.00019620470751811455,
      "loss": 0.359,
      "step": 2182
    },
    {
      "epoch": 0.7938181818181819,
      "grad_norm": 0.05413997545838356,
      "learning_rate": 0.00019620124224928296,
      "loss": 0.2784,
      "step": 2183
    },
    {
      "epoch": 0.7941818181818182,
      "grad_norm": 0.05792247876524925,
      "learning_rate": 0.00019619777542983092,
      "loss": 0.3316,
      "step": 2184
    },
    {
      "epoch": 0.7945454545454546,
      "grad_norm": 0.05890015885233879,
      "learning_rate": 0.0001961943070598143,
      "loss": 0.3058,
      "step": 2185
    },
    {
      "epoch": 0.7949090909090909,
      "grad_norm": 0.04943637549877167,
      "learning_rate": 0.00019619083713928902,
      "loss": 0.2842,
      "step": 2186
    },
    {
      "epoch": 0.7952727272727272,
      "grad_norm": 0.048954471945762634,
      "learning_rate": 0.00019618736566831096,
      "loss": 0.2818,
      "step": 2187
    },
    {
      "epoch": 0.7956363636363636,
      "grad_norm": 0.072797030210495,
      "learning_rate": 0.00019618389264693616,
      "loss": 0.3656,
      "step": 2188
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.04887743666768074,
      "learning_rate": 0.00019618041807522053,
      "loss": 0.327,
      "step": 2189
    },
    {
      "epoch": 0.7963636363636364,
      "grad_norm": 0.06761610507965088,
      "learning_rate": 0.00019617694195322012,
      "loss": 0.3773,
      "step": 2190
    },
    {
      "epoch": 0.7967272727272727,
      "grad_norm": 0.053730785846710205,
      "learning_rate": 0.00019617346428099094,
      "loss": 0.2599,
      "step": 2191
    },
    {
      "epoch": 0.7970909090909091,
      "grad_norm": 0.06831543892621994,
      "learning_rate": 0.000196169985058589,
      "loss": 0.3165,
      "step": 2192
    },
    {
      "epoch": 0.7974545454545454,
      "grad_norm": 0.05942172929644585,
      "learning_rate": 0.00019616650428607048,
      "loss": 0.2905,
      "step": 2193
    },
    {
      "epoch": 0.7978181818181819,
      "grad_norm": 0.053757164627313614,
      "learning_rate": 0.00019616302196349142,
      "loss": 0.3441,
      "step": 2194
    },
    {
      "epoch": 0.7981818181818182,
      "grad_norm": 0.0641375258564949,
      "learning_rate": 0.00019615953809090793,
      "loss": 0.3892,
      "step": 2195
    },
    {
      "epoch": 0.7985454545454546,
      "grad_norm": 0.06108653172850609,
      "learning_rate": 0.00019615605266837624,
      "loss": 0.3677,
      "step": 2196
    },
    {
      "epoch": 0.7989090909090909,
      "grad_norm": 0.059434302151203156,
      "learning_rate": 0.00019615256569595243,
      "loss": 0.2489,
      "step": 2197
    },
    {
      "epoch": 0.7992727272727272,
      "grad_norm": 0.05519318953156471,
      "learning_rate": 0.00019614907717369277,
      "loss": 0.3353,
      "step": 2198
    },
    {
      "epoch": 0.7996363636363636,
      "grad_norm": 0.052746277302503586,
      "learning_rate": 0.0001961455871016535,
      "loss": 0.275,
      "step": 2199
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06001376733183861,
      "learning_rate": 0.00019614209547989085,
      "loss": 0.3292,
      "step": 2200
    },
    {
      "epoch": 0.8003636363636364,
      "grad_norm": 0.06575646251440048,
      "learning_rate": 0.0001961386023084611,
      "loss": 0.3575,
      "step": 2201
    },
    {
      "epoch": 0.8007272727272727,
      "grad_norm": 0.0481828972697258,
      "learning_rate": 0.00019613510758742056,
      "loss": 0.2897,
      "step": 2202
    },
    {
      "epoch": 0.8010909090909091,
      "grad_norm": 0.054434385150671005,
      "learning_rate": 0.00019613161131682553,
      "loss": 0.3168,
      "step": 2203
    },
    {
      "epoch": 0.8014545454545454,
      "grad_norm": 0.06565309315919876,
      "learning_rate": 0.00019612811349673241,
      "loss": 0.3699,
      "step": 2204
    },
    {
      "epoch": 0.8018181818181818,
      "grad_norm": 0.06903436779975891,
      "learning_rate": 0.00019612461412719758,
      "loss": 0.2982,
      "step": 2205
    },
    {
      "epoch": 0.8021818181818182,
      "grad_norm": 0.06426820158958435,
      "learning_rate": 0.00019612111320827742,
      "loss": 0.388,
      "step": 2206
    },
    {
      "epoch": 0.8025454545454546,
      "grad_norm": 0.04896465688943863,
      "learning_rate": 0.00019611761074002834,
      "loss": 0.2995,
      "step": 2207
    },
    {
      "epoch": 0.8029090909090909,
      "grad_norm": 0.04802371561527252,
      "learning_rate": 0.00019611410672250686,
      "loss": 0.2768,
      "step": 2208
    },
    {
      "epoch": 0.8032727272727272,
      "grad_norm": 0.06512222439050674,
      "learning_rate": 0.0001961106011557694,
      "loss": 0.364,
      "step": 2209
    },
    {
      "epoch": 0.8036363636363636,
      "grad_norm": 0.08008502423763275,
      "learning_rate": 0.00019610709403987246,
      "loss": 0.4035,
      "step": 2210
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.055138763040304184,
      "learning_rate": 0.00019610358537487264,
      "loss": 0.3169,
      "step": 2211
    },
    {
      "epoch": 0.8043636363636364,
      "grad_norm": 0.06061110645532608,
      "learning_rate": 0.0001961000751608264,
      "loss": 0.3163,
      "step": 2212
    },
    {
      "epoch": 0.8047272727272727,
      "grad_norm": 0.06430075317621231,
      "learning_rate": 0.00019609656339779038,
      "loss": 0.3183,
      "step": 2213
    },
    {
      "epoch": 0.8050909090909091,
      "grad_norm": 0.061304304748773575,
      "learning_rate": 0.0001960930500858212,
      "loss": 0.3245,
      "step": 2214
    },
    {
      "epoch": 0.8054545454545454,
      "grad_norm": 0.05801108106970787,
      "learning_rate": 0.00019608953522497544,
      "loss": 0.322,
      "step": 2215
    },
    {
      "epoch": 0.8058181818181818,
      "grad_norm": 0.06420037895441055,
      "learning_rate": 0.0001960860188153098,
      "loss": 0.3533,
      "step": 2216
    },
    {
      "epoch": 0.8061818181818182,
      "grad_norm": 0.055068884044885635,
      "learning_rate": 0.00019608250085688088,
      "loss": 0.303,
      "step": 2217
    },
    {
      "epoch": 0.8065454545454546,
      "grad_norm": 0.06380680203437805,
      "learning_rate": 0.0001960789813497455,
      "loss": 0.3459,
      "step": 2218
    },
    {
      "epoch": 0.8069090909090909,
      "grad_norm": 0.05244680866599083,
      "learning_rate": 0.0001960754602939603,
      "loss": 0.3313,
      "step": 2219
    },
    {
      "epoch": 0.8072727272727273,
      "grad_norm": 0.051615312695503235,
      "learning_rate": 0.00019607193768958204,
      "loss": 0.3245,
      "step": 2220
    },
    {
      "epoch": 0.8076363636363636,
      "grad_norm": 0.05610067397356033,
      "learning_rate": 0.00019606841353666754,
      "loss": 0.3758,
      "step": 2221
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.06070853769779205,
      "learning_rate": 0.0001960648878352736,
      "loss": 0.312,
      "step": 2222
    },
    {
      "epoch": 0.8083636363636364,
      "grad_norm": 0.06342186033725739,
      "learning_rate": 0.00019606136058545703,
      "loss": 0.3698,
      "step": 2223
    },
    {
      "epoch": 0.8087272727272727,
      "grad_norm": 0.04857093468308449,
      "learning_rate": 0.0001960578317872747,
      "loss": 0.3375,
      "step": 2224
    },
    {
      "epoch": 0.8090909090909091,
      "grad_norm": 0.0414765365421772,
      "learning_rate": 0.00019605430144078344,
      "loss": 0.2579,
      "step": 2225
    },
    {
      "epoch": 0.8094545454545454,
      "grad_norm": 0.051558710634708405,
      "learning_rate": 0.00019605076954604022,
      "loss": 0.2802,
      "step": 2226
    },
    {
      "epoch": 0.8098181818181818,
      "grad_norm": 0.06121465936303139,
      "learning_rate": 0.00019604723610310194,
      "loss": 0.3262,
      "step": 2227
    },
    {
      "epoch": 0.8101818181818182,
      "grad_norm": 0.051740556955337524,
      "learning_rate": 0.00019604370111202555,
      "loss": 0.2799,
      "step": 2228
    },
    {
      "epoch": 0.8105454545454546,
      "grad_norm": 0.05079362913966179,
      "learning_rate": 0.00019604016457286803,
      "loss": 0.3182,
      "step": 2229
    },
    {
      "epoch": 0.8109090909090909,
      "grad_norm": 0.04693468660116196,
      "learning_rate": 0.00019603662648568637,
      "loss": 0.3127,
      "step": 2230
    },
    {
      "epoch": 0.8112727272727273,
      "grad_norm": 0.053612466901540756,
      "learning_rate": 0.00019603308685053764,
      "loss": 0.3052,
      "step": 2231
    },
    {
      "epoch": 0.8116363636363636,
      "grad_norm": 0.06572353839874268,
      "learning_rate": 0.00019602954566747886,
      "loss": 0.4042,
      "step": 2232
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.048922762274742126,
      "learning_rate": 0.0001960260029365671,
      "loss": 0.3292,
      "step": 2233
    },
    {
      "epoch": 0.8123636363636364,
      "grad_norm": 0.056700028479099274,
      "learning_rate": 0.00019602245865785954,
      "loss": 0.2794,
      "step": 2234
    },
    {
      "epoch": 0.8127272727272727,
      "grad_norm": 0.056136056780815125,
      "learning_rate": 0.0001960189128314132,
      "loss": 0.3376,
      "step": 2235
    },
    {
      "epoch": 0.8130909090909091,
      "grad_norm": 0.052978187799453735,
      "learning_rate": 0.0001960153654572853,
      "loss": 0.3578,
      "step": 2236
    },
    {
      "epoch": 0.8134545454545454,
      "grad_norm": 0.057482603937387466,
      "learning_rate": 0.000196011816535533,
      "loss": 0.3136,
      "step": 2237
    },
    {
      "epoch": 0.8138181818181818,
      "grad_norm": 0.05674605816602707,
      "learning_rate": 0.0001960082660662135,
      "loss": 0.3459,
      "step": 2238
    },
    {
      "epoch": 0.8141818181818182,
      "grad_norm": 0.05173960700631142,
      "learning_rate": 0.00019600471404938402,
      "loss": 0.3471,
      "step": 2239
    },
    {
      "epoch": 0.8145454545454546,
      "grad_norm": 0.05238189920783043,
      "learning_rate": 0.00019600116048510183,
      "loss": 0.3614,
      "step": 2240
    },
    {
      "epoch": 0.8149090909090909,
      "grad_norm": 0.056060463190078735,
      "learning_rate": 0.00019599760537342424,
      "loss": 0.2954,
      "step": 2241
    },
    {
      "epoch": 0.8152727272727273,
      "grad_norm": 0.053911443799734116,
      "learning_rate": 0.0001959940487144085,
      "loss": 0.3234,
      "step": 2242
    },
    {
      "epoch": 0.8156363636363636,
      "grad_norm": 0.06193011999130249,
      "learning_rate": 0.00019599049050811194,
      "loss": 0.3847,
      "step": 2243
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.054082050919532776,
      "learning_rate": 0.00019598693075459193,
      "loss": 0.3608,
      "step": 2244
    },
    {
      "epoch": 0.8163636363636364,
      "grad_norm": 0.05871853604912758,
      "learning_rate": 0.00019598336945390586,
      "loss": 0.3722,
      "step": 2245
    },
    {
      "epoch": 0.8167272727272727,
      "grad_norm": 0.06249329075217247,
      "learning_rate": 0.0001959798066061111,
      "loss": 0.3354,
      "step": 2246
    },
    {
      "epoch": 0.8170909090909091,
      "grad_norm": 0.05971847474575043,
      "learning_rate": 0.00019597624221126512,
      "loss": 0.3651,
      "step": 2247
    },
    {
      "epoch": 0.8174545454545454,
      "grad_norm": 0.08149266242980957,
      "learning_rate": 0.00019597267626942535,
      "loss": 0.4121,
      "step": 2248
    },
    {
      "epoch": 0.8178181818181818,
      "grad_norm": 0.06740129739046097,
      "learning_rate": 0.00019596910878064928,
      "loss": 0.3648,
      "step": 2249
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.06154842674732208,
      "learning_rate": 0.00019596553974499434,
      "loss": 0.365,
      "step": 2250
    },
    {
      "epoch": 0.8185454545454546,
      "grad_norm": 0.053917452692985535,
      "learning_rate": 0.00019596196916251817,
      "loss": 0.3065,
      "step": 2251
    },
    {
      "epoch": 0.8189090909090909,
      "grad_norm": 0.06314229965209961,
      "learning_rate": 0.00019595839703327825,
      "loss": 0.3677,
      "step": 2252
    },
    {
      "epoch": 0.8192727272727273,
      "grad_norm": 0.06238196790218353,
      "learning_rate": 0.00019595482335733218,
      "loss": 0.4,
      "step": 2253
    },
    {
      "epoch": 0.8196363636363636,
      "grad_norm": 0.05839764326810837,
      "learning_rate": 0.00019595124813473756,
      "loss": 0.324,
      "step": 2254
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.05813681334257126,
      "learning_rate": 0.00019594767136555205,
      "loss": 0.3439,
      "step": 2255
    },
    {
      "epoch": 0.8203636363636364,
      "grad_norm": 0.05884237959980965,
      "learning_rate": 0.00019594409304983323,
      "loss": 0.3828,
      "step": 2256
    },
    {
      "epoch": 0.8207272727272727,
      "grad_norm": 0.06522248685359955,
      "learning_rate": 0.0001959405131876388,
      "loss": 0.3603,
      "step": 2257
    },
    {
      "epoch": 0.8210909090909091,
      "grad_norm": 0.06284040957689285,
      "learning_rate": 0.00019593693177902654,
      "loss": 0.3191,
      "step": 2258
    },
    {
      "epoch": 0.8214545454545454,
      "grad_norm": 0.05208228528499603,
      "learning_rate": 0.00019593334882405408,
      "loss": 0.3324,
      "step": 2259
    },
    {
      "epoch": 0.8218181818181818,
      "grad_norm": 0.062419746071100235,
      "learning_rate": 0.00019592976432277918,
      "loss": 0.3371,
      "step": 2260
    },
    {
      "epoch": 0.8221818181818182,
      "grad_norm": 0.06547464430332184,
      "learning_rate": 0.00019592617827525967,
      "loss": 0.3646,
      "step": 2261
    },
    {
      "epoch": 0.8225454545454546,
      "grad_norm": 0.06127830967307091,
      "learning_rate": 0.00019592259068155335,
      "loss": 0.3312,
      "step": 2262
    },
    {
      "epoch": 0.8229090909090909,
      "grad_norm": 0.05275121331214905,
      "learning_rate": 0.000195919001541718,
      "loss": 0.37,
      "step": 2263
    },
    {
      "epoch": 0.8232727272727273,
      "grad_norm": 0.07301057875156403,
      "learning_rate": 0.0001959154108558115,
      "loss": 0.3748,
      "step": 2264
    },
    {
      "epoch": 0.8236363636363636,
      "grad_norm": 0.056681226938962936,
      "learning_rate": 0.00019591181862389168,
      "loss": 0.4029,
      "step": 2265
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.070468008518219,
      "learning_rate": 0.00019590822484601654,
      "loss": 0.4019,
      "step": 2266
    },
    {
      "epoch": 0.8243636363636364,
      "grad_norm": 0.05457311496138573,
      "learning_rate": 0.00019590462952224393,
      "loss": 0.2712,
      "step": 2267
    },
    {
      "epoch": 0.8247272727272728,
      "grad_norm": 0.07289063930511475,
      "learning_rate": 0.00019590103265263183,
      "loss": 0.3685,
      "step": 2268
    },
    {
      "epoch": 0.8250909090909091,
      "grad_norm": 0.0628143846988678,
      "learning_rate": 0.00019589743423723815,
      "loss": 0.3801,
      "step": 2269
    },
    {
      "epoch": 0.8254545454545454,
      "grad_norm": 0.04925917834043503,
      "learning_rate": 0.000195893834276121,
      "loss": 0.3197,
      "step": 2270
    },
    {
      "epoch": 0.8258181818181818,
      "grad_norm": 0.06304856389760971,
      "learning_rate": 0.00019589023276933833,
      "loss": 0.3591,
      "step": 2271
    },
    {
      "epoch": 0.8261818181818181,
      "grad_norm": 0.06288425624370575,
      "learning_rate": 0.00019588662971694825,
      "loss": 0.421,
      "step": 2272
    },
    {
      "epoch": 0.8265454545454546,
      "grad_norm": 0.06389150768518448,
      "learning_rate": 0.00019588302511900878,
      "loss": 0.3482,
      "step": 2273
    },
    {
      "epoch": 0.8269090909090909,
      "grad_norm": 0.0699102133512497,
      "learning_rate": 0.000195879418975578,
      "loss": 0.414,
      "step": 2274
    },
    {
      "epoch": 0.8272727272727273,
      "grad_norm": 0.06869745999574661,
      "learning_rate": 0.00019587581128671414,
      "loss": 0.3975,
      "step": 2275
    },
    {
      "epoch": 0.8276363636363636,
      "grad_norm": 0.06227564439177513,
      "learning_rate": 0.00019587220205247524,
      "loss": 0.3691,
      "step": 2276
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.047312840819358826,
      "learning_rate": 0.00019586859127291954,
      "loss": 0.3131,
      "step": 2277
    },
    {
      "epoch": 0.8283636363636364,
      "grad_norm": 0.08103816211223602,
      "learning_rate": 0.00019586497894810522,
      "loss": 0.3756,
      "step": 2278
    },
    {
      "epoch": 0.8287272727272728,
      "grad_norm": 0.05620922893285751,
      "learning_rate": 0.0001958613650780905,
      "loss": 0.3583,
      "step": 2279
    },
    {
      "epoch": 0.8290909090909091,
      "grad_norm": 0.057588841766119,
      "learning_rate": 0.00019585774966293364,
      "loss": 0.381,
      "step": 2280
    },
    {
      "epoch": 0.8294545454545454,
      "grad_norm": 0.05761693790555,
      "learning_rate": 0.00019585413270269296,
      "loss": 0.3858,
      "step": 2281
    },
    {
      "epoch": 0.8298181818181818,
      "grad_norm": 0.06816484779119492,
      "learning_rate": 0.00019585051419742663,
      "loss": 0.388,
      "step": 2282
    },
    {
      "epoch": 0.8301818181818181,
      "grad_norm": 0.06982986629009247,
      "learning_rate": 0.00019584689414719312,
      "loss": 0.4145,
      "step": 2283
    },
    {
      "epoch": 0.8305454545454546,
      "grad_norm": 0.06632467359304428,
      "learning_rate": 0.00019584327255205071,
      "loss": 0.4323,
      "step": 2284
    },
    {
      "epoch": 0.8309090909090909,
      "grad_norm": 0.0611453615128994,
      "learning_rate": 0.00019583964941205775,
      "loss": 0.395,
      "step": 2285
    },
    {
      "epoch": 0.8312727272727273,
      "grad_norm": 0.050289999693632126,
      "learning_rate": 0.0001958360247272727,
      "loss": 0.3264,
      "step": 2286
    },
    {
      "epoch": 0.8316363636363636,
      "grad_norm": 0.04744977131485939,
      "learning_rate": 0.00019583239849775395,
      "loss": 0.2758,
      "step": 2287
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.050504013895988464,
      "learning_rate": 0.00019582877072355995,
      "loss": 0.3223,
      "step": 2288
    },
    {
      "epoch": 0.8323636363636364,
      "grad_norm": 0.05703079327940941,
      "learning_rate": 0.00019582514140474916,
      "loss": 0.3449,
      "step": 2289
    },
    {
      "epoch": 0.8327272727272728,
      "grad_norm": 0.05997050553560257,
      "learning_rate": 0.00019582151054138014,
      "loss": 0.398,
      "step": 2290
    },
    {
      "epoch": 0.8330909090909091,
      "grad_norm": 0.048453446477651596,
      "learning_rate": 0.00019581787813351137,
      "loss": 0.3018,
      "step": 2291
    },
    {
      "epoch": 0.8334545454545454,
      "grad_norm": 0.06349595636129379,
      "learning_rate": 0.00019581424418120137,
      "loss": 0.3039,
      "step": 2292
    },
    {
      "epoch": 0.8338181818181818,
      "grad_norm": 0.05604994297027588,
      "learning_rate": 0.00019581060868450879,
      "loss": 0.3975,
      "step": 2293
    },
    {
      "epoch": 0.8341818181818181,
      "grad_norm": 0.06555813550949097,
      "learning_rate": 0.00019580697164349215,
      "loss": 0.3458,
      "step": 2294
    },
    {
      "epoch": 0.8345454545454546,
      "grad_norm": 0.05576039478182793,
      "learning_rate": 0.00019580333305821012,
      "loss": 0.3683,
      "step": 2295
    },
    {
      "epoch": 0.8349090909090909,
      "grad_norm": 0.050477899610996246,
      "learning_rate": 0.00019579969292872135,
      "loss": 0.3377,
      "step": 2296
    },
    {
      "epoch": 0.8352727272727273,
      "grad_norm": 0.05511299893260002,
      "learning_rate": 0.0001957960512550845,
      "loss": 0.3462,
      "step": 2297
    },
    {
      "epoch": 0.8356363636363636,
      "grad_norm": 0.04912237450480461,
      "learning_rate": 0.00019579240803735824,
      "loss": 0.3202,
      "step": 2298
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.051365483552217484,
      "learning_rate": 0.00019578876327560135,
      "loss": 0.3392,
      "step": 2299
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.044782157987356186,
      "learning_rate": 0.0001957851169698725,
      "loss": 0.2748,
      "step": 2300
    },
    {
      "epoch": 0.8367272727272728,
      "grad_norm": 0.07374116033315659,
      "learning_rate": 0.00019578146912023058,
      "loss": 0.2961,
      "step": 2301
    },
    {
      "epoch": 0.8370909090909091,
      "grad_norm": 0.04966215789318085,
      "learning_rate": 0.0001957778197267343,
      "loss": 0.2884,
      "step": 2302
    },
    {
      "epoch": 0.8374545454545455,
      "grad_norm": 0.06083008274435997,
      "learning_rate": 0.0001957741687894425,
      "loss": 0.3302,
      "step": 2303
    },
    {
      "epoch": 0.8378181818181818,
      "grad_norm": 0.053594920784235,
      "learning_rate": 0.00019577051630841402,
      "loss": 0.2837,
      "step": 2304
    },
    {
      "epoch": 0.8381818181818181,
      "grad_norm": 0.05756353214383125,
      "learning_rate": 0.00019576686228370776,
      "loss": 0.3732,
      "step": 2305
    },
    {
      "epoch": 0.8385454545454546,
      "grad_norm": 0.05397535115480423,
      "learning_rate": 0.0001957632067153826,
      "loss": 0.3374,
      "step": 2306
    },
    {
      "epoch": 0.8389090909090909,
      "grad_norm": 0.05293893814086914,
      "learning_rate": 0.00019575954960349745,
      "loss": 0.3507,
      "step": 2307
    },
    {
      "epoch": 0.8392727272727273,
      "grad_norm": 0.05899008736014366,
      "learning_rate": 0.00019575589094811128,
      "loss": 0.3273,
      "step": 2308
    },
    {
      "epoch": 0.8396363636363636,
      "grad_norm": 0.0542943999171257,
      "learning_rate": 0.0001957522307492831,
      "loss": 0.3294,
      "step": 2309
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.058856286108493805,
      "learning_rate": 0.0001957485690070718,
      "loss": 0.4128,
      "step": 2310
    },
    {
      "epoch": 0.8403636363636363,
      "grad_norm": 0.04544420540332794,
      "learning_rate": 0.00019574490572153648,
      "loss": 0.2825,
      "step": 2311
    },
    {
      "epoch": 0.8407272727272728,
      "grad_norm": 0.05009322613477707,
      "learning_rate": 0.00019574124089273616,
      "loss": 0.3101,
      "step": 2312
    },
    {
      "epoch": 0.8410909090909091,
      "grad_norm": 0.05885821580886841,
      "learning_rate": 0.00019573757452072993,
      "loss": 0.3834,
      "step": 2313
    },
    {
      "epoch": 0.8414545454545455,
      "grad_norm": 0.04793078824877739,
      "learning_rate": 0.00019573390660557687,
      "loss": 0.2564,
      "step": 2314
    },
    {
      "epoch": 0.8418181818181818,
      "grad_norm": 0.054978057742118835,
      "learning_rate": 0.0001957302371473361,
      "loss": 0.3312,
      "step": 2315
    },
    {
      "epoch": 0.8421818181818181,
      "grad_norm": 0.05591444671154022,
      "learning_rate": 0.0001957265661460668,
      "loss": 0.3828,
      "step": 2316
    },
    {
      "epoch": 0.8425454545454546,
      "grad_norm": 0.06219423934817314,
      "learning_rate": 0.00019572289360182812,
      "loss": 0.3535,
      "step": 2317
    },
    {
      "epoch": 0.8429090909090909,
      "grad_norm": 0.08000575006008148,
      "learning_rate": 0.0001957192195146792,
      "loss": 0.3742,
      "step": 2318
    },
    {
      "epoch": 0.8432727272727273,
      "grad_norm": 0.05942310392856598,
      "learning_rate": 0.00019571554388467938,
      "loss": 0.304,
      "step": 2319
    },
    {
      "epoch": 0.8436363636363636,
      "grad_norm": 0.05401439219713211,
      "learning_rate": 0.0001957118667118878,
      "loss": 0.2634,
      "step": 2320
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.07402303814888,
      "learning_rate": 0.00019570818799636376,
      "loss": 0.4505,
      "step": 2321
    },
    {
      "epoch": 0.8443636363636363,
      "grad_norm": 0.04567595571279526,
      "learning_rate": 0.0001957045077381666,
      "loss": 0.2928,
      "step": 2322
    },
    {
      "epoch": 0.8447272727272728,
      "grad_norm": 0.06514474749565125,
      "learning_rate": 0.00019570082593735558,
      "loss": 0.3225,
      "step": 2323
    },
    {
      "epoch": 0.8450909090909091,
      "grad_norm": 0.06112918257713318,
      "learning_rate": 0.00019569714259399005,
      "loss": 0.3547,
      "step": 2324
    },
    {
      "epoch": 0.8454545454545455,
      "grad_norm": 0.06038696691393852,
      "learning_rate": 0.00019569345770812944,
      "loss": 0.3816,
      "step": 2325
    },
    {
      "epoch": 0.8458181818181818,
      "grad_norm": 0.06162218004465103,
      "learning_rate": 0.00019568977127983308,
      "loss": 0.3143,
      "step": 2326
    },
    {
      "epoch": 0.8461818181818181,
      "grad_norm": 0.05372387543320656,
      "learning_rate": 0.00019568608330916042,
      "loss": 0.3085,
      "step": 2327
    },
    {
      "epoch": 0.8465454545454546,
      "grad_norm": 0.06291045993566513,
      "learning_rate": 0.00019568239379617088,
      "loss": 0.3616,
      "step": 2328
    },
    {
      "epoch": 0.846909090909091,
      "grad_norm": 0.05423780158162117,
      "learning_rate": 0.00019567870274092396,
      "loss": 0.3948,
      "step": 2329
    },
    {
      "epoch": 0.8472727272727273,
      "grad_norm": 0.07215525954961777,
      "learning_rate": 0.00019567501014347918,
      "loss": 0.4005,
      "step": 2330
    },
    {
      "epoch": 0.8476363636363636,
      "grad_norm": 0.04996762424707413,
      "learning_rate": 0.00019567131600389597,
      "loss": 0.3789,
      "step": 2331
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.0682842954993248,
      "learning_rate": 0.00019566762032223394,
      "loss": 0.3536,
      "step": 2332
    },
    {
      "epoch": 0.8483636363636363,
      "grad_norm": 0.05527469888329506,
      "learning_rate": 0.00019566392309855265,
      "loss": 0.3189,
      "step": 2333
    },
    {
      "epoch": 0.8487272727272728,
      "grad_norm": 0.059678152203559875,
      "learning_rate": 0.00019566022433291168,
      "loss": 0.3574,
      "step": 2334
    },
    {
      "epoch": 0.8490909090909091,
      "grad_norm": 0.07027534395456314,
      "learning_rate": 0.00019565652402537067,
      "loss": 0.3708,
      "step": 2335
    },
    {
      "epoch": 0.8494545454545455,
      "grad_norm": 0.06619975715875626,
      "learning_rate": 0.00019565282217598923,
      "loss": 0.3772,
      "step": 2336
    },
    {
      "epoch": 0.8498181818181818,
      "grad_norm": 0.06413432210683823,
      "learning_rate": 0.00019564911878482705,
      "loss": 0.3913,
      "step": 2337
    },
    {
      "epoch": 0.8501818181818181,
      "grad_norm": 0.047924984246492386,
      "learning_rate": 0.00019564541385194386,
      "loss": 0.3169,
      "step": 2338
    },
    {
      "epoch": 0.8505454545454545,
      "grad_norm": 0.06428317725658417,
      "learning_rate": 0.00019564170737739928,
      "loss": 0.3377,
      "step": 2339
    },
    {
      "epoch": 0.850909090909091,
      "grad_norm": 0.05403987690806389,
      "learning_rate": 0.00019563799936125315,
      "loss": 0.2857,
      "step": 2340
    },
    {
      "epoch": 0.8512727272727273,
      "grad_norm": 0.0656968504190445,
      "learning_rate": 0.00019563428980356518,
      "loss": 0.3581,
      "step": 2341
    },
    {
      "epoch": 0.8516363636363636,
      "grad_norm": 0.06847719103097916,
      "learning_rate": 0.00019563057870439522,
      "loss": 0.3962,
      "step": 2342
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.05422191321849823,
      "learning_rate": 0.00019562686606380302,
      "loss": 0.3375,
      "step": 2343
    },
    {
      "epoch": 0.8523636363636363,
      "grad_norm": 0.052671682089567184,
      "learning_rate": 0.00019562315188184845,
      "loss": 0.2859,
      "step": 2344
    },
    {
      "epoch": 0.8527272727272728,
      "grad_norm": 0.059311140328645706,
      "learning_rate": 0.0001956194361585914,
      "loss": 0.3062,
      "step": 2345
    },
    {
      "epoch": 0.8530909090909091,
      "grad_norm": 0.05251910164952278,
      "learning_rate": 0.00019561571889409172,
      "loss": 0.3747,
      "step": 2346
    },
    {
      "epoch": 0.8534545454545455,
      "grad_norm": 0.061720460653305054,
      "learning_rate": 0.00019561200008840935,
      "loss": 0.319,
      "step": 2347
    },
    {
      "epoch": 0.8538181818181818,
      "grad_norm": 0.04712929576635361,
      "learning_rate": 0.00019560827974160426,
      "loss": 0.3007,
      "step": 2348
    },
    {
      "epoch": 0.8541818181818182,
      "grad_norm": 0.06664367765188217,
      "learning_rate": 0.00019560455785373634,
      "loss": 0.3758,
      "step": 2349
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.058772094547748566,
      "learning_rate": 0.00019560083442486564,
      "loss": 0.368,
      "step": 2350
    },
    {
      "epoch": 0.854909090909091,
      "grad_norm": 0.06416335701942444,
      "learning_rate": 0.00019559710945505216,
      "loss": 0.3827,
      "step": 2351
    },
    {
      "epoch": 0.8552727272727273,
      "grad_norm": 0.05476945638656616,
      "learning_rate": 0.00019559338294435595,
      "loss": 0.3279,
      "step": 2352
    },
    {
      "epoch": 0.8556363636363636,
      "grad_norm": 0.04735362529754639,
      "learning_rate": 0.0001955896548928371,
      "loss": 0.3098,
      "step": 2353
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.05845305696129799,
      "learning_rate": 0.00019558592530055565,
      "loss": 0.364,
      "step": 2354
    },
    {
      "epoch": 0.8563636363636363,
      "grad_norm": 0.05717431753873825,
      "learning_rate": 0.0001955821941675717,
      "loss": 0.3722,
      "step": 2355
    },
    {
      "epoch": 0.8567272727272728,
      "grad_norm": 0.05205507203936577,
      "learning_rate": 0.00019557846149394545,
      "loss": 0.3399,
      "step": 2356
    },
    {
      "epoch": 0.8570909090909091,
      "grad_norm": 0.05242934823036194,
      "learning_rate": 0.00019557472727973707,
      "loss": 0.319,
      "step": 2357
    },
    {
      "epoch": 0.8574545454545455,
      "grad_norm": 0.06779282540082932,
      "learning_rate": 0.0001955709915250067,
      "loss": 0.3969,
      "step": 2358
    },
    {
      "epoch": 0.8578181818181818,
      "grad_norm": 0.06919548660516739,
      "learning_rate": 0.00019556725422981456,
      "loss": 0.3281,
      "step": 2359
    },
    {
      "epoch": 0.8581818181818182,
      "grad_norm": 0.05737598240375519,
      "learning_rate": 0.00019556351539422092,
      "loss": 0.3734,
      "step": 2360
    },
    {
      "epoch": 0.8585454545454545,
      "grad_norm": 0.055773697793483734,
      "learning_rate": 0.00019555977501828603,
      "loss": 0.376,
      "step": 2361
    },
    {
      "epoch": 0.858909090909091,
      "grad_norm": 0.04999808594584465,
      "learning_rate": 0.00019555603310207018,
      "loss": 0.3084,
      "step": 2362
    },
    {
      "epoch": 0.8592727272727273,
      "grad_norm": 0.06428910791873932,
      "learning_rate": 0.0001955522896456337,
      "loss": 0.3389,
      "step": 2363
    },
    {
      "epoch": 0.8596363636363636,
      "grad_norm": 0.04906076192855835,
      "learning_rate": 0.0001955485446490369,
      "loss": 0.3095,
      "step": 2364
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.05114921182394028,
      "learning_rate": 0.00019554479811234016,
      "loss": 0.3065,
      "step": 2365
    },
    {
      "epoch": 0.8603636363636363,
      "grad_norm": 0.043973684310913086,
      "learning_rate": 0.00019554105003560386,
      "loss": 0.2909,
      "step": 2366
    },
    {
      "epoch": 0.8607272727272727,
      "grad_norm": 0.05141313746571541,
      "learning_rate": 0.0001955373004188884,
      "loss": 0.3109,
      "step": 2367
    },
    {
      "epoch": 0.8610909090909091,
      "grad_norm": 0.053454216569662094,
      "learning_rate": 0.00019553354926225426,
      "loss": 0.3407,
      "step": 2368
    },
    {
      "epoch": 0.8614545454545455,
      "grad_norm": 0.056103967130184174,
      "learning_rate": 0.00019552979656576187,
      "loss": 0.389,
      "step": 2369
    },
    {
      "epoch": 0.8618181818181818,
      "grad_norm": 0.05550123006105423,
      "learning_rate": 0.00019552604232947173,
      "loss": 0.3786,
      "step": 2370
    },
    {
      "epoch": 0.8621818181818182,
      "grad_norm": 0.07628828287124634,
      "learning_rate": 0.00019552228655344433,
      "loss": 0.4222,
      "step": 2371
    },
    {
      "epoch": 0.8625454545454545,
      "grad_norm": 0.059842549264431,
      "learning_rate": 0.00019551852923774027,
      "loss": 0.3708,
      "step": 2372
    },
    {
      "epoch": 0.862909090909091,
      "grad_norm": 0.049547646194696426,
      "learning_rate": 0.00019551477038242006,
      "loss": 0.3118,
      "step": 2373
    },
    {
      "epoch": 0.8632727272727273,
      "grad_norm": 0.06490731984376907,
      "learning_rate": 0.00019551100998754426,
      "loss": 0.4402,
      "step": 2374
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.0554937869310379,
      "learning_rate": 0.00019550724805317355,
      "loss": 0.3724,
      "step": 2375
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.04603312537074089,
      "learning_rate": 0.00019550348457936852,
      "loss": 0.3219,
      "step": 2376
    },
    {
      "epoch": 0.8643636363636363,
      "grad_norm": 0.044064294546842575,
      "learning_rate": 0.00019549971956618986,
      "loss": 0.3264,
      "step": 2377
    },
    {
      "epoch": 0.8647272727272727,
      "grad_norm": 0.045430988073349,
      "learning_rate": 0.00019549595301369824,
      "loss": 0.3252,
      "step": 2378
    },
    {
      "epoch": 0.8650909090909091,
      "grad_norm": 0.04864330217242241,
      "learning_rate": 0.00019549218492195437,
      "loss": 0.276,
      "step": 2379
    },
    {
      "epoch": 0.8654545454545455,
      "grad_norm": 0.04492944851517677,
      "learning_rate": 0.00019548841529101903,
      "loss": 0.3009,
      "step": 2380
    },
    {
      "epoch": 0.8658181818181818,
      "grad_norm": 0.04596537724137306,
      "learning_rate": 0.0001954846441209529,
      "loss": 0.3451,
      "step": 2381
    },
    {
      "epoch": 0.8661818181818182,
      "grad_norm": 0.043812595307826996,
      "learning_rate": 0.0001954808714118168,
      "loss": 0.3161,
      "step": 2382
    },
    {
      "epoch": 0.8665454545454545,
      "grad_norm": 0.06319582462310791,
      "learning_rate": 0.00019547709716367158,
      "loss": 0.4135,
      "step": 2383
    },
    {
      "epoch": 0.866909090909091,
      "grad_norm": 0.04661430045962334,
      "learning_rate": 0.00019547332137657804,
      "loss": 0.353,
      "step": 2384
    },
    {
      "epoch": 0.8672727272727273,
      "grad_norm": 0.062306154519319534,
      "learning_rate": 0.000195469544050597,
      "loss": 0.361,
      "step": 2385
    },
    {
      "epoch": 0.8676363636363637,
      "grad_norm": 0.05545508489012718,
      "learning_rate": 0.00019546576518578943,
      "loss": 0.3181,
      "step": 2386
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.05950487405061722,
      "learning_rate": 0.0001954619847822162,
      "loss": 0.389,
      "step": 2387
    },
    {
      "epoch": 0.8683636363636363,
      "grad_norm": 0.05446891859173775,
      "learning_rate": 0.00019545820283993823,
      "loss": 0.3398,
      "step": 2388
    },
    {
      "epoch": 0.8687272727272727,
      "grad_norm": 0.05287514626979828,
      "learning_rate": 0.00019545441935901647,
      "loss": 0.3242,
      "step": 2389
    },
    {
      "epoch": 0.8690909090909091,
      "grad_norm": 0.04366028308868408,
      "learning_rate": 0.00019545063433951194,
      "loss": 0.2559,
      "step": 2390
    },
    {
      "epoch": 0.8694545454545455,
      "grad_norm": 0.07009780406951904,
      "learning_rate": 0.00019544684778148566,
      "loss": 0.382,
      "step": 2391
    },
    {
      "epoch": 0.8698181818181818,
      "grad_norm": 0.06563711166381836,
      "learning_rate": 0.00019544305968499862,
      "loss": 0.4241,
      "step": 2392
    },
    {
      "epoch": 0.8701818181818182,
      "grad_norm": 0.04994687810540199,
      "learning_rate": 0.00019543927005011188,
      "loss": 0.2634,
      "step": 2393
    },
    {
      "epoch": 0.8705454545454545,
      "grad_norm": 0.04880126565694809,
      "learning_rate": 0.00019543547887688657,
      "loss": 0.3087,
      "step": 2394
    },
    {
      "epoch": 0.8709090909090909,
      "grad_norm": 0.05027628690004349,
      "learning_rate": 0.00019543168616538374,
      "loss": 0.3149,
      "step": 2395
    },
    {
      "epoch": 0.8712727272727273,
      "grad_norm": 0.05696689337491989,
      "learning_rate": 0.00019542789191566456,
      "loss": 0.3559,
      "step": 2396
    },
    {
      "epoch": 0.8716363636363637,
      "grad_norm": 0.06192326173186302,
      "learning_rate": 0.00019542409612779017,
      "loss": 0.3903,
      "step": 2397
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.045689430087804794,
      "learning_rate": 0.0001954202988018218,
      "loss": 0.2821,
      "step": 2398
    },
    {
      "epoch": 0.8723636363636363,
      "grad_norm": 0.05132630467414856,
      "learning_rate": 0.00019541649993782057,
      "loss": 0.3663,
      "step": 2399
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.04586102068424225,
      "learning_rate": 0.0001954126995358478,
      "loss": 0.3087,
      "step": 2400
    },
    {
      "epoch": 0.8730909090909091,
      "grad_norm": 0.04665743559598923,
      "learning_rate": 0.00019540889759596465,
      "loss": 0.3157,
      "step": 2401
    },
    {
      "epoch": 0.8734545454545455,
      "grad_norm": 0.04547727853059769,
      "learning_rate": 0.00019540509411823253,
      "loss": 0.2741,
      "step": 2402
    },
    {
      "epoch": 0.8738181818181818,
      "grad_norm": 0.04774491861462593,
      "learning_rate": 0.00019540128910271263,
      "loss": 0.367,
      "step": 2403
    },
    {
      "epoch": 0.8741818181818182,
      "grad_norm": 0.051426857709884644,
      "learning_rate": 0.00019539748254946633,
      "loss": 0.3466,
      "step": 2404
    },
    {
      "epoch": 0.8745454545454545,
      "grad_norm": 0.051717180758714676,
      "learning_rate": 0.00019539367445855498,
      "loss": 0.2982,
      "step": 2405
    },
    {
      "epoch": 0.8749090909090909,
      "grad_norm": 0.07227339595556259,
      "learning_rate": 0.00019538986483004,
      "loss": 0.4198,
      "step": 2406
    },
    {
      "epoch": 0.8752727272727273,
      "grad_norm": 0.04454885423183441,
      "learning_rate": 0.0001953860536639827,
      "loss": 0.328,
      "step": 2407
    },
    {
      "epoch": 0.8756363636363637,
      "grad_norm": 0.04773448780179024,
      "learning_rate": 0.00019538224096044463,
      "loss": 0.2813,
      "step": 2408
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.05724949762225151,
      "learning_rate": 0.00019537842671948712,
      "loss": 0.2898,
      "step": 2409
    },
    {
      "epoch": 0.8763636363636363,
      "grad_norm": 0.0665166825056076,
      "learning_rate": 0.00019537461094117173,
      "loss": 0.3941,
      "step": 2410
    },
    {
      "epoch": 0.8767272727272727,
      "grad_norm": 0.07121860235929489,
      "learning_rate": 0.00019537079362555999,
      "loss": 0.3034,
      "step": 2411
    },
    {
      "epoch": 0.8770909090909091,
      "grad_norm": 0.05265384167432785,
      "learning_rate": 0.00019536697477271335,
      "loss": 0.3553,
      "step": 2412
    },
    {
      "epoch": 0.8774545454545455,
      "grad_norm": 0.053484100848436356,
      "learning_rate": 0.00019536315438269342,
      "loss": 0.3269,
      "step": 2413
    },
    {
      "epoch": 0.8778181818181818,
      "grad_norm": 0.05106482282280922,
      "learning_rate": 0.00019535933245556173,
      "loss": 0.3452,
      "step": 2414
    },
    {
      "epoch": 0.8781818181818182,
      "grad_norm": 0.04336182028055191,
      "learning_rate": 0.00019535550899137994,
      "loss": 0.2661,
      "step": 2415
    },
    {
      "epoch": 0.8785454545454545,
      "grad_norm": 0.05046467110514641,
      "learning_rate": 0.00019535168399020965,
      "loss": 0.3653,
      "step": 2416
    },
    {
      "epoch": 0.8789090909090909,
      "grad_norm": 0.048551809042692184,
      "learning_rate": 0.00019534785745211253,
      "loss": 0.3321,
      "step": 2417
    },
    {
      "epoch": 0.8792727272727273,
      "grad_norm": 0.0547238364815712,
      "learning_rate": 0.00019534402937715023,
      "loss": 0.3313,
      "step": 2418
    },
    {
      "epoch": 0.8796363636363637,
      "grad_norm": 0.05594540759921074,
      "learning_rate": 0.00019534019976538445,
      "loss": 0.3487,
      "step": 2419
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.04663039371371269,
      "learning_rate": 0.00019533636861687696,
      "loss": 0.359,
      "step": 2420
    },
    {
      "epoch": 0.8803636363636363,
      "grad_norm": 0.049889180809259415,
      "learning_rate": 0.0001953325359316895,
      "loss": 0.3521,
      "step": 2421
    },
    {
      "epoch": 0.8807272727272727,
      "grad_norm": 0.054184623062610626,
      "learning_rate": 0.0001953287017098838,
      "loss": 0.3452,
      "step": 2422
    },
    {
      "epoch": 0.881090909090909,
      "grad_norm": 0.05358860641717911,
      "learning_rate": 0.0001953248659515217,
      "loss": 0.3471,
      "step": 2423
    },
    {
      "epoch": 0.8814545454545455,
      "grad_norm": 0.05486834794282913,
      "learning_rate": 0.00019532102865666505,
      "loss": 0.3477,
      "step": 2424
    },
    {
      "epoch": 0.8818181818181818,
      "grad_norm": 0.0633624866604805,
      "learning_rate": 0.00019531718982537566,
      "loss": 0.3905,
      "step": 2425
    },
    {
      "epoch": 0.8821818181818182,
      "grad_norm": 0.04850931838154793,
      "learning_rate": 0.00019531334945771542,
      "loss": 0.3045,
      "step": 2426
    },
    {
      "epoch": 0.8825454545454545,
      "grad_norm": 0.051116351038217545,
      "learning_rate": 0.00019530950755374622,
      "loss": 0.3454,
      "step": 2427
    },
    {
      "epoch": 0.8829090909090909,
      "grad_norm": 0.052262090146541595,
      "learning_rate": 0.00019530566411353,
      "loss": 0.2957,
      "step": 2428
    },
    {
      "epoch": 0.8832727272727273,
      "grad_norm": 0.05727868527173996,
      "learning_rate": 0.00019530181913712872,
      "loss": 0.3803,
      "step": 2429
    },
    {
      "epoch": 0.8836363636363637,
      "grad_norm": 0.04090183973312378,
      "learning_rate": 0.00019529797262460435,
      "loss": 0.2909,
      "step": 2430
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.054130323231220245,
      "learning_rate": 0.00019529412457601887,
      "loss": 0.2689,
      "step": 2431
    },
    {
      "epoch": 0.8843636363636364,
      "grad_norm": 0.05085660144686699,
      "learning_rate": 0.0001952902749914343,
      "loss": 0.3297,
      "step": 2432
    },
    {
      "epoch": 0.8847272727272727,
      "grad_norm": 0.04580255225300789,
      "learning_rate": 0.00019528642387091276,
      "loss": 0.3114,
      "step": 2433
    },
    {
      "epoch": 0.885090909090909,
      "grad_norm": 0.05526701733469963,
      "learning_rate": 0.00019528257121451622,
      "loss": 0.3389,
      "step": 2434
    },
    {
      "epoch": 0.8854545454545455,
      "grad_norm": 0.049829691648483276,
      "learning_rate": 0.00019527871702230685,
      "loss": 0.3203,
      "step": 2435
    },
    {
      "epoch": 0.8858181818181818,
      "grad_norm": 0.05743805691599846,
      "learning_rate": 0.00019527486129434677,
      "loss": 0.3629,
      "step": 2436
    },
    {
      "epoch": 0.8861818181818182,
      "grad_norm": 0.07465896755456924,
      "learning_rate": 0.00019527100403069812,
      "loss": 0.4314,
      "step": 2437
    },
    {
      "epoch": 0.8865454545454545,
      "grad_norm": 0.0725192278623581,
      "learning_rate": 0.00019526714523142302,
      "loss": 0.3462,
      "step": 2438
    },
    {
      "epoch": 0.8869090909090909,
      "grad_norm": 0.06269069761037827,
      "learning_rate": 0.00019526328489658377,
      "loss": 0.3721,
      "step": 2439
    },
    {
      "epoch": 0.8872727272727273,
      "grad_norm": 0.04627225548028946,
      "learning_rate": 0.0001952594230262425,
      "loss": 0.3064,
      "step": 2440
    },
    {
      "epoch": 0.8876363636363637,
      "grad_norm": 0.06055116280913353,
      "learning_rate": 0.0001952555596204615,
      "loss": 0.3265,
      "step": 2441
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.05470171943306923,
      "learning_rate": 0.00019525169467930303,
      "loss": 0.3219,
      "step": 2442
    },
    {
      "epoch": 0.8883636363636364,
      "grad_norm": 0.05750682204961777,
      "learning_rate": 0.00019524782820282943,
      "loss": 0.3335,
      "step": 2443
    },
    {
      "epoch": 0.8887272727272727,
      "grad_norm": 0.05672356113791466,
      "learning_rate": 0.00019524396019110295,
      "loss": 0.3357,
      "step": 2444
    },
    {
      "epoch": 0.889090909090909,
      "grad_norm": 0.05072270333766937,
      "learning_rate": 0.000195240090644186,
      "loss": 0.3006,
      "step": 2445
    },
    {
      "epoch": 0.8894545454545455,
      "grad_norm": 0.040014516562223434,
      "learning_rate": 0.0001952362195621409,
      "loss": 0.2711,
      "step": 2446
    },
    {
      "epoch": 0.8898181818181818,
      "grad_norm": 0.07899701595306396,
      "learning_rate": 0.0001952323469450301,
      "loss": 0.3686,
      "step": 2447
    },
    {
      "epoch": 0.8901818181818182,
      "grad_norm": 0.04432325065135956,
      "learning_rate": 0.000195228472792916,
      "loss": 0.275,
      "step": 2448
    },
    {
      "epoch": 0.8905454545454545,
      "grad_norm": 0.052153874188661575,
      "learning_rate": 0.00019522459710586098,
      "loss": 0.297,
      "step": 2449
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.05808703601360321,
      "learning_rate": 0.0001952207198839276,
      "loss": 0.3148,
      "step": 2450
    },
    {
      "epoch": 0.8912727272727273,
      "grad_norm": 0.048950571566820145,
      "learning_rate": 0.00019521684112717833,
      "loss": 0.3473,
      "step": 2451
    },
    {
      "epoch": 0.8916363636363637,
      "grad_norm": 0.05331537500023842,
      "learning_rate": 0.00019521296083567567,
      "loss": 0.3665,
      "step": 2452
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.052875835448503494,
      "learning_rate": 0.00019520907900948218,
      "loss": 0.3548,
      "step": 2453
    },
    {
      "epoch": 0.8923636363636364,
      "grad_norm": 0.06257304549217224,
      "learning_rate": 0.0001952051956486604,
      "loss": 0.3915,
      "step": 2454
    },
    {
      "epoch": 0.8927272727272727,
      "grad_norm": 0.04525791108608246,
      "learning_rate": 0.00019520131075327298,
      "loss": 0.3388,
      "step": 2455
    },
    {
      "epoch": 0.893090909090909,
      "grad_norm": 0.051892172545194626,
      "learning_rate": 0.00019519742432338254,
      "loss": 0.3351,
      "step": 2456
    },
    {
      "epoch": 0.8934545454545455,
      "grad_norm": 0.051587335765361786,
      "learning_rate": 0.00019519353635905165,
      "loss": 0.3329,
      "step": 2457
    },
    {
      "epoch": 0.8938181818181818,
      "grad_norm": 0.05833864212036133,
      "learning_rate": 0.000195189646860343,
      "loss": 0.407,
      "step": 2458
    },
    {
      "epoch": 0.8941818181818182,
      "grad_norm": 0.04616083204746246,
      "learning_rate": 0.00019518575582731936,
      "loss": 0.3229,
      "step": 2459
    },
    {
      "epoch": 0.8945454545454545,
      "grad_norm": 0.04931912198662758,
      "learning_rate": 0.00019518186326004334,
      "loss": 0.2937,
      "step": 2460
    },
    {
      "epoch": 0.8949090909090909,
      "grad_norm": 0.0497848205268383,
      "learning_rate": 0.00019517796915857776,
      "loss": 0.3201,
      "step": 2461
    },
    {
      "epoch": 0.8952727272727272,
      "grad_norm": 0.06341951340436935,
      "learning_rate": 0.00019517407352298536,
      "loss": 0.3846,
      "step": 2462
    },
    {
      "epoch": 0.8956363636363637,
      "grad_norm": 0.048314157873392105,
      "learning_rate": 0.0001951701763533289,
      "loss": 0.4071,
      "step": 2463
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.0586979053914547,
      "learning_rate": 0.00019516627764967125,
      "loss": 0.3387,
      "step": 2464
    },
    {
      "epoch": 0.8963636363636364,
      "grad_norm": 0.05853739380836487,
      "learning_rate": 0.00019516237741207525,
      "loss": 0.3696,
      "step": 2465
    },
    {
      "epoch": 0.8967272727272727,
      "grad_norm": 0.04204816371202469,
      "learning_rate": 0.00019515847564060374,
      "loss": 0.2705,
      "step": 2466
    },
    {
      "epoch": 0.897090909090909,
      "grad_norm": 0.061713166534900665,
      "learning_rate": 0.0001951545723353196,
      "loss": 0.3239,
      "step": 2467
    },
    {
      "epoch": 0.8974545454545455,
      "grad_norm": 0.04687447473406792,
      "learning_rate": 0.00019515066749628576,
      "loss": 0.2794,
      "step": 2468
    },
    {
      "epoch": 0.8978181818181818,
      "grad_norm": 0.04323013871908188,
      "learning_rate": 0.00019514676112356518,
      "loss": 0.318,
      "step": 2469
    },
    {
      "epoch": 0.8981818181818182,
      "grad_norm": 0.05132758617401123,
      "learning_rate": 0.00019514285321722082,
      "loss": 0.323,
      "step": 2470
    },
    {
      "epoch": 0.8985454545454545,
      "grad_norm": 0.04808453097939491,
      "learning_rate": 0.0001951389437773156,
      "loss": 0.3447,
      "step": 2471
    },
    {
      "epoch": 0.8989090909090909,
      "grad_norm": 0.052818238735198975,
      "learning_rate": 0.00019513503280391264,
      "loss": 0.3328,
      "step": 2472
    },
    {
      "epoch": 0.8992727272727272,
      "grad_norm": 0.04870014637708664,
      "learning_rate": 0.00019513112029707493,
      "loss": 0.2936,
      "step": 2473
    },
    {
      "epoch": 0.8996363636363637,
      "grad_norm": 0.04201716557145119,
      "learning_rate": 0.00019512720625686553,
      "loss": 0.312,
      "step": 2474
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.046983689069747925,
      "learning_rate": 0.0001951232906833475,
      "loss": 0.3323,
      "step": 2475
    },
    {
      "epoch": 0.9003636363636364,
      "grad_norm": 0.0570279099047184,
      "learning_rate": 0.000195119373576584,
      "loss": 0.3549,
      "step": 2476
    },
    {
      "epoch": 0.9007272727272727,
      "grad_norm": 0.04869997501373291,
      "learning_rate": 0.00019511545493663816,
      "loss": 0.3791,
      "step": 2477
    },
    {
      "epoch": 0.901090909090909,
      "grad_norm": 0.05322806164622307,
      "learning_rate": 0.00019511153476357317,
      "loss": 0.3823,
      "step": 2478
    },
    {
      "epoch": 0.9014545454545455,
      "grad_norm": 0.056161947548389435,
      "learning_rate": 0.0001951076130574521,
      "loss": 0.2803,
      "step": 2479
    },
    {
      "epoch": 0.9018181818181819,
      "grad_norm": 0.0476723350584507,
      "learning_rate": 0.00019510368981833832,
      "loss": 0.3376,
      "step": 2480
    },
    {
      "epoch": 0.9021818181818182,
      "grad_norm": 0.05643802136182785,
      "learning_rate": 0.00019509976504629496,
      "loss": 0.3729,
      "step": 2481
    },
    {
      "epoch": 0.9025454545454545,
      "grad_norm": 0.04554709792137146,
      "learning_rate": 0.00019509583874138532,
      "loss": 0.3007,
      "step": 2482
    },
    {
      "epoch": 0.9029090909090909,
      "grad_norm": 0.05457789823412895,
      "learning_rate": 0.0001950919109036727,
      "loss": 0.3106,
      "step": 2483
    },
    {
      "epoch": 0.9032727272727272,
      "grad_norm": 0.058291949331760406,
      "learning_rate": 0.00019508798153322035,
      "loss": 0.3719,
      "step": 2484
    },
    {
      "epoch": 0.9036363636363637,
      "grad_norm": 0.06501329690217972,
      "learning_rate": 0.00019508405063009166,
      "loss": 0.3765,
      "step": 2485
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.047717440873384476,
      "learning_rate": 0.00019508011819435001,
      "loss": 0.3299,
      "step": 2486
    },
    {
      "epoch": 0.9043636363636364,
      "grad_norm": 0.04313724860548973,
      "learning_rate": 0.00019507618422605873,
      "loss": 0.2675,
      "step": 2487
    },
    {
      "epoch": 0.9047272727272727,
      "grad_norm": 0.06318601220846176,
      "learning_rate": 0.00019507224872528125,
      "loss": 0.3974,
      "step": 2488
    },
    {
      "epoch": 0.9050909090909091,
      "grad_norm": 0.061678752303123474,
      "learning_rate": 0.000195068311692081,
      "loss": 0.3347,
      "step": 2489
    },
    {
      "epoch": 0.9054545454545454,
      "grad_norm": 0.0582156628370285,
      "learning_rate": 0.00019506437312652146,
      "loss": 0.3877,
      "step": 2490
    },
    {
      "epoch": 0.9058181818181819,
      "grad_norm": 0.04847310110926628,
      "learning_rate": 0.00019506043302866609,
      "loss": 0.3292,
      "step": 2491
    },
    {
      "epoch": 0.9061818181818182,
      "grad_norm": 0.0723884254693985,
      "learning_rate": 0.0001950564913985784,
      "loss": 0.4327,
      "step": 2492
    },
    {
      "epoch": 0.9065454545454545,
      "grad_norm": 0.05292096361517906,
      "learning_rate": 0.00019505254823632193,
      "loss": 0.2943,
      "step": 2493
    },
    {
      "epoch": 0.9069090909090909,
      "grad_norm": 0.057824570685625076,
      "learning_rate": 0.00019504860354196027,
      "loss": 0.2849,
      "step": 2494
    },
    {
      "epoch": 0.9072727272727272,
      "grad_norm": 0.057678017765283585,
      "learning_rate": 0.00019504465731555692,
      "loss": 0.3633,
      "step": 2495
    },
    {
      "epoch": 0.9076363636363637,
      "grad_norm": 0.051134075969457626,
      "learning_rate": 0.00019504070955717558,
      "loss": 0.3518,
      "step": 2496
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.05996786430478096,
      "learning_rate": 0.0001950367602668798,
      "loss": 0.3687,
      "step": 2497
    },
    {
      "epoch": 0.9083636363636364,
      "grad_norm": 0.057688575237989426,
      "learning_rate": 0.0001950328094447333,
      "loss": 0.4032,
      "step": 2498
    },
    {
      "epoch": 0.9087272727272727,
      "grad_norm": 0.04895554855465889,
      "learning_rate": 0.00019502885709079973,
      "loss": 0.2952,
      "step": 2499
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.04527316614985466,
      "learning_rate": 0.0001950249032051428,
      "loss": 0.2732,
      "step": 2500
    },
    {
      "epoch": 0.9090909090909091,
      "eval_loss": 0.3255099058151245,
      "eval_runtime": 358.7558,
      "eval_samples_per_second": 0.279,
      "eval_steps_per_second": 0.279,
      "step": 2500
    },
    {
      "epoch": 0.9094545454545454,
      "grad_norm": 0.054319240152835846,
      "learning_rate": 0.0001950209477878263,
      "loss": 0.3474,
      "step": 2501
    },
    {
      "epoch": 0.9098181818181819,
      "grad_norm": 0.0390821248292923,
      "learning_rate": 0.0001950169908389139,
      "loss": 0.2595,
      "step": 2502
    },
    {
      "epoch": 0.9101818181818182,
      "grad_norm": 0.04462587088346481,
      "learning_rate": 0.00019501303235846938,
      "loss": 0.3299,
      "step": 2503
    },
    {
      "epoch": 0.9105454545454545,
      "grad_norm": 0.04630080610513687,
      "learning_rate": 0.0001950090723465566,
      "loss": 0.3004,
      "step": 2504
    },
    {
      "epoch": 0.9109090909090909,
      "grad_norm": 0.055610984563827515,
      "learning_rate": 0.00019500511080323935,
      "loss": 0.345,
      "step": 2505
    },
    {
      "epoch": 0.9112727272727272,
      "grad_norm": 0.04914401099085808,
      "learning_rate": 0.0001950011477285815,
      "loss": 0.3304,
      "step": 2506
    },
    {
      "epoch": 0.9116363636363637,
      "grad_norm": 0.05357888340950012,
      "learning_rate": 0.00019499718312264693,
      "loss": 0.3688,
      "step": 2507
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.05121532827615738,
      "learning_rate": 0.00019499321698549955,
      "loss": 0.2632,
      "step": 2508
    },
    {
      "epoch": 0.9123636363636364,
      "grad_norm": 0.04733453691005707,
      "learning_rate": 0.0001949892493172033,
      "loss": 0.3138,
      "step": 2509
    },
    {
      "epoch": 0.9127272727272727,
      "grad_norm": 0.05304988846182823,
      "learning_rate": 0.00019498528011782206,
      "loss": 0.3148,
      "step": 2510
    },
    {
      "epoch": 0.9130909090909091,
      "grad_norm": 0.05063488334417343,
      "learning_rate": 0.0001949813093874199,
      "loss": 0.3029,
      "step": 2511
    },
    {
      "epoch": 0.9134545454545454,
      "grad_norm": 0.05189124494791031,
      "learning_rate": 0.00019497733712606078,
      "loss": 0.2984,
      "step": 2512
    },
    {
      "epoch": 0.9138181818181819,
      "grad_norm": 0.048100389540195465,
      "learning_rate": 0.0001949733633338087,
      "loss": 0.3184,
      "step": 2513
    },
    {
      "epoch": 0.9141818181818182,
      "grad_norm": 0.057375501841306686,
      "learning_rate": 0.00019496938801072776,
      "loss": 0.3191,
      "step": 2514
    },
    {
      "epoch": 0.9145454545454546,
      "grad_norm": 0.05088436231017113,
      "learning_rate": 0.00019496541115688202,
      "loss": 0.3464,
      "step": 2515
    },
    {
      "epoch": 0.9149090909090909,
      "grad_norm": 0.04351317882537842,
      "learning_rate": 0.00019496143277233555,
      "loss": 0.3157,
      "step": 2516
    },
    {
      "epoch": 0.9152727272727272,
      "grad_norm": 0.04407469555735588,
      "learning_rate": 0.00019495745285715254,
      "loss": 0.3217,
      "step": 2517
    },
    {
      "epoch": 0.9156363636363636,
      "grad_norm": 0.06507428735494614,
      "learning_rate": 0.0001949534714113971,
      "loss": 0.414,
      "step": 2518
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.046290598809719086,
      "learning_rate": 0.0001949494884351334,
      "loss": 0.3126,
      "step": 2519
    },
    {
      "epoch": 0.9163636363636364,
      "grad_norm": 0.05457916483283043,
      "learning_rate": 0.00019494550392842565,
      "loss": 0.3348,
      "step": 2520
    },
    {
      "epoch": 0.9167272727272727,
      "grad_norm": 0.05217694491147995,
      "learning_rate": 0.00019494151789133807,
      "loss": 0.2523,
      "step": 2521
    },
    {
      "epoch": 0.9170909090909091,
      "grad_norm": 0.06830007582902908,
      "learning_rate": 0.00019493753032393492,
      "loss": 0.4295,
      "step": 2522
    },
    {
      "epoch": 0.9174545454545454,
      "grad_norm": 0.05055186152458191,
      "learning_rate": 0.00019493354122628048,
      "loss": 0.3207,
      "step": 2523
    },
    {
      "epoch": 0.9178181818181819,
      "grad_norm": 0.04861760884523392,
      "learning_rate": 0.000194929550598439,
      "loss": 0.3188,
      "step": 2524
    },
    {
      "epoch": 0.9181818181818182,
      "grad_norm": 0.05657278001308441,
      "learning_rate": 0.00019492555844047486,
      "loss": 0.3167,
      "step": 2525
    },
    {
      "epoch": 0.9185454545454546,
      "grad_norm": 0.04924967139959335,
      "learning_rate": 0.00019492156475245235,
      "loss": 0.3123,
      "step": 2526
    },
    {
      "epoch": 0.9189090909090909,
      "grad_norm": 0.056403663009405136,
      "learning_rate": 0.0001949175695344359,
      "loss": 0.318,
      "step": 2527
    },
    {
      "epoch": 0.9192727272727272,
      "grad_norm": 0.051156528294086456,
      "learning_rate": 0.0001949135727864899,
      "loss": 0.3492,
      "step": 2528
    },
    {
      "epoch": 0.9196363636363636,
      "grad_norm": 0.05069895461201668,
      "learning_rate": 0.00019490957450867872,
      "loss": 0.3149,
      "step": 2529
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.05431334301829338,
      "learning_rate": 0.00019490557470106686,
      "loss": 0.3763,
      "step": 2530
    },
    {
      "epoch": 0.9203636363636364,
      "grad_norm": 0.05894423648715019,
      "learning_rate": 0.0001949015733637188,
      "loss": 0.3982,
      "step": 2531
    },
    {
      "epoch": 0.9207272727272727,
      "grad_norm": 0.04249952733516693,
      "learning_rate": 0.00019489757049669895,
      "loss": 0.2285,
      "step": 2532
    },
    {
      "epoch": 0.9210909090909091,
      "grad_norm": 0.05620104819536209,
      "learning_rate": 0.00019489356610007192,
      "loss": 0.2694,
      "step": 2533
    },
    {
      "epoch": 0.9214545454545454,
      "grad_norm": 0.060701172798871994,
      "learning_rate": 0.00019488956017390226,
      "loss": 0.3458,
      "step": 2534
    },
    {
      "epoch": 0.9218181818181819,
      "grad_norm": 0.05432138219475746,
      "learning_rate": 0.00019488555271825444,
      "loss": 0.3629,
      "step": 2535
    },
    {
      "epoch": 0.9221818181818182,
      "grad_norm": 0.05724603682756424,
      "learning_rate": 0.00019488154373319315,
      "loss": 0.3188,
      "step": 2536
    },
    {
      "epoch": 0.9225454545454546,
      "grad_norm": 0.06293283402919769,
      "learning_rate": 0.00019487753321878297,
      "loss": 0.3699,
      "step": 2537
    },
    {
      "epoch": 0.9229090909090909,
      "grad_norm": 0.05486124008893967,
      "learning_rate": 0.00019487352117508855,
      "loss": 0.4227,
      "step": 2538
    },
    {
      "epoch": 0.9232727272727272,
      "grad_norm": 0.046568404883146286,
      "learning_rate": 0.00019486950760217453,
      "loss": 0.3345,
      "step": 2539
    },
    {
      "epoch": 0.9236363636363636,
      "grad_norm": 0.05405017361044884,
      "learning_rate": 0.00019486549250010565,
      "loss": 0.3276,
      "step": 2540
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.05185917019844055,
      "learning_rate": 0.00019486147586894663,
      "loss": 0.3105,
      "step": 2541
    },
    {
      "epoch": 0.9243636363636364,
      "grad_norm": 0.0603862926363945,
      "learning_rate": 0.00019485745770876216,
      "loss": 0.3473,
      "step": 2542
    },
    {
      "epoch": 0.9247272727272727,
      "grad_norm": 0.052804313600063324,
      "learning_rate": 0.00019485343801961706,
      "loss": 0.3391,
      "step": 2543
    },
    {
      "epoch": 0.9250909090909091,
      "grad_norm": 0.06068284437060356,
      "learning_rate": 0.00019484941680157606,
      "loss": 0.3804,
      "step": 2544
    },
    {
      "epoch": 0.9254545454545454,
      "grad_norm": 0.047574885189533234,
      "learning_rate": 0.00019484539405470406,
      "loss": 0.3889,
      "step": 2545
    },
    {
      "epoch": 0.9258181818181819,
      "grad_norm": 0.050268881022930145,
      "learning_rate": 0.0001948413697790658,
      "loss": 0.3464,
      "step": 2546
    },
    {
      "epoch": 0.9261818181818182,
      "grad_norm": 0.04963025078177452,
      "learning_rate": 0.00019483734397472628,
      "loss": 0.3132,
      "step": 2547
    },
    {
      "epoch": 0.9265454545454546,
      "grad_norm": 0.046523287892341614,
      "learning_rate": 0.00019483331664175025,
      "loss": 0.3246,
      "step": 2548
    },
    {
      "epoch": 0.9269090909090909,
      "grad_norm": 0.052834879606962204,
      "learning_rate": 0.0001948292877802027,
      "loss": 0.3553,
      "step": 2549
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.06098887324333191,
      "learning_rate": 0.00019482525739014856,
      "loss": 0.3802,
      "step": 2550
    },
    {
      "epoch": 0.9276363636363636,
      "grad_norm": 0.04912972450256348,
      "learning_rate": 0.0001948212254716528,
      "loss": 0.3401,
      "step": 2551
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.048166293650865555,
      "learning_rate": 0.00019481719202478032,
      "loss": 0.3364,
      "step": 2552
    },
    {
      "epoch": 0.9283636363636364,
      "grad_norm": 0.04932403564453125,
      "learning_rate": 0.0001948131570495963,
      "loss": 0.3206,
      "step": 2553
    },
    {
      "epoch": 0.9287272727272727,
      "grad_norm": 0.04156037047505379,
      "learning_rate": 0.00019480912054616566,
      "loss": 0.268,
      "step": 2554
    },
    {
      "epoch": 0.9290909090909091,
      "grad_norm": 0.056677281856536865,
      "learning_rate": 0.00019480508251455345,
      "loss": 0.3833,
      "step": 2555
    },
    {
      "epoch": 0.9294545454545454,
      "grad_norm": 0.05988434702157974,
      "learning_rate": 0.00019480104295482482,
      "loss": 0.3787,
      "step": 2556
    },
    {
      "epoch": 0.9298181818181818,
      "grad_norm": 0.05223598703742027,
      "learning_rate": 0.00019479700186704487,
      "loss": 0.3711,
      "step": 2557
    },
    {
      "epoch": 0.9301818181818182,
      "grad_norm": 0.04875669628381729,
      "learning_rate": 0.00019479295925127873,
      "loss": 0.2888,
      "step": 2558
    },
    {
      "epoch": 0.9305454545454546,
      "grad_norm": 0.07382306456565857,
      "learning_rate": 0.00019478891510759153,
      "loss": 0.3867,
      "step": 2559
    },
    {
      "epoch": 0.9309090909090909,
      "grad_norm": 0.06950609385967255,
      "learning_rate": 0.00019478486943604853,
      "loss": 0.2996,
      "step": 2560
    },
    {
      "epoch": 0.9312727272727273,
      "grad_norm": 0.043674878776073456,
      "learning_rate": 0.00019478082223671483,
      "loss": 0.2974,
      "step": 2561
    },
    {
      "epoch": 0.9316363636363636,
      "grad_norm": 0.048254936933517456,
      "learning_rate": 0.00019477677350965576,
      "loss": 0.3139,
      "step": 2562
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.05430993065237999,
      "learning_rate": 0.00019477272325493653,
      "loss": 0.3608,
      "step": 2563
    },
    {
      "epoch": 0.9323636363636364,
      "grad_norm": 0.051331862807273865,
      "learning_rate": 0.00019476867147262246,
      "loss": 0.3098,
      "step": 2564
    },
    {
      "epoch": 0.9327272727272727,
      "grad_norm": 0.04236968606710434,
      "learning_rate": 0.00019476461816277883,
      "loss": 0.2709,
      "step": 2565
    },
    {
      "epoch": 0.9330909090909091,
      "grad_norm": 0.049531999975442886,
      "learning_rate": 0.00019476056332547095,
      "loss": 0.2847,
      "step": 2566
    },
    {
      "epoch": 0.9334545454545454,
      "grad_norm": 0.06293221563100815,
      "learning_rate": 0.00019475650696076425,
      "loss": 0.3503,
      "step": 2567
    },
    {
      "epoch": 0.9338181818181818,
      "grad_norm": 0.06958216428756714,
      "learning_rate": 0.00019475244906872404,
      "loss": 0.3193,
      "step": 2568
    },
    {
      "epoch": 0.9341818181818182,
      "grad_norm": 0.04532288759946823,
      "learning_rate": 0.00019474838964941577,
      "loss": 0.3221,
      "step": 2569
    },
    {
      "epoch": 0.9345454545454546,
      "grad_norm": 0.060514915734529495,
      "learning_rate": 0.00019474432870290488,
      "loss": 0.4267,
      "step": 2570
    },
    {
      "epoch": 0.9349090909090909,
      "grad_norm": 0.06127491593360901,
      "learning_rate": 0.0001947402662292568,
      "loss": 0.3431,
      "step": 2571
    },
    {
      "epoch": 0.9352727272727273,
      "grad_norm": 0.04178087040781975,
      "learning_rate": 0.00019473620222853702,
      "loss": 0.2917,
      "step": 2572
    },
    {
      "epoch": 0.9356363636363636,
      "grad_norm": 0.06284894049167633,
      "learning_rate": 0.00019473213670081102,
      "loss": 0.3889,
      "step": 2573
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.04237009212374687,
      "learning_rate": 0.00019472806964614437,
      "loss": 0.3092,
      "step": 2574
    },
    {
      "epoch": 0.9363636363636364,
      "grad_norm": 0.04824372008442879,
      "learning_rate": 0.00019472400106460263,
      "loss": 0.3036,
      "step": 2575
    },
    {
      "epoch": 0.9367272727272727,
      "grad_norm": 0.04628528654575348,
      "learning_rate": 0.00019471993095625131,
      "loss": 0.3357,
      "step": 2576
    },
    {
      "epoch": 0.9370909090909091,
      "grad_norm": 0.04387099668383598,
      "learning_rate": 0.0001947158593211561,
      "loss": 0.2608,
      "step": 2577
    },
    {
      "epoch": 0.9374545454545454,
      "grad_norm": 0.055980581790208817,
      "learning_rate": 0.00019471178615938257,
      "loss": 0.3962,
      "step": 2578
    },
    {
      "epoch": 0.9378181818181818,
      "grad_norm": 0.052729103714227676,
      "learning_rate": 0.0001947077114709964,
      "loss": 0.3948,
      "step": 2579
    },
    {
      "epoch": 0.9381818181818182,
      "grad_norm": 0.06333263963460922,
      "learning_rate": 0.0001947036352560633,
      "loss": 0.3576,
      "step": 2580
    },
    {
      "epoch": 0.9385454545454546,
      "grad_norm": 0.04563681036233902,
      "learning_rate": 0.0001946995575146489,
      "loss": 0.2724,
      "step": 2581
    },
    {
      "epoch": 0.9389090909090909,
      "grad_norm": 0.04988474026322365,
      "learning_rate": 0.00019469547824681896,
      "loss": 0.3602,
      "step": 2582
    },
    {
      "epoch": 0.9392727272727273,
      "grad_norm": 0.05613988637924194,
      "learning_rate": 0.00019469139745263922,
      "loss": 0.3718,
      "step": 2583
    },
    {
      "epoch": 0.9396363636363636,
      "grad_norm": 0.0421164408326149,
      "learning_rate": 0.00019468731513217548,
      "loss": 0.2915,
      "step": 2584
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.0394107885658741,
      "learning_rate": 0.00019468323128549354,
      "loss": 0.247,
      "step": 2585
    },
    {
      "epoch": 0.9403636363636364,
      "grad_norm": 0.058635164052248,
      "learning_rate": 0.00019467914591265924,
      "loss": 0.3714,
      "step": 2586
    },
    {
      "epoch": 0.9407272727272727,
      "grad_norm": 0.05308814346790314,
      "learning_rate": 0.00019467505901373837,
      "loss": 0.3446,
      "step": 2587
    },
    {
      "epoch": 0.9410909090909091,
      "grad_norm": 0.04857170581817627,
      "learning_rate": 0.00019467097058879684,
      "loss": 0.3686,
      "step": 2588
    },
    {
      "epoch": 0.9414545454545454,
      "grad_norm": 0.06573423743247986,
      "learning_rate": 0.00019466688063790057,
      "loss": 0.348,
      "step": 2589
    },
    {
      "epoch": 0.9418181818181818,
      "grad_norm": 0.058272574096918106,
      "learning_rate": 0.00019466278916111546,
      "loss": 0.3568,
      "step": 2590
    },
    {
      "epoch": 0.9421818181818182,
      "grad_norm": 0.0444505400955677,
      "learning_rate": 0.00019465869615850747,
      "loss": 0.3507,
      "step": 2591
    },
    {
      "epoch": 0.9425454545454546,
      "grad_norm": 0.053565774112939835,
      "learning_rate": 0.00019465460163014257,
      "loss": 0.3955,
      "step": 2592
    },
    {
      "epoch": 0.9429090909090909,
      "grad_norm": 0.05831389129161835,
      "learning_rate": 0.00019465050557608673,
      "loss": 0.3536,
      "step": 2593
    },
    {
      "epoch": 0.9432727272727273,
      "grad_norm": 0.05400100722908974,
      "learning_rate": 0.000194646407996406,
      "loss": 0.3168,
      "step": 2594
    },
    {
      "epoch": 0.9436363636363636,
      "grad_norm": 0.05275120586156845,
      "learning_rate": 0.0001946423088911664,
      "loss": 0.3132,
      "step": 2595
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.05768372118473053,
      "learning_rate": 0.00019463820826043406,
      "loss": 0.3695,
      "step": 2596
    },
    {
      "epoch": 0.9443636363636364,
      "grad_norm": 0.05027757212519646,
      "learning_rate": 0.00019463410610427503,
      "loss": 0.3108,
      "step": 2597
    },
    {
      "epoch": 0.9447272727272727,
      "grad_norm": 0.049635984003543854,
      "learning_rate": 0.00019463000242275546,
      "loss": 0.3014,
      "step": 2598
    },
    {
      "epoch": 0.9450909090909091,
      "grad_norm": 0.04874684661626816,
      "learning_rate": 0.00019462589721594146,
      "loss": 0.3349,
      "step": 2599
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.04714135080575943,
      "learning_rate": 0.00019462179048389922,
      "loss": 0.2482,
      "step": 2600
    },
    {
      "epoch": 0.9458181818181818,
      "grad_norm": 0.05835697427392006,
      "learning_rate": 0.0001946176822266949,
      "loss": 0.3655,
      "step": 2601
    },
    {
      "epoch": 0.9461818181818182,
      "grad_norm": 0.04824088141322136,
      "learning_rate": 0.00019461357244439479,
      "loss": 0.2799,
      "step": 2602
    },
    {
      "epoch": 0.9465454545454546,
      "grad_norm": 0.05526304990053177,
      "learning_rate": 0.0001946094611370651,
      "loss": 0.3448,
      "step": 2603
    },
    {
      "epoch": 0.9469090909090909,
      "grad_norm": 0.052228767424821854,
      "learning_rate": 0.00019460534830477205,
      "loss": 0.3208,
      "step": 2604
    },
    {
      "epoch": 0.9472727272727273,
      "grad_norm": 0.05336790904402733,
      "learning_rate": 0.000194601233947582,
      "loss": 0.3189,
      "step": 2605
    },
    {
      "epoch": 0.9476363636363636,
      "grad_norm": 0.0574507974088192,
      "learning_rate": 0.00019459711806556125,
      "loss": 0.3759,
      "step": 2606
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.04852554202079773,
      "learning_rate": 0.00019459300065877613,
      "loss": 0.2483,
      "step": 2607
    },
    {
      "epoch": 0.9483636363636364,
      "grad_norm": 0.0559416189789772,
      "learning_rate": 0.000194588881727293,
      "loss": 0.3742,
      "step": 2608
    },
    {
      "epoch": 0.9487272727272728,
      "grad_norm": 0.05256263539195061,
      "learning_rate": 0.00019458476127117824,
      "loss": 0.3646,
      "step": 2609
    },
    {
      "epoch": 0.9490909090909091,
      "grad_norm": 0.04639572277665138,
      "learning_rate": 0.0001945806392904983,
      "loss": 0.3179,
      "step": 2610
    },
    {
      "epoch": 0.9494545454545454,
      "grad_norm": 0.04904365539550781,
      "learning_rate": 0.0001945765157853196,
      "loss": 0.3305,
      "step": 2611
    },
    {
      "epoch": 0.9498181818181818,
      "grad_norm": 0.04830481857061386,
      "learning_rate": 0.00019457239075570864,
      "loss": 0.3382,
      "step": 2612
    },
    {
      "epoch": 0.9501818181818181,
      "grad_norm": 0.055286359041929245,
      "learning_rate": 0.00019456826420173186,
      "loss": 0.3561,
      "step": 2613
    },
    {
      "epoch": 0.9505454545454546,
      "grad_norm": 0.058159973472356796,
      "learning_rate": 0.00019456413612345579,
      "loss": 0.3591,
      "step": 2614
    },
    {
      "epoch": 0.9509090909090909,
      "grad_norm": 0.05668868497014046,
      "learning_rate": 0.00019456000652094696,
      "loss": 0.334,
      "step": 2615
    },
    {
      "epoch": 0.9512727272727273,
      "grad_norm": 0.048583898693323135,
      "learning_rate": 0.00019455587539427197,
      "loss": 0.3458,
      "step": 2616
    },
    {
      "epoch": 0.9516363636363636,
      "grad_norm": 0.06921517848968506,
      "learning_rate": 0.00019455174274349738,
      "loss": 0.3886,
      "step": 2617
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.03969191759824753,
      "learning_rate": 0.00019454760856868982,
      "loss": 0.2732,
      "step": 2618
    },
    {
      "epoch": 0.9523636363636364,
      "grad_norm": 0.04921400174498558,
      "learning_rate": 0.00019454347286991587,
      "loss": 0.3438,
      "step": 2619
    },
    {
      "epoch": 0.9527272727272728,
      "grad_norm": 0.043703362345695496,
      "learning_rate": 0.00019453933564724226,
      "loss": 0.2912,
      "step": 2620
    },
    {
      "epoch": 0.9530909090909091,
      "grad_norm": 0.0492156520485878,
      "learning_rate": 0.00019453519690073565,
      "loss": 0.2863,
      "step": 2621
    },
    {
      "epoch": 0.9534545454545454,
      "grad_norm": 0.04969670996069908,
      "learning_rate": 0.00019453105663046273,
      "loss": 0.2853,
      "step": 2622
    },
    {
      "epoch": 0.9538181818181818,
      "grad_norm": 0.06280636787414551,
      "learning_rate": 0.0001945269148364903,
      "loss": 0.3478,
      "step": 2623
    },
    {
      "epoch": 0.9541818181818181,
      "grad_norm": 0.05214471369981766,
      "learning_rate": 0.00019452277151888505,
      "loss": 0.36,
      "step": 2624
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.06309054791927338,
      "learning_rate": 0.0001945186266777138,
      "loss": 0.3651,
      "step": 2625
    },
    {
      "epoch": 0.9549090909090909,
      "grad_norm": 0.06813282519578934,
      "learning_rate": 0.0001945144803130433,
      "loss": 0.3935,
      "step": 2626
    },
    {
      "epoch": 0.9552727272727273,
      "grad_norm": 0.06679025292396545,
      "learning_rate": 0.00019451033242494047,
      "loss": 0.3721,
      "step": 2627
    },
    {
      "epoch": 0.9556363636363636,
      "grad_norm": 0.04196988418698311,
      "learning_rate": 0.0001945061830134721,
      "loss": 0.3123,
      "step": 2628
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.05367395281791687,
      "learning_rate": 0.00019450203207870513,
      "loss": 0.3679,
      "step": 2629
    },
    {
      "epoch": 0.9563636363636364,
      "grad_norm": 0.06420688331127167,
      "learning_rate": 0.00019449787962070642,
      "loss": 0.3441,
      "step": 2630
    },
    {
      "epoch": 0.9567272727272728,
      "grad_norm": 0.05229686200618744,
      "learning_rate": 0.00019449372563954293,
      "loss": 0.3375,
      "step": 2631
    },
    {
      "epoch": 0.9570909090909091,
      "grad_norm": 0.04889163374900818,
      "learning_rate": 0.00019448957013528162,
      "loss": 0.3077,
      "step": 2632
    },
    {
      "epoch": 0.9574545454545454,
      "grad_norm": 0.043233472853899,
      "learning_rate": 0.00019448541310798943,
      "loss": 0.2963,
      "step": 2633
    },
    {
      "epoch": 0.9578181818181818,
      "grad_norm": 0.04283548891544342,
      "learning_rate": 0.00019448125455773338,
      "loss": 0.2665,
      "step": 2634
    },
    {
      "epoch": 0.9581818181818181,
      "grad_norm": 0.06056009978055954,
      "learning_rate": 0.00019447709448458053,
      "loss": 0.3216,
      "step": 2635
    },
    {
      "epoch": 0.9585454545454546,
      "grad_norm": 0.05434839800000191,
      "learning_rate": 0.00019447293288859792,
      "loss": 0.389,
      "step": 2636
    },
    {
      "epoch": 0.9589090909090909,
      "grad_norm": 0.0436418354511261,
      "learning_rate": 0.0001944687697698526,
      "loss": 0.3033,
      "step": 2637
    },
    {
      "epoch": 0.9592727272727273,
      "grad_norm": 0.05560232326388359,
      "learning_rate": 0.00019446460512841172,
      "loss": 0.3378,
      "step": 2638
    },
    {
      "epoch": 0.9596363636363636,
      "grad_norm": 0.05720346421003342,
      "learning_rate": 0.00019446043896434234,
      "loss": 0.3538,
      "step": 2639
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.04664064943790436,
      "learning_rate": 0.0001944562712777117,
      "loss": 0.3205,
      "step": 2640
    },
    {
      "epoch": 0.9603636363636363,
      "grad_norm": 0.05753329023718834,
      "learning_rate": 0.00019445210206858694,
      "loss": 0.3543,
      "step": 2641
    },
    {
      "epoch": 0.9607272727272728,
      "grad_norm": 0.052649226039648056,
      "learning_rate": 0.00019444793133703523,
      "loss": 0.3157,
      "step": 2642
    },
    {
      "epoch": 0.9610909090909091,
      "grad_norm": 0.05092157796025276,
      "learning_rate": 0.0001944437590831238,
      "loss": 0.3366,
      "step": 2643
    },
    {
      "epoch": 0.9614545454545455,
      "grad_norm": 0.0630478709936142,
      "learning_rate": 0.00019443958530691996,
      "loss": 0.3447,
      "step": 2644
    },
    {
      "epoch": 0.9618181818181818,
      "grad_norm": 0.05891703814268112,
      "learning_rate": 0.00019443541000849094,
      "loss": 0.3808,
      "step": 2645
    },
    {
      "epoch": 0.9621818181818181,
      "grad_norm": 0.048874381929636,
      "learning_rate": 0.00019443123318790403,
      "loss": 0.3408,
      "step": 2646
    },
    {
      "epoch": 0.9625454545454546,
      "grad_norm": 0.048514775931835175,
      "learning_rate": 0.0001944270548452266,
      "loss": 0.3047,
      "step": 2647
    },
    {
      "epoch": 0.9629090909090909,
      "grad_norm": 0.06610708683729172,
      "learning_rate": 0.00019442287498052592,
      "loss": 0.3266,
      "step": 2648
    },
    {
      "epoch": 0.9632727272727273,
      "grad_norm": 0.055901411920785904,
      "learning_rate": 0.00019441869359386943,
      "loss": 0.3366,
      "step": 2649
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.058267757296562195,
      "learning_rate": 0.00019441451068532453,
      "loss": 0.4122,
      "step": 2650
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.046191032975912094,
      "learning_rate": 0.00019441032625495857,
      "loss": 0.3666,
      "step": 2651
    },
    {
      "epoch": 0.9643636363636363,
      "grad_norm": 0.052848707884550095,
      "learning_rate": 0.0001944061403028391,
      "loss": 0.3314,
      "step": 2652
    },
    {
      "epoch": 0.9647272727272728,
      "grad_norm": 0.05217501521110535,
      "learning_rate": 0.00019440195282903353,
      "loss": 0.3963,
      "step": 2653
    },
    {
      "epoch": 0.9650909090909091,
      "grad_norm": 0.04620582237839699,
      "learning_rate": 0.00019439776383360936,
      "loss": 0.2851,
      "step": 2654
    },
    {
      "epoch": 0.9654545454545455,
      "grad_norm": 0.03544848784804344,
      "learning_rate": 0.00019439357331663414,
      "loss": 0.2703,
      "step": 2655
    },
    {
      "epoch": 0.9658181818181818,
      "grad_norm": 0.04813139885663986,
      "learning_rate": 0.00019438938127817534,
      "loss": 0.3793,
      "step": 2656
    },
    {
      "epoch": 0.9661818181818181,
      "grad_norm": 0.050355616956949234,
      "learning_rate": 0.0001943851877183006,
      "loss": 0.344,
      "step": 2657
    },
    {
      "epoch": 0.9665454545454546,
      "grad_norm": 0.047779280692338943,
      "learning_rate": 0.0001943809926370775,
      "loss": 0.3246,
      "step": 2658
    },
    {
      "epoch": 0.9669090909090909,
      "grad_norm": 0.05090154707431793,
      "learning_rate": 0.00019437679603457366,
      "loss": 0.3417,
      "step": 2659
    },
    {
      "epoch": 0.9672727272727273,
      "grad_norm": 0.05483202636241913,
      "learning_rate": 0.0001943725979108567,
      "loss": 0.3399,
      "step": 2660
    },
    {
      "epoch": 0.9676363636363636,
      "grad_norm": 0.0569537952542305,
      "learning_rate": 0.00019436839826599434,
      "loss": 0.3814,
      "step": 2661
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.04982296749949455,
      "learning_rate": 0.00019436419710005417,
      "loss": 0.3445,
      "step": 2662
    },
    {
      "epoch": 0.9683636363636363,
      "grad_norm": 0.0454854890704155,
      "learning_rate": 0.000194359994413104,
      "loss": 0.3126,
      "step": 2663
    },
    {
      "epoch": 0.9687272727272728,
      "grad_norm": 0.055460620671510696,
      "learning_rate": 0.00019435579020521155,
      "loss": 0.3378,
      "step": 2664
    },
    {
      "epoch": 0.9690909090909091,
      "grad_norm": 0.04046863690018654,
      "learning_rate": 0.0001943515844764446,
      "loss": 0.3165,
      "step": 2665
    },
    {
      "epoch": 0.9694545454545455,
      "grad_norm": 0.04488573595881462,
      "learning_rate": 0.00019434737722687088,
      "loss": 0.3129,
      "step": 2666
    },
    {
      "epoch": 0.9698181818181818,
      "grad_norm": 0.05207867547869682,
      "learning_rate": 0.00019434316845655828,
      "loss": 0.3307,
      "step": 2667
    },
    {
      "epoch": 0.9701818181818181,
      "grad_norm": 0.054681312292814255,
      "learning_rate": 0.00019433895816557456,
      "loss": 0.3376,
      "step": 2668
    },
    {
      "epoch": 0.9705454545454546,
      "grad_norm": 0.04633500054478645,
      "learning_rate": 0.00019433474635398765,
      "loss": 0.3245,
      "step": 2669
    },
    {
      "epoch": 0.9709090909090909,
      "grad_norm": 0.058984823524951935,
      "learning_rate": 0.00019433053302186538,
      "loss": 0.3513,
      "step": 2670
    },
    {
      "epoch": 0.9712727272727273,
      "grad_norm": 0.0599743016064167,
      "learning_rate": 0.00019432631816927574,
      "loss": 0.3865,
      "step": 2671
    },
    {
      "epoch": 0.9716363636363636,
      "grad_norm": 0.04263070598244667,
      "learning_rate": 0.00019432210179628662,
      "loss": 0.2915,
      "step": 2672
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.046313054859638214,
      "learning_rate": 0.00019431788390296596,
      "loss": 0.3335,
      "step": 2673
    },
    {
      "epoch": 0.9723636363636363,
      "grad_norm": 0.04987554997205734,
      "learning_rate": 0.0001943136644893818,
      "loss": 0.3258,
      "step": 2674
    },
    {
      "epoch": 0.9727272727272728,
      "grad_norm": 0.04895868897438049,
      "learning_rate": 0.0001943094435556021,
      "loss": 0.3273,
      "step": 2675
    },
    {
      "epoch": 0.9730909090909091,
      "grad_norm": 0.05554201453924179,
      "learning_rate": 0.0001943052211016949,
      "loss": 0.389,
      "step": 2676
    },
    {
      "epoch": 0.9734545454545455,
      "grad_norm": 0.05414998158812523,
      "learning_rate": 0.00019430099712772832,
      "loss": 0.3725,
      "step": 2677
    },
    {
      "epoch": 0.9738181818181818,
      "grad_norm": 0.07198149710893631,
      "learning_rate": 0.00019429677163377036,
      "loss": 0.3695,
      "step": 2678
    },
    {
      "epoch": 0.9741818181818181,
      "grad_norm": 0.05537962168455124,
      "learning_rate": 0.0001942925446198892,
      "loss": 0.315,
      "step": 2679
    },
    {
      "epoch": 0.9745454545454545,
      "grad_norm": 0.04600094258785248,
      "learning_rate": 0.00019428831608615292,
      "loss": 0.2983,
      "step": 2680
    },
    {
      "epoch": 0.974909090909091,
      "grad_norm": 0.06236017122864723,
      "learning_rate": 0.0001942840860326297,
      "loss": 0.3242,
      "step": 2681
    },
    {
      "epoch": 0.9752727272727273,
      "grad_norm": 0.047456517815589905,
      "learning_rate": 0.00019427985445938774,
      "loss": 0.2812,
      "step": 2682
    },
    {
      "epoch": 0.9756363636363636,
      "grad_norm": 0.05628233402967453,
      "learning_rate": 0.00019427562136649522,
      "loss": 0.409,
      "step": 2683
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.05020700767636299,
      "learning_rate": 0.00019427138675402037,
      "loss": 0.378,
      "step": 2684
    },
    {
      "epoch": 0.9763636363636363,
      "grad_norm": 0.05744899809360504,
      "learning_rate": 0.00019426715062203148,
      "loss": 0.3862,
      "step": 2685
    },
    {
      "epoch": 0.9767272727272728,
      "grad_norm": 0.057475246489048004,
      "learning_rate": 0.00019426291297059678,
      "loss": 0.3355,
      "step": 2686
    },
    {
      "epoch": 0.9770909090909091,
      "grad_norm": 0.043588362634181976,
      "learning_rate": 0.0001942586737997846,
      "loss": 0.3301,
      "step": 2687
    },
    {
      "epoch": 0.9774545454545455,
      "grad_norm": 0.040450289845466614,
      "learning_rate": 0.0001942544331096633,
      "loss": 0.3058,
      "step": 2688
    },
    {
      "epoch": 0.9778181818181818,
      "grad_norm": 0.05712924525141716,
      "learning_rate": 0.00019425019090030116,
      "loss": 0.3817,
      "step": 2689
    },
    {
      "epoch": 0.9781818181818182,
      "grad_norm": 0.06463014334440231,
      "learning_rate": 0.00019424594717176662,
      "loss": 0.4109,
      "step": 2690
    },
    {
      "epoch": 0.9785454545454545,
      "grad_norm": 0.05056050047278404,
      "learning_rate": 0.0001942417019241281,
      "loss": 0.3933,
      "step": 2691
    },
    {
      "epoch": 0.978909090909091,
      "grad_norm": 0.0515429712831974,
      "learning_rate": 0.00019423745515745395,
      "loss": 0.352,
      "step": 2692
    },
    {
      "epoch": 0.9792727272727273,
      "grad_norm": 0.06302455067634583,
      "learning_rate": 0.00019423320687181268,
      "loss": 0.2996,
      "step": 2693
    },
    {
      "epoch": 0.9796363636363636,
      "grad_norm": 0.06462179124355316,
      "learning_rate": 0.00019422895706727272,
      "loss": 0.341,
      "step": 2694
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05535314232110977,
      "learning_rate": 0.0001942247057439026,
      "loss": 0.3888,
      "step": 2695
    },
    {
      "epoch": 0.9803636363636363,
      "grad_norm": 0.047198303043842316,
      "learning_rate": 0.00019422045290177088,
      "loss": 0.3343,
      "step": 2696
    },
    {
      "epoch": 0.9807272727272728,
      "grad_norm": 0.04275090992450714,
      "learning_rate": 0.00019421619854094607,
      "loss": 0.2679,
      "step": 2697
    },
    {
      "epoch": 0.9810909090909091,
      "grad_norm": 0.07575711607933044,
      "learning_rate": 0.00019421194266149673,
      "loss": 0.4203,
      "step": 2698
    },
    {
      "epoch": 0.9814545454545455,
      "grad_norm": 0.04959042742848396,
      "learning_rate": 0.00019420768526349146,
      "loss": 0.3882,
      "step": 2699
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.05025819316506386,
      "learning_rate": 0.0001942034263469989,
      "loss": 0.3768,
      "step": 2700
    },
    {
      "epoch": 0.9821818181818182,
      "grad_norm": 0.07595686614513397,
      "learning_rate": 0.00019419916591208774,
      "loss": 0.4365,
      "step": 2701
    },
    {
      "epoch": 0.9825454545454545,
      "grad_norm": 0.06565579771995544,
      "learning_rate": 0.00019419490395882657,
      "loss": 0.3743,
      "step": 2702
    },
    {
      "epoch": 0.982909090909091,
      "grad_norm": 0.043028511106967926,
      "learning_rate": 0.00019419064048728415,
      "loss": 0.263,
      "step": 2703
    },
    {
      "epoch": 0.9832727272727273,
      "grad_norm": 0.04040798172354698,
      "learning_rate": 0.00019418637549752915,
      "loss": 0.2904,
      "step": 2704
    },
    {
      "epoch": 0.9836363636363636,
      "grad_norm": 0.07736767083406448,
      "learning_rate": 0.00019418210898963033,
      "loss": 0.3937,
      "step": 2705
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.05796680599451065,
      "learning_rate": 0.0001941778409636565,
      "loss": 0.3398,
      "step": 2706
    },
    {
      "epoch": 0.9843636363636363,
      "grad_norm": 0.04591090604662895,
      "learning_rate": 0.0001941735714196764,
      "loss": 0.3619,
      "step": 2707
    },
    {
      "epoch": 0.9847272727272727,
      "grad_norm": 0.04886840656399727,
      "learning_rate": 0.0001941693003577589,
      "loss": 0.3599,
      "step": 2708
    },
    {
      "epoch": 0.9850909090909091,
      "grad_norm": 0.05732060596346855,
      "learning_rate": 0.00019416502777797282,
      "loss": 0.3467,
      "step": 2709
    },
    {
      "epoch": 0.9854545454545455,
      "grad_norm": 0.06423567235469818,
      "learning_rate": 0.000194160753680387,
      "loss": 0.358,
      "step": 2710
    },
    {
      "epoch": 0.9858181818181818,
      "grad_norm": 0.047455523163080215,
      "learning_rate": 0.00019415647806507035,
      "loss": 0.4161,
      "step": 2711
    },
    {
      "epoch": 0.9861818181818182,
      "grad_norm": 0.04961945489048958,
      "learning_rate": 0.0001941522009320918,
      "loss": 0.3344,
      "step": 2712
    },
    {
      "epoch": 0.9865454545454545,
      "grad_norm": 0.05378695949912071,
      "learning_rate": 0.0001941479222815203,
      "loss": 0.4019,
      "step": 2713
    },
    {
      "epoch": 0.986909090909091,
      "grad_norm": 0.05120297521352768,
      "learning_rate": 0.00019414364211342477,
      "loss": 0.3311,
      "step": 2714
    },
    {
      "epoch": 0.9872727272727273,
      "grad_norm": 0.055276233702898026,
      "learning_rate": 0.00019413936042787425,
      "loss": 0.3379,
      "step": 2715
    },
    {
      "epoch": 0.9876363636363636,
      "grad_norm": 0.051433514803647995,
      "learning_rate": 0.00019413507722493775,
      "loss": 0.3286,
      "step": 2716
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.05243045091629028,
      "learning_rate": 0.00019413079250468423,
      "loss": 0.3295,
      "step": 2717
    },
    {
      "epoch": 0.9883636363636363,
      "grad_norm": 0.04617932438850403,
      "learning_rate": 0.00019412650626718288,
      "loss": 0.3651,
      "step": 2718
    },
    {
      "epoch": 0.9887272727272727,
      "grad_norm": 0.048182517290115356,
      "learning_rate": 0.00019412221851250268,
      "loss": 0.3691,
      "step": 2719
    },
    {
      "epoch": 0.9890909090909091,
      "grad_norm": 0.05080140009522438,
      "learning_rate": 0.0001941179292407128,
      "loss": 0.344,
      "step": 2720
    },
    {
      "epoch": 0.9894545454545455,
      "grad_norm": 0.05801050364971161,
      "learning_rate": 0.00019411363845188236,
      "loss": 0.3669,
      "step": 2721
    },
    {
      "epoch": 0.9898181818181818,
      "grad_norm": 0.05711909756064415,
      "learning_rate": 0.00019410934614608053,
      "loss": 0.3698,
      "step": 2722
    },
    {
      "epoch": 0.9901818181818182,
      "grad_norm": 0.06204373389482498,
      "learning_rate": 0.00019410505232337648,
      "loss": 0.3345,
      "step": 2723
    },
    {
      "epoch": 0.9905454545454545,
      "grad_norm": 0.059957489371299744,
      "learning_rate": 0.00019410075698383944,
      "loss": 0.3367,
      "step": 2724
    },
    {
      "epoch": 0.990909090909091,
      "grad_norm": 0.04945145547389984,
      "learning_rate": 0.0001940964601275386,
      "loss": 0.3415,
      "step": 2725
    },
    {
      "epoch": 0.9912727272727273,
      "grad_norm": 0.03755589574575424,
      "learning_rate": 0.0001940921617545433,
      "loss": 0.277,
      "step": 2726
    },
    {
      "epoch": 0.9916363636363636,
      "grad_norm": 0.041683346033096313,
      "learning_rate": 0.00019408786186492276,
      "loss": 0.3327,
      "step": 2727
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.047457631677389145,
      "learning_rate": 0.00019408356045874629,
      "loss": 0.353,
      "step": 2728
    },
    {
      "epoch": 0.9923636363636363,
      "grad_norm": 0.0448698028922081,
      "learning_rate": 0.00019407925753608325,
      "loss": 0.3078,
      "step": 2729
    },
    {
      "epoch": 0.9927272727272727,
      "grad_norm": 0.046413347125053406,
      "learning_rate": 0.00019407495309700296,
      "loss": 0.3231,
      "step": 2730
    },
    {
      "epoch": 0.9930909090909091,
      "grad_norm": 0.05072342976927757,
      "learning_rate": 0.00019407064714157483,
      "loss": 0.3255,
      "step": 2731
    },
    {
      "epoch": 0.9934545454545455,
      "grad_norm": 0.03937511518597603,
      "learning_rate": 0.00019406633966986828,
      "loss": 0.3088,
      "step": 2732
    },
    {
      "epoch": 0.9938181818181818,
      "grad_norm": 0.05774229019880295,
      "learning_rate": 0.00019406203068195267,
      "loss": 0.372,
      "step": 2733
    },
    {
      "epoch": 0.9941818181818182,
      "grad_norm": 0.0497862845659256,
      "learning_rate": 0.00019405772017789757,
      "loss": 0.3336,
      "step": 2734
    },
    {
      "epoch": 0.9945454545454545,
      "grad_norm": 0.04180046543478966,
      "learning_rate": 0.00019405340815777233,
      "loss": 0.2837,
      "step": 2735
    },
    {
      "epoch": 0.9949090909090909,
      "grad_norm": 0.05618199333548546,
      "learning_rate": 0.00019404909462164652,
      "loss": 0.3472,
      "step": 2736
    },
    {
      "epoch": 0.9952727272727273,
      "grad_norm": 0.03898731619119644,
      "learning_rate": 0.0001940447795695897,
      "loss": 0.2565,
      "step": 2737
    },
    {
      "epoch": 0.9956363636363637,
      "grad_norm": 0.041321564465761185,
      "learning_rate": 0.00019404046300167138,
      "loss": 0.2876,
      "step": 2738
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.05835813283920288,
      "learning_rate": 0.00019403614491796112,
      "loss": 0.3678,
      "step": 2739
    },
    {
      "epoch": 0.9963636363636363,
      "grad_norm": 0.04967145249247551,
      "learning_rate": 0.00019403182531852858,
      "loss": 0.3778,
      "step": 2740
    },
    {
      "epoch": 0.9967272727272727,
      "grad_norm": 0.038158684968948364,
      "learning_rate": 0.0001940275042034433,
      "loss": 0.2487,
      "step": 2741
    },
    {
      "epoch": 0.9970909090909091,
      "grad_norm": 0.04917467013001442,
      "learning_rate": 0.00019402318157277503,
      "loss": 0.3393,
      "step": 2742
    },
    {
      "epoch": 0.9974545454545455,
      "grad_norm": 0.052850741893053055,
      "learning_rate": 0.00019401885742659336,
      "loss": 0.3571,
      "step": 2743
    },
    {
      "epoch": 0.9978181818181818,
      "grad_norm": 0.04348541423678398,
      "learning_rate": 0.00019401453176496803,
      "loss": 0.3169,
      "step": 2744
    },
    {
      "epoch": 0.9981818181818182,
      "grad_norm": 0.05024336650967598,
      "learning_rate": 0.0001940102045879687,
      "loss": 0.3259,
      "step": 2745
    },
    {
      "epoch": 0.9985454545454545,
      "grad_norm": 0.049108073115348816,
      "learning_rate": 0.00019400587589566524,
      "loss": 0.3376,
      "step": 2746
    },
    {
      "epoch": 0.9989090909090909,
      "grad_norm": 0.05748865380883217,
      "learning_rate": 0.00019400154568812735,
      "loss": 0.4193,
      "step": 2747
    },
    {
      "epoch": 0.9992727272727273,
      "grad_norm": 0.05332498997449875,
      "learning_rate": 0.00019399721396542484,
      "loss": 0.3518,
      "step": 2748
    },
    {
      "epoch": 0.9996363636363637,
      "grad_norm": 0.04472661018371582,
      "learning_rate": 0.00019399288072762747,
      "loss": 0.2985,
      "step": 2749
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.048854414373636246,
      "learning_rate": 0.00019398854597480516,
      "loss": 0.3479,
      "step": 2750
    },
    {
      "epoch": 1.0003636363636363,
      "grad_norm": 0.044797107577323914,
      "learning_rate": 0.00019398420970702778,
      "loss": 0.3221,
      "step": 2751
    },
    {
      "epoch": 1.0007272727272727,
      "grad_norm": 0.07408121973276138,
      "learning_rate": 0.00019397987192436517,
      "loss": 0.324,
      "step": 2752
    },
    {
      "epoch": 1.001090909090909,
      "grad_norm": 0.042732108384370804,
      "learning_rate": 0.0001939755326268873,
      "loss": 0.2594,
      "step": 2753
    },
    {
      "epoch": 1.0014545454545454,
      "grad_norm": 0.047745514661073685,
      "learning_rate": 0.0001939711918146641,
      "loss": 0.3052,
      "step": 2754
    },
    {
      "epoch": 1.0018181818181817,
      "grad_norm": 0.050990134477615356,
      "learning_rate": 0.0001939668494877655,
      "loss": 0.3119,
      "step": 2755
    },
    {
      "epoch": 1.0021818181818183,
      "grad_norm": 0.04562705382704735,
      "learning_rate": 0.00019396250564626154,
      "loss": 0.3162,
      "step": 2756
    },
    {
      "epoch": 1.0025454545454546,
      "grad_norm": 0.054294951260089874,
      "learning_rate": 0.0001939581602902222,
      "loss": 0.3785,
      "step": 2757
    },
    {
      "epoch": 1.002909090909091,
      "grad_norm": 0.058207299560308456,
      "learning_rate": 0.00019395381341971756,
      "loss": 0.3435,
      "step": 2758
    },
    {
      "epoch": 1.0032727272727273,
      "grad_norm": 0.049144987016916275,
      "learning_rate": 0.00019394946503481766,
      "loss": 0.3127,
      "step": 2759
    },
    {
      "epoch": 1.0036363636363637,
      "grad_norm": 0.04545418918132782,
      "learning_rate": 0.0001939451151355926,
      "loss": 0.3483,
      "step": 2760
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.05281701683998108,
      "learning_rate": 0.0001939407637221125,
      "loss": 0.3733,
      "step": 2761
    },
    {
      "epoch": 1.0043636363636363,
      "grad_norm": 0.048977043479681015,
      "learning_rate": 0.00019393641079444744,
      "loss": 0.2903,
      "step": 2762
    },
    {
      "epoch": 1.0047272727272727,
      "grad_norm": 0.05502701550722122,
      "learning_rate": 0.00019393205635266764,
      "loss": 0.334,
      "step": 2763
    },
    {
      "epoch": 1.005090909090909,
      "grad_norm": 0.04697242006659508,
      "learning_rate": 0.0001939277003968433,
      "loss": 0.337,
      "step": 2764
    },
    {
      "epoch": 1.0054545454545454,
      "grad_norm": 0.043512046337127686,
      "learning_rate": 0.0001939233429270446,
      "loss": 0.2861,
      "step": 2765
    },
    {
      "epoch": 1.0058181818181817,
      "grad_norm": 0.04272622615098953,
      "learning_rate": 0.00019391898394334177,
      "loss": 0.3086,
      "step": 2766
    },
    {
      "epoch": 1.0061818181818183,
      "grad_norm": 0.050984691828489304,
      "learning_rate": 0.0001939146234458051,
      "loss": 0.3142,
      "step": 2767
    },
    {
      "epoch": 1.0065454545454546,
      "grad_norm": 0.056716643273830414,
      "learning_rate": 0.00019391026143450484,
      "loss": 0.3629,
      "step": 2768
    },
    {
      "epoch": 1.006909090909091,
      "grad_norm": 0.04617821425199509,
      "learning_rate": 0.00019390589790951133,
      "loss": 0.2827,
      "step": 2769
    },
    {
      "epoch": 1.0072727272727273,
      "grad_norm": 0.059245515614748,
      "learning_rate": 0.00019390153287089488,
      "loss": 0.291,
      "step": 2770
    },
    {
      "epoch": 1.0076363636363637,
      "grad_norm": 0.05893249064683914,
      "learning_rate": 0.00019389716631872586,
      "loss": 0.2924,
      "step": 2771
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.053495246917009354,
      "learning_rate": 0.00019389279825307468,
      "loss": 0.3971,
      "step": 2772
    },
    {
      "epoch": 1.0083636363636364,
      "grad_norm": 0.05086805671453476,
      "learning_rate": 0.00019388842867401168,
      "loss": 0.3727,
      "step": 2773
    },
    {
      "epoch": 1.0087272727272727,
      "grad_norm": 0.05507691577076912,
      "learning_rate": 0.00019388405758160733,
      "loss": 0.3147,
      "step": 2774
    },
    {
      "epoch": 1.009090909090909,
      "grad_norm": 0.04644935950636864,
      "learning_rate": 0.00019387968497593209,
      "loss": 0.2732,
      "step": 2775
    },
    {
      "epoch": 1.0094545454545454,
      "grad_norm": 0.05405730754137039,
      "learning_rate": 0.00019387531085705644,
      "loss": 0.3196,
      "step": 2776
    },
    {
      "epoch": 1.0098181818181817,
      "grad_norm": 0.061820995062589645,
      "learning_rate": 0.00019387093522505087,
      "loss": 0.3493,
      "step": 2777
    },
    {
      "epoch": 1.0101818181818183,
      "grad_norm": 0.051317181438207626,
      "learning_rate": 0.00019386655807998593,
      "loss": 0.324,
      "step": 2778
    },
    {
      "epoch": 1.0105454545454546,
      "grad_norm": 0.05159345641732216,
      "learning_rate": 0.00019386217942193214,
      "loss": 0.3786,
      "step": 2779
    },
    {
      "epoch": 1.010909090909091,
      "grad_norm": 0.03988030552864075,
      "learning_rate": 0.0001938577992509601,
      "loss": 0.2944,
      "step": 2780
    },
    {
      "epoch": 1.0112727272727273,
      "grad_norm": 0.050516512244939804,
      "learning_rate": 0.0001938534175671404,
      "loss": 0.2812,
      "step": 2781
    },
    {
      "epoch": 1.0116363636363637,
      "grad_norm": 0.04404573515057564,
      "learning_rate": 0.00019384903437054372,
      "loss": 0.256,
      "step": 2782
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.052761856466531754,
      "learning_rate": 0.00019384464966124064,
      "loss": 0.3074,
      "step": 2783
    },
    {
      "epoch": 1.0123636363636364,
      "grad_norm": 0.05835343897342682,
      "learning_rate": 0.00019384026343930187,
      "loss": 0.3933,
      "step": 2784
    },
    {
      "epoch": 1.0127272727272727,
      "grad_norm": 0.053415413945913315,
      "learning_rate": 0.00019383587570479807,
      "loss": 0.3267,
      "step": 2785
    },
    {
      "epoch": 1.013090909090909,
      "grad_norm": 0.06308944523334503,
      "learning_rate": 0.0001938314864578,
      "loss": 0.334,
      "step": 2786
    },
    {
      "epoch": 1.0134545454545454,
      "grad_norm": 0.04596053436398506,
      "learning_rate": 0.0001938270956983784,
      "loss": 0.3238,
      "step": 2787
    },
    {
      "epoch": 1.0138181818181817,
      "grad_norm": 0.044919513165950775,
      "learning_rate": 0.00019382270342660406,
      "loss": 0.3172,
      "step": 2788
    },
    {
      "epoch": 1.014181818181818,
      "grad_norm": 0.0516391284763813,
      "learning_rate": 0.00019381830964254777,
      "loss": 0.329,
      "step": 2789
    },
    {
      "epoch": 1.0145454545454546,
      "grad_norm": 0.04451641067862511,
      "learning_rate": 0.00019381391434628034,
      "loss": 0.3067,
      "step": 2790
    },
    {
      "epoch": 1.014909090909091,
      "grad_norm": 0.061106156557798386,
      "learning_rate": 0.0001938095175378726,
      "loss": 0.3364,
      "step": 2791
    },
    {
      "epoch": 1.0152727272727273,
      "grad_norm": 0.04880058020353317,
      "learning_rate": 0.00019380511921739544,
      "loss": 0.31,
      "step": 2792
    },
    {
      "epoch": 1.0156363636363637,
      "grad_norm": 0.058981459587812424,
      "learning_rate": 0.00019380071938491977,
      "loss": 0.3318,
      "step": 2793
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.0519065223634243,
      "learning_rate": 0.0001937963180405165,
      "loss": 0.2867,
      "step": 2794
    },
    {
      "epoch": 1.0163636363636364,
      "grad_norm": 0.051546234637498856,
      "learning_rate": 0.00019379191518425652,
      "loss": 0.2961,
      "step": 2795
    },
    {
      "epoch": 1.0167272727272727,
      "grad_norm": 0.05558865889906883,
      "learning_rate": 0.00019378751081621087,
      "loss": 0.3434,
      "step": 2796
    },
    {
      "epoch": 1.017090909090909,
      "grad_norm": 0.04977747052907944,
      "learning_rate": 0.0001937831049364505,
      "loss": 0.3229,
      "step": 2797
    },
    {
      "epoch": 1.0174545454545454,
      "grad_norm": 0.05103537440299988,
      "learning_rate": 0.00019377869754504644,
      "loss": 0.3306,
      "step": 2798
    },
    {
      "epoch": 1.0178181818181817,
      "grad_norm": 0.062978096306324,
      "learning_rate": 0.00019377428864206974,
      "loss": 0.3379,
      "step": 2799
    },
    {
      "epoch": 1.018181818181818,
      "grad_norm": 0.04193829372525215,
      "learning_rate": 0.00019376987822759147,
      "loss": 0.3423,
      "step": 2800
    },
    {
      "epoch": 1.0185454545454546,
      "grad_norm": 0.04772457852959633,
      "learning_rate": 0.00019376546630168272,
      "loss": 0.2727,
      "step": 2801
    },
    {
      "epoch": 1.018909090909091,
      "grad_norm": 0.06431178003549576,
      "learning_rate": 0.00019376105286441454,
      "loss": 0.3907,
      "step": 2802
    },
    {
      "epoch": 1.0192727272727273,
      "grad_norm": 0.047292694449424744,
      "learning_rate": 0.00019375663791585814,
      "loss": 0.3111,
      "step": 2803
    },
    {
      "epoch": 1.0196363636363637,
      "grad_norm": 0.055703211575746536,
      "learning_rate": 0.00019375222145608464,
      "loss": 0.3708,
      "step": 2804
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.0686737596988678,
      "learning_rate": 0.00019374780348516526,
      "loss": 0.3549,
      "step": 2805
    },
    {
      "epoch": 1.0203636363636364,
      "grad_norm": 0.06103099510073662,
      "learning_rate": 0.00019374338400317118,
      "loss": 0.426,
      "step": 2806
    },
    {
      "epoch": 1.0207272727272727,
      "grad_norm": 0.047763220965862274,
      "learning_rate": 0.00019373896301017366,
      "loss": 0.3068,
      "step": 2807
    },
    {
      "epoch": 1.021090909090909,
      "grad_norm": 0.045925356447696686,
      "learning_rate": 0.00019373454050624397,
      "loss": 0.3432,
      "step": 2808
    },
    {
      "epoch": 1.0214545454545454,
      "grad_norm": 0.04612945020198822,
      "learning_rate": 0.0001937301164914534,
      "loss": 0.3197,
      "step": 2809
    },
    {
      "epoch": 1.0218181818181817,
      "grad_norm": 0.05305027216672897,
      "learning_rate": 0.00019372569096587317,
      "loss": 0.2863,
      "step": 2810
    },
    {
      "epoch": 1.022181818181818,
      "grad_norm": 0.06040637567639351,
      "learning_rate": 0.00019372126392957473,
      "loss": 0.3366,
      "step": 2811
    },
    {
      "epoch": 1.0225454545454546,
      "grad_norm": 0.05509769916534424,
      "learning_rate": 0.00019371683538262934,
      "loss": 0.3582,
      "step": 2812
    },
    {
      "epoch": 1.022909090909091,
      "grad_norm": 0.044468045234680176,
      "learning_rate": 0.00019371240532510843,
      "loss": 0.301,
      "step": 2813
    },
    {
      "epoch": 1.0232727272727273,
      "grad_norm": 0.0625515952706337,
      "learning_rate": 0.00019370797375708345,
      "loss": 0.4257,
      "step": 2814
    },
    {
      "epoch": 1.0236363636363637,
      "grad_norm": 0.05207233875989914,
      "learning_rate": 0.00019370354067862574,
      "loss": 0.3704,
      "step": 2815
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.05149446427822113,
      "learning_rate": 0.00019369910608980678,
      "loss": 0.3496,
      "step": 2816
    },
    {
      "epoch": 1.0243636363636364,
      "grad_norm": 0.05290752649307251,
      "learning_rate": 0.00019369466999069813,
      "loss": 0.368,
      "step": 2817
    },
    {
      "epoch": 1.0247272727272727,
      "grad_norm": 0.05571000277996063,
      "learning_rate": 0.0001936902323813712,
      "loss": 0.3489,
      "step": 2818
    },
    {
      "epoch": 1.025090909090909,
      "grad_norm": 0.05067383125424385,
      "learning_rate": 0.00019368579326189752,
      "loss": 0.3262,
      "step": 2819
    },
    {
      "epoch": 1.0254545454545454,
      "grad_norm": 0.06379490345716476,
      "learning_rate": 0.00019368135263234871,
      "loss": 0.3859,
      "step": 2820
    },
    {
      "epoch": 1.0258181818181817,
      "grad_norm": 0.04513254761695862,
      "learning_rate": 0.00019367691049279625,
      "loss": 0.332,
      "step": 2821
    },
    {
      "epoch": 1.026181818181818,
      "grad_norm": 0.057337626814842224,
      "learning_rate": 0.00019367246684331185,
      "loss": 0.4273,
      "step": 2822
    },
    {
      "epoch": 1.0265454545454546,
      "grad_norm": 0.05029873549938202,
      "learning_rate": 0.00019366802168396704,
      "loss": 0.3609,
      "step": 2823
    },
    {
      "epoch": 1.026909090909091,
      "grad_norm": 0.05287620797753334,
      "learning_rate": 0.00019366357501483355,
      "loss": 0.3325,
      "step": 2824
    },
    {
      "epoch": 1.0272727272727273,
      "grad_norm": 0.042726702988147736,
      "learning_rate": 0.00019365912683598298,
      "loss": 0.2883,
      "step": 2825
    },
    {
      "epoch": 1.0276363636363637,
      "grad_norm": 0.03025307133793831,
      "learning_rate": 0.00019365467714748706,
      "loss": 0.2392,
      "step": 2826
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.03557124361395836,
      "learning_rate": 0.00019365022594941756,
      "loss": 0.2942,
      "step": 2827
    },
    {
      "epoch": 1.0283636363636364,
      "grad_norm": 0.05153089761734009,
      "learning_rate": 0.00019364577324184614,
      "loss": 0.3568,
      "step": 2828
    },
    {
      "epoch": 1.0287272727272727,
      "grad_norm": 0.05540826544165611,
      "learning_rate": 0.0001936413190248446,
      "loss": 0.3126,
      "step": 2829
    },
    {
      "epoch": 1.029090909090909,
      "grad_norm": 0.03261549025774002,
      "learning_rate": 0.0001936368632984848,
      "loss": 0.2071,
      "step": 2830
    },
    {
      "epoch": 1.0294545454545454,
      "grad_norm": 0.05111924558877945,
      "learning_rate": 0.00019363240606283844,
      "loss": 0.3206,
      "step": 2831
    },
    {
      "epoch": 1.0298181818181817,
      "grad_norm": 0.05570730194449425,
      "learning_rate": 0.0001936279473179775,
      "loss": 0.3322,
      "step": 2832
    },
    {
      "epoch": 1.030181818181818,
      "grad_norm": 0.055437203496694565,
      "learning_rate": 0.00019362348706397373,
      "loss": 0.3338,
      "step": 2833
    },
    {
      "epoch": 1.0305454545454547,
      "grad_norm": 0.04248616844415665,
      "learning_rate": 0.00019361902530089912,
      "loss": 0.3148,
      "step": 2834
    },
    {
      "epoch": 1.030909090909091,
      "grad_norm": 0.04638921841979027,
      "learning_rate": 0.0001936145620288255,
      "loss": 0.3103,
      "step": 2835
    },
    {
      "epoch": 1.0312727272727273,
      "grad_norm": 0.05537167191505432,
      "learning_rate": 0.00019361009724782485,
      "loss": 0.3495,
      "step": 2836
    },
    {
      "epoch": 1.0316363636363637,
      "grad_norm": 0.05288480222225189,
      "learning_rate": 0.0001936056309579691,
      "loss": 0.3432,
      "step": 2837
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.04813016206026077,
      "learning_rate": 0.00019360116315933032,
      "loss": 0.3194,
      "step": 2838
    },
    {
      "epoch": 1.0323636363636364,
      "grad_norm": 0.04415365308523178,
      "learning_rate": 0.0001935966938519805,
      "loss": 0.3282,
      "step": 2839
    },
    {
      "epoch": 1.0327272727272727,
      "grad_norm": 0.04110909625887871,
      "learning_rate": 0.00019359222303599164,
      "loss": 0.2635,
      "step": 2840
    },
    {
      "epoch": 1.033090909090909,
      "grad_norm": 0.055613722652196884,
      "learning_rate": 0.0001935877507114358,
      "loss": 0.3414,
      "step": 2841
    },
    {
      "epoch": 1.0334545454545454,
      "grad_norm": 0.050221674144268036,
      "learning_rate": 0.0001935832768783851,
      "loss": 0.3454,
      "step": 2842
    },
    {
      "epoch": 1.0338181818181817,
      "grad_norm": 0.05891602486371994,
      "learning_rate": 0.00019357880153691163,
      "loss": 0.4113,
      "step": 2843
    },
    {
      "epoch": 1.034181818181818,
      "grad_norm": 0.05496171861886978,
      "learning_rate": 0.00019357432468708756,
      "loss": 0.3258,
      "step": 2844
    },
    {
      "epoch": 1.0345454545454547,
      "grad_norm": 0.05603494867682457,
      "learning_rate": 0.000193569846328985,
      "loss": 0.3581,
      "step": 2845
    },
    {
      "epoch": 1.034909090909091,
      "grad_norm": 0.04785000905394554,
      "learning_rate": 0.0001935653664626762,
      "loss": 0.3101,
      "step": 2846
    },
    {
      "epoch": 1.0352727272727273,
      "grad_norm": 0.051849108189344406,
      "learning_rate": 0.0001935608850882333,
      "loss": 0.4004,
      "step": 2847
    },
    {
      "epoch": 1.0356363636363637,
      "grad_norm": 0.04502946510910988,
      "learning_rate": 0.00019355640220572858,
      "loss": 0.3225,
      "step": 2848
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.05145775526762009,
      "learning_rate": 0.00019355191781523425,
      "loss": 0.3245,
      "step": 2849
    },
    {
      "epoch": 1.0363636363636364,
      "grad_norm": 0.05578811466693878,
      "learning_rate": 0.00019354743191682264,
      "loss": 0.3612,
      "step": 2850
    },
    {
      "epoch": 1.0367272727272727,
      "grad_norm": 0.053337547928094864,
      "learning_rate": 0.00019354294451056603,
      "loss": 0.336,
      "step": 2851
    },
    {
      "epoch": 1.037090909090909,
      "grad_norm": 0.05467696860432625,
      "learning_rate": 0.00019353845559653675,
      "loss": 0.3925,
      "step": 2852
    },
    {
      "epoch": 1.0374545454545454,
      "grad_norm": 0.06682315468788147,
      "learning_rate": 0.0001935339651748072,
      "loss": 0.3779,
      "step": 2853
    },
    {
      "epoch": 1.0378181818181818,
      "grad_norm": 0.04738469421863556,
      "learning_rate": 0.00019352947324544967,
      "loss": 0.3206,
      "step": 2854
    },
    {
      "epoch": 1.038181818181818,
      "grad_norm": 0.05621457099914551,
      "learning_rate": 0.00019352497980853665,
      "loss": 0.2956,
      "step": 2855
    },
    {
      "epoch": 1.0385454545454544,
      "grad_norm": 0.045808617025613785,
      "learning_rate": 0.00019352048486414049,
      "loss": 0.3207,
      "step": 2856
    },
    {
      "epoch": 1.038909090909091,
      "grad_norm": 0.04662981256842613,
      "learning_rate": 0.00019351598841233373,
      "loss": 0.3062,
      "step": 2857
    },
    {
      "epoch": 1.0392727272727273,
      "grad_norm": 0.056149065494537354,
      "learning_rate": 0.00019351149045318877,
      "loss": 0.3217,
      "step": 2858
    },
    {
      "epoch": 1.0396363636363637,
      "grad_norm": 0.06296186149120331,
      "learning_rate": 0.00019350699098677815,
      "loss": 0.3997,
      "step": 2859
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.047982245683670044,
      "learning_rate": 0.00019350249001317442,
      "loss": 0.3404,
      "step": 2860
    },
    {
      "epoch": 1.0403636363636364,
      "grad_norm": 0.053386252373456955,
      "learning_rate": 0.00019349798753245004,
      "loss": 0.366,
      "step": 2861
    },
    {
      "epoch": 1.0407272727272727,
      "grad_norm": 0.06329549849033356,
      "learning_rate": 0.00019349348354467767,
      "loss": 0.3056,
      "step": 2862
    },
    {
      "epoch": 1.041090909090909,
      "grad_norm": 0.049273598939180374,
      "learning_rate": 0.00019348897804992988,
      "loss": 0.3231,
      "step": 2863
    },
    {
      "epoch": 1.0414545454545454,
      "grad_norm": 0.049330439418554306,
      "learning_rate": 0.00019348447104827922,
      "loss": 0.3314,
      "step": 2864
    },
    {
      "epoch": 1.0418181818181818,
      "grad_norm": 0.03630363941192627,
      "learning_rate": 0.00019347996253979847,
      "loss": 0.2387,
      "step": 2865
    },
    {
      "epoch": 1.042181818181818,
      "grad_norm": 0.04598826915025711,
      "learning_rate": 0.0001934754525245602,
      "loss": 0.3142,
      "step": 2866
    },
    {
      "epoch": 1.0425454545454544,
      "grad_norm": 0.06461518257856369,
      "learning_rate": 0.00019347094100263717,
      "loss": 0.3992,
      "step": 2867
    },
    {
      "epoch": 1.042909090909091,
      "grad_norm": 0.039732590317726135,
      "learning_rate": 0.00019346642797410203,
      "loss": 0.3187,
      "step": 2868
    },
    {
      "epoch": 1.0432727272727274,
      "grad_norm": 0.053473688662052155,
      "learning_rate": 0.0001934619134390276,
      "loss": 0.3694,
      "step": 2869
    },
    {
      "epoch": 1.0436363636363637,
      "grad_norm": 0.051831163465976715,
      "learning_rate": 0.00019345739739748657,
      "loss": 0.3452,
      "step": 2870
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.05682859942317009,
      "learning_rate": 0.0001934528798495518,
      "loss": 0.3698,
      "step": 2871
    },
    {
      "epoch": 1.0443636363636364,
      "grad_norm": 0.04568559303879738,
      "learning_rate": 0.00019344836079529605,
      "loss": 0.3006,
      "step": 2872
    },
    {
      "epoch": 1.0447272727272727,
      "grad_norm": 0.05382751300930977,
      "learning_rate": 0.00019344384023479218,
      "loss": 0.3501,
      "step": 2873
    },
    {
      "epoch": 1.045090909090909,
      "grad_norm": 0.048248279839754105,
      "learning_rate": 0.00019343931816811306,
      "loss": 0.3209,
      "step": 2874
    },
    {
      "epoch": 1.0454545454545454,
      "grad_norm": 0.05998901650309563,
      "learning_rate": 0.0001934347945953316,
      "loss": 0.3624,
      "step": 2875
    },
    {
      "epoch": 1.0458181818181818,
      "grad_norm": 0.05262095853686333,
      "learning_rate": 0.0001934302695165207,
      "loss": 0.3156,
      "step": 2876
    },
    {
      "epoch": 1.046181818181818,
      "grad_norm": 0.043377868831157684,
      "learning_rate": 0.00019342574293175328,
      "loss": 0.3139,
      "step": 2877
    },
    {
      "epoch": 1.0465454545454544,
      "grad_norm": 0.040476489812135696,
      "learning_rate": 0.0001934212148411023,
      "loss": 0.2743,
      "step": 2878
    },
    {
      "epoch": 1.046909090909091,
      "grad_norm": 0.055304963141679764,
      "learning_rate": 0.00019341668524464075,
      "loss": 0.3568,
      "step": 2879
    },
    {
      "epoch": 1.0472727272727274,
      "grad_norm": 0.045168764889240265,
      "learning_rate": 0.0001934121541424417,
      "loss": 0.3225,
      "step": 2880
    },
    {
      "epoch": 1.0476363636363637,
      "grad_norm": 0.0449557788670063,
      "learning_rate": 0.0001934076215345781,
      "loss": 0.3299,
      "step": 2881
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.04830868914723396,
      "learning_rate": 0.00019340308742112303,
      "loss": 0.3278,
      "step": 2882
    },
    {
      "epoch": 1.0483636363636364,
      "grad_norm": 0.04808586835861206,
      "learning_rate": 0.00019339855180214958,
      "loss": 0.3118,
      "step": 2883
    },
    {
      "epoch": 1.0487272727272727,
      "grad_norm": 0.07128245383501053,
      "learning_rate": 0.00019339401467773087,
      "loss": 0.3677,
      "step": 2884
    },
    {
      "epoch": 1.049090909090909,
      "grad_norm": 0.03266613185405731,
      "learning_rate": 0.00019338947604794006,
      "loss": 0.2429,
      "step": 2885
    },
    {
      "epoch": 1.0494545454545454,
      "grad_norm": 0.061997897922992706,
      "learning_rate": 0.00019338493591285023,
      "loss": 0.3538,
      "step": 2886
    },
    {
      "epoch": 1.0498181818181818,
      "grad_norm": 0.05756297707557678,
      "learning_rate": 0.0001933803942725346,
      "loss": 0.3923,
      "step": 2887
    },
    {
      "epoch": 1.050181818181818,
      "grad_norm": 0.058814313262701035,
      "learning_rate": 0.0001933758511270664,
      "loss": 0.317,
      "step": 2888
    },
    {
      "epoch": 1.0505454545454544,
      "grad_norm": 0.045752715319395065,
      "learning_rate": 0.00019337130647651882,
      "loss": 0.3157,
      "step": 2889
    },
    {
      "epoch": 1.050909090909091,
      "grad_norm": 0.053435053676366806,
      "learning_rate": 0.00019336676032096511,
      "loss": 0.3709,
      "step": 2890
    },
    {
      "epoch": 1.0512727272727274,
      "grad_norm": 0.05642249435186386,
      "learning_rate": 0.0001933622126604786,
      "loss": 0.3678,
      "step": 2891
    },
    {
      "epoch": 1.0516363636363637,
      "grad_norm": 0.05763829126954079,
      "learning_rate": 0.00019335766349513255,
      "loss": 0.3773,
      "step": 2892
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.04924032837152481,
      "learning_rate": 0.00019335311282500027,
      "loss": 0.3092,
      "step": 2893
    },
    {
      "epoch": 1.0523636363636364,
      "grad_norm": 0.05933147296309471,
      "learning_rate": 0.00019334856065015513,
      "loss": 0.3877,
      "step": 2894
    },
    {
      "epoch": 1.0527272727272727,
      "grad_norm": 0.05346270278096199,
      "learning_rate": 0.0001933440069706705,
      "loss": 0.3577,
      "step": 2895
    },
    {
      "epoch": 1.053090909090909,
      "grad_norm": 0.04928552359342575,
      "learning_rate": 0.00019333945178661978,
      "loss": 0.335,
      "step": 2896
    },
    {
      "epoch": 1.0534545454545454,
      "grad_norm": 0.05948653072118759,
      "learning_rate": 0.00019333489509807642,
      "loss": 0.3693,
      "step": 2897
    },
    {
      "epoch": 1.0538181818181818,
      "grad_norm": 0.05148376524448395,
      "learning_rate": 0.00019333033690511383,
      "loss": 0.36,
      "step": 2898
    },
    {
      "epoch": 1.054181818181818,
      "grad_norm": 0.05520187318325043,
      "learning_rate": 0.00019332577720780552,
      "loss": 0.3431,
      "step": 2899
    },
    {
      "epoch": 1.0545454545454545,
      "grad_norm": 0.04852534830570221,
      "learning_rate": 0.00019332121600622494,
      "loss": 0.3141,
      "step": 2900
    },
    {
      "epoch": 1.0549090909090908,
      "grad_norm": 0.05686906352639198,
      "learning_rate": 0.0001933166533004456,
      "loss": 0.3751,
      "step": 2901
    },
    {
      "epoch": 1.0552727272727274,
      "grad_norm": 0.048220790922641754,
      "learning_rate": 0.00019331208909054113,
      "loss": 0.2796,
      "step": 2902
    },
    {
      "epoch": 1.0556363636363637,
      "grad_norm": 0.04371405020356178,
      "learning_rate": 0.000193307523376585,
      "loss": 0.2592,
      "step": 2903
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.05320574343204498,
      "learning_rate": 0.00019330295615865089,
      "loss": 0.3056,
      "step": 2904
    },
    {
      "epoch": 1.0563636363636364,
      "grad_norm": 0.05624157562851906,
      "learning_rate": 0.00019329838743681232,
      "loss": 0.3331,
      "step": 2905
    },
    {
      "epoch": 1.0567272727272727,
      "grad_norm": 0.04619855061173439,
      "learning_rate": 0.000193293817211143,
      "loss": 0.3018,
      "step": 2906
    },
    {
      "epoch": 1.057090909090909,
      "grad_norm": 0.05046658590435982,
      "learning_rate": 0.00019328924548171658,
      "loss": 0.3579,
      "step": 2907
    },
    {
      "epoch": 1.0574545454545454,
      "grad_norm": 0.04843171685934067,
      "learning_rate": 0.00019328467224860674,
      "loss": 0.2985,
      "step": 2908
    },
    {
      "epoch": 1.0578181818181818,
      "grad_norm": 0.04356221854686737,
      "learning_rate": 0.00019328009751188723,
      "loss": 0.2956,
      "step": 2909
    },
    {
      "epoch": 1.0581818181818181,
      "grad_norm": 0.05736733600497246,
      "learning_rate": 0.00019327552127163173,
      "loss": 0.3481,
      "step": 2910
    },
    {
      "epoch": 1.0585454545454545,
      "grad_norm": 0.05181386321783066,
      "learning_rate": 0.00019327094352791403,
      "loss": 0.3411,
      "step": 2911
    },
    {
      "epoch": 1.0589090909090908,
      "grad_norm": 0.04822801798582077,
      "learning_rate": 0.00019326636428080795,
      "loss": 0.3157,
      "step": 2912
    },
    {
      "epoch": 1.0592727272727274,
      "grad_norm": 0.04572610929608345,
      "learning_rate": 0.00019326178353038724,
      "loss": 0.333,
      "step": 2913
    },
    {
      "epoch": 1.0596363636363637,
      "grad_norm": 0.044149335473775864,
      "learning_rate": 0.00019325720127672577,
      "loss": 0.2751,
      "step": 2914
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.043707896023988724,
      "learning_rate": 0.00019325261751989737,
      "loss": 0.3125,
      "step": 2915
    },
    {
      "epoch": 1.0603636363636364,
      "grad_norm": 0.04387328773736954,
      "learning_rate": 0.000193248032259976,
      "loss": 0.3026,
      "step": 2916
    },
    {
      "epoch": 1.0607272727272727,
      "grad_norm": 0.058038387447595596,
      "learning_rate": 0.00019324344549703545,
      "loss": 0.3834,
      "step": 2917
    },
    {
      "epoch": 1.061090909090909,
      "grad_norm": 0.04849988594651222,
      "learning_rate": 0.00019323885723114973,
      "loss": 0.3115,
      "step": 2918
    },
    {
      "epoch": 1.0614545454545454,
      "grad_norm": 0.04460446536540985,
      "learning_rate": 0.0001932342674623928,
      "loss": 0.309,
      "step": 2919
    },
    {
      "epoch": 1.0618181818181818,
      "grad_norm": 0.03975624963641167,
      "learning_rate": 0.00019322967619083862,
      "loss": 0.2722,
      "step": 2920
    },
    {
      "epoch": 1.0621818181818181,
      "grad_norm": 0.048531610518693924,
      "learning_rate": 0.0001932250834165612,
      "loss": 0.346,
      "step": 2921
    },
    {
      "epoch": 1.0625454545454545,
      "grad_norm": 0.048247843980789185,
      "learning_rate": 0.00019322048913963455,
      "loss": 0.3489,
      "step": 2922
    },
    {
      "epoch": 1.0629090909090908,
      "grad_norm": 0.048437654972076416,
      "learning_rate": 0.00019321589336013272,
      "loss": 0.3279,
      "step": 2923
    },
    {
      "epoch": 1.0632727272727274,
      "grad_norm": 0.04102987423539162,
      "learning_rate": 0.00019321129607812983,
      "loss": 0.3402,
      "step": 2924
    },
    {
      "epoch": 1.0636363636363637,
      "grad_norm": 0.05047319829463959,
      "learning_rate": 0.0001932066972937,
      "loss": 0.3051,
      "step": 2925
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.04593629390001297,
      "learning_rate": 0.0001932020970069172,
      "loss": 0.3559,
      "step": 2926
    },
    {
      "epoch": 1.0643636363636364,
      "grad_norm": 0.05423964932560921,
      "learning_rate": 0.00019319749521785578,
      "loss": 0.345,
      "step": 2927
    },
    {
      "epoch": 1.0647272727272727,
      "grad_norm": 0.038142286241054535,
      "learning_rate": 0.00019319289192658979,
      "loss": 0.2435,
      "step": 2928
    },
    {
      "epoch": 1.065090909090909,
      "grad_norm": 0.04634799435734749,
      "learning_rate": 0.00019318828713319345,
      "loss": 0.3068,
      "step": 2929
    },
    {
      "epoch": 1.0654545454545454,
      "grad_norm": 0.04605512693524361,
      "learning_rate": 0.000193183680837741,
      "loss": 0.3283,
      "step": 2930
    },
    {
      "epoch": 1.0658181818181818,
      "grad_norm": 0.038553375750780106,
      "learning_rate": 0.00019317907304030674,
      "loss": 0.3078,
      "step": 2931
    },
    {
      "epoch": 1.0661818181818181,
      "grad_norm": 0.05939150229096413,
      "learning_rate": 0.00019317446374096482,
      "loss": 0.3511,
      "step": 2932
    },
    {
      "epoch": 1.0665454545454545,
      "grad_norm": 0.04734216630458832,
      "learning_rate": 0.00019316985293978963,
      "loss": 0.271,
      "step": 2933
    },
    {
      "epoch": 1.0669090909090908,
      "grad_norm": 0.03749239444732666,
      "learning_rate": 0.0001931652406368554,
      "loss": 0.2991,
      "step": 2934
    },
    {
      "epoch": 1.0672727272727274,
      "grad_norm": 0.04499142989516258,
      "learning_rate": 0.0001931606268322366,
      "loss": 0.3229,
      "step": 2935
    },
    {
      "epoch": 1.0676363636363637,
      "grad_norm": 0.041156597435474396,
      "learning_rate": 0.0001931560115260075,
      "loss": 0.2816,
      "step": 2936
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.057134971022605896,
      "learning_rate": 0.00019315139471824251,
      "loss": 0.3523,
      "step": 2937
    },
    {
      "epoch": 1.0683636363636364,
      "grad_norm": 0.04680316522717476,
      "learning_rate": 0.00019314677640901607,
      "loss": 0.3276,
      "step": 2938
    },
    {
      "epoch": 1.0687272727272727,
      "grad_norm": 0.0502425953745842,
      "learning_rate": 0.00019314215659840258,
      "loss": 0.3558,
      "step": 2939
    },
    {
      "epoch": 1.069090909090909,
      "grad_norm": 0.057223543524742126,
      "learning_rate": 0.00019313753528647657,
      "loss": 0.3365,
      "step": 2940
    },
    {
      "epoch": 1.0694545454545454,
      "grad_norm": 0.045285701751708984,
      "learning_rate": 0.00019313291247331248,
      "loss": 0.339,
      "step": 2941
    },
    {
      "epoch": 1.0698181818181818,
      "grad_norm": 0.04560269042849541,
      "learning_rate": 0.00019312828815898478,
      "loss": 0.3263,
      "step": 2942
    },
    {
      "epoch": 1.0701818181818181,
      "grad_norm": 0.05478660762310028,
      "learning_rate": 0.0001931236623435681,
      "loss": 0.3544,
      "step": 2943
    },
    {
      "epoch": 1.0705454545454545,
      "grad_norm": 0.06139419972896576,
      "learning_rate": 0.00019311903502713699,
      "loss": 0.3865,
      "step": 2944
    },
    {
      "epoch": 1.0709090909090908,
      "grad_norm": 0.048583365976810455,
      "learning_rate": 0.00019311440620976597,
      "loss": 0.3157,
      "step": 2945
    },
    {
      "epoch": 1.0712727272727274,
      "grad_norm": 0.047664035111665726,
      "learning_rate": 0.00019310977589152967,
      "loss": 0.3076,
      "step": 2946
    },
    {
      "epoch": 1.0716363636363637,
      "grad_norm": 0.05095154419541359,
      "learning_rate": 0.00019310514407250276,
      "loss": 0.3166,
      "step": 2947
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.05122724920511246,
      "learning_rate": 0.0001931005107527599,
      "loss": 0.349,
      "step": 2948
    },
    {
      "epoch": 1.0723636363636364,
      "grad_norm": 0.05005538463592529,
      "learning_rate": 0.0001930958759323757,
      "loss": 0.3257,
      "step": 2949
    },
    {
      "epoch": 1.0727272727272728,
      "grad_norm": 0.05916360393166542,
      "learning_rate": 0.00019309123961142497,
      "loss": 0.3821,
      "step": 2950
    },
    {
      "epoch": 1.073090909090909,
      "grad_norm": 0.05712290480732918,
      "learning_rate": 0.00019308660178998235,
      "loss": 0.3463,
      "step": 2951
    },
    {
      "epoch": 1.0734545454545454,
      "grad_norm": 0.04567938670516014,
      "learning_rate": 0.00019308196246812265,
      "loss": 0.3041,
      "step": 2952
    },
    {
      "epoch": 1.0738181818181818,
      "grad_norm": 0.04934408515691757,
      "learning_rate": 0.0001930773216459206,
      "loss": 0.364,
      "step": 2953
    },
    {
      "epoch": 1.0741818181818181,
      "grad_norm": 0.05427588149905205,
      "learning_rate": 0.00019307267932345106,
      "loss": 0.3956,
      "step": 2954
    },
    {
      "epoch": 1.0745454545454545,
      "grad_norm": 0.05210798978805542,
      "learning_rate": 0.00019306803550078882,
      "loss": 0.3321,
      "step": 2955
    },
    {
      "epoch": 1.0749090909090908,
      "grad_norm": 0.041960105299949646,
      "learning_rate": 0.00019306339017800874,
      "loss": 0.3399,
      "step": 2956
    },
    {
      "epoch": 1.0752727272727274,
      "grad_norm": 0.03866816684603691,
      "learning_rate": 0.00019305874335518572,
      "loss": 0.2711,
      "step": 2957
    },
    {
      "epoch": 1.0756363636363637,
      "grad_norm": 0.042117681354284286,
      "learning_rate": 0.0001930540950323946,
      "loss": 0.3481,
      "step": 2958
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.05649157613515854,
      "learning_rate": 0.00019304944520971037,
      "loss": 0.3953,
      "step": 2959
    },
    {
      "epoch": 1.0763636363636364,
      "grad_norm": 0.05121730640530586,
      "learning_rate": 0.00019304479388720794,
      "loss": 0.3374,
      "step": 2960
    },
    {
      "epoch": 1.0767272727272728,
      "grad_norm": 0.04228063300251961,
      "learning_rate": 0.00019304014106496232,
      "loss": 0.3626,
      "step": 2961
    },
    {
      "epoch": 1.077090909090909,
      "grad_norm": 0.04664112254977226,
      "learning_rate": 0.00019303548674304844,
      "loss": 0.3342,
      "step": 2962
    },
    {
      "epoch": 1.0774545454545454,
      "grad_norm": 0.059312477707862854,
      "learning_rate": 0.00019303083092154136,
      "loss": 0.4185,
      "step": 2963
    },
    {
      "epoch": 1.0778181818181818,
      "grad_norm": 0.05311594903469086,
      "learning_rate": 0.00019302617360051613,
      "loss": 0.3743,
      "step": 2964
    },
    {
      "epoch": 1.0781818181818181,
      "grad_norm": 0.0485435426235199,
      "learning_rate": 0.0001930215147800478,
      "loss": 0.3794,
      "step": 2965
    },
    {
      "epoch": 1.0785454545454545,
      "grad_norm": 0.055091287940740585,
      "learning_rate": 0.00019301685446021153,
      "loss": 0.3224,
      "step": 2966
    },
    {
      "epoch": 1.0789090909090908,
      "grad_norm": 0.05374684929847717,
      "learning_rate": 0.00019301219264108234,
      "loss": 0.3266,
      "step": 2967
    },
    {
      "epoch": 1.0792727272727274,
      "grad_norm": 0.05340292677283287,
      "learning_rate": 0.00019300752932273541,
      "loss": 0.3647,
      "step": 2968
    },
    {
      "epoch": 1.0796363636363637,
      "grad_norm": 0.04860026016831398,
      "learning_rate": 0.0001930028645052459,
      "loss": 0.3546,
      "step": 2969
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.04018968343734741,
      "learning_rate": 0.00019299819818868906,
      "loss": 0.3089,
      "step": 2970
    },
    {
      "epoch": 1.0803636363636364,
      "grad_norm": 0.04034484177827835,
      "learning_rate": 0.00019299353037314005,
      "loss": 0.2964,
      "step": 2971
    },
    {
      "epoch": 1.0807272727272728,
      "grad_norm": 0.045553360134363174,
      "learning_rate": 0.0001929888610586741,
      "loss": 0.339,
      "step": 2972
    },
    {
      "epoch": 1.081090909090909,
      "grad_norm": 0.04711223393678665,
      "learning_rate": 0.00019298419024536644,
      "loss": 0.3948,
      "step": 2973
    },
    {
      "epoch": 1.0814545454545454,
      "grad_norm": 0.06388281285762787,
      "learning_rate": 0.00019297951793329244,
      "loss": 0.3147,
      "step": 2974
    },
    {
      "epoch": 1.0818181818181818,
      "grad_norm": 0.05865244194865227,
      "learning_rate": 0.0001929748441225274,
      "loss": 0.3617,
      "step": 2975
    },
    {
      "epoch": 1.0821818181818181,
      "grad_norm": 0.04665720835328102,
      "learning_rate": 0.0001929701688131466,
      "loss": 0.3363,
      "step": 2976
    },
    {
      "epoch": 1.0825454545454545,
      "grad_norm": 0.05475363880395889,
      "learning_rate": 0.00019296549200522543,
      "loss": 0.3441,
      "step": 2977
    },
    {
      "epoch": 1.0829090909090908,
      "grad_norm": 0.04587004706263542,
      "learning_rate": 0.00019296081369883926,
      "loss": 0.3265,
      "step": 2978
    },
    {
      "epoch": 1.0832727272727274,
      "grad_norm": 0.04655419662594795,
      "learning_rate": 0.00019295613389406352,
      "loss": 0.3223,
      "step": 2979
    },
    {
      "epoch": 1.0836363636363637,
      "grad_norm": 0.054894112050533295,
      "learning_rate": 0.00019295145259097364,
      "loss": 0.3526,
      "step": 2980
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.03739006072282791,
      "learning_rate": 0.00019294676978964505,
      "loss": 0.2912,
      "step": 2981
    },
    {
      "epoch": 1.0843636363636364,
      "grad_norm": 0.033991023898124695,
      "learning_rate": 0.00019294208549015324,
      "loss": 0.2374,
      "step": 2982
    },
    {
      "epoch": 1.0847272727272728,
      "grad_norm": 0.041829414665699005,
      "learning_rate": 0.00019293739969257373,
      "loss": 0.3427,
      "step": 2983
    },
    {
      "epoch": 1.085090909090909,
      "grad_norm": 0.0537307932972908,
      "learning_rate": 0.00019293271239698202,
      "loss": 0.3544,
      "step": 2984
    },
    {
      "epoch": 1.0854545454545454,
      "grad_norm": 0.04606612026691437,
      "learning_rate": 0.0001929280236034537,
      "loss": 0.3245,
      "step": 2985
    },
    {
      "epoch": 1.0858181818181818,
      "grad_norm": 0.05052072927355766,
      "learning_rate": 0.00019292333331206432,
      "loss": 0.3295,
      "step": 2986
    },
    {
      "epoch": 1.0861818181818181,
      "grad_norm": 0.051218919456005096,
      "learning_rate": 0.00019291864152288946,
      "loss": 0.2913,
      "step": 2987
    },
    {
      "epoch": 1.0865454545454545,
      "grad_norm": 0.054320327937603,
      "learning_rate": 0.00019291394823600479,
      "loss": 0.3535,
      "step": 2988
    },
    {
      "epoch": 1.0869090909090908,
      "grad_norm": 0.06185079365968704,
      "learning_rate": 0.00019290925345148593,
      "loss": 0.3897,
      "step": 2989
    },
    {
      "epoch": 1.0872727272727274,
      "grad_norm": 0.04858234524726868,
      "learning_rate": 0.00019290455716940857,
      "loss": 0.3228,
      "step": 2990
    },
    {
      "epoch": 1.0876363636363637,
      "grad_norm": 0.04881161451339722,
      "learning_rate": 0.0001928998593898484,
      "loss": 0.3133,
      "step": 2991
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.046762820333242416,
      "learning_rate": 0.00019289516011288113,
      "loss": 0.3162,
      "step": 2992
    },
    {
      "epoch": 1.0883636363636364,
      "grad_norm": 0.05524272471666336,
      "learning_rate": 0.00019289045933858253,
      "loss": 0.4172,
      "step": 2993
    },
    {
      "epoch": 1.0887272727272728,
      "grad_norm": 0.05303141474723816,
      "learning_rate": 0.00019288575706702835,
      "loss": 0.3651,
      "step": 2994
    },
    {
      "epoch": 1.089090909090909,
      "grad_norm": 0.050090596079826355,
      "learning_rate": 0.00019288105329829438,
      "loss": 0.3624,
      "step": 2995
    },
    {
      "epoch": 1.0894545454545455,
      "grad_norm": 0.07491319626569748,
      "learning_rate": 0.00019287634803245645,
      "loss": 0.411,
      "step": 2996
    },
    {
      "epoch": 1.0898181818181818,
      "grad_norm": 0.044859498739242554,
      "learning_rate": 0.00019287164126959038,
      "loss": 0.319,
      "step": 2997
    },
    {
      "epoch": 1.0901818181818181,
      "grad_norm": 0.04597020521759987,
      "learning_rate": 0.0001928669330097721,
      "loss": 0.3624,
      "step": 2998
    },
    {
      "epoch": 1.0905454545454545,
      "grad_norm": 0.060585252940654755,
      "learning_rate": 0.0001928622232530774,
      "loss": 0.3682,
      "step": 2999
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.05553984269499779,
      "learning_rate": 0.0001928575119995823,
      "loss": 0.3617,
      "step": 3000
    },
    {
      "epoch": 1.0909090909090908,
      "eval_loss": 0.3241868317127228,
      "eval_runtime": 358.1633,
      "eval_samples_per_second": 0.279,
      "eval_steps_per_second": 0.279,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 24750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.02237803592532e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
