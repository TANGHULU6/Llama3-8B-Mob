{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 3750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002666666666666667,
      "grad_norm": 2.7769594192504883,
      "learning_rate": 4e-05,
      "loss": 2.336,
      "step": 1
    },
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 2.7651798725128174,
      "learning_rate": 8e-05,
      "loss": 2.3194,
      "step": 2
    },
    {
      "epoch": 0.0008,
      "grad_norm": 2.432774543762207,
      "learning_rate": 0.00012,
      "loss": 2.2167,
      "step": 3
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 1.904971957206726,
      "learning_rate": 0.00016,
      "loss": 2.1945,
      "step": 4
    },
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 1.549332618713379,
      "learning_rate": 0.0002,
      "loss": 1.9534,
      "step": 5
    },
    {
      "epoch": 0.0016,
      "grad_norm": 1.893057942390442,
      "learning_rate": 0.00019994659546061417,
      "loss": 1.8332,
      "step": 6
    },
    {
      "epoch": 0.0018666666666666666,
      "grad_norm": 1.706571102142334,
      "learning_rate": 0.00019989319092122832,
      "loss": 1.8074,
      "step": 7
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 1.2866756916046143,
      "learning_rate": 0.00019983978638184245,
      "loss": 1.7021,
      "step": 8
    },
    {
      "epoch": 0.0024,
      "grad_norm": 1.0608012676239014,
      "learning_rate": 0.0001997863818424566,
      "loss": 1.5156,
      "step": 9
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 1.7502690553665161,
      "learning_rate": 0.00019973297730307076,
      "loss": 1.5191,
      "step": 10
    },
    {
      "epoch": 0.0029333333333333334,
      "grad_norm": 1.2797855138778687,
      "learning_rate": 0.00019967957276368492,
      "loss": 1.2791,
      "step": 11
    },
    {
      "epoch": 0.0032,
      "grad_norm": 1.1631938219070435,
      "learning_rate": 0.00019962616822429908,
      "loss": 1.2011,
      "step": 12
    },
    {
      "epoch": 0.0034666666666666665,
      "grad_norm": 1.2260103225708008,
      "learning_rate": 0.00019957276368491323,
      "loss": 1.3472,
      "step": 13
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 1.4021657705307007,
      "learning_rate": 0.00019951935914552736,
      "loss": 1.1316,
      "step": 14
    },
    {
      "epoch": 0.004,
      "grad_norm": 1.3979381322860718,
      "learning_rate": 0.00019946595460614152,
      "loss": 1.2952,
      "step": 15
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.7434128522872925,
      "learning_rate": 0.00019941255006675568,
      "loss": 1.1325,
      "step": 16
    },
    {
      "epoch": 0.004533333333333334,
      "grad_norm": 1.4764959812164307,
      "learning_rate": 0.00019935914552736983,
      "loss": 1.0529,
      "step": 17
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.7966116070747375,
      "learning_rate": 0.000199305740987984,
      "loss": 1.1962,
      "step": 18
    },
    {
      "epoch": 0.005066666666666666,
      "grad_norm": 1.6291779279708862,
      "learning_rate": 0.00019925233644859814,
      "loss": 1.2027,
      "step": 19
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 1.0013524293899536,
      "learning_rate": 0.00019919893190921227,
      "loss": 1.0327,
      "step": 20
    },
    {
      "epoch": 0.0056,
      "grad_norm": 0.8231979608535767,
      "learning_rate": 0.00019914552736982643,
      "loss": 0.8297,
      "step": 21
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 1.087308406829834,
      "learning_rate": 0.00019909212283044059,
      "loss": 0.8927,
      "step": 22
    },
    {
      "epoch": 0.0061333333333333335,
      "grad_norm": 1.1726454496383667,
      "learning_rate": 0.00019903871829105477,
      "loss": 0.8465,
      "step": 23
    },
    {
      "epoch": 0.0064,
      "grad_norm": 1.109255075454712,
      "learning_rate": 0.0001989853137516689,
      "loss": 0.7538,
      "step": 24
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.8580666780471802,
      "learning_rate": 0.00019893190921228305,
      "loss": 0.7384,
      "step": 25
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 2.9793009757995605,
      "learning_rate": 0.0001988785046728972,
      "loss": 0.7654,
      "step": 26
    },
    {
      "epoch": 0.0072,
      "grad_norm": 1.3270888328552246,
      "learning_rate": 0.00019882510013351137,
      "loss": 0.6709,
      "step": 27
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 1.9862452745437622,
      "learning_rate": 0.00019877169559412552,
      "loss": 0.5954,
      "step": 28
    },
    {
      "epoch": 0.007733333333333333,
      "grad_norm": 1.6068570613861084,
      "learning_rate": 0.00019871829105473968,
      "loss": 0.6658,
      "step": 29
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.6070213317871094,
      "learning_rate": 0.0001986648865153538,
      "loss": 0.6217,
      "step": 30
    },
    {
      "epoch": 0.008266666666666667,
      "grad_norm": 0.7539510130882263,
      "learning_rate": 0.00019861148197596797,
      "loss": 0.5663,
      "step": 31
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.518711507320404,
      "learning_rate": 0.00019855807743658212,
      "loss": 0.5553,
      "step": 32
    },
    {
      "epoch": 0.0088,
      "grad_norm": 0.5963220000267029,
      "learning_rate": 0.00019850467289719628,
      "loss": 0.5298,
      "step": 33
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.4638374149799347,
      "learning_rate": 0.00019845126835781043,
      "loss": 0.3457,
      "step": 34
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 0.6868871450424194,
      "learning_rate": 0.0001983978638184246,
      "loss": 0.4345,
      "step": 35
    },
    {
      "epoch": 0.0096,
      "grad_norm": 1.3103516101837158,
      "learning_rate": 0.00019834445927903872,
      "loss": 0.4842,
      "step": 36
    },
    {
      "epoch": 0.009866666666666666,
      "grad_norm": 0.8075094223022461,
      "learning_rate": 0.00019829105473965288,
      "loss": 0.6091,
      "step": 37
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 2.1792590618133545,
      "learning_rate": 0.00019823765020026703,
      "loss": 0.5874,
      "step": 38
    },
    {
      "epoch": 0.0104,
      "grad_norm": 0.4785192906856537,
      "learning_rate": 0.0001981842456608812,
      "loss": 0.491,
      "step": 39
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.38771843910217285,
      "learning_rate": 0.00019813084112149535,
      "loss": 0.5198,
      "step": 40
    },
    {
      "epoch": 0.010933333333333333,
      "grad_norm": 0.4661853015422821,
      "learning_rate": 0.00019807743658210947,
      "loss": 0.4254,
      "step": 41
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.4573037922382355,
      "learning_rate": 0.00019802403204272363,
      "loss": 0.5416,
      "step": 42
    },
    {
      "epoch": 0.011466666666666667,
      "grad_norm": 0.38187652826309204,
      "learning_rate": 0.0001979706275033378,
      "loss": 0.4736,
      "step": 43
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.4050770401954651,
      "learning_rate": 0.00019791722296395194,
      "loss": 0.5397,
      "step": 44
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.37234482169151306,
      "learning_rate": 0.0001978638184245661,
      "loss": 0.4673,
      "step": 45
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.3493889272212982,
      "learning_rate": 0.00019781041388518026,
      "loss": 0.4936,
      "step": 46
    },
    {
      "epoch": 0.012533333333333334,
      "grad_norm": 0.3541891872882843,
      "learning_rate": 0.00019775700934579439,
      "loss": 0.4598,
      "step": 47
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.2514253556728363,
      "learning_rate": 0.00019770360480640854,
      "loss": 0.4593,
      "step": 48
    },
    {
      "epoch": 0.013066666666666667,
      "grad_norm": 0.29808932542800903,
      "learning_rate": 0.0001976502002670227,
      "loss": 0.5103,
      "step": 49
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.32638800144195557,
      "learning_rate": 0.00019759679572763685,
      "loss": 0.3857,
      "step": 50
    },
    {
      "epoch": 0.0136,
      "grad_norm": 0.17986002564430237,
      "learning_rate": 0.000197543391188251,
      "loss": 0.4226,
      "step": 51
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.2814393937587738,
      "learning_rate": 0.00019748998664886517,
      "loss": 0.4644,
      "step": 52
    },
    {
      "epoch": 0.014133333333333333,
      "grad_norm": 0.24514342844486237,
      "learning_rate": 0.0001974365821094793,
      "loss": 0.4477,
      "step": 53
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.19955702126026154,
      "learning_rate": 0.00019738317757009345,
      "loss": 0.4017,
      "step": 54
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 0.15592168271541595,
      "learning_rate": 0.0001973297730307076,
      "loss": 0.3833,
      "step": 55
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.16741843521595,
      "learning_rate": 0.00019727636849132177,
      "loss": 0.4679,
      "step": 56
    },
    {
      "epoch": 0.0152,
      "grad_norm": 0.21229234337806702,
      "learning_rate": 0.00019722296395193592,
      "loss": 0.4145,
      "step": 57
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.1794004887342453,
      "learning_rate": 0.00019716955941255008,
      "loss": 0.3435,
      "step": 58
    },
    {
      "epoch": 0.015733333333333332,
      "grad_norm": 0.24429193139076233,
      "learning_rate": 0.00019711615487316423,
      "loss": 0.4664,
      "step": 59
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.16682615876197815,
      "learning_rate": 0.0001970627503337784,
      "loss": 0.3584,
      "step": 60
    },
    {
      "epoch": 0.016266666666666665,
      "grad_norm": 0.22093820571899414,
      "learning_rate": 0.00019700934579439255,
      "loss": 0.3543,
      "step": 61
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.18776819109916687,
      "learning_rate": 0.0001969559412550067,
      "loss": 0.39,
      "step": 62
    },
    {
      "epoch": 0.0168,
      "grad_norm": 0.18788760900497437,
      "learning_rate": 0.00019690253671562083,
      "loss": 0.4054,
      "step": 63
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.2678370773792267,
      "learning_rate": 0.000196849132176235,
      "loss": 0.4578,
      "step": 64
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 0.1840418577194214,
      "learning_rate": 0.00019679572763684915,
      "loss": 0.3128,
      "step": 65
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.18931452929973602,
      "learning_rate": 0.0001967423230974633,
      "loss": 0.4024,
      "step": 66
    },
    {
      "epoch": 0.017866666666666666,
      "grad_norm": 0.14103643596172333,
      "learning_rate": 0.00019668891855807746,
      "loss": 0.3623,
      "step": 67
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.1486661732196808,
      "learning_rate": 0.00019663551401869161,
      "loss": 0.3788,
      "step": 68
    },
    {
      "epoch": 0.0184,
      "grad_norm": 0.2165614813566208,
      "learning_rate": 0.00019658210947930574,
      "loss": 0.5018,
      "step": 69
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.14085222780704498,
      "learning_rate": 0.0001965287049399199,
      "loss": 0.3814,
      "step": 70
    },
    {
      "epoch": 0.018933333333333333,
      "grad_norm": 0.12641431391239166,
      "learning_rate": 0.00019647530040053406,
      "loss": 0.3073,
      "step": 71
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.1765206903219223,
      "learning_rate": 0.0001964218958611482,
      "loss": 0.4217,
      "step": 72
    },
    {
      "epoch": 0.019466666666666667,
      "grad_norm": 0.1404988169670105,
      "learning_rate": 0.00019636849132176237,
      "loss": 0.4165,
      "step": 73
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.1310402750968933,
      "learning_rate": 0.0001963150867823765,
      "loss": 0.3566,
      "step": 74
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.18080885708332062,
      "learning_rate": 0.00019626168224299065,
      "loss": 0.4102,
      "step": 75
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.1589186191558838,
      "learning_rate": 0.0001962082777036048,
      "loss": 0.3838,
      "step": 76
    },
    {
      "epoch": 0.020533333333333334,
      "grad_norm": 0.0990830734372139,
      "learning_rate": 0.00019615487316421897,
      "loss": 0.3952,
      "step": 77
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.2560701072216034,
      "learning_rate": 0.00019610146862483312,
      "loss": 0.3804,
      "step": 78
    },
    {
      "epoch": 0.021066666666666668,
      "grad_norm": 0.10100302845239639,
      "learning_rate": 0.00019604806408544728,
      "loss": 0.3013,
      "step": 79
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.13057103753089905,
      "learning_rate": 0.0001959946595460614,
      "loss": 0.3703,
      "step": 80
    },
    {
      "epoch": 0.0216,
      "grad_norm": 0.1439691185951233,
      "learning_rate": 0.00019594125500667557,
      "loss": 0.3934,
      "step": 81
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.1368185430765152,
      "learning_rate": 0.00019588785046728972,
      "loss": 0.4451,
      "step": 82
    },
    {
      "epoch": 0.022133333333333335,
      "grad_norm": 0.10208338499069214,
      "learning_rate": 0.00019583444592790388,
      "loss": 0.4134,
      "step": 83
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.1145336851477623,
      "learning_rate": 0.00019578104138851803,
      "loss": 0.3747,
      "step": 84
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 0.13336890935897827,
      "learning_rate": 0.0001957276368491322,
      "loss": 0.4036,
      "step": 85
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.13209907710552216,
      "learning_rate": 0.00019567423230974632,
      "loss": 0.3445,
      "step": 86
    },
    {
      "epoch": 0.0232,
      "grad_norm": 0.1405683010816574,
      "learning_rate": 0.00019562082777036048,
      "loss": 0.3472,
      "step": 87
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.10996885597705841,
      "learning_rate": 0.00019556742323097463,
      "loss": 0.3202,
      "step": 88
    },
    {
      "epoch": 0.023733333333333332,
      "grad_norm": 0.172655388712883,
      "learning_rate": 0.0001955140186915888,
      "loss": 0.4418,
      "step": 89
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.1665622591972351,
      "learning_rate": 0.00019546061415220295,
      "loss": 0.4682,
      "step": 90
    },
    {
      "epoch": 0.024266666666666666,
      "grad_norm": 0.13141299784183502,
      "learning_rate": 0.00019540720961281707,
      "loss": 0.3323,
      "step": 91
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.12653924524784088,
      "learning_rate": 0.00019535380507343126,
      "loss": 0.3737,
      "step": 92
    },
    {
      "epoch": 0.0248,
      "grad_norm": 0.16690529882907867,
      "learning_rate": 0.00019530040053404541,
      "loss": 0.3131,
      "step": 93
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.17689360678195953,
      "learning_rate": 0.00019524699599465957,
      "loss": 0.4814,
      "step": 94
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 0.20763857662677765,
      "learning_rate": 0.00019519359145527373,
      "loss": 0.4656,
      "step": 95
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.19238555431365967,
      "learning_rate": 0.00019514018691588786,
      "loss": 0.35,
      "step": 96
    },
    {
      "epoch": 0.025866666666666666,
      "grad_norm": 0.14376568794250488,
      "learning_rate": 0.000195086782376502,
      "loss": 0.4031,
      "step": 97
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.19300998747348785,
      "learning_rate": 0.00019503337783711617,
      "loss": 0.3119,
      "step": 98
    },
    {
      "epoch": 0.0264,
      "grad_norm": 0.15350781381130219,
      "learning_rate": 0.00019497997329773033,
      "loss": 0.3906,
      "step": 99
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.1436162143945694,
      "learning_rate": 0.00019492656875834448,
      "loss": 0.2991,
      "step": 100
    },
    {
      "epoch": 0.026933333333333333,
      "grad_norm": 0.20172013342380524,
      "learning_rate": 0.00019487316421895864,
      "loss": 0.4589,
      "step": 101
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.11807870119810104,
      "learning_rate": 0.00019481975967957277,
      "loss": 0.365,
      "step": 102
    },
    {
      "epoch": 0.027466666666666667,
      "grad_norm": 0.10632023960351944,
      "learning_rate": 0.00019476635514018692,
      "loss": 0.3452,
      "step": 103
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.11992714554071426,
      "learning_rate": 0.00019471295060080108,
      "loss": 0.3754,
      "step": 104
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.1488943248987198,
      "learning_rate": 0.00019465954606141524,
      "loss": 0.3539,
      "step": 105
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.2117643505334854,
      "learning_rate": 0.0001946061415220294,
      "loss": 0.4575,
      "step": 106
    },
    {
      "epoch": 0.028533333333333334,
      "grad_norm": 0.12733204662799835,
      "learning_rate": 0.00019455273698264352,
      "loss": 0.3835,
      "step": 107
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.22344721853733063,
      "learning_rate": 0.00019449933244325768,
      "loss": 0.3382,
      "step": 108
    },
    {
      "epoch": 0.029066666666666668,
      "grad_norm": 0.18072274327278137,
      "learning_rate": 0.00019444592790387183,
      "loss": 0.3457,
      "step": 109
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.11175094544887543,
      "learning_rate": 0.000194392523364486,
      "loss": 0.3082,
      "step": 110
    },
    {
      "epoch": 0.0296,
      "grad_norm": 0.13356728851795197,
      "learning_rate": 0.00019433911882510015,
      "loss": 0.4004,
      "step": 111
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.15116216242313385,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.445,
      "step": 112
    },
    {
      "epoch": 0.030133333333333335,
      "grad_norm": 0.1655212640762329,
      "learning_rate": 0.00019423230974632843,
      "loss": 0.3406,
      "step": 113
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.11419760435819626,
      "learning_rate": 0.0001941789052069426,
      "loss": 0.3168,
      "step": 114
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 0.13604223728179932,
      "learning_rate": 0.00019412550066755675,
      "loss": 0.3524,
      "step": 115
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.1595555692911148,
      "learning_rate": 0.0001940720961281709,
      "loss": 0.2893,
      "step": 116
    },
    {
      "epoch": 0.0312,
      "grad_norm": 0.1399867683649063,
      "learning_rate": 0.00019401869158878506,
      "loss": 0.3092,
      "step": 117
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.10856565833091736,
      "learning_rate": 0.00019396528704939921,
      "loss": 0.2785,
      "step": 118
    },
    {
      "epoch": 0.031733333333333336,
      "grad_norm": 0.1294429749250412,
      "learning_rate": 0.00019391188251001334,
      "loss": 0.4279,
      "step": 119
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.09701752662658691,
      "learning_rate": 0.0001938584779706275,
      "loss": 0.2969,
      "step": 120
    },
    {
      "epoch": 0.032266666666666666,
      "grad_norm": 0.13393566012382507,
      "learning_rate": 0.00019380507343124166,
      "loss": 0.3236,
      "step": 121
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.19966834783554077,
      "learning_rate": 0.0001937516688918558,
      "loss": 0.4025,
      "step": 122
    },
    {
      "epoch": 0.0328,
      "grad_norm": 0.09011684358119965,
      "learning_rate": 0.00019369826435246997,
      "loss": 0.3113,
      "step": 123
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.15907980501651764,
      "learning_rate": 0.00019364485981308413,
      "loss": 0.3196,
      "step": 124
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.15844875574111938,
      "learning_rate": 0.00019359145527369825,
      "loss": 0.3016,
      "step": 125
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.12091420590877533,
      "learning_rate": 0.0001935380507343124,
      "loss": 0.342,
      "step": 126
    },
    {
      "epoch": 0.03386666666666667,
      "grad_norm": 0.12567315995693207,
      "learning_rate": 0.0001934846461949266,
      "loss": 0.3357,
      "step": 127
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.11429810523986816,
      "learning_rate": 0.00019343124165554075,
      "loss": 0.4163,
      "step": 128
    },
    {
      "epoch": 0.0344,
      "grad_norm": 0.11124319583177567,
      "learning_rate": 0.00019337783711615488,
      "loss": 0.3311,
      "step": 129
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.14839474856853485,
      "learning_rate": 0.00019332443257676904,
      "loss": 0.3895,
      "step": 130
    },
    {
      "epoch": 0.03493333333333333,
      "grad_norm": 0.11905749142169952,
      "learning_rate": 0.0001932710280373832,
      "loss": 0.3557,
      "step": 131
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.11633364111185074,
      "learning_rate": 0.00019321762349799735,
      "loss": 0.4014,
      "step": 132
    },
    {
      "epoch": 0.03546666666666667,
      "grad_norm": 0.11608057469129562,
      "learning_rate": 0.0001931642189586115,
      "loss": 0.3494,
      "step": 133
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.11919421702623367,
      "learning_rate": 0.00019311081441922566,
      "loss": 0.4083,
      "step": 134
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.15235432982444763,
      "learning_rate": 0.0001930574098798398,
      "loss": 0.4484,
      "step": 135
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.12104572355747223,
      "learning_rate": 0.00019300400534045395,
      "loss": 0.3444,
      "step": 136
    },
    {
      "epoch": 0.036533333333333334,
      "grad_norm": 0.18770046532154083,
      "learning_rate": 0.0001929506008010681,
      "loss": 0.3617,
      "step": 137
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.1275883913040161,
      "learning_rate": 0.00019289719626168226,
      "loss": 0.4286,
      "step": 138
    },
    {
      "epoch": 0.037066666666666664,
      "grad_norm": 0.11587175726890564,
      "learning_rate": 0.00019284379172229642,
      "loss": 0.345,
      "step": 139
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.1619970053434372,
      "learning_rate": 0.00019279038718291057,
      "loss": 0.3758,
      "step": 140
    },
    {
      "epoch": 0.0376,
      "grad_norm": 0.11526530981063843,
      "learning_rate": 0.0001927369826435247,
      "loss": 0.2962,
      "step": 141
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.10995334386825562,
      "learning_rate": 0.00019268357810413886,
      "loss": 0.355,
      "step": 142
    },
    {
      "epoch": 0.03813333333333333,
      "grad_norm": 0.1100979819893837,
      "learning_rate": 0.00019263017356475301,
      "loss": 0.2951,
      "step": 143
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.12010076642036438,
      "learning_rate": 0.00019257676902536717,
      "loss": 0.353,
      "step": 144
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 0.18297837674617767,
      "learning_rate": 0.00019252336448598133,
      "loss": 0.4417,
      "step": 145
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.11261758953332901,
      "learning_rate": 0.00019246995994659546,
      "loss": 0.3932,
      "step": 146
    },
    {
      "epoch": 0.0392,
      "grad_norm": 0.10015744715929031,
      "learning_rate": 0.0001924165554072096,
      "loss": 0.3556,
      "step": 147
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.128092959523201,
      "learning_rate": 0.00019236315086782377,
      "loss": 0.3444,
      "step": 148
    },
    {
      "epoch": 0.039733333333333336,
      "grad_norm": 0.08879384398460388,
      "learning_rate": 0.00019230974632843793,
      "loss": 0.2762,
      "step": 149
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.12194343656301498,
      "learning_rate": 0.00019225634178905208,
      "loss": 0.3507,
      "step": 150
    },
    {
      "epoch": 0.040266666666666666,
      "grad_norm": 0.12395896017551422,
      "learning_rate": 0.00019220293724966624,
      "loss": 0.3724,
      "step": 151
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.12378206849098206,
      "learning_rate": 0.00019214953271028037,
      "loss": 0.3421,
      "step": 152
    },
    {
      "epoch": 0.0408,
      "grad_norm": 0.1351013034582138,
      "learning_rate": 0.00019209612817089452,
      "loss": 0.3514,
      "step": 153
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.14909370243549347,
      "learning_rate": 0.00019204272363150868,
      "loss": 0.4072,
      "step": 154
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 0.08645568042993546,
      "learning_rate": 0.00019198931909212284,
      "loss": 0.3462,
      "step": 155
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.10950063914060593,
      "learning_rate": 0.000191935914552737,
      "loss": 0.4117,
      "step": 156
    },
    {
      "epoch": 0.04186666666666667,
      "grad_norm": 0.1332060694694519,
      "learning_rate": 0.00019188251001335115,
      "loss": 0.4158,
      "step": 157
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.0849861428141594,
      "learning_rate": 0.00019182910547396528,
      "loss": 0.3003,
      "step": 158
    },
    {
      "epoch": 0.0424,
      "grad_norm": 0.14107772707939148,
      "learning_rate": 0.00019177570093457943,
      "loss": 0.324,
      "step": 159
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.12191599607467651,
      "learning_rate": 0.0001917222963951936,
      "loss": 0.3818,
      "step": 160
    },
    {
      "epoch": 0.04293333333333333,
      "grad_norm": 0.17537343502044678,
      "learning_rate": 0.00019166889185580775,
      "loss": 0.4634,
      "step": 161
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.15070728957653046,
      "learning_rate": 0.0001916154873164219,
      "loss": 0.416,
      "step": 162
    },
    {
      "epoch": 0.04346666666666667,
      "grad_norm": 0.12158092111349106,
      "learning_rate": 0.00019156208277703606,
      "loss": 0.3645,
      "step": 163
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.1030833050608635,
      "learning_rate": 0.00019150867823765022,
      "loss": 0.3322,
      "step": 164
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.12252455204725266,
      "learning_rate": 0.00019145527369826437,
      "loss": 0.3188,
      "step": 165
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.11769659072160721,
      "learning_rate": 0.00019140186915887853,
      "loss": 0.3771,
      "step": 166
    },
    {
      "epoch": 0.044533333333333334,
      "grad_norm": 0.13840539753437042,
      "learning_rate": 0.00019134846461949269,
      "loss": 0.3924,
      "step": 167
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.17646847665309906,
      "learning_rate": 0.00019129506008010681,
      "loss": 0.406,
      "step": 168
    },
    {
      "epoch": 0.045066666666666665,
      "grad_norm": 0.09752658009529114,
      "learning_rate": 0.00019124165554072097,
      "loss": 0.3287,
      "step": 169
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.1111004501581192,
      "learning_rate": 0.00019118825100133513,
      "loss": 0.3063,
      "step": 170
    },
    {
      "epoch": 0.0456,
      "grad_norm": 0.2093600183725357,
      "learning_rate": 0.00019113484646194928,
      "loss": 0.3778,
      "step": 171
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.11585865914821625,
      "learning_rate": 0.00019108144192256344,
      "loss": 0.3169,
      "step": 172
    },
    {
      "epoch": 0.04613333333333333,
      "grad_norm": 0.11165179312229156,
      "learning_rate": 0.0001910280373831776,
      "loss": 0.3355,
      "step": 173
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.12171965837478638,
      "learning_rate": 0.00019097463284379173,
      "loss": 0.3078,
      "step": 174
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.14584897458553314,
      "learning_rate": 0.00019092122830440588,
      "loss": 0.2791,
      "step": 175
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.12469722330570221,
      "learning_rate": 0.00019086782376502004,
      "loss": 0.3327,
      "step": 176
    },
    {
      "epoch": 0.0472,
      "grad_norm": 0.1252918392419815,
      "learning_rate": 0.0001908144192256342,
      "loss": 0.3726,
      "step": 177
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.12764520943164825,
      "learning_rate": 0.00019076101468624835,
      "loss": 0.435,
      "step": 178
    },
    {
      "epoch": 0.047733333333333336,
      "grad_norm": 0.18018434941768646,
      "learning_rate": 0.00019070761014686248,
      "loss": 0.412,
      "step": 179
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.12284374982118607,
      "learning_rate": 0.00019065420560747664,
      "loss": 0.3333,
      "step": 180
    },
    {
      "epoch": 0.048266666666666666,
      "grad_norm": 0.08914666622877121,
      "learning_rate": 0.0001906008010680908,
      "loss": 0.2739,
      "step": 181
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.1757606565952301,
      "learning_rate": 0.00019054739652870495,
      "loss": 0.4352,
      "step": 182
    },
    {
      "epoch": 0.0488,
      "grad_norm": 0.11608554422855377,
      "learning_rate": 0.0001904939919893191,
      "loss": 0.3881,
      "step": 183
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.0818665400147438,
      "learning_rate": 0.00019044058744993326,
      "loss": 0.2758,
      "step": 184
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 0.14201554656028748,
      "learning_rate": 0.0001903871829105474,
      "loss": 0.4241,
      "step": 185
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.13084033131599426,
      "learning_rate": 0.00019033377837116155,
      "loss": 0.3875,
      "step": 186
    },
    {
      "epoch": 0.04986666666666666,
      "grad_norm": 0.09889712929725647,
      "learning_rate": 0.0001902803738317757,
      "loss": 0.272,
      "step": 187
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.13070794939994812,
      "learning_rate": 0.00019022696929238986,
      "loss": 0.3755,
      "step": 188
    },
    {
      "epoch": 0.0504,
      "grad_norm": 0.155448317527771,
      "learning_rate": 0.00019017356475300402,
      "loss": 0.3556,
      "step": 189
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.11069970577955246,
      "learning_rate": 0.00019012016021361817,
      "loss": 0.3297,
      "step": 190
    },
    {
      "epoch": 0.05093333333333333,
      "grad_norm": 0.08097248524427414,
      "learning_rate": 0.0001900667556742323,
      "loss": 0.279,
      "step": 191
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.1457800418138504,
      "learning_rate": 0.00019001335113484646,
      "loss": 0.3863,
      "step": 192
    },
    {
      "epoch": 0.05146666666666667,
      "grad_norm": 0.14904601871967316,
      "learning_rate": 0.00018995994659546061,
      "loss": 0.3551,
      "step": 193
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.08232361078262329,
      "learning_rate": 0.00018990654205607477,
      "loss": 0.3031,
      "step": 194
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.11883440613746643,
      "learning_rate": 0.00018985313751668893,
      "loss": 0.3811,
      "step": 195
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.12429578602313995,
      "learning_rate": 0.00018979973297730308,
      "loss": 0.4302,
      "step": 196
    },
    {
      "epoch": 0.052533333333333335,
      "grad_norm": 0.09985548257827759,
      "learning_rate": 0.00018974632843791724,
      "loss": 0.3279,
      "step": 197
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.1271156221628189,
      "learning_rate": 0.0001896929238985314,
      "loss": 0.3719,
      "step": 198
    },
    {
      "epoch": 0.053066666666666665,
      "grad_norm": 0.10673320293426514,
      "learning_rate": 0.00018963951935914555,
      "loss": 0.2858,
      "step": 199
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10793055593967438,
      "learning_rate": 0.0001895861148197597,
      "loss": 0.2603,
      "step": 200
    },
    {
      "epoch": 0.0536,
      "grad_norm": 0.12885496020317078,
      "learning_rate": 0.00018953271028037384,
      "loss": 0.449,
      "step": 201
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.17503347992897034,
      "learning_rate": 0.000189479305740988,
      "loss": 0.375,
      "step": 202
    },
    {
      "epoch": 0.05413333333333333,
      "grad_norm": 0.11579659581184387,
      "learning_rate": 0.00018942590120160215,
      "loss": 0.326,
      "step": 203
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.13037091493606567,
      "learning_rate": 0.0001893724966622163,
      "loss": 0.374,
      "step": 204
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 0.09736350178718567,
      "learning_rate": 0.00018931909212283046,
      "loss": 0.2968,
      "step": 205
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.1340724229812622,
      "learning_rate": 0.00018926568758344462,
      "loss": 0.3667,
      "step": 206
    },
    {
      "epoch": 0.0552,
      "grad_norm": 0.08980623632669449,
      "learning_rate": 0.00018921228304405875,
      "loss": 0.3628,
      "step": 207
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.09628087282180786,
      "learning_rate": 0.0001891588785046729,
      "loss": 0.2897,
      "step": 208
    },
    {
      "epoch": 0.055733333333333336,
      "grad_norm": 0.09890598058700562,
      "learning_rate": 0.00018910547396528706,
      "loss": 0.2801,
      "step": 209
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.09937585890293121,
      "learning_rate": 0.00018905206942590122,
      "loss": 0.3304,
      "step": 210
    },
    {
      "epoch": 0.056266666666666666,
      "grad_norm": 0.17049841582775116,
      "learning_rate": 0.00018899866488651537,
      "loss": 0.3618,
      "step": 211
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.12683941423892975,
      "learning_rate": 0.0001889452603471295,
      "loss": 0.3975,
      "step": 212
    },
    {
      "epoch": 0.0568,
      "grad_norm": 0.1168665885925293,
      "learning_rate": 0.00018889185580774366,
      "loss": 0.3286,
      "step": 213
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.1563466191291809,
      "learning_rate": 0.00018883845126835782,
      "loss": 0.4136,
      "step": 214
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 0.0925186276435852,
      "learning_rate": 0.00018878504672897197,
      "loss": 0.3282,
      "step": 215
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.12716545164585114,
      "learning_rate": 0.00018873164218958613,
      "loss": 0.4319,
      "step": 216
    },
    {
      "epoch": 0.057866666666666663,
      "grad_norm": 0.09222135692834854,
      "learning_rate": 0.00018867823765020028,
      "loss": 0.3501,
      "step": 217
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.08521480113267899,
      "learning_rate": 0.00018862483311081441,
      "loss": 0.3013,
      "step": 218
    },
    {
      "epoch": 0.0584,
      "grad_norm": 0.10591351240873337,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3375,
      "step": 219
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.11623245477676392,
      "learning_rate": 0.00018851802403204273,
      "loss": 0.4407,
      "step": 220
    },
    {
      "epoch": 0.05893333333333333,
      "grad_norm": 0.08938480168581009,
      "learning_rate": 0.00018846461949265688,
      "loss": 0.3763,
      "step": 221
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.16583426296710968,
      "learning_rate": 0.00018841121495327104,
      "loss": 0.3806,
      "step": 222
    },
    {
      "epoch": 0.05946666666666667,
      "grad_norm": 0.09945130348205566,
      "learning_rate": 0.0001883578104138852,
      "loss": 0.3457,
      "step": 223
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.09420512616634369,
      "learning_rate": 0.00018830440587449932,
      "loss": 0.3744,
      "step": 224
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11239046603441238,
      "learning_rate": 0.00018825100133511348,
      "loss": 0.3097,
      "step": 225
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.13298054039478302,
      "learning_rate": 0.00018819759679572764,
      "loss": 0.3273,
      "step": 226
    },
    {
      "epoch": 0.060533333333333335,
      "grad_norm": 0.083345428109169,
      "learning_rate": 0.0001881441922563418,
      "loss": 0.2886,
      "step": 227
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.09918737411499023,
      "learning_rate": 0.00018809078771695595,
      "loss": 0.3892,
      "step": 228
    },
    {
      "epoch": 0.061066666666666665,
      "grad_norm": 0.17659562826156616,
      "learning_rate": 0.00018803738317757008,
      "loss": 0.2988,
      "step": 229
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.08680819720029831,
      "learning_rate": 0.00018798397863818424,
      "loss": 0.3886,
      "step": 230
    },
    {
      "epoch": 0.0616,
      "grad_norm": 0.10720266401767731,
      "learning_rate": 0.00018793057409879842,
      "loss": 0.322,
      "step": 231
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.09973270446062088,
      "learning_rate": 0.00018787716955941258,
      "loss": 0.3807,
      "step": 232
    },
    {
      "epoch": 0.06213333333333333,
      "grad_norm": 0.10070598870515823,
      "learning_rate": 0.00018782376502002673,
      "loss": 0.3325,
      "step": 233
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.11900436133146286,
      "learning_rate": 0.00018777036048064086,
      "loss": 0.4054,
      "step": 234
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.10074819624423981,
      "learning_rate": 0.00018771695594125502,
      "loss": 0.3834,
      "step": 235
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.09200602769851685,
      "learning_rate": 0.00018766355140186917,
      "loss": 0.3373,
      "step": 236
    },
    {
      "epoch": 0.0632,
      "grad_norm": 0.11878497898578644,
      "learning_rate": 0.00018761014686248333,
      "loss": 0.41,
      "step": 237
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.10810259729623795,
      "learning_rate": 0.0001875567423230975,
      "loss": 0.339,
      "step": 238
    },
    {
      "epoch": 0.06373333333333334,
      "grad_norm": 0.09370433539152145,
      "learning_rate": 0.00018750333778371164,
      "loss": 0.321,
      "step": 239
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.09535923600196838,
      "learning_rate": 0.00018744993324432577,
      "loss": 0.3221,
      "step": 240
    },
    {
      "epoch": 0.06426666666666667,
      "grad_norm": 0.07852452993392944,
      "learning_rate": 0.00018739652870493993,
      "loss": 0.3245,
      "step": 241
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.13336415588855743,
      "learning_rate": 0.00018734312416555408,
      "loss": 0.4125,
      "step": 242
    },
    {
      "epoch": 0.0648,
      "grad_norm": 0.10594101250171661,
      "learning_rate": 0.00018728971962616824,
      "loss": 0.2767,
      "step": 243
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.12402519583702087,
      "learning_rate": 0.0001872363150867824,
      "loss": 0.3291,
      "step": 244
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 0.10640190541744232,
      "learning_rate": 0.00018718291054739653,
      "loss": 0.3419,
      "step": 245
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.08042649924755096,
      "learning_rate": 0.00018712950600801068,
      "loss": 0.3322,
      "step": 246
    },
    {
      "epoch": 0.06586666666666667,
      "grad_norm": 0.12702935934066772,
      "learning_rate": 0.00018707610146862484,
      "loss": 0.3911,
      "step": 247
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.10496889054775238,
      "learning_rate": 0.000187022696929239,
      "loss": 0.3446,
      "step": 248
    },
    {
      "epoch": 0.0664,
      "grad_norm": 0.10453516244888306,
      "learning_rate": 0.00018696929238985315,
      "loss": 0.3812,
      "step": 249
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.27628394961357117,
      "learning_rate": 0.0001869158878504673,
      "loss": 0.449,
      "step": 250
    },
    {
      "epoch": 0.06693333333333333,
      "grad_norm": 0.0800919383764267,
      "learning_rate": 0.00018686248331108144,
      "loss": 0.3109,
      "step": 251
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.15524238348007202,
      "learning_rate": 0.0001868090787716956,
      "loss": 0.3681,
      "step": 252
    },
    {
      "epoch": 0.06746666666666666,
      "grad_norm": 0.11795137077569962,
      "learning_rate": 0.00018675567423230975,
      "loss": 0.3178,
      "step": 253
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.1363411247730255,
      "learning_rate": 0.0001867022696929239,
      "loss": 0.3777,
      "step": 254
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.06862572580575943,
      "learning_rate": 0.00018664886515353806,
      "loss": 0.3436,
      "step": 255
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.11015625298023224,
      "learning_rate": 0.00018659546061415222,
      "loss": 0.4006,
      "step": 256
    },
    {
      "epoch": 0.06853333333333333,
      "grad_norm": 0.09544744342565536,
      "learning_rate": 0.00018654205607476635,
      "loss": 0.3619,
      "step": 257
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.06837030500173569,
      "learning_rate": 0.0001864886515353805,
      "loss": 0.3099,
      "step": 258
    },
    {
      "epoch": 0.06906666666666667,
      "grad_norm": 0.10089077800512314,
      "learning_rate": 0.00018643524699599466,
      "loss": 0.3678,
      "step": 259
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.07501330226659775,
      "learning_rate": 0.00018638184245660882,
      "loss": 0.2806,
      "step": 260
    },
    {
      "epoch": 0.0696,
      "grad_norm": 0.10709015280008316,
      "learning_rate": 0.00018632843791722297,
      "loss": 0.3752,
      "step": 261
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.12912361323833466,
      "learning_rate": 0.0001862750333778371,
      "loss": 0.3451,
      "step": 262
    },
    {
      "epoch": 0.07013333333333334,
      "grad_norm": 0.10534986853599548,
      "learning_rate": 0.00018622162883845126,
      "loss": 0.3555,
      "step": 263
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.12017720192670822,
      "learning_rate": 0.00018616822429906542,
      "loss": 0.2731,
      "step": 264
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 0.07401938736438751,
      "learning_rate": 0.00018611481975967957,
      "loss": 0.3414,
      "step": 265
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.12359147518873215,
      "learning_rate": 0.00018606141522029376,
      "loss": 0.3524,
      "step": 266
    },
    {
      "epoch": 0.0712,
      "grad_norm": 0.09959493577480316,
      "learning_rate": 0.00018600801068090788,
      "loss": 0.3591,
      "step": 267
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.10503039509057999,
      "learning_rate": 0.00018595460614152204,
      "loss": 0.4066,
      "step": 268
    },
    {
      "epoch": 0.07173333333333333,
      "grad_norm": 0.10504454374313354,
      "learning_rate": 0.0001859012016021362,
      "loss": 0.3323,
      "step": 269
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.10397278517484665,
      "learning_rate": 0.00018584779706275035,
      "loss": 0.3388,
      "step": 270
    },
    {
      "epoch": 0.07226666666666667,
      "grad_norm": 0.11638778448104858,
      "learning_rate": 0.0001857943925233645,
      "loss": 0.4342,
      "step": 271
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.0659443736076355,
      "learning_rate": 0.00018574098798397867,
      "loss": 0.2589,
      "step": 272
    },
    {
      "epoch": 0.0728,
      "grad_norm": 0.1155213713645935,
      "learning_rate": 0.0001856875834445928,
      "loss": 0.292,
      "step": 273
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.08533418923616409,
      "learning_rate": 0.00018563417890520695,
      "loss": 0.2912,
      "step": 274
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09533689171075821,
      "learning_rate": 0.0001855807743658211,
      "loss": 0.325,
      "step": 275
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.09169981628656387,
      "learning_rate": 0.00018552736982643526,
      "loss": 0.3647,
      "step": 276
    },
    {
      "epoch": 0.07386666666666666,
      "grad_norm": 0.10702133178710938,
      "learning_rate": 0.00018547396528704942,
      "loss": 0.3699,
      "step": 277
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.09436537325382233,
      "learning_rate": 0.00018542056074766355,
      "loss": 0.3627,
      "step": 278
    },
    {
      "epoch": 0.0744,
      "grad_norm": 0.12457992881536484,
      "learning_rate": 0.0001853671562082777,
      "loss": 0.275,
      "step": 279
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.08151577413082123,
      "learning_rate": 0.00018531375166889186,
      "loss": 0.2957,
      "step": 280
    },
    {
      "epoch": 0.07493333333333334,
      "grad_norm": 0.12035655975341797,
      "learning_rate": 0.00018526034712950602,
      "loss": 0.3427,
      "step": 281
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.11815600842237473,
      "learning_rate": 0.00018520694259012018,
      "loss": 0.3818,
      "step": 282
    },
    {
      "epoch": 0.07546666666666667,
      "grad_norm": 0.08029612898826599,
      "learning_rate": 0.00018515353805073433,
      "loss": 0.3087,
      "step": 283
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.12576709687709808,
      "learning_rate": 0.00018510013351134846,
      "loss": 0.3483,
      "step": 284
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.08345084637403488,
      "learning_rate": 0.00018504672897196262,
      "loss": 0.3296,
      "step": 285
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.10913220793008804,
      "learning_rate": 0.00018499332443257677,
      "loss": 0.31,
      "step": 286
    },
    {
      "epoch": 0.07653333333333333,
      "grad_norm": 0.09327565133571625,
      "learning_rate": 0.00018493991989319093,
      "loss": 0.3615,
      "step": 287
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.08907847106456757,
      "learning_rate": 0.00018488651535380509,
      "loss": 0.343,
      "step": 288
    },
    {
      "epoch": 0.07706666666666667,
      "grad_norm": 0.09010271728038788,
      "learning_rate": 0.00018483311081441924,
      "loss": 0.3414,
      "step": 289
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.12383982539176941,
      "learning_rate": 0.00018477970627503337,
      "loss": 0.3528,
      "step": 290
    },
    {
      "epoch": 0.0776,
      "grad_norm": 0.11043582856655121,
      "learning_rate": 0.00018472630173564753,
      "loss": 0.3634,
      "step": 291
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.1111551821231842,
      "learning_rate": 0.00018467289719626168,
      "loss": 0.3788,
      "step": 292
    },
    {
      "epoch": 0.07813333333333333,
      "grad_norm": 0.08939313143491745,
      "learning_rate": 0.00018461949265687584,
      "loss": 0.3178,
      "step": 293
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.10902975499629974,
      "learning_rate": 0.00018456608811749,
      "loss": 0.3306,
      "step": 294
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 0.1235845685005188,
      "learning_rate": 0.00018451268357810413,
      "loss": 0.3216,
      "step": 295
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.10116824507713318,
      "learning_rate": 0.00018445927903871828,
      "loss": 0.2877,
      "step": 296
    },
    {
      "epoch": 0.0792,
      "grad_norm": 0.16232015192508698,
      "learning_rate": 0.00018440587449933244,
      "loss": 0.4181,
      "step": 297
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.09413047134876251,
      "learning_rate": 0.0001843524699599466,
      "loss": 0.3181,
      "step": 298
    },
    {
      "epoch": 0.07973333333333334,
      "grad_norm": 0.1030365377664566,
      "learning_rate": 0.00018429906542056075,
      "loss": 0.2937,
      "step": 299
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.12266381084918976,
      "learning_rate": 0.0001842456608811749,
      "loss": 0.3671,
      "step": 300
    },
    {
      "epoch": 0.08026666666666667,
      "grad_norm": 0.11648906767368317,
      "learning_rate": 0.00018419225634178906,
      "loss": 0.463,
      "step": 301
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.08957839012145996,
      "learning_rate": 0.00018413885180240322,
      "loss": 0.3273,
      "step": 302
    },
    {
      "epoch": 0.0808,
      "grad_norm": 0.12123125046491623,
      "learning_rate": 0.00018408544726301738,
      "loss": 0.3442,
      "step": 303
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.09023109823465347,
      "learning_rate": 0.00018403204272363153,
      "loss": 0.2624,
      "step": 304
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 0.10406859964132309,
      "learning_rate": 0.0001839786381842457,
      "loss": 0.3178,
      "step": 305
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.09043893218040466,
      "learning_rate": 0.00018392523364485982,
      "loss": 0.3527,
      "step": 306
    },
    {
      "epoch": 0.08186666666666667,
      "grad_norm": 0.11430690437555313,
      "learning_rate": 0.00018387182910547398,
      "loss": 0.3445,
      "step": 307
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.08870529383420944,
      "learning_rate": 0.00018381842456608813,
      "loss": 0.3414,
      "step": 308
    },
    {
      "epoch": 0.0824,
      "grad_norm": 0.15425847470760345,
      "learning_rate": 0.0001837650200267023,
      "loss": 0.4035,
      "step": 309
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.07275878638029099,
      "learning_rate": 0.00018371161548731644,
      "loss": 0.287,
      "step": 310
    },
    {
      "epoch": 0.08293333333333333,
      "grad_norm": 0.08277386426925659,
      "learning_rate": 0.00018365821094793057,
      "loss": 0.3542,
      "step": 311
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.07811281830072403,
      "learning_rate": 0.00018360480640854473,
      "loss": 0.3465,
      "step": 312
    },
    {
      "epoch": 0.08346666666666666,
      "grad_norm": 0.08803758025169373,
      "learning_rate": 0.00018355140186915889,
      "loss": 0.3247,
      "step": 313
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.10004935413599014,
      "learning_rate": 0.00018349799732977304,
      "loss": 0.401,
      "step": 314
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.0781673714518547,
      "learning_rate": 0.0001834445927903872,
      "loss": 0.2821,
      "step": 315
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.09176547080278397,
      "learning_rate": 0.00018339118825100136,
      "loss": 0.3623,
      "step": 316
    },
    {
      "epoch": 0.08453333333333334,
      "grad_norm": 0.13072100281715393,
      "learning_rate": 0.00018333778371161548,
      "loss": 0.4092,
      "step": 317
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.06539219617843628,
      "learning_rate": 0.00018328437917222964,
      "loss": 0.2762,
      "step": 318
    },
    {
      "epoch": 0.08506666666666667,
      "grad_norm": 0.10760210454463959,
      "learning_rate": 0.0001832309746328438,
      "loss": 0.3771,
      "step": 319
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.09418851882219315,
      "learning_rate": 0.00018317757009345795,
      "loss": 0.3324,
      "step": 320
    },
    {
      "epoch": 0.0856,
      "grad_norm": 0.09633705019950867,
      "learning_rate": 0.0001831241655540721,
      "loss": 0.296,
      "step": 321
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.08490312844514847,
      "learning_rate": 0.00018307076101468627,
      "loss": 0.3346,
      "step": 322
    },
    {
      "epoch": 0.08613333333333334,
      "grad_norm": 0.10471192002296448,
      "learning_rate": 0.0001830173564753004,
      "loss": 0.3709,
      "step": 323
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.09079064428806305,
      "learning_rate": 0.00018296395193591455,
      "loss": 0.3329,
      "step": 324
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.07000778615474701,
      "learning_rate": 0.0001829105473965287,
      "loss": 0.2454,
      "step": 325
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.08757971227169037,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.2939,
      "step": 326
    },
    {
      "epoch": 0.0872,
      "grad_norm": 0.0803586095571518,
      "learning_rate": 0.00018280373831775702,
      "loss": 0.3068,
      "step": 327
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.08517909795045853,
      "learning_rate": 0.00018275033377837115,
      "loss": 0.3234,
      "step": 328
    },
    {
      "epoch": 0.08773333333333333,
      "grad_norm": 0.0956772044301033,
      "learning_rate": 0.0001826969292389853,
      "loss": 0.3296,
      "step": 329
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.1021014079451561,
      "learning_rate": 0.00018264352469959946,
      "loss": 0.3686,
      "step": 330
    },
    {
      "epoch": 0.08826666666666666,
      "grad_norm": 0.06662089377641678,
      "learning_rate": 0.00018259012016021362,
      "loss": 0.3285,
      "step": 331
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.09295922517776489,
      "learning_rate": 0.00018253671562082778,
      "loss": 0.3033,
      "step": 332
    },
    {
      "epoch": 0.0888,
      "grad_norm": 0.11561484634876251,
      "learning_rate": 0.00018248331108144193,
      "loss": 0.3848,
      "step": 333
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.09677468985319138,
      "learning_rate": 0.00018242990654205606,
      "loss": 0.3679,
      "step": 334
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.11748673766851425,
      "learning_rate": 0.00018237650200267024,
      "loss": 0.2837,
      "step": 335
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.09182518720626831,
      "learning_rate": 0.0001823230974632844,
      "loss": 0.3022,
      "step": 336
    },
    {
      "epoch": 0.08986666666666666,
      "grad_norm": 0.10123071819543839,
      "learning_rate": 0.00018226969292389856,
      "loss": 0.4102,
      "step": 337
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.08239907026290894,
      "learning_rate": 0.0001822162883845127,
      "loss": 0.382,
      "step": 338
    },
    {
      "epoch": 0.0904,
      "grad_norm": 0.08865305781364441,
      "learning_rate": 0.00018216288384512684,
      "loss": 0.3433,
      "step": 339
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.12360157072544098,
      "learning_rate": 0.000182109479305741,
      "loss": 0.3047,
      "step": 340
    },
    {
      "epoch": 0.09093333333333334,
      "grad_norm": 0.08250956237316132,
      "learning_rate": 0.00018205607476635516,
      "loss": 0.3738,
      "step": 341
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.08227312564849854,
      "learning_rate": 0.0001820026702269693,
      "loss": 0.3741,
      "step": 342
    },
    {
      "epoch": 0.09146666666666667,
      "grad_norm": 0.08315666019916534,
      "learning_rate": 0.00018194926568758347,
      "loss": 0.3399,
      "step": 343
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.11330034583806992,
      "learning_rate": 0.0001818958611481976,
      "loss": 0.3599,
      "step": 344
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.07047869265079498,
      "learning_rate": 0.00018184245660881175,
      "loss": 0.2838,
      "step": 345
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.09761319309473038,
      "learning_rate": 0.0001817890520694259,
      "loss": 0.365,
      "step": 346
    },
    {
      "epoch": 0.09253333333333333,
      "grad_norm": 0.0946047380566597,
      "learning_rate": 0.00018173564753004007,
      "loss": 0.312,
      "step": 347
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.08160915970802307,
      "learning_rate": 0.00018168224299065422,
      "loss": 0.3221,
      "step": 348
    },
    {
      "epoch": 0.09306666666666667,
      "grad_norm": 0.10196121037006378,
      "learning_rate": 0.00018162883845126838,
      "loss": 0.4244,
      "step": 349
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.0740332081913948,
      "learning_rate": 0.0001815754339118825,
      "loss": 0.2463,
      "step": 350
    },
    {
      "epoch": 0.0936,
      "grad_norm": 0.09823732823133469,
      "learning_rate": 0.00018152202937249666,
      "loss": 0.3782,
      "step": 351
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.11142392456531525,
      "learning_rate": 0.00018146862483311082,
      "loss": 0.3483,
      "step": 352
    },
    {
      "epoch": 0.09413333333333333,
      "grad_norm": 0.13181866705417633,
      "learning_rate": 0.00018141522029372498,
      "loss": 0.3999,
      "step": 353
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.11011475324630737,
      "learning_rate": 0.00018136181575433913,
      "loss": 0.3535,
      "step": 354
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 0.08889131247997284,
      "learning_rate": 0.0001813084112149533,
      "loss": 0.3095,
      "step": 355
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.18175657093524933,
      "learning_rate": 0.00018125500667556742,
      "loss": 0.4212,
      "step": 356
    },
    {
      "epoch": 0.0952,
      "grad_norm": 0.12061215192079544,
      "learning_rate": 0.00018120160213618158,
      "loss": 0.3364,
      "step": 357
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.11756326258182526,
      "learning_rate": 0.00018114819759679573,
      "loss": 0.3334,
      "step": 358
    },
    {
      "epoch": 0.09573333333333334,
      "grad_norm": 0.09401579946279526,
      "learning_rate": 0.0001810947930574099,
      "loss": 0.3116,
      "step": 359
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.08736762404441833,
      "learning_rate": 0.00018104138851802404,
      "loss": 0.3545,
      "step": 360
    },
    {
      "epoch": 0.09626666666666667,
      "grad_norm": 0.07576965540647507,
      "learning_rate": 0.0001809879839786382,
      "loss": 0.3177,
      "step": 361
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.10332034528255463,
      "learning_rate": 0.00018093457943925233,
      "loss": 0.3631,
      "step": 362
    },
    {
      "epoch": 0.0968,
      "grad_norm": 0.08649254590272903,
      "learning_rate": 0.00018088117489986649,
      "loss": 0.3471,
      "step": 363
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.07940498739480972,
      "learning_rate": 0.00018082777036048064,
      "loss": 0.343,
      "step": 364
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 0.09086126834154129,
      "learning_rate": 0.0001807743658210948,
      "loss": 0.3885,
      "step": 365
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.0857425332069397,
      "learning_rate": 0.00018072096128170896,
      "loss": 0.3966,
      "step": 366
    },
    {
      "epoch": 0.09786666666666667,
      "grad_norm": 0.12055695056915283,
      "learning_rate": 0.00018066755674232308,
      "loss": 0.3158,
      "step": 367
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.07988156378269196,
      "learning_rate": 0.00018061415220293724,
      "loss": 0.2774,
      "step": 368
    },
    {
      "epoch": 0.0984,
      "grad_norm": 0.06659630686044693,
      "learning_rate": 0.0001805607476635514,
      "loss": 0.3236,
      "step": 369
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.15708456933498383,
      "learning_rate": 0.00018050734312416558,
      "loss": 0.3724,
      "step": 370
    },
    {
      "epoch": 0.09893333333333333,
      "grad_norm": 0.07169601321220398,
      "learning_rate": 0.00018045393858477974,
      "loss": 0.2664,
      "step": 371
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.07468192279338837,
      "learning_rate": 0.00018040053404539387,
      "loss": 0.3354,
      "step": 372
    },
    {
      "epoch": 0.09946666666666666,
      "grad_norm": 0.10080711543560028,
      "learning_rate": 0.00018034712950600802,
      "loss": 0.331,
      "step": 373
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.1319732517004013,
      "learning_rate": 0.00018029372496662218,
      "loss": 0.2848,
      "step": 374
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.09208314120769501,
      "learning_rate": 0.00018024032042723633,
      "loss": 0.2939,
      "step": 375
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.10687708109617233,
      "learning_rate": 0.0001801869158878505,
      "loss": 0.3646,
      "step": 376
    },
    {
      "epoch": 0.10053333333333334,
      "grad_norm": 0.16577519476413727,
      "learning_rate": 0.00018013351134846465,
      "loss": 0.4477,
      "step": 377
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.08952967822551727,
      "learning_rate": 0.00018008010680907878,
      "loss": 0.3802,
      "step": 378
    },
    {
      "epoch": 0.10106666666666667,
      "grad_norm": 0.12839622795581818,
      "learning_rate": 0.00018002670226969293,
      "loss": 0.3481,
      "step": 379
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.10851524770259857,
      "learning_rate": 0.0001799732977303071,
      "loss": 0.3391,
      "step": 380
    },
    {
      "epoch": 0.1016,
      "grad_norm": 0.10551099479198456,
      "learning_rate": 0.00017991989319092125,
      "loss": 0.3821,
      "step": 381
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.07586202025413513,
      "learning_rate": 0.0001798664886515354,
      "loss": 0.3524,
      "step": 382
    },
    {
      "epoch": 0.10213333333333334,
      "grad_norm": 0.0842336043715477,
      "learning_rate": 0.00017981308411214953,
      "loss": 0.3283,
      "step": 383
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.12792328000068665,
      "learning_rate": 0.0001797596795727637,
      "loss": 0.3679,
      "step": 384
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 0.09130851179361343,
      "learning_rate": 0.00017970627503337784,
      "loss": 0.3874,
      "step": 385
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.09045165777206421,
      "learning_rate": 0.000179652870493992,
      "loss": 0.3769,
      "step": 386
    },
    {
      "epoch": 0.1032,
      "grad_norm": 0.12986332178115845,
      "learning_rate": 0.00017959946595460616,
      "loss": 0.3467,
      "step": 387
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.1044159010052681,
      "learning_rate": 0.0001795460614152203,
      "loss": 0.3924,
      "step": 388
    },
    {
      "epoch": 0.10373333333333333,
      "grad_norm": 0.1237383782863617,
      "learning_rate": 0.00017949265687583444,
      "loss": 0.3678,
      "step": 389
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.06824375689029694,
      "learning_rate": 0.0001794392523364486,
      "loss": 0.2877,
      "step": 390
    },
    {
      "epoch": 0.10426666666666666,
      "grad_norm": 0.11664523184299469,
      "learning_rate": 0.00017938584779706275,
      "loss": 0.3265,
      "step": 391
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.09297315031290054,
      "learning_rate": 0.0001793324432576769,
      "loss": 0.3396,
      "step": 392
    },
    {
      "epoch": 0.1048,
      "grad_norm": 0.07559403032064438,
      "learning_rate": 0.00017927903871829107,
      "loss": 0.3101,
      "step": 393
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.07870034128427505,
      "learning_rate": 0.00017922563417890522,
      "loss": 0.3239,
      "step": 394
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 0.07008389383554459,
      "learning_rate": 0.00017917222963951935,
      "loss": 0.3279,
      "step": 395
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.10542725771665573,
      "learning_rate": 0.0001791188251001335,
      "loss": 0.3977,
      "step": 396
    },
    {
      "epoch": 0.10586666666666666,
      "grad_norm": 0.07911863178014755,
      "learning_rate": 0.00017906542056074767,
      "loss": 0.3385,
      "step": 397
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.0968279168009758,
      "learning_rate": 0.00017901201602136182,
      "loss": 0.327,
      "step": 398
    },
    {
      "epoch": 0.1064,
      "grad_norm": 0.06420813500881195,
      "learning_rate": 0.00017895861148197598,
      "loss": 0.3115,
      "step": 399
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.08770374208688736,
      "learning_rate": 0.0001789052069425901,
      "loss": 0.365,
      "step": 400
    },
    {
      "epoch": 0.10693333333333334,
      "grad_norm": 0.0768328607082367,
      "learning_rate": 0.00017885180240320426,
      "loss": 0.3268,
      "step": 401
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.11329832673072815,
      "learning_rate": 0.00017879839786381842,
      "loss": 0.3431,
      "step": 402
    },
    {
      "epoch": 0.10746666666666667,
      "grad_norm": 0.07099627703428268,
      "learning_rate": 0.00017874499332443258,
      "loss": 0.3672,
      "step": 403
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.06802398711442947,
      "learning_rate": 0.00017869158878504676,
      "loss": 0.3391,
      "step": 404
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.06539471447467804,
      "learning_rate": 0.0001786381842456609,
      "loss": 0.2524,
      "step": 405
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.08650172501802444,
      "learning_rate": 0.00017858477970627505,
      "loss": 0.3642,
      "step": 406
    },
    {
      "epoch": 0.10853333333333333,
      "grad_norm": 0.08286081999540329,
      "learning_rate": 0.0001785313751668892,
      "loss": 0.302,
      "step": 407
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.09322597086429596,
      "learning_rate": 0.00017847797062750336,
      "loss": 0.3036,
      "step": 408
    },
    {
      "epoch": 0.10906666666666667,
      "grad_norm": 0.08305513858795166,
      "learning_rate": 0.00017842456608811751,
      "loss": 0.3456,
      "step": 409
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.09060156345367432,
      "learning_rate": 0.00017837116154873167,
      "loss": 0.3612,
      "step": 410
    },
    {
      "epoch": 0.1096,
      "grad_norm": 0.09960044920444489,
      "learning_rate": 0.0001783177570093458,
      "loss": 0.4104,
      "step": 411
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.07005556672811508,
      "learning_rate": 0.00017826435246995996,
      "loss": 0.3464,
      "step": 412
    },
    {
      "epoch": 0.11013333333333333,
      "grad_norm": 0.0678430125117302,
      "learning_rate": 0.0001782109479305741,
      "loss": 0.3195,
      "step": 413
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.09948781132698059,
      "learning_rate": 0.00017815754339118827,
      "loss": 0.3701,
      "step": 414
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 0.08884744346141815,
      "learning_rate": 0.00017810413885180243,
      "loss": 0.3273,
      "step": 415
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.05975028872489929,
      "learning_rate": 0.00017805073431241655,
      "loss": 0.2901,
      "step": 416
    },
    {
      "epoch": 0.1112,
      "grad_norm": 0.05936853215098381,
      "learning_rate": 0.0001779973297730307,
      "loss": 0.3235,
      "step": 417
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.09137233346700668,
      "learning_rate": 0.00017794392523364487,
      "loss": 0.4222,
      "step": 418
    },
    {
      "epoch": 0.11173333333333334,
      "grad_norm": 0.06650623679161072,
      "learning_rate": 0.00017789052069425902,
      "loss": 0.3408,
      "step": 419
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.09001171588897705,
      "learning_rate": 0.00017783711615487318,
      "loss": 0.3075,
      "step": 420
    },
    {
      "epoch": 0.11226666666666667,
      "grad_norm": 0.0930580422282219,
      "learning_rate": 0.00017778371161548734,
      "loss": 0.4254,
      "step": 421
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.0870460718870163,
      "learning_rate": 0.00017773030707610147,
      "loss": 0.4007,
      "step": 422
    },
    {
      "epoch": 0.1128,
      "grad_norm": 0.09903411567211151,
      "learning_rate": 0.00017767690253671562,
      "loss": 0.3432,
      "step": 423
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.08626968413591385,
      "learning_rate": 0.00017762349799732978,
      "loss": 0.3563,
      "step": 424
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.10148075222969055,
      "learning_rate": 0.00017757009345794393,
      "loss": 0.403,
      "step": 425
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.05859281122684479,
      "learning_rate": 0.0001775166889185581,
      "loss": 0.324,
      "step": 426
    },
    {
      "epoch": 0.11386666666666667,
      "grad_norm": 0.07453446835279465,
      "learning_rate": 0.00017746328437917225,
      "loss": 0.3028,
      "step": 427
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.09780862182378769,
      "learning_rate": 0.00017740987983978638,
      "loss": 0.4126,
      "step": 428
    },
    {
      "epoch": 0.1144,
      "grad_norm": 0.06770612299442291,
      "learning_rate": 0.00017735647530040053,
      "loss": 0.3033,
      "step": 429
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.09072615206241608,
      "learning_rate": 0.0001773030707610147,
      "loss": 0.4095,
      "step": 430
    },
    {
      "epoch": 0.11493333333333333,
      "grad_norm": 0.10782740265130997,
      "learning_rate": 0.00017724966622162885,
      "loss": 0.4254,
      "step": 431
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.06843294948339462,
      "learning_rate": 0.000177196261682243,
      "loss": 0.3197,
      "step": 432
    },
    {
      "epoch": 0.11546666666666666,
      "grad_norm": 0.09044941514730453,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3559,
      "step": 433
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.10473693162202835,
      "learning_rate": 0.0001770894526034713,
      "loss": 0.3778,
      "step": 434
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.08448461443185806,
      "learning_rate": 0.00017703604806408544,
      "loss": 0.2679,
      "step": 435
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.07866625487804413,
      "learning_rate": 0.0001769826435246996,
      "loss": 0.3077,
      "step": 436
    },
    {
      "epoch": 0.11653333333333334,
      "grad_norm": 0.12555983662605286,
      "learning_rate": 0.00017692923898531376,
      "loss": 0.3476,
      "step": 437
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.09736067056655884,
      "learning_rate": 0.0001768758344459279,
      "loss": 0.2986,
      "step": 438
    },
    {
      "epoch": 0.11706666666666667,
      "grad_norm": 0.09498608857393265,
      "learning_rate": 0.00017682242990654207,
      "loss": 0.3686,
      "step": 439
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.11742834001779556,
      "learning_rate": 0.00017676902536715623,
      "loss": 0.3455,
      "step": 440
    },
    {
      "epoch": 0.1176,
      "grad_norm": 0.0749160423874855,
      "learning_rate": 0.00017671562082777038,
      "loss": 0.2838,
      "step": 441
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.07864227890968323,
      "learning_rate": 0.00017666221628838454,
      "loss": 0.3965,
      "step": 442
    },
    {
      "epoch": 0.11813333333333334,
      "grad_norm": 0.07354313880205154,
      "learning_rate": 0.0001766088117489987,
      "loss": 0.2898,
      "step": 443
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.09334352612495422,
      "learning_rate": 0.00017655540720961282,
      "loss": 0.3427,
      "step": 444
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.07691550999879837,
      "learning_rate": 0.00017650200267022698,
      "loss": 0.2448,
      "step": 445
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.08415346592664719,
      "learning_rate": 0.00017644859813084114,
      "loss": 0.3322,
      "step": 446
    },
    {
      "epoch": 0.1192,
      "grad_norm": 0.09482723474502563,
      "learning_rate": 0.0001763951935914553,
      "loss": 0.3624,
      "step": 447
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.07187402248382568,
      "learning_rate": 0.00017634178905206945,
      "loss": 0.3142,
      "step": 448
    },
    {
      "epoch": 0.11973333333333333,
      "grad_norm": 0.07815659046173096,
      "learning_rate": 0.00017628838451268358,
      "loss": 0.301,
      "step": 449
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07737161964178085,
      "learning_rate": 0.00017623497997329773,
      "loss": 0.2388,
      "step": 450
    },
    {
      "epoch": 0.12026666666666666,
      "grad_norm": 0.11356117576360703,
      "learning_rate": 0.0001761815754339119,
      "loss": 0.3262,
      "step": 451
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.08623028546571732,
      "learning_rate": 0.00017612817089452605,
      "loss": 0.3254,
      "step": 452
    },
    {
      "epoch": 0.1208,
      "grad_norm": 0.08847487717866898,
      "learning_rate": 0.0001760747663551402,
      "loss": 0.2902,
      "step": 453
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.0677693784236908,
      "learning_rate": 0.00017602136181575436,
      "loss": 0.2832,
      "step": 454
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.0804901048541069,
      "learning_rate": 0.0001759679572763685,
      "loss": 0.3597,
      "step": 455
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.09832211583852768,
      "learning_rate": 0.00017591455273698265,
      "loss": 0.3555,
      "step": 456
    },
    {
      "epoch": 0.12186666666666666,
      "grad_norm": 0.07221118360757828,
      "learning_rate": 0.0001758611481975968,
      "loss": 0.2904,
      "step": 457
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.10371185094118118,
      "learning_rate": 0.00017580774365821096,
      "loss": 0.3235,
      "step": 458
    },
    {
      "epoch": 0.1224,
      "grad_norm": 0.08967738598585129,
      "learning_rate": 0.00017575433911882511,
      "loss": 0.3093,
      "step": 459
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.08456101268529892,
      "learning_rate": 0.00017570093457943927,
      "loss": 0.3619,
      "step": 460
    },
    {
      "epoch": 0.12293333333333334,
      "grad_norm": 0.09551294893026352,
      "learning_rate": 0.0001756475300400534,
      "loss": 0.3136,
      "step": 461
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.07097040116786957,
      "learning_rate": 0.00017559412550066756,
      "loss": 0.2533,
      "step": 462
    },
    {
      "epoch": 0.12346666666666667,
      "grad_norm": 0.0985770896077156,
      "learning_rate": 0.0001755407209612817,
      "loss": 0.3204,
      "step": 463
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.10977625101804733,
      "learning_rate": 0.00017548731642189587,
      "loss": 0.4052,
      "step": 464
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.09588313847780228,
      "learning_rate": 0.00017543391188251003,
      "loss": 0.3732,
      "step": 465
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.09597121179103851,
      "learning_rate": 0.00017538050734312415,
      "loss": 0.3587,
      "step": 466
    },
    {
      "epoch": 0.12453333333333333,
      "grad_norm": 0.06087057292461395,
      "learning_rate": 0.0001753271028037383,
      "loss": 0.2842,
      "step": 467
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.07403556257486343,
      "learning_rate": 0.00017527369826435247,
      "loss": 0.3579,
      "step": 468
    },
    {
      "epoch": 0.12506666666666666,
      "grad_norm": 0.07791051268577576,
      "learning_rate": 0.00017522029372496662,
      "loss": 0.3652,
      "step": 469
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.09844592213630676,
      "learning_rate": 0.00017516688918558078,
      "loss": 0.286,
      "step": 470
    },
    {
      "epoch": 0.1256,
      "grad_norm": 0.10063371807336807,
      "learning_rate": 0.00017511348464619494,
      "loss": 0.3689,
      "step": 471
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.07232058048248291,
      "learning_rate": 0.00017506008010680907,
      "loss": 0.3139,
      "step": 472
    },
    {
      "epoch": 0.12613333333333332,
      "grad_norm": 0.11370675265789032,
      "learning_rate": 0.00017500667556742322,
      "loss": 0.4123,
      "step": 473
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.07439430803060532,
      "learning_rate": 0.0001749532710280374,
      "loss": 0.3456,
      "step": 474
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.07983537763357162,
      "learning_rate": 0.00017489986648865156,
      "loss": 0.3298,
      "step": 475
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.11442352086305618,
      "learning_rate": 0.00017484646194926572,
      "loss": 0.3863,
      "step": 476
    },
    {
      "epoch": 0.1272,
      "grad_norm": 0.08353516459465027,
      "learning_rate": 0.00017479305740987985,
      "loss": 0.3679,
      "step": 477
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.06448070704936981,
      "learning_rate": 0.000174739652870494,
      "loss": 0.2669,
      "step": 478
    },
    {
      "epoch": 0.12773333333333334,
      "grad_norm": 0.09045962989330292,
      "learning_rate": 0.00017468624833110816,
      "loss": 0.3732,
      "step": 479
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.07262478023767471,
      "learning_rate": 0.00017463284379172232,
      "loss": 0.2424,
      "step": 480
    },
    {
      "epoch": 0.12826666666666667,
      "grad_norm": 0.09678027033805847,
      "learning_rate": 0.00017457943925233647,
      "loss": 0.3229,
      "step": 481
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.0964018777012825,
      "learning_rate": 0.0001745260347129506,
      "loss": 0.3212,
      "step": 482
    },
    {
      "epoch": 0.1288,
      "grad_norm": 0.0634017363190651,
      "learning_rate": 0.00017447263017356476,
      "loss": 0.3058,
      "step": 483
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.09900142252445221,
      "learning_rate": 0.00017441922563417891,
      "loss": 0.3803,
      "step": 484
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 0.08725389838218689,
      "learning_rate": 0.00017436582109479307,
      "loss": 0.308,
      "step": 485
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.09229782223701477,
      "learning_rate": 0.00017431241655540723,
      "loss": 0.4091,
      "step": 486
    },
    {
      "epoch": 0.12986666666666666,
      "grad_norm": 0.05471213161945343,
      "learning_rate": 0.00017425901201602138,
      "loss": 0.3344,
      "step": 487
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.08777163177728653,
      "learning_rate": 0.0001742056074766355,
      "loss": 0.3282,
      "step": 488
    },
    {
      "epoch": 0.1304,
      "grad_norm": 0.09983504563570023,
      "learning_rate": 0.00017415220293724967,
      "loss": 0.3957,
      "step": 489
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.0753464549779892,
      "learning_rate": 0.00017409879839786383,
      "loss": 0.3131,
      "step": 490
    },
    {
      "epoch": 0.13093333333333335,
      "grad_norm": 0.09565792977809906,
      "learning_rate": 0.00017404539385847798,
      "loss": 0.3165,
      "step": 491
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.07867994904518127,
      "learning_rate": 0.00017399198931909214,
      "loss": 0.344,
      "step": 492
    },
    {
      "epoch": 0.13146666666666668,
      "grad_norm": 0.07086551934480667,
      "learning_rate": 0.0001739385847797063,
      "loss": 0.359,
      "step": 493
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.11533576250076294,
      "learning_rate": 0.00017388518024032042,
      "loss": 0.3598,
      "step": 494
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.09127259999513626,
      "learning_rate": 0.00017383177570093458,
      "loss": 0.3876,
      "step": 495
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.06626589596271515,
      "learning_rate": 0.00017377837116154874,
      "loss": 0.3113,
      "step": 496
    },
    {
      "epoch": 0.13253333333333334,
      "grad_norm": 0.0856238305568695,
      "learning_rate": 0.0001737249666221629,
      "loss": 0.3327,
      "step": 497
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.11523126810789108,
      "learning_rate": 0.00017367156208277705,
      "loss": 0.3926,
      "step": 498
    },
    {
      "epoch": 0.13306666666666667,
      "grad_norm": 0.08474228531122208,
      "learning_rate": 0.00017361815754339118,
      "loss": 0.3423,
      "step": 499
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.09596377611160278,
      "learning_rate": 0.00017356475300400533,
      "loss": 0.3092,
      "step": 500
    },
    {
      "epoch": 0.1336,
      "grad_norm": 0.07695401459932327,
      "learning_rate": 0.0001735113484646195,
      "loss": 0.331,
      "step": 501
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.08109140396118164,
      "learning_rate": 0.00017345794392523365,
      "loss": 0.2828,
      "step": 502
    },
    {
      "epoch": 0.13413333333333333,
      "grad_norm": 0.08007713407278061,
      "learning_rate": 0.0001734045393858478,
      "loss": 0.3732,
      "step": 503
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.06817664206027985,
      "learning_rate": 0.00017335113484646196,
      "loss": 0.328,
      "step": 504
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 0.09666606783866882,
      "learning_rate": 0.0001732977303070761,
      "loss": 0.429,
      "step": 505
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.06670768558979034,
      "learning_rate": 0.00017324432576769025,
      "loss": 0.3323,
      "step": 506
    },
    {
      "epoch": 0.1352,
      "grad_norm": 0.08304927498102188,
      "learning_rate": 0.0001731909212283044,
      "loss": 0.3636,
      "step": 507
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.062269583344459534,
      "learning_rate": 0.00017313751668891859,
      "loss": 0.315,
      "step": 508
    },
    {
      "epoch": 0.13573333333333334,
      "grad_norm": 0.12643331289291382,
      "learning_rate": 0.00017308411214953274,
      "loss": 0.3733,
      "step": 509
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.07792993634939194,
      "learning_rate": 0.00017303070761014687,
      "loss": 0.3558,
      "step": 510
    },
    {
      "epoch": 0.13626666666666667,
      "grad_norm": 0.06367392838001251,
      "learning_rate": 0.00017297730307076103,
      "loss": 0.3215,
      "step": 511
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.0732932910323143,
      "learning_rate": 0.00017292389853137518,
      "loss": 0.3407,
      "step": 512
    },
    {
      "epoch": 0.1368,
      "grad_norm": 0.08043991029262543,
      "learning_rate": 0.00017287049399198934,
      "loss": 0.3343,
      "step": 513
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.07424119859933853,
      "learning_rate": 0.0001728170894526035,
      "loss": 0.3614,
      "step": 514
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 0.06248525530099869,
      "learning_rate": 0.00017276368491321763,
      "loss": 0.3005,
      "step": 515
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.06615480035543442,
      "learning_rate": 0.00017271028037383178,
      "loss": 0.3226,
      "step": 516
    },
    {
      "epoch": 0.13786666666666667,
      "grad_norm": 0.07016195356845856,
      "learning_rate": 0.00017265687583444594,
      "loss": 0.3384,
      "step": 517
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.07127232104539871,
      "learning_rate": 0.0001726034712950601,
      "loss": 0.3408,
      "step": 518
    },
    {
      "epoch": 0.1384,
      "grad_norm": 0.0807282105088234,
      "learning_rate": 0.00017255006675567425,
      "loss": 0.3206,
      "step": 519
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.07847002148628235,
      "learning_rate": 0.0001724966622162884,
      "loss": 0.2889,
      "step": 520
    },
    {
      "epoch": 0.13893333333333333,
      "grad_norm": 0.07397522777318954,
      "learning_rate": 0.00017244325767690254,
      "loss": 0.3285,
      "step": 521
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.09588655829429626,
      "learning_rate": 0.0001723898531375167,
      "loss": 0.3988,
      "step": 522
    },
    {
      "epoch": 0.13946666666666666,
      "grad_norm": 0.06682709604501724,
      "learning_rate": 0.00017233644859813085,
      "loss": 0.3222,
      "step": 523
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.08041860163211823,
      "learning_rate": 0.000172283044058745,
      "loss": 0.3262,
      "step": 524
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0967840701341629,
      "learning_rate": 0.00017222963951935916,
      "loss": 0.3784,
      "step": 525
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.08178833872079849,
      "learning_rate": 0.00017217623497997332,
      "loss": 0.3307,
      "step": 526
    },
    {
      "epoch": 0.14053333333333334,
      "grad_norm": 0.08116485178470612,
      "learning_rate": 0.00017212283044058745,
      "loss": 0.3414,
      "step": 527
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.08975369483232498,
      "learning_rate": 0.0001720694259012016,
      "loss": 0.3772,
      "step": 528
    },
    {
      "epoch": 0.14106666666666667,
      "grad_norm": 0.06632278859615326,
      "learning_rate": 0.00017201602136181576,
      "loss": 0.3339,
      "step": 529
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.10109684616327286,
      "learning_rate": 0.00017196261682242992,
      "loss": 0.3212,
      "step": 530
    },
    {
      "epoch": 0.1416,
      "grad_norm": 0.07773547619581223,
      "learning_rate": 0.00017190921228304407,
      "loss": 0.2805,
      "step": 531
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.0810297504067421,
      "learning_rate": 0.0001718558077436582,
      "loss": 0.2833,
      "step": 532
    },
    {
      "epoch": 0.14213333333333333,
      "grad_norm": 0.07729069143533707,
      "learning_rate": 0.00017180240320427236,
      "loss": 0.3445,
      "step": 533
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.07123406231403351,
      "learning_rate": 0.00017174899866488651,
      "loss": 0.3252,
      "step": 534
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 0.08452446758747101,
      "learning_rate": 0.00017169559412550067,
      "loss": 0.3126,
      "step": 535
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.10175430774688721,
      "learning_rate": 0.00017164218958611483,
      "loss": 0.394,
      "step": 536
    },
    {
      "epoch": 0.1432,
      "grad_norm": 0.056594230234622955,
      "learning_rate": 0.00017158878504672898,
      "loss": 0.2518,
      "step": 537
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.0765652284026146,
      "learning_rate": 0.0001715353805073431,
      "loss": 0.3954,
      "step": 538
    },
    {
      "epoch": 0.14373333333333332,
      "grad_norm": 0.06413234025239944,
      "learning_rate": 0.00017148197596795727,
      "loss": 0.3477,
      "step": 539
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.059577230364084244,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.2792,
      "step": 540
    },
    {
      "epoch": 0.14426666666666665,
      "grad_norm": 0.06292002648115158,
      "learning_rate": 0.00017137516688918558,
      "loss": 0.2387,
      "step": 541
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.10126267373561859,
      "learning_rate": 0.00017132176234979974,
      "loss": 0.3702,
      "step": 542
    },
    {
      "epoch": 0.1448,
      "grad_norm": 0.0824870690703392,
      "learning_rate": 0.0001712683578104139,
      "loss": 0.351,
      "step": 543
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.0876840204000473,
      "learning_rate": 0.00017121495327102805,
      "loss": 0.3578,
      "step": 544
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 0.08379214257001877,
      "learning_rate": 0.0001711615487316422,
      "loss": 0.337,
      "step": 545
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.08447424322366714,
      "learning_rate": 0.00017110814419225636,
      "loss": 0.3111,
      "step": 546
    },
    {
      "epoch": 0.14586666666666667,
      "grad_norm": 0.08998953551054001,
      "learning_rate": 0.00017105473965287052,
      "loss": 0.346,
      "step": 547
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.10532931238412857,
      "learning_rate": 0.00017100133511348465,
      "loss": 0.3928,
      "step": 548
    },
    {
      "epoch": 0.1464,
      "grad_norm": 0.08020518720149994,
      "learning_rate": 0.0001709479305740988,
      "loss": 0.3271,
      "step": 549
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.06572943925857544,
      "learning_rate": 0.00017089452603471296,
      "loss": 0.2656,
      "step": 550
    },
    {
      "epoch": 0.14693333333333333,
      "grad_norm": 0.08691238611936569,
      "learning_rate": 0.00017084112149532712,
      "loss": 0.3415,
      "step": 551
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.07528454810380936,
      "learning_rate": 0.00017078771695594127,
      "loss": 0.285,
      "step": 552
    },
    {
      "epoch": 0.14746666666666666,
      "grad_norm": 0.1115829274058342,
      "learning_rate": 0.00017073431241655543,
      "loss": 0.3903,
      "step": 553
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.07032116502523422,
      "learning_rate": 0.00017068090787716956,
      "loss": 0.31,
      "step": 554
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.09440396726131439,
      "learning_rate": 0.00017062750333778372,
      "loss": 0.4066,
      "step": 555
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.08527984470129013,
      "learning_rate": 0.00017057409879839787,
      "loss": 0.3705,
      "step": 556
    },
    {
      "epoch": 0.14853333333333332,
      "grad_norm": 0.13117530941963196,
      "learning_rate": 0.00017052069425901203,
      "loss": 0.3801,
      "step": 557
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.0597238764166832,
      "learning_rate": 0.00017046728971962618,
      "loss": 0.2863,
      "step": 558
    },
    {
      "epoch": 0.14906666666666665,
      "grad_norm": 0.09870890527963638,
      "learning_rate": 0.00017041388518024034,
      "loss": 0.3952,
      "step": 559
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.10457999259233475,
      "learning_rate": 0.00017036048064085447,
      "loss": 0.355,
      "step": 560
    },
    {
      "epoch": 0.1496,
      "grad_norm": 0.08640239387750626,
      "learning_rate": 0.00017030707610146863,
      "loss": 0.383,
      "step": 561
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.10141647607088089,
      "learning_rate": 0.00017025367156208278,
      "loss": 0.4111,
      "step": 562
    },
    {
      "epoch": 0.15013333333333334,
      "grad_norm": 0.10913051664829254,
      "learning_rate": 0.00017020026702269694,
      "loss": 0.3283,
      "step": 563
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.09825543314218521,
      "learning_rate": 0.0001701468624833111,
      "loss": 0.2841,
      "step": 564
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.06742497533559799,
      "learning_rate": 0.00017009345794392523,
      "loss": 0.3303,
      "step": 565
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.08524424582719803,
      "learning_rate": 0.00017004005340453938,
      "loss": 0.3554,
      "step": 566
    },
    {
      "epoch": 0.1512,
      "grad_norm": 0.0846102237701416,
      "learning_rate": 0.00016998664886515354,
      "loss": 0.2787,
      "step": 567
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.07462649792432785,
      "learning_rate": 0.0001699332443257677,
      "loss": 0.3244,
      "step": 568
    },
    {
      "epoch": 0.15173333333333333,
      "grad_norm": 0.07796921581029892,
      "learning_rate": 0.00016987983978638185,
      "loss": 0.2829,
      "step": 569
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.08305349200963974,
      "learning_rate": 0.000169826435246996,
      "loss": 0.3275,
      "step": 570
    },
    {
      "epoch": 0.15226666666666666,
      "grad_norm": 0.06874802708625793,
      "learning_rate": 0.00016977303070761014,
      "loss": 0.306,
      "step": 571
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.09661352634429932,
      "learning_rate": 0.0001697196261682243,
      "loss": 0.3778,
      "step": 572
    },
    {
      "epoch": 0.1528,
      "grad_norm": 0.08370783179998398,
      "learning_rate": 0.00016966622162883845,
      "loss": 0.3864,
      "step": 573
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.06460944563150406,
      "learning_rate": 0.0001696128170894526,
      "loss": 0.2962,
      "step": 574
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08705464005470276,
      "learning_rate": 0.00016955941255006676,
      "loss": 0.3198,
      "step": 575
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.07552189379930496,
      "learning_rate": 0.00016950600801068092,
      "loss": 0.3095,
      "step": 576
    },
    {
      "epoch": 0.15386666666666668,
      "grad_norm": 0.06323166191577911,
      "learning_rate": 0.00016945260347129505,
      "loss": 0.3304,
      "step": 577
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.08717628568410873,
      "learning_rate": 0.00016939919893190923,
      "loss": 0.4153,
      "step": 578
    },
    {
      "epoch": 0.1544,
      "grad_norm": 0.07031723856925964,
      "learning_rate": 0.0001693457943925234,
      "loss": 0.3163,
      "step": 579
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.06572666764259338,
      "learning_rate": 0.00016929238985313754,
      "loss": 0.3163,
      "step": 580
    },
    {
      "epoch": 0.15493333333333334,
      "grad_norm": 0.09458070248365402,
      "learning_rate": 0.00016923898531375167,
      "loss": 0.4304,
      "step": 581
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.0618613176047802,
      "learning_rate": 0.00016918558077436583,
      "loss": 0.3297,
      "step": 582
    },
    {
      "epoch": 0.15546666666666667,
      "grad_norm": 0.06649681180715561,
      "learning_rate": 0.00016913217623497998,
      "loss": 0.3226,
      "step": 583
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.09570804983377457,
      "learning_rate": 0.00016907877169559414,
      "loss": 0.3241,
      "step": 584
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.061959415674209595,
      "learning_rate": 0.0001690253671562083,
      "loss": 0.3178,
      "step": 585
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.07238022238016129,
      "learning_rate": 0.00016897196261682245,
      "loss": 0.2787,
      "step": 586
    },
    {
      "epoch": 0.15653333333333333,
      "grad_norm": 0.07731423527002335,
      "learning_rate": 0.00016891855807743658,
      "loss": 0.2924,
      "step": 587
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.08851916342973709,
      "learning_rate": 0.00016886515353805074,
      "loss": 0.3394,
      "step": 588
    },
    {
      "epoch": 0.15706666666666666,
      "grad_norm": 0.10078781098127365,
      "learning_rate": 0.0001688117489986649,
      "loss": 0.4183,
      "step": 589
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.07443995773792267,
      "learning_rate": 0.00016875834445927905,
      "loss": 0.332,
      "step": 590
    },
    {
      "epoch": 0.1576,
      "grad_norm": 0.07760163396596909,
      "learning_rate": 0.0001687049399198932,
      "loss": 0.3314,
      "step": 591
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.07698250561952591,
      "learning_rate": 0.00016865153538050736,
      "loss": 0.3112,
      "step": 592
    },
    {
      "epoch": 0.15813333333333332,
      "grad_norm": 0.06868021935224533,
      "learning_rate": 0.0001685981308411215,
      "loss": 0.3252,
      "step": 593
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.06070118770003319,
      "learning_rate": 0.00016854472630173565,
      "loss": 0.3099,
      "step": 594
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.06211910769343376,
      "learning_rate": 0.0001684913217623498,
      "loss": 0.2718,
      "step": 595
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.07392508536577225,
      "learning_rate": 0.00016843791722296396,
      "loss": 0.3257,
      "step": 596
    },
    {
      "epoch": 0.1592,
      "grad_norm": 0.06800773739814758,
      "learning_rate": 0.00016838451268357812,
      "loss": 0.2681,
      "step": 597
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.09500826150178909,
      "learning_rate": 0.00016833110814419228,
      "loss": 0.376,
      "step": 598
    },
    {
      "epoch": 0.15973333333333334,
      "grad_norm": 0.07222486287355423,
      "learning_rate": 0.0001682777036048064,
      "loss": 0.3405,
      "step": 599
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.07662103325128555,
      "learning_rate": 0.00016822429906542056,
      "loss": 0.3568,
      "step": 600
    },
    {
      "epoch": 0.16026666666666667,
      "grad_norm": 0.07359003275632858,
      "learning_rate": 0.00016817089452603472,
      "loss": 0.2795,
      "step": 601
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.0708722248673439,
      "learning_rate": 0.00016811748998664887,
      "loss": 0.3129,
      "step": 602
    },
    {
      "epoch": 0.1608,
      "grad_norm": 0.06920729577541351,
      "learning_rate": 0.00016806408544726303,
      "loss": 0.3193,
      "step": 603
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.10779676586389542,
      "learning_rate": 0.00016801068090787716,
      "loss": 0.3918,
      "step": 604
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 0.050505027174949646,
      "learning_rate": 0.00016795727636849132,
      "loss": 0.2507,
      "step": 605
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.10936803370714188,
      "learning_rate": 0.00016790387182910547,
      "loss": 0.3918,
      "step": 606
    },
    {
      "epoch": 0.16186666666666666,
      "grad_norm": 0.04830148071050644,
      "learning_rate": 0.00016785046728971963,
      "loss": 0.2601,
      "step": 607
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.07091572880744934,
      "learning_rate": 0.00016779706275033378,
      "loss": 0.314,
      "step": 608
    },
    {
      "epoch": 0.1624,
      "grad_norm": 0.0964217334985733,
      "learning_rate": 0.00016774365821094794,
      "loss": 0.353,
      "step": 609
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.08143746107816696,
      "learning_rate": 0.00016769025367156207,
      "loss": 0.3806,
      "step": 610
    },
    {
      "epoch": 0.16293333333333335,
      "grad_norm": 0.0837395116686821,
      "learning_rate": 0.00016763684913217623,
      "loss": 0.2937,
      "step": 611
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.07170621305704117,
      "learning_rate": 0.0001675834445927904,
      "loss": 0.3469,
      "step": 612
    },
    {
      "epoch": 0.16346666666666668,
      "grad_norm": 0.066038116812706,
      "learning_rate": 0.00016753004005340457,
      "loss": 0.3282,
      "step": 613
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.07573680579662323,
      "learning_rate": 0.00016747663551401872,
      "loss": 0.3004,
      "step": 614
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.12009257078170776,
      "learning_rate": 0.00016742323097463285,
      "loss": 0.4231,
      "step": 615
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.05991929769515991,
      "learning_rate": 0.000167369826435247,
      "loss": 0.3092,
      "step": 616
    },
    {
      "epoch": 0.16453333333333334,
      "grad_norm": 0.07657445967197418,
      "learning_rate": 0.00016731642189586116,
      "loss": 0.3664,
      "step": 617
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.09544384479522705,
      "learning_rate": 0.00016726301735647532,
      "loss": 0.3873,
      "step": 618
    },
    {
      "epoch": 0.16506666666666667,
      "grad_norm": 0.11640424281358719,
      "learning_rate": 0.00016720961281708948,
      "loss": 0.3207,
      "step": 619
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.06673858314752579,
      "learning_rate": 0.0001671562082777036,
      "loss": 0.3274,
      "step": 620
    },
    {
      "epoch": 0.1656,
      "grad_norm": 0.07482188940048218,
      "learning_rate": 0.00016710280373831776,
      "loss": 0.3065,
      "step": 621
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.09819796681404114,
      "learning_rate": 0.00016704939919893192,
      "loss": 0.3332,
      "step": 622
    },
    {
      "epoch": 0.16613333333333333,
      "grad_norm": 0.08695679903030396,
      "learning_rate": 0.00016699599465954608,
      "loss": 0.3392,
      "step": 623
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.08808541297912598,
      "learning_rate": 0.00016694259012016023,
      "loss": 0.3146,
      "step": 624
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.07071485370397568,
      "learning_rate": 0.0001668891855807744,
      "loss": 0.3311,
      "step": 625
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.11760467290878296,
      "learning_rate": 0.00016683578104138852,
      "loss": 0.3139,
      "step": 626
    },
    {
      "epoch": 0.1672,
      "grad_norm": 0.10881310701370239,
      "learning_rate": 0.00016678237650200267,
      "loss": 0.4622,
      "step": 627
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.08329245448112488,
      "learning_rate": 0.00016672897196261683,
      "loss": 0.3914,
      "step": 628
    },
    {
      "epoch": 0.16773333333333335,
      "grad_norm": 0.07250135391950607,
      "learning_rate": 0.000166675567423231,
      "loss": 0.2806,
      "step": 629
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.0699727013707161,
      "learning_rate": 0.00016662216288384514,
      "loss": 0.312,
      "step": 630
    },
    {
      "epoch": 0.16826666666666668,
      "grad_norm": 0.08878324180841446,
      "learning_rate": 0.0001665687583444593,
      "loss": 0.3404,
      "step": 631
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.077143095433712,
      "learning_rate": 0.00016651535380507343,
      "loss": 0.2763,
      "step": 632
    },
    {
      "epoch": 0.1688,
      "grad_norm": 0.07478050887584686,
      "learning_rate": 0.00016646194926568758,
      "loss": 0.3922,
      "step": 633
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.10555914044380188,
      "learning_rate": 0.00016640854472630174,
      "loss": 0.3525,
      "step": 634
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 0.09803512692451477,
      "learning_rate": 0.0001663551401869159,
      "loss": 0.3218,
      "step": 635
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.08671561628580093,
      "learning_rate": 0.00016630173564753005,
      "loss": 0.3342,
      "step": 636
    },
    {
      "epoch": 0.16986666666666667,
      "grad_norm": 0.07843976467847824,
      "learning_rate": 0.00016624833110814418,
      "loss": 0.3281,
      "step": 637
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.09396295249462128,
      "learning_rate": 0.00016619492656875834,
      "loss": 0.3857,
      "step": 638
    },
    {
      "epoch": 0.1704,
      "grad_norm": 0.07006579637527466,
      "learning_rate": 0.0001661415220293725,
      "loss": 0.2957,
      "step": 639
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.0722842887043953,
      "learning_rate": 0.00016608811748998665,
      "loss": 0.3239,
      "step": 640
    },
    {
      "epoch": 0.17093333333333333,
      "grad_norm": 0.06619086861610413,
      "learning_rate": 0.0001660347129506008,
      "loss": 0.3242,
      "step": 641
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.06523226201534271,
      "learning_rate": 0.00016598130841121496,
      "loss": 0.2862,
      "step": 642
    },
    {
      "epoch": 0.17146666666666666,
      "grad_norm": 0.07447078824043274,
      "learning_rate": 0.0001659279038718291,
      "loss": 0.259,
      "step": 643
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.08228430151939392,
      "learning_rate": 0.00016587449933244325,
      "loss": 0.3194,
      "step": 644
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.07260783016681671,
      "learning_rate": 0.0001658210947930574,
      "loss": 0.3776,
      "step": 645
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.08457666635513306,
      "learning_rate": 0.00016576769025367156,
      "loss": 0.343,
      "step": 646
    },
    {
      "epoch": 0.17253333333333334,
      "grad_norm": 0.06862106919288635,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.3245,
      "step": 647
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.07687240839004517,
      "learning_rate": 0.00016566088117489988,
      "loss": 0.2972,
      "step": 648
    },
    {
      "epoch": 0.17306666666666667,
      "grad_norm": 0.07314649969339371,
      "learning_rate": 0.00016560747663551403,
      "loss": 0.3237,
      "step": 649
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.09624132513999939,
      "learning_rate": 0.0001655540720961282,
      "loss": 0.3019,
      "step": 650
    },
    {
      "epoch": 0.1736,
      "grad_norm": 0.07498180121183395,
      "learning_rate": 0.00016550066755674234,
      "loss": 0.3137,
      "step": 651
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.07271599024534225,
      "learning_rate": 0.0001654472630173565,
      "loss": 0.3146,
      "step": 652
    },
    {
      "epoch": 0.17413333333333333,
      "grad_norm": 0.10356056690216064,
      "learning_rate": 0.00016539385847797063,
      "loss": 0.3761,
      "step": 653
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.04879394546151161,
      "learning_rate": 0.00016534045393858479,
      "loss": 0.2372,
      "step": 654
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 0.09073370695114136,
      "learning_rate": 0.00016528704939919894,
      "loss": 0.3271,
      "step": 655
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.0919504165649414,
      "learning_rate": 0.0001652336448598131,
      "loss": 0.3667,
      "step": 656
    },
    {
      "epoch": 0.1752,
      "grad_norm": 0.07698387652635574,
      "learning_rate": 0.00016518024032042726,
      "loss": 0.3952,
      "step": 657
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.06553047895431519,
      "learning_rate": 0.0001651268357810414,
      "loss": 0.3306,
      "step": 658
    },
    {
      "epoch": 0.17573333333333332,
      "grad_norm": 0.06611963361501694,
      "learning_rate": 0.00016507343124165554,
      "loss": 0.3207,
      "step": 659
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.07836012542247772,
      "learning_rate": 0.0001650200267022697,
      "loss": 0.3227,
      "step": 660
    },
    {
      "epoch": 0.17626666666666665,
      "grad_norm": 0.07116267085075378,
      "learning_rate": 0.00016496662216288385,
      "loss": 0.2691,
      "step": 661
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.08854691684246063,
      "learning_rate": 0.000164913217623498,
      "loss": 0.3416,
      "step": 662
    },
    {
      "epoch": 0.1768,
      "grad_norm": 0.11222916096448898,
      "learning_rate": 0.00016485981308411217,
      "loss": 0.3193,
      "step": 663
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.10423111915588379,
      "learning_rate": 0.00016480640854472632,
      "loss": 0.3437,
      "step": 664
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.07314144819974899,
      "learning_rate": 0.00016475300400534045,
      "loss": 0.3444,
      "step": 665
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.07897305488586426,
      "learning_rate": 0.0001646995994659546,
      "loss": 0.3834,
      "step": 666
    },
    {
      "epoch": 0.17786666666666667,
      "grad_norm": 0.0815199539065361,
      "learning_rate": 0.00016464619492656876,
      "loss": 0.2994,
      "step": 667
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.06671357154846191,
      "learning_rate": 0.00016459279038718292,
      "loss": 0.251,
      "step": 668
    },
    {
      "epoch": 0.1784,
      "grad_norm": 0.07867403328418732,
      "learning_rate": 0.00016453938584779708,
      "loss": 0.3033,
      "step": 669
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.07009617984294891,
      "learning_rate": 0.0001644859813084112,
      "loss": 0.3019,
      "step": 670
    },
    {
      "epoch": 0.17893333333333333,
      "grad_norm": 0.10659627616405487,
      "learning_rate": 0.00016443257676902536,
      "loss": 0.3955,
      "step": 671
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.07120306044816971,
      "learning_rate": 0.00016437917222963952,
      "loss": 0.2843,
      "step": 672
    },
    {
      "epoch": 0.17946666666666666,
      "grad_norm": 0.06646528095006943,
      "learning_rate": 0.00016432576769025368,
      "loss": 0.3029,
      "step": 673
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.07365937530994415,
      "learning_rate": 0.00016427236315086783,
      "loss": 0.3372,
      "step": 674
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.06724347174167633,
      "learning_rate": 0.000164218958611482,
      "loss": 0.3319,
      "step": 675
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.07433656603097916,
      "learning_rate": 0.00016416555407209612,
      "loss": 0.3209,
      "step": 676
    },
    {
      "epoch": 0.18053333333333332,
      "grad_norm": 0.05976719409227371,
      "learning_rate": 0.00016411214953271027,
      "loss": 0.2923,
      "step": 677
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.05114075168967247,
      "learning_rate": 0.00016405874499332443,
      "loss": 0.2824,
      "step": 678
    },
    {
      "epoch": 0.18106666666666665,
      "grad_norm": 0.07574819773435593,
      "learning_rate": 0.00016400534045393859,
      "loss": 0.3767,
      "step": 679
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.07814066112041473,
      "learning_rate": 0.00016395193591455274,
      "loss": 0.3709,
      "step": 680
    },
    {
      "epoch": 0.1816,
      "grad_norm": 0.08543433248996735,
      "learning_rate": 0.0001638985313751669,
      "loss": 0.2663,
      "step": 681
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.07652167230844498,
      "learning_rate": 0.00016384512683578106,
      "loss": 0.3082,
      "step": 682
    },
    {
      "epoch": 0.18213333333333334,
      "grad_norm": 0.09232743829488754,
      "learning_rate": 0.0001637917222963952,
      "loss": 0.3608,
      "step": 683
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.08668959885835648,
      "learning_rate": 0.00016373831775700937,
      "loss": 0.3382,
      "step": 684
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.11449681222438812,
      "learning_rate": 0.00016368491321762352,
      "loss": 0.3719,
      "step": 685
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.07575265318155289,
      "learning_rate": 0.00016363150867823765,
      "loss": 0.2745,
      "step": 686
    },
    {
      "epoch": 0.1832,
      "grad_norm": 0.08537815511226654,
      "learning_rate": 0.0001635781041388518,
      "loss": 0.3643,
      "step": 687
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.10408803820610046,
      "learning_rate": 0.00016352469959946597,
      "loss": 0.3103,
      "step": 688
    },
    {
      "epoch": 0.18373333333333333,
      "grad_norm": 0.09466411173343658,
      "learning_rate": 0.00016347129506008012,
      "loss": 0.3289,
      "step": 689
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.0835084542632103,
      "learning_rate": 0.00016341789052069428,
      "loss": 0.3627,
      "step": 690
    },
    {
      "epoch": 0.18426666666666666,
      "grad_norm": 0.05751155689358711,
      "learning_rate": 0.00016336448598130844,
      "loss": 0.2182,
      "step": 691
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.08468402177095413,
      "learning_rate": 0.00016331108144192256,
      "loss": 0.3064,
      "step": 692
    },
    {
      "epoch": 0.1848,
      "grad_norm": 0.09640204906463623,
      "learning_rate": 0.00016325767690253672,
      "loss": 0.3217,
      "step": 693
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.07266205549240112,
      "learning_rate": 0.00016320427236315088,
      "loss": 0.3459,
      "step": 694
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.05575372278690338,
      "learning_rate": 0.00016315086782376503,
      "loss": 0.2766,
      "step": 695
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.07315599173307419,
      "learning_rate": 0.0001630974632843792,
      "loss": 0.3215,
      "step": 696
    },
    {
      "epoch": 0.18586666666666668,
      "grad_norm": 0.09269905835390091,
      "learning_rate": 0.00016304405874499335,
      "loss": 0.3603,
      "step": 697
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.08015652000904083,
      "learning_rate": 0.00016299065420560748,
      "loss": 0.3086,
      "step": 698
    },
    {
      "epoch": 0.1864,
      "grad_norm": 0.06994649767875671,
      "learning_rate": 0.00016293724966622163,
      "loss": 0.3064,
      "step": 699
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.08362378180027008,
      "learning_rate": 0.0001628838451268358,
      "loss": 0.3899,
      "step": 700
    },
    {
      "epoch": 0.18693333333333334,
      "grad_norm": 0.06522245705127716,
      "learning_rate": 0.00016283044058744994,
      "loss": 0.2753,
      "step": 701
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.06679972261190414,
      "learning_rate": 0.0001627770360480641,
      "loss": 0.3822,
      "step": 702
    },
    {
      "epoch": 0.18746666666666667,
      "grad_norm": 0.09941327571868896,
      "learning_rate": 0.00016272363150867823,
      "loss": 0.4375,
      "step": 703
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.14570558071136475,
      "learning_rate": 0.00016267022696929239,
      "loss": 0.4024,
      "step": 704
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.0938330814242363,
      "learning_rate": 0.00016261682242990654,
      "loss": 0.3421,
      "step": 705
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.06589429080486298,
      "learning_rate": 0.0001625634178905207,
      "loss": 0.3165,
      "step": 706
    },
    {
      "epoch": 0.18853333333333333,
      "grad_norm": 0.07429680973291397,
      "learning_rate": 0.00016251001335113486,
      "loss": 0.3259,
      "step": 707
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.08019206672906876,
      "learning_rate": 0.000162456608811749,
      "loss": 0.3358,
      "step": 708
    },
    {
      "epoch": 0.18906666666666666,
      "grad_norm": 0.08518360555171967,
      "learning_rate": 0.00016240320427236314,
      "loss": 0.2746,
      "step": 709
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.08445699512958527,
      "learning_rate": 0.0001623497997329773,
      "loss": 0.328,
      "step": 710
    },
    {
      "epoch": 0.1896,
      "grad_norm": 0.10472280532121658,
      "learning_rate": 0.00016229639519359145,
      "loss": 0.3136,
      "step": 711
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.08063805103302002,
      "learning_rate": 0.0001622429906542056,
      "loss": 0.4023,
      "step": 712
    },
    {
      "epoch": 0.19013333333333332,
      "grad_norm": 0.06398416310548782,
      "learning_rate": 0.00016218958611481977,
      "loss": 0.3073,
      "step": 713
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.09996719658374786,
      "learning_rate": 0.00016213618157543392,
      "loss": 0.4005,
      "step": 714
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.0700005441904068,
      "learning_rate": 0.00016208277703604805,
      "loss": 0.3411,
      "step": 715
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.0746229887008667,
      "learning_rate": 0.00016202937249666224,
      "loss": 0.3139,
      "step": 716
    },
    {
      "epoch": 0.1912,
      "grad_norm": 0.10606686770915985,
      "learning_rate": 0.0001619759679572764,
      "loss": 0.2577,
      "step": 717
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.14957059919834137,
      "learning_rate": 0.00016192256341789055,
      "loss": 0.4132,
      "step": 718
    },
    {
      "epoch": 0.19173333333333334,
      "grad_norm": 0.08337666839361191,
      "learning_rate": 0.00016186915887850468,
      "loss": 0.3315,
      "step": 719
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.08944013714790344,
      "learning_rate": 0.00016181575433911883,
      "loss": 0.3453,
      "step": 720
    },
    {
      "epoch": 0.19226666666666667,
      "grad_norm": 0.07708635926246643,
      "learning_rate": 0.000161762349799733,
      "loss": 0.3118,
      "step": 721
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.11284539848566055,
      "learning_rate": 0.00016170894526034715,
      "loss": 0.3704,
      "step": 722
    },
    {
      "epoch": 0.1928,
      "grad_norm": 0.06226154416799545,
      "learning_rate": 0.0001616555407209613,
      "loss": 0.3721,
      "step": 723
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.06741467863321304,
      "learning_rate": 0.00016160213618157546,
      "loss": 0.3043,
      "step": 724
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.06305187195539474,
      "learning_rate": 0.0001615487316421896,
      "loss": 0.332,
      "step": 725
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.09364154934883118,
      "learning_rate": 0.00016149532710280374,
      "loss": 0.3724,
      "step": 726
    },
    {
      "epoch": 0.19386666666666666,
      "grad_norm": 0.09584496170282364,
      "learning_rate": 0.0001614419225634179,
      "loss": 0.3455,
      "step": 727
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.0711049810051918,
      "learning_rate": 0.00016138851802403206,
      "loss": 0.3425,
      "step": 728
    },
    {
      "epoch": 0.1944,
      "grad_norm": 0.07556731253862381,
      "learning_rate": 0.0001613351134846462,
      "loss": 0.3333,
      "step": 729
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.06450247764587402,
      "learning_rate": 0.00016128170894526037,
      "loss": 0.3771,
      "step": 730
    },
    {
      "epoch": 0.19493333333333332,
      "grad_norm": 0.07582598179578781,
      "learning_rate": 0.0001612283044058745,
      "loss": 0.3481,
      "step": 731
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.06663677841424942,
      "learning_rate": 0.00016117489986648866,
      "loss": 0.3302,
      "step": 732
    },
    {
      "epoch": 0.19546666666666668,
      "grad_norm": 0.06724204868078232,
      "learning_rate": 0.0001611214953271028,
      "loss": 0.3124,
      "step": 733
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.06450898945331573,
      "learning_rate": 0.00016106809078771697,
      "loss": 0.29,
      "step": 734
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.07638248801231384,
      "learning_rate": 0.00016101468624833112,
      "loss": 0.3342,
      "step": 735
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.07160694152116776,
      "learning_rate": 0.00016096128170894525,
      "loss": 0.3237,
      "step": 736
    },
    {
      "epoch": 0.19653333333333334,
      "grad_norm": 0.08766034990549088,
      "learning_rate": 0.0001609078771695594,
      "loss": 0.3818,
      "step": 737
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.0664760172367096,
      "learning_rate": 0.00016085447263017357,
      "loss": 0.3393,
      "step": 738
    },
    {
      "epoch": 0.19706666666666667,
      "grad_norm": 0.0729757696390152,
      "learning_rate": 0.00016080106809078772,
      "loss": 0.4008,
      "step": 739
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.08481191843748093,
      "learning_rate": 0.00016074766355140188,
      "loss": 0.3219,
      "step": 740
    },
    {
      "epoch": 0.1976,
      "grad_norm": 0.08696860820055008,
      "learning_rate": 0.00016069425901201603,
      "loss": 0.416,
      "step": 741
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.08007068932056427,
      "learning_rate": 0.00016064085447263016,
      "loss": 0.3064,
      "step": 742
    },
    {
      "epoch": 0.19813333333333333,
      "grad_norm": 0.08007191121578217,
      "learning_rate": 0.00016058744993324432,
      "loss": 0.3303,
      "step": 743
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.06677020341157913,
      "learning_rate": 0.00016053404539385848,
      "loss": 0.3428,
      "step": 744
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.07025568187236786,
      "learning_rate": 0.00016048064085447263,
      "loss": 0.2885,
      "step": 745
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.09660477936267853,
      "learning_rate": 0.0001604272363150868,
      "loss": 0.3197,
      "step": 746
    },
    {
      "epoch": 0.1992,
      "grad_norm": 0.05273813381791115,
      "learning_rate": 0.00016037383177570095,
      "loss": 0.2738,
      "step": 747
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.05864313989877701,
      "learning_rate": 0.00016032042723631508,
      "loss": 0.3047,
      "step": 748
    },
    {
      "epoch": 0.19973333333333335,
      "grad_norm": 0.08619988709688187,
      "learning_rate": 0.00016026702269692923,
      "loss": 0.3329,
      "step": 749
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.07488847523927689,
      "learning_rate": 0.0001602136181575434,
      "loss": 0.3134,
      "step": 750
    },
    {
      "epoch": 0.20026666666666668,
      "grad_norm": 0.055937156081199646,
      "learning_rate": 0.00016016021361815757,
      "loss": 0.2661,
      "step": 751
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.06706548482179642,
      "learning_rate": 0.0001601068090787717,
      "loss": 0.3531,
      "step": 752
    },
    {
      "epoch": 0.2008,
      "grad_norm": 0.08174266666173935,
      "learning_rate": 0.00016005340453938586,
      "loss": 0.3094,
      "step": 753
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.06839088350534439,
      "learning_rate": 0.00016,
      "loss": 0.3058,
      "step": 754
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.06848971545696259,
      "learning_rate": 0.00015994659546061417,
      "loss": 0.3483,
      "step": 755
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.06294761598110199,
      "learning_rate": 0.00015989319092122833,
      "loss": 0.3424,
      "step": 756
    },
    {
      "epoch": 0.20186666666666667,
      "grad_norm": 0.05712522566318512,
      "learning_rate": 0.00015983978638184248,
      "loss": 0.3276,
      "step": 757
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.09484566748142242,
      "learning_rate": 0.0001597863818424566,
      "loss": 0.3778,
      "step": 758
    },
    {
      "epoch": 0.2024,
      "grad_norm": 0.07455500960350037,
      "learning_rate": 0.00015973297730307077,
      "loss": 0.3373,
      "step": 759
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.0769118145108223,
      "learning_rate": 0.00015967957276368492,
      "loss": 0.3218,
      "step": 760
    },
    {
      "epoch": 0.20293333333333333,
      "grad_norm": 0.06551986187696457,
      "learning_rate": 0.00015962616822429908,
      "loss": 0.3404,
      "step": 761
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.0781547874212265,
      "learning_rate": 0.00015957276368491324,
      "loss": 0.3426,
      "step": 762
    },
    {
      "epoch": 0.20346666666666666,
      "grad_norm": 0.078884556889534,
      "learning_rate": 0.0001595193591455274,
      "loss": 0.341,
      "step": 763
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.08706927299499512,
      "learning_rate": 0.00015946595460614152,
      "loss": 0.3225,
      "step": 764
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.08581821620464325,
      "learning_rate": 0.00015941255006675568,
      "loss": 0.3365,
      "step": 765
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.0833544060587883,
      "learning_rate": 0.00015935914552736983,
      "loss": 0.3282,
      "step": 766
    },
    {
      "epoch": 0.20453333333333334,
      "grad_norm": 0.09168609231710434,
      "learning_rate": 0.000159305740987984,
      "loss": 0.3185,
      "step": 767
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.0842631384730339,
      "learning_rate": 0.00015925233644859815,
      "loss": 0.3366,
      "step": 768
    },
    {
      "epoch": 0.20506666666666667,
      "grad_norm": 0.05289807170629501,
      "learning_rate": 0.00015919893190921228,
      "loss": 0.2495,
      "step": 769
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.08438991010189056,
      "learning_rate": 0.00015914552736982643,
      "loss": 0.3817,
      "step": 770
    },
    {
      "epoch": 0.2056,
      "grad_norm": 0.0842108279466629,
      "learning_rate": 0.0001590921228304406,
      "loss": 0.3659,
      "step": 771
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.0700967013835907,
      "learning_rate": 0.00015903871829105475,
      "loss": 0.284,
      "step": 772
    },
    {
      "epoch": 0.20613333333333334,
      "grad_norm": 0.07434815913438797,
      "learning_rate": 0.0001589853137516689,
      "loss": 0.3143,
      "step": 773
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.06587336212396622,
      "learning_rate": 0.00015893190921228306,
      "loss": 0.314,
      "step": 774
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06339559704065323,
      "learning_rate": 0.0001588785046728972,
      "loss": 0.3072,
      "step": 775
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.07543035596609116,
      "learning_rate": 0.00015882510013351134,
      "loss": 0.3364,
      "step": 776
    },
    {
      "epoch": 0.2072,
      "grad_norm": 0.07452290505170822,
      "learning_rate": 0.0001587716955941255,
      "loss": 0.366,
      "step": 777
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.07148873060941696,
      "learning_rate": 0.00015871829105473966,
      "loss": 0.4137,
      "step": 778
    },
    {
      "epoch": 0.20773333333333333,
      "grad_norm": 0.05358431115746498,
      "learning_rate": 0.0001586648865153538,
      "loss": 0.3042,
      "step": 779
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.07279808074235916,
      "learning_rate": 0.00015861148197596797,
      "loss": 0.3069,
      "step": 780
    },
    {
      "epoch": 0.20826666666666666,
      "grad_norm": 0.054188016802072525,
      "learning_rate": 0.0001585580774365821,
      "loss": 0.2958,
      "step": 781
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.06960541754961014,
      "learning_rate": 0.00015850467289719625,
      "loss": 0.358,
      "step": 782
    },
    {
      "epoch": 0.2088,
      "grad_norm": 0.07657449692487717,
      "learning_rate": 0.0001584512683578104,
      "loss": 0.4188,
      "step": 783
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.0881974920630455,
      "learning_rate": 0.00015839786381842457,
      "loss": 0.3558,
      "step": 784
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.049927160143852234,
      "learning_rate": 0.00015834445927903872,
      "loss": 0.2312,
      "step": 785
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.0589725561439991,
      "learning_rate": 0.00015829105473965288,
      "loss": 0.2868,
      "step": 786
    },
    {
      "epoch": 0.20986666666666667,
      "grad_norm": 0.05656092241406441,
      "learning_rate": 0.00015823765020026704,
      "loss": 0.282,
      "step": 787
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.06087689474225044,
      "learning_rate": 0.0001581842456608812,
      "loss": 0.2498,
      "step": 788
    },
    {
      "epoch": 0.2104,
      "grad_norm": 0.07498946785926819,
      "learning_rate": 0.00015813084112149535,
      "loss": 0.3503,
      "step": 789
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.05419553443789482,
      "learning_rate": 0.0001580774365821095,
      "loss": 0.2686,
      "step": 790
    },
    {
      "epoch": 0.21093333333333333,
      "grad_norm": 0.052204228937625885,
      "learning_rate": 0.00015802403204272363,
      "loss": 0.2486,
      "step": 791
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.05942447483539581,
      "learning_rate": 0.0001579706275033378,
      "loss": 0.2777,
      "step": 792
    },
    {
      "epoch": 0.21146666666666666,
      "grad_norm": 0.06910344213247299,
      "learning_rate": 0.00015791722296395195,
      "loss": 0.3024,
      "step": 793
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.05480262637138367,
      "learning_rate": 0.0001578638184245661,
      "loss": 0.2791,
      "step": 794
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.07652819156646729,
      "learning_rate": 0.00015781041388518026,
      "loss": 0.3381,
      "step": 795
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.07476698607206345,
      "learning_rate": 0.00015775700934579442,
      "loss": 0.3105,
      "step": 796
    },
    {
      "epoch": 0.21253333333333332,
      "grad_norm": 0.05740299075841904,
      "learning_rate": 0.00015770360480640855,
      "loss": 0.2876,
      "step": 797
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.06636771559715271,
      "learning_rate": 0.0001576502002670227,
      "loss": 0.3408,
      "step": 798
    },
    {
      "epoch": 0.21306666666666665,
      "grad_norm": 0.07113157212734222,
      "learning_rate": 0.00015759679572763686,
      "loss": 0.3951,
      "step": 799
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06985699385404587,
      "learning_rate": 0.00015754339118825101,
      "loss": 0.3265,
      "step": 800
    },
    {
      "epoch": 0.2136,
      "grad_norm": 0.0814833864569664,
      "learning_rate": 0.00015748998664886517,
      "loss": 0.3337,
      "step": 801
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.11291235685348511,
      "learning_rate": 0.0001574365821094793,
      "loss": 0.3694,
      "step": 802
    },
    {
      "epoch": 0.21413333333333334,
      "grad_norm": 0.056812796741724014,
      "learning_rate": 0.00015738317757009346,
      "loss": 0.3378,
      "step": 803
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.06068000942468643,
      "learning_rate": 0.0001573297730307076,
      "loss": 0.2854,
      "step": 804
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 0.08603786677122116,
      "learning_rate": 0.00015727636849132177,
      "loss": 0.3509,
      "step": 805
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.08548382669687271,
      "learning_rate": 0.00015722296395193593,
      "loss": 0.3353,
      "step": 806
    },
    {
      "epoch": 0.2152,
      "grad_norm": 0.07118932902812958,
      "learning_rate": 0.00015716955941255008,
      "loss": 0.3717,
      "step": 807
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.07692452520132065,
      "learning_rate": 0.0001571161548731642,
      "loss": 0.3001,
      "step": 808
    },
    {
      "epoch": 0.21573333333333333,
      "grad_norm": 0.07749362289905548,
      "learning_rate": 0.00015706275033377837,
      "loss": 0.3554,
      "step": 809
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.06963516026735306,
      "learning_rate": 0.00015700934579439252,
      "loss": 0.2823,
      "step": 810
    },
    {
      "epoch": 0.21626666666666666,
      "grad_norm": 0.0662803202867508,
      "learning_rate": 0.00015695594125500668,
      "loss": 0.3094,
      "step": 811
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.06168455630540848,
      "learning_rate": 0.00015690253671562084,
      "loss": 0.265,
      "step": 812
    },
    {
      "epoch": 0.2168,
      "grad_norm": 0.061856385320425034,
      "learning_rate": 0.000156849132176235,
      "loss": 0.2755,
      "step": 813
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.06045820564031601,
      "learning_rate": 0.00015679572763684912,
      "loss": 0.3103,
      "step": 814
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.07206481695175171,
      "learning_rate": 0.00015674232309746328,
      "loss": 0.2875,
      "step": 815
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.0738525390625,
      "learning_rate": 0.00015668891855807743,
      "loss": 0.3264,
      "step": 816
    },
    {
      "epoch": 0.21786666666666665,
      "grad_norm": 0.10573196411132812,
      "learning_rate": 0.0001566355140186916,
      "loss": 0.3786,
      "step": 817
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.06295195966959,
      "learning_rate": 0.00015658210947930575,
      "loss": 0.3243,
      "step": 818
    },
    {
      "epoch": 0.2184,
      "grad_norm": 0.09895118325948715,
      "learning_rate": 0.0001565287049399199,
      "loss": 0.3409,
      "step": 819
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.11919766664505005,
      "learning_rate": 0.00015647530040053406,
      "loss": 0.3594,
      "step": 820
    },
    {
      "epoch": 0.21893333333333334,
      "grad_norm": 0.06122515723109245,
      "learning_rate": 0.00015642189586114822,
      "loss": 0.3426,
      "step": 821
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.0531817264854908,
      "learning_rate": 0.00015636849132176237,
      "loss": 0.2498,
      "step": 822
    },
    {
      "epoch": 0.21946666666666667,
      "grad_norm": 0.0796823725104332,
      "learning_rate": 0.00015631508678237653,
      "loss": 0.2974,
      "step": 823
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.11580086499452591,
      "learning_rate": 0.00015626168224299066,
      "loss": 0.3827,
      "step": 824
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.06227444112300873,
      "learning_rate": 0.00015620827770360481,
      "loss": 0.3208,
      "step": 825
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.06257370859384537,
      "learning_rate": 0.00015615487316421897,
      "loss": 0.3019,
      "step": 826
    },
    {
      "epoch": 0.22053333333333333,
      "grad_norm": 0.07016675919294357,
      "learning_rate": 0.00015610146862483313,
      "loss": 0.408,
      "step": 827
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.06111014634370804,
      "learning_rate": 0.00015604806408544728,
      "loss": 0.2605,
      "step": 828
    },
    {
      "epoch": 0.22106666666666666,
      "grad_norm": 0.07499274611473083,
      "learning_rate": 0.00015599465954606144,
      "loss": 0.3143,
      "step": 829
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.05697950720787048,
      "learning_rate": 0.00015594125500667557,
      "loss": 0.3173,
      "step": 830
    },
    {
      "epoch": 0.2216,
      "grad_norm": 0.07473555207252502,
      "learning_rate": 0.00015588785046728973,
      "loss": 0.3685,
      "step": 831
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.07275936007499695,
      "learning_rate": 0.00015583444592790388,
      "loss": 0.3163,
      "step": 832
    },
    {
      "epoch": 0.22213333333333332,
      "grad_norm": 0.06330128014087677,
      "learning_rate": 0.00015578104138851804,
      "loss": 0.2791,
      "step": 833
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.0933128073811531,
      "learning_rate": 0.0001557276368491322,
      "loss": 0.4138,
      "step": 834
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.06133772060275078,
      "learning_rate": 0.00015567423230974635,
      "loss": 0.3219,
      "step": 835
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.07876644283533096,
      "learning_rate": 0.00015562082777036048,
      "loss": 0.3845,
      "step": 836
    },
    {
      "epoch": 0.2232,
      "grad_norm": 0.08594964444637299,
      "learning_rate": 0.00015556742323097464,
      "loss": 0.3764,
      "step": 837
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.07663939893245697,
      "learning_rate": 0.0001555140186915888,
      "loss": 0.3362,
      "step": 838
    },
    {
      "epoch": 0.22373333333333334,
      "grad_norm": 0.05112661048769951,
      "learning_rate": 0.00015546061415220295,
      "loss": 0.2501,
      "step": 839
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.07590912282466888,
      "learning_rate": 0.0001554072096128171,
      "loss": 0.3418,
      "step": 840
    },
    {
      "epoch": 0.22426666666666667,
      "grad_norm": 0.07189663499593735,
      "learning_rate": 0.00015535380507343123,
      "loss": 0.3904,
      "step": 841
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.10325278341770172,
      "learning_rate": 0.0001553004005340454,
      "loss": 0.3822,
      "step": 842
    },
    {
      "epoch": 0.2248,
      "grad_norm": 0.06231984868645668,
      "learning_rate": 0.00015524699599465955,
      "loss": 0.2906,
      "step": 843
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.0664716511964798,
      "learning_rate": 0.0001551935914552737,
      "loss": 0.3624,
      "step": 844
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 0.09245821833610535,
      "learning_rate": 0.00015514018691588786,
      "loss": 0.34,
      "step": 845
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.07317652553319931,
      "learning_rate": 0.00015508678237650202,
      "loss": 0.3515,
      "step": 846
    },
    {
      "epoch": 0.22586666666666666,
      "grad_norm": 0.06941148638725281,
      "learning_rate": 0.00015503337783711615,
      "loss": 0.3457,
      "step": 847
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.06818634271621704,
      "learning_rate": 0.0001549799732977303,
      "loss": 0.339,
      "step": 848
    },
    {
      "epoch": 0.2264,
      "grad_norm": 0.060265205800533295,
      "learning_rate": 0.00015492656875834446,
      "loss": 0.2537,
      "step": 849
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.07589879631996155,
      "learning_rate": 0.00015487316421895861,
      "loss": 0.3973,
      "step": 850
    },
    {
      "epoch": 0.22693333333333332,
      "grad_norm": 0.06422147154808044,
      "learning_rate": 0.00015481975967957277,
      "loss": 0.354,
      "step": 851
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.07352346181869507,
      "learning_rate": 0.00015476635514018693,
      "loss": 0.3664,
      "step": 852
    },
    {
      "epoch": 0.22746666666666668,
      "grad_norm": 0.07632400840520859,
      "learning_rate": 0.00015471295060080106,
      "loss": 0.3814,
      "step": 853
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.06408054381608963,
      "learning_rate": 0.0001546595460614152,
      "loss": 0.3203,
      "step": 854
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.05619586259126663,
      "learning_rate": 0.0001546061415220294,
      "loss": 0.3025,
      "step": 855
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.06693291664123535,
      "learning_rate": 0.00015455273698264355,
      "loss": 0.3047,
      "step": 856
    },
    {
      "epoch": 0.22853333333333334,
      "grad_norm": 0.07476229220628738,
      "learning_rate": 0.00015449933244325768,
      "loss": 0.2913,
      "step": 857
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.07042822241783142,
      "learning_rate": 0.00015444592790387184,
      "loss": 0.3277,
      "step": 858
    },
    {
      "epoch": 0.22906666666666667,
      "grad_norm": 0.06761964410543442,
      "learning_rate": 0.000154392523364486,
      "loss": 0.3232,
      "step": 859
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.0678616315126419,
      "learning_rate": 0.00015433911882510015,
      "loss": 0.3383,
      "step": 860
    },
    {
      "epoch": 0.2296,
      "grad_norm": 0.0785217434167862,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3954,
      "step": 861
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.0659957230091095,
      "learning_rate": 0.00015423230974632846,
      "loss": 0.3344,
      "step": 862
    },
    {
      "epoch": 0.23013333333333333,
      "grad_norm": 0.0732218325138092,
      "learning_rate": 0.0001541789052069426,
      "loss": 0.3197,
      "step": 863
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.06440157443284988,
      "learning_rate": 0.00015412550066755675,
      "loss": 0.328,
      "step": 864
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 0.05480730161070824,
      "learning_rate": 0.0001540720961281709,
      "loss": 0.2465,
      "step": 865
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.06445811688899994,
      "learning_rate": 0.00015401869158878506,
      "loss": 0.3164,
      "step": 866
    },
    {
      "epoch": 0.2312,
      "grad_norm": 0.06648431718349457,
      "learning_rate": 0.00015396528704939922,
      "loss": 0.328,
      "step": 867
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.06969618797302246,
      "learning_rate": 0.00015391188251001337,
      "loss": 0.351,
      "step": 868
    },
    {
      "epoch": 0.23173333333333335,
      "grad_norm": 0.09392930567264557,
      "learning_rate": 0.0001538584779706275,
      "loss": 0.3845,
      "step": 869
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.061019234359264374,
      "learning_rate": 0.00015380507343124166,
      "loss": 0.3318,
      "step": 870
    },
    {
      "epoch": 0.23226666666666668,
      "grad_norm": 0.07892068475484848,
      "learning_rate": 0.00015375166889185582,
      "loss": 0.3209,
      "step": 871
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.06410123407840729,
      "learning_rate": 0.00015369826435246997,
      "loss": 0.3733,
      "step": 872
    },
    {
      "epoch": 0.2328,
      "grad_norm": 0.06223737820982933,
      "learning_rate": 0.00015364485981308413,
      "loss": 0.394,
      "step": 873
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.07604173570871353,
      "learning_rate": 0.00015359145527369826,
      "loss": 0.3677,
      "step": 874
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.054797496646642685,
      "learning_rate": 0.00015353805073431241,
      "loss": 0.2739,
      "step": 875
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.05743510648608208,
      "learning_rate": 0.00015348464619492657,
      "loss": 0.3084,
      "step": 876
    },
    {
      "epoch": 0.23386666666666667,
      "grad_norm": 0.07868494093418121,
      "learning_rate": 0.00015343124165554073,
      "loss": 0.419,
      "step": 877
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.06703056395053864,
      "learning_rate": 0.00015337783711615488,
      "loss": 0.3167,
      "step": 878
    },
    {
      "epoch": 0.2344,
      "grad_norm": 0.06630902737379074,
      "learning_rate": 0.00015332443257676904,
      "loss": 0.2916,
      "step": 879
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.06530257314443588,
      "learning_rate": 0.00015327102803738317,
      "loss": 0.3055,
      "step": 880
    },
    {
      "epoch": 0.23493333333333333,
      "grad_norm": 0.07406895607709885,
      "learning_rate": 0.00015321762349799733,
      "loss": 0.3131,
      "step": 881
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.07141461223363876,
      "learning_rate": 0.00015316421895861148,
      "loss": 0.3706,
      "step": 882
    },
    {
      "epoch": 0.23546666666666666,
      "grad_norm": 0.055683303624391556,
      "learning_rate": 0.00015311081441922564,
      "loss": 0.2929,
      "step": 883
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.07901982218027115,
      "learning_rate": 0.0001530574098798398,
      "loss": 0.4307,
      "step": 884
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.06796899437904358,
      "learning_rate": 0.00015300400534045395,
      "loss": 0.3231,
      "step": 885
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.07519524544477463,
      "learning_rate": 0.00015295060080106808,
      "loss": 0.2679,
      "step": 886
    },
    {
      "epoch": 0.23653333333333335,
      "grad_norm": 0.0688776895403862,
      "learning_rate": 0.00015289719626168224,
      "loss": 0.3033,
      "step": 887
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.05358230695128441,
      "learning_rate": 0.0001528437917222964,
      "loss": 0.3378,
      "step": 888
    },
    {
      "epoch": 0.23706666666666668,
      "grad_norm": 0.06412819772958755,
      "learning_rate": 0.00015279038718291055,
      "loss": 0.2806,
      "step": 889
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.07089069485664368,
      "learning_rate": 0.0001527369826435247,
      "loss": 0.3999,
      "step": 890
    },
    {
      "epoch": 0.2376,
      "grad_norm": 0.06613447517156601,
      "learning_rate": 0.00015268357810413886,
      "loss": 0.3729,
      "step": 891
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.05468335002660751,
      "learning_rate": 0.00015263017356475302,
      "loss": 0.2719,
      "step": 892
    },
    {
      "epoch": 0.23813333333333334,
      "grad_norm": 0.06327205896377563,
      "learning_rate": 0.00015257676902536717,
      "loss": 0.2684,
      "step": 893
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.10525119304656982,
      "learning_rate": 0.00015252336448598133,
      "loss": 0.4008,
      "step": 894
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 0.08927681297063828,
      "learning_rate": 0.0001524699599465955,
      "loss": 0.4175,
      "step": 895
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.06974387913942337,
      "learning_rate": 0.00015241655540720962,
      "loss": 0.3376,
      "step": 896
    },
    {
      "epoch": 0.2392,
      "grad_norm": 0.07371831685304642,
      "learning_rate": 0.00015236315086782377,
      "loss": 0.3333,
      "step": 897
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.07948505133390427,
      "learning_rate": 0.00015230974632843793,
      "loss": 0.3816,
      "step": 898
    },
    {
      "epoch": 0.23973333333333333,
      "grad_norm": 0.05994774028658867,
      "learning_rate": 0.00015225634178905209,
      "loss": 0.3527,
      "step": 899
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.07789943367242813,
      "learning_rate": 0.00015220293724966624,
      "loss": 0.3309,
      "step": 900
    },
    {
      "epoch": 0.24026666666666666,
      "grad_norm": 0.10727785527706146,
      "learning_rate": 0.0001521495327102804,
      "loss": 0.3632,
      "step": 901
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.06824137270450592,
      "learning_rate": 0.00015209612817089453,
      "loss": 0.336,
      "step": 902
    },
    {
      "epoch": 0.2408,
      "grad_norm": 0.0543215349316597,
      "learning_rate": 0.00015204272363150868,
      "loss": 0.3252,
      "step": 903
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.08822443336248398,
      "learning_rate": 0.00015198931909212284,
      "loss": 0.3087,
      "step": 904
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 0.07737923413515091,
      "learning_rate": 0.000151935914552737,
      "loss": 0.336,
      "step": 905
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.12146222591400146,
      "learning_rate": 0.00015188251001335115,
      "loss": 0.4375,
      "step": 906
    },
    {
      "epoch": 0.24186666666666667,
      "grad_norm": 0.08575833588838577,
      "learning_rate": 0.00015182910547396528,
      "loss": 0.3807,
      "step": 907
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.10344252735376358,
      "learning_rate": 0.00015177570093457944,
      "loss": 0.3758,
      "step": 908
    },
    {
      "epoch": 0.2424,
      "grad_norm": 0.09246370196342468,
      "learning_rate": 0.0001517222963951936,
      "loss": 0.3527,
      "step": 909
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.0803242176771164,
      "learning_rate": 0.00015166889185580775,
      "loss": 0.3415,
      "step": 910
    },
    {
      "epoch": 0.24293333333333333,
      "grad_norm": 0.06770353764295578,
      "learning_rate": 0.0001516154873164219,
      "loss": 0.3448,
      "step": 911
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.07054365426301956,
      "learning_rate": 0.00015156208277703606,
      "loss": 0.3313,
      "step": 912
    },
    {
      "epoch": 0.24346666666666666,
      "grad_norm": 0.0753784105181694,
      "learning_rate": 0.0001515086782376502,
      "loss": 0.2567,
      "step": 913
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.052803974598646164,
      "learning_rate": 0.00015145527369826435,
      "loss": 0.2666,
      "step": 914
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.06894426047801971,
      "learning_rate": 0.0001514018691588785,
      "loss": 0.2984,
      "step": 915
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.06799088418483734,
      "learning_rate": 0.00015134846461949266,
      "loss": 0.3035,
      "step": 916
    },
    {
      "epoch": 0.24453333333333332,
      "grad_norm": 0.07721834629774094,
      "learning_rate": 0.00015129506008010682,
      "loss": 0.3509,
      "step": 917
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.06792556494474411,
      "learning_rate": 0.00015124165554072097,
      "loss": 0.2962,
      "step": 918
    },
    {
      "epoch": 0.24506666666666665,
      "grad_norm": 0.05440962314605713,
      "learning_rate": 0.0001511882510013351,
      "loss": 0.2892,
      "step": 919
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.077764593064785,
      "learning_rate": 0.00015113484646194926,
      "loss": 0.3388,
      "step": 920
    },
    {
      "epoch": 0.2456,
      "grad_norm": 0.06745945662260056,
      "learning_rate": 0.00015108144192256342,
      "loss": 0.3631,
      "step": 921
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.07436452805995941,
      "learning_rate": 0.00015102803738317757,
      "loss": 0.3534,
      "step": 922
    },
    {
      "epoch": 0.24613333333333334,
      "grad_norm": 0.06124414876103401,
      "learning_rate": 0.00015097463284379173,
      "loss": 0.3323,
      "step": 923
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.06504630297422409,
      "learning_rate": 0.00015092122830440588,
      "loss": 0.2763,
      "step": 924
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06227119639515877,
      "learning_rate": 0.00015086782376502004,
      "loss": 0.3548,
      "step": 925
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.053958211094141006,
      "learning_rate": 0.0001508144192256342,
      "loss": 0.2802,
      "step": 926
    },
    {
      "epoch": 0.2472,
      "grad_norm": 0.05712038278579712,
      "learning_rate": 0.00015076101468624835,
      "loss": 0.279,
      "step": 927
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.06118054687976837,
      "learning_rate": 0.0001507076101468625,
      "loss": 0.3318,
      "step": 928
    },
    {
      "epoch": 0.24773333333333333,
      "grad_norm": 0.06271147727966309,
      "learning_rate": 0.00015065420560747664,
      "loss": 0.371,
      "step": 929
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.08435315638780594,
      "learning_rate": 0.0001506008010680908,
      "loss": 0.31,
      "step": 930
    },
    {
      "epoch": 0.24826666666666666,
      "grad_norm": 0.057584647089242935,
      "learning_rate": 0.00015054739652870495,
      "loss": 0.3562,
      "step": 931
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.05850639566779137,
      "learning_rate": 0.0001504939919893191,
      "loss": 0.2991,
      "step": 932
    },
    {
      "epoch": 0.2488,
      "grad_norm": 0.06759059429168701,
      "learning_rate": 0.00015044058744993326,
      "loss": 0.3075,
      "step": 933
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.05403807759284973,
      "learning_rate": 0.00015038718291054742,
      "loss": 0.3088,
      "step": 934
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.07063811272382736,
      "learning_rate": 0.00015033377837116155,
      "loss": 0.3426,
      "step": 935
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.05151514336466789,
      "learning_rate": 0.0001502803738317757,
      "loss": 0.2918,
      "step": 936
    },
    {
      "epoch": 0.24986666666666665,
      "grad_norm": 0.056726645678281784,
      "learning_rate": 0.00015022696929238986,
      "loss": 0.3069,
      "step": 937
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.06030208617448807,
      "learning_rate": 0.00015017356475300402,
      "loss": 0.3256,
      "step": 938
    },
    {
      "epoch": 0.2504,
      "grad_norm": 0.07994183152914047,
      "learning_rate": 0.00015012016021361818,
      "loss": 0.3698,
      "step": 939
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.06099320948123932,
      "learning_rate": 0.0001500667556742323,
      "loss": 0.368,
      "step": 940
    },
    {
      "epoch": 0.25093333333333334,
      "grad_norm": 0.0947309210896492,
      "learning_rate": 0.00015001335113484646,
      "loss": 0.3892,
      "step": 941
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.06513157486915588,
      "learning_rate": 0.00014995994659546062,
      "loss": 0.3877,
      "step": 942
    },
    {
      "epoch": 0.25146666666666667,
      "grad_norm": 0.0738687738776207,
      "learning_rate": 0.00014990654205607477,
      "loss": 0.3355,
      "step": 943
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.07412846386432648,
      "learning_rate": 0.00014985313751668893,
      "loss": 0.3815,
      "step": 944
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.06612829118967056,
      "learning_rate": 0.0001497997329773031,
      "loss": 0.4027,
      "step": 945
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.08323018997907639,
      "learning_rate": 0.00014974632843791722,
      "loss": 0.3796,
      "step": 946
    },
    {
      "epoch": 0.25253333333333333,
      "grad_norm": 0.05030507594347,
      "learning_rate": 0.00014969292389853137,
      "loss": 0.3011,
      "step": 947
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.05457798391580582,
      "learning_rate": 0.00014963951935914553,
      "loss": 0.3215,
      "step": 948
    },
    {
      "epoch": 0.25306666666666666,
      "grad_norm": 0.05658984184265137,
      "learning_rate": 0.00014958611481975968,
      "loss": 0.3866,
      "step": 949
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06277742236852646,
      "learning_rate": 0.00014953271028037384,
      "loss": 0.2563,
      "step": 950
    },
    {
      "epoch": 0.2536,
      "grad_norm": 0.07023227214813232,
      "learning_rate": 0.000149479305740988,
      "loss": 0.2606,
      "step": 951
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.07052144408226013,
      "learning_rate": 0.00014942590120160213,
      "loss": 0.2937,
      "step": 952
    },
    {
      "epoch": 0.2541333333333333,
      "grad_norm": 0.08532286435365677,
      "learning_rate": 0.00014937249666221628,
      "loss": 0.347,
      "step": 953
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.08269710093736649,
      "learning_rate": 0.00014931909212283044,
      "loss": 0.3256,
      "step": 954
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 0.06784111261367798,
      "learning_rate": 0.0001492656875834446,
      "loss": 0.3221,
      "step": 955
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.06427446007728577,
      "learning_rate": 0.00014921228304405875,
      "loss": 0.3364,
      "step": 956
    },
    {
      "epoch": 0.2552,
      "grad_norm": 0.07152988016605377,
      "learning_rate": 0.00014915887850467288,
      "loss": 0.3294,
      "step": 957
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.06949548423290253,
      "learning_rate": 0.00014910547396528704,
      "loss": 0.2936,
      "step": 958
    },
    {
      "epoch": 0.2557333333333333,
      "grad_norm": 0.0682649314403534,
      "learning_rate": 0.00014905206942590122,
      "loss": 0.3473,
      "step": 959
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.06139140576124191,
      "learning_rate": 0.00014899866488651538,
      "loss": 0.2798,
      "step": 960
    },
    {
      "epoch": 0.25626666666666664,
      "grad_norm": 0.06487306952476501,
      "learning_rate": 0.00014894526034712953,
      "loss": 0.2667,
      "step": 961
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.07114638388156891,
      "learning_rate": 0.00014889185580774366,
      "loss": 0.2995,
      "step": 962
    },
    {
      "epoch": 0.2568,
      "grad_norm": 0.05633902922272682,
      "learning_rate": 0.00014883845126835782,
      "loss": 0.2877,
      "step": 963
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.075100377202034,
      "learning_rate": 0.00014878504672897198,
      "loss": 0.3293,
      "step": 964
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.060371559113264084,
      "learning_rate": 0.00014873164218958613,
      "loss": 0.2977,
      "step": 965
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.06947380304336548,
      "learning_rate": 0.0001486782376502003,
      "loss": 0.3019,
      "step": 966
    },
    {
      "epoch": 0.2578666666666667,
      "grad_norm": 0.0572645477950573,
      "learning_rate": 0.00014862483311081444,
      "loss": 0.3066,
      "step": 967
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.06512116640806198,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3128,
      "step": 968
    },
    {
      "epoch": 0.2584,
      "grad_norm": 0.07358608394861221,
      "learning_rate": 0.00014851802403204273,
      "loss": 0.353,
      "step": 969
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.07054357975721359,
      "learning_rate": 0.0001484646194926569,
      "loss": 0.3349,
      "step": 970
    },
    {
      "epoch": 0.25893333333333335,
      "grad_norm": 0.05310411378741264,
      "learning_rate": 0.00014841121495327104,
      "loss": 0.3128,
      "step": 971
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.09022241830825806,
      "learning_rate": 0.0001483578104138852,
      "loss": 0.2912,
      "step": 972
    },
    {
      "epoch": 0.2594666666666667,
      "grad_norm": 0.09860136359930038,
      "learning_rate": 0.00014830440587449933,
      "loss": 0.3839,
      "step": 973
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.06806454062461853,
      "learning_rate": 0.00014825100133511348,
      "loss": 0.3584,
      "step": 974
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06392164528369904,
      "learning_rate": 0.00014819759679572764,
      "loss": 0.2988,
      "step": 975
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.06658978760242462,
      "learning_rate": 0.0001481441922563418,
      "loss": 0.2791,
      "step": 976
    },
    {
      "epoch": 0.26053333333333334,
      "grad_norm": 0.07869037240743637,
      "learning_rate": 0.00014809078771695595,
      "loss": 0.3575,
      "step": 977
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.06228485703468323,
      "learning_rate": 0.0001480373831775701,
      "loss": 0.3731,
      "step": 978
    },
    {
      "epoch": 0.26106666666666667,
      "grad_norm": 0.09318240731954575,
      "learning_rate": 0.00014798397863818424,
      "loss": 0.3746,
      "step": 979
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.07519177347421646,
      "learning_rate": 0.0001479305740987984,
      "loss": 0.2998,
      "step": 980
    },
    {
      "epoch": 0.2616,
      "grad_norm": 0.07454466074705124,
      "learning_rate": 0.00014787716955941255,
      "loss": 0.2914,
      "step": 981
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.09901148825883865,
      "learning_rate": 0.0001478237650200267,
      "loss": 0.391,
      "step": 982
    },
    {
      "epoch": 0.26213333333333333,
      "grad_norm": 0.06686660647392273,
      "learning_rate": 0.00014777036048064086,
      "loss": 0.333,
      "step": 983
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.07416641712188721,
      "learning_rate": 0.00014771695594125502,
      "loss": 0.4003,
      "step": 984
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.10334895551204681,
      "learning_rate": 0.00014766355140186915,
      "loss": 0.3274,
      "step": 985
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.05349384993314743,
      "learning_rate": 0.0001476101468624833,
      "loss": 0.2597,
      "step": 986
    },
    {
      "epoch": 0.2632,
      "grad_norm": 0.06004596874117851,
      "learning_rate": 0.00014755674232309746,
      "loss": 0.2894,
      "step": 987
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.05195402354001999,
      "learning_rate": 0.00014750333778371162,
      "loss": 0.2281,
      "step": 988
    },
    {
      "epoch": 0.2637333333333333,
      "grad_norm": 0.09842541813850403,
      "learning_rate": 0.00014744993324432578,
      "loss": 0.3782,
      "step": 989
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.06187164783477783,
      "learning_rate": 0.0001473965287049399,
      "loss": 0.2496,
      "step": 990
    },
    {
      "epoch": 0.26426666666666665,
      "grad_norm": 0.05859510600566864,
      "learning_rate": 0.00014734312416555406,
      "loss": 0.3022,
      "step": 991
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.056823406368494034,
      "learning_rate": 0.00014728971962616822,
      "loss": 0.3148,
      "step": 992
    },
    {
      "epoch": 0.2648,
      "grad_norm": 0.07529011368751526,
      "learning_rate": 0.00014723631508678237,
      "loss": 0.3642,
      "step": 993
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.06220423802733421,
      "learning_rate": 0.00014718291054739656,
      "loss": 0.2782,
      "step": 994
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.06041784584522247,
      "learning_rate": 0.0001471295060080107,
      "loss": 0.3059,
      "step": 995
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.058142874389886856,
      "learning_rate": 0.00014707610146862484,
      "loss": 0.3404,
      "step": 996
    },
    {
      "epoch": 0.26586666666666664,
      "grad_norm": 0.0630897507071495,
      "learning_rate": 0.000147022696929239,
      "loss": 0.3285,
      "step": 997
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.06448612362146378,
      "learning_rate": 0.00014696929238985316,
      "loss": 0.3692,
      "step": 998
    },
    {
      "epoch": 0.2664,
      "grad_norm": 0.08227842301130295,
      "learning_rate": 0.0001469158878504673,
      "loss": 0.3453,
      "step": 999
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.05495784431695938,
      "learning_rate": 0.00014686248331108147,
      "loss": 0.3521,
      "step": 1000
    },
    {
      "epoch": 0.26693333333333336,
      "grad_norm": 0.060713253915309906,
      "learning_rate": 0.0001468090787716956,
      "loss": 0.2985,
      "step": 1001
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.07375608384609222,
      "learning_rate": 0.00014675567423230975,
      "loss": 0.3594,
      "step": 1002
    },
    {
      "epoch": 0.2674666666666667,
      "grad_norm": 0.06517252326011658,
      "learning_rate": 0.0001467022696929239,
      "loss": 0.3003,
      "step": 1003
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.06954677402973175,
      "learning_rate": 0.00014664886515353807,
      "loss": 0.3137,
      "step": 1004
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.07742604613304138,
      "learning_rate": 0.00014659546061415222,
      "loss": 0.3197,
      "step": 1005
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.10394717007875443,
      "learning_rate": 0.00014654205607476635,
      "loss": 0.4026,
      "step": 1006
    },
    {
      "epoch": 0.26853333333333335,
      "grad_norm": 0.06885116547346115,
      "learning_rate": 0.0001464886515353805,
      "loss": 0.3016,
      "step": 1007
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.0700179934501648,
      "learning_rate": 0.00014643524699599466,
      "loss": 0.3136,
      "step": 1008
    },
    {
      "epoch": 0.2690666666666667,
      "grad_norm": 0.05938117578625679,
      "learning_rate": 0.00014638184245660882,
      "loss": 0.3484,
      "step": 1009
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.054760776460170746,
      "learning_rate": 0.00014632843791722298,
      "loss": 0.2862,
      "step": 1010
    },
    {
      "epoch": 0.2696,
      "grad_norm": 0.06442096829414368,
      "learning_rate": 0.00014627503337783713,
      "loss": 0.3213,
      "step": 1011
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.07765213400125504,
      "learning_rate": 0.00014622162883845126,
      "loss": 0.3368,
      "step": 1012
    },
    {
      "epoch": 0.27013333333333334,
      "grad_norm": 0.05341548100113869,
      "learning_rate": 0.00014616822429906542,
      "loss": 0.323,
      "step": 1013
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.051777034997940063,
      "learning_rate": 0.00014611481975967958,
      "loss": 0.287,
      "step": 1014
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 0.053576719015836716,
      "learning_rate": 0.00014606141522029373,
      "loss": 0.3246,
      "step": 1015
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.06570032984018326,
      "learning_rate": 0.0001460080106809079,
      "loss": 0.3557,
      "step": 1016
    },
    {
      "epoch": 0.2712,
      "grad_norm": 0.05390648543834686,
      "learning_rate": 0.00014595460614152204,
      "loss": 0.2726,
      "step": 1017
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.08269491046667099,
      "learning_rate": 0.00014590120160213617,
      "loss": 0.3363,
      "step": 1018
    },
    {
      "epoch": 0.2717333333333333,
      "grad_norm": 0.06314590573310852,
      "learning_rate": 0.00014584779706275033,
      "loss": 0.306,
      "step": 1019
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.07715094089508057,
      "learning_rate": 0.00014579439252336449,
      "loss": 0.3406,
      "step": 1020
    },
    {
      "epoch": 0.27226666666666666,
      "grad_norm": 0.09109067916870117,
      "learning_rate": 0.00014574098798397864,
      "loss": 0.3067,
      "step": 1021
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.06289444118738174,
      "learning_rate": 0.0001456875834445928,
      "loss": 0.3008,
      "step": 1022
    },
    {
      "epoch": 0.2728,
      "grad_norm": 0.05127595365047455,
      "learning_rate": 0.00014563417890520693,
      "loss": 0.3018,
      "step": 1023
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.08348485082387924,
      "learning_rate": 0.00014558077436582108,
      "loss": 0.3721,
      "step": 1024
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.05504639074206352,
      "learning_rate": 0.00014552736982643524,
      "loss": 0.311,
      "step": 1025
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.08874675631523132,
      "learning_rate": 0.0001454739652870494,
      "loss": 0.381,
      "step": 1026
    },
    {
      "epoch": 0.27386666666666665,
      "grad_norm": 0.05817785486578941,
      "learning_rate": 0.00014542056074766355,
      "loss": 0.3136,
      "step": 1027
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.10566782206296921,
      "learning_rate": 0.0001453671562082777,
      "loss": 0.3296,
      "step": 1028
    },
    {
      "epoch": 0.2744,
      "grad_norm": 0.07142860442399979,
      "learning_rate": 0.00014531375166889187,
      "loss": 0.3109,
      "step": 1029
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.07089819014072418,
      "learning_rate": 0.00014526034712950602,
      "loss": 0.2969,
      "step": 1030
    },
    {
      "epoch": 0.2749333333333333,
      "grad_norm": 0.07807804644107819,
      "learning_rate": 0.00014520694259012018,
      "loss": 0.3775,
      "step": 1031
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.054646920412778854,
      "learning_rate": 0.00014515353805073434,
      "loss": 0.2529,
      "step": 1032
    },
    {
      "epoch": 0.2754666666666667,
      "grad_norm": 0.06697669625282288,
      "learning_rate": 0.0001451001335113485,
      "loss": 0.3651,
      "step": 1033
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.06273366510868073,
      "learning_rate": 0.00014504672897196262,
      "loss": 0.368,
      "step": 1034
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.06601391732692719,
      "learning_rate": 0.00014499332443257678,
      "loss": 0.3258,
      "step": 1035
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.06197958067059517,
      "learning_rate": 0.00014493991989319093,
      "loss": 0.2941,
      "step": 1036
    },
    {
      "epoch": 0.27653333333333335,
      "grad_norm": 0.06282608211040497,
      "learning_rate": 0.0001448865153538051,
      "loss": 0.3182,
      "step": 1037
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.05185728892683983,
      "learning_rate": 0.00014483311081441925,
      "loss": 0.2738,
      "step": 1038
    },
    {
      "epoch": 0.2770666666666667,
      "grad_norm": 0.071136474609375,
      "learning_rate": 0.00014477970627503338,
      "loss": 0.3518,
      "step": 1039
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.07597915828227997,
      "learning_rate": 0.00014472630173564753,
      "loss": 0.3589,
      "step": 1040
    },
    {
      "epoch": 0.2776,
      "grad_norm": 0.0639675185084343,
      "learning_rate": 0.0001446728971962617,
      "loss": 0.3702,
      "step": 1041
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.06712927669286728,
      "learning_rate": 0.00014461949265687584,
      "loss": 0.3641,
      "step": 1042
    },
    {
      "epoch": 0.27813333333333334,
      "grad_norm": 0.05315150320529938,
      "learning_rate": 0.00014456608811749,
      "loss": 0.2634,
      "step": 1043
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.053090643137693405,
      "learning_rate": 0.00014451268357810416,
      "loss": 0.2795,
      "step": 1044
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.07125851511955261,
      "learning_rate": 0.00014445927903871829,
      "loss": 0.3729,
      "step": 1045
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.057894837111234665,
      "learning_rate": 0.00014440587449933244,
      "loss": 0.3192,
      "step": 1046
    },
    {
      "epoch": 0.2792,
      "grad_norm": 0.05429009720683098,
      "learning_rate": 0.0001443524699599466,
      "loss": 0.3103,
      "step": 1047
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.060201745480298996,
      "learning_rate": 0.00014429906542056076,
      "loss": 0.3343,
      "step": 1048
    },
    {
      "epoch": 0.27973333333333333,
      "grad_norm": 0.05536565184593201,
      "learning_rate": 0.0001442456608811749,
      "loss": 0.3137,
      "step": 1049
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.0653749406337738,
      "learning_rate": 0.00014419225634178907,
      "loss": 0.3445,
      "step": 1050
    },
    {
      "epoch": 0.28026666666666666,
      "grad_norm": 0.06778773665428162,
      "learning_rate": 0.0001441388518024032,
      "loss": 0.3703,
      "step": 1051
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.058288704603910446,
      "learning_rate": 0.00014408544726301735,
      "loss": 0.3491,
      "step": 1052
    },
    {
      "epoch": 0.2808,
      "grad_norm": 0.07330206036567688,
      "learning_rate": 0.0001440320427236315,
      "loss": 0.3809,
      "step": 1053
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.06844216585159302,
      "learning_rate": 0.00014397863818424567,
      "loss": 0.3473,
      "step": 1054
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.05138380080461502,
      "learning_rate": 0.00014392523364485982,
      "loss": 0.2815,
      "step": 1055
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.07529434561729431,
      "learning_rate": 0.00014387182910547398,
      "loss": 0.369,
      "step": 1056
    },
    {
      "epoch": 0.28186666666666665,
      "grad_norm": 0.05367032811045647,
      "learning_rate": 0.0001438184245660881,
      "loss": 0.2816,
      "step": 1057
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.08156903088092804,
      "learning_rate": 0.00014376502002670226,
      "loss": 0.3634,
      "step": 1058
    },
    {
      "epoch": 0.2824,
      "grad_norm": 0.07030189037322998,
      "learning_rate": 0.00014371161548731642,
      "loss": 0.3331,
      "step": 1059
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.06637845933437347,
      "learning_rate": 0.00014365821094793058,
      "loss": 0.3062,
      "step": 1060
    },
    {
      "epoch": 0.2829333333333333,
      "grad_norm": 0.07943741232156754,
      "learning_rate": 0.00014360480640854473,
      "loss": 0.3914,
      "step": 1061
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.08539264649152756,
      "learning_rate": 0.00014355140186915886,
      "loss": 0.367,
      "step": 1062
    },
    {
      "epoch": 0.28346666666666664,
      "grad_norm": 0.07371573895215988,
      "learning_rate": 0.00014349799732977305,
      "loss": 0.3736,
      "step": 1063
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.05250915512442589,
      "learning_rate": 0.0001434445927903872,
      "loss": 0.3412,
      "step": 1064
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.08223921060562134,
      "learning_rate": 0.00014339118825100136,
      "loss": 0.3483,
      "step": 1065
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.06535636633634567,
      "learning_rate": 0.00014333778371161552,
      "loss": 0.3617,
      "step": 1066
    },
    {
      "epoch": 0.28453333333333336,
      "grad_norm": 0.05262763798236847,
      "learning_rate": 0.00014328437917222964,
      "loss": 0.3149,
      "step": 1067
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.05069050192832947,
      "learning_rate": 0.0001432309746328438,
      "loss": 0.2848,
      "step": 1068
    },
    {
      "epoch": 0.2850666666666667,
      "grad_norm": 0.06226356700062752,
      "learning_rate": 0.00014317757009345796,
      "loss": 0.3489,
      "step": 1069
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.07432150840759277,
      "learning_rate": 0.0001431241655540721,
      "loss": 0.3331,
      "step": 1070
    },
    {
      "epoch": 0.2856,
      "grad_norm": 0.06192019581794739,
      "learning_rate": 0.00014307076101468627,
      "loss": 0.3669,
      "step": 1071
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.06390435993671417,
      "learning_rate": 0.00014301735647530043,
      "loss": 0.3178,
      "step": 1072
    },
    {
      "epoch": 0.28613333333333335,
      "grad_norm": 0.05230532959103584,
      "learning_rate": 0.00014296395193591456,
      "loss": 0.3233,
      "step": 1073
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.0625694990158081,
      "learning_rate": 0.0001429105473965287,
      "loss": 0.3571,
      "step": 1074
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05895225331187248,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.2919,
      "step": 1075
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.04974387213587761,
      "learning_rate": 0.00014280373831775702,
      "loss": 0.2864,
      "step": 1076
    },
    {
      "epoch": 0.2872,
      "grad_norm": 0.07645951211452484,
      "learning_rate": 0.00014275033377837118,
      "loss": 0.3517,
      "step": 1077
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.09570543467998505,
      "learning_rate": 0.0001426969292389853,
      "loss": 0.3772,
      "step": 1078
    },
    {
      "epoch": 0.28773333333333334,
      "grad_norm": 0.05417245626449585,
      "learning_rate": 0.00014264352469959947,
      "loss": 0.2536,
      "step": 1079
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.05499735102057457,
      "learning_rate": 0.00014259012016021362,
      "loss": 0.3071,
      "step": 1080
    },
    {
      "epoch": 0.28826666666666667,
      "grad_norm": 0.08260630816221237,
      "learning_rate": 0.00014253671562082778,
      "loss": 0.342,
      "step": 1081
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.05813819542527199,
      "learning_rate": 0.00014248331108144194,
      "loss": 0.3018,
      "step": 1082
    },
    {
      "epoch": 0.2888,
      "grad_norm": 0.0592048242688179,
      "learning_rate": 0.0001424299065420561,
      "loss": 0.3407,
      "step": 1083
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.08401241153478622,
      "learning_rate": 0.00014237650200267022,
      "loss": 0.3939,
      "step": 1084
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 0.05491220951080322,
      "learning_rate": 0.00014232309746328438,
      "loss": 0.3102,
      "step": 1085
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.06713564693927765,
      "learning_rate": 0.00014226969292389853,
      "loss": 0.3553,
      "step": 1086
    },
    {
      "epoch": 0.28986666666666666,
      "grad_norm": 0.0549057237803936,
      "learning_rate": 0.0001422162883845127,
      "loss": 0.2876,
      "step": 1087
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.08149028569459915,
      "learning_rate": 0.00014216288384512685,
      "loss": 0.3207,
      "step": 1088
    },
    {
      "epoch": 0.2904,
      "grad_norm": 0.06527961045503616,
      "learning_rate": 0.000142109479305741,
      "loss": 0.3441,
      "step": 1089
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.09254694730043411,
      "learning_rate": 0.00014205607476635513,
      "loss": 0.3728,
      "step": 1090
    },
    {
      "epoch": 0.2909333333333333,
      "grad_norm": 0.08202872425317764,
      "learning_rate": 0.0001420026702269693,
      "loss": 0.3386,
      "step": 1091
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.06836182624101639,
      "learning_rate": 0.00014194926568758344,
      "loss": 0.3126,
      "step": 1092
    },
    {
      "epoch": 0.29146666666666665,
      "grad_norm": 0.05454970896244049,
      "learning_rate": 0.0001418958611481976,
      "loss": 0.2953,
      "step": 1093
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.08607900142669678,
      "learning_rate": 0.00014184245660881176,
      "loss": 0.3352,
      "step": 1094
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.07440969347953796,
      "learning_rate": 0.00014178905206942589,
      "loss": 0.3134,
      "step": 1095
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.05012337490916252,
      "learning_rate": 0.00014173564753004004,
      "loss": 0.2787,
      "step": 1096
    },
    {
      "epoch": 0.2925333333333333,
      "grad_norm": 0.0484117828309536,
      "learning_rate": 0.0001416822429906542,
      "loss": 0.2014,
      "step": 1097
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.0677737295627594,
      "learning_rate": 0.00014162883845126838,
      "loss": 0.3175,
      "step": 1098
    },
    {
      "epoch": 0.29306666666666664,
      "grad_norm": 0.06807170808315277,
      "learning_rate": 0.00014157543391188254,
      "loss": 0.3585,
      "step": 1099
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.07448046654462814,
      "learning_rate": 0.00014152202937249667,
      "loss": 0.3645,
      "step": 1100
    },
    {
      "epoch": 0.2936,
      "grad_norm": 0.08564332127571106,
      "learning_rate": 0.00014146862483311082,
      "loss": 0.3505,
      "step": 1101
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.04584605619311333,
      "learning_rate": 0.00014141522029372498,
      "loss": 0.3135,
      "step": 1102
    },
    {
      "epoch": 0.29413333333333336,
      "grad_norm": 0.042225614190101624,
      "learning_rate": 0.00014136181575433914,
      "loss": 0.2258,
      "step": 1103
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.08724581450223923,
      "learning_rate": 0.0001413084112149533,
      "loss": 0.2957,
      "step": 1104
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.064189113676548,
      "learning_rate": 0.00014125500667556745,
      "loss": 0.3241,
      "step": 1105
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.05961647629737854,
      "learning_rate": 0.00014120160213618158,
      "loss": 0.2841,
      "step": 1106
    },
    {
      "epoch": 0.2952,
      "grad_norm": 0.06568103283643723,
      "learning_rate": 0.00014114819759679573,
      "loss": 0.3838,
      "step": 1107
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.07916383445262909,
      "learning_rate": 0.0001410947930574099,
      "loss": 0.38,
      "step": 1108
    },
    {
      "epoch": 0.29573333333333335,
      "grad_norm": 0.05103488266468048,
      "learning_rate": 0.00014104138851802405,
      "loss": 0.2601,
      "step": 1109
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.057101547718048096,
      "learning_rate": 0.0001409879839786382,
      "loss": 0.3047,
      "step": 1110
    },
    {
      "epoch": 0.2962666666666667,
      "grad_norm": 0.06729666143655777,
      "learning_rate": 0.00014093457943925233,
      "loss": 0.3569,
      "step": 1111
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.07332628220319748,
      "learning_rate": 0.0001408811748998665,
      "loss": 0.3758,
      "step": 1112
    },
    {
      "epoch": 0.2968,
      "grad_norm": 0.058173879981040955,
      "learning_rate": 0.00014082777036048065,
      "loss": 0.3181,
      "step": 1113
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.06332224607467651,
      "learning_rate": 0.0001407743658210948,
      "loss": 0.2829,
      "step": 1114
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 0.10364952683448792,
      "learning_rate": 0.00014072096128170896,
      "loss": 0.3922,
      "step": 1115
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.06039774417877197,
      "learning_rate": 0.00014066755674232311,
      "loss": 0.3321,
      "step": 1116
    },
    {
      "epoch": 0.29786666666666667,
      "grad_norm": 0.04837064445018768,
      "learning_rate": 0.00014061415220293724,
      "loss": 0.2891,
      "step": 1117
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.08530789613723755,
      "learning_rate": 0.0001405607476635514,
      "loss": 0.4195,
      "step": 1118
    },
    {
      "epoch": 0.2984,
      "grad_norm": 0.05819200724363327,
      "learning_rate": 0.00014050734312416556,
      "loss": 0.3566,
      "step": 1119
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.056523073464632034,
      "learning_rate": 0.0001404539385847797,
      "loss": 0.3029,
      "step": 1120
    },
    {
      "epoch": 0.29893333333333333,
      "grad_norm": 0.07532848417758942,
      "learning_rate": 0.00014040053404539387,
      "loss": 0.3152,
      "step": 1121
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.08724751323461533,
      "learning_rate": 0.00014034712950600803,
      "loss": 0.3083,
      "step": 1122
    },
    {
      "epoch": 0.29946666666666666,
      "grad_norm": 0.07675933092832565,
      "learning_rate": 0.00014029372496662216,
      "loss": 0.3079,
      "step": 1123
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.07860776782035828,
      "learning_rate": 0.0001402403204272363,
      "loss": 0.2996,
      "step": 1124
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.09993881732225418,
      "learning_rate": 0.00014018691588785047,
      "loss": 0.3816,
      "step": 1125
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.09216576814651489,
      "learning_rate": 0.00014013351134846462,
      "loss": 0.3836,
      "step": 1126
    },
    {
      "epoch": 0.3005333333333333,
      "grad_norm": 0.07360250502824783,
      "learning_rate": 0.00014008010680907878,
      "loss": 0.3946,
      "step": 1127
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.05420467630028725,
      "learning_rate": 0.0001400267022696929,
      "loss": 0.2747,
      "step": 1128
    },
    {
      "epoch": 0.30106666666666665,
      "grad_norm": 0.07579467445611954,
      "learning_rate": 0.00013997329773030707,
      "loss": 0.3239,
      "step": 1129
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.06303258240222931,
      "learning_rate": 0.00013991989319092122,
      "loss": 0.3165,
      "step": 1130
    },
    {
      "epoch": 0.3016,
      "grad_norm": 0.05345706641674042,
      "learning_rate": 0.00013986648865153538,
      "loss": 0.2867,
      "step": 1131
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.0702735185623169,
      "learning_rate": 0.00013981308411214956,
      "loss": 0.3594,
      "step": 1132
    },
    {
      "epoch": 0.3021333333333333,
      "grad_norm": 0.07198568433523178,
      "learning_rate": 0.0001397596795727637,
      "loss": 0.351,
      "step": 1133
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.05774008482694626,
      "learning_rate": 0.00013970627503337785,
      "loss": 0.2919,
      "step": 1134
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.08026933670043945,
      "learning_rate": 0.000139652870493992,
      "loss": 0.3239,
      "step": 1135
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.06179715692996979,
      "learning_rate": 0.00013959946595460616,
      "loss": 0.3647,
      "step": 1136
    },
    {
      "epoch": 0.3032,
      "grad_norm": 0.06469818204641342,
      "learning_rate": 0.00013954606141522032,
      "loss": 0.3171,
      "step": 1137
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.05676870793104172,
      "learning_rate": 0.00013949265687583447,
      "loss": 0.2814,
      "step": 1138
    },
    {
      "epoch": 0.30373333333333336,
      "grad_norm": 0.07036134600639343,
      "learning_rate": 0.0001394392523364486,
      "loss": 0.2985,
      "step": 1139
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.06393849849700928,
      "learning_rate": 0.00013938584779706276,
      "loss": 0.3077,
      "step": 1140
    },
    {
      "epoch": 0.3042666666666667,
      "grad_norm": 0.057524990290403366,
      "learning_rate": 0.00013933244325767691,
      "loss": 0.3798,
      "step": 1141
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.04784324765205383,
      "learning_rate": 0.00013927903871829107,
      "loss": 0.2724,
      "step": 1142
    },
    {
      "epoch": 0.3048,
      "grad_norm": 0.05304102972149849,
      "learning_rate": 0.00013922563417890523,
      "loss": 0.2573,
      "step": 1143
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.04378385841846466,
      "learning_rate": 0.00013917222963951936,
      "loss": 0.2707,
      "step": 1144
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.06264975666999817,
      "learning_rate": 0.0001391188251001335,
      "loss": 0.3039,
      "step": 1145
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.056575749069452286,
      "learning_rate": 0.00013906542056074767,
      "loss": 0.3116,
      "step": 1146
    },
    {
      "epoch": 0.3058666666666667,
      "grad_norm": 0.056133318692445755,
      "learning_rate": 0.00013901201602136183,
      "loss": 0.2645,
      "step": 1147
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.06635259091854095,
      "learning_rate": 0.00013895861148197598,
      "loss": 0.3589,
      "step": 1148
    },
    {
      "epoch": 0.3064,
      "grad_norm": 0.07815856486558914,
      "learning_rate": 0.00013890520694259014,
      "loss": 0.335,
      "step": 1149
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.05877958610653877,
      "learning_rate": 0.00013885180240320427,
      "loss": 0.3536,
      "step": 1150
    },
    {
      "epoch": 0.30693333333333334,
      "grad_norm": 0.054742928594350815,
      "learning_rate": 0.00013879839786381842,
      "loss": 0.3303,
      "step": 1151
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.06010567769408226,
      "learning_rate": 0.00013874499332443258,
      "loss": 0.3096,
      "step": 1152
    },
    {
      "epoch": 0.30746666666666667,
      "grad_norm": 0.09066811203956604,
      "learning_rate": 0.00013869158878504674,
      "loss": 0.3505,
      "step": 1153
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.057238880544900894,
      "learning_rate": 0.0001386381842456609,
      "loss": 0.3213,
      "step": 1154
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.06590936332941055,
      "learning_rate": 0.00013858477970627505,
      "loss": 0.326,
      "step": 1155
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.08875398337841034,
      "learning_rate": 0.00013853137516688918,
      "loss": 0.3097,
      "step": 1156
    },
    {
      "epoch": 0.3085333333333333,
      "grad_norm": 0.05929291620850563,
      "learning_rate": 0.00013847797062750333,
      "loss": 0.307,
      "step": 1157
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.04671715199947357,
      "learning_rate": 0.0001384245660881175,
      "loss": 0.3418,
      "step": 1158
    },
    {
      "epoch": 0.30906666666666666,
      "grad_norm": 0.052012257277965546,
      "learning_rate": 0.00013837116154873165,
      "loss": 0.33,
      "step": 1159
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.0674552395939827,
      "learning_rate": 0.0001383177570093458,
      "loss": 0.3871,
      "step": 1160
    },
    {
      "epoch": 0.3096,
      "grad_norm": 0.06288791447877884,
      "learning_rate": 0.00013826435246995993,
      "loss": 0.296,
      "step": 1161
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.06251386553049088,
      "learning_rate": 0.0001382109479305741,
      "loss": 0.2601,
      "step": 1162
    },
    {
      "epoch": 0.3101333333333333,
      "grad_norm": 0.05847044289112091,
      "learning_rate": 0.00013815754339118825,
      "loss": 0.2907,
      "step": 1163
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.06935805827379227,
      "learning_rate": 0.0001381041388518024,
      "loss": 0.4005,
      "step": 1164
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 0.07115436345338821,
      "learning_rate": 0.00013805073431241656,
      "loss": 0.3914,
      "step": 1165
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.05697980150580406,
      "learning_rate": 0.00013799732977303071,
      "loss": 0.3403,
      "step": 1166
    },
    {
      "epoch": 0.3112,
      "grad_norm": 0.059407610446214676,
      "learning_rate": 0.00013794392523364487,
      "loss": 0.3028,
      "step": 1167
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.07185986638069153,
      "learning_rate": 0.00013789052069425903,
      "loss": 0.3232,
      "step": 1168
    },
    {
      "epoch": 0.3117333333333333,
      "grad_norm": 0.04755396768450737,
      "learning_rate": 0.00013783711615487318,
      "loss": 0.2677,
      "step": 1169
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.07561750710010529,
      "learning_rate": 0.00013778371161548734,
      "loss": 0.3864,
      "step": 1170
    },
    {
      "epoch": 0.3122666666666667,
      "grad_norm": 0.05582465976476669,
      "learning_rate": 0.0001377303070761015,
      "loss": 0.3258,
      "step": 1171
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.05308065190911293,
      "learning_rate": 0.00013767690253671563,
      "loss": 0.339,
      "step": 1172
    },
    {
      "epoch": 0.3128,
      "grad_norm": 0.05515925958752632,
      "learning_rate": 0.00013762349799732978,
      "loss": 0.3066,
      "step": 1173
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.05330146476626396,
      "learning_rate": 0.00013757009345794394,
      "loss": 0.2868,
      "step": 1174
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.060240745544433594,
      "learning_rate": 0.0001375166889185581,
      "loss": 0.3518,
      "step": 1175
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.05848933383822441,
      "learning_rate": 0.00013746328437917225,
      "loss": 0.337,
      "step": 1176
    },
    {
      "epoch": 0.3138666666666667,
      "grad_norm": 0.05749897286295891,
      "learning_rate": 0.00013740987983978638,
      "loss": 0.3451,
      "step": 1177
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.07398201525211334,
      "learning_rate": 0.00013735647530040054,
      "loss": 0.3198,
      "step": 1178
    },
    {
      "epoch": 0.3144,
      "grad_norm": 0.06671486049890518,
      "learning_rate": 0.0001373030707610147,
      "loss": 0.2862,
      "step": 1179
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.07866735011339188,
      "learning_rate": 0.00013724966622162885,
      "loss": 0.2996,
      "step": 1180
    },
    {
      "epoch": 0.31493333333333334,
      "grad_norm": 0.08078844845294952,
      "learning_rate": 0.000137196261682243,
      "loss": 0.3254,
      "step": 1181
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.06877657026052475,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.3322,
      "step": 1182
    },
    {
      "epoch": 0.3154666666666667,
      "grad_norm": 0.05478227138519287,
      "learning_rate": 0.0001370894526034713,
      "loss": 0.3192,
      "step": 1183
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.05898825079202652,
      "learning_rate": 0.00013703604806408545,
      "loss": 0.3042,
      "step": 1184
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.0547964945435524,
      "learning_rate": 0.0001369826435246996,
      "loss": 0.331,
      "step": 1185
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.05447946861386299,
      "learning_rate": 0.00013692923898531376,
      "loss": 0.2864,
      "step": 1186
    },
    {
      "epoch": 0.31653333333333333,
      "grad_norm": 0.0635455921292305,
      "learning_rate": 0.00013687583444592792,
      "loss": 0.248,
      "step": 1187
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.07020466774702072,
      "learning_rate": 0.00013682242990654207,
      "loss": 0.3398,
      "step": 1188
    },
    {
      "epoch": 0.31706666666666666,
      "grad_norm": 0.05307571962475777,
      "learning_rate": 0.0001367690253671562,
      "loss": 0.2639,
      "step": 1189
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.07473059743642807,
      "learning_rate": 0.00013671562082777036,
      "loss": 0.35,
      "step": 1190
    },
    {
      "epoch": 0.3176,
      "grad_norm": 0.05565929412841797,
      "learning_rate": 0.00013666221628838451,
      "loss": 0.3025,
      "step": 1191
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.06851345300674438,
      "learning_rate": 0.00013660881174899867,
      "loss": 0.3547,
      "step": 1192
    },
    {
      "epoch": 0.3181333333333333,
      "grad_norm": 0.04545728862285614,
      "learning_rate": 0.00013655540720961283,
      "loss": 0.2988,
      "step": 1193
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.0672125592827797,
      "learning_rate": 0.00013650200267022696,
      "loss": 0.3647,
      "step": 1194
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.09556903690099716,
      "learning_rate": 0.0001364485981308411,
      "loss": 0.3665,
      "step": 1195
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.06411953270435333,
      "learning_rate": 0.00013639519359145527,
      "loss": 0.3372,
      "step": 1196
    },
    {
      "epoch": 0.3192,
      "grad_norm": 0.051399536430835724,
      "learning_rate": 0.00013634178905206943,
      "loss": 0.2929,
      "step": 1197
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.06097942218184471,
      "learning_rate": 0.00013628838451268358,
      "loss": 0.2902,
      "step": 1198
    },
    {
      "epoch": 0.3197333333333333,
      "grad_norm": 0.07720297574996948,
      "learning_rate": 0.00013623497997329774,
      "loss": 0.3113,
      "step": 1199
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0643567368388176,
      "learning_rate": 0.00013618157543391187,
      "loss": 0.2784,
      "step": 1200
    },
    {
      "epoch": 0.32026666666666664,
      "grad_norm": 0.06972035765647888,
      "learning_rate": 0.00013612817089452602,
      "loss": 0.3407,
      "step": 1201
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.04934775084257126,
      "learning_rate": 0.0001360747663551402,
      "loss": 0.279,
      "step": 1202
    },
    {
      "epoch": 0.3208,
      "grad_norm": 0.07137257605791092,
      "learning_rate": 0.00013602136181575436,
      "loss": 0.3295,
      "step": 1203
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.07476745545864105,
      "learning_rate": 0.00013596795727636852,
      "loss": 0.32,
      "step": 1204
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.07386697828769684,
      "learning_rate": 0.00013591455273698265,
      "loss": 0.3183,
      "step": 1205
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.056941792368888855,
      "learning_rate": 0.0001358611481975968,
      "loss": 0.3258,
      "step": 1206
    },
    {
      "epoch": 0.3218666666666667,
      "grad_norm": 0.050437044352293015,
      "learning_rate": 0.00013580774365821096,
      "loss": 0.2827,
      "step": 1207
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.057385195046663284,
      "learning_rate": 0.00013575433911882512,
      "loss": 0.2472,
      "step": 1208
    },
    {
      "epoch": 0.3224,
      "grad_norm": 0.07401428371667862,
      "learning_rate": 0.00013570093457943927,
      "loss": 0.2872,
      "step": 1209
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.06227228417992592,
      "learning_rate": 0.0001356475300400534,
      "loss": 0.3247,
      "step": 1210
    },
    {
      "epoch": 0.32293333333333335,
      "grad_norm": 0.06507855653762817,
      "learning_rate": 0.00013559412550066756,
      "loss": 0.3351,
      "step": 1211
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.05688003450632095,
      "learning_rate": 0.00013554072096128172,
      "loss": 0.3308,
      "step": 1212
    },
    {
      "epoch": 0.3234666666666667,
      "grad_norm": 0.07475518435239792,
      "learning_rate": 0.00013548731642189587,
      "loss": 0.3443,
      "step": 1213
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.06572191417217255,
      "learning_rate": 0.00013543391188251003,
      "loss": 0.3022,
      "step": 1214
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.04831339418888092,
      "learning_rate": 0.00013538050734312419,
      "loss": 0.281,
      "step": 1215
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.06760513037443161,
      "learning_rate": 0.00013532710280373831,
      "loss": 0.2558,
      "step": 1216
    },
    {
      "epoch": 0.32453333333333334,
      "grad_norm": 0.05786044895648956,
      "learning_rate": 0.00013527369826435247,
      "loss": 0.2672,
      "step": 1217
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.059648241847753525,
      "learning_rate": 0.00013522029372496663,
      "loss": 0.3156,
      "step": 1218
    },
    {
      "epoch": 0.32506666666666667,
      "grad_norm": 0.050527285784482956,
      "learning_rate": 0.00013516688918558078,
      "loss": 0.2628,
      "step": 1219
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.09160274267196655,
      "learning_rate": 0.00013511348464619494,
      "loss": 0.368,
      "step": 1220
    },
    {
      "epoch": 0.3256,
      "grad_norm": 0.08736630529165268,
      "learning_rate": 0.0001350600801068091,
      "loss": 0.3231,
      "step": 1221
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.06482022255659103,
      "learning_rate": 0.00013500667556742323,
      "loss": 0.2878,
      "step": 1222
    },
    {
      "epoch": 0.32613333333333333,
      "grad_norm": 0.060562580823898315,
      "learning_rate": 0.00013495327102803738,
      "loss": 0.3106,
      "step": 1223
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.09207971394062042,
      "learning_rate": 0.00013489986648865154,
      "loss": 0.3806,
      "step": 1224
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.06821224838495255,
      "learning_rate": 0.0001348464619492657,
      "loss": 0.3242,
      "step": 1225
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.0654153898358345,
      "learning_rate": 0.00013479305740987985,
      "loss": 0.2674,
      "step": 1226
    },
    {
      "epoch": 0.3272,
      "grad_norm": 0.07195566594600677,
      "learning_rate": 0.00013473965287049398,
      "loss": 0.347,
      "step": 1227
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.07097448408603668,
      "learning_rate": 0.00013468624833110814,
      "loss": 0.3508,
      "step": 1228
    },
    {
      "epoch": 0.3277333333333333,
      "grad_norm": 0.06887666136026382,
      "learning_rate": 0.0001346328437917223,
      "loss": 0.3518,
      "step": 1229
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.07090577483177185,
      "learning_rate": 0.00013457943925233645,
      "loss": 0.3849,
      "step": 1230
    },
    {
      "epoch": 0.32826666666666665,
      "grad_norm": 0.053542010486125946,
      "learning_rate": 0.0001345260347129506,
      "loss": 0.2594,
      "step": 1231
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.06294418126344681,
      "learning_rate": 0.00013447263017356476,
      "loss": 0.318,
      "step": 1232
    },
    {
      "epoch": 0.3288,
      "grad_norm": 0.0547235943377018,
      "learning_rate": 0.0001344192256341789,
      "loss": 0.262,
      "step": 1233
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.09127293527126312,
      "learning_rate": 0.00013436582109479305,
      "loss": 0.2982,
      "step": 1234
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.06132272258400917,
      "learning_rate": 0.0001343124165554072,
      "loss": 0.3013,
      "step": 1235
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.06885690242052078,
      "learning_rate": 0.0001342590120160214,
      "loss": 0.3097,
      "step": 1236
    },
    {
      "epoch": 0.32986666666666664,
      "grad_norm": 0.062303368002176285,
      "learning_rate": 0.00013420560747663554,
      "loss": 0.3877,
      "step": 1237
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.07420872896909714,
      "learning_rate": 0.00013415220293724967,
      "loss": 0.3233,
      "step": 1238
    },
    {
      "epoch": 0.3304,
      "grad_norm": 0.07480204850435257,
      "learning_rate": 0.00013409879839786383,
      "loss": 0.3797,
      "step": 1239
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.05938045680522919,
      "learning_rate": 0.00013404539385847799,
      "loss": 0.3179,
      "step": 1240
    },
    {
      "epoch": 0.33093333333333336,
      "grad_norm": 0.056985653936862946,
      "learning_rate": 0.00013399198931909214,
      "loss": 0.3112,
      "step": 1241
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.06845220178365707,
      "learning_rate": 0.0001339385847797063,
      "loss": 0.392,
      "step": 1242
    },
    {
      "epoch": 0.3314666666666667,
      "grad_norm": 0.06354344636201859,
      "learning_rate": 0.00013388518024032043,
      "loss": 0.2983,
      "step": 1243
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.04746983200311661,
      "learning_rate": 0.00013383177570093458,
      "loss": 0.2512,
      "step": 1244
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.06537669152021408,
      "learning_rate": 0.00013377837116154874,
      "loss": 0.3775,
      "step": 1245
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.05121348798274994,
      "learning_rate": 0.0001337249666221629,
      "loss": 0.3275,
      "step": 1246
    },
    {
      "epoch": 0.33253333333333335,
      "grad_norm": 0.059040963649749756,
      "learning_rate": 0.00013367156208277705,
      "loss": 0.3141,
      "step": 1247
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.048537421971559525,
      "learning_rate": 0.0001336181575433912,
      "loss": 0.3318,
      "step": 1248
    },
    {
      "epoch": 0.3330666666666667,
      "grad_norm": 0.06081146001815796,
      "learning_rate": 0.00013356475300400534,
      "loss": 0.3279,
      "step": 1249
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05972672626376152,
      "learning_rate": 0.0001335113484646195,
      "loss": 0.3285,
      "step": 1250
    },
    {
      "epoch": 0.3336,
      "grad_norm": 0.06257440894842148,
      "learning_rate": 0.00013345794392523365,
      "loss": 0.2991,
      "step": 1251
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.05447505787014961,
      "learning_rate": 0.0001334045393858478,
      "loss": 0.2892,
      "step": 1252
    },
    {
      "epoch": 0.33413333333333334,
      "grad_norm": 0.055259719491004944,
      "learning_rate": 0.00013335113484646196,
      "loss": 0.3304,
      "step": 1253
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.060410067439079285,
      "learning_rate": 0.00013329773030707612,
      "loss": 0.3475,
      "step": 1254
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.06460214406251907,
      "learning_rate": 0.00013324432576769025,
      "loss": 0.3745,
      "step": 1255
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.06379414349794388,
      "learning_rate": 0.0001331909212283044,
      "loss": 0.3222,
      "step": 1256
    },
    {
      "epoch": 0.3352,
      "grad_norm": 0.0563228614628315,
      "learning_rate": 0.00013313751668891856,
      "loss": 0.2566,
      "step": 1257
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.060039009898900986,
      "learning_rate": 0.00013308411214953272,
      "loss": 0.3345,
      "step": 1258
    },
    {
      "epoch": 0.33573333333333333,
      "grad_norm": 0.06900875270366669,
      "learning_rate": 0.00013303070761014687,
      "loss": 0.3394,
      "step": 1259
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.06527948379516602,
      "learning_rate": 0.000132977303070761,
      "loss": 0.2603,
      "step": 1260
    },
    {
      "epoch": 0.33626666666666666,
      "grad_norm": 0.06261299550533295,
      "learning_rate": 0.00013292389853137516,
      "loss": 0.3175,
      "step": 1261
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.06984537094831467,
      "learning_rate": 0.00013287049399198932,
      "loss": 0.3573,
      "step": 1262
    },
    {
      "epoch": 0.3368,
      "grad_norm": 0.0609700046479702,
      "learning_rate": 0.00013281708945260347,
      "loss": 0.2954,
      "step": 1263
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.06441601365804672,
      "learning_rate": 0.00013276368491321763,
      "loss": 0.319,
      "step": 1264
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.062210869044065475,
      "learning_rate": 0.00013271028037383179,
      "loss": 0.3454,
      "step": 1265
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.06606946140527725,
      "learning_rate": 0.00013265687583444591,
      "loss": 0.275,
      "step": 1266
    },
    {
      "epoch": 0.33786666666666665,
      "grad_norm": 0.05835841968655586,
      "learning_rate": 0.00013260347129506007,
      "loss": 0.3014,
      "step": 1267
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.07146205753087997,
      "learning_rate": 0.00013255006675567423,
      "loss": 0.3872,
      "step": 1268
    },
    {
      "epoch": 0.3384,
      "grad_norm": 0.0641193613409996,
      "learning_rate": 0.00013249666221628838,
      "loss": 0.2586,
      "step": 1269
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.06357329338788986,
      "learning_rate": 0.00013244325767690254,
      "loss": 0.3542,
      "step": 1270
    },
    {
      "epoch": 0.3389333333333333,
      "grad_norm": 0.05996754392981529,
      "learning_rate": 0.0001323898531375167,
      "loss": 0.311,
      "step": 1271
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.06685177981853485,
      "learning_rate": 0.00013233644859813085,
      "loss": 0.3723,
      "step": 1272
    },
    {
      "epoch": 0.3394666666666667,
      "grad_norm": 0.06727738678455353,
      "learning_rate": 0.000132283044058745,
      "loss": 0.3943,
      "step": 1273
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.05414885655045509,
      "learning_rate": 0.00013222963951935916,
      "loss": 0.2524,
      "step": 1274
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.07865908741950989,
      "learning_rate": 0.00013217623497997332,
      "loss": 0.3232,
      "step": 1275
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.05283288285136223,
      "learning_rate": 0.00013212283044058745,
      "loss": 0.318,
      "step": 1276
    },
    {
      "epoch": 0.34053333333333335,
      "grad_norm": 0.07978688180446625,
      "learning_rate": 0.0001320694259012016,
      "loss": 0.2846,
      "step": 1277
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.05634547770023346,
      "learning_rate": 0.00013201602136181576,
      "loss": 0.3207,
      "step": 1278
    },
    {
      "epoch": 0.3410666666666667,
      "grad_norm": 0.06823878735303879,
      "learning_rate": 0.00013196261682242992,
      "loss": 0.3372,
      "step": 1279
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.06988022476434708,
      "learning_rate": 0.00013190921228304408,
      "loss": 0.3748,
      "step": 1280
    },
    {
      "epoch": 0.3416,
      "grad_norm": 0.06107247620820999,
      "learning_rate": 0.00013185580774365823,
      "loss": 0.2976,
      "step": 1281
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.06328079849481583,
      "learning_rate": 0.00013180240320427236,
      "loss": 0.3091,
      "step": 1282
    },
    {
      "epoch": 0.34213333333333334,
      "grad_norm": 0.06792032718658447,
      "learning_rate": 0.00013174899866488652,
      "loss": 0.3542,
      "step": 1283
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.08011873066425323,
      "learning_rate": 0.00013169559412550067,
      "loss": 0.3842,
      "step": 1284
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.06187251955270767,
      "learning_rate": 0.00013164218958611483,
      "loss": 0.3307,
      "step": 1285
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.06763330847024918,
      "learning_rate": 0.000131588785046729,
      "loss": 0.3426,
      "step": 1286
    },
    {
      "epoch": 0.3432,
      "grad_norm": 0.05438258498907089,
      "learning_rate": 0.00013153538050734314,
      "loss": 0.2707,
      "step": 1287
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.05872199684381485,
      "learning_rate": 0.00013148197596795727,
      "loss": 0.3124,
      "step": 1288
    },
    {
      "epoch": 0.34373333333333334,
      "grad_norm": 0.06616108864545822,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.2911,
      "step": 1289
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.07442876696586609,
      "learning_rate": 0.00013137516688918559,
      "loss": 0.4199,
      "step": 1290
    },
    {
      "epoch": 0.34426666666666667,
      "grad_norm": 0.07588093727827072,
      "learning_rate": 0.00013132176234979974,
      "loss": 0.2959,
      "step": 1291
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.06495486199855804,
      "learning_rate": 0.0001312683578104139,
      "loss": 0.3577,
      "step": 1292
    },
    {
      "epoch": 0.3448,
      "grad_norm": 0.044555045664310455,
      "learning_rate": 0.00013121495327102805,
      "loss": 0.2972,
      "step": 1293
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.04869595915079117,
      "learning_rate": 0.00013116154873164218,
      "loss": 0.3021,
      "step": 1294
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.05980495363473892,
      "learning_rate": 0.00013110814419225634,
      "loss": 0.3117,
      "step": 1295
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.08124250173568726,
      "learning_rate": 0.0001310547396528705,
      "loss": 0.3389,
      "step": 1296
    },
    {
      "epoch": 0.34586666666666666,
      "grad_norm": 0.0611608512699604,
      "learning_rate": 0.00013100133511348465,
      "loss": 0.3323,
      "step": 1297
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.05183875188231468,
      "learning_rate": 0.0001309479305740988,
      "loss": 0.2596,
      "step": 1298
    },
    {
      "epoch": 0.3464,
      "grad_norm": 0.056661173701286316,
      "learning_rate": 0.00013089452603471294,
      "loss": 0.371,
      "step": 1299
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.06530727446079254,
      "learning_rate": 0.0001308411214953271,
      "loss": 0.3347,
      "step": 1300
    },
    {
      "epoch": 0.3469333333333333,
      "grad_norm": 0.056473568081855774,
      "learning_rate": 0.00013078771695594125,
      "loss": 0.2799,
      "step": 1301
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.06341703236103058,
      "learning_rate": 0.0001307343124165554,
      "loss": 0.3902,
      "step": 1302
    },
    {
      "epoch": 0.34746666666666665,
      "grad_norm": 0.08750025928020477,
      "learning_rate": 0.00013068090787716956,
      "loss": 0.4119,
      "step": 1303
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.05511658638715744,
      "learning_rate": 0.00013062750333778372,
      "loss": 0.2913,
      "step": 1304
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.0689530223608017,
      "learning_rate": 0.00013057409879839785,
      "loss": 0.3469,
      "step": 1305
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.0672469511628151,
      "learning_rate": 0.00013052069425901203,
      "loss": 0.3289,
      "step": 1306
    },
    {
      "epoch": 0.3485333333333333,
      "grad_norm": 0.06367912143468857,
      "learning_rate": 0.0001304672897196262,
      "loss": 0.3354,
      "step": 1307
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.059535443782806396,
      "learning_rate": 0.00013041388518024034,
      "loss": 0.2781,
      "step": 1308
    },
    {
      "epoch": 0.3490666666666667,
      "grad_norm": 0.0804111510515213,
      "learning_rate": 0.0001303604806408545,
      "loss": 0.3906,
      "step": 1309
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.06763431429862976,
      "learning_rate": 0.00013030707610146863,
      "loss": 0.3308,
      "step": 1310
    },
    {
      "epoch": 0.3496,
      "grad_norm": 0.0526733361184597,
      "learning_rate": 0.0001302536715620828,
      "loss": 0.2658,
      "step": 1311
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.07247716933488846,
      "learning_rate": 0.00013020026702269694,
      "loss": 0.3922,
      "step": 1312
    },
    {
      "epoch": 0.35013333333333335,
      "grad_norm": 0.0669400617480278,
      "learning_rate": 0.0001301468624833111,
      "loss": 0.3212,
      "step": 1313
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.06698792427778244,
      "learning_rate": 0.00013009345794392526,
      "loss": 0.3661,
      "step": 1314
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.06972913444042206,
      "learning_rate": 0.00013004005340453938,
      "loss": 0.3812,
      "step": 1315
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.05576160550117493,
      "learning_rate": 0.00012998664886515354,
      "loss": 0.319,
      "step": 1316
    },
    {
      "epoch": 0.3512,
      "grad_norm": 0.05955864489078522,
      "learning_rate": 0.0001299332443257677,
      "loss": 0.3244,
      "step": 1317
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.07007205486297607,
      "learning_rate": 0.00012987983978638185,
      "loss": 0.3458,
      "step": 1318
    },
    {
      "epoch": 0.35173333333333334,
      "grad_norm": 0.06047246605157852,
      "learning_rate": 0.000129826435246996,
      "loss": 0.3538,
      "step": 1319
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.06938895583152771,
      "learning_rate": 0.00012977303070761017,
      "loss": 0.3286,
      "step": 1320
    },
    {
      "epoch": 0.3522666666666667,
      "grad_norm": 0.06391724944114685,
      "learning_rate": 0.0001297196261682243,
      "loss": 0.3679,
      "step": 1321
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.06912291049957275,
      "learning_rate": 0.00012966622162883845,
      "loss": 0.3797,
      "step": 1322
    },
    {
      "epoch": 0.3528,
      "grad_norm": 0.06808339804410934,
      "learning_rate": 0.0001296128170894526,
      "loss": 0.4102,
      "step": 1323
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.054233331233263016,
      "learning_rate": 0.00012955941255006676,
      "loss": 0.3249,
      "step": 1324
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.06231587007641792,
      "learning_rate": 0.00012950600801068092,
      "loss": 0.3296,
      "step": 1325
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.07716379314661026,
      "learning_rate": 0.00012945260347129508,
      "loss": 0.2655,
      "step": 1326
    },
    {
      "epoch": 0.35386666666666666,
      "grad_norm": 0.06078747287392616,
      "learning_rate": 0.0001293991989319092,
      "loss": 0.3758,
      "step": 1327
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.06575173139572144,
      "learning_rate": 0.00012934579439252336,
      "loss": 0.3833,
      "step": 1328
    },
    {
      "epoch": 0.3544,
      "grad_norm": 0.07115989178419113,
      "learning_rate": 0.00012929238985313752,
      "loss": 0.3571,
      "step": 1329
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.07315856963396072,
      "learning_rate": 0.00012923898531375168,
      "loss": 0.4244,
      "step": 1330
    },
    {
      "epoch": 0.3549333333333333,
      "grad_norm": 0.07096440345048904,
      "learning_rate": 0.00012918558077436583,
      "loss": 0.3236,
      "step": 1331
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.05855315551161766,
      "learning_rate": 0.00012913217623497996,
      "loss": 0.3282,
      "step": 1332
    },
    {
      "epoch": 0.35546666666666665,
      "grad_norm": 0.05617103725671768,
      "learning_rate": 0.00012907877169559412,
      "loss": 0.2965,
      "step": 1333
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.06034928187727928,
      "learning_rate": 0.00012902536715620827,
      "loss": 0.2544,
      "step": 1334
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.051006097346544266,
      "learning_rate": 0.00012897196261682243,
      "loss": 0.2989,
      "step": 1335
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.07880070060491562,
      "learning_rate": 0.0001289185580774366,
      "loss": 0.3902,
      "step": 1336
    },
    {
      "epoch": 0.3565333333333333,
      "grad_norm": 0.06462255120277405,
      "learning_rate": 0.00012886515353805074,
      "loss": 0.3375,
      "step": 1337
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.0582280158996582,
      "learning_rate": 0.00012881174899866487,
      "loss": 0.2981,
      "step": 1338
    },
    {
      "epoch": 0.35706666666666664,
      "grad_norm": 0.054418835788965225,
      "learning_rate": 0.00012875834445927903,
      "loss": 0.2845,
      "step": 1339
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.059004057198762894,
      "learning_rate": 0.00012870493991989318,
      "loss": 0.3182,
      "step": 1340
    },
    {
      "epoch": 0.3576,
      "grad_norm": 0.06856931000947952,
      "learning_rate": 0.00012865153538050737,
      "loss": 0.3633,
      "step": 1341
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.050946854054927826,
      "learning_rate": 0.00012859813084112152,
      "loss": 0.2636,
      "step": 1342
    },
    {
      "epoch": 0.35813333333333336,
      "grad_norm": 0.062276165932416916,
      "learning_rate": 0.00012854472630173565,
      "loss": 0.3245,
      "step": 1343
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.057517118752002716,
      "learning_rate": 0.0001284913217623498,
      "loss": 0.2907,
      "step": 1344
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.049320757389068604,
      "learning_rate": 0.00012843791722296397,
      "loss": 0.3008,
      "step": 1345
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.062557153403759,
      "learning_rate": 0.00012838451268357812,
      "loss": 0.3749,
      "step": 1346
    },
    {
      "epoch": 0.3592,
      "grad_norm": 0.050774700939655304,
      "learning_rate": 0.00012833110814419228,
      "loss": 0.2907,
      "step": 1347
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.052473992109298706,
      "learning_rate": 0.0001282777036048064,
      "loss": 0.3308,
      "step": 1348
    },
    {
      "epoch": 0.35973333333333335,
      "grad_norm": 0.07765430957078934,
      "learning_rate": 0.00012822429906542056,
      "loss": 0.3895,
      "step": 1349
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.06120366230607033,
      "learning_rate": 0.00012817089452603472,
      "loss": 0.3555,
      "step": 1350
    },
    {
      "epoch": 0.3602666666666667,
      "grad_norm": 0.0410136803984642,
      "learning_rate": 0.00012811748998664888,
      "loss": 0.237,
      "step": 1351
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.06957694888114929,
      "learning_rate": 0.00012806408544726303,
      "loss": 0.3883,
      "step": 1352
    },
    {
      "epoch": 0.3608,
      "grad_norm": 0.0597413070499897,
      "learning_rate": 0.0001280106809078772,
      "loss": 0.3382,
      "step": 1353
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.0571049302816391,
      "learning_rate": 0.00012795727636849132,
      "loss": 0.3394,
      "step": 1354
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.059317342936992645,
      "learning_rate": 0.00012790387182910548,
      "loss": 0.3295,
      "step": 1355
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.05692492425441742,
      "learning_rate": 0.00012785046728971963,
      "loss": 0.3103,
      "step": 1356
    },
    {
      "epoch": 0.36186666666666667,
      "grad_norm": 0.07006081193685532,
      "learning_rate": 0.0001277970627503338,
      "loss": 0.4015,
      "step": 1357
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.06218891218304634,
      "learning_rate": 0.00012774365821094794,
      "loss": 0.3763,
      "step": 1358
    },
    {
      "epoch": 0.3624,
      "grad_norm": 0.05124988779425621,
      "learning_rate": 0.0001276902536715621,
      "loss": 0.3017,
      "step": 1359
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.05195429176092148,
      "learning_rate": 0.00012763684913217623,
      "loss": 0.3299,
      "step": 1360
    },
    {
      "epoch": 0.36293333333333333,
      "grad_norm": 0.07320646941661835,
      "learning_rate": 0.0001275834445927904,
      "loss": 0.3542,
      "step": 1361
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.060338933020830154,
      "learning_rate": 0.00012753004005340454,
      "loss": 0.3324,
      "step": 1362
    },
    {
      "epoch": 0.36346666666666666,
      "grad_norm": 0.055520690977573395,
      "learning_rate": 0.0001274766355140187,
      "loss": 0.3113,
      "step": 1363
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.06978434324264526,
      "learning_rate": 0.00012742323097463286,
      "loss": 0.3275,
      "step": 1364
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.05984038487076759,
      "learning_rate": 0.00012736982643524698,
      "loss": 0.302,
      "step": 1365
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.08678030222654343,
      "learning_rate": 0.00012731642189586114,
      "loss": 0.3602,
      "step": 1366
    },
    {
      "epoch": 0.3645333333333333,
      "grad_norm": 0.054032739251852036,
      "learning_rate": 0.0001272630173564753,
      "loss": 0.2535,
      "step": 1367
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.09499543160200119,
      "learning_rate": 0.00012720961281708945,
      "loss": 0.4468,
      "step": 1368
    },
    {
      "epoch": 0.36506666666666665,
      "grad_norm": 0.05585523694753647,
      "learning_rate": 0.0001271562082777036,
      "loss": 0.2732,
      "step": 1369
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.0929914340376854,
      "learning_rate": 0.00012710280373831777,
      "loss": 0.3001,
      "step": 1370
    },
    {
      "epoch": 0.3656,
      "grad_norm": 0.058111246675252914,
      "learning_rate": 0.0001270493991989319,
      "loss": 0.3064,
      "step": 1371
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.0579741969704628,
      "learning_rate": 0.00012699599465954605,
      "loss": 0.3245,
      "step": 1372
    },
    {
      "epoch": 0.3661333333333333,
      "grad_norm": 0.06953153759241104,
      "learning_rate": 0.0001269425901201602,
      "loss": 0.3432,
      "step": 1373
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.05803076550364494,
      "learning_rate": 0.00012688918558077436,
      "loss": 0.2834,
      "step": 1374
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.05344618484377861,
      "learning_rate": 0.00012683578104138855,
      "loss": 0.3068,
      "step": 1375
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.05781693384051323,
      "learning_rate": 0.00012678237650200268,
      "loss": 0.319,
      "step": 1376
    },
    {
      "epoch": 0.3672,
      "grad_norm": 0.07814262062311172,
      "learning_rate": 0.00012672897196261683,
      "loss": 0.4013,
      "step": 1377
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.044886596500873566,
      "learning_rate": 0.000126675567423231,
      "loss": 0.2707,
      "step": 1378
    },
    {
      "epoch": 0.36773333333333336,
      "grad_norm": 0.053706616163253784,
      "learning_rate": 0.00012662216288384515,
      "loss": 0.3091,
      "step": 1379
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.0805421769618988,
      "learning_rate": 0.0001265687583444593,
      "loss": 0.396,
      "step": 1380
    },
    {
      "epoch": 0.3682666666666667,
      "grad_norm": 0.0529852919280529,
      "learning_rate": 0.00012651535380507343,
      "loss": 0.34,
      "step": 1381
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.060075193643569946,
      "learning_rate": 0.0001264619492656876,
      "loss": 0.3498,
      "step": 1382
    },
    {
      "epoch": 0.3688,
      "grad_norm": 0.05839712917804718,
      "learning_rate": 0.00012640854472630174,
      "loss": 0.3384,
      "step": 1383
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.04992634430527687,
      "learning_rate": 0.0001263551401869159,
      "loss": 0.2799,
      "step": 1384
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 0.06605444103479385,
      "learning_rate": 0.00012630173564753006,
      "loss": 0.3362,
      "step": 1385
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.06615228950977325,
      "learning_rate": 0.0001262483311081442,
      "loss": 0.283,
      "step": 1386
    },
    {
      "epoch": 0.3698666666666667,
      "grad_norm": 0.058935098350048065,
      "learning_rate": 0.00012619492656875834,
      "loss": 0.342,
      "step": 1387
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.0521317720413208,
      "learning_rate": 0.0001261415220293725,
      "loss": 0.3588,
      "step": 1388
    },
    {
      "epoch": 0.3704,
      "grad_norm": 0.050209470093250275,
      "learning_rate": 0.00012608811748998666,
      "loss": 0.3529,
      "step": 1389
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.047687117010354996,
      "learning_rate": 0.0001260347129506008,
      "loss": 0.2959,
      "step": 1390
    },
    {
      "epoch": 0.37093333333333334,
      "grad_norm": 0.07276923954486847,
      "learning_rate": 0.00012598130841121497,
      "loss": 0.4105,
      "step": 1391
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.07537892460823059,
      "learning_rate": 0.00012592790387182912,
      "loss": 0.4172,
      "step": 1392
    },
    {
      "epoch": 0.37146666666666667,
      "grad_norm": 0.05429767072200775,
      "learning_rate": 0.00012587449933244325,
      "loss": 0.2785,
      "step": 1393
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.06955639272928238,
      "learning_rate": 0.0001258210947930574,
      "loss": 0.3155,
      "step": 1394
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.050413940101861954,
      "learning_rate": 0.00012576769025367157,
      "loss": 0.3358,
      "step": 1395
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.04902634397149086,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.2686,
      "step": 1396
    },
    {
      "epoch": 0.3725333333333333,
      "grad_norm": 0.07215770334005356,
      "learning_rate": 0.00012566088117489988,
      "loss": 0.33,
      "step": 1397
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.04121732339262962,
      "learning_rate": 0.000125607476635514,
      "loss": 0.2561,
      "step": 1398
    },
    {
      "epoch": 0.37306666666666666,
      "grad_norm": 0.06429517269134521,
      "learning_rate": 0.00012555407209612816,
      "loss": 0.3226,
      "step": 1399
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.052286162972450256,
      "learning_rate": 0.00012550066755674232,
      "loss": 0.2861,
      "step": 1400
    },
    {
      "epoch": 0.3736,
      "grad_norm": 0.08042081445455551,
      "learning_rate": 0.00012544726301735648,
      "loss": 0.3547,
      "step": 1401
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.06216096132993698,
      "learning_rate": 0.00012539385847797063,
      "loss": 0.2973,
      "step": 1402
    },
    {
      "epoch": 0.3741333333333333,
      "grad_norm": 0.0632333755493164,
      "learning_rate": 0.0001253404539385848,
      "loss": 0.3145,
      "step": 1403
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.05849790945649147,
      "learning_rate": 0.00012528704939919892,
      "loss": 0.3327,
      "step": 1404
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.06759673357009888,
      "learning_rate": 0.00012523364485981308,
      "loss": 0.3918,
      "step": 1405
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.06600145250558853,
      "learning_rate": 0.00012518024032042723,
      "loss": 0.4413,
      "step": 1406
    },
    {
      "epoch": 0.3752,
      "grad_norm": 0.07266248017549515,
      "learning_rate": 0.0001251268357810414,
      "loss": 0.3478,
      "step": 1407
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.07663781940937042,
      "learning_rate": 0.00012507343124165554,
      "loss": 0.3577,
      "step": 1408
    },
    {
      "epoch": 0.3757333333333333,
      "grad_norm": 0.08195891231298447,
      "learning_rate": 0.0001250200267022697,
      "loss": 0.2934,
      "step": 1409
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.06447643786668777,
      "learning_rate": 0.00012496662216288386,
      "loss": 0.2834,
      "step": 1410
    },
    {
      "epoch": 0.3762666666666667,
      "grad_norm": 0.09160066395998001,
      "learning_rate": 0.000124913217623498,
      "loss": 0.3356,
      "step": 1411
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.08504050970077515,
      "learning_rate": 0.00012485981308411217,
      "loss": 0.3391,
      "step": 1412
    },
    {
      "epoch": 0.3768,
      "grad_norm": 0.05412517860531807,
      "learning_rate": 0.00012480640854472633,
      "loss": 0.2913,
      "step": 1413
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.0518077127635479,
      "learning_rate": 0.00012475300400534046,
      "loss": 0.3152,
      "step": 1414
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.05488555133342743,
      "learning_rate": 0.0001246995994659546,
      "loss": 0.2943,
      "step": 1415
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.06536974757909775,
      "learning_rate": 0.00012464619492656877,
      "loss": 0.3026,
      "step": 1416
    },
    {
      "epoch": 0.3778666666666667,
      "grad_norm": 0.05732326954603195,
      "learning_rate": 0.00012459279038718292,
      "loss": 0.2463,
      "step": 1417
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.08465509116649628,
      "learning_rate": 0.00012453938584779708,
      "loss": 0.3079,
      "step": 1418
    },
    {
      "epoch": 0.3784,
      "grad_norm": 0.06760886311531067,
      "learning_rate": 0.00012448598130841124,
      "loss": 0.4325,
      "step": 1419
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.05848197638988495,
      "learning_rate": 0.00012443257676902537,
      "loss": 0.2737,
      "step": 1420
    },
    {
      "epoch": 0.37893333333333334,
      "grad_norm": 0.08123359084129333,
      "learning_rate": 0.00012437917222963952,
      "loss": 0.3718,
      "step": 1421
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.08074753731489182,
      "learning_rate": 0.00012432576769025368,
      "loss": 0.3448,
      "step": 1422
    },
    {
      "epoch": 0.3794666666666667,
      "grad_norm": 0.05303777754306793,
      "learning_rate": 0.00012427236315086784,
      "loss": 0.2422,
      "step": 1423
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.04895983636379242,
      "learning_rate": 0.000124218958611482,
      "loss": 0.2843,
      "step": 1424
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.06421887129545212,
      "learning_rate": 0.00012416555407209615,
      "loss": 0.2893,
      "step": 1425
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.044275347143411636,
      "learning_rate": 0.00012411214953271028,
      "loss": 0.262,
      "step": 1426
    },
    {
      "epoch": 0.38053333333333333,
      "grad_norm": 0.05019005760550499,
      "learning_rate": 0.00012405874499332443,
      "loss": 0.3043,
      "step": 1427
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.08222441375255585,
      "learning_rate": 0.0001240053404539386,
      "loss": 0.3893,
      "step": 1428
    },
    {
      "epoch": 0.38106666666666666,
      "grad_norm": 0.05574163794517517,
      "learning_rate": 0.00012395193591455275,
      "loss": 0.235,
      "step": 1429
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.06130244582891464,
      "learning_rate": 0.0001238985313751669,
      "loss": 0.3834,
      "step": 1430
    },
    {
      "epoch": 0.3816,
      "grad_norm": 0.058591559529304504,
      "learning_rate": 0.00012384512683578103,
      "loss": 0.3191,
      "step": 1431
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.04968021437525749,
      "learning_rate": 0.0001237917222963952,
      "loss": 0.2863,
      "step": 1432
    },
    {
      "epoch": 0.3821333333333333,
      "grad_norm": 0.04637040197849274,
      "learning_rate": 0.00012373831775700934,
      "loss": 0.2821,
      "step": 1433
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.11048147827386856,
      "learning_rate": 0.0001236849132176235,
      "loss": 0.3716,
      "step": 1434
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.06005575880408287,
      "learning_rate": 0.00012363150867823766,
      "loss": 0.3786,
      "step": 1435
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.06264922767877579,
      "learning_rate": 0.0001235781041388518,
      "loss": 0.3493,
      "step": 1436
    },
    {
      "epoch": 0.3832,
      "grad_norm": 0.06922733783721924,
      "learning_rate": 0.00012352469959946594,
      "loss": 0.3277,
      "step": 1437
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.09212552011013031,
      "learning_rate": 0.0001234712950600801,
      "loss": 0.3298,
      "step": 1438
    },
    {
      "epoch": 0.3837333333333333,
      "grad_norm": 0.05521707609295845,
      "learning_rate": 0.00012341789052069426,
      "loss": 0.2807,
      "step": 1439
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.07865595072507858,
      "learning_rate": 0.0001233644859813084,
      "loss": 0.3837,
      "step": 1440
    },
    {
      "epoch": 0.38426666666666665,
      "grad_norm": 0.06251365691423416,
      "learning_rate": 0.00012331108144192257,
      "loss": 0.3008,
      "step": 1441
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.05210570991039276,
      "learning_rate": 0.00012325767690253672,
      "loss": 0.2859,
      "step": 1442
    },
    {
      "epoch": 0.3848,
      "grad_norm": 0.06350023299455643,
      "learning_rate": 0.00012320427236315085,
      "loss": 0.3369,
      "step": 1443
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.049352943897247314,
      "learning_rate": 0.000123150867823765,
      "loss": 0.2826,
      "step": 1444
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 0.0848701149225235,
      "learning_rate": 0.0001230974632843792,
      "loss": 0.3551,
      "step": 1445
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.061777856200933456,
      "learning_rate": 0.00012304405874499335,
      "loss": 0.2866,
      "step": 1446
    },
    {
      "epoch": 0.3858666666666667,
      "grad_norm": 0.061776284128427505,
      "learning_rate": 0.00012299065420560748,
      "loss": 0.3397,
      "step": 1447
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.056961994618177414,
      "learning_rate": 0.00012293724966622164,
      "loss": 0.3291,
      "step": 1448
    },
    {
      "epoch": 0.3864,
      "grad_norm": 0.05971767008304596,
      "learning_rate": 0.0001228838451268358,
      "loss": 0.3416,
      "step": 1449
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.05939321964979172,
      "learning_rate": 0.00012283044058744995,
      "loss": 0.3044,
      "step": 1450
    },
    {
      "epoch": 0.38693333333333335,
      "grad_norm": 0.05869133025407791,
      "learning_rate": 0.0001227770360480641,
      "loss": 0.2967,
      "step": 1451
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.05722855404019356,
      "learning_rate": 0.00012272363150867826,
      "loss": 0.3025,
      "step": 1452
    },
    {
      "epoch": 0.3874666666666667,
      "grad_norm": 0.05426274612545967,
      "learning_rate": 0.0001226702269692924,
      "loss": 0.3206,
      "step": 1453
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.058647528290748596,
      "learning_rate": 0.00012261682242990655,
      "loss": 0.3614,
      "step": 1454
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.06551716476678848,
      "learning_rate": 0.0001225634178905207,
      "loss": 0.3614,
      "step": 1455
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.07937531173229218,
      "learning_rate": 0.00012251001335113486,
      "loss": 0.3242,
      "step": 1456
    },
    {
      "epoch": 0.38853333333333334,
      "grad_norm": 0.06963635981082916,
      "learning_rate": 0.00012245660881174902,
      "loss": 0.3678,
      "step": 1457
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.07023852318525314,
      "learning_rate": 0.00012240320427236317,
      "loss": 0.4104,
      "step": 1458
    },
    {
      "epoch": 0.38906666666666667,
      "grad_norm": 0.060199934989213943,
      "learning_rate": 0.0001223497997329773,
      "loss": 0.3719,
      "step": 1459
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.05317392945289612,
      "learning_rate": 0.00012229639519359146,
      "loss": 0.3019,
      "step": 1460
    },
    {
      "epoch": 0.3896,
      "grad_norm": 0.048679087311029434,
      "learning_rate": 0.0001222429906542056,
      "loss": 0.3093,
      "step": 1461
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.057655248790979385,
      "learning_rate": 0.00012218958611481977,
      "loss": 0.3663,
      "step": 1462
    },
    {
      "epoch": 0.39013333333333333,
      "grad_norm": 0.05392909795045853,
      "learning_rate": 0.00012213618157543393,
      "loss": 0.2875,
      "step": 1463
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.06918700784444809,
      "learning_rate": 0.00012208277703604806,
      "loss": 0.3222,
      "step": 1464
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 0.06630835682153702,
      "learning_rate": 0.00012202937249666223,
      "loss": 0.3735,
      "step": 1465
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.057670798152685165,
      "learning_rate": 0.00012197596795727637,
      "loss": 0.3907,
      "step": 1466
    },
    {
      "epoch": 0.3912,
      "grad_norm": 0.057850006967782974,
      "learning_rate": 0.00012192256341789052,
      "loss": 0.2875,
      "step": 1467
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.06446364521980286,
      "learning_rate": 0.00012186915887850468,
      "loss": 0.3413,
      "step": 1468
    },
    {
      "epoch": 0.3917333333333333,
      "grad_norm": 0.06708066165447235,
      "learning_rate": 0.00012181575433911882,
      "loss": 0.3445,
      "step": 1469
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.07219486683607101,
      "learning_rate": 0.00012176234979973298,
      "loss": 0.3207,
      "step": 1470
    },
    {
      "epoch": 0.39226666666666665,
      "grad_norm": 0.06139172241091728,
      "learning_rate": 0.00012170894526034714,
      "loss": 0.3366,
      "step": 1471
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.06066446378827095,
      "learning_rate": 0.00012165554072096128,
      "loss": 0.3133,
      "step": 1472
    },
    {
      "epoch": 0.3928,
      "grad_norm": 0.0901591032743454,
      "learning_rate": 0.00012160213618157544,
      "loss": 0.4248,
      "step": 1473
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.07809210568666458,
      "learning_rate": 0.00012154873164218958,
      "loss": 0.3813,
      "step": 1474
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.06712842732667923,
      "learning_rate": 0.00012149532710280373,
      "loss": 0.3315,
      "step": 1475
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.05615214258432388,
      "learning_rate": 0.00012144192256341789,
      "loss": 0.3315,
      "step": 1476
    },
    {
      "epoch": 0.39386666666666664,
      "grad_norm": 0.07422196120023727,
      "learning_rate": 0.00012138851802403203,
      "loss": 0.2652,
      "step": 1477
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.0646897703409195,
      "learning_rate": 0.00012133511348464619,
      "loss": 0.3198,
      "step": 1478
    },
    {
      "epoch": 0.3944,
      "grad_norm": 0.06477710604667664,
      "learning_rate": 0.00012128170894526036,
      "loss": 0.2996,
      "step": 1479
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.0627635046839714,
      "learning_rate": 0.00012122830440587452,
      "loss": 0.2241,
      "step": 1480
    },
    {
      "epoch": 0.39493333333333336,
      "grad_norm": 0.06568373739719391,
      "learning_rate": 0.00012117489986648867,
      "loss": 0.3123,
      "step": 1481
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.0524199903011322,
      "learning_rate": 0.00012112149532710281,
      "loss": 0.2532,
      "step": 1482
    },
    {
      "epoch": 0.3954666666666667,
      "grad_norm": 0.06702849268913269,
      "learning_rate": 0.00012106809078771697,
      "loss": 0.3187,
      "step": 1483
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.0633050724864006,
      "learning_rate": 0.00012101468624833113,
      "loss": 0.3492,
      "step": 1484
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.05699915811419487,
      "learning_rate": 0.00012096128170894527,
      "loss": 0.3019,
      "step": 1485
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.08078477531671524,
      "learning_rate": 0.00012090787716955943,
      "loss": 0.3749,
      "step": 1486
    },
    {
      "epoch": 0.39653333333333335,
      "grad_norm": 0.06387370079755783,
      "learning_rate": 0.00012085447263017358,
      "loss": 0.3077,
      "step": 1487
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.0620572566986084,
      "learning_rate": 0.00012080106809078773,
      "loss": 0.3223,
      "step": 1488
    },
    {
      "epoch": 0.3970666666666667,
      "grad_norm": 0.05546221137046814,
      "learning_rate": 0.00012074766355140188,
      "loss": 0.3029,
      "step": 1489
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.06774169951677322,
      "learning_rate": 0.00012069425901201602,
      "loss": 0.3581,
      "step": 1490
    },
    {
      "epoch": 0.3976,
      "grad_norm": 0.0576593354344368,
      "learning_rate": 0.00012064085447263018,
      "loss": 0.3185,
      "step": 1491
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.06942460685968399,
      "learning_rate": 0.00012058744993324434,
      "loss": 0.2728,
      "step": 1492
    },
    {
      "epoch": 0.39813333333333334,
      "grad_norm": 0.06288208812475204,
      "learning_rate": 0.00012053404539385848,
      "loss": 0.316,
      "step": 1493
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.049373943358659744,
      "learning_rate": 0.00012048064085447264,
      "loss": 0.2979,
      "step": 1494
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.049689196050167084,
      "learning_rate": 0.00012042723631508679,
      "loss": 0.3118,
      "step": 1495
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.06559206545352936,
      "learning_rate": 0.00012037383177570094,
      "loss": 0.3547,
      "step": 1496
    },
    {
      "epoch": 0.3992,
      "grad_norm": 0.05080946907401085,
      "learning_rate": 0.00012032042723631509,
      "loss": 0.3191,
      "step": 1497
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.05740411952137947,
      "learning_rate": 0.00012026702269692925,
      "loss": 0.35,
      "step": 1498
    },
    {
      "epoch": 0.39973333333333333,
      "grad_norm": 0.048390477895736694,
      "learning_rate": 0.00012021361815754339,
      "loss": 0.3065,
      "step": 1499
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07382179796695709,
      "learning_rate": 0.00012016021361815755,
      "loss": 0.3955,
      "step": 1500
    },
    {
      "epoch": 0.40026666666666666,
      "grad_norm": 0.05323496088385582,
      "learning_rate": 0.0001201068090787717,
      "loss": 0.2353,
      "step": 1501
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.05854908004403114,
      "learning_rate": 0.00012005340453938585,
      "loss": 0.3394,
      "step": 1502
    },
    {
      "epoch": 0.4008,
      "grad_norm": 0.05849643424153328,
      "learning_rate": 0.00012,
      "loss": 0.329,
      "step": 1503
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.05838862806558609,
      "learning_rate": 0.00011994659546061416,
      "loss": 0.3035,
      "step": 1504
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.0648280531167984,
      "learning_rate": 0.0001198931909212283,
      "loss": 0.3294,
      "step": 1505
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.0635860338807106,
      "learning_rate": 0.00011983978638184246,
      "loss": 0.3471,
      "step": 1506
    },
    {
      "epoch": 0.40186666666666665,
      "grad_norm": 0.07437004148960114,
      "learning_rate": 0.0001197863818424566,
      "loss": 0.3315,
      "step": 1507
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.09423940628767014,
      "learning_rate": 0.00011973297730307076,
      "loss": 0.3411,
      "step": 1508
    },
    {
      "epoch": 0.4024,
      "grad_norm": 0.07259566336870193,
      "learning_rate": 0.00011967957276368491,
      "loss": 0.4,
      "step": 1509
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.0645168200135231,
      "learning_rate": 0.00011962616822429906,
      "loss": 0.4067,
      "step": 1510
    },
    {
      "epoch": 0.4029333333333333,
      "grad_norm": 0.06212250515818596,
      "learning_rate": 0.00011957276368491321,
      "loss": 0.3205,
      "step": 1511
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.06128627434372902,
      "learning_rate": 0.00011951935914552737,
      "loss": 0.3226,
      "step": 1512
    },
    {
      "epoch": 0.40346666666666664,
      "grad_norm": 0.08338560163974762,
      "learning_rate": 0.00011946595460614151,
      "loss": 0.3193,
      "step": 1513
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.0819338709115982,
      "learning_rate": 0.0001194125500667557,
      "loss": 0.3543,
      "step": 1514
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.06242648884654045,
      "learning_rate": 0.00011935914552736984,
      "loss": 0.3367,
      "step": 1515
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.07193408161401749,
      "learning_rate": 0.000119305740987984,
      "loss": 0.3538,
      "step": 1516
    },
    {
      "epoch": 0.40453333333333336,
      "grad_norm": 0.0802217572927475,
      "learning_rate": 0.00011925233644859815,
      "loss": 0.3166,
      "step": 1517
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.0664571076631546,
      "learning_rate": 0.0001191989319092123,
      "loss": 0.3489,
      "step": 1518
    },
    {
      "epoch": 0.4050666666666667,
      "grad_norm": 0.07792245596647263,
      "learning_rate": 0.00011914552736982645,
      "loss": 0.3722,
      "step": 1519
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.07732876390218735,
      "learning_rate": 0.0001190921228304406,
      "loss": 0.2903,
      "step": 1520
    },
    {
      "epoch": 0.4056,
      "grad_norm": 0.0566679984331131,
      "learning_rate": 0.00011903871829105475,
      "loss": 0.2881,
      "step": 1521
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.05915263295173645,
      "learning_rate": 0.0001189853137516689,
      "loss": 0.3057,
      "step": 1522
    },
    {
      "epoch": 0.40613333333333335,
      "grad_norm": 0.0584111213684082,
      "learning_rate": 0.00011893190921228305,
      "loss": 0.3245,
      "step": 1523
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.0751003623008728,
      "learning_rate": 0.0001188785046728972,
      "loss": 0.3724,
      "step": 1524
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.043277207762002945,
      "learning_rate": 0.00011882510013351136,
      "loss": 0.2864,
      "step": 1525
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.05519914627075195,
      "learning_rate": 0.0001187716955941255,
      "loss": 0.2963,
      "step": 1526
    },
    {
      "epoch": 0.4072,
      "grad_norm": 0.05707861855626106,
      "learning_rate": 0.00011871829105473966,
      "loss": 0.3028,
      "step": 1527
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.06964876502752304,
      "learning_rate": 0.00011866488651535382,
      "loss": 0.3223,
      "step": 1528
    },
    {
      "epoch": 0.40773333333333334,
      "grad_norm": 0.06475809961557388,
      "learning_rate": 0.00011861148197596796,
      "loss": 0.3206,
      "step": 1529
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.07281330972909927,
      "learning_rate": 0.00011855807743658212,
      "loss": 0.3677,
      "step": 1530
    },
    {
      "epoch": 0.40826666666666667,
      "grad_norm": 0.05026456341147423,
      "learning_rate": 0.00011850467289719627,
      "loss": 0.293,
      "step": 1531
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.06624607741832733,
      "learning_rate": 0.00011845126835781041,
      "loss": 0.401,
      "step": 1532
    },
    {
      "epoch": 0.4088,
      "grad_norm": 0.07191386073827744,
      "learning_rate": 0.00011839786381842457,
      "loss": 0.3572,
      "step": 1533
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.05218880623579025,
      "learning_rate": 0.00011834445927903873,
      "loss": 0.3564,
      "step": 1534
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.061865705996751785,
      "learning_rate": 0.00011829105473965287,
      "loss": 0.4168,
      "step": 1535
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.05247488617897034,
      "learning_rate": 0.00011823765020026703,
      "loss": 0.3241,
      "step": 1536
    },
    {
      "epoch": 0.40986666666666666,
      "grad_norm": 0.04700260981917381,
      "learning_rate": 0.00011818424566088118,
      "loss": 0.2812,
      "step": 1537
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.061720110476017,
      "learning_rate": 0.00011813084112149533,
      "loss": 0.3222,
      "step": 1538
    },
    {
      "epoch": 0.4104,
      "grad_norm": 0.06334497034549713,
      "learning_rate": 0.00011807743658210948,
      "loss": 0.3212,
      "step": 1539
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.05116094648838043,
      "learning_rate": 0.00011802403204272362,
      "loss": 0.2906,
      "step": 1540
    },
    {
      "epoch": 0.4109333333333333,
      "grad_norm": 0.05519143491983414,
      "learning_rate": 0.00011797062750333778,
      "loss": 0.3387,
      "step": 1541
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.07651403546333313,
      "learning_rate": 0.00011791722296395194,
      "loss": 0.331,
      "step": 1542
    },
    {
      "epoch": 0.41146666666666665,
      "grad_norm": 0.05275394022464752,
      "learning_rate": 0.00011786381842456608,
      "loss": 0.2821,
      "step": 1543
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.05193027853965759,
      "learning_rate": 0.00011781041388518024,
      "loss": 0.344,
      "step": 1544
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.062304019927978516,
      "learning_rate": 0.00011775700934579439,
      "loss": 0.2382,
      "step": 1545
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.0413065068423748,
      "learning_rate": 0.00011770360480640854,
      "loss": 0.2776,
      "step": 1546
    },
    {
      "epoch": 0.4125333333333333,
      "grad_norm": 0.06443360447883606,
      "learning_rate": 0.00011765020026702269,
      "loss": 0.3285,
      "step": 1547
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.06701041758060455,
      "learning_rate": 0.00011759679572763685,
      "loss": 0.372,
      "step": 1548
    },
    {
      "epoch": 0.4130666666666667,
      "grad_norm": 0.061344414949417114,
      "learning_rate": 0.00011754339118825102,
      "loss": 0.3385,
      "step": 1549
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.05301518738269806,
      "learning_rate": 0.00011748998664886517,
      "loss": 0.2928,
      "step": 1550
    },
    {
      "epoch": 0.4136,
      "grad_norm": 0.056262705475091934,
      "learning_rate": 0.00011743658210947932,
      "loss": 0.3043,
      "step": 1551
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.048724040389060974,
      "learning_rate": 0.00011738317757009347,
      "loss": 0.2746,
      "step": 1552
    },
    {
      "epoch": 0.41413333333333335,
      "grad_norm": 0.05144873633980751,
      "learning_rate": 0.00011732977303070763,
      "loss": 0.3341,
      "step": 1553
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.053187932819128036,
      "learning_rate": 0.00011727636849132177,
      "loss": 0.3306,
      "step": 1554
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.08453661948442459,
      "learning_rate": 0.00011722296395193593,
      "loss": 0.4023,
      "step": 1555
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.06817862391471863,
      "learning_rate": 0.00011716955941255007,
      "loss": 0.3121,
      "step": 1556
    },
    {
      "epoch": 0.4152,
      "grad_norm": 0.05305018648505211,
      "learning_rate": 0.00011711615487316423,
      "loss": 0.2731,
      "step": 1557
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.0651867464184761,
      "learning_rate": 0.00011706275033377838,
      "loss": 0.3394,
      "step": 1558
    },
    {
      "epoch": 0.41573333333333334,
      "grad_norm": 0.06455747783184052,
      "learning_rate": 0.00011700934579439253,
      "loss": 0.359,
      "step": 1559
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.07609791308641434,
      "learning_rate": 0.00011695594125500668,
      "loss": 0.3612,
      "step": 1560
    },
    {
      "epoch": 0.4162666666666667,
      "grad_norm": 0.05632037669420242,
      "learning_rate": 0.00011690253671562084,
      "loss": 0.269,
      "step": 1561
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.06306445598602295,
      "learning_rate": 0.00011684913217623498,
      "loss": 0.3942,
      "step": 1562
    },
    {
      "epoch": 0.4168,
      "grad_norm": 0.05845905467867851,
      "learning_rate": 0.00011679572763684914,
      "loss": 0.2929,
      "step": 1563
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.043757714331150055,
      "learning_rate": 0.0001167423230974633,
      "loss": 0.265,
      "step": 1564
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 0.06405707448720932,
      "learning_rate": 0.00011668891855807744,
      "loss": 0.3322,
      "step": 1565
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.041448816657066345,
      "learning_rate": 0.0001166355140186916,
      "loss": 0.2956,
      "step": 1566
    },
    {
      "epoch": 0.41786666666666666,
      "grad_norm": 0.0702892318367958,
      "learning_rate": 0.00011658210947930575,
      "loss": 0.3935,
      "step": 1567
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.06439763307571411,
      "learning_rate": 0.0001165287049399199,
      "loss": 0.3619,
      "step": 1568
    },
    {
      "epoch": 0.4184,
      "grad_norm": 0.056215789169073105,
      "learning_rate": 0.00011647530040053405,
      "loss": 0.2302,
      "step": 1569
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.055950913578271866,
      "learning_rate": 0.0001164218958611482,
      "loss": 0.3038,
      "step": 1570
    },
    {
      "epoch": 0.4189333333333333,
      "grad_norm": 0.06719779968261719,
      "learning_rate": 0.00011636849132176235,
      "loss": 0.3396,
      "step": 1571
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.05934930220246315,
      "learning_rate": 0.0001163150867823765,
      "loss": 0.3073,
      "step": 1572
    },
    {
      "epoch": 0.41946666666666665,
      "grad_norm": 0.057633403688669205,
      "learning_rate": 0.00011626168224299065,
      "loss": 0.3364,
      "step": 1573
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.05202968418598175,
      "learning_rate": 0.0001162082777036048,
      "loss": 0.3061,
      "step": 1574
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.051008038222789764,
      "learning_rate": 0.00011615487316421896,
      "loss": 0.2869,
      "step": 1575
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.062075115740299225,
      "learning_rate": 0.0001161014686248331,
      "loss": 0.3599,
      "step": 1576
    },
    {
      "epoch": 0.4205333333333333,
      "grad_norm": 0.08445237576961517,
      "learning_rate": 0.00011604806408544726,
      "loss": 0.2639,
      "step": 1577
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.04594869539141655,
      "learning_rate": 0.00011599465954606142,
      "loss": 0.293,
      "step": 1578
    },
    {
      "epoch": 0.42106666666666664,
      "grad_norm": 0.05293704569339752,
      "learning_rate": 0.00011594125500667556,
      "loss": 0.3009,
      "step": 1579
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.06447562575340271,
      "learning_rate": 0.00011588785046728972,
      "loss": 0.3566,
      "step": 1580
    },
    {
      "epoch": 0.4216,
      "grad_norm": 0.06431763619184494,
      "learning_rate": 0.00011583444592790387,
      "loss": 0.3235,
      "step": 1581
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.08398499339818954,
      "learning_rate": 0.00011578104138851801,
      "loss": 0.4042,
      "step": 1582
    },
    {
      "epoch": 0.42213333333333336,
      "grad_norm": 0.060270149260759354,
      "learning_rate": 0.0001157276368491322,
      "loss": 0.3195,
      "step": 1583
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.04906638339161873,
      "learning_rate": 0.00011567423230974634,
      "loss": 0.2519,
      "step": 1584
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 0.05456717312335968,
      "learning_rate": 0.0001156208277703605,
      "loss": 0.2983,
      "step": 1585
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.0701819434762001,
      "learning_rate": 0.00011556742323097465,
      "loss": 0.3464,
      "step": 1586
    },
    {
      "epoch": 0.4232,
      "grad_norm": 0.048314813524484634,
      "learning_rate": 0.0001155140186915888,
      "loss": 0.3268,
      "step": 1587
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.06569098681211472,
      "learning_rate": 0.00011546061415220295,
      "loss": 0.3185,
      "step": 1588
    },
    {
      "epoch": 0.42373333333333335,
      "grad_norm": 0.06315194070339203,
      "learning_rate": 0.0001154072096128171,
      "loss": 0.292,
      "step": 1589
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.06465861946344376,
      "learning_rate": 0.00011535380507343125,
      "loss": 0.3455,
      "step": 1590
    },
    {
      "epoch": 0.4242666666666667,
      "grad_norm": 0.06007011979818344,
      "learning_rate": 0.00011530040053404541,
      "loss": 0.3322,
      "step": 1591
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.07203167676925659,
      "learning_rate": 0.00011524699599465955,
      "loss": 0.3894,
      "step": 1592
    },
    {
      "epoch": 0.4248,
      "grad_norm": 0.054964229464530945,
      "learning_rate": 0.00011519359145527371,
      "loss": 0.2355,
      "step": 1593
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.05129411816596985,
      "learning_rate": 0.00011514018691588786,
      "loss": 0.2817,
      "step": 1594
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.05185965076088905,
      "learning_rate": 0.000115086782376502,
      "loss": 0.257,
      "step": 1595
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.11024106293916702,
      "learning_rate": 0.00011503337783711616,
      "loss": 0.3649,
      "step": 1596
    },
    {
      "epoch": 0.42586666666666667,
      "grad_norm": 0.057461049407720566,
      "learning_rate": 0.00011497997329773032,
      "loss": 0.3749,
      "step": 1597
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.06538426131010056,
      "learning_rate": 0.00011492656875834446,
      "loss": 0.3418,
      "step": 1598
    },
    {
      "epoch": 0.4264,
      "grad_norm": 0.05309143289923668,
      "learning_rate": 0.00011487316421895862,
      "loss": 0.2719,
      "step": 1599
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.08642259240150452,
      "learning_rate": 0.00011481975967957277,
      "loss": 0.3815,
      "step": 1600
    },
    {
      "epoch": 0.42693333333333333,
      "grad_norm": 0.0592072419822216,
      "learning_rate": 0.00011476635514018692,
      "loss": 0.3652,
      "step": 1601
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.07334697246551514,
      "learning_rate": 0.00011471295060080107,
      "loss": 0.3033,
      "step": 1602
    },
    {
      "epoch": 0.42746666666666666,
      "grad_norm": 0.050819557160139084,
      "learning_rate": 0.00011465954606141523,
      "loss": 0.3048,
      "step": 1603
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.07248211652040482,
      "learning_rate": 0.00011460614152202937,
      "loss": 0.3478,
      "step": 1604
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.06540228426456451,
      "learning_rate": 0.00011455273698264353,
      "loss": 0.3001,
      "step": 1605
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.06367446482181549,
      "learning_rate": 0.00011449933244325769,
      "loss": 0.2762,
      "step": 1606
    },
    {
      "epoch": 0.4285333333333333,
      "grad_norm": 0.07947766780853271,
      "learning_rate": 0.00011444592790387183,
      "loss": 0.3763,
      "step": 1607
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.05996070057153702,
      "learning_rate": 0.00011439252336448598,
      "loss": 0.3674,
      "step": 1608
    },
    {
      "epoch": 0.42906666666666665,
      "grad_norm": 0.04473351314663887,
      "learning_rate": 0.00011433911882510013,
      "loss": 0.2466,
      "step": 1609
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.05243626981973648,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.287,
      "step": 1610
    },
    {
      "epoch": 0.4296,
      "grad_norm": 0.06287049502134323,
      "learning_rate": 0.00011423230974632844,
      "loss": 0.3712,
      "step": 1611
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.05879243090748787,
      "learning_rate": 0.00011417890520694258,
      "loss": 0.2788,
      "step": 1612
    },
    {
      "epoch": 0.4301333333333333,
      "grad_norm": 0.06806281954050064,
      "learning_rate": 0.00011412550066755674,
      "loss": 0.3786,
      "step": 1613
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.05159532651305199,
      "learning_rate": 0.0001140720961281709,
      "loss": 0.3595,
      "step": 1614
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.0505521260201931,
      "learning_rate": 0.00011401869158878504,
      "loss": 0.3247,
      "step": 1615
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.06244470179080963,
      "learning_rate": 0.0001139652870493992,
      "loss": 0.3672,
      "step": 1616
    },
    {
      "epoch": 0.4312,
      "grad_norm": 0.052668455988168716,
      "learning_rate": 0.00011391188251001335,
      "loss": 0.3108,
      "step": 1617
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.05353381857275963,
      "learning_rate": 0.00011385847797062752,
      "loss": 0.2427,
      "step": 1618
    },
    {
      "epoch": 0.43173333333333336,
      "grad_norm": 0.06404650956392288,
      "learning_rate": 0.00011380507343124168,
      "loss": 0.2955,
      "step": 1619
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.055817022919654846,
      "learning_rate": 0.00011375166889185582,
      "loss": 0.3033,
      "step": 1620
    },
    {
      "epoch": 0.4322666666666667,
      "grad_norm": 0.06718819588422775,
      "learning_rate": 0.00011369826435246998,
      "loss": 0.3449,
      "step": 1621
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.06902915984392166,
      "learning_rate": 0.00011364485981308413,
      "loss": 0.327,
      "step": 1622
    },
    {
      "epoch": 0.4328,
      "grad_norm": 0.06374948471784592,
      "learning_rate": 0.00011359145527369828,
      "loss": 0.3192,
      "step": 1623
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.05546210706233978,
      "learning_rate": 0.00011353805073431243,
      "loss": 0.3639,
      "step": 1624
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.04502623900771141,
      "learning_rate": 0.00011348464619492657,
      "loss": 0.3139,
      "step": 1625
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.07632835954427719,
      "learning_rate": 0.00011343124165554073,
      "loss": 0.344,
      "step": 1626
    },
    {
      "epoch": 0.4338666666666667,
      "grad_norm": 0.05858560651540756,
      "learning_rate": 0.00011337783711615489,
      "loss": 0.3202,
      "step": 1627
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.05937889590859413,
      "learning_rate": 0.00011332443257676903,
      "loss": 0.3037,
      "step": 1628
    },
    {
      "epoch": 0.4344,
      "grad_norm": 0.08180191367864609,
      "learning_rate": 0.00011327102803738319,
      "loss": 0.3768,
      "step": 1629
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.07243826240301132,
      "learning_rate": 0.00011321762349799734,
      "loss": 0.3321,
      "step": 1630
    },
    {
      "epoch": 0.43493333333333334,
      "grad_norm": 0.08080075681209564,
      "learning_rate": 0.00011316421895861149,
      "loss": 0.3364,
      "step": 1631
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.05844520777463913,
      "learning_rate": 0.00011311081441922564,
      "loss": 0.3051,
      "step": 1632
    },
    {
      "epoch": 0.43546666666666667,
      "grad_norm": 0.0699884369969368,
      "learning_rate": 0.0001130574098798398,
      "loss": 0.3672,
      "step": 1633
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.061696358025074005,
      "learning_rate": 0.00011300400534045394,
      "loss": 0.268,
      "step": 1634
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.05518960952758789,
      "learning_rate": 0.0001129506008010681,
      "loss": 0.3048,
      "step": 1635
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.08839907497167587,
      "learning_rate": 0.00011289719626168225,
      "loss": 0.4093,
      "step": 1636
    },
    {
      "epoch": 0.43653333333333333,
      "grad_norm": 0.06496666371822357,
      "learning_rate": 0.0001128437917222964,
      "loss": 0.3783,
      "step": 1637
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.048751045018434525,
      "learning_rate": 0.00011279038718291055,
      "loss": 0.2881,
      "step": 1638
    },
    {
      "epoch": 0.43706666666666666,
      "grad_norm": 0.0541413351893425,
      "learning_rate": 0.00011273698264352471,
      "loss": 0.3309,
      "step": 1639
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.051900722086429596,
      "learning_rate": 0.00011268357810413885,
      "loss": 0.2871,
      "step": 1640
    },
    {
      "epoch": 0.4376,
      "grad_norm": 0.07315493375062943,
      "learning_rate": 0.00011263017356475301,
      "loss": 0.355,
      "step": 1641
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.06008773297071457,
      "learning_rate": 0.00011257676902536715,
      "loss": 0.3448,
      "step": 1642
    },
    {
      "epoch": 0.4381333333333333,
      "grad_norm": 0.05531318858265877,
      "learning_rate": 0.00011252336448598131,
      "loss": 0.3241,
      "step": 1643
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.04963191598653793,
      "learning_rate": 0.00011246995994659546,
      "loss": 0.3013,
      "step": 1644
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 0.04890671372413635,
      "learning_rate": 0.0001124165554072096,
      "loss": 0.2792,
      "step": 1645
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.07434070855379105,
      "learning_rate": 0.00011236315086782376,
      "loss": 0.3525,
      "step": 1646
    },
    {
      "epoch": 0.4392,
      "grad_norm": 0.059341393411159515,
      "learning_rate": 0.00011230974632843792,
      "loss": 0.3248,
      "step": 1647
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.047056667506694794,
      "learning_rate": 0.00011225634178905206,
      "loss": 0.2893,
      "step": 1648
    },
    {
      "epoch": 0.4397333333333333,
      "grad_norm": 0.053234972059726715,
      "learning_rate": 0.00011220293724966622,
      "loss": 0.2533,
      "step": 1649
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.058069244027137756,
      "learning_rate": 0.00011214953271028037,
      "loss": 0.3007,
      "step": 1650
    },
    {
      "epoch": 0.44026666666666664,
      "grad_norm": 0.06607415527105331,
      "learning_rate": 0.00011209612817089452,
      "loss": 0.3395,
      "step": 1651
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.051922284066677094,
      "learning_rate": 0.00011204272363150867,
      "loss": 0.3094,
      "step": 1652
    },
    {
      "epoch": 0.4408,
      "grad_norm": 0.061919424682855606,
      "learning_rate": 0.00011198931909212284,
      "loss": 0.3099,
      "step": 1653
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.04977339506149292,
      "learning_rate": 0.000111935914552737,
      "loss": 0.2912,
      "step": 1654
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 0.048905692994594574,
      "learning_rate": 0.00011188251001335116,
      "loss": 0.2799,
      "step": 1655
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.05811166390776634,
      "learning_rate": 0.0001118291054739653,
      "loss": 0.3374,
      "step": 1656
    },
    {
      "epoch": 0.4418666666666667,
      "grad_norm": 0.074895940721035,
      "learning_rate": 0.00011177570093457945,
      "loss": 0.4073,
      "step": 1657
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.06303705275058746,
      "learning_rate": 0.0001117222963951936,
      "loss": 0.3572,
      "step": 1658
    },
    {
      "epoch": 0.4424,
      "grad_norm": 0.057198841124773026,
      "learning_rate": 0.00011166889185580775,
      "loss": 0.3184,
      "step": 1659
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.056163057684898376,
      "learning_rate": 0.00011161548731642191,
      "loss": 0.338,
      "step": 1660
    },
    {
      "epoch": 0.44293333333333335,
      "grad_norm": 0.05982083082199097,
      "learning_rate": 0.00011156208277703605,
      "loss": 0.3193,
      "step": 1661
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.058893073350191116,
      "learning_rate": 0.00011150867823765021,
      "loss": 0.3272,
      "step": 1662
    },
    {
      "epoch": 0.4434666666666667,
      "grad_norm": 0.07219948619604111,
      "learning_rate": 0.00011145527369826437,
      "loss": 0.4023,
      "step": 1663
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.07124616205692291,
      "learning_rate": 0.00011140186915887851,
      "loss": 0.3561,
      "step": 1664
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.0816664844751358,
      "learning_rate": 0.00011134846461949266,
      "loss": 0.4089,
      "step": 1665
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.04830779880285263,
      "learning_rate": 0.00011129506008010682,
      "loss": 0.2423,
      "step": 1666
    },
    {
      "epoch": 0.44453333333333334,
      "grad_norm": 0.06875797361135483,
      "learning_rate": 0.00011124165554072096,
      "loss": 0.4057,
      "step": 1667
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.08251487463712692,
      "learning_rate": 0.00011118825100133512,
      "loss": 0.3991,
      "step": 1668
    },
    {
      "epoch": 0.44506666666666667,
      "grad_norm": 0.061968591064214706,
      "learning_rate": 0.00011113484646194928,
      "loss": 0.2985,
      "step": 1669
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.05382483825087547,
      "learning_rate": 0.00011108144192256342,
      "loss": 0.2881,
      "step": 1670
    },
    {
      "epoch": 0.4456,
      "grad_norm": 0.06583580374717712,
      "learning_rate": 0.00011102803738317758,
      "loss": 0.3543,
      "step": 1671
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.05961899086833,
      "learning_rate": 0.00011097463284379173,
      "loss": 0.2999,
      "step": 1672
    },
    {
      "epoch": 0.4461333333333333,
      "grad_norm": 0.06221534311771393,
      "learning_rate": 0.00011092122830440587,
      "loss": 0.3236,
      "step": 1673
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.05380064249038696,
      "learning_rate": 0.00011086782376502003,
      "loss": 0.3108,
      "step": 1674
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.057285357266664505,
      "learning_rate": 0.00011081441922563417,
      "loss": 0.3126,
      "step": 1675
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.06785731017589569,
      "learning_rate": 0.00011076101468624833,
      "loss": 0.2777,
      "step": 1676
    },
    {
      "epoch": 0.4472,
      "grad_norm": 0.05850160866975784,
      "learning_rate": 0.00011070761014686249,
      "loss": 0.3215,
      "step": 1677
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.07469242066144943,
      "learning_rate": 0.00011065420560747663,
      "loss": 0.31,
      "step": 1678
    },
    {
      "epoch": 0.4477333333333333,
      "grad_norm": 0.06386471539735794,
      "learning_rate": 0.00011060080106809079,
      "loss": 0.3323,
      "step": 1679
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.0548611581325531,
      "learning_rate": 0.00011054739652870494,
      "loss": 0.3265,
      "step": 1680
    },
    {
      "epoch": 0.44826666666666665,
      "grad_norm": 0.06918410956859589,
      "learning_rate": 0.00011049399198931908,
      "loss": 0.3394,
      "step": 1681
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.057855527848005295,
      "learning_rate": 0.00011044058744993324,
      "loss": 0.2999,
      "step": 1682
    },
    {
      "epoch": 0.4488,
      "grad_norm": 0.05491358041763306,
      "learning_rate": 0.0001103871829105474,
      "loss": 0.3042,
      "step": 1683
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.06059984490275383,
      "learning_rate": 0.00011033377837116154,
      "loss": 0.3423,
      "step": 1684
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.04965104162693024,
      "learning_rate": 0.0001102803738317757,
      "loss": 0.3187,
      "step": 1685
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.04620928317308426,
      "learning_rate": 0.00011022696929238985,
      "loss": 0.2821,
      "step": 1686
    },
    {
      "epoch": 0.4498666666666667,
      "grad_norm": 0.04416584223508835,
      "learning_rate": 0.00011017356475300402,
      "loss": 0.3115,
      "step": 1687
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.0620097741484642,
      "learning_rate": 0.00011012016021361818,
      "loss": 0.4195,
      "step": 1688
    },
    {
      "epoch": 0.4504,
      "grad_norm": 0.08007428795099258,
      "learning_rate": 0.00011006675567423232,
      "loss": 0.4124,
      "step": 1689
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.06490495055913925,
      "learning_rate": 0.00011001335113484648,
      "loss": 0.3146,
      "step": 1690
    },
    {
      "epoch": 0.45093333333333335,
      "grad_norm": 0.06167960166931152,
      "learning_rate": 0.00010995994659546062,
      "loss": 0.3848,
      "step": 1691
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.06216864660382271,
      "learning_rate": 0.00010990654205607478,
      "loss": 0.3063,
      "step": 1692
    },
    {
      "epoch": 0.4514666666666667,
      "grad_norm": 0.06382141262292862,
      "learning_rate": 0.00010985313751668893,
      "loss": 0.3655,
      "step": 1693
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.0518064983189106,
      "learning_rate": 0.00010979973297730308,
      "loss": 0.2877,
      "step": 1694
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.06635042279958725,
      "learning_rate": 0.00010974632843791723,
      "loss": 0.3602,
      "step": 1695
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.06585810333490372,
      "learning_rate": 0.00010969292389853139,
      "loss": 0.3313,
      "step": 1696
    },
    {
      "epoch": 0.45253333333333334,
      "grad_norm": 0.08583711087703705,
      "learning_rate": 0.00010963951935914553,
      "loss": 0.3652,
      "step": 1697
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.055672671645879745,
      "learning_rate": 0.00010958611481975969,
      "loss": 0.3398,
      "step": 1698
    },
    {
      "epoch": 0.4530666666666667,
      "grad_norm": 0.05947249382734299,
      "learning_rate": 0.00010953271028037384,
      "loss": 0.3257,
      "step": 1699
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.07037676870822906,
      "learning_rate": 0.00010947930574098799,
      "loss": 0.3662,
      "step": 1700
    },
    {
      "epoch": 0.4536,
      "grad_norm": 0.06324117630720139,
      "learning_rate": 0.00010942590120160214,
      "loss": 0.309,
      "step": 1701
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.05210670083761215,
      "learning_rate": 0.0001093724966622163,
      "loss": 0.2918,
      "step": 1702
    },
    {
      "epoch": 0.45413333333333333,
      "grad_norm": 0.06244460865855217,
      "learning_rate": 0.00010931909212283044,
      "loss": 0.2882,
      "step": 1703
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.05319499596953392,
      "learning_rate": 0.0001092656875834446,
      "loss": 0.2533,
      "step": 1704
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.05433094874024391,
      "learning_rate": 0.00010921228304405876,
      "loss": 0.2897,
      "step": 1705
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.06554040312767029,
      "learning_rate": 0.0001091588785046729,
      "loss": 0.3696,
      "step": 1706
    },
    {
      "epoch": 0.4552,
      "grad_norm": 0.04940896853804588,
      "learning_rate": 0.00010910547396528705,
      "loss": 0.2699,
      "step": 1707
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.06055612489581108,
      "learning_rate": 0.00010905206942590121,
      "loss": 0.3026,
      "step": 1708
    },
    {
      "epoch": 0.4557333333333333,
      "grad_norm": 0.05675224959850311,
      "learning_rate": 0.00010899866488651535,
      "loss": 0.284,
      "step": 1709
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.07330077141523361,
      "learning_rate": 0.00010894526034712951,
      "loss": 0.3339,
      "step": 1710
    },
    {
      "epoch": 0.45626666666666665,
      "grad_norm": 0.05697714909911156,
      "learning_rate": 0.00010889185580774365,
      "loss": 0.3208,
      "step": 1711
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.05363856256008148,
      "learning_rate": 0.00010883845126835781,
      "loss": 0.3113,
      "step": 1712
    },
    {
      "epoch": 0.4568,
      "grad_norm": 0.05329718813300133,
      "learning_rate": 0.00010878504672897197,
      "loss": 0.3015,
      "step": 1713
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.05301010236144066,
      "learning_rate": 0.00010873164218958611,
      "loss": 0.3273,
      "step": 1714
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 0.054594557732343674,
      "learning_rate": 0.00010867823765020026,
      "loss": 0.3649,
      "step": 1715
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.06496988236904144,
      "learning_rate": 0.00010862483311081442,
      "loss": 0.3521,
      "step": 1716
    },
    {
      "epoch": 0.45786666666666664,
      "grad_norm": 0.06851930916309357,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.3888,
      "step": 1717
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.046498265117406845,
      "learning_rate": 0.00010851802403204272,
      "loss": 0.302,
      "step": 1718
    },
    {
      "epoch": 0.4584,
      "grad_norm": 0.05956584960222244,
      "learning_rate": 0.00010846461949265688,
      "loss": 0.3207,
      "step": 1719
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.057051099836826324,
      "learning_rate": 0.00010841121495327102,
      "loss": 0.325,
      "step": 1720
    },
    {
      "epoch": 0.45893333333333336,
      "grad_norm": 0.06285061687231064,
      "learning_rate": 0.00010835781041388518,
      "loss": 0.3358,
      "step": 1721
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.05320417508482933,
      "learning_rate": 0.00010830440587449935,
      "loss": 0.2769,
      "step": 1722
    },
    {
      "epoch": 0.4594666666666667,
      "grad_norm": 0.062306031584739685,
      "learning_rate": 0.0001082510013351135,
      "loss": 0.3352,
      "step": 1723
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.044099047780036926,
      "learning_rate": 0.00010819759679572766,
      "loss": 0.3104,
      "step": 1724
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05081017687916756,
      "learning_rate": 0.0001081441922563418,
      "loss": 0.3106,
      "step": 1725
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.06346925348043442,
      "learning_rate": 0.00010809078771695596,
      "loss": 0.3592,
      "step": 1726
    },
    {
      "epoch": 0.46053333333333335,
      "grad_norm": 0.06570092588663101,
      "learning_rate": 0.0001080373831775701,
      "loss": 0.3641,
      "step": 1727
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.06693852692842484,
      "learning_rate": 0.00010798397863818426,
      "loss": 0.2985,
      "step": 1728
    },
    {
      "epoch": 0.4610666666666667,
      "grad_norm": 0.05346140265464783,
      "learning_rate": 0.00010793057409879841,
      "loss": 0.2775,
      "step": 1729
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.05304823815822601,
      "learning_rate": 0.00010787716955941256,
      "loss": 0.2784,
      "step": 1730
    },
    {
      "epoch": 0.4616,
      "grad_norm": 0.06539824604988098,
      "learning_rate": 0.00010782376502002671,
      "loss": 0.3426,
      "step": 1731
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.0539160780608654,
      "learning_rate": 0.00010777036048064087,
      "loss": 0.2731,
      "step": 1732
    },
    {
      "epoch": 0.46213333333333334,
      "grad_norm": 0.06244490668177605,
      "learning_rate": 0.00010771695594125501,
      "loss": 0.3142,
      "step": 1733
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.06541390717029572,
      "learning_rate": 0.00010766355140186917,
      "loss": 0.3425,
      "step": 1734
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.05491091310977936,
      "learning_rate": 0.00010761014686248332,
      "loss": 0.3063,
      "step": 1735
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.05291639640927315,
      "learning_rate": 0.00010755674232309747,
      "loss": 0.3107,
      "step": 1736
    },
    {
      "epoch": 0.4632,
      "grad_norm": 0.07654184103012085,
      "learning_rate": 0.00010750333778371162,
      "loss": 0.3388,
      "step": 1737
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.0954933911561966,
      "learning_rate": 0.00010744993324432578,
      "loss": 0.369,
      "step": 1738
    },
    {
      "epoch": 0.46373333333333333,
      "grad_norm": 0.7367336750030518,
      "learning_rate": 0.00010739652870493992,
      "loss": 0.369,
      "step": 1739
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.4662455916404724,
      "learning_rate": 0.00010734312416555408,
      "loss": 0.3626,
      "step": 1740
    },
    {
      "epoch": 0.46426666666666666,
      "grad_norm": 0.21948929131031036,
      "learning_rate": 0.00010728971962616823,
      "loss": 0.3674,
      "step": 1741
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.13430461287498474,
      "learning_rate": 0.00010723631508678238,
      "loss": 0.3736,
      "step": 1742
    },
    {
      "epoch": 0.4648,
      "grad_norm": 0.09677550196647644,
      "learning_rate": 0.00010718291054739653,
      "loss": 0.2738,
      "step": 1743
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.08752217888832092,
      "learning_rate": 0.00010712950600801068,
      "loss": 0.3632,
      "step": 1744
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.0704965591430664,
      "learning_rate": 0.00010707610146862483,
      "loss": 0.3013,
      "step": 1745
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.0661359429359436,
      "learning_rate": 0.00010702269692923899,
      "loss": 0.3304,
      "step": 1746
    },
    {
      "epoch": 0.46586666666666665,
      "grad_norm": 0.07591904699802399,
      "learning_rate": 0.00010696929238985313,
      "loss": 0.3716,
      "step": 1747
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.06160949915647507,
      "learning_rate": 0.00010691588785046729,
      "loss": 0.3227,
      "step": 1748
    },
    {
      "epoch": 0.4664,
      "grad_norm": 0.06014207378029823,
      "learning_rate": 0.00010686248331108144,
      "loss": 0.3166,
      "step": 1749
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06005009636282921,
      "learning_rate": 0.00010680907877169559,
      "loss": 0.3665,
      "step": 1750
    },
    {
      "epoch": 0.4669333333333333,
      "grad_norm": 0.06336371600627899,
      "learning_rate": 0.00010675567423230974,
      "loss": 0.3751,
      "step": 1751
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.07170159369707108,
      "learning_rate": 0.0001067022696929239,
      "loss": 0.3941,
      "step": 1752
    },
    {
      "epoch": 0.46746666666666664,
      "grad_norm": 0.06257610768079758,
      "learning_rate": 0.00010664886515353804,
      "loss": 0.2669,
      "step": 1753
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.06801079213619232,
      "learning_rate": 0.0001065954606141522,
      "loss": 0.3891,
      "step": 1754
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.05132060870528221,
      "learning_rate": 0.00010654205607476636,
      "loss": 0.2722,
      "step": 1755
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.062384724617004395,
      "learning_rate": 0.0001064886515353805,
      "loss": 0.3767,
      "step": 1756
    },
    {
      "epoch": 0.46853333333333336,
      "grad_norm": 0.06281084567308426,
      "learning_rate": 0.00010643524699599468,
      "loss": 0.2908,
      "step": 1757
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.055209577083587646,
      "learning_rate": 0.00010638184245660882,
      "loss": 0.2972,
      "step": 1758
    },
    {
      "epoch": 0.4690666666666667,
      "grad_norm": 0.07523579150438309,
      "learning_rate": 0.00010632843791722298,
      "loss": 0.3533,
      "step": 1759
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.053994741290807724,
      "learning_rate": 0.00010627503337783712,
      "loss": 0.3141,
      "step": 1760
    },
    {
      "epoch": 0.4696,
      "grad_norm": 0.05574481561779976,
      "learning_rate": 0.00010622162883845128,
      "loss": 0.3351,
      "step": 1761
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.05366192013025284,
      "learning_rate": 0.00010616822429906544,
      "loss": 0.3509,
      "step": 1762
    },
    {
      "epoch": 0.47013333333333335,
      "grad_norm": 0.07169441133737564,
      "learning_rate": 0.00010611481975967958,
      "loss": 0.3733,
      "step": 1763
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.057108040899038315,
      "learning_rate": 0.00010606141522029374,
      "loss": 0.3441,
      "step": 1764
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.05754297226667404,
      "learning_rate": 0.00010600801068090789,
      "loss": 0.3583,
      "step": 1765
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.04508013278245926,
      "learning_rate": 0.00010595460614152203,
      "loss": 0.2873,
      "step": 1766
    },
    {
      "epoch": 0.4712,
      "grad_norm": 0.05654730275273323,
      "learning_rate": 0.00010590120160213619,
      "loss": 0.3146,
      "step": 1767
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.07573413848876953,
      "learning_rate": 0.00010584779706275035,
      "loss": 0.3394,
      "step": 1768
    },
    {
      "epoch": 0.47173333333333334,
      "grad_norm": 0.07593090832233429,
      "learning_rate": 0.00010579439252336449,
      "loss": 0.4022,
      "step": 1769
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.050657160580158234,
      "learning_rate": 0.00010574098798397865,
      "loss": 0.2789,
      "step": 1770
    },
    {
      "epoch": 0.47226666666666667,
      "grad_norm": 0.04674619063735008,
      "learning_rate": 0.0001056875834445928,
      "loss": 0.2942,
      "step": 1771
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.06681814044713974,
      "learning_rate": 0.00010563417890520695,
      "loss": 0.2953,
      "step": 1772
    },
    {
      "epoch": 0.4728,
      "grad_norm": 0.057311177253723145,
      "learning_rate": 0.0001055807743658211,
      "loss": 0.3005,
      "step": 1773
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.07466019690036774,
      "learning_rate": 0.00010552736982643526,
      "loss": 0.3392,
      "step": 1774
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.04793877899646759,
      "learning_rate": 0.0001054739652870494,
      "loss": 0.3233,
      "step": 1775
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.061304569244384766,
      "learning_rate": 0.00010542056074766356,
      "loss": 0.3471,
      "step": 1776
    },
    {
      "epoch": 0.47386666666666666,
      "grad_norm": 0.04848381504416466,
      "learning_rate": 0.0001053671562082777,
      "loss": 0.297,
      "step": 1777
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.0660659596323967,
      "learning_rate": 0.00010531375166889186,
      "loss": 0.3034,
      "step": 1778
    },
    {
      "epoch": 0.4744,
      "grad_norm": 0.06238226965069771,
      "learning_rate": 0.00010526034712950601,
      "loss": 0.3232,
      "step": 1779
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.0970638319849968,
      "learning_rate": 0.00010520694259012016,
      "loss": 0.4215,
      "step": 1780
    },
    {
      "epoch": 0.4749333333333333,
      "grad_norm": 0.06516526639461517,
      "learning_rate": 0.00010515353805073431,
      "loss": 0.3458,
      "step": 1781
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.043705131858587265,
      "learning_rate": 0.00010510013351134847,
      "loss": 0.2423,
      "step": 1782
    },
    {
      "epoch": 0.47546666666666665,
      "grad_norm": 0.060932837426662445,
      "learning_rate": 0.00010504672897196261,
      "loss": 0.2932,
      "step": 1783
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.08023107051849365,
      "learning_rate": 0.00010499332443257677,
      "loss": 0.3214,
      "step": 1784
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.06228727474808693,
      "learning_rate": 0.00010493991989319092,
      "loss": 0.3081,
      "step": 1785
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.06601261347532272,
      "learning_rate": 0.00010488651535380507,
      "loss": 0.3228,
      "step": 1786
    },
    {
      "epoch": 0.4765333333333333,
      "grad_norm": 0.060100823640823364,
      "learning_rate": 0.00010483311081441922,
      "loss": 0.3357,
      "step": 1787
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.06860043108463287,
      "learning_rate": 0.00010477970627503338,
      "loss": 0.3145,
      "step": 1788
    },
    {
      "epoch": 0.4770666666666667,
      "grad_norm": 0.06977561116218567,
      "learning_rate": 0.00010472630173564752,
      "loss": 0.3529,
      "step": 1789
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.06027434766292572,
      "learning_rate": 0.00010467289719626168,
      "loss": 0.3141,
      "step": 1790
    },
    {
      "epoch": 0.4776,
      "grad_norm": 0.07001104950904846,
      "learning_rate": 0.00010461949265687585,
      "loss": 0.3718,
      "step": 1791
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.042892616242170334,
      "learning_rate": 0.00010456608811749,
      "loss": 0.242,
      "step": 1792
    },
    {
      "epoch": 0.47813333333333335,
      "grad_norm": 0.07805313915014267,
      "learning_rate": 0.00010451268357810415,
      "loss": 0.2873,
      "step": 1793
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.05581345409154892,
      "learning_rate": 0.0001044592790387183,
      "loss": 0.2955,
      "step": 1794
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 0.06403550505638123,
      "learning_rate": 0.00010440587449933246,
      "loss": 0.3456,
      "step": 1795
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.0441514253616333,
      "learning_rate": 0.0001043524699599466,
      "loss": 0.2686,
      "step": 1796
    },
    {
      "epoch": 0.4792,
      "grad_norm": 0.05625525861978531,
      "learning_rate": 0.00010429906542056076,
      "loss": 0.3475,
      "step": 1797
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.05531277880072594,
      "learning_rate": 0.00010424566088117492,
      "loss": 0.3169,
      "step": 1798
    },
    {
      "epoch": 0.47973333333333334,
      "grad_norm": 0.08072429150342941,
      "learning_rate": 0.00010419225634178906,
      "loss": 0.389,
      "step": 1799
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.07107318937778473,
      "learning_rate": 0.00010413885180240321,
      "loss": 0.3564,
      "step": 1800
    },
    {
      "epoch": 0.4802666666666667,
      "grad_norm": 0.04727642238140106,
      "learning_rate": 0.00010408544726301737,
      "loss": 0.3076,
      "step": 1801
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.06598728895187378,
      "learning_rate": 0.00010403204272363151,
      "loss": 0.3666,
      "step": 1802
    },
    {
      "epoch": 0.4808,
      "grad_norm": 0.0953231230378151,
      "learning_rate": 0.00010397863818424567,
      "loss": 0.2633,
      "step": 1803
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.04671798273921013,
      "learning_rate": 0.00010392523364485983,
      "loss": 0.3045,
      "step": 1804
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.06778017431497574,
      "learning_rate": 0.00010387182910547397,
      "loss": 0.3204,
      "step": 1805
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.05750603601336479,
      "learning_rate": 0.00010381842456608813,
      "loss": 0.3035,
      "step": 1806
    },
    {
      "epoch": 0.48186666666666667,
      "grad_norm": 0.052968304604291916,
      "learning_rate": 0.00010376502002670228,
      "loss": 0.3386,
      "step": 1807
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.04974520951509476,
      "learning_rate": 0.00010371161548731642,
      "loss": 0.3304,
      "step": 1808
    },
    {
      "epoch": 0.4824,
      "grad_norm": 0.051784027367830276,
      "learning_rate": 0.00010365821094793058,
      "loss": 0.3515,
      "step": 1809
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.05930367112159729,
      "learning_rate": 0.00010360480640854472,
      "loss": 0.2859,
      "step": 1810
    },
    {
      "epoch": 0.4829333333333333,
      "grad_norm": 0.0567689873278141,
      "learning_rate": 0.00010355140186915888,
      "loss": 0.3036,
      "step": 1811
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.05250195413827896,
      "learning_rate": 0.00010349799732977304,
      "loss": 0.3307,
      "step": 1812
    },
    {
      "epoch": 0.48346666666666666,
      "grad_norm": 0.06854613125324249,
      "learning_rate": 0.00010344459279038718,
      "loss": 0.3676,
      "step": 1813
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.05537686496973038,
      "learning_rate": 0.00010339118825100134,
      "loss": 0.3233,
      "step": 1814
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.0521528385579586,
      "learning_rate": 0.00010333778371161549,
      "loss": 0.3017,
      "step": 1815
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.0676887258887291,
      "learning_rate": 0.00010328437917222963,
      "loss": 0.3915,
      "step": 1816
    },
    {
      "epoch": 0.4845333333333333,
      "grad_norm": 0.06289927661418915,
      "learning_rate": 0.00010323097463284379,
      "loss": 0.3423,
      "step": 1817
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.046452596783638,
      "learning_rate": 0.00010317757009345795,
      "loss": 0.2487,
      "step": 1818
    },
    {
      "epoch": 0.48506666666666665,
      "grad_norm": 0.051979660987854004,
      "learning_rate": 0.00010312416555407209,
      "loss": 0.2919,
      "step": 1819
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.04445130378007889,
      "learning_rate": 0.00010307076101468625,
      "loss": 0.2671,
      "step": 1820
    },
    {
      "epoch": 0.4856,
      "grad_norm": 0.05115977302193642,
      "learning_rate": 0.0001030173564753004,
      "loss": 0.2966,
      "step": 1821
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.055811651051044464,
      "learning_rate": 0.00010296395193591455,
      "loss": 0.3608,
      "step": 1822
    },
    {
      "epoch": 0.4861333333333333,
      "grad_norm": 0.053838446736335754,
      "learning_rate": 0.0001029105473965287,
      "loss": 0.3039,
      "step": 1823
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.06328210234642029,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.3944,
      "step": 1824
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.04590817168354988,
      "learning_rate": 0.000102803738317757,
      "loss": 0.2714,
      "step": 1825
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.04181963577866554,
      "learning_rate": 0.00010275033377837117,
      "loss": 0.2492,
      "step": 1826
    },
    {
      "epoch": 0.4872,
      "grad_norm": 0.050034474581480026,
      "learning_rate": 0.00010269692923898533,
      "loss": 0.2691,
      "step": 1827
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.053918514400720596,
      "learning_rate": 0.00010264352469959948,
      "loss": 0.3342,
      "step": 1828
    },
    {
      "epoch": 0.48773333333333335,
      "grad_norm": 0.04454757273197174,
      "learning_rate": 0.00010259012016021363,
      "loss": 0.2678,
      "step": 1829
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.04152946546673775,
      "learning_rate": 0.00010253671562082778,
      "loss": 0.223,
      "step": 1830
    },
    {
      "epoch": 0.4882666666666667,
      "grad_norm": 0.054716769605875015,
      "learning_rate": 0.00010248331108144194,
      "loss": 0.284,
      "step": 1831
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.06062765792012215,
      "learning_rate": 0.00010242990654205608,
      "loss": 0.3694,
      "step": 1832
    },
    {
      "epoch": 0.4888,
      "grad_norm": 0.06580507010221481,
      "learning_rate": 0.00010237650200267024,
      "loss": 0.3283,
      "step": 1833
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.04991217702627182,
      "learning_rate": 0.0001023230974632844,
      "loss": 0.2894,
      "step": 1834
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.059161197394132614,
      "learning_rate": 0.00010226969292389854,
      "loss": 0.333,
      "step": 1835
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.0529964379966259,
      "learning_rate": 0.00010221628838451269,
      "loss": 0.2929,
      "step": 1836
    },
    {
      "epoch": 0.4898666666666667,
      "grad_norm": 0.052910178899765015,
      "learning_rate": 0.00010216288384512685,
      "loss": 0.3062,
      "step": 1837
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.04554171860218048,
      "learning_rate": 0.00010210947930574099,
      "loss": 0.2619,
      "step": 1838
    },
    {
      "epoch": 0.4904,
      "grad_norm": 0.05467395856976509,
      "learning_rate": 0.00010205607476635515,
      "loss": 0.3367,
      "step": 1839
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.048694908618927,
      "learning_rate": 0.0001020026702269693,
      "loss": 0.2766,
      "step": 1840
    },
    {
      "epoch": 0.49093333333333333,
      "grad_norm": 0.06995008140802383,
      "learning_rate": 0.00010194926568758345,
      "loss": 0.3269,
      "step": 1841
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.06065911054611206,
      "learning_rate": 0.0001018958611481976,
      "loss": 0.3335,
      "step": 1842
    },
    {
      "epoch": 0.49146666666666666,
      "grad_norm": 0.05029160901904106,
      "learning_rate": 0.00010184245660881176,
      "loss": 0.2514,
      "step": 1843
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.05932445451617241,
      "learning_rate": 0.0001017890520694259,
      "loss": 0.3484,
      "step": 1844
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.05412336438894272,
      "learning_rate": 0.00010173564753004006,
      "loss": 0.2784,
      "step": 1845
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.06846698373556137,
      "learning_rate": 0.0001016822429906542,
      "loss": 0.379,
      "step": 1846
    },
    {
      "epoch": 0.4925333333333333,
      "grad_norm": 0.06783466041088104,
      "learning_rate": 0.00010162883845126836,
      "loss": 0.3214,
      "step": 1847
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.06012648344039917,
      "learning_rate": 0.00010157543391188251,
      "loss": 0.3216,
      "step": 1848
    },
    {
      "epoch": 0.49306666666666665,
      "grad_norm": 0.061694297939538956,
      "learning_rate": 0.00010152202937249666,
      "loss": 0.3226,
      "step": 1849
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.05505120009183884,
      "learning_rate": 0.00010146862483311081,
      "loss": 0.3186,
      "step": 1850
    },
    {
      "epoch": 0.4936,
      "grad_norm": 0.08001081645488739,
      "learning_rate": 0.00010141522029372497,
      "loss": 0.4225,
      "step": 1851
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.05068717151880264,
      "learning_rate": 0.00010136181575433911,
      "loss": 0.3412,
      "step": 1852
    },
    {
      "epoch": 0.4941333333333333,
      "grad_norm": 0.062778539955616,
      "learning_rate": 0.00010130841121495327,
      "loss": 0.3712,
      "step": 1853
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.0691741406917572,
      "learning_rate": 0.00010125500667556743,
      "loss": 0.3664,
      "step": 1854
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.0609850212931633,
      "learning_rate": 0.00010120160213618157,
      "loss": 0.3432,
      "step": 1855
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.041214730590581894,
      "learning_rate": 0.00010114819759679572,
      "loss": 0.2626,
      "step": 1856
    },
    {
      "epoch": 0.4952,
      "grad_norm": 0.07352021336555481,
      "learning_rate": 0.00010109479305740988,
      "loss": 0.3546,
      "step": 1857
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.09231318533420563,
      "learning_rate": 0.00010104138851802402,
      "loss": 0.4136,
      "step": 1858
    },
    {
      "epoch": 0.49573333333333336,
      "grad_norm": 0.06492282450199127,
      "learning_rate": 0.00010098798397863818,
      "loss": 0.3883,
      "step": 1859
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.05357828736305237,
      "learning_rate": 0.00010093457943925234,
      "loss": 0.2559,
      "step": 1860
    },
    {
      "epoch": 0.4962666666666667,
      "grad_norm": 0.057058852165937424,
      "learning_rate": 0.0001008811748998665,
      "loss": 0.3005,
      "step": 1861
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.06343602389097214,
      "learning_rate": 0.00010082777036048065,
      "loss": 0.3525,
      "step": 1862
    },
    {
      "epoch": 0.4968,
      "grad_norm": 0.05241534113883972,
      "learning_rate": 0.0001007743658210948,
      "loss": 0.3029,
      "step": 1863
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.05130999535322189,
      "learning_rate": 0.00010072096128170896,
      "loss": 0.2732,
      "step": 1864
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.07148531079292297,
      "learning_rate": 0.0001006675567423231,
      "loss": 0.3796,
      "step": 1865
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.06274497509002686,
      "learning_rate": 0.00010061415220293726,
      "loss": 0.3875,
      "step": 1866
    },
    {
      "epoch": 0.4978666666666667,
      "grad_norm": 0.05078214034438133,
      "learning_rate": 0.00010056074766355142,
      "loss": 0.259,
      "step": 1867
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.051864124834537506,
      "learning_rate": 0.00010050734312416556,
      "loss": 0.3004,
      "step": 1868
    },
    {
      "epoch": 0.4984,
      "grad_norm": 0.05869746208190918,
      "learning_rate": 0.00010045393858477972,
      "loss": 0.291,
      "step": 1869
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.055384621024131775,
      "learning_rate": 0.00010040053404539387,
      "loss": 0.29,
      "step": 1870
    },
    {
      "epoch": 0.49893333333333334,
      "grad_norm": 0.051553815603256226,
      "learning_rate": 0.00010034712950600802,
      "loss": 0.2619,
      "step": 1871
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.0605868436396122,
      "learning_rate": 0.00010029372496662217,
      "loss": 0.3698,
      "step": 1872
    },
    {
      "epoch": 0.49946666666666667,
      "grad_norm": 0.050904929637908936,
      "learning_rate": 0.00010024032042723633,
      "loss": 0.3029,
      "step": 1873
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.04844805225729942,
      "learning_rate": 0.00010018691588785047,
      "loss": 0.264,
      "step": 1874
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.0746764987707138,
      "learning_rate": 0.00010013351134846463,
      "loss": 0.404,
      "step": 1875
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.053586434572935104,
      "learning_rate": 0.00010008010680907878,
      "loss": 0.2897,
      "step": 1876
    },
    {
      "epoch": 0.5005333333333334,
      "grad_norm": 0.054897911846637726,
      "learning_rate": 0.00010002670226969293,
      "loss": 0.3555,
      "step": 1877
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.0798938050866127,
      "learning_rate": 9.997329773030708e-05,
      "loss": 0.3859,
      "step": 1878
    },
    {
      "epoch": 0.5010666666666667,
      "grad_norm": 0.05565184727311134,
      "learning_rate": 9.991989319092123e-05,
      "loss": 0.3462,
      "step": 1879
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.05269164219498634,
      "learning_rate": 9.986648865153538e-05,
      "loss": 0.3116,
      "step": 1880
    },
    {
      "epoch": 0.5016,
      "grad_norm": 0.05656471103429794,
      "learning_rate": 9.981308411214954e-05,
      "loss": 0.3484,
      "step": 1881
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.05937613919377327,
      "learning_rate": 9.975967957276368e-05,
      "loss": 0.3636,
      "step": 1882
    },
    {
      "epoch": 0.5021333333333333,
      "grad_norm": 0.05910308286547661,
      "learning_rate": 9.970627503337784e-05,
      "loss": 0.2992,
      "step": 1883
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.05583491548895836,
      "learning_rate": 9.9652870493992e-05,
      "loss": 0.3278,
      "step": 1884
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.048502709716558456,
      "learning_rate": 9.959946595460614e-05,
      "loss": 0.3272,
      "step": 1885
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.060611542314291,
      "learning_rate": 9.954606141522029e-05,
      "loss": 0.386,
      "step": 1886
    },
    {
      "epoch": 0.5032,
      "grad_norm": 0.0587766095995903,
      "learning_rate": 9.949265687583445e-05,
      "loss": 0.2971,
      "step": 1887
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.058207035064697266,
      "learning_rate": 9.94392523364486e-05,
      "loss": 0.3604,
      "step": 1888
    },
    {
      "epoch": 0.5037333333333334,
      "grad_norm": 0.05764780193567276,
      "learning_rate": 9.938584779706276e-05,
      "loss": 0.3167,
      "step": 1889
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.05460767820477486,
      "learning_rate": 9.93324432576769e-05,
      "loss": 0.3484,
      "step": 1890
    },
    {
      "epoch": 0.5042666666666666,
      "grad_norm": 0.04801659286022186,
      "learning_rate": 9.927903871829106e-05,
      "loss": 0.2601,
      "step": 1891
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.043096110224723816,
      "learning_rate": 9.922563417890522e-05,
      "loss": 0.2455,
      "step": 1892
    },
    {
      "epoch": 0.5048,
      "grad_norm": 0.05264756456017494,
      "learning_rate": 9.917222963951936e-05,
      "loss": 0.3061,
      "step": 1893
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.06641598790884018,
      "learning_rate": 9.911882510013352e-05,
      "loss": 0.3587,
      "step": 1894
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.06980396807193756,
      "learning_rate": 9.906542056074767e-05,
      "loss": 0.3315,
      "step": 1895
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.0694701299071312,
      "learning_rate": 9.901201602136182e-05,
      "loss": 0.3574,
      "step": 1896
    },
    {
      "epoch": 0.5058666666666667,
      "grad_norm": 0.04695313051342964,
      "learning_rate": 9.895861148197597e-05,
      "loss": 0.3031,
      "step": 1897
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.05070984363555908,
      "learning_rate": 9.890520694259013e-05,
      "loss": 0.2903,
      "step": 1898
    },
    {
      "epoch": 0.5064,
      "grad_norm": 0.05387413501739502,
      "learning_rate": 9.885180240320427e-05,
      "loss": 0.3263,
      "step": 1899
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.050376661121845245,
      "learning_rate": 9.879839786381843e-05,
      "loss": 0.3354,
      "step": 1900
    },
    {
      "epoch": 0.5069333333333333,
      "grad_norm": 0.051896486431360245,
      "learning_rate": 9.874499332443258e-05,
      "loss": 0.279,
      "step": 1901
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.05770314857363701,
      "learning_rate": 9.869158878504673e-05,
      "loss": 0.3107,
      "step": 1902
    },
    {
      "epoch": 0.5074666666666666,
      "grad_norm": 0.05193652957677841,
      "learning_rate": 9.863818424566088e-05,
      "loss": 0.2922,
      "step": 1903
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.04839196056127548,
      "learning_rate": 9.858477970627504e-05,
      "loss": 0.2903,
      "step": 1904
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.062276918441057205,
      "learning_rate": 9.85313751668892e-05,
      "loss": 0.3311,
      "step": 1905
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.055836260318756104,
      "learning_rate": 9.847797062750335e-05,
      "loss": 0.3091,
      "step": 1906
    },
    {
      "epoch": 0.5085333333333333,
      "grad_norm": 0.05082792043685913,
      "learning_rate": 9.84245660881175e-05,
      "loss": 0.3347,
      "step": 1907
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.055910397320985794,
      "learning_rate": 9.837116154873165e-05,
      "loss": 0.3044,
      "step": 1908
    },
    {
      "epoch": 0.5090666666666667,
      "grad_norm": 0.053122736513614655,
      "learning_rate": 9.831775700934581e-05,
      "loss": 0.306,
      "step": 1909
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.0489368736743927,
      "learning_rate": 9.826435246995995e-05,
      "loss": 0.2711,
      "step": 1910
    },
    {
      "epoch": 0.5096,
      "grad_norm": 0.053533829748630524,
      "learning_rate": 9.82109479305741e-05,
      "loss": 0.2716,
      "step": 1911
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.0508405864238739,
      "learning_rate": 9.815754339118825e-05,
      "loss": 0.2879,
      "step": 1912
    },
    {
      "epoch": 0.5101333333333333,
      "grad_norm": 0.06722412258386612,
      "learning_rate": 9.81041388518024e-05,
      "loss": 0.352,
      "step": 1913
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.05377538129687309,
      "learning_rate": 9.805073431241656e-05,
      "loss": 0.3682,
      "step": 1914
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 0.06628522276878357,
      "learning_rate": 9.79973297730307e-05,
      "loss": 0.3106,
      "step": 1915
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.07019951194524765,
      "learning_rate": 9.794392523364486e-05,
      "loss": 0.3794,
      "step": 1916
    },
    {
      "epoch": 0.5112,
      "grad_norm": 0.06683560460805893,
      "learning_rate": 9.789052069425902e-05,
      "loss": 0.3508,
      "step": 1917
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.05656495317816734,
      "learning_rate": 9.783711615487316e-05,
      "loss": 0.363,
      "step": 1918
    },
    {
      "epoch": 0.5117333333333334,
      "grad_norm": 0.05716612562537193,
      "learning_rate": 9.778371161548732e-05,
      "loss": 0.2787,
      "step": 1919
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.07622074335813522,
      "learning_rate": 9.773030707610147e-05,
      "loss": 0.3593,
      "step": 1920
    },
    {
      "epoch": 0.5122666666666666,
      "grad_norm": 0.06367193907499313,
      "learning_rate": 9.767690253671563e-05,
      "loss": 0.3839,
      "step": 1921
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.04836982861161232,
      "learning_rate": 9.762349799732979e-05,
      "loss": 0.2861,
      "step": 1922
    },
    {
      "epoch": 0.5128,
      "grad_norm": 0.04903450235724449,
      "learning_rate": 9.757009345794393e-05,
      "loss": 0.2799,
      "step": 1923
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.06111935153603554,
      "learning_rate": 9.751668891855808e-05,
      "loss": 0.276,
      "step": 1924
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06624282896518707,
      "learning_rate": 9.746328437917224e-05,
      "loss": 0.3361,
      "step": 1925
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.0835774838924408,
      "learning_rate": 9.740987983978638e-05,
      "loss": 0.3826,
      "step": 1926
    },
    {
      "epoch": 0.5138666666666667,
      "grad_norm": 0.07445036619901657,
      "learning_rate": 9.735647530040054e-05,
      "loss": 0.3901,
      "step": 1927
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.056271992623806,
      "learning_rate": 9.73030707610147e-05,
      "loss": 0.3382,
      "step": 1928
    },
    {
      "epoch": 0.5144,
      "grad_norm": 0.06826264411211014,
      "learning_rate": 9.724966622162884e-05,
      "loss": 0.3818,
      "step": 1929
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.08306325972080231,
      "learning_rate": 9.7196261682243e-05,
      "loss": 0.4244,
      "step": 1930
    },
    {
      "epoch": 0.5149333333333334,
      "grad_norm": 0.054366279393434525,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.2928,
      "step": 1931
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.05206862837076187,
      "learning_rate": 9.70894526034713e-05,
      "loss": 0.3202,
      "step": 1932
    },
    {
      "epoch": 0.5154666666666666,
      "grad_norm": 0.0629056766629219,
      "learning_rate": 9.703604806408545e-05,
      "loss": 0.2788,
      "step": 1933
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.07096124440431595,
      "learning_rate": 9.698264352469961e-05,
      "loss": 0.2882,
      "step": 1934
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.05324796959757805,
      "learning_rate": 9.692923898531375e-05,
      "loss": 0.2913,
      "step": 1935
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.06806936115026474,
      "learning_rate": 9.68758344459279e-05,
      "loss": 0.3557,
      "step": 1936
    },
    {
      "epoch": 0.5165333333333333,
      "grad_norm": 0.07165978103876114,
      "learning_rate": 9.682242990654206e-05,
      "loss": 0.3978,
      "step": 1937
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.07477136701345444,
      "learning_rate": 9.67690253671562e-05,
      "loss": 0.3729,
      "step": 1938
    },
    {
      "epoch": 0.5170666666666667,
      "grad_norm": 0.054079700261354446,
      "learning_rate": 9.671562082777038e-05,
      "loss": 0.3417,
      "step": 1939
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.0544196218252182,
      "learning_rate": 9.666221628838452e-05,
      "loss": 0.3451,
      "step": 1940
    },
    {
      "epoch": 0.5176,
      "grad_norm": 0.08027596026659012,
      "learning_rate": 9.660881174899867e-05,
      "loss": 0.3827,
      "step": 1941
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.0605449378490448,
      "learning_rate": 9.655540720961283e-05,
      "loss": 0.265,
      "step": 1942
    },
    {
      "epoch": 0.5181333333333333,
      "grad_norm": 0.06921107321977615,
      "learning_rate": 9.650200267022697e-05,
      "loss": 0.3326,
      "step": 1943
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.061369165778160095,
      "learning_rate": 9.644859813084113e-05,
      "loss": 0.3906,
      "step": 1944
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.0575287900865078,
      "learning_rate": 9.639519359145529e-05,
      "loss": 0.3004,
      "step": 1945
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.05841856077313423,
      "learning_rate": 9.634178905206943e-05,
      "loss": 0.3073,
      "step": 1946
    },
    {
      "epoch": 0.5192,
      "grad_norm": 0.06286553293466568,
      "learning_rate": 9.628838451268359e-05,
      "loss": 0.3042,
      "step": 1947
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.054298035800457,
      "learning_rate": 9.623497997329773e-05,
      "loss": 0.2983,
      "step": 1948
    },
    {
      "epoch": 0.5197333333333334,
      "grad_norm": 0.06327617913484573,
      "learning_rate": 9.618157543391188e-05,
      "loss": 0.3566,
      "step": 1949
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.05453167110681534,
      "learning_rate": 9.612817089452604e-05,
      "loss": 0.307,
      "step": 1950
    },
    {
      "epoch": 0.5202666666666667,
      "grad_norm": 0.05326606705784798,
      "learning_rate": 9.607476635514018e-05,
      "loss": 0.3267,
      "step": 1951
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.044788800179958344,
      "learning_rate": 9.602136181575434e-05,
      "loss": 0.2691,
      "step": 1952
    },
    {
      "epoch": 0.5208,
      "grad_norm": 0.07256478071212769,
      "learning_rate": 9.59679572763685e-05,
      "loss": 0.3707,
      "step": 1953
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.06744828820228577,
      "learning_rate": 9.591455273698264e-05,
      "loss": 0.2462,
      "step": 1954
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 0.06771145015954971,
      "learning_rate": 9.58611481975968e-05,
      "loss": 0.3739,
      "step": 1955
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.05601077154278755,
      "learning_rate": 9.580774365821095e-05,
      "loss": 0.3235,
      "step": 1956
    },
    {
      "epoch": 0.5218666666666667,
      "grad_norm": 0.06905015558004379,
      "learning_rate": 9.575433911882511e-05,
      "loss": 0.3308,
      "step": 1957
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.07601624727249146,
      "learning_rate": 9.570093457943926e-05,
      "loss": 0.3691,
      "step": 1958
    },
    {
      "epoch": 0.5224,
      "grad_norm": 0.060514505952596664,
      "learning_rate": 9.564753004005341e-05,
      "loss": 0.3185,
      "step": 1959
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.07952789217233658,
      "learning_rate": 9.559412550066756e-05,
      "loss": 0.3548,
      "step": 1960
    },
    {
      "epoch": 0.5229333333333334,
      "grad_norm": 0.08193915337324142,
      "learning_rate": 9.554072096128172e-05,
      "loss": 0.4019,
      "step": 1961
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.07268062233924866,
      "learning_rate": 9.548731642189586e-05,
      "loss": 0.3533,
      "step": 1962
    },
    {
      "epoch": 0.5234666666666666,
      "grad_norm": 0.08828102797269821,
      "learning_rate": 9.543391188251002e-05,
      "loss": 0.3278,
      "step": 1963
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.07296506315469742,
      "learning_rate": 9.538050734312418e-05,
      "loss": 0.3154,
      "step": 1964
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.0684768334031105,
      "learning_rate": 9.532710280373832e-05,
      "loss": 0.3407,
      "step": 1965
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.049500420689582825,
      "learning_rate": 9.527369826435247e-05,
      "loss": 0.318,
      "step": 1966
    },
    {
      "epoch": 0.5245333333333333,
      "grad_norm": 0.06618671864271164,
      "learning_rate": 9.522029372496663e-05,
      "loss": 0.4035,
      "step": 1967
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.05927274748682976,
      "learning_rate": 9.516688918558077e-05,
      "loss": 0.3593,
      "step": 1968
    },
    {
      "epoch": 0.5250666666666667,
      "grad_norm": 0.05325235053896904,
      "learning_rate": 9.511348464619493e-05,
      "loss": 0.285,
      "step": 1969
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.06799636036157608,
      "learning_rate": 9.506008010680909e-05,
      "loss": 0.3531,
      "step": 1970
    },
    {
      "epoch": 0.5256,
      "grad_norm": 0.05653410032391548,
      "learning_rate": 9.500667556742323e-05,
      "loss": 0.2528,
      "step": 1971
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.05941971391439438,
      "learning_rate": 9.495327102803739e-05,
      "loss": 0.3798,
      "step": 1972
    },
    {
      "epoch": 0.5261333333333333,
      "grad_norm": 0.05785854533314705,
      "learning_rate": 9.489986648865154e-05,
      "loss": 0.3333,
      "step": 1973
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.0531151182949543,
      "learning_rate": 9.48464619492657e-05,
      "loss": 0.3182,
      "step": 1974
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07031098008155823,
      "learning_rate": 9.479305740987985e-05,
      "loss": 0.2781,
      "step": 1975
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.046044010668992996,
      "learning_rate": 9.4739652870494e-05,
      "loss": 0.3002,
      "step": 1976
    },
    {
      "epoch": 0.5272,
      "grad_norm": 0.07039892673492432,
      "learning_rate": 9.468624833110815e-05,
      "loss": 0.3242,
      "step": 1977
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.05130987986922264,
      "learning_rate": 9.463284379172231e-05,
      "loss": 0.3174,
      "step": 1978
    },
    {
      "epoch": 0.5277333333333334,
      "grad_norm": 0.06401058286428452,
      "learning_rate": 9.457943925233645e-05,
      "loss": 0.4012,
      "step": 1979
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.05644233524799347,
      "learning_rate": 9.452603471295061e-05,
      "loss": 0.2615,
      "step": 1980
    },
    {
      "epoch": 0.5282666666666667,
      "grad_norm": 0.05914827063679695,
      "learning_rate": 9.447263017356475e-05,
      "loss": 0.3573,
      "step": 1981
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.041626203805208206,
      "learning_rate": 9.441922563417891e-05,
      "loss": 0.2764,
      "step": 1982
    },
    {
      "epoch": 0.5288,
      "grad_norm": 0.05748754367232323,
      "learning_rate": 9.436582109479306e-05,
      "loss": 0.2991,
      "step": 1983
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.0641600638628006,
      "learning_rate": 9.431241655540721e-05,
      "loss": 0.3282,
      "step": 1984
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 0.04987616091966629,
      "learning_rate": 9.425901201602136e-05,
      "loss": 0.2954,
      "step": 1985
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.05466584116220474,
      "learning_rate": 9.420560747663552e-05,
      "loss": 0.326,
      "step": 1986
    },
    {
      "epoch": 0.5298666666666667,
      "grad_norm": 0.05933709070086479,
      "learning_rate": 9.415220293724966e-05,
      "loss": 0.3341,
      "step": 1987
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.06061980500817299,
      "learning_rate": 9.409879839786382e-05,
      "loss": 0.3645,
      "step": 1988
    },
    {
      "epoch": 0.5304,
      "grad_norm": 0.059162553399801254,
      "learning_rate": 9.404539385847798e-05,
      "loss": 0.3784,
      "step": 1989
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.06112009286880493,
      "learning_rate": 9.399198931909212e-05,
      "loss": 0.3304,
      "step": 1990
    },
    {
      "epoch": 0.5309333333333334,
      "grad_norm": 0.05137182027101517,
      "learning_rate": 9.393858477970629e-05,
      "loss": 0.3071,
      "step": 1991
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.048495616763830185,
      "learning_rate": 9.388518024032043e-05,
      "loss": 0.292,
      "step": 1992
    },
    {
      "epoch": 0.5314666666666666,
      "grad_norm": 0.05208347365260124,
      "learning_rate": 9.383177570093459e-05,
      "loss": 0.3107,
      "step": 1993
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.08375947177410126,
      "learning_rate": 9.377837116154874e-05,
      "loss": 0.4185,
      "step": 1994
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.05108371749520302,
      "learning_rate": 9.372496662216289e-05,
      "loss": 0.3247,
      "step": 1995
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.054015979170799255,
      "learning_rate": 9.367156208277704e-05,
      "loss": 0.3407,
      "step": 1996
    },
    {
      "epoch": 0.5325333333333333,
      "grad_norm": 0.049825269728899,
      "learning_rate": 9.36181575433912e-05,
      "loss": 0.3119,
      "step": 1997
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.0551937110722065,
      "learning_rate": 9.356475300400534e-05,
      "loss": 0.2426,
      "step": 1998
    },
    {
      "epoch": 0.5330666666666667,
      "grad_norm": 0.06285084784030914,
      "learning_rate": 9.35113484646195e-05,
      "loss": 0.2775,
      "step": 1999
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.0633360743522644,
      "learning_rate": 9.345794392523365e-05,
      "loss": 0.3199,
      "step": 2000
    },
    {
      "epoch": 0.5336,
      "grad_norm": 0.061003584414720535,
      "learning_rate": 9.34045393858478e-05,
      "loss": 0.3371,
      "step": 2001
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.050257764756679535,
      "learning_rate": 9.335113484646195e-05,
      "loss": 0.2876,
      "step": 2002
    },
    {
      "epoch": 0.5341333333333333,
      "grad_norm": 0.04881599545478821,
      "learning_rate": 9.329773030707611e-05,
      "loss": 0.2909,
      "step": 2003
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.0631013736128807,
      "learning_rate": 9.324432576769025e-05,
      "loss": 0.3764,
      "step": 2004
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.06581658124923706,
      "learning_rate": 9.319092122830441e-05,
      "loss": 0.3563,
      "step": 2005
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.08303344249725342,
      "learning_rate": 9.313751668891855e-05,
      "loss": 0.3602,
      "step": 2006
    },
    {
      "epoch": 0.5352,
      "grad_norm": 0.05020184814929962,
      "learning_rate": 9.308411214953271e-05,
      "loss": 0.3228,
      "step": 2007
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.04273674637079239,
      "learning_rate": 9.303070761014688e-05,
      "loss": 0.2739,
      "step": 2008
    },
    {
      "epoch": 0.5357333333333333,
      "grad_norm": 0.06695763766765594,
      "learning_rate": 9.297730307076102e-05,
      "loss": 0.3632,
      "step": 2009
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.06572885066270828,
      "learning_rate": 9.292389853137518e-05,
      "loss": 0.3678,
      "step": 2010
    },
    {
      "epoch": 0.5362666666666667,
      "grad_norm": 0.04815279692411423,
      "learning_rate": 9.287049399198933e-05,
      "loss": 0.2829,
      "step": 2011
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.06080298870801926,
      "learning_rate": 9.281708945260348e-05,
      "loss": 0.3985,
      "step": 2012
    },
    {
      "epoch": 0.5368,
      "grad_norm": 0.051417648792266846,
      "learning_rate": 9.276368491321763e-05,
      "loss": 0.3242,
      "step": 2013
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.0519324392080307,
      "learning_rate": 9.271028037383178e-05,
      "loss": 0.3468,
      "step": 2014
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.05187870189547539,
      "learning_rate": 9.265687583444593e-05,
      "loss": 0.3502,
      "step": 2015
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.05527493357658386,
      "learning_rate": 9.260347129506009e-05,
      "loss": 0.3269,
      "step": 2016
    },
    {
      "epoch": 0.5378666666666667,
      "grad_norm": 0.06459318101406097,
      "learning_rate": 9.255006675567423e-05,
      "loss": 0.3162,
      "step": 2017
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.051723018288612366,
      "learning_rate": 9.249666221628839e-05,
      "loss": 0.29,
      "step": 2018
    },
    {
      "epoch": 0.5384,
      "grad_norm": 0.051828525960445404,
      "learning_rate": 9.244325767690254e-05,
      "loss": 0.2988,
      "step": 2019
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.05941321328282356,
      "learning_rate": 9.238985313751669e-05,
      "loss": 0.2695,
      "step": 2020
    },
    {
      "epoch": 0.5389333333333334,
      "grad_norm": 0.06186698377132416,
      "learning_rate": 9.233644859813084e-05,
      "loss": 0.2991,
      "step": 2021
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.05596698820590973,
      "learning_rate": 9.2283044058745e-05,
      "loss": 0.2463,
      "step": 2022
    },
    {
      "epoch": 0.5394666666666666,
      "grad_norm": 0.050207216292619705,
      "learning_rate": 9.222963951935914e-05,
      "loss": 0.3085,
      "step": 2023
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.055070098489522934,
      "learning_rate": 9.21762349799733e-05,
      "loss": 0.3106,
      "step": 2024
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.05655217543244362,
      "learning_rate": 9.212283044058745e-05,
      "loss": 0.3646,
      "step": 2025
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.05280410870909691,
      "learning_rate": 9.206942590120161e-05,
      "loss": 0.3551,
      "step": 2026
    },
    {
      "epoch": 0.5405333333333333,
      "grad_norm": 0.049570776522159576,
      "learning_rate": 9.201602136181577e-05,
      "loss": 0.3042,
      "step": 2027
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.04980062320828438,
      "learning_rate": 9.196261682242991e-05,
      "loss": 0.3231,
      "step": 2028
    },
    {
      "epoch": 0.5410666666666667,
      "grad_norm": 0.03981997072696686,
      "learning_rate": 9.190921228304407e-05,
      "loss": 0.3006,
      "step": 2029
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.04890649765729904,
      "learning_rate": 9.185580774365822e-05,
      "loss": 0.2986,
      "step": 2030
    },
    {
      "epoch": 0.5416,
      "grad_norm": 0.05322601646184921,
      "learning_rate": 9.180240320427236e-05,
      "loss": 0.3519,
      "step": 2031
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.05366755649447441,
      "learning_rate": 9.174899866488652e-05,
      "loss": 0.2791,
      "step": 2032
    },
    {
      "epoch": 0.5421333333333334,
      "grad_norm": 0.07339806854724884,
      "learning_rate": 9.169559412550068e-05,
      "loss": 0.3183,
      "step": 2033
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.06185603886842728,
      "learning_rate": 9.164218958611482e-05,
      "loss": 0.3534,
      "step": 2034
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.05281907320022583,
      "learning_rate": 9.158878504672898e-05,
      "loss": 0.2642,
      "step": 2035
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.04938306659460068,
      "learning_rate": 9.153538050734313e-05,
      "loss": 0.2795,
      "step": 2036
    },
    {
      "epoch": 0.5432,
      "grad_norm": 0.043073706328868866,
      "learning_rate": 9.148197596795728e-05,
      "loss": 0.2598,
      "step": 2037
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.04825069010257721,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.2861,
      "step": 2038
    },
    {
      "epoch": 0.5437333333333333,
      "grad_norm": 0.0458039715886116,
      "learning_rate": 9.137516688918557e-05,
      "loss": 0.2651,
      "step": 2039
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.049104053527116776,
      "learning_rate": 9.132176234979973e-05,
      "loss": 0.2944,
      "step": 2040
    },
    {
      "epoch": 0.5442666666666667,
      "grad_norm": 0.08238884061574936,
      "learning_rate": 9.126835781041389e-05,
      "loss": 0.3844,
      "step": 2041
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.06687892973423004,
      "learning_rate": 9.121495327102803e-05,
      "loss": 0.3956,
      "step": 2042
    },
    {
      "epoch": 0.5448,
      "grad_norm": 0.04611071199178696,
      "learning_rate": 9.11615487316422e-05,
      "loss": 0.2397,
      "step": 2043
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.06348957121372223,
      "learning_rate": 9.110814419225636e-05,
      "loss": 0.3189,
      "step": 2044
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 0.06080150976777077,
      "learning_rate": 9.10547396528705e-05,
      "loss": 0.3255,
      "step": 2045
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.07291526347398758,
      "learning_rate": 9.100133511348466e-05,
      "loss": 0.366,
      "step": 2046
    },
    {
      "epoch": 0.5458666666666666,
      "grad_norm": 0.052786096930503845,
      "learning_rate": 9.09479305740988e-05,
      "loss": 0.3314,
      "step": 2047
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.05820774286985397,
      "learning_rate": 9.089452603471295e-05,
      "loss": 0.3587,
      "step": 2048
    },
    {
      "epoch": 0.5464,
      "grad_norm": 0.05192025750875473,
      "learning_rate": 9.084112149532711e-05,
      "loss": 0.3378,
      "step": 2049
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.07682306319475174,
      "learning_rate": 9.078771695594125e-05,
      "loss": 0.3538,
      "step": 2050
    },
    {
      "epoch": 0.5469333333333334,
      "grad_norm": 0.0534873865544796,
      "learning_rate": 9.073431241655541e-05,
      "loss": 0.3108,
      "step": 2051
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.0528823547065258,
      "learning_rate": 9.068090787716957e-05,
      "loss": 0.3854,
      "step": 2052
    },
    {
      "epoch": 0.5474666666666667,
      "grad_norm": 0.05230621248483658,
      "learning_rate": 9.062750333778371e-05,
      "loss": 0.3391,
      "step": 2053
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.0660797581076622,
      "learning_rate": 9.057409879839787e-05,
      "loss": 0.341,
      "step": 2054
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.04509476199746132,
      "learning_rate": 9.052069425901202e-05,
      "loss": 0.2774,
      "step": 2055
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.056872304528951645,
      "learning_rate": 9.046728971962616e-05,
      "loss": 0.2986,
      "step": 2056
    },
    {
      "epoch": 0.5485333333333333,
      "grad_norm": 0.054030343890190125,
      "learning_rate": 9.041388518024032e-05,
      "loss": 0.3191,
      "step": 2057
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.042458102107048035,
      "learning_rate": 9.036048064085448e-05,
      "loss": 0.2968,
      "step": 2058
    },
    {
      "epoch": 0.5490666666666667,
      "grad_norm": 0.03770986571907997,
      "learning_rate": 9.030707610146862e-05,
      "loss": 0.2561,
      "step": 2059
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.0591430626809597,
      "learning_rate": 9.025367156208279e-05,
      "loss": 0.286,
      "step": 2060
    },
    {
      "epoch": 0.5496,
      "grad_norm": 0.05278536677360535,
      "learning_rate": 9.020026702269693e-05,
      "loss": 0.3025,
      "step": 2061
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.07289789617061615,
      "learning_rate": 9.014686248331109e-05,
      "loss": 0.3738,
      "step": 2062
    },
    {
      "epoch": 0.5501333333333334,
      "grad_norm": 0.04413141310214996,
      "learning_rate": 9.009345794392525e-05,
      "loss": 0.3105,
      "step": 2063
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.06492765247821808,
      "learning_rate": 9.004005340453939e-05,
      "loss": 0.3622,
      "step": 2064
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.04529567435383797,
      "learning_rate": 8.998664886515354e-05,
      "loss": 0.2787,
      "step": 2065
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.07172831892967224,
      "learning_rate": 8.99332443257677e-05,
      "loss": 0.3783,
      "step": 2066
    },
    {
      "epoch": 0.5512,
      "grad_norm": 0.05158282071352005,
      "learning_rate": 8.987983978638184e-05,
      "loss": 0.3388,
      "step": 2067
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.05339943245053291,
      "learning_rate": 8.9826435246996e-05,
      "loss": 0.3249,
      "step": 2068
    },
    {
      "epoch": 0.5517333333333333,
      "grad_norm": 0.07193693518638611,
      "learning_rate": 8.977303070761016e-05,
      "loss": 0.3393,
      "step": 2069
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.06127611920237541,
      "learning_rate": 8.97196261682243e-05,
      "loss": 0.366,
      "step": 2070
    },
    {
      "epoch": 0.5522666666666667,
      "grad_norm": 0.05214826762676239,
      "learning_rate": 8.966622162883846e-05,
      "loss": 0.2822,
      "step": 2071
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.04719530791044235,
      "learning_rate": 8.961281708945261e-05,
      "loss": 0.3229,
      "step": 2072
    },
    {
      "epoch": 0.5528,
      "grad_norm": 0.0504070445895195,
      "learning_rate": 8.955941255006675e-05,
      "loss": 0.3394,
      "step": 2073
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.04728842154145241,
      "learning_rate": 8.950600801068091e-05,
      "loss": 0.2747,
      "step": 2074
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.050886012613773346,
      "learning_rate": 8.945260347129505e-05,
      "loss": 0.3379,
      "step": 2075
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.036328621208667755,
      "learning_rate": 8.939919893190921e-05,
      "loss": 0.227,
      "step": 2076
    },
    {
      "epoch": 0.5538666666666666,
      "grad_norm": 0.05904504656791687,
      "learning_rate": 8.934579439252338e-05,
      "loss": 0.3356,
      "step": 2077
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.05928822606801987,
      "learning_rate": 8.929238985313752e-05,
      "loss": 0.2467,
      "step": 2078
    },
    {
      "epoch": 0.5544,
      "grad_norm": 0.04352617636322975,
      "learning_rate": 8.923898531375168e-05,
      "loss": 0.2734,
      "step": 2079
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.04994674772024155,
      "learning_rate": 8.918558077436584e-05,
      "loss": 0.3131,
      "step": 2080
    },
    {
      "epoch": 0.5549333333333333,
      "grad_norm": 0.04738302528858185,
      "learning_rate": 8.913217623497998e-05,
      "loss": 0.2973,
      "step": 2081
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.056422777473926544,
      "learning_rate": 8.907877169559413e-05,
      "loss": 0.3876,
      "step": 2082
    },
    {
      "epoch": 0.5554666666666667,
      "grad_norm": 0.04928131401538849,
      "learning_rate": 8.902536715620828e-05,
      "loss": 0.2721,
      "step": 2083
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.054196614772081375,
      "learning_rate": 8.897196261682243e-05,
      "loss": 0.3001,
      "step": 2084
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.06763918697834015,
      "learning_rate": 8.891855807743659e-05,
      "loss": 0.3813,
      "step": 2085
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.04414749518036842,
      "learning_rate": 8.886515353805073e-05,
      "loss": 0.2648,
      "step": 2086
    },
    {
      "epoch": 0.5565333333333333,
      "grad_norm": 0.0583006776869297,
      "learning_rate": 8.881174899866489e-05,
      "loss": 0.3245,
      "step": 2087
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.05056437477469444,
      "learning_rate": 8.875834445927905e-05,
      "loss": 0.2873,
      "step": 2088
    },
    {
      "epoch": 0.5570666666666667,
      "grad_norm": 0.06968369334936142,
      "learning_rate": 8.870493991989319e-05,
      "loss": 0.3683,
      "step": 2089
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.06071693077683449,
      "learning_rate": 8.865153538050734e-05,
      "loss": 0.4153,
      "step": 2090
    },
    {
      "epoch": 0.5576,
      "grad_norm": 0.059044983237981796,
      "learning_rate": 8.85981308411215e-05,
      "loss": 0.3115,
      "step": 2091
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.054678596556186676,
      "learning_rate": 8.854472630173564e-05,
      "loss": 0.2892,
      "step": 2092
    },
    {
      "epoch": 0.5581333333333334,
      "grad_norm": 0.068939208984375,
      "learning_rate": 8.84913217623498e-05,
      "loss": 0.3169,
      "step": 2093
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.053282786160707474,
      "learning_rate": 8.843791722296396e-05,
      "loss": 0.3531,
      "step": 2094
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.06492121517658234,
      "learning_rate": 8.838451268357811e-05,
      "loss": 0.3537,
      "step": 2095
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.05840704217553139,
      "learning_rate": 8.833110814419227e-05,
      "loss": 0.3049,
      "step": 2096
    },
    {
      "epoch": 0.5592,
      "grad_norm": 0.06851811707019806,
      "learning_rate": 8.827770360480641e-05,
      "loss": 0.3277,
      "step": 2097
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.07819657772779465,
      "learning_rate": 8.822429906542057e-05,
      "loss": 0.3776,
      "step": 2098
    },
    {
      "epoch": 0.5597333333333333,
      "grad_norm": 0.060674287378787994,
      "learning_rate": 8.817089452603472e-05,
      "loss": 0.3175,
      "step": 2099
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.05501188710331917,
      "learning_rate": 8.811748998664887e-05,
      "loss": 0.3423,
      "step": 2100
    },
    {
      "epoch": 0.5602666666666667,
      "grad_norm": 0.06148599460721016,
      "learning_rate": 8.806408544726302e-05,
      "loss": 0.3995,
      "step": 2101
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.0547109991312027,
      "learning_rate": 8.801068090787718e-05,
      "loss": 0.2992,
      "step": 2102
    },
    {
      "epoch": 0.5608,
      "grad_norm": 0.0685858502984047,
      "learning_rate": 8.795727636849132e-05,
      "loss": 0.2963,
      "step": 2103
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.056465182453393936,
      "learning_rate": 8.790387182910548e-05,
      "loss": 0.3112,
      "step": 2104
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.07921687513589859,
      "learning_rate": 8.785046728971964e-05,
      "loss": 0.3494,
      "step": 2105
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.05754527077078819,
      "learning_rate": 8.779706275033378e-05,
      "loss": 0.3417,
      "step": 2106
    },
    {
      "epoch": 0.5618666666666666,
      "grad_norm": 0.062040168792009354,
      "learning_rate": 8.774365821094793e-05,
      "loss": 0.3901,
      "step": 2107
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.046385105699300766,
      "learning_rate": 8.769025367156208e-05,
      "loss": 0.2905,
      "step": 2108
    },
    {
      "epoch": 0.5624,
      "grad_norm": 0.04911357909440994,
      "learning_rate": 8.763684913217623e-05,
      "loss": 0.2171,
      "step": 2109
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.05299529805779457,
      "learning_rate": 8.758344459279039e-05,
      "loss": 0.3073,
      "step": 2110
    },
    {
      "epoch": 0.5629333333333333,
      "grad_norm": 0.05432106927037239,
      "learning_rate": 8.753004005340453e-05,
      "loss": 0.3205,
      "step": 2111
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.057151030749082565,
      "learning_rate": 8.74766355140187e-05,
      "loss": 0.3618,
      "step": 2112
    },
    {
      "epoch": 0.5634666666666667,
      "grad_norm": 0.05488760024309158,
      "learning_rate": 8.742323097463286e-05,
      "loss": 0.342,
      "step": 2113
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.07881304621696472,
      "learning_rate": 8.7369826435247e-05,
      "loss": 0.3575,
      "step": 2114
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.04829104617238045,
      "learning_rate": 8.731642189586116e-05,
      "loss": 0.3083,
      "step": 2115
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.055664218962192535,
      "learning_rate": 8.72630173564753e-05,
      "loss": 0.3026,
      "step": 2116
    },
    {
      "epoch": 0.5645333333333333,
      "grad_norm": 0.05771782249212265,
      "learning_rate": 8.720961281708946e-05,
      "loss": 0.3113,
      "step": 2117
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.05690954625606537,
      "learning_rate": 8.715620827770361e-05,
      "loss": 0.3469,
      "step": 2118
    },
    {
      "epoch": 0.5650666666666667,
      "grad_norm": 0.04505201429128647,
      "learning_rate": 8.710280373831776e-05,
      "loss": 0.2591,
      "step": 2119
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.04454224929213524,
      "learning_rate": 8.704939919893191e-05,
      "loss": 0.2873,
      "step": 2120
    },
    {
      "epoch": 0.5656,
      "grad_norm": 0.06053631752729416,
      "learning_rate": 8.699599465954607e-05,
      "loss": 0.3435,
      "step": 2121
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.04474702477455139,
      "learning_rate": 8.694259012016021e-05,
      "loss": 0.2603,
      "step": 2122
    },
    {
      "epoch": 0.5661333333333334,
      "grad_norm": 0.06293785572052002,
      "learning_rate": 8.688918558077437e-05,
      "loss": 0.2824,
      "step": 2123
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.05032305791974068,
      "learning_rate": 8.683578104138852e-05,
      "loss": 0.3311,
      "step": 2124
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.057508036494255066,
      "learning_rate": 8.678237650200267e-05,
      "loss": 0.3462,
      "step": 2125
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.040386784821748734,
      "learning_rate": 8.672897196261682e-05,
      "loss": 0.2485,
      "step": 2126
    },
    {
      "epoch": 0.5672,
      "grad_norm": 0.06717340648174286,
      "learning_rate": 8.667556742323098e-05,
      "loss": 0.3288,
      "step": 2127
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.0416143424808979,
      "learning_rate": 8.662216288384512e-05,
      "loss": 0.2686,
      "step": 2128
    },
    {
      "epoch": 0.5677333333333333,
      "grad_norm": 0.04386245831847191,
      "learning_rate": 8.656875834445929e-05,
      "loss": 0.2851,
      "step": 2129
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.07878731936216354,
      "learning_rate": 8.651535380507344e-05,
      "loss": 0.37,
      "step": 2130
    },
    {
      "epoch": 0.5682666666666667,
      "grad_norm": 0.04889469966292381,
      "learning_rate": 8.646194926568759e-05,
      "loss": 0.3023,
      "step": 2131
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.04534424841403961,
      "learning_rate": 8.640854472630175e-05,
      "loss": 0.2678,
      "step": 2132
    },
    {
      "epoch": 0.5688,
      "grad_norm": 0.05689619109034538,
      "learning_rate": 8.635514018691589e-05,
      "loss": 0.2718,
      "step": 2133
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.04576991870999336,
      "learning_rate": 8.630173564753005e-05,
      "loss": 0.2954,
      "step": 2134
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 0.04464512690901756,
      "learning_rate": 8.62483311081442e-05,
      "loss": 0.2319,
      "step": 2135
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.051917459815740585,
      "learning_rate": 8.619492656875835e-05,
      "loss": 0.3067,
      "step": 2136
    },
    {
      "epoch": 0.5698666666666666,
      "grad_norm": 0.06907928735017776,
      "learning_rate": 8.61415220293725e-05,
      "loss": 0.4047,
      "step": 2137
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.047099702060222626,
      "learning_rate": 8.608811748998666e-05,
      "loss": 0.2892,
      "step": 2138
    },
    {
      "epoch": 0.5704,
      "grad_norm": 0.06039952114224434,
      "learning_rate": 8.60347129506008e-05,
      "loss": 0.3319,
      "step": 2139
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.0692330077290535,
      "learning_rate": 8.598130841121496e-05,
      "loss": 0.3998,
      "step": 2140
    },
    {
      "epoch": 0.5709333333333333,
      "grad_norm": 0.04860660433769226,
      "learning_rate": 8.59279038718291e-05,
      "loss": 0.3023,
      "step": 2141
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.06295249611139297,
      "learning_rate": 8.587449933244326e-05,
      "loss": 0.3918,
      "step": 2142
    },
    {
      "epoch": 0.5714666666666667,
      "grad_norm": 0.052702803164720535,
      "learning_rate": 8.582109479305741e-05,
      "loss": 0.3098,
      "step": 2143
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.058109186589717865,
      "learning_rate": 8.576769025367156e-05,
      "loss": 0.2823,
      "step": 2144
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.06097925081849098,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.3872,
      "step": 2145
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.06179967150092125,
      "learning_rate": 8.566088117489987e-05,
      "loss": 0.3122,
      "step": 2146
    },
    {
      "epoch": 0.5725333333333333,
      "grad_norm": 0.058992501348257065,
      "learning_rate": 8.560747663551403e-05,
      "loss": 0.3832,
      "step": 2147
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.08848529309034348,
      "learning_rate": 8.555407209612818e-05,
      "loss": 0.3698,
      "step": 2148
    },
    {
      "epoch": 0.5730666666666666,
      "grad_norm": 0.055381253361701965,
      "learning_rate": 8.550066755674232e-05,
      "loss": 0.2688,
      "step": 2149
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.05476875230669975,
      "learning_rate": 8.544726301735648e-05,
      "loss": 0.3131,
      "step": 2150
    },
    {
      "epoch": 0.5736,
      "grad_norm": 0.04452929273247719,
      "learning_rate": 8.539385847797064e-05,
      "loss": 0.3358,
      "step": 2151
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.055869653820991516,
      "learning_rate": 8.534045393858478e-05,
      "loss": 0.2948,
      "step": 2152
    },
    {
      "epoch": 0.5741333333333334,
      "grad_norm": 0.04814572259783745,
      "learning_rate": 8.528704939919894e-05,
      "loss": 0.2996,
      "step": 2153
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.06782105565071106,
      "learning_rate": 8.523364485981309e-05,
      "loss": 0.3359,
      "step": 2154
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 0.05307309329509735,
      "learning_rate": 8.518024032042724e-05,
      "loss": 0.2911,
      "step": 2155
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.06787905097007751,
      "learning_rate": 8.512683578104139e-05,
      "loss": 0.4394,
      "step": 2156
    },
    {
      "epoch": 0.5752,
      "grad_norm": 0.05793537199497223,
      "learning_rate": 8.507343124165555e-05,
      "loss": 0.3363,
      "step": 2157
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.057016413658857346,
      "learning_rate": 8.502002670226969e-05,
      "loss": 0.3969,
      "step": 2158
    },
    {
      "epoch": 0.5757333333333333,
      "grad_norm": 0.05432581901550293,
      "learning_rate": 8.496662216288385e-05,
      "loss": 0.3,
      "step": 2159
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.046398598700761795,
      "learning_rate": 8.4913217623498e-05,
      "loss": 0.2428,
      "step": 2160
    },
    {
      "epoch": 0.5762666666666667,
      "grad_norm": 0.06485716253519058,
      "learning_rate": 8.485981308411215e-05,
      "loss": 0.3183,
      "step": 2161
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.06036822497844696,
      "learning_rate": 8.48064085447263e-05,
      "loss": 0.3124,
      "step": 2162
    },
    {
      "epoch": 0.5768,
      "grad_norm": 0.07708508521318436,
      "learning_rate": 8.475300400534046e-05,
      "loss": 0.3239,
      "step": 2163
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.05162462964653969,
      "learning_rate": 8.469959946595462e-05,
      "loss": 0.3128,
      "step": 2164
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.055229298770427704,
      "learning_rate": 8.464619492656877e-05,
      "loss": 0.2568,
      "step": 2165
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.04718450829386711,
      "learning_rate": 8.459279038718291e-05,
      "loss": 0.3098,
      "step": 2166
    },
    {
      "epoch": 0.5778666666666666,
      "grad_norm": 0.08052486181259155,
      "learning_rate": 8.453938584779707e-05,
      "loss": 0.3191,
      "step": 2167
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.04602828994393349,
      "learning_rate": 8.448598130841123e-05,
      "loss": 0.3004,
      "step": 2168
    },
    {
      "epoch": 0.5784,
      "grad_norm": 0.056956782937049866,
      "learning_rate": 8.443257676902537e-05,
      "loss": 0.3222,
      "step": 2169
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.04918791353702545,
      "learning_rate": 8.437917222963953e-05,
      "loss": 0.2411,
      "step": 2170
    },
    {
      "epoch": 0.5789333333333333,
      "grad_norm": 0.049550797790288925,
      "learning_rate": 8.432576769025368e-05,
      "loss": 0.3102,
      "step": 2171
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.05223238095641136,
      "learning_rate": 8.427236315086783e-05,
      "loss": 0.3085,
      "step": 2172
    },
    {
      "epoch": 0.5794666666666667,
      "grad_norm": 0.05160817131400108,
      "learning_rate": 8.421895861148198e-05,
      "loss": 0.252,
      "step": 2173
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.06840468943119049,
      "learning_rate": 8.416555407209614e-05,
      "loss": 0.3348,
      "step": 2174
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.057086121290922165,
      "learning_rate": 8.411214953271028e-05,
      "loss": 0.285,
      "step": 2175
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.05265956372022629,
      "learning_rate": 8.405874499332444e-05,
      "loss": 0.2991,
      "step": 2176
    },
    {
      "epoch": 0.5805333333333333,
      "grad_norm": 0.06891647726297379,
      "learning_rate": 8.400534045393858e-05,
      "loss": 0.3799,
      "step": 2177
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.05633201822638512,
      "learning_rate": 8.395193591455274e-05,
      "loss": 0.3191,
      "step": 2178
    },
    {
      "epoch": 0.5810666666666666,
      "grad_norm": 0.05235455185174942,
      "learning_rate": 8.389853137516689e-05,
      "loss": 0.3165,
      "step": 2179
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.06328912079334259,
      "learning_rate": 8.384512683578104e-05,
      "loss": 0.3203,
      "step": 2180
    },
    {
      "epoch": 0.5816,
      "grad_norm": 0.0579732246696949,
      "learning_rate": 8.37917222963952e-05,
      "loss": 0.3189,
      "step": 2181
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.04613204672932625,
      "learning_rate": 8.373831775700936e-05,
      "loss": 0.3305,
      "step": 2182
    },
    {
      "epoch": 0.5821333333333333,
      "grad_norm": 0.0579666867852211,
      "learning_rate": 8.36849132176235e-05,
      "loss": 0.3519,
      "step": 2183
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.0761706754565239,
      "learning_rate": 8.363150867823766e-05,
      "loss": 0.3477,
      "step": 2184
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.048558324575424194,
      "learning_rate": 8.35781041388518e-05,
      "loss": 0.3081,
      "step": 2185
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.04405654966831207,
      "learning_rate": 8.352469959946596e-05,
      "loss": 0.2782,
      "step": 2186
    },
    {
      "epoch": 0.5832,
      "grad_norm": 0.04173494130373001,
      "learning_rate": 8.347129506008012e-05,
      "loss": 0.2586,
      "step": 2187
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.05021173134446144,
      "learning_rate": 8.341789052069426e-05,
      "loss": 0.2946,
      "step": 2188
    },
    {
      "epoch": 0.5837333333333333,
      "grad_norm": 0.06287375092506409,
      "learning_rate": 8.336448598130842e-05,
      "loss": 0.3927,
      "step": 2189
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.04802561551332474,
      "learning_rate": 8.331108144192257e-05,
      "loss": 0.31,
      "step": 2190
    },
    {
      "epoch": 0.5842666666666667,
      "grad_norm": 0.04975387454032898,
      "learning_rate": 8.325767690253671e-05,
      "loss": 0.3268,
      "step": 2191
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.049935974180698395,
      "learning_rate": 8.320427236315087e-05,
      "loss": 0.2922,
      "step": 2192
    },
    {
      "epoch": 0.5848,
      "grad_norm": 0.04847044125199318,
      "learning_rate": 8.315086782376503e-05,
      "loss": 0.2966,
      "step": 2193
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.05630483850836754,
      "learning_rate": 8.309746328437917e-05,
      "loss": 0.3518,
      "step": 2194
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.055492714047431946,
      "learning_rate": 8.304405874499333e-05,
      "loss": 0.3668,
      "step": 2195
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.04426097124814987,
      "learning_rate": 8.299065420560748e-05,
      "loss": 0.2638,
      "step": 2196
    },
    {
      "epoch": 0.5858666666666666,
      "grad_norm": 0.05878639966249466,
      "learning_rate": 8.293724966622163e-05,
      "loss": 0.3024,
      "step": 2197
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.04864776134490967,
      "learning_rate": 8.288384512683578e-05,
      "loss": 0.3339,
      "step": 2198
    },
    {
      "epoch": 0.5864,
      "grad_norm": 0.054567914456129074,
      "learning_rate": 8.283044058744994e-05,
      "loss": 0.3102,
      "step": 2199
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.05899261310696602,
      "learning_rate": 8.27770360480641e-05,
      "loss": 0.3826,
      "step": 2200
    },
    {
      "epoch": 0.5869333333333333,
      "grad_norm": 0.05520480126142502,
      "learning_rate": 8.272363150867825e-05,
      "loss": 0.3041,
      "step": 2201
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.04889727756381035,
      "learning_rate": 8.267022696929239e-05,
      "loss": 0.2671,
      "step": 2202
    },
    {
      "epoch": 0.5874666666666667,
      "grad_norm": 0.051127780228853226,
      "learning_rate": 8.261682242990655e-05,
      "loss": 0.2774,
      "step": 2203
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.0546022467315197,
      "learning_rate": 8.25634178905207e-05,
      "loss": 0.3537,
      "step": 2204
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.06779325753450394,
      "learning_rate": 8.251001335113485e-05,
      "loss": 0.3963,
      "step": 2205
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.06652338057756424,
      "learning_rate": 8.2456608811749e-05,
      "loss": 0.4074,
      "step": 2206
    },
    {
      "epoch": 0.5885333333333334,
      "grad_norm": 0.045576825737953186,
      "learning_rate": 8.240320427236316e-05,
      "loss": 0.2472,
      "step": 2207
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.057482726871967316,
      "learning_rate": 8.23497997329773e-05,
      "loss": 0.3596,
      "step": 2208
    },
    {
      "epoch": 0.5890666666666666,
      "grad_norm": 0.06311127543449402,
      "learning_rate": 8.229639519359146e-05,
      "loss": 0.3029,
      "step": 2209
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.04617143049836159,
      "learning_rate": 8.22429906542056e-05,
      "loss": 0.2557,
      "step": 2210
    },
    {
      "epoch": 0.5896,
      "grad_norm": 0.05895448103547096,
      "learning_rate": 8.218958611481976e-05,
      "loss": 0.3452,
      "step": 2211
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.04400954768061638,
      "learning_rate": 8.213618157543392e-05,
      "loss": 0.2531,
      "step": 2212
    },
    {
      "epoch": 0.5901333333333333,
      "grad_norm": 0.04604983329772949,
      "learning_rate": 8.208277703604806e-05,
      "loss": 0.2752,
      "step": 2213
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.06381207704544067,
      "learning_rate": 8.202937249666221e-05,
      "loss": 0.3335,
      "step": 2214
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 0.06854193657636642,
      "learning_rate": 8.197596795727637e-05,
      "loss": 0.3415,
      "step": 2215
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.0474807471036911,
      "learning_rate": 8.192256341789053e-05,
      "loss": 0.3038,
      "step": 2216
    },
    {
      "epoch": 0.5912,
      "grad_norm": 0.06620773673057556,
      "learning_rate": 8.186915887850468e-05,
      "loss": 0.381,
      "step": 2217
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.04927816241979599,
      "learning_rate": 8.181575433911883e-05,
      "loss": 0.289,
      "step": 2218
    },
    {
      "epoch": 0.5917333333333333,
      "grad_norm": 0.05255031958222389,
      "learning_rate": 8.176234979973298e-05,
      "loss": 0.3579,
      "step": 2219
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.060897644609212875,
      "learning_rate": 8.170894526034714e-05,
      "loss": 0.3044,
      "step": 2220
    },
    {
      "epoch": 0.5922666666666667,
      "grad_norm": 0.06554660946130753,
      "learning_rate": 8.165554072096128e-05,
      "loss": 0.3789,
      "step": 2221
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.06564357876777649,
      "learning_rate": 8.160213618157544e-05,
      "loss": 0.4004,
      "step": 2222
    },
    {
      "epoch": 0.5928,
      "grad_norm": 0.050786662846803665,
      "learning_rate": 8.15487316421896e-05,
      "loss": 0.3093,
      "step": 2223
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.05614625662565231,
      "learning_rate": 8.149532710280374e-05,
      "loss": 0.2829,
      "step": 2224
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.04638944938778877,
      "learning_rate": 8.14419225634179e-05,
      "loss": 0.3055,
      "step": 2225
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.06354270130395889,
      "learning_rate": 8.138851802403205e-05,
      "loss": 0.3759,
      "step": 2226
    },
    {
      "epoch": 0.5938666666666667,
      "grad_norm": 0.05888904258608818,
      "learning_rate": 8.133511348464619e-05,
      "loss": 0.3044,
      "step": 2227
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.05578817427158356,
      "learning_rate": 8.128170894526035e-05,
      "loss": 0.3176,
      "step": 2228
    },
    {
      "epoch": 0.5944,
      "grad_norm": 0.05294456705451012,
      "learning_rate": 8.12283044058745e-05,
      "loss": 0.2962,
      "step": 2229
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.057447031140327454,
      "learning_rate": 8.117489986648865e-05,
      "loss": 0.308,
      "step": 2230
    },
    {
      "epoch": 0.5949333333333333,
      "grad_norm": 0.044100210070610046,
      "learning_rate": 8.11214953271028e-05,
      "loss": 0.2468,
      "step": 2231
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.05275057628750801,
      "learning_rate": 8.106809078771696e-05,
      "loss": 0.3188,
      "step": 2232
    },
    {
      "epoch": 0.5954666666666667,
      "grad_norm": 0.06408274173736572,
      "learning_rate": 8.101468624833112e-05,
      "loss": 0.321,
      "step": 2233
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.04309584200382233,
      "learning_rate": 8.096128170894527e-05,
      "loss": 0.2894,
      "step": 2234
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.06532064080238342,
      "learning_rate": 8.090787716955942e-05,
      "loss": 0.3747,
      "step": 2235
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.06746917963027954,
      "learning_rate": 8.085447263017357e-05,
      "loss": 0.3061,
      "step": 2236
    },
    {
      "epoch": 0.5965333333333334,
      "grad_norm": 0.05484816059470177,
      "learning_rate": 8.080106809078773e-05,
      "loss": 0.3079,
      "step": 2237
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.06017300859093666,
      "learning_rate": 8.074766355140187e-05,
      "loss": 0.367,
      "step": 2238
    },
    {
      "epoch": 0.5970666666666666,
      "grad_norm": 0.044003926217556,
      "learning_rate": 8.069425901201603e-05,
      "loss": 0.2753,
      "step": 2239
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.058647818863391876,
      "learning_rate": 8.064085447263018e-05,
      "loss": 0.3454,
      "step": 2240
    },
    {
      "epoch": 0.5976,
      "grad_norm": 0.06662964075803757,
      "learning_rate": 8.058744993324433e-05,
      "loss": 0.3595,
      "step": 2241
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.0479523204267025,
      "learning_rate": 8.053404539385848e-05,
      "loss": 0.2383,
      "step": 2242
    },
    {
      "epoch": 0.5981333333333333,
      "grad_norm": 0.04448952525854111,
      "learning_rate": 8.048064085447263e-05,
      "loss": 0.2889,
      "step": 2243
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.05212186649441719,
      "learning_rate": 8.042723631508678e-05,
      "loss": 0.2668,
      "step": 2244
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.06402561068534851,
      "learning_rate": 8.037383177570094e-05,
      "loss": 0.3826,
      "step": 2245
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.05606416240334511,
      "learning_rate": 8.032042723631508e-05,
      "loss": 0.3195,
      "step": 2246
    },
    {
      "epoch": 0.5992,
      "grad_norm": 0.054701704531908035,
      "learning_rate": 8.026702269692924e-05,
      "loss": 0.3113,
      "step": 2247
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.05626337230205536,
      "learning_rate": 8.02136181575434e-05,
      "loss": 0.3186,
      "step": 2248
    },
    {
      "epoch": 0.5997333333333333,
      "grad_norm": 0.0554429367184639,
      "learning_rate": 8.016021361815754e-05,
      "loss": 0.3487,
      "step": 2249
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04728640615940094,
      "learning_rate": 8.01068090787717e-05,
      "loss": 0.2798,
      "step": 2250
    },
    {
      "epoch": 0.6002666666666666,
      "grad_norm": 0.07151315361261368,
      "learning_rate": 8.005340453938585e-05,
      "loss": 0.4098,
      "step": 2251
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.06878028064966202,
      "learning_rate": 8e-05,
      "loss": 0.3713,
      "step": 2252
    },
    {
      "epoch": 0.6008,
      "grad_norm": 0.06382346153259277,
      "learning_rate": 7.994659546061416e-05,
      "loss": 0.3239,
      "step": 2253
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.060875777155160904,
      "learning_rate": 7.98931909212283e-05,
      "loss": 0.3669,
      "step": 2254
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.060269713401794434,
      "learning_rate": 7.983978638184246e-05,
      "loss": 0.3253,
      "step": 2255
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.047996729612350464,
      "learning_rate": 7.978638184245662e-05,
      "loss": 0.2792,
      "step": 2256
    },
    {
      "epoch": 0.6018666666666667,
      "grad_norm": 0.04473169520497322,
      "learning_rate": 7.973297730307076e-05,
      "loss": 0.2504,
      "step": 2257
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.08107296377420425,
      "learning_rate": 7.967957276368492e-05,
      "loss": 0.3546,
      "step": 2258
    },
    {
      "epoch": 0.6024,
      "grad_norm": 0.051662154495716095,
      "learning_rate": 7.962616822429907e-05,
      "loss": 0.2978,
      "step": 2259
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.05315504968166351,
      "learning_rate": 7.957276368491322e-05,
      "loss": 0.31,
      "step": 2260
    },
    {
      "epoch": 0.6029333333333333,
      "grad_norm": 0.05299018323421478,
      "learning_rate": 7.951935914552737e-05,
      "loss": 0.3264,
      "step": 2261
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.04883914440870285,
      "learning_rate": 7.946595460614153e-05,
      "loss": 0.2512,
      "step": 2262
    },
    {
      "epoch": 0.6034666666666667,
      "grad_norm": 0.045542921870946884,
      "learning_rate": 7.941255006675567e-05,
      "loss": 0.3013,
      "step": 2263
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.05402005463838577,
      "learning_rate": 7.935914552736983e-05,
      "loss": 0.2484,
      "step": 2264
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.0598875992000103,
      "learning_rate": 7.930574098798398e-05,
      "loss": 0.3234,
      "step": 2265
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.059043046087026596,
      "learning_rate": 7.925233644859813e-05,
      "loss": 0.3369,
      "step": 2266
    },
    {
      "epoch": 0.6045333333333334,
      "grad_norm": 0.06262720376253128,
      "learning_rate": 7.919893190921228e-05,
      "loss": 0.3523,
      "step": 2267
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.05924385413527489,
      "learning_rate": 7.914552736982644e-05,
      "loss": 0.301,
      "step": 2268
    },
    {
      "epoch": 0.6050666666666666,
      "grad_norm": 0.04722832515835762,
      "learning_rate": 7.90921228304406e-05,
      "loss": 0.3237,
      "step": 2269
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.044955506920814514,
      "learning_rate": 7.903871829105475e-05,
      "loss": 0.2355,
      "step": 2270
    },
    {
      "epoch": 0.6056,
      "grad_norm": 0.04612782225012779,
      "learning_rate": 7.89853137516689e-05,
      "loss": 0.2531,
      "step": 2271
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.0640505999326706,
      "learning_rate": 7.893190921228305e-05,
      "loss": 0.3462,
      "step": 2272
    },
    {
      "epoch": 0.6061333333333333,
      "grad_norm": 0.046163275837898254,
      "learning_rate": 7.887850467289721e-05,
      "loss": 0.261,
      "step": 2273
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.0601765476167202,
      "learning_rate": 7.882510013351135e-05,
      "loss": 0.2965,
      "step": 2274
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.059424132108688354,
      "learning_rate": 7.877169559412551e-05,
      "loss": 0.3721,
      "step": 2275
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.05392314866185188,
      "learning_rate": 7.871829105473965e-05,
      "loss": 0.3516,
      "step": 2276
    },
    {
      "epoch": 0.6072,
      "grad_norm": 0.05802631750702858,
      "learning_rate": 7.86648865153538e-05,
      "loss": 0.2207,
      "step": 2277
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.051772210747003555,
      "learning_rate": 7.861148197596796e-05,
      "loss": 0.2899,
      "step": 2278
    },
    {
      "epoch": 0.6077333333333333,
      "grad_norm": 0.049870606511831284,
      "learning_rate": 7.85580774365821e-05,
      "loss": 0.2967,
      "step": 2279
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.05600665137171745,
      "learning_rate": 7.850467289719626e-05,
      "loss": 0.3817,
      "step": 2280
    },
    {
      "epoch": 0.6082666666666666,
      "grad_norm": 0.06253787130117416,
      "learning_rate": 7.845126835781042e-05,
      "loss": 0.3797,
      "step": 2281
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.055785391479730606,
      "learning_rate": 7.839786381842456e-05,
      "loss": 0.3114,
      "step": 2282
    },
    {
      "epoch": 0.6088,
      "grad_norm": 0.0642683357000351,
      "learning_rate": 7.834445927903872e-05,
      "loss": 0.3499,
      "step": 2283
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.047299232333898544,
      "learning_rate": 7.829105473965287e-05,
      "loss": 0.2981,
      "step": 2284
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.048362914472818375,
      "learning_rate": 7.823765020026703e-05,
      "loss": 0.3333,
      "step": 2285
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.06647505611181259,
      "learning_rate": 7.818424566088119e-05,
      "loss": 0.3208,
      "step": 2286
    },
    {
      "epoch": 0.6098666666666667,
      "grad_norm": 0.06586118787527084,
      "learning_rate": 7.813084112149533e-05,
      "loss": 0.3409,
      "step": 2287
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.07633107155561447,
      "learning_rate": 7.807743658210949e-05,
      "loss": 0.3892,
      "step": 2288
    },
    {
      "epoch": 0.6104,
      "grad_norm": 0.06590767949819565,
      "learning_rate": 7.802403204272364e-05,
      "loss": 0.2997,
      "step": 2289
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.04556238278746605,
      "learning_rate": 7.797062750333778e-05,
      "loss": 0.2913,
      "step": 2290
    },
    {
      "epoch": 0.6109333333333333,
      "grad_norm": 0.05628935992717743,
      "learning_rate": 7.791722296395194e-05,
      "loss": 0.3172,
      "step": 2291
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.05049237236380577,
      "learning_rate": 7.78638184245661e-05,
      "loss": 0.2788,
      "step": 2292
    },
    {
      "epoch": 0.6114666666666667,
      "grad_norm": 0.054463181644678116,
      "learning_rate": 7.781041388518024e-05,
      "loss": 0.3153,
      "step": 2293
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.0611545667052269,
      "learning_rate": 7.77570093457944e-05,
      "loss": 0.2841,
      "step": 2294
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.0609879270195961,
      "learning_rate": 7.770360480640855e-05,
      "loss": 0.3654,
      "step": 2295
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.048222072422504425,
      "learning_rate": 7.76502002670227e-05,
      "loss": 0.2709,
      "step": 2296
    },
    {
      "epoch": 0.6125333333333334,
      "grad_norm": 0.06401277333498001,
      "learning_rate": 7.759679572763685e-05,
      "loss": 0.3394,
      "step": 2297
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.05580888316035271,
      "learning_rate": 7.754339118825101e-05,
      "loss": 0.3502,
      "step": 2298
    },
    {
      "epoch": 0.6130666666666666,
      "grad_norm": 0.07045914232730865,
      "learning_rate": 7.748998664886515e-05,
      "loss": 0.3526,
      "step": 2299
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.05480235069990158,
      "learning_rate": 7.743658210947931e-05,
      "loss": 0.2824,
      "step": 2300
    },
    {
      "epoch": 0.6136,
      "grad_norm": 0.06900127977132797,
      "learning_rate": 7.738317757009346e-05,
      "loss": 0.4046,
      "step": 2301
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.06737373769283295,
      "learning_rate": 7.73297730307076e-05,
      "loss": 0.37,
      "step": 2302
    },
    {
      "epoch": 0.6141333333333333,
      "grad_norm": 0.05163855105638504,
      "learning_rate": 7.727636849132178e-05,
      "loss": 0.3193,
      "step": 2303
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.049035683274269104,
      "learning_rate": 7.722296395193592e-05,
      "loss": 0.2839,
      "step": 2304
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.05503738671541214,
      "learning_rate": 7.716955941255008e-05,
      "loss": 0.2782,
      "step": 2305
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.05705302208662033,
      "learning_rate": 7.711615487316423e-05,
      "loss": 0.3426,
      "step": 2306
    },
    {
      "epoch": 0.6152,
      "grad_norm": 0.06426290422677994,
      "learning_rate": 7.706275033377837e-05,
      "loss": 0.352,
      "step": 2307
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.0571623258292675,
      "learning_rate": 7.700934579439253e-05,
      "loss": 0.3274,
      "step": 2308
    },
    {
      "epoch": 0.6157333333333334,
      "grad_norm": 0.04191785678267479,
      "learning_rate": 7.695594125500669e-05,
      "loss": 0.2666,
      "step": 2309
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.040663812309503555,
      "learning_rate": 7.690253671562083e-05,
      "loss": 0.2642,
      "step": 2310
    },
    {
      "epoch": 0.6162666666666666,
      "grad_norm": 0.04983358085155487,
      "learning_rate": 7.684913217623499e-05,
      "loss": 0.2786,
      "step": 2311
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.048613179475069046,
      "learning_rate": 7.679572763684913e-05,
      "loss": 0.2721,
      "step": 2312
    },
    {
      "epoch": 0.6168,
      "grad_norm": 0.046826958656311035,
      "learning_rate": 7.674232309746329e-05,
      "loss": 0.2938,
      "step": 2313
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.0484330989420414,
      "learning_rate": 7.668891855807744e-05,
      "loss": 0.2766,
      "step": 2314
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.05251442641019821,
      "learning_rate": 7.663551401869158e-05,
      "loss": 0.3275,
      "step": 2315
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.04733272269368172,
      "learning_rate": 7.658210947930574e-05,
      "loss": 0.2955,
      "step": 2316
    },
    {
      "epoch": 0.6178666666666667,
      "grad_norm": 0.05229875072836876,
      "learning_rate": 7.65287049399199e-05,
      "loss": 0.3317,
      "step": 2317
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.05667095631361008,
      "learning_rate": 7.647530040053404e-05,
      "loss": 0.3153,
      "step": 2318
    },
    {
      "epoch": 0.6184,
      "grad_norm": 0.05012683570384979,
      "learning_rate": 7.64218958611482e-05,
      "loss": 0.3436,
      "step": 2319
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.06299381703138351,
      "learning_rate": 7.636849132176235e-05,
      "loss": 0.3158,
      "step": 2320
    },
    {
      "epoch": 0.6189333333333333,
      "grad_norm": 0.0699426457285881,
      "learning_rate": 7.631508678237651e-05,
      "loss": 0.3494,
      "step": 2321
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.06311653554439545,
      "learning_rate": 7.626168224299067e-05,
      "loss": 0.3256,
      "step": 2322
    },
    {
      "epoch": 0.6194666666666667,
      "grad_norm": 0.04762696474790573,
      "learning_rate": 7.620827770360481e-05,
      "loss": 0.2998,
      "step": 2323
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.06412415951490402,
      "learning_rate": 7.615487316421896e-05,
      "loss": 0.374,
      "step": 2324
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.04942700266838074,
      "learning_rate": 7.610146862483312e-05,
      "loss": 0.2945,
      "step": 2325
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.054078470915555954,
      "learning_rate": 7.604806408544726e-05,
      "loss": 0.2683,
      "step": 2326
    },
    {
      "epoch": 0.6205333333333334,
      "grad_norm": 0.06297279894351959,
      "learning_rate": 7.599465954606142e-05,
      "loss": 0.2892,
      "step": 2327
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.07002100348472595,
      "learning_rate": 7.594125500667558e-05,
      "loss": 0.3379,
      "step": 2328
    },
    {
      "epoch": 0.6210666666666667,
      "grad_norm": 0.057450227439403534,
      "learning_rate": 7.588785046728972e-05,
      "loss": 0.3276,
      "step": 2329
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.050996165722608566,
      "learning_rate": 7.583444592790388e-05,
      "loss": 0.3247,
      "step": 2330
    },
    {
      "epoch": 0.6216,
      "grad_norm": 0.05793742835521698,
      "learning_rate": 7.578104138851803e-05,
      "loss": 0.3372,
      "step": 2331
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.097939133644104,
      "learning_rate": 7.572763684913217e-05,
      "loss": 0.3094,
      "step": 2332
    },
    {
      "epoch": 0.6221333333333333,
      "grad_norm": 0.058532267808914185,
      "learning_rate": 7.567423230974633e-05,
      "loss": 0.3606,
      "step": 2333
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.052668701857328415,
      "learning_rate": 7.562082777036049e-05,
      "loss": 0.3311,
      "step": 2334
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.05914457514882088,
      "learning_rate": 7.556742323097463e-05,
      "loss": 0.3133,
      "step": 2335
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.045773040503263474,
      "learning_rate": 7.551401869158879e-05,
      "loss": 0.2884,
      "step": 2336
    },
    {
      "epoch": 0.6232,
      "grad_norm": 0.05073098838329315,
      "learning_rate": 7.546061415220294e-05,
      "loss": 0.284,
      "step": 2337
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.051214151084423065,
      "learning_rate": 7.54072096128171e-05,
      "loss": 0.2656,
      "step": 2338
    },
    {
      "epoch": 0.6237333333333334,
      "grad_norm": 0.047444749623537064,
      "learning_rate": 7.535380507343126e-05,
      "loss": 0.2797,
      "step": 2339
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.05201632156968117,
      "learning_rate": 7.53004005340454e-05,
      "loss": 0.3433,
      "step": 2340
    },
    {
      "epoch": 0.6242666666666666,
      "grad_norm": 0.04852276295423508,
      "learning_rate": 7.524699599465955e-05,
      "loss": 0.2927,
      "step": 2341
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.03849390149116516,
      "learning_rate": 7.519359145527371e-05,
      "loss": 0.2531,
      "step": 2342
    },
    {
      "epoch": 0.6248,
      "grad_norm": 0.07469802349805832,
      "learning_rate": 7.514018691588785e-05,
      "loss": 0.4065,
      "step": 2343
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.04798667132854462,
      "learning_rate": 7.508678237650201e-05,
      "loss": 0.2991,
      "step": 2344
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.052518121898174286,
      "learning_rate": 7.503337783711615e-05,
      "loss": 0.2975,
      "step": 2345
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.0583636648952961,
      "learning_rate": 7.497997329773031e-05,
      "loss": 0.3211,
      "step": 2346
    },
    {
      "epoch": 0.6258666666666667,
      "grad_norm": 0.05452501401305199,
      "learning_rate": 7.492656875834447e-05,
      "loss": 0.3411,
      "step": 2347
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.058962006121873856,
      "learning_rate": 7.487316421895861e-05,
      "loss": 0.349,
      "step": 2348
    },
    {
      "epoch": 0.6264,
      "grad_norm": 0.06200900673866272,
      "learning_rate": 7.481975967957276e-05,
      "loss": 0.3271,
      "step": 2349
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.054855965077877045,
      "learning_rate": 7.476635514018692e-05,
      "loss": 0.2939,
      "step": 2350
    },
    {
      "epoch": 0.6269333333333333,
      "grad_norm": 0.052056483924388885,
      "learning_rate": 7.471295060080106e-05,
      "loss": 0.3112,
      "step": 2351
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.04254452884197235,
      "learning_rate": 7.465954606141522e-05,
      "loss": 0.2838,
      "step": 2352
    },
    {
      "epoch": 0.6274666666666666,
      "grad_norm": 0.054709360003471375,
      "learning_rate": 7.460614152202938e-05,
      "loss": 0.3475,
      "step": 2353
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.056017789989709854,
      "learning_rate": 7.455273698264352e-05,
      "loss": 0.3052,
      "step": 2354
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.0573541559278965,
      "learning_rate": 7.449933244325769e-05,
      "loss": 0.2643,
      "step": 2355
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.06106696277856827,
      "learning_rate": 7.444592790387183e-05,
      "loss": 0.3592,
      "step": 2356
    },
    {
      "epoch": 0.6285333333333334,
      "grad_norm": 0.04895985126495361,
      "learning_rate": 7.439252336448599e-05,
      "loss": 0.2869,
      "step": 2357
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.045676376670598984,
      "learning_rate": 7.433911882510014e-05,
      "loss": 0.2868,
      "step": 2358
    },
    {
      "epoch": 0.6290666666666667,
      "grad_norm": 0.04972654581069946,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.3167,
      "step": 2359
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.06918134540319443,
      "learning_rate": 7.423230974632844e-05,
      "loss": 0.366,
      "step": 2360
    },
    {
      "epoch": 0.6296,
      "grad_norm": 0.05597590282559395,
      "learning_rate": 7.41789052069426e-05,
      "loss": 0.3296,
      "step": 2361
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.05593318119645119,
      "learning_rate": 7.412550066755674e-05,
      "loss": 0.3158,
      "step": 2362
    },
    {
      "epoch": 0.6301333333333333,
      "grad_norm": 0.05086497217416763,
      "learning_rate": 7.40720961281709e-05,
      "loss": 0.2746,
      "step": 2363
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.06927716732025146,
      "learning_rate": 7.401869158878506e-05,
      "loss": 0.384,
      "step": 2364
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 0.05188533663749695,
      "learning_rate": 7.39652870493992e-05,
      "loss": 0.3549,
      "step": 2365
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.05355178192257881,
      "learning_rate": 7.391188251001335e-05,
      "loss": 0.3643,
      "step": 2366
    },
    {
      "epoch": 0.6312,
      "grad_norm": 0.04188466817140579,
      "learning_rate": 7.385847797062751e-05,
      "loss": 0.2788,
      "step": 2367
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.047773465514183044,
      "learning_rate": 7.380507343124165e-05,
      "loss": 0.2782,
      "step": 2368
    },
    {
      "epoch": 0.6317333333333334,
      "grad_norm": 0.05306050926446915,
      "learning_rate": 7.375166889185581e-05,
      "loss": 0.2987,
      "step": 2369
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.06686136871576309,
      "learning_rate": 7.369826435246995e-05,
      "loss": 0.2697,
      "step": 2370
    },
    {
      "epoch": 0.6322666666666666,
      "grad_norm": 0.06327085942029953,
      "learning_rate": 7.364485981308411e-05,
      "loss": 0.4057,
      "step": 2371
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.05776102840900421,
      "learning_rate": 7.359145527369828e-05,
      "loss": 0.29,
      "step": 2372
    },
    {
      "epoch": 0.6328,
      "grad_norm": 0.05054124444723129,
      "learning_rate": 7.353805073431242e-05,
      "loss": 0.3737,
      "step": 2373
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.04670098423957825,
      "learning_rate": 7.348464619492658e-05,
      "loss": 0.2639,
      "step": 2374
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.05699268355965614,
      "learning_rate": 7.343124165554073e-05,
      "loss": 0.3762,
      "step": 2375
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.05039673298597336,
      "learning_rate": 7.337783711615488e-05,
      "loss": 0.3334,
      "step": 2376
    },
    {
      "epoch": 0.6338666666666667,
      "grad_norm": 0.043750397861003876,
      "learning_rate": 7.332443257676903e-05,
      "loss": 0.2874,
      "step": 2377
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.06856250762939453,
      "learning_rate": 7.327102803738318e-05,
      "loss": 0.346,
      "step": 2378
    },
    {
      "epoch": 0.6344,
      "grad_norm": 0.04905683919787407,
      "learning_rate": 7.321762349799733e-05,
      "loss": 0.2976,
      "step": 2379
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.05518878996372223,
      "learning_rate": 7.316421895861149e-05,
      "loss": 0.2812,
      "step": 2380
    },
    {
      "epoch": 0.6349333333333333,
      "grad_norm": 0.04870549589395523,
      "learning_rate": 7.311081441922563e-05,
      "loss": 0.3202,
      "step": 2381
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.05617964267730713,
      "learning_rate": 7.305740987983979e-05,
      "loss": 0.2859,
      "step": 2382
    },
    {
      "epoch": 0.6354666666666666,
      "grad_norm": 0.07110628485679626,
      "learning_rate": 7.300400534045394e-05,
      "loss": 0.3796,
      "step": 2383
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.060637105256319046,
      "learning_rate": 7.295060080106809e-05,
      "loss": 0.3758,
      "step": 2384
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.05412253364920616,
      "learning_rate": 7.289719626168224e-05,
      "loss": 0.301,
      "step": 2385
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.04674958437681198,
      "learning_rate": 7.28437917222964e-05,
      "loss": 0.284,
      "step": 2386
    },
    {
      "epoch": 0.6365333333333333,
      "grad_norm": 0.05119146406650543,
      "learning_rate": 7.279038718291054e-05,
      "loss": 0.2893,
      "step": 2387
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.05186848342418671,
      "learning_rate": 7.27369826435247e-05,
      "loss": 0.3389,
      "step": 2388
    },
    {
      "epoch": 0.6370666666666667,
      "grad_norm": 0.04610108584165573,
      "learning_rate": 7.268357810413885e-05,
      "loss": 0.3012,
      "step": 2389
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.051489558070898056,
      "learning_rate": 7.263017356475301e-05,
      "loss": 0.2807,
      "step": 2390
    },
    {
      "epoch": 0.6376,
      "grad_norm": 0.05464235693216324,
      "learning_rate": 7.257676902536717e-05,
      "loss": 0.3628,
      "step": 2391
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.05521269515156746,
      "learning_rate": 7.252336448598131e-05,
      "loss": 0.3102,
      "step": 2392
    },
    {
      "epoch": 0.6381333333333333,
      "grad_norm": 0.05341680347919464,
      "learning_rate": 7.246995994659547e-05,
      "loss": 0.384,
      "step": 2393
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.05064660310745239,
      "learning_rate": 7.241655540720962e-05,
      "loss": 0.2583,
      "step": 2394
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 0.047799814492464066,
      "learning_rate": 7.236315086782377e-05,
      "loss": 0.2974,
      "step": 2395
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.05045534670352936,
      "learning_rate": 7.230974632843792e-05,
      "loss": 0.3095,
      "step": 2396
    },
    {
      "epoch": 0.6392,
      "grad_norm": 0.061625298112630844,
      "learning_rate": 7.225634178905208e-05,
      "loss": 0.3421,
      "step": 2397
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.058047741651535034,
      "learning_rate": 7.220293724966622e-05,
      "loss": 0.3605,
      "step": 2398
    },
    {
      "epoch": 0.6397333333333334,
      "grad_norm": 0.0532648041844368,
      "learning_rate": 7.214953271028038e-05,
      "loss": 0.3072,
      "step": 2399
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.05638938024640083,
      "learning_rate": 7.209612817089453e-05,
      "loss": 0.2857,
      "step": 2400
    },
    {
      "epoch": 0.6402666666666667,
      "grad_norm": 0.050889212638139725,
      "learning_rate": 7.204272363150868e-05,
      "loss": 0.2953,
      "step": 2401
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.043511465191841125,
      "learning_rate": 7.198931909212283e-05,
      "loss": 0.2957,
      "step": 2402
    },
    {
      "epoch": 0.6408,
      "grad_norm": 0.0586034394800663,
      "learning_rate": 7.193591455273699e-05,
      "loss": 0.3635,
      "step": 2403
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.042624883353710175,
      "learning_rate": 7.188251001335113e-05,
      "loss": 0.2681,
      "step": 2404
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.06100308150053024,
      "learning_rate": 7.182910547396529e-05,
      "loss": 0.3224,
      "step": 2405
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.05204516276717186,
      "learning_rate": 7.177570093457943e-05,
      "loss": 0.2988,
      "step": 2406
    },
    {
      "epoch": 0.6418666666666667,
      "grad_norm": 0.05576854199171066,
      "learning_rate": 7.17222963951936e-05,
      "loss": 0.3414,
      "step": 2407
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.04922765865921974,
      "learning_rate": 7.166889185580776e-05,
      "loss": 0.3213,
      "step": 2408
    },
    {
      "epoch": 0.6424,
      "grad_norm": 0.049958907067775726,
      "learning_rate": 7.16154873164219e-05,
      "loss": 0.3343,
      "step": 2409
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.0526292510330677,
      "learning_rate": 7.156208277703606e-05,
      "loss": 0.3152,
      "step": 2410
    },
    {
      "epoch": 0.6429333333333334,
      "grad_norm": 0.04555627703666687,
      "learning_rate": 7.150867823765021e-05,
      "loss": 0.2845,
      "step": 2411
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.04732869192957878,
      "learning_rate": 7.145527369826436e-05,
      "loss": 0.2715,
      "step": 2412
    },
    {
      "epoch": 0.6434666666666666,
      "grad_norm": 0.05558439716696739,
      "learning_rate": 7.140186915887851e-05,
      "loss": 0.297,
      "step": 2413
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.05476979911327362,
      "learning_rate": 7.134846461949265e-05,
      "loss": 0.2894,
      "step": 2414
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.04802623391151428,
      "learning_rate": 7.129506008010681e-05,
      "loss": 0.2959,
      "step": 2415
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.05704300105571747,
      "learning_rate": 7.124165554072097e-05,
      "loss": 0.3077,
      "step": 2416
    },
    {
      "epoch": 0.6445333333333333,
      "grad_norm": 0.0568353645503521,
      "learning_rate": 7.118825100133511e-05,
      "loss": 0.3275,
      "step": 2417
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.06441717594861984,
      "learning_rate": 7.113484646194927e-05,
      "loss": 0.3222,
      "step": 2418
    },
    {
      "epoch": 0.6450666666666667,
      "grad_norm": 0.05685964971780777,
      "learning_rate": 7.108144192256342e-05,
      "loss": 0.3333,
      "step": 2419
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.06208135932683945,
      "learning_rate": 7.102803738317757e-05,
      "loss": 0.4113,
      "step": 2420
    },
    {
      "epoch": 0.6456,
      "grad_norm": 0.0488571859896183,
      "learning_rate": 7.097463284379172e-05,
      "loss": 0.3236,
      "step": 2421
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.04720805585384369,
      "learning_rate": 7.092122830440588e-05,
      "loss": 0.2809,
      "step": 2422
    },
    {
      "epoch": 0.6461333333333333,
      "grad_norm": 0.05706614628434181,
      "learning_rate": 7.086782376502002e-05,
      "loss": 0.3119,
      "step": 2423
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.04163157939910889,
      "learning_rate": 7.081441922563419e-05,
      "loss": 0.31,
      "step": 2424
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.0531226322054863,
      "learning_rate": 7.076101468624833e-05,
      "loss": 0.2763,
      "step": 2425
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.047985728830099106,
      "learning_rate": 7.070761014686249e-05,
      "loss": 0.3238,
      "step": 2426
    },
    {
      "epoch": 0.6472,
      "grad_norm": 0.05015327036380768,
      "learning_rate": 7.065420560747665e-05,
      "loss": 0.2922,
      "step": 2427
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.05773547664284706,
      "learning_rate": 7.060080106809079e-05,
      "loss": 0.2997,
      "step": 2428
    },
    {
      "epoch": 0.6477333333333334,
      "grad_norm": 0.051639098674058914,
      "learning_rate": 7.054739652870495e-05,
      "loss": 0.2752,
      "step": 2429
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.04331545904278755,
      "learning_rate": 7.04939919893191e-05,
      "loss": 0.2517,
      "step": 2430
    },
    {
      "epoch": 0.6482666666666667,
      "grad_norm": 0.05474596470594406,
      "learning_rate": 7.044058744993324e-05,
      "loss": 0.3242,
      "step": 2431
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.04267202317714691,
      "learning_rate": 7.03871829105474e-05,
      "loss": 0.2839,
      "step": 2432
    },
    {
      "epoch": 0.6488,
      "grad_norm": 0.05187568441033363,
      "learning_rate": 7.033377837116156e-05,
      "loss": 0.2857,
      "step": 2433
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.06886210292577744,
      "learning_rate": 7.02803738317757e-05,
      "loss": 0.3266,
      "step": 2434
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.06038821488618851,
      "learning_rate": 7.022696929238986e-05,
      "loss": 0.3919,
      "step": 2435
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.054075732827186584,
      "learning_rate": 7.017356475300401e-05,
      "loss": 0.3423,
      "step": 2436
    },
    {
      "epoch": 0.6498666666666667,
      "grad_norm": 0.05194190517067909,
      "learning_rate": 7.012016021361816e-05,
      "loss": 0.3176,
      "step": 2437
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.0583948977291584,
      "learning_rate": 7.006675567423231e-05,
      "loss": 0.3367,
      "step": 2438
    },
    {
      "epoch": 0.6504,
      "grad_norm": 0.04732896760106087,
      "learning_rate": 7.001335113484645e-05,
      "loss": 0.2999,
      "step": 2439
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.06389882415533066,
      "learning_rate": 6.995994659546061e-05,
      "loss": 0.3299,
      "step": 2440
    },
    {
      "epoch": 0.6509333333333334,
      "grad_norm": 0.049381695687770844,
      "learning_rate": 6.990654205607478e-05,
      "loss": 0.3164,
      "step": 2441
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.04942577704787254,
      "learning_rate": 6.985313751668892e-05,
      "loss": 0.2845,
      "step": 2442
    },
    {
      "epoch": 0.6514666666666666,
      "grad_norm": 0.047866880893707275,
      "learning_rate": 6.979973297730308e-05,
      "loss": 0.276,
      "step": 2443
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.05978565290570259,
      "learning_rate": 6.974632843791724e-05,
      "loss": 0.3144,
      "step": 2444
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.06283634901046753,
      "learning_rate": 6.969292389853138e-05,
      "loss": 0.287,
      "step": 2445
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.05618331953883171,
      "learning_rate": 6.963951935914554e-05,
      "loss": 0.344,
      "step": 2446
    },
    {
      "epoch": 0.6525333333333333,
      "grad_norm": 0.04859462380409241,
      "learning_rate": 6.958611481975968e-05,
      "loss": 0.312,
      "step": 2447
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.043898485600948334,
      "learning_rate": 6.953271028037383e-05,
      "loss": 0.3051,
      "step": 2448
    },
    {
      "epoch": 0.6530666666666667,
      "grad_norm": 0.04897279292345047,
      "learning_rate": 6.947930574098799e-05,
      "loss": 0.3428,
      "step": 2449
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.05079194903373718,
      "learning_rate": 6.942590120160213e-05,
      "loss": 0.2464,
      "step": 2450
    },
    {
      "epoch": 0.6536,
      "grad_norm": 0.044930264353752136,
      "learning_rate": 6.937249666221629e-05,
      "loss": 0.2884,
      "step": 2451
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.059731390327215195,
      "learning_rate": 6.931909212283045e-05,
      "loss": 0.3333,
      "step": 2452
    },
    {
      "epoch": 0.6541333333333333,
      "grad_norm": 0.05126374587416649,
      "learning_rate": 6.926568758344459e-05,
      "loss": 0.3277,
      "step": 2453
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.07490295171737671,
      "learning_rate": 6.921228304405875e-05,
      "loss": 0.3987,
      "step": 2454
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 0.05318479612469673,
      "learning_rate": 6.91588785046729e-05,
      "loss": 0.2859,
      "step": 2455
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.05405789613723755,
      "learning_rate": 6.910547396528704e-05,
      "loss": 0.3203,
      "step": 2456
    },
    {
      "epoch": 0.6552,
      "grad_norm": 0.056784119457006454,
      "learning_rate": 6.90520694259012e-05,
      "loss": 0.3226,
      "step": 2457
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.05329490453004837,
      "learning_rate": 6.899866488651536e-05,
      "loss": 0.2882,
      "step": 2458
    },
    {
      "epoch": 0.6557333333333333,
      "grad_norm": 0.05966225266456604,
      "learning_rate": 6.894526034712951e-05,
      "loss": 0.2845,
      "step": 2459
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.06720095872879028,
      "learning_rate": 6.889185580774367e-05,
      "loss": 0.3813,
      "step": 2460
    },
    {
      "epoch": 0.6562666666666667,
      "grad_norm": 0.051936570554971695,
      "learning_rate": 6.883845126835781e-05,
      "loss": 0.3222,
      "step": 2461
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.05915965884923935,
      "learning_rate": 6.878504672897197e-05,
      "loss": 0.3741,
      "step": 2462
    },
    {
      "epoch": 0.6568,
      "grad_norm": 0.05094707012176514,
      "learning_rate": 6.873164218958613e-05,
      "loss": 0.2998,
      "step": 2463
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.05035902187228203,
      "learning_rate": 6.867823765020027e-05,
      "loss": 0.2915,
      "step": 2464
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.040650490671396255,
      "learning_rate": 6.862483311081442e-05,
      "loss": 0.2435,
      "step": 2465
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.051694098860025406,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.2626,
      "step": 2466
    },
    {
      "epoch": 0.6578666666666667,
      "grad_norm": 0.05686885491013527,
      "learning_rate": 6.851802403204272e-05,
      "loss": 0.2976,
      "step": 2467
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.0692630261182785,
      "learning_rate": 6.846461949265688e-05,
      "loss": 0.3474,
      "step": 2468
    },
    {
      "epoch": 0.6584,
      "grad_norm": 0.05200010538101196,
      "learning_rate": 6.841121495327104e-05,
      "loss": 0.322,
      "step": 2469
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.045926667749881744,
      "learning_rate": 6.835781041388518e-05,
      "loss": 0.3173,
      "step": 2470
    },
    {
      "epoch": 0.6589333333333334,
      "grad_norm": 0.07089710235595703,
      "learning_rate": 6.830440587449934e-05,
      "loss": 0.3362,
      "step": 2471
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.05013534054160118,
      "learning_rate": 6.825100133511348e-05,
      "loss": 0.2904,
      "step": 2472
    },
    {
      "epoch": 0.6594666666666666,
      "grad_norm": 0.054637838155031204,
      "learning_rate": 6.819759679572763e-05,
      "loss": 0.3382,
      "step": 2473
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.054047685116529465,
      "learning_rate": 6.814419225634179e-05,
      "loss": 0.3425,
      "step": 2474
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.05772693082690239,
      "learning_rate": 6.809078771695593e-05,
      "loss": 0.3031,
      "step": 2475
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.05756322667002678,
      "learning_rate": 6.80373831775701e-05,
      "loss": 0.3416,
      "step": 2476
    },
    {
      "epoch": 0.6605333333333333,
      "grad_norm": 0.05090245231986046,
      "learning_rate": 6.798397863818426e-05,
      "loss": 0.2592,
      "step": 2477
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.05155250430107117,
      "learning_rate": 6.79305740987984e-05,
      "loss": 0.3235,
      "step": 2478
    },
    {
      "epoch": 0.6610666666666667,
      "grad_norm": 0.05499284714460373,
      "learning_rate": 6.787716955941256e-05,
      "loss": 0.3461,
      "step": 2479
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.05564381927251816,
      "learning_rate": 6.78237650200267e-05,
      "loss": 0.3271,
      "step": 2480
    },
    {
      "epoch": 0.6616,
      "grad_norm": 0.05977500602602959,
      "learning_rate": 6.777036048064086e-05,
      "loss": 0.3193,
      "step": 2481
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.06175116077065468,
      "learning_rate": 6.771695594125501e-05,
      "loss": 0.32,
      "step": 2482
    },
    {
      "epoch": 0.6621333333333334,
      "grad_norm": 0.05197984725236893,
      "learning_rate": 6.766355140186916e-05,
      "loss": 0.3156,
      "step": 2483
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.046008527278900146,
      "learning_rate": 6.761014686248331e-05,
      "loss": 0.3019,
      "step": 2484
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.058097824454307556,
      "learning_rate": 6.755674232309747e-05,
      "loss": 0.3753,
      "step": 2485
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.05635860189795494,
      "learning_rate": 6.750333778371161e-05,
      "loss": 0.3234,
      "step": 2486
    },
    {
      "epoch": 0.6632,
      "grad_norm": 0.05869145318865776,
      "learning_rate": 6.744993324432577e-05,
      "loss": 0.3291,
      "step": 2487
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.05040318891406059,
      "learning_rate": 6.739652870493993e-05,
      "loss": 0.3205,
      "step": 2488
    },
    {
      "epoch": 0.6637333333333333,
      "grad_norm": 0.0619247704744339,
      "learning_rate": 6.734312416555407e-05,
      "loss": 0.3275,
      "step": 2489
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.06483152508735657,
      "learning_rate": 6.728971962616822e-05,
      "loss": 0.2898,
      "step": 2490
    },
    {
      "epoch": 0.6642666666666667,
      "grad_norm": 0.06038782373070717,
      "learning_rate": 6.723631508678238e-05,
      "loss": 0.3896,
      "step": 2491
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.04277924448251724,
      "learning_rate": 6.718291054739652e-05,
      "loss": 0.289,
      "step": 2492
    },
    {
      "epoch": 0.6648,
      "grad_norm": 0.05580785125494003,
      "learning_rate": 6.71295060080107e-05,
      "loss": 0.2926,
      "step": 2493
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.07575579732656479,
      "learning_rate": 6.707610146862484e-05,
      "loss": 0.3177,
      "step": 2494
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.04104727879166603,
      "learning_rate": 6.702269692923899e-05,
      "loss": 0.257,
      "step": 2495
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.056409455835819244,
      "learning_rate": 6.696929238985315e-05,
      "loss": 0.3288,
      "step": 2496
    },
    {
      "epoch": 0.6658666666666667,
      "grad_norm": 0.06841076910495758,
      "learning_rate": 6.691588785046729e-05,
      "loss": 0.3472,
      "step": 2497
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.05841659754514694,
      "learning_rate": 6.686248331108145e-05,
      "loss": 0.3511,
      "step": 2498
    },
    {
      "epoch": 0.6664,
      "grad_norm": 0.05533731356263161,
      "learning_rate": 6.68090787716956e-05,
      "loss": 0.2995,
      "step": 2499
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.06587733328342438,
      "learning_rate": 6.675567423230975e-05,
      "loss": 0.3418,
      "step": 2500
    },
    {
      "epoch": 0.6669333333333334,
      "grad_norm": 0.05436881631612778,
      "learning_rate": 6.67022696929239e-05,
      "loss": 0.2788,
      "step": 2501
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.06786821782588959,
      "learning_rate": 6.664886515353806e-05,
      "loss": 0.3876,
      "step": 2502
    },
    {
      "epoch": 0.6674666666666667,
      "grad_norm": 0.058093782514333725,
      "learning_rate": 6.65954606141522e-05,
      "loss": 0.3874,
      "step": 2503
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.05336947366595268,
      "learning_rate": 6.654205607476636e-05,
      "loss": 0.2886,
      "step": 2504
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.05684288218617439,
      "learning_rate": 6.64886515353805e-05,
      "loss": 0.3409,
      "step": 2505
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.04257061704993248,
      "learning_rate": 6.643524699599466e-05,
      "loss": 0.265,
      "step": 2506
    },
    {
      "epoch": 0.6685333333333333,
      "grad_norm": 0.055923085659742355,
      "learning_rate": 6.638184245660881e-05,
      "loss": 0.3676,
      "step": 2507
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.05003206804394722,
      "learning_rate": 6.632843791722296e-05,
      "loss": 0.299,
      "step": 2508
    },
    {
      "epoch": 0.6690666666666667,
      "grad_norm": 0.06619299203157425,
      "learning_rate": 6.627503337783711e-05,
      "loss": 0.3196,
      "step": 2509
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.06042228639125824,
      "learning_rate": 6.622162883845127e-05,
      "loss": 0.3472,
      "step": 2510
    },
    {
      "epoch": 0.6696,
      "grad_norm": 0.05621985346078873,
      "learning_rate": 6.616822429906543e-05,
      "loss": 0.3469,
      "step": 2511
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.04842915013432503,
      "learning_rate": 6.611481975967958e-05,
      "loss": 0.2752,
      "step": 2512
    },
    {
      "epoch": 0.6701333333333334,
      "grad_norm": 0.03869454190135002,
      "learning_rate": 6.606141522029373e-05,
      "loss": 0.2459,
      "step": 2513
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.057870104908943176,
      "learning_rate": 6.600801068090788e-05,
      "loss": 0.3201,
      "step": 2514
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 0.043495483696460724,
      "learning_rate": 6.595460614152204e-05,
      "loss": 0.2889,
      "step": 2515
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.05924033373594284,
      "learning_rate": 6.590120160213618e-05,
      "loss": 0.3386,
      "step": 2516
    },
    {
      "epoch": 0.6712,
      "grad_norm": 0.046570613980293274,
      "learning_rate": 6.584779706275034e-05,
      "loss": 0.2577,
      "step": 2517
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.051061272621154785,
      "learning_rate": 6.57943925233645e-05,
      "loss": 0.3143,
      "step": 2518
    },
    {
      "epoch": 0.6717333333333333,
      "grad_norm": 0.05109141767024994,
      "learning_rate": 6.574098798397864e-05,
      "loss": 0.2742,
      "step": 2519
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.05309278890490532,
      "learning_rate": 6.568758344459279e-05,
      "loss": 0.3171,
      "step": 2520
    },
    {
      "epoch": 0.6722666666666667,
      "grad_norm": 0.0750003531575203,
      "learning_rate": 6.563417890520695e-05,
      "loss": 0.3707,
      "step": 2521
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.05613533779978752,
      "learning_rate": 6.558077436582109e-05,
      "loss": 0.3577,
      "step": 2522
    },
    {
      "epoch": 0.6728,
      "grad_norm": 0.0558476522564888,
      "learning_rate": 6.552736982643525e-05,
      "loss": 0.2899,
      "step": 2523
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.06378046423196793,
      "learning_rate": 6.54739652870494e-05,
      "loss": 0.3826,
      "step": 2524
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.0522749088704586,
      "learning_rate": 6.542056074766355e-05,
      "loss": 0.3176,
      "step": 2525
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.0386606901884079,
      "learning_rate": 6.53671562082777e-05,
      "loss": 0.2361,
      "step": 2526
    },
    {
      "epoch": 0.6738666666666666,
      "grad_norm": 0.046241845935583115,
      "learning_rate": 6.531375166889186e-05,
      "loss": 0.2663,
      "step": 2527
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.04879136011004448,
      "learning_rate": 6.526034712950602e-05,
      "loss": 0.2825,
      "step": 2528
    },
    {
      "epoch": 0.6744,
      "grad_norm": 0.05566388741135597,
      "learning_rate": 6.520694259012017e-05,
      "loss": 0.3186,
      "step": 2529
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.05771813541650772,
      "learning_rate": 6.515353805073432e-05,
      "loss": 0.3534,
      "step": 2530
    },
    {
      "epoch": 0.6749333333333334,
      "grad_norm": 0.045407138764858246,
      "learning_rate": 6.510013351134847e-05,
      "loss": 0.3024,
      "step": 2531
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.060162484645843506,
      "learning_rate": 6.504672897196263e-05,
      "loss": 0.3139,
      "step": 2532
    },
    {
      "epoch": 0.6754666666666667,
      "grad_norm": 0.05750347673892975,
      "learning_rate": 6.499332443257677e-05,
      "loss": 0.2839,
      "step": 2533
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.04669323191046715,
      "learning_rate": 6.493991989319093e-05,
      "loss": 0.3044,
      "step": 2534
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.05692455172538757,
      "learning_rate": 6.488651535380508e-05,
      "loss": 0.3707,
      "step": 2535
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.08869355171918869,
      "learning_rate": 6.483311081441923e-05,
      "loss": 0.3637,
      "step": 2536
    },
    {
      "epoch": 0.6765333333333333,
      "grad_norm": 0.0603531114757061,
      "learning_rate": 6.477970627503338e-05,
      "loss": 0.3487,
      "step": 2537
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.05565760284662247,
      "learning_rate": 6.472630173564754e-05,
      "loss": 0.2948,
      "step": 2538
    },
    {
      "epoch": 0.6770666666666667,
      "grad_norm": 0.05826367810368538,
      "learning_rate": 6.467289719626168e-05,
      "loss": 0.3651,
      "step": 2539
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.051817625761032104,
      "learning_rate": 6.461949265687584e-05,
      "loss": 0.3087,
      "step": 2540
    },
    {
      "epoch": 0.6776,
      "grad_norm": 0.0450906939804554,
      "learning_rate": 6.456608811748998e-05,
      "loss": 0.3228,
      "step": 2541
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.053459033370018005,
      "learning_rate": 6.451268357810414e-05,
      "loss": 0.2839,
      "step": 2542
    },
    {
      "epoch": 0.6781333333333334,
      "grad_norm": 0.04911709949374199,
      "learning_rate": 6.44592790387183e-05,
      "loss": 0.2755,
      "step": 2543
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.06305734813213348,
      "learning_rate": 6.440587449933244e-05,
      "loss": 0.3616,
      "step": 2544
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 0.04703293368220329,
      "learning_rate": 6.435246995994659e-05,
      "loss": 0.3032,
      "step": 2545
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.052166420966386795,
      "learning_rate": 6.429906542056076e-05,
      "loss": 0.3276,
      "step": 2546
    },
    {
      "epoch": 0.6792,
      "grad_norm": 0.06741601973772049,
      "learning_rate": 6.42456608811749e-05,
      "loss": 0.412,
      "step": 2547
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.05560075491666794,
      "learning_rate": 6.419225634178906e-05,
      "loss": 0.3524,
      "step": 2548
    },
    {
      "epoch": 0.6797333333333333,
      "grad_norm": 0.048649851232767105,
      "learning_rate": 6.41388518024032e-05,
      "loss": 0.2972,
      "step": 2549
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.056617289781570435,
      "learning_rate": 6.408544726301736e-05,
      "loss": 0.3283,
      "step": 2550
    },
    {
      "epoch": 0.6802666666666667,
      "grad_norm": 0.046211063861846924,
      "learning_rate": 6.403204272363152e-05,
      "loss": 0.2939,
      "step": 2551
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.060744509100914,
      "learning_rate": 6.397863818424566e-05,
      "loss": 0.2614,
      "step": 2552
    },
    {
      "epoch": 0.6808,
      "grad_norm": 0.0762980654835701,
      "learning_rate": 6.392523364485982e-05,
      "loss": 0.3315,
      "step": 2553
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.054547786712646484,
      "learning_rate": 6.387182910547397e-05,
      "loss": 0.2575,
      "step": 2554
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 0.07933137565851212,
      "learning_rate": 6.381842456608812e-05,
      "loss": 0.3772,
      "step": 2555
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.0807061716914177,
      "learning_rate": 6.376502002670227e-05,
      "loss": 0.3331,
      "step": 2556
    },
    {
      "epoch": 0.6818666666666666,
      "grad_norm": 0.07030322402715683,
      "learning_rate": 6.371161548731643e-05,
      "loss": 0.3508,
      "step": 2557
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.056711144745349884,
      "learning_rate": 6.365821094793057e-05,
      "loss": 0.3396,
      "step": 2558
    },
    {
      "epoch": 0.6824,
      "grad_norm": 0.05972766876220703,
      "learning_rate": 6.360480640854473e-05,
      "loss": 0.3596,
      "step": 2559
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.07842697203159332,
      "learning_rate": 6.355140186915888e-05,
      "loss": 0.3125,
      "step": 2560
    },
    {
      "epoch": 0.6829333333333333,
      "grad_norm": 0.05819174647331238,
      "learning_rate": 6.349799732977303e-05,
      "loss": 0.2732,
      "step": 2561
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.05432337895035744,
      "learning_rate": 6.344459279038718e-05,
      "loss": 0.2911,
      "step": 2562
    },
    {
      "epoch": 0.6834666666666667,
      "grad_norm": 0.10035808384418488,
      "learning_rate": 6.339118825100134e-05,
      "loss": 0.4027,
      "step": 2563
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.06041249260306358,
      "learning_rate": 6.33377837116155e-05,
      "loss": 0.3169,
      "step": 2564
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.08918003737926483,
      "learning_rate": 6.328437917222965e-05,
      "loss": 0.3863,
      "step": 2565
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.0717552974820137,
      "learning_rate": 6.32309746328438e-05,
      "loss": 0.3656,
      "step": 2566
    },
    {
      "epoch": 0.6845333333333333,
      "grad_norm": 0.05509797856211662,
      "learning_rate": 6.317757009345795e-05,
      "loss": 0.3092,
      "step": 2567
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.05930035561323166,
      "learning_rate": 6.31241655540721e-05,
      "loss": 0.3304,
      "step": 2568
    },
    {
      "epoch": 0.6850666666666667,
      "grad_norm": 0.0881553441286087,
      "learning_rate": 6.307076101468625e-05,
      "loss": 0.3762,
      "step": 2569
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.05105570703744888,
      "learning_rate": 6.30173564753004e-05,
      "loss": 0.3029,
      "step": 2570
    },
    {
      "epoch": 0.6856,
      "grad_norm": 0.06423594057559967,
      "learning_rate": 6.296395193591456e-05,
      "loss": 0.2844,
      "step": 2571
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.06806401908397675,
      "learning_rate": 6.29105473965287e-05,
      "loss": 0.333,
      "step": 2572
    },
    {
      "epoch": 0.6861333333333334,
      "grad_norm": 0.06407200545072556,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.3301,
      "step": 2573
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.04822886735200882,
      "learning_rate": 6.2803738317757e-05,
      "loss": 0.2967,
      "step": 2574
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.052670445293188095,
      "learning_rate": 6.275033377837116e-05,
      "loss": 0.2635,
      "step": 2575
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.08711521327495575,
      "learning_rate": 6.269692923898532e-05,
      "loss": 0.4094,
      "step": 2576
    },
    {
      "epoch": 0.6872,
      "grad_norm": 0.04830809310078621,
      "learning_rate": 6.264352469959946e-05,
      "loss": 0.2776,
      "step": 2577
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.046023473143577576,
      "learning_rate": 6.259012016021362e-05,
      "loss": 0.2591,
      "step": 2578
    },
    {
      "epoch": 0.6877333333333333,
      "grad_norm": 0.058812353760004044,
      "learning_rate": 6.253671562082777e-05,
      "loss": 0.345,
      "step": 2579
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.04898199066519737,
      "learning_rate": 6.248331108144193e-05,
      "loss": 0.323,
      "step": 2580
    },
    {
      "epoch": 0.6882666666666667,
      "grad_norm": 0.05327333137392998,
      "learning_rate": 6.242990654205608e-05,
      "loss": 0.3497,
      "step": 2581
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.0701669454574585,
      "learning_rate": 6.237650200267023e-05,
      "loss": 0.3279,
      "step": 2582
    },
    {
      "epoch": 0.6888,
      "grad_norm": 0.05903010442852974,
      "learning_rate": 6.232309746328438e-05,
      "loss": 0.2956,
      "step": 2583
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.05082646384835243,
      "learning_rate": 6.226969292389854e-05,
      "loss": 0.3037,
      "step": 2584
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 0.048864077776670456,
      "learning_rate": 6.221628838451268e-05,
      "loss": 0.3079,
      "step": 2585
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.06502058357000351,
      "learning_rate": 6.216288384512684e-05,
      "loss": 0.3569,
      "step": 2586
    },
    {
      "epoch": 0.6898666666666666,
      "grad_norm": 0.052306611090898514,
      "learning_rate": 6.2109479305741e-05,
      "loss": 0.2897,
      "step": 2587
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.06399673223495483,
      "learning_rate": 6.205607476635514e-05,
      "loss": 0.3295,
      "step": 2588
    },
    {
      "epoch": 0.6904,
      "grad_norm": 0.05250643193721771,
      "learning_rate": 6.20026702269693e-05,
      "loss": 0.2974,
      "step": 2589
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.05048593878746033,
      "learning_rate": 6.194926568758345e-05,
      "loss": 0.3255,
      "step": 2590
    },
    {
      "epoch": 0.6909333333333333,
      "grad_norm": 0.05042153596878052,
      "learning_rate": 6.18958611481976e-05,
      "loss": 0.3109,
      "step": 2591
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.04974929243326187,
      "learning_rate": 6.184245660881175e-05,
      "loss": 0.2851,
      "step": 2592
    },
    {
      "epoch": 0.6914666666666667,
      "grad_norm": 0.060782451182603836,
      "learning_rate": 6.17890520694259e-05,
      "loss": 0.4021,
      "step": 2593
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.04257839918136597,
      "learning_rate": 6.173564753004005e-05,
      "loss": 0.2768,
      "step": 2594
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.06384297460317612,
      "learning_rate": 6.16822429906542e-05,
      "loss": 0.3916,
      "step": 2595
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.044601116329431534,
      "learning_rate": 6.162883845126836e-05,
      "loss": 0.2694,
      "step": 2596
    },
    {
      "epoch": 0.6925333333333333,
      "grad_norm": 0.053236763924360275,
      "learning_rate": 6.15754339118825e-05,
      "loss": 0.3207,
      "step": 2597
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.05107392743229866,
      "learning_rate": 6.152202937249667e-05,
      "loss": 0.3348,
      "step": 2598
    },
    {
      "epoch": 0.6930666666666667,
      "grad_norm": 0.04900287836790085,
      "learning_rate": 6.146862483311082e-05,
      "loss": 0.2692,
      "step": 2599
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.060009490698575974,
      "learning_rate": 6.141522029372497e-05,
      "loss": 0.3357,
      "step": 2600
    },
    {
      "epoch": 0.6936,
      "grad_norm": 0.05334503948688507,
      "learning_rate": 6.136181575433913e-05,
      "loss": 0.2865,
      "step": 2601
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.06859235465526581,
      "learning_rate": 6.130841121495327e-05,
      "loss": 0.3534,
      "step": 2602
    },
    {
      "epoch": 0.6941333333333334,
      "grad_norm": 0.05000920593738556,
      "learning_rate": 6.125500667556743e-05,
      "loss": 0.2801,
      "step": 2603
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.046354200690984726,
      "learning_rate": 6.120160213618159e-05,
      "loss": 0.2661,
      "step": 2604
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 0.06402645260095596,
      "learning_rate": 6.114819759679573e-05,
      "loss": 0.3727,
      "step": 2605
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.05052579566836357,
      "learning_rate": 6.109479305740988e-05,
      "loss": 0.3453,
      "step": 2606
    },
    {
      "epoch": 0.6952,
      "grad_norm": 0.0566665343940258,
      "learning_rate": 6.104138851802403e-05,
      "loss": 0.3157,
      "step": 2607
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.048566777259111404,
      "learning_rate": 6.0987983978638184e-05,
      "loss": 0.2904,
      "step": 2608
    },
    {
      "epoch": 0.6957333333333333,
      "grad_norm": 0.0508221834897995,
      "learning_rate": 6.093457943925234e-05,
      "loss": 0.2759,
      "step": 2609
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.04991442710161209,
      "learning_rate": 6.088117489986649e-05,
      "loss": 0.2667,
      "step": 2610
    },
    {
      "epoch": 0.6962666666666667,
      "grad_norm": 0.051613710820674896,
      "learning_rate": 6.082777036048064e-05,
      "loss": 0.3202,
      "step": 2611
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.04954196885228157,
      "learning_rate": 6.077436582109479e-05,
      "loss": 0.3121,
      "step": 2612
    },
    {
      "epoch": 0.6968,
      "grad_norm": 0.048546645790338516,
      "learning_rate": 6.0720961281708945e-05,
      "loss": 0.2969,
      "step": 2613
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.04660342261195183,
      "learning_rate": 6.0667556742323095e-05,
      "loss": 0.3204,
      "step": 2614
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 0.06716721504926682,
      "learning_rate": 6.061415220293726e-05,
      "loss": 0.3778,
      "step": 2615
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.050270456820726395,
      "learning_rate": 6.056074766355141e-05,
      "loss": 0.3213,
      "step": 2616
    },
    {
      "epoch": 0.6978666666666666,
      "grad_norm": 0.045320067554712296,
      "learning_rate": 6.0507343124165564e-05,
      "loss": 0.2991,
      "step": 2617
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.0688372403383255,
      "learning_rate": 6.045393858477971e-05,
      "loss": 0.3591,
      "step": 2618
    },
    {
      "epoch": 0.6984,
      "grad_norm": 0.05468448996543884,
      "learning_rate": 6.040053404539386e-05,
      "loss": 0.3125,
      "step": 2619
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.056289978325366974,
      "learning_rate": 6.034712950600801e-05,
      "loss": 0.2804,
      "step": 2620
    },
    {
      "epoch": 0.6989333333333333,
      "grad_norm": 0.06673374027013779,
      "learning_rate": 6.029372496662217e-05,
      "loss": 0.3263,
      "step": 2621
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.04738839715719223,
      "learning_rate": 6.024032042723632e-05,
      "loss": 0.3144,
      "step": 2622
    },
    {
      "epoch": 0.6994666666666667,
      "grad_norm": 0.05800233781337738,
      "learning_rate": 6.018691588785047e-05,
      "loss": 0.3724,
      "step": 2623
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.050309907644987106,
      "learning_rate": 6.0133511348464624e-05,
      "loss": 0.2772,
      "step": 2624
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04574282094836235,
      "learning_rate": 6.0080106809078774e-05,
      "loss": 0.261,
      "step": 2625
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.07682577520608902,
      "learning_rate": 6.002670226969292e-05,
      "loss": 0.3372,
      "step": 2626
    },
    {
      "epoch": 0.7005333333333333,
      "grad_norm": 0.06364359706640244,
      "learning_rate": 5.997329773030708e-05,
      "loss": 0.3311,
      "step": 2627
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.052804846316576004,
      "learning_rate": 5.991989319092123e-05,
      "loss": 0.2806,
      "step": 2628
    },
    {
      "epoch": 0.7010666666666666,
      "grad_norm": 0.06573684513568878,
      "learning_rate": 5.986648865153538e-05,
      "loss": 0.3465,
      "step": 2629
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.05284901335835457,
      "learning_rate": 5.981308411214953e-05,
      "loss": 0.2811,
      "step": 2630
    },
    {
      "epoch": 0.7016,
      "grad_norm": 0.04809744283556938,
      "learning_rate": 5.9759679572763685e-05,
      "loss": 0.2889,
      "step": 2631
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.051649417728185654,
      "learning_rate": 5.970627503337785e-05,
      "loss": 0.3266,
      "step": 2632
    },
    {
      "epoch": 0.7021333333333334,
      "grad_norm": 0.043196901679039,
      "learning_rate": 5.9652870493992e-05,
      "loss": 0.2894,
      "step": 2633
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.05018940567970276,
      "learning_rate": 5.959946595460615e-05,
      "loss": 0.372,
      "step": 2634
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 0.05272448807954788,
      "learning_rate": 5.95460614152203e-05,
      "loss": 0.3522,
      "step": 2635
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.051105793565511703,
      "learning_rate": 5.949265687583445e-05,
      "loss": 0.325,
      "step": 2636
    },
    {
      "epoch": 0.7032,
      "grad_norm": 0.04918399453163147,
      "learning_rate": 5.94392523364486e-05,
      "loss": 0.2856,
      "step": 2637
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.054267767816782,
      "learning_rate": 5.938584779706275e-05,
      "loss": 0.3379,
      "step": 2638
    },
    {
      "epoch": 0.7037333333333333,
      "grad_norm": 0.052005209028720856,
      "learning_rate": 5.933244325767691e-05,
      "loss": 0.3423,
      "step": 2639
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.05131370201706886,
      "learning_rate": 5.927903871829106e-05,
      "loss": 0.3407,
      "step": 2640
    },
    {
      "epoch": 0.7042666666666667,
      "grad_norm": 0.06655870378017426,
      "learning_rate": 5.922563417890521e-05,
      "loss": 0.3993,
      "step": 2641
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.04579082131385803,
      "learning_rate": 5.9172229639519364e-05,
      "loss": 0.249,
      "step": 2642
    },
    {
      "epoch": 0.7048,
      "grad_norm": 0.04263895004987717,
      "learning_rate": 5.911882510013351e-05,
      "loss": 0.2738,
      "step": 2643
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.04372845217585564,
      "learning_rate": 5.906542056074766e-05,
      "loss": 0.2601,
      "step": 2644
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 0.05131801217794418,
      "learning_rate": 5.901201602136181e-05,
      "loss": 0.2953,
      "step": 2645
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.05544527247548103,
      "learning_rate": 5.895861148197597e-05,
      "loss": 0.2853,
      "step": 2646
    },
    {
      "epoch": 0.7058666666666666,
      "grad_norm": 0.045260973274707794,
      "learning_rate": 5.890520694259012e-05,
      "loss": 0.3173,
      "step": 2647
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.04515370726585388,
      "learning_rate": 5.885180240320427e-05,
      "loss": 0.2718,
      "step": 2648
    },
    {
      "epoch": 0.7064,
      "grad_norm": 0.04448939114809036,
      "learning_rate": 5.8798397863818424e-05,
      "loss": 0.2694,
      "step": 2649
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.05941714346408844,
      "learning_rate": 5.874499332443259e-05,
      "loss": 0.3826,
      "step": 2650
    },
    {
      "epoch": 0.7069333333333333,
      "grad_norm": 0.05183626338839531,
      "learning_rate": 5.869158878504674e-05,
      "loss": 0.3476,
      "step": 2651
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.062365759164094925,
      "learning_rate": 5.8638184245660886e-05,
      "loss": 0.3575,
      "step": 2652
    },
    {
      "epoch": 0.7074666666666667,
      "grad_norm": 0.0425330325961113,
      "learning_rate": 5.8584779706275036e-05,
      "loss": 0.2336,
      "step": 2653
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.04956573247909546,
      "learning_rate": 5.853137516688919e-05,
      "loss": 0.2906,
      "step": 2654
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.05657707527279854,
      "learning_rate": 5.847797062750334e-05,
      "loss": 0.3234,
      "step": 2655
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.046429622918367386,
      "learning_rate": 5.842456608811749e-05,
      "loss": 0.2921,
      "step": 2656
    },
    {
      "epoch": 0.7085333333333333,
      "grad_norm": 0.051922403275966644,
      "learning_rate": 5.837116154873165e-05,
      "loss": 0.3184,
      "step": 2657
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.055277105420827866,
      "learning_rate": 5.83177570093458e-05,
      "loss": 0.3615,
      "step": 2658
    },
    {
      "epoch": 0.7090666666666666,
      "grad_norm": 0.054879266768693924,
      "learning_rate": 5.826435246995995e-05,
      "loss": 0.3526,
      "step": 2659
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.04709900915622711,
      "learning_rate": 5.82109479305741e-05,
      "loss": 0.2821,
      "step": 2660
    },
    {
      "epoch": 0.7096,
      "grad_norm": 0.04622010886669159,
      "learning_rate": 5.815754339118825e-05,
      "loss": 0.2788,
      "step": 2661
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.0554107204079628,
      "learning_rate": 5.81041388518024e-05,
      "loss": 0.302,
      "step": 2662
    },
    {
      "epoch": 0.7101333333333333,
      "grad_norm": 0.05525251105427742,
      "learning_rate": 5.805073431241655e-05,
      "loss": 0.3419,
      "step": 2663
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.07606817781925201,
      "learning_rate": 5.799732977303071e-05,
      "loss": 0.3209,
      "step": 2664
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 0.052596814930438995,
      "learning_rate": 5.794392523364486e-05,
      "loss": 0.2868,
      "step": 2665
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.04970468580722809,
      "learning_rate": 5.789052069425901e-05,
      "loss": 0.3232,
      "step": 2666
    },
    {
      "epoch": 0.7112,
      "grad_norm": 0.053235918283462524,
      "learning_rate": 5.783711615487317e-05,
      "loss": 0.2987,
      "step": 2667
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.06298412382602692,
      "learning_rate": 5.778371161548733e-05,
      "loss": 0.3632,
      "step": 2668
    },
    {
      "epoch": 0.7117333333333333,
      "grad_norm": 0.052995387464761734,
      "learning_rate": 5.7730307076101476e-05,
      "loss": 0.302,
      "step": 2669
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.05235585197806358,
      "learning_rate": 5.7676902536715626e-05,
      "loss": 0.3289,
      "step": 2670
    },
    {
      "epoch": 0.7122666666666667,
      "grad_norm": 0.04362255334854126,
      "learning_rate": 5.7623497997329775e-05,
      "loss": 0.2638,
      "step": 2671
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.11936294287443161,
      "learning_rate": 5.757009345794393e-05,
      "loss": 0.35,
      "step": 2672
    },
    {
      "epoch": 0.7128,
      "grad_norm": 0.06241285055875778,
      "learning_rate": 5.751668891855808e-05,
      "loss": 0.3427,
      "step": 2673
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.056798432022333145,
      "learning_rate": 5.746328437917223e-05,
      "loss": 0.3362,
      "step": 2674
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.03981203958392143,
      "learning_rate": 5.740987983978639e-05,
      "loss": 0.2582,
      "step": 2675
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.04566694796085358,
      "learning_rate": 5.735647530040054e-05,
      "loss": 0.2405,
      "step": 2676
    },
    {
      "epoch": 0.7138666666666666,
      "grad_norm": 0.07762476801872253,
      "learning_rate": 5.7303070761014686e-05,
      "loss": 0.4545,
      "step": 2677
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.05993777886033058,
      "learning_rate": 5.724966622162884e-05,
      "loss": 0.3305,
      "step": 2678
    },
    {
      "epoch": 0.7144,
      "grad_norm": 0.0592041052877903,
      "learning_rate": 5.719626168224299e-05,
      "loss": 0.3611,
      "step": 2679
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.07060132175683975,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.408,
      "step": 2680
    },
    {
      "epoch": 0.7149333333333333,
      "grad_norm": 0.050979044288396835,
      "learning_rate": 5.708945260347129e-05,
      "loss": 0.2802,
      "step": 2681
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.05016510561108589,
      "learning_rate": 5.703604806408545e-05,
      "loss": 0.3151,
      "step": 2682
    },
    {
      "epoch": 0.7154666666666667,
      "grad_norm": 0.04850418120622635,
      "learning_rate": 5.69826435246996e-05,
      "loss": 0.2465,
      "step": 2683
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.04656927287578583,
      "learning_rate": 5.692923898531376e-05,
      "loss": 0.2702,
      "step": 2684
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.052952542901039124,
      "learning_rate": 5.687583444592791e-05,
      "loss": 0.3657,
      "step": 2685
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.05480334162712097,
      "learning_rate": 5.6822429906542066e-05,
      "loss": 0.3251,
      "step": 2686
    },
    {
      "epoch": 0.7165333333333334,
      "grad_norm": 0.06154532730579376,
      "learning_rate": 5.6769025367156216e-05,
      "loss": 0.3238,
      "step": 2687
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.05256657302379608,
      "learning_rate": 5.6715620827770365e-05,
      "loss": 0.3374,
      "step": 2688
    },
    {
      "epoch": 0.7170666666666666,
      "grad_norm": 0.054094281047582626,
      "learning_rate": 5.6662216288384515e-05,
      "loss": 0.2707,
      "step": 2689
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.05137333273887634,
      "learning_rate": 5.660881174899867e-05,
      "loss": 0.2887,
      "step": 2690
    },
    {
      "epoch": 0.7176,
      "grad_norm": 0.06116362288594246,
      "learning_rate": 5.655540720961282e-05,
      "loss": 0.3346,
      "step": 2691
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.063546322286129,
      "learning_rate": 5.650200267022697e-05,
      "loss": 0.3622,
      "step": 2692
    },
    {
      "epoch": 0.7181333333333333,
      "grad_norm": 0.06410595029592514,
      "learning_rate": 5.644859813084113e-05,
      "loss": 0.3808,
      "step": 2693
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.04601920023560524,
      "learning_rate": 5.6395193591455276e-05,
      "loss": 0.3452,
      "step": 2694
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 0.04784644767642021,
      "learning_rate": 5.6341789052069426e-05,
      "loss": 0.2762,
      "step": 2695
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.061471451073884964,
      "learning_rate": 5.6288384512683575e-05,
      "loss": 0.3044,
      "step": 2696
    },
    {
      "epoch": 0.7192,
      "grad_norm": 0.053353630006313324,
      "learning_rate": 5.623497997329773e-05,
      "loss": 0.2999,
      "step": 2697
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.05761970579624176,
      "learning_rate": 5.618157543391188e-05,
      "loss": 0.3634,
      "step": 2698
    },
    {
      "epoch": 0.7197333333333333,
      "grad_norm": 0.045307643711566925,
      "learning_rate": 5.612817089452603e-05,
      "loss": 0.2584,
      "step": 2699
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.07844792306423187,
      "learning_rate": 5.607476635514019e-05,
      "loss": 0.4215,
      "step": 2700
    },
    {
      "epoch": 0.7202666666666667,
      "grad_norm": 0.06031152233481407,
      "learning_rate": 5.602136181575434e-05,
      "loss": 0.3569,
      "step": 2701
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.060411207377910614,
      "learning_rate": 5.59679572763685e-05,
      "loss": 0.3927,
      "step": 2702
    },
    {
      "epoch": 0.7208,
      "grad_norm": 0.06888188421726227,
      "learning_rate": 5.591455273698265e-05,
      "loss": 0.3649,
      "step": 2703
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.04091275855898857,
      "learning_rate": 5.58611481975968e-05,
      "loss": 0.2622,
      "step": 2704
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 0.0592728853225708,
      "learning_rate": 5.5807743658210955e-05,
      "loss": 0.3653,
      "step": 2705
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.053654689341783524,
      "learning_rate": 5.5754339118825105e-05,
      "loss": 0.3033,
      "step": 2706
    },
    {
      "epoch": 0.7218666666666667,
      "grad_norm": 0.05582986772060394,
      "learning_rate": 5.5700934579439254e-05,
      "loss": 0.2654,
      "step": 2707
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.05140594393014908,
      "learning_rate": 5.564753004005341e-05,
      "loss": 0.3335,
      "step": 2708
    },
    {
      "epoch": 0.7224,
      "grad_norm": 0.05271881818771362,
      "learning_rate": 5.559412550066756e-05,
      "loss": 0.3048,
      "step": 2709
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.05417559668421745,
      "learning_rate": 5.554072096128171e-05,
      "loss": 0.3203,
      "step": 2710
    },
    {
      "epoch": 0.7229333333333333,
      "grad_norm": 0.04630957171320915,
      "learning_rate": 5.5487316421895866e-05,
      "loss": 0.3253,
      "step": 2711
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.0474981814622879,
      "learning_rate": 5.5433911882510016e-05,
      "loss": 0.2944,
      "step": 2712
    },
    {
      "epoch": 0.7234666666666667,
      "grad_norm": 0.048356976360082626,
      "learning_rate": 5.5380507343124165e-05,
      "loss": 0.325,
      "step": 2713
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.052980855107307434,
      "learning_rate": 5.5327102803738315e-05,
      "loss": 0.3538,
      "step": 2714
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.05686424672603607,
      "learning_rate": 5.527369826435247e-05,
      "loss": 0.3619,
      "step": 2715
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.05122363194823265,
      "learning_rate": 5.522029372496662e-05,
      "loss": 0.3223,
      "step": 2716
    },
    {
      "epoch": 0.7245333333333334,
      "grad_norm": 0.06218938156962395,
      "learning_rate": 5.516688918558077e-05,
      "loss": 0.2832,
      "step": 2717
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.05122939869761467,
      "learning_rate": 5.5113484646194927e-05,
      "loss": 0.3473,
      "step": 2718
    },
    {
      "epoch": 0.7250666666666666,
      "grad_norm": 0.05558103322982788,
      "learning_rate": 5.506008010680909e-05,
      "loss": 0.2965,
      "step": 2719
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.05219664052128792,
      "learning_rate": 5.500667556742324e-05,
      "loss": 0.3333,
      "step": 2720
    },
    {
      "epoch": 0.7256,
      "grad_norm": 0.04947962239384651,
      "learning_rate": 5.495327102803739e-05,
      "loss": 0.3225,
      "step": 2721
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.0534052811563015,
      "learning_rate": 5.489986648865154e-05,
      "loss": 0.3494,
      "step": 2722
    },
    {
      "epoch": 0.7261333333333333,
      "grad_norm": 0.05071335658431053,
      "learning_rate": 5.4846461949265695e-05,
      "loss": 0.3221,
      "step": 2723
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.05677429586648941,
      "learning_rate": 5.4793057409879844e-05,
      "loss": 0.3612,
      "step": 2724
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.04789393022656441,
      "learning_rate": 5.4739652870493994e-05,
      "loss": 0.2858,
      "step": 2725
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.05379248037934303,
      "learning_rate": 5.468624833110815e-05,
      "loss": 0.3074,
      "step": 2726
    },
    {
      "epoch": 0.7272,
      "grad_norm": 0.05299894139170647,
      "learning_rate": 5.46328437917223e-05,
      "loss": 0.3226,
      "step": 2727
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.056853704154491425,
      "learning_rate": 5.457943925233645e-05,
      "loss": 0.3065,
      "step": 2728
    },
    {
      "epoch": 0.7277333333333333,
      "grad_norm": 0.05321761220693588,
      "learning_rate": 5.4526034712950606e-05,
      "loss": 0.2952,
      "step": 2729
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.048310790210962296,
      "learning_rate": 5.4472630173564755e-05,
      "loss": 0.2554,
      "step": 2730
    },
    {
      "epoch": 0.7282666666666666,
      "grad_norm": 0.05205312743782997,
      "learning_rate": 5.4419225634178905e-05,
      "loss": 0.2984,
      "step": 2731
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.047552287578582764,
      "learning_rate": 5.4365821094793054e-05,
      "loss": 0.2944,
      "step": 2732
    },
    {
      "epoch": 0.7288,
      "grad_norm": 0.045196518301963806,
      "learning_rate": 5.431241655540721e-05,
      "loss": 0.2883,
      "step": 2733
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.05127692222595215,
      "learning_rate": 5.425901201602136e-05,
      "loss": 0.3231,
      "step": 2734
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 0.047716978937387466,
      "learning_rate": 5.420560747663551e-05,
      "loss": 0.2921,
      "step": 2735
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.04940817505121231,
      "learning_rate": 5.415220293724967e-05,
      "loss": 0.2765,
      "step": 2736
    },
    {
      "epoch": 0.7298666666666667,
      "grad_norm": 0.06794828176498413,
      "learning_rate": 5.409879839786383e-05,
      "loss": 0.3164,
      "step": 2737
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.055456068366765976,
      "learning_rate": 5.404539385847798e-05,
      "loss": 0.3342,
      "step": 2738
    },
    {
      "epoch": 0.7304,
      "grad_norm": 0.057543303817510605,
      "learning_rate": 5.399198931909213e-05,
      "loss": 0.2841,
      "step": 2739
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.06350280344486237,
      "learning_rate": 5.393858477970628e-05,
      "loss": 0.3749,
      "step": 2740
    },
    {
      "epoch": 0.7309333333333333,
      "grad_norm": 0.06007319688796997,
      "learning_rate": 5.3885180240320434e-05,
      "loss": 0.313,
      "step": 2741
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.05327468365430832,
      "learning_rate": 5.3831775700934584e-05,
      "loss": 0.3204,
      "step": 2742
    },
    {
      "epoch": 0.7314666666666667,
      "grad_norm": 0.06378302723169327,
      "learning_rate": 5.377837116154873e-05,
      "loss": 0.3447,
      "step": 2743
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.04768187925219536,
      "learning_rate": 5.372496662216289e-05,
      "loss": 0.2701,
      "step": 2744
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.05939093604683876,
      "learning_rate": 5.367156208277704e-05,
      "loss": 0.3559,
      "step": 2745
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.04367827624082565,
      "learning_rate": 5.361815754339119e-05,
      "loss": 0.2702,
      "step": 2746
    },
    {
      "epoch": 0.7325333333333334,
      "grad_norm": 0.05601527541875839,
      "learning_rate": 5.356475300400534e-05,
      "loss": 0.3295,
      "step": 2747
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.04516719654202461,
      "learning_rate": 5.3511348464619495e-05,
      "loss": 0.2864,
      "step": 2748
    },
    {
      "epoch": 0.7330666666666666,
      "grad_norm": 0.053905170410871506,
      "learning_rate": 5.3457943925233644e-05,
      "loss": 0.2997,
      "step": 2749
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.06790586560964584,
      "learning_rate": 5.3404539385847794e-05,
      "loss": 0.3485,
      "step": 2750
    },
    {
      "epoch": 0.7336,
      "grad_norm": 0.052304208278656006,
      "learning_rate": 5.335113484646195e-05,
      "loss": 0.2662,
      "step": 2751
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.06383367627859116,
      "learning_rate": 5.32977303070761e-05,
      "loss": 0.3592,
      "step": 2752
    },
    {
      "epoch": 0.7341333333333333,
      "grad_norm": 0.04202956706285477,
      "learning_rate": 5.324432576769025e-05,
      "loss": 0.2839,
      "step": 2753
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.051756810396909714,
      "learning_rate": 5.319092122830441e-05,
      "loss": 0.3044,
      "step": 2754
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 0.05446208640933037,
      "learning_rate": 5.313751668891856e-05,
      "loss": 0.2981,
      "step": 2755
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.06498019397258759,
      "learning_rate": 5.308411214953272e-05,
      "loss": 0.3713,
      "step": 2756
    },
    {
      "epoch": 0.7352,
      "grad_norm": 0.04145423695445061,
      "learning_rate": 5.303070761014687e-05,
      "loss": 0.2482,
      "step": 2757
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.03959004953503609,
      "learning_rate": 5.297730307076102e-05,
      "loss": 0.2674,
      "step": 2758
    },
    {
      "epoch": 0.7357333333333334,
      "grad_norm": 0.053488053381443024,
      "learning_rate": 5.2923898531375174e-05,
      "loss": 0.3231,
      "step": 2759
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.053632281720638275,
      "learning_rate": 5.287049399198932e-05,
      "loss": 0.3435,
      "step": 2760
    },
    {
      "epoch": 0.7362666666666666,
      "grad_norm": 0.052895437926054,
      "learning_rate": 5.281708945260347e-05,
      "loss": 0.3712,
      "step": 2761
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.056513991206884384,
      "learning_rate": 5.276368491321763e-05,
      "loss": 0.2952,
      "step": 2762
    },
    {
      "epoch": 0.7368,
      "grad_norm": 0.059343356639146805,
      "learning_rate": 5.271028037383178e-05,
      "loss": 0.3805,
      "step": 2763
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.05523409694433212,
      "learning_rate": 5.265687583444593e-05,
      "loss": 0.3202,
      "step": 2764
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 0.053748514503240585,
      "learning_rate": 5.260347129506008e-05,
      "loss": 0.318,
      "step": 2765
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.05183622986078262,
      "learning_rate": 5.2550066755674234e-05,
      "loss": 0.3231,
      "step": 2766
    },
    {
      "epoch": 0.7378666666666667,
      "grad_norm": 0.0510900542140007,
      "learning_rate": 5.2496662216288384e-05,
      "loss": 0.3539,
      "step": 2767
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.06216525286436081,
      "learning_rate": 5.244325767690253e-05,
      "loss": 0.4041,
      "step": 2768
    },
    {
      "epoch": 0.7384,
      "grad_norm": 0.056861281394958496,
      "learning_rate": 5.238985313751669e-05,
      "loss": 0.3562,
      "step": 2769
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.045888375490903854,
      "learning_rate": 5.233644859813084e-05,
      "loss": 0.2814,
      "step": 2770
    },
    {
      "epoch": 0.7389333333333333,
      "grad_norm": 0.059442196041345596,
      "learning_rate": 5.2283044058745e-05,
      "loss": 0.3826,
      "step": 2771
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.042104560881853104,
      "learning_rate": 5.222963951935915e-05,
      "loss": 0.2776,
      "step": 2772
    },
    {
      "epoch": 0.7394666666666667,
      "grad_norm": 0.04905456304550171,
      "learning_rate": 5.21762349799733e-05,
      "loss": 0.3324,
      "step": 2773
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.05083806440234184,
      "learning_rate": 5.212283044058746e-05,
      "loss": 0.3038,
      "step": 2774
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.046506643295288086,
      "learning_rate": 5.206942590120161e-05,
      "loss": 0.2856,
      "step": 2775
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.046965304762125015,
      "learning_rate": 5.201602136181576e-05,
      "loss": 0.2956,
      "step": 2776
    },
    {
      "epoch": 0.7405333333333334,
      "grad_norm": 0.05179565027356148,
      "learning_rate": 5.196261682242991e-05,
      "loss": 0.3121,
      "step": 2777
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.05823642760515213,
      "learning_rate": 5.190921228304406e-05,
      "loss": 0.3584,
      "step": 2778
    },
    {
      "epoch": 0.7410666666666667,
      "grad_norm": 0.0589311309158802,
      "learning_rate": 5.185580774365821e-05,
      "loss": 0.3345,
      "step": 2779
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.0768708810210228,
      "learning_rate": 5.180240320427236e-05,
      "loss": 0.3906,
      "step": 2780
    },
    {
      "epoch": 0.7416,
      "grad_norm": 0.044610656797885895,
      "learning_rate": 5.174899866488652e-05,
      "loss": 0.2745,
      "step": 2781
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.07030993700027466,
      "learning_rate": 5.169559412550067e-05,
      "loss": 0.3509,
      "step": 2782
    },
    {
      "epoch": 0.7421333333333333,
      "grad_norm": 0.042077042162418365,
      "learning_rate": 5.164218958611482e-05,
      "loss": 0.2458,
      "step": 2783
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.05106040835380554,
      "learning_rate": 5.1588785046728973e-05,
      "loss": 0.2858,
      "step": 2784
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 0.048029039055109024,
      "learning_rate": 5.153538050734312e-05,
      "loss": 0.2824,
      "step": 2785
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.05313100665807724,
      "learning_rate": 5.148197596795727e-05,
      "loss": 0.2613,
      "step": 2786
    },
    {
      "epoch": 0.7432,
      "grad_norm": 0.05038712918758392,
      "learning_rate": 5.142857142857143e-05,
      "loss": 0.2951,
      "step": 2787
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.05819343402981758,
      "learning_rate": 5.1375166889185585e-05,
      "loss": 0.3672,
      "step": 2788
    },
    {
      "epoch": 0.7437333333333334,
      "grad_norm": 0.060723841190338135,
      "learning_rate": 5.132176234979974e-05,
      "loss": 0.359,
      "step": 2789
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.05019490048289299,
      "learning_rate": 5.126835781041389e-05,
      "loss": 0.2483,
      "step": 2790
    },
    {
      "epoch": 0.7442666666666666,
      "grad_norm": 0.051197268068790436,
      "learning_rate": 5.121495327102804e-05,
      "loss": 0.2891,
      "step": 2791
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.05171043425798416,
      "learning_rate": 5.11615487316422e-05,
      "loss": 0.3161,
      "step": 2792
    },
    {
      "epoch": 0.7448,
      "grad_norm": 0.05960867181420326,
      "learning_rate": 5.1108144192256347e-05,
      "loss": 0.3883,
      "step": 2793
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.06063712015748024,
      "learning_rate": 5.1054739652870496e-05,
      "loss": 0.354,
      "step": 2794
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 0.053568508476018906,
      "learning_rate": 5.100133511348465e-05,
      "loss": 0.3486,
      "step": 2795
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.042113855481147766,
      "learning_rate": 5.09479305740988e-05,
      "loss": 0.2315,
      "step": 2796
    },
    {
      "epoch": 0.7458666666666667,
      "grad_norm": 0.04888640716671944,
      "learning_rate": 5.089452603471295e-05,
      "loss": 0.287,
      "step": 2797
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.03724685311317444,
      "learning_rate": 5.08411214953271e-05,
      "loss": 0.282,
      "step": 2798
    },
    {
      "epoch": 0.7464,
      "grad_norm": 0.05427892878651619,
      "learning_rate": 5.078771695594126e-05,
      "loss": 0.2998,
      "step": 2799
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05921631678938866,
      "learning_rate": 5.073431241655541e-05,
      "loss": 0.357,
      "step": 2800
    },
    {
      "epoch": 0.7469333333333333,
      "grad_norm": 0.04849695414304733,
      "learning_rate": 5.0680907877169557e-05,
      "loss": 0.3082,
      "step": 2801
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.06391573697328568,
      "learning_rate": 5.062750333778371e-05,
      "loss": 0.3441,
      "step": 2802
    },
    {
      "epoch": 0.7474666666666666,
      "grad_norm": 0.05910094454884529,
      "learning_rate": 5.057409879839786e-05,
      "loss": 0.2672,
      "step": 2803
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.05268170312047005,
      "learning_rate": 5.052069425901201e-05,
      "loss": 0.3376,
      "step": 2804
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.06471113115549088,
      "learning_rate": 5.046728971962617e-05,
      "loss": 0.3541,
      "step": 2805
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.0521593876183033,
      "learning_rate": 5.0413885180240325e-05,
      "loss": 0.3027,
      "step": 2806
    },
    {
      "epoch": 0.7485333333333334,
      "grad_norm": 0.05097493156790733,
      "learning_rate": 5.036048064085448e-05,
      "loss": 0.2866,
      "step": 2807
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.055005546659231186,
      "learning_rate": 5.030707610146863e-05,
      "loss": 0.3175,
      "step": 2808
    },
    {
      "epoch": 0.7490666666666667,
      "grad_norm": 0.04592027887701988,
      "learning_rate": 5.025367156208278e-05,
      "loss": 0.284,
      "step": 2809
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.060225777328014374,
      "learning_rate": 5.0200267022696936e-05,
      "loss": 0.3716,
      "step": 2810
    },
    {
      "epoch": 0.7496,
      "grad_norm": 0.04958578199148178,
      "learning_rate": 5.0146862483311086e-05,
      "loss": 0.2771,
      "step": 2811
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.04735877364873886,
      "learning_rate": 5.0093457943925236e-05,
      "loss": 0.2673,
      "step": 2812
    },
    {
      "epoch": 0.7501333333333333,
      "grad_norm": 0.059285104274749756,
      "learning_rate": 5.004005340453939e-05,
      "loss": 0.3293,
      "step": 2813
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.05872807651758194,
      "learning_rate": 4.998664886515354e-05,
      "loss": 0.3349,
      "step": 2814
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 0.052615124732255936,
      "learning_rate": 4.993324432576769e-05,
      "loss": 0.3158,
      "step": 2815
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.05055106058716774,
      "learning_rate": 4.987983978638184e-05,
      "loss": 0.3057,
      "step": 2816
    },
    {
      "epoch": 0.7512,
      "grad_norm": 0.05450186878442764,
      "learning_rate": 4.9826435246996e-05,
      "loss": 0.3115,
      "step": 2817
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.0389295294880867,
      "learning_rate": 4.9773030707610146e-05,
      "loss": 0.2318,
      "step": 2818
    },
    {
      "epoch": 0.7517333333333334,
      "grad_norm": 0.052052926272153854,
      "learning_rate": 4.97196261682243e-05,
      "loss": 0.3315,
      "step": 2819
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.05154779925942421,
      "learning_rate": 4.966622162883845e-05,
      "loss": 0.3206,
      "step": 2820
    },
    {
      "epoch": 0.7522666666666666,
      "grad_norm": 0.05328041687607765,
      "learning_rate": 4.961281708945261e-05,
      "loss": 0.353,
      "step": 2821
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.054030634462833405,
      "learning_rate": 4.955941255006676e-05,
      "loss": 0.3158,
      "step": 2822
    },
    {
      "epoch": 0.7528,
      "grad_norm": 0.05633651837706566,
      "learning_rate": 4.950600801068091e-05,
      "loss": 0.3125,
      "step": 2823
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.05955628305673599,
      "learning_rate": 4.9452603471295064e-05,
      "loss": 0.3435,
      "step": 2824
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06638877838850021,
      "learning_rate": 4.9399198931909214e-05,
      "loss": 0.3277,
      "step": 2825
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.04787740111351013,
      "learning_rate": 4.934579439252336e-05,
      "loss": 0.2597,
      "step": 2826
    },
    {
      "epoch": 0.7538666666666667,
      "grad_norm": 0.05584867298603058,
      "learning_rate": 4.929238985313752e-05,
      "loss": 0.2846,
      "step": 2827
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.0610528364777565,
      "learning_rate": 4.9238985313751676e-05,
      "loss": 0.4019,
      "step": 2828
    },
    {
      "epoch": 0.7544,
      "grad_norm": 0.06504693627357483,
      "learning_rate": 4.9185580774365825e-05,
      "loss": 0.3642,
      "step": 2829
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.05333661660552025,
      "learning_rate": 4.9132176234979975e-05,
      "loss": 0.3612,
      "step": 2830
    },
    {
      "epoch": 0.7549333333333333,
      "grad_norm": 0.04423503577709198,
      "learning_rate": 4.9078771695594125e-05,
      "loss": 0.2806,
      "step": 2831
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.06418043375015259,
      "learning_rate": 4.902536715620828e-05,
      "loss": 0.2951,
      "step": 2832
    },
    {
      "epoch": 0.7554666666666666,
      "grad_norm": 0.06417957693338394,
      "learning_rate": 4.897196261682243e-05,
      "loss": 0.3693,
      "step": 2833
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.04677965119481087,
      "learning_rate": 4.891855807743658e-05,
      "loss": 0.3036,
      "step": 2834
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.04980103299021721,
      "learning_rate": 4.8865153538050736e-05,
      "loss": 0.3281,
      "step": 2835
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.05404127761721611,
      "learning_rate": 4.881174899866489e-05,
      "loss": 0.365,
      "step": 2836
    },
    {
      "epoch": 0.7565333333333333,
      "grad_norm": 0.05372900143265724,
      "learning_rate": 4.875834445927904e-05,
      "loss": 0.3043,
      "step": 2837
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.05074203386902809,
      "learning_rate": 4.870493991989319e-05,
      "loss": 0.2934,
      "step": 2838
    },
    {
      "epoch": 0.7570666666666667,
      "grad_norm": 0.10844339430332184,
      "learning_rate": 4.865153538050735e-05,
      "loss": 0.2861,
      "step": 2839
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.06841035932302475,
      "learning_rate": 4.85981308411215e-05,
      "loss": 0.3772,
      "step": 2840
    },
    {
      "epoch": 0.7576,
      "grad_norm": 0.0463494211435318,
      "learning_rate": 4.854472630173565e-05,
      "loss": 0.2613,
      "step": 2841
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.05717337876558304,
      "learning_rate": 4.8491321762349804e-05,
      "loss": 0.3735,
      "step": 2842
    },
    {
      "epoch": 0.7581333333333333,
      "grad_norm": 0.05527246370911598,
      "learning_rate": 4.843791722296395e-05,
      "loss": 0.3462,
      "step": 2843
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.04864921793341637,
      "learning_rate": 4.83845126835781e-05,
      "loss": 0.2772,
      "step": 2844
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 0.058671995997428894,
      "learning_rate": 4.833110814419226e-05,
      "loss": 0.3229,
      "step": 2845
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.0477723702788353,
      "learning_rate": 4.8277703604806415e-05,
      "loss": 0.2712,
      "step": 2846
    },
    {
      "epoch": 0.7592,
      "grad_norm": 0.06940369307994843,
      "learning_rate": 4.8224299065420565e-05,
      "loss": 0.4246,
      "step": 2847
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.06339899450540543,
      "learning_rate": 4.8170894526034715e-05,
      "loss": 0.2625,
      "step": 2848
    },
    {
      "epoch": 0.7597333333333334,
      "grad_norm": 0.0812022015452385,
      "learning_rate": 4.8117489986648864e-05,
      "loss": 0.3362,
      "step": 2849
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.0864991620182991,
      "learning_rate": 4.806408544726302e-05,
      "loss": 0.3178,
      "step": 2850
    },
    {
      "epoch": 0.7602666666666666,
      "grad_norm": 0.06979867070913315,
      "learning_rate": 4.801068090787717e-05,
      "loss": 0.2952,
      "step": 2851
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.09349093586206436,
      "learning_rate": 4.795727636849132e-05,
      "loss": 0.3221,
      "step": 2852
    },
    {
      "epoch": 0.7608,
      "grad_norm": 0.05850201100111008,
      "learning_rate": 4.7903871829105476e-05,
      "loss": 0.3275,
      "step": 2853
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.04448575899004936,
      "learning_rate": 4.785046728971963e-05,
      "loss": 0.2579,
      "step": 2854
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 0.06617317348718643,
      "learning_rate": 4.779706275033378e-05,
      "loss": 0.2749,
      "step": 2855
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.05588015541434288,
      "learning_rate": 4.774365821094793e-05,
      "loss": 0.3527,
      "step": 2856
    },
    {
      "epoch": 0.7618666666666667,
      "grad_norm": 0.06003484129905701,
      "learning_rate": 4.769025367156209e-05,
      "loss": 0.36,
      "step": 2857
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.0468209944665432,
      "learning_rate": 4.763684913217624e-05,
      "loss": 0.2962,
      "step": 2858
    },
    {
      "epoch": 0.7624,
      "grad_norm": 0.048925213515758514,
      "learning_rate": 4.758344459279039e-05,
      "loss": 0.3239,
      "step": 2859
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.04745756462216377,
      "learning_rate": 4.753004005340454e-05,
      "loss": 0.2462,
      "step": 2860
    },
    {
      "epoch": 0.7629333333333334,
      "grad_norm": 0.042826298624277115,
      "learning_rate": 4.747663551401869e-05,
      "loss": 0.2882,
      "step": 2861
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.03881143033504486,
      "learning_rate": 4.742323097463285e-05,
      "loss": 0.2714,
      "step": 2862
    },
    {
      "epoch": 0.7634666666666666,
      "grad_norm": 0.04898492246866226,
      "learning_rate": 4.7369826435247e-05,
      "loss": 0.2892,
      "step": 2863
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.05650850385427475,
      "learning_rate": 4.7316421895861155e-05,
      "loss": 0.341,
      "step": 2864
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.05060625076293945,
      "learning_rate": 4.7263017356475304e-05,
      "loss": 0.3333,
      "step": 2865
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.0515592060983181,
      "learning_rate": 4.7209612817089454e-05,
      "loss": 0.3497,
      "step": 2866
    },
    {
      "epoch": 0.7645333333333333,
      "grad_norm": 0.053727347403764725,
      "learning_rate": 4.7156208277703604e-05,
      "loss": 0.3477,
      "step": 2867
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.05596420541405678,
      "learning_rate": 4.710280373831776e-05,
      "loss": 0.2884,
      "step": 2868
    },
    {
      "epoch": 0.7650666666666667,
      "grad_norm": 0.04139881953597069,
      "learning_rate": 4.704939919893191e-05,
      "loss": 0.2742,
      "step": 2869
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.04856697842478752,
      "learning_rate": 4.699599465954606e-05,
      "loss": 0.3014,
      "step": 2870
    },
    {
      "epoch": 0.7656,
      "grad_norm": 0.06115080788731575,
      "learning_rate": 4.6942590120160215e-05,
      "loss": 0.3454,
      "step": 2871
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.0403079055249691,
      "learning_rate": 4.688918558077437e-05,
      "loss": 0.2499,
      "step": 2872
    },
    {
      "epoch": 0.7661333333333333,
      "grad_norm": 0.04583972319960594,
      "learning_rate": 4.683578104138852e-05,
      "loss": 0.3082,
      "step": 2873
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.055441200733184814,
      "learning_rate": 4.678237650200267e-05,
      "loss": 0.3599,
      "step": 2874
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.04850846529006958,
      "learning_rate": 4.672897196261683e-05,
      "loss": 0.2927,
      "step": 2875
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.053296562284231186,
      "learning_rate": 4.667556742323098e-05,
      "loss": 0.3423,
      "step": 2876
    },
    {
      "epoch": 0.7672,
      "grad_norm": 0.04962329566478729,
      "learning_rate": 4.6622162883845126e-05,
      "loss": 0.3044,
      "step": 2877
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.05099685117602348,
      "learning_rate": 4.6568758344459276e-05,
      "loss": 0.3108,
      "step": 2878
    },
    {
      "epoch": 0.7677333333333334,
      "grad_norm": 0.0523458756506443,
      "learning_rate": 4.651535380507344e-05,
      "loss": 0.3215,
      "step": 2879
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.05052405968308449,
      "learning_rate": 4.646194926568759e-05,
      "loss": 0.3465,
      "step": 2880
    },
    {
      "epoch": 0.7682666666666667,
      "grad_norm": 0.0643330067396164,
      "learning_rate": 4.640854472630174e-05,
      "loss": 0.3859,
      "step": 2881
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.05338635295629501,
      "learning_rate": 4.635514018691589e-05,
      "loss": 0.3072,
      "step": 2882
    },
    {
      "epoch": 0.7688,
      "grad_norm": 0.054939426481723785,
      "learning_rate": 4.6301735647530044e-05,
      "loss": 0.4223,
      "step": 2883
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.04656857252120972,
      "learning_rate": 4.6248331108144193e-05,
      "loss": 0.2866,
      "step": 2884
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 0.05165693163871765,
      "learning_rate": 4.619492656875834e-05,
      "loss": 0.3016,
      "step": 2885
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.08148620277643204,
      "learning_rate": 4.61415220293725e-05,
      "loss": 0.429,
      "step": 2886
    },
    {
      "epoch": 0.7698666666666667,
      "grad_norm": 0.061369363218545914,
      "learning_rate": 4.608811748998665e-05,
      "loss": 0.4043,
      "step": 2887
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.049290675669908524,
      "learning_rate": 4.6034712950600805e-05,
      "loss": 0.2415,
      "step": 2888
    },
    {
      "epoch": 0.7704,
      "grad_norm": 0.05025513842701912,
      "learning_rate": 4.5981308411214955e-05,
      "loss": 0.2916,
      "step": 2889
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.06041260063648224,
      "learning_rate": 4.592790387182911e-05,
      "loss": 0.2775,
      "step": 2890
    },
    {
      "epoch": 0.7709333333333334,
      "grad_norm": 0.055141154676675797,
      "learning_rate": 4.587449933244326e-05,
      "loss": 0.2988,
      "step": 2891
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.06788402050733566,
      "learning_rate": 4.582109479305741e-05,
      "loss": 0.3765,
      "step": 2892
    },
    {
      "epoch": 0.7714666666666666,
      "grad_norm": 0.060448646545410156,
      "learning_rate": 4.5767690253671567e-05,
      "loss": 0.3392,
      "step": 2893
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.067169688642025,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.388,
      "step": 2894
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.06230553239583969,
      "learning_rate": 4.5660881174899866e-05,
      "loss": 0.381,
      "step": 2895
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.053276900202035904,
      "learning_rate": 4.5607476635514015e-05,
      "loss": 0.3249,
      "step": 2896
    },
    {
      "epoch": 0.7725333333333333,
      "grad_norm": 0.04797988012433052,
      "learning_rate": 4.555407209612818e-05,
      "loss": 0.3121,
      "step": 2897
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.06025715917348862,
      "learning_rate": 4.550066755674233e-05,
      "loss": 0.3445,
      "step": 2898
    },
    {
      "epoch": 0.7730666666666667,
      "grad_norm": 0.05269958823919296,
      "learning_rate": 4.544726301735648e-05,
      "loss": 0.3285,
      "step": 2899
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.052099164575338364,
      "learning_rate": 4.539385847797063e-05,
      "loss": 0.3389,
      "step": 2900
    },
    {
      "epoch": 0.7736,
      "grad_norm": 0.042231351137161255,
      "learning_rate": 4.534045393858478e-05,
      "loss": 0.275,
      "step": 2901
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.07153216004371643,
      "learning_rate": 4.528704939919893e-05,
      "loss": 0.3324,
      "step": 2902
    },
    {
      "epoch": 0.7741333333333333,
      "grad_norm": 0.048351868987083435,
      "learning_rate": 4.523364485981308e-05,
      "loss": 0.3257,
      "step": 2903
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.051814641803503036,
      "learning_rate": 4.518024032042724e-05,
      "loss": 0.2999,
      "step": 2904
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 0.059807926416397095,
      "learning_rate": 4.5126835781041395e-05,
      "loss": 0.3751,
      "step": 2905
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.05798633396625519,
      "learning_rate": 4.5073431241655545e-05,
      "loss": 0.3851,
      "step": 2906
    },
    {
      "epoch": 0.7752,
      "grad_norm": 0.05299603193998337,
      "learning_rate": 4.5020026702269694e-05,
      "loss": 0.2793,
      "step": 2907
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.05645394325256348,
      "learning_rate": 4.496662216288385e-05,
      "loss": 0.311,
      "step": 2908
    },
    {
      "epoch": 0.7757333333333334,
      "grad_norm": 0.05607357993721962,
      "learning_rate": 4.4913217623498e-05,
      "loss": 0.3432,
      "step": 2909
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.040349654853343964,
      "learning_rate": 4.485981308411215e-05,
      "loss": 0.2781,
      "step": 2910
    },
    {
      "epoch": 0.7762666666666667,
      "grad_norm": 0.06198332831263542,
      "learning_rate": 4.4806408544726306e-05,
      "loss": 0.334,
      "step": 2911
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.05156999081373215,
      "learning_rate": 4.4753004005340456e-05,
      "loss": 0.3148,
      "step": 2912
    },
    {
      "epoch": 0.7768,
      "grad_norm": 0.054830800741910934,
      "learning_rate": 4.4699599465954605e-05,
      "loss": 0.3543,
      "step": 2913
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.04848594591021538,
      "learning_rate": 4.464619492656876e-05,
      "loss": 0.2833,
      "step": 2914
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 0.05845852568745613,
      "learning_rate": 4.459279038718292e-05,
      "loss": 0.3098,
      "step": 2915
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.05320463702082634,
      "learning_rate": 4.453938584779707e-05,
      "loss": 0.3215,
      "step": 2916
    },
    {
      "epoch": 0.7778666666666667,
      "grad_norm": 0.041409824043512344,
      "learning_rate": 4.448598130841122e-05,
      "loss": 0.2824,
      "step": 2917
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.04542399197816849,
      "learning_rate": 4.4432576769025366e-05,
      "loss": 0.3285,
      "step": 2918
    },
    {
      "epoch": 0.7784,
      "grad_norm": 0.055833276361227036,
      "learning_rate": 4.437917222963952e-05,
      "loss": 0.3656,
      "step": 2919
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.057562701404094696,
      "learning_rate": 4.432576769025367e-05,
      "loss": 0.4197,
      "step": 2920
    },
    {
      "epoch": 0.7789333333333334,
      "grad_norm": 0.053236301988363266,
      "learning_rate": 4.427236315086782e-05,
      "loss": 0.3454,
      "step": 2921
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.056196391582489014,
      "learning_rate": 4.421895861148198e-05,
      "loss": 0.4116,
      "step": 2922
    },
    {
      "epoch": 0.7794666666666666,
      "grad_norm": 0.055693596601486206,
      "learning_rate": 4.4165554072096135e-05,
      "loss": 0.3746,
      "step": 2923
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.052837442606687546,
      "learning_rate": 4.4112149532710284e-05,
      "loss": 0.3165,
      "step": 2924
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04940689727663994,
      "learning_rate": 4.4058744993324434e-05,
      "loss": 0.2973,
      "step": 2925
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.0613677054643631,
      "learning_rate": 4.400534045393859e-05,
      "loss": 0.3297,
      "step": 2926
    },
    {
      "epoch": 0.7805333333333333,
      "grad_norm": 0.06648392230272293,
      "learning_rate": 4.395193591455274e-05,
      "loss": 0.3379,
      "step": 2927
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.061140868812799454,
      "learning_rate": 4.389853137516689e-05,
      "loss": 0.3376,
      "step": 2928
    },
    {
      "epoch": 0.7810666666666667,
      "grad_norm": 0.056598205119371414,
      "learning_rate": 4.384512683578104e-05,
      "loss": 0.3418,
      "step": 2929
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.058332670480012894,
      "learning_rate": 4.3791722296395195e-05,
      "loss": 0.2898,
      "step": 2930
    },
    {
      "epoch": 0.7816,
      "grad_norm": 0.04818242788314819,
      "learning_rate": 4.373831775700935e-05,
      "loss": 0.3393,
      "step": 2931
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.0500512421131134,
      "learning_rate": 4.36849132176235e-05,
      "loss": 0.2942,
      "step": 2932
    },
    {
      "epoch": 0.7821333333333333,
      "grad_norm": 0.05308542028069496,
      "learning_rate": 4.363150867823765e-05,
      "loss": 0.2458,
      "step": 2933
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.07156915217638016,
      "learning_rate": 4.357810413885181e-05,
      "loss": 0.4043,
      "step": 2934
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 0.04584156349301338,
      "learning_rate": 4.3524699599465956e-05,
      "loss": 0.3054,
      "step": 2935
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.05307059362530708,
      "learning_rate": 4.3471295060080106e-05,
      "loss": 0.2987,
      "step": 2936
    },
    {
      "epoch": 0.7832,
      "grad_norm": 0.05771121010184288,
      "learning_rate": 4.341789052069426e-05,
      "loss": 0.3682,
      "step": 2937
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.06028955802321434,
      "learning_rate": 4.336448598130841e-05,
      "loss": 0.304,
      "step": 2938
    },
    {
      "epoch": 0.7837333333333333,
      "grad_norm": 0.04850613698363304,
      "learning_rate": 4.331108144192256e-05,
      "loss": 0.2816,
      "step": 2939
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.06054255738854408,
      "learning_rate": 4.325767690253672e-05,
      "loss": 0.2886,
      "step": 2940
    },
    {
      "epoch": 0.7842666666666667,
      "grad_norm": 0.06486614793539047,
      "learning_rate": 4.3204272363150874e-05,
      "loss": 0.3087,
      "step": 2941
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.047603096812963486,
      "learning_rate": 4.3150867823765024e-05,
      "loss": 0.2899,
      "step": 2942
    },
    {
      "epoch": 0.7848,
      "grad_norm": 0.05171214044094086,
      "learning_rate": 4.309746328437917e-05,
      "loss": 0.2745,
      "step": 2943
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.06377708911895752,
      "learning_rate": 4.304405874499333e-05,
      "loss": 0.3866,
      "step": 2944
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 0.06219178065657616,
      "learning_rate": 4.299065420560748e-05,
      "loss": 0.3918,
      "step": 2945
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.059924110770225525,
      "learning_rate": 4.293724966622163e-05,
      "loss": 0.3392,
      "step": 2946
    },
    {
      "epoch": 0.7858666666666667,
      "grad_norm": 0.061642836779356,
      "learning_rate": 4.288384512683578e-05,
      "loss": 0.355,
      "step": 2947
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.03881426528096199,
      "learning_rate": 4.2830440587449934e-05,
      "loss": 0.2699,
      "step": 2948
    },
    {
      "epoch": 0.7864,
      "grad_norm": 0.06201564893126488,
      "learning_rate": 4.277703604806409e-05,
      "loss": 0.3717,
      "step": 2949
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.05234915018081665,
      "learning_rate": 4.272363150867824e-05,
      "loss": 0.3489,
      "step": 2950
    },
    {
      "epoch": 0.7869333333333334,
      "grad_norm": 0.045876797288656235,
      "learning_rate": 4.267022696929239e-05,
      "loss": 0.2806,
      "step": 2951
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.05930866301059723,
      "learning_rate": 4.2616822429906546e-05,
      "loss": 0.3314,
      "step": 2952
    },
    {
      "epoch": 0.7874666666666666,
      "grad_norm": 0.06191802769899368,
      "learning_rate": 4.2563417890520696e-05,
      "loss": 0.3326,
      "step": 2953
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.05892610922455788,
      "learning_rate": 4.2510013351134845e-05,
      "loss": 0.3856,
      "step": 2954
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.07583881169557571,
      "learning_rate": 4.2456608811749e-05,
      "loss": 0.2822,
      "step": 2955
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.05559522286057472,
      "learning_rate": 4.240320427236315e-05,
      "loss": 0.2643,
      "step": 2956
    },
    {
      "epoch": 0.7885333333333333,
      "grad_norm": 0.060681190341711044,
      "learning_rate": 4.234979973297731e-05,
      "loss": 0.3275,
      "step": 2957
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.06514177471399307,
      "learning_rate": 4.229639519359146e-05,
      "loss": 0.3178,
      "step": 2958
    },
    {
      "epoch": 0.7890666666666667,
      "grad_norm": 0.07010998576879501,
      "learning_rate": 4.2242990654205613e-05,
      "loss": 0.3509,
      "step": 2959
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.056534431874752045,
      "learning_rate": 4.218958611481976e-05,
      "loss": 0.3672,
      "step": 2960
    },
    {
      "epoch": 0.7896,
      "grad_norm": 0.06283225119113922,
      "learning_rate": 4.213618157543391e-05,
      "loss": 0.3606,
      "step": 2961
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.05593285709619522,
      "learning_rate": 4.208277703604807e-05,
      "loss": 0.3141,
      "step": 2962
    },
    {
      "epoch": 0.7901333333333334,
      "grad_norm": 0.059755273163318634,
      "learning_rate": 4.202937249666222e-05,
      "loss": 0.3574,
      "step": 2963
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.08227410167455673,
      "learning_rate": 4.197596795727637e-05,
      "loss": 0.3982,
      "step": 2964
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 0.06125104799866676,
      "learning_rate": 4.192256341789052e-05,
      "loss": 0.2899,
      "step": 2965
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.05071253329515457,
      "learning_rate": 4.186915887850468e-05,
      "loss": 0.2485,
      "step": 2966
    },
    {
      "epoch": 0.7912,
      "grad_norm": 0.057618532329797745,
      "learning_rate": 4.181575433911883e-05,
      "loss": 0.3347,
      "step": 2967
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.053826216608285904,
      "learning_rate": 4.176234979973298e-05,
      "loss": 0.3223,
      "step": 2968
    },
    {
      "epoch": 0.7917333333333333,
      "grad_norm": 0.06399296224117279,
      "learning_rate": 4.170894526034713e-05,
      "loss": 0.3535,
      "step": 2969
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.04469239339232445,
      "learning_rate": 4.1655540720961286e-05,
      "loss": 0.3223,
      "step": 2970
    },
    {
      "epoch": 0.7922666666666667,
      "grad_norm": 0.05688152462244034,
      "learning_rate": 4.1602136181575435e-05,
      "loss": 0.3651,
      "step": 2971
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.04937669634819031,
      "learning_rate": 4.1548731642189585e-05,
      "loss": 0.3135,
      "step": 2972
    },
    {
      "epoch": 0.7928,
      "grad_norm": 0.0619073361158371,
      "learning_rate": 4.149532710280374e-05,
      "loss": 0.3522,
      "step": 2973
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.06405341625213623,
      "learning_rate": 4.144192256341789e-05,
      "loss": 0.3424,
      "step": 2974
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.04974169656634331,
      "learning_rate": 4.138851802403205e-05,
      "loss": 0.3507,
      "step": 2975
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.046391963958740234,
      "learning_rate": 4.1335113484646197e-05,
      "loss": 0.2472,
      "step": 2976
    },
    {
      "epoch": 0.7938666666666667,
      "grad_norm": 0.04469498619437218,
      "learning_rate": 4.128170894526035e-05,
      "loss": 0.2959,
      "step": 2977
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.04341373220086098,
      "learning_rate": 4.12283044058745e-05,
      "loss": 0.2547,
      "step": 2978
    },
    {
      "epoch": 0.7944,
      "grad_norm": 0.06725867092609406,
      "learning_rate": 4.117489986648865e-05,
      "loss": 0.369,
      "step": 2979
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.04954522103071213,
      "learning_rate": 4.11214953271028e-05,
      "loss": 0.3199,
      "step": 2980
    },
    {
      "epoch": 0.7949333333333334,
      "grad_norm": 0.046688709408044815,
      "learning_rate": 4.106809078771696e-05,
      "loss": 0.2865,
      "step": 2981
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.055623412132263184,
      "learning_rate": 4.101468624833111e-05,
      "loss": 0.3594,
      "step": 2982
    },
    {
      "epoch": 0.7954666666666667,
      "grad_norm": 0.04196680337190628,
      "learning_rate": 4.0961281708945264e-05,
      "loss": 0.2536,
      "step": 2983
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.0617339164018631,
      "learning_rate": 4.090787716955941e-05,
      "loss": 0.3418,
      "step": 2984
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.05225189030170441,
      "learning_rate": 4.085447263017357e-05,
      "loss": 0.3035,
      "step": 2985
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.06219339370727539,
      "learning_rate": 4.080106809078772e-05,
      "loss": 0.3393,
      "step": 2986
    },
    {
      "epoch": 0.7965333333333333,
      "grad_norm": 0.049988456070423126,
      "learning_rate": 4.074766355140187e-05,
      "loss": 0.3188,
      "step": 2987
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.04252488166093826,
      "learning_rate": 4.0694259012016025e-05,
      "loss": 0.2728,
      "step": 2988
    },
    {
      "epoch": 0.7970666666666667,
      "grad_norm": 0.05638964846730232,
      "learning_rate": 4.0640854472630175e-05,
      "loss": 0.3271,
      "step": 2989
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.04944472387433052,
      "learning_rate": 4.0587449933244324e-05,
      "loss": 0.3132,
      "step": 2990
    },
    {
      "epoch": 0.7976,
      "grad_norm": 0.0590326301753521,
      "learning_rate": 4.053404539385848e-05,
      "loss": 0.3461,
      "step": 2991
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.056601256132125854,
      "learning_rate": 4.048064085447264e-05,
      "loss": 0.322,
      "step": 2992
    },
    {
      "epoch": 0.7981333333333334,
      "grad_norm": 0.055689822882413864,
      "learning_rate": 4.0427236315086786e-05,
      "loss": 0.3328,
      "step": 2993
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.05054374784231186,
      "learning_rate": 4.0373831775700936e-05,
      "loss": 0.3595,
      "step": 2994
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 0.05906994640827179,
      "learning_rate": 4.032042723631509e-05,
      "loss": 0.3167,
      "step": 2995
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.04891723021864891,
      "learning_rate": 4.026702269692924e-05,
      "loss": 0.2819,
      "step": 2996
    },
    {
      "epoch": 0.7992,
      "grad_norm": 0.04724984988570213,
      "learning_rate": 4.021361815754339e-05,
      "loss": 0.296,
      "step": 2997
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.05060705170035362,
      "learning_rate": 4.016021361815754e-05,
      "loss": 0.3226,
      "step": 2998
    },
    {
      "epoch": 0.7997333333333333,
      "grad_norm": 0.07109188288450241,
      "learning_rate": 4.01068090787717e-05,
      "loss": 0.3011,
      "step": 2999
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.08128658682107925,
      "learning_rate": 4.005340453938585e-05,
      "loss": 0.353,
      "step": 3000
    },
    {
      "epoch": 0.8002666666666667,
      "grad_norm": 0.06748449057340622,
      "learning_rate": 4e-05,
      "loss": 0.3084,
      "step": 3001
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.066419817507267,
      "learning_rate": 3.994659546061415e-05,
      "loss": 0.3782,
      "step": 3002
    },
    {
      "epoch": 0.8008,
      "grad_norm": 0.06247643753886223,
      "learning_rate": 3.989319092122831e-05,
      "loss": 0.3668,
      "step": 3003
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.061375971883535385,
      "learning_rate": 3.983978638184246e-05,
      "loss": 0.2944,
      "step": 3004
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 0.048962291330099106,
      "learning_rate": 3.978638184245661e-05,
      "loss": 0.3111,
      "step": 3005
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.04562351480126381,
      "learning_rate": 3.9732977303070765e-05,
      "loss": 0.2972,
      "step": 3006
    },
    {
      "epoch": 0.8018666666666666,
      "grad_norm": 0.0587194561958313,
      "learning_rate": 3.9679572763684914e-05,
      "loss": 0.3707,
      "step": 3007
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.04486680403351784,
      "learning_rate": 3.9626168224299064e-05,
      "loss": 0.3103,
      "step": 3008
    },
    {
      "epoch": 0.8024,
      "grad_norm": 0.05111442133784294,
      "learning_rate": 3.957276368491322e-05,
      "loss": 0.302,
      "step": 3009
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.0481448620557785,
      "learning_rate": 3.9519359145527376e-05,
      "loss": 0.2853,
      "step": 3010
    },
    {
      "epoch": 0.8029333333333334,
      "grad_norm": 0.06809297204017639,
      "learning_rate": 3.9465954606141526e-05,
      "loss": 0.4294,
      "step": 3011
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.05856625735759735,
      "learning_rate": 3.9412550066755676e-05,
      "loss": 0.305,
      "step": 3012
    },
    {
      "epoch": 0.8034666666666667,
      "grad_norm": 0.049278415739536285,
      "learning_rate": 3.9359145527369825e-05,
      "loss": 0.2491,
      "step": 3013
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.06706684827804565,
      "learning_rate": 3.930574098798398e-05,
      "loss": 0.3617,
      "step": 3014
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.05505535006523132,
      "learning_rate": 3.925233644859813e-05,
      "loss": 0.3304,
      "step": 3015
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.04823901504278183,
      "learning_rate": 3.919893190921228e-05,
      "loss": 0.3084,
      "step": 3016
    },
    {
      "epoch": 0.8045333333333333,
      "grad_norm": 0.06488321721553802,
      "learning_rate": 3.914552736982644e-05,
      "loss": 0.3326,
      "step": 3017
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.06978045403957367,
      "learning_rate": 3.909212283044059e-05,
      "loss": 0.3716,
      "step": 3018
    },
    {
      "epoch": 0.8050666666666667,
      "grad_norm": 0.07340327650308609,
      "learning_rate": 3.903871829105474e-05,
      "loss": 0.392,
      "step": 3019
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.05853249132633209,
      "learning_rate": 3.898531375166889e-05,
      "loss": 0.3474,
      "step": 3020
    },
    {
      "epoch": 0.8056,
      "grad_norm": 0.051307596266269684,
      "learning_rate": 3.893190921228305e-05,
      "loss": 0.3428,
      "step": 3021
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.04380664601922035,
      "learning_rate": 3.88785046728972e-05,
      "loss": 0.2725,
      "step": 3022
    },
    {
      "epoch": 0.8061333333333334,
      "grad_norm": 0.05828535929322243,
      "learning_rate": 3.882510013351135e-05,
      "loss": 0.2965,
      "step": 3023
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.05208864435553551,
      "learning_rate": 3.8771695594125504e-05,
      "loss": 0.3114,
      "step": 3024
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.04134906455874443,
      "learning_rate": 3.8718291054739654e-05,
      "loss": 0.2115,
      "step": 3025
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.07744661718606949,
      "learning_rate": 3.86648865153538e-05,
      "loss": 0.3205,
      "step": 3026
    },
    {
      "epoch": 0.8072,
      "grad_norm": 0.05199962854385376,
      "learning_rate": 3.861148197596796e-05,
      "loss": 0.2868,
      "step": 3027
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.0605122409760952,
      "learning_rate": 3.8558077436582116e-05,
      "loss": 0.3459,
      "step": 3028
    },
    {
      "epoch": 0.8077333333333333,
      "grad_norm": 0.06427732110023499,
      "learning_rate": 3.8504672897196265e-05,
      "loss": 0.3494,
      "step": 3029
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.0821203738451004,
      "learning_rate": 3.8451268357810415e-05,
      "loss": 0.3899,
      "step": 3030
    },
    {
      "epoch": 0.8082666666666667,
      "grad_norm": 0.07661095261573792,
      "learning_rate": 3.8397863818424565e-05,
      "loss": 0.3045,
      "step": 3031
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.06256478279829025,
      "learning_rate": 3.834445927903872e-05,
      "loss": 0.3355,
      "step": 3032
    },
    {
      "epoch": 0.8088,
      "grad_norm": 0.05203647539019585,
      "learning_rate": 3.829105473965287e-05,
      "loss": 0.2997,
      "step": 3033
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.050320908427238464,
      "learning_rate": 3.823765020026702e-05,
      "loss": 0.3409,
      "step": 3034
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 0.05517459660768509,
      "learning_rate": 3.8184245660881176e-05,
      "loss": 0.2937,
      "step": 3035
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.0664297565817833,
      "learning_rate": 3.813084112149533e-05,
      "loss": 0.3264,
      "step": 3036
    },
    {
      "epoch": 0.8098666666666666,
      "grad_norm": 0.08421570807695389,
      "learning_rate": 3.807743658210948e-05,
      "loss": 0.3087,
      "step": 3037
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.052786823362112045,
      "learning_rate": 3.802403204272363e-05,
      "loss": 0.3646,
      "step": 3038
    },
    {
      "epoch": 0.8104,
      "grad_norm": 0.08635247498750687,
      "learning_rate": 3.797062750333779e-05,
      "loss": 0.3761,
      "step": 3039
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.04816360026597977,
      "learning_rate": 3.791722296395194e-05,
      "loss": 0.3266,
      "step": 3040
    },
    {
      "epoch": 0.8109333333333333,
      "grad_norm": 0.05089913681149483,
      "learning_rate": 3.786381842456609e-05,
      "loss": 0.2809,
      "step": 3041
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.04810071736574173,
      "learning_rate": 3.7810413885180244e-05,
      "loss": 0.3246,
      "step": 3042
    },
    {
      "epoch": 0.8114666666666667,
      "grad_norm": 0.04466325417160988,
      "learning_rate": 3.775700934579439e-05,
      "loss": 0.2934,
      "step": 3043
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.04907738417387009,
      "learning_rate": 3.770360480640855e-05,
      "loss": 0.3304,
      "step": 3044
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.04978383332490921,
      "learning_rate": 3.76502002670227e-05,
      "loss": 0.3218,
      "step": 3045
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.04535636305809021,
      "learning_rate": 3.7596795727636855e-05,
      "loss": 0.28,
      "step": 3046
    },
    {
      "epoch": 0.8125333333333333,
      "grad_norm": 0.0576825849711895,
      "learning_rate": 3.7543391188251005e-05,
      "loss": 0.3216,
      "step": 3047
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.06971310079097748,
      "learning_rate": 3.7489986648865154e-05,
      "loss": 0.3632,
      "step": 3048
    },
    {
      "epoch": 0.8130666666666667,
      "grad_norm": 0.060246143490076065,
      "learning_rate": 3.7436582109479304e-05,
      "loss": 0.4037,
      "step": 3049
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.0521354116499424,
      "learning_rate": 3.738317757009346e-05,
      "loss": 0.3082,
      "step": 3050
    },
    {
      "epoch": 0.8136,
      "grad_norm": 0.05942073464393616,
      "learning_rate": 3.732977303070761e-05,
      "loss": 0.3474,
      "step": 3051
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.04656656086444855,
      "learning_rate": 3.727636849132176e-05,
      "loss": 0.2883,
      "step": 3052
    },
    {
      "epoch": 0.8141333333333334,
      "grad_norm": 0.056045256555080414,
      "learning_rate": 3.7222963951935916e-05,
      "loss": 0.3347,
      "step": 3053
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.05090823769569397,
      "learning_rate": 3.716955941255007e-05,
      "loss": 0.3043,
      "step": 3054
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 0.04940904304385185,
      "learning_rate": 3.711615487316422e-05,
      "loss": 0.2826,
      "step": 3055
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.049975525587797165,
      "learning_rate": 3.706275033377837e-05,
      "loss": 0.3068,
      "step": 3056
    },
    {
      "epoch": 0.8152,
      "grad_norm": 0.054656513035297394,
      "learning_rate": 3.700934579439253e-05,
      "loss": 0.3071,
      "step": 3057
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.07579714059829712,
      "learning_rate": 3.695594125500668e-05,
      "loss": 0.4032,
      "step": 3058
    },
    {
      "epoch": 0.8157333333333333,
      "grad_norm": 0.06385301053524017,
      "learning_rate": 3.690253671562083e-05,
      "loss": 0.3863,
      "step": 3059
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.05549027398228645,
      "learning_rate": 3.6849132176234976e-05,
      "loss": 0.4245,
      "step": 3060
    },
    {
      "epoch": 0.8162666666666667,
      "grad_norm": 0.04924604296684265,
      "learning_rate": 3.679572763684914e-05,
      "loss": 0.2991,
      "step": 3061
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.07444915175437927,
      "learning_rate": 3.674232309746329e-05,
      "loss": 0.3872,
      "step": 3062
    },
    {
      "epoch": 0.8168,
      "grad_norm": 0.04059735685586929,
      "learning_rate": 3.668891855807744e-05,
      "loss": 0.2326,
      "step": 3063
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.05014118552207947,
      "learning_rate": 3.663551401869159e-05,
      "loss": 0.2676,
      "step": 3064
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 0.053166285157203674,
      "learning_rate": 3.6582109479305744e-05,
      "loss": 0.3072,
      "step": 3065
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.05245112255215645,
      "learning_rate": 3.6528704939919894e-05,
      "loss": 0.3146,
      "step": 3066
    },
    {
      "epoch": 0.8178666666666666,
      "grad_norm": 0.0493708997964859,
      "learning_rate": 3.6475300400534043e-05,
      "loss": 0.3041,
      "step": 3067
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.07035113126039505,
      "learning_rate": 3.64218958611482e-05,
      "loss": 0.4379,
      "step": 3068
    },
    {
      "epoch": 0.8184,
      "grad_norm": 0.06283595412969589,
      "learning_rate": 3.636849132176235e-05,
      "loss": 0.2674,
      "step": 3069
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.06019619107246399,
      "learning_rate": 3.6315086782376506e-05,
      "loss": 0.3387,
      "step": 3070
    },
    {
      "epoch": 0.8189333333333333,
      "grad_norm": 0.059603624045848846,
      "learning_rate": 3.6261682242990655e-05,
      "loss": 0.3398,
      "step": 3071
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.05507117509841919,
      "learning_rate": 3.620827770360481e-05,
      "loss": 0.3166,
      "step": 3072
    },
    {
      "epoch": 0.8194666666666667,
      "grad_norm": 0.050673969089984894,
      "learning_rate": 3.615487316421896e-05,
      "loss": 0.3036,
      "step": 3073
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.057741302996873856,
      "learning_rate": 3.610146862483311e-05,
      "loss": 0.3423,
      "step": 3074
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.06684709340333939,
      "learning_rate": 3.604806408544727e-05,
      "loss": 0.4035,
      "step": 3075
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.051144763827323914,
      "learning_rate": 3.5994659546061417e-05,
      "loss": 0.2771,
      "step": 3076
    },
    {
      "epoch": 0.8205333333333333,
      "grad_norm": 0.04637344554066658,
      "learning_rate": 3.5941255006675566e-05,
      "loss": 0.2892,
      "step": 3077
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.05155516043305397,
      "learning_rate": 3.5887850467289716e-05,
      "loss": 0.3179,
      "step": 3078
    },
    {
      "epoch": 0.8210666666666666,
      "grad_norm": 0.05136296525597572,
      "learning_rate": 3.583444592790388e-05,
      "loss": 0.3056,
      "step": 3079
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.05506741255521774,
      "learning_rate": 3.578104138851803e-05,
      "loss": 0.3161,
      "step": 3080
    },
    {
      "epoch": 0.8216,
      "grad_norm": 0.054685816168785095,
      "learning_rate": 3.572763684913218e-05,
      "loss": 0.3332,
      "step": 3081
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.06286488473415375,
      "learning_rate": 3.567423230974633e-05,
      "loss": 0.3376,
      "step": 3082
    },
    {
      "epoch": 0.8221333333333334,
      "grad_norm": 0.06066691130399704,
      "learning_rate": 3.5620827770360484e-05,
      "loss": 0.3677,
      "step": 3083
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.05541742220520973,
      "learning_rate": 3.556742323097463e-05,
      "loss": 0.2912,
      "step": 3084
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 0.04932662099599838,
      "learning_rate": 3.551401869158878e-05,
      "loss": 0.2649,
      "step": 3085
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.050595927983522415,
      "learning_rate": 3.546061415220294e-05,
      "loss": 0.3096,
      "step": 3086
    },
    {
      "epoch": 0.8232,
      "grad_norm": 0.06039847433567047,
      "learning_rate": 3.5407209612817096e-05,
      "loss": 0.4022,
      "step": 3087
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.06406636536121368,
      "learning_rate": 3.5353805073431245e-05,
      "loss": 0.3421,
      "step": 3088
    },
    {
      "epoch": 0.8237333333333333,
      "grad_norm": 0.05013369023799896,
      "learning_rate": 3.5300400534045395e-05,
      "loss": 0.3339,
      "step": 3089
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.05588747188448906,
      "learning_rate": 3.524699599465955e-05,
      "loss": 0.3061,
      "step": 3090
    },
    {
      "epoch": 0.8242666666666667,
      "grad_norm": 0.04835806041955948,
      "learning_rate": 3.51935914552737e-05,
      "loss": 0.2684,
      "step": 3091
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.05888399854302406,
      "learning_rate": 3.514018691588785e-05,
      "loss": 0.2873,
      "step": 3092
    },
    {
      "epoch": 0.8248,
      "grad_norm": 0.052970897406339645,
      "learning_rate": 3.5086782376502006e-05,
      "loss": 0.3258,
      "step": 3093
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.053582098335027695,
      "learning_rate": 3.5033377837116156e-05,
      "loss": 0.3104,
      "step": 3094
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 0.07878077030181885,
      "learning_rate": 3.4979973297730306e-05,
      "loss": 0.3697,
      "step": 3095
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.05746486037969589,
      "learning_rate": 3.492656875834446e-05,
      "loss": 0.3573,
      "step": 3096
    },
    {
      "epoch": 0.8258666666666666,
      "grad_norm": 0.05968494340777397,
      "learning_rate": 3.487316421895862e-05,
      "loss": 0.3738,
      "step": 3097
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.08012524992227554,
      "learning_rate": 3.481975967957277e-05,
      "loss": 0.3875,
      "step": 3098
    },
    {
      "epoch": 0.8264,
      "grad_norm": 0.03937100991606712,
      "learning_rate": 3.476635514018692e-05,
      "loss": 0.2812,
      "step": 3099
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.05517641827464104,
      "learning_rate": 3.471295060080107e-05,
      "loss": 0.3344,
      "step": 3100
    },
    {
      "epoch": 0.8269333333333333,
      "grad_norm": 0.05269072577357292,
      "learning_rate": 3.465954606141522e-05,
      "loss": 0.2806,
      "step": 3101
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.04924265667796135,
      "learning_rate": 3.460614152202937e-05,
      "loss": 0.3055,
      "step": 3102
    },
    {
      "epoch": 0.8274666666666667,
      "grad_norm": 0.051137518137693405,
      "learning_rate": 3.455273698264352e-05,
      "loss": 0.3154,
      "step": 3103
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.04825546219944954,
      "learning_rate": 3.449933244325768e-05,
      "loss": 0.2341,
      "step": 3104
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.06410833448171616,
      "learning_rate": 3.4445927903871835e-05,
      "loss": 0.3368,
      "step": 3105
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.05131211876869202,
      "learning_rate": 3.4392523364485985e-05,
      "loss": 0.2843,
      "step": 3106
    },
    {
      "epoch": 0.8285333333333333,
      "grad_norm": 0.055994629859924316,
      "learning_rate": 3.4339118825100134e-05,
      "loss": 0.3017,
      "step": 3107
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.05358019098639488,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.3206,
      "step": 3108
    },
    {
      "epoch": 0.8290666666666666,
      "grad_norm": 0.04959554225206375,
      "learning_rate": 3.423230974632844e-05,
      "loss": 0.2825,
      "step": 3109
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.05220702663064003,
      "learning_rate": 3.417890520694259e-05,
      "loss": 0.3187,
      "step": 3110
    },
    {
      "epoch": 0.8296,
      "grad_norm": 0.05967751890420914,
      "learning_rate": 3.412550066755674e-05,
      "loss": 0.3268,
      "step": 3111
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.054960768669843674,
      "learning_rate": 3.4072096128170895e-05,
      "loss": 0.3606,
      "step": 3112
    },
    {
      "epoch": 0.8301333333333333,
      "grad_norm": 0.05409424751996994,
      "learning_rate": 3.401869158878505e-05,
      "loss": 0.3223,
      "step": 3113
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.06785207241773605,
      "learning_rate": 3.39652870493992e-05,
      "loss": 0.3126,
      "step": 3114
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 0.050923168659210205,
      "learning_rate": 3.391188251001335e-05,
      "loss": 0.332,
      "step": 3115
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.04806923866271973,
      "learning_rate": 3.385847797062751e-05,
      "loss": 0.2865,
      "step": 3116
    },
    {
      "epoch": 0.8312,
      "grad_norm": 0.05930202826857567,
      "learning_rate": 3.380507343124166e-05,
      "loss": 0.4128,
      "step": 3117
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.050314947962760925,
      "learning_rate": 3.3751668891855806e-05,
      "loss": 0.284,
      "step": 3118
    },
    {
      "epoch": 0.8317333333333333,
      "grad_norm": 0.0550445057451725,
      "learning_rate": 3.369826435246996e-05,
      "loss": 0.3402,
      "step": 3119
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.07386259734630585,
      "learning_rate": 3.364485981308411e-05,
      "loss": 0.3659,
      "step": 3120
    },
    {
      "epoch": 0.8322666666666667,
      "grad_norm": 0.06132885813713074,
      "learning_rate": 3.359145527369826e-05,
      "loss": 0.3674,
      "step": 3121
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.060686808079481125,
      "learning_rate": 3.353805073431242e-05,
      "loss": 0.371,
      "step": 3122
    },
    {
      "epoch": 0.8328,
      "grad_norm": 0.053593214601278305,
      "learning_rate": 3.3484646194926574e-05,
      "loss": 0.3328,
      "step": 3123
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.04991692304611206,
      "learning_rate": 3.3431241655540724e-05,
      "loss": 0.3121,
      "step": 3124
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.04829549044370651,
      "learning_rate": 3.3377837116154874e-05,
      "loss": 0.3222,
      "step": 3125
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.06025639921426773,
      "learning_rate": 3.332443257676903e-05,
      "loss": 0.2699,
      "step": 3126
    },
    {
      "epoch": 0.8338666666666666,
      "grad_norm": 0.06588083505630493,
      "learning_rate": 3.327102803738318e-05,
      "loss": 0.3958,
      "step": 3127
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.05870331451296806,
      "learning_rate": 3.321762349799733e-05,
      "loss": 0.358,
      "step": 3128
    },
    {
      "epoch": 0.8344,
      "grad_norm": 0.04938977584242821,
      "learning_rate": 3.316421895861148e-05,
      "loss": 0.308,
      "step": 3129
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.04850431904196739,
      "learning_rate": 3.3110814419225635e-05,
      "loss": 0.2886,
      "step": 3130
    },
    {
      "epoch": 0.8349333333333333,
      "grad_norm": 0.06258149445056915,
      "learning_rate": 3.305740987983979e-05,
      "loss": 0.3224,
      "step": 3131
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.057218097150325775,
      "learning_rate": 3.300400534045394e-05,
      "loss": 0.3572,
      "step": 3132
    },
    {
      "epoch": 0.8354666666666667,
      "grad_norm": 0.04303477704524994,
      "learning_rate": 3.295060080106809e-05,
      "loss": 0.2221,
      "step": 3133
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.054250407963991165,
      "learning_rate": 3.289719626168225e-05,
      "loss": 0.2739,
      "step": 3134
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.05128490924835205,
      "learning_rate": 3.2843791722296396e-05,
      "loss": 0.2995,
      "step": 3135
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.051489006727933884,
      "learning_rate": 3.2790387182910546e-05,
      "loss": 0.3163,
      "step": 3136
    },
    {
      "epoch": 0.8365333333333334,
      "grad_norm": 0.051145125180482864,
      "learning_rate": 3.27369826435247e-05,
      "loss": 0.2651,
      "step": 3137
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.046226028352975845,
      "learning_rate": 3.268357810413885e-05,
      "loss": 0.2929,
      "step": 3138
    },
    {
      "epoch": 0.8370666666666666,
      "grad_norm": 0.053375232964754105,
      "learning_rate": 3.263017356475301e-05,
      "loss": 0.3101,
      "step": 3139
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.053977277129888535,
      "learning_rate": 3.257676902536716e-05,
      "loss": 0.3059,
      "step": 3140
    },
    {
      "epoch": 0.8376,
      "grad_norm": 0.051632627844810486,
      "learning_rate": 3.2523364485981314e-05,
      "loss": 0.329,
      "step": 3141
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.04607478901743889,
      "learning_rate": 3.2469959946595463e-05,
      "loss": 0.2831,
      "step": 3142
    },
    {
      "epoch": 0.8381333333333333,
      "grad_norm": 0.046901870518922806,
      "learning_rate": 3.241655540720961e-05,
      "loss": 0.3084,
      "step": 3143
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.06002049148082733,
      "learning_rate": 3.236315086782377e-05,
      "loss": 0.3342,
      "step": 3144
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 0.046014491468667984,
      "learning_rate": 3.230974632843792e-05,
      "loss": 0.2835,
      "step": 3145
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.054327744990587234,
      "learning_rate": 3.225634178905207e-05,
      "loss": 0.3627,
      "step": 3146
    },
    {
      "epoch": 0.8392,
      "grad_norm": 0.03989509493112564,
      "learning_rate": 3.220293724966622e-05,
      "loss": 0.2535,
      "step": 3147
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.06146515533328056,
      "learning_rate": 3.214953271028038e-05,
      "loss": 0.3359,
      "step": 3148
    },
    {
      "epoch": 0.8397333333333333,
      "grad_norm": 0.07620752602815628,
      "learning_rate": 3.209612817089453e-05,
      "loss": 0.3822,
      "step": 3149
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.050939884036779404,
      "learning_rate": 3.204272363150868e-05,
      "loss": 0.2942,
      "step": 3150
    },
    {
      "epoch": 0.8402666666666667,
      "grad_norm": 0.05214531719684601,
      "learning_rate": 3.198931909212283e-05,
      "loss": 0.337,
      "step": 3151
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.06152813136577606,
      "learning_rate": 3.1935914552736986e-05,
      "loss": 0.32,
      "step": 3152
    },
    {
      "epoch": 0.8408,
      "grad_norm": 0.052604708820581436,
      "learning_rate": 3.1882510013351136e-05,
      "loss": 0.3088,
      "step": 3153
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.04955445975065231,
      "learning_rate": 3.1829105473965285e-05,
      "loss": 0.3609,
      "step": 3154
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 0.05734426528215408,
      "learning_rate": 3.177570093457944e-05,
      "loss": 0.364,
      "step": 3155
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.06075029447674751,
      "learning_rate": 3.172229639519359e-05,
      "loss": 0.3663,
      "step": 3156
    },
    {
      "epoch": 0.8418666666666667,
      "grad_norm": 0.06472977250814438,
      "learning_rate": 3.166889185580775e-05,
      "loss": 0.4035,
      "step": 3157
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.05670548602938652,
      "learning_rate": 3.16154873164219e-05,
      "loss": 0.3125,
      "step": 3158
    },
    {
      "epoch": 0.8424,
      "grad_norm": 0.07882346212863922,
      "learning_rate": 3.156208277703605e-05,
      "loss": 0.3428,
      "step": 3159
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.03820153698325157,
      "learning_rate": 3.15086782376502e-05,
      "loss": 0.2611,
      "step": 3160
    },
    {
      "epoch": 0.8429333333333333,
      "grad_norm": 0.059353057295084,
      "learning_rate": 3.145527369826435e-05,
      "loss": 0.3675,
      "step": 3161
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.05117865279316902,
      "learning_rate": 3.14018691588785e-05,
      "loss": 0.2622,
      "step": 3162
    },
    {
      "epoch": 0.8434666666666667,
      "grad_norm": 0.06361821293830872,
      "learning_rate": 3.134846461949266e-05,
      "loss": 0.3316,
      "step": 3163
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.05172596871852875,
      "learning_rate": 3.129506008010681e-05,
      "loss": 0.2447,
      "step": 3164
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.058058831840753555,
      "learning_rate": 3.1241655540720964e-05,
      "loss": 0.3324,
      "step": 3165
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.05463919788599014,
      "learning_rate": 3.1188251001335114e-05,
      "loss": 0.2788,
      "step": 3166
    },
    {
      "epoch": 0.8445333333333334,
      "grad_norm": 0.06404789537191391,
      "learning_rate": 3.113484646194927e-05,
      "loss": 0.3572,
      "step": 3167
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.07235024869441986,
      "learning_rate": 3.108144192256342e-05,
      "loss": 0.3618,
      "step": 3168
    },
    {
      "epoch": 0.8450666666666666,
      "grad_norm": 0.06939736753702164,
      "learning_rate": 3.102803738317757e-05,
      "loss": 0.3859,
      "step": 3169
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.04182770103216171,
      "learning_rate": 3.0974632843791726e-05,
      "loss": 0.2628,
      "step": 3170
    },
    {
      "epoch": 0.8456,
      "grad_norm": 0.06200658529996872,
      "learning_rate": 3.0921228304405875e-05,
      "loss": 0.3298,
      "step": 3171
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.05944669246673584,
      "learning_rate": 3.0867823765020025e-05,
      "loss": 0.2863,
      "step": 3172
    },
    {
      "epoch": 0.8461333333333333,
      "grad_norm": 0.05642139911651611,
      "learning_rate": 3.081441922563418e-05,
      "loss": 0.3112,
      "step": 3173
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.05745429918169975,
      "learning_rate": 3.076101468624834e-05,
      "loss": 0.3551,
      "step": 3174
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.049724604934453964,
      "learning_rate": 3.070761014686249e-05,
      "loss": 0.2838,
      "step": 3175
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.0474996380507946,
      "learning_rate": 3.0654205607476637e-05,
      "loss": 0.2728,
      "step": 3176
    },
    {
      "epoch": 0.8472,
      "grad_norm": 0.05410626158118248,
      "learning_rate": 3.060080106809079e-05,
      "loss": 0.296,
      "step": 3177
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.059119466692209244,
      "learning_rate": 3.054739652870494e-05,
      "loss": 0.3284,
      "step": 3178
    },
    {
      "epoch": 0.8477333333333333,
      "grad_norm": 0.055030617862939835,
      "learning_rate": 3.0493991989319092e-05,
      "loss": 0.3388,
      "step": 3179
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.059655215591192245,
      "learning_rate": 3.0440587449933245e-05,
      "loss": 0.305,
      "step": 3180
    },
    {
      "epoch": 0.8482666666666666,
      "grad_norm": 0.051661428064107895,
      "learning_rate": 3.0387182910547394e-05,
      "loss": 0.3163,
      "step": 3181
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.06613148748874664,
      "learning_rate": 3.0333778371161547e-05,
      "loss": 0.4047,
      "step": 3182
    },
    {
      "epoch": 0.8488,
      "grad_norm": 0.051848627626895905,
      "learning_rate": 3.0280373831775704e-05,
      "loss": 0.3189,
      "step": 3183
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.044196967035532,
      "learning_rate": 3.0226969292389857e-05,
      "loss": 0.3144,
      "step": 3184
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 0.09292087703943253,
      "learning_rate": 3.0173564753004006e-05,
      "loss": 0.3934,
      "step": 3185
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.04794679954648018,
      "learning_rate": 3.012016021361816e-05,
      "loss": 0.3046,
      "step": 3186
    },
    {
      "epoch": 0.8498666666666667,
      "grad_norm": 0.049542706459760666,
      "learning_rate": 3.0066755674232312e-05,
      "loss": 0.2917,
      "step": 3187
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.05785363167524338,
      "learning_rate": 3.001335113484646e-05,
      "loss": 0.249,
      "step": 3188
    },
    {
      "epoch": 0.8504,
      "grad_norm": 0.05182716250419617,
      "learning_rate": 2.9959946595460615e-05,
      "loss": 0.3097,
      "step": 3189
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.046933092176914215,
      "learning_rate": 2.9906542056074764e-05,
      "loss": 0.2714,
      "step": 3190
    },
    {
      "epoch": 0.8509333333333333,
      "grad_norm": 0.04255587235093117,
      "learning_rate": 2.9853137516688924e-05,
      "loss": 0.2469,
      "step": 3191
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.05581728741526604,
      "learning_rate": 2.9799732977303073e-05,
      "loss": 0.3542,
      "step": 3192
    },
    {
      "epoch": 0.8514666666666667,
      "grad_norm": 0.0635269433259964,
      "learning_rate": 2.9746328437917226e-05,
      "loss": 0.3125,
      "step": 3193
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.050967998802661896,
      "learning_rate": 2.9692923898531376e-05,
      "loss": 0.2894,
      "step": 3194
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.06447315216064453,
      "learning_rate": 2.963951935914553e-05,
      "loss": 0.3573,
      "step": 3195
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.05758596956729889,
      "learning_rate": 2.9586114819759682e-05,
      "loss": 0.3386,
      "step": 3196
    },
    {
      "epoch": 0.8525333333333334,
      "grad_norm": 0.04751063138246536,
      "learning_rate": 2.953271028037383e-05,
      "loss": 0.3505,
      "step": 3197
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.04689626768231392,
      "learning_rate": 2.9479305740987984e-05,
      "loss": 0.2928,
      "step": 3198
    },
    {
      "epoch": 0.8530666666666666,
      "grad_norm": 0.0582079254090786,
      "learning_rate": 2.9425901201602134e-05,
      "loss": 0.3219,
      "step": 3199
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.05483851954340935,
      "learning_rate": 2.9372496662216294e-05,
      "loss": 0.3448,
      "step": 3200
    },
    {
      "epoch": 0.8536,
      "grad_norm": 0.059858180582523346,
      "learning_rate": 2.9319092122830443e-05,
      "loss": 0.3778,
      "step": 3201
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.059915777295827866,
      "learning_rate": 2.9265687583444596e-05,
      "loss": 0.3536,
      "step": 3202
    },
    {
      "epoch": 0.8541333333333333,
      "grad_norm": 0.04645131900906563,
      "learning_rate": 2.9212283044058746e-05,
      "loss": 0.322,
      "step": 3203
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.043677642941474915,
      "learning_rate": 2.91588785046729e-05,
      "loss": 0.3149,
      "step": 3204
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 0.04970826208591461,
      "learning_rate": 2.910547396528705e-05,
      "loss": 0.3113,
      "step": 3205
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.07124963402748108,
      "learning_rate": 2.90520694259012e-05,
      "loss": 0.4382,
      "step": 3206
    },
    {
      "epoch": 0.8552,
      "grad_norm": 0.05150851979851723,
      "learning_rate": 2.8998664886515354e-05,
      "loss": 0.2991,
      "step": 3207
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.05563399940729141,
      "learning_rate": 2.8945260347129504e-05,
      "loss": 0.3006,
      "step": 3208
    },
    {
      "epoch": 0.8557333333333333,
      "grad_norm": 0.0541619248688221,
      "learning_rate": 2.8891855807743663e-05,
      "loss": 0.3191,
      "step": 3209
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.042581576853990555,
      "learning_rate": 2.8838451268357813e-05,
      "loss": 0.2967,
      "step": 3210
    },
    {
      "epoch": 0.8562666666666666,
      "grad_norm": 0.057554397732019424,
      "learning_rate": 2.8785046728971966e-05,
      "loss": 0.2805,
      "step": 3211
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.05368315055966377,
      "learning_rate": 2.8731642189586115e-05,
      "loss": 0.3264,
      "step": 3212
    },
    {
      "epoch": 0.8568,
      "grad_norm": 0.052691031247377396,
      "learning_rate": 2.867823765020027e-05,
      "loss": 0.3078,
      "step": 3213
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.06149915233254433,
      "learning_rate": 2.862483311081442e-05,
      "loss": 0.3258,
      "step": 3214
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 0.05268818512558937,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.3777,
      "step": 3215
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.05424612760543823,
      "learning_rate": 2.8518024032042724e-05,
      "loss": 0.3221,
      "step": 3216
    },
    {
      "epoch": 0.8578666666666667,
      "grad_norm": 0.05416440963745117,
      "learning_rate": 2.846461949265688e-05,
      "loss": 0.3796,
      "step": 3217
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.07063746452331543,
      "learning_rate": 2.8411214953271033e-05,
      "loss": 0.3675,
      "step": 3218
    },
    {
      "epoch": 0.8584,
      "grad_norm": 0.04246433079242706,
      "learning_rate": 2.8357810413885183e-05,
      "loss": 0.2785,
      "step": 3219
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.055935412645339966,
      "learning_rate": 2.8304405874499336e-05,
      "loss": 0.3334,
      "step": 3220
    },
    {
      "epoch": 0.8589333333333333,
      "grad_norm": 0.047254547476768494,
      "learning_rate": 2.8251001335113485e-05,
      "loss": 0.2685,
      "step": 3221
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.05851553753018379,
      "learning_rate": 2.8197596795727638e-05,
      "loss": 0.3417,
      "step": 3222
    },
    {
      "epoch": 0.8594666666666667,
      "grad_norm": 0.06290983408689499,
      "learning_rate": 2.8144192256341788e-05,
      "loss": 0.3485,
      "step": 3223
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.05483830347657204,
      "learning_rate": 2.809078771695594e-05,
      "loss": 0.3505,
      "step": 3224
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.05107930302619934,
      "learning_rate": 2.8037383177570094e-05,
      "loss": 0.3261,
      "step": 3225
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.04753124713897705,
      "learning_rate": 2.798397863818425e-05,
      "loss": 0.326,
      "step": 3226
    },
    {
      "epoch": 0.8605333333333334,
      "grad_norm": 0.06415398418903351,
      "learning_rate": 2.79305740987984e-05,
      "loss": 0.3624,
      "step": 3227
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.052122753113508224,
      "learning_rate": 2.7877169559412552e-05,
      "loss": 0.3373,
      "step": 3228
    },
    {
      "epoch": 0.8610666666666666,
      "grad_norm": 0.055849768221378326,
      "learning_rate": 2.7823765020026705e-05,
      "loss": 0.329,
      "step": 3229
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.04958055913448334,
      "learning_rate": 2.7770360480640855e-05,
      "loss": 0.2865,
      "step": 3230
    },
    {
      "epoch": 0.8616,
      "grad_norm": 0.060618769377470016,
      "learning_rate": 2.7716955941255008e-05,
      "loss": 0.2856,
      "step": 3231
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.04712553694844246,
      "learning_rate": 2.7663551401869157e-05,
      "loss": 0.2601,
      "step": 3232
    },
    {
      "epoch": 0.8621333333333333,
      "grad_norm": 0.057072822004556656,
      "learning_rate": 2.761014686248331e-05,
      "loss": 0.3726,
      "step": 3233
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.05474599450826645,
      "learning_rate": 2.7556742323097463e-05,
      "loss": 0.2788,
      "step": 3234
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 0.047516822814941406,
      "learning_rate": 2.750333778371162e-05,
      "loss": 0.2948,
      "step": 3235
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.06313865631818771,
      "learning_rate": 2.744993324432577e-05,
      "loss": 0.301,
      "step": 3236
    },
    {
      "epoch": 0.8632,
      "grad_norm": 0.06140514090657234,
      "learning_rate": 2.7396528704939922e-05,
      "loss": 0.3282,
      "step": 3237
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.05493456870317459,
      "learning_rate": 2.7343124165554075e-05,
      "loss": 0.3706,
      "step": 3238
    },
    {
      "epoch": 0.8637333333333334,
      "grad_norm": 0.05623910203576088,
      "learning_rate": 2.7289719626168225e-05,
      "loss": 0.3381,
      "step": 3239
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.05074499174952507,
      "learning_rate": 2.7236315086782378e-05,
      "loss": 0.2783,
      "step": 3240
    },
    {
      "epoch": 0.8642666666666666,
      "grad_norm": 0.052261773496866226,
      "learning_rate": 2.7182910547396527e-05,
      "loss": 0.3121,
      "step": 3241
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.05217895656824112,
      "learning_rate": 2.712950600801068e-05,
      "loss": 0.3153,
      "step": 3242
    },
    {
      "epoch": 0.8648,
      "grad_norm": 0.052175123244524,
      "learning_rate": 2.7076101468624836e-05,
      "loss": 0.3322,
      "step": 3243
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.05446331202983856,
      "learning_rate": 2.702269692923899e-05,
      "loss": 0.3347,
      "step": 3244
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 0.05020739138126373,
      "learning_rate": 2.696929238985314e-05,
      "loss": 0.2939,
      "step": 3245
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.06021244451403618,
      "learning_rate": 2.6915887850467292e-05,
      "loss": 0.3175,
      "step": 3246
    },
    {
      "epoch": 0.8658666666666667,
      "grad_norm": 0.05678007751703262,
      "learning_rate": 2.6862483311081445e-05,
      "loss": 0.353,
      "step": 3247
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.044679757207632065,
      "learning_rate": 2.6809078771695594e-05,
      "loss": 0.2805,
      "step": 3248
    },
    {
      "epoch": 0.8664,
      "grad_norm": 0.0597795732319355,
      "learning_rate": 2.6755674232309747e-05,
      "loss": 0.342,
      "step": 3249
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.0749550312757492,
      "learning_rate": 2.6702269692923897e-05,
      "loss": 0.3196,
      "step": 3250
    },
    {
      "epoch": 0.8669333333333333,
      "grad_norm": 0.051072340458631516,
      "learning_rate": 2.664886515353805e-05,
      "loss": 0.3654,
      "step": 3251
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.048204582184553146,
      "learning_rate": 2.6595460614152206e-05,
      "loss": 0.2643,
      "step": 3252
    },
    {
      "epoch": 0.8674666666666667,
      "grad_norm": 0.05895762890577316,
      "learning_rate": 2.654205607476636e-05,
      "loss": 0.3726,
      "step": 3253
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.04811788350343704,
      "learning_rate": 2.648865153538051e-05,
      "loss": 0.3288,
      "step": 3254
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.04447155445814133,
      "learning_rate": 2.643524699599466e-05,
      "loss": 0.2696,
      "step": 3255
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.05596796050667763,
      "learning_rate": 2.6381842456608814e-05,
      "loss": 0.3209,
      "step": 3256
    },
    {
      "epoch": 0.8685333333333334,
      "grad_norm": 0.05837446078658104,
      "learning_rate": 2.6328437917222964e-05,
      "loss": 0.3514,
      "step": 3257
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.05172913148999214,
      "learning_rate": 2.6275033377837117e-05,
      "loss": 0.2559,
      "step": 3258
    },
    {
      "epoch": 0.8690666666666667,
      "grad_norm": 0.06325934082269669,
      "learning_rate": 2.6221628838451267e-05,
      "loss": 0.3076,
      "step": 3259
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.04306032881140709,
      "learning_rate": 2.616822429906542e-05,
      "loss": 0.2588,
      "step": 3260
    },
    {
      "epoch": 0.8696,
      "grad_norm": 0.06486131995916367,
      "learning_rate": 2.6114819759679576e-05,
      "loss": 0.3681,
      "step": 3261
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.05118991434574127,
      "learning_rate": 2.606141522029373e-05,
      "loss": 0.3473,
      "step": 3262
    },
    {
      "epoch": 0.8701333333333333,
      "grad_norm": 0.0638781487941742,
      "learning_rate": 2.600801068090788e-05,
      "loss": 0.3087,
      "step": 3263
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.062268201261758804,
      "learning_rate": 2.595460614152203e-05,
      "loss": 0.3702,
      "step": 3264
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 0.05904198810458183,
      "learning_rate": 2.590120160213618e-05,
      "loss": 0.3839,
      "step": 3265
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.07770176231861115,
      "learning_rate": 2.5847797062750334e-05,
      "loss": 0.3347,
      "step": 3266
    },
    {
      "epoch": 0.8712,
      "grad_norm": 0.055353228002786636,
      "learning_rate": 2.5794392523364487e-05,
      "loss": 0.3315,
      "step": 3267
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.05720091983675957,
      "learning_rate": 2.5740987983978636e-05,
      "loss": 0.3339,
      "step": 3268
    },
    {
      "epoch": 0.8717333333333334,
      "grad_norm": 0.06174354627728462,
      "learning_rate": 2.5687583444592793e-05,
      "loss": 0.3343,
      "step": 3269
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.04877746105194092,
      "learning_rate": 2.5634178905206946e-05,
      "loss": 0.2636,
      "step": 3270
    },
    {
      "epoch": 0.8722666666666666,
      "grad_norm": 0.05462177097797394,
      "learning_rate": 2.55807743658211e-05,
      "loss": 0.3379,
      "step": 3271
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.051583293825387955,
      "learning_rate": 2.5527369826435248e-05,
      "loss": 0.3056,
      "step": 3272
    },
    {
      "epoch": 0.8728,
      "grad_norm": 0.05711523815989494,
      "learning_rate": 2.54739652870494e-05,
      "loss": 0.3039,
      "step": 3273
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.05535215884447098,
      "learning_rate": 2.542056074766355e-05,
      "loss": 0.3265,
      "step": 3274
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.06977662444114685,
      "learning_rate": 2.5367156208277704e-05,
      "loss": 0.3779,
      "step": 3275
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.054973118007183075,
      "learning_rate": 2.5313751668891856e-05,
      "loss": 0.3287,
      "step": 3276
    },
    {
      "epoch": 0.8738666666666667,
      "grad_norm": 0.044552288949489594,
      "learning_rate": 2.5260347129506006e-05,
      "loss": 0.2621,
      "step": 3277
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.050831448286771774,
      "learning_rate": 2.5206942590120162e-05,
      "loss": 0.2813,
      "step": 3278
    },
    {
      "epoch": 0.8744,
      "grad_norm": 0.06529450416564941,
      "learning_rate": 2.5153538050734315e-05,
      "loss": 0.3126,
      "step": 3279
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.06776419281959534,
      "learning_rate": 2.5100133511348468e-05,
      "loss": 0.3572,
      "step": 3280
    },
    {
      "epoch": 0.8749333333333333,
      "grad_norm": 0.06761466711759567,
      "learning_rate": 2.5046728971962618e-05,
      "loss": 0.3419,
      "step": 3281
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.045894116163253784,
      "learning_rate": 2.499332443257677e-05,
      "loss": 0.327,
      "step": 3282
    },
    {
      "epoch": 0.8754666666666666,
      "grad_norm": 0.05407235398888588,
      "learning_rate": 2.493991989319092e-05,
      "loss": 0.2953,
      "step": 3283
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.06123460829257965,
      "learning_rate": 2.4886515353805073e-05,
      "loss": 0.3897,
      "step": 3284
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.06429119408130646,
      "learning_rate": 2.4833110814419226e-05,
      "loss": 0.2736,
      "step": 3285
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.052708182483911514,
      "learning_rate": 2.477970627503338e-05,
      "loss": 0.3558,
      "step": 3286
    },
    {
      "epoch": 0.8765333333333334,
      "grad_norm": 0.08334037661552429,
      "learning_rate": 2.4726301735647532e-05,
      "loss": 0.3841,
      "step": 3287
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.04527423903346062,
      "learning_rate": 2.467289719626168e-05,
      "loss": 0.2874,
      "step": 3288
    },
    {
      "epoch": 0.8770666666666667,
      "grad_norm": 0.06298094242811203,
      "learning_rate": 2.4619492656875838e-05,
      "loss": 0.3865,
      "step": 3289
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.05597043037414551,
      "learning_rate": 2.4566088117489988e-05,
      "loss": 0.3133,
      "step": 3290
    },
    {
      "epoch": 0.8776,
      "grad_norm": 0.05946977809071541,
      "learning_rate": 2.451268357810414e-05,
      "loss": 0.3599,
      "step": 3291
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.06185538321733475,
      "learning_rate": 2.445927903871829e-05,
      "loss": 0.3345,
      "step": 3292
    },
    {
      "epoch": 0.8781333333333333,
      "grad_norm": 0.05618124082684517,
      "learning_rate": 2.4405874499332446e-05,
      "loss": 0.2967,
      "step": 3293
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.04916810989379883,
      "learning_rate": 2.4352469959946596e-05,
      "loss": 0.2745,
      "step": 3294
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 0.06860542297363281,
      "learning_rate": 2.429906542056075e-05,
      "loss": 0.3455,
      "step": 3295
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.04513123258948326,
      "learning_rate": 2.4245660881174902e-05,
      "loss": 0.2836,
      "step": 3296
    },
    {
      "epoch": 0.8792,
      "grad_norm": 0.048415470868349075,
      "learning_rate": 2.419225634178905e-05,
      "loss": 0.303,
      "step": 3297
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.044319432228803635,
      "learning_rate": 2.4138851802403208e-05,
      "loss": 0.2947,
      "step": 3298
    },
    {
      "epoch": 0.8797333333333334,
      "grad_norm": 0.056133829057216644,
      "learning_rate": 2.4085447263017357e-05,
      "loss": 0.3943,
      "step": 3299
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.04651753231883049,
      "learning_rate": 2.403204272363151e-05,
      "loss": 0.2751,
      "step": 3300
    },
    {
      "epoch": 0.8802666666666666,
      "grad_norm": 0.06632202118635178,
      "learning_rate": 2.397863818424566e-05,
      "loss": 0.3379,
      "step": 3301
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.05277792736887932,
      "learning_rate": 2.3925233644859816e-05,
      "loss": 0.2837,
      "step": 3302
    },
    {
      "epoch": 0.8808,
      "grad_norm": 0.05549732595682144,
      "learning_rate": 2.3871829105473966e-05,
      "loss": 0.3639,
      "step": 3303
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.047394074499607086,
      "learning_rate": 2.381842456608812e-05,
      "loss": 0.2768,
      "step": 3304
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 0.06756072491407394,
      "learning_rate": 2.376502002670227e-05,
      "loss": 0.3898,
      "step": 3305
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.06276679039001465,
      "learning_rate": 2.3711615487316424e-05,
      "loss": 0.3795,
      "step": 3306
    },
    {
      "epoch": 0.8818666666666667,
      "grad_norm": 0.046147171407938004,
      "learning_rate": 2.3658210947930577e-05,
      "loss": 0.2861,
      "step": 3307
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.06373259425163269,
      "learning_rate": 2.3604806408544727e-05,
      "loss": 0.3634,
      "step": 3308
    },
    {
      "epoch": 0.8824,
      "grad_norm": 0.05221245437860489,
      "learning_rate": 2.355140186915888e-05,
      "loss": 0.3264,
      "step": 3309
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.05174974724650383,
      "learning_rate": 2.349799732977303e-05,
      "loss": 0.3307,
      "step": 3310
    },
    {
      "epoch": 0.8829333333333333,
      "grad_norm": 0.039890702813863754,
      "learning_rate": 2.3444592790387186e-05,
      "loss": 0.2291,
      "step": 3311
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.04585011675953865,
      "learning_rate": 2.3391188251001335e-05,
      "loss": 0.2851,
      "step": 3312
    },
    {
      "epoch": 0.8834666666666666,
      "grad_norm": 0.05221477523446083,
      "learning_rate": 2.333778371161549e-05,
      "loss": 0.2784,
      "step": 3313
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.05600669980049133,
      "learning_rate": 2.3284379172229638e-05,
      "loss": 0.3256,
      "step": 3314
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.06703169643878937,
      "learning_rate": 2.3230974632843794e-05,
      "loss": 0.3524,
      "step": 3315
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.059233929961919785,
      "learning_rate": 2.3177570093457944e-05,
      "loss": 0.384,
      "step": 3316
    },
    {
      "epoch": 0.8845333333333333,
      "grad_norm": 0.05676303431391716,
      "learning_rate": 2.3124165554072097e-05,
      "loss": 0.2975,
      "step": 3317
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.04324337840080261,
      "learning_rate": 2.307076101468625e-05,
      "loss": 0.269,
      "step": 3318
    },
    {
      "epoch": 0.8850666666666667,
      "grad_norm": 0.05603695660829544,
      "learning_rate": 2.3017356475300403e-05,
      "loss": 0.2968,
      "step": 3319
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.08167821913957596,
      "learning_rate": 2.2963951935914556e-05,
      "loss": 0.343,
      "step": 3320
    },
    {
      "epoch": 0.8856,
      "grad_norm": 0.05261705815792084,
      "learning_rate": 2.2910547396528705e-05,
      "loss": 0.3268,
      "step": 3321
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.05494699627161026,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 0.3734,
      "step": 3322
    },
    {
      "epoch": 0.8861333333333333,
      "grad_norm": 0.06071152910590172,
      "learning_rate": 2.2803738317757008e-05,
      "loss": 0.3601,
      "step": 3323
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.0555315725505352,
      "learning_rate": 2.2750333778371164e-05,
      "loss": 0.3254,
      "step": 3324
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.05267986282706261,
      "learning_rate": 2.2696929238985313e-05,
      "loss": 0.3585,
      "step": 3325
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.04696125164628029,
      "learning_rate": 2.2643524699599466e-05,
      "loss": 0.2993,
      "step": 3326
    },
    {
      "epoch": 0.8872,
      "grad_norm": 0.06595010310411453,
      "learning_rate": 2.259012016021362e-05,
      "loss": 0.2953,
      "step": 3327
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.055349793285131454,
      "learning_rate": 2.2536715620827772e-05,
      "loss": 0.3123,
      "step": 3328
    },
    {
      "epoch": 0.8877333333333334,
      "grad_norm": 0.05607815831899643,
      "learning_rate": 2.2483311081441925e-05,
      "loss": 0.3575,
      "step": 3329
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.05296692997217178,
      "learning_rate": 2.2429906542056075e-05,
      "loss": 0.306,
      "step": 3330
    },
    {
      "epoch": 0.8882666666666666,
      "grad_norm": 0.04601508751511574,
      "learning_rate": 2.2376502002670228e-05,
      "loss": 0.2812,
      "step": 3331
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.05380770564079285,
      "learning_rate": 2.232309746328438e-05,
      "loss": 0.3188,
      "step": 3332
    },
    {
      "epoch": 0.8888,
      "grad_norm": 0.062411703169345856,
      "learning_rate": 2.2269692923898534e-05,
      "loss": 0.3546,
      "step": 3333
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.05008833482861519,
      "learning_rate": 2.2216288384512683e-05,
      "loss": 0.2684,
      "step": 3334
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 0.052897825837135315,
      "learning_rate": 2.2162883845126836e-05,
      "loss": 0.296,
      "step": 3335
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.06744291633367538,
      "learning_rate": 2.210947930574099e-05,
      "loss": 0.3688,
      "step": 3336
    },
    {
      "epoch": 0.8898666666666667,
      "grad_norm": 0.05572498217225075,
      "learning_rate": 2.2056074766355142e-05,
      "loss": 0.3401,
      "step": 3337
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.05002233386039734,
      "learning_rate": 2.2002670226969295e-05,
      "loss": 0.2836,
      "step": 3338
    },
    {
      "epoch": 0.8904,
      "grad_norm": 0.0571599043905735,
      "learning_rate": 2.1949265687583445e-05,
      "loss": 0.358,
      "step": 3339
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.06343211978673935,
      "learning_rate": 2.1895861148197598e-05,
      "loss": 0.3708,
      "step": 3340
    },
    {
      "epoch": 0.8909333333333334,
      "grad_norm": 0.04495147988200188,
      "learning_rate": 2.184245660881175e-05,
      "loss": 0.2515,
      "step": 3341
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.04351746663451195,
      "learning_rate": 2.1789052069425903e-05,
      "loss": 0.2782,
      "step": 3342
    },
    {
      "epoch": 0.8914666666666666,
      "grad_norm": 0.06941026449203491,
      "learning_rate": 2.1735647530040053e-05,
      "loss": 0.3844,
      "step": 3343
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.054411500692367554,
      "learning_rate": 2.1682242990654206e-05,
      "loss": 0.3521,
      "step": 3344
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.06368692219257355,
      "learning_rate": 2.162883845126836e-05,
      "loss": 0.3528,
      "step": 3345
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.05577621981501579,
      "learning_rate": 2.1575433911882512e-05,
      "loss": 0.3566,
      "step": 3346
    },
    {
      "epoch": 0.8925333333333333,
      "grad_norm": 0.04746841639280319,
      "learning_rate": 2.1522029372496665e-05,
      "loss": 0.2863,
      "step": 3347
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.04819340258836746,
      "learning_rate": 2.1468624833110814e-05,
      "loss": 0.2963,
      "step": 3348
    },
    {
      "epoch": 0.8930666666666667,
      "grad_norm": 0.04464313015341759,
      "learning_rate": 2.1415220293724967e-05,
      "loss": 0.2766,
      "step": 3349
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.0643429160118103,
      "learning_rate": 2.136181575433912e-05,
      "loss": 0.4027,
      "step": 3350
    },
    {
      "epoch": 0.8936,
      "grad_norm": 0.059144388884305954,
      "learning_rate": 2.1308411214953273e-05,
      "loss": 0.3391,
      "step": 3351
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.05881209298968315,
      "learning_rate": 2.1255006675567423e-05,
      "loss": 0.3607,
      "step": 3352
    },
    {
      "epoch": 0.8941333333333333,
      "grad_norm": 0.05547570437192917,
      "learning_rate": 2.1201602136181576e-05,
      "loss": 0.3444,
      "step": 3353
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.05192386358976364,
      "learning_rate": 2.114819759679573e-05,
      "loss": 0.2991,
      "step": 3354
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 0.047899819910526276,
      "learning_rate": 2.109479305740988e-05,
      "loss": 0.2781,
      "step": 3355
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.04596161097288132,
      "learning_rate": 2.1041388518024034e-05,
      "loss": 0.2766,
      "step": 3356
    },
    {
      "epoch": 0.8952,
      "grad_norm": 0.055901095271110535,
      "learning_rate": 2.0987983978638184e-05,
      "loss": 0.3299,
      "step": 3357
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.0534140020608902,
      "learning_rate": 2.093457943925234e-05,
      "loss": 0.2802,
      "step": 3358
    },
    {
      "epoch": 0.8957333333333334,
      "grad_norm": 0.04716017469763756,
      "learning_rate": 2.088117489986649e-05,
      "loss": 0.2867,
      "step": 3359
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.04157819226384163,
      "learning_rate": 2.0827770360480643e-05,
      "loss": 0.2836,
      "step": 3360
    },
    {
      "epoch": 0.8962666666666667,
      "grad_norm": 0.05170978605747223,
      "learning_rate": 2.0774365821094792e-05,
      "loss": 0.2771,
      "step": 3361
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.049239788204431534,
      "learning_rate": 2.0720961281708945e-05,
      "loss": 0.3193,
      "step": 3362
    },
    {
      "epoch": 0.8968,
      "grad_norm": 0.06797261536121368,
      "learning_rate": 2.0667556742323098e-05,
      "loss": 0.3666,
      "step": 3363
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.07105430960655212,
      "learning_rate": 2.061415220293725e-05,
      "loss": 0.3627,
      "step": 3364
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 0.05174762383103371,
      "learning_rate": 2.05607476635514e-05,
      "loss": 0.3507,
      "step": 3365
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.05171073600649834,
      "learning_rate": 2.0507343124165554e-05,
      "loss": 0.3125,
      "step": 3366
    },
    {
      "epoch": 0.8978666666666667,
      "grad_norm": 0.07030663639307022,
      "learning_rate": 2.0453938584779707e-05,
      "loss": 0.3192,
      "step": 3367
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.051414571702480316,
      "learning_rate": 2.040053404539386e-05,
      "loss": 0.2631,
      "step": 3368
    },
    {
      "epoch": 0.8984,
      "grad_norm": 0.05253930762410164,
      "learning_rate": 2.0347129506008013e-05,
      "loss": 0.2736,
      "step": 3369
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.05471436679363251,
      "learning_rate": 2.0293724966622162e-05,
      "loss": 0.3177,
      "step": 3370
    },
    {
      "epoch": 0.8989333333333334,
      "grad_norm": 0.04543914273381233,
      "learning_rate": 2.024032042723632e-05,
      "loss": 0.2639,
      "step": 3371
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.05608060210943222,
      "learning_rate": 2.0186915887850468e-05,
      "loss": 0.3356,
      "step": 3372
    },
    {
      "epoch": 0.8994666666666666,
      "grad_norm": 0.04397137835621834,
      "learning_rate": 2.013351134846462e-05,
      "loss": 0.2674,
      "step": 3373
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.04858413711190224,
      "learning_rate": 2.008010680907877e-05,
      "loss": 0.3265,
      "step": 3374
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.05900249630212784,
      "learning_rate": 2.0026702269692923e-05,
      "loss": 0.3381,
      "step": 3375
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.05214649438858032,
      "learning_rate": 1.9973297730307076e-05,
      "loss": 0.2661,
      "step": 3376
    },
    {
      "epoch": 0.9005333333333333,
      "grad_norm": 0.06858813017606735,
      "learning_rate": 1.991989319092123e-05,
      "loss": 0.3557,
      "step": 3377
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.04881979897618294,
      "learning_rate": 1.9866488651535382e-05,
      "loss": 0.2902,
      "step": 3378
    },
    {
      "epoch": 0.9010666666666667,
      "grad_norm": 0.04923681169748306,
      "learning_rate": 1.9813084112149532e-05,
      "loss": 0.2674,
      "step": 3379
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.06062920391559601,
      "learning_rate": 1.9759679572763688e-05,
      "loss": 0.3521,
      "step": 3380
    },
    {
      "epoch": 0.9016,
      "grad_norm": 0.06321801990270615,
      "learning_rate": 1.9706275033377838e-05,
      "loss": 0.2926,
      "step": 3381
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.048744723200798035,
      "learning_rate": 1.965287049399199e-05,
      "loss": 0.3171,
      "step": 3382
    },
    {
      "epoch": 0.9021333333333333,
      "grad_norm": 0.05933045223355293,
      "learning_rate": 1.959946595460614e-05,
      "loss": 0.3274,
      "step": 3383
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.04245350882411003,
      "learning_rate": 1.9546061415220297e-05,
      "loss": 0.2445,
      "step": 3384
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 0.05402955040335655,
      "learning_rate": 1.9492656875834446e-05,
      "loss": 0.3182,
      "step": 3385
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.06303953379392624,
      "learning_rate": 1.94392523364486e-05,
      "loss": 0.3751,
      "step": 3386
    },
    {
      "epoch": 0.9032,
      "grad_norm": 0.07058203965425491,
      "learning_rate": 1.9385847797062752e-05,
      "loss": 0.3686,
      "step": 3387
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.06010781228542328,
      "learning_rate": 1.93324432576769e-05,
      "loss": 0.386,
      "step": 3388
    },
    {
      "epoch": 0.9037333333333334,
      "grad_norm": 0.05599416419863701,
      "learning_rate": 1.9279038718291058e-05,
      "loss": 0.3501,
      "step": 3389
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.054705243557691574,
      "learning_rate": 1.9225634178905207e-05,
      "loss": 0.3448,
      "step": 3390
    },
    {
      "epoch": 0.9042666666666667,
      "grad_norm": 0.06345929950475693,
      "learning_rate": 1.917222963951936e-05,
      "loss": 0.3494,
      "step": 3391
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.060524530708789825,
      "learning_rate": 1.911882510013351e-05,
      "loss": 0.3968,
      "step": 3392
    },
    {
      "epoch": 0.9048,
      "grad_norm": 0.05226682499051094,
      "learning_rate": 1.9065420560747666e-05,
      "loss": 0.3205,
      "step": 3393
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.06655708700418472,
      "learning_rate": 1.9012016021361816e-05,
      "loss": 0.3411,
      "step": 3394
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 0.051503125578165054,
      "learning_rate": 1.895861148197597e-05,
      "loss": 0.278,
      "step": 3395
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.05615726858377457,
      "learning_rate": 1.8905206942590122e-05,
      "loss": 0.3282,
      "step": 3396
    },
    {
      "epoch": 0.9058666666666667,
      "grad_norm": 0.05696937441825867,
      "learning_rate": 1.8851802403204275e-05,
      "loss": 0.3304,
      "step": 3397
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.0511481799185276,
      "learning_rate": 1.8798397863818428e-05,
      "loss": 0.3223,
      "step": 3398
    },
    {
      "epoch": 0.9064,
      "grad_norm": 0.04212336987257004,
      "learning_rate": 1.8744993324432577e-05,
      "loss": 0.2746,
      "step": 3399
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.05615374445915222,
      "learning_rate": 1.869158878504673e-05,
      "loss": 0.3068,
      "step": 3400
    },
    {
      "epoch": 0.9069333333333334,
      "grad_norm": 0.057489801198244095,
      "learning_rate": 1.863818424566088e-05,
      "loss": 0.2687,
      "step": 3401
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.050096120685338974,
      "learning_rate": 1.8584779706275036e-05,
      "loss": 0.2989,
      "step": 3402
    },
    {
      "epoch": 0.9074666666666666,
      "grad_norm": 0.05397387593984604,
      "learning_rate": 1.8531375166889186e-05,
      "loss": 0.321,
      "step": 3403
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.059194255620241165,
      "learning_rate": 1.847797062750334e-05,
      "loss": 0.3627,
      "step": 3404
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.055501651018857956,
      "learning_rate": 1.8424566088117488e-05,
      "loss": 0.3207,
      "step": 3405
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.0528562068939209,
      "learning_rate": 1.8371161548731644e-05,
      "loss": 0.2627,
      "step": 3406
    },
    {
      "epoch": 0.9085333333333333,
      "grad_norm": 0.06358302384614944,
      "learning_rate": 1.8317757009345794e-05,
      "loss": 0.3037,
      "step": 3407
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.05485237389802933,
      "learning_rate": 1.8264352469959947e-05,
      "loss": 0.3148,
      "step": 3408
    },
    {
      "epoch": 0.9090666666666667,
      "grad_norm": 0.04527449235320091,
      "learning_rate": 1.82109479305741e-05,
      "loss": 0.2983,
      "step": 3409
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.04956677183508873,
      "learning_rate": 1.8157543391188253e-05,
      "loss": 0.2965,
      "step": 3410
    },
    {
      "epoch": 0.9096,
      "grad_norm": 0.05885177105665207,
      "learning_rate": 1.8104138851802406e-05,
      "loss": 0.3217,
      "step": 3411
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.05696861818432808,
      "learning_rate": 1.8050734312416555e-05,
      "loss": 0.3445,
      "step": 3412
    },
    {
      "epoch": 0.9101333333333333,
      "grad_norm": 0.0450475849211216,
      "learning_rate": 1.7997329773030708e-05,
      "loss": 0.2599,
      "step": 3413
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.05938475579023361,
      "learning_rate": 1.7943925233644858e-05,
      "loss": 0.3527,
      "step": 3414
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 0.0560716949403286,
      "learning_rate": 1.7890520694259014e-05,
      "loss": 0.3662,
      "step": 3415
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.03908617049455643,
      "learning_rate": 1.7837116154873164e-05,
      "loss": 0.2438,
      "step": 3416
    },
    {
      "epoch": 0.9112,
      "grad_norm": 0.06640919297933578,
      "learning_rate": 1.7783711615487317e-05,
      "loss": 0.3717,
      "step": 3417
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.05458756908774376,
      "learning_rate": 1.773030707610147e-05,
      "loss": 0.3183,
      "step": 3418
    },
    {
      "epoch": 0.9117333333333333,
      "grad_norm": 0.049545906484127045,
      "learning_rate": 1.7676902536715623e-05,
      "loss": 0.2998,
      "step": 3419
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.05700679123401642,
      "learning_rate": 1.7623497997329775e-05,
      "loss": 0.3141,
      "step": 3420
    },
    {
      "epoch": 0.9122666666666667,
      "grad_norm": 0.06039409711956978,
      "learning_rate": 1.7570093457943925e-05,
      "loss": 0.3283,
      "step": 3421
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.06803203374147415,
      "learning_rate": 1.7516688918558078e-05,
      "loss": 0.3757,
      "step": 3422
    },
    {
      "epoch": 0.9128,
      "grad_norm": 0.0409495048224926,
      "learning_rate": 1.746328437917223e-05,
      "loss": 0.2866,
      "step": 3423
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.04496939107775688,
      "learning_rate": 1.7409879839786384e-05,
      "loss": 0.2485,
      "step": 3424
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.06633711606264114,
      "learning_rate": 1.7356475300400533e-05,
      "loss": 0.3083,
      "step": 3425
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.05329316854476929,
      "learning_rate": 1.7303070761014686e-05,
      "loss": 0.2982,
      "step": 3426
    },
    {
      "epoch": 0.9138666666666667,
      "grad_norm": 0.0588044673204422,
      "learning_rate": 1.724966622162884e-05,
      "loss": 0.3546,
      "step": 3427
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.06133667379617691,
      "learning_rate": 1.7196261682242992e-05,
      "loss": 0.3225,
      "step": 3428
    },
    {
      "epoch": 0.9144,
      "grad_norm": 0.05424904823303223,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 0.3575,
      "step": 3429
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.05974256619811058,
      "learning_rate": 1.7089452603471295e-05,
      "loss": 0.3329,
      "step": 3430
    },
    {
      "epoch": 0.9149333333333334,
      "grad_norm": 0.057119887322187424,
      "learning_rate": 1.7036048064085448e-05,
      "loss": 0.3108,
      "step": 3431
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.052386779338121414,
      "learning_rate": 1.69826435246996e-05,
      "loss": 0.3003,
      "step": 3432
    },
    {
      "epoch": 0.9154666666666667,
      "grad_norm": 0.05821243301033974,
      "learning_rate": 1.6929238985313754e-05,
      "loss": 0.3757,
      "step": 3433
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.046065863221883774,
      "learning_rate": 1.6875834445927903e-05,
      "loss": 0.2777,
      "step": 3434
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.04464128240942955,
      "learning_rate": 1.6822429906542056e-05,
      "loss": 0.2782,
      "step": 3435
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.0449724942445755,
      "learning_rate": 1.676902536715621e-05,
      "loss": 0.2971,
      "step": 3436
    },
    {
      "epoch": 0.9165333333333333,
      "grad_norm": 0.043550241738557816,
      "learning_rate": 1.6715620827770362e-05,
      "loss": 0.268,
      "step": 3437
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.04811706393957138,
      "learning_rate": 1.6662216288384515e-05,
      "loss": 0.303,
      "step": 3438
    },
    {
      "epoch": 0.9170666666666667,
      "grad_norm": 0.05151713266968727,
      "learning_rate": 1.6608811748998665e-05,
      "loss": 0.3183,
      "step": 3439
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.047263193875551224,
      "learning_rate": 1.6555407209612817e-05,
      "loss": 0.2918,
      "step": 3440
    },
    {
      "epoch": 0.9176,
      "grad_norm": 0.04090301692485809,
      "learning_rate": 1.650200267022697e-05,
      "loss": 0.2485,
      "step": 3441
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.07114968448877335,
      "learning_rate": 1.6448598130841123e-05,
      "loss": 0.3947,
      "step": 3442
    },
    {
      "epoch": 0.9181333333333334,
      "grad_norm": 0.06059786677360535,
      "learning_rate": 1.6395193591455273e-05,
      "loss": 0.3416,
      "step": 3443
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.07254020869731903,
      "learning_rate": 1.6341789052069426e-05,
      "loss": 0.3099,
      "step": 3444
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 0.05805886536836624,
      "learning_rate": 1.628838451268358e-05,
      "loss": 0.3197,
      "step": 3445
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.08553028851747513,
      "learning_rate": 1.6234979973297732e-05,
      "loss": 0.3763,
      "step": 3446
    },
    {
      "epoch": 0.9192,
      "grad_norm": 0.06850183755159378,
      "learning_rate": 1.6181575433911885e-05,
      "loss": 0.3454,
      "step": 3447
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.06310278177261353,
      "learning_rate": 1.6128170894526034e-05,
      "loss": 0.3594,
      "step": 3448
    },
    {
      "epoch": 0.9197333333333333,
      "grad_norm": 0.053709693253040314,
      "learning_rate": 1.607476635514019e-05,
      "loss": 0.2657,
      "step": 3449
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.047906145453453064,
      "learning_rate": 1.602136181575434e-05,
      "loss": 0.2685,
      "step": 3450
    },
    {
      "epoch": 0.9202666666666667,
      "grad_norm": 0.06461863219738007,
      "learning_rate": 1.5967957276368493e-05,
      "loss": 0.3795,
      "step": 3451
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.04930487647652626,
      "learning_rate": 1.5914552736982643e-05,
      "loss": 0.3409,
      "step": 3452
    },
    {
      "epoch": 0.9208,
      "grad_norm": 0.04493779316544533,
      "learning_rate": 1.5861148197596796e-05,
      "loss": 0.2772,
      "step": 3453
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.06324335932731628,
      "learning_rate": 1.580774365821095e-05,
      "loss": 0.3517,
      "step": 3454
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 0.07090342044830322,
      "learning_rate": 1.57543391188251e-05,
      "loss": 0.3865,
      "step": 3455
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.05981725454330444,
      "learning_rate": 1.570093457943925e-05,
      "loss": 0.3624,
      "step": 3456
    },
    {
      "epoch": 0.9218666666666666,
      "grad_norm": 0.04878793656826019,
      "learning_rate": 1.5647530040053404e-05,
      "loss": 0.3252,
      "step": 3457
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.05410812795162201,
      "learning_rate": 1.5594125500667557e-05,
      "loss": 0.3434,
      "step": 3458
    },
    {
      "epoch": 0.9224,
      "grad_norm": 0.053174685686826706,
      "learning_rate": 1.554072096128171e-05,
      "loss": 0.3235,
      "step": 3459
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.062045250087976456,
      "learning_rate": 1.5487316421895863e-05,
      "loss": 0.368,
      "step": 3460
    },
    {
      "epoch": 0.9229333333333334,
      "grad_norm": 0.04060349613428116,
      "learning_rate": 1.5433911882510012e-05,
      "loss": 0.2572,
      "step": 3461
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.04767046123743057,
      "learning_rate": 1.538050734312417e-05,
      "loss": 0.2699,
      "step": 3462
    },
    {
      "epoch": 0.9234666666666667,
      "grad_norm": 0.06471201777458191,
      "learning_rate": 1.5327102803738318e-05,
      "loss": 0.3948,
      "step": 3463
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.06499422341585159,
      "learning_rate": 1.527369826435247e-05,
      "loss": 0.3547,
      "step": 3464
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.05497440695762634,
      "learning_rate": 1.5220293724966622e-05,
      "loss": 0.3497,
      "step": 3465
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.05745139345526695,
      "learning_rate": 1.5166889185580774e-05,
      "loss": 0.3343,
      "step": 3466
    },
    {
      "epoch": 0.9245333333333333,
      "grad_norm": 0.05555236339569092,
      "learning_rate": 1.5113484646194928e-05,
      "loss": 0.3187,
      "step": 3467
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.046413905918598175,
      "learning_rate": 1.506008010680908e-05,
      "loss": 0.3008,
      "step": 3468
    },
    {
      "epoch": 0.9250666666666667,
      "grad_norm": 0.043836791068315506,
      "learning_rate": 1.500667556742323e-05,
      "loss": 0.2899,
      "step": 3469
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.06191932037472725,
      "learning_rate": 1.4953271028037382e-05,
      "loss": 0.3267,
      "step": 3470
    },
    {
      "epoch": 0.9256,
      "grad_norm": 0.052778586745262146,
      "learning_rate": 1.4899866488651537e-05,
      "loss": 0.2924,
      "step": 3471
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.05591890215873718,
      "learning_rate": 1.4846461949265688e-05,
      "loss": 0.3471,
      "step": 3472
    },
    {
      "epoch": 0.9261333333333334,
      "grad_norm": 0.05485501512885094,
      "learning_rate": 1.4793057409879841e-05,
      "loss": 0.3277,
      "step": 3473
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.06760907173156738,
      "learning_rate": 1.4739652870493992e-05,
      "loss": 0.3916,
      "step": 3474
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.05895300954580307,
      "learning_rate": 1.4686248331108147e-05,
      "loss": 0.3257,
      "step": 3475
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.05255134403705597,
      "learning_rate": 1.4632843791722298e-05,
      "loss": 0.3504,
      "step": 3476
    },
    {
      "epoch": 0.9272,
      "grad_norm": 0.06099660322070122,
      "learning_rate": 1.457943925233645e-05,
      "loss": 0.3331,
      "step": 3477
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.05472617968916893,
      "learning_rate": 1.45260347129506e-05,
      "loss": 0.3701,
      "step": 3478
    },
    {
      "epoch": 0.9277333333333333,
      "grad_norm": 0.050360098481178284,
      "learning_rate": 1.4472630173564752e-05,
      "loss": 0.297,
      "step": 3479
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.050306543707847595,
      "learning_rate": 1.4419225634178906e-05,
      "loss": 0.2602,
      "step": 3480
    },
    {
      "epoch": 0.9282666666666667,
      "grad_norm": 0.043200068175792694,
      "learning_rate": 1.4365821094793058e-05,
      "loss": 0.2629,
      "step": 3481
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.05669685825705528,
      "learning_rate": 1.431241655540721e-05,
      "loss": 0.3389,
      "step": 3482
    },
    {
      "epoch": 0.9288,
      "grad_norm": 0.04931484907865524,
      "learning_rate": 1.4259012016021362e-05,
      "loss": 0.2717,
      "step": 3483
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.05020325630903244,
      "learning_rate": 1.4205607476635517e-05,
      "loss": 0.303,
      "step": 3484
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 0.060948196798563004,
      "learning_rate": 1.4152202937249668e-05,
      "loss": 0.3761,
      "step": 3485
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.045186031609773636,
      "learning_rate": 1.4098798397863819e-05,
      "loss": 0.2743,
      "step": 3486
    },
    {
      "epoch": 0.9298666666666666,
      "grad_norm": 0.05521911755204201,
      "learning_rate": 1.404539385847797e-05,
      "loss": 0.3428,
      "step": 3487
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.05849802494049072,
      "learning_rate": 1.3991989319092125e-05,
      "loss": 0.2638,
      "step": 3488
    },
    {
      "epoch": 0.9304,
      "grad_norm": 0.05041295289993286,
      "learning_rate": 1.3938584779706276e-05,
      "loss": 0.3454,
      "step": 3489
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.05619024112820625,
      "learning_rate": 1.3885180240320427e-05,
      "loss": 0.3319,
      "step": 3490
    },
    {
      "epoch": 0.9309333333333333,
      "grad_norm": 0.051440946757793427,
      "learning_rate": 1.3831775700934579e-05,
      "loss": 0.3292,
      "step": 3491
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.05832672491669655,
      "learning_rate": 1.3778371161548732e-05,
      "loss": 0.3127,
      "step": 3492
    },
    {
      "epoch": 0.9314666666666667,
      "grad_norm": 0.05975625291466713,
      "learning_rate": 1.3724966622162885e-05,
      "loss": 0.3198,
      "step": 3493
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.04518038406968117,
      "learning_rate": 1.3671562082777038e-05,
      "loss": 0.2806,
      "step": 3494
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.060466814786195755,
      "learning_rate": 1.3618157543391189e-05,
      "loss": 0.3604,
      "step": 3495
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.06736646592617035,
      "learning_rate": 1.356475300400534e-05,
      "loss": 0.3777,
      "step": 3496
    },
    {
      "epoch": 0.9325333333333333,
      "grad_norm": 0.04845323786139488,
      "learning_rate": 1.3511348464619495e-05,
      "loss": 0.3098,
      "step": 3497
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.05306268855929375,
      "learning_rate": 1.3457943925233646e-05,
      "loss": 0.3408,
      "step": 3498
    },
    {
      "epoch": 0.9330666666666667,
      "grad_norm": 0.05567976087331772,
      "learning_rate": 1.3404539385847797e-05,
      "loss": 0.3443,
      "step": 3499
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04491748660802841,
      "learning_rate": 1.3351134846461948e-05,
      "loss": 0.2473,
      "step": 3500
    },
    {
      "epoch": 0.9336,
      "grad_norm": 0.04808812960982323,
      "learning_rate": 1.3297730307076103e-05,
      "loss": 0.3008,
      "step": 3501
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.05569624528288841,
      "learning_rate": 1.3244325767690254e-05,
      "loss": 0.2919,
      "step": 3502
    },
    {
      "epoch": 0.9341333333333334,
      "grad_norm": 0.04860261082649231,
      "learning_rate": 1.3190921228304407e-05,
      "loss": 0.2825,
      "step": 3503
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.04645869508385658,
      "learning_rate": 1.3137516688918559e-05,
      "loss": 0.2956,
      "step": 3504
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 0.04833414405584335,
      "learning_rate": 1.308411214953271e-05,
      "loss": 0.3397,
      "step": 3505
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.06319709867238998,
      "learning_rate": 1.3030707610146864e-05,
      "loss": 0.3484,
      "step": 3506
    },
    {
      "epoch": 0.9352,
      "grad_norm": 0.04515380412340164,
      "learning_rate": 1.2977303070761016e-05,
      "loss": 0.2417,
      "step": 3507
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.05712980777025223,
      "learning_rate": 1.2923898531375167e-05,
      "loss": 0.3633,
      "step": 3508
    },
    {
      "epoch": 0.9357333333333333,
      "grad_norm": 0.04749539867043495,
      "learning_rate": 1.2870493991989318e-05,
      "loss": 0.3469,
      "step": 3509
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.0696125254034996,
      "learning_rate": 1.2817089452603473e-05,
      "loss": 0.3932,
      "step": 3510
    },
    {
      "epoch": 0.9362666666666667,
      "grad_norm": 0.050091590732336044,
      "learning_rate": 1.2763684913217624e-05,
      "loss": 0.2788,
      "step": 3511
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.07328066974878311,
      "learning_rate": 1.2710280373831775e-05,
      "loss": 0.2832,
      "step": 3512
    },
    {
      "epoch": 0.9368,
      "grad_norm": 0.05391313508152962,
      "learning_rate": 1.2656875834445928e-05,
      "loss": 0.3232,
      "step": 3513
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.07241621613502502,
      "learning_rate": 1.2603471295060081e-05,
      "loss": 0.347,
      "step": 3514
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 0.05963519960641861,
      "learning_rate": 1.2550066755674234e-05,
      "loss": 0.3629,
      "step": 3515
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.07121017575263977,
      "learning_rate": 1.2496662216288385e-05,
      "loss": 0.3541,
      "step": 3516
    },
    {
      "epoch": 0.9378666666666666,
      "grad_norm": 0.05124971643090248,
      "learning_rate": 1.2443257676902537e-05,
      "loss": 0.3217,
      "step": 3517
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.06707954406738281,
      "learning_rate": 1.238985313751669e-05,
      "loss": 0.3825,
      "step": 3518
    },
    {
      "epoch": 0.9384,
      "grad_norm": 0.054761212319135666,
      "learning_rate": 1.233644859813084e-05,
      "loss": 0.3284,
      "step": 3519
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.07140294462442398,
      "learning_rate": 1.2283044058744994e-05,
      "loss": 0.3921,
      "step": 3520
    },
    {
      "epoch": 0.9389333333333333,
      "grad_norm": 0.0592675618827343,
      "learning_rate": 1.2229639519359145e-05,
      "loss": 0.3235,
      "step": 3521
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.053138796240091324,
      "learning_rate": 1.2176234979973298e-05,
      "loss": 0.2832,
      "step": 3522
    },
    {
      "epoch": 0.9394666666666667,
      "grad_norm": 0.055884141474962234,
      "learning_rate": 1.2122830440587451e-05,
      "loss": 0.313,
      "step": 3523
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.04676283895969391,
      "learning_rate": 1.2069425901201604e-05,
      "loss": 0.2158,
      "step": 3524
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.049880675971508026,
      "learning_rate": 1.2016021361815755e-05,
      "loss": 0.2713,
      "step": 3525
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.067009836435318,
      "learning_rate": 1.1962616822429908e-05,
      "loss": 0.3922,
      "step": 3526
    },
    {
      "epoch": 0.9405333333333333,
      "grad_norm": 0.07127971202135086,
      "learning_rate": 1.190921228304406e-05,
      "loss": 0.3993,
      "step": 3527
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.049122683703899384,
      "learning_rate": 1.1855807743658212e-05,
      "loss": 0.2928,
      "step": 3528
    },
    {
      "epoch": 0.9410666666666667,
      "grad_norm": 0.05176506191492081,
      "learning_rate": 1.1802403204272363e-05,
      "loss": 0.33,
      "step": 3529
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.06840739399194717,
      "learning_rate": 1.1748998664886515e-05,
      "loss": 0.3739,
      "step": 3530
    },
    {
      "epoch": 0.9416,
      "grad_norm": 0.04887668043375015,
      "learning_rate": 1.1695594125500668e-05,
      "loss": 0.3228,
      "step": 3531
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.054008640348911285,
      "learning_rate": 1.1642189586114819e-05,
      "loss": 0.2958,
      "step": 3532
    },
    {
      "epoch": 0.9421333333333334,
      "grad_norm": 0.053168538957834244,
      "learning_rate": 1.1588785046728972e-05,
      "loss": 0.3282,
      "step": 3533
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.0634562149643898,
      "learning_rate": 1.1535380507343125e-05,
      "loss": 0.3618,
      "step": 3534
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 0.054537493735551834,
      "learning_rate": 1.1481975967957278e-05,
      "loss": 0.3342,
      "step": 3535
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.050605595111846924,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 0.3071,
      "step": 3536
    },
    {
      "epoch": 0.9432,
      "grad_norm": 0.0513925664126873,
      "learning_rate": 1.1375166889185582e-05,
      "loss": 0.3117,
      "step": 3537
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.05176229402422905,
      "learning_rate": 1.1321762349799733e-05,
      "loss": 0.2929,
      "step": 3538
    },
    {
      "epoch": 0.9437333333333333,
      "grad_norm": 0.06604184210300446,
      "learning_rate": 1.1268357810413886e-05,
      "loss": 0.3462,
      "step": 3539
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.053553689271211624,
      "learning_rate": 1.1214953271028037e-05,
      "loss": 0.2796,
      "step": 3540
    },
    {
      "epoch": 0.9442666666666667,
      "grad_norm": 0.05530151352286339,
      "learning_rate": 1.116154873164219e-05,
      "loss": 0.3498,
      "step": 3541
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.06193261966109276,
      "learning_rate": 1.1108144192256342e-05,
      "loss": 0.3349,
      "step": 3542
    },
    {
      "epoch": 0.9448,
      "grad_norm": 0.04300963133573532,
      "learning_rate": 1.1054739652870495e-05,
      "loss": 0.2445,
      "step": 3543
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.06404811143875122,
      "learning_rate": 1.1001335113484647e-05,
      "loss": 0.3251,
      "step": 3544
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 0.04932306706905365,
      "learning_rate": 1.0947930574098799e-05,
      "loss": 0.2693,
      "step": 3545
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.04862338304519653,
      "learning_rate": 1.0894526034712952e-05,
      "loss": 0.298,
      "step": 3546
    },
    {
      "epoch": 0.9458666666666666,
      "grad_norm": 0.05100785195827484,
      "learning_rate": 1.0841121495327103e-05,
      "loss": 0.3214,
      "step": 3547
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.041490789502859116,
      "learning_rate": 1.0787716955941256e-05,
      "loss": 0.2763,
      "step": 3548
    },
    {
      "epoch": 0.9464,
      "grad_norm": 0.047110773622989655,
      "learning_rate": 1.0734312416555407e-05,
      "loss": 0.2844,
      "step": 3549
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.040735650807619095,
      "learning_rate": 1.068090787716956e-05,
      "loss": 0.2648,
      "step": 3550
    },
    {
      "epoch": 0.9469333333333333,
      "grad_norm": 0.05252457037568092,
      "learning_rate": 1.0627503337783711e-05,
      "loss": 0.2864,
      "step": 3551
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.05892517417669296,
      "learning_rate": 1.0574098798397864e-05,
      "loss": 0.3247,
      "step": 3552
    },
    {
      "epoch": 0.9474666666666667,
      "grad_norm": 0.04832996800541878,
      "learning_rate": 1.0520694259012017e-05,
      "loss": 0.2777,
      "step": 3553
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.05786546692252159,
      "learning_rate": 1.046728971962617e-05,
      "loss": 0.3594,
      "step": 3554
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.05481768399477005,
      "learning_rate": 1.0413885180240321e-05,
      "loss": 0.3766,
      "step": 3555
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.045109763741493225,
      "learning_rate": 1.0360480640854473e-05,
      "loss": 0.2884,
      "step": 3556
    },
    {
      "epoch": 0.9485333333333333,
      "grad_norm": 0.05035906657576561,
      "learning_rate": 1.0307076101468626e-05,
      "loss": 0.3163,
      "step": 3557
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.06114507094025612,
      "learning_rate": 1.0253671562082777e-05,
      "loss": 0.3769,
      "step": 3558
    },
    {
      "epoch": 0.9490666666666666,
      "grad_norm": 0.04740035533905029,
      "learning_rate": 1.020026702269693e-05,
      "loss": 0.286,
      "step": 3559
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.04549281299114227,
      "learning_rate": 1.0146862483311081e-05,
      "loss": 0.3208,
      "step": 3560
    },
    {
      "epoch": 0.9496,
      "grad_norm": 0.05847187340259552,
      "learning_rate": 1.0093457943925234e-05,
      "loss": 0.3479,
      "step": 3561
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.05815308913588524,
      "learning_rate": 1.0040053404539385e-05,
      "loss": 0.3953,
      "step": 3562
    },
    {
      "epoch": 0.9501333333333334,
      "grad_norm": 0.052996449172496796,
      "learning_rate": 9.986648865153538e-06,
      "loss": 0.3156,
      "step": 3563
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.04238734394311905,
      "learning_rate": 9.933244325767691e-06,
      "loss": 0.2759,
      "step": 3564
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 0.05983903259038925,
      "learning_rate": 9.879839786381844e-06,
      "loss": 0.39,
      "step": 3565
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.06492067128419876,
      "learning_rate": 9.826435246995995e-06,
      "loss": 0.3667,
      "step": 3566
    },
    {
      "epoch": 0.9512,
      "grad_norm": 0.06579995900392532,
      "learning_rate": 9.773030707610148e-06,
      "loss": 0.4139,
      "step": 3567
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.054703325033187866,
      "learning_rate": 9.7196261682243e-06,
      "loss": 0.3282,
      "step": 3568
    },
    {
      "epoch": 0.9517333333333333,
      "grad_norm": 0.039893366396427155,
      "learning_rate": 9.66622162883845e-06,
      "loss": 0.2623,
      "step": 3569
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.04840763658285141,
      "learning_rate": 9.612817089452604e-06,
      "loss": 0.2883,
      "step": 3570
    },
    {
      "epoch": 0.9522666666666667,
      "grad_norm": 0.05212243273854256,
      "learning_rate": 9.559412550066755e-06,
      "loss": 0.2949,
      "step": 3571
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.06337061524391174,
      "learning_rate": 9.506008010680908e-06,
      "loss": 0.3967,
      "step": 3572
    },
    {
      "epoch": 0.9528,
      "grad_norm": 0.05761460214853287,
      "learning_rate": 9.452603471295061e-06,
      "loss": 0.3398,
      "step": 3573
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.051596175879240036,
      "learning_rate": 9.399198931909214e-06,
      "loss": 0.3338,
      "step": 3574
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.05321934074163437,
      "learning_rate": 9.345794392523365e-06,
      "loss": 0.2942,
      "step": 3575
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.053921591490507126,
      "learning_rate": 9.292389853137518e-06,
      "loss": 0.3323,
      "step": 3576
    },
    {
      "epoch": 0.9538666666666666,
      "grad_norm": 0.046985361725091934,
      "learning_rate": 9.23898531375167e-06,
      "loss": 0.3062,
      "step": 3577
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.048364534974098206,
      "learning_rate": 9.185580774365822e-06,
      "loss": 0.2863,
      "step": 3578
    },
    {
      "epoch": 0.9544,
      "grad_norm": 0.05744531378149986,
      "learning_rate": 9.132176234979973e-06,
      "loss": 0.3281,
      "step": 3579
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.053312771022319794,
      "learning_rate": 9.078771695594126e-06,
      "loss": 0.3331,
      "step": 3580
    },
    {
      "epoch": 0.9549333333333333,
      "grad_norm": 0.045342106372117996,
      "learning_rate": 9.025367156208278e-06,
      "loss": 0.2931,
      "step": 3581
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.062438804656267166,
      "learning_rate": 8.971962616822429e-06,
      "loss": 0.3692,
      "step": 3582
    },
    {
      "epoch": 0.9554666666666667,
      "grad_norm": 0.05728939548134804,
      "learning_rate": 8.918558077436582e-06,
      "loss": 0.3433,
      "step": 3583
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.04807015880942345,
      "learning_rate": 8.865153538050735e-06,
      "loss": 0.2961,
      "step": 3584
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.06407906860113144,
      "learning_rate": 8.811748998664888e-06,
      "loss": 0.3778,
      "step": 3585
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.07209997624158859,
      "learning_rate": 8.758344459279039e-06,
      "loss": 0.3494,
      "step": 3586
    },
    {
      "epoch": 0.9565333333333333,
      "grad_norm": 0.052314743399620056,
      "learning_rate": 8.704939919893192e-06,
      "loss": 0.3124,
      "step": 3587
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.04330986738204956,
      "learning_rate": 8.651535380507343e-06,
      "loss": 0.2358,
      "step": 3588
    },
    {
      "epoch": 0.9570666666666666,
      "grad_norm": 0.05350111052393913,
      "learning_rate": 8.598130841121496e-06,
      "loss": 0.3581,
      "step": 3589
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.06854414939880371,
      "learning_rate": 8.544726301735647e-06,
      "loss": 0.3565,
      "step": 3590
    },
    {
      "epoch": 0.9576,
      "grad_norm": 0.04776236042380333,
      "learning_rate": 8.4913217623498e-06,
      "loss": 0.3144,
      "step": 3591
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.07578156143426895,
      "learning_rate": 8.437917222963952e-06,
      "loss": 0.3485,
      "step": 3592
    },
    {
      "epoch": 0.9581333333333333,
      "grad_norm": 0.069597527384758,
      "learning_rate": 8.384512683578105e-06,
      "loss": 0.3722,
      "step": 3593
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.05227397754788399,
      "learning_rate": 8.331108144192257e-06,
      "loss": 0.3109,
      "step": 3594
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 0.051796041429042816,
      "learning_rate": 8.277703604806409e-06,
      "loss": 0.2983,
      "step": 3595
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.04784352332353592,
      "learning_rate": 8.224299065420562e-06,
      "loss": 0.3108,
      "step": 3596
    },
    {
      "epoch": 0.9592,
      "grad_norm": 0.05396782234311104,
      "learning_rate": 8.170894526034713e-06,
      "loss": 0.3202,
      "step": 3597
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.05810039862990379,
      "learning_rate": 8.117489986648866e-06,
      "loss": 0.3627,
      "step": 3598
    },
    {
      "epoch": 0.9597333333333333,
      "grad_norm": 0.04483795538544655,
      "learning_rate": 8.064085447263017e-06,
      "loss": 0.3179,
      "step": 3599
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.054367370903491974,
      "learning_rate": 8.01068090787717e-06,
      "loss": 0.351,
      "step": 3600
    },
    {
      "epoch": 0.9602666666666667,
      "grad_norm": 0.046619344502687454,
      "learning_rate": 7.957276368491321e-06,
      "loss": 0.2839,
      "step": 3601
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.067709781229496,
      "learning_rate": 7.903871829105474e-06,
      "loss": 0.4073,
      "step": 3602
    },
    {
      "epoch": 0.9608,
      "grad_norm": 0.05112426355481148,
      "learning_rate": 7.850467289719626e-06,
      "loss": 0.2918,
      "step": 3603
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.054016195237636566,
      "learning_rate": 7.797062750333778e-06,
      "loss": 0.3643,
      "step": 3604
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 0.049858458340168,
      "learning_rate": 7.743658210947931e-06,
      "loss": 0.3567,
      "step": 3605
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.05106142908334732,
      "learning_rate": 7.690253671562084e-06,
      "loss": 0.2752,
      "step": 3606
    },
    {
      "epoch": 0.9618666666666666,
      "grad_norm": 0.04943662881851196,
      "learning_rate": 7.636849132176236e-06,
      "loss": 0.3125,
      "step": 3607
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.0546298548579216,
      "learning_rate": 7.583444592790387e-06,
      "loss": 0.2568,
      "step": 3608
    },
    {
      "epoch": 0.9624,
      "grad_norm": 0.05965463072061539,
      "learning_rate": 7.53004005340454e-06,
      "loss": 0.3449,
      "step": 3609
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.055349670350551605,
      "learning_rate": 7.476635514018691e-06,
      "loss": 0.2819,
      "step": 3610
    },
    {
      "epoch": 0.9629333333333333,
      "grad_norm": 0.0569474957883358,
      "learning_rate": 7.423230974632844e-06,
      "loss": 0.2977,
      "step": 3611
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.06284430623054504,
      "learning_rate": 7.369826435246996e-06,
      "loss": 0.3038,
      "step": 3612
    },
    {
      "epoch": 0.9634666666666667,
      "grad_norm": 0.0491187684237957,
      "learning_rate": 7.316421895861149e-06,
      "loss": 0.2728,
      "step": 3613
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.058603547513484955,
      "learning_rate": 7.2630173564753e-06,
      "loss": 0.313,
      "step": 3614
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.038162149488925934,
      "learning_rate": 7.209612817089453e-06,
      "loss": 0.2353,
      "step": 3615
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.061377231031656265,
      "learning_rate": 7.156208277703605e-06,
      "loss": 0.3402,
      "step": 3616
    },
    {
      "epoch": 0.9645333333333334,
      "grad_norm": 0.06031468138098717,
      "learning_rate": 7.102803738317758e-06,
      "loss": 0.2708,
      "step": 3617
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.04742085933685303,
      "learning_rate": 7.0493991989319095e-06,
      "loss": 0.3085,
      "step": 3618
    },
    {
      "epoch": 0.9650666666666666,
      "grad_norm": 0.05087599530816078,
      "learning_rate": 6.9959946595460625e-06,
      "loss": 0.2931,
      "step": 3619
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.05092906579375267,
      "learning_rate": 6.942590120160214e-06,
      "loss": 0.3138,
      "step": 3620
    },
    {
      "epoch": 0.9656,
      "grad_norm": 0.05297166109085083,
      "learning_rate": 6.889185580774366e-06,
      "loss": 0.2927,
      "step": 3621
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.03927762806415558,
      "learning_rate": 6.835781041388519e-06,
      "loss": 0.2687,
      "step": 3622
    },
    {
      "epoch": 0.9661333333333333,
      "grad_norm": 0.05338124558329582,
      "learning_rate": 6.78237650200267e-06,
      "loss": 0.3126,
      "step": 3623
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.04505737125873566,
      "learning_rate": 6.728971962616823e-06,
      "loss": 0.2618,
      "step": 3624
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.0503726564347744,
      "learning_rate": 6.675567423230974e-06,
      "loss": 0.3014,
      "step": 3625
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.04148836433887482,
      "learning_rate": 6.622162883845127e-06,
      "loss": 0.2712,
      "step": 3626
    },
    {
      "epoch": 0.9672,
      "grad_norm": 0.05954417958855629,
      "learning_rate": 6.568758344459279e-06,
      "loss": 0.3802,
      "step": 3627
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.05234672874212265,
      "learning_rate": 6.515353805073432e-06,
      "loss": 0.3132,
      "step": 3628
    },
    {
      "epoch": 0.9677333333333333,
      "grad_norm": 0.047261737287044525,
      "learning_rate": 6.4619492656875834e-06,
      "loss": 0.2912,
      "step": 3629
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.04951005056500435,
      "learning_rate": 6.408544726301736e-06,
      "loss": 0.3327,
      "step": 3630
    },
    {
      "epoch": 0.9682666666666667,
      "grad_norm": 0.04539226368069649,
      "learning_rate": 6.355140186915888e-06,
      "loss": 0.2788,
      "step": 3631
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.05345688387751579,
      "learning_rate": 6.301735647530041e-06,
      "loss": 0.3724,
      "step": 3632
    },
    {
      "epoch": 0.9688,
      "grad_norm": 0.05154109373688698,
      "learning_rate": 6.248331108144193e-06,
      "loss": 0.3075,
      "step": 3633
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.04833501577377319,
      "learning_rate": 6.194926568758345e-06,
      "loss": 0.2981,
      "step": 3634
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 0.05888030305504799,
      "learning_rate": 6.141522029372497e-06,
      "loss": 0.3837,
      "step": 3635
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.04469545558094978,
      "learning_rate": 6.088117489986649e-06,
      "loss": 0.3094,
      "step": 3636
    },
    {
      "epoch": 0.9698666666666667,
      "grad_norm": 0.04801635816693306,
      "learning_rate": 6.034712950600802e-06,
      "loss": 0.3116,
      "step": 3637
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.05272487550973892,
      "learning_rate": 5.981308411214954e-06,
      "loss": 0.2844,
      "step": 3638
    },
    {
      "epoch": 0.9704,
      "grad_norm": 0.0722561702132225,
      "learning_rate": 5.927903871829106e-06,
      "loss": 0.3698,
      "step": 3639
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.049033116549253464,
      "learning_rate": 5.874499332443257e-06,
      "loss": 0.3188,
      "step": 3640
    },
    {
      "epoch": 0.9709333333333333,
      "grad_norm": 0.05835983529686928,
      "learning_rate": 5.8210947930574095e-06,
      "loss": 0.3253,
      "step": 3641
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.052643317729234695,
      "learning_rate": 5.767690253671562e-06,
      "loss": 0.3351,
      "step": 3642
    },
    {
      "epoch": 0.9714666666666667,
      "grad_norm": 0.058097757399082184,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 0.3366,
      "step": 3643
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.04889368638396263,
      "learning_rate": 5.660881174899867e-06,
      "loss": 0.3148,
      "step": 3644
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.049504004418849945,
      "learning_rate": 5.607476635514019e-06,
      "loss": 0.2933,
      "step": 3645
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.06067974492907524,
      "learning_rate": 5.554072096128171e-06,
      "loss": 0.2974,
      "step": 3646
    },
    {
      "epoch": 0.9725333333333334,
      "grad_norm": 0.054935213178396225,
      "learning_rate": 5.500667556742324e-06,
      "loss": 0.2991,
      "step": 3647
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.06350862234830856,
      "learning_rate": 5.447263017356476e-06,
      "loss": 0.3669,
      "step": 3648
    },
    {
      "epoch": 0.9730666666666666,
      "grad_norm": 0.049273718148469925,
      "learning_rate": 5.393858477970628e-06,
      "loss": 0.2959,
      "step": 3649
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.0681617334485054,
      "learning_rate": 5.34045393858478e-06,
      "loss": 0.4076,
      "step": 3650
    },
    {
      "epoch": 0.9736,
      "grad_norm": 0.05449450761079788,
      "learning_rate": 5.287049399198932e-06,
      "loss": 0.3996,
      "step": 3651
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.056020744144916534,
      "learning_rate": 5.233644859813085e-06,
      "loss": 0.2578,
      "step": 3652
    },
    {
      "epoch": 0.9741333333333333,
      "grad_norm": 0.06406126171350479,
      "learning_rate": 5.180240320427236e-06,
      "loss": 0.3597,
      "step": 3653
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.04270996153354645,
      "learning_rate": 5.1268357810413884e-06,
      "loss": 0.3121,
      "step": 3654
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 0.05634377524256706,
      "learning_rate": 5.0734312416555405e-06,
      "loss": 0.3443,
      "step": 3655
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.050609100610017776,
      "learning_rate": 5.020026702269693e-06,
      "loss": 0.31,
      "step": 3656
    },
    {
      "epoch": 0.9752,
      "grad_norm": 0.06424733251333237,
      "learning_rate": 4.9666221628838456e-06,
      "loss": 0.38,
      "step": 3657
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.05783110484480858,
      "learning_rate": 4.913217623497998e-06,
      "loss": 0.389,
      "step": 3658
    },
    {
      "epoch": 0.9757333333333333,
      "grad_norm": 0.04882937669754028,
      "learning_rate": 4.85981308411215e-06,
      "loss": 0.3191,
      "step": 3659
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.06224003806710243,
      "learning_rate": 4.806408544726302e-06,
      "loss": 0.2869,
      "step": 3660
    },
    {
      "epoch": 0.9762666666666666,
      "grad_norm": 0.04561174288392067,
      "learning_rate": 4.753004005340454e-06,
      "loss": 0.2493,
      "step": 3661
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.05056038871407509,
      "learning_rate": 4.699599465954607e-06,
      "loss": 0.3402,
      "step": 3662
    },
    {
      "epoch": 0.9768,
      "grad_norm": 0.050831329077482224,
      "learning_rate": 4.646194926568759e-06,
      "loss": 0.2917,
      "step": 3663
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.06301853060722351,
      "learning_rate": 4.592790387182911e-06,
      "loss": 0.3701,
      "step": 3664
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 0.043865419924259186,
      "learning_rate": 4.539385847797063e-06,
      "loss": 0.2961,
      "step": 3665
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.059241972863674164,
      "learning_rate": 4.4859813084112145e-06,
      "loss": 0.302,
      "step": 3666
    },
    {
      "epoch": 0.9778666666666667,
      "grad_norm": 0.05989690497517586,
      "learning_rate": 4.432576769025367e-06,
      "loss": 0.3684,
      "step": 3667
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.05932776257395744,
      "learning_rate": 4.3791722296395195e-06,
      "loss": 0.3438,
      "step": 3668
    },
    {
      "epoch": 0.9784,
      "grad_norm": 0.05525525286793709,
      "learning_rate": 4.325767690253672e-06,
      "loss": 0.3208,
      "step": 3669
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.04980740323662758,
      "learning_rate": 4.272363150867824e-06,
      "loss": 0.2984,
      "step": 3670
    },
    {
      "epoch": 0.9789333333333333,
      "grad_norm": 0.06395353376865387,
      "learning_rate": 4.218958611481976e-06,
      "loss": 0.3533,
      "step": 3671
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.04806894063949585,
      "learning_rate": 4.165554072096129e-06,
      "loss": 0.2976,
      "step": 3672
    },
    {
      "epoch": 0.9794666666666667,
      "grad_norm": 0.055949144065380096,
      "learning_rate": 4.112149532710281e-06,
      "loss": 0.2858,
      "step": 3673
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.05801185965538025,
      "learning_rate": 4.058744993324433e-06,
      "loss": 0.2786,
      "step": 3674
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05118623375892639,
      "learning_rate": 4.005340453938585e-06,
      "loss": 0.2777,
      "step": 3675
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.04116043448448181,
      "learning_rate": 3.951935914552737e-06,
      "loss": 0.2439,
      "step": 3676
    },
    {
      "epoch": 0.9805333333333334,
      "grad_norm": 0.04788718372583389,
      "learning_rate": 3.898531375166889e-06,
      "loss": 0.2784,
      "step": 3677
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.0687275156378746,
      "learning_rate": 3.845126835781042e-06,
      "loss": 0.4444,
      "step": 3678
    },
    {
      "epoch": 0.9810666666666666,
      "grad_norm": 0.04996970295906067,
      "learning_rate": 3.7917222963951934e-06,
      "loss": 0.2866,
      "step": 3679
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.047074418514966965,
      "learning_rate": 3.7383177570093455e-06,
      "loss": 0.2763,
      "step": 3680
    },
    {
      "epoch": 0.9816,
      "grad_norm": 0.04276343062520027,
      "learning_rate": 3.684913217623498e-06,
      "loss": 0.2745,
      "step": 3681
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.05228443071246147,
      "learning_rate": 3.63150867823765e-06,
      "loss": 0.2877,
      "step": 3682
    },
    {
      "epoch": 0.9821333333333333,
      "grad_norm": 0.05460330471396446,
      "learning_rate": 3.5781041388518027e-06,
      "loss": 0.2791,
      "step": 3683
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.04902452230453491,
      "learning_rate": 3.5246995994659548e-06,
      "loss": 0.332,
      "step": 3684
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 0.03932252526283264,
      "learning_rate": 3.471295060080107e-06,
      "loss": 0.2593,
      "step": 3685
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.05618184432387352,
      "learning_rate": 3.4178905206942594e-06,
      "loss": 0.3621,
      "step": 3686
    },
    {
      "epoch": 0.9832,
      "grad_norm": 0.057835884392261505,
      "learning_rate": 3.3644859813084115e-06,
      "loss": 0.332,
      "step": 3687
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.06083015352487564,
      "learning_rate": 3.3110814419225636e-06,
      "loss": 0.3401,
      "step": 3688
    },
    {
      "epoch": 0.9837333333333333,
      "grad_norm": 0.053699981421232224,
      "learning_rate": 3.257676902536716e-06,
      "loss": 0.3146,
      "step": 3689
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.05363808199763298,
      "learning_rate": 3.204272363150868e-06,
      "loss": 0.333,
      "step": 3690
    },
    {
      "epoch": 0.9842666666666666,
      "grad_norm": 0.05824535712599754,
      "learning_rate": 3.1508678237650203e-06,
      "loss": 0.3306,
      "step": 3691
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.0645725354552269,
      "learning_rate": 3.0974632843791724e-06,
      "loss": 0.3873,
      "step": 3692
    },
    {
      "epoch": 0.9848,
      "grad_norm": 0.06553222984075546,
      "learning_rate": 3.0440587449933245e-06,
      "loss": 0.3617,
      "step": 3693
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.06447003036737442,
      "learning_rate": 2.990654205607477e-06,
      "loss": 0.429,
      "step": 3694
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 0.050709351897239685,
      "learning_rate": 2.9372496662216287e-06,
      "loss": 0.3021,
      "step": 3695
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.04903360456228256,
      "learning_rate": 2.883845126835781e-06,
      "loss": 0.2777,
      "step": 3696
    },
    {
      "epoch": 0.9858666666666667,
      "grad_norm": 0.07736431807279587,
      "learning_rate": 2.8304405874499333e-06,
      "loss": 0.3024,
      "step": 3697
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.047878507524728775,
      "learning_rate": 2.7770360480640854e-06,
      "loss": 0.2929,
      "step": 3698
    },
    {
      "epoch": 0.9864,
      "grad_norm": 0.06857465207576752,
      "learning_rate": 2.723631508678238e-06,
      "loss": 0.3899,
      "step": 3699
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.059242963790893555,
      "learning_rate": 2.67022696929239e-06,
      "loss": 0.2785,
      "step": 3700
    },
    {
      "epoch": 0.9869333333333333,
      "grad_norm": 0.04362432658672333,
      "learning_rate": 2.6168224299065425e-06,
      "loss": 0.2811,
      "step": 3701
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.06648553907871246,
      "learning_rate": 2.5634178905206942e-06,
      "loss": 0.3519,
      "step": 3702
    },
    {
      "epoch": 0.9874666666666667,
      "grad_norm": 0.0712350457906723,
      "learning_rate": 2.5100133511348463e-06,
      "loss": 0.4488,
      "step": 3703
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.06608695536851883,
      "learning_rate": 2.456608811748999e-06,
      "loss": 0.3505,
      "step": 3704
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.037679512053728104,
      "learning_rate": 2.403204272363151e-06,
      "loss": 0.2549,
      "step": 3705
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.05317175015807152,
      "learning_rate": 2.3497997329773035e-06,
      "loss": 0.3489,
      "step": 3706
    },
    {
      "epoch": 0.9885333333333334,
      "grad_norm": 0.04547269269824028,
      "learning_rate": 2.2963951935914556e-06,
      "loss": 0.2573,
      "step": 3707
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.04721422493457794,
      "learning_rate": 2.2429906542056072e-06,
      "loss": 0.2685,
      "step": 3708
    },
    {
      "epoch": 0.9890666666666666,
      "grad_norm": 0.055574774742126465,
      "learning_rate": 2.1895861148197598e-06,
      "loss": 0.3506,
      "step": 3709
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.05098820477724075,
      "learning_rate": 2.136181575433912e-06,
      "loss": 0.2862,
      "step": 3710
    },
    {
      "epoch": 0.9896,
      "grad_norm": 0.06282372772693634,
      "learning_rate": 2.0827770360480644e-06,
      "loss": 0.3166,
      "step": 3711
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.053557123988866806,
      "learning_rate": 2.0293724966622165e-06,
      "loss": 0.3051,
      "step": 3712
    },
    {
      "epoch": 0.9901333333333333,
      "grad_norm": 0.03455989807844162,
      "learning_rate": 1.9759679572763686e-06,
      "loss": 0.2213,
      "step": 3713
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.051788780838251114,
      "learning_rate": 1.922563417890521e-06,
      "loss": 0.3081,
      "step": 3714
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 0.05097398906946182,
      "learning_rate": 1.8691588785046728e-06,
      "loss": 0.3271,
      "step": 3715
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.07312652468681335,
      "learning_rate": 1.815754339118825e-06,
      "loss": 0.4401,
      "step": 3716
    },
    {
      "epoch": 0.9912,
      "grad_norm": 0.04537266120314598,
      "learning_rate": 1.7623497997329774e-06,
      "loss": 0.2781,
      "step": 3717
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.041123710572719574,
      "learning_rate": 1.7089452603471297e-06,
      "loss": 0.271,
      "step": 3718
    },
    {
      "epoch": 0.9917333333333334,
      "grad_norm": 0.04028773307800293,
      "learning_rate": 1.6555407209612818e-06,
      "loss": 0.2502,
      "step": 3719
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.04927540943026543,
      "learning_rate": 1.602136181575434e-06,
      "loss": 0.3387,
      "step": 3720
    },
    {
      "epoch": 0.9922666666666666,
      "grad_norm": 0.061309218406677246,
      "learning_rate": 1.5487316421895862e-06,
      "loss": 0.317,
      "step": 3721
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.05670403316617012,
      "learning_rate": 1.4953271028037385e-06,
      "loss": 0.3217,
      "step": 3722
    },
    {
      "epoch": 0.9928,
      "grad_norm": 0.060231804847717285,
      "learning_rate": 1.4419225634178906e-06,
      "loss": 0.3273,
      "step": 3723
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.05105529725551605,
      "learning_rate": 1.3885180240320427e-06,
      "loss": 0.3263,
      "step": 3724
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.050777435302734375,
      "learning_rate": 1.335113484646195e-06,
      "loss": 0.2846,
      "step": 3725
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.04819432273507118,
      "learning_rate": 1.2817089452603471e-06,
      "loss": 0.2516,
      "step": 3726
    },
    {
      "epoch": 0.9938666666666667,
      "grad_norm": 0.05075119435787201,
      "learning_rate": 1.2283044058744994e-06,
      "loss": 0.305,
      "step": 3727
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.05571449175477028,
      "learning_rate": 1.1748998664886517e-06,
      "loss": 0.307,
      "step": 3728
    },
    {
      "epoch": 0.9944,
      "grad_norm": 0.04770120605826378,
      "learning_rate": 1.1214953271028036e-06,
      "loss": 0.2935,
      "step": 3729
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.05918964371085167,
      "learning_rate": 1.068090787716956e-06,
      "loss": 0.3246,
      "step": 3730
    },
    {
      "epoch": 0.9949333333333333,
      "grad_norm": 0.0654287040233612,
      "learning_rate": 1.0146862483311082e-06,
      "loss": 0.3633,
      "step": 3731
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.057477500289678574,
      "learning_rate": 9.612817089452605e-07,
      "loss": 0.3911,
      "step": 3732
    },
    {
      "epoch": 0.9954666666666667,
      "grad_norm": 0.06406187266111374,
      "learning_rate": 9.078771695594125e-07,
      "loss": 0.3964,
      "step": 3733
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.05906510353088379,
      "learning_rate": 8.544726301735648e-07,
      "loss": 0.3985,
      "step": 3734
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.03873192518949509,
      "learning_rate": 8.01068090787717e-07,
      "loss": 0.2732,
      "step": 3735
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.06381683796644211,
      "learning_rate": 7.476635514018693e-07,
      "loss": 0.3523,
      "step": 3736
    },
    {
      "epoch": 0.9965333333333334,
      "grad_norm": 0.05878053978085518,
      "learning_rate": 6.942590120160214e-07,
      "loss": 0.366,
      "step": 3737
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.04776016250252724,
      "learning_rate": 6.408544726301736e-07,
      "loss": 0.2915,
      "step": 3738
    },
    {
      "epoch": 0.9970666666666667,
      "grad_norm": 0.05291847139596939,
      "learning_rate": 5.874499332443259e-07,
      "loss": 0.2891,
      "step": 3739
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.049207184463739395,
      "learning_rate": 5.34045393858478e-07,
      "loss": 0.3003,
      "step": 3740
    },
    {
      "epoch": 0.9976,
      "grad_norm": 0.051765091717243195,
      "learning_rate": 4.806408544726303e-07,
      "loss": 0.3107,
      "step": 3741
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.055726777762174606,
      "learning_rate": 4.272363150867824e-07,
      "loss": 0.3432,
      "step": 3742
    },
    {
      "epoch": 0.9981333333333333,
      "grad_norm": 0.05286937579512596,
      "learning_rate": 3.738317757009346e-07,
      "loss": 0.2742,
      "step": 3743
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.04467969387769699,
      "learning_rate": 3.204272363150868e-07,
      "loss": 0.2734,
      "step": 3744
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 0.06253127753734589,
      "learning_rate": 2.67022696929239e-07,
      "loss": 0.3834,
      "step": 3745
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.051215656101703644,
      "learning_rate": 2.136181575433912e-07,
      "loss": 0.3003,
      "step": 3746
    },
    {
      "epoch": 0.9992,
      "grad_norm": 0.05098038166761398,
      "learning_rate": 1.602136181575434e-07,
      "loss": 0.3624,
      "step": 3747
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.044000446796417236,
      "learning_rate": 1.068090787716956e-07,
      "loss": 0.2657,
      "step": 3748
    },
    {
      "epoch": 0.9997333333333334,
      "grad_norm": 0.040747180581092834,
      "learning_rate": 5.34045393858478e-08,
      "loss": 0.249,
      "step": 3749
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06085944175720215,
      "learning_rate": 0.0,
      "loss": 0.3691,
      "step": 3750
    }
  ],
  "logging_steps": 1,
  "max_steps": 3750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.19939114941208e+19,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
