{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2499,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00040016006402561027,
      "grad_norm": 0.22946873307228088,
      "learning_rate": 4e-05,
      "loss": 0.4692,
      "step": 1
    },
    {
      "epoch": 0.0008003201280512205,
      "grad_norm": 0.16390261054039001,
      "learning_rate": 8e-05,
      "loss": 0.3807,
      "step": 2
    },
    {
      "epoch": 0.0012004801920768306,
      "grad_norm": 0.26059892773628235,
      "learning_rate": 0.00012,
      "loss": 0.5289,
      "step": 3
    },
    {
      "epoch": 0.001600640256102441,
      "grad_norm": 0.4976859986782074,
      "learning_rate": 0.00016,
      "loss": 0.4337,
      "step": 4
    },
    {
      "epoch": 0.0020008003201280513,
      "grad_norm": 0.3747754991054535,
      "learning_rate": 0.0002,
      "loss": 0.4729,
      "step": 5
    },
    {
      "epoch": 0.0024009603841536613,
      "grad_norm": 0.5714130997657776,
      "learning_rate": 0.00019991980753809142,
      "loss": 0.4508,
      "step": 6
    },
    {
      "epoch": 0.0028011204481792717,
      "grad_norm": 0.3919544219970703,
      "learning_rate": 0.00019983961507618285,
      "loss": 0.4079,
      "step": 7
    },
    {
      "epoch": 0.003201280512204882,
      "grad_norm": 0.23712654411792755,
      "learning_rate": 0.00019975942261427426,
      "loss": 0.5978,
      "step": 8
    },
    {
      "epoch": 0.003601440576230492,
      "grad_norm": 0.22808951139450073,
      "learning_rate": 0.0001996792301523657,
      "loss": 0.4997,
      "step": 9
    },
    {
      "epoch": 0.004001600640256103,
      "grad_norm": 0.12574392557144165,
      "learning_rate": 0.0001995990376904571,
      "loss": 0.3915,
      "step": 10
    },
    {
      "epoch": 0.004401760704281713,
      "grad_norm": 0.16919584572315216,
      "learning_rate": 0.00019951884522854854,
      "loss": 0.4628,
      "step": 11
    },
    {
      "epoch": 0.004801920768307323,
      "grad_norm": 0.18785926699638367,
      "learning_rate": 0.00019943865276663995,
      "loss": 0.4591,
      "step": 12
    },
    {
      "epoch": 0.005202080832332933,
      "grad_norm": 0.15934772789478302,
      "learning_rate": 0.00019935846030473135,
      "loss": 0.4936,
      "step": 13
    },
    {
      "epoch": 0.0056022408963585435,
      "grad_norm": 0.1776677519083023,
      "learning_rate": 0.00019927826784282276,
      "loss": 0.357,
      "step": 14
    },
    {
      "epoch": 0.006002400960384154,
      "grad_norm": 0.17960716784000397,
      "learning_rate": 0.0001991980753809142,
      "loss": 0.5314,
      "step": 15
    },
    {
      "epoch": 0.006402561024409764,
      "grad_norm": 0.14224982261657715,
      "learning_rate": 0.0001991178829190056,
      "loss": 0.4277,
      "step": 16
    },
    {
      "epoch": 0.006802721088435374,
      "grad_norm": 0.1837865263223648,
      "learning_rate": 0.00019903769045709704,
      "loss": 0.5311,
      "step": 17
    },
    {
      "epoch": 0.007202881152460984,
      "grad_norm": 0.14006249606609344,
      "learning_rate": 0.00019895749799518845,
      "loss": 0.49,
      "step": 18
    },
    {
      "epoch": 0.007603041216486595,
      "grad_norm": 0.15497927367687225,
      "learning_rate": 0.00019887730553327988,
      "loss": 0.5023,
      "step": 19
    },
    {
      "epoch": 0.008003201280512205,
      "grad_norm": 0.1743759661912918,
      "learning_rate": 0.0001987971130713713,
      "loss": 0.4607,
      "step": 20
    },
    {
      "epoch": 0.008403361344537815,
      "grad_norm": 0.15628352761268616,
      "learning_rate": 0.00019871692060946273,
      "loss": 0.4854,
      "step": 21
    },
    {
      "epoch": 0.008803521408563426,
      "grad_norm": 0.15339666604995728,
      "learning_rate": 0.00019863672814755413,
      "loss": 0.4505,
      "step": 22
    },
    {
      "epoch": 0.009203681472589036,
      "grad_norm": 0.1642875373363495,
      "learning_rate": 0.00019855653568564557,
      "loss": 0.5261,
      "step": 23
    },
    {
      "epoch": 0.009603841536614645,
      "grad_norm": 0.17513947188854218,
      "learning_rate": 0.00019847634322373698,
      "loss": 0.5614,
      "step": 24
    },
    {
      "epoch": 0.010004001600640256,
      "grad_norm": 0.17406989634037018,
      "learning_rate": 0.0001983961507618284,
      "loss": 0.5059,
      "step": 25
    },
    {
      "epoch": 0.010404161664665866,
      "grad_norm": 0.1399659812450409,
      "learning_rate": 0.00019831595829991982,
      "loss": 0.4831,
      "step": 26
    },
    {
      "epoch": 0.010804321728691477,
      "grad_norm": 0.15993866324424744,
      "learning_rate": 0.00019823576583801125,
      "loss": 0.5134,
      "step": 27
    },
    {
      "epoch": 0.011204481792717087,
      "grad_norm": 0.147577702999115,
      "learning_rate": 0.00019815557337610266,
      "loss": 0.4907,
      "step": 28
    },
    {
      "epoch": 0.011604641856742696,
      "grad_norm": 0.18316051363945007,
      "learning_rate": 0.00019807538091419407,
      "loss": 0.5415,
      "step": 29
    },
    {
      "epoch": 0.012004801920768308,
      "grad_norm": 0.13442182540893555,
      "learning_rate": 0.00019799518845228548,
      "loss": 0.4482,
      "step": 30
    },
    {
      "epoch": 0.012404961984793917,
      "grad_norm": 0.14758652448654175,
      "learning_rate": 0.0001979149959903769,
      "loss": 0.4662,
      "step": 31
    },
    {
      "epoch": 0.012805122048819529,
      "grad_norm": 0.13469131290912628,
      "learning_rate": 0.00019783480352846832,
      "loss": 0.4239,
      "step": 32
    },
    {
      "epoch": 0.013205282112845138,
      "grad_norm": 0.15882259607315063,
      "learning_rate": 0.00019775461106655976,
      "loss": 0.4956,
      "step": 33
    },
    {
      "epoch": 0.013605442176870748,
      "grad_norm": 0.14396321773529053,
      "learning_rate": 0.00019767441860465116,
      "loss": 0.5383,
      "step": 34
    },
    {
      "epoch": 0.014005602240896359,
      "grad_norm": 0.17287583649158478,
      "learning_rate": 0.0001975942261427426,
      "loss": 0.4434,
      "step": 35
    },
    {
      "epoch": 0.014405762304921969,
      "grad_norm": 0.1848708838224411,
      "learning_rate": 0.000197514033680834,
      "loss": 0.4147,
      "step": 36
    },
    {
      "epoch": 0.014805922368947578,
      "grad_norm": 0.1444273144006729,
      "learning_rate": 0.00019743384121892544,
      "loss": 0.5027,
      "step": 37
    },
    {
      "epoch": 0.01520608243297319,
      "grad_norm": 0.17688049376010895,
      "learning_rate": 0.00019735364875701685,
      "loss": 0.4522,
      "step": 38
    },
    {
      "epoch": 0.015606242496998799,
      "grad_norm": 0.17187055945396423,
      "learning_rate": 0.00019727345629510829,
      "loss": 0.4511,
      "step": 39
    },
    {
      "epoch": 0.01600640256102441,
      "grad_norm": 0.13287261128425598,
      "learning_rate": 0.0001971932638331997,
      "loss": 0.4586,
      "step": 40
    },
    {
      "epoch": 0.01640656262505002,
      "grad_norm": 0.1346621811389923,
      "learning_rate": 0.0001971130713712911,
      "loss": 0.3949,
      "step": 41
    },
    {
      "epoch": 0.01680672268907563,
      "grad_norm": 0.18018732964992523,
      "learning_rate": 0.00019703287890938254,
      "loss": 0.542,
      "step": 42
    },
    {
      "epoch": 0.01720688275310124,
      "grad_norm": 0.14330074191093445,
      "learning_rate": 0.00019695268644747394,
      "loss": 0.5262,
      "step": 43
    },
    {
      "epoch": 0.017607042817126852,
      "grad_norm": 0.16825029253959656,
      "learning_rate": 0.00019687249398556538,
      "loss": 0.4817,
      "step": 44
    },
    {
      "epoch": 0.01800720288115246,
      "grad_norm": 0.12784413993358612,
      "learning_rate": 0.0001967923015236568,
      "loss": 0.4303,
      "step": 45
    },
    {
      "epoch": 0.01840736294517807,
      "grad_norm": 0.1517990082502365,
      "learning_rate": 0.0001967121090617482,
      "loss": 0.553,
      "step": 46
    },
    {
      "epoch": 0.018807523009203683,
      "grad_norm": 0.10310398042201996,
      "learning_rate": 0.0001966319165998396,
      "loss": 0.3557,
      "step": 47
    },
    {
      "epoch": 0.01920768307322929,
      "grad_norm": 0.14064480364322662,
      "learning_rate": 0.00019655172413793104,
      "loss": 0.4677,
      "step": 48
    },
    {
      "epoch": 0.0196078431372549,
      "grad_norm": 0.13959263265132904,
      "learning_rate": 0.00019647153167602245,
      "loss": 0.5422,
      "step": 49
    },
    {
      "epoch": 0.020008003201280513,
      "grad_norm": 0.13795213401317596,
      "learning_rate": 0.00019639133921411388,
      "loss": 0.4465,
      "step": 50
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 0.16808612644672394,
      "learning_rate": 0.0001963111467522053,
      "loss": 0.5849,
      "step": 51
    },
    {
      "epoch": 0.020808323329331732,
      "grad_norm": 0.11101402342319489,
      "learning_rate": 0.00019623095429029672,
      "loss": 0.4572,
      "step": 52
    },
    {
      "epoch": 0.021208483393357343,
      "grad_norm": 0.1317504644393921,
      "learning_rate": 0.00019615076182838813,
      "loss": 0.3873,
      "step": 53
    },
    {
      "epoch": 0.021608643457382955,
      "grad_norm": 0.17213749885559082,
      "learning_rate": 0.00019607056936647957,
      "loss": 0.5734,
      "step": 54
    },
    {
      "epoch": 0.022008803521408563,
      "grad_norm": 0.1286506950855255,
      "learning_rate": 0.00019599037690457097,
      "loss": 0.4729,
      "step": 55
    },
    {
      "epoch": 0.022408963585434174,
      "grad_norm": 0.13250727951526642,
      "learning_rate": 0.0001959101844426624,
      "loss": 0.4639,
      "step": 56
    },
    {
      "epoch": 0.022809123649459785,
      "grad_norm": 0.16937655210494995,
      "learning_rate": 0.00019582999198075382,
      "loss": 0.4967,
      "step": 57
    },
    {
      "epoch": 0.023209283713485393,
      "grad_norm": 0.14461025595664978,
      "learning_rate": 0.00019574979951884525,
      "loss": 0.4212,
      "step": 58
    },
    {
      "epoch": 0.023609443777511004,
      "grad_norm": 0.12467870861291885,
      "learning_rate": 0.00019566960705693666,
      "loss": 0.48,
      "step": 59
    },
    {
      "epoch": 0.024009603841536616,
      "grad_norm": 0.14800018072128296,
      "learning_rate": 0.0001955894145950281,
      "loss": 0.519,
      "step": 60
    },
    {
      "epoch": 0.024409763905562223,
      "grad_norm": 0.11576331406831741,
      "learning_rate": 0.00019550922213311948,
      "loss": 0.4167,
      "step": 61
    },
    {
      "epoch": 0.024809923969587835,
      "grad_norm": 0.15244458615779877,
      "learning_rate": 0.0001954290296712109,
      "loss": 0.4834,
      "step": 62
    },
    {
      "epoch": 0.025210084033613446,
      "grad_norm": 0.10282991826534271,
      "learning_rate": 0.00019534883720930232,
      "loss": 0.3936,
      "step": 63
    },
    {
      "epoch": 0.025610244097639057,
      "grad_norm": 0.12172392010688782,
      "learning_rate": 0.00019526864474739375,
      "loss": 0.4041,
      "step": 64
    },
    {
      "epoch": 0.026010404161664665,
      "grad_norm": 0.12906822562217712,
      "learning_rate": 0.00019518845228548516,
      "loss": 0.4056,
      "step": 65
    },
    {
      "epoch": 0.026410564225690276,
      "grad_norm": 0.11636485904455185,
      "learning_rate": 0.0001951082598235766,
      "loss": 0.3584,
      "step": 66
    },
    {
      "epoch": 0.026810724289715888,
      "grad_norm": 0.13368813693523407,
      "learning_rate": 0.000195028067361668,
      "loss": 0.3486,
      "step": 67
    },
    {
      "epoch": 0.027210884353741496,
      "grad_norm": 0.11729364097118378,
      "learning_rate": 0.00019494787489975944,
      "loss": 0.4542,
      "step": 68
    },
    {
      "epoch": 0.027611044417767107,
      "grad_norm": 0.14878401160240173,
      "learning_rate": 0.00019486768243785085,
      "loss": 0.4081,
      "step": 69
    },
    {
      "epoch": 0.028011204481792718,
      "grad_norm": 0.1566425859928131,
      "learning_rate": 0.00019478748997594228,
      "loss": 0.4563,
      "step": 70
    },
    {
      "epoch": 0.028411364545818326,
      "grad_norm": 0.13982544839382172,
      "learning_rate": 0.0001947072975140337,
      "loss": 0.4815,
      "step": 71
    },
    {
      "epoch": 0.028811524609843937,
      "grad_norm": 0.11852291971445084,
      "learning_rate": 0.00019462710505212513,
      "loss": 0.3915,
      "step": 72
    },
    {
      "epoch": 0.02921168467386955,
      "grad_norm": 0.15591681003570557,
      "learning_rate": 0.00019454691259021653,
      "loss": 0.5166,
      "step": 73
    },
    {
      "epoch": 0.029611844737895156,
      "grad_norm": 0.13018865883350372,
      "learning_rate": 0.00019446672012830797,
      "loss": 0.441,
      "step": 74
    },
    {
      "epoch": 0.030012004801920768,
      "grad_norm": 0.12293136119842529,
      "learning_rate": 0.00019438652766639938,
      "loss": 0.5128,
      "step": 75
    },
    {
      "epoch": 0.03041216486594638,
      "grad_norm": 0.12453330308198929,
      "learning_rate": 0.00019430633520449078,
      "loss": 0.4661,
      "step": 76
    },
    {
      "epoch": 0.03081232492997199,
      "grad_norm": 0.13392303884029388,
      "learning_rate": 0.0001942261427425822,
      "loss": 0.4712,
      "step": 77
    },
    {
      "epoch": 0.031212484993997598,
      "grad_norm": 0.12176399677991867,
      "learning_rate": 0.00019414595028067363,
      "loss": 0.4389,
      "step": 78
    },
    {
      "epoch": 0.031612645058023206,
      "grad_norm": 0.1300990879535675,
      "learning_rate": 0.00019406575781876504,
      "loss": 0.4751,
      "step": 79
    },
    {
      "epoch": 0.03201280512204882,
      "grad_norm": 0.11435059458017349,
      "learning_rate": 0.00019398556535685647,
      "loss": 0.4576,
      "step": 80
    },
    {
      "epoch": 0.03241296518607443,
      "grad_norm": 0.0858248695731163,
      "learning_rate": 0.00019390537289494788,
      "loss": 0.3387,
      "step": 81
    },
    {
      "epoch": 0.03281312525010004,
      "grad_norm": 0.13226385414600372,
      "learning_rate": 0.00019382518043303929,
      "loss": 0.4493,
      "step": 82
    },
    {
      "epoch": 0.03321328531412565,
      "grad_norm": 0.15288126468658447,
      "learning_rate": 0.00019374498797113072,
      "loss": 0.538,
      "step": 83
    },
    {
      "epoch": 0.03361344537815126,
      "grad_norm": 0.10629519820213318,
      "learning_rate": 0.00019366479550922213,
      "loss": 0.4083,
      "step": 84
    },
    {
      "epoch": 0.034013605442176874,
      "grad_norm": 0.13565696775913239,
      "learning_rate": 0.00019358460304731356,
      "loss": 0.4495,
      "step": 85
    },
    {
      "epoch": 0.03441376550620248,
      "grad_norm": 0.11949463933706284,
      "learning_rate": 0.00019350441058540497,
      "loss": 0.449,
      "step": 86
    },
    {
      "epoch": 0.03481392557022809,
      "grad_norm": 0.14017391204833984,
      "learning_rate": 0.0001934242181234964,
      "loss": 0.5416,
      "step": 87
    },
    {
      "epoch": 0.035214085634253704,
      "grad_norm": 0.12003178894519806,
      "learning_rate": 0.00019334402566158782,
      "loss": 0.4357,
      "step": 88
    },
    {
      "epoch": 0.03561424569827931,
      "grad_norm": 0.131962850689888,
      "learning_rate": 0.00019326383319967925,
      "loss": 0.4381,
      "step": 89
    },
    {
      "epoch": 0.03601440576230492,
      "grad_norm": 0.11337488144636154,
      "learning_rate": 0.00019318364073777066,
      "loss": 0.4262,
      "step": 90
    },
    {
      "epoch": 0.036414565826330535,
      "grad_norm": 0.14590921998023987,
      "learning_rate": 0.0001931034482758621,
      "loss": 0.5096,
      "step": 91
    },
    {
      "epoch": 0.03681472589035614,
      "grad_norm": 0.13093169033527374,
      "learning_rate": 0.0001930232558139535,
      "loss": 0.4865,
      "step": 92
    },
    {
      "epoch": 0.03721488595438175,
      "grad_norm": 0.14437992870807648,
      "learning_rate": 0.0001929430633520449,
      "loss": 0.4666,
      "step": 93
    },
    {
      "epoch": 0.037615046018407365,
      "grad_norm": 0.10006094723939896,
      "learning_rate": 0.00019286287089013632,
      "loss": 0.4532,
      "step": 94
    },
    {
      "epoch": 0.03801520608243297,
      "grad_norm": 0.20455269515514374,
      "learning_rate": 0.00019278267842822775,
      "loss": 0.529,
      "step": 95
    },
    {
      "epoch": 0.03841536614645858,
      "grad_norm": 0.13493114709854126,
      "learning_rate": 0.00019270248596631916,
      "loss": 0.4921,
      "step": 96
    },
    {
      "epoch": 0.038815526210484196,
      "grad_norm": 0.12510214745998383,
      "learning_rate": 0.0001926222935044106,
      "loss": 0.5435,
      "step": 97
    },
    {
      "epoch": 0.0392156862745098,
      "grad_norm": 0.10947107523679733,
      "learning_rate": 0.000192542101042502,
      "loss": 0.3436,
      "step": 98
    },
    {
      "epoch": 0.03961584633853541,
      "grad_norm": 0.1674061119556427,
      "learning_rate": 0.00019246190858059344,
      "loss": 0.4791,
      "step": 99
    },
    {
      "epoch": 0.040016006402561026,
      "grad_norm": 0.12110971659421921,
      "learning_rate": 0.00019238171611868485,
      "loss": 0.3824,
      "step": 100
    },
    {
      "epoch": 0.040416166466586634,
      "grad_norm": 0.1303078532218933,
      "learning_rate": 0.00019230152365677628,
      "loss": 0.4806,
      "step": 101
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 0.13087669014930725,
      "learning_rate": 0.0001922213311948677,
      "loss": 0.394,
      "step": 102
    },
    {
      "epoch": 0.041216486594637856,
      "grad_norm": 0.1451854407787323,
      "learning_rate": 0.00019214113873295912,
      "loss": 0.4675,
      "step": 103
    },
    {
      "epoch": 0.041616646658663464,
      "grad_norm": 0.1276141256093979,
      "learning_rate": 0.00019206094627105053,
      "loss": 0.4524,
      "step": 104
    },
    {
      "epoch": 0.04201680672268908,
      "grad_norm": 0.13451483845710754,
      "learning_rate": 0.00019198075380914197,
      "loss": 0.5279,
      "step": 105
    },
    {
      "epoch": 0.04241696678671469,
      "grad_norm": 0.1201162338256836,
      "learning_rate": 0.00019190056134723337,
      "loss": 0.4301,
      "step": 106
    },
    {
      "epoch": 0.042817126850740295,
      "grad_norm": 0.1359904557466507,
      "learning_rate": 0.0001918203688853248,
      "loss": 0.3869,
      "step": 107
    },
    {
      "epoch": 0.04321728691476591,
      "grad_norm": 0.09089353680610657,
      "learning_rate": 0.00019174017642341622,
      "loss": 0.4132,
      "step": 108
    },
    {
      "epoch": 0.04361744697879152,
      "grad_norm": 0.11473874747753143,
      "learning_rate": 0.00019165998396150763,
      "loss": 0.4032,
      "step": 109
    },
    {
      "epoch": 0.044017607042817125,
      "grad_norm": 0.13859650492668152,
      "learning_rate": 0.00019157979149959903,
      "loss": 0.4179,
      "step": 110
    },
    {
      "epoch": 0.04441776710684274,
      "grad_norm": 0.11934302747249603,
      "learning_rate": 0.00019149959903769047,
      "loss": 0.4429,
      "step": 111
    },
    {
      "epoch": 0.04481792717086835,
      "grad_norm": 0.1345134824514389,
      "learning_rate": 0.00019141940657578188,
      "loss": 0.4739,
      "step": 112
    },
    {
      "epoch": 0.045218087234893956,
      "grad_norm": 0.14646725356578827,
      "learning_rate": 0.0001913392141138733,
      "loss": 0.4861,
      "step": 113
    },
    {
      "epoch": 0.04561824729891957,
      "grad_norm": 0.1378999948501587,
      "learning_rate": 0.00019125902165196472,
      "loss": 0.5629,
      "step": 114
    },
    {
      "epoch": 0.04601840736294518,
      "grad_norm": 0.12542778253555298,
      "learning_rate": 0.00019117882919005615,
      "loss": 0.5017,
      "step": 115
    },
    {
      "epoch": 0.046418567426970786,
      "grad_norm": 0.10740013420581818,
      "learning_rate": 0.00019109863672814756,
      "loss": 0.438,
      "step": 116
    },
    {
      "epoch": 0.0468187274909964,
      "grad_norm": 0.13227002322673798,
      "learning_rate": 0.00019101844426623897,
      "loss": 0.4622,
      "step": 117
    },
    {
      "epoch": 0.04721888755502201,
      "grad_norm": 0.1616428643465042,
      "learning_rate": 0.0001909382518043304,
      "loss": 0.4689,
      "step": 118
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.13426005840301514,
      "learning_rate": 0.0001908580593424218,
      "loss": 0.4501,
      "step": 119
    },
    {
      "epoch": 0.04801920768307323,
      "grad_norm": 0.11059197783470154,
      "learning_rate": 0.00019077786688051325,
      "loss": 0.4602,
      "step": 120
    },
    {
      "epoch": 0.04841936774709884,
      "grad_norm": 0.1128101721405983,
      "learning_rate": 0.00019069767441860466,
      "loss": 0.4863,
      "step": 121
    },
    {
      "epoch": 0.04881952781112445,
      "grad_norm": 0.13223111629486084,
      "learning_rate": 0.0001906174819566961,
      "loss": 0.5131,
      "step": 122
    },
    {
      "epoch": 0.04921968787515006,
      "grad_norm": 0.15044215321540833,
      "learning_rate": 0.0001905372894947875,
      "loss": 0.5166,
      "step": 123
    },
    {
      "epoch": 0.04961984793917567,
      "grad_norm": 0.17137274146080017,
      "learning_rate": 0.00019045709703287893,
      "loss": 0.5792,
      "step": 124
    },
    {
      "epoch": 0.05002000800320128,
      "grad_norm": 0.14729808270931244,
      "learning_rate": 0.00019037690457097031,
      "loss": 0.4801,
      "step": 125
    },
    {
      "epoch": 0.05042016806722689,
      "grad_norm": 0.14504823088645935,
      "learning_rate": 0.00019029671210906175,
      "loss": 0.5508,
      "step": 126
    },
    {
      "epoch": 0.0508203281312525,
      "grad_norm": 0.15063397586345673,
      "learning_rate": 0.00019021651964715316,
      "loss": 0.4911,
      "step": 127
    },
    {
      "epoch": 0.051220488195278115,
      "grad_norm": 0.11778245866298676,
      "learning_rate": 0.0001901363271852446,
      "loss": 0.4418,
      "step": 128
    },
    {
      "epoch": 0.05162064825930372,
      "grad_norm": 0.1319017857313156,
      "learning_rate": 0.000190056134723336,
      "loss": 0.4512,
      "step": 129
    },
    {
      "epoch": 0.05202080832332933,
      "grad_norm": 0.12440775334835052,
      "learning_rate": 0.00018997594226142744,
      "loss": 0.426,
      "step": 130
    },
    {
      "epoch": 0.052420968387354945,
      "grad_norm": 0.12055116146802902,
      "learning_rate": 0.00018989574979951884,
      "loss": 0.469,
      "step": 131
    },
    {
      "epoch": 0.05282112845138055,
      "grad_norm": 0.10702915489673615,
      "learning_rate": 0.00018981555733761028,
      "loss": 0.4418,
      "step": 132
    },
    {
      "epoch": 0.05322128851540616,
      "grad_norm": 0.12964826822280884,
      "learning_rate": 0.00018973536487570169,
      "loss": 0.367,
      "step": 133
    },
    {
      "epoch": 0.053621448579431776,
      "grad_norm": 0.141257643699646,
      "learning_rate": 0.00018965517241379312,
      "loss": 0.505,
      "step": 134
    },
    {
      "epoch": 0.05402160864345738,
      "grad_norm": 0.15194900333881378,
      "learning_rate": 0.00018957497995188453,
      "loss": 0.4828,
      "step": 135
    },
    {
      "epoch": 0.05442176870748299,
      "grad_norm": 0.10905466973781586,
      "learning_rate": 0.00018949478748997596,
      "loss": 0.3756,
      "step": 136
    },
    {
      "epoch": 0.054821928771508606,
      "grad_norm": 0.1474345475435257,
      "learning_rate": 0.00018941459502806737,
      "loss": 0.4389,
      "step": 137
    },
    {
      "epoch": 0.055222088835534214,
      "grad_norm": 0.15416045486927032,
      "learning_rate": 0.0001893344025661588,
      "loss": 0.4676,
      "step": 138
    },
    {
      "epoch": 0.05562224889955982,
      "grad_norm": 0.12918829917907715,
      "learning_rate": 0.00018925421010425022,
      "loss": 0.4654,
      "step": 139
    },
    {
      "epoch": 0.056022408963585436,
      "grad_norm": 0.14582864940166473,
      "learning_rate": 0.00018917401764234165,
      "loss": 0.4925,
      "step": 140
    },
    {
      "epoch": 0.056422569027611044,
      "grad_norm": 0.1267489343881607,
      "learning_rate": 0.00018909382518043303,
      "loss": 0.4228,
      "step": 141
    },
    {
      "epoch": 0.05682272909163665,
      "grad_norm": 0.11785811185836792,
      "learning_rate": 0.00018901363271852447,
      "loss": 0.4243,
      "step": 142
    },
    {
      "epoch": 0.05722288915566227,
      "grad_norm": 0.14111536741256714,
      "learning_rate": 0.00018893344025661587,
      "loss": 0.4538,
      "step": 143
    },
    {
      "epoch": 0.057623049219687875,
      "grad_norm": 0.14035436511039734,
      "learning_rate": 0.0001888532477947073,
      "loss": 0.5006,
      "step": 144
    },
    {
      "epoch": 0.05802320928371348,
      "grad_norm": 0.1330406367778778,
      "learning_rate": 0.00018877305533279872,
      "loss": 0.4405,
      "step": 145
    },
    {
      "epoch": 0.0584233693477391,
      "grad_norm": 0.12746116518974304,
      "learning_rate": 0.00018869286287089015,
      "loss": 0.4879,
      "step": 146
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.16939005255699158,
      "learning_rate": 0.00018861267040898156,
      "loss": 0.5499,
      "step": 147
    },
    {
      "epoch": 0.05922368947579031,
      "grad_norm": 0.14871060848236084,
      "learning_rate": 0.000188532477947073,
      "loss": 0.5435,
      "step": 148
    },
    {
      "epoch": 0.05962384953981593,
      "grad_norm": 0.14285235106945038,
      "learning_rate": 0.0001884522854851644,
      "loss": 0.5179,
      "step": 149
    },
    {
      "epoch": 0.060024009603841535,
      "grad_norm": 0.13702325522899628,
      "learning_rate": 0.00018837209302325584,
      "loss": 0.4719,
      "step": 150
    },
    {
      "epoch": 0.06042416966786715,
      "grad_norm": 0.13655483722686768,
      "learning_rate": 0.00018829190056134725,
      "loss": 0.4643,
      "step": 151
    },
    {
      "epoch": 0.06082432973189276,
      "grad_norm": 0.10196033865213394,
      "learning_rate": 0.00018821170809943865,
      "loss": 0.3487,
      "step": 152
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 0.1465970277786255,
      "learning_rate": 0.0001881315156375301,
      "loss": 0.5418,
      "step": 153
    },
    {
      "epoch": 0.06162464985994398,
      "grad_norm": 0.09615792334079742,
      "learning_rate": 0.0001880513231756215,
      "loss": 0.4011,
      "step": 154
    },
    {
      "epoch": 0.06202480992396959,
      "grad_norm": 0.1325608491897583,
      "learning_rate": 0.00018797113071371293,
      "loss": 0.4912,
      "step": 155
    },
    {
      "epoch": 0.062424969987995196,
      "grad_norm": 0.1483178287744522,
      "learning_rate": 0.00018789093825180434,
      "loss": 0.5028,
      "step": 156
    },
    {
      "epoch": 0.06282513005202081,
      "grad_norm": 0.12851116061210632,
      "learning_rate": 0.00018781074578989575,
      "loss": 0.4656,
      "step": 157
    },
    {
      "epoch": 0.06322529011604641,
      "grad_norm": 0.10209093987941742,
      "learning_rate": 0.00018773055332798716,
      "loss": 0.4995,
      "step": 158
    },
    {
      "epoch": 0.06362545018007203,
      "grad_norm": 0.13402999937534332,
      "learning_rate": 0.0001876503608660786,
      "loss": 0.4858,
      "step": 159
    },
    {
      "epoch": 0.06402561024409764,
      "grad_norm": 0.14808642864227295,
      "learning_rate": 0.00018757016840417,
      "loss": 0.4868,
      "step": 160
    },
    {
      "epoch": 0.06442577030812324,
      "grad_norm": 0.13608327507972717,
      "learning_rate": 0.00018748997594226143,
      "loss": 0.5324,
      "step": 161
    },
    {
      "epoch": 0.06482593037214886,
      "grad_norm": 0.1304795891046524,
      "learning_rate": 0.00018740978348035284,
      "loss": 0.4693,
      "step": 162
    },
    {
      "epoch": 0.06522609043617447,
      "grad_norm": 0.13183492422103882,
      "learning_rate": 0.00018732959101844428,
      "loss": 0.4127,
      "step": 163
    },
    {
      "epoch": 0.06562625050020009,
      "grad_norm": 0.13964495062828064,
      "learning_rate": 0.00018724939855653568,
      "loss": 0.5497,
      "step": 164
    },
    {
      "epoch": 0.06602641056422569,
      "grad_norm": 0.12011177837848663,
      "learning_rate": 0.00018716920609462712,
      "loss": 0.512,
      "step": 165
    },
    {
      "epoch": 0.0664265706282513,
      "grad_norm": 0.19295620918273926,
      "learning_rate": 0.00018708901363271853,
      "loss": 0.5392,
      "step": 166
    },
    {
      "epoch": 0.06682673069227692,
      "grad_norm": 0.13760393857955933,
      "learning_rate": 0.00018700882117080996,
      "loss": 0.4647,
      "step": 167
    },
    {
      "epoch": 0.06722689075630252,
      "grad_norm": 0.0975688174366951,
      "learning_rate": 0.00018692862870890137,
      "loss": 0.4718,
      "step": 168
    },
    {
      "epoch": 0.06762705082032813,
      "grad_norm": 0.10798799991607666,
      "learning_rate": 0.0001868484362469928,
      "loss": 0.4052,
      "step": 169
    },
    {
      "epoch": 0.06802721088435375,
      "grad_norm": 0.15086914598941803,
      "learning_rate": 0.0001867682437850842,
      "loss": 0.531,
      "step": 170
    },
    {
      "epoch": 0.06842737094837935,
      "grad_norm": 0.10849977284669876,
      "learning_rate": 0.00018668805132317565,
      "loss": 0.4254,
      "step": 171
    },
    {
      "epoch": 0.06882753101240496,
      "grad_norm": 0.11629898101091385,
      "learning_rate": 0.00018660785886126706,
      "loss": 0.4436,
      "step": 172
    },
    {
      "epoch": 0.06922769107643058,
      "grad_norm": 0.1284409463405609,
      "learning_rate": 0.00018652766639935846,
      "loss": 0.5305,
      "step": 173
    },
    {
      "epoch": 0.06962785114045618,
      "grad_norm": 0.129461869597435,
      "learning_rate": 0.00018644747393744987,
      "loss": 0.5353,
      "step": 174
    },
    {
      "epoch": 0.0700280112044818,
      "grad_norm": 0.14519193768501282,
      "learning_rate": 0.0001863672814755413,
      "loss": 0.5614,
      "step": 175
    },
    {
      "epoch": 0.07042817126850741,
      "grad_norm": 0.1073053851723671,
      "learning_rate": 0.00018628708901363271,
      "loss": 0.4797,
      "step": 176
    },
    {
      "epoch": 0.07082833133253301,
      "grad_norm": 0.11377490311861038,
      "learning_rate": 0.00018620689655172415,
      "loss": 0.5087,
      "step": 177
    },
    {
      "epoch": 0.07122849139655862,
      "grad_norm": 0.12268966436386108,
      "learning_rate": 0.00018612670408981556,
      "loss": 0.4885,
      "step": 178
    },
    {
      "epoch": 0.07162865146058424,
      "grad_norm": 0.14001303911209106,
      "learning_rate": 0.000186046511627907,
      "loss": 0.4721,
      "step": 179
    },
    {
      "epoch": 0.07202881152460984,
      "grad_norm": 0.1424003541469574,
      "learning_rate": 0.0001859663191659984,
      "loss": 0.4694,
      "step": 180
    },
    {
      "epoch": 0.07242897158863545,
      "grad_norm": 0.15293265879154205,
      "learning_rate": 0.00018588612670408984,
      "loss": 0.5452,
      "step": 181
    },
    {
      "epoch": 0.07282913165266107,
      "grad_norm": 0.1378236711025238,
      "learning_rate": 0.00018580593424218124,
      "loss": 0.5064,
      "step": 182
    },
    {
      "epoch": 0.07322929171668667,
      "grad_norm": 0.12121786177158356,
      "learning_rate": 0.00018572574178027268,
      "loss": 0.453,
      "step": 183
    },
    {
      "epoch": 0.07362945178071229,
      "grad_norm": 0.11671614646911621,
      "learning_rate": 0.00018564554931836409,
      "loss": 0.4374,
      "step": 184
    },
    {
      "epoch": 0.0740296118447379,
      "grad_norm": 0.10158341377973557,
      "learning_rate": 0.0001855653568564555,
      "loss": 0.4552,
      "step": 185
    },
    {
      "epoch": 0.0744297719087635,
      "grad_norm": 0.15136249363422394,
      "learning_rate": 0.00018548516439454693,
      "loss": 0.4898,
      "step": 186
    },
    {
      "epoch": 0.07482993197278912,
      "grad_norm": 0.12793879210948944,
      "learning_rate": 0.00018540497193263834,
      "loss": 0.415,
      "step": 187
    },
    {
      "epoch": 0.07523009203681473,
      "grad_norm": 0.12706665694713593,
      "learning_rate": 0.00018532477947072977,
      "loss": 0.4537,
      "step": 188
    },
    {
      "epoch": 0.07563025210084033,
      "grad_norm": 0.16407187283039093,
      "learning_rate": 0.00018524458700882118,
      "loss": 0.5277,
      "step": 189
    },
    {
      "epoch": 0.07603041216486595,
      "grad_norm": 0.14706821739673615,
      "learning_rate": 0.0001851643945469126,
      "loss": 0.3744,
      "step": 190
    },
    {
      "epoch": 0.07643057222889156,
      "grad_norm": 0.11390958726406097,
      "learning_rate": 0.00018508420208500402,
      "loss": 0.4619,
      "step": 191
    },
    {
      "epoch": 0.07683073229291716,
      "grad_norm": 0.12826699018478394,
      "learning_rate": 0.00018500400962309543,
      "loss": 0.4003,
      "step": 192
    },
    {
      "epoch": 0.07723089235694278,
      "grad_norm": 0.12163887172937393,
      "learning_rate": 0.00018492381716118684,
      "loss": 0.5077,
      "step": 193
    },
    {
      "epoch": 0.07763105242096839,
      "grad_norm": 0.09692992269992828,
      "learning_rate": 0.00018484362469927827,
      "loss": 0.4123,
      "step": 194
    },
    {
      "epoch": 0.07803121248499399,
      "grad_norm": 0.11546153575181961,
      "learning_rate": 0.00018476343223736968,
      "loss": 0.4667,
      "step": 195
    },
    {
      "epoch": 0.0784313725490196,
      "grad_norm": 0.12225581705570221,
      "learning_rate": 0.00018468323977546112,
      "loss": 0.5018,
      "step": 196
    },
    {
      "epoch": 0.07883153261304522,
      "grad_norm": 0.12957295775413513,
      "learning_rate": 0.00018460304731355252,
      "loss": 0.4366,
      "step": 197
    },
    {
      "epoch": 0.07923169267707082,
      "grad_norm": 0.09262481331825256,
      "learning_rate": 0.00018452285485164396,
      "loss": 0.3721,
      "step": 198
    },
    {
      "epoch": 0.07963185274109644,
      "grad_norm": 0.10136529058218002,
      "learning_rate": 0.00018444266238973537,
      "loss": 0.4445,
      "step": 199
    },
    {
      "epoch": 0.08003201280512205,
      "grad_norm": 0.11607563495635986,
      "learning_rate": 0.0001843624699278268,
      "loss": 0.4583,
      "step": 200
    },
    {
      "epoch": 0.08043217286914765,
      "grad_norm": 0.13737931847572327,
      "learning_rate": 0.0001842822774659182,
      "loss": 0.5201,
      "step": 201
    },
    {
      "epoch": 0.08083233293317327,
      "grad_norm": 0.08676411956548691,
      "learning_rate": 0.00018420208500400965,
      "loss": 0.4234,
      "step": 202
    },
    {
      "epoch": 0.08123249299719888,
      "grad_norm": 0.10479661077260971,
      "learning_rate": 0.00018412189254210105,
      "loss": 0.3939,
      "step": 203
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.10750117152929306,
      "learning_rate": 0.0001840417000801925,
      "loss": 0.3755,
      "step": 204
    },
    {
      "epoch": 0.0820328131252501,
      "grad_norm": 0.14931219816207886,
      "learning_rate": 0.00018396150761828387,
      "loss": 0.5099,
      "step": 205
    },
    {
      "epoch": 0.08243297318927571,
      "grad_norm": 0.11829051375389099,
      "learning_rate": 0.0001838813151563753,
      "loss": 0.4346,
      "step": 206
    },
    {
      "epoch": 0.08283313325330131,
      "grad_norm": 0.11502698063850403,
      "learning_rate": 0.0001838011226944667,
      "loss": 0.4818,
      "step": 207
    },
    {
      "epoch": 0.08323329331732693,
      "grad_norm": 0.10770168900489807,
      "learning_rate": 0.00018372093023255815,
      "loss": 0.4135,
      "step": 208
    },
    {
      "epoch": 0.08363345338135254,
      "grad_norm": 0.12374719977378845,
      "learning_rate": 0.00018364073777064956,
      "loss": 0.5234,
      "step": 209
    },
    {
      "epoch": 0.08403361344537816,
      "grad_norm": 0.13255833089351654,
      "learning_rate": 0.000183560545308741,
      "loss": 0.4704,
      "step": 210
    },
    {
      "epoch": 0.08443377350940376,
      "grad_norm": 0.16977503895759583,
      "learning_rate": 0.0001834803528468324,
      "loss": 0.5657,
      "step": 211
    },
    {
      "epoch": 0.08483393357342937,
      "grad_norm": 0.12468747049570084,
      "learning_rate": 0.00018340016038492383,
      "loss": 0.489,
      "step": 212
    },
    {
      "epoch": 0.08523409363745499,
      "grad_norm": 0.12520770728588104,
      "learning_rate": 0.00018331996792301524,
      "loss": 0.4905,
      "step": 213
    },
    {
      "epoch": 0.08563425370148059,
      "grad_norm": 0.13210442662239075,
      "learning_rate": 0.00018323977546110668,
      "loss": 0.4607,
      "step": 214
    },
    {
      "epoch": 0.0860344137655062,
      "grad_norm": 0.1425142139196396,
      "learning_rate": 0.00018315958299919808,
      "loss": 0.4508,
      "step": 215
    },
    {
      "epoch": 0.08643457382953182,
      "grad_norm": 0.11364100128412247,
      "learning_rate": 0.00018307939053728952,
      "loss": 0.4831,
      "step": 216
    },
    {
      "epoch": 0.08683473389355742,
      "grad_norm": 0.13135170936584473,
      "learning_rate": 0.00018299919807538093,
      "loss": 0.4568,
      "step": 217
    },
    {
      "epoch": 0.08723489395758303,
      "grad_norm": 0.13805334270000458,
      "learning_rate": 0.00018291900561347236,
      "loss": 0.5375,
      "step": 218
    },
    {
      "epoch": 0.08763505402160865,
      "grad_norm": 0.14687907695770264,
      "learning_rate": 0.00018283881315156377,
      "loss": 0.5156,
      "step": 219
    },
    {
      "epoch": 0.08803521408563425,
      "grad_norm": 0.11880427598953247,
      "learning_rate": 0.00018275862068965518,
      "loss": 0.4659,
      "step": 220
    },
    {
      "epoch": 0.08843537414965986,
      "grad_norm": 0.13191868364810944,
      "learning_rate": 0.00018267842822774659,
      "loss": 0.4912,
      "step": 221
    },
    {
      "epoch": 0.08883553421368548,
      "grad_norm": 0.13169026374816895,
      "learning_rate": 0.00018259823576583802,
      "loss": 0.4472,
      "step": 222
    },
    {
      "epoch": 0.08923569427771108,
      "grad_norm": 0.129400372505188,
      "learning_rate": 0.00018251804330392943,
      "loss": 0.5469,
      "step": 223
    },
    {
      "epoch": 0.0896358543417367,
      "grad_norm": 0.10501869767904282,
      "learning_rate": 0.00018243785084202086,
      "loss": 0.4962,
      "step": 224
    },
    {
      "epoch": 0.09003601440576231,
      "grad_norm": 0.142854705452919,
      "learning_rate": 0.00018235765838011227,
      "loss": 0.4244,
      "step": 225
    },
    {
      "epoch": 0.09043617446978791,
      "grad_norm": 0.12474144995212555,
      "learning_rate": 0.0001822774659182037,
      "loss": 0.5021,
      "step": 226
    },
    {
      "epoch": 0.09083633453381353,
      "grad_norm": 0.12875868380069733,
      "learning_rate": 0.00018219727345629511,
      "loss": 0.506,
      "step": 227
    },
    {
      "epoch": 0.09123649459783914,
      "grad_norm": 0.12214354425668716,
      "learning_rate": 0.00018211708099438652,
      "loss": 0.4917,
      "step": 228
    },
    {
      "epoch": 0.09163665466186474,
      "grad_norm": 0.12947089970111847,
      "learning_rate": 0.00018203688853247796,
      "loss": 0.519,
      "step": 229
    },
    {
      "epoch": 0.09203681472589036,
      "grad_norm": 0.11945495754480362,
      "learning_rate": 0.00018195669607056937,
      "loss": 0.3956,
      "step": 230
    },
    {
      "epoch": 0.09243697478991597,
      "grad_norm": 0.11920713633298874,
      "learning_rate": 0.0001818765036086608,
      "loss": 0.4845,
      "step": 231
    },
    {
      "epoch": 0.09283713485394157,
      "grad_norm": 0.09881073236465454,
      "learning_rate": 0.0001817963111467522,
      "loss": 0.3795,
      "step": 232
    },
    {
      "epoch": 0.09323729491796719,
      "grad_norm": 0.1333152800798416,
      "learning_rate": 0.00018171611868484364,
      "loss": 0.4739,
      "step": 233
    },
    {
      "epoch": 0.0936374549819928,
      "grad_norm": 0.21321454644203186,
      "learning_rate": 0.00018163592622293505,
      "loss": 0.5071,
      "step": 234
    },
    {
      "epoch": 0.0940376150460184,
      "grad_norm": 0.10220582038164139,
      "learning_rate": 0.00018155573376102649,
      "loss": 0.3969,
      "step": 235
    },
    {
      "epoch": 0.09443777511004402,
      "grad_norm": 0.1244305744767189,
      "learning_rate": 0.0001814755412991179,
      "loss": 0.5034,
      "step": 236
    },
    {
      "epoch": 0.09483793517406963,
      "grad_norm": 0.15226584672927856,
      "learning_rate": 0.0001813953488372093,
      "loss": 0.4636,
      "step": 237
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.11543595045804977,
      "learning_rate": 0.0001813151563753007,
      "loss": 0.4607,
      "step": 238
    },
    {
      "epoch": 0.09563825530212085,
      "grad_norm": 0.13535559177398682,
      "learning_rate": 0.00018123496391339215,
      "loss": 0.4461,
      "step": 239
    },
    {
      "epoch": 0.09603841536614646,
      "grad_norm": 0.11766539514064789,
      "learning_rate": 0.00018115477145148355,
      "loss": 0.4675,
      "step": 240
    },
    {
      "epoch": 0.09643857543017206,
      "grad_norm": 0.11271712929010391,
      "learning_rate": 0.000181074578989575,
      "loss": 0.4652,
      "step": 241
    },
    {
      "epoch": 0.09683873549419768,
      "grad_norm": 0.13060280680656433,
      "learning_rate": 0.0001809943865276664,
      "loss": 0.4804,
      "step": 242
    },
    {
      "epoch": 0.09723889555822329,
      "grad_norm": 0.12125249952077866,
      "learning_rate": 0.00018091419406575783,
      "loss": 0.3948,
      "step": 243
    },
    {
      "epoch": 0.0976390556222489,
      "grad_norm": 0.13155825436115265,
      "learning_rate": 0.00018083400160384924,
      "loss": 0.4633,
      "step": 244
    },
    {
      "epoch": 0.09803921568627451,
      "grad_norm": 0.09374501556158066,
      "learning_rate": 0.00018075380914194067,
      "loss": 0.4154,
      "step": 245
    },
    {
      "epoch": 0.09843937575030012,
      "grad_norm": 0.09847457706928253,
      "learning_rate": 0.00018067361668003208,
      "loss": 0.354,
      "step": 246
    },
    {
      "epoch": 0.09883953581432572,
      "grad_norm": 0.11667906492948532,
      "learning_rate": 0.00018059342421812352,
      "loss": 0.4854,
      "step": 247
    },
    {
      "epoch": 0.09923969587835134,
      "grad_norm": 0.16436767578125,
      "learning_rate": 0.00018051323175621492,
      "loss": 0.466,
      "step": 248
    },
    {
      "epoch": 0.09963985594237695,
      "grad_norm": 0.12961305677890778,
      "learning_rate": 0.00018043303929430636,
      "loss": 0.4584,
      "step": 249
    },
    {
      "epoch": 0.10004001600640255,
      "grad_norm": 0.1056259423494339,
      "learning_rate": 0.00018035284683239777,
      "loss": 0.3701,
      "step": 250
    },
    {
      "epoch": 0.10044017607042817,
      "grad_norm": 0.10037931799888611,
      "learning_rate": 0.0001802726543704892,
      "loss": 0.5,
      "step": 251
    },
    {
      "epoch": 0.10084033613445378,
      "grad_norm": 0.09612792730331421,
      "learning_rate": 0.0001801924619085806,
      "loss": 0.3741,
      "step": 252
    },
    {
      "epoch": 0.10124049619847938,
      "grad_norm": 0.1512654423713684,
      "learning_rate": 0.00018011226944667202,
      "loss": 0.5165,
      "step": 253
    },
    {
      "epoch": 0.101640656262505,
      "grad_norm": 0.10586408525705338,
      "learning_rate": 0.00018003207698476343,
      "loss": 0.3451,
      "step": 254
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 0.12365169078111649,
      "learning_rate": 0.00017995188452285486,
      "loss": 0.4748,
      "step": 255
    },
    {
      "epoch": 0.10244097639055623,
      "grad_norm": 0.10974916815757751,
      "learning_rate": 0.00017987169206094627,
      "loss": 0.4835,
      "step": 256
    },
    {
      "epoch": 0.10284113645458183,
      "grad_norm": 0.14079660177230835,
      "learning_rate": 0.0001797914995990377,
      "loss": 0.4903,
      "step": 257
    },
    {
      "epoch": 0.10324129651860744,
      "grad_norm": 0.09693942964076996,
      "learning_rate": 0.0001797113071371291,
      "loss": 0.4342,
      "step": 258
    },
    {
      "epoch": 0.10364145658263306,
      "grad_norm": 0.10766295343637466,
      "learning_rate": 0.00017963111467522055,
      "loss": 0.459,
      "step": 259
    },
    {
      "epoch": 0.10404161664665866,
      "grad_norm": 0.11035313457250595,
      "learning_rate": 0.00017955092221331196,
      "loss": 0.4589,
      "step": 260
    },
    {
      "epoch": 0.10444177671068428,
      "grad_norm": 0.10679485648870468,
      "learning_rate": 0.0001794707297514034,
      "loss": 0.4398,
      "step": 261
    },
    {
      "epoch": 0.10484193677470989,
      "grad_norm": 0.10394807159900665,
      "learning_rate": 0.0001793905372894948,
      "loss": 0.4935,
      "step": 262
    },
    {
      "epoch": 0.10524209683873549,
      "grad_norm": 0.13123735785484314,
      "learning_rate": 0.0001793103448275862,
      "loss": 0.5435,
      "step": 263
    },
    {
      "epoch": 0.1056422569027611,
      "grad_norm": 0.1056336984038353,
      "learning_rate": 0.00017923015236567764,
      "loss": 0.3922,
      "step": 264
    },
    {
      "epoch": 0.10604241696678672,
      "grad_norm": 0.11917416006326675,
      "learning_rate": 0.00017914995990376905,
      "loss": 0.5228,
      "step": 265
    },
    {
      "epoch": 0.10644257703081232,
      "grad_norm": 0.12100663036108017,
      "learning_rate": 0.00017906976744186048,
      "loss": 0.5714,
      "step": 266
    },
    {
      "epoch": 0.10684273709483794,
      "grad_norm": 0.11408606171607971,
      "learning_rate": 0.0001789895749799519,
      "loss": 0.4862,
      "step": 267
    },
    {
      "epoch": 0.10724289715886355,
      "grad_norm": 0.1179388090968132,
      "learning_rate": 0.00017890938251804333,
      "loss": 0.4293,
      "step": 268
    },
    {
      "epoch": 0.10764305722288915,
      "grad_norm": 0.12212846428155899,
      "learning_rate": 0.0001788291900561347,
      "loss": 0.4425,
      "step": 269
    },
    {
      "epoch": 0.10804321728691477,
      "grad_norm": 0.12375491112470627,
      "learning_rate": 0.00017874899759422614,
      "loss": 0.4792,
      "step": 270
    },
    {
      "epoch": 0.10844337735094038,
      "grad_norm": 0.15605853497982025,
      "learning_rate": 0.00017866880513231755,
      "loss": 0.5188,
      "step": 271
    },
    {
      "epoch": 0.10884353741496598,
      "grad_norm": 0.10675527155399323,
      "learning_rate": 0.00017858861267040899,
      "loss": 0.4343,
      "step": 272
    },
    {
      "epoch": 0.1092436974789916,
      "grad_norm": 0.1305278092622757,
      "learning_rate": 0.0001785084202085004,
      "loss": 0.5762,
      "step": 273
    },
    {
      "epoch": 0.10964385754301721,
      "grad_norm": 0.1937091052532196,
      "learning_rate": 0.00017842822774659183,
      "loss": 0.5146,
      "step": 274
    },
    {
      "epoch": 0.11004401760704281,
      "grad_norm": 0.09900829941034317,
      "learning_rate": 0.00017834803528468324,
      "loss": 0.4083,
      "step": 275
    },
    {
      "epoch": 0.11044417767106843,
      "grad_norm": 0.14200322329998016,
      "learning_rate": 0.00017826784282277467,
      "loss": 0.4381,
      "step": 276
    },
    {
      "epoch": 0.11084433773509404,
      "grad_norm": 0.14532408118247986,
      "learning_rate": 0.00017818765036086608,
      "loss": 0.465,
      "step": 277
    },
    {
      "epoch": 0.11124449779911964,
      "grad_norm": 0.12740176916122437,
      "learning_rate": 0.00017810745789895751,
      "loss": 0.4958,
      "step": 278
    },
    {
      "epoch": 0.11164465786314526,
      "grad_norm": 0.08783189207315445,
      "learning_rate": 0.00017802726543704892,
      "loss": 0.393,
      "step": 279
    },
    {
      "epoch": 0.11204481792717087,
      "grad_norm": 0.11317205429077148,
      "learning_rate": 0.00017794707297514036,
      "loss": 0.5153,
      "step": 280
    },
    {
      "epoch": 0.11244497799119647,
      "grad_norm": 0.10008110105991364,
      "learning_rate": 0.00017786688051323177,
      "loss": 0.4835,
      "step": 281
    },
    {
      "epoch": 0.11284513805522209,
      "grad_norm": 0.11175697296857834,
      "learning_rate": 0.0001777866880513232,
      "loss": 0.4736,
      "step": 282
    },
    {
      "epoch": 0.1132452981192477,
      "grad_norm": 0.1468551605939865,
      "learning_rate": 0.0001777064955894146,
      "loss": 0.401,
      "step": 283
    },
    {
      "epoch": 0.1136454581832733,
      "grad_norm": 0.13485932350158691,
      "learning_rate": 0.00017762630312750604,
      "loss": 0.543,
      "step": 284
    },
    {
      "epoch": 0.11404561824729892,
      "grad_norm": 0.11521459370851517,
      "learning_rate": 0.00017754611066559742,
      "loss": 0.4094,
      "step": 285
    },
    {
      "epoch": 0.11444577831132453,
      "grad_norm": 0.11055447161197662,
      "learning_rate": 0.00017746591820368886,
      "loss": 0.4408,
      "step": 286
    },
    {
      "epoch": 0.11484593837535013,
      "grad_norm": 0.10583039373159409,
      "learning_rate": 0.00017738572574178027,
      "loss": 0.4295,
      "step": 287
    },
    {
      "epoch": 0.11524609843937575,
      "grad_norm": 0.12624934315681458,
      "learning_rate": 0.0001773055332798717,
      "loss": 0.5609,
      "step": 288
    },
    {
      "epoch": 0.11564625850340136,
      "grad_norm": 0.11090149730443954,
      "learning_rate": 0.0001772253408179631,
      "loss": 0.4221,
      "step": 289
    },
    {
      "epoch": 0.11604641856742696,
      "grad_norm": 0.11755429953336716,
      "learning_rate": 0.00017714514835605455,
      "loss": 0.4751,
      "step": 290
    },
    {
      "epoch": 0.11644657863145258,
      "grad_norm": 0.13662026822566986,
      "learning_rate": 0.00017706495589414595,
      "loss": 0.3998,
      "step": 291
    },
    {
      "epoch": 0.1168467386954782,
      "grad_norm": 0.13170164823532104,
      "learning_rate": 0.0001769847634322374,
      "loss": 0.5786,
      "step": 292
    },
    {
      "epoch": 0.1172468987595038,
      "grad_norm": 0.12707193195819855,
      "learning_rate": 0.0001769045709703288,
      "loss": 0.5697,
      "step": 293
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.1291714608669281,
      "learning_rate": 0.00017682437850842023,
      "loss": 0.4786,
      "step": 294
    },
    {
      "epoch": 0.11804721888755502,
      "grad_norm": 0.11673704534769058,
      "learning_rate": 0.00017674418604651164,
      "loss": 0.4205,
      "step": 295
    },
    {
      "epoch": 0.11844737895158063,
      "grad_norm": 0.1406221091747284,
      "learning_rate": 0.00017666399358460305,
      "loss": 0.4725,
      "step": 296
    },
    {
      "epoch": 0.11884753901560624,
      "grad_norm": 0.10736479610204697,
      "learning_rate": 0.00017658380112269448,
      "loss": 0.4096,
      "step": 297
    },
    {
      "epoch": 0.11924769907963186,
      "grad_norm": 0.1359228491783142,
      "learning_rate": 0.0001765036086607859,
      "loss": 0.4967,
      "step": 298
    },
    {
      "epoch": 0.11964785914365746,
      "grad_norm": 0.10314254462718964,
      "learning_rate": 0.00017642341619887732,
      "loss": 0.3891,
      "step": 299
    },
    {
      "epoch": 0.12004801920768307,
      "grad_norm": 0.1572863757610321,
      "learning_rate": 0.00017634322373696873,
      "loss": 0.5153,
      "step": 300
    },
    {
      "epoch": 0.12044817927170869,
      "grad_norm": 0.11621096730232239,
      "learning_rate": 0.00017626303127506014,
      "loss": 0.4936,
      "step": 301
    },
    {
      "epoch": 0.1208483393357343,
      "grad_norm": 0.12380082160234451,
      "learning_rate": 0.00017618283881315158,
      "loss": 0.3939,
      "step": 302
    },
    {
      "epoch": 0.1212484993997599,
      "grad_norm": 0.13955582678318024,
      "learning_rate": 0.00017610264635124298,
      "loss": 0.521,
      "step": 303
    },
    {
      "epoch": 0.12164865946378552,
      "grad_norm": 0.12239433079957962,
      "learning_rate": 0.0001760224538893344,
      "loss": 0.497,
      "step": 304
    },
    {
      "epoch": 0.12204881952781113,
      "grad_norm": 0.11798510700464249,
      "learning_rate": 0.00017594226142742583,
      "loss": 0.4406,
      "step": 305
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 0.12374099344015121,
      "learning_rate": 0.00017586206896551723,
      "loss": 0.4515,
      "step": 306
    },
    {
      "epoch": 0.12284913965586235,
      "grad_norm": 0.09981679171323776,
      "learning_rate": 0.00017578187650360867,
      "loss": 0.3844,
      "step": 307
    },
    {
      "epoch": 0.12324929971988796,
      "grad_norm": 0.12497119605541229,
      "learning_rate": 0.00017570168404170008,
      "loss": 0.5248,
      "step": 308
    },
    {
      "epoch": 0.12364945978391356,
      "grad_norm": 0.11952691525220871,
      "learning_rate": 0.0001756214915797915,
      "loss": 0.5037,
      "step": 309
    },
    {
      "epoch": 0.12404961984793918,
      "grad_norm": 0.13054552674293518,
      "learning_rate": 0.00017554129911788292,
      "loss": 0.5707,
      "step": 310
    },
    {
      "epoch": 0.12444977991196479,
      "grad_norm": 0.11273949593305588,
      "learning_rate": 0.00017546110665597436,
      "loss": 0.4412,
      "step": 311
    },
    {
      "epoch": 0.12484993997599039,
      "grad_norm": 0.17011168599128723,
      "learning_rate": 0.00017538091419406576,
      "loss": 0.5804,
      "step": 312
    },
    {
      "epoch": 0.125250100040016,
      "grad_norm": 0.11116588115692139,
      "learning_rate": 0.0001753007217321572,
      "loss": 0.4479,
      "step": 313
    },
    {
      "epoch": 0.12565026010404162,
      "grad_norm": 0.14362746477127075,
      "learning_rate": 0.0001752205292702486,
      "loss": 0.5173,
      "step": 314
    },
    {
      "epoch": 0.12605042016806722,
      "grad_norm": 0.10549671947956085,
      "learning_rate": 0.00017514033680834004,
      "loss": 0.4456,
      "step": 315
    },
    {
      "epoch": 0.12645058023209282,
      "grad_norm": 0.12621377408504486,
      "learning_rate": 0.00017506014434643145,
      "loss": 0.5298,
      "step": 316
    },
    {
      "epoch": 0.12685074029611845,
      "grad_norm": 0.10526707023382187,
      "learning_rate": 0.00017497995188452286,
      "loss": 0.4735,
      "step": 317
    },
    {
      "epoch": 0.12725090036014405,
      "grad_norm": 0.12645408511161804,
      "learning_rate": 0.00017489975942261426,
      "loss": 0.4554,
      "step": 318
    },
    {
      "epoch": 0.12765106042416965,
      "grad_norm": 0.1174137145280838,
      "learning_rate": 0.0001748195669607057,
      "loss": 0.5211,
      "step": 319
    },
    {
      "epoch": 0.12805122048819528,
      "grad_norm": 0.1203165203332901,
      "learning_rate": 0.0001747393744987971,
      "loss": 0.4488,
      "step": 320
    },
    {
      "epoch": 0.12845138055222088,
      "grad_norm": 0.11035666614770889,
      "learning_rate": 0.00017465918203688854,
      "loss": 0.4815,
      "step": 321
    },
    {
      "epoch": 0.12885154061624648,
      "grad_norm": 0.12337521463632584,
      "learning_rate": 0.00017457898957497995,
      "loss": 0.4624,
      "step": 322
    },
    {
      "epoch": 0.1292517006802721,
      "grad_norm": 0.1361181139945984,
      "learning_rate": 0.00017449879711307139,
      "loss": 0.4383,
      "step": 323
    },
    {
      "epoch": 0.12965186074429771,
      "grad_norm": 0.11422773450613022,
      "learning_rate": 0.0001744186046511628,
      "loss": 0.4721,
      "step": 324
    },
    {
      "epoch": 0.13005202080832334,
      "grad_norm": 0.12968993186950684,
      "learning_rate": 0.00017433841218925423,
      "loss": 0.5627,
      "step": 325
    },
    {
      "epoch": 0.13045218087234894,
      "grad_norm": 0.13785965740680695,
      "learning_rate": 0.00017425821972734564,
      "loss": 0.5087,
      "step": 326
    },
    {
      "epoch": 0.13085234093637454,
      "grad_norm": 0.08839952945709229,
      "learning_rate": 0.00017417802726543707,
      "loss": 0.4361,
      "step": 327
    },
    {
      "epoch": 0.13125250100040017,
      "grad_norm": 0.12111512571573257,
      "learning_rate": 0.00017409783480352848,
      "loss": 0.5447,
      "step": 328
    },
    {
      "epoch": 0.13165266106442577,
      "grad_norm": 0.1203237771987915,
      "learning_rate": 0.00017401764234161991,
      "loss": 0.3946,
      "step": 329
    },
    {
      "epoch": 0.13205282112845138,
      "grad_norm": 0.1114087700843811,
      "learning_rate": 0.00017393744987971132,
      "loss": 0.4766,
      "step": 330
    },
    {
      "epoch": 0.132452981192477,
      "grad_norm": 0.1127285435795784,
      "learning_rate": 0.00017385725741780273,
      "loss": 0.4124,
      "step": 331
    },
    {
      "epoch": 0.1328531412565026,
      "grad_norm": 0.10884755849838257,
      "learning_rate": 0.00017377706495589417,
      "loss": 0.4588,
      "step": 332
    },
    {
      "epoch": 0.1332533013205282,
      "grad_norm": 0.10835112631320953,
      "learning_rate": 0.00017369687249398557,
      "loss": 0.4079,
      "step": 333
    },
    {
      "epoch": 0.13365346138455383,
      "grad_norm": 0.10405492037534714,
      "learning_rate": 0.00017361668003207698,
      "loss": 0.4032,
      "step": 334
    },
    {
      "epoch": 0.13405362144857944,
      "grad_norm": 0.14053793251514435,
      "learning_rate": 0.00017353648757016842,
      "loss": 0.5664,
      "step": 335
    },
    {
      "epoch": 0.13445378151260504,
      "grad_norm": 0.1297028511762619,
      "learning_rate": 0.00017345629510825982,
      "loss": 0.4983,
      "step": 336
    },
    {
      "epoch": 0.13485394157663066,
      "grad_norm": 0.1314513087272644,
      "learning_rate": 0.00017337610264635126,
      "loss": 0.4583,
      "step": 337
    },
    {
      "epoch": 0.13525410164065627,
      "grad_norm": 0.16063039004802704,
      "learning_rate": 0.00017329591018444267,
      "loss": 0.5617,
      "step": 338
    },
    {
      "epoch": 0.13565426170468187,
      "grad_norm": 0.13425534963607788,
      "learning_rate": 0.00017321571772253408,
      "loss": 0.4064,
      "step": 339
    },
    {
      "epoch": 0.1360544217687075,
      "grad_norm": 0.09534755349159241,
      "learning_rate": 0.0001731355252606255,
      "loss": 0.4619,
      "step": 340
    },
    {
      "epoch": 0.1364545818327331,
      "grad_norm": 0.12575189769268036,
      "learning_rate": 0.00017305533279871692,
      "loss": 0.4923,
      "step": 341
    },
    {
      "epoch": 0.1368547418967587,
      "grad_norm": 0.1369265913963318,
      "learning_rate": 0.00017297514033680835,
      "loss": 0.5129,
      "step": 342
    },
    {
      "epoch": 0.13725490196078433,
      "grad_norm": 0.11007128655910492,
      "learning_rate": 0.00017289494787489976,
      "loss": 0.4752,
      "step": 343
    },
    {
      "epoch": 0.13765506202480993,
      "grad_norm": 0.11221540719270706,
      "learning_rate": 0.0001728147554129912,
      "loss": 0.4917,
      "step": 344
    },
    {
      "epoch": 0.13805522208883553,
      "grad_norm": 0.11158888041973114,
      "learning_rate": 0.0001727345629510826,
      "loss": 0.5108,
      "step": 345
    },
    {
      "epoch": 0.13845538215286116,
      "grad_norm": 0.11136064678430557,
      "learning_rate": 0.00017265437048917404,
      "loss": 0.4805,
      "step": 346
    },
    {
      "epoch": 0.13885554221688676,
      "grad_norm": 0.1117139682173729,
      "learning_rate": 0.00017257417802726545,
      "loss": 0.4159,
      "step": 347
    },
    {
      "epoch": 0.13925570228091236,
      "grad_norm": 0.12443190813064575,
      "learning_rate": 0.00017249398556535688,
      "loss": 0.4529,
      "step": 348
    },
    {
      "epoch": 0.139655862344938,
      "grad_norm": 0.1258106380701065,
      "learning_rate": 0.00017241379310344826,
      "loss": 0.5022,
      "step": 349
    },
    {
      "epoch": 0.1400560224089636,
      "grad_norm": 0.11179228127002716,
      "learning_rate": 0.0001723336006415397,
      "loss": 0.4809,
      "step": 350
    },
    {
      "epoch": 0.1404561824729892,
      "grad_norm": 0.1076909527182579,
      "learning_rate": 0.0001722534081796311,
      "loss": 0.4603,
      "step": 351
    },
    {
      "epoch": 0.14085634253701482,
      "grad_norm": 0.11836329847574234,
      "learning_rate": 0.00017217321571772254,
      "loss": 0.4798,
      "step": 352
    },
    {
      "epoch": 0.14125650260104042,
      "grad_norm": 0.12494666874408722,
      "learning_rate": 0.00017209302325581395,
      "loss": 0.4897,
      "step": 353
    },
    {
      "epoch": 0.14165666266506602,
      "grad_norm": 0.13211901485919952,
      "learning_rate": 0.00017201283079390538,
      "loss": 0.5272,
      "step": 354
    },
    {
      "epoch": 0.14205682272909165,
      "grad_norm": 0.10374327003955841,
      "learning_rate": 0.0001719326383319968,
      "loss": 0.3856,
      "step": 355
    },
    {
      "epoch": 0.14245698279311725,
      "grad_norm": 0.10176452994346619,
      "learning_rate": 0.00017185244587008823,
      "loss": 0.4218,
      "step": 356
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.11028917878866196,
      "learning_rate": 0.00017177225340817963,
      "loss": 0.5246,
      "step": 357
    },
    {
      "epoch": 0.14325730292116848,
      "grad_norm": 0.09067730605602264,
      "learning_rate": 0.00017169206094627107,
      "loss": 0.3673,
      "step": 358
    },
    {
      "epoch": 0.14365746298519408,
      "grad_norm": 0.09684707969427109,
      "learning_rate": 0.00017161186848436248,
      "loss": 0.4441,
      "step": 359
    },
    {
      "epoch": 0.14405762304921968,
      "grad_norm": 0.1187102198600769,
      "learning_rate": 0.0001715316760224539,
      "loss": 0.4855,
      "step": 360
    },
    {
      "epoch": 0.1444577831132453,
      "grad_norm": 0.11125773936510086,
      "learning_rate": 0.00017145148356054532,
      "loss": 0.4581,
      "step": 361
    },
    {
      "epoch": 0.1448579431772709,
      "grad_norm": 0.13981030881404877,
      "learning_rate": 0.00017137129109863676,
      "loss": 0.5043,
      "step": 362
    },
    {
      "epoch": 0.1452581032412965,
      "grad_norm": 0.12813803553581238,
      "learning_rate": 0.00017129109863672816,
      "loss": 0.4848,
      "step": 363
    },
    {
      "epoch": 0.14565826330532214,
      "grad_norm": 0.08456263691186905,
      "learning_rate": 0.0001712109061748196,
      "loss": 0.3236,
      "step": 364
    },
    {
      "epoch": 0.14605842336934774,
      "grad_norm": 0.13357853889465332,
      "learning_rate": 0.00017113071371291098,
      "loss": 0.5086,
      "step": 365
    },
    {
      "epoch": 0.14645858343337334,
      "grad_norm": 0.11530838906764984,
      "learning_rate": 0.00017105052125100241,
      "loss": 0.5666,
      "step": 366
    },
    {
      "epoch": 0.14685874349739897,
      "grad_norm": 0.09979753196239471,
      "learning_rate": 0.00017097032878909382,
      "loss": 0.3689,
      "step": 367
    },
    {
      "epoch": 0.14725890356142457,
      "grad_norm": 0.1561024934053421,
      "learning_rate": 0.00017089013632718526,
      "loss": 0.4745,
      "step": 368
    },
    {
      "epoch": 0.14765906362545017,
      "grad_norm": 0.10678814351558685,
      "learning_rate": 0.00017080994386527666,
      "loss": 0.3846,
      "step": 369
    },
    {
      "epoch": 0.1480592236894758,
      "grad_norm": 0.0987798348069191,
      "learning_rate": 0.0001707297514033681,
      "loss": 0.3925,
      "step": 370
    },
    {
      "epoch": 0.1484593837535014,
      "grad_norm": 0.12061325460672379,
      "learning_rate": 0.0001706495589414595,
      "loss": 0.4914,
      "step": 371
    },
    {
      "epoch": 0.148859543817527,
      "grad_norm": 0.09760754555463791,
      "learning_rate": 0.00017056936647955094,
      "loss": 0.4775,
      "step": 372
    },
    {
      "epoch": 0.14925970388155263,
      "grad_norm": 0.09186648577451706,
      "learning_rate": 0.00017048917401764235,
      "loss": 0.4746,
      "step": 373
    },
    {
      "epoch": 0.14965986394557823,
      "grad_norm": 0.10475005954504013,
      "learning_rate": 0.00017040898155573376,
      "loss": 0.5157,
      "step": 374
    },
    {
      "epoch": 0.15006002400960383,
      "grad_norm": 0.11618597060441971,
      "learning_rate": 0.0001703287890938252,
      "loss": 0.5434,
      "step": 375
    },
    {
      "epoch": 0.15046018407362946,
      "grad_norm": 0.09750402718782425,
      "learning_rate": 0.0001702485966319166,
      "loss": 0.4448,
      "step": 376
    },
    {
      "epoch": 0.15086034413765506,
      "grad_norm": 0.09555304050445557,
      "learning_rate": 0.00017016840417000804,
      "loss": 0.484,
      "step": 377
    },
    {
      "epoch": 0.15126050420168066,
      "grad_norm": 0.10915882140398026,
      "learning_rate": 0.00017008821170809944,
      "loss": 0.4575,
      "step": 378
    },
    {
      "epoch": 0.1516606642657063,
      "grad_norm": 0.1095331460237503,
      "learning_rate": 0.00017000801924619088,
      "loss": 0.4696,
      "step": 379
    },
    {
      "epoch": 0.1520608243297319,
      "grad_norm": 0.10085665434598923,
      "learning_rate": 0.0001699278267842823,
      "loss": 0.4205,
      "step": 380
    },
    {
      "epoch": 0.1524609843937575,
      "grad_norm": 0.10788648575544357,
      "learning_rate": 0.0001698476343223737,
      "loss": 0.3818,
      "step": 381
    },
    {
      "epoch": 0.15286114445778312,
      "grad_norm": 0.12082894891500473,
      "learning_rate": 0.0001697674418604651,
      "loss": 0.5246,
      "step": 382
    },
    {
      "epoch": 0.15326130452180872,
      "grad_norm": 0.10244151949882507,
      "learning_rate": 0.00016968724939855654,
      "loss": 0.4348,
      "step": 383
    },
    {
      "epoch": 0.15366146458583432,
      "grad_norm": 0.09938404709100723,
      "learning_rate": 0.00016960705693664795,
      "loss": 0.4506,
      "step": 384
    },
    {
      "epoch": 0.15406162464985995,
      "grad_norm": 0.09856906533241272,
      "learning_rate": 0.00016952686447473938,
      "loss": 0.3809,
      "step": 385
    },
    {
      "epoch": 0.15446178471388555,
      "grad_norm": 0.12787072360515594,
      "learning_rate": 0.0001694466720128308,
      "loss": 0.4216,
      "step": 386
    },
    {
      "epoch": 0.15486194477791115,
      "grad_norm": 0.09817204624414444,
      "learning_rate": 0.00016936647955092222,
      "loss": 0.4613,
      "step": 387
    },
    {
      "epoch": 0.15526210484193678,
      "grad_norm": 0.12146846204996109,
      "learning_rate": 0.00016928628708901363,
      "loss": 0.4937,
      "step": 388
    },
    {
      "epoch": 0.15566226490596238,
      "grad_norm": 0.10243633389472961,
      "learning_rate": 0.00016920609462710507,
      "loss": 0.4093,
      "step": 389
    },
    {
      "epoch": 0.15606242496998798,
      "grad_norm": 0.13339482247829437,
      "learning_rate": 0.00016912590216519648,
      "loss": 0.5252,
      "step": 390
    },
    {
      "epoch": 0.1564625850340136,
      "grad_norm": 0.11929335445165634,
      "learning_rate": 0.0001690457097032879,
      "loss": 0.4622,
      "step": 391
    },
    {
      "epoch": 0.1568627450980392,
      "grad_norm": 0.10773039609193802,
      "learning_rate": 0.00016896551724137932,
      "loss": 0.5269,
      "step": 392
    },
    {
      "epoch": 0.15726290516206481,
      "grad_norm": 0.09540361166000366,
      "learning_rate": 0.00016888532477947075,
      "loss": 0.4192,
      "step": 393
    },
    {
      "epoch": 0.15766306522609044,
      "grad_norm": 0.10514029115438461,
      "learning_rate": 0.00016880513231756216,
      "loss": 0.4241,
      "step": 394
    },
    {
      "epoch": 0.15806322529011604,
      "grad_norm": 0.11016156524419785,
      "learning_rate": 0.0001687249398556536,
      "loss": 0.4529,
      "step": 395
    },
    {
      "epoch": 0.15846338535414164,
      "grad_norm": 0.10774089395999908,
      "learning_rate": 0.000168644747393745,
      "loss": 0.4657,
      "step": 396
    },
    {
      "epoch": 0.15886354541816727,
      "grad_norm": 0.1095195934176445,
      "learning_rate": 0.0001685645549318364,
      "loss": 0.4473,
      "step": 397
    },
    {
      "epoch": 0.15926370548219287,
      "grad_norm": 0.10802184045314789,
      "learning_rate": 0.00016848436246992782,
      "loss": 0.45,
      "step": 398
    },
    {
      "epoch": 0.15966386554621848,
      "grad_norm": 0.09583262354135513,
      "learning_rate": 0.00016840417000801925,
      "loss": 0.3974,
      "step": 399
    },
    {
      "epoch": 0.1600640256102441,
      "grad_norm": 0.11612385511398315,
      "learning_rate": 0.00016832397754611066,
      "loss": 0.4723,
      "step": 400
    },
    {
      "epoch": 0.1604641856742697,
      "grad_norm": 0.11543251574039459,
      "learning_rate": 0.0001682437850842021,
      "loss": 0.4373,
      "step": 401
    },
    {
      "epoch": 0.1608643457382953,
      "grad_norm": 0.10931137204170227,
      "learning_rate": 0.0001681635926222935,
      "loss": 0.5034,
      "step": 402
    },
    {
      "epoch": 0.16126450580232093,
      "grad_norm": 0.09334681183099747,
      "learning_rate": 0.00016808340016038494,
      "loss": 0.4197,
      "step": 403
    },
    {
      "epoch": 0.16166466586634654,
      "grad_norm": 0.11418827623128891,
      "learning_rate": 0.00016800320769847635,
      "loss": 0.5319,
      "step": 404
    },
    {
      "epoch": 0.16206482593037214,
      "grad_norm": 0.15250597894191742,
      "learning_rate": 0.00016792301523656778,
      "loss": 0.4587,
      "step": 405
    },
    {
      "epoch": 0.16246498599439776,
      "grad_norm": 0.08031617850065231,
      "learning_rate": 0.0001678428227746592,
      "loss": 0.3797,
      "step": 406
    },
    {
      "epoch": 0.16286514605842337,
      "grad_norm": 0.11269550025463104,
      "learning_rate": 0.0001677626303127506,
      "loss": 0.4888,
      "step": 407
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.1360541135072708,
      "learning_rate": 0.00016768243785084203,
      "loss": 0.4529,
      "step": 408
    },
    {
      "epoch": 0.1636654661864746,
      "grad_norm": 0.10395000129938126,
      "learning_rate": 0.00016760224538893344,
      "loss": 0.4864,
      "step": 409
    },
    {
      "epoch": 0.1640656262505002,
      "grad_norm": 0.12377123534679413,
      "learning_rate": 0.00016752205292702488,
      "loss": 0.4995,
      "step": 410
    },
    {
      "epoch": 0.1644657863145258,
      "grad_norm": 0.09497114270925522,
      "learning_rate": 0.00016744186046511629,
      "loss": 0.3642,
      "step": 411
    },
    {
      "epoch": 0.16486594637855143,
      "grad_norm": 0.12263194471597672,
      "learning_rate": 0.00016736166800320772,
      "loss": 0.4754,
      "step": 412
    },
    {
      "epoch": 0.16526610644257703,
      "grad_norm": 0.10228072106838226,
      "learning_rate": 0.00016728147554129913,
      "loss": 0.4232,
      "step": 413
    },
    {
      "epoch": 0.16566626650660263,
      "grad_norm": 0.11278390884399414,
      "learning_rate": 0.00016720128307939054,
      "loss": 0.3807,
      "step": 414
    },
    {
      "epoch": 0.16606642657062826,
      "grad_norm": 0.16033034026622772,
      "learning_rate": 0.00016712109061748194,
      "loss": 0.6185,
      "step": 415
    },
    {
      "epoch": 0.16646658663465386,
      "grad_norm": 0.1128154918551445,
      "learning_rate": 0.00016704089815557338,
      "loss": 0.4818,
      "step": 416
    },
    {
      "epoch": 0.16686674669867949,
      "grad_norm": 0.11332529783248901,
      "learning_rate": 0.0001669607056936648,
      "loss": 0.469,
      "step": 417
    },
    {
      "epoch": 0.1672669067627051,
      "grad_norm": 0.10386919230222702,
      "learning_rate": 0.00016688051323175622,
      "loss": 0.3989,
      "step": 418
    },
    {
      "epoch": 0.1676670668267307,
      "grad_norm": 0.11381728947162628,
      "learning_rate": 0.00016680032076984763,
      "loss": 0.4467,
      "step": 419
    },
    {
      "epoch": 0.16806722689075632,
      "grad_norm": 0.12440313398838043,
      "learning_rate": 0.00016672012830793906,
      "loss": 0.4507,
      "step": 420
    },
    {
      "epoch": 0.16846738695478192,
      "grad_norm": 0.09180522710084915,
      "learning_rate": 0.00016663993584603047,
      "loss": 0.4394,
      "step": 421
    },
    {
      "epoch": 0.16886754701880752,
      "grad_norm": 0.10706024616956711,
      "learning_rate": 0.0001665597433841219,
      "loss": 0.487,
      "step": 422
    },
    {
      "epoch": 0.16926770708283315,
      "grad_norm": 0.12211337685585022,
      "learning_rate": 0.00016647955092221332,
      "loss": 0.5312,
      "step": 423
    },
    {
      "epoch": 0.16966786714685875,
      "grad_norm": 0.11429573595523834,
      "learning_rate": 0.00016639935846030475,
      "loss": 0.4751,
      "step": 424
    },
    {
      "epoch": 0.17006802721088435,
      "grad_norm": 0.11027617007493973,
      "learning_rate": 0.00016631916599839616,
      "loss": 0.476,
      "step": 425
    },
    {
      "epoch": 0.17046818727490998,
      "grad_norm": 0.12389478832483292,
      "learning_rate": 0.0001662389735364876,
      "loss": 0.4632,
      "step": 426
    },
    {
      "epoch": 0.17086834733893558,
      "grad_norm": 0.12682238221168518,
      "learning_rate": 0.000166158781074579,
      "loss": 0.4672,
      "step": 427
    },
    {
      "epoch": 0.17126850740296118,
      "grad_norm": 0.09478241205215454,
      "learning_rate": 0.00016607858861267044,
      "loss": 0.4157,
      "step": 428
    },
    {
      "epoch": 0.1716686674669868,
      "grad_norm": 0.1543552726507187,
      "learning_rate": 0.00016599839615076182,
      "loss": 0.5299,
      "step": 429
    },
    {
      "epoch": 0.1720688275310124,
      "grad_norm": 0.10749566555023193,
      "learning_rate": 0.00016591820368885325,
      "loss": 0.4686,
      "step": 430
    },
    {
      "epoch": 0.172468987595038,
      "grad_norm": 0.13217896223068237,
      "learning_rate": 0.00016583801122694466,
      "loss": 0.5202,
      "step": 431
    },
    {
      "epoch": 0.17286914765906364,
      "grad_norm": 0.09995675832033157,
      "learning_rate": 0.0001657578187650361,
      "loss": 0.3972,
      "step": 432
    },
    {
      "epoch": 0.17326930772308924,
      "grad_norm": 0.099616639316082,
      "learning_rate": 0.0001656776263031275,
      "loss": 0.5,
      "step": 433
    },
    {
      "epoch": 0.17366946778711484,
      "grad_norm": 0.10888225585222244,
      "learning_rate": 0.00016559743384121894,
      "loss": 0.5139,
      "step": 434
    },
    {
      "epoch": 0.17406962785114047,
      "grad_norm": 0.10737816244363785,
      "learning_rate": 0.00016551724137931035,
      "loss": 0.4395,
      "step": 435
    },
    {
      "epoch": 0.17446978791516607,
      "grad_norm": 0.10688190162181854,
      "learning_rate": 0.00016543704891740178,
      "loss": 0.4147,
      "step": 436
    },
    {
      "epoch": 0.17486994797919167,
      "grad_norm": 0.09064700454473495,
      "learning_rate": 0.0001653568564554932,
      "loss": 0.4397,
      "step": 437
    },
    {
      "epoch": 0.1752701080432173,
      "grad_norm": 0.10624825209379196,
      "learning_rate": 0.00016527666399358462,
      "loss": 0.408,
      "step": 438
    },
    {
      "epoch": 0.1756702681072429,
      "grad_norm": 0.0889757052063942,
      "learning_rate": 0.00016519647153167603,
      "loss": 0.4418,
      "step": 439
    },
    {
      "epoch": 0.1760704281712685,
      "grad_norm": 0.12173134833574295,
      "learning_rate": 0.00016511627906976747,
      "loss": 0.5359,
      "step": 440
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.12943217158317566,
      "learning_rate": 0.00016503608660785888,
      "loss": 0.5427,
      "step": 441
    },
    {
      "epoch": 0.17687074829931973,
      "grad_norm": 0.09590510278940201,
      "learning_rate": 0.00016495589414595028,
      "loss": 0.4583,
      "step": 442
    },
    {
      "epoch": 0.17727090836334533,
      "grad_norm": 0.10100230574607849,
      "learning_rate": 0.00016487570168404172,
      "loss": 0.5098,
      "step": 443
    },
    {
      "epoch": 0.17767106842737096,
      "grad_norm": 0.1111992597579956,
      "learning_rate": 0.00016479550922213313,
      "loss": 0.4435,
      "step": 444
    },
    {
      "epoch": 0.17807122849139656,
      "grad_norm": 0.11293987184762955,
      "learning_rate": 0.00016471531676022453,
      "loss": 0.4399,
      "step": 445
    },
    {
      "epoch": 0.17847138855542216,
      "grad_norm": 0.13352513313293457,
      "learning_rate": 0.00016463512429831597,
      "loss": 0.4092,
      "step": 446
    },
    {
      "epoch": 0.1788715486194478,
      "grad_norm": 0.11819305270910263,
      "learning_rate": 0.00016455493183640738,
      "loss": 0.5256,
      "step": 447
    },
    {
      "epoch": 0.1792717086834734,
      "grad_norm": 0.09756582975387573,
      "learning_rate": 0.0001644747393744988,
      "loss": 0.3467,
      "step": 448
    },
    {
      "epoch": 0.179671868747499,
      "grad_norm": 0.10889743268489838,
      "learning_rate": 0.00016439454691259022,
      "loss": 0.5133,
      "step": 449
    },
    {
      "epoch": 0.18007202881152462,
      "grad_norm": 0.12313370406627655,
      "learning_rate": 0.00016431435445068163,
      "loss": 0.5422,
      "step": 450
    },
    {
      "epoch": 0.18047218887555022,
      "grad_norm": 0.13783803582191467,
      "learning_rate": 0.00016423416198877306,
      "loss": 0.4429,
      "step": 451
    },
    {
      "epoch": 0.18087234893957582,
      "grad_norm": 0.09798813611268997,
      "learning_rate": 0.00016415396952686447,
      "loss": 0.4477,
      "step": 452
    },
    {
      "epoch": 0.18127250900360145,
      "grad_norm": 0.11045589298009872,
      "learning_rate": 0.0001640737770649559,
      "loss": 0.4083,
      "step": 453
    },
    {
      "epoch": 0.18167266906762705,
      "grad_norm": 0.10568922758102417,
      "learning_rate": 0.0001639935846030473,
      "loss": 0.508,
      "step": 454
    },
    {
      "epoch": 0.18207282913165265,
      "grad_norm": 0.08680666983127594,
      "learning_rate": 0.00016391339214113875,
      "loss": 0.4307,
      "step": 455
    },
    {
      "epoch": 0.18247298919567828,
      "grad_norm": 0.10682111233472824,
      "learning_rate": 0.00016383319967923016,
      "loss": 0.4703,
      "step": 456
    },
    {
      "epoch": 0.18287314925970388,
      "grad_norm": 0.10381560027599335,
      "learning_rate": 0.0001637530072173216,
      "loss": 0.4458,
      "step": 457
    },
    {
      "epoch": 0.18327330932372948,
      "grad_norm": 0.09116524457931519,
      "learning_rate": 0.000163672814755413,
      "loss": 0.4521,
      "step": 458
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 0.10218220204114914,
      "learning_rate": 0.00016359262229350443,
      "loss": 0.3931,
      "step": 459
    },
    {
      "epoch": 0.1840736294517807,
      "grad_norm": 0.11209532618522644,
      "learning_rate": 0.00016351242983159584,
      "loss": 0.5067,
      "step": 460
    },
    {
      "epoch": 0.1844737895158063,
      "grad_norm": 0.11690597236156464,
      "learning_rate": 0.00016343223736968725,
      "loss": 0.4767,
      "step": 461
    },
    {
      "epoch": 0.18487394957983194,
      "grad_norm": 0.10228600353002548,
      "learning_rate": 0.00016335204490777866,
      "loss": 0.4961,
      "step": 462
    },
    {
      "epoch": 0.18527410964385754,
      "grad_norm": 0.10662172734737396,
      "learning_rate": 0.0001632718524458701,
      "loss": 0.4135,
      "step": 463
    },
    {
      "epoch": 0.18567426970788314,
      "grad_norm": 0.11126268655061722,
      "learning_rate": 0.0001631916599839615,
      "loss": 0.4438,
      "step": 464
    },
    {
      "epoch": 0.18607442977190877,
      "grad_norm": 0.1104714646935463,
      "learning_rate": 0.00016311146752205294,
      "loss": 0.4465,
      "step": 465
    },
    {
      "epoch": 0.18647458983593437,
      "grad_norm": 0.09419477730989456,
      "learning_rate": 0.00016303127506014434,
      "loss": 0.4064,
      "step": 466
    },
    {
      "epoch": 0.18687474989995997,
      "grad_norm": 0.11288101971149445,
      "learning_rate": 0.00016295108259823578,
      "loss": 0.4446,
      "step": 467
    },
    {
      "epoch": 0.1872749099639856,
      "grad_norm": 0.12478205561637878,
      "learning_rate": 0.0001628708901363272,
      "loss": 0.5268,
      "step": 468
    },
    {
      "epoch": 0.1876750700280112,
      "grad_norm": 0.12878870964050293,
      "learning_rate": 0.00016279069767441862,
      "loss": 0.5244,
      "step": 469
    },
    {
      "epoch": 0.1880752300920368,
      "grad_norm": 0.11776390671730042,
      "learning_rate": 0.00016271050521251003,
      "loss": 0.5274,
      "step": 470
    },
    {
      "epoch": 0.18847539015606243,
      "grad_norm": 0.13141551613807678,
      "learning_rate": 0.00016263031275060146,
      "loss": 0.5275,
      "step": 471
    },
    {
      "epoch": 0.18887555022008803,
      "grad_norm": 0.13568970561027527,
      "learning_rate": 0.00016255012028869287,
      "loss": 0.4357,
      "step": 472
    },
    {
      "epoch": 0.18927571028411364,
      "grad_norm": 0.14077135920524597,
      "learning_rate": 0.0001624699278267843,
      "loss": 0.5637,
      "step": 473
    },
    {
      "epoch": 0.18967587034813926,
      "grad_norm": 0.07844144105911255,
      "learning_rate": 0.00016238973536487572,
      "loss": 0.3779,
      "step": 474
    },
    {
      "epoch": 0.19007603041216486,
      "grad_norm": 0.1034446656703949,
      "learning_rate": 0.00016230954290296715,
      "loss": 0.4779,
      "step": 475
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.12724757194519043,
      "learning_rate": 0.00016222935044105856,
      "loss": 0.4757,
      "step": 476
    },
    {
      "epoch": 0.1908763505402161,
      "grad_norm": 0.1189434677362442,
      "learning_rate": 0.00016214915797914997,
      "loss": 0.488,
      "step": 477
    },
    {
      "epoch": 0.1912765106042417,
      "grad_norm": 0.1115308403968811,
      "learning_rate": 0.00016206896551724137,
      "loss": 0.5036,
      "step": 478
    },
    {
      "epoch": 0.1916766706682673,
      "grad_norm": 0.09522669017314911,
      "learning_rate": 0.0001619887730553328,
      "loss": 0.4545,
      "step": 479
    },
    {
      "epoch": 0.19207683073229292,
      "grad_norm": 0.108955018222332,
      "learning_rate": 0.00016190858059342422,
      "loss": 0.4825,
      "step": 480
    },
    {
      "epoch": 0.19247699079631853,
      "grad_norm": 0.12519310414791107,
      "learning_rate": 0.00016182838813151565,
      "loss": 0.57,
      "step": 481
    },
    {
      "epoch": 0.19287715086034413,
      "grad_norm": 0.14481234550476074,
      "learning_rate": 0.00016174819566960706,
      "loss": 0.4503,
      "step": 482
    },
    {
      "epoch": 0.19327731092436976,
      "grad_norm": 0.10824614018201828,
      "learning_rate": 0.0001616680032076985,
      "loss": 0.4371,
      "step": 483
    },
    {
      "epoch": 0.19367747098839536,
      "grad_norm": 0.10712645202875137,
      "learning_rate": 0.0001615878107457899,
      "loss": 0.4575,
      "step": 484
    },
    {
      "epoch": 0.19407763105242096,
      "grad_norm": 0.13719598948955536,
      "learning_rate": 0.0001615076182838813,
      "loss": 0.519,
      "step": 485
    },
    {
      "epoch": 0.19447779111644659,
      "grad_norm": 0.11803009361028671,
      "learning_rate": 0.00016142742582197275,
      "loss": 0.4316,
      "step": 486
    },
    {
      "epoch": 0.1948779511804722,
      "grad_norm": 0.1473352015018463,
      "learning_rate": 0.00016134723336006415,
      "loss": 0.535,
      "step": 487
    },
    {
      "epoch": 0.1952781112444978,
      "grad_norm": 0.10205501317977905,
      "learning_rate": 0.0001612670408981556,
      "loss": 0.4562,
      "step": 488
    },
    {
      "epoch": 0.19567827130852342,
      "grad_norm": 0.13764865696430206,
      "learning_rate": 0.000161186848436247,
      "loss": 0.5613,
      "step": 489
    },
    {
      "epoch": 0.19607843137254902,
      "grad_norm": 0.11124549061059952,
      "learning_rate": 0.00016110665597433843,
      "loss": 0.4485,
      "step": 490
    },
    {
      "epoch": 0.19647859143657462,
      "grad_norm": 0.11892858892679214,
      "learning_rate": 0.00016102646351242984,
      "loss": 0.558,
      "step": 491
    },
    {
      "epoch": 0.19687875150060025,
      "grad_norm": 0.11151297390460968,
      "learning_rate": 0.00016094627105052128,
      "loss": 0.4032,
      "step": 492
    },
    {
      "epoch": 0.19727891156462585,
      "grad_norm": 0.08672972023487091,
      "learning_rate": 0.00016086607858861266,
      "loss": 0.3667,
      "step": 493
    },
    {
      "epoch": 0.19767907162865145,
      "grad_norm": 0.0961608961224556,
      "learning_rate": 0.0001607858861267041,
      "loss": 0.4499,
      "step": 494
    },
    {
      "epoch": 0.19807923169267708,
      "grad_norm": 0.11347705125808716,
      "learning_rate": 0.0001607056936647955,
      "loss": 0.4791,
      "step": 495
    },
    {
      "epoch": 0.19847939175670268,
      "grad_norm": 0.11131388694047928,
      "learning_rate": 0.00016062550120288693,
      "loss": 0.5169,
      "step": 496
    },
    {
      "epoch": 0.19887955182072828,
      "grad_norm": 0.11383651196956635,
      "learning_rate": 0.00016054530874097834,
      "loss": 0.4702,
      "step": 497
    },
    {
      "epoch": 0.1992797118847539,
      "grad_norm": 0.1742691546678543,
      "learning_rate": 0.00016046511627906978,
      "loss": 0.5,
      "step": 498
    },
    {
      "epoch": 0.1996798719487795,
      "grad_norm": 0.1191936507821083,
      "learning_rate": 0.00016038492381716118,
      "loss": 0.4621,
      "step": 499
    },
    {
      "epoch": 0.2000800320128051,
      "grad_norm": 0.10533282905817032,
      "learning_rate": 0.00016030473135525262,
      "loss": 0.4328,
      "step": 500
    },
    {
      "epoch": 0.20048019207683074,
      "grad_norm": 0.1160888671875,
      "learning_rate": 0.00016022453889334403,
      "loss": 0.4742,
      "step": 501
    },
    {
      "epoch": 0.20088035214085634,
      "grad_norm": 0.10843116044998169,
      "learning_rate": 0.00016014434643143546,
      "loss": 0.4429,
      "step": 502
    },
    {
      "epoch": 0.20128051220488194,
      "grad_norm": 0.11954541504383087,
      "learning_rate": 0.00016006415396952687,
      "loss": 0.4511,
      "step": 503
    },
    {
      "epoch": 0.20168067226890757,
      "grad_norm": 0.11967387050390244,
      "learning_rate": 0.0001599839615076183,
      "loss": 0.4666,
      "step": 504
    },
    {
      "epoch": 0.20208083233293317,
      "grad_norm": 0.11037088930606842,
      "learning_rate": 0.0001599037690457097,
      "loss": 0.4169,
      "step": 505
    },
    {
      "epoch": 0.20248099239695877,
      "grad_norm": 0.10434439778327942,
      "learning_rate": 0.00015982357658380115,
      "loss": 0.4771,
      "step": 506
    },
    {
      "epoch": 0.2028811524609844,
      "grad_norm": 0.13806341588497162,
      "learning_rate": 0.00015974338412189256,
      "loss": 0.5667,
      "step": 507
    },
    {
      "epoch": 0.20328131252501,
      "grad_norm": 0.1054348573088646,
      "learning_rate": 0.000159663191659984,
      "loss": 0.462,
      "step": 508
    },
    {
      "epoch": 0.2036814725890356,
      "grad_norm": 0.11910860985517502,
      "learning_rate": 0.00015958299919807537,
      "loss": 0.4165,
      "step": 509
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 0.10214203596115112,
      "learning_rate": 0.0001595028067361668,
      "loss": 0.4752,
      "step": 510
    },
    {
      "epoch": 0.20448179271708683,
      "grad_norm": 0.1117975041270256,
      "learning_rate": 0.00015942261427425822,
      "loss": 0.5461,
      "step": 511
    },
    {
      "epoch": 0.20488195278111246,
      "grad_norm": 0.11039382219314575,
      "learning_rate": 0.00015934242181234965,
      "loss": 0.4268,
      "step": 512
    },
    {
      "epoch": 0.20528211284513806,
      "grad_norm": 0.0977049246430397,
      "learning_rate": 0.00015926222935044106,
      "loss": 0.4644,
      "step": 513
    },
    {
      "epoch": 0.20568227290916366,
      "grad_norm": 0.10313583165407181,
      "learning_rate": 0.0001591820368885325,
      "loss": 0.4182,
      "step": 514
    },
    {
      "epoch": 0.2060824329731893,
      "grad_norm": 0.12206972390413284,
      "learning_rate": 0.0001591018444266239,
      "loss": 0.4937,
      "step": 515
    },
    {
      "epoch": 0.2064825930372149,
      "grad_norm": 0.10930175334215164,
      "learning_rate": 0.00015902165196471534,
      "loss": 0.5066,
      "step": 516
    },
    {
      "epoch": 0.2068827531012405,
      "grad_norm": 0.10916141420602798,
      "learning_rate": 0.00015894145950280674,
      "loss": 0.5132,
      "step": 517
    },
    {
      "epoch": 0.20728291316526612,
      "grad_norm": 0.08731237053871155,
      "learning_rate": 0.00015886126704089815,
      "loss": 0.3646,
      "step": 518
    },
    {
      "epoch": 0.20768307322929172,
      "grad_norm": 0.13562361896038055,
      "learning_rate": 0.0001587810745789896,
      "loss": 0.5005,
      "step": 519
    },
    {
      "epoch": 0.20808323329331732,
      "grad_norm": 0.12175433337688446,
      "learning_rate": 0.000158700882117081,
      "loss": 0.5344,
      "step": 520
    },
    {
      "epoch": 0.20848339335734295,
      "grad_norm": 0.09746820479631424,
      "learning_rate": 0.00015862068965517243,
      "loss": 0.4221,
      "step": 521
    },
    {
      "epoch": 0.20888355342136855,
      "grad_norm": 0.10892573744058609,
      "learning_rate": 0.00015854049719326384,
      "loss": 0.4983,
      "step": 522
    },
    {
      "epoch": 0.20928371348539415,
      "grad_norm": 0.09814619272947311,
      "learning_rate": 0.00015846030473135527,
      "loss": 0.4307,
      "step": 523
    },
    {
      "epoch": 0.20968387354941978,
      "grad_norm": 0.10518758744001389,
      "learning_rate": 0.00015838011226944668,
      "loss": 0.5303,
      "step": 524
    },
    {
      "epoch": 0.21008403361344538,
      "grad_norm": 0.10021954029798508,
      "learning_rate": 0.0001582999198075381,
      "loss": 0.4463,
      "step": 525
    },
    {
      "epoch": 0.21048419367747098,
      "grad_norm": 0.1072564348578453,
      "learning_rate": 0.0001582197273456295,
      "loss": 0.4992,
      "step": 526
    },
    {
      "epoch": 0.2108843537414966,
      "grad_norm": 0.10118383914232254,
      "learning_rate": 0.00015813953488372093,
      "loss": 0.3794,
      "step": 527
    },
    {
      "epoch": 0.2112845138055222,
      "grad_norm": 0.11332004517316818,
      "learning_rate": 0.00015805934242181234,
      "loss": 0.4881,
      "step": 528
    },
    {
      "epoch": 0.2116846738695478,
      "grad_norm": 0.08984694629907608,
      "learning_rate": 0.00015797914995990377,
      "loss": 0.4022,
      "step": 529
    },
    {
      "epoch": 0.21208483393357344,
      "grad_norm": 0.11011867970228195,
      "learning_rate": 0.00015789895749799518,
      "loss": 0.4383,
      "step": 530
    },
    {
      "epoch": 0.21248499399759904,
      "grad_norm": 0.09662603586912155,
      "learning_rate": 0.00015781876503608662,
      "loss": 0.5044,
      "step": 531
    },
    {
      "epoch": 0.21288515406162464,
      "grad_norm": 0.10162687301635742,
      "learning_rate": 0.00015773857257417803,
      "loss": 0.4554,
      "step": 532
    },
    {
      "epoch": 0.21328531412565027,
      "grad_norm": 0.0948612317442894,
      "learning_rate": 0.00015765838011226946,
      "loss": 0.4403,
      "step": 533
    },
    {
      "epoch": 0.21368547418967587,
      "grad_norm": 0.1090465858578682,
      "learning_rate": 0.00015757818765036087,
      "loss": 0.4698,
      "step": 534
    },
    {
      "epoch": 0.21408563425370147,
      "grad_norm": 0.09007816016674042,
      "learning_rate": 0.0001574979951884523,
      "loss": 0.4941,
      "step": 535
    },
    {
      "epoch": 0.2144857943177271,
      "grad_norm": 0.10303910821676254,
      "learning_rate": 0.0001574178027265437,
      "loss": 0.4549,
      "step": 536
    },
    {
      "epoch": 0.2148859543817527,
      "grad_norm": 0.08443200588226318,
      "learning_rate": 0.00015733761026463515,
      "loss": 0.3848,
      "step": 537
    },
    {
      "epoch": 0.2152861144457783,
      "grad_norm": 0.10711009800434113,
      "learning_rate": 0.00015725741780272655,
      "loss": 0.5173,
      "step": 538
    },
    {
      "epoch": 0.21568627450980393,
      "grad_norm": 0.09983476996421814,
      "learning_rate": 0.000157177225340818,
      "loss": 0.3563,
      "step": 539
    },
    {
      "epoch": 0.21608643457382953,
      "grad_norm": 0.11906463652849197,
      "learning_rate": 0.0001570970328789094,
      "loss": 0.5026,
      "step": 540
    },
    {
      "epoch": 0.21648659463785513,
      "grad_norm": 0.09050139784812927,
      "learning_rate": 0.0001570168404170008,
      "loss": 0.429,
      "step": 541
    },
    {
      "epoch": 0.21688675470188076,
      "grad_norm": 0.10380709916353226,
      "learning_rate": 0.0001569366479550922,
      "loss": 0.4936,
      "step": 542
    },
    {
      "epoch": 0.21728691476590636,
      "grad_norm": 0.09506339579820633,
      "learning_rate": 0.00015685645549318365,
      "loss": 0.4488,
      "step": 543
    },
    {
      "epoch": 0.21768707482993196,
      "grad_norm": 0.08807029575109482,
      "learning_rate": 0.00015677626303127506,
      "loss": 0.3382,
      "step": 544
    },
    {
      "epoch": 0.2180872348939576,
      "grad_norm": 0.1818714290857315,
      "learning_rate": 0.0001566960705693665,
      "loss": 0.4613,
      "step": 545
    },
    {
      "epoch": 0.2184873949579832,
      "grad_norm": 0.11654231697320938,
      "learning_rate": 0.0001566158781074579,
      "loss": 0.562,
      "step": 546
    },
    {
      "epoch": 0.2188875550220088,
      "grad_norm": 0.11283345520496368,
      "learning_rate": 0.00015653568564554933,
      "loss": 0.5256,
      "step": 547
    },
    {
      "epoch": 0.21928771508603442,
      "grad_norm": 0.11111574620008469,
      "learning_rate": 0.00015645549318364074,
      "loss": 0.3989,
      "step": 548
    },
    {
      "epoch": 0.21968787515006002,
      "grad_norm": 0.10976814478635788,
      "learning_rate": 0.00015637530072173218,
      "loss": 0.376,
      "step": 549
    },
    {
      "epoch": 0.22008803521408563,
      "grad_norm": 0.1063736081123352,
      "learning_rate": 0.00015629510825982358,
      "loss": 0.4571,
      "step": 550
    },
    {
      "epoch": 0.22048819527811125,
      "grad_norm": 0.11049526929855347,
      "learning_rate": 0.00015621491579791502,
      "loss": 0.4888,
      "step": 551
    },
    {
      "epoch": 0.22088835534213686,
      "grad_norm": 0.09996826201677322,
      "learning_rate": 0.00015613472333600643,
      "loss": 0.4624,
      "step": 552
    },
    {
      "epoch": 0.22128851540616246,
      "grad_norm": 0.1468735635280609,
      "learning_rate": 0.00015605453087409784,
      "loss": 0.6041,
      "step": 553
    },
    {
      "epoch": 0.22168867547018808,
      "grad_norm": 0.13568057119846344,
      "learning_rate": 0.00015597433841218927,
      "loss": 0.5357,
      "step": 554
    },
    {
      "epoch": 0.22208883553421369,
      "grad_norm": 0.08762109279632568,
      "learning_rate": 0.00015589414595028068,
      "loss": 0.3982,
      "step": 555
    },
    {
      "epoch": 0.2224889955982393,
      "grad_norm": 0.11665282398462296,
      "learning_rate": 0.0001558139534883721,
      "loss": 0.4449,
      "step": 556
    },
    {
      "epoch": 0.22288915566226492,
      "grad_norm": 0.10949048399925232,
      "learning_rate": 0.00015573376102646352,
      "loss": 0.4971,
      "step": 557
    },
    {
      "epoch": 0.22328931572629052,
      "grad_norm": 0.11357007920742035,
      "learning_rate": 0.00015565356856455493,
      "loss": 0.4584,
      "step": 558
    },
    {
      "epoch": 0.22368947579031612,
      "grad_norm": 0.1092696338891983,
      "learning_rate": 0.00015557337610264636,
      "loss": 0.4333,
      "step": 559
    },
    {
      "epoch": 0.22408963585434175,
      "grad_norm": 0.10017597675323486,
      "learning_rate": 0.00015549318364073777,
      "loss": 0.4169,
      "step": 560
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 0.13644687831401825,
      "learning_rate": 0.00015541299117882918,
      "loss": 0.5403,
      "step": 561
    },
    {
      "epoch": 0.22488995598239295,
      "grad_norm": 0.13831111788749695,
      "learning_rate": 0.00015533279871692062,
      "loss": 0.5634,
      "step": 562
    },
    {
      "epoch": 0.22529011604641858,
      "grad_norm": 0.10011740028858185,
      "learning_rate": 0.00015525260625501202,
      "loss": 0.4926,
      "step": 563
    },
    {
      "epoch": 0.22569027611044418,
      "grad_norm": 0.10702542960643768,
      "learning_rate": 0.00015517241379310346,
      "loss": 0.4516,
      "step": 564
    },
    {
      "epoch": 0.22609043617446978,
      "grad_norm": 0.10409373044967651,
      "learning_rate": 0.00015509222133119487,
      "loss": 0.4416,
      "step": 565
    },
    {
      "epoch": 0.2264905962384954,
      "grad_norm": 0.1399667114019394,
      "learning_rate": 0.0001550120288692863,
      "loss": 0.4732,
      "step": 566
    },
    {
      "epoch": 0.226890756302521,
      "grad_norm": 0.1140410527586937,
      "learning_rate": 0.0001549318364073777,
      "loss": 0.5271,
      "step": 567
    },
    {
      "epoch": 0.2272909163665466,
      "grad_norm": 0.1065244972705841,
      "learning_rate": 0.00015485164394546914,
      "loss": 0.4946,
      "step": 568
    },
    {
      "epoch": 0.22769107643057224,
      "grad_norm": 0.08497174829244614,
      "learning_rate": 0.00015477145148356055,
      "loss": 0.4402,
      "step": 569
    },
    {
      "epoch": 0.22809123649459784,
      "grad_norm": 0.1022646427154541,
      "learning_rate": 0.000154691259021652,
      "loss": 0.4573,
      "step": 570
    },
    {
      "epoch": 0.22849139655862344,
      "grad_norm": 0.10326431691646576,
      "learning_rate": 0.0001546110665597434,
      "loss": 0.4892,
      "step": 571
    },
    {
      "epoch": 0.22889155662264907,
      "grad_norm": 0.12093506753444672,
      "learning_rate": 0.00015453087409783483,
      "loss": 0.5144,
      "step": 572
    },
    {
      "epoch": 0.22929171668667467,
      "grad_norm": 0.08344414085149765,
      "learning_rate": 0.0001544506816359262,
      "loss": 0.3962,
      "step": 573
    },
    {
      "epoch": 0.22969187675070027,
      "grad_norm": 0.09203273057937622,
      "learning_rate": 0.00015437048917401765,
      "loss": 0.4559,
      "step": 574
    },
    {
      "epoch": 0.2300920368147259,
      "grad_norm": 0.09557068347930908,
      "learning_rate": 0.00015429029671210905,
      "loss": 0.4063,
      "step": 575
    },
    {
      "epoch": 0.2304921968787515,
      "grad_norm": 0.10753446817398071,
      "learning_rate": 0.0001542101042502005,
      "loss": 0.458,
      "step": 576
    },
    {
      "epoch": 0.2308923569427771,
      "grad_norm": 0.11689016968011856,
      "learning_rate": 0.0001541299117882919,
      "loss": 0.5108,
      "step": 577
    },
    {
      "epoch": 0.23129251700680273,
      "grad_norm": 0.11085210740566254,
      "learning_rate": 0.00015404971932638333,
      "loss": 0.4983,
      "step": 578
    },
    {
      "epoch": 0.23169267707082833,
      "grad_norm": 0.09784694015979767,
      "learning_rate": 0.00015396952686447474,
      "loss": 0.4347,
      "step": 579
    },
    {
      "epoch": 0.23209283713485393,
      "grad_norm": 0.10006105899810791,
      "learning_rate": 0.00015388933440256617,
      "loss": 0.4412,
      "step": 580
    },
    {
      "epoch": 0.23249299719887956,
      "grad_norm": 0.09885857254266739,
      "learning_rate": 0.00015380914194065758,
      "loss": 0.4598,
      "step": 581
    },
    {
      "epoch": 0.23289315726290516,
      "grad_norm": 0.09985047578811646,
      "learning_rate": 0.00015372894947874902,
      "loss": 0.4232,
      "step": 582
    },
    {
      "epoch": 0.23329331732693076,
      "grad_norm": 0.10441979765892029,
      "learning_rate": 0.00015364875701684043,
      "loss": 0.4935,
      "step": 583
    },
    {
      "epoch": 0.2336934773909564,
      "grad_norm": 0.10115625709295273,
      "learning_rate": 0.00015356856455493186,
      "loss": 0.4498,
      "step": 584
    },
    {
      "epoch": 0.234093637454982,
      "grad_norm": 0.12995608150959015,
      "learning_rate": 0.00015348837209302327,
      "loss": 0.4394,
      "step": 585
    },
    {
      "epoch": 0.2344937975190076,
      "grad_norm": 0.12374968081712723,
      "learning_rate": 0.0001534081796311147,
      "loss": 0.5315,
      "step": 586
    },
    {
      "epoch": 0.23489395758303322,
      "grad_norm": 0.10165902972221375,
      "learning_rate": 0.0001533279871692061,
      "loss": 0.3812,
      "step": 587
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.1285983771085739,
      "learning_rate": 0.00015324779470729752,
      "loss": 0.567,
      "step": 588
    },
    {
      "epoch": 0.23569427771108442,
      "grad_norm": 0.1033625453710556,
      "learning_rate": 0.00015316760224538893,
      "loss": 0.4611,
      "step": 589
    },
    {
      "epoch": 0.23609443777511005,
      "grad_norm": 0.10416607558727264,
      "learning_rate": 0.00015308740978348036,
      "loss": 0.4449,
      "step": 590
    },
    {
      "epoch": 0.23649459783913565,
      "grad_norm": 0.1311168372631073,
      "learning_rate": 0.00015300721732157177,
      "loss": 0.5123,
      "step": 591
    },
    {
      "epoch": 0.23689475790316125,
      "grad_norm": 0.11353563517332077,
      "learning_rate": 0.0001529270248596632,
      "loss": 0.5141,
      "step": 592
    },
    {
      "epoch": 0.23729491796718688,
      "grad_norm": 0.11651010811328888,
      "learning_rate": 0.0001528468323977546,
      "loss": 0.5117,
      "step": 593
    },
    {
      "epoch": 0.23769507803121248,
      "grad_norm": 0.1087644100189209,
      "learning_rate": 0.00015276663993584602,
      "loss": 0.4504,
      "step": 594
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.10093679279088974,
      "learning_rate": 0.00015268644747393746,
      "loss": 0.4421,
      "step": 595
    },
    {
      "epoch": 0.2384953981592637,
      "grad_norm": 0.09877624362707138,
      "learning_rate": 0.00015260625501202886,
      "loss": 0.4421,
      "step": 596
    },
    {
      "epoch": 0.2388955582232893,
      "grad_norm": 0.11335496604442596,
      "learning_rate": 0.0001525260625501203,
      "loss": 0.577,
      "step": 597
    },
    {
      "epoch": 0.2392957182873149,
      "grad_norm": 0.09093920141458511,
      "learning_rate": 0.0001524458700882117,
      "loss": 0.4437,
      "step": 598
    },
    {
      "epoch": 0.23969587835134054,
      "grad_norm": 0.11731240153312683,
      "learning_rate": 0.00015236567762630314,
      "loss": 0.534,
      "step": 599
    },
    {
      "epoch": 0.24009603841536614,
      "grad_norm": 0.11159759014844894,
      "learning_rate": 0.00015228548516439455,
      "loss": 0.4871,
      "step": 600
    },
    {
      "epoch": 0.24049619847939174,
      "grad_norm": 0.09929663687944412,
      "learning_rate": 0.00015220529270248598,
      "loss": 0.4738,
      "step": 601
    },
    {
      "epoch": 0.24089635854341737,
      "grad_norm": 0.10101604461669922,
      "learning_rate": 0.0001521251002405774,
      "loss": 0.3715,
      "step": 602
    },
    {
      "epoch": 0.24129651860744297,
      "grad_norm": 0.10067626088857651,
      "learning_rate": 0.00015204490777866883,
      "loss": 0.4279,
      "step": 603
    },
    {
      "epoch": 0.2416966786714686,
      "grad_norm": 0.10034160315990448,
      "learning_rate": 0.00015196471531676024,
      "loss": 0.5081,
      "step": 604
    },
    {
      "epoch": 0.2420968387354942,
      "grad_norm": 0.11264333873987198,
      "learning_rate": 0.00015188452285485164,
      "loss": 0.5241,
      "step": 605
    },
    {
      "epoch": 0.2424969987995198,
      "grad_norm": 0.10770173370838165,
      "learning_rate": 0.00015180433039294305,
      "loss": 0.5183,
      "step": 606
    },
    {
      "epoch": 0.24289715886354543,
      "grad_norm": 0.10427700728178024,
      "learning_rate": 0.00015172413793103449,
      "loss": 0.4096,
      "step": 607
    },
    {
      "epoch": 0.24329731892757103,
      "grad_norm": 0.08093017339706421,
      "learning_rate": 0.0001516439454691259,
      "loss": 0.4154,
      "step": 608
    },
    {
      "epoch": 0.24369747899159663,
      "grad_norm": 0.11212027817964554,
      "learning_rate": 0.00015156375300721733,
      "loss": 0.5223,
      "step": 609
    },
    {
      "epoch": 0.24409763905562226,
      "grad_norm": 0.1275569498538971,
      "learning_rate": 0.00015148356054530874,
      "loss": 0.4916,
      "step": 610
    },
    {
      "epoch": 0.24449779911964786,
      "grad_norm": 0.09744749218225479,
      "learning_rate": 0.00015140336808340017,
      "loss": 0.4528,
      "step": 611
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 0.08010856062173843,
      "learning_rate": 0.00015132317562149158,
      "loss": 0.3901,
      "step": 612
    },
    {
      "epoch": 0.2452981192476991,
      "grad_norm": 0.11392361670732498,
      "learning_rate": 0.00015124298315958302,
      "loss": 0.5071,
      "step": 613
    },
    {
      "epoch": 0.2456982793117247,
      "grad_norm": 0.11288876831531525,
      "learning_rate": 0.00015116279069767442,
      "loss": 0.5049,
      "step": 614
    },
    {
      "epoch": 0.2460984393757503,
      "grad_norm": 0.11707416921854019,
      "learning_rate": 0.00015108259823576586,
      "loss": 0.4834,
      "step": 615
    },
    {
      "epoch": 0.24649859943977592,
      "grad_norm": 0.12204820662736893,
      "learning_rate": 0.00015100240577385727,
      "loss": 0.5522,
      "step": 616
    },
    {
      "epoch": 0.24689875950380152,
      "grad_norm": 0.09483850747346878,
      "learning_rate": 0.0001509222133119487,
      "loss": 0.4179,
      "step": 617
    },
    {
      "epoch": 0.24729891956782712,
      "grad_norm": 0.15113462507724762,
      "learning_rate": 0.0001508420208500401,
      "loss": 0.4507,
      "step": 618
    },
    {
      "epoch": 0.24769907963185275,
      "grad_norm": 0.12420926988124847,
      "learning_rate": 0.00015076182838813154,
      "loss": 0.4824,
      "step": 619
    },
    {
      "epoch": 0.24809923969587835,
      "grad_norm": 0.13444222509860992,
      "learning_rate": 0.00015068163592622295,
      "loss": 0.4717,
      "step": 620
    },
    {
      "epoch": 0.24849939975990396,
      "grad_norm": 0.10181350260972977,
      "learning_rate": 0.00015060144346431436,
      "loss": 0.4463,
      "step": 621
    },
    {
      "epoch": 0.24889955982392958,
      "grad_norm": 0.09818095713853836,
      "learning_rate": 0.00015052125100240577,
      "loss": 0.4772,
      "step": 622
    },
    {
      "epoch": 0.24929971988795518,
      "grad_norm": 0.13970068097114563,
      "learning_rate": 0.0001504410585404972,
      "loss": 0.3764,
      "step": 623
    },
    {
      "epoch": 0.24969987995198079,
      "grad_norm": 0.1007903590798378,
      "learning_rate": 0.0001503608660785886,
      "loss": 0.5153,
      "step": 624
    },
    {
      "epoch": 0.2501000400160064,
      "grad_norm": 0.11617309600114822,
      "learning_rate": 0.00015028067361668005,
      "loss": 0.5223,
      "step": 625
    },
    {
      "epoch": 0.250500200080032,
      "grad_norm": 0.11103373020887375,
      "learning_rate": 0.00015020048115477145,
      "loss": 0.4606,
      "step": 626
    },
    {
      "epoch": 0.25090036014405764,
      "grad_norm": 0.09844488650560379,
      "learning_rate": 0.0001501202886928629,
      "loss": 0.4762,
      "step": 627
    },
    {
      "epoch": 0.25130052020808324,
      "grad_norm": 0.09487011283636093,
      "learning_rate": 0.0001500400962309543,
      "loss": 0.4364,
      "step": 628
    },
    {
      "epoch": 0.25170068027210885,
      "grad_norm": 0.11896095424890518,
      "learning_rate": 0.0001499599037690457,
      "loss": 0.4763,
      "step": 629
    },
    {
      "epoch": 0.25210084033613445,
      "grad_norm": 0.10128217935562134,
      "learning_rate": 0.00014987971130713714,
      "loss": 0.4114,
      "step": 630
    },
    {
      "epoch": 0.25250100040016005,
      "grad_norm": 0.1111731082201004,
      "learning_rate": 0.00014979951884522855,
      "loss": 0.4626,
      "step": 631
    },
    {
      "epoch": 0.25290116046418565,
      "grad_norm": 0.11060844361782074,
      "learning_rate": 0.00014971932638331998,
      "loss": 0.4731,
      "step": 632
    },
    {
      "epoch": 0.2533013205282113,
      "grad_norm": 0.12293235957622528,
      "learning_rate": 0.0001496391339214114,
      "loss": 0.4352,
      "step": 633
    },
    {
      "epoch": 0.2537014805922369,
      "grad_norm": 0.11928591877222061,
      "learning_rate": 0.00014955894145950283,
      "loss": 0.4955,
      "step": 634
    },
    {
      "epoch": 0.2541016406562625,
      "grad_norm": 0.08913033455610275,
      "learning_rate": 0.00014947874899759423,
      "loss": 0.3893,
      "step": 635
    },
    {
      "epoch": 0.2545018007202881,
      "grad_norm": 0.12374843657016754,
      "learning_rate": 0.00014939855653568567,
      "loss": 0.503,
      "step": 636
    },
    {
      "epoch": 0.2549019607843137,
      "grad_norm": 0.11081800609827042,
      "learning_rate": 0.00014931836407377705,
      "loss": 0.4359,
      "step": 637
    },
    {
      "epoch": 0.2553021208483393,
      "grad_norm": 0.11783795058727264,
      "learning_rate": 0.00014923817161186848,
      "loss": 0.4609,
      "step": 638
    },
    {
      "epoch": 0.25570228091236497,
      "grad_norm": 0.09768394380807877,
      "learning_rate": 0.0001491579791499599,
      "loss": 0.4639,
      "step": 639
    },
    {
      "epoch": 0.25610244097639057,
      "grad_norm": 0.11859265714883804,
      "learning_rate": 0.00014907778668805133,
      "loss": 0.4606,
      "step": 640
    },
    {
      "epoch": 0.25650260104041617,
      "grad_norm": 0.10905152559280396,
      "learning_rate": 0.00014899759422614273,
      "loss": 0.522,
      "step": 641
    },
    {
      "epoch": 0.25690276110444177,
      "grad_norm": 0.1337617188692093,
      "learning_rate": 0.00014891740176423417,
      "loss": 0.5279,
      "step": 642
    },
    {
      "epoch": 0.25730292116846737,
      "grad_norm": 0.08858658373355865,
      "learning_rate": 0.00014883720930232558,
      "loss": 0.4295,
      "step": 643
    },
    {
      "epoch": 0.25770308123249297,
      "grad_norm": 0.084568090736866,
      "learning_rate": 0.000148757016840417,
      "loss": 0.3942,
      "step": 644
    },
    {
      "epoch": 0.2581032412965186,
      "grad_norm": 0.08678965270519257,
      "learning_rate": 0.00014867682437850842,
      "loss": 0.4248,
      "step": 645
    },
    {
      "epoch": 0.2585034013605442,
      "grad_norm": 0.12031874060630798,
      "learning_rate": 0.00014859663191659986,
      "loss": 0.4623,
      "step": 646
    },
    {
      "epoch": 0.25890356142456983,
      "grad_norm": 0.11512953042984009,
      "learning_rate": 0.00014851643945469126,
      "loss": 0.4745,
      "step": 647
    },
    {
      "epoch": 0.25930372148859543,
      "grad_norm": 0.10514448583126068,
      "learning_rate": 0.0001484362469927827,
      "loss": 0.4495,
      "step": 648
    },
    {
      "epoch": 0.25970388155262103,
      "grad_norm": 0.0951298251748085,
      "learning_rate": 0.0001483560545308741,
      "loss": 0.4539,
      "step": 649
    },
    {
      "epoch": 0.2601040416166467,
      "grad_norm": 0.1276664286851883,
      "learning_rate": 0.00014827586206896554,
      "loss": 0.5236,
      "step": 650
    },
    {
      "epoch": 0.2605042016806723,
      "grad_norm": 0.1136435717344284,
      "learning_rate": 0.00014819566960705695,
      "loss": 0.555,
      "step": 651
    },
    {
      "epoch": 0.2609043617446979,
      "grad_norm": 0.10328882187604904,
      "learning_rate": 0.00014811547714514838,
      "loss": 0.3516,
      "step": 652
    },
    {
      "epoch": 0.2613045218087235,
      "grad_norm": 0.12139716744422913,
      "learning_rate": 0.00014803528468323977,
      "loss": 0.4733,
      "step": 653
    },
    {
      "epoch": 0.2617046818727491,
      "grad_norm": 0.08822692185640335,
      "learning_rate": 0.0001479550922213312,
      "loss": 0.4554,
      "step": 654
    },
    {
      "epoch": 0.2621048419367747,
      "grad_norm": 0.10249420255422592,
      "learning_rate": 0.0001478748997594226,
      "loss": 0.4853,
      "step": 655
    },
    {
      "epoch": 0.26250500200080035,
      "grad_norm": 0.11476226150989532,
      "learning_rate": 0.00014779470729751404,
      "loss": 0.3675,
      "step": 656
    },
    {
      "epoch": 0.26290516206482595,
      "grad_norm": 0.09590647369623184,
      "learning_rate": 0.00014771451483560545,
      "loss": 0.4285,
      "step": 657
    },
    {
      "epoch": 0.26330532212885155,
      "grad_norm": 0.1151551678776741,
      "learning_rate": 0.00014763432237369689,
      "loss": 0.5336,
      "step": 658
    },
    {
      "epoch": 0.26370548219287715,
      "grad_norm": 0.09129573404788971,
      "learning_rate": 0.0001475541299117883,
      "loss": 0.4078,
      "step": 659
    },
    {
      "epoch": 0.26410564225690275,
      "grad_norm": 0.10804015398025513,
      "learning_rate": 0.00014747393744987973,
      "loss": 0.4895,
      "step": 660
    },
    {
      "epoch": 0.26450580232092835,
      "grad_norm": 0.12198785692453384,
      "learning_rate": 0.00014739374498797114,
      "loss": 0.4355,
      "step": 661
    },
    {
      "epoch": 0.264905962384954,
      "grad_norm": 0.10453958064317703,
      "learning_rate": 0.00014731355252606257,
      "loss": 0.507,
      "step": 662
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 0.09970512986183167,
      "learning_rate": 0.00014723336006415398,
      "loss": 0.4832,
      "step": 663
    },
    {
      "epoch": 0.2657062825130052,
      "grad_norm": 0.10025846213102341,
      "learning_rate": 0.0001471531676022454,
      "loss": 0.4285,
      "step": 664
    },
    {
      "epoch": 0.2661064425770308,
      "grad_norm": 0.08513985574245453,
      "learning_rate": 0.00014707297514033682,
      "loss": 0.4113,
      "step": 665
    },
    {
      "epoch": 0.2665066026410564,
      "grad_norm": 0.10014118254184723,
      "learning_rate": 0.00014699278267842823,
      "loss": 0.5011,
      "step": 666
    },
    {
      "epoch": 0.266906762705082,
      "grad_norm": 0.11526813358068466,
      "learning_rate": 0.00014691259021651967,
      "loss": 0.512,
      "step": 667
    },
    {
      "epoch": 0.26730692276910767,
      "grad_norm": 0.13163970410823822,
      "learning_rate": 0.00014683239775461107,
      "loss": 0.5541,
      "step": 668
    },
    {
      "epoch": 0.26770708283313327,
      "grad_norm": 0.09756971150636673,
      "learning_rate": 0.00014675220529270248,
      "loss": 0.3857,
      "step": 669
    },
    {
      "epoch": 0.26810724289715887,
      "grad_norm": 0.10816815495491028,
      "learning_rate": 0.00014667201283079392,
      "loss": 0.4502,
      "step": 670
    },
    {
      "epoch": 0.26850740296118447,
      "grad_norm": 0.09872962534427643,
      "learning_rate": 0.00014659182036888532,
      "loss": 0.4926,
      "step": 671
    },
    {
      "epoch": 0.2689075630252101,
      "grad_norm": 0.09441084414720535,
      "learning_rate": 0.00014651162790697673,
      "loss": 0.421,
      "step": 672
    },
    {
      "epoch": 0.2693077230892357,
      "grad_norm": 0.10542917996644974,
      "learning_rate": 0.00014643143544506817,
      "loss": 0.4816,
      "step": 673
    },
    {
      "epoch": 0.26970788315326133,
      "grad_norm": 0.10315228998661041,
      "learning_rate": 0.00014635124298315958,
      "loss": 0.3764,
      "step": 674
    },
    {
      "epoch": 0.27010804321728693,
      "grad_norm": 0.09836337715387344,
      "learning_rate": 0.000146271050521251,
      "loss": 0.4452,
      "step": 675
    },
    {
      "epoch": 0.27050820328131253,
      "grad_norm": 0.10716263949871063,
      "learning_rate": 0.00014619085805934242,
      "loss": 0.4608,
      "step": 676
    },
    {
      "epoch": 0.27090836334533813,
      "grad_norm": 0.08322879672050476,
      "learning_rate": 0.00014611066559743385,
      "loss": 0.4604,
      "step": 677
    },
    {
      "epoch": 0.27130852340936373,
      "grad_norm": 0.10748372226953506,
      "learning_rate": 0.00014603047313552526,
      "loss": 0.5291,
      "step": 678
    },
    {
      "epoch": 0.27170868347338933,
      "grad_norm": 0.10038422048091888,
      "learning_rate": 0.0001459502806736167,
      "loss": 0.4763,
      "step": 679
    },
    {
      "epoch": 0.272108843537415,
      "grad_norm": 0.11350353062152863,
      "learning_rate": 0.0001458700882117081,
      "loss": 0.5378,
      "step": 680
    },
    {
      "epoch": 0.2725090036014406,
      "grad_norm": 0.10374245792627335,
      "learning_rate": 0.00014578989574979954,
      "loss": 0.5288,
      "step": 681
    },
    {
      "epoch": 0.2729091636654662,
      "grad_norm": 0.08633337169885635,
      "learning_rate": 0.00014570970328789095,
      "loss": 0.42,
      "step": 682
    },
    {
      "epoch": 0.2733093237294918,
      "grad_norm": 0.11000722646713257,
      "learning_rate": 0.00014562951082598238,
      "loss": 0.5332,
      "step": 683
    },
    {
      "epoch": 0.2737094837935174,
      "grad_norm": 0.09239591658115387,
      "learning_rate": 0.0001455493183640738,
      "loss": 0.4683,
      "step": 684
    },
    {
      "epoch": 0.274109643857543,
      "grad_norm": 0.07881788909435272,
      "learning_rate": 0.0001454691259021652,
      "loss": 0.3933,
      "step": 685
    },
    {
      "epoch": 0.27450980392156865,
      "grad_norm": 0.08492598682641983,
      "learning_rate": 0.0001453889334402566,
      "loss": 0.3702,
      "step": 686
    },
    {
      "epoch": 0.27490996398559425,
      "grad_norm": 0.10537252575159073,
      "learning_rate": 0.00014530874097834804,
      "loss": 0.4332,
      "step": 687
    },
    {
      "epoch": 0.27531012404961985,
      "grad_norm": 0.11362538486719131,
      "learning_rate": 0.00014522854851643945,
      "loss": 0.5365,
      "step": 688
    },
    {
      "epoch": 0.27571028411364545,
      "grad_norm": 0.1036703884601593,
      "learning_rate": 0.00014514835605453088,
      "loss": 0.4826,
      "step": 689
    },
    {
      "epoch": 0.27611044417767105,
      "grad_norm": 0.0623314268887043,
      "learning_rate": 0.0001450681635926223,
      "loss": 0.3002,
      "step": 690
    },
    {
      "epoch": 0.27651060424169666,
      "grad_norm": 0.13337503373622894,
      "learning_rate": 0.00014498797113071373,
      "loss": 0.5113,
      "step": 691
    },
    {
      "epoch": 0.2769107643057223,
      "grad_norm": 0.08505397289991379,
      "learning_rate": 0.00014490777866880513,
      "loss": 0.3933,
      "step": 692
    },
    {
      "epoch": 0.2773109243697479,
      "grad_norm": 0.12064342200756073,
      "learning_rate": 0.00014482758620689657,
      "loss": 0.45,
      "step": 693
    },
    {
      "epoch": 0.2777110844337735,
      "grad_norm": 0.09858442842960358,
      "learning_rate": 0.00014474739374498798,
      "loss": 0.387,
      "step": 694
    },
    {
      "epoch": 0.2781112444977991,
      "grad_norm": 0.08867453038692474,
      "learning_rate": 0.0001446672012830794,
      "loss": 0.3491,
      "step": 695
    },
    {
      "epoch": 0.2785114045618247,
      "grad_norm": 0.11412611603736877,
      "learning_rate": 0.00014458700882117082,
      "loss": 0.3885,
      "step": 696
    },
    {
      "epoch": 0.2789115646258503,
      "grad_norm": 0.1103549376130104,
      "learning_rate": 0.00014450681635926226,
      "loss": 0.4508,
      "step": 697
    },
    {
      "epoch": 0.279311724689876,
      "grad_norm": 0.10054961591959,
      "learning_rate": 0.00014442662389735366,
      "loss": 0.4766,
      "step": 698
    },
    {
      "epoch": 0.2797118847539016,
      "grad_norm": 0.10721568018198013,
      "learning_rate": 0.00014434643143544507,
      "loss": 0.4758,
      "step": 699
    },
    {
      "epoch": 0.2801120448179272,
      "grad_norm": 0.1476898193359375,
      "learning_rate": 0.0001442662389735365,
      "loss": 0.5123,
      "step": 700
    },
    {
      "epoch": 0.2805122048819528,
      "grad_norm": 0.11125335097312927,
      "learning_rate": 0.00014418604651162791,
      "loss": 0.3473,
      "step": 701
    },
    {
      "epoch": 0.2809123649459784,
      "grad_norm": 0.11582662165164948,
      "learning_rate": 0.00014410585404971932,
      "loss": 0.531,
      "step": 702
    },
    {
      "epoch": 0.281312525010004,
      "grad_norm": 0.09548637270927429,
      "learning_rate": 0.00014402566158781076,
      "loss": 0.4487,
      "step": 703
    },
    {
      "epoch": 0.28171268507402963,
      "grad_norm": 0.12128285318613052,
      "learning_rate": 0.00014394546912590217,
      "loss": 0.4889,
      "step": 704
    },
    {
      "epoch": 0.28211284513805523,
      "grad_norm": 0.09255630522966385,
      "learning_rate": 0.00014386527666399357,
      "loss": 0.3741,
      "step": 705
    },
    {
      "epoch": 0.28251300520208084,
      "grad_norm": 0.10286837071180344,
      "learning_rate": 0.000143785084202085,
      "loss": 0.4455,
      "step": 706
    },
    {
      "epoch": 0.28291316526610644,
      "grad_norm": 0.12863335013389587,
      "learning_rate": 0.00014370489174017642,
      "loss": 0.5148,
      "step": 707
    },
    {
      "epoch": 0.28331332533013204,
      "grad_norm": 0.09076917171478271,
      "learning_rate": 0.00014362469927826785,
      "loss": 0.4182,
      "step": 708
    },
    {
      "epoch": 0.28371348539415764,
      "grad_norm": 0.0996486097574234,
      "learning_rate": 0.00014354450681635926,
      "loss": 0.4602,
      "step": 709
    },
    {
      "epoch": 0.2841136454581833,
      "grad_norm": 0.08388403803110123,
      "learning_rate": 0.0001434643143544507,
      "loss": 0.3761,
      "step": 710
    },
    {
      "epoch": 0.2845138055222089,
      "grad_norm": 0.09970948845148087,
      "learning_rate": 0.0001433841218925421,
      "loss": 0.4972,
      "step": 711
    },
    {
      "epoch": 0.2849139655862345,
      "grad_norm": 0.09980638325214386,
      "learning_rate": 0.00014330392943063354,
      "loss": 0.3837,
      "step": 712
    },
    {
      "epoch": 0.2853141256502601,
      "grad_norm": 0.10571427643299103,
      "learning_rate": 0.00014322373696872495,
      "loss": 0.4944,
      "step": 713
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.10152791440486908,
      "learning_rate": 0.00014314354450681638,
      "loss": 0.3959,
      "step": 714
    },
    {
      "epoch": 0.2861144457783113,
      "grad_norm": 0.12270516902208328,
      "learning_rate": 0.0001430633520449078,
      "loss": 0.5333,
      "step": 715
    },
    {
      "epoch": 0.28651460584233696,
      "grad_norm": 0.10325590521097183,
      "learning_rate": 0.00014298315958299922,
      "loss": 0.4536,
      "step": 716
    },
    {
      "epoch": 0.28691476590636256,
      "grad_norm": 0.09722822159528732,
      "learning_rate": 0.0001429029671210906,
      "loss": 0.413,
      "step": 717
    },
    {
      "epoch": 0.28731492597038816,
      "grad_norm": 0.1077728345990181,
      "learning_rate": 0.00014282277465918204,
      "loss": 0.4849,
      "step": 718
    },
    {
      "epoch": 0.28771508603441376,
      "grad_norm": 0.09992499649524689,
      "learning_rate": 0.00014274258219727345,
      "loss": 0.456,
      "step": 719
    },
    {
      "epoch": 0.28811524609843936,
      "grad_norm": 0.10984133183956146,
      "learning_rate": 0.00014266238973536488,
      "loss": 0.5268,
      "step": 720
    },
    {
      "epoch": 0.28851540616246496,
      "grad_norm": 0.09274203330278397,
      "learning_rate": 0.0001425821972734563,
      "loss": 0.3816,
      "step": 721
    },
    {
      "epoch": 0.2889155662264906,
      "grad_norm": 0.12189579755067825,
      "learning_rate": 0.00014250200481154772,
      "loss": 0.4198,
      "step": 722
    },
    {
      "epoch": 0.2893157262905162,
      "grad_norm": 0.09223438054323196,
      "learning_rate": 0.00014242181234963913,
      "loss": 0.4538,
      "step": 723
    },
    {
      "epoch": 0.2897158863545418,
      "grad_norm": 0.11291654407978058,
      "learning_rate": 0.00014234161988773057,
      "loss": 0.5288,
      "step": 724
    },
    {
      "epoch": 0.2901160464185674,
      "grad_norm": 0.07849812507629395,
      "learning_rate": 0.00014226142742582198,
      "loss": 0.3912,
      "step": 725
    },
    {
      "epoch": 0.290516206482593,
      "grad_norm": 0.11184898763895035,
      "learning_rate": 0.0001421812349639134,
      "loss": 0.5219,
      "step": 726
    },
    {
      "epoch": 0.2909163665466186,
      "grad_norm": 0.10119235515594482,
      "learning_rate": 0.00014210104250200482,
      "loss": 0.4659,
      "step": 727
    },
    {
      "epoch": 0.2913165266106443,
      "grad_norm": 0.08812941610813141,
      "learning_rate": 0.00014202085004009625,
      "loss": 0.3995,
      "step": 728
    },
    {
      "epoch": 0.2917166866746699,
      "grad_norm": 0.12856195867061615,
      "learning_rate": 0.00014194065757818766,
      "loss": 0.4926,
      "step": 729
    },
    {
      "epoch": 0.2921168467386955,
      "grad_norm": 0.11763761937618256,
      "learning_rate": 0.0001418604651162791,
      "loss": 0.5173,
      "step": 730
    },
    {
      "epoch": 0.2925170068027211,
      "grad_norm": 0.1411912590265274,
      "learning_rate": 0.0001417802726543705,
      "loss": 0.4555,
      "step": 731
    },
    {
      "epoch": 0.2929171668667467,
      "grad_norm": 0.12087266892194748,
      "learning_rate": 0.00014170008019246194,
      "loss": 0.3916,
      "step": 732
    },
    {
      "epoch": 0.2933173269307723,
      "grad_norm": 0.12341030687093735,
      "learning_rate": 0.00014161988773055332,
      "loss": 0.4669,
      "step": 733
    },
    {
      "epoch": 0.29371748699479794,
      "grad_norm": 0.11130162328481674,
      "learning_rate": 0.00014153969526864476,
      "loss": 0.5001,
      "step": 734
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.11604002863168716,
      "learning_rate": 0.00014145950280673616,
      "loss": 0.5402,
      "step": 735
    },
    {
      "epoch": 0.29451780712284914,
      "grad_norm": 0.09595400094985962,
      "learning_rate": 0.0001413793103448276,
      "loss": 0.4365,
      "step": 736
    },
    {
      "epoch": 0.29491796718687474,
      "grad_norm": 0.08956795185804367,
      "learning_rate": 0.000141299117882919,
      "loss": 0.3968,
      "step": 737
    },
    {
      "epoch": 0.29531812725090034,
      "grad_norm": 0.08805252611637115,
      "learning_rate": 0.00014121892542101044,
      "loss": 0.4229,
      "step": 738
    },
    {
      "epoch": 0.29571828731492594,
      "grad_norm": 0.10169295221567154,
      "learning_rate": 0.00014113873295910185,
      "loss": 0.4026,
      "step": 739
    },
    {
      "epoch": 0.2961184473789516,
      "grad_norm": 0.09983669966459274,
      "learning_rate": 0.00014105854049719326,
      "loss": 0.4633,
      "step": 740
    },
    {
      "epoch": 0.2965186074429772,
      "grad_norm": 0.11083000898361206,
      "learning_rate": 0.0001409783480352847,
      "loss": 0.4428,
      "step": 741
    },
    {
      "epoch": 0.2969187675070028,
      "grad_norm": 0.12056352943181992,
      "learning_rate": 0.0001408981555733761,
      "loss": 0.5416,
      "step": 742
    },
    {
      "epoch": 0.2973189275710284,
      "grad_norm": 0.11290112882852554,
      "learning_rate": 0.00014081796311146753,
      "loss": 0.4971,
      "step": 743
    },
    {
      "epoch": 0.297719087635054,
      "grad_norm": 0.09833595156669617,
      "learning_rate": 0.00014073777064955894,
      "loss": 0.4137,
      "step": 744
    },
    {
      "epoch": 0.29811924769907966,
      "grad_norm": 0.11409157514572144,
      "learning_rate": 0.00014065757818765038,
      "loss": 0.5374,
      "step": 745
    },
    {
      "epoch": 0.29851940776310526,
      "grad_norm": 0.09998801350593567,
      "learning_rate": 0.00014057738572574179,
      "loss": 0.3865,
      "step": 746
    },
    {
      "epoch": 0.29891956782713086,
      "grad_norm": 0.10509955137968063,
      "learning_rate": 0.00014049719326383322,
      "loss": 0.4366,
      "step": 747
    },
    {
      "epoch": 0.29931972789115646,
      "grad_norm": 0.11832384765148163,
      "learning_rate": 0.00014041700080192463,
      "loss": 0.5409,
      "step": 748
    },
    {
      "epoch": 0.29971988795518206,
      "grad_norm": 0.10924042016267776,
      "learning_rate": 0.00014033680834001604,
      "loss": 0.4921,
      "step": 749
    },
    {
      "epoch": 0.30012004801920766,
      "grad_norm": 0.10502813011407852,
      "learning_rate": 0.00014025661587810744,
      "loss": 0.468,
      "step": 750
    },
    {
      "epoch": 0.3005202080832333,
      "grad_norm": 0.08681441843509674,
      "learning_rate": 0.00014017642341619888,
      "loss": 0.4023,
      "step": 751
    },
    {
      "epoch": 0.3009203681472589,
      "grad_norm": 0.11188258230686188,
      "learning_rate": 0.0001400962309542903,
      "loss": 0.4723,
      "step": 752
    },
    {
      "epoch": 0.3013205282112845,
      "grad_norm": 0.10683540254831314,
      "learning_rate": 0.00014001603849238172,
      "loss": 0.4822,
      "step": 753
    },
    {
      "epoch": 0.3017206882753101,
      "grad_norm": 0.09656763821840286,
      "learning_rate": 0.00013993584603047313,
      "loss": 0.4327,
      "step": 754
    },
    {
      "epoch": 0.3021208483393357,
      "grad_norm": 0.10985096544027328,
      "learning_rate": 0.00013985565356856457,
      "loss": 0.5096,
      "step": 755
    },
    {
      "epoch": 0.3025210084033613,
      "grad_norm": 0.09251323342323303,
      "learning_rate": 0.00013977546110665597,
      "loss": 0.4773,
      "step": 756
    },
    {
      "epoch": 0.302921168467387,
      "grad_norm": 0.10544517636299133,
      "learning_rate": 0.0001396952686447474,
      "loss": 0.4649,
      "step": 757
    },
    {
      "epoch": 0.3033213285314126,
      "grad_norm": 0.1275033950805664,
      "learning_rate": 0.00013961507618283882,
      "loss": 0.4783,
      "step": 758
    },
    {
      "epoch": 0.3037214885954382,
      "grad_norm": 0.11376877129077911,
      "learning_rate": 0.00013953488372093025,
      "loss": 0.4918,
      "step": 759
    },
    {
      "epoch": 0.3041216486594638,
      "grad_norm": 0.12291640043258667,
      "learning_rate": 0.00013945469125902166,
      "loss": 0.4232,
      "step": 760
    },
    {
      "epoch": 0.3045218087234894,
      "grad_norm": 0.08316820859909058,
      "learning_rate": 0.0001393744987971131,
      "loss": 0.3898,
      "step": 761
    },
    {
      "epoch": 0.304921968787515,
      "grad_norm": 0.09135390818119049,
      "learning_rate": 0.0001392943063352045,
      "loss": 0.4017,
      "step": 762
    },
    {
      "epoch": 0.30532212885154064,
      "grad_norm": 0.09020765870809555,
      "learning_rate": 0.00013921411387329594,
      "loss": 0.428,
      "step": 763
    },
    {
      "epoch": 0.30572228891556624,
      "grad_norm": 0.12810572981834412,
      "learning_rate": 0.00013913392141138735,
      "loss": 0.4887,
      "step": 764
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 0.10381315648555756,
      "learning_rate": 0.00013905372894947875,
      "loss": 0.4243,
      "step": 765
    },
    {
      "epoch": 0.30652260904361744,
      "grad_norm": 0.12025300413370132,
      "learning_rate": 0.00013897353648757016,
      "loss": 0.6022,
      "step": 766
    },
    {
      "epoch": 0.30692276910764305,
      "grad_norm": 0.09695085138082504,
      "learning_rate": 0.0001388933440256616,
      "loss": 0.4788,
      "step": 767
    },
    {
      "epoch": 0.30732292917166865,
      "grad_norm": 0.1084648072719574,
      "learning_rate": 0.000138813151563753,
      "loss": 0.4219,
      "step": 768
    },
    {
      "epoch": 0.3077230892356943,
      "grad_norm": 0.09261946380138397,
      "learning_rate": 0.00013873295910184444,
      "loss": 0.4499,
      "step": 769
    },
    {
      "epoch": 0.3081232492997199,
      "grad_norm": 0.11121386289596558,
      "learning_rate": 0.00013865276663993585,
      "loss": 0.4656,
      "step": 770
    },
    {
      "epoch": 0.3085234093637455,
      "grad_norm": 0.14905573427677155,
      "learning_rate": 0.00013857257417802728,
      "loss": 0.5507,
      "step": 771
    },
    {
      "epoch": 0.3089235694277711,
      "grad_norm": 0.10576284676790237,
      "learning_rate": 0.0001384923817161187,
      "loss": 0.4612,
      "step": 772
    },
    {
      "epoch": 0.3093237294917967,
      "grad_norm": 0.11000300198793411,
      "learning_rate": 0.00013841218925421012,
      "loss": 0.4853,
      "step": 773
    },
    {
      "epoch": 0.3097238895558223,
      "grad_norm": 0.1311342716217041,
      "learning_rate": 0.00013833199679230153,
      "loss": 0.5502,
      "step": 774
    },
    {
      "epoch": 0.31012404961984796,
      "grad_norm": 0.10611163824796677,
      "learning_rate": 0.00013825180433039294,
      "loss": 0.4647,
      "step": 775
    },
    {
      "epoch": 0.31052420968387356,
      "grad_norm": 0.09327955543994904,
      "learning_rate": 0.00013817161186848438,
      "loss": 0.4709,
      "step": 776
    },
    {
      "epoch": 0.31092436974789917,
      "grad_norm": 0.0963180661201477,
      "learning_rate": 0.00013809141940657578,
      "loss": 0.4694,
      "step": 777
    },
    {
      "epoch": 0.31132452981192477,
      "grad_norm": 0.10744789242744446,
      "learning_rate": 0.00013801122694466722,
      "loss": 0.4218,
      "step": 778
    },
    {
      "epoch": 0.31172468987595037,
      "grad_norm": 0.10301816463470459,
      "learning_rate": 0.00013793103448275863,
      "loss": 0.4624,
      "step": 779
    },
    {
      "epoch": 0.31212484993997597,
      "grad_norm": 0.08755705505609512,
      "learning_rate": 0.00013785084202085006,
      "loss": 0.4389,
      "step": 780
    },
    {
      "epoch": 0.3125250100040016,
      "grad_norm": 0.08121785521507263,
      "learning_rate": 0.00013777064955894147,
      "loss": 0.3992,
      "step": 781
    },
    {
      "epoch": 0.3129251700680272,
      "grad_norm": 0.11059805005788803,
      "learning_rate": 0.00013769045709703288,
      "loss": 0.425,
      "step": 782
    },
    {
      "epoch": 0.3133253301320528,
      "grad_norm": 0.08713692426681519,
      "learning_rate": 0.00013761026463512429,
      "loss": 0.4437,
      "step": 783
    },
    {
      "epoch": 0.3137254901960784,
      "grad_norm": 0.10183542221784592,
      "learning_rate": 0.00013753007217321572,
      "loss": 0.4109,
      "step": 784
    },
    {
      "epoch": 0.31412565026010403,
      "grad_norm": 0.10797328501939774,
      "learning_rate": 0.00013744987971130713,
      "loss": 0.5571,
      "step": 785
    },
    {
      "epoch": 0.31452581032412963,
      "grad_norm": 0.09564205259084702,
      "learning_rate": 0.00013736968724939856,
      "loss": 0.4389,
      "step": 786
    },
    {
      "epoch": 0.3149259703881553,
      "grad_norm": 0.09687046706676483,
      "learning_rate": 0.00013728949478748997,
      "loss": 0.3928,
      "step": 787
    },
    {
      "epoch": 0.3153261304521809,
      "grad_norm": 0.08223414421081543,
      "learning_rate": 0.0001372093023255814,
      "loss": 0.3929,
      "step": 788
    },
    {
      "epoch": 0.3157262905162065,
      "grad_norm": 0.10127416998147964,
      "learning_rate": 0.00013712910986367281,
      "loss": 0.4717,
      "step": 789
    },
    {
      "epoch": 0.3161264505802321,
      "grad_norm": 0.09481684863567352,
      "learning_rate": 0.00013704891740176425,
      "loss": 0.4456,
      "step": 790
    },
    {
      "epoch": 0.3165266106442577,
      "grad_norm": 0.09201471507549286,
      "learning_rate": 0.00013696872493985566,
      "loss": 0.4254,
      "step": 791
    },
    {
      "epoch": 0.3169267707082833,
      "grad_norm": 0.10141505300998688,
      "learning_rate": 0.0001368885324779471,
      "loss": 0.3976,
      "step": 792
    },
    {
      "epoch": 0.31732693077230895,
      "grad_norm": 0.09647074341773987,
      "learning_rate": 0.0001368083400160385,
      "loss": 0.4155,
      "step": 793
    },
    {
      "epoch": 0.31772709083633455,
      "grad_norm": 0.1080864667892456,
      "learning_rate": 0.00013672814755412993,
      "loss": 0.5309,
      "step": 794
    },
    {
      "epoch": 0.31812725090036015,
      "grad_norm": 0.10780368745326996,
      "learning_rate": 0.00013664795509222134,
      "loss": 0.4586,
      "step": 795
    },
    {
      "epoch": 0.31852741096438575,
      "grad_norm": 0.0973954126238823,
      "learning_rate": 0.00013656776263031278,
      "loss": 0.4478,
      "step": 796
    },
    {
      "epoch": 0.31892757102841135,
      "grad_norm": 0.09241922944784164,
      "learning_rate": 0.00013648757016840416,
      "loss": 0.4519,
      "step": 797
    },
    {
      "epoch": 0.31932773109243695,
      "grad_norm": 0.11277785897254944,
      "learning_rate": 0.0001364073777064956,
      "loss": 0.53,
      "step": 798
    },
    {
      "epoch": 0.3197278911564626,
      "grad_norm": 0.09339315444231033,
      "learning_rate": 0.000136327185244587,
      "loss": 0.4782,
      "step": 799
    },
    {
      "epoch": 0.3201280512204882,
      "grad_norm": 0.11507996171712875,
      "learning_rate": 0.00013624699278267844,
      "loss": 0.5386,
      "step": 800
    },
    {
      "epoch": 0.3205282112845138,
      "grad_norm": 0.12348192930221558,
      "learning_rate": 0.00013616680032076984,
      "loss": 0.5294,
      "step": 801
    },
    {
      "epoch": 0.3209283713485394,
      "grad_norm": 0.07744405418634415,
      "learning_rate": 0.00013608660785886128,
      "loss": 0.3658,
      "step": 802
    },
    {
      "epoch": 0.321328531412565,
      "grad_norm": 0.11697675287723541,
      "learning_rate": 0.0001360064153969527,
      "loss": 0.4428,
      "step": 803
    },
    {
      "epoch": 0.3217286914765906,
      "grad_norm": 0.12192779034376144,
      "learning_rate": 0.00013592622293504412,
      "loss": 0.5767,
      "step": 804
    },
    {
      "epoch": 0.32212885154061627,
      "grad_norm": 0.10577291995286942,
      "learning_rate": 0.00013584603047313553,
      "loss": 0.522,
      "step": 805
    },
    {
      "epoch": 0.32252901160464187,
      "grad_norm": 0.0956336110830307,
      "learning_rate": 0.00013576583801122697,
      "loss": 0.4785,
      "step": 806
    },
    {
      "epoch": 0.32292917166866747,
      "grad_norm": 0.11227645725011826,
      "learning_rate": 0.00013568564554931837,
      "loss": 0.4709,
      "step": 807
    },
    {
      "epoch": 0.32332933173269307,
      "grad_norm": 0.08518116176128387,
      "learning_rate": 0.0001356054530874098,
      "loss": 0.4499,
      "step": 808
    },
    {
      "epoch": 0.32372949179671867,
      "grad_norm": 0.09598628431558609,
      "learning_rate": 0.00013552526062550122,
      "loss": 0.4173,
      "step": 809
    },
    {
      "epoch": 0.3241296518607443,
      "grad_norm": 0.10107007622718811,
      "learning_rate": 0.00013544506816359262,
      "loss": 0.4119,
      "step": 810
    },
    {
      "epoch": 0.32452981192476993,
      "grad_norm": 0.10326924175024033,
      "learning_rate": 0.00013536487570168406,
      "loss": 0.464,
      "step": 811
    },
    {
      "epoch": 0.32492997198879553,
      "grad_norm": 0.10528437793254852,
      "learning_rate": 0.00013528468323977547,
      "loss": 0.4231,
      "step": 812
    },
    {
      "epoch": 0.32533013205282113,
      "grad_norm": 0.0899898111820221,
      "learning_rate": 0.00013520449077786687,
      "loss": 0.4538,
      "step": 813
    },
    {
      "epoch": 0.32573029211684673,
      "grad_norm": 0.1111583560705185,
      "learning_rate": 0.0001351242983159583,
      "loss": 0.4608,
      "step": 814
    },
    {
      "epoch": 0.32613045218087233,
      "grad_norm": 0.09797842800617218,
      "learning_rate": 0.00013504410585404972,
      "loss": 0.4337,
      "step": 815
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 0.1260267049074173,
      "learning_rate": 0.00013496391339214113,
      "loss": 0.5211,
      "step": 816
    },
    {
      "epoch": 0.3269307723089236,
      "grad_norm": 0.0814935490489006,
      "learning_rate": 0.00013488372093023256,
      "loss": 0.4034,
      "step": 817
    },
    {
      "epoch": 0.3273309323729492,
      "grad_norm": 0.10073037445545197,
      "learning_rate": 0.00013480352846832397,
      "loss": 0.4685,
      "step": 818
    },
    {
      "epoch": 0.3277310924369748,
      "grad_norm": 0.09826426208019257,
      "learning_rate": 0.0001347233360064154,
      "loss": 0.5106,
      "step": 819
    },
    {
      "epoch": 0.3281312525010004,
      "grad_norm": 0.08741270750761032,
      "learning_rate": 0.0001346431435445068,
      "loss": 0.3931,
      "step": 820
    },
    {
      "epoch": 0.328531412565026,
      "grad_norm": 0.07894144207239151,
      "learning_rate": 0.00013456295108259825,
      "loss": 0.4159,
      "step": 821
    },
    {
      "epoch": 0.3289315726290516,
      "grad_norm": 0.0993923768401146,
      "learning_rate": 0.00013448275862068965,
      "loss": 0.5325,
      "step": 822
    },
    {
      "epoch": 0.32933173269307725,
      "grad_norm": 0.11350177228450775,
      "learning_rate": 0.0001344025661587811,
      "loss": 0.5489,
      "step": 823
    },
    {
      "epoch": 0.32973189275710285,
      "grad_norm": 0.10129334777593613,
      "learning_rate": 0.0001343223736968725,
      "loss": 0.4224,
      "step": 824
    },
    {
      "epoch": 0.33013205282112845,
      "grad_norm": 0.11718065291643143,
      "learning_rate": 0.00013424218123496393,
      "loss": 0.4367,
      "step": 825
    },
    {
      "epoch": 0.33053221288515405,
      "grad_norm": 0.11634234338998795,
      "learning_rate": 0.00013416198877305534,
      "loss": 0.4801,
      "step": 826
    },
    {
      "epoch": 0.33093237294917965,
      "grad_norm": 0.117782361805439,
      "learning_rate": 0.00013408179631114678,
      "loss": 0.4992,
      "step": 827
    },
    {
      "epoch": 0.33133253301320525,
      "grad_norm": 0.10326941311359406,
      "learning_rate": 0.00013400160384923818,
      "loss": 0.4448,
      "step": 828
    },
    {
      "epoch": 0.3317326930772309,
      "grad_norm": 0.10553112626075745,
      "learning_rate": 0.0001339214113873296,
      "loss": 0.4534,
      "step": 829
    },
    {
      "epoch": 0.3321328531412565,
      "grad_norm": 0.11964339762926102,
      "learning_rate": 0.000133841218925421,
      "loss": 0.4917,
      "step": 830
    },
    {
      "epoch": 0.3325330132052821,
      "grad_norm": 0.10904645174741745,
      "learning_rate": 0.00013376102646351243,
      "loss": 0.4659,
      "step": 831
    },
    {
      "epoch": 0.3329331732693077,
      "grad_norm": 0.10159751772880554,
      "learning_rate": 0.00013368083400160384,
      "loss": 0.4095,
      "step": 832
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.11381979286670685,
      "learning_rate": 0.00013360064153969528,
      "loss": 0.4409,
      "step": 833
    },
    {
      "epoch": 0.33373349339735897,
      "grad_norm": 0.0823332816362381,
      "learning_rate": 0.00013352044907778669,
      "loss": 0.4038,
      "step": 834
    },
    {
      "epoch": 0.33413365346138457,
      "grad_norm": 0.0870141088962555,
      "learning_rate": 0.00013344025661587812,
      "loss": 0.4556,
      "step": 835
    },
    {
      "epoch": 0.3345338135254102,
      "grad_norm": 0.08270920813083649,
      "learning_rate": 0.00013336006415396953,
      "loss": 0.4038,
      "step": 836
    },
    {
      "epoch": 0.3349339735894358,
      "grad_norm": 0.10817073285579681,
      "learning_rate": 0.00013327987169206096,
      "loss": 0.5338,
      "step": 837
    },
    {
      "epoch": 0.3353341336534614,
      "grad_norm": 0.12431511282920837,
      "learning_rate": 0.00013319967923015237,
      "loss": 0.4463,
      "step": 838
    },
    {
      "epoch": 0.335734293717487,
      "grad_norm": 0.10714436322450638,
      "learning_rate": 0.0001331194867682438,
      "loss": 0.4425,
      "step": 839
    },
    {
      "epoch": 0.33613445378151263,
      "grad_norm": 0.11001157760620117,
      "learning_rate": 0.00013303929430633521,
      "loss": 0.5209,
      "step": 840
    },
    {
      "epoch": 0.33653461384553823,
      "grad_norm": 0.12274233251810074,
      "learning_rate": 0.00013295910184442665,
      "loss": 0.4784,
      "step": 841
    },
    {
      "epoch": 0.33693477390956383,
      "grad_norm": 0.11953774839639664,
      "learning_rate": 0.00013287890938251806,
      "loss": 0.5512,
      "step": 842
    },
    {
      "epoch": 0.33733493397358943,
      "grad_norm": 0.11099031567573547,
      "learning_rate": 0.0001327987169206095,
      "loss": 0.5205,
      "step": 843
    },
    {
      "epoch": 0.33773509403761504,
      "grad_norm": 0.10352311283349991,
      "learning_rate": 0.0001327185244587009,
      "loss": 0.4217,
      "step": 844
    },
    {
      "epoch": 0.33813525410164064,
      "grad_norm": 0.12576209008693695,
      "learning_rate": 0.0001326383319967923,
      "loss": 0.6032,
      "step": 845
    },
    {
      "epoch": 0.3385354141656663,
      "grad_norm": 0.09505904465913773,
      "learning_rate": 0.00013255813953488372,
      "loss": 0.4965,
      "step": 846
    },
    {
      "epoch": 0.3389355742296919,
      "grad_norm": 0.08186458051204681,
      "learning_rate": 0.00013247794707297515,
      "loss": 0.3779,
      "step": 847
    },
    {
      "epoch": 0.3393357342937175,
      "grad_norm": 0.10890454053878784,
      "learning_rate": 0.00013239775461106656,
      "loss": 0.4873,
      "step": 848
    },
    {
      "epoch": 0.3397358943577431,
      "grad_norm": 0.09332189708948135,
      "learning_rate": 0.000132317562149158,
      "loss": 0.4848,
      "step": 849
    },
    {
      "epoch": 0.3401360544217687,
      "grad_norm": 0.09703756868839264,
      "learning_rate": 0.0001322373696872494,
      "loss": 0.4455,
      "step": 850
    },
    {
      "epoch": 0.3405362144857943,
      "grad_norm": 0.10383162647485733,
      "learning_rate": 0.0001321571772253408,
      "loss": 0.4152,
      "step": 851
    },
    {
      "epoch": 0.34093637454981995,
      "grad_norm": 0.10366512089967728,
      "learning_rate": 0.00013207698476343224,
      "loss": 0.4747,
      "step": 852
    },
    {
      "epoch": 0.34133653461384555,
      "grad_norm": 0.10632091760635376,
      "learning_rate": 0.00013199679230152365,
      "loss": 0.4239,
      "step": 853
    },
    {
      "epoch": 0.34173669467787116,
      "grad_norm": 0.09707171469926834,
      "learning_rate": 0.0001319165998396151,
      "loss": 0.4307,
      "step": 854
    },
    {
      "epoch": 0.34213685474189676,
      "grad_norm": 0.08127851039171219,
      "learning_rate": 0.0001318364073777065,
      "loss": 0.3252,
      "step": 855
    },
    {
      "epoch": 0.34253701480592236,
      "grad_norm": 0.10788492858409882,
      "learning_rate": 0.00013175621491579793,
      "loss": 0.4892,
      "step": 856
    },
    {
      "epoch": 0.34293717486994796,
      "grad_norm": 0.09902296960353851,
      "learning_rate": 0.00013167602245388934,
      "loss": 0.4554,
      "step": 857
    },
    {
      "epoch": 0.3433373349339736,
      "grad_norm": 0.10489381104707718,
      "learning_rate": 0.00013159582999198077,
      "loss": 0.5247,
      "step": 858
    },
    {
      "epoch": 0.3437374949979992,
      "grad_norm": 0.10308690369129181,
      "learning_rate": 0.00013151563753007218,
      "loss": 0.4687,
      "step": 859
    },
    {
      "epoch": 0.3441376550620248,
      "grad_norm": 0.0964018851518631,
      "learning_rate": 0.00013143544506816362,
      "loss": 0.4629,
      "step": 860
    },
    {
      "epoch": 0.3445378151260504,
      "grad_norm": 0.12953303754329681,
      "learning_rate": 0.000131355252606255,
      "loss": 0.5189,
      "step": 861
    },
    {
      "epoch": 0.344937975190076,
      "grad_norm": 0.0936390683054924,
      "learning_rate": 0.00013127506014434643,
      "loss": 0.4404,
      "step": 862
    },
    {
      "epoch": 0.3453381352541016,
      "grad_norm": 0.09864021092653275,
      "learning_rate": 0.00013119486768243784,
      "loss": 0.4495,
      "step": 863
    },
    {
      "epoch": 0.3457382953181273,
      "grad_norm": 0.10743598639965057,
      "learning_rate": 0.00013111467522052927,
      "loss": 0.4691,
      "step": 864
    },
    {
      "epoch": 0.3461384553821529,
      "grad_norm": 0.09985540807247162,
      "learning_rate": 0.00013103448275862068,
      "loss": 0.4418,
      "step": 865
    },
    {
      "epoch": 0.3465386154461785,
      "grad_norm": 0.11351116746664047,
      "learning_rate": 0.00013095429029671212,
      "loss": 0.4676,
      "step": 866
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 0.09684112668037415,
      "learning_rate": 0.00013087409783480353,
      "loss": 0.3991,
      "step": 867
    },
    {
      "epoch": 0.3473389355742297,
      "grad_norm": 0.12864072620868683,
      "learning_rate": 0.00013079390537289496,
      "loss": 0.5408,
      "step": 868
    },
    {
      "epoch": 0.3477390956382553,
      "grad_norm": 0.1156664788722992,
      "learning_rate": 0.00013071371291098637,
      "loss": 0.5143,
      "step": 869
    },
    {
      "epoch": 0.34813925570228094,
      "grad_norm": 0.08616460859775543,
      "learning_rate": 0.0001306335204490778,
      "loss": 0.3817,
      "step": 870
    },
    {
      "epoch": 0.34853941576630654,
      "grad_norm": 0.07527689635753632,
      "learning_rate": 0.0001305533279871692,
      "loss": 0.3432,
      "step": 871
    },
    {
      "epoch": 0.34893957583033214,
      "grad_norm": 0.10563810169696808,
      "learning_rate": 0.00013047313552526065,
      "loss": 0.4898,
      "step": 872
    },
    {
      "epoch": 0.34933973589435774,
      "grad_norm": 0.08305829763412476,
      "learning_rate": 0.00013039294306335205,
      "loss": 0.4125,
      "step": 873
    },
    {
      "epoch": 0.34973989595838334,
      "grad_norm": 0.10172298550605774,
      "learning_rate": 0.0001303127506014435,
      "loss": 0.4357,
      "step": 874
    },
    {
      "epoch": 0.35014005602240894,
      "grad_norm": 0.1158669963479042,
      "learning_rate": 0.0001302325581395349,
      "loss": 0.5312,
      "step": 875
    },
    {
      "epoch": 0.3505402160864346,
      "grad_norm": 0.0874931812286377,
      "learning_rate": 0.00013015236567762633,
      "loss": 0.4258,
      "step": 876
    },
    {
      "epoch": 0.3509403761504602,
      "grad_norm": 0.09174644947052002,
      "learning_rate": 0.0001300721732157177,
      "loss": 0.4869,
      "step": 877
    },
    {
      "epoch": 0.3513405362144858,
      "grad_norm": 0.10778476297855377,
      "learning_rate": 0.00012999198075380915,
      "loss": 0.3824,
      "step": 878
    },
    {
      "epoch": 0.3517406962785114,
      "grad_norm": 0.11188918352127075,
      "learning_rate": 0.00012991178829190056,
      "loss": 0.4883,
      "step": 879
    },
    {
      "epoch": 0.352140856342537,
      "grad_norm": 0.09689442068338394,
      "learning_rate": 0.000129831595829992,
      "loss": 0.4613,
      "step": 880
    },
    {
      "epoch": 0.3525410164065626,
      "grad_norm": 0.08895816653966904,
      "learning_rate": 0.0001297514033680834,
      "loss": 0.4002,
      "step": 881
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.08785424381494522,
      "learning_rate": 0.00012967121090617483,
      "loss": 0.4133,
      "step": 882
    },
    {
      "epoch": 0.35334133653461386,
      "grad_norm": 0.08949248492717743,
      "learning_rate": 0.00012959101844426624,
      "loss": 0.4725,
      "step": 883
    },
    {
      "epoch": 0.35374149659863946,
      "grad_norm": 0.12256266176700592,
      "learning_rate": 0.00012951082598235768,
      "loss": 0.4792,
      "step": 884
    },
    {
      "epoch": 0.35414165666266506,
      "grad_norm": 0.11019055545330048,
      "learning_rate": 0.00012943063352044909,
      "loss": 0.5086,
      "step": 885
    },
    {
      "epoch": 0.35454181672669066,
      "grad_norm": 0.09475860744714737,
      "learning_rate": 0.0001293504410585405,
      "loss": 0.5037,
      "step": 886
    },
    {
      "epoch": 0.35494197679071626,
      "grad_norm": 0.08429504185914993,
      "learning_rate": 0.00012927024859663193,
      "loss": 0.4642,
      "step": 887
    },
    {
      "epoch": 0.3553421368547419,
      "grad_norm": 0.08867741376161575,
      "learning_rate": 0.00012919005613472334,
      "loss": 0.4314,
      "step": 888
    },
    {
      "epoch": 0.3557422969187675,
      "grad_norm": 0.10270152240991592,
      "learning_rate": 0.00012910986367281477,
      "loss": 0.5304,
      "step": 889
    },
    {
      "epoch": 0.3561424569827931,
      "grad_norm": 0.09221524745225906,
      "learning_rate": 0.00012902967121090618,
      "loss": 0.4249,
      "step": 890
    },
    {
      "epoch": 0.3565426170468187,
      "grad_norm": 0.11888480186462402,
      "learning_rate": 0.00012894947874899761,
      "loss": 0.4706,
      "step": 891
    },
    {
      "epoch": 0.3569427771108443,
      "grad_norm": 0.12552154064178467,
      "learning_rate": 0.00012886928628708902,
      "loss": 0.5414,
      "step": 892
    },
    {
      "epoch": 0.3573429371748699,
      "grad_norm": 0.09957411885261536,
      "learning_rate": 0.00012878909382518043,
      "loss": 0.3767,
      "step": 893
    },
    {
      "epoch": 0.3577430972388956,
      "grad_norm": 0.11073484271764755,
      "learning_rate": 0.00012870890136327184,
      "loss": 0.4945,
      "step": 894
    },
    {
      "epoch": 0.3581432573029212,
      "grad_norm": 0.092812180519104,
      "learning_rate": 0.00012862870890136327,
      "loss": 0.3399,
      "step": 895
    },
    {
      "epoch": 0.3585434173669468,
      "grad_norm": 0.10905023664236069,
      "learning_rate": 0.00012854851643945468,
      "loss": 0.5036,
      "step": 896
    },
    {
      "epoch": 0.3589435774309724,
      "grad_norm": 0.08793145418167114,
      "learning_rate": 0.00012846832397754612,
      "loss": 0.4592,
      "step": 897
    },
    {
      "epoch": 0.359343737494998,
      "grad_norm": 0.10445988923311234,
      "learning_rate": 0.00012838813151563752,
      "loss": 0.4544,
      "step": 898
    },
    {
      "epoch": 0.3597438975590236,
      "grad_norm": 0.10874912142753601,
      "learning_rate": 0.00012830793905372896,
      "loss": 0.4294,
      "step": 899
    },
    {
      "epoch": 0.36014405762304924,
      "grad_norm": 0.08639749884605408,
      "learning_rate": 0.00012822774659182037,
      "loss": 0.425,
      "step": 900
    },
    {
      "epoch": 0.36054421768707484,
      "grad_norm": 0.11312730610370636,
      "learning_rate": 0.0001281475541299118,
      "loss": 0.5205,
      "step": 901
    },
    {
      "epoch": 0.36094437775110044,
      "grad_norm": 0.1147288829088211,
      "learning_rate": 0.0001280673616680032,
      "loss": 0.4582,
      "step": 902
    },
    {
      "epoch": 0.36134453781512604,
      "grad_norm": 0.09369063377380371,
      "learning_rate": 0.00012798716920609464,
      "loss": 0.4004,
      "step": 903
    },
    {
      "epoch": 0.36174469787915164,
      "grad_norm": 0.11085744202136993,
      "learning_rate": 0.00012790697674418605,
      "loss": 0.5118,
      "step": 904
    },
    {
      "epoch": 0.36214485794317725,
      "grad_norm": 0.10781159996986389,
      "learning_rate": 0.0001278267842822775,
      "loss": 0.3999,
      "step": 905
    },
    {
      "epoch": 0.3625450180072029,
      "grad_norm": 0.0998811349272728,
      "learning_rate": 0.0001277465918203689,
      "loss": 0.4715,
      "step": 906
    },
    {
      "epoch": 0.3629451780712285,
      "grad_norm": 0.10227172821760178,
      "learning_rate": 0.00012766639935846033,
      "loss": 0.4592,
      "step": 907
    },
    {
      "epoch": 0.3633453381352541,
      "grad_norm": 0.09998227655887604,
      "learning_rate": 0.00012758620689655174,
      "loss": 0.4121,
      "step": 908
    },
    {
      "epoch": 0.3637454981992797,
      "grad_norm": 0.10679224878549576,
      "learning_rate": 0.00012750601443464315,
      "loss": 0.4474,
      "step": 909
    },
    {
      "epoch": 0.3641456582633053,
      "grad_norm": 0.11299879848957062,
      "learning_rate": 0.00012742582197273455,
      "loss": 0.5052,
      "step": 910
    },
    {
      "epoch": 0.3645458183273309,
      "grad_norm": 0.1396021693944931,
      "learning_rate": 0.000127345629510826,
      "loss": 0.5255,
      "step": 911
    },
    {
      "epoch": 0.36494597839135656,
      "grad_norm": 0.08488576859235764,
      "learning_rate": 0.0001272654370489174,
      "loss": 0.4379,
      "step": 912
    },
    {
      "epoch": 0.36534613845538216,
      "grad_norm": 0.11598130315542221,
      "learning_rate": 0.00012718524458700883,
      "loss": 0.5417,
      "step": 913
    },
    {
      "epoch": 0.36574629851940776,
      "grad_norm": 0.1106208860874176,
      "learning_rate": 0.00012710505212510024,
      "loss": 0.4871,
      "step": 914
    },
    {
      "epoch": 0.36614645858343337,
      "grad_norm": 0.09620669484138489,
      "learning_rate": 0.00012702485966319167,
      "loss": 0.4549,
      "step": 915
    },
    {
      "epoch": 0.36654661864745897,
      "grad_norm": 0.09303651005029678,
      "learning_rate": 0.00012694466720128308,
      "loss": 0.404,
      "step": 916
    },
    {
      "epoch": 0.36694677871148457,
      "grad_norm": 0.0972958505153656,
      "learning_rate": 0.00012686447473937452,
      "loss": 0.515,
      "step": 917
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 0.10718216747045517,
      "learning_rate": 0.00012678428227746593,
      "loss": 0.4706,
      "step": 918
    },
    {
      "epoch": 0.3677470988395358,
      "grad_norm": 0.09831789135932922,
      "learning_rate": 0.00012670408981555736,
      "loss": 0.3899,
      "step": 919
    },
    {
      "epoch": 0.3681472589035614,
      "grad_norm": 0.09999827295541763,
      "learning_rate": 0.00012662389735364877,
      "loss": 0.4664,
      "step": 920
    },
    {
      "epoch": 0.368547418967587,
      "grad_norm": 0.08009864389896393,
      "learning_rate": 0.00012654370489174018,
      "loss": 0.4084,
      "step": 921
    },
    {
      "epoch": 0.3689475790316126,
      "grad_norm": 0.11294786632061005,
      "learning_rate": 0.0001264635124298316,
      "loss": 0.4717,
      "step": 922
    },
    {
      "epoch": 0.36934773909563823,
      "grad_norm": 0.10362854599952698,
      "learning_rate": 0.00012638331996792302,
      "loss": 0.4022,
      "step": 923
    },
    {
      "epoch": 0.3697478991596639,
      "grad_norm": 0.11292123794555664,
      "learning_rate": 0.00012630312750601445,
      "loss": 0.5442,
      "step": 924
    },
    {
      "epoch": 0.3701480592236895,
      "grad_norm": 0.08657101541757584,
      "learning_rate": 0.00012622293504410586,
      "loss": 0.4268,
      "step": 925
    },
    {
      "epoch": 0.3705482192877151,
      "grad_norm": 0.08094339072704315,
      "learning_rate": 0.00012614274258219727,
      "loss": 0.4326,
      "step": 926
    },
    {
      "epoch": 0.3709483793517407,
      "grad_norm": 0.10361724346876144,
      "learning_rate": 0.00012606255012028868,
      "loss": 0.4893,
      "step": 927
    },
    {
      "epoch": 0.3713485394157663,
      "grad_norm": 0.12519586086273193,
      "learning_rate": 0.0001259823576583801,
      "loss": 0.4488,
      "step": 928
    },
    {
      "epoch": 0.37174869947979194,
      "grad_norm": 0.09303881227970123,
      "learning_rate": 0.00012590216519647152,
      "loss": 0.4467,
      "step": 929
    },
    {
      "epoch": 0.37214885954381755,
      "grad_norm": 0.12719079852104187,
      "learning_rate": 0.00012582197273456296,
      "loss": 0.5759,
      "step": 930
    },
    {
      "epoch": 0.37254901960784315,
      "grad_norm": 0.10059569776058197,
      "learning_rate": 0.00012574178027265436,
      "loss": 0.5179,
      "step": 931
    },
    {
      "epoch": 0.37294917967186875,
      "grad_norm": 0.12322176247835159,
      "learning_rate": 0.0001256615878107458,
      "loss": 0.474,
      "step": 932
    },
    {
      "epoch": 0.37334933973589435,
      "grad_norm": 0.10927732288837433,
      "learning_rate": 0.0001255813953488372,
      "loss": 0.4446,
      "step": 933
    },
    {
      "epoch": 0.37374949979991995,
      "grad_norm": 0.10128636658191681,
      "learning_rate": 0.00012550120288692864,
      "loss": 0.4681,
      "step": 934
    },
    {
      "epoch": 0.3741496598639456,
      "grad_norm": 0.10654326528310776,
      "learning_rate": 0.00012542101042502005,
      "loss": 0.4113,
      "step": 935
    },
    {
      "epoch": 0.3745498199279712,
      "grad_norm": 0.14077594876289368,
      "learning_rate": 0.00012534081796311149,
      "loss": 0.4629,
      "step": 936
    },
    {
      "epoch": 0.3749499799919968,
      "grad_norm": 0.12123745679855347,
      "learning_rate": 0.0001252606255012029,
      "loss": 0.4094,
      "step": 937
    },
    {
      "epoch": 0.3753501400560224,
      "grad_norm": 0.12496654689311981,
      "learning_rate": 0.00012518043303929433,
      "loss": 0.5552,
      "step": 938
    },
    {
      "epoch": 0.375750300120048,
      "grad_norm": 0.10202284157276154,
      "learning_rate": 0.00012510024057738574,
      "loss": 0.5197,
      "step": 939
    },
    {
      "epoch": 0.3761504601840736,
      "grad_norm": 0.0984744057059288,
      "learning_rate": 0.00012502004811547717,
      "loss": 0.3847,
      "step": 940
    },
    {
      "epoch": 0.37655062024809927,
      "grad_norm": 0.11844607442617416,
      "learning_rate": 0.00012493985565356855,
      "loss": 0.4845,
      "step": 941
    },
    {
      "epoch": 0.37695078031212487,
      "grad_norm": 0.12712927162647247,
      "learning_rate": 0.00012485966319166,
      "loss": 0.4563,
      "step": 942
    },
    {
      "epoch": 0.37735094037615047,
      "grad_norm": 0.10561415553092957,
      "learning_rate": 0.0001247794707297514,
      "loss": 0.4948,
      "step": 943
    },
    {
      "epoch": 0.37775110044017607,
      "grad_norm": 0.08933191746473312,
      "learning_rate": 0.00012469927826784283,
      "loss": 0.4377,
      "step": 944
    },
    {
      "epoch": 0.37815126050420167,
      "grad_norm": 0.1238190159201622,
      "learning_rate": 0.00012461908580593424,
      "loss": 0.4948,
      "step": 945
    },
    {
      "epoch": 0.37855142056822727,
      "grad_norm": 0.13692092895507812,
      "learning_rate": 0.00012453889334402567,
      "loss": 0.4535,
      "step": 946
    },
    {
      "epoch": 0.3789515806322529,
      "grad_norm": 0.1114872470498085,
      "learning_rate": 0.00012445870088211708,
      "loss": 0.4653,
      "step": 947
    },
    {
      "epoch": 0.3793517406962785,
      "grad_norm": 0.1288745254278183,
      "learning_rate": 0.00012437850842020852,
      "loss": 0.5398,
      "step": 948
    },
    {
      "epoch": 0.37975190076030413,
      "grad_norm": 0.10739979147911072,
      "learning_rate": 0.00012429831595829992,
      "loss": 0.499,
      "step": 949
    },
    {
      "epoch": 0.38015206082432973,
      "grad_norm": 0.0950557217001915,
      "learning_rate": 0.00012421812349639136,
      "loss": 0.4436,
      "step": 950
    },
    {
      "epoch": 0.38055222088835533,
      "grad_norm": 0.1055738553404808,
      "learning_rate": 0.00012413793103448277,
      "loss": 0.4559,
      "step": 951
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.1145840734243393,
      "learning_rate": 0.0001240577385725742,
      "loss": 0.433,
      "step": 952
    },
    {
      "epoch": 0.3813525410164066,
      "grad_norm": 0.10033485293388367,
      "learning_rate": 0.0001239775461106656,
      "loss": 0.4345,
      "step": 953
    },
    {
      "epoch": 0.3817527010804322,
      "grad_norm": 0.11014028638601303,
      "learning_rate": 0.00012389735364875702,
      "loss": 0.5009,
      "step": 954
    },
    {
      "epoch": 0.3821528611444578,
      "grad_norm": 0.1154341921210289,
      "learning_rate": 0.00012381716118684845,
      "loss": 0.4504,
      "step": 955
    },
    {
      "epoch": 0.3825530212084834,
      "grad_norm": 0.09904512017965317,
      "learning_rate": 0.00012373696872493986,
      "loss": 0.4112,
      "step": 956
    },
    {
      "epoch": 0.382953181272509,
      "grad_norm": 0.11396723985671997,
      "learning_rate": 0.00012365677626303127,
      "loss": 0.4599,
      "step": 957
    },
    {
      "epoch": 0.3833533413365346,
      "grad_norm": 0.0992651879787445,
      "learning_rate": 0.0001235765838011227,
      "loss": 0.4616,
      "step": 958
    },
    {
      "epoch": 0.38375350140056025,
      "grad_norm": 0.07261059433221817,
      "learning_rate": 0.0001234963913392141,
      "loss": 0.3435,
      "step": 959
    },
    {
      "epoch": 0.38415366146458585,
      "grad_norm": 0.13498134911060333,
      "learning_rate": 0.00012341619887730555,
      "loss": 0.5524,
      "step": 960
    },
    {
      "epoch": 0.38455382152861145,
      "grad_norm": 0.08315921574831009,
      "learning_rate": 0.00012333600641539695,
      "loss": 0.4458,
      "step": 961
    },
    {
      "epoch": 0.38495398159263705,
      "grad_norm": 0.109076127409935,
      "learning_rate": 0.00012325581395348836,
      "loss": 0.4812,
      "step": 962
    },
    {
      "epoch": 0.38535414165666265,
      "grad_norm": 0.0948994904756546,
      "learning_rate": 0.0001231756214915798,
      "loss": 0.4325,
      "step": 963
    },
    {
      "epoch": 0.38575430172068825,
      "grad_norm": 0.1168752908706665,
      "learning_rate": 0.0001230954290296712,
      "loss": 0.4616,
      "step": 964
    },
    {
      "epoch": 0.3861544617847139,
      "grad_norm": 0.09148790687322617,
      "learning_rate": 0.00012301523656776264,
      "loss": 0.5055,
      "step": 965
    },
    {
      "epoch": 0.3865546218487395,
      "grad_norm": 0.08745431900024414,
      "learning_rate": 0.00012293504410585405,
      "loss": 0.3946,
      "step": 966
    },
    {
      "epoch": 0.3869547819127651,
      "grad_norm": 0.09174071997404099,
      "learning_rate": 0.00012285485164394548,
      "loss": 0.4237,
      "step": 967
    },
    {
      "epoch": 0.3873549419767907,
      "grad_norm": 0.07580828666687012,
      "learning_rate": 0.0001227746591820369,
      "loss": 0.3732,
      "step": 968
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 0.10520707815885544,
      "learning_rate": 0.00012269446672012833,
      "loss": 0.4892,
      "step": 969
    },
    {
      "epoch": 0.3881552621048419,
      "grad_norm": 0.10453734546899796,
      "learning_rate": 0.00012261427425821973,
      "loss": 0.5021,
      "step": 970
    },
    {
      "epoch": 0.38855542216886757,
      "grad_norm": 0.09407006949186325,
      "learning_rate": 0.00012253408179631117,
      "loss": 0.476,
      "step": 971
    },
    {
      "epoch": 0.38895558223289317,
      "grad_norm": 0.07126469165086746,
      "learning_rate": 0.00012245388933440258,
      "loss": 0.3734,
      "step": 972
    },
    {
      "epoch": 0.38935574229691877,
      "grad_norm": 0.0954594537615776,
      "learning_rate": 0.00012237369687249398,
      "loss": 0.4722,
      "step": 973
    },
    {
      "epoch": 0.3897559023609444,
      "grad_norm": 0.10290424525737762,
      "learning_rate": 0.0001222935044105854,
      "loss": 0.4831,
      "step": 974
    },
    {
      "epoch": 0.39015606242497,
      "grad_norm": 0.1039825901389122,
      "learning_rate": 0.00012221331194867683,
      "loss": 0.4606,
      "step": 975
    },
    {
      "epoch": 0.3905562224889956,
      "grad_norm": 0.12161862850189209,
      "learning_rate": 0.00012213311948676824,
      "loss": 0.4546,
      "step": 976
    },
    {
      "epoch": 0.39095638255302123,
      "grad_norm": 0.11032464355230331,
      "learning_rate": 0.00012205292702485967,
      "loss": 0.5478,
      "step": 977
    },
    {
      "epoch": 0.39135654261704683,
      "grad_norm": 0.09499000757932663,
      "learning_rate": 0.00012197273456295109,
      "loss": 0.4037,
      "step": 978
    },
    {
      "epoch": 0.39175670268107243,
      "grad_norm": 0.11285819858312607,
      "learning_rate": 0.0001218925421010425,
      "loss": 0.5718,
      "step": 979
    },
    {
      "epoch": 0.39215686274509803,
      "grad_norm": 0.09150286763906479,
      "learning_rate": 0.00012181234963913392,
      "loss": 0.4166,
      "step": 980
    },
    {
      "epoch": 0.39255702280912363,
      "grad_norm": 0.08108262717723846,
      "learning_rate": 0.00012173215717722534,
      "loss": 0.449,
      "step": 981
    },
    {
      "epoch": 0.39295718287314924,
      "grad_norm": 0.10555905103683472,
      "learning_rate": 0.00012165196471531676,
      "loss": 0.4885,
      "step": 982
    },
    {
      "epoch": 0.3933573429371749,
      "grad_norm": 0.08708937466144562,
      "learning_rate": 0.00012157177225340819,
      "loss": 0.3985,
      "step": 983
    },
    {
      "epoch": 0.3937575030012005,
      "grad_norm": 0.08791977912187576,
      "learning_rate": 0.00012149157979149961,
      "loss": 0.4292,
      "step": 984
    },
    {
      "epoch": 0.3941576630652261,
      "grad_norm": 0.07463593035936356,
      "learning_rate": 0.00012141138732959103,
      "loss": 0.3926,
      "step": 985
    },
    {
      "epoch": 0.3945578231292517,
      "grad_norm": 0.09198116511106491,
      "learning_rate": 0.00012133119486768245,
      "loss": 0.4495,
      "step": 986
    },
    {
      "epoch": 0.3949579831932773,
      "grad_norm": 0.09917920082807541,
      "learning_rate": 0.00012125100240577387,
      "loss": 0.4719,
      "step": 987
    },
    {
      "epoch": 0.3953581432573029,
      "grad_norm": 0.08785839378833771,
      "learning_rate": 0.00012117080994386529,
      "loss": 0.4365,
      "step": 988
    },
    {
      "epoch": 0.39575830332132855,
      "grad_norm": 0.09882707893848419,
      "learning_rate": 0.00012109061748195669,
      "loss": 0.546,
      "step": 989
    },
    {
      "epoch": 0.39615846338535415,
      "grad_norm": 0.07223038375377655,
      "learning_rate": 0.00012101042502004811,
      "loss": 0.3468,
      "step": 990
    },
    {
      "epoch": 0.39655862344937975,
      "grad_norm": 0.08672124147415161,
      "learning_rate": 0.00012093023255813953,
      "loss": 0.444,
      "step": 991
    },
    {
      "epoch": 0.39695878351340536,
      "grad_norm": 0.08908138424158096,
      "learning_rate": 0.00012085004009623095,
      "loss": 0.3807,
      "step": 992
    },
    {
      "epoch": 0.39735894357743096,
      "grad_norm": 0.10331722348928452,
      "learning_rate": 0.00012076984763432237,
      "loss": 0.4782,
      "step": 993
    },
    {
      "epoch": 0.39775910364145656,
      "grad_norm": 0.09014694392681122,
      "learning_rate": 0.0001206896551724138,
      "loss": 0.4122,
      "step": 994
    },
    {
      "epoch": 0.3981592637054822,
      "grad_norm": 0.11006547510623932,
      "learning_rate": 0.00012060946271050522,
      "loss": 0.5023,
      "step": 995
    },
    {
      "epoch": 0.3985594237695078,
      "grad_norm": 0.08964911103248596,
      "learning_rate": 0.00012052927024859664,
      "loss": 0.4533,
      "step": 996
    },
    {
      "epoch": 0.3989595838335334,
      "grad_norm": 0.10616513341665268,
      "learning_rate": 0.00012044907778668806,
      "loss": 0.4933,
      "step": 997
    },
    {
      "epoch": 0.399359743897559,
      "grad_norm": 0.08955554664134979,
      "learning_rate": 0.00012036888532477948,
      "loss": 0.4598,
      "step": 998
    },
    {
      "epoch": 0.3997599039615846,
      "grad_norm": 0.12506873905658722,
      "learning_rate": 0.0001202886928628709,
      "loss": 0.5151,
      "step": 999
    },
    {
      "epoch": 0.4001600640256102,
      "grad_norm": 0.0950968936085701,
      "learning_rate": 0.00012020850040096232,
      "loss": 0.4378,
      "step": 1000
    },
    {
      "epoch": 0.4005602240896359,
      "grad_norm": 0.11523471772670746,
      "learning_rate": 0.00012012830793905374,
      "loss": 0.5287,
      "step": 1001
    },
    {
      "epoch": 0.4009603841536615,
      "grad_norm": 0.11192558705806732,
      "learning_rate": 0.00012004811547714517,
      "loss": 0.5765,
      "step": 1002
    },
    {
      "epoch": 0.4013605442176871,
      "grad_norm": 0.10451066493988037,
      "learning_rate": 0.00011996792301523659,
      "loss": 0.5421,
      "step": 1003
    },
    {
      "epoch": 0.4017607042817127,
      "grad_norm": 0.10410527884960175,
      "learning_rate": 0.00011988773055332801,
      "loss": 0.4662,
      "step": 1004
    },
    {
      "epoch": 0.4021608643457383,
      "grad_norm": 0.10887360572814941,
      "learning_rate": 0.0001198075380914194,
      "loss": 0.4722,
      "step": 1005
    },
    {
      "epoch": 0.4025610244097639,
      "grad_norm": 0.09962458908557892,
      "learning_rate": 0.00011972734562951083,
      "loss": 0.3662,
      "step": 1006
    },
    {
      "epoch": 0.40296118447378954,
      "grad_norm": 0.09082963317632675,
      "learning_rate": 0.00011964715316760225,
      "loss": 0.5353,
      "step": 1007
    },
    {
      "epoch": 0.40336134453781514,
      "grad_norm": 0.11066561192274094,
      "learning_rate": 0.00011956696070569367,
      "loss": 0.5352,
      "step": 1008
    },
    {
      "epoch": 0.40376150460184074,
      "grad_norm": 0.121410071849823,
      "learning_rate": 0.00011948676824378509,
      "loss": 0.5174,
      "step": 1009
    },
    {
      "epoch": 0.40416166466586634,
      "grad_norm": 0.09594480693340302,
      "learning_rate": 0.00011940657578187651,
      "loss": 0.4636,
      "step": 1010
    },
    {
      "epoch": 0.40456182472989194,
      "grad_norm": 0.10278338193893433,
      "learning_rate": 0.00011932638331996793,
      "loss": 0.4139,
      "step": 1011
    },
    {
      "epoch": 0.40496198479391754,
      "grad_norm": 0.10629842430353165,
      "learning_rate": 0.00011924619085805935,
      "loss": 0.4371,
      "step": 1012
    },
    {
      "epoch": 0.4053621448579432,
      "grad_norm": 0.10679800063371658,
      "learning_rate": 0.00011916599839615076,
      "loss": 0.5122,
      "step": 1013
    },
    {
      "epoch": 0.4057623049219688,
      "grad_norm": 0.08449514210224152,
      "learning_rate": 0.00011908580593424218,
      "loss": 0.432,
      "step": 1014
    },
    {
      "epoch": 0.4061624649859944,
      "grad_norm": 0.1062932088971138,
      "learning_rate": 0.0001190056134723336,
      "loss": 0.5266,
      "step": 1015
    },
    {
      "epoch": 0.40656262505002,
      "grad_norm": 0.08913715928792953,
      "learning_rate": 0.00011892542101042503,
      "loss": 0.4849,
      "step": 1016
    },
    {
      "epoch": 0.4069627851140456,
      "grad_norm": 0.09433549642562866,
      "learning_rate": 0.00011884522854851645,
      "loss": 0.4654,
      "step": 1017
    },
    {
      "epoch": 0.4073629451780712,
      "grad_norm": 0.1036926880478859,
      "learning_rate": 0.00011876503608660787,
      "loss": 0.4226,
      "step": 1018
    },
    {
      "epoch": 0.40776310524209686,
      "grad_norm": 0.10021283477544785,
      "learning_rate": 0.00011868484362469929,
      "loss": 0.4712,
      "step": 1019
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.10152693092823029,
      "learning_rate": 0.00011860465116279071,
      "loss": 0.4311,
      "step": 1020
    },
    {
      "epoch": 0.40856342537014806,
      "grad_norm": 0.0986657440662384,
      "learning_rate": 0.0001185244587008821,
      "loss": 0.4732,
      "step": 1021
    },
    {
      "epoch": 0.40896358543417366,
      "grad_norm": 0.09487032890319824,
      "learning_rate": 0.00011844426623897353,
      "loss": 0.4249,
      "step": 1022
    },
    {
      "epoch": 0.40936374549819926,
      "grad_norm": 0.10613259673118591,
      "learning_rate": 0.00011836407377706495,
      "loss": 0.4765,
      "step": 1023
    },
    {
      "epoch": 0.4097639055622249,
      "grad_norm": 0.0861942321062088,
      "learning_rate": 0.00011828388131515637,
      "loss": 0.3913,
      "step": 1024
    },
    {
      "epoch": 0.4101640656262505,
      "grad_norm": 0.12135373055934906,
      "learning_rate": 0.00011820368885324779,
      "loss": 0.5679,
      "step": 1025
    },
    {
      "epoch": 0.4105642256902761,
      "grad_norm": 0.11178461462259293,
      "learning_rate": 0.00011812349639133921,
      "loss": 0.4656,
      "step": 1026
    },
    {
      "epoch": 0.4109643857543017,
      "grad_norm": 0.11113381385803223,
      "learning_rate": 0.00011804330392943064,
      "loss": 0.4402,
      "step": 1027
    },
    {
      "epoch": 0.4113645458183273,
      "grad_norm": 0.11054138094186783,
      "learning_rate": 0.00011796311146752206,
      "loss": 0.4551,
      "step": 1028
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.09864194691181183,
      "learning_rate": 0.00011788291900561348,
      "loss": 0.4229,
      "step": 1029
    },
    {
      "epoch": 0.4121648659463786,
      "grad_norm": 0.09197727590799332,
      "learning_rate": 0.0001178027265437049,
      "loss": 0.3667,
      "step": 1030
    },
    {
      "epoch": 0.4125650260104042,
      "grad_norm": 0.11741810292005539,
      "learning_rate": 0.00011772253408179632,
      "loss": 0.5149,
      "step": 1031
    },
    {
      "epoch": 0.4129651860744298,
      "grad_norm": 0.11260685324668884,
      "learning_rate": 0.00011764234161988774,
      "loss": 0.465,
      "step": 1032
    },
    {
      "epoch": 0.4133653461384554,
      "grad_norm": 0.09899036586284637,
      "learning_rate": 0.00011756214915797916,
      "loss": 0.4366,
      "step": 1033
    },
    {
      "epoch": 0.413765506202481,
      "grad_norm": 0.11546590179204941,
      "learning_rate": 0.00011748195669607059,
      "loss": 0.5143,
      "step": 1034
    },
    {
      "epoch": 0.4141656662665066,
      "grad_norm": 0.09084999561309814,
      "learning_rate": 0.00011740176423416201,
      "loss": 0.4955,
      "step": 1035
    },
    {
      "epoch": 0.41456582633053224,
      "grad_norm": 0.14533154666423798,
      "learning_rate": 0.00011732157177225343,
      "loss": 0.5245,
      "step": 1036
    },
    {
      "epoch": 0.41496598639455784,
      "grad_norm": 0.08788584917783737,
      "learning_rate": 0.00011724137931034482,
      "loss": 0.4382,
      "step": 1037
    },
    {
      "epoch": 0.41536614645858344,
      "grad_norm": 0.11063651740550995,
      "learning_rate": 0.00011716118684843624,
      "loss": 0.5282,
      "step": 1038
    },
    {
      "epoch": 0.41576630652260904,
      "grad_norm": 0.12584221363067627,
      "learning_rate": 0.00011708099438652767,
      "loss": 0.4878,
      "step": 1039
    },
    {
      "epoch": 0.41616646658663464,
      "grad_norm": 0.11645576357841492,
      "learning_rate": 0.00011700080192461909,
      "loss": 0.4714,
      "step": 1040
    },
    {
      "epoch": 0.41656662665066024,
      "grad_norm": 0.15434414148330688,
      "learning_rate": 0.00011692060946271051,
      "loss": 0.5564,
      "step": 1041
    },
    {
      "epoch": 0.4169667867146859,
      "grad_norm": 0.08511015772819519,
      "learning_rate": 0.00011684041700080193,
      "loss": 0.4217,
      "step": 1042
    },
    {
      "epoch": 0.4173669467787115,
      "grad_norm": 0.08282849192619324,
      "learning_rate": 0.00011676022453889335,
      "loss": 0.3742,
      "step": 1043
    },
    {
      "epoch": 0.4177671068427371,
      "grad_norm": 0.08697018772363663,
      "learning_rate": 0.00011668003207698477,
      "loss": 0.4833,
      "step": 1044
    },
    {
      "epoch": 0.4181672669067627,
      "grad_norm": 0.10194952040910721,
      "learning_rate": 0.0001165998396150762,
      "loss": 0.4525,
      "step": 1045
    },
    {
      "epoch": 0.4185674269707883,
      "grad_norm": 0.11417055130004883,
      "learning_rate": 0.00011651964715316762,
      "loss": 0.4723,
      "step": 1046
    },
    {
      "epoch": 0.4189675870348139,
      "grad_norm": 0.09107904881238937,
      "learning_rate": 0.00011643945469125904,
      "loss": 0.4306,
      "step": 1047
    },
    {
      "epoch": 0.41936774709883956,
      "grad_norm": 0.10658539086580276,
      "learning_rate": 0.00011635926222935045,
      "loss": 0.511,
      "step": 1048
    },
    {
      "epoch": 0.41976790716286516,
      "grad_norm": 0.14177854359149933,
      "learning_rate": 0.00011627906976744187,
      "loss": 0.5228,
      "step": 1049
    },
    {
      "epoch": 0.42016806722689076,
      "grad_norm": 0.09324681758880615,
      "learning_rate": 0.00011619887730553329,
      "loss": 0.4495,
      "step": 1050
    },
    {
      "epoch": 0.42056822729091636,
      "grad_norm": 0.0960022583603859,
      "learning_rate": 0.00011611868484362471,
      "loss": 0.4893,
      "step": 1051
    },
    {
      "epoch": 0.42096838735494196,
      "grad_norm": 0.09837029129266739,
      "learning_rate": 0.00011603849238171613,
      "loss": 0.4297,
      "step": 1052
    },
    {
      "epoch": 0.42136854741896757,
      "grad_norm": 0.10397377610206604,
      "learning_rate": 0.00011595829991980754,
      "loss": 0.4334,
      "step": 1053
    },
    {
      "epoch": 0.4217687074829932,
      "grad_norm": 0.0857522115111351,
      "learning_rate": 0.00011587810745789896,
      "loss": 0.4497,
      "step": 1054
    },
    {
      "epoch": 0.4221688675470188,
      "grad_norm": 0.09681940823793411,
      "learning_rate": 0.00011579791499599037,
      "loss": 0.4146,
      "step": 1055
    },
    {
      "epoch": 0.4225690276110444,
      "grad_norm": 0.10400238633155823,
      "learning_rate": 0.00011571772253408179,
      "loss": 0.4554,
      "step": 1056
    },
    {
      "epoch": 0.42296918767507,
      "grad_norm": 0.10535331070423126,
      "learning_rate": 0.00011563753007217321,
      "loss": 0.4783,
      "step": 1057
    },
    {
      "epoch": 0.4233693477390956,
      "grad_norm": 0.09807813912630081,
      "learning_rate": 0.00011555733761026463,
      "loss": 0.4396,
      "step": 1058
    },
    {
      "epoch": 0.4237695078031212,
      "grad_norm": 0.10076496005058289,
      "learning_rate": 0.00011547714514835605,
      "loss": 0.527,
      "step": 1059
    },
    {
      "epoch": 0.4241696678671469,
      "grad_norm": 0.09518662840127945,
      "learning_rate": 0.00011539695268644748,
      "loss": 0.4726,
      "step": 1060
    },
    {
      "epoch": 0.4245698279311725,
      "grad_norm": 0.09172680228948593,
      "learning_rate": 0.0001153167602245389,
      "loss": 0.4748,
      "step": 1061
    },
    {
      "epoch": 0.4249699879951981,
      "grad_norm": 0.09929398447275162,
      "learning_rate": 0.00011523656776263032,
      "loss": 0.5121,
      "step": 1062
    },
    {
      "epoch": 0.4253701480592237,
      "grad_norm": 0.10813269764184952,
      "learning_rate": 0.00011515637530072174,
      "loss": 0.4857,
      "step": 1063
    },
    {
      "epoch": 0.4257703081232493,
      "grad_norm": 0.08838058263063431,
      "learning_rate": 0.00011507618283881316,
      "loss": 0.5393,
      "step": 1064
    },
    {
      "epoch": 0.4261704681872749,
      "grad_norm": 0.09898588061332703,
      "learning_rate": 0.00011499599037690458,
      "loss": 0.4206,
      "step": 1065
    },
    {
      "epoch": 0.42657062825130054,
      "grad_norm": 0.09559787809848785,
      "learning_rate": 0.000114915797914996,
      "loss": 0.4409,
      "step": 1066
    },
    {
      "epoch": 0.42697078831532614,
      "grad_norm": 0.11957086622714996,
      "learning_rate": 0.00011483560545308743,
      "loss": 0.4066,
      "step": 1067
    },
    {
      "epoch": 0.42737094837935174,
      "grad_norm": 0.08849751204252243,
      "learning_rate": 0.00011475541299117885,
      "loss": 0.4569,
      "step": 1068
    },
    {
      "epoch": 0.42777110844337735,
      "grad_norm": 0.09162834286689758,
      "learning_rate": 0.00011467522052927024,
      "loss": 0.4646,
      "step": 1069
    },
    {
      "epoch": 0.42817126850740295,
      "grad_norm": 0.09426417946815491,
      "learning_rate": 0.00011459502806736166,
      "loss": 0.4223,
      "step": 1070
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.10009044408798218,
      "learning_rate": 0.00011451483560545309,
      "loss": 0.4341,
      "step": 1071
    },
    {
      "epoch": 0.4289715886354542,
      "grad_norm": 0.1401372104883194,
      "learning_rate": 0.0001144346431435445,
      "loss": 0.5494,
      "step": 1072
    },
    {
      "epoch": 0.4293717486994798,
      "grad_norm": 0.10280915349721909,
      "learning_rate": 0.00011435445068163593,
      "loss": 0.453,
      "step": 1073
    },
    {
      "epoch": 0.4297719087635054,
      "grad_norm": 0.07629837840795517,
      "learning_rate": 0.00011427425821972735,
      "loss": 0.3915,
      "step": 1074
    },
    {
      "epoch": 0.430172068827531,
      "grad_norm": 0.0780298039317131,
      "learning_rate": 0.00011419406575781877,
      "loss": 0.452,
      "step": 1075
    },
    {
      "epoch": 0.4305722288915566,
      "grad_norm": 0.09238196909427643,
      "learning_rate": 0.00011411387329591019,
      "loss": 0.4403,
      "step": 1076
    },
    {
      "epoch": 0.4309723889555822,
      "grad_norm": 0.13983137905597687,
      "learning_rate": 0.00011403368083400161,
      "loss": 0.5071,
      "step": 1077
    },
    {
      "epoch": 0.43137254901960786,
      "grad_norm": 0.0919516533613205,
      "learning_rate": 0.00011395348837209304,
      "loss": 0.436,
      "step": 1078
    },
    {
      "epoch": 0.43177270908363347,
      "grad_norm": 0.09337438642978668,
      "learning_rate": 0.00011387329591018446,
      "loss": 0.4114,
      "step": 1079
    },
    {
      "epoch": 0.43217286914765907,
      "grad_norm": 0.10250286757946014,
      "learning_rate": 0.00011379310344827588,
      "loss": 0.508,
      "step": 1080
    },
    {
      "epoch": 0.43257302921168467,
      "grad_norm": 0.11164781451225281,
      "learning_rate": 0.0001137129109863673,
      "loss": 0.5213,
      "step": 1081
    },
    {
      "epoch": 0.43297318927571027,
      "grad_norm": 0.11460950970649719,
      "learning_rate": 0.00011363271852445872,
      "loss": 0.5166,
      "step": 1082
    },
    {
      "epoch": 0.43337334933973587,
      "grad_norm": 0.09059172123670578,
      "learning_rate": 0.00011355252606255013,
      "loss": 0.4297,
      "step": 1083
    },
    {
      "epoch": 0.4337735094037615,
      "grad_norm": 0.09552048146724701,
      "learning_rate": 0.00011347233360064155,
      "loss": 0.4478,
      "step": 1084
    },
    {
      "epoch": 0.4341736694677871,
      "grad_norm": 0.11125129461288452,
      "learning_rate": 0.00011339214113873296,
      "loss": 0.4312,
      "step": 1085
    },
    {
      "epoch": 0.4345738295318127,
      "grad_norm": 0.12418052554130554,
      "learning_rate": 0.00011331194867682438,
      "loss": 0.5457,
      "step": 1086
    },
    {
      "epoch": 0.43497398959583833,
      "grad_norm": 0.07436348497867584,
      "learning_rate": 0.0001132317562149158,
      "loss": 0.3285,
      "step": 1087
    },
    {
      "epoch": 0.43537414965986393,
      "grad_norm": 0.08587199449539185,
      "learning_rate": 0.00011315156375300722,
      "loss": 0.4661,
      "step": 1088
    },
    {
      "epoch": 0.43577430972388953,
      "grad_norm": 0.08683481067419052,
      "learning_rate": 0.00011307137129109864,
      "loss": 0.4337,
      "step": 1089
    },
    {
      "epoch": 0.4361744697879152,
      "grad_norm": 0.09019768238067627,
      "learning_rate": 0.00011299117882919005,
      "loss": 0.473,
      "step": 1090
    },
    {
      "epoch": 0.4365746298519408,
      "grad_norm": 0.10283432900905609,
      "learning_rate": 0.00011291098636728147,
      "loss": 0.4584,
      "step": 1091
    },
    {
      "epoch": 0.4369747899159664,
      "grad_norm": 0.12413600832223892,
      "learning_rate": 0.0001128307939053729,
      "loss": 0.4933,
      "step": 1092
    },
    {
      "epoch": 0.437374949979992,
      "grad_norm": 0.08103317022323608,
      "learning_rate": 0.00011275060144346432,
      "loss": 0.4361,
      "step": 1093
    },
    {
      "epoch": 0.4377751100440176,
      "grad_norm": 0.08317156136035919,
      "learning_rate": 0.00011267040898155574,
      "loss": 0.4867,
      "step": 1094
    },
    {
      "epoch": 0.4381752701080432,
      "grad_norm": 0.08681374043226242,
      "learning_rate": 0.00011259021651964716,
      "loss": 0.4282,
      "step": 1095
    },
    {
      "epoch": 0.43857543017206885,
      "grad_norm": 0.1007266640663147,
      "learning_rate": 0.00011251002405773858,
      "loss": 0.4967,
      "step": 1096
    },
    {
      "epoch": 0.43897559023609445,
      "grad_norm": 0.10239899158477783,
      "learning_rate": 0.00011242983159583,
      "loss": 0.4383,
      "step": 1097
    },
    {
      "epoch": 0.43937575030012005,
      "grad_norm": 0.08387213200330734,
      "learning_rate": 0.00011234963913392142,
      "loss": 0.4131,
      "step": 1098
    },
    {
      "epoch": 0.43977591036414565,
      "grad_norm": 0.1178210973739624,
      "learning_rate": 0.00011226944667201285,
      "loss": 0.4923,
      "step": 1099
    },
    {
      "epoch": 0.44017607042817125,
      "grad_norm": 0.08642078191041946,
      "learning_rate": 0.00011218925421010427,
      "loss": 0.4165,
      "step": 1100
    },
    {
      "epoch": 0.44057623049219685,
      "grad_norm": 0.09876736998558044,
      "learning_rate": 0.00011210906174819566,
      "loss": 0.4229,
      "step": 1101
    },
    {
      "epoch": 0.4409763905562225,
      "grad_norm": 0.10600544512271881,
      "learning_rate": 0.00011202886928628708,
      "loss": 0.4774,
      "step": 1102
    },
    {
      "epoch": 0.4413765506202481,
      "grad_norm": 0.08371894061565399,
      "learning_rate": 0.0001119486768243785,
      "loss": 0.4037,
      "step": 1103
    },
    {
      "epoch": 0.4417767106842737,
      "grad_norm": 0.08336224406957626,
      "learning_rate": 0.00011186848436246993,
      "loss": 0.4662,
      "step": 1104
    },
    {
      "epoch": 0.4421768707482993,
      "grad_norm": 0.09817691147327423,
      "learning_rate": 0.00011178829190056135,
      "loss": 0.3925,
      "step": 1105
    },
    {
      "epoch": 0.4425770308123249,
      "grad_norm": 0.0861620306968689,
      "learning_rate": 0.00011170809943865277,
      "loss": 0.4549,
      "step": 1106
    },
    {
      "epoch": 0.4429771908763505,
      "grad_norm": 0.10363716632127762,
      "learning_rate": 0.00011162790697674419,
      "loss": 0.5132,
      "step": 1107
    },
    {
      "epoch": 0.44337735094037617,
      "grad_norm": 0.09328047186136246,
      "learning_rate": 0.00011154771451483561,
      "loss": 0.4196,
      "step": 1108
    },
    {
      "epoch": 0.44377751100440177,
      "grad_norm": 0.09907538443803787,
      "learning_rate": 0.00011146752205292703,
      "loss": 0.5364,
      "step": 1109
    },
    {
      "epoch": 0.44417767106842737,
      "grad_norm": 0.09044646471738815,
      "learning_rate": 0.00011138732959101845,
      "loss": 0.4556,
      "step": 1110
    },
    {
      "epoch": 0.44457783113245297,
      "grad_norm": 0.0923818051815033,
      "learning_rate": 0.00011130713712910988,
      "loss": 0.46,
      "step": 1111
    },
    {
      "epoch": 0.4449779911964786,
      "grad_norm": 0.08970952779054642,
      "learning_rate": 0.0001112269446672013,
      "loss": 0.3584,
      "step": 1112
    },
    {
      "epoch": 0.44537815126050423,
      "grad_norm": 0.09960010647773743,
      "learning_rate": 0.00011114675220529272,
      "loss": 0.525,
      "step": 1113
    },
    {
      "epoch": 0.44577831132452983,
      "grad_norm": 0.11098507046699524,
      "learning_rate": 0.00011106655974338414,
      "loss": 0.45,
      "step": 1114
    },
    {
      "epoch": 0.44617847138855543,
      "grad_norm": 0.10520576685667038,
      "learning_rate": 0.00011098636728147556,
      "loss": 0.5114,
      "step": 1115
    },
    {
      "epoch": 0.44657863145258103,
      "grad_norm": 0.08139341324567795,
      "learning_rate": 0.00011090617481956698,
      "loss": 0.4243,
      "step": 1116
    },
    {
      "epoch": 0.44697879151660663,
      "grad_norm": 0.0992484837770462,
      "learning_rate": 0.00011082598235765838,
      "loss": 0.4866,
      "step": 1117
    },
    {
      "epoch": 0.44737895158063223,
      "grad_norm": 0.10080177336931229,
      "learning_rate": 0.0001107457898957498,
      "loss": 0.5055,
      "step": 1118
    },
    {
      "epoch": 0.4477791116446579,
      "grad_norm": 0.11132116615772247,
      "learning_rate": 0.00011066559743384122,
      "loss": 0.5004,
      "step": 1119
    },
    {
      "epoch": 0.4481792717086835,
      "grad_norm": 0.08846233785152435,
      "learning_rate": 0.00011058540497193264,
      "loss": 0.4216,
      "step": 1120
    },
    {
      "epoch": 0.4485794317727091,
      "grad_norm": 0.10563316941261292,
      "learning_rate": 0.00011050521251002406,
      "loss": 0.5151,
      "step": 1121
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 0.11371307075023651,
      "learning_rate": 0.00011042502004811549,
      "loss": 0.4819,
      "step": 1122
    },
    {
      "epoch": 0.4493797519007603,
      "grad_norm": 0.09212236106395721,
      "learning_rate": 0.0001103448275862069,
      "loss": 0.4171,
      "step": 1123
    },
    {
      "epoch": 0.4497799119647859,
      "grad_norm": 0.09352688491344452,
      "learning_rate": 0.00011026463512429831,
      "loss": 0.4554,
      "step": 1124
    },
    {
      "epoch": 0.45018007202881155,
      "grad_norm": 0.08897562325000763,
      "learning_rate": 0.00011018444266238974,
      "loss": 0.3947,
      "step": 1125
    },
    {
      "epoch": 0.45058023209283715,
      "grad_norm": 0.11105905473232269,
      "learning_rate": 0.00011010425020048116,
      "loss": 0.5261,
      "step": 1126
    },
    {
      "epoch": 0.45098039215686275,
      "grad_norm": 0.11109276860952377,
      "learning_rate": 0.00011002405773857258,
      "loss": 0.5173,
      "step": 1127
    },
    {
      "epoch": 0.45138055222088835,
      "grad_norm": 0.09519001841545105,
      "learning_rate": 0.000109943865276664,
      "loss": 0.488,
      "step": 1128
    },
    {
      "epoch": 0.45178071228491395,
      "grad_norm": 0.09718795120716095,
      "learning_rate": 0.00010986367281475542,
      "loss": 0.5137,
      "step": 1129
    },
    {
      "epoch": 0.45218087234893956,
      "grad_norm": 0.11562681198120117,
      "learning_rate": 0.00010978348035284684,
      "loss": 0.5012,
      "step": 1130
    },
    {
      "epoch": 0.4525810324129652,
      "grad_norm": 0.11124735325574875,
      "learning_rate": 0.00010970328789093826,
      "loss": 0.4752,
      "step": 1131
    },
    {
      "epoch": 0.4529811924769908,
      "grad_norm": 0.09622682631015778,
      "learning_rate": 0.00010962309542902969,
      "loss": 0.4201,
      "step": 1132
    },
    {
      "epoch": 0.4533813525410164,
      "grad_norm": 0.1089363768696785,
      "learning_rate": 0.00010954290296712108,
      "loss": 0.5042,
      "step": 1133
    },
    {
      "epoch": 0.453781512605042,
      "grad_norm": 0.06615132093429565,
      "learning_rate": 0.0001094627105052125,
      "loss": 0.3973,
      "step": 1134
    },
    {
      "epoch": 0.4541816726690676,
      "grad_norm": 0.1224646121263504,
      "learning_rate": 0.00010938251804330392,
      "loss": 0.4503,
      "step": 1135
    },
    {
      "epoch": 0.4545818327330932,
      "grad_norm": 0.09658320993185043,
      "learning_rate": 0.00010930232558139534,
      "loss": 0.4152,
      "step": 1136
    },
    {
      "epoch": 0.4549819927971189,
      "grad_norm": 0.12544041872024536,
      "learning_rate": 0.00010922213311948677,
      "loss": 0.5401,
      "step": 1137
    },
    {
      "epoch": 0.4553821528611445,
      "grad_norm": 0.1012214869260788,
      "learning_rate": 0.00010914194065757819,
      "loss": 0.4522,
      "step": 1138
    },
    {
      "epoch": 0.4557823129251701,
      "grad_norm": 0.10895460098981857,
      "learning_rate": 0.00010906174819566961,
      "loss": 0.4793,
      "step": 1139
    },
    {
      "epoch": 0.4561824729891957,
      "grad_norm": 0.11566303670406342,
      "learning_rate": 0.00010898155573376103,
      "loss": 0.5075,
      "step": 1140
    },
    {
      "epoch": 0.4565826330532213,
      "grad_norm": 0.10157070308923721,
      "learning_rate": 0.00010890136327185245,
      "loss": 0.3912,
      "step": 1141
    },
    {
      "epoch": 0.4569827931172469,
      "grad_norm": 0.09550430625677109,
      "learning_rate": 0.00010882117080994387,
      "loss": 0.4828,
      "step": 1142
    },
    {
      "epoch": 0.45738295318127253,
      "grad_norm": 0.09947625547647476,
      "learning_rate": 0.0001087409783480353,
      "loss": 0.4915,
      "step": 1143
    },
    {
      "epoch": 0.45778311324529813,
      "grad_norm": 0.08482134342193604,
      "learning_rate": 0.00010866078588612672,
      "loss": 0.3864,
      "step": 1144
    },
    {
      "epoch": 0.45818327330932374,
      "grad_norm": 0.07723525166511536,
      "learning_rate": 0.00010858059342421814,
      "loss": 0.3874,
      "step": 1145
    },
    {
      "epoch": 0.45858343337334934,
      "grad_norm": 0.10337643325328827,
      "learning_rate": 0.00010850040096230956,
      "loss": 0.426,
      "step": 1146
    },
    {
      "epoch": 0.45898359343737494,
      "grad_norm": 0.08164754509925842,
      "learning_rate": 0.00010842020850040098,
      "loss": 0.4594,
      "step": 1147
    },
    {
      "epoch": 0.45938375350140054,
      "grad_norm": 0.10058070719242096,
      "learning_rate": 0.0001083400160384924,
      "loss": 0.4933,
      "step": 1148
    },
    {
      "epoch": 0.4597839135654262,
      "grad_norm": 0.10350310802459717,
      "learning_rate": 0.0001082598235765838,
      "loss": 0.4622,
      "step": 1149
    },
    {
      "epoch": 0.4601840736294518,
      "grad_norm": 0.09768911451101303,
      "learning_rate": 0.00010817963111467522,
      "loss": 0.5116,
      "step": 1150
    },
    {
      "epoch": 0.4605842336934774,
      "grad_norm": 0.09016308188438416,
      "learning_rate": 0.00010809943865276664,
      "loss": 0.4104,
      "step": 1151
    },
    {
      "epoch": 0.460984393757503,
      "grad_norm": 0.11958757042884827,
      "learning_rate": 0.00010801924619085806,
      "loss": 0.5346,
      "step": 1152
    },
    {
      "epoch": 0.4613845538215286,
      "grad_norm": 0.09158502519130707,
      "learning_rate": 0.00010793905372894948,
      "loss": 0.4206,
      "step": 1153
    },
    {
      "epoch": 0.4617847138855542,
      "grad_norm": 0.10573466122150421,
      "learning_rate": 0.0001078588612670409,
      "loss": 0.456,
      "step": 1154
    },
    {
      "epoch": 0.46218487394957986,
      "grad_norm": 0.106371209025383,
      "learning_rate": 0.00010777866880513233,
      "loss": 0.469,
      "step": 1155
    },
    {
      "epoch": 0.46258503401360546,
      "grad_norm": 0.11058425158262253,
      "learning_rate": 0.00010769847634322375,
      "loss": 0.4999,
      "step": 1156
    },
    {
      "epoch": 0.46298519407763106,
      "grad_norm": 0.0901288315653801,
      "learning_rate": 0.00010761828388131517,
      "loss": 0.4204,
      "step": 1157
    },
    {
      "epoch": 0.46338535414165666,
      "grad_norm": 0.0918613150715828,
      "learning_rate": 0.00010753809141940659,
      "loss": 0.4413,
      "step": 1158
    },
    {
      "epoch": 0.46378551420568226,
      "grad_norm": 0.09154076874256134,
      "learning_rate": 0.000107457898957498,
      "loss": 0.4585,
      "step": 1159
    },
    {
      "epoch": 0.46418567426970786,
      "grad_norm": 0.09168063849210739,
      "learning_rate": 0.00010737770649558942,
      "loss": 0.4401,
      "step": 1160
    },
    {
      "epoch": 0.4645858343337335,
      "grad_norm": 0.0780988410115242,
      "learning_rate": 0.00010729751403368084,
      "loss": 0.3701,
      "step": 1161
    },
    {
      "epoch": 0.4649859943977591,
      "grad_norm": 0.09291480481624603,
      "learning_rate": 0.00010721732157177226,
      "loss": 0.4815,
      "step": 1162
    },
    {
      "epoch": 0.4653861544617847,
      "grad_norm": 0.07677217572927475,
      "learning_rate": 0.00010713712910986368,
      "loss": 0.3695,
      "step": 1163
    },
    {
      "epoch": 0.4657863145258103,
      "grad_norm": 0.09251762181520462,
      "learning_rate": 0.0001070569366479551,
      "loss": 0.5181,
      "step": 1164
    },
    {
      "epoch": 0.4661864745898359,
      "grad_norm": 0.07461909204721451,
      "learning_rate": 0.00010697674418604651,
      "loss": 0.3652,
      "step": 1165
    },
    {
      "epoch": 0.4665866346538615,
      "grad_norm": 0.10125123709440231,
      "learning_rate": 0.00010689655172413792,
      "loss": 0.4922,
      "step": 1166
    },
    {
      "epoch": 0.4669867947178872,
      "grad_norm": 0.09619778394699097,
      "learning_rate": 0.00010681635926222934,
      "loss": 0.4815,
      "step": 1167
    },
    {
      "epoch": 0.4673869547819128,
      "grad_norm": 0.08698978275060654,
      "learning_rate": 0.00010673616680032076,
      "loss": 0.3978,
      "step": 1168
    },
    {
      "epoch": 0.4677871148459384,
      "grad_norm": 0.0833466500043869,
      "learning_rate": 0.00010665597433841219,
      "loss": 0.408,
      "step": 1169
    },
    {
      "epoch": 0.468187274909964,
      "grad_norm": 0.10920971632003784,
      "learning_rate": 0.00010657578187650361,
      "loss": 0.5197,
      "step": 1170
    },
    {
      "epoch": 0.4685874349739896,
      "grad_norm": 0.08936520665884018,
      "learning_rate": 0.00010649558941459503,
      "loss": 0.4429,
      "step": 1171
    },
    {
      "epoch": 0.4689875950380152,
      "grad_norm": 0.0908113569021225,
      "learning_rate": 0.00010641539695268645,
      "loss": 0.322,
      "step": 1172
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 0.10045351088047028,
      "learning_rate": 0.00010633520449077787,
      "loss": 0.4717,
      "step": 1173
    },
    {
      "epoch": 0.46978791516606644,
      "grad_norm": 0.12949365377426147,
      "learning_rate": 0.00010625501202886929,
      "loss": 0.5267,
      "step": 1174
    },
    {
      "epoch": 0.47018807523009204,
      "grad_norm": 0.09872793406248093,
      "learning_rate": 0.00010617481956696071,
      "loss": 0.3962,
      "step": 1175
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.09864099323749542,
      "learning_rate": 0.00010609462710505214,
      "loss": 0.4727,
      "step": 1176
    },
    {
      "epoch": 0.47098839535814324,
      "grad_norm": 0.11061331629753113,
      "learning_rate": 0.00010601443464314356,
      "loss": 0.5371,
      "step": 1177
    },
    {
      "epoch": 0.47138855542216884,
      "grad_norm": 0.1050955131649971,
      "learning_rate": 0.00010593424218123498,
      "loss": 0.5051,
      "step": 1178
    },
    {
      "epoch": 0.4717887154861945,
      "grad_norm": 0.11800223588943481,
      "learning_rate": 0.0001058540497193264,
      "loss": 0.4728,
      "step": 1179
    },
    {
      "epoch": 0.4721888755502201,
      "grad_norm": 0.09409110993146896,
      "learning_rate": 0.00010577385725741782,
      "loss": 0.4029,
      "step": 1180
    },
    {
      "epoch": 0.4725890356142457,
      "grad_norm": 0.11403032392263412,
      "learning_rate": 0.00010569366479550922,
      "loss": 0.4719,
      "step": 1181
    },
    {
      "epoch": 0.4729891956782713,
      "grad_norm": 0.10911532491445541,
      "learning_rate": 0.00010561347233360064,
      "loss": 0.5456,
      "step": 1182
    },
    {
      "epoch": 0.4733893557422969,
      "grad_norm": 0.08469131588935852,
      "learning_rate": 0.00010553327987169206,
      "loss": 0.3946,
      "step": 1183
    },
    {
      "epoch": 0.4737895158063225,
      "grad_norm": 0.13287755846977234,
      "learning_rate": 0.00010545308740978348,
      "loss": 0.436,
      "step": 1184
    },
    {
      "epoch": 0.47418967587034816,
      "grad_norm": 0.0969180315732956,
      "learning_rate": 0.0001053728949478749,
      "loss": 0.4013,
      "step": 1185
    },
    {
      "epoch": 0.47458983593437376,
      "grad_norm": 0.1102292463183403,
      "learning_rate": 0.00010529270248596632,
      "loss": 0.5212,
      "step": 1186
    },
    {
      "epoch": 0.47498999599839936,
      "grad_norm": 0.09876353293657303,
      "learning_rate": 0.00010521251002405774,
      "loss": 0.4757,
      "step": 1187
    },
    {
      "epoch": 0.47539015606242496,
      "grad_norm": 0.08562988042831421,
      "learning_rate": 0.00010513231756214917,
      "loss": 0.4293,
      "step": 1188
    },
    {
      "epoch": 0.47579031612645056,
      "grad_norm": 0.09256604313850403,
      "learning_rate": 0.00010505212510024059,
      "loss": 0.4131,
      "step": 1189
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.09081859141588211,
      "learning_rate": 0.00010497193263833201,
      "loss": 0.4234,
      "step": 1190
    },
    {
      "epoch": 0.4765906362545018,
      "grad_norm": 0.10380852967500687,
      "learning_rate": 0.00010489174017642343,
      "loss": 0.4716,
      "step": 1191
    },
    {
      "epoch": 0.4769907963185274,
      "grad_norm": 0.07273276895284653,
      "learning_rate": 0.00010481154771451485,
      "loss": 0.3992,
      "step": 1192
    },
    {
      "epoch": 0.477390956382553,
      "grad_norm": 0.11703380942344666,
      "learning_rate": 0.00010473135525260626,
      "loss": 0.506,
      "step": 1193
    },
    {
      "epoch": 0.4777911164465786,
      "grad_norm": 0.0965091735124588,
      "learning_rate": 0.00010465116279069768,
      "loss": 0.3968,
      "step": 1194
    },
    {
      "epoch": 0.4781912765106042,
      "grad_norm": 0.0860842913389206,
      "learning_rate": 0.0001045709703287891,
      "loss": 0.4848,
      "step": 1195
    },
    {
      "epoch": 0.4785914365746298,
      "grad_norm": 0.08283058553934097,
      "learning_rate": 0.00010449077786688052,
      "loss": 0.4069,
      "step": 1196
    },
    {
      "epoch": 0.4789915966386555,
      "grad_norm": 0.08474832773208618,
      "learning_rate": 0.00010441058540497193,
      "loss": 0.395,
      "step": 1197
    },
    {
      "epoch": 0.4793917567026811,
      "grad_norm": 0.0763401910662651,
      "learning_rate": 0.00010433039294306335,
      "loss": 0.394,
      "step": 1198
    },
    {
      "epoch": 0.4797919167667067,
      "grad_norm": 0.09847937524318695,
      "learning_rate": 0.00010425020048115478,
      "loss": 0.4616,
      "step": 1199
    },
    {
      "epoch": 0.4801920768307323,
      "grad_norm": 0.09747204929590225,
      "learning_rate": 0.0001041700080192462,
      "loss": 0.4757,
      "step": 1200
    },
    {
      "epoch": 0.4805922368947579,
      "grad_norm": 0.09688470512628555,
      "learning_rate": 0.0001040898155573376,
      "loss": 0.3975,
      "step": 1201
    },
    {
      "epoch": 0.4809923969587835,
      "grad_norm": 0.11767160147428513,
      "learning_rate": 0.00010400962309542903,
      "loss": 0.4762,
      "step": 1202
    },
    {
      "epoch": 0.48139255702280914,
      "grad_norm": 0.10516846179962158,
      "learning_rate": 0.00010392943063352045,
      "loss": 0.5091,
      "step": 1203
    },
    {
      "epoch": 0.48179271708683474,
      "grad_norm": 0.09034105390310287,
      "learning_rate": 0.00010384923817161187,
      "loss": 0.4218,
      "step": 1204
    },
    {
      "epoch": 0.48219287715086034,
      "grad_norm": 0.09202263504266739,
      "learning_rate": 0.00010376904570970329,
      "loss": 0.483,
      "step": 1205
    },
    {
      "epoch": 0.48259303721488594,
      "grad_norm": 0.09683241695165634,
      "learning_rate": 0.00010368885324779471,
      "loss": 0.4977,
      "step": 1206
    },
    {
      "epoch": 0.48299319727891155,
      "grad_norm": 0.09316375106573105,
      "learning_rate": 0.00010360866078588613,
      "loss": 0.4406,
      "step": 1207
    },
    {
      "epoch": 0.4833933573429372,
      "grad_norm": 0.0872921496629715,
      "learning_rate": 0.00010352846832397756,
      "loss": 0.4361,
      "step": 1208
    },
    {
      "epoch": 0.4837935174069628,
      "grad_norm": 0.1073889285326004,
      "learning_rate": 0.00010344827586206898,
      "loss": 0.4776,
      "step": 1209
    },
    {
      "epoch": 0.4841936774709884,
      "grad_norm": 0.10181231796741486,
      "learning_rate": 0.0001033680834001604,
      "loss": 0.4639,
      "step": 1210
    },
    {
      "epoch": 0.484593837535014,
      "grad_norm": 0.09851769357919693,
      "learning_rate": 0.00010328789093825182,
      "loss": 0.4776,
      "step": 1211
    },
    {
      "epoch": 0.4849939975990396,
      "grad_norm": 0.116099514067173,
      "learning_rate": 0.00010320769847634324,
      "loss": 0.5192,
      "step": 1212
    },
    {
      "epoch": 0.4853941576630652,
      "grad_norm": 0.09925894439220428,
      "learning_rate": 0.00010312750601443464,
      "loss": 0.4558,
      "step": 1213
    },
    {
      "epoch": 0.48579431772709086,
      "grad_norm": 0.10957067459821701,
      "learning_rate": 0.00010304731355252606,
      "loss": 0.5172,
      "step": 1214
    },
    {
      "epoch": 0.48619447779111646,
      "grad_norm": 0.10888145118951797,
      "learning_rate": 0.00010296712109061748,
      "loss": 0.4646,
      "step": 1215
    },
    {
      "epoch": 0.48659463785514206,
      "grad_norm": 0.09670209884643555,
      "learning_rate": 0.0001028869286287089,
      "loss": 0.448,
      "step": 1216
    },
    {
      "epoch": 0.48699479791916767,
      "grad_norm": 0.10104179382324219,
      "learning_rate": 0.00010280673616680032,
      "loss": 0.4208,
      "step": 1217
    },
    {
      "epoch": 0.48739495798319327,
      "grad_norm": 0.09336549043655396,
      "learning_rate": 0.00010272654370489174,
      "loss": 0.4173,
      "step": 1218
    },
    {
      "epoch": 0.48779511804721887,
      "grad_norm": 0.10289730131626129,
      "learning_rate": 0.00010264635124298316,
      "loss": 0.4612,
      "step": 1219
    },
    {
      "epoch": 0.4881952781112445,
      "grad_norm": 0.11371099948883057,
      "learning_rate": 0.00010256615878107459,
      "loss": 0.4524,
      "step": 1220
    },
    {
      "epoch": 0.4885954381752701,
      "grad_norm": 0.101377934217453,
      "learning_rate": 0.00010248596631916601,
      "loss": 0.506,
      "step": 1221
    },
    {
      "epoch": 0.4889955982392957,
      "grad_norm": 0.09550256282091141,
      "learning_rate": 0.00010240577385725743,
      "loss": 0.5072,
      "step": 1222
    },
    {
      "epoch": 0.4893957583033213,
      "grad_norm": 0.0938519686460495,
      "learning_rate": 0.00010232558139534885,
      "loss": 0.457,
      "step": 1223
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 0.0887598767876625,
      "learning_rate": 0.00010224538893344027,
      "loss": 0.3663,
      "step": 1224
    },
    {
      "epoch": 0.49019607843137253,
      "grad_norm": 0.1256086230278015,
      "learning_rate": 0.00010216519647153169,
      "loss": 0.5243,
      "step": 1225
    },
    {
      "epoch": 0.4905962384953982,
      "grad_norm": 0.07728073000907898,
      "learning_rate": 0.00010208500400962311,
      "loss": 0.4116,
      "step": 1226
    },
    {
      "epoch": 0.4909963985594238,
      "grad_norm": 0.08449961245059967,
      "learning_rate": 0.00010200481154771454,
      "loss": 0.4172,
      "step": 1227
    },
    {
      "epoch": 0.4913965586234494,
      "grad_norm": 0.08922360092401505,
      "learning_rate": 0.00010192461908580594,
      "loss": 0.4331,
      "step": 1228
    },
    {
      "epoch": 0.491796718687475,
      "grad_norm": 0.08065003901720047,
      "learning_rate": 0.00010184442662389735,
      "loss": 0.3396,
      "step": 1229
    },
    {
      "epoch": 0.4921968787515006,
      "grad_norm": 0.09036591649055481,
      "learning_rate": 0.00010176423416198877,
      "loss": 0.4444,
      "step": 1230
    },
    {
      "epoch": 0.4925970388155262,
      "grad_norm": 0.14561346173286438,
      "learning_rate": 0.0001016840417000802,
      "loss": 0.5233,
      "step": 1231
    },
    {
      "epoch": 0.49299719887955185,
      "grad_norm": 0.10174443572759628,
      "learning_rate": 0.00010160384923817162,
      "loss": 0.4471,
      "step": 1232
    },
    {
      "epoch": 0.49339735894357745,
      "grad_norm": 0.10458733886480331,
      "learning_rate": 0.00010152365677626304,
      "loss": 0.5508,
      "step": 1233
    },
    {
      "epoch": 0.49379751900760305,
      "grad_norm": 0.0788370668888092,
      "learning_rate": 0.00010144346431435446,
      "loss": 0.4168,
      "step": 1234
    },
    {
      "epoch": 0.49419767907162865,
      "grad_norm": 0.08430352807044983,
      "learning_rate": 0.00010136327185244587,
      "loss": 0.4063,
      "step": 1235
    },
    {
      "epoch": 0.49459783913565425,
      "grad_norm": 0.0897006243467331,
      "learning_rate": 0.00010128307939053729,
      "loss": 0.3703,
      "step": 1236
    },
    {
      "epoch": 0.49499799919967985,
      "grad_norm": 0.10811793059110641,
      "learning_rate": 0.00010120288692862871,
      "loss": 0.4779,
      "step": 1237
    },
    {
      "epoch": 0.4953981592637055,
      "grad_norm": 0.12702658772468567,
      "learning_rate": 0.00010112269446672013,
      "loss": 0.5174,
      "step": 1238
    },
    {
      "epoch": 0.4957983193277311,
      "grad_norm": 0.10110004246234894,
      "learning_rate": 0.00010104250200481155,
      "loss": 0.5166,
      "step": 1239
    },
    {
      "epoch": 0.4961984793917567,
      "grad_norm": 0.09833161532878876,
      "learning_rate": 0.00010096230954290297,
      "loss": 0.4798,
      "step": 1240
    },
    {
      "epoch": 0.4965986394557823,
      "grad_norm": 0.11569608747959137,
      "learning_rate": 0.0001008821170809944,
      "loss": 0.5341,
      "step": 1241
    },
    {
      "epoch": 0.4969987995198079,
      "grad_norm": 0.10689708590507507,
      "learning_rate": 0.00010080192461908582,
      "loss": 0.5008,
      "step": 1242
    },
    {
      "epoch": 0.4973989595838335,
      "grad_norm": 0.107818104326725,
      "learning_rate": 0.00010072173215717724,
      "loss": 0.4841,
      "step": 1243
    },
    {
      "epoch": 0.49779911964785917,
      "grad_norm": 0.0998820960521698,
      "learning_rate": 0.00010064153969526866,
      "loss": 0.4671,
      "step": 1244
    },
    {
      "epoch": 0.49819927971188477,
      "grad_norm": 0.11094346642494202,
      "learning_rate": 0.00010056134723336005,
      "loss": 0.4467,
      "step": 1245
    },
    {
      "epoch": 0.49859943977591037,
      "grad_norm": 0.09107193350791931,
      "learning_rate": 0.00010048115477145148,
      "loss": 0.4271,
      "step": 1246
    },
    {
      "epoch": 0.49899959983993597,
      "grad_norm": 0.11805848777294159,
      "learning_rate": 0.0001004009623095429,
      "loss": 0.6013,
      "step": 1247
    },
    {
      "epoch": 0.49939975990396157,
      "grad_norm": 0.12476091831922531,
      "learning_rate": 0.00010032076984763432,
      "loss": 0.5271,
      "step": 1248
    },
    {
      "epoch": 0.49979991996798717,
      "grad_norm": 0.11802069842815399,
      "learning_rate": 0.00010024057738572574,
      "loss": 0.5035,
      "step": 1249
    },
    {
      "epoch": 0.5002000800320128,
      "grad_norm": 0.0890762135386467,
      "learning_rate": 0.00010016038492381716,
      "loss": 0.4567,
      "step": 1250
    },
    {
      "epoch": 0.5006002400960384,
      "grad_norm": 0.108780138194561,
      "learning_rate": 0.00010008019246190858,
      "loss": 0.4525,
      "step": 1251
    },
    {
      "epoch": 0.501000400160064,
      "grad_norm": 0.10802146047353745,
      "learning_rate": 0.0001,
      "loss": 0.4755,
      "step": 1252
    },
    {
      "epoch": 0.5014005602240896,
      "grad_norm": 0.09499883651733398,
      "learning_rate": 9.991980753809143e-05,
      "loss": 0.4581,
      "step": 1253
    },
    {
      "epoch": 0.5018007202881153,
      "grad_norm": 0.08896485716104507,
      "learning_rate": 9.983961507618285e-05,
      "loss": 0.4928,
      "step": 1254
    },
    {
      "epoch": 0.5022008803521408,
      "grad_norm": 0.09771466255187988,
      "learning_rate": 9.975942261427427e-05,
      "loss": 0.5044,
      "step": 1255
    },
    {
      "epoch": 0.5026010404161665,
      "grad_norm": 0.10418819636106491,
      "learning_rate": 9.967923015236568e-05,
      "loss": 0.6007,
      "step": 1256
    },
    {
      "epoch": 0.503001200480192,
      "grad_norm": 0.08356066793203354,
      "learning_rate": 9.95990376904571e-05,
      "loss": 0.4208,
      "step": 1257
    },
    {
      "epoch": 0.5034013605442177,
      "grad_norm": 0.10556333512067795,
      "learning_rate": 9.951884522854852e-05,
      "loss": 0.537,
      "step": 1258
    },
    {
      "epoch": 0.5038015206082433,
      "grad_norm": 0.10144053399562836,
      "learning_rate": 9.943865276663994e-05,
      "loss": 0.4299,
      "step": 1259
    },
    {
      "epoch": 0.5042016806722689,
      "grad_norm": 0.09374960511922836,
      "learning_rate": 9.935846030473136e-05,
      "loss": 0.4724,
      "step": 1260
    },
    {
      "epoch": 0.5046018407362945,
      "grad_norm": 0.10291188955307007,
      "learning_rate": 9.927826784282278e-05,
      "loss": 0.4997,
      "step": 1261
    },
    {
      "epoch": 0.5050020008003201,
      "grad_norm": 0.09626956284046173,
      "learning_rate": 9.91980753809142e-05,
      "loss": 0.3217,
      "step": 1262
    },
    {
      "epoch": 0.5054021608643458,
      "grad_norm": 0.1011495366692543,
      "learning_rate": 9.911788291900563e-05,
      "loss": 0.4887,
      "step": 1263
    },
    {
      "epoch": 0.5058023209283713,
      "grad_norm": 0.11317428946495056,
      "learning_rate": 9.903769045709704e-05,
      "loss": 0.4714,
      "step": 1264
    },
    {
      "epoch": 0.506202480992397,
      "grad_norm": 0.10255718976259232,
      "learning_rate": 9.895749799518846e-05,
      "loss": 0.477,
      "step": 1265
    },
    {
      "epoch": 0.5066026410564226,
      "grad_norm": 0.09391181915998459,
      "learning_rate": 9.887730553327988e-05,
      "loss": 0.4226,
      "step": 1266
    },
    {
      "epoch": 0.5070028011204482,
      "grad_norm": 0.0879550650715828,
      "learning_rate": 9.87971130713713e-05,
      "loss": 0.4726,
      "step": 1267
    },
    {
      "epoch": 0.5074029611844738,
      "grad_norm": 0.07826511561870575,
      "learning_rate": 9.871692060946272e-05,
      "loss": 0.4481,
      "step": 1268
    },
    {
      "epoch": 0.5078031212484994,
      "grad_norm": 0.08693559467792511,
      "learning_rate": 9.863672814755414e-05,
      "loss": 0.4131,
      "step": 1269
    },
    {
      "epoch": 0.508203281312525,
      "grad_norm": 0.0935586541891098,
      "learning_rate": 9.855653568564555e-05,
      "loss": 0.4327,
      "step": 1270
    },
    {
      "epoch": 0.5086034413765507,
      "grad_norm": 0.10275640338659286,
      "learning_rate": 9.847634322373697e-05,
      "loss": 0.4858,
      "step": 1271
    },
    {
      "epoch": 0.5090036014405762,
      "grad_norm": 0.0925198644399643,
      "learning_rate": 9.83961507618284e-05,
      "loss": 0.4451,
      "step": 1272
    },
    {
      "epoch": 0.5094037615046019,
      "grad_norm": 0.0982157289981842,
      "learning_rate": 9.83159582999198e-05,
      "loss": 0.4384,
      "step": 1273
    },
    {
      "epoch": 0.5098039215686274,
      "grad_norm": 0.08416546881198883,
      "learning_rate": 9.823576583801122e-05,
      "loss": 0.4563,
      "step": 1274
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 0.09125150740146637,
      "learning_rate": 9.815557337610264e-05,
      "loss": 0.4589,
      "step": 1275
    },
    {
      "epoch": 0.5106042416966786,
      "grad_norm": 0.10500575602054596,
      "learning_rate": 9.807538091419407e-05,
      "loss": 0.4407,
      "step": 1276
    },
    {
      "epoch": 0.5110044017607043,
      "grad_norm": 0.08086974918842316,
      "learning_rate": 9.799518845228549e-05,
      "loss": 0.4097,
      "step": 1277
    },
    {
      "epoch": 0.5114045618247299,
      "grad_norm": 0.1056004986166954,
      "learning_rate": 9.791499599037691e-05,
      "loss": 0.4571,
      "step": 1278
    },
    {
      "epoch": 0.5118047218887555,
      "grad_norm": 0.09625917673110962,
      "learning_rate": 9.783480352846833e-05,
      "loss": 0.443,
      "step": 1279
    },
    {
      "epoch": 0.5122048819527811,
      "grad_norm": 0.08985067158937454,
      "learning_rate": 9.775461106655974e-05,
      "loss": 0.4382,
      "step": 1280
    },
    {
      "epoch": 0.5126050420168067,
      "grad_norm": 0.10469522327184677,
      "learning_rate": 9.767441860465116e-05,
      "loss": 0.4807,
      "step": 1281
    },
    {
      "epoch": 0.5130052020808323,
      "grad_norm": 0.06411147117614746,
      "learning_rate": 9.759422614274258e-05,
      "loss": 0.3673,
      "step": 1282
    },
    {
      "epoch": 0.513405362144858,
      "grad_norm": 0.1056363582611084,
      "learning_rate": 9.7514033680834e-05,
      "loss": 0.4738,
      "step": 1283
    },
    {
      "epoch": 0.5138055222088835,
      "grad_norm": 0.10926006734371185,
      "learning_rate": 9.743384121892542e-05,
      "loss": 0.508,
      "step": 1284
    },
    {
      "epoch": 0.5142056822729092,
      "grad_norm": 0.08234646916389465,
      "learning_rate": 9.735364875701685e-05,
      "loss": 0.3377,
      "step": 1285
    },
    {
      "epoch": 0.5146058423369347,
      "grad_norm": 0.08234904706478119,
      "learning_rate": 9.727345629510827e-05,
      "loss": 0.394,
      "step": 1286
    },
    {
      "epoch": 0.5150060024009604,
      "grad_norm": 0.11218922585248947,
      "learning_rate": 9.719326383319969e-05,
      "loss": 0.4666,
      "step": 1287
    },
    {
      "epoch": 0.5154061624649859,
      "grad_norm": 0.11574356257915497,
      "learning_rate": 9.71130713712911e-05,
      "loss": 0.4268,
      "step": 1288
    },
    {
      "epoch": 0.5158063225290116,
      "grad_norm": 0.09053755551576614,
      "learning_rate": 9.703287890938252e-05,
      "loss": 0.3938,
      "step": 1289
    },
    {
      "epoch": 0.5162064825930373,
      "grad_norm": 0.08413472771644592,
      "learning_rate": 9.695268644747394e-05,
      "loss": 0.3483,
      "step": 1290
    },
    {
      "epoch": 0.5166066426570628,
      "grad_norm": 0.08458658307790756,
      "learning_rate": 9.687249398556536e-05,
      "loss": 0.4329,
      "step": 1291
    },
    {
      "epoch": 0.5170068027210885,
      "grad_norm": 0.08323202282190323,
      "learning_rate": 9.679230152365678e-05,
      "loss": 0.3874,
      "step": 1292
    },
    {
      "epoch": 0.517406962785114,
      "grad_norm": 0.09103763848543167,
      "learning_rate": 9.67121090617482e-05,
      "loss": 0.389,
      "step": 1293
    },
    {
      "epoch": 0.5178071228491397,
      "grad_norm": 0.11393555998802185,
      "learning_rate": 9.663191659983963e-05,
      "loss": 0.4973,
      "step": 1294
    },
    {
      "epoch": 0.5182072829131653,
      "grad_norm": 0.10227024555206299,
      "learning_rate": 9.655172413793105e-05,
      "loss": 0.4707,
      "step": 1295
    },
    {
      "epoch": 0.5186074429771909,
      "grad_norm": 0.09196633100509644,
      "learning_rate": 9.647153167602245e-05,
      "loss": 0.444,
      "step": 1296
    },
    {
      "epoch": 0.5190076030412165,
      "grad_norm": 0.10429036617279053,
      "learning_rate": 9.639133921411388e-05,
      "loss": 0.469,
      "step": 1297
    },
    {
      "epoch": 0.5194077631052421,
      "grad_norm": 0.09433944523334503,
      "learning_rate": 9.63111467522053e-05,
      "loss": 0.4148,
      "step": 1298
    },
    {
      "epoch": 0.5198079231692677,
      "grad_norm": 0.100466288626194,
      "learning_rate": 9.623095429029672e-05,
      "loss": 0.5418,
      "step": 1299
    },
    {
      "epoch": 0.5202080832332934,
      "grad_norm": 0.0927063375711441,
      "learning_rate": 9.615076182838814e-05,
      "loss": 0.4885,
      "step": 1300
    },
    {
      "epoch": 0.5206082432973189,
      "grad_norm": 0.08905693143606186,
      "learning_rate": 9.607056936647956e-05,
      "loss": 0.3649,
      "step": 1301
    },
    {
      "epoch": 0.5210084033613446,
      "grad_norm": 0.11787210404872894,
      "learning_rate": 9.599037690457098e-05,
      "loss": 0.4958,
      "step": 1302
    },
    {
      "epoch": 0.5214085634253701,
      "grad_norm": 0.08271889388561249,
      "learning_rate": 9.59101844426624e-05,
      "loss": 0.3993,
      "step": 1303
    },
    {
      "epoch": 0.5218087234893958,
      "grad_norm": 0.09487780183553696,
      "learning_rate": 9.582999198075381e-05,
      "loss": 0.4424,
      "step": 1304
    },
    {
      "epoch": 0.5222088835534213,
      "grad_norm": 0.09599895775318146,
      "learning_rate": 9.574979951884523e-05,
      "loss": 0.5008,
      "step": 1305
    },
    {
      "epoch": 0.522609043617447,
      "grad_norm": 0.1084233820438385,
      "learning_rate": 9.566960705693666e-05,
      "loss": 0.5298,
      "step": 1306
    },
    {
      "epoch": 0.5230092036814726,
      "grad_norm": 0.08145881444215775,
      "learning_rate": 9.558941459502808e-05,
      "loss": 0.4489,
      "step": 1307
    },
    {
      "epoch": 0.5234093637454982,
      "grad_norm": 0.09224049001932144,
      "learning_rate": 9.550922213311949e-05,
      "loss": 0.4263,
      "step": 1308
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.08819730579853058,
      "learning_rate": 9.54290296712109e-05,
      "loss": 0.4731,
      "step": 1309
    },
    {
      "epoch": 0.5242096838735494,
      "grad_norm": 0.12365785241127014,
      "learning_rate": 9.534883720930233e-05,
      "loss": 0.4937,
      "step": 1310
    },
    {
      "epoch": 0.524609843937575,
      "grad_norm": 0.08724664896726608,
      "learning_rate": 9.526864474739375e-05,
      "loss": 0.4688,
      "step": 1311
    },
    {
      "epoch": 0.5250100040016007,
      "grad_norm": 0.09414392709732056,
      "learning_rate": 9.518845228548516e-05,
      "loss": 0.4871,
      "step": 1312
    },
    {
      "epoch": 0.5254101640656262,
      "grad_norm": 0.08547277748584747,
      "learning_rate": 9.510825982357658e-05,
      "loss": 0.403,
      "step": 1313
    },
    {
      "epoch": 0.5258103241296519,
      "grad_norm": 0.09936480969190598,
      "learning_rate": 9.5028067361668e-05,
      "loss": 0.5088,
      "step": 1314
    },
    {
      "epoch": 0.5262104841936774,
      "grad_norm": 0.09366049617528915,
      "learning_rate": 9.494787489975942e-05,
      "loss": 0.4372,
      "step": 1315
    },
    {
      "epoch": 0.5266106442577031,
      "grad_norm": 0.11571111530065536,
      "learning_rate": 9.486768243785084e-05,
      "loss": 0.5464,
      "step": 1316
    },
    {
      "epoch": 0.5270108043217286,
      "grad_norm": 0.098357655107975,
      "learning_rate": 9.478748997594226e-05,
      "loss": 0.3974,
      "step": 1317
    },
    {
      "epoch": 0.5274109643857543,
      "grad_norm": 0.10452237725257874,
      "learning_rate": 9.470729751403369e-05,
      "loss": 0.4638,
      "step": 1318
    },
    {
      "epoch": 0.52781112444978,
      "grad_norm": 0.09913579374551773,
      "learning_rate": 9.462710505212511e-05,
      "loss": 0.4457,
      "step": 1319
    },
    {
      "epoch": 0.5282112845138055,
      "grad_norm": 0.11604087799787521,
      "learning_rate": 9.454691259021652e-05,
      "loss": 0.5876,
      "step": 1320
    },
    {
      "epoch": 0.5286114445778312,
      "grad_norm": 0.08609975129365921,
      "learning_rate": 9.446672012830794e-05,
      "loss": 0.4137,
      "step": 1321
    },
    {
      "epoch": 0.5290116046418567,
      "grad_norm": 0.07375194132328033,
      "learning_rate": 9.438652766639936e-05,
      "loss": 0.2824,
      "step": 1322
    },
    {
      "epoch": 0.5294117647058824,
      "grad_norm": 0.11498292535543442,
      "learning_rate": 9.430633520449078e-05,
      "loss": 0.4586,
      "step": 1323
    },
    {
      "epoch": 0.529811924769908,
      "grad_norm": 0.08765692263841629,
      "learning_rate": 9.42261427425822e-05,
      "loss": 0.4213,
      "step": 1324
    },
    {
      "epoch": 0.5302120848339336,
      "grad_norm": 0.09159576147794724,
      "learning_rate": 9.414595028067362e-05,
      "loss": 0.4619,
      "step": 1325
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 0.10039863735437393,
      "learning_rate": 9.406575781876504e-05,
      "loss": 0.4506,
      "step": 1326
    },
    {
      "epoch": 0.5310124049619848,
      "grad_norm": 0.06938427686691284,
      "learning_rate": 9.398556535685647e-05,
      "loss": 0.3932,
      "step": 1327
    },
    {
      "epoch": 0.5314125650260104,
      "grad_norm": 0.09700144827365875,
      "learning_rate": 9.390537289494787e-05,
      "loss": 0.5072,
      "step": 1328
    },
    {
      "epoch": 0.531812725090036,
      "grad_norm": 0.11087781935930252,
      "learning_rate": 9.38251804330393e-05,
      "loss": 0.473,
      "step": 1329
    },
    {
      "epoch": 0.5322128851540616,
      "grad_norm": 0.10246948152780533,
      "learning_rate": 9.374498797113072e-05,
      "loss": 0.4678,
      "step": 1330
    },
    {
      "epoch": 0.5326130452180873,
      "grad_norm": 0.1107712835073471,
      "learning_rate": 9.366479550922214e-05,
      "loss": 0.501,
      "step": 1331
    },
    {
      "epoch": 0.5330132052821128,
      "grad_norm": 0.09313911944627762,
      "learning_rate": 9.358460304731356e-05,
      "loss": 0.4916,
      "step": 1332
    },
    {
      "epoch": 0.5334133653461385,
      "grad_norm": 0.09404215961694717,
      "learning_rate": 9.350441058540498e-05,
      "loss": 0.499,
      "step": 1333
    },
    {
      "epoch": 0.533813525410164,
      "grad_norm": 0.07871028035879135,
      "learning_rate": 9.34242181234964e-05,
      "loss": 0.47,
      "step": 1334
    },
    {
      "epoch": 0.5342136854741897,
      "grad_norm": 0.12259797006845474,
      "learning_rate": 9.334402566158782e-05,
      "loss": 0.4864,
      "step": 1335
    },
    {
      "epoch": 0.5346138455382153,
      "grad_norm": 0.10160084813833237,
      "learning_rate": 9.326383319967923e-05,
      "loss": 0.5535,
      "step": 1336
    },
    {
      "epoch": 0.5350140056022409,
      "grad_norm": 0.11357774585485458,
      "learning_rate": 9.318364073777065e-05,
      "loss": 0.5605,
      "step": 1337
    },
    {
      "epoch": 0.5354141656662665,
      "grad_norm": 0.1262708306312561,
      "learning_rate": 9.310344827586207e-05,
      "loss": 0.5708,
      "step": 1338
    },
    {
      "epoch": 0.5358143257302921,
      "grad_norm": 0.08657581359148026,
      "learning_rate": 9.30232558139535e-05,
      "loss": 0.4143,
      "step": 1339
    },
    {
      "epoch": 0.5362144857943177,
      "grad_norm": 0.10557174682617188,
      "learning_rate": 9.294306335204492e-05,
      "loss": 0.5218,
      "step": 1340
    },
    {
      "epoch": 0.5366146458583433,
      "grad_norm": 0.10391118377447128,
      "learning_rate": 9.286287089013634e-05,
      "loss": 0.4859,
      "step": 1341
    },
    {
      "epoch": 0.5370148059223689,
      "grad_norm": 0.11249544471502304,
      "learning_rate": 9.278267842822775e-05,
      "loss": 0.5436,
      "step": 1342
    },
    {
      "epoch": 0.5374149659863946,
      "grad_norm": 0.09408850222826004,
      "learning_rate": 9.270248596631917e-05,
      "loss": 0.493,
      "step": 1343
    },
    {
      "epoch": 0.5378151260504201,
      "grad_norm": 0.09883435815572739,
      "learning_rate": 9.262229350441059e-05,
      "loss": 0.482,
      "step": 1344
    },
    {
      "epoch": 0.5382152861144458,
      "grad_norm": 0.11181621998548508,
      "learning_rate": 9.254210104250201e-05,
      "loss": 0.5292,
      "step": 1345
    },
    {
      "epoch": 0.5386154461784713,
      "grad_norm": 0.10235941410064697,
      "learning_rate": 9.246190858059342e-05,
      "loss": 0.5353,
      "step": 1346
    },
    {
      "epoch": 0.539015606242497,
      "grad_norm": 0.11616203933954239,
      "learning_rate": 9.238171611868484e-05,
      "loss": 0.5447,
      "step": 1347
    },
    {
      "epoch": 0.5394157663065227,
      "grad_norm": 0.09571994841098785,
      "learning_rate": 9.230152365677626e-05,
      "loss": 0.5308,
      "step": 1348
    },
    {
      "epoch": 0.5398159263705482,
      "grad_norm": 0.10572952032089233,
      "learning_rate": 9.222133119486768e-05,
      "loss": 0.5927,
      "step": 1349
    },
    {
      "epoch": 0.5402160864345739,
      "grad_norm": 0.08565967530012131,
      "learning_rate": 9.21411387329591e-05,
      "loss": 0.365,
      "step": 1350
    },
    {
      "epoch": 0.5406162464985994,
      "grad_norm": 0.09805146604776382,
      "learning_rate": 9.206094627105053e-05,
      "loss": 0.4782,
      "step": 1351
    },
    {
      "epoch": 0.5410164065626251,
      "grad_norm": 0.08121727406978607,
      "learning_rate": 9.198075380914193e-05,
      "loss": 0.4361,
      "step": 1352
    },
    {
      "epoch": 0.5414165666266506,
      "grad_norm": 0.10113593190908432,
      "learning_rate": 9.190056134723336e-05,
      "loss": 0.4504,
      "step": 1353
    },
    {
      "epoch": 0.5418167266906763,
      "grad_norm": 0.12651830911636353,
      "learning_rate": 9.182036888532478e-05,
      "loss": 0.4931,
      "step": 1354
    },
    {
      "epoch": 0.5422168867547019,
      "grad_norm": 0.11207452416419983,
      "learning_rate": 9.17401764234162e-05,
      "loss": 0.4966,
      "step": 1355
    },
    {
      "epoch": 0.5426170468187275,
      "grad_norm": 0.13606494665145874,
      "learning_rate": 9.165998396150762e-05,
      "loss": 0.535,
      "step": 1356
    },
    {
      "epoch": 0.5430172068827531,
      "grad_norm": 0.09510021656751633,
      "learning_rate": 9.157979149959904e-05,
      "loss": 0.4703,
      "step": 1357
    },
    {
      "epoch": 0.5434173669467787,
      "grad_norm": 0.0952828899025917,
      "learning_rate": 9.149959903769046e-05,
      "loss": 0.4727,
      "step": 1358
    },
    {
      "epoch": 0.5438175270108043,
      "grad_norm": 0.07563173025846481,
      "learning_rate": 9.141940657578188e-05,
      "loss": 0.3807,
      "step": 1359
    },
    {
      "epoch": 0.54421768707483,
      "grad_norm": 0.10059352964162827,
      "learning_rate": 9.133921411387329e-05,
      "loss": 0.4461,
      "step": 1360
    },
    {
      "epoch": 0.5446178471388555,
      "grad_norm": 0.09331872314214706,
      "learning_rate": 9.125902165196471e-05,
      "loss": 0.4198,
      "step": 1361
    },
    {
      "epoch": 0.5450180072028812,
      "grad_norm": 0.11275536566972733,
      "learning_rate": 9.117882919005614e-05,
      "loss": 0.5588,
      "step": 1362
    },
    {
      "epoch": 0.5454181672669067,
      "grad_norm": 0.08280109614133835,
      "learning_rate": 9.109863672814756e-05,
      "loss": 0.3837,
      "step": 1363
    },
    {
      "epoch": 0.5458183273309324,
      "grad_norm": 0.08472751826047897,
      "learning_rate": 9.101844426623898e-05,
      "loss": 0.4813,
      "step": 1364
    },
    {
      "epoch": 0.5462184873949579,
      "grad_norm": 0.09681776911020279,
      "learning_rate": 9.09382518043304e-05,
      "loss": 0.4763,
      "step": 1365
    },
    {
      "epoch": 0.5466186474589836,
      "grad_norm": 0.10075978189706802,
      "learning_rate": 9.085805934242182e-05,
      "loss": 0.4808,
      "step": 1366
    },
    {
      "epoch": 0.5470188075230092,
      "grad_norm": 0.09868668764829636,
      "learning_rate": 9.077786688051324e-05,
      "loss": 0.518,
      "step": 1367
    },
    {
      "epoch": 0.5474189675870348,
      "grad_norm": 0.10456728935241699,
      "learning_rate": 9.069767441860465e-05,
      "loss": 0.5031,
      "step": 1368
    },
    {
      "epoch": 0.5478191276510604,
      "grad_norm": 0.1013646200299263,
      "learning_rate": 9.061748195669607e-05,
      "loss": 0.5073,
      "step": 1369
    },
    {
      "epoch": 0.548219287715086,
      "grad_norm": 0.09065838158130646,
      "learning_rate": 9.05372894947875e-05,
      "loss": 0.45,
      "step": 1370
    },
    {
      "epoch": 0.5486194477791116,
      "grad_norm": 0.09156855940818787,
      "learning_rate": 9.045709703287892e-05,
      "loss": 0.4457,
      "step": 1371
    },
    {
      "epoch": 0.5490196078431373,
      "grad_norm": 0.10120506584644318,
      "learning_rate": 9.037690457097034e-05,
      "loss": 0.5022,
      "step": 1372
    },
    {
      "epoch": 0.5494197679071628,
      "grad_norm": 0.13457639515399933,
      "learning_rate": 9.029671210906176e-05,
      "loss": 0.5632,
      "step": 1373
    },
    {
      "epoch": 0.5498199279711885,
      "grad_norm": 0.10311321914196014,
      "learning_rate": 9.021651964715318e-05,
      "loss": 0.5031,
      "step": 1374
    },
    {
      "epoch": 0.550220088035214,
      "grad_norm": 0.1164599284529686,
      "learning_rate": 9.01363271852446e-05,
      "loss": 0.531,
      "step": 1375
    },
    {
      "epoch": 0.5506202480992397,
      "grad_norm": 0.07603322714567184,
      "learning_rate": 9.005613472333601e-05,
      "loss": 0.3967,
      "step": 1376
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 0.10754028707742691,
      "learning_rate": 8.997594226142743e-05,
      "loss": 0.4992,
      "step": 1377
    },
    {
      "epoch": 0.5514205682272909,
      "grad_norm": 0.1000593900680542,
      "learning_rate": 8.989574979951885e-05,
      "loss": 0.4087,
      "step": 1378
    },
    {
      "epoch": 0.5518207282913166,
      "grad_norm": 0.12422724813222885,
      "learning_rate": 8.981555733761027e-05,
      "loss": 0.5189,
      "step": 1379
    },
    {
      "epoch": 0.5522208883553421,
      "grad_norm": 0.10126390308141708,
      "learning_rate": 8.97353648757017e-05,
      "loss": 0.5127,
      "step": 1380
    },
    {
      "epoch": 0.5526210484193678,
      "grad_norm": 0.08093487471342087,
      "learning_rate": 8.96551724137931e-05,
      "loss": 0.379,
      "step": 1381
    },
    {
      "epoch": 0.5530212084833933,
      "grad_norm": 0.07558295875787735,
      "learning_rate": 8.957497995188452e-05,
      "loss": 0.3554,
      "step": 1382
    },
    {
      "epoch": 0.553421368547419,
      "grad_norm": 0.08214796334505081,
      "learning_rate": 8.949478748997595e-05,
      "loss": 0.4665,
      "step": 1383
    },
    {
      "epoch": 0.5538215286114446,
      "grad_norm": 0.1067107617855072,
      "learning_rate": 8.941459502806735e-05,
      "loss": 0.4341,
      "step": 1384
    },
    {
      "epoch": 0.5542216886754702,
      "grad_norm": 0.0978003516793251,
      "learning_rate": 8.933440256615878e-05,
      "loss": 0.4707,
      "step": 1385
    },
    {
      "epoch": 0.5546218487394958,
      "grad_norm": 0.10911732912063599,
      "learning_rate": 8.92542101042502e-05,
      "loss": 0.4768,
      "step": 1386
    },
    {
      "epoch": 0.5550220088035214,
      "grad_norm": 0.10537680238485336,
      "learning_rate": 8.917401764234162e-05,
      "loss": 0.5765,
      "step": 1387
    },
    {
      "epoch": 0.555422168867547,
      "grad_norm": 0.09787910431623459,
      "learning_rate": 8.909382518043304e-05,
      "loss": 0.4798,
      "step": 1388
    },
    {
      "epoch": 0.5558223289315727,
      "grad_norm": 0.1052498146891594,
      "learning_rate": 8.901363271852446e-05,
      "loss": 0.5234,
      "step": 1389
    },
    {
      "epoch": 0.5562224889955982,
      "grad_norm": 0.09639964997768402,
      "learning_rate": 8.893344025661588e-05,
      "loss": 0.4466,
      "step": 1390
    },
    {
      "epoch": 0.5566226490596239,
      "grad_norm": 0.07294156402349472,
      "learning_rate": 8.88532477947073e-05,
      "loss": 0.325,
      "step": 1391
    },
    {
      "epoch": 0.5570228091236494,
      "grad_norm": 0.13725735247135162,
      "learning_rate": 8.877305533279871e-05,
      "loss": 0.5536,
      "step": 1392
    },
    {
      "epoch": 0.5574229691876751,
      "grad_norm": 0.08663837611675262,
      "learning_rate": 8.869286287089013e-05,
      "loss": 0.4089,
      "step": 1393
    },
    {
      "epoch": 0.5578231292517006,
      "grad_norm": 0.09804115444421768,
      "learning_rate": 8.861267040898156e-05,
      "loss": 0.5231,
      "step": 1394
    },
    {
      "epoch": 0.5582232893157263,
      "grad_norm": 0.09358413517475128,
      "learning_rate": 8.853247794707298e-05,
      "loss": 0.5102,
      "step": 1395
    },
    {
      "epoch": 0.558623449379752,
      "grad_norm": 0.08957011252641678,
      "learning_rate": 8.84522854851644e-05,
      "loss": 0.4264,
      "step": 1396
    },
    {
      "epoch": 0.5590236094437775,
      "grad_norm": 0.10706499963998795,
      "learning_rate": 8.837209302325582e-05,
      "loss": 0.4924,
      "step": 1397
    },
    {
      "epoch": 0.5594237695078031,
      "grad_norm": 0.09232097864151001,
      "learning_rate": 8.829190056134724e-05,
      "loss": 0.4326,
      "step": 1398
    },
    {
      "epoch": 0.5598239295718287,
      "grad_norm": 0.09585981070995331,
      "learning_rate": 8.821170809943866e-05,
      "loss": 0.4275,
      "step": 1399
    },
    {
      "epoch": 0.5602240896358543,
      "grad_norm": 0.0974683165550232,
      "learning_rate": 8.813151563753007e-05,
      "loss": 0.5151,
      "step": 1400
    },
    {
      "epoch": 0.56062424969988,
      "grad_norm": 0.09082184731960297,
      "learning_rate": 8.805132317562149e-05,
      "loss": 0.4725,
      "step": 1401
    },
    {
      "epoch": 0.5610244097639056,
      "grad_norm": 0.1018221378326416,
      "learning_rate": 8.797113071371291e-05,
      "loss": 0.5026,
      "step": 1402
    },
    {
      "epoch": 0.5614245698279312,
      "grad_norm": 0.09059032797813416,
      "learning_rate": 8.789093825180433e-05,
      "loss": 0.4648,
      "step": 1403
    },
    {
      "epoch": 0.5618247298919568,
      "grad_norm": 0.08061771094799042,
      "learning_rate": 8.781074578989576e-05,
      "loss": 0.4727,
      "step": 1404
    },
    {
      "epoch": 0.5622248899559824,
      "grad_norm": 0.10375486314296722,
      "learning_rate": 8.773055332798718e-05,
      "loss": 0.4017,
      "step": 1405
    },
    {
      "epoch": 0.562625050020008,
      "grad_norm": 0.1065104678273201,
      "learning_rate": 8.76503608660786e-05,
      "loss": 0.4612,
      "step": 1406
    },
    {
      "epoch": 0.5630252100840336,
      "grad_norm": 0.11011837422847748,
      "learning_rate": 8.757016840417002e-05,
      "loss": 0.5455,
      "step": 1407
    },
    {
      "epoch": 0.5634253701480593,
      "grad_norm": 0.11241969466209412,
      "learning_rate": 8.748997594226143e-05,
      "loss": 0.5189,
      "step": 1408
    },
    {
      "epoch": 0.5638255302120848,
      "grad_norm": 0.07911563664674759,
      "learning_rate": 8.740978348035285e-05,
      "loss": 0.3955,
      "step": 1409
    },
    {
      "epoch": 0.5642256902761105,
      "grad_norm": 0.07853098958730698,
      "learning_rate": 8.732959101844427e-05,
      "loss": 0.3811,
      "step": 1410
    },
    {
      "epoch": 0.564625850340136,
      "grad_norm": 0.0817602127790451,
      "learning_rate": 8.724939855653569e-05,
      "loss": 0.3899,
      "step": 1411
    },
    {
      "epoch": 0.5650260104041617,
      "grad_norm": 0.10588596761226654,
      "learning_rate": 8.716920609462711e-05,
      "loss": 0.4823,
      "step": 1412
    },
    {
      "epoch": 0.5654261704681873,
      "grad_norm": 0.08491206914186478,
      "learning_rate": 8.708901363271854e-05,
      "loss": 0.4315,
      "step": 1413
    },
    {
      "epoch": 0.5658263305322129,
      "grad_norm": 0.1007683053612709,
      "learning_rate": 8.700882117080996e-05,
      "loss": 0.43,
      "step": 1414
    },
    {
      "epoch": 0.5662264905962385,
      "grad_norm": 0.08900223672389984,
      "learning_rate": 8.692862870890137e-05,
      "loss": 0.4425,
      "step": 1415
    },
    {
      "epoch": 0.5666266506602641,
      "grad_norm": 0.09155648201704025,
      "learning_rate": 8.684843624699279e-05,
      "loss": 0.4381,
      "step": 1416
    },
    {
      "epoch": 0.5670268107242897,
      "grad_norm": 0.08282478898763657,
      "learning_rate": 8.676824378508421e-05,
      "loss": 0.4044,
      "step": 1417
    },
    {
      "epoch": 0.5674269707883153,
      "grad_norm": 0.10418140143156052,
      "learning_rate": 8.668805132317563e-05,
      "loss": 0.4952,
      "step": 1418
    },
    {
      "epoch": 0.5678271308523409,
      "grad_norm": 0.09186633676290512,
      "learning_rate": 8.660785886126704e-05,
      "loss": 0.4485,
      "step": 1419
    },
    {
      "epoch": 0.5682272909163666,
      "grad_norm": 0.12482082843780518,
      "learning_rate": 8.652766639935846e-05,
      "loss": 0.3829,
      "step": 1420
    },
    {
      "epoch": 0.5686274509803921,
      "grad_norm": 0.09900031983852386,
      "learning_rate": 8.644747393744988e-05,
      "loss": 0.4386,
      "step": 1421
    },
    {
      "epoch": 0.5690276110444178,
      "grad_norm": 0.09452925622463226,
      "learning_rate": 8.63672814755413e-05,
      "loss": 0.4557,
      "step": 1422
    },
    {
      "epoch": 0.5694277711084433,
      "grad_norm": 0.1027890145778656,
      "learning_rate": 8.628708901363272e-05,
      "loss": 0.4511,
      "step": 1423
    },
    {
      "epoch": 0.569827931172469,
      "grad_norm": 0.08758499473333359,
      "learning_rate": 8.620689655172413e-05,
      "loss": 0.4258,
      "step": 1424
    },
    {
      "epoch": 0.5702280912364946,
      "grad_norm": 0.09386399388313293,
      "learning_rate": 8.612670408981555e-05,
      "loss": 0.4266,
      "step": 1425
    },
    {
      "epoch": 0.5706282513005202,
      "grad_norm": 0.08935915678739548,
      "learning_rate": 8.604651162790697e-05,
      "loss": 0.4329,
      "step": 1426
    },
    {
      "epoch": 0.5710284113645459,
      "grad_norm": 0.10582776367664337,
      "learning_rate": 8.59663191659984e-05,
      "loss": 0.4307,
      "step": 1427
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.10549893975257874,
      "learning_rate": 8.588612670408982e-05,
      "loss": 0.4738,
      "step": 1428
    },
    {
      "epoch": 0.571828731492597,
      "grad_norm": 0.09794189035892487,
      "learning_rate": 8.580593424218124e-05,
      "loss": 0.5147,
      "step": 1429
    },
    {
      "epoch": 0.5722288915566226,
      "grad_norm": 0.08130902051925659,
      "learning_rate": 8.572574178027266e-05,
      "loss": 0.3954,
      "step": 1430
    },
    {
      "epoch": 0.5726290516206483,
      "grad_norm": 0.08459024876356125,
      "learning_rate": 8.564554931836408e-05,
      "loss": 0.3548,
      "step": 1431
    },
    {
      "epoch": 0.5730292116846739,
      "grad_norm": 0.11267077177762985,
      "learning_rate": 8.556535685645549e-05,
      "loss": 0.5143,
      "step": 1432
    },
    {
      "epoch": 0.5734293717486995,
      "grad_norm": 0.08155310153961182,
      "learning_rate": 8.548516439454691e-05,
      "loss": 0.4378,
      "step": 1433
    },
    {
      "epoch": 0.5738295318127251,
      "grad_norm": 0.09018702805042267,
      "learning_rate": 8.540497193263833e-05,
      "loss": 0.493,
      "step": 1434
    },
    {
      "epoch": 0.5742296918767507,
      "grad_norm": 0.11298264563083649,
      "learning_rate": 8.532477947072975e-05,
      "loss": 0.3892,
      "step": 1435
    },
    {
      "epoch": 0.5746298519407763,
      "grad_norm": 0.09833948314189911,
      "learning_rate": 8.524458700882118e-05,
      "loss": 0.4919,
      "step": 1436
    },
    {
      "epoch": 0.575030012004802,
      "grad_norm": 0.09041136503219604,
      "learning_rate": 8.51643945469126e-05,
      "loss": 0.4119,
      "step": 1437
    },
    {
      "epoch": 0.5754301720688275,
      "grad_norm": 0.08530160039663315,
      "learning_rate": 8.508420208500402e-05,
      "loss": 0.4009,
      "step": 1438
    },
    {
      "epoch": 0.5758303321328532,
      "grad_norm": 0.10273928195238113,
      "learning_rate": 8.500400962309544e-05,
      "loss": 0.3912,
      "step": 1439
    },
    {
      "epoch": 0.5762304921968787,
      "grad_norm": 0.11267959326505661,
      "learning_rate": 8.492381716118685e-05,
      "loss": 0.5115,
      "step": 1440
    },
    {
      "epoch": 0.5766306522609044,
      "grad_norm": 0.09360060095787048,
      "learning_rate": 8.484362469927827e-05,
      "loss": 0.4709,
      "step": 1441
    },
    {
      "epoch": 0.5770308123249299,
      "grad_norm": 0.10684001445770264,
      "learning_rate": 8.476343223736969e-05,
      "loss": 0.5406,
      "step": 1442
    },
    {
      "epoch": 0.5774309723889556,
      "grad_norm": 0.08724807947874069,
      "learning_rate": 8.468323977546111e-05,
      "loss": 0.4298,
      "step": 1443
    },
    {
      "epoch": 0.5778311324529812,
      "grad_norm": 0.08349840342998505,
      "learning_rate": 8.460304731355253e-05,
      "loss": 0.4336,
      "step": 1444
    },
    {
      "epoch": 0.5782312925170068,
      "grad_norm": 0.10630670189857483,
      "learning_rate": 8.452285485164396e-05,
      "loss": 0.4866,
      "step": 1445
    },
    {
      "epoch": 0.5786314525810324,
      "grad_norm": 0.10643954575061798,
      "learning_rate": 8.444266238973538e-05,
      "loss": 0.4612,
      "step": 1446
    },
    {
      "epoch": 0.579031612645058,
      "grad_norm": 0.09366004168987274,
      "learning_rate": 8.43624699278268e-05,
      "loss": 0.4334,
      "step": 1447
    },
    {
      "epoch": 0.5794317727090836,
      "grad_norm": 0.09161847084760666,
      "learning_rate": 8.42822774659182e-05,
      "loss": 0.4298,
      "step": 1448
    },
    {
      "epoch": 0.5798319327731093,
      "grad_norm": 0.08947345614433289,
      "learning_rate": 8.420208500400963e-05,
      "loss": 0.466,
      "step": 1449
    },
    {
      "epoch": 0.5802320928371348,
      "grad_norm": 0.08071393519639969,
      "learning_rate": 8.412189254210105e-05,
      "loss": 0.4204,
      "step": 1450
    },
    {
      "epoch": 0.5806322529011605,
      "grad_norm": 0.07874172180891037,
      "learning_rate": 8.404170008019247e-05,
      "loss": 0.3719,
      "step": 1451
    },
    {
      "epoch": 0.581032412965186,
      "grad_norm": 0.09517621248960495,
      "learning_rate": 8.396150761828389e-05,
      "loss": 0.4462,
      "step": 1452
    },
    {
      "epoch": 0.5814325730292117,
      "grad_norm": 0.1096143126487732,
      "learning_rate": 8.38813151563753e-05,
      "loss": 0.4339,
      "step": 1453
    },
    {
      "epoch": 0.5818327330932372,
      "grad_norm": 0.08548357337713242,
      "learning_rate": 8.380112269446672e-05,
      "loss": 0.4042,
      "step": 1454
    },
    {
      "epoch": 0.5822328931572629,
      "grad_norm": 0.08905036747455597,
      "learning_rate": 8.372093023255814e-05,
      "loss": 0.4822,
      "step": 1455
    },
    {
      "epoch": 0.5826330532212886,
      "grad_norm": 0.10205740481615067,
      "learning_rate": 8.364073777064956e-05,
      "loss": 0.4461,
      "step": 1456
    },
    {
      "epoch": 0.5830332132853141,
      "grad_norm": 0.08627036958932877,
      "learning_rate": 8.356054530874097e-05,
      "loss": 0.4485,
      "step": 1457
    },
    {
      "epoch": 0.5834333733493398,
      "grad_norm": 0.09371685236692429,
      "learning_rate": 8.34803528468324e-05,
      "loss": 0.4207,
      "step": 1458
    },
    {
      "epoch": 0.5838335334133653,
      "grad_norm": 0.09738893061876297,
      "learning_rate": 8.340016038492381e-05,
      "loss": 0.4802,
      "step": 1459
    },
    {
      "epoch": 0.584233693477391,
      "grad_norm": 0.09827012568712234,
      "learning_rate": 8.331996792301524e-05,
      "loss": 0.5171,
      "step": 1460
    },
    {
      "epoch": 0.5846338535414166,
      "grad_norm": 0.09658502042293549,
      "learning_rate": 8.323977546110666e-05,
      "loss": 0.5101,
      "step": 1461
    },
    {
      "epoch": 0.5850340136054422,
      "grad_norm": 0.09805308282375336,
      "learning_rate": 8.315958299919808e-05,
      "loss": 0.4864,
      "step": 1462
    },
    {
      "epoch": 0.5854341736694678,
      "grad_norm": 0.10981486737728119,
      "learning_rate": 8.30793905372895e-05,
      "loss": 0.5412,
      "step": 1463
    },
    {
      "epoch": 0.5858343337334934,
      "grad_norm": 0.0930679589509964,
      "learning_rate": 8.299919807538091e-05,
      "loss": 0.4277,
      "step": 1464
    },
    {
      "epoch": 0.586234493797519,
      "grad_norm": 0.09495984762907028,
      "learning_rate": 8.291900561347233e-05,
      "loss": 0.4812,
      "step": 1465
    },
    {
      "epoch": 0.5866346538615446,
      "grad_norm": 0.10508105903863907,
      "learning_rate": 8.283881315156375e-05,
      "loss": 0.4766,
      "step": 1466
    },
    {
      "epoch": 0.5870348139255702,
      "grad_norm": 0.09136592596769333,
      "learning_rate": 8.275862068965517e-05,
      "loss": 0.4608,
      "step": 1467
    },
    {
      "epoch": 0.5874349739895959,
      "grad_norm": 0.08435924351215363,
      "learning_rate": 8.26784282277466e-05,
      "loss": 0.3834,
      "step": 1468
    },
    {
      "epoch": 0.5878351340536214,
      "grad_norm": 0.08777226507663727,
      "learning_rate": 8.259823576583802e-05,
      "loss": 0.4388,
      "step": 1469
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.0956997349858284,
      "learning_rate": 8.251804330392944e-05,
      "loss": 0.4372,
      "step": 1470
    },
    {
      "epoch": 0.5886354541816726,
      "grad_norm": 0.09644581377506256,
      "learning_rate": 8.243785084202086e-05,
      "loss": 0.4624,
      "step": 1471
    },
    {
      "epoch": 0.5890356142456983,
      "grad_norm": 0.10362788289785385,
      "learning_rate": 8.235765838011227e-05,
      "loss": 0.4865,
      "step": 1472
    },
    {
      "epoch": 0.5894357743097239,
      "grad_norm": 0.08172886818647385,
      "learning_rate": 8.227746591820369e-05,
      "loss": 0.4113,
      "step": 1473
    },
    {
      "epoch": 0.5898359343737495,
      "grad_norm": 0.13017892837524414,
      "learning_rate": 8.219727345629511e-05,
      "loss": 0.5169,
      "step": 1474
    },
    {
      "epoch": 0.5902360944377751,
      "grad_norm": 0.10733441263437271,
      "learning_rate": 8.211708099438653e-05,
      "loss": 0.508,
      "step": 1475
    },
    {
      "epoch": 0.5906362545018007,
      "grad_norm": 0.09320121258497238,
      "learning_rate": 8.203688853247795e-05,
      "loss": 0.4769,
      "step": 1476
    },
    {
      "epoch": 0.5910364145658263,
      "grad_norm": 0.11116909980773926,
      "learning_rate": 8.195669607056937e-05,
      "loss": 0.5195,
      "step": 1477
    },
    {
      "epoch": 0.5914365746298519,
      "grad_norm": 0.11942523717880249,
      "learning_rate": 8.18765036086608e-05,
      "loss": 0.4758,
      "step": 1478
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 0.10215666145086288,
      "learning_rate": 8.179631114675222e-05,
      "loss": 0.493,
      "step": 1479
    },
    {
      "epoch": 0.5922368947579032,
      "grad_norm": 0.08336465060710907,
      "learning_rate": 8.171611868484363e-05,
      "loss": 0.3919,
      "step": 1480
    },
    {
      "epoch": 0.5926370548219287,
      "grad_norm": 0.09075050055980682,
      "learning_rate": 8.163592622293505e-05,
      "loss": 0.4219,
      "step": 1481
    },
    {
      "epoch": 0.5930372148859544,
      "grad_norm": 0.09530679881572723,
      "learning_rate": 8.155573376102647e-05,
      "loss": 0.4802,
      "step": 1482
    },
    {
      "epoch": 0.59343737494998,
      "grad_norm": 0.09831620007753372,
      "learning_rate": 8.147554129911789e-05,
      "loss": 0.461,
      "step": 1483
    },
    {
      "epoch": 0.5938375350140056,
      "grad_norm": 0.08941315114498138,
      "learning_rate": 8.139534883720931e-05,
      "loss": 0.4701,
      "step": 1484
    },
    {
      "epoch": 0.5942376950780313,
      "grad_norm": 0.07848015427589417,
      "learning_rate": 8.131515637530073e-05,
      "loss": 0.3895,
      "step": 1485
    },
    {
      "epoch": 0.5946378551420568,
      "grad_norm": 0.1008671373128891,
      "learning_rate": 8.123496391339215e-05,
      "loss": 0.4512,
      "step": 1486
    },
    {
      "epoch": 0.5950380152060825,
      "grad_norm": 0.09405452013015747,
      "learning_rate": 8.115477145148358e-05,
      "loss": 0.4824,
      "step": 1487
    },
    {
      "epoch": 0.595438175270108,
      "grad_norm": 0.07562588155269623,
      "learning_rate": 8.107457898957498e-05,
      "loss": 0.3943,
      "step": 1488
    },
    {
      "epoch": 0.5958383353341337,
      "grad_norm": 0.0914849266409874,
      "learning_rate": 8.09943865276664e-05,
      "loss": 0.4952,
      "step": 1489
    },
    {
      "epoch": 0.5962384953981593,
      "grad_norm": 0.10124869644641876,
      "learning_rate": 8.091419406575783e-05,
      "loss": 0.444,
      "step": 1490
    },
    {
      "epoch": 0.5966386554621849,
      "grad_norm": 0.09856744110584259,
      "learning_rate": 8.083400160384925e-05,
      "loss": 0.4317,
      "step": 1491
    },
    {
      "epoch": 0.5970388155262105,
      "grad_norm": 0.08481252193450928,
      "learning_rate": 8.075380914194066e-05,
      "loss": 0.4681,
      "step": 1492
    },
    {
      "epoch": 0.5974389755902361,
      "grad_norm": 0.08992123603820801,
      "learning_rate": 8.067361668003208e-05,
      "loss": 0.4732,
      "step": 1493
    },
    {
      "epoch": 0.5978391356542617,
      "grad_norm": 0.1005624458193779,
      "learning_rate": 8.05934242181235e-05,
      "loss": 0.5221,
      "step": 1494
    },
    {
      "epoch": 0.5982392957182873,
      "grad_norm": 0.0952453464269638,
      "learning_rate": 8.051323175621492e-05,
      "loss": 0.484,
      "step": 1495
    },
    {
      "epoch": 0.5986394557823129,
      "grad_norm": 0.09046610444784164,
      "learning_rate": 8.043303929430633e-05,
      "loss": 0.4848,
      "step": 1496
    },
    {
      "epoch": 0.5990396158463386,
      "grad_norm": 0.0903797447681427,
      "learning_rate": 8.035284683239775e-05,
      "loss": 0.4545,
      "step": 1497
    },
    {
      "epoch": 0.5994397759103641,
      "grad_norm": 0.0950595885515213,
      "learning_rate": 8.027265437048917e-05,
      "loss": 0.4356,
      "step": 1498
    },
    {
      "epoch": 0.5998399359743898,
      "grad_norm": 0.10761325061321259,
      "learning_rate": 8.019246190858059e-05,
      "loss": 0.3972,
      "step": 1499
    },
    {
      "epoch": 0.6002400960384153,
      "grad_norm": 0.09232805669307709,
      "learning_rate": 8.011226944667201e-05,
      "loss": 0.3767,
      "step": 1500
    },
    {
      "epoch": 0.600640256102441,
      "grad_norm": 0.08397376537322998,
      "learning_rate": 8.003207698476344e-05,
      "loss": 0.403,
      "step": 1501
    },
    {
      "epoch": 0.6010404161664666,
      "grad_norm": 0.08039404451847076,
      "learning_rate": 7.995188452285486e-05,
      "loss": 0.4139,
      "step": 1502
    },
    {
      "epoch": 0.6014405762304922,
      "grad_norm": 0.09118648618459702,
      "learning_rate": 7.987169206094628e-05,
      "loss": 0.4292,
      "step": 1503
    },
    {
      "epoch": 0.6018407362945178,
      "grad_norm": 0.09273175895214081,
      "learning_rate": 7.979149959903769e-05,
      "loss": 0.4283,
      "step": 1504
    },
    {
      "epoch": 0.6022408963585434,
      "grad_norm": 0.10826399177312851,
      "learning_rate": 7.971130713712911e-05,
      "loss": 0.4386,
      "step": 1505
    },
    {
      "epoch": 0.602641056422569,
      "grad_norm": 0.08981431275606155,
      "learning_rate": 7.963111467522053e-05,
      "loss": 0.4427,
      "step": 1506
    },
    {
      "epoch": 0.6030412164865946,
      "grad_norm": 0.09053473174571991,
      "learning_rate": 7.955092221331195e-05,
      "loss": 0.4528,
      "step": 1507
    },
    {
      "epoch": 0.6034413765506202,
      "grad_norm": 0.10499104112386703,
      "learning_rate": 7.947072975140337e-05,
      "loss": 0.457,
      "step": 1508
    },
    {
      "epoch": 0.6038415366146459,
      "grad_norm": 0.09130037575960159,
      "learning_rate": 7.93905372894948e-05,
      "loss": 0.3914,
      "step": 1509
    },
    {
      "epoch": 0.6042416966786714,
      "grad_norm": 0.11219008266925812,
      "learning_rate": 7.931034482758621e-05,
      "loss": 0.512,
      "step": 1510
    },
    {
      "epoch": 0.6046418567426971,
      "grad_norm": 0.08949705958366394,
      "learning_rate": 7.923015236567764e-05,
      "loss": 0.4046,
      "step": 1511
    },
    {
      "epoch": 0.6050420168067226,
      "grad_norm": 0.11495734751224518,
      "learning_rate": 7.914995990376904e-05,
      "loss": 0.5492,
      "step": 1512
    },
    {
      "epoch": 0.6054421768707483,
      "grad_norm": 0.09983863681554794,
      "learning_rate": 7.906976744186047e-05,
      "loss": 0.4863,
      "step": 1513
    },
    {
      "epoch": 0.605842336934774,
      "grad_norm": 0.09836240857839584,
      "learning_rate": 7.898957497995189e-05,
      "loss": 0.3796,
      "step": 1514
    },
    {
      "epoch": 0.6062424969987995,
      "grad_norm": 0.10110045224428177,
      "learning_rate": 7.890938251804331e-05,
      "loss": 0.4502,
      "step": 1515
    },
    {
      "epoch": 0.6066426570628252,
      "grad_norm": 0.1068403348326683,
      "learning_rate": 7.882919005613473e-05,
      "loss": 0.4845,
      "step": 1516
    },
    {
      "epoch": 0.6070428171268507,
      "grad_norm": 0.08493093401193619,
      "learning_rate": 7.874899759422615e-05,
      "loss": 0.3706,
      "step": 1517
    },
    {
      "epoch": 0.6074429771908764,
      "grad_norm": 0.09384837746620178,
      "learning_rate": 7.866880513231757e-05,
      "loss": 0.4144,
      "step": 1518
    },
    {
      "epoch": 0.6078431372549019,
      "grad_norm": 0.09614480286836624,
      "learning_rate": 7.8588612670409e-05,
      "loss": 0.4758,
      "step": 1519
    },
    {
      "epoch": 0.6082432973189276,
      "grad_norm": 0.09315233677625656,
      "learning_rate": 7.85084202085004e-05,
      "loss": 0.4647,
      "step": 1520
    },
    {
      "epoch": 0.6086434573829532,
      "grad_norm": 0.10287734121084213,
      "learning_rate": 7.842822774659182e-05,
      "loss": 0.4576,
      "step": 1521
    },
    {
      "epoch": 0.6090436174469788,
      "grad_norm": 0.11282729357481003,
      "learning_rate": 7.834803528468325e-05,
      "loss": 0.4914,
      "step": 1522
    },
    {
      "epoch": 0.6094437775110044,
      "grad_norm": 0.08179449290037155,
      "learning_rate": 7.826784282277467e-05,
      "loss": 0.4326,
      "step": 1523
    },
    {
      "epoch": 0.60984393757503,
      "grad_norm": 0.10924109816551208,
      "learning_rate": 7.818765036086609e-05,
      "loss": 0.5305,
      "step": 1524
    },
    {
      "epoch": 0.6102440976390556,
      "grad_norm": 0.1079617366194725,
      "learning_rate": 7.810745789895751e-05,
      "loss": 0.4251,
      "step": 1525
    },
    {
      "epoch": 0.6106442577030813,
      "grad_norm": 0.10141873359680176,
      "learning_rate": 7.802726543704892e-05,
      "loss": 0.4013,
      "step": 1526
    },
    {
      "epoch": 0.6110444177671068,
      "grad_norm": 0.09130421280860901,
      "learning_rate": 7.794707297514034e-05,
      "loss": 0.4848,
      "step": 1527
    },
    {
      "epoch": 0.6114445778311325,
      "grad_norm": 0.10360829532146454,
      "learning_rate": 7.786688051323176e-05,
      "loss": 0.4409,
      "step": 1528
    },
    {
      "epoch": 0.611844737895158,
      "grad_norm": 0.09546978771686554,
      "learning_rate": 7.778668805132318e-05,
      "loss": 0.432,
      "step": 1529
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 0.08511387556791306,
      "learning_rate": 7.770649558941459e-05,
      "loss": 0.4437,
      "step": 1530
    },
    {
      "epoch": 0.6126450580232092,
      "grad_norm": 0.07896820455789566,
      "learning_rate": 7.762630312750601e-05,
      "loss": 0.3717,
      "step": 1531
    },
    {
      "epoch": 0.6130452180872349,
      "grad_norm": 0.07635410875082016,
      "learning_rate": 7.754611066559743e-05,
      "loss": 0.4144,
      "step": 1532
    },
    {
      "epoch": 0.6134453781512605,
      "grad_norm": 0.1003580167889595,
      "learning_rate": 7.746591820368885e-05,
      "loss": 0.4633,
      "step": 1533
    },
    {
      "epoch": 0.6138455382152861,
      "grad_norm": 0.08999363332986832,
      "learning_rate": 7.738572574178028e-05,
      "loss": 0.4065,
      "step": 1534
    },
    {
      "epoch": 0.6142456982793117,
      "grad_norm": 0.07110165059566498,
      "learning_rate": 7.73055332798717e-05,
      "loss": 0.3194,
      "step": 1535
    },
    {
      "epoch": 0.6146458583433373,
      "grad_norm": 0.11893170326948166,
      "learning_rate": 7.72253408179631e-05,
      "loss": 0.6004,
      "step": 1536
    },
    {
      "epoch": 0.615046018407363,
      "grad_norm": 0.08332990109920502,
      "learning_rate": 7.714514835605453e-05,
      "loss": 0.4614,
      "step": 1537
    },
    {
      "epoch": 0.6154461784713886,
      "grad_norm": 0.10133935511112213,
      "learning_rate": 7.706495589414595e-05,
      "loss": 0.4253,
      "step": 1538
    },
    {
      "epoch": 0.6158463385354142,
      "grad_norm": 0.1101047620177269,
      "learning_rate": 7.698476343223737e-05,
      "loss": 0.4638,
      "step": 1539
    },
    {
      "epoch": 0.6162464985994398,
      "grad_norm": 0.09594506025314331,
      "learning_rate": 7.690457097032879e-05,
      "loss": 0.464,
      "step": 1540
    },
    {
      "epoch": 0.6166466586634654,
      "grad_norm": 0.09424664825201035,
      "learning_rate": 7.682437850842021e-05,
      "loss": 0.4675,
      "step": 1541
    },
    {
      "epoch": 0.617046818727491,
      "grad_norm": 0.08359847217798233,
      "learning_rate": 7.674418604651163e-05,
      "loss": 0.4417,
      "step": 1542
    },
    {
      "epoch": 0.6174469787915166,
      "grad_norm": 0.11008752882480621,
      "learning_rate": 7.666399358460306e-05,
      "loss": 0.5052,
      "step": 1543
    },
    {
      "epoch": 0.6178471388555422,
      "grad_norm": 0.10789318382740021,
      "learning_rate": 7.658380112269446e-05,
      "loss": 0.5234,
      "step": 1544
    },
    {
      "epoch": 0.6182472989195679,
      "grad_norm": 0.0990828424692154,
      "learning_rate": 7.650360866078588e-05,
      "loss": 0.4432,
      "step": 1545
    },
    {
      "epoch": 0.6186474589835934,
      "grad_norm": 0.11590643227100372,
      "learning_rate": 7.64234161988773e-05,
      "loss": 0.5662,
      "step": 1546
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.093072809278965,
      "learning_rate": 7.634322373696873e-05,
      "loss": 0.4998,
      "step": 1547
    },
    {
      "epoch": 0.6194477791116446,
      "grad_norm": 0.09906575828790665,
      "learning_rate": 7.626303127506015e-05,
      "loss": 0.5369,
      "step": 1548
    },
    {
      "epoch": 0.6198479391756703,
      "grad_norm": 0.10355152189731598,
      "learning_rate": 7.618283881315157e-05,
      "loss": 0.4265,
      "step": 1549
    },
    {
      "epoch": 0.6202480992396959,
      "grad_norm": 0.07689967006444931,
      "learning_rate": 7.610264635124299e-05,
      "loss": 0.4806,
      "step": 1550
    },
    {
      "epoch": 0.6206482593037215,
      "grad_norm": 0.10133414715528488,
      "learning_rate": 7.602245388933441e-05,
      "loss": 0.4394,
      "step": 1551
    },
    {
      "epoch": 0.6210484193677471,
      "grad_norm": 0.08055134117603302,
      "learning_rate": 7.594226142742582e-05,
      "loss": 0.428,
      "step": 1552
    },
    {
      "epoch": 0.6214485794317727,
      "grad_norm": 0.07991435378789902,
      "learning_rate": 7.586206896551724e-05,
      "loss": 0.4231,
      "step": 1553
    },
    {
      "epoch": 0.6218487394957983,
      "grad_norm": 0.09140610694885254,
      "learning_rate": 7.578187650360866e-05,
      "loss": 0.4115,
      "step": 1554
    },
    {
      "epoch": 0.6222488995598239,
      "grad_norm": 0.09730977565050125,
      "learning_rate": 7.570168404170009e-05,
      "loss": 0.4504,
      "step": 1555
    },
    {
      "epoch": 0.6226490596238495,
      "grad_norm": 0.10829445719718933,
      "learning_rate": 7.562149157979151e-05,
      "loss": 0.4413,
      "step": 1556
    },
    {
      "epoch": 0.6230492196878752,
      "grad_norm": 0.09633602201938629,
      "learning_rate": 7.554129911788293e-05,
      "loss": 0.4538,
      "step": 1557
    },
    {
      "epoch": 0.6234493797519007,
      "grad_norm": 0.09913451224565506,
      "learning_rate": 7.546110665597435e-05,
      "loss": 0.3591,
      "step": 1558
    },
    {
      "epoch": 0.6238495398159264,
      "grad_norm": 0.09540776163339615,
      "learning_rate": 7.538091419406577e-05,
      "loss": 0.4414,
      "step": 1559
    },
    {
      "epoch": 0.6242496998799519,
      "grad_norm": 0.08845537155866623,
      "learning_rate": 7.530072173215718e-05,
      "loss": 0.4933,
      "step": 1560
    },
    {
      "epoch": 0.6246498599439776,
      "grad_norm": 0.0922662690281868,
      "learning_rate": 7.52205292702486e-05,
      "loss": 0.4311,
      "step": 1561
    },
    {
      "epoch": 0.6250500200080032,
      "grad_norm": 0.07905222475528717,
      "learning_rate": 7.514033680834002e-05,
      "loss": 0.42,
      "step": 1562
    },
    {
      "epoch": 0.6254501800720288,
      "grad_norm": 0.10486150532960892,
      "learning_rate": 7.506014434643144e-05,
      "loss": 0.4209,
      "step": 1563
    },
    {
      "epoch": 0.6258503401360545,
      "grad_norm": 0.08707479387521744,
      "learning_rate": 7.497995188452285e-05,
      "loss": 0.5237,
      "step": 1564
    },
    {
      "epoch": 0.62625050020008,
      "grad_norm": 0.09958942234516144,
      "learning_rate": 7.489975942261427e-05,
      "loss": 0.4885,
      "step": 1565
    },
    {
      "epoch": 0.6266506602641057,
      "grad_norm": 0.10663225501775742,
      "learning_rate": 7.48195669607057e-05,
      "loss": 0.5246,
      "step": 1566
    },
    {
      "epoch": 0.6270508203281312,
      "grad_norm": 0.08033891022205353,
      "learning_rate": 7.473937449879712e-05,
      "loss": 0.4224,
      "step": 1567
    },
    {
      "epoch": 0.6274509803921569,
      "grad_norm": 0.08989396691322327,
      "learning_rate": 7.465918203688852e-05,
      "loss": 0.4633,
      "step": 1568
    },
    {
      "epoch": 0.6278511404561825,
      "grad_norm": 0.09560436755418777,
      "learning_rate": 7.457898957497995e-05,
      "loss": 0.4433,
      "step": 1569
    },
    {
      "epoch": 0.6282513005202081,
      "grad_norm": 0.11007288098335266,
      "learning_rate": 7.449879711307137e-05,
      "loss": 0.5484,
      "step": 1570
    },
    {
      "epoch": 0.6286514605842337,
      "grad_norm": 0.07988498359918594,
      "learning_rate": 7.441860465116279e-05,
      "loss": 0.4197,
      "step": 1571
    },
    {
      "epoch": 0.6290516206482593,
      "grad_norm": 0.0988219603896141,
      "learning_rate": 7.433841218925421e-05,
      "loss": 0.5115,
      "step": 1572
    },
    {
      "epoch": 0.6294517807122849,
      "grad_norm": 0.09046261757612228,
      "learning_rate": 7.425821972734563e-05,
      "loss": 0.4519,
      "step": 1573
    },
    {
      "epoch": 0.6298519407763106,
      "grad_norm": 0.07160439342260361,
      "learning_rate": 7.417802726543705e-05,
      "loss": 0.3947,
      "step": 1574
    },
    {
      "epoch": 0.6302521008403361,
      "grad_norm": 0.09272312372922897,
      "learning_rate": 7.409783480352847e-05,
      "loss": 0.4789,
      "step": 1575
    },
    {
      "epoch": 0.6306522609043618,
      "grad_norm": 0.09856297075748444,
      "learning_rate": 7.401764234161988e-05,
      "loss": 0.5055,
      "step": 1576
    },
    {
      "epoch": 0.6310524209683873,
      "grad_norm": 0.09055957198143005,
      "learning_rate": 7.39374498797113e-05,
      "loss": 0.4933,
      "step": 1577
    },
    {
      "epoch": 0.631452581032413,
      "grad_norm": 0.10697062313556671,
      "learning_rate": 7.385725741780273e-05,
      "loss": 0.5291,
      "step": 1578
    },
    {
      "epoch": 0.6318527410964386,
      "grad_norm": 0.10475084185600281,
      "learning_rate": 7.377706495589415e-05,
      "loss": 0.5066,
      "step": 1579
    },
    {
      "epoch": 0.6322529011604642,
      "grad_norm": 0.09397928416728973,
      "learning_rate": 7.369687249398557e-05,
      "loss": 0.5226,
      "step": 1580
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 0.11093656718730927,
      "learning_rate": 7.361668003207699e-05,
      "loss": 0.5862,
      "step": 1581
    },
    {
      "epoch": 0.6330532212885154,
      "grad_norm": 0.0952422097325325,
      "learning_rate": 7.353648757016841e-05,
      "loss": 0.3755,
      "step": 1582
    },
    {
      "epoch": 0.633453381352541,
      "grad_norm": 0.1057000607252121,
      "learning_rate": 7.345629510825983e-05,
      "loss": 0.4419,
      "step": 1583
    },
    {
      "epoch": 0.6338535414165666,
      "grad_norm": 0.10126806795597076,
      "learning_rate": 7.337610264635124e-05,
      "loss": 0.3995,
      "step": 1584
    },
    {
      "epoch": 0.6342537014805922,
      "grad_norm": 0.09945087879896164,
      "learning_rate": 7.329591018444266e-05,
      "loss": 0.4923,
      "step": 1585
    },
    {
      "epoch": 0.6346538615446179,
      "grad_norm": 0.11359933763742447,
      "learning_rate": 7.321571772253408e-05,
      "loss": 0.5046,
      "step": 1586
    },
    {
      "epoch": 0.6350540216086434,
      "grad_norm": 0.09767630696296692,
      "learning_rate": 7.31355252606255e-05,
      "loss": 0.4803,
      "step": 1587
    },
    {
      "epoch": 0.6354541816726691,
      "grad_norm": 0.09801146388053894,
      "learning_rate": 7.305533279871693e-05,
      "loss": 0.4982,
      "step": 1588
    },
    {
      "epoch": 0.6358543417366946,
      "grad_norm": 0.09487155079841614,
      "learning_rate": 7.297514033680835e-05,
      "loss": 0.4964,
      "step": 1589
    },
    {
      "epoch": 0.6362545018007203,
      "grad_norm": 0.08960910141468048,
      "learning_rate": 7.289494787489977e-05,
      "loss": 0.4741,
      "step": 1590
    },
    {
      "epoch": 0.636654661864746,
      "grad_norm": 0.09014817327260971,
      "learning_rate": 7.281475541299119e-05,
      "loss": 0.4707,
      "step": 1591
    },
    {
      "epoch": 0.6370548219287715,
      "grad_norm": 0.09937448799610138,
      "learning_rate": 7.27345629510826e-05,
      "loss": 0.4453,
      "step": 1592
    },
    {
      "epoch": 0.6374549819927972,
      "grad_norm": 0.10212158411741257,
      "learning_rate": 7.265437048917402e-05,
      "loss": 0.4397,
      "step": 1593
    },
    {
      "epoch": 0.6378551420568227,
      "grad_norm": 0.0746571272611618,
      "learning_rate": 7.257417802726544e-05,
      "loss": 0.4066,
      "step": 1594
    },
    {
      "epoch": 0.6382553021208484,
      "grad_norm": 0.09124435484409332,
      "learning_rate": 7.249398556535686e-05,
      "loss": 0.4131,
      "step": 1595
    },
    {
      "epoch": 0.6386554621848739,
      "grad_norm": 0.1065841093659401,
      "learning_rate": 7.241379310344828e-05,
      "loss": 0.504,
      "step": 1596
    },
    {
      "epoch": 0.6390556222488996,
      "grad_norm": 0.09528651088476181,
      "learning_rate": 7.23336006415397e-05,
      "loss": 0.4686,
      "step": 1597
    },
    {
      "epoch": 0.6394557823129252,
      "grad_norm": 0.09063774347305298,
      "learning_rate": 7.225340817963113e-05,
      "loss": 0.4093,
      "step": 1598
    },
    {
      "epoch": 0.6398559423769508,
      "grad_norm": 0.1012401208281517,
      "learning_rate": 7.217321571772254e-05,
      "loss": 0.5418,
      "step": 1599
    },
    {
      "epoch": 0.6402561024409764,
      "grad_norm": 0.10104255378246307,
      "learning_rate": 7.209302325581396e-05,
      "loss": 0.4807,
      "step": 1600
    },
    {
      "epoch": 0.640656262505002,
      "grad_norm": 0.08584481477737427,
      "learning_rate": 7.201283079390538e-05,
      "loss": 0.4587,
      "step": 1601
    },
    {
      "epoch": 0.6410564225690276,
      "grad_norm": 0.08533907681703568,
      "learning_rate": 7.193263833199679e-05,
      "loss": 0.4818,
      "step": 1602
    },
    {
      "epoch": 0.6414565826330533,
      "grad_norm": 0.08455231785774231,
      "learning_rate": 7.185244587008821e-05,
      "loss": 0.4482,
      "step": 1603
    },
    {
      "epoch": 0.6418567426970788,
      "grad_norm": 0.10101599246263504,
      "learning_rate": 7.177225340817963e-05,
      "loss": 0.4603,
      "step": 1604
    },
    {
      "epoch": 0.6422569027611045,
      "grad_norm": 0.08437757194042206,
      "learning_rate": 7.169206094627105e-05,
      "loss": 0.3442,
      "step": 1605
    },
    {
      "epoch": 0.64265706282513,
      "grad_norm": 0.1420544683933258,
      "learning_rate": 7.161186848436247e-05,
      "loss": 0.4223,
      "step": 1606
    },
    {
      "epoch": 0.6430572228891557,
      "grad_norm": 0.08786524832248688,
      "learning_rate": 7.15316760224539e-05,
      "loss": 0.4847,
      "step": 1607
    },
    {
      "epoch": 0.6434573829531812,
      "grad_norm": 0.08123292028903961,
      "learning_rate": 7.14514835605453e-05,
      "loss": 0.4507,
      "step": 1608
    },
    {
      "epoch": 0.6438575430172069,
      "grad_norm": 0.08587247878313065,
      "learning_rate": 7.137129109863672e-05,
      "loss": 0.4438,
      "step": 1609
    },
    {
      "epoch": 0.6442577030812325,
      "grad_norm": 0.08313431590795517,
      "learning_rate": 7.129109863672814e-05,
      "loss": 0.4422,
      "step": 1610
    },
    {
      "epoch": 0.6446578631452581,
      "grad_norm": 0.12283353507518768,
      "learning_rate": 7.121090617481957e-05,
      "loss": 0.5243,
      "step": 1611
    },
    {
      "epoch": 0.6450580232092837,
      "grad_norm": 0.10903307795524597,
      "learning_rate": 7.113071371291099e-05,
      "loss": 0.4519,
      "step": 1612
    },
    {
      "epoch": 0.6454581832733093,
      "grad_norm": 0.08974403142929077,
      "learning_rate": 7.105052125100241e-05,
      "loss": 0.4964,
      "step": 1613
    },
    {
      "epoch": 0.6458583433373349,
      "grad_norm": 0.0829300582408905,
      "learning_rate": 7.097032878909383e-05,
      "loss": 0.4511,
      "step": 1614
    },
    {
      "epoch": 0.6462585034013606,
      "grad_norm": 0.09399339556694031,
      "learning_rate": 7.089013632718525e-05,
      "loss": 0.5025,
      "step": 1615
    },
    {
      "epoch": 0.6466586634653861,
      "grad_norm": 0.11711759120225906,
      "learning_rate": 7.080994386527666e-05,
      "loss": 0.4914,
      "step": 1616
    },
    {
      "epoch": 0.6470588235294118,
      "grad_norm": 0.0873970165848732,
      "learning_rate": 7.072975140336808e-05,
      "loss": 0.5026,
      "step": 1617
    },
    {
      "epoch": 0.6474589835934373,
      "grad_norm": 0.09344804286956787,
      "learning_rate": 7.06495589414595e-05,
      "loss": 0.4205,
      "step": 1618
    },
    {
      "epoch": 0.647859143657463,
      "grad_norm": 0.09183362126350403,
      "learning_rate": 7.056936647955092e-05,
      "loss": 0.4537,
      "step": 1619
    },
    {
      "epoch": 0.6482593037214885,
      "grad_norm": 0.08434618264436722,
      "learning_rate": 7.048917401764235e-05,
      "loss": 0.4535,
      "step": 1620
    },
    {
      "epoch": 0.6486594637855142,
      "grad_norm": 0.09536410868167877,
      "learning_rate": 7.040898155573377e-05,
      "loss": 0.3723,
      "step": 1621
    },
    {
      "epoch": 0.6490596238495399,
      "grad_norm": 0.09768786281347275,
      "learning_rate": 7.032878909382519e-05,
      "loss": 0.486,
      "step": 1622
    },
    {
      "epoch": 0.6494597839135654,
      "grad_norm": 0.09882649034261703,
      "learning_rate": 7.024859663191661e-05,
      "loss": 0.5561,
      "step": 1623
    },
    {
      "epoch": 0.6498599439775911,
      "grad_norm": 0.06502481549978256,
      "learning_rate": 7.016840417000802e-05,
      "loss": 0.3618,
      "step": 1624
    },
    {
      "epoch": 0.6502601040416166,
      "grad_norm": 0.09232743829488754,
      "learning_rate": 7.008821170809944e-05,
      "loss": 0.4424,
      "step": 1625
    },
    {
      "epoch": 0.6506602641056423,
      "grad_norm": 0.1128447949886322,
      "learning_rate": 7.000801924619086e-05,
      "loss": 0.5385,
      "step": 1626
    },
    {
      "epoch": 0.6510604241696679,
      "grad_norm": 0.10022638738155365,
      "learning_rate": 6.992782678428228e-05,
      "loss": 0.4261,
      "step": 1627
    },
    {
      "epoch": 0.6514605842336935,
      "grad_norm": 0.10185672342777252,
      "learning_rate": 6.98476343223737e-05,
      "loss": 0.4313,
      "step": 1628
    },
    {
      "epoch": 0.6518607442977191,
      "grad_norm": 0.08713079988956451,
      "learning_rate": 6.976744186046513e-05,
      "loss": 0.4449,
      "step": 1629
    },
    {
      "epoch": 0.6522609043617447,
      "grad_norm": 0.10459496080875397,
      "learning_rate": 6.968724939855655e-05,
      "loss": 0.5523,
      "step": 1630
    },
    {
      "epoch": 0.6526610644257703,
      "grad_norm": 0.09861193597316742,
      "learning_rate": 6.960705693664797e-05,
      "loss": 0.4941,
      "step": 1631
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 0.09605596214532852,
      "learning_rate": 6.952686447473938e-05,
      "loss": 0.515,
      "step": 1632
    },
    {
      "epoch": 0.6534613845538215,
      "grad_norm": 0.08597969263792038,
      "learning_rate": 6.94466720128308e-05,
      "loss": 0.4504,
      "step": 1633
    },
    {
      "epoch": 0.6538615446178472,
      "grad_norm": 0.07997982203960419,
      "learning_rate": 6.936647955092222e-05,
      "loss": 0.3812,
      "step": 1634
    },
    {
      "epoch": 0.6542617046818727,
      "grad_norm": 0.11325766891241074,
      "learning_rate": 6.928628708901364e-05,
      "loss": 0.4756,
      "step": 1635
    },
    {
      "epoch": 0.6546618647458984,
      "grad_norm": 0.10642606765031815,
      "learning_rate": 6.920609462710506e-05,
      "loss": 0.4512,
      "step": 1636
    },
    {
      "epoch": 0.6550620248099239,
      "grad_norm": 0.07088098675012589,
      "learning_rate": 6.912590216519647e-05,
      "loss": 0.3798,
      "step": 1637
    },
    {
      "epoch": 0.6554621848739496,
      "grad_norm": 0.0925675705075264,
      "learning_rate": 6.904570970328789e-05,
      "loss": 0.4876,
      "step": 1638
    },
    {
      "epoch": 0.6558623449379752,
      "grad_norm": 0.08884979784488678,
      "learning_rate": 6.896551724137931e-05,
      "loss": 0.3906,
      "step": 1639
    },
    {
      "epoch": 0.6562625050020008,
      "grad_norm": 0.09598593413829803,
      "learning_rate": 6.888532477947073e-05,
      "loss": 0.4568,
      "step": 1640
    },
    {
      "epoch": 0.6566626650660264,
      "grad_norm": 0.08760569989681244,
      "learning_rate": 6.880513231756214e-05,
      "loss": 0.4701,
      "step": 1641
    },
    {
      "epoch": 0.657062825130052,
      "grad_norm": 0.09072168916463852,
      "learning_rate": 6.872493985565356e-05,
      "loss": 0.386,
      "step": 1642
    },
    {
      "epoch": 0.6574629851940776,
      "grad_norm": 0.10051611065864563,
      "learning_rate": 6.864474739374499e-05,
      "loss": 0.4675,
      "step": 1643
    },
    {
      "epoch": 0.6578631452581032,
      "grad_norm": 0.10648149996995926,
      "learning_rate": 6.856455493183641e-05,
      "loss": 0.4857,
      "step": 1644
    },
    {
      "epoch": 0.6582633053221288,
      "grad_norm": 0.08849401772022247,
      "learning_rate": 6.848436246992783e-05,
      "loss": 0.4574,
      "step": 1645
    },
    {
      "epoch": 0.6586634653861545,
      "grad_norm": 0.08503343164920807,
      "learning_rate": 6.840417000801925e-05,
      "loss": 0.4617,
      "step": 1646
    },
    {
      "epoch": 0.65906362545018,
      "grad_norm": 0.10000544041395187,
      "learning_rate": 6.832397754611067e-05,
      "loss": 0.4791,
      "step": 1647
    },
    {
      "epoch": 0.6594637855142057,
      "grad_norm": 0.10688068717718124,
      "learning_rate": 6.824378508420208e-05,
      "loss": 0.4851,
      "step": 1648
    },
    {
      "epoch": 0.6598639455782312,
      "grad_norm": 0.10354425013065338,
      "learning_rate": 6.81635926222935e-05,
      "loss": 0.4548,
      "step": 1649
    },
    {
      "epoch": 0.6602641056422569,
      "grad_norm": 0.10380133241415024,
      "learning_rate": 6.808340016038492e-05,
      "loss": 0.4909,
      "step": 1650
    },
    {
      "epoch": 0.6606642657062826,
      "grad_norm": 0.08930250257253647,
      "learning_rate": 6.800320769847634e-05,
      "loss": 0.3703,
      "step": 1651
    },
    {
      "epoch": 0.6610644257703081,
      "grad_norm": 0.11772067099809647,
      "learning_rate": 6.792301523656777e-05,
      "loss": 0.6061,
      "step": 1652
    },
    {
      "epoch": 0.6614645858343338,
      "grad_norm": 0.1008109301328659,
      "learning_rate": 6.784282277465919e-05,
      "loss": 0.4825,
      "step": 1653
    },
    {
      "epoch": 0.6618647458983593,
      "grad_norm": 0.08405763655900955,
      "learning_rate": 6.776263031275061e-05,
      "loss": 0.4526,
      "step": 1654
    },
    {
      "epoch": 0.662264905962385,
      "grad_norm": 0.08459007740020752,
      "learning_rate": 6.768243785084203e-05,
      "loss": 0.4511,
      "step": 1655
    },
    {
      "epoch": 0.6626650660264105,
      "grad_norm": 0.12086694687604904,
      "learning_rate": 6.760224538893344e-05,
      "loss": 0.495,
      "step": 1656
    },
    {
      "epoch": 0.6630652260904362,
      "grad_norm": 0.09311782568693161,
      "learning_rate": 6.752205292702486e-05,
      "loss": 0.4063,
      "step": 1657
    },
    {
      "epoch": 0.6634653861544618,
      "grad_norm": 0.07603108137845993,
      "learning_rate": 6.744186046511628e-05,
      "loss": 0.4111,
      "step": 1658
    },
    {
      "epoch": 0.6638655462184874,
      "grad_norm": 0.09470438957214355,
      "learning_rate": 6.73616680032077e-05,
      "loss": 0.4956,
      "step": 1659
    },
    {
      "epoch": 0.664265706282513,
      "grad_norm": 0.09610463678836823,
      "learning_rate": 6.728147554129912e-05,
      "loss": 0.451,
      "step": 1660
    },
    {
      "epoch": 0.6646658663465386,
      "grad_norm": 0.11495799571275711,
      "learning_rate": 6.720128307939054e-05,
      "loss": 0.5217,
      "step": 1661
    },
    {
      "epoch": 0.6650660264105642,
      "grad_norm": 0.08619161695241928,
      "learning_rate": 6.712109061748197e-05,
      "loss": 0.4627,
      "step": 1662
    },
    {
      "epoch": 0.6654661864745899,
      "grad_norm": 0.07578571140766144,
      "learning_rate": 6.704089815557339e-05,
      "loss": 0.3602,
      "step": 1663
    },
    {
      "epoch": 0.6658663465386154,
      "grad_norm": 0.10765708982944489,
      "learning_rate": 6.69607056936648e-05,
      "loss": 0.5067,
      "step": 1664
    },
    {
      "epoch": 0.6662665066026411,
      "grad_norm": 0.09059885889291763,
      "learning_rate": 6.688051323175622e-05,
      "loss": 0.4621,
      "step": 1665
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.08856280148029327,
      "learning_rate": 6.680032076984764e-05,
      "loss": 0.4847,
      "step": 1666
    },
    {
      "epoch": 0.6670668267306923,
      "grad_norm": 0.08964621275663376,
      "learning_rate": 6.672012830793906e-05,
      "loss": 0.436,
      "step": 1667
    },
    {
      "epoch": 0.6674669867947179,
      "grad_norm": 0.10116934031248093,
      "learning_rate": 6.663993584603048e-05,
      "loss": 0.5093,
      "step": 1668
    },
    {
      "epoch": 0.6678671468587435,
      "grad_norm": 0.08608013391494751,
      "learning_rate": 6.65597433841219e-05,
      "loss": 0.4557,
      "step": 1669
    },
    {
      "epoch": 0.6682673069227691,
      "grad_norm": 0.0832476019859314,
      "learning_rate": 6.647955092221332e-05,
      "loss": 0.3975,
      "step": 1670
    },
    {
      "epoch": 0.6686674669867947,
      "grad_norm": 0.08481474965810776,
      "learning_rate": 6.639935846030475e-05,
      "loss": 0.4142,
      "step": 1671
    },
    {
      "epoch": 0.6690676270508203,
      "grad_norm": 0.09833536297082901,
      "learning_rate": 6.631916599839615e-05,
      "loss": 0.5237,
      "step": 1672
    },
    {
      "epoch": 0.6694677871148459,
      "grad_norm": 0.10572939366102219,
      "learning_rate": 6.623897353648758e-05,
      "loss": 0.432,
      "step": 1673
    },
    {
      "epoch": 0.6698679471788715,
      "grad_norm": 0.08044665306806564,
      "learning_rate": 6.6158781074579e-05,
      "loss": 0.3402,
      "step": 1674
    },
    {
      "epoch": 0.6702681072428972,
      "grad_norm": 0.09487546235322952,
      "learning_rate": 6.60785886126704e-05,
      "loss": 0.4419,
      "step": 1675
    },
    {
      "epoch": 0.6706682673069227,
      "grad_norm": 0.10131967067718506,
      "learning_rate": 6.599839615076183e-05,
      "loss": 0.4977,
      "step": 1676
    },
    {
      "epoch": 0.6710684273709484,
      "grad_norm": 0.10632254928350449,
      "learning_rate": 6.591820368885325e-05,
      "loss": 0.5201,
      "step": 1677
    },
    {
      "epoch": 0.671468587434974,
      "grad_norm": 0.08249625563621521,
      "learning_rate": 6.583801122694467e-05,
      "loss": 0.4358,
      "step": 1678
    },
    {
      "epoch": 0.6718687474989996,
      "grad_norm": 0.10157369822263718,
      "learning_rate": 6.575781876503609e-05,
      "loss": 0.534,
      "step": 1679
    },
    {
      "epoch": 0.6722689075630253,
      "grad_norm": 0.10066180676221848,
      "learning_rate": 6.56776263031275e-05,
      "loss": 0.5093,
      "step": 1680
    },
    {
      "epoch": 0.6726690676270508,
      "grad_norm": 0.09234119206666946,
      "learning_rate": 6.559743384121892e-05,
      "loss": 0.4169,
      "step": 1681
    },
    {
      "epoch": 0.6730692276910765,
      "grad_norm": 0.09491248428821564,
      "learning_rate": 6.551724137931034e-05,
      "loss": 0.4544,
      "step": 1682
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 0.08412507176399231,
      "learning_rate": 6.543704891740176e-05,
      "loss": 0.444,
      "step": 1683
    },
    {
      "epoch": 0.6738695478191277,
      "grad_norm": 0.10194941610097885,
      "learning_rate": 6.535685645549318e-05,
      "loss": 0.5895,
      "step": 1684
    },
    {
      "epoch": 0.6742697078831532,
      "grad_norm": 0.1067633107304573,
      "learning_rate": 6.52766639935846e-05,
      "loss": 0.509,
      "step": 1685
    },
    {
      "epoch": 0.6746698679471789,
      "grad_norm": 0.08279143273830414,
      "learning_rate": 6.519647153167603e-05,
      "loss": 0.3805,
      "step": 1686
    },
    {
      "epoch": 0.6750700280112045,
      "grad_norm": 0.1029934510588646,
      "learning_rate": 6.511627906976745e-05,
      "loss": 0.4744,
      "step": 1687
    },
    {
      "epoch": 0.6754701880752301,
      "grad_norm": 0.08469559252262115,
      "learning_rate": 6.503608660785886e-05,
      "loss": 0.4407,
      "step": 1688
    },
    {
      "epoch": 0.6758703481392557,
      "grad_norm": 0.09705282747745514,
      "learning_rate": 6.495589414595028e-05,
      "loss": 0.4918,
      "step": 1689
    },
    {
      "epoch": 0.6762705082032813,
      "grad_norm": 0.08866050094366074,
      "learning_rate": 6.48757016840417e-05,
      "loss": 0.4255,
      "step": 1690
    },
    {
      "epoch": 0.6766706682673069,
      "grad_norm": 0.09953572601079941,
      "learning_rate": 6.479550922213312e-05,
      "loss": 0.5169,
      "step": 1691
    },
    {
      "epoch": 0.6770708283313326,
      "grad_norm": 0.09873667359352112,
      "learning_rate": 6.471531676022454e-05,
      "loss": 0.5096,
      "step": 1692
    },
    {
      "epoch": 0.6774709883953581,
      "grad_norm": 0.09444113075733185,
      "learning_rate": 6.463512429831596e-05,
      "loss": 0.4745,
      "step": 1693
    },
    {
      "epoch": 0.6778711484593838,
      "grad_norm": 0.09075658768415451,
      "learning_rate": 6.455493183640739e-05,
      "loss": 0.4425,
      "step": 1694
    },
    {
      "epoch": 0.6782713085234093,
      "grad_norm": 0.09908711910247803,
      "learning_rate": 6.447473937449881e-05,
      "loss": 0.4759,
      "step": 1695
    },
    {
      "epoch": 0.678671468587435,
      "grad_norm": 0.10400489717721939,
      "learning_rate": 6.439454691259021e-05,
      "loss": 0.5311,
      "step": 1696
    },
    {
      "epoch": 0.6790716286514605,
      "grad_norm": 0.07881160825490952,
      "learning_rate": 6.431435445068164e-05,
      "loss": 0.4738,
      "step": 1697
    },
    {
      "epoch": 0.6794717887154862,
      "grad_norm": 0.10935453325510025,
      "learning_rate": 6.423416198877306e-05,
      "loss": 0.5464,
      "step": 1698
    },
    {
      "epoch": 0.6798719487795118,
      "grad_norm": 0.09338100999593735,
      "learning_rate": 6.415396952686448e-05,
      "loss": 0.4451,
      "step": 1699
    },
    {
      "epoch": 0.6802721088435374,
      "grad_norm": 0.08947993069887161,
      "learning_rate": 6.40737770649559e-05,
      "loss": 0.4708,
      "step": 1700
    },
    {
      "epoch": 0.680672268907563,
      "grad_norm": 0.08804546296596527,
      "learning_rate": 6.399358460304732e-05,
      "loss": 0.3965,
      "step": 1701
    },
    {
      "epoch": 0.6810724289715886,
      "grad_norm": 0.09956992417573929,
      "learning_rate": 6.391339214113874e-05,
      "loss": 0.4606,
      "step": 1702
    },
    {
      "epoch": 0.6814725890356143,
      "grad_norm": 0.10326087474822998,
      "learning_rate": 6.383319967923017e-05,
      "loss": 0.5942,
      "step": 1703
    },
    {
      "epoch": 0.6818727490996399,
      "grad_norm": 0.07930471003055573,
      "learning_rate": 6.375300721732157e-05,
      "loss": 0.3902,
      "step": 1704
    },
    {
      "epoch": 0.6822729091636655,
      "grad_norm": 0.08077254891395569,
      "learning_rate": 6.3672814755413e-05,
      "loss": 0.4579,
      "step": 1705
    },
    {
      "epoch": 0.6826730692276911,
      "grad_norm": 0.09773712605237961,
      "learning_rate": 6.359262229350442e-05,
      "loss": 0.4855,
      "step": 1706
    },
    {
      "epoch": 0.6830732292917167,
      "grad_norm": 0.09688672423362732,
      "learning_rate": 6.351242983159584e-05,
      "loss": 0.4537,
      "step": 1707
    },
    {
      "epoch": 0.6834733893557423,
      "grad_norm": 0.08491608500480652,
      "learning_rate": 6.343223736968726e-05,
      "loss": 0.3675,
      "step": 1708
    },
    {
      "epoch": 0.6838735494197679,
      "grad_norm": 0.09067057818174362,
      "learning_rate": 6.335204490777868e-05,
      "loss": 0.4429,
      "step": 1709
    },
    {
      "epoch": 0.6842737094837935,
      "grad_norm": 0.11012502014636993,
      "learning_rate": 6.327185244587009e-05,
      "loss": 0.5336,
      "step": 1710
    },
    {
      "epoch": 0.6846738695478192,
      "grad_norm": 0.07875946909189224,
      "learning_rate": 6.319165998396151e-05,
      "loss": 0.3829,
      "step": 1711
    },
    {
      "epoch": 0.6850740296118447,
      "grad_norm": 0.10605867207050323,
      "learning_rate": 6.311146752205293e-05,
      "loss": 0.4874,
      "step": 1712
    },
    {
      "epoch": 0.6854741896758704,
      "grad_norm": 0.08994528651237488,
      "learning_rate": 6.303127506014434e-05,
      "loss": 0.4226,
      "step": 1713
    },
    {
      "epoch": 0.6858743497398959,
      "grad_norm": 0.09326179325580597,
      "learning_rate": 6.295108259823576e-05,
      "loss": 0.5051,
      "step": 1714
    },
    {
      "epoch": 0.6862745098039216,
      "grad_norm": 0.08721382915973663,
      "learning_rate": 6.287089013632718e-05,
      "loss": 0.4358,
      "step": 1715
    },
    {
      "epoch": 0.6866746698679472,
      "grad_norm": 0.1362324208021164,
      "learning_rate": 6.27906976744186e-05,
      "loss": 0.4973,
      "step": 1716
    },
    {
      "epoch": 0.6870748299319728,
      "grad_norm": 0.14078781008720398,
      "learning_rate": 6.271050521251003e-05,
      "loss": 0.4574,
      "step": 1717
    },
    {
      "epoch": 0.6874749899959984,
      "grad_norm": 0.08300276100635529,
      "learning_rate": 6.263031275060145e-05,
      "loss": 0.4661,
      "step": 1718
    },
    {
      "epoch": 0.687875150060024,
      "grad_norm": 0.07588561624288559,
      "learning_rate": 6.255012028869287e-05,
      "loss": 0.4101,
      "step": 1719
    },
    {
      "epoch": 0.6882753101240496,
      "grad_norm": 0.09752386808395386,
      "learning_rate": 6.246992782678428e-05,
      "loss": 0.4483,
      "step": 1720
    },
    {
      "epoch": 0.6886754701880752,
      "grad_norm": 0.11213736981153488,
      "learning_rate": 6.23897353648757e-05,
      "loss": 0.4815,
      "step": 1721
    },
    {
      "epoch": 0.6890756302521008,
      "grad_norm": 0.08859740942716599,
      "learning_rate": 6.230954290296712e-05,
      "loss": 0.4852,
      "step": 1722
    },
    {
      "epoch": 0.6894757903161265,
      "grad_norm": 0.09864768385887146,
      "learning_rate": 6.222935044105854e-05,
      "loss": 0.4799,
      "step": 1723
    },
    {
      "epoch": 0.689875950380152,
      "grad_norm": 0.09497899562120438,
      "learning_rate": 6.214915797914996e-05,
      "loss": 0.4466,
      "step": 1724
    },
    {
      "epoch": 0.6902761104441777,
      "grad_norm": 0.07448089867830276,
      "learning_rate": 6.206896551724138e-05,
      "loss": 0.3414,
      "step": 1725
    },
    {
      "epoch": 0.6906762705082032,
      "grad_norm": 0.11583319306373596,
      "learning_rate": 6.19887730553328e-05,
      "loss": 0.3966,
      "step": 1726
    },
    {
      "epoch": 0.6910764305722289,
      "grad_norm": 0.09274005144834518,
      "learning_rate": 6.190858059342423e-05,
      "loss": 0.4712,
      "step": 1727
    },
    {
      "epoch": 0.6914765906362546,
      "grad_norm": 0.09861522167921066,
      "learning_rate": 6.182838813151563e-05,
      "loss": 0.48,
      "step": 1728
    },
    {
      "epoch": 0.6918767507002801,
      "grad_norm": 0.09074477851390839,
      "learning_rate": 6.174819566960706e-05,
      "loss": 0.4171,
      "step": 1729
    },
    {
      "epoch": 0.6922769107643058,
      "grad_norm": 0.11409597843885422,
      "learning_rate": 6.166800320769848e-05,
      "loss": 0.5635,
      "step": 1730
    },
    {
      "epoch": 0.6926770708283313,
      "grad_norm": 0.10441668331623077,
      "learning_rate": 6.15878107457899e-05,
      "loss": 0.4807,
      "step": 1731
    },
    {
      "epoch": 0.693077230892357,
      "grad_norm": 0.09385158866643906,
      "learning_rate": 6.150761828388132e-05,
      "loss": 0.4178,
      "step": 1732
    },
    {
      "epoch": 0.6934773909563825,
      "grad_norm": 0.07511308044195175,
      "learning_rate": 6.142742582197274e-05,
      "loss": 0.4032,
      "step": 1733
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 0.10360389202833176,
      "learning_rate": 6.134723336006416e-05,
      "loss": 0.4664,
      "step": 1734
    },
    {
      "epoch": 0.6942777110844338,
      "grad_norm": 0.09422662109136581,
      "learning_rate": 6.126704089815558e-05,
      "loss": 0.487,
      "step": 1735
    },
    {
      "epoch": 0.6946778711484594,
      "grad_norm": 0.09135615080595016,
      "learning_rate": 6.118684843624699e-05,
      "loss": 0.4479,
      "step": 1736
    },
    {
      "epoch": 0.695078031212485,
      "grad_norm": 0.12185682356357574,
      "learning_rate": 6.110665597433841e-05,
      "loss": 0.5141,
      "step": 1737
    },
    {
      "epoch": 0.6954781912765106,
      "grad_norm": 0.08012803643941879,
      "learning_rate": 6.1026463512429835e-05,
      "loss": 0.3512,
      "step": 1738
    },
    {
      "epoch": 0.6958783513405362,
      "grad_norm": 0.10030112415552139,
      "learning_rate": 6.094627105052125e-05,
      "loss": 0.512,
      "step": 1739
    },
    {
      "epoch": 0.6962785114045619,
      "grad_norm": 0.09268786013126373,
      "learning_rate": 6.086607858861267e-05,
      "loss": 0.4215,
      "step": 1740
    },
    {
      "epoch": 0.6966786714685874,
      "grad_norm": 0.0861307829618454,
      "learning_rate": 6.078588612670409e-05,
      "loss": 0.4531,
      "step": 1741
    },
    {
      "epoch": 0.6970788315326131,
      "grad_norm": 0.09213358908891678,
      "learning_rate": 6.0705693664795514e-05,
      "loss": 0.4661,
      "step": 1742
    },
    {
      "epoch": 0.6974789915966386,
      "grad_norm": 0.10847621411085129,
      "learning_rate": 6.0625501202886936e-05,
      "loss": 0.5108,
      "step": 1743
    },
    {
      "epoch": 0.6978791516606643,
      "grad_norm": 0.09719639271497726,
      "learning_rate": 6.0545308740978344e-05,
      "loss": 0.4413,
      "step": 1744
    },
    {
      "epoch": 0.6982793117246898,
      "grad_norm": 0.10190671682357788,
      "learning_rate": 6.0465116279069765e-05,
      "loss": 0.4437,
      "step": 1745
    },
    {
      "epoch": 0.6986794717887155,
      "grad_norm": 0.07504241168498993,
      "learning_rate": 6.0384923817161187e-05,
      "loss": 0.3886,
      "step": 1746
    },
    {
      "epoch": 0.6990796318527411,
      "grad_norm": 0.08574007451534271,
      "learning_rate": 6.030473135525261e-05,
      "loss": 0.4532,
      "step": 1747
    },
    {
      "epoch": 0.6994797919167667,
      "grad_norm": 0.08209647983312607,
      "learning_rate": 6.022453889334403e-05,
      "loss": 0.4048,
      "step": 1748
    },
    {
      "epoch": 0.6998799519807923,
      "grad_norm": 0.09856390953063965,
      "learning_rate": 6.014434643143545e-05,
      "loss": 0.4814,
      "step": 1749
    },
    {
      "epoch": 0.7002801120448179,
      "grad_norm": 0.13951516151428223,
      "learning_rate": 6.006415396952687e-05,
      "loss": 0.5375,
      "step": 1750
    },
    {
      "epoch": 0.7006802721088435,
      "grad_norm": 0.10019529610872269,
      "learning_rate": 5.9983961507618294e-05,
      "loss": 0.4665,
      "step": 1751
    },
    {
      "epoch": 0.7010804321728692,
      "grad_norm": 0.07653411477804184,
      "learning_rate": 5.99037690457097e-05,
      "loss": 0.3987,
      "step": 1752
    },
    {
      "epoch": 0.7014805922368947,
      "grad_norm": 0.09163090586662292,
      "learning_rate": 5.982357658380112e-05,
      "loss": 0.4528,
      "step": 1753
    },
    {
      "epoch": 0.7018807523009204,
      "grad_norm": 0.11078517884016037,
      "learning_rate": 5.9743384121892545e-05,
      "loss": 0.5017,
      "step": 1754
    },
    {
      "epoch": 0.7022809123649459,
      "grad_norm": 0.0932239517569542,
      "learning_rate": 5.9663191659983966e-05,
      "loss": 0.3847,
      "step": 1755
    },
    {
      "epoch": 0.7026810724289716,
      "grad_norm": 0.0872136726975441,
      "learning_rate": 5.958299919807538e-05,
      "loss": 0.3957,
      "step": 1756
    },
    {
      "epoch": 0.7030812324929971,
      "grad_norm": 0.12492025643587112,
      "learning_rate": 5.95028067361668e-05,
      "loss": 0.4945,
      "step": 1757
    },
    {
      "epoch": 0.7034813925570228,
      "grad_norm": 0.08812065422534943,
      "learning_rate": 5.9422614274258224e-05,
      "loss": 0.4021,
      "step": 1758
    },
    {
      "epoch": 0.7038815526210485,
      "grad_norm": 0.09306441992521286,
      "learning_rate": 5.9342421812349645e-05,
      "loss": 0.4189,
      "step": 1759
    },
    {
      "epoch": 0.704281712685074,
      "grad_norm": 0.10259267687797546,
      "learning_rate": 5.926222935044105e-05,
      "loss": 0.5096,
      "step": 1760
    },
    {
      "epoch": 0.7046818727490997,
      "grad_norm": 0.10848097503185272,
      "learning_rate": 5.9182036888532475e-05,
      "loss": 0.4984,
      "step": 1761
    },
    {
      "epoch": 0.7050820328131252,
      "grad_norm": 0.06951668858528137,
      "learning_rate": 5.9101844426623896e-05,
      "loss": 0.3267,
      "step": 1762
    },
    {
      "epoch": 0.7054821928771509,
      "grad_norm": 0.07767124474048615,
      "learning_rate": 5.902165196471532e-05,
      "loss": 0.3998,
      "step": 1763
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.1043190136551857,
      "learning_rate": 5.894145950280674e-05,
      "loss": 0.5521,
      "step": 1764
    },
    {
      "epoch": 0.7062825130052021,
      "grad_norm": 0.0949595719575882,
      "learning_rate": 5.886126704089816e-05,
      "loss": 0.4655,
      "step": 1765
    },
    {
      "epoch": 0.7066826730692277,
      "grad_norm": 0.10621224343776703,
      "learning_rate": 5.878107457898958e-05,
      "loss": 0.4742,
      "step": 1766
    },
    {
      "epoch": 0.7070828331332533,
      "grad_norm": 0.10880033671855927,
      "learning_rate": 5.8700882117081004e-05,
      "loss": 0.5162,
      "step": 1767
    },
    {
      "epoch": 0.7074829931972789,
      "grad_norm": 0.08163837343454361,
      "learning_rate": 5.862068965517241e-05,
      "loss": 0.3698,
      "step": 1768
    },
    {
      "epoch": 0.7078831532613046,
      "grad_norm": 0.1018977239727974,
      "learning_rate": 5.854049719326383e-05,
      "loss": 0.5401,
      "step": 1769
    },
    {
      "epoch": 0.7082833133253301,
      "grad_norm": 0.08670952916145325,
      "learning_rate": 5.8460304731355254e-05,
      "loss": 0.4418,
      "step": 1770
    },
    {
      "epoch": 0.7086834733893558,
      "grad_norm": 0.10257134586572647,
      "learning_rate": 5.8380112269446676e-05,
      "loss": 0.4756,
      "step": 1771
    },
    {
      "epoch": 0.7090836334533813,
      "grad_norm": 0.09866853803396225,
      "learning_rate": 5.82999198075381e-05,
      "loss": 0.4475,
      "step": 1772
    },
    {
      "epoch": 0.709483793517407,
      "grad_norm": 0.11088234186172485,
      "learning_rate": 5.821972734562952e-05,
      "loss": 0.4826,
      "step": 1773
    },
    {
      "epoch": 0.7098839535814325,
      "grad_norm": 0.09525793045759201,
      "learning_rate": 5.8139534883720933e-05,
      "loss": 0.4535,
      "step": 1774
    },
    {
      "epoch": 0.7102841136454582,
      "grad_norm": 0.0976431742310524,
      "learning_rate": 5.8059342421812355e-05,
      "loss": 0.4765,
      "step": 1775
    },
    {
      "epoch": 0.7106842737094838,
      "grad_norm": 0.11050764471292496,
      "learning_rate": 5.797914995990377e-05,
      "loss": 0.481,
      "step": 1776
    },
    {
      "epoch": 0.7110844337735094,
      "grad_norm": 0.10532745718955994,
      "learning_rate": 5.7898957497995184e-05,
      "loss": 0.3925,
      "step": 1777
    },
    {
      "epoch": 0.711484593837535,
      "grad_norm": 0.09398999065160751,
      "learning_rate": 5.7818765036086606e-05,
      "loss": 0.4481,
      "step": 1778
    },
    {
      "epoch": 0.7118847539015606,
      "grad_norm": 0.08435992896556854,
      "learning_rate": 5.773857257417803e-05,
      "loss": 0.4156,
      "step": 1779
    },
    {
      "epoch": 0.7122849139655862,
      "grad_norm": 0.10432165861129761,
      "learning_rate": 5.765838011226945e-05,
      "loss": 0.3999,
      "step": 1780
    },
    {
      "epoch": 0.7126850740296119,
      "grad_norm": 0.10315001010894775,
      "learning_rate": 5.757818765036087e-05,
      "loss": 0.5099,
      "step": 1781
    },
    {
      "epoch": 0.7130852340936374,
      "grad_norm": 0.08416556566953659,
      "learning_rate": 5.749799518845229e-05,
      "loss": 0.4498,
      "step": 1782
    },
    {
      "epoch": 0.7134853941576631,
      "grad_norm": 0.10351750999689102,
      "learning_rate": 5.741780272654371e-05,
      "loss": 0.52,
      "step": 1783
    },
    {
      "epoch": 0.7138855542216886,
      "grad_norm": 0.08908351510763168,
      "learning_rate": 5.733761026463512e-05,
      "loss": 0.4592,
      "step": 1784
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.09338049590587616,
      "learning_rate": 5.725741780272654e-05,
      "loss": 0.4141,
      "step": 1785
    },
    {
      "epoch": 0.7146858743497398,
      "grad_norm": 0.09664034843444824,
      "learning_rate": 5.7177225340817964e-05,
      "loss": 0.4888,
      "step": 1786
    },
    {
      "epoch": 0.7150860344137655,
      "grad_norm": 0.09884978830814362,
      "learning_rate": 5.7097032878909385e-05,
      "loss": 0.4815,
      "step": 1787
    },
    {
      "epoch": 0.7154861944777912,
      "grad_norm": 0.10309471935033798,
      "learning_rate": 5.701684041700081e-05,
      "loss": 0.4575,
      "step": 1788
    },
    {
      "epoch": 0.7158863545418167,
      "grad_norm": 0.08359106630086899,
      "learning_rate": 5.693664795509223e-05,
      "loss": 0.3827,
      "step": 1789
    },
    {
      "epoch": 0.7162865146058424,
      "grad_norm": 0.0947619080543518,
      "learning_rate": 5.685645549318365e-05,
      "loss": 0.4967,
      "step": 1790
    },
    {
      "epoch": 0.7166866746698679,
      "grad_norm": 0.09524118900299072,
      "learning_rate": 5.6776263031275065e-05,
      "loss": 0.4102,
      "step": 1791
    },
    {
      "epoch": 0.7170868347338936,
      "grad_norm": 0.07709728181362152,
      "learning_rate": 5.669607056936648e-05,
      "loss": 0.4018,
      "step": 1792
    },
    {
      "epoch": 0.7174869947979192,
      "grad_norm": 0.0920797809958458,
      "learning_rate": 5.66158781074579e-05,
      "loss": 0.3717,
      "step": 1793
    },
    {
      "epoch": 0.7178871548619448,
      "grad_norm": 0.10057340562343597,
      "learning_rate": 5.653568564554932e-05,
      "loss": 0.4513,
      "step": 1794
    },
    {
      "epoch": 0.7182873149259704,
      "grad_norm": 0.10295984894037247,
      "learning_rate": 5.645549318364074e-05,
      "loss": 0.4728,
      "step": 1795
    },
    {
      "epoch": 0.718687474989996,
      "grad_norm": 0.11428675800561905,
      "learning_rate": 5.637530072173216e-05,
      "loss": 0.4592,
      "step": 1796
    },
    {
      "epoch": 0.7190876350540216,
      "grad_norm": 0.09536296129226685,
      "learning_rate": 5.629510825982358e-05,
      "loss": 0.4365,
      "step": 1797
    },
    {
      "epoch": 0.7194877951180472,
      "grad_norm": 0.10213350504636765,
      "learning_rate": 5.6214915797915e-05,
      "loss": 0.4671,
      "step": 1798
    },
    {
      "epoch": 0.7198879551820728,
      "grad_norm": 0.08528649061918259,
      "learning_rate": 5.613472333600642e-05,
      "loss": 0.4149,
      "step": 1799
    },
    {
      "epoch": 0.7202881152460985,
      "grad_norm": 0.08344094455242157,
      "learning_rate": 5.605453087409783e-05,
      "loss": 0.4738,
      "step": 1800
    },
    {
      "epoch": 0.720688275310124,
      "grad_norm": 0.09531949460506439,
      "learning_rate": 5.597433841218925e-05,
      "loss": 0.4846,
      "step": 1801
    },
    {
      "epoch": 0.7210884353741497,
      "grad_norm": 0.10097714513540268,
      "learning_rate": 5.5894145950280674e-05,
      "loss": 0.5146,
      "step": 1802
    },
    {
      "epoch": 0.7214885954381752,
      "grad_norm": 0.08706950396299362,
      "learning_rate": 5.5813953488372095e-05,
      "loss": 0.4536,
      "step": 1803
    },
    {
      "epoch": 0.7218887555022009,
      "grad_norm": 0.08587762713432312,
      "learning_rate": 5.5733761026463517e-05,
      "loss": 0.4121,
      "step": 1804
    },
    {
      "epoch": 0.7222889155662265,
      "grad_norm": 0.09505595266819,
      "learning_rate": 5.565356856455494e-05,
      "loss": 0.4417,
      "step": 1805
    },
    {
      "epoch": 0.7226890756302521,
      "grad_norm": 0.09053917229175568,
      "learning_rate": 5.557337610264636e-05,
      "loss": 0.3989,
      "step": 1806
    },
    {
      "epoch": 0.7230892356942777,
      "grad_norm": 0.08651704341173172,
      "learning_rate": 5.549318364073778e-05,
      "loss": 0.453,
      "step": 1807
    },
    {
      "epoch": 0.7234893957583033,
      "grad_norm": 0.10634373873472214,
      "learning_rate": 5.541299117882919e-05,
      "loss": 0.483,
      "step": 1808
    },
    {
      "epoch": 0.723889555822329,
      "grad_norm": 0.11002514511346817,
      "learning_rate": 5.533279871692061e-05,
      "loss": 0.528,
      "step": 1809
    },
    {
      "epoch": 0.7242897158863545,
      "grad_norm": 0.09464747458696365,
      "learning_rate": 5.525260625501203e-05,
      "loss": 0.4745,
      "step": 1810
    },
    {
      "epoch": 0.7246898759503801,
      "grad_norm": 0.11490446329116821,
      "learning_rate": 5.517241379310345e-05,
      "loss": 0.5555,
      "step": 1811
    },
    {
      "epoch": 0.7250900360144058,
      "grad_norm": 0.07946891337633133,
      "learning_rate": 5.509222133119487e-05,
      "loss": 0.3856,
      "step": 1812
    },
    {
      "epoch": 0.7254901960784313,
      "grad_norm": 0.09287279844284058,
      "learning_rate": 5.501202886928629e-05,
      "loss": 0.4324,
      "step": 1813
    },
    {
      "epoch": 0.725890356142457,
      "grad_norm": 0.09258216619491577,
      "learning_rate": 5.493183640737771e-05,
      "loss": 0.4688,
      "step": 1814
    },
    {
      "epoch": 0.7262905162064826,
      "grad_norm": 0.08318067342042923,
      "learning_rate": 5.485164394546913e-05,
      "loss": 0.4294,
      "step": 1815
    },
    {
      "epoch": 0.7266906762705082,
      "grad_norm": 0.09813552349805832,
      "learning_rate": 5.477145148356054e-05,
      "loss": 0.5099,
      "step": 1816
    },
    {
      "epoch": 0.7270908363345339,
      "grad_norm": 0.11078527569770813,
      "learning_rate": 5.469125902165196e-05,
      "loss": 0.502,
      "step": 1817
    },
    {
      "epoch": 0.7274909963985594,
      "grad_norm": 0.09844609349966049,
      "learning_rate": 5.461106655974338e-05,
      "loss": 0.4943,
      "step": 1818
    },
    {
      "epoch": 0.7278911564625851,
      "grad_norm": 0.10460347682237625,
      "learning_rate": 5.4530874097834805e-05,
      "loss": 0.4409,
      "step": 1819
    },
    {
      "epoch": 0.7282913165266106,
      "grad_norm": 0.08206338435411453,
      "learning_rate": 5.4450681635926226e-05,
      "loss": 0.4457,
      "step": 1820
    },
    {
      "epoch": 0.7286914765906363,
      "grad_norm": 0.08404327929019928,
      "learning_rate": 5.437048917401765e-05,
      "loss": 0.3869,
      "step": 1821
    },
    {
      "epoch": 0.7290916366546618,
      "grad_norm": 0.09530226141214371,
      "learning_rate": 5.429029671210907e-05,
      "loss": 0.4556,
      "step": 1822
    },
    {
      "epoch": 0.7294917967186875,
      "grad_norm": 0.08576106280088425,
      "learning_rate": 5.421010425020049e-05,
      "loss": 0.3875,
      "step": 1823
    },
    {
      "epoch": 0.7298919567827131,
      "grad_norm": 0.09007736295461655,
      "learning_rate": 5.41299117882919e-05,
      "loss": 0.3942,
      "step": 1824
    },
    {
      "epoch": 0.7302921168467387,
      "grad_norm": 0.0954076424241066,
      "learning_rate": 5.404971932638332e-05,
      "loss": 0.4524,
      "step": 1825
    },
    {
      "epoch": 0.7306922769107643,
      "grad_norm": 0.09009493142366409,
      "learning_rate": 5.396952686447474e-05,
      "loss": 0.4575,
      "step": 1826
    },
    {
      "epoch": 0.7310924369747899,
      "grad_norm": 0.09642116725444794,
      "learning_rate": 5.388933440256616e-05,
      "loss": 0.4484,
      "step": 1827
    },
    {
      "epoch": 0.7314925970388155,
      "grad_norm": 0.10025447607040405,
      "learning_rate": 5.3809141940657584e-05,
      "loss": 0.4974,
      "step": 1828
    },
    {
      "epoch": 0.7318927571028412,
      "grad_norm": 0.10917023569345474,
      "learning_rate": 5.3728949478749e-05,
      "loss": 0.5122,
      "step": 1829
    },
    {
      "epoch": 0.7322929171668667,
      "grad_norm": 0.08391834795475006,
      "learning_rate": 5.364875701684042e-05,
      "loss": 0.4054,
      "step": 1830
    },
    {
      "epoch": 0.7326930772308924,
      "grad_norm": 0.09917700290679932,
      "learning_rate": 5.356856455493184e-05,
      "loss": 0.4712,
      "step": 1831
    },
    {
      "epoch": 0.7330932372949179,
      "grad_norm": 0.09184026718139648,
      "learning_rate": 5.348837209302326e-05,
      "loss": 0.4112,
      "step": 1832
    },
    {
      "epoch": 0.7334933973589436,
      "grad_norm": 0.09901433438062668,
      "learning_rate": 5.340817963111467e-05,
      "loss": 0.5255,
      "step": 1833
    },
    {
      "epoch": 0.7338935574229691,
      "grad_norm": 0.11524026095867157,
      "learning_rate": 5.332798716920609e-05,
      "loss": 0.5335,
      "step": 1834
    },
    {
      "epoch": 0.7342937174869948,
      "grad_norm": 0.08933906257152557,
      "learning_rate": 5.3247794707297514e-05,
      "loss": 0.455,
      "step": 1835
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 0.07366573810577393,
      "learning_rate": 5.3167602245388936e-05,
      "loss": 0.3862,
      "step": 1836
    },
    {
      "epoch": 0.735094037615046,
      "grad_norm": 0.10052178800106049,
      "learning_rate": 5.308740978348036e-05,
      "loss": 0.4073,
      "step": 1837
    },
    {
      "epoch": 0.7354941976790716,
      "grad_norm": 0.10653365403413773,
      "learning_rate": 5.300721732157178e-05,
      "loss": 0.5484,
      "step": 1838
    },
    {
      "epoch": 0.7358943577430972,
      "grad_norm": 0.0926133543252945,
      "learning_rate": 5.29270248596632e-05,
      "loss": 0.4241,
      "step": 1839
    },
    {
      "epoch": 0.7362945178071229,
      "grad_norm": 0.08966054022312164,
      "learning_rate": 5.284683239775461e-05,
      "loss": 0.4597,
      "step": 1840
    },
    {
      "epoch": 0.7366946778711485,
      "grad_norm": 0.08848331868648529,
      "learning_rate": 5.276663993584603e-05,
      "loss": 0.4595,
      "step": 1841
    },
    {
      "epoch": 0.737094837935174,
      "grad_norm": 0.08106957376003265,
      "learning_rate": 5.268644747393745e-05,
      "loss": 0.4445,
      "step": 1842
    },
    {
      "epoch": 0.7374949979991997,
      "grad_norm": 0.10242576152086258,
      "learning_rate": 5.260625501202887e-05,
      "loss": 0.4856,
      "step": 1843
    },
    {
      "epoch": 0.7378951580632253,
      "grad_norm": 0.0945504829287529,
      "learning_rate": 5.2526062550120294e-05,
      "loss": 0.3902,
      "step": 1844
    },
    {
      "epoch": 0.7382953181272509,
      "grad_norm": 0.12190410494804382,
      "learning_rate": 5.2445870088211715e-05,
      "loss": 0.4652,
      "step": 1845
    },
    {
      "epoch": 0.7386954781912765,
      "grad_norm": 0.09449099004268646,
      "learning_rate": 5.236567762630313e-05,
      "loss": 0.4413,
      "step": 1846
    },
    {
      "epoch": 0.7390956382553021,
      "grad_norm": 0.08331531286239624,
      "learning_rate": 5.228548516439455e-05,
      "loss": 0.4228,
      "step": 1847
    },
    {
      "epoch": 0.7394957983193278,
      "grad_norm": 0.08203490823507309,
      "learning_rate": 5.2205292702485966e-05,
      "loss": 0.3988,
      "step": 1848
    },
    {
      "epoch": 0.7398959583833533,
      "grad_norm": 0.09589104354381561,
      "learning_rate": 5.212510024057739e-05,
      "loss": 0.4748,
      "step": 1849
    },
    {
      "epoch": 0.740296118447379,
      "grad_norm": 0.09208422154188156,
      "learning_rate": 5.20449077786688e-05,
      "loss": 0.4389,
      "step": 1850
    },
    {
      "epoch": 0.7406962785114045,
      "grad_norm": 0.0861496552824974,
      "learning_rate": 5.1964715316760224e-05,
      "loss": 0.3493,
      "step": 1851
    },
    {
      "epoch": 0.7410964385754302,
      "grad_norm": 0.0809701532125473,
      "learning_rate": 5.1884522854851645e-05,
      "loss": 0.3579,
      "step": 1852
    },
    {
      "epoch": 0.7414965986394558,
      "grad_norm": 0.0887753963470459,
      "learning_rate": 5.180433039294307e-05,
      "loss": 0.4541,
      "step": 1853
    },
    {
      "epoch": 0.7418967587034814,
      "grad_norm": 0.07858260720968246,
      "learning_rate": 5.172413793103449e-05,
      "loss": 0.3815,
      "step": 1854
    },
    {
      "epoch": 0.742296918767507,
      "grad_norm": 0.08795666694641113,
      "learning_rate": 5.164394546912591e-05,
      "loss": 0.4931,
      "step": 1855
    },
    {
      "epoch": 0.7426970788315326,
      "grad_norm": 0.08821457624435425,
      "learning_rate": 5.156375300721732e-05,
      "loss": 0.4435,
      "step": 1856
    },
    {
      "epoch": 0.7430972388955582,
      "grad_norm": 0.11692610383033752,
      "learning_rate": 5.148356054530874e-05,
      "loss": 0.5092,
      "step": 1857
    },
    {
      "epoch": 0.7434973989595839,
      "grad_norm": 0.09980563074350357,
      "learning_rate": 5.140336808340016e-05,
      "loss": 0.4143,
      "step": 1858
    },
    {
      "epoch": 0.7438975590236094,
      "grad_norm": 0.09048918634653091,
      "learning_rate": 5.132317562149158e-05,
      "loss": 0.4652,
      "step": 1859
    },
    {
      "epoch": 0.7442977190876351,
      "grad_norm": 0.08574050664901733,
      "learning_rate": 5.1242983159583004e-05,
      "loss": 0.4075,
      "step": 1860
    },
    {
      "epoch": 0.7446978791516606,
      "grad_norm": 0.07356761395931244,
      "learning_rate": 5.1162790697674425e-05,
      "loss": 0.4596,
      "step": 1861
    },
    {
      "epoch": 0.7450980392156863,
      "grad_norm": 0.10722853988409042,
      "learning_rate": 5.1082598235765846e-05,
      "loss": 0.5076,
      "step": 1862
    },
    {
      "epoch": 0.7454981992797118,
      "grad_norm": 0.09220220148563385,
      "learning_rate": 5.100240577385727e-05,
      "loss": 0.4734,
      "step": 1863
    },
    {
      "epoch": 0.7458983593437375,
      "grad_norm": 0.08690039813518524,
      "learning_rate": 5.0922213311948676e-05,
      "loss": 0.4531,
      "step": 1864
    },
    {
      "epoch": 0.7462985194077632,
      "grad_norm": 0.06886638700962067,
      "learning_rate": 5.08420208500401e-05,
      "loss": 0.3803,
      "step": 1865
    },
    {
      "epoch": 0.7466986794717887,
      "grad_norm": 0.09316528588533401,
      "learning_rate": 5.076182838813152e-05,
      "loss": 0.4997,
      "step": 1866
    },
    {
      "epoch": 0.7470988395358144,
      "grad_norm": 0.10011114180088043,
      "learning_rate": 5.0681635926222933e-05,
      "loss": 0.4888,
      "step": 1867
    },
    {
      "epoch": 0.7474989995998399,
      "grad_norm": 0.1086992621421814,
      "learning_rate": 5.0601443464314355e-05,
      "loss": 0.469,
      "step": 1868
    },
    {
      "epoch": 0.7478991596638656,
      "grad_norm": 0.09459409862756729,
      "learning_rate": 5.0521251002405776e-05,
      "loss": 0.4344,
      "step": 1869
    },
    {
      "epoch": 0.7482993197278912,
      "grad_norm": 0.10673585534095764,
      "learning_rate": 5.04410585404972e-05,
      "loss": 0.4962,
      "step": 1870
    },
    {
      "epoch": 0.7486994797919168,
      "grad_norm": 0.07567637413740158,
      "learning_rate": 5.036086607858862e-05,
      "loss": 0.3563,
      "step": 1871
    },
    {
      "epoch": 0.7490996398559424,
      "grad_norm": 0.09449189156293869,
      "learning_rate": 5.028067361668003e-05,
      "loss": 0.4478,
      "step": 1872
    },
    {
      "epoch": 0.749499799919968,
      "grad_norm": 0.10223741084337234,
      "learning_rate": 5.020048115477145e-05,
      "loss": 0.4459,
      "step": 1873
    },
    {
      "epoch": 0.7498999599839936,
      "grad_norm": 0.10871109366416931,
      "learning_rate": 5.012028869286287e-05,
      "loss": 0.4674,
      "step": 1874
    },
    {
      "epoch": 0.7503001200480192,
      "grad_norm": 0.08198776096105576,
      "learning_rate": 5.004009623095429e-05,
      "loss": 0.4021,
      "step": 1875
    },
    {
      "epoch": 0.7507002801120448,
      "grad_norm": 0.1041976734995842,
      "learning_rate": 4.995990376904571e-05,
      "loss": 0.4794,
      "step": 1876
    },
    {
      "epoch": 0.7511004401760705,
      "grad_norm": 0.10073122382164001,
      "learning_rate": 4.9879711307137135e-05,
      "loss": 0.4018,
      "step": 1877
    },
    {
      "epoch": 0.751500600240096,
      "grad_norm": 0.10525158047676086,
      "learning_rate": 4.979951884522855e-05,
      "loss": 0.488,
      "step": 1878
    },
    {
      "epoch": 0.7519007603041217,
      "grad_norm": 0.09709255397319794,
      "learning_rate": 4.971932638331997e-05,
      "loss": 0.4245,
      "step": 1879
    },
    {
      "epoch": 0.7523009203681472,
      "grad_norm": 0.10764552652835846,
      "learning_rate": 4.963913392141139e-05,
      "loss": 0.5053,
      "step": 1880
    },
    {
      "epoch": 0.7527010804321729,
      "grad_norm": 0.0940561443567276,
      "learning_rate": 4.9558941459502814e-05,
      "loss": 0.4192,
      "step": 1881
    },
    {
      "epoch": 0.7531012404961985,
      "grad_norm": 0.10016981512308121,
      "learning_rate": 4.947874899759423e-05,
      "loss": 0.4462,
      "step": 1882
    },
    {
      "epoch": 0.7535014005602241,
      "grad_norm": 0.09566881507635117,
      "learning_rate": 4.939855653568565e-05,
      "loss": 0.451,
      "step": 1883
    },
    {
      "epoch": 0.7539015606242497,
      "grad_norm": 0.08756941556930542,
      "learning_rate": 4.931836407377707e-05,
      "loss": 0.482,
      "step": 1884
    },
    {
      "epoch": 0.7543017206882753,
      "grad_norm": 0.09813668578863144,
      "learning_rate": 4.9238171611868486e-05,
      "loss": 0.4815,
      "step": 1885
    },
    {
      "epoch": 0.7547018807523009,
      "grad_norm": 0.10414516925811768,
      "learning_rate": 4.91579791499599e-05,
      "loss": 0.4587,
      "step": 1886
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 0.11286807805299759,
      "learning_rate": 4.907778668805132e-05,
      "loss": 0.3556,
      "step": 1887
    },
    {
      "epoch": 0.7555022008803521,
      "grad_norm": 0.09240863472223282,
      "learning_rate": 4.8997594226142744e-05,
      "loss": 0.4931,
      "step": 1888
    },
    {
      "epoch": 0.7559023609443778,
      "grad_norm": 0.0934312492609024,
      "learning_rate": 4.8917401764234165e-05,
      "loss": 0.5245,
      "step": 1889
    },
    {
      "epoch": 0.7563025210084033,
      "grad_norm": 0.09185314178466797,
      "learning_rate": 4.883720930232558e-05,
      "loss": 0.4806,
      "step": 1890
    },
    {
      "epoch": 0.756702681072429,
      "grad_norm": 0.09363170713186264,
      "learning_rate": 4.8757016840417e-05,
      "loss": 0.4027,
      "step": 1891
    },
    {
      "epoch": 0.7571028411364545,
      "grad_norm": 0.10818943381309509,
      "learning_rate": 4.867682437850842e-05,
      "loss": 0.5155,
      "step": 1892
    },
    {
      "epoch": 0.7575030012004802,
      "grad_norm": 0.10254091769456863,
      "learning_rate": 4.8596631916599844e-05,
      "loss": 0.4779,
      "step": 1893
    },
    {
      "epoch": 0.7579031612645059,
      "grad_norm": 0.09831252694129944,
      "learning_rate": 4.851643945469126e-05,
      "loss": 0.4274,
      "step": 1894
    },
    {
      "epoch": 0.7583033213285314,
      "grad_norm": 0.08510337024927139,
      "learning_rate": 4.843624699278268e-05,
      "loss": 0.4106,
      "step": 1895
    },
    {
      "epoch": 0.758703481392557,
      "grad_norm": 0.09707077592611313,
      "learning_rate": 4.83560545308741e-05,
      "loss": 0.4807,
      "step": 1896
    },
    {
      "epoch": 0.7591036414565826,
      "grad_norm": 0.09691819548606873,
      "learning_rate": 4.827586206896552e-05,
      "loss": 0.5287,
      "step": 1897
    },
    {
      "epoch": 0.7595038015206083,
      "grad_norm": 0.07659832388162613,
      "learning_rate": 4.819566960705694e-05,
      "loss": 0.4114,
      "step": 1898
    },
    {
      "epoch": 0.7599039615846338,
      "grad_norm": 0.10258391499519348,
      "learning_rate": 4.811547714514836e-05,
      "loss": 0.4654,
      "step": 1899
    },
    {
      "epoch": 0.7603041216486595,
      "grad_norm": 0.09700511395931244,
      "learning_rate": 4.803528468323978e-05,
      "loss": 0.5547,
      "step": 1900
    },
    {
      "epoch": 0.7607042817126851,
      "grad_norm": 0.10386975854635239,
      "learning_rate": 4.79550922213312e-05,
      "loss": 0.4453,
      "step": 1901
    },
    {
      "epoch": 0.7611044417767107,
      "grad_norm": 0.0974382609128952,
      "learning_rate": 4.787489975942262e-05,
      "loss": 0.5114,
      "step": 1902
    },
    {
      "epoch": 0.7615046018407363,
      "grad_norm": 0.0954550951719284,
      "learning_rate": 4.779470729751404e-05,
      "loss": 0.449,
      "step": 1903
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.10022837668657303,
      "learning_rate": 4.771451483560545e-05,
      "loss": 0.4526,
      "step": 1904
    },
    {
      "epoch": 0.7623049219687875,
      "grad_norm": 0.0985601618885994,
      "learning_rate": 4.7634322373696875e-05,
      "loss": 0.566,
      "step": 1905
    },
    {
      "epoch": 0.7627050820328132,
      "grad_norm": 0.09601498395204544,
      "learning_rate": 4.755412991178829e-05,
      "loss": 0.4053,
      "step": 1906
    },
    {
      "epoch": 0.7631052420968387,
      "grad_norm": 0.08517031371593475,
      "learning_rate": 4.747393744987971e-05,
      "loss": 0.4856,
      "step": 1907
    },
    {
      "epoch": 0.7635054021608644,
      "grad_norm": 0.08347728848457336,
      "learning_rate": 4.739374498797113e-05,
      "loss": 0.4545,
      "step": 1908
    },
    {
      "epoch": 0.7639055622248899,
      "grad_norm": 0.07598443329334259,
      "learning_rate": 4.7313552526062554e-05,
      "loss": 0.3894,
      "step": 1909
    },
    {
      "epoch": 0.7643057222889156,
      "grad_norm": 0.08796889334917068,
      "learning_rate": 4.723336006415397e-05,
      "loss": 0.4744,
      "step": 1910
    },
    {
      "epoch": 0.7647058823529411,
      "grad_norm": 0.08781806379556656,
      "learning_rate": 4.715316760224539e-05,
      "loss": 0.5062,
      "step": 1911
    },
    {
      "epoch": 0.7651060424169668,
      "grad_norm": 0.08875224739313126,
      "learning_rate": 4.707297514033681e-05,
      "loss": 0.4683,
      "step": 1912
    },
    {
      "epoch": 0.7655062024809924,
      "grad_norm": 0.08810225129127502,
      "learning_rate": 4.699278267842823e-05,
      "loss": 0.419,
      "step": 1913
    },
    {
      "epoch": 0.765906362545018,
      "grad_norm": 0.10656715929508209,
      "learning_rate": 4.691259021651965e-05,
      "loss": 0.4609,
      "step": 1914
    },
    {
      "epoch": 0.7663065226090436,
      "grad_norm": 0.102941133081913,
      "learning_rate": 4.683239775461107e-05,
      "loss": 0.541,
      "step": 1915
    },
    {
      "epoch": 0.7667066826730692,
      "grad_norm": 0.0945926159620285,
      "learning_rate": 4.675220529270249e-05,
      "loss": 0.4407,
      "step": 1916
    },
    {
      "epoch": 0.7671068427370948,
      "grad_norm": 0.08827441185712814,
      "learning_rate": 4.667201283079391e-05,
      "loss": 0.4115,
      "step": 1917
    },
    {
      "epoch": 0.7675070028011205,
      "grad_norm": 0.11192525178194046,
      "learning_rate": 4.659182036888533e-05,
      "loss": 0.4485,
      "step": 1918
    },
    {
      "epoch": 0.767907162865146,
      "grad_norm": 0.11421848833560944,
      "learning_rate": 4.651162790697675e-05,
      "loss": 0.547,
      "step": 1919
    },
    {
      "epoch": 0.7683073229291717,
      "grad_norm": 0.11768656969070435,
      "learning_rate": 4.643143544506817e-05,
      "loss": 0.5347,
      "step": 1920
    },
    {
      "epoch": 0.7687074829931972,
      "grad_norm": 0.10072844475507736,
      "learning_rate": 4.6351242983159584e-05,
      "loss": 0.5411,
      "step": 1921
    },
    {
      "epoch": 0.7691076430572229,
      "grad_norm": 0.0783194750547409,
      "learning_rate": 4.6271050521251006e-05,
      "loss": 0.3725,
      "step": 1922
    },
    {
      "epoch": 0.7695078031212484,
      "grad_norm": 0.10498709976673126,
      "learning_rate": 4.619085805934242e-05,
      "loss": 0.4625,
      "step": 1923
    },
    {
      "epoch": 0.7699079631852741,
      "grad_norm": 0.08539535105228424,
      "learning_rate": 4.611066559743384e-05,
      "loss": 0.5078,
      "step": 1924
    },
    {
      "epoch": 0.7703081232492998,
      "grad_norm": 0.10713059455156326,
      "learning_rate": 4.6030473135525263e-05,
      "loss": 0.5631,
      "step": 1925
    },
    {
      "epoch": 0.7707082833133253,
      "grad_norm": 0.1013650894165039,
      "learning_rate": 4.595028067361668e-05,
      "loss": 0.4332,
      "step": 1926
    },
    {
      "epoch": 0.771108443377351,
      "grad_norm": 0.10244657099246979,
      "learning_rate": 4.58700882117081e-05,
      "loss": 0.4564,
      "step": 1927
    },
    {
      "epoch": 0.7715086034413765,
      "grad_norm": 0.09869068115949631,
      "learning_rate": 4.578989574979952e-05,
      "loss": 0.5123,
      "step": 1928
    },
    {
      "epoch": 0.7719087635054022,
      "grad_norm": 0.0871502086520195,
      "learning_rate": 4.570970328789094e-05,
      "loss": 0.512,
      "step": 1929
    },
    {
      "epoch": 0.7723089235694278,
      "grad_norm": 0.10082902759313583,
      "learning_rate": 4.562951082598236e-05,
      "loss": 0.5489,
      "step": 1930
    },
    {
      "epoch": 0.7727090836334534,
      "grad_norm": 0.0856356993317604,
      "learning_rate": 4.554931836407378e-05,
      "loss": 0.5092,
      "step": 1931
    },
    {
      "epoch": 0.773109243697479,
      "grad_norm": 0.09806042909622192,
      "learning_rate": 4.54691259021652e-05,
      "loss": 0.5185,
      "step": 1932
    },
    {
      "epoch": 0.7735094037615046,
      "grad_norm": 0.09019239991903305,
      "learning_rate": 4.538893344025662e-05,
      "loss": 0.4798,
      "step": 1933
    },
    {
      "epoch": 0.7739095638255302,
      "grad_norm": 0.09262482076883316,
      "learning_rate": 4.5308740978348036e-05,
      "loss": 0.4417,
      "step": 1934
    },
    {
      "epoch": 0.7743097238895558,
      "grad_norm": 0.08751022070646286,
      "learning_rate": 4.522854851643946e-05,
      "loss": 0.4502,
      "step": 1935
    },
    {
      "epoch": 0.7747098839535814,
      "grad_norm": 0.10433977097272873,
      "learning_rate": 4.514835605453088e-05,
      "loss": 0.4661,
      "step": 1936
    },
    {
      "epoch": 0.7751100440176071,
      "grad_norm": 0.09520666301250458,
      "learning_rate": 4.50681635926223e-05,
      "loss": 0.4673,
      "step": 1937
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 0.09899567067623138,
      "learning_rate": 4.4987971130713715e-05,
      "loss": 0.4101,
      "step": 1938
    },
    {
      "epoch": 0.7759103641456583,
      "grad_norm": 0.10065416246652603,
      "learning_rate": 4.490777866880514e-05,
      "loss": 0.4578,
      "step": 1939
    },
    {
      "epoch": 0.7763105242096838,
      "grad_norm": 0.08941834419965744,
      "learning_rate": 4.482758620689655e-05,
      "loss": 0.4712,
      "step": 1940
    },
    {
      "epoch": 0.7767106842737095,
      "grad_norm": 0.10610845685005188,
      "learning_rate": 4.474739374498797e-05,
      "loss": 0.4902,
      "step": 1941
    },
    {
      "epoch": 0.7771108443377351,
      "grad_norm": 0.08040135353803635,
      "learning_rate": 4.466720128307939e-05,
      "loss": 0.401,
      "step": 1942
    },
    {
      "epoch": 0.7775110044017607,
      "grad_norm": 0.08409787714481354,
      "learning_rate": 4.458700882117081e-05,
      "loss": 0.3897,
      "step": 1943
    },
    {
      "epoch": 0.7779111644657863,
      "grad_norm": 0.08343497663736343,
      "learning_rate": 4.450681635926223e-05,
      "loss": 0.3815,
      "step": 1944
    },
    {
      "epoch": 0.7783113245298119,
      "grad_norm": 0.10753708332777023,
      "learning_rate": 4.442662389735365e-05,
      "loss": 0.4435,
      "step": 1945
    },
    {
      "epoch": 0.7787114845938375,
      "grad_norm": 0.08785193413496017,
      "learning_rate": 4.434643143544507e-05,
      "loss": 0.4214,
      "step": 1946
    },
    {
      "epoch": 0.7791116446578632,
      "grad_norm": 0.0994923934340477,
      "learning_rate": 4.426623897353649e-05,
      "loss": 0.4918,
      "step": 1947
    },
    {
      "epoch": 0.7795118047218887,
      "grad_norm": 0.07671165466308594,
      "learning_rate": 4.418604651162791e-05,
      "loss": 0.3681,
      "step": 1948
    },
    {
      "epoch": 0.7799119647859144,
      "grad_norm": 0.0791635811328888,
      "learning_rate": 4.410585404971933e-05,
      "loss": 0.3502,
      "step": 1949
    },
    {
      "epoch": 0.78031212484994,
      "grad_norm": 0.10388088971376419,
      "learning_rate": 4.4025661587810746e-05,
      "loss": 0.494,
      "step": 1950
    },
    {
      "epoch": 0.7807122849139656,
      "grad_norm": 0.11804495751857758,
      "learning_rate": 4.394546912590217e-05,
      "loss": 0.4635,
      "step": 1951
    },
    {
      "epoch": 0.7811124449779911,
      "grad_norm": 0.11177155375480652,
      "learning_rate": 4.386527666399359e-05,
      "loss": 0.5375,
      "step": 1952
    },
    {
      "epoch": 0.7815126050420168,
      "grad_norm": 0.10865313559770584,
      "learning_rate": 4.378508420208501e-05,
      "loss": 0.4367,
      "step": 1953
    },
    {
      "epoch": 0.7819127651060425,
      "grad_norm": 0.09099269658327103,
      "learning_rate": 4.3704891740176425e-05,
      "loss": 0.439,
      "step": 1954
    },
    {
      "epoch": 0.782312925170068,
      "grad_norm": 0.09897348284721375,
      "learning_rate": 4.3624699278267846e-05,
      "loss": 0.52,
      "step": 1955
    },
    {
      "epoch": 0.7827130852340937,
      "grad_norm": 0.11817537248134613,
      "learning_rate": 4.354450681635927e-05,
      "loss": 0.4419,
      "step": 1956
    },
    {
      "epoch": 0.7831132452981192,
      "grad_norm": 0.08340352028608322,
      "learning_rate": 4.346431435445068e-05,
      "loss": 0.4265,
      "step": 1957
    },
    {
      "epoch": 0.7835134053621449,
      "grad_norm": 0.10484445840120316,
      "learning_rate": 4.3384121892542104e-05,
      "loss": 0.5024,
      "step": 1958
    },
    {
      "epoch": 0.7839135654261705,
      "grad_norm": 0.10866449028253555,
      "learning_rate": 4.330392943063352e-05,
      "loss": 0.3898,
      "step": 1959
    },
    {
      "epoch": 0.7843137254901961,
      "grad_norm": 0.08924414962530136,
      "learning_rate": 4.322373696872494e-05,
      "loss": 0.4097,
      "step": 1960
    },
    {
      "epoch": 0.7847138855542217,
      "grad_norm": 0.09675541520118713,
      "learning_rate": 4.314354450681636e-05,
      "loss": 0.3584,
      "step": 1961
    },
    {
      "epoch": 0.7851140456182473,
      "grad_norm": 0.09402813762426376,
      "learning_rate": 4.3063352044907776e-05,
      "loss": 0.4345,
      "step": 1962
    },
    {
      "epoch": 0.7855142056822729,
      "grad_norm": 0.1003534197807312,
      "learning_rate": 4.29831595829992e-05,
      "loss": 0.4601,
      "step": 1963
    },
    {
      "epoch": 0.7859143657462985,
      "grad_norm": 0.11787979304790497,
      "learning_rate": 4.290296712109062e-05,
      "loss": 0.5002,
      "step": 1964
    },
    {
      "epoch": 0.7863145258103241,
      "grad_norm": 0.0937638133764267,
      "learning_rate": 4.282277465918204e-05,
      "loss": 0.4596,
      "step": 1965
    },
    {
      "epoch": 0.7867146858743498,
      "grad_norm": 0.08808267116546631,
      "learning_rate": 4.2742582197273455e-05,
      "loss": 0.3921,
      "step": 1966
    },
    {
      "epoch": 0.7871148459383753,
      "grad_norm": 0.08618311583995819,
      "learning_rate": 4.266238973536488e-05,
      "loss": 0.4348,
      "step": 1967
    },
    {
      "epoch": 0.787515006002401,
      "grad_norm": 0.08480279892683029,
      "learning_rate": 4.25821972734563e-05,
      "loss": 0.4001,
      "step": 1968
    },
    {
      "epoch": 0.7879151660664265,
      "grad_norm": 0.10809377580881119,
      "learning_rate": 4.250200481154772e-05,
      "loss": 0.5372,
      "step": 1969
    },
    {
      "epoch": 0.7883153261304522,
      "grad_norm": 0.10580804944038391,
      "learning_rate": 4.2421812349639135e-05,
      "loss": 0.4614,
      "step": 1970
    },
    {
      "epoch": 0.7887154861944778,
      "grad_norm": 0.10939965397119522,
      "learning_rate": 4.2341619887730556e-05,
      "loss": 0.4579,
      "step": 1971
    },
    {
      "epoch": 0.7891156462585034,
      "grad_norm": 0.10893861204385757,
      "learning_rate": 4.226142742582198e-05,
      "loss": 0.517,
      "step": 1972
    },
    {
      "epoch": 0.789515806322529,
      "grad_norm": 0.0984027162194252,
      "learning_rate": 4.21812349639134e-05,
      "loss": 0.4236,
      "step": 1973
    },
    {
      "epoch": 0.7899159663865546,
      "grad_norm": 0.11983399838209152,
      "learning_rate": 4.2101042502004814e-05,
      "loss": 0.3721,
      "step": 1974
    },
    {
      "epoch": 0.7903161264505802,
      "grad_norm": 0.08736562728881836,
      "learning_rate": 4.2020850040096235e-05,
      "loss": 0.4478,
      "step": 1975
    },
    {
      "epoch": 0.7907162865146058,
      "grad_norm": 0.10861973464488983,
      "learning_rate": 4.194065757818765e-05,
      "loss": 0.4754,
      "step": 1976
    },
    {
      "epoch": 0.7911164465786314,
      "grad_norm": 0.10864321142435074,
      "learning_rate": 4.186046511627907e-05,
      "loss": 0.5158,
      "step": 1977
    },
    {
      "epoch": 0.7915166066426571,
      "grad_norm": 0.09725893288850784,
      "learning_rate": 4.1780272654370486e-05,
      "loss": 0.4688,
      "step": 1978
    },
    {
      "epoch": 0.7919167667066827,
      "grad_norm": 0.09336603432893753,
      "learning_rate": 4.170008019246191e-05,
      "loss": 0.3897,
      "step": 1979
    },
    {
      "epoch": 0.7923169267707083,
      "grad_norm": 0.10146067291498184,
      "learning_rate": 4.161988773055333e-05,
      "loss": 0.4511,
      "step": 1980
    },
    {
      "epoch": 0.7927170868347339,
      "grad_norm": 0.10775581002235413,
      "learning_rate": 4.153969526864475e-05,
      "loss": 0.4322,
      "step": 1981
    },
    {
      "epoch": 0.7931172468987595,
      "grad_norm": 0.0809892863035202,
      "learning_rate": 4.1459502806736165e-05,
      "loss": 0.416,
      "step": 1982
    },
    {
      "epoch": 0.7935174069627852,
      "grad_norm": 0.08393702656030655,
      "learning_rate": 4.1379310344827587e-05,
      "loss": 0.4519,
      "step": 1983
    },
    {
      "epoch": 0.7939175670268107,
      "grad_norm": 0.1021948829293251,
      "learning_rate": 4.129911788291901e-05,
      "loss": 0.5195,
      "step": 1984
    },
    {
      "epoch": 0.7943177270908364,
      "grad_norm": 0.10666944086551666,
      "learning_rate": 4.121892542101043e-05,
      "loss": 0.5067,
      "step": 1985
    },
    {
      "epoch": 0.7947178871548619,
      "grad_norm": 0.11134424060583115,
      "learning_rate": 4.1138732959101844e-05,
      "loss": 0.4376,
      "step": 1986
    },
    {
      "epoch": 0.7951180472188876,
      "grad_norm": 0.0843426063656807,
      "learning_rate": 4.1058540497193266e-05,
      "loss": 0.4602,
      "step": 1987
    },
    {
      "epoch": 0.7955182072829131,
      "grad_norm": 0.08042240142822266,
      "learning_rate": 4.097834803528469e-05,
      "loss": 0.4295,
      "step": 1988
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 0.10946281999349594,
      "learning_rate": 4.089815557337611e-05,
      "loss": 0.5575,
      "step": 1989
    },
    {
      "epoch": 0.7963185274109644,
      "grad_norm": 0.0950012356042862,
      "learning_rate": 4.081796311146752e-05,
      "loss": 0.4105,
      "step": 1990
    },
    {
      "epoch": 0.79671868747499,
      "grad_norm": 0.08610996603965759,
      "learning_rate": 4.0737770649558945e-05,
      "loss": 0.38,
      "step": 1991
    },
    {
      "epoch": 0.7971188475390156,
      "grad_norm": 0.10125786811113358,
      "learning_rate": 4.0657578187650366e-05,
      "loss": 0.4442,
      "step": 1992
    },
    {
      "epoch": 0.7975190076030412,
      "grad_norm": 0.09987455606460571,
      "learning_rate": 4.057738572574179e-05,
      "loss": 0.5077,
      "step": 1993
    },
    {
      "epoch": 0.7979191676670668,
      "grad_norm": 0.10344497859477997,
      "learning_rate": 4.04971932638332e-05,
      "loss": 0.4898,
      "step": 1994
    },
    {
      "epoch": 0.7983193277310925,
      "grad_norm": 0.09554927051067352,
      "learning_rate": 4.0417000801924624e-05,
      "loss": 0.4845,
      "step": 1995
    },
    {
      "epoch": 0.798719487795118,
      "grad_norm": 0.10167738795280457,
      "learning_rate": 4.033680834001604e-05,
      "loss": 0.5165,
      "step": 1996
    },
    {
      "epoch": 0.7991196478591437,
      "grad_norm": 0.09400682896375656,
      "learning_rate": 4.025661587810746e-05,
      "loss": 0.4573,
      "step": 1997
    },
    {
      "epoch": 0.7995198079231692,
      "grad_norm": 0.08902110159397125,
      "learning_rate": 4.0176423416198875e-05,
      "loss": 0.4563,
      "step": 1998
    },
    {
      "epoch": 0.7999199679871949,
      "grad_norm": 0.08161738514900208,
      "learning_rate": 4.0096230954290296e-05,
      "loss": 0.4095,
      "step": 1999
    },
    {
      "epoch": 0.8003201280512204,
      "grad_norm": 0.08590369671583176,
      "learning_rate": 4.001603849238172e-05,
      "loss": 0.4747,
      "step": 2000
    },
    {
      "epoch": 0.8007202881152461,
      "grad_norm": 0.09928359091281891,
      "learning_rate": 3.993584603047314e-05,
      "loss": 0.4958,
      "step": 2001
    },
    {
      "epoch": 0.8011204481792717,
      "grad_norm": 0.10364052653312683,
      "learning_rate": 3.9855653568564554e-05,
      "loss": 0.4714,
      "step": 2002
    },
    {
      "epoch": 0.8015206082432973,
      "grad_norm": 0.09058904647827148,
      "learning_rate": 3.9775461106655975e-05,
      "loss": 0.3717,
      "step": 2003
    },
    {
      "epoch": 0.801920768307323,
      "grad_norm": 0.09996795654296875,
      "learning_rate": 3.96952686447474e-05,
      "loss": 0.4644,
      "step": 2004
    },
    {
      "epoch": 0.8023209283713485,
      "grad_norm": 0.11301668733358383,
      "learning_rate": 3.961507618283882e-05,
      "loss": 0.5104,
      "step": 2005
    },
    {
      "epoch": 0.8027210884353742,
      "grad_norm": 0.10247011482715607,
      "learning_rate": 3.953488372093023e-05,
      "loss": 0.3875,
      "step": 2006
    },
    {
      "epoch": 0.8031212484993998,
      "grad_norm": 0.0767265111207962,
      "learning_rate": 3.9454691259021654e-05,
      "loss": 0.3414,
      "step": 2007
    },
    {
      "epoch": 0.8035214085634254,
      "grad_norm": 0.08274094015359879,
      "learning_rate": 3.9374498797113076e-05,
      "loss": 0.426,
      "step": 2008
    },
    {
      "epoch": 0.803921568627451,
      "grad_norm": 0.10198420286178589,
      "learning_rate": 3.92943063352045e-05,
      "loss": 0.4794,
      "step": 2009
    },
    {
      "epoch": 0.8043217286914766,
      "grad_norm": 0.1201816126704216,
      "learning_rate": 3.921411387329591e-05,
      "loss": 0.534,
      "step": 2010
    },
    {
      "epoch": 0.8047218887555022,
      "grad_norm": 0.09323246777057648,
      "learning_rate": 3.9133921411387333e-05,
      "loss": 0.3395,
      "step": 2011
    },
    {
      "epoch": 0.8051220488195278,
      "grad_norm": 0.09362755715847015,
      "learning_rate": 3.9053728949478755e-05,
      "loss": 0.4467,
      "step": 2012
    },
    {
      "epoch": 0.8055222088835534,
      "grad_norm": 0.10462263971567154,
      "learning_rate": 3.897353648757017e-05,
      "loss": 0.5239,
      "step": 2013
    },
    {
      "epoch": 0.8059223689475791,
      "grad_norm": 0.09006280452013016,
      "learning_rate": 3.889334402566159e-05,
      "loss": 0.4476,
      "step": 2014
    },
    {
      "epoch": 0.8063225290116046,
      "grad_norm": 0.11252333968877792,
      "learning_rate": 3.8813151563753006e-05,
      "loss": 0.4794,
      "step": 2015
    },
    {
      "epoch": 0.8067226890756303,
      "grad_norm": 0.09648938477039337,
      "learning_rate": 3.873295910184443e-05,
      "loss": 0.4925,
      "step": 2016
    },
    {
      "epoch": 0.8071228491396558,
      "grad_norm": 0.08805526047945023,
      "learning_rate": 3.865276663993585e-05,
      "loss": 0.4935,
      "step": 2017
    },
    {
      "epoch": 0.8075230092036815,
      "grad_norm": 0.08413508534431458,
      "learning_rate": 3.857257417802726e-05,
      "loss": 0.3919,
      "step": 2018
    },
    {
      "epoch": 0.8079231692677071,
      "grad_norm": 0.10167526453733444,
      "learning_rate": 3.8492381716118685e-05,
      "loss": 0.3896,
      "step": 2019
    },
    {
      "epoch": 0.8083233293317327,
      "grad_norm": 0.09905955940485,
      "learning_rate": 3.8412189254210106e-05,
      "loss": 0.4279,
      "step": 2020
    },
    {
      "epoch": 0.8087234893957583,
      "grad_norm": 0.08595161139965057,
      "learning_rate": 3.833199679230153e-05,
      "loss": 0.4005,
      "step": 2021
    },
    {
      "epoch": 0.8091236494597839,
      "grad_norm": 0.08652879297733307,
      "learning_rate": 3.825180433039294e-05,
      "loss": 0.4418,
      "step": 2022
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.10101266950368881,
      "learning_rate": 3.8171611868484364e-05,
      "loss": 0.4807,
      "step": 2023
    },
    {
      "epoch": 0.8099239695878351,
      "grad_norm": 0.09833244979381561,
      "learning_rate": 3.8091419406575785e-05,
      "loss": 0.4391,
      "step": 2024
    },
    {
      "epoch": 0.8103241296518607,
      "grad_norm": 0.11564866453409195,
      "learning_rate": 3.801122694466721e-05,
      "loss": 0.4501,
      "step": 2025
    },
    {
      "epoch": 0.8107242897158864,
      "grad_norm": 0.10629183799028397,
      "learning_rate": 3.793103448275862e-05,
      "loss": 0.4818,
      "step": 2026
    },
    {
      "epoch": 0.8111244497799119,
      "grad_norm": 0.08895901590585709,
      "learning_rate": 3.785084202085004e-05,
      "loss": 0.4439,
      "step": 2027
    },
    {
      "epoch": 0.8115246098439376,
      "grad_norm": 0.09658870100975037,
      "learning_rate": 3.7770649558941465e-05,
      "loss": 0.5225,
      "step": 2028
    },
    {
      "epoch": 0.8119247699079631,
      "grad_norm": 0.10134138911962509,
      "learning_rate": 3.7690457097032886e-05,
      "loss": 0.4665,
      "step": 2029
    },
    {
      "epoch": 0.8123249299719888,
      "grad_norm": 0.10885049402713776,
      "learning_rate": 3.76102646351243e-05,
      "loss": 0.4695,
      "step": 2030
    },
    {
      "epoch": 0.8127250900360145,
      "grad_norm": 0.09768392890691757,
      "learning_rate": 3.753007217321572e-05,
      "loss": 0.4571,
      "step": 2031
    },
    {
      "epoch": 0.81312525010004,
      "grad_norm": 0.08155114948749542,
      "learning_rate": 3.744987971130714e-05,
      "loss": 0.4006,
      "step": 2032
    },
    {
      "epoch": 0.8135254101640657,
      "grad_norm": 0.08422590792179108,
      "learning_rate": 3.736968724939856e-05,
      "loss": 0.4762,
      "step": 2033
    },
    {
      "epoch": 0.8139255702280912,
      "grad_norm": 0.07617289572954178,
      "learning_rate": 3.728949478748997e-05,
      "loss": 0.3909,
      "step": 2034
    },
    {
      "epoch": 0.8143257302921169,
      "grad_norm": 0.0935085266828537,
      "learning_rate": 3.7209302325581394e-05,
      "loss": 0.4528,
      "step": 2035
    },
    {
      "epoch": 0.8147258903561424,
      "grad_norm": 0.09751499444246292,
      "learning_rate": 3.7129109863672816e-05,
      "loss": 0.5624,
      "step": 2036
    },
    {
      "epoch": 0.8151260504201681,
      "grad_norm": 0.09600226581096649,
      "learning_rate": 3.704891740176424e-05,
      "loss": 0.5402,
      "step": 2037
    },
    {
      "epoch": 0.8155262104841937,
      "grad_norm": 0.09860710799694061,
      "learning_rate": 3.696872493985565e-05,
      "loss": 0.4236,
      "step": 2038
    },
    {
      "epoch": 0.8159263705482193,
      "grad_norm": 0.08924972265958786,
      "learning_rate": 3.6888532477947074e-05,
      "loss": 0.4806,
      "step": 2039
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.10928446054458618,
      "learning_rate": 3.6808340016038495e-05,
      "loss": 0.5073,
      "step": 2040
    },
    {
      "epoch": 0.8167266906762705,
      "grad_norm": 0.09482402354478836,
      "learning_rate": 3.6728147554129916e-05,
      "loss": 0.4508,
      "step": 2041
    },
    {
      "epoch": 0.8171268507402961,
      "grad_norm": 0.09702252596616745,
      "learning_rate": 3.664795509222133e-05,
      "loss": 0.4409,
      "step": 2042
    },
    {
      "epoch": 0.8175270108043218,
      "grad_norm": 0.08048701286315918,
      "learning_rate": 3.656776263031275e-05,
      "loss": 0.4585,
      "step": 2043
    },
    {
      "epoch": 0.8179271708683473,
      "grad_norm": 0.1413080394268036,
      "learning_rate": 3.6487570168404174e-05,
      "loss": 0.5249,
      "step": 2044
    },
    {
      "epoch": 0.818327330932373,
      "grad_norm": 0.10065623372793198,
      "learning_rate": 3.6407377706495596e-05,
      "loss": 0.3669,
      "step": 2045
    },
    {
      "epoch": 0.8187274909963985,
      "grad_norm": 0.10355666279792786,
      "learning_rate": 3.632718524458701e-05,
      "loss": 0.4766,
      "step": 2046
    },
    {
      "epoch": 0.8191276510604242,
      "grad_norm": 0.09290578216314316,
      "learning_rate": 3.624699278267843e-05,
      "loss": 0.4663,
      "step": 2047
    },
    {
      "epoch": 0.8195278111244498,
      "grad_norm": 0.08334269374608994,
      "learning_rate": 3.616680032076985e-05,
      "loss": 0.4423,
      "step": 2048
    },
    {
      "epoch": 0.8199279711884754,
      "grad_norm": 0.10294037312269211,
      "learning_rate": 3.608660785886127e-05,
      "loss": 0.4942,
      "step": 2049
    },
    {
      "epoch": 0.820328131252501,
      "grad_norm": 0.11496546119451523,
      "learning_rate": 3.600641539695269e-05,
      "loss": 0.5792,
      "step": 2050
    },
    {
      "epoch": 0.8207282913165266,
      "grad_norm": 0.08966340124607086,
      "learning_rate": 3.5926222935044104e-05,
      "loss": 0.4581,
      "step": 2051
    },
    {
      "epoch": 0.8211284513805522,
      "grad_norm": 0.08893401175737381,
      "learning_rate": 3.5846030473135526e-05,
      "loss": 0.4051,
      "step": 2052
    },
    {
      "epoch": 0.8215286114445778,
      "grad_norm": 0.10359489917755127,
      "learning_rate": 3.576583801122695e-05,
      "loss": 0.5099,
      "step": 2053
    },
    {
      "epoch": 0.8219287715086034,
      "grad_norm": 0.11365018784999847,
      "learning_rate": 3.568564554931836e-05,
      "loss": 0.3938,
      "step": 2054
    },
    {
      "epoch": 0.8223289315726291,
      "grad_norm": 0.08638829737901688,
      "learning_rate": 3.560545308740978e-05,
      "loss": 0.4758,
      "step": 2055
    },
    {
      "epoch": 0.8227290916366546,
      "grad_norm": 0.08947845548391342,
      "learning_rate": 3.5525260625501205e-05,
      "loss": 0.4319,
      "step": 2056
    },
    {
      "epoch": 0.8231292517006803,
      "grad_norm": 0.09097374230623245,
      "learning_rate": 3.5445068163592626e-05,
      "loss": 0.4905,
      "step": 2057
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 0.09615984559059143,
      "learning_rate": 3.536487570168404e-05,
      "loss": 0.4307,
      "step": 2058
    },
    {
      "epoch": 0.8239295718287315,
      "grad_norm": 0.09039877355098724,
      "learning_rate": 3.528468323977546e-05,
      "loss": 0.5093,
      "step": 2059
    },
    {
      "epoch": 0.8243297318927572,
      "grad_norm": 0.0951724648475647,
      "learning_rate": 3.5204490777866884e-05,
      "loss": 0.4568,
      "step": 2060
    },
    {
      "epoch": 0.8247298919567827,
      "grad_norm": 0.09686581045389175,
      "learning_rate": 3.5124298315958305e-05,
      "loss": 0.4897,
      "step": 2061
    },
    {
      "epoch": 0.8251300520208084,
      "grad_norm": 0.09626106917858124,
      "learning_rate": 3.504410585404972e-05,
      "loss": 0.515,
      "step": 2062
    },
    {
      "epoch": 0.8255302120848339,
      "grad_norm": 0.10696335136890411,
      "learning_rate": 3.496391339214114e-05,
      "loss": 0.4668,
      "step": 2063
    },
    {
      "epoch": 0.8259303721488596,
      "grad_norm": 0.08375649154186249,
      "learning_rate": 3.488372093023256e-05,
      "loss": 0.4287,
      "step": 2064
    },
    {
      "epoch": 0.8263305322128851,
      "grad_norm": 0.08674248307943344,
      "learning_rate": 3.4803528468323984e-05,
      "loss": 0.4086,
      "step": 2065
    },
    {
      "epoch": 0.8267306922769108,
      "grad_norm": 0.08754484355449677,
      "learning_rate": 3.47233360064154e-05,
      "loss": 0.4723,
      "step": 2066
    },
    {
      "epoch": 0.8271308523409364,
      "grad_norm": 0.09229908138513565,
      "learning_rate": 3.464314354450682e-05,
      "loss": 0.4385,
      "step": 2067
    },
    {
      "epoch": 0.827531012404962,
      "grad_norm": 0.10576111078262329,
      "learning_rate": 3.4562951082598235e-05,
      "loss": 0.4244,
      "step": 2068
    },
    {
      "epoch": 0.8279311724689876,
      "grad_norm": 0.10128409415483475,
      "learning_rate": 3.4482758620689657e-05,
      "loss": 0.413,
      "step": 2069
    },
    {
      "epoch": 0.8283313325330132,
      "grad_norm": 0.08742134273052216,
      "learning_rate": 3.440256615878107e-05,
      "loss": 0.4279,
      "step": 2070
    },
    {
      "epoch": 0.8287314925970388,
      "grad_norm": 0.09585162252187729,
      "learning_rate": 3.432237369687249e-05,
      "loss": 0.4601,
      "step": 2071
    },
    {
      "epoch": 0.8291316526610645,
      "grad_norm": 0.08564861118793488,
      "learning_rate": 3.4242181234963914e-05,
      "loss": 0.4177,
      "step": 2072
    },
    {
      "epoch": 0.82953181272509,
      "grad_norm": 0.07018374651670456,
      "learning_rate": 3.4161988773055336e-05,
      "loss": 0.3465,
      "step": 2073
    },
    {
      "epoch": 0.8299319727891157,
      "grad_norm": 0.09279748797416687,
      "learning_rate": 3.408179631114675e-05,
      "loss": 0.4422,
      "step": 2074
    },
    {
      "epoch": 0.8303321328531412,
      "grad_norm": 0.0930287092924118,
      "learning_rate": 3.400160384923817e-05,
      "loss": 0.4178,
      "step": 2075
    },
    {
      "epoch": 0.8307322929171669,
      "grad_norm": 0.10733487457036972,
      "learning_rate": 3.392141138732959e-05,
      "loss": 0.461,
      "step": 2076
    },
    {
      "epoch": 0.8311324529811924,
      "grad_norm": 0.09446343034505844,
      "learning_rate": 3.3841218925421015e-05,
      "loss": 0.4341,
      "step": 2077
    },
    {
      "epoch": 0.8315326130452181,
      "grad_norm": 0.07145536690950394,
      "learning_rate": 3.376102646351243e-05,
      "loss": 0.3864,
      "step": 2078
    },
    {
      "epoch": 0.8319327731092437,
      "grad_norm": 0.08278729021549225,
      "learning_rate": 3.368083400160385e-05,
      "loss": 0.351,
      "step": 2079
    },
    {
      "epoch": 0.8323329331732693,
      "grad_norm": 0.08639855682849884,
      "learning_rate": 3.360064153969527e-05,
      "loss": 0.4622,
      "step": 2080
    },
    {
      "epoch": 0.8327330932372949,
      "grad_norm": 0.09673593938350677,
      "learning_rate": 3.3520449077786694e-05,
      "loss": 0.49,
      "step": 2081
    },
    {
      "epoch": 0.8331332533013205,
      "grad_norm": 0.07270769774913788,
      "learning_rate": 3.344025661587811e-05,
      "loss": 0.3854,
      "step": 2082
    },
    {
      "epoch": 0.8335334133653461,
      "grad_norm": 0.09840808063745499,
      "learning_rate": 3.336006415396953e-05,
      "loss": 0.4105,
      "step": 2083
    },
    {
      "epoch": 0.8339335734293718,
      "grad_norm": 0.0895611122250557,
      "learning_rate": 3.327987169206095e-05,
      "loss": 0.4599,
      "step": 2084
    },
    {
      "epoch": 0.8343337334933973,
      "grad_norm": 0.0999559760093689,
      "learning_rate": 3.319967923015237e-05,
      "loss": 0.46,
      "step": 2085
    },
    {
      "epoch": 0.834733893557423,
      "grad_norm": 0.0866829976439476,
      "learning_rate": 3.311948676824379e-05,
      "loss": 0.4027,
      "step": 2086
    },
    {
      "epoch": 0.8351340536214485,
      "grad_norm": 0.11933156847953796,
      "learning_rate": 3.30392943063352e-05,
      "loss": 0.5354,
      "step": 2087
    },
    {
      "epoch": 0.8355342136854742,
      "grad_norm": 0.09372946619987488,
      "learning_rate": 3.2959101844426624e-05,
      "loss": 0.4556,
      "step": 2088
    },
    {
      "epoch": 0.8359343737494997,
      "grad_norm": 0.08057989180088043,
      "learning_rate": 3.2878909382518045e-05,
      "loss": 0.4079,
      "step": 2089
    },
    {
      "epoch": 0.8363345338135254,
      "grad_norm": 0.09614475816488266,
      "learning_rate": 3.279871692060946e-05,
      "loss": 0.451,
      "step": 2090
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 0.07613158226013184,
      "learning_rate": 3.271852445870088e-05,
      "loss": 0.3969,
      "step": 2091
    },
    {
      "epoch": 0.8371348539415766,
      "grad_norm": 0.08963502943515778,
      "learning_rate": 3.26383319967923e-05,
      "loss": 0.4382,
      "step": 2092
    },
    {
      "epoch": 0.8375350140056023,
      "grad_norm": 0.11197859793901443,
      "learning_rate": 3.2558139534883724e-05,
      "loss": 0.5339,
      "step": 2093
    },
    {
      "epoch": 0.8379351740696278,
      "grad_norm": 0.093267522752285,
      "learning_rate": 3.247794707297514e-05,
      "loss": 0.433,
      "step": 2094
    },
    {
      "epoch": 0.8383353341336535,
      "grad_norm": 0.08270087838172913,
      "learning_rate": 3.239775461106656e-05,
      "loss": 0.4003,
      "step": 2095
    },
    {
      "epoch": 0.8387354941976791,
      "grad_norm": 0.0870816707611084,
      "learning_rate": 3.231756214915798e-05,
      "loss": 0.4331,
      "step": 2096
    },
    {
      "epoch": 0.8391356542617047,
      "grad_norm": 0.11918195337057114,
      "learning_rate": 3.2237369687249403e-05,
      "loss": 0.5481,
      "step": 2097
    },
    {
      "epoch": 0.8395358143257303,
      "grad_norm": 0.08676254749298096,
      "learning_rate": 3.215717722534082e-05,
      "loss": 0.4844,
      "step": 2098
    },
    {
      "epoch": 0.8399359743897559,
      "grad_norm": 0.08421976864337921,
      "learning_rate": 3.207698476343224e-05,
      "loss": 0.3833,
      "step": 2099
    },
    {
      "epoch": 0.8403361344537815,
      "grad_norm": 0.08453816175460815,
      "learning_rate": 3.199679230152366e-05,
      "loss": 0.4325,
      "step": 2100
    },
    {
      "epoch": 0.8407362945178071,
      "grad_norm": 0.10178837925195694,
      "learning_rate": 3.191659983961508e-05,
      "loss": 0.418,
      "step": 2101
    },
    {
      "epoch": 0.8411364545818327,
      "grad_norm": 0.10094982385635376,
      "learning_rate": 3.18364073777065e-05,
      "loss": 0.5124,
      "step": 2102
    },
    {
      "epoch": 0.8415366146458584,
      "grad_norm": 0.0913056954741478,
      "learning_rate": 3.175621491579792e-05,
      "loss": 0.3677,
      "step": 2103
    },
    {
      "epoch": 0.8419367747098839,
      "grad_norm": 0.103488989174366,
      "learning_rate": 3.167602245388934e-05,
      "loss": 0.4836,
      "step": 2104
    },
    {
      "epoch": 0.8423369347739096,
      "grad_norm": 0.10417862981557846,
      "learning_rate": 3.1595829991980755e-05,
      "loss": 0.5116,
      "step": 2105
    },
    {
      "epoch": 0.8427370948379351,
      "grad_norm": 0.11216050386428833,
      "learning_rate": 3.151563753007217e-05,
      "loss": 0.517,
      "step": 2106
    },
    {
      "epoch": 0.8431372549019608,
      "grad_norm": 0.09065698832273483,
      "learning_rate": 3.143544506816359e-05,
      "loss": 0.4699,
      "step": 2107
    },
    {
      "epoch": 0.8435374149659864,
      "grad_norm": 0.1001441702246666,
      "learning_rate": 3.135525260625501e-05,
      "loss": 0.395,
      "step": 2108
    },
    {
      "epoch": 0.843937575030012,
      "grad_norm": 0.09912700206041336,
      "learning_rate": 3.1275060144346434e-05,
      "loss": 0.4819,
      "step": 2109
    },
    {
      "epoch": 0.8443377350940376,
      "grad_norm": 0.08569461107254028,
      "learning_rate": 3.119486768243785e-05,
      "loss": 0.439,
      "step": 2110
    },
    {
      "epoch": 0.8447378951580632,
      "grad_norm": 0.09517976641654968,
      "learning_rate": 3.111467522052927e-05,
      "loss": 0.4189,
      "step": 2111
    },
    {
      "epoch": 0.8451380552220888,
      "grad_norm": 0.09468182176351547,
      "learning_rate": 3.103448275862069e-05,
      "loss": 0.477,
      "step": 2112
    },
    {
      "epoch": 0.8455382152861144,
      "grad_norm": 0.09151163697242737,
      "learning_rate": 3.095429029671211e-05,
      "loss": 0.4225,
      "step": 2113
    },
    {
      "epoch": 0.84593837535014,
      "grad_norm": 0.09325399994850159,
      "learning_rate": 3.087409783480353e-05,
      "loss": 0.4066,
      "step": 2114
    },
    {
      "epoch": 0.8463385354141657,
      "grad_norm": 0.10006613284349442,
      "learning_rate": 3.079390537289495e-05,
      "loss": 0.5079,
      "step": 2115
    },
    {
      "epoch": 0.8467386954781913,
      "grad_norm": 0.09910446405410767,
      "learning_rate": 3.071371291098637e-05,
      "loss": 0.5103,
      "step": 2116
    },
    {
      "epoch": 0.8471388555422169,
      "grad_norm": 0.0930723175406456,
      "learning_rate": 3.063352044907779e-05,
      "loss": 0.4965,
      "step": 2117
    },
    {
      "epoch": 0.8475390156062425,
      "grad_norm": 0.0801524966955185,
      "learning_rate": 3.055332798716921e-05,
      "loss": 0.4308,
      "step": 2118
    },
    {
      "epoch": 0.8479391756702681,
      "grad_norm": 0.124026358127594,
      "learning_rate": 3.0473135525260625e-05,
      "loss": 0.5132,
      "step": 2119
    },
    {
      "epoch": 0.8483393357342938,
      "grad_norm": 0.07675876468420029,
      "learning_rate": 3.0392943063352046e-05,
      "loss": 0.3801,
      "step": 2120
    },
    {
      "epoch": 0.8487394957983193,
      "grad_norm": 0.08487264066934586,
      "learning_rate": 3.0312750601443468e-05,
      "loss": 0.3842,
      "step": 2121
    },
    {
      "epoch": 0.849139655862345,
      "grad_norm": 0.09669055044651031,
      "learning_rate": 3.0232558139534883e-05,
      "loss": 0.4827,
      "step": 2122
    },
    {
      "epoch": 0.8495398159263705,
      "grad_norm": 0.08059608936309814,
      "learning_rate": 3.0152365677626304e-05,
      "loss": 0.4482,
      "step": 2123
    },
    {
      "epoch": 0.8499399759903962,
      "grad_norm": 0.09507828205823898,
      "learning_rate": 3.0072173215717726e-05,
      "loss": 0.4731,
      "step": 2124
    },
    {
      "epoch": 0.8503401360544217,
      "grad_norm": 0.09696406871080399,
      "learning_rate": 2.9991980753809147e-05,
      "loss": 0.3963,
      "step": 2125
    },
    {
      "epoch": 0.8507402961184474,
      "grad_norm": 0.11361099034547806,
      "learning_rate": 2.991178829190056e-05,
      "loss": 0.4856,
      "step": 2126
    },
    {
      "epoch": 0.851140456182473,
      "grad_norm": 0.10694912821054459,
      "learning_rate": 2.9831595829991983e-05,
      "loss": 0.4964,
      "step": 2127
    },
    {
      "epoch": 0.8515406162464986,
      "grad_norm": 0.08753882348537445,
      "learning_rate": 2.97514033680834e-05,
      "loss": 0.454,
      "step": 2128
    },
    {
      "epoch": 0.8519407763105242,
      "grad_norm": 0.10876245051622391,
      "learning_rate": 2.9671210906174823e-05,
      "loss": 0.5123,
      "step": 2129
    },
    {
      "epoch": 0.8523409363745498,
      "grad_norm": 0.11665822565555573,
      "learning_rate": 2.9591018444266237e-05,
      "loss": 0.4373,
      "step": 2130
    },
    {
      "epoch": 0.8527410964385754,
      "grad_norm": 0.09172168374061584,
      "learning_rate": 2.951082598235766e-05,
      "loss": 0.4999,
      "step": 2131
    },
    {
      "epoch": 0.8531412565026011,
      "grad_norm": 0.08805391192436218,
      "learning_rate": 2.943063352044908e-05,
      "loss": 0.4192,
      "step": 2132
    },
    {
      "epoch": 0.8535414165666266,
      "grad_norm": 0.09467556327581406,
      "learning_rate": 2.9350441058540502e-05,
      "loss": 0.452,
      "step": 2133
    },
    {
      "epoch": 0.8539415766306523,
      "grad_norm": 0.10347863286733627,
      "learning_rate": 2.9270248596631916e-05,
      "loss": 0.4785,
      "step": 2134
    },
    {
      "epoch": 0.8543417366946778,
      "grad_norm": 0.10160476714372635,
      "learning_rate": 2.9190056134723338e-05,
      "loss": 0.4376,
      "step": 2135
    },
    {
      "epoch": 0.8547418967587035,
      "grad_norm": 0.10312657058238983,
      "learning_rate": 2.910986367281476e-05,
      "loss": 0.5152,
      "step": 2136
    },
    {
      "epoch": 0.8551420568227291,
      "grad_norm": 0.10070628672838211,
      "learning_rate": 2.9029671210906177e-05,
      "loss": 0.4972,
      "step": 2137
    },
    {
      "epoch": 0.8555422168867547,
      "grad_norm": 0.10054179280996323,
      "learning_rate": 2.8949478748997592e-05,
      "loss": 0.4994,
      "step": 2138
    },
    {
      "epoch": 0.8559423769507803,
      "grad_norm": 0.11355245113372803,
      "learning_rate": 2.8869286287089014e-05,
      "loss": 0.5559,
      "step": 2139
    },
    {
      "epoch": 0.8563425370148059,
      "grad_norm": 0.12042655050754547,
      "learning_rate": 2.8789093825180435e-05,
      "loss": 0.5255,
      "step": 2140
    },
    {
      "epoch": 0.8567426970788315,
      "grad_norm": 0.0887511745095253,
      "learning_rate": 2.8708901363271857e-05,
      "loss": 0.5217,
      "step": 2141
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.08907318115234375,
      "learning_rate": 2.862870890136327e-05,
      "loss": 0.4285,
      "step": 2142
    },
    {
      "epoch": 0.8575430172068828,
      "grad_norm": 0.08639213442802429,
      "learning_rate": 2.8548516439454693e-05,
      "loss": 0.4459,
      "step": 2143
    },
    {
      "epoch": 0.8579431772709084,
      "grad_norm": 0.09200975298881531,
      "learning_rate": 2.8468323977546114e-05,
      "loss": 0.4065,
      "step": 2144
    },
    {
      "epoch": 0.858343337334934,
      "grad_norm": 0.1065359115600586,
      "learning_rate": 2.8388131515637532e-05,
      "loss": 0.541,
      "step": 2145
    },
    {
      "epoch": 0.8587434973989596,
      "grad_norm": 0.0949152410030365,
      "learning_rate": 2.830793905372895e-05,
      "loss": 0.445,
      "step": 2146
    },
    {
      "epoch": 0.8591436574629852,
      "grad_norm": 0.09137151390314102,
      "learning_rate": 2.822774659182037e-05,
      "loss": 0.5399,
      "step": 2147
    },
    {
      "epoch": 0.8595438175270108,
      "grad_norm": 0.078838050365448,
      "learning_rate": 2.814755412991179e-05,
      "loss": 0.4783,
      "step": 2148
    },
    {
      "epoch": 0.8599439775910365,
      "grad_norm": 0.10117875784635544,
      "learning_rate": 2.806736166800321e-05,
      "loss": 0.4736,
      "step": 2149
    },
    {
      "epoch": 0.860344137655062,
      "grad_norm": 0.07488255202770233,
      "learning_rate": 2.7987169206094626e-05,
      "loss": 0.3542,
      "step": 2150
    },
    {
      "epoch": 0.8607442977190877,
      "grad_norm": 0.09474018961191177,
      "learning_rate": 2.7906976744186048e-05,
      "loss": 0.4667,
      "step": 2151
    },
    {
      "epoch": 0.8611444577831132,
      "grad_norm": 0.08068902790546417,
      "learning_rate": 2.782678428227747e-05,
      "loss": 0.4501,
      "step": 2152
    },
    {
      "epoch": 0.8615446178471389,
      "grad_norm": 0.08637302368879318,
      "learning_rate": 2.774659182036889e-05,
      "loss": 0.414,
      "step": 2153
    },
    {
      "epoch": 0.8619447779111644,
      "grad_norm": 0.09460025280714035,
      "learning_rate": 2.7666399358460305e-05,
      "loss": 0.474,
      "step": 2154
    },
    {
      "epoch": 0.8623449379751901,
      "grad_norm": 0.0841122642159462,
      "learning_rate": 2.7586206896551727e-05,
      "loss": 0.4637,
      "step": 2155
    },
    {
      "epoch": 0.8627450980392157,
      "grad_norm": 0.08497589826583862,
      "learning_rate": 2.7506014434643145e-05,
      "loss": 0.441,
      "step": 2156
    },
    {
      "epoch": 0.8631452581032413,
      "grad_norm": 0.10231737047433853,
      "learning_rate": 2.7425821972734566e-05,
      "loss": 0.502,
      "step": 2157
    },
    {
      "epoch": 0.8635454181672669,
      "grad_norm": 0.09654945880174637,
      "learning_rate": 2.734562951082598e-05,
      "loss": 0.5077,
      "step": 2158
    },
    {
      "epoch": 0.8639455782312925,
      "grad_norm": 0.09419191628694534,
      "learning_rate": 2.7265437048917402e-05,
      "loss": 0.5127,
      "step": 2159
    },
    {
      "epoch": 0.8643457382953181,
      "grad_norm": 0.10645811259746552,
      "learning_rate": 2.7185244587008824e-05,
      "loss": 0.4968,
      "step": 2160
    },
    {
      "epoch": 0.8647458983593438,
      "grad_norm": 0.10690116137266159,
      "learning_rate": 2.7105052125100245e-05,
      "loss": 0.4827,
      "step": 2161
    },
    {
      "epoch": 0.8651460584233693,
      "grad_norm": 0.07426075637340546,
      "learning_rate": 2.702485966319166e-05,
      "loss": 0.3972,
      "step": 2162
    },
    {
      "epoch": 0.865546218487395,
      "grad_norm": 0.09564604610204697,
      "learning_rate": 2.694466720128308e-05,
      "loss": 0.4588,
      "step": 2163
    },
    {
      "epoch": 0.8659463785514205,
      "grad_norm": 0.10965223610401154,
      "learning_rate": 2.68644747393745e-05,
      "loss": 0.5104,
      "step": 2164
    },
    {
      "epoch": 0.8663465386154462,
      "grad_norm": 0.08356994390487671,
      "learning_rate": 2.678428227746592e-05,
      "loss": 0.401,
      "step": 2165
    },
    {
      "epoch": 0.8667466986794717,
      "grad_norm": 0.1072864904999733,
      "learning_rate": 2.6704089815557336e-05,
      "loss": 0.449,
      "step": 2166
    },
    {
      "epoch": 0.8671468587434974,
      "grad_norm": 0.0912882536649704,
      "learning_rate": 2.6623897353648757e-05,
      "loss": 0.4495,
      "step": 2167
    },
    {
      "epoch": 0.867547018807523,
      "grad_norm": 0.10527558624744415,
      "learning_rate": 2.654370489174018e-05,
      "loss": 0.5145,
      "step": 2168
    },
    {
      "epoch": 0.8679471788715486,
      "grad_norm": 0.09220767766237259,
      "learning_rate": 2.64635124298316e-05,
      "loss": 0.5103,
      "step": 2169
    },
    {
      "epoch": 0.8683473389355743,
      "grad_norm": 0.10843947529792786,
      "learning_rate": 2.6383319967923015e-05,
      "loss": 0.4524,
      "step": 2170
    },
    {
      "epoch": 0.8687474989995998,
      "grad_norm": 0.11258033663034439,
      "learning_rate": 2.6303127506014436e-05,
      "loss": 0.5593,
      "step": 2171
    },
    {
      "epoch": 0.8691476590636255,
      "grad_norm": 0.08891855180263519,
      "learning_rate": 2.6222935044105858e-05,
      "loss": 0.421,
      "step": 2172
    },
    {
      "epoch": 0.8695478191276511,
      "grad_norm": 0.08735103160142899,
      "learning_rate": 2.6142742582197276e-05,
      "loss": 0.4754,
      "step": 2173
    },
    {
      "epoch": 0.8699479791916767,
      "grad_norm": 0.10226095467805862,
      "learning_rate": 2.6062550120288694e-05,
      "loss": 0.5059,
      "step": 2174
    },
    {
      "epoch": 0.8703481392557023,
      "grad_norm": 0.09823039919137955,
      "learning_rate": 2.5982357658380112e-05,
      "loss": 0.4612,
      "step": 2175
    },
    {
      "epoch": 0.8707482993197279,
      "grad_norm": 0.11733551323413849,
      "learning_rate": 2.5902165196471533e-05,
      "loss": 0.4674,
      "step": 2176
    },
    {
      "epoch": 0.8711484593837535,
      "grad_norm": 0.09875968098640442,
      "learning_rate": 2.5821972734562955e-05,
      "loss": 0.4765,
      "step": 2177
    },
    {
      "epoch": 0.8715486194477791,
      "grad_norm": 0.08798050135374069,
      "learning_rate": 2.574178027265437e-05,
      "loss": 0.5013,
      "step": 2178
    },
    {
      "epoch": 0.8719487795118047,
      "grad_norm": 0.09336337447166443,
      "learning_rate": 2.566158781074579e-05,
      "loss": 0.464,
      "step": 2179
    },
    {
      "epoch": 0.8723489395758304,
      "grad_norm": 0.1101294606924057,
      "learning_rate": 2.5581395348837212e-05,
      "loss": 0.4523,
      "step": 2180
    },
    {
      "epoch": 0.8727490996398559,
      "grad_norm": 0.11833149939775467,
      "learning_rate": 2.5501202886928634e-05,
      "loss": 0.4081,
      "step": 2181
    },
    {
      "epoch": 0.8731492597038816,
      "grad_norm": 0.08864247798919678,
      "learning_rate": 2.542101042502005e-05,
      "loss": 0.453,
      "step": 2182
    },
    {
      "epoch": 0.8735494197679071,
      "grad_norm": 0.10173758119344711,
      "learning_rate": 2.5340817963111467e-05,
      "loss": 0.4943,
      "step": 2183
    },
    {
      "epoch": 0.8739495798319328,
      "grad_norm": 0.08351060003042221,
      "learning_rate": 2.5260625501202888e-05,
      "loss": 0.4158,
      "step": 2184
    },
    {
      "epoch": 0.8743497398959584,
      "grad_norm": 0.0897316113114357,
      "learning_rate": 2.518043303929431e-05,
      "loss": 0.444,
      "step": 2185
    },
    {
      "epoch": 0.874749899959984,
      "grad_norm": 0.09738361090421677,
      "learning_rate": 2.5100240577385724e-05,
      "loss": 0.4927,
      "step": 2186
    },
    {
      "epoch": 0.8751500600240096,
      "grad_norm": 0.09995218366384506,
      "learning_rate": 2.5020048115477146e-05,
      "loss": 0.4775,
      "step": 2187
    },
    {
      "epoch": 0.8755502200880352,
      "grad_norm": 0.10455207526683807,
      "learning_rate": 2.4939855653568567e-05,
      "loss": 0.5044,
      "step": 2188
    },
    {
      "epoch": 0.8759503801520608,
      "grad_norm": 0.0982242003083229,
      "learning_rate": 2.4859663191659985e-05,
      "loss": 0.4385,
      "step": 2189
    },
    {
      "epoch": 0.8763505402160864,
      "grad_norm": 0.10063540935516357,
      "learning_rate": 2.4779470729751407e-05,
      "loss": 0.417,
      "step": 2190
    },
    {
      "epoch": 0.876750700280112,
      "grad_norm": 0.09408817440271378,
      "learning_rate": 2.4699278267842825e-05,
      "loss": 0.468,
      "step": 2191
    },
    {
      "epoch": 0.8771508603441377,
      "grad_norm": 0.09470509737730026,
      "learning_rate": 2.4619085805934243e-05,
      "loss": 0.3863,
      "step": 2192
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 0.08904016017913818,
      "learning_rate": 2.453889334402566e-05,
      "loss": 0.4831,
      "step": 2193
    },
    {
      "epoch": 0.8779511804721889,
      "grad_norm": 0.09272145479917526,
      "learning_rate": 2.4458700882117083e-05,
      "loss": 0.4096,
      "step": 2194
    },
    {
      "epoch": 0.8783513405362144,
      "grad_norm": 0.08221037685871124,
      "learning_rate": 2.43785084202085e-05,
      "loss": 0.4293,
      "step": 2195
    },
    {
      "epoch": 0.8787515006002401,
      "grad_norm": 0.11998064815998077,
      "learning_rate": 2.4298315958299922e-05,
      "loss": 0.4886,
      "step": 2196
    },
    {
      "epoch": 0.8791516606642658,
      "grad_norm": 0.08443481475114822,
      "learning_rate": 2.421812349639134e-05,
      "loss": 0.4797,
      "step": 2197
    },
    {
      "epoch": 0.8795518207282913,
      "grad_norm": 0.10046087205410004,
      "learning_rate": 2.413793103448276e-05,
      "loss": 0.4472,
      "step": 2198
    },
    {
      "epoch": 0.879951980792317,
      "grad_norm": 0.10289169102907181,
      "learning_rate": 2.405773857257418e-05,
      "loss": 0.5316,
      "step": 2199
    },
    {
      "epoch": 0.8803521408563425,
      "grad_norm": 0.11156323552131653,
      "learning_rate": 2.39775461106656e-05,
      "loss": 0.4602,
      "step": 2200
    },
    {
      "epoch": 0.8807523009203682,
      "grad_norm": 0.07554008066654205,
      "learning_rate": 2.389735364875702e-05,
      "loss": 0.3925,
      "step": 2201
    },
    {
      "epoch": 0.8811524609843937,
      "grad_norm": 0.09082119166851044,
      "learning_rate": 2.3817161186848437e-05,
      "loss": 0.4471,
      "step": 2202
    },
    {
      "epoch": 0.8815526210484194,
      "grad_norm": 0.10386975109577179,
      "learning_rate": 2.3736968724939855e-05,
      "loss": 0.4791,
      "step": 2203
    },
    {
      "epoch": 0.881952781112445,
      "grad_norm": 0.10559950023889542,
      "learning_rate": 2.3656776263031277e-05,
      "loss": 0.4603,
      "step": 2204
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 0.08940492570400238,
      "learning_rate": 2.3576583801122695e-05,
      "loss": 0.5098,
      "step": 2205
    },
    {
      "epoch": 0.8827531012404962,
      "grad_norm": 0.11890259385108948,
      "learning_rate": 2.3496391339214116e-05,
      "loss": 0.4651,
      "step": 2206
    },
    {
      "epoch": 0.8831532613045218,
      "grad_norm": 0.09768911451101303,
      "learning_rate": 2.3416198877305535e-05,
      "loss": 0.4774,
      "step": 2207
    },
    {
      "epoch": 0.8835534213685474,
      "grad_norm": 0.07806241512298584,
      "learning_rate": 2.3336006415396956e-05,
      "loss": 0.3971,
      "step": 2208
    },
    {
      "epoch": 0.8839535814325731,
      "grad_norm": 0.11299318075180054,
      "learning_rate": 2.3255813953488374e-05,
      "loss": 0.456,
      "step": 2209
    },
    {
      "epoch": 0.8843537414965986,
      "grad_norm": 0.10790801793336868,
      "learning_rate": 2.3175621491579792e-05,
      "loss": 0.5258,
      "step": 2210
    },
    {
      "epoch": 0.8847539015606243,
      "grad_norm": 0.12081506103277206,
      "learning_rate": 2.309542902967121e-05,
      "loss": 0.5412,
      "step": 2211
    },
    {
      "epoch": 0.8851540616246498,
      "grad_norm": 0.094133161008358,
      "learning_rate": 2.3015236567762632e-05,
      "loss": 0.5132,
      "step": 2212
    },
    {
      "epoch": 0.8855542216886755,
      "grad_norm": 0.09100306034088135,
      "learning_rate": 2.293504410585405e-05,
      "loss": 0.4385,
      "step": 2213
    },
    {
      "epoch": 0.885954381752701,
      "grad_norm": 0.07943055778741837,
      "learning_rate": 2.285485164394547e-05,
      "loss": 0.3843,
      "step": 2214
    },
    {
      "epoch": 0.8863545418167267,
      "grad_norm": 0.10440155118703842,
      "learning_rate": 2.277465918203689e-05,
      "loss": 0.5285,
      "step": 2215
    },
    {
      "epoch": 0.8867547018807523,
      "grad_norm": 0.07944213598966599,
      "learning_rate": 2.269446672012831e-05,
      "loss": 0.4091,
      "step": 2216
    },
    {
      "epoch": 0.8871548619447779,
      "grad_norm": 0.07587409019470215,
      "learning_rate": 2.261427425821973e-05,
      "loss": 0.3815,
      "step": 2217
    },
    {
      "epoch": 0.8875550220088035,
      "grad_norm": 0.10367178916931152,
      "learning_rate": 2.253408179631115e-05,
      "loss": 0.5819,
      "step": 2218
    },
    {
      "epoch": 0.8879551820728291,
      "grad_norm": 0.08865367621183395,
      "learning_rate": 2.245388933440257e-05,
      "loss": 0.4436,
      "step": 2219
    },
    {
      "epoch": 0.8883553421368547,
      "grad_norm": 0.09487404674291611,
      "learning_rate": 2.2373696872493987e-05,
      "loss": 0.4517,
      "step": 2220
    },
    {
      "epoch": 0.8887555022008804,
      "grad_norm": 0.06989588588476181,
      "learning_rate": 2.2293504410585405e-05,
      "loss": 0.3641,
      "step": 2221
    },
    {
      "epoch": 0.8891556622649059,
      "grad_norm": 0.0952228382229805,
      "learning_rate": 2.2213311948676826e-05,
      "loss": 0.4646,
      "step": 2222
    },
    {
      "epoch": 0.8895558223289316,
      "grad_norm": 0.08837955445051193,
      "learning_rate": 2.2133119486768244e-05,
      "loss": 0.3581,
      "step": 2223
    },
    {
      "epoch": 0.8899559823929571,
      "grad_norm": 0.09745553135871887,
      "learning_rate": 2.2052927024859666e-05,
      "loss": 0.4949,
      "step": 2224
    },
    {
      "epoch": 0.8903561424569828,
      "grad_norm": 0.08501418679952621,
      "learning_rate": 2.1972734562951084e-05,
      "loss": 0.4202,
      "step": 2225
    },
    {
      "epoch": 0.8907563025210085,
      "grad_norm": 0.1015038788318634,
      "learning_rate": 2.1892542101042505e-05,
      "loss": 0.4955,
      "step": 2226
    },
    {
      "epoch": 0.891156462585034,
      "grad_norm": 0.11300816386938095,
      "learning_rate": 2.1812349639133923e-05,
      "loss": 0.5206,
      "step": 2227
    },
    {
      "epoch": 0.8915566226490597,
      "grad_norm": 0.09584014862775803,
      "learning_rate": 2.173215717722534e-05,
      "loss": 0.4988,
      "step": 2228
    },
    {
      "epoch": 0.8919567827130852,
      "grad_norm": 0.11015225201845169,
      "learning_rate": 2.165196471531676e-05,
      "loss": 0.5274,
      "step": 2229
    },
    {
      "epoch": 0.8923569427771109,
      "grad_norm": 0.09016480296850204,
      "learning_rate": 2.157177225340818e-05,
      "loss": 0.4753,
      "step": 2230
    },
    {
      "epoch": 0.8927571028411364,
      "grad_norm": 0.0896129459142685,
      "learning_rate": 2.14915797914996e-05,
      "loss": 0.5309,
      "step": 2231
    },
    {
      "epoch": 0.8931572629051621,
      "grad_norm": 0.08326588571071625,
      "learning_rate": 2.141138732959102e-05,
      "loss": 0.3618,
      "step": 2232
    },
    {
      "epoch": 0.8935574229691877,
      "grad_norm": 0.10458891838788986,
      "learning_rate": 2.133119486768244e-05,
      "loss": 0.4898,
      "step": 2233
    },
    {
      "epoch": 0.8939575830332133,
      "grad_norm": 0.08909368515014648,
      "learning_rate": 2.125100240577386e-05,
      "loss": 0.4383,
      "step": 2234
    },
    {
      "epoch": 0.8943577430972389,
      "grad_norm": 0.096866674721241,
      "learning_rate": 2.1170809943865278e-05,
      "loss": 0.5191,
      "step": 2235
    },
    {
      "epoch": 0.8947579031612645,
      "grad_norm": 0.08889336884021759,
      "learning_rate": 2.10906174819567e-05,
      "loss": 0.4174,
      "step": 2236
    },
    {
      "epoch": 0.8951580632252901,
      "grad_norm": 0.09671428054571152,
      "learning_rate": 2.1010425020048118e-05,
      "loss": 0.5242,
      "step": 2237
    },
    {
      "epoch": 0.8955582232893158,
      "grad_norm": 0.09569836407899857,
      "learning_rate": 2.0930232558139536e-05,
      "loss": 0.5374,
      "step": 2238
    },
    {
      "epoch": 0.8959583833533413,
      "grad_norm": 0.093319833278656,
      "learning_rate": 2.0850040096230954e-05,
      "loss": 0.461,
      "step": 2239
    },
    {
      "epoch": 0.896358543417367,
      "grad_norm": 0.11211418360471725,
      "learning_rate": 2.0769847634322375e-05,
      "loss": 0.5085,
      "step": 2240
    },
    {
      "epoch": 0.8967587034813925,
      "grad_norm": 0.0998154729604721,
      "learning_rate": 2.0689655172413793e-05,
      "loss": 0.456,
      "step": 2241
    },
    {
      "epoch": 0.8971588635454182,
      "grad_norm": 0.09046145528554916,
      "learning_rate": 2.0609462710505215e-05,
      "loss": 0.5026,
      "step": 2242
    },
    {
      "epoch": 0.8975590236094437,
      "grad_norm": 0.09721292555332184,
      "learning_rate": 2.0529270248596633e-05,
      "loss": 0.4942,
      "step": 2243
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 0.0978165715932846,
      "learning_rate": 2.0449077786688054e-05,
      "loss": 0.4756,
      "step": 2244
    },
    {
      "epoch": 0.898359343737495,
      "grad_norm": 0.08653675764799118,
      "learning_rate": 2.0368885324779472e-05,
      "loss": 0.3845,
      "step": 2245
    },
    {
      "epoch": 0.8987595038015206,
      "grad_norm": 0.11152290552854538,
      "learning_rate": 2.0288692862870894e-05,
      "loss": 0.5939,
      "step": 2246
    },
    {
      "epoch": 0.8991596638655462,
      "grad_norm": 0.08665372431278229,
      "learning_rate": 2.0208500400962312e-05,
      "loss": 0.4433,
      "step": 2247
    },
    {
      "epoch": 0.8995598239295718,
      "grad_norm": 0.09360811114311218,
      "learning_rate": 2.012830793905373e-05,
      "loss": 0.5371,
      "step": 2248
    },
    {
      "epoch": 0.8999599839935974,
      "grad_norm": 0.07406190037727356,
      "learning_rate": 2.0048115477145148e-05,
      "loss": 0.3984,
      "step": 2249
    },
    {
      "epoch": 0.9003601440576231,
      "grad_norm": 0.09380578249692917,
      "learning_rate": 1.996792301523657e-05,
      "loss": 0.4981,
      "step": 2250
    },
    {
      "epoch": 0.9007603041216486,
      "grad_norm": 0.09659907966852188,
      "learning_rate": 1.9887730553327988e-05,
      "loss": 0.4176,
      "step": 2251
    },
    {
      "epoch": 0.9011604641856743,
      "grad_norm": 0.08754938840866089,
      "learning_rate": 1.980753809141941e-05,
      "loss": 0.4524,
      "step": 2252
    },
    {
      "epoch": 0.9015606242496998,
      "grad_norm": 0.07460085302591324,
      "learning_rate": 1.9727345629510827e-05,
      "loss": 0.4132,
      "step": 2253
    },
    {
      "epoch": 0.9019607843137255,
      "grad_norm": 0.08767298609018326,
      "learning_rate": 1.964715316760225e-05,
      "loss": 0.4988,
      "step": 2254
    },
    {
      "epoch": 0.902360944377751,
      "grad_norm": 0.090704545378685,
      "learning_rate": 1.9566960705693667e-05,
      "loss": 0.4325,
      "step": 2255
    },
    {
      "epoch": 0.9027611044417767,
      "grad_norm": 0.09926063567399979,
      "learning_rate": 1.9486768243785085e-05,
      "loss": 0.4398,
      "step": 2256
    },
    {
      "epoch": 0.9031612645058024,
      "grad_norm": 0.08979697525501251,
      "learning_rate": 1.9406575781876503e-05,
      "loss": 0.4125,
      "step": 2257
    },
    {
      "epoch": 0.9035614245698279,
      "grad_norm": 0.10767213255167007,
      "learning_rate": 1.9326383319967924e-05,
      "loss": 0.5211,
      "step": 2258
    },
    {
      "epoch": 0.9039615846338536,
      "grad_norm": 0.07611598074436188,
      "learning_rate": 1.9246190858059342e-05,
      "loss": 0.4439,
      "step": 2259
    },
    {
      "epoch": 0.9043617446978791,
      "grad_norm": 0.09530066698789597,
      "learning_rate": 1.9165998396150764e-05,
      "loss": 0.4665,
      "step": 2260
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.0927698016166687,
      "learning_rate": 1.9085805934242182e-05,
      "loss": 0.5148,
      "step": 2261
    },
    {
      "epoch": 0.9051620648259304,
      "grad_norm": 0.10132893174886703,
      "learning_rate": 1.9005613472333603e-05,
      "loss": 0.4348,
      "step": 2262
    },
    {
      "epoch": 0.905562224889956,
      "grad_norm": 0.08645711839199066,
      "learning_rate": 1.892542101042502e-05,
      "loss": 0.5062,
      "step": 2263
    },
    {
      "epoch": 0.9059623849539816,
      "grad_norm": 0.09333771467208862,
      "learning_rate": 1.8845228548516443e-05,
      "loss": 0.398,
      "step": 2264
    },
    {
      "epoch": 0.9063625450180072,
      "grad_norm": 0.08369898796081543,
      "learning_rate": 1.876503608660786e-05,
      "loss": 0.3937,
      "step": 2265
    },
    {
      "epoch": 0.9067627050820328,
      "grad_norm": 0.09531933814287186,
      "learning_rate": 1.868484362469928e-05,
      "loss": 0.4455,
      "step": 2266
    },
    {
      "epoch": 0.9071628651460584,
      "grad_norm": 0.09971001744270325,
      "learning_rate": 1.8604651162790697e-05,
      "loss": 0.5156,
      "step": 2267
    },
    {
      "epoch": 0.907563025210084,
      "grad_norm": 0.08828013390302658,
      "learning_rate": 1.852445870088212e-05,
      "loss": 0.4841,
      "step": 2268
    },
    {
      "epoch": 0.9079631852741097,
      "grad_norm": 0.09414661675691605,
      "learning_rate": 1.8444266238973537e-05,
      "loss": 0.4869,
      "step": 2269
    },
    {
      "epoch": 0.9083633453381352,
      "grad_norm": 0.09154659509658813,
      "learning_rate": 1.8364073777064958e-05,
      "loss": 0.4872,
      "step": 2270
    },
    {
      "epoch": 0.9087635054021609,
      "grad_norm": 0.08446275442838669,
      "learning_rate": 1.8283881315156376e-05,
      "loss": 0.481,
      "step": 2271
    },
    {
      "epoch": 0.9091636654661864,
      "grad_norm": 0.08994998782873154,
      "learning_rate": 1.8203688853247798e-05,
      "loss": 0.493,
      "step": 2272
    },
    {
      "epoch": 0.9095638255302121,
      "grad_norm": 0.13439157605171204,
      "learning_rate": 1.8123496391339216e-05,
      "loss": 0.4912,
      "step": 2273
    },
    {
      "epoch": 0.9099639855942377,
      "grad_norm": 0.09879519790410995,
      "learning_rate": 1.8043303929430634e-05,
      "loss": 0.5141,
      "step": 2274
    },
    {
      "epoch": 0.9103641456582633,
      "grad_norm": 0.1053324043750763,
      "learning_rate": 1.7963111467522052e-05,
      "loss": 0.3628,
      "step": 2275
    },
    {
      "epoch": 0.910764305722289,
      "grad_norm": 0.0980721265077591,
      "learning_rate": 1.7882919005613473e-05,
      "loss": 0.481,
      "step": 2276
    },
    {
      "epoch": 0.9111644657863145,
      "grad_norm": 0.10701952874660492,
      "learning_rate": 1.780272654370489e-05,
      "loss": 0.498,
      "step": 2277
    },
    {
      "epoch": 0.9115646258503401,
      "grad_norm": 0.09664370864629745,
      "learning_rate": 1.7722534081796313e-05,
      "loss": 0.3888,
      "step": 2278
    },
    {
      "epoch": 0.9119647859143657,
      "grad_norm": 0.08241278678178787,
      "learning_rate": 1.764234161988773e-05,
      "loss": 0.3582,
      "step": 2279
    },
    {
      "epoch": 0.9123649459783914,
      "grad_norm": 0.10851865261793137,
      "learning_rate": 1.7562149157979153e-05,
      "loss": 0.5162,
      "step": 2280
    },
    {
      "epoch": 0.912765106042417,
      "grad_norm": 0.07890085875988007,
      "learning_rate": 1.748195669607057e-05,
      "loss": 0.3855,
      "step": 2281
    },
    {
      "epoch": 0.9131652661064426,
      "grad_norm": 0.10516287386417389,
      "learning_rate": 1.7401764234161992e-05,
      "loss": 0.5621,
      "step": 2282
    },
    {
      "epoch": 0.9135654261704682,
      "grad_norm": 0.09220366179943085,
      "learning_rate": 1.732157177225341e-05,
      "loss": 0.5081,
      "step": 2283
    },
    {
      "epoch": 0.9139655862344938,
      "grad_norm": 0.100623719394207,
      "learning_rate": 1.7241379310344828e-05,
      "loss": 0.4029,
      "step": 2284
    },
    {
      "epoch": 0.9143657462985194,
      "grad_norm": 0.10238193720579147,
      "learning_rate": 1.7161186848436246e-05,
      "loss": 0.5036,
      "step": 2285
    },
    {
      "epoch": 0.9147659063625451,
      "grad_norm": 0.12076060473918915,
      "learning_rate": 1.7080994386527668e-05,
      "loss": 0.5141,
      "step": 2286
    },
    {
      "epoch": 0.9151660664265706,
      "grad_norm": 0.08249957114458084,
      "learning_rate": 1.7000801924619086e-05,
      "loss": 0.4078,
      "step": 2287
    },
    {
      "epoch": 0.9155662264905963,
      "grad_norm": 0.08202573657035828,
      "learning_rate": 1.6920609462710507e-05,
      "loss": 0.4032,
      "step": 2288
    },
    {
      "epoch": 0.9159663865546218,
      "grad_norm": 0.09022572636604309,
      "learning_rate": 1.6840417000801925e-05,
      "loss": 0.3889,
      "step": 2289
    },
    {
      "epoch": 0.9163665466186475,
      "grad_norm": 0.07979968935251236,
      "learning_rate": 1.6760224538893347e-05,
      "loss": 0.4395,
      "step": 2290
    },
    {
      "epoch": 0.916766706682673,
      "grad_norm": 0.10389071702957153,
      "learning_rate": 1.6680032076984765e-05,
      "loss": 0.4754,
      "step": 2291
    },
    {
      "epoch": 0.9171668667466987,
      "grad_norm": 0.10309845954179764,
      "learning_rate": 1.6599839615076186e-05,
      "loss": 0.5112,
      "step": 2292
    },
    {
      "epoch": 0.9175670268107243,
      "grad_norm": 0.09457187354564667,
      "learning_rate": 1.65196471531676e-05,
      "loss": 0.4617,
      "step": 2293
    },
    {
      "epoch": 0.9179671868747499,
      "grad_norm": 0.10361681878566742,
      "learning_rate": 1.6439454691259023e-05,
      "loss": 0.5114,
      "step": 2294
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 0.08390287309885025,
      "learning_rate": 1.635926222935044e-05,
      "loss": 0.4882,
      "step": 2295
    },
    {
      "epoch": 0.9187675070028011,
      "grad_norm": 0.10741320997476578,
      "learning_rate": 1.6279069767441862e-05,
      "loss": 0.5182,
      "step": 2296
    },
    {
      "epoch": 0.9191676670668267,
      "grad_norm": 0.09869121760129929,
      "learning_rate": 1.619887730553328e-05,
      "loss": 0.4951,
      "step": 2297
    },
    {
      "epoch": 0.9195678271308524,
      "grad_norm": 0.1147468239068985,
      "learning_rate": 1.6118684843624702e-05,
      "loss": 0.512,
      "step": 2298
    },
    {
      "epoch": 0.9199679871948779,
      "grad_norm": 0.07200605422258377,
      "learning_rate": 1.603849238171612e-05,
      "loss": 0.3947,
      "step": 2299
    },
    {
      "epoch": 0.9203681472589036,
      "grad_norm": 0.08945031464099884,
      "learning_rate": 1.595829991980754e-05,
      "loss": 0.4411,
      "step": 2300
    },
    {
      "epoch": 0.9207683073229291,
      "grad_norm": 0.0887795239686966,
      "learning_rate": 1.587810745789896e-05,
      "loss": 0.4041,
      "step": 2301
    },
    {
      "epoch": 0.9211684673869548,
      "grad_norm": 0.10823703557252884,
      "learning_rate": 1.5797914995990377e-05,
      "loss": 0.5402,
      "step": 2302
    },
    {
      "epoch": 0.9215686274509803,
      "grad_norm": 0.07779581099748611,
      "learning_rate": 1.5717722534081796e-05,
      "loss": 0.4032,
      "step": 2303
    },
    {
      "epoch": 0.921968787515006,
      "grad_norm": 0.09646649658679962,
      "learning_rate": 1.5637530072173217e-05,
      "loss": 0.5038,
      "step": 2304
    },
    {
      "epoch": 0.9223689475790317,
      "grad_norm": 0.09207645803689957,
      "learning_rate": 1.5557337610264635e-05,
      "loss": 0.4774,
      "step": 2305
    },
    {
      "epoch": 0.9227691076430572,
      "grad_norm": 0.0969780907034874,
      "learning_rate": 1.5477145148356057e-05,
      "loss": 0.4769,
      "step": 2306
    },
    {
      "epoch": 0.9231692677070829,
      "grad_norm": 0.10397428274154663,
      "learning_rate": 1.5396952686447475e-05,
      "loss": 0.5233,
      "step": 2307
    },
    {
      "epoch": 0.9235694277711084,
      "grad_norm": 0.0971587747335434,
      "learning_rate": 1.5316760224538896e-05,
      "loss": 0.5395,
      "step": 2308
    },
    {
      "epoch": 0.923969587835134,
      "grad_norm": 0.10797533392906189,
      "learning_rate": 1.5236567762630312e-05,
      "loss": 0.4083,
      "step": 2309
    },
    {
      "epoch": 0.9243697478991597,
      "grad_norm": 0.09441357105970383,
      "learning_rate": 1.5156375300721734e-05,
      "loss": 0.3707,
      "step": 2310
    },
    {
      "epoch": 0.9247699079631853,
      "grad_norm": 0.09610941261053085,
      "learning_rate": 1.5076182838813152e-05,
      "loss": 0.4642,
      "step": 2311
    },
    {
      "epoch": 0.9251700680272109,
      "grad_norm": 0.1022442951798439,
      "learning_rate": 1.4995990376904573e-05,
      "loss": 0.5825,
      "step": 2312
    },
    {
      "epoch": 0.9255702280912365,
      "grad_norm": 0.08159589767456055,
      "learning_rate": 1.4915797914995992e-05,
      "loss": 0.3807,
      "step": 2313
    },
    {
      "epoch": 0.9259703881552621,
      "grad_norm": 0.08876026421785355,
      "learning_rate": 1.4835605453087411e-05,
      "loss": 0.4508,
      "step": 2314
    },
    {
      "epoch": 0.9263705482192878,
      "grad_norm": 0.09888941049575806,
      "learning_rate": 1.475541299117883e-05,
      "loss": 0.4176,
      "step": 2315
    },
    {
      "epoch": 0.9267707082833133,
      "grad_norm": 0.07580701261758804,
      "learning_rate": 1.4675220529270251e-05,
      "loss": 0.3833,
      "step": 2316
    },
    {
      "epoch": 0.927170868347339,
      "grad_norm": 0.09311798959970474,
      "learning_rate": 1.4595028067361669e-05,
      "loss": 0.4417,
      "step": 2317
    },
    {
      "epoch": 0.9275710284113645,
      "grad_norm": 0.07682865113019943,
      "learning_rate": 1.4514835605453089e-05,
      "loss": 0.3865,
      "step": 2318
    },
    {
      "epoch": 0.9279711884753902,
      "grad_norm": 0.10243681818246841,
      "learning_rate": 1.4434643143544507e-05,
      "loss": 0.4277,
      "step": 2319
    },
    {
      "epoch": 0.9283713485394157,
      "grad_norm": 0.11251744627952576,
      "learning_rate": 1.4354450681635928e-05,
      "loss": 0.5574,
      "step": 2320
    },
    {
      "epoch": 0.9287715086034414,
      "grad_norm": 0.101315438747406,
      "learning_rate": 1.4274258219727346e-05,
      "loss": 0.4934,
      "step": 2321
    },
    {
      "epoch": 0.929171668667467,
      "grad_norm": 0.0857962891459465,
      "learning_rate": 1.4194065757818766e-05,
      "loss": 0.4399,
      "step": 2322
    },
    {
      "epoch": 0.9295718287314926,
      "grad_norm": 0.08175299316644669,
      "learning_rate": 1.4113873295910184e-05,
      "loss": 0.4486,
      "step": 2323
    },
    {
      "epoch": 0.9299719887955182,
      "grad_norm": 0.07668652385473251,
      "learning_rate": 1.4033680834001606e-05,
      "loss": 0.4273,
      "step": 2324
    },
    {
      "epoch": 0.9303721488595438,
      "grad_norm": 0.09983833134174347,
      "learning_rate": 1.3953488372093024e-05,
      "loss": 0.5212,
      "step": 2325
    },
    {
      "epoch": 0.9307723089235694,
      "grad_norm": 0.08992517739534378,
      "learning_rate": 1.3873295910184445e-05,
      "loss": 0.3696,
      "step": 2326
    },
    {
      "epoch": 0.9311724689875951,
      "grad_norm": 0.10551674664020538,
      "learning_rate": 1.3793103448275863e-05,
      "loss": 0.4998,
      "step": 2327
    },
    {
      "epoch": 0.9315726290516206,
      "grad_norm": 0.09811422973871231,
      "learning_rate": 1.3712910986367283e-05,
      "loss": 0.3543,
      "step": 2328
    },
    {
      "epoch": 0.9319727891156463,
      "grad_norm": 0.11980956792831421,
      "learning_rate": 1.3632718524458701e-05,
      "loss": 0.5358,
      "step": 2329
    },
    {
      "epoch": 0.9323729491796718,
      "grad_norm": 0.0958748534321785,
      "learning_rate": 1.3552526062550123e-05,
      "loss": 0.429,
      "step": 2330
    },
    {
      "epoch": 0.9327731092436975,
      "grad_norm": 0.08479239791631699,
      "learning_rate": 1.347233360064154e-05,
      "loss": 0.4222,
      "step": 2331
    },
    {
      "epoch": 0.933173269307723,
      "grad_norm": 0.09507296979427338,
      "learning_rate": 1.339214113873296e-05,
      "loss": 0.4412,
      "step": 2332
    },
    {
      "epoch": 0.9335734293717487,
      "grad_norm": 0.10904303193092346,
      "learning_rate": 1.3311948676824379e-05,
      "loss": 0.5125,
      "step": 2333
    },
    {
      "epoch": 0.9339735894357744,
      "grad_norm": 0.09506190568208694,
      "learning_rate": 1.32317562149158e-05,
      "loss": 0.4636,
      "step": 2334
    },
    {
      "epoch": 0.9343737494997999,
      "grad_norm": 0.09518192708492279,
      "learning_rate": 1.3151563753007218e-05,
      "loss": 0.4586,
      "step": 2335
    },
    {
      "epoch": 0.9347739095638256,
      "grad_norm": 0.08955838531255722,
      "learning_rate": 1.3071371291098638e-05,
      "loss": 0.4643,
      "step": 2336
    },
    {
      "epoch": 0.9351740696278511,
      "grad_norm": 0.08337748795747757,
      "learning_rate": 1.2991178829190056e-05,
      "loss": 0.4731,
      "step": 2337
    },
    {
      "epoch": 0.9355742296918768,
      "grad_norm": 0.09360156953334808,
      "learning_rate": 1.2910986367281477e-05,
      "loss": 0.5677,
      "step": 2338
    },
    {
      "epoch": 0.9359743897559024,
      "grad_norm": 0.10625862330198288,
      "learning_rate": 1.2830793905372896e-05,
      "loss": 0.4764,
      "step": 2339
    },
    {
      "epoch": 0.936374549819928,
      "grad_norm": 0.10076084733009338,
      "learning_rate": 1.2750601443464317e-05,
      "loss": 0.4693,
      "step": 2340
    },
    {
      "epoch": 0.9367747098839536,
      "grad_norm": 0.09132368862628937,
      "learning_rate": 1.2670408981555733e-05,
      "loss": 0.4722,
      "step": 2341
    },
    {
      "epoch": 0.9371748699479792,
      "grad_norm": 0.11533522605895996,
      "learning_rate": 1.2590216519647155e-05,
      "loss": 0.4425,
      "step": 2342
    },
    {
      "epoch": 0.9375750300120048,
      "grad_norm": 0.08223284780979156,
      "learning_rate": 1.2510024057738573e-05,
      "loss": 0.3974,
      "step": 2343
    },
    {
      "epoch": 0.9379751900760304,
      "grad_norm": 0.10524643212556839,
      "learning_rate": 1.2429831595829993e-05,
      "loss": 0.4536,
      "step": 2344
    },
    {
      "epoch": 0.938375350140056,
      "grad_norm": 0.08838672190904617,
      "learning_rate": 1.2349639133921412e-05,
      "loss": 0.4568,
      "step": 2345
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 0.09843185544013977,
      "learning_rate": 1.226944667201283e-05,
      "loss": 0.5379,
      "step": 2346
    },
    {
      "epoch": 0.9391756702681072,
      "grad_norm": 0.09502602368593216,
      "learning_rate": 1.218925421010425e-05,
      "loss": 0.48,
      "step": 2347
    },
    {
      "epoch": 0.9395758303321329,
      "grad_norm": 0.08844342082738876,
      "learning_rate": 1.210906174819567e-05,
      "loss": 0.4459,
      "step": 2348
    },
    {
      "epoch": 0.9399759903961584,
      "grad_norm": 0.08113285154104233,
      "learning_rate": 1.202886928628709e-05,
      "loss": 0.3764,
      "step": 2349
    },
    {
      "epoch": 0.9403761504601841,
      "grad_norm": 0.10864038020372391,
      "learning_rate": 1.194867682437851e-05,
      "loss": 0.5145,
      "step": 2350
    },
    {
      "epoch": 0.9407763105242097,
      "grad_norm": 0.105216845870018,
      "learning_rate": 1.1868484362469928e-05,
      "loss": 0.4776,
      "step": 2351
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 0.09196165949106216,
      "learning_rate": 1.1788291900561347e-05,
      "loss": 0.4424,
      "step": 2352
    },
    {
      "epoch": 0.9415766306522609,
      "grad_norm": 0.09106648713350296,
      "learning_rate": 1.1708099438652767e-05,
      "loss": 0.4671,
      "step": 2353
    },
    {
      "epoch": 0.9419767907162865,
      "grad_norm": 0.10798854380846024,
      "learning_rate": 1.1627906976744187e-05,
      "loss": 0.6098,
      "step": 2354
    },
    {
      "epoch": 0.9423769507803121,
      "grad_norm": 0.08751384913921356,
      "learning_rate": 1.1547714514835605e-05,
      "loss": 0.3205,
      "step": 2355
    },
    {
      "epoch": 0.9427771108443377,
      "grad_norm": 0.08357823640108109,
      "learning_rate": 1.1467522052927025e-05,
      "loss": 0.4115,
      "step": 2356
    },
    {
      "epoch": 0.9431772709083633,
      "grad_norm": 0.07897672802209854,
      "learning_rate": 1.1387329591018445e-05,
      "loss": 0.3779,
      "step": 2357
    },
    {
      "epoch": 0.943577430972389,
      "grad_norm": 0.09094276279211044,
      "learning_rate": 1.1307137129109864e-05,
      "loss": 0.4135,
      "step": 2358
    },
    {
      "epoch": 0.9439775910364145,
      "grad_norm": 0.11639294773340225,
      "learning_rate": 1.1226944667201284e-05,
      "loss": 0.5012,
      "step": 2359
    },
    {
      "epoch": 0.9443777511004402,
      "grad_norm": 0.09616521000862122,
      "learning_rate": 1.1146752205292702e-05,
      "loss": 0.4442,
      "step": 2360
    },
    {
      "epoch": 0.9447779111644657,
      "grad_norm": 0.08624470978975296,
      "learning_rate": 1.1066559743384122e-05,
      "loss": 0.4019,
      "step": 2361
    },
    {
      "epoch": 0.9451780712284914,
      "grad_norm": 0.07693489640951157,
      "learning_rate": 1.0986367281475542e-05,
      "loss": 0.4354,
      "step": 2362
    },
    {
      "epoch": 0.9455782312925171,
      "grad_norm": 0.11081277579069138,
      "learning_rate": 1.0906174819566962e-05,
      "loss": 0.5351,
      "step": 2363
    },
    {
      "epoch": 0.9459783913565426,
      "grad_norm": 0.09058383852243423,
      "learning_rate": 1.082598235765838e-05,
      "loss": 0.4533,
      "step": 2364
    },
    {
      "epoch": 0.9463785514205683,
      "grad_norm": 0.09106536209583282,
      "learning_rate": 1.07457898957498e-05,
      "loss": 0.4266,
      "step": 2365
    },
    {
      "epoch": 0.9467787114845938,
      "grad_norm": 0.09246557950973511,
      "learning_rate": 1.066559743384122e-05,
      "loss": 0.497,
      "step": 2366
    },
    {
      "epoch": 0.9471788715486195,
      "grad_norm": 0.10802585631608963,
      "learning_rate": 1.0585404971932639e-05,
      "loss": 0.4776,
      "step": 2367
    },
    {
      "epoch": 0.947579031612645,
      "grad_norm": 0.10068293660879135,
      "learning_rate": 1.0505212510024059e-05,
      "loss": 0.4698,
      "step": 2368
    },
    {
      "epoch": 0.9479791916766707,
      "grad_norm": 0.09290174394845963,
      "learning_rate": 1.0425020048115477e-05,
      "loss": 0.4617,
      "step": 2369
    },
    {
      "epoch": 0.9483793517406963,
      "grad_norm": 0.07910428941249847,
      "learning_rate": 1.0344827586206897e-05,
      "loss": 0.3811,
      "step": 2370
    },
    {
      "epoch": 0.9487795118047219,
      "grad_norm": 0.07512665539979935,
      "learning_rate": 1.0264635124298316e-05,
      "loss": 0.3922,
      "step": 2371
    },
    {
      "epoch": 0.9491796718687475,
      "grad_norm": 0.09164443612098694,
      "learning_rate": 1.0184442662389736e-05,
      "loss": 0.4511,
      "step": 2372
    },
    {
      "epoch": 0.9495798319327731,
      "grad_norm": 0.07988259941339493,
      "learning_rate": 1.0104250200481156e-05,
      "loss": 0.3923,
      "step": 2373
    },
    {
      "epoch": 0.9499799919967987,
      "grad_norm": 0.07463051378726959,
      "learning_rate": 1.0024057738572574e-05,
      "loss": 0.3722,
      "step": 2374
    },
    {
      "epoch": 0.9503801520608244,
      "grad_norm": 0.09415227919816971,
      "learning_rate": 9.943865276663994e-06,
      "loss": 0.4541,
      "step": 2375
    },
    {
      "epoch": 0.9507803121248499,
      "grad_norm": 0.09245305508375168,
      "learning_rate": 9.863672814755414e-06,
      "loss": 0.4749,
      "step": 2376
    },
    {
      "epoch": 0.9511804721888756,
      "grad_norm": 0.08743377029895782,
      "learning_rate": 9.783480352846833e-06,
      "loss": 0.4356,
      "step": 2377
    },
    {
      "epoch": 0.9515806322529011,
      "grad_norm": 0.09744677692651749,
      "learning_rate": 9.703287890938251e-06,
      "loss": 0.4368,
      "step": 2378
    },
    {
      "epoch": 0.9519807923169268,
      "grad_norm": 0.10428798943758011,
      "learning_rate": 9.623095429029671e-06,
      "loss": 0.543,
      "step": 2379
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.0996709018945694,
      "learning_rate": 9.542902967121091e-06,
      "loss": 0.4831,
      "step": 2380
    },
    {
      "epoch": 0.952781112444978,
      "grad_norm": 0.12000589072704315,
      "learning_rate": 9.46271050521251e-06,
      "loss": 0.5258,
      "step": 2381
    },
    {
      "epoch": 0.9531812725090036,
      "grad_norm": 0.09280874580144882,
      "learning_rate": 9.38251804330393e-06,
      "loss": 0.4446,
      "step": 2382
    },
    {
      "epoch": 0.9535814325730292,
      "grad_norm": 0.09969113767147064,
      "learning_rate": 9.302325581395349e-06,
      "loss": 0.468,
      "step": 2383
    },
    {
      "epoch": 0.9539815926370548,
      "grad_norm": 0.08732146769762039,
      "learning_rate": 9.222133119486768e-06,
      "loss": 0.479,
      "step": 2384
    },
    {
      "epoch": 0.9543817527010804,
      "grad_norm": 0.09383171051740646,
      "learning_rate": 9.141940657578188e-06,
      "loss": 0.4389,
      "step": 2385
    },
    {
      "epoch": 0.954781912765106,
      "grad_norm": 0.09067737311124802,
      "learning_rate": 9.061748195669608e-06,
      "loss": 0.4789,
      "step": 2386
    },
    {
      "epoch": 0.9551820728291317,
      "grad_norm": 0.09171104431152344,
      "learning_rate": 8.981555733761026e-06,
      "loss": 0.4463,
      "step": 2387
    },
    {
      "epoch": 0.9555822328931572,
      "grad_norm": 0.1051148772239685,
      "learning_rate": 8.901363271852446e-06,
      "loss": 0.4838,
      "step": 2388
    },
    {
      "epoch": 0.9559823929571829,
      "grad_norm": 0.09112934023141861,
      "learning_rate": 8.821170809943866e-06,
      "loss": 0.4545,
      "step": 2389
    },
    {
      "epoch": 0.9563825530212084,
      "grad_norm": 0.10854486376047134,
      "learning_rate": 8.740978348035285e-06,
      "loss": 0.4723,
      "step": 2390
    },
    {
      "epoch": 0.9567827130852341,
      "grad_norm": 0.11922795325517654,
      "learning_rate": 8.660785886126705e-06,
      "loss": 0.457,
      "step": 2391
    },
    {
      "epoch": 0.9571828731492596,
      "grad_norm": 0.09078501164913177,
      "learning_rate": 8.580593424218123e-06,
      "loss": 0.4522,
      "step": 2392
    },
    {
      "epoch": 0.9575830332132853,
      "grad_norm": 0.0986049696803093,
      "learning_rate": 8.500400962309543e-06,
      "loss": 0.4453,
      "step": 2393
    },
    {
      "epoch": 0.957983193277311,
      "grad_norm": 0.1022779569029808,
      "learning_rate": 8.420208500400963e-06,
      "loss": 0.5206,
      "step": 2394
    },
    {
      "epoch": 0.9583833533413365,
      "grad_norm": 0.10569363832473755,
      "learning_rate": 8.340016038492383e-06,
      "loss": 0.5122,
      "step": 2395
    },
    {
      "epoch": 0.9587835134053622,
      "grad_norm": 0.0931241363286972,
      "learning_rate": 8.2598235765838e-06,
      "loss": 0.4164,
      "step": 2396
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 0.085421621799469,
      "learning_rate": 8.17963111467522e-06,
      "loss": 0.4573,
      "step": 2397
    },
    {
      "epoch": 0.9595838335334134,
      "grad_norm": 0.09810041636228561,
      "learning_rate": 8.09943865276664e-06,
      "loss": 0.4685,
      "step": 2398
    },
    {
      "epoch": 0.959983993597439,
      "grad_norm": 0.10363864153623581,
      "learning_rate": 8.01924619085806e-06,
      "loss": 0.5429,
      "step": 2399
    },
    {
      "epoch": 0.9603841536614646,
      "grad_norm": 0.1133815199136734,
      "learning_rate": 7.93905372894948e-06,
      "loss": 0.4646,
      "step": 2400
    },
    {
      "epoch": 0.9607843137254902,
      "grad_norm": 0.09397833794355392,
      "learning_rate": 7.858861267040898e-06,
      "loss": 0.4728,
      "step": 2401
    },
    {
      "epoch": 0.9611844737895158,
      "grad_norm": 0.10359295457601547,
      "learning_rate": 7.778668805132318e-06,
      "loss": 0.4567,
      "step": 2402
    },
    {
      "epoch": 0.9615846338535414,
      "grad_norm": 0.0790417492389679,
      "learning_rate": 7.698476343223737e-06,
      "loss": 0.4245,
      "step": 2403
    },
    {
      "epoch": 0.961984793917567,
      "grad_norm": 0.1035088449716568,
      "learning_rate": 7.618283881315156e-06,
      "loss": 0.4861,
      "step": 2404
    },
    {
      "epoch": 0.9623849539815926,
      "grad_norm": 0.10402202606201172,
      "learning_rate": 7.538091419406576e-06,
      "loss": 0.5358,
      "step": 2405
    },
    {
      "epoch": 0.9627851140456183,
      "grad_norm": 0.12025599181652069,
      "learning_rate": 7.457898957497996e-06,
      "loss": 0.5608,
      "step": 2406
    },
    {
      "epoch": 0.9631852741096438,
      "grad_norm": 0.08467356860637665,
      "learning_rate": 7.377706495589415e-06,
      "loss": 0.4568,
      "step": 2407
    },
    {
      "epoch": 0.9635854341736695,
      "grad_norm": 0.07789286226034164,
      "learning_rate": 7.2975140336808345e-06,
      "loss": 0.3962,
      "step": 2408
    },
    {
      "epoch": 0.963985594237695,
      "grad_norm": 0.0794578418135643,
      "learning_rate": 7.217321571772253e-06,
      "loss": 0.4463,
      "step": 2409
    },
    {
      "epoch": 0.9643857543017207,
      "grad_norm": 0.09664154052734375,
      "learning_rate": 7.137129109863673e-06,
      "loss": 0.4489,
      "step": 2410
    },
    {
      "epoch": 0.9647859143657463,
      "grad_norm": 0.08454082161188126,
      "learning_rate": 7.056936647955092e-06,
      "loss": 0.423,
      "step": 2411
    },
    {
      "epoch": 0.9651860744297719,
      "grad_norm": 0.0873173177242279,
      "learning_rate": 6.976744186046512e-06,
      "loss": 0.4572,
      "step": 2412
    },
    {
      "epoch": 0.9655862344937975,
      "grad_norm": 0.0841737911105156,
      "learning_rate": 6.896551724137932e-06,
      "loss": 0.4366,
      "step": 2413
    },
    {
      "epoch": 0.9659863945578231,
      "grad_norm": 0.09251468628644943,
      "learning_rate": 6.816359262229351e-06,
      "loss": 0.4561,
      "step": 2414
    },
    {
      "epoch": 0.9663865546218487,
      "grad_norm": 0.0939469039440155,
      "learning_rate": 6.73616680032077e-06,
      "loss": 0.3783,
      "step": 2415
    },
    {
      "epoch": 0.9667867146858744,
      "grad_norm": 0.09109009802341461,
      "learning_rate": 6.655974338412189e-06,
      "loss": 0.4265,
      "step": 2416
    },
    {
      "epoch": 0.9671868747499,
      "grad_norm": 0.06920834630727768,
      "learning_rate": 6.575781876503609e-06,
      "loss": 0.3638,
      "step": 2417
    },
    {
      "epoch": 0.9675870348139256,
      "grad_norm": 0.09871891140937805,
      "learning_rate": 6.495589414595028e-06,
      "loss": 0.5094,
      "step": 2418
    },
    {
      "epoch": 0.9679871948779512,
      "grad_norm": 0.08801211416721344,
      "learning_rate": 6.415396952686448e-06,
      "loss": 0.4689,
      "step": 2419
    },
    {
      "epoch": 0.9683873549419768,
      "grad_norm": 0.09913869202136993,
      "learning_rate": 6.335204490777867e-06,
      "loss": 0.4623,
      "step": 2420
    },
    {
      "epoch": 0.9687875150060024,
      "grad_norm": 0.07201255857944489,
      "learning_rate": 6.2550120288692865e-06,
      "loss": 0.3578,
      "step": 2421
    },
    {
      "epoch": 0.969187675070028,
      "grad_norm": 0.09166831523180008,
      "learning_rate": 6.174819566960706e-06,
      "loss": 0.4266,
      "step": 2422
    },
    {
      "epoch": 0.9695878351340537,
      "grad_norm": 0.09875480085611343,
      "learning_rate": 6.094627105052125e-06,
      "loss": 0.4724,
      "step": 2423
    },
    {
      "epoch": 0.9699879951980792,
      "grad_norm": 0.09069348126649857,
      "learning_rate": 6.014434643143545e-06,
      "loss": 0.4342,
      "step": 2424
    },
    {
      "epoch": 0.9703881552621049,
      "grad_norm": 0.08604297041893005,
      "learning_rate": 5.934242181234964e-06,
      "loss": 0.461,
      "step": 2425
    },
    {
      "epoch": 0.9707883153261304,
      "grad_norm": 0.09691524505615234,
      "learning_rate": 5.854049719326384e-06,
      "loss": 0.4478,
      "step": 2426
    },
    {
      "epoch": 0.9711884753901561,
      "grad_norm": 0.0995931327342987,
      "learning_rate": 5.7738572574178026e-06,
      "loss": 0.4928,
      "step": 2427
    },
    {
      "epoch": 0.9715886354541817,
      "grad_norm": 0.09456714242696762,
      "learning_rate": 5.693664795509222e-06,
      "loss": 0.4794,
      "step": 2428
    },
    {
      "epoch": 0.9719887955182073,
      "grad_norm": 0.07627145946025848,
      "learning_rate": 5.613472333600642e-06,
      "loss": 0.3864,
      "step": 2429
    },
    {
      "epoch": 0.9723889555822329,
      "grad_norm": 0.07246484607458115,
      "learning_rate": 5.533279871692061e-06,
      "loss": 0.3874,
      "step": 2430
    },
    {
      "epoch": 0.9727891156462585,
      "grad_norm": 0.0870789885520935,
      "learning_rate": 5.453087409783481e-06,
      "loss": 0.308,
      "step": 2431
    },
    {
      "epoch": 0.9731892757102841,
      "grad_norm": 0.08546022325754166,
      "learning_rate": 5.3728949478749e-06,
      "loss": 0.4768,
      "step": 2432
    },
    {
      "epoch": 0.9735894357743097,
      "grad_norm": 0.08033007383346558,
      "learning_rate": 5.2927024859663195e-06,
      "loss": 0.4115,
      "step": 2433
    },
    {
      "epoch": 0.9739895958383353,
      "grad_norm": 0.08909222483634949,
      "learning_rate": 5.2125100240577384e-06,
      "loss": 0.533,
      "step": 2434
    },
    {
      "epoch": 0.974389755902361,
      "grad_norm": 0.08960781246423721,
      "learning_rate": 5.132317562149158e-06,
      "loss": 0.4682,
      "step": 2435
    },
    {
      "epoch": 0.9747899159663865,
      "grad_norm": 0.11811463534832001,
      "learning_rate": 5.052125100240578e-06,
      "loss": 0.5267,
      "step": 2436
    },
    {
      "epoch": 0.9751900760304122,
      "grad_norm": 0.09528446942567825,
      "learning_rate": 4.971932638331997e-06,
      "loss": 0.4627,
      "step": 2437
    },
    {
      "epoch": 0.9755902360944377,
      "grad_norm": 0.07975005358457565,
      "learning_rate": 4.891740176423417e-06,
      "loss": 0.3829,
      "step": 2438
    },
    {
      "epoch": 0.9759903961584634,
      "grad_norm": 0.09160936623811722,
      "learning_rate": 4.811547714514836e-06,
      "loss": 0.4649,
      "step": 2439
    },
    {
      "epoch": 0.976390556222489,
      "grad_norm": 0.09519515931606293,
      "learning_rate": 4.731355252606255e-06,
      "loss": 0.4996,
      "step": 2440
    },
    {
      "epoch": 0.9767907162865146,
      "grad_norm": 0.1023242324590683,
      "learning_rate": 4.651162790697674e-06,
      "loss": 0.4841,
      "step": 2441
    },
    {
      "epoch": 0.9771908763505402,
      "grad_norm": 0.07464183866977692,
      "learning_rate": 4.570970328789094e-06,
      "loss": 0.3776,
      "step": 2442
    },
    {
      "epoch": 0.9775910364145658,
      "grad_norm": 0.0829780325293541,
      "learning_rate": 4.490777866880513e-06,
      "loss": 0.4306,
      "step": 2443
    },
    {
      "epoch": 0.9779911964785915,
      "grad_norm": 0.07617580890655518,
      "learning_rate": 4.410585404971933e-06,
      "loss": 0.3922,
      "step": 2444
    },
    {
      "epoch": 0.978391356542617,
      "grad_norm": 0.09908550977706909,
      "learning_rate": 4.3303929430633526e-06,
      "loss": 0.4373,
      "step": 2445
    },
    {
      "epoch": 0.9787915166066427,
      "grad_norm": 0.09429746866226196,
      "learning_rate": 4.2502004811547715e-06,
      "loss": 0.5028,
      "step": 2446
    },
    {
      "epoch": 0.9791916766706683,
      "grad_norm": 0.09614692628383636,
      "learning_rate": 4.170008019246191e-06,
      "loss": 0.3586,
      "step": 2447
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 0.08845332264900208,
      "learning_rate": 4.08981555733761e-06,
      "loss": 0.4567,
      "step": 2448
    },
    {
      "epoch": 0.9799919967987195,
      "grad_norm": 0.07284221798181534,
      "learning_rate": 4.00962309542903e-06,
      "loss": 0.398,
      "step": 2449
    },
    {
      "epoch": 0.9803921568627451,
      "grad_norm": 0.10520424693822861,
      "learning_rate": 3.929430633520449e-06,
      "loss": 0.52,
      "step": 2450
    },
    {
      "epoch": 0.9807923169267707,
      "grad_norm": 0.09779679030179977,
      "learning_rate": 3.849238171611869e-06,
      "loss": 0.5286,
      "step": 2451
    },
    {
      "epoch": 0.9811924769907964,
      "grad_norm": 0.08230739831924438,
      "learning_rate": 3.769045709703288e-06,
      "loss": 0.4592,
      "step": 2452
    },
    {
      "epoch": 0.9815926370548219,
      "grad_norm": 0.09689074009656906,
      "learning_rate": 3.6888532477947074e-06,
      "loss": 0.4165,
      "step": 2453
    },
    {
      "epoch": 0.9819927971188476,
      "grad_norm": 0.08063064515590668,
      "learning_rate": 3.6086607858861267e-06,
      "loss": 0.3763,
      "step": 2454
    },
    {
      "epoch": 0.9823929571828731,
      "grad_norm": 0.1021181046962738,
      "learning_rate": 3.528468323977546e-06,
      "loss": 0.4693,
      "step": 2455
    },
    {
      "epoch": 0.9827931172468988,
      "grad_norm": 0.09363941848278046,
      "learning_rate": 3.448275862068966e-06,
      "loss": 0.5147,
      "step": 2456
    },
    {
      "epoch": 0.9831932773109243,
      "grad_norm": 0.08874614536762238,
      "learning_rate": 3.368083400160385e-06,
      "loss": 0.4794,
      "step": 2457
    },
    {
      "epoch": 0.98359343737495,
      "grad_norm": 0.08658841997385025,
      "learning_rate": 3.2878909382518045e-06,
      "loss": 0.5093,
      "step": 2458
    },
    {
      "epoch": 0.9839935974389756,
      "grad_norm": 0.11623188853263855,
      "learning_rate": 3.207698476343224e-06,
      "loss": 0.5103,
      "step": 2459
    },
    {
      "epoch": 0.9843937575030012,
      "grad_norm": 0.10004670172929764,
      "learning_rate": 3.1275060144346432e-06,
      "loss": 0.5111,
      "step": 2460
    },
    {
      "epoch": 0.9847939175670268,
      "grad_norm": 0.10818064212799072,
      "learning_rate": 3.0473135525260626e-06,
      "loss": 0.5215,
      "step": 2461
    },
    {
      "epoch": 0.9851940776310524,
      "grad_norm": 0.12802836298942566,
      "learning_rate": 2.967121090617482e-06,
      "loss": 0.5399,
      "step": 2462
    },
    {
      "epoch": 0.985594237695078,
      "grad_norm": 0.08621560037136078,
      "learning_rate": 2.8869286287089013e-06,
      "loss": 0.4376,
      "step": 2463
    },
    {
      "epoch": 0.9859943977591037,
      "grad_norm": 0.10924474895000458,
      "learning_rate": 2.806736166800321e-06,
      "loss": 0.5532,
      "step": 2464
    },
    {
      "epoch": 0.9863945578231292,
      "grad_norm": 0.1419282853603363,
      "learning_rate": 2.7265437048917404e-06,
      "loss": 0.606,
      "step": 2465
    },
    {
      "epoch": 0.9867947178871549,
      "grad_norm": 0.1024845689535141,
      "learning_rate": 2.6463512429831598e-06,
      "loss": 0.4801,
      "step": 2466
    },
    {
      "epoch": 0.9871948779511804,
      "grad_norm": 0.09872609376907349,
      "learning_rate": 2.566158781074579e-06,
      "loss": 0.5364,
      "step": 2467
    },
    {
      "epoch": 0.9875950380152061,
      "grad_norm": 0.10555863380432129,
      "learning_rate": 2.4859663191659985e-06,
      "loss": 0.4947,
      "step": 2468
    },
    {
      "epoch": 0.9879951980792316,
      "grad_norm": 0.08175671100616455,
      "learning_rate": 2.405773857257418e-06,
      "loss": 0.4362,
      "step": 2469
    },
    {
      "epoch": 0.9883953581432573,
      "grad_norm": 0.08459910750389099,
      "learning_rate": 2.325581395348837e-06,
      "loss": 0.3876,
      "step": 2470
    },
    {
      "epoch": 0.988795518207283,
      "grad_norm": 0.08342643827199936,
      "learning_rate": 2.2453889334402565e-06,
      "loss": 0.3903,
      "step": 2471
    },
    {
      "epoch": 0.9891956782713085,
      "grad_norm": 0.08941958099603653,
      "learning_rate": 2.1651964715316763e-06,
      "loss": 0.4441,
      "step": 2472
    },
    {
      "epoch": 0.9895958383353342,
      "grad_norm": 0.09840328246355057,
      "learning_rate": 2.0850040096230956e-06,
      "loss": 0.4372,
      "step": 2473
    },
    {
      "epoch": 0.9899959983993597,
      "grad_norm": 0.07408503443002701,
      "learning_rate": 2.004811547714515e-06,
      "loss": 0.4122,
      "step": 2474
    },
    {
      "epoch": 0.9903961584633854,
      "grad_norm": 0.09107345342636108,
      "learning_rate": 1.9246190858059343e-06,
      "loss": 0.4946,
      "step": 2475
    },
    {
      "epoch": 0.990796318527411,
      "grad_norm": 0.09263879060745239,
      "learning_rate": 1.8444266238973537e-06,
      "loss": 0.4518,
      "step": 2476
    },
    {
      "epoch": 0.9911964785914366,
      "grad_norm": 0.10426385700702667,
      "learning_rate": 1.764234161988773e-06,
      "loss": 0.4748,
      "step": 2477
    },
    {
      "epoch": 0.9915966386554622,
      "grad_norm": 0.11398245394229889,
      "learning_rate": 1.6840417000801926e-06,
      "loss": 0.4806,
      "step": 2478
    },
    {
      "epoch": 0.9919967987194878,
      "grad_norm": 0.08137011528015137,
      "learning_rate": 1.603849238171612e-06,
      "loss": 0.451,
      "step": 2479
    },
    {
      "epoch": 0.9923969587835134,
      "grad_norm": 0.1004779115319252,
      "learning_rate": 1.5236567762630313e-06,
      "loss": 0.5003,
      "step": 2480
    },
    {
      "epoch": 0.992797118847539,
      "grad_norm": 0.08742643147706985,
      "learning_rate": 1.4434643143544506e-06,
      "loss": 0.4529,
      "step": 2481
    },
    {
      "epoch": 0.9931972789115646,
      "grad_norm": 0.08466426283121109,
      "learning_rate": 1.3632718524458702e-06,
      "loss": 0.4217,
      "step": 2482
    },
    {
      "epoch": 0.9935974389755903,
      "grad_norm": 0.07500582188367844,
      "learning_rate": 1.2830793905372896e-06,
      "loss": 0.4009,
      "step": 2483
    },
    {
      "epoch": 0.9939975990396158,
      "grad_norm": 0.09571966528892517,
      "learning_rate": 1.202886928628709e-06,
      "loss": 0.4594,
      "step": 2484
    },
    {
      "epoch": 0.9943977591036415,
      "grad_norm": 0.10095645487308502,
      "learning_rate": 1.1226944667201283e-06,
      "loss": 0.4595,
      "step": 2485
    },
    {
      "epoch": 0.994797919167667,
      "grad_norm": 0.07877865433692932,
      "learning_rate": 1.0425020048115478e-06,
      "loss": 0.4102,
      "step": 2486
    },
    {
      "epoch": 0.9951980792316927,
      "grad_norm": 0.08825583010911942,
      "learning_rate": 9.623095429029672e-07,
      "loss": 0.4668,
      "step": 2487
    },
    {
      "epoch": 0.9955982392957183,
      "grad_norm": 0.08906417340040207,
      "learning_rate": 8.821170809943865e-07,
      "loss": 0.4904,
      "step": 2488
    },
    {
      "epoch": 0.9959983993597439,
      "grad_norm": 0.0963551253080368,
      "learning_rate": 8.01924619085806e-07,
      "loss": 0.4957,
      "step": 2489
    },
    {
      "epoch": 0.9963985594237695,
      "grad_norm": 0.09913015365600586,
      "learning_rate": 7.217321571772253e-07,
      "loss": 0.4756,
      "step": 2490
    },
    {
      "epoch": 0.9967987194877951,
      "grad_norm": 0.07494878768920898,
      "learning_rate": 6.415396952686448e-07,
      "loss": 0.381,
      "step": 2491
    },
    {
      "epoch": 0.9971988795518207,
      "grad_norm": 0.10418003052473068,
      "learning_rate": 5.613472333600641e-07,
      "loss": 0.546,
      "step": 2492
    },
    {
      "epoch": 0.9975990396158463,
      "grad_norm": 0.09538412094116211,
      "learning_rate": 4.811547714514836e-07,
      "loss": 0.5098,
      "step": 2493
    },
    {
      "epoch": 0.9979991996798719,
      "grad_norm": 0.1066771149635315,
      "learning_rate": 4.00962309542903e-07,
      "loss": 0.4692,
      "step": 2494
    },
    {
      "epoch": 0.9983993597438976,
      "grad_norm": 0.09398037940263748,
      "learning_rate": 3.207698476343224e-07,
      "loss": 0.4175,
      "step": 2495
    },
    {
      "epoch": 0.9987995198079231,
      "grad_norm": 0.0853305235505104,
      "learning_rate": 2.405773857257418e-07,
      "loss": 0.4291,
      "step": 2496
    },
    {
      "epoch": 0.9991996798719488,
      "grad_norm": 0.11618635803461075,
      "learning_rate": 1.603849238171612e-07,
      "loss": 0.5316,
      "step": 2497
    },
    {
      "epoch": 0.9995998399359743,
      "grad_norm": 0.09333972632884979,
      "learning_rate": 8.01924619085806e-08,
      "loss": 0.4567,
      "step": 2498
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.09385065734386444,
      "learning_rate": 0.0,
      "loss": 0.4183,
      "step": 2499
    }
  ],
  "logging_steps": 1,
  "max_steps": 2499,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.1552316435707003e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
