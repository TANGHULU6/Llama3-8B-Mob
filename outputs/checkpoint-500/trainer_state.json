{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.2000800320128051,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00040016006402561027,
      "grad_norm": 0.22946873307228088,
      "learning_rate": 4e-05,
      "loss": 0.4692,
      "step": 1
    },
    {
      "epoch": 0.0008003201280512205,
      "grad_norm": 0.16390261054039001,
      "learning_rate": 8e-05,
      "loss": 0.3807,
      "step": 2
    },
    {
      "epoch": 0.0012004801920768306,
      "grad_norm": 0.26059892773628235,
      "learning_rate": 0.00012,
      "loss": 0.5289,
      "step": 3
    },
    {
      "epoch": 0.001600640256102441,
      "grad_norm": 0.4976859986782074,
      "learning_rate": 0.00016,
      "loss": 0.4337,
      "step": 4
    },
    {
      "epoch": 0.0020008003201280513,
      "grad_norm": 0.3747754991054535,
      "learning_rate": 0.0002,
      "loss": 0.4729,
      "step": 5
    },
    {
      "epoch": 0.0024009603841536613,
      "grad_norm": 0.5714130997657776,
      "learning_rate": 0.00019991980753809142,
      "loss": 0.4508,
      "step": 6
    },
    {
      "epoch": 0.0028011204481792717,
      "grad_norm": 0.3919544219970703,
      "learning_rate": 0.00019983961507618285,
      "loss": 0.4079,
      "step": 7
    },
    {
      "epoch": 0.003201280512204882,
      "grad_norm": 0.23712654411792755,
      "learning_rate": 0.00019975942261427426,
      "loss": 0.5978,
      "step": 8
    },
    {
      "epoch": 0.003601440576230492,
      "grad_norm": 0.22808951139450073,
      "learning_rate": 0.0001996792301523657,
      "loss": 0.4997,
      "step": 9
    },
    {
      "epoch": 0.004001600640256103,
      "grad_norm": 0.12574392557144165,
      "learning_rate": 0.0001995990376904571,
      "loss": 0.3915,
      "step": 10
    },
    {
      "epoch": 0.004401760704281713,
      "grad_norm": 0.16919584572315216,
      "learning_rate": 0.00019951884522854854,
      "loss": 0.4628,
      "step": 11
    },
    {
      "epoch": 0.004801920768307323,
      "grad_norm": 0.18785926699638367,
      "learning_rate": 0.00019943865276663995,
      "loss": 0.4591,
      "step": 12
    },
    {
      "epoch": 0.005202080832332933,
      "grad_norm": 0.15934772789478302,
      "learning_rate": 0.00019935846030473135,
      "loss": 0.4936,
      "step": 13
    },
    {
      "epoch": 0.0056022408963585435,
      "grad_norm": 0.1776677519083023,
      "learning_rate": 0.00019927826784282276,
      "loss": 0.357,
      "step": 14
    },
    {
      "epoch": 0.006002400960384154,
      "grad_norm": 0.17960716784000397,
      "learning_rate": 0.0001991980753809142,
      "loss": 0.5314,
      "step": 15
    },
    {
      "epoch": 0.006402561024409764,
      "grad_norm": 0.14224982261657715,
      "learning_rate": 0.0001991178829190056,
      "loss": 0.4277,
      "step": 16
    },
    {
      "epoch": 0.006802721088435374,
      "grad_norm": 0.1837865263223648,
      "learning_rate": 0.00019903769045709704,
      "loss": 0.5311,
      "step": 17
    },
    {
      "epoch": 0.007202881152460984,
      "grad_norm": 0.14006249606609344,
      "learning_rate": 0.00019895749799518845,
      "loss": 0.49,
      "step": 18
    },
    {
      "epoch": 0.007603041216486595,
      "grad_norm": 0.15497927367687225,
      "learning_rate": 0.00019887730553327988,
      "loss": 0.5023,
      "step": 19
    },
    {
      "epoch": 0.008003201280512205,
      "grad_norm": 0.1743759661912918,
      "learning_rate": 0.0001987971130713713,
      "loss": 0.4607,
      "step": 20
    },
    {
      "epoch": 0.008403361344537815,
      "grad_norm": 0.15628352761268616,
      "learning_rate": 0.00019871692060946273,
      "loss": 0.4854,
      "step": 21
    },
    {
      "epoch": 0.008803521408563426,
      "grad_norm": 0.15339666604995728,
      "learning_rate": 0.00019863672814755413,
      "loss": 0.4505,
      "step": 22
    },
    {
      "epoch": 0.009203681472589036,
      "grad_norm": 0.1642875373363495,
      "learning_rate": 0.00019855653568564557,
      "loss": 0.5261,
      "step": 23
    },
    {
      "epoch": 0.009603841536614645,
      "grad_norm": 0.17513947188854218,
      "learning_rate": 0.00019847634322373698,
      "loss": 0.5614,
      "step": 24
    },
    {
      "epoch": 0.010004001600640256,
      "grad_norm": 0.17406989634037018,
      "learning_rate": 0.0001983961507618284,
      "loss": 0.5059,
      "step": 25
    },
    {
      "epoch": 0.010404161664665866,
      "grad_norm": 0.1399659812450409,
      "learning_rate": 0.00019831595829991982,
      "loss": 0.4831,
      "step": 26
    },
    {
      "epoch": 0.010804321728691477,
      "grad_norm": 0.15993866324424744,
      "learning_rate": 0.00019823576583801125,
      "loss": 0.5134,
      "step": 27
    },
    {
      "epoch": 0.011204481792717087,
      "grad_norm": 0.147577702999115,
      "learning_rate": 0.00019815557337610266,
      "loss": 0.4907,
      "step": 28
    },
    {
      "epoch": 0.011604641856742696,
      "grad_norm": 0.18316051363945007,
      "learning_rate": 0.00019807538091419407,
      "loss": 0.5415,
      "step": 29
    },
    {
      "epoch": 0.012004801920768308,
      "grad_norm": 0.13442182540893555,
      "learning_rate": 0.00019799518845228548,
      "loss": 0.4482,
      "step": 30
    },
    {
      "epoch": 0.012404961984793917,
      "grad_norm": 0.14758652448654175,
      "learning_rate": 0.0001979149959903769,
      "loss": 0.4662,
      "step": 31
    },
    {
      "epoch": 0.012805122048819529,
      "grad_norm": 0.13469131290912628,
      "learning_rate": 0.00019783480352846832,
      "loss": 0.4239,
      "step": 32
    },
    {
      "epoch": 0.013205282112845138,
      "grad_norm": 0.15882259607315063,
      "learning_rate": 0.00019775461106655976,
      "loss": 0.4956,
      "step": 33
    },
    {
      "epoch": 0.013605442176870748,
      "grad_norm": 0.14396321773529053,
      "learning_rate": 0.00019767441860465116,
      "loss": 0.5383,
      "step": 34
    },
    {
      "epoch": 0.014005602240896359,
      "grad_norm": 0.17287583649158478,
      "learning_rate": 0.0001975942261427426,
      "loss": 0.4434,
      "step": 35
    },
    {
      "epoch": 0.014405762304921969,
      "grad_norm": 0.1848708838224411,
      "learning_rate": 0.000197514033680834,
      "loss": 0.4147,
      "step": 36
    },
    {
      "epoch": 0.014805922368947578,
      "grad_norm": 0.1444273144006729,
      "learning_rate": 0.00019743384121892544,
      "loss": 0.5027,
      "step": 37
    },
    {
      "epoch": 0.01520608243297319,
      "grad_norm": 0.17688049376010895,
      "learning_rate": 0.00019735364875701685,
      "loss": 0.4522,
      "step": 38
    },
    {
      "epoch": 0.015606242496998799,
      "grad_norm": 0.17187055945396423,
      "learning_rate": 0.00019727345629510829,
      "loss": 0.4511,
      "step": 39
    },
    {
      "epoch": 0.01600640256102441,
      "grad_norm": 0.13287261128425598,
      "learning_rate": 0.0001971932638331997,
      "loss": 0.4586,
      "step": 40
    },
    {
      "epoch": 0.01640656262505002,
      "grad_norm": 0.1346621811389923,
      "learning_rate": 0.0001971130713712911,
      "loss": 0.3949,
      "step": 41
    },
    {
      "epoch": 0.01680672268907563,
      "grad_norm": 0.18018732964992523,
      "learning_rate": 0.00019703287890938254,
      "loss": 0.542,
      "step": 42
    },
    {
      "epoch": 0.01720688275310124,
      "grad_norm": 0.14330074191093445,
      "learning_rate": 0.00019695268644747394,
      "loss": 0.5262,
      "step": 43
    },
    {
      "epoch": 0.017607042817126852,
      "grad_norm": 0.16825029253959656,
      "learning_rate": 0.00019687249398556538,
      "loss": 0.4817,
      "step": 44
    },
    {
      "epoch": 0.01800720288115246,
      "grad_norm": 0.12784413993358612,
      "learning_rate": 0.0001967923015236568,
      "loss": 0.4303,
      "step": 45
    },
    {
      "epoch": 0.01840736294517807,
      "grad_norm": 0.1517990082502365,
      "learning_rate": 0.0001967121090617482,
      "loss": 0.553,
      "step": 46
    },
    {
      "epoch": 0.018807523009203683,
      "grad_norm": 0.10310398042201996,
      "learning_rate": 0.0001966319165998396,
      "loss": 0.3557,
      "step": 47
    },
    {
      "epoch": 0.01920768307322929,
      "grad_norm": 0.14064480364322662,
      "learning_rate": 0.00019655172413793104,
      "loss": 0.4677,
      "step": 48
    },
    {
      "epoch": 0.0196078431372549,
      "grad_norm": 0.13959263265132904,
      "learning_rate": 0.00019647153167602245,
      "loss": 0.5422,
      "step": 49
    },
    {
      "epoch": 0.020008003201280513,
      "grad_norm": 0.13795213401317596,
      "learning_rate": 0.00019639133921411388,
      "loss": 0.4465,
      "step": 50
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 0.16808612644672394,
      "learning_rate": 0.0001963111467522053,
      "loss": 0.5849,
      "step": 51
    },
    {
      "epoch": 0.020808323329331732,
      "grad_norm": 0.11101402342319489,
      "learning_rate": 0.00019623095429029672,
      "loss": 0.4572,
      "step": 52
    },
    {
      "epoch": 0.021208483393357343,
      "grad_norm": 0.1317504644393921,
      "learning_rate": 0.00019615076182838813,
      "loss": 0.3873,
      "step": 53
    },
    {
      "epoch": 0.021608643457382955,
      "grad_norm": 0.17213749885559082,
      "learning_rate": 0.00019607056936647957,
      "loss": 0.5734,
      "step": 54
    },
    {
      "epoch": 0.022008803521408563,
      "grad_norm": 0.1286506950855255,
      "learning_rate": 0.00019599037690457097,
      "loss": 0.4729,
      "step": 55
    },
    {
      "epoch": 0.022408963585434174,
      "grad_norm": 0.13250727951526642,
      "learning_rate": 0.0001959101844426624,
      "loss": 0.4639,
      "step": 56
    },
    {
      "epoch": 0.022809123649459785,
      "grad_norm": 0.16937655210494995,
      "learning_rate": 0.00019582999198075382,
      "loss": 0.4967,
      "step": 57
    },
    {
      "epoch": 0.023209283713485393,
      "grad_norm": 0.14461025595664978,
      "learning_rate": 0.00019574979951884525,
      "loss": 0.4212,
      "step": 58
    },
    {
      "epoch": 0.023609443777511004,
      "grad_norm": 0.12467870861291885,
      "learning_rate": 0.00019566960705693666,
      "loss": 0.48,
      "step": 59
    },
    {
      "epoch": 0.024009603841536616,
      "grad_norm": 0.14800018072128296,
      "learning_rate": 0.0001955894145950281,
      "loss": 0.519,
      "step": 60
    },
    {
      "epoch": 0.024409763905562223,
      "grad_norm": 0.11576331406831741,
      "learning_rate": 0.00019550922213311948,
      "loss": 0.4167,
      "step": 61
    },
    {
      "epoch": 0.024809923969587835,
      "grad_norm": 0.15244458615779877,
      "learning_rate": 0.0001954290296712109,
      "loss": 0.4834,
      "step": 62
    },
    {
      "epoch": 0.025210084033613446,
      "grad_norm": 0.10282991826534271,
      "learning_rate": 0.00019534883720930232,
      "loss": 0.3936,
      "step": 63
    },
    {
      "epoch": 0.025610244097639057,
      "grad_norm": 0.12172392010688782,
      "learning_rate": 0.00019526864474739375,
      "loss": 0.4041,
      "step": 64
    },
    {
      "epoch": 0.026010404161664665,
      "grad_norm": 0.12906822562217712,
      "learning_rate": 0.00019518845228548516,
      "loss": 0.4056,
      "step": 65
    },
    {
      "epoch": 0.026410564225690276,
      "grad_norm": 0.11636485904455185,
      "learning_rate": 0.0001951082598235766,
      "loss": 0.3584,
      "step": 66
    },
    {
      "epoch": 0.026810724289715888,
      "grad_norm": 0.13368813693523407,
      "learning_rate": 0.000195028067361668,
      "loss": 0.3486,
      "step": 67
    },
    {
      "epoch": 0.027210884353741496,
      "grad_norm": 0.11729364097118378,
      "learning_rate": 0.00019494787489975944,
      "loss": 0.4542,
      "step": 68
    },
    {
      "epoch": 0.027611044417767107,
      "grad_norm": 0.14878401160240173,
      "learning_rate": 0.00019486768243785085,
      "loss": 0.4081,
      "step": 69
    },
    {
      "epoch": 0.028011204481792718,
      "grad_norm": 0.1566425859928131,
      "learning_rate": 0.00019478748997594228,
      "loss": 0.4563,
      "step": 70
    },
    {
      "epoch": 0.028411364545818326,
      "grad_norm": 0.13982544839382172,
      "learning_rate": 0.0001947072975140337,
      "loss": 0.4815,
      "step": 71
    },
    {
      "epoch": 0.028811524609843937,
      "grad_norm": 0.11852291971445084,
      "learning_rate": 0.00019462710505212513,
      "loss": 0.3915,
      "step": 72
    },
    {
      "epoch": 0.02921168467386955,
      "grad_norm": 0.15591681003570557,
      "learning_rate": 0.00019454691259021653,
      "loss": 0.5166,
      "step": 73
    },
    {
      "epoch": 0.029611844737895156,
      "grad_norm": 0.13018865883350372,
      "learning_rate": 0.00019446672012830797,
      "loss": 0.441,
      "step": 74
    },
    {
      "epoch": 0.030012004801920768,
      "grad_norm": 0.12293136119842529,
      "learning_rate": 0.00019438652766639938,
      "loss": 0.5128,
      "step": 75
    },
    {
      "epoch": 0.03041216486594638,
      "grad_norm": 0.12453330308198929,
      "learning_rate": 0.00019430633520449078,
      "loss": 0.4661,
      "step": 76
    },
    {
      "epoch": 0.03081232492997199,
      "grad_norm": 0.13392303884029388,
      "learning_rate": 0.0001942261427425822,
      "loss": 0.4712,
      "step": 77
    },
    {
      "epoch": 0.031212484993997598,
      "grad_norm": 0.12176399677991867,
      "learning_rate": 0.00019414595028067363,
      "loss": 0.4389,
      "step": 78
    },
    {
      "epoch": 0.031612645058023206,
      "grad_norm": 0.1300990879535675,
      "learning_rate": 0.00019406575781876504,
      "loss": 0.4751,
      "step": 79
    },
    {
      "epoch": 0.03201280512204882,
      "grad_norm": 0.11435059458017349,
      "learning_rate": 0.00019398556535685647,
      "loss": 0.4576,
      "step": 80
    },
    {
      "epoch": 0.03241296518607443,
      "grad_norm": 0.0858248695731163,
      "learning_rate": 0.00019390537289494788,
      "loss": 0.3387,
      "step": 81
    },
    {
      "epoch": 0.03281312525010004,
      "grad_norm": 0.13226385414600372,
      "learning_rate": 0.00019382518043303929,
      "loss": 0.4493,
      "step": 82
    },
    {
      "epoch": 0.03321328531412565,
      "grad_norm": 0.15288126468658447,
      "learning_rate": 0.00019374498797113072,
      "loss": 0.538,
      "step": 83
    },
    {
      "epoch": 0.03361344537815126,
      "grad_norm": 0.10629519820213318,
      "learning_rate": 0.00019366479550922213,
      "loss": 0.4083,
      "step": 84
    },
    {
      "epoch": 0.034013605442176874,
      "grad_norm": 0.13565696775913239,
      "learning_rate": 0.00019358460304731356,
      "loss": 0.4495,
      "step": 85
    },
    {
      "epoch": 0.03441376550620248,
      "grad_norm": 0.11949463933706284,
      "learning_rate": 0.00019350441058540497,
      "loss": 0.449,
      "step": 86
    },
    {
      "epoch": 0.03481392557022809,
      "grad_norm": 0.14017391204833984,
      "learning_rate": 0.0001934242181234964,
      "loss": 0.5416,
      "step": 87
    },
    {
      "epoch": 0.035214085634253704,
      "grad_norm": 0.12003178894519806,
      "learning_rate": 0.00019334402566158782,
      "loss": 0.4357,
      "step": 88
    },
    {
      "epoch": 0.03561424569827931,
      "grad_norm": 0.131962850689888,
      "learning_rate": 0.00019326383319967925,
      "loss": 0.4381,
      "step": 89
    },
    {
      "epoch": 0.03601440576230492,
      "grad_norm": 0.11337488144636154,
      "learning_rate": 0.00019318364073777066,
      "loss": 0.4262,
      "step": 90
    },
    {
      "epoch": 0.036414565826330535,
      "grad_norm": 0.14590921998023987,
      "learning_rate": 0.0001931034482758621,
      "loss": 0.5096,
      "step": 91
    },
    {
      "epoch": 0.03681472589035614,
      "grad_norm": 0.13093169033527374,
      "learning_rate": 0.0001930232558139535,
      "loss": 0.4865,
      "step": 92
    },
    {
      "epoch": 0.03721488595438175,
      "grad_norm": 0.14437992870807648,
      "learning_rate": 0.0001929430633520449,
      "loss": 0.4666,
      "step": 93
    },
    {
      "epoch": 0.037615046018407365,
      "grad_norm": 0.10006094723939896,
      "learning_rate": 0.00019286287089013632,
      "loss": 0.4532,
      "step": 94
    },
    {
      "epoch": 0.03801520608243297,
      "grad_norm": 0.20455269515514374,
      "learning_rate": 0.00019278267842822775,
      "loss": 0.529,
      "step": 95
    },
    {
      "epoch": 0.03841536614645858,
      "grad_norm": 0.13493114709854126,
      "learning_rate": 0.00019270248596631916,
      "loss": 0.4921,
      "step": 96
    },
    {
      "epoch": 0.038815526210484196,
      "grad_norm": 0.12510214745998383,
      "learning_rate": 0.0001926222935044106,
      "loss": 0.5435,
      "step": 97
    },
    {
      "epoch": 0.0392156862745098,
      "grad_norm": 0.10947107523679733,
      "learning_rate": 0.000192542101042502,
      "loss": 0.3436,
      "step": 98
    },
    {
      "epoch": 0.03961584633853541,
      "grad_norm": 0.1674061119556427,
      "learning_rate": 0.00019246190858059344,
      "loss": 0.4791,
      "step": 99
    },
    {
      "epoch": 0.040016006402561026,
      "grad_norm": 0.12110971659421921,
      "learning_rate": 0.00019238171611868485,
      "loss": 0.3824,
      "step": 100
    },
    {
      "epoch": 0.040416166466586634,
      "grad_norm": 0.1303078532218933,
      "learning_rate": 0.00019230152365677628,
      "loss": 0.4806,
      "step": 101
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 0.13087669014930725,
      "learning_rate": 0.0001922213311948677,
      "loss": 0.394,
      "step": 102
    },
    {
      "epoch": 0.041216486594637856,
      "grad_norm": 0.1451854407787323,
      "learning_rate": 0.00019214113873295912,
      "loss": 0.4675,
      "step": 103
    },
    {
      "epoch": 0.041616646658663464,
      "grad_norm": 0.1276141256093979,
      "learning_rate": 0.00019206094627105053,
      "loss": 0.4524,
      "step": 104
    },
    {
      "epoch": 0.04201680672268908,
      "grad_norm": 0.13451483845710754,
      "learning_rate": 0.00019198075380914197,
      "loss": 0.5279,
      "step": 105
    },
    {
      "epoch": 0.04241696678671469,
      "grad_norm": 0.1201162338256836,
      "learning_rate": 0.00019190056134723337,
      "loss": 0.4301,
      "step": 106
    },
    {
      "epoch": 0.042817126850740295,
      "grad_norm": 0.1359904557466507,
      "learning_rate": 0.0001918203688853248,
      "loss": 0.3869,
      "step": 107
    },
    {
      "epoch": 0.04321728691476591,
      "grad_norm": 0.09089353680610657,
      "learning_rate": 0.00019174017642341622,
      "loss": 0.4132,
      "step": 108
    },
    {
      "epoch": 0.04361744697879152,
      "grad_norm": 0.11473874747753143,
      "learning_rate": 0.00019165998396150763,
      "loss": 0.4032,
      "step": 109
    },
    {
      "epoch": 0.044017607042817125,
      "grad_norm": 0.13859650492668152,
      "learning_rate": 0.00019157979149959903,
      "loss": 0.4179,
      "step": 110
    },
    {
      "epoch": 0.04441776710684274,
      "grad_norm": 0.11934302747249603,
      "learning_rate": 0.00019149959903769047,
      "loss": 0.4429,
      "step": 111
    },
    {
      "epoch": 0.04481792717086835,
      "grad_norm": 0.1345134824514389,
      "learning_rate": 0.00019141940657578188,
      "loss": 0.4739,
      "step": 112
    },
    {
      "epoch": 0.045218087234893956,
      "grad_norm": 0.14646725356578827,
      "learning_rate": 0.0001913392141138733,
      "loss": 0.4861,
      "step": 113
    },
    {
      "epoch": 0.04561824729891957,
      "grad_norm": 0.1378999948501587,
      "learning_rate": 0.00019125902165196472,
      "loss": 0.5629,
      "step": 114
    },
    {
      "epoch": 0.04601840736294518,
      "grad_norm": 0.12542778253555298,
      "learning_rate": 0.00019117882919005615,
      "loss": 0.5017,
      "step": 115
    },
    {
      "epoch": 0.046418567426970786,
      "grad_norm": 0.10740013420581818,
      "learning_rate": 0.00019109863672814756,
      "loss": 0.438,
      "step": 116
    },
    {
      "epoch": 0.0468187274909964,
      "grad_norm": 0.13227002322673798,
      "learning_rate": 0.00019101844426623897,
      "loss": 0.4622,
      "step": 117
    },
    {
      "epoch": 0.04721888755502201,
      "grad_norm": 0.1616428643465042,
      "learning_rate": 0.0001909382518043304,
      "loss": 0.4689,
      "step": 118
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.13426005840301514,
      "learning_rate": 0.0001908580593424218,
      "loss": 0.4501,
      "step": 119
    },
    {
      "epoch": 0.04801920768307323,
      "grad_norm": 0.11059197783470154,
      "learning_rate": 0.00019077786688051325,
      "loss": 0.4602,
      "step": 120
    },
    {
      "epoch": 0.04841936774709884,
      "grad_norm": 0.1128101721405983,
      "learning_rate": 0.00019069767441860466,
      "loss": 0.4863,
      "step": 121
    },
    {
      "epoch": 0.04881952781112445,
      "grad_norm": 0.13223111629486084,
      "learning_rate": 0.0001906174819566961,
      "loss": 0.5131,
      "step": 122
    },
    {
      "epoch": 0.04921968787515006,
      "grad_norm": 0.15044215321540833,
      "learning_rate": 0.0001905372894947875,
      "loss": 0.5166,
      "step": 123
    },
    {
      "epoch": 0.04961984793917567,
      "grad_norm": 0.17137274146080017,
      "learning_rate": 0.00019045709703287893,
      "loss": 0.5792,
      "step": 124
    },
    {
      "epoch": 0.05002000800320128,
      "grad_norm": 0.14729808270931244,
      "learning_rate": 0.00019037690457097031,
      "loss": 0.4801,
      "step": 125
    },
    {
      "epoch": 0.05042016806722689,
      "grad_norm": 0.14504823088645935,
      "learning_rate": 0.00019029671210906175,
      "loss": 0.5508,
      "step": 126
    },
    {
      "epoch": 0.0508203281312525,
      "grad_norm": 0.15063397586345673,
      "learning_rate": 0.00019021651964715316,
      "loss": 0.4911,
      "step": 127
    },
    {
      "epoch": 0.051220488195278115,
      "grad_norm": 0.11778245866298676,
      "learning_rate": 0.0001901363271852446,
      "loss": 0.4418,
      "step": 128
    },
    {
      "epoch": 0.05162064825930372,
      "grad_norm": 0.1319017857313156,
      "learning_rate": 0.000190056134723336,
      "loss": 0.4512,
      "step": 129
    },
    {
      "epoch": 0.05202080832332933,
      "grad_norm": 0.12440775334835052,
      "learning_rate": 0.00018997594226142744,
      "loss": 0.426,
      "step": 130
    },
    {
      "epoch": 0.052420968387354945,
      "grad_norm": 0.12055116146802902,
      "learning_rate": 0.00018989574979951884,
      "loss": 0.469,
      "step": 131
    },
    {
      "epoch": 0.05282112845138055,
      "grad_norm": 0.10702915489673615,
      "learning_rate": 0.00018981555733761028,
      "loss": 0.4418,
      "step": 132
    },
    {
      "epoch": 0.05322128851540616,
      "grad_norm": 0.12964826822280884,
      "learning_rate": 0.00018973536487570169,
      "loss": 0.367,
      "step": 133
    },
    {
      "epoch": 0.053621448579431776,
      "grad_norm": 0.141257643699646,
      "learning_rate": 0.00018965517241379312,
      "loss": 0.505,
      "step": 134
    },
    {
      "epoch": 0.05402160864345738,
      "grad_norm": 0.15194900333881378,
      "learning_rate": 0.00018957497995188453,
      "loss": 0.4828,
      "step": 135
    },
    {
      "epoch": 0.05442176870748299,
      "grad_norm": 0.10905466973781586,
      "learning_rate": 0.00018949478748997596,
      "loss": 0.3756,
      "step": 136
    },
    {
      "epoch": 0.054821928771508606,
      "grad_norm": 0.1474345475435257,
      "learning_rate": 0.00018941459502806737,
      "loss": 0.4389,
      "step": 137
    },
    {
      "epoch": 0.055222088835534214,
      "grad_norm": 0.15416045486927032,
      "learning_rate": 0.0001893344025661588,
      "loss": 0.4676,
      "step": 138
    },
    {
      "epoch": 0.05562224889955982,
      "grad_norm": 0.12918829917907715,
      "learning_rate": 0.00018925421010425022,
      "loss": 0.4654,
      "step": 139
    },
    {
      "epoch": 0.056022408963585436,
      "grad_norm": 0.14582864940166473,
      "learning_rate": 0.00018917401764234165,
      "loss": 0.4925,
      "step": 140
    },
    {
      "epoch": 0.056422569027611044,
      "grad_norm": 0.1267489343881607,
      "learning_rate": 0.00018909382518043303,
      "loss": 0.4228,
      "step": 141
    },
    {
      "epoch": 0.05682272909163665,
      "grad_norm": 0.11785811185836792,
      "learning_rate": 0.00018901363271852447,
      "loss": 0.4243,
      "step": 142
    },
    {
      "epoch": 0.05722288915566227,
      "grad_norm": 0.14111536741256714,
      "learning_rate": 0.00018893344025661587,
      "loss": 0.4538,
      "step": 143
    },
    {
      "epoch": 0.057623049219687875,
      "grad_norm": 0.14035436511039734,
      "learning_rate": 0.0001888532477947073,
      "loss": 0.5006,
      "step": 144
    },
    {
      "epoch": 0.05802320928371348,
      "grad_norm": 0.1330406367778778,
      "learning_rate": 0.00018877305533279872,
      "loss": 0.4405,
      "step": 145
    },
    {
      "epoch": 0.0584233693477391,
      "grad_norm": 0.12746116518974304,
      "learning_rate": 0.00018869286287089015,
      "loss": 0.4879,
      "step": 146
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.16939005255699158,
      "learning_rate": 0.00018861267040898156,
      "loss": 0.5499,
      "step": 147
    },
    {
      "epoch": 0.05922368947579031,
      "grad_norm": 0.14871060848236084,
      "learning_rate": 0.000188532477947073,
      "loss": 0.5435,
      "step": 148
    },
    {
      "epoch": 0.05962384953981593,
      "grad_norm": 0.14285235106945038,
      "learning_rate": 0.0001884522854851644,
      "loss": 0.5179,
      "step": 149
    },
    {
      "epoch": 0.060024009603841535,
      "grad_norm": 0.13702325522899628,
      "learning_rate": 0.00018837209302325584,
      "loss": 0.4719,
      "step": 150
    },
    {
      "epoch": 0.06042416966786715,
      "grad_norm": 0.13655483722686768,
      "learning_rate": 0.00018829190056134725,
      "loss": 0.4643,
      "step": 151
    },
    {
      "epoch": 0.06082432973189276,
      "grad_norm": 0.10196033865213394,
      "learning_rate": 0.00018821170809943865,
      "loss": 0.3487,
      "step": 152
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 0.1465970277786255,
      "learning_rate": 0.0001881315156375301,
      "loss": 0.5418,
      "step": 153
    },
    {
      "epoch": 0.06162464985994398,
      "grad_norm": 0.09615792334079742,
      "learning_rate": 0.0001880513231756215,
      "loss": 0.4011,
      "step": 154
    },
    {
      "epoch": 0.06202480992396959,
      "grad_norm": 0.1325608491897583,
      "learning_rate": 0.00018797113071371293,
      "loss": 0.4912,
      "step": 155
    },
    {
      "epoch": 0.062424969987995196,
      "grad_norm": 0.1483178287744522,
      "learning_rate": 0.00018789093825180434,
      "loss": 0.5028,
      "step": 156
    },
    {
      "epoch": 0.06282513005202081,
      "grad_norm": 0.12851116061210632,
      "learning_rate": 0.00018781074578989575,
      "loss": 0.4656,
      "step": 157
    },
    {
      "epoch": 0.06322529011604641,
      "grad_norm": 0.10209093987941742,
      "learning_rate": 0.00018773055332798716,
      "loss": 0.4995,
      "step": 158
    },
    {
      "epoch": 0.06362545018007203,
      "grad_norm": 0.13402999937534332,
      "learning_rate": 0.0001876503608660786,
      "loss": 0.4858,
      "step": 159
    },
    {
      "epoch": 0.06402561024409764,
      "grad_norm": 0.14808642864227295,
      "learning_rate": 0.00018757016840417,
      "loss": 0.4868,
      "step": 160
    },
    {
      "epoch": 0.06442577030812324,
      "grad_norm": 0.13608327507972717,
      "learning_rate": 0.00018748997594226143,
      "loss": 0.5324,
      "step": 161
    },
    {
      "epoch": 0.06482593037214886,
      "grad_norm": 0.1304795891046524,
      "learning_rate": 0.00018740978348035284,
      "loss": 0.4693,
      "step": 162
    },
    {
      "epoch": 0.06522609043617447,
      "grad_norm": 0.13183492422103882,
      "learning_rate": 0.00018732959101844428,
      "loss": 0.4127,
      "step": 163
    },
    {
      "epoch": 0.06562625050020009,
      "grad_norm": 0.13964495062828064,
      "learning_rate": 0.00018724939855653568,
      "loss": 0.5497,
      "step": 164
    },
    {
      "epoch": 0.06602641056422569,
      "grad_norm": 0.12011177837848663,
      "learning_rate": 0.00018716920609462712,
      "loss": 0.512,
      "step": 165
    },
    {
      "epoch": 0.0664265706282513,
      "grad_norm": 0.19295620918273926,
      "learning_rate": 0.00018708901363271853,
      "loss": 0.5392,
      "step": 166
    },
    {
      "epoch": 0.06682673069227692,
      "grad_norm": 0.13760393857955933,
      "learning_rate": 0.00018700882117080996,
      "loss": 0.4647,
      "step": 167
    },
    {
      "epoch": 0.06722689075630252,
      "grad_norm": 0.0975688174366951,
      "learning_rate": 0.00018692862870890137,
      "loss": 0.4718,
      "step": 168
    },
    {
      "epoch": 0.06762705082032813,
      "grad_norm": 0.10798799991607666,
      "learning_rate": 0.0001868484362469928,
      "loss": 0.4052,
      "step": 169
    },
    {
      "epoch": 0.06802721088435375,
      "grad_norm": 0.15086914598941803,
      "learning_rate": 0.0001867682437850842,
      "loss": 0.531,
      "step": 170
    },
    {
      "epoch": 0.06842737094837935,
      "grad_norm": 0.10849977284669876,
      "learning_rate": 0.00018668805132317565,
      "loss": 0.4254,
      "step": 171
    },
    {
      "epoch": 0.06882753101240496,
      "grad_norm": 0.11629898101091385,
      "learning_rate": 0.00018660785886126706,
      "loss": 0.4436,
      "step": 172
    },
    {
      "epoch": 0.06922769107643058,
      "grad_norm": 0.1284409463405609,
      "learning_rate": 0.00018652766639935846,
      "loss": 0.5305,
      "step": 173
    },
    {
      "epoch": 0.06962785114045618,
      "grad_norm": 0.129461869597435,
      "learning_rate": 0.00018644747393744987,
      "loss": 0.5353,
      "step": 174
    },
    {
      "epoch": 0.0700280112044818,
      "grad_norm": 0.14519193768501282,
      "learning_rate": 0.0001863672814755413,
      "loss": 0.5614,
      "step": 175
    },
    {
      "epoch": 0.07042817126850741,
      "grad_norm": 0.1073053851723671,
      "learning_rate": 0.00018628708901363271,
      "loss": 0.4797,
      "step": 176
    },
    {
      "epoch": 0.07082833133253301,
      "grad_norm": 0.11377490311861038,
      "learning_rate": 0.00018620689655172415,
      "loss": 0.5087,
      "step": 177
    },
    {
      "epoch": 0.07122849139655862,
      "grad_norm": 0.12268966436386108,
      "learning_rate": 0.00018612670408981556,
      "loss": 0.4885,
      "step": 178
    },
    {
      "epoch": 0.07162865146058424,
      "grad_norm": 0.14001303911209106,
      "learning_rate": 0.000186046511627907,
      "loss": 0.4721,
      "step": 179
    },
    {
      "epoch": 0.07202881152460984,
      "grad_norm": 0.1424003541469574,
      "learning_rate": 0.0001859663191659984,
      "loss": 0.4694,
      "step": 180
    },
    {
      "epoch": 0.07242897158863545,
      "grad_norm": 0.15293265879154205,
      "learning_rate": 0.00018588612670408984,
      "loss": 0.5452,
      "step": 181
    },
    {
      "epoch": 0.07282913165266107,
      "grad_norm": 0.1378236711025238,
      "learning_rate": 0.00018580593424218124,
      "loss": 0.5064,
      "step": 182
    },
    {
      "epoch": 0.07322929171668667,
      "grad_norm": 0.12121786177158356,
      "learning_rate": 0.00018572574178027268,
      "loss": 0.453,
      "step": 183
    },
    {
      "epoch": 0.07362945178071229,
      "grad_norm": 0.11671614646911621,
      "learning_rate": 0.00018564554931836409,
      "loss": 0.4374,
      "step": 184
    },
    {
      "epoch": 0.0740296118447379,
      "grad_norm": 0.10158341377973557,
      "learning_rate": 0.0001855653568564555,
      "loss": 0.4552,
      "step": 185
    },
    {
      "epoch": 0.0744297719087635,
      "grad_norm": 0.15136249363422394,
      "learning_rate": 0.00018548516439454693,
      "loss": 0.4898,
      "step": 186
    },
    {
      "epoch": 0.07482993197278912,
      "grad_norm": 0.12793879210948944,
      "learning_rate": 0.00018540497193263834,
      "loss": 0.415,
      "step": 187
    },
    {
      "epoch": 0.07523009203681473,
      "grad_norm": 0.12706665694713593,
      "learning_rate": 0.00018532477947072977,
      "loss": 0.4537,
      "step": 188
    },
    {
      "epoch": 0.07563025210084033,
      "grad_norm": 0.16407187283039093,
      "learning_rate": 0.00018524458700882118,
      "loss": 0.5277,
      "step": 189
    },
    {
      "epoch": 0.07603041216486595,
      "grad_norm": 0.14706821739673615,
      "learning_rate": 0.0001851643945469126,
      "loss": 0.3744,
      "step": 190
    },
    {
      "epoch": 0.07643057222889156,
      "grad_norm": 0.11390958726406097,
      "learning_rate": 0.00018508420208500402,
      "loss": 0.4619,
      "step": 191
    },
    {
      "epoch": 0.07683073229291716,
      "grad_norm": 0.12826699018478394,
      "learning_rate": 0.00018500400962309543,
      "loss": 0.4003,
      "step": 192
    },
    {
      "epoch": 0.07723089235694278,
      "grad_norm": 0.12163887172937393,
      "learning_rate": 0.00018492381716118684,
      "loss": 0.5077,
      "step": 193
    },
    {
      "epoch": 0.07763105242096839,
      "grad_norm": 0.09692992269992828,
      "learning_rate": 0.00018484362469927827,
      "loss": 0.4123,
      "step": 194
    },
    {
      "epoch": 0.07803121248499399,
      "grad_norm": 0.11546153575181961,
      "learning_rate": 0.00018476343223736968,
      "loss": 0.4667,
      "step": 195
    },
    {
      "epoch": 0.0784313725490196,
      "grad_norm": 0.12225581705570221,
      "learning_rate": 0.00018468323977546112,
      "loss": 0.5018,
      "step": 196
    },
    {
      "epoch": 0.07883153261304522,
      "grad_norm": 0.12957295775413513,
      "learning_rate": 0.00018460304731355252,
      "loss": 0.4366,
      "step": 197
    },
    {
      "epoch": 0.07923169267707082,
      "grad_norm": 0.09262481331825256,
      "learning_rate": 0.00018452285485164396,
      "loss": 0.3721,
      "step": 198
    },
    {
      "epoch": 0.07963185274109644,
      "grad_norm": 0.10136529058218002,
      "learning_rate": 0.00018444266238973537,
      "loss": 0.4445,
      "step": 199
    },
    {
      "epoch": 0.08003201280512205,
      "grad_norm": 0.11607563495635986,
      "learning_rate": 0.0001843624699278268,
      "loss": 0.4583,
      "step": 200
    },
    {
      "epoch": 0.08043217286914765,
      "grad_norm": 0.13737931847572327,
      "learning_rate": 0.0001842822774659182,
      "loss": 0.5201,
      "step": 201
    },
    {
      "epoch": 0.08083233293317327,
      "grad_norm": 0.08676411956548691,
      "learning_rate": 0.00018420208500400965,
      "loss": 0.4234,
      "step": 202
    },
    {
      "epoch": 0.08123249299719888,
      "grad_norm": 0.10479661077260971,
      "learning_rate": 0.00018412189254210105,
      "loss": 0.3939,
      "step": 203
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.10750117152929306,
      "learning_rate": 0.0001840417000801925,
      "loss": 0.3755,
      "step": 204
    },
    {
      "epoch": 0.0820328131252501,
      "grad_norm": 0.14931219816207886,
      "learning_rate": 0.00018396150761828387,
      "loss": 0.5099,
      "step": 205
    },
    {
      "epoch": 0.08243297318927571,
      "grad_norm": 0.11829051375389099,
      "learning_rate": 0.0001838813151563753,
      "loss": 0.4346,
      "step": 206
    },
    {
      "epoch": 0.08283313325330131,
      "grad_norm": 0.11502698063850403,
      "learning_rate": 0.0001838011226944667,
      "loss": 0.4818,
      "step": 207
    },
    {
      "epoch": 0.08323329331732693,
      "grad_norm": 0.10770168900489807,
      "learning_rate": 0.00018372093023255815,
      "loss": 0.4135,
      "step": 208
    },
    {
      "epoch": 0.08363345338135254,
      "grad_norm": 0.12374719977378845,
      "learning_rate": 0.00018364073777064956,
      "loss": 0.5234,
      "step": 209
    },
    {
      "epoch": 0.08403361344537816,
      "grad_norm": 0.13255833089351654,
      "learning_rate": 0.000183560545308741,
      "loss": 0.4704,
      "step": 210
    },
    {
      "epoch": 0.08443377350940376,
      "grad_norm": 0.16977503895759583,
      "learning_rate": 0.0001834803528468324,
      "loss": 0.5657,
      "step": 211
    },
    {
      "epoch": 0.08483393357342937,
      "grad_norm": 0.12468747049570084,
      "learning_rate": 0.00018340016038492383,
      "loss": 0.489,
      "step": 212
    },
    {
      "epoch": 0.08523409363745499,
      "grad_norm": 0.12520770728588104,
      "learning_rate": 0.00018331996792301524,
      "loss": 0.4905,
      "step": 213
    },
    {
      "epoch": 0.08563425370148059,
      "grad_norm": 0.13210442662239075,
      "learning_rate": 0.00018323977546110668,
      "loss": 0.4607,
      "step": 214
    },
    {
      "epoch": 0.0860344137655062,
      "grad_norm": 0.1425142139196396,
      "learning_rate": 0.00018315958299919808,
      "loss": 0.4508,
      "step": 215
    },
    {
      "epoch": 0.08643457382953182,
      "grad_norm": 0.11364100128412247,
      "learning_rate": 0.00018307939053728952,
      "loss": 0.4831,
      "step": 216
    },
    {
      "epoch": 0.08683473389355742,
      "grad_norm": 0.13135170936584473,
      "learning_rate": 0.00018299919807538093,
      "loss": 0.4568,
      "step": 217
    },
    {
      "epoch": 0.08723489395758303,
      "grad_norm": 0.13805334270000458,
      "learning_rate": 0.00018291900561347236,
      "loss": 0.5375,
      "step": 218
    },
    {
      "epoch": 0.08763505402160865,
      "grad_norm": 0.14687907695770264,
      "learning_rate": 0.00018283881315156377,
      "loss": 0.5156,
      "step": 219
    },
    {
      "epoch": 0.08803521408563425,
      "grad_norm": 0.11880427598953247,
      "learning_rate": 0.00018275862068965518,
      "loss": 0.4659,
      "step": 220
    },
    {
      "epoch": 0.08843537414965986,
      "grad_norm": 0.13191868364810944,
      "learning_rate": 0.00018267842822774659,
      "loss": 0.4912,
      "step": 221
    },
    {
      "epoch": 0.08883553421368548,
      "grad_norm": 0.13169026374816895,
      "learning_rate": 0.00018259823576583802,
      "loss": 0.4472,
      "step": 222
    },
    {
      "epoch": 0.08923569427771108,
      "grad_norm": 0.129400372505188,
      "learning_rate": 0.00018251804330392943,
      "loss": 0.5469,
      "step": 223
    },
    {
      "epoch": 0.0896358543417367,
      "grad_norm": 0.10501869767904282,
      "learning_rate": 0.00018243785084202086,
      "loss": 0.4962,
      "step": 224
    },
    {
      "epoch": 0.09003601440576231,
      "grad_norm": 0.142854705452919,
      "learning_rate": 0.00018235765838011227,
      "loss": 0.4244,
      "step": 225
    },
    {
      "epoch": 0.09043617446978791,
      "grad_norm": 0.12474144995212555,
      "learning_rate": 0.0001822774659182037,
      "loss": 0.5021,
      "step": 226
    },
    {
      "epoch": 0.09083633453381353,
      "grad_norm": 0.12875868380069733,
      "learning_rate": 0.00018219727345629511,
      "loss": 0.506,
      "step": 227
    },
    {
      "epoch": 0.09123649459783914,
      "grad_norm": 0.12214354425668716,
      "learning_rate": 0.00018211708099438652,
      "loss": 0.4917,
      "step": 228
    },
    {
      "epoch": 0.09163665466186474,
      "grad_norm": 0.12947089970111847,
      "learning_rate": 0.00018203688853247796,
      "loss": 0.519,
      "step": 229
    },
    {
      "epoch": 0.09203681472589036,
      "grad_norm": 0.11945495754480362,
      "learning_rate": 0.00018195669607056937,
      "loss": 0.3956,
      "step": 230
    },
    {
      "epoch": 0.09243697478991597,
      "grad_norm": 0.11920713633298874,
      "learning_rate": 0.0001818765036086608,
      "loss": 0.4845,
      "step": 231
    },
    {
      "epoch": 0.09283713485394157,
      "grad_norm": 0.09881073236465454,
      "learning_rate": 0.0001817963111467522,
      "loss": 0.3795,
      "step": 232
    },
    {
      "epoch": 0.09323729491796719,
      "grad_norm": 0.1333152800798416,
      "learning_rate": 0.00018171611868484364,
      "loss": 0.4739,
      "step": 233
    },
    {
      "epoch": 0.0936374549819928,
      "grad_norm": 0.21321454644203186,
      "learning_rate": 0.00018163592622293505,
      "loss": 0.5071,
      "step": 234
    },
    {
      "epoch": 0.0940376150460184,
      "grad_norm": 0.10220582038164139,
      "learning_rate": 0.00018155573376102649,
      "loss": 0.3969,
      "step": 235
    },
    {
      "epoch": 0.09443777511004402,
      "grad_norm": 0.1244305744767189,
      "learning_rate": 0.0001814755412991179,
      "loss": 0.5034,
      "step": 236
    },
    {
      "epoch": 0.09483793517406963,
      "grad_norm": 0.15226584672927856,
      "learning_rate": 0.0001813953488372093,
      "loss": 0.4636,
      "step": 237
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.11543595045804977,
      "learning_rate": 0.0001813151563753007,
      "loss": 0.4607,
      "step": 238
    },
    {
      "epoch": 0.09563825530212085,
      "grad_norm": 0.13535559177398682,
      "learning_rate": 0.00018123496391339215,
      "loss": 0.4461,
      "step": 239
    },
    {
      "epoch": 0.09603841536614646,
      "grad_norm": 0.11766539514064789,
      "learning_rate": 0.00018115477145148355,
      "loss": 0.4675,
      "step": 240
    },
    {
      "epoch": 0.09643857543017206,
      "grad_norm": 0.11271712929010391,
      "learning_rate": 0.000181074578989575,
      "loss": 0.4652,
      "step": 241
    },
    {
      "epoch": 0.09683873549419768,
      "grad_norm": 0.13060280680656433,
      "learning_rate": 0.0001809943865276664,
      "loss": 0.4804,
      "step": 242
    },
    {
      "epoch": 0.09723889555822329,
      "grad_norm": 0.12125249952077866,
      "learning_rate": 0.00018091419406575783,
      "loss": 0.3948,
      "step": 243
    },
    {
      "epoch": 0.0976390556222489,
      "grad_norm": 0.13155825436115265,
      "learning_rate": 0.00018083400160384924,
      "loss": 0.4633,
      "step": 244
    },
    {
      "epoch": 0.09803921568627451,
      "grad_norm": 0.09374501556158066,
      "learning_rate": 0.00018075380914194067,
      "loss": 0.4154,
      "step": 245
    },
    {
      "epoch": 0.09843937575030012,
      "grad_norm": 0.09847457706928253,
      "learning_rate": 0.00018067361668003208,
      "loss": 0.354,
      "step": 246
    },
    {
      "epoch": 0.09883953581432572,
      "grad_norm": 0.11667906492948532,
      "learning_rate": 0.00018059342421812352,
      "loss": 0.4854,
      "step": 247
    },
    {
      "epoch": 0.09923969587835134,
      "grad_norm": 0.16436767578125,
      "learning_rate": 0.00018051323175621492,
      "loss": 0.466,
      "step": 248
    },
    {
      "epoch": 0.09963985594237695,
      "grad_norm": 0.12961305677890778,
      "learning_rate": 0.00018043303929430636,
      "loss": 0.4584,
      "step": 249
    },
    {
      "epoch": 0.10004001600640255,
      "grad_norm": 0.1056259423494339,
      "learning_rate": 0.00018035284683239777,
      "loss": 0.3701,
      "step": 250
    },
    {
      "epoch": 0.10044017607042817,
      "grad_norm": 0.10037931799888611,
      "learning_rate": 0.0001802726543704892,
      "loss": 0.5,
      "step": 251
    },
    {
      "epoch": 0.10084033613445378,
      "grad_norm": 0.09612792730331421,
      "learning_rate": 0.0001801924619085806,
      "loss": 0.3741,
      "step": 252
    },
    {
      "epoch": 0.10124049619847938,
      "grad_norm": 0.1512654423713684,
      "learning_rate": 0.00018011226944667202,
      "loss": 0.5165,
      "step": 253
    },
    {
      "epoch": 0.101640656262505,
      "grad_norm": 0.10586408525705338,
      "learning_rate": 0.00018003207698476343,
      "loss": 0.3451,
      "step": 254
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 0.12365169078111649,
      "learning_rate": 0.00017995188452285486,
      "loss": 0.4748,
      "step": 255
    },
    {
      "epoch": 0.10244097639055623,
      "grad_norm": 0.10974916815757751,
      "learning_rate": 0.00017987169206094627,
      "loss": 0.4835,
      "step": 256
    },
    {
      "epoch": 0.10284113645458183,
      "grad_norm": 0.14079660177230835,
      "learning_rate": 0.0001797914995990377,
      "loss": 0.4903,
      "step": 257
    },
    {
      "epoch": 0.10324129651860744,
      "grad_norm": 0.09693942964076996,
      "learning_rate": 0.0001797113071371291,
      "loss": 0.4342,
      "step": 258
    },
    {
      "epoch": 0.10364145658263306,
      "grad_norm": 0.10766295343637466,
      "learning_rate": 0.00017963111467522055,
      "loss": 0.459,
      "step": 259
    },
    {
      "epoch": 0.10404161664665866,
      "grad_norm": 0.11035313457250595,
      "learning_rate": 0.00017955092221331196,
      "loss": 0.4589,
      "step": 260
    },
    {
      "epoch": 0.10444177671068428,
      "grad_norm": 0.10679485648870468,
      "learning_rate": 0.0001794707297514034,
      "loss": 0.4398,
      "step": 261
    },
    {
      "epoch": 0.10484193677470989,
      "grad_norm": 0.10394807159900665,
      "learning_rate": 0.0001793905372894948,
      "loss": 0.4935,
      "step": 262
    },
    {
      "epoch": 0.10524209683873549,
      "grad_norm": 0.13123735785484314,
      "learning_rate": 0.0001793103448275862,
      "loss": 0.5435,
      "step": 263
    },
    {
      "epoch": 0.1056422569027611,
      "grad_norm": 0.1056336984038353,
      "learning_rate": 0.00017923015236567764,
      "loss": 0.3922,
      "step": 264
    },
    {
      "epoch": 0.10604241696678672,
      "grad_norm": 0.11917416006326675,
      "learning_rate": 0.00017914995990376905,
      "loss": 0.5228,
      "step": 265
    },
    {
      "epoch": 0.10644257703081232,
      "grad_norm": 0.12100663036108017,
      "learning_rate": 0.00017906976744186048,
      "loss": 0.5714,
      "step": 266
    },
    {
      "epoch": 0.10684273709483794,
      "grad_norm": 0.11408606171607971,
      "learning_rate": 0.0001789895749799519,
      "loss": 0.4862,
      "step": 267
    },
    {
      "epoch": 0.10724289715886355,
      "grad_norm": 0.1179388090968132,
      "learning_rate": 0.00017890938251804333,
      "loss": 0.4293,
      "step": 268
    },
    {
      "epoch": 0.10764305722288915,
      "grad_norm": 0.12212846428155899,
      "learning_rate": 0.0001788291900561347,
      "loss": 0.4425,
      "step": 269
    },
    {
      "epoch": 0.10804321728691477,
      "grad_norm": 0.12375491112470627,
      "learning_rate": 0.00017874899759422614,
      "loss": 0.4792,
      "step": 270
    },
    {
      "epoch": 0.10844337735094038,
      "grad_norm": 0.15605853497982025,
      "learning_rate": 0.00017866880513231755,
      "loss": 0.5188,
      "step": 271
    },
    {
      "epoch": 0.10884353741496598,
      "grad_norm": 0.10675527155399323,
      "learning_rate": 0.00017858861267040899,
      "loss": 0.4343,
      "step": 272
    },
    {
      "epoch": 0.1092436974789916,
      "grad_norm": 0.1305278092622757,
      "learning_rate": 0.0001785084202085004,
      "loss": 0.5762,
      "step": 273
    },
    {
      "epoch": 0.10964385754301721,
      "grad_norm": 0.1937091052532196,
      "learning_rate": 0.00017842822774659183,
      "loss": 0.5146,
      "step": 274
    },
    {
      "epoch": 0.11004401760704281,
      "grad_norm": 0.09900829941034317,
      "learning_rate": 0.00017834803528468324,
      "loss": 0.4083,
      "step": 275
    },
    {
      "epoch": 0.11044417767106843,
      "grad_norm": 0.14200322329998016,
      "learning_rate": 0.00017826784282277467,
      "loss": 0.4381,
      "step": 276
    },
    {
      "epoch": 0.11084433773509404,
      "grad_norm": 0.14532408118247986,
      "learning_rate": 0.00017818765036086608,
      "loss": 0.465,
      "step": 277
    },
    {
      "epoch": 0.11124449779911964,
      "grad_norm": 0.12740176916122437,
      "learning_rate": 0.00017810745789895751,
      "loss": 0.4958,
      "step": 278
    },
    {
      "epoch": 0.11164465786314526,
      "grad_norm": 0.08783189207315445,
      "learning_rate": 0.00017802726543704892,
      "loss": 0.393,
      "step": 279
    },
    {
      "epoch": 0.11204481792717087,
      "grad_norm": 0.11317205429077148,
      "learning_rate": 0.00017794707297514036,
      "loss": 0.5153,
      "step": 280
    },
    {
      "epoch": 0.11244497799119647,
      "grad_norm": 0.10008110105991364,
      "learning_rate": 0.00017786688051323177,
      "loss": 0.4835,
      "step": 281
    },
    {
      "epoch": 0.11284513805522209,
      "grad_norm": 0.11175697296857834,
      "learning_rate": 0.0001777866880513232,
      "loss": 0.4736,
      "step": 282
    },
    {
      "epoch": 0.1132452981192477,
      "grad_norm": 0.1468551605939865,
      "learning_rate": 0.0001777064955894146,
      "loss": 0.401,
      "step": 283
    },
    {
      "epoch": 0.1136454581832733,
      "grad_norm": 0.13485932350158691,
      "learning_rate": 0.00017762630312750604,
      "loss": 0.543,
      "step": 284
    },
    {
      "epoch": 0.11404561824729892,
      "grad_norm": 0.11521459370851517,
      "learning_rate": 0.00017754611066559742,
      "loss": 0.4094,
      "step": 285
    },
    {
      "epoch": 0.11444577831132453,
      "grad_norm": 0.11055447161197662,
      "learning_rate": 0.00017746591820368886,
      "loss": 0.4408,
      "step": 286
    },
    {
      "epoch": 0.11484593837535013,
      "grad_norm": 0.10583039373159409,
      "learning_rate": 0.00017738572574178027,
      "loss": 0.4295,
      "step": 287
    },
    {
      "epoch": 0.11524609843937575,
      "grad_norm": 0.12624934315681458,
      "learning_rate": 0.0001773055332798717,
      "loss": 0.5609,
      "step": 288
    },
    {
      "epoch": 0.11564625850340136,
      "grad_norm": 0.11090149730443954,
      "learning_rate": 0.0001772253408179631,
      "loss": 0.4221,
      "step": 289
    },
    {
      "epoch": 0.11604641856742696,
      "grad_norm": 0.11755429953336716,
      "learning_rate": 0.00017714514835605455,
      "loss": 0.4751,
      "step": 290
    },
    {
      "epoch": 0.11644657863145258,
      "grad_norm": 0.13662026822566986,
      "learning_rate": 0.00017706495589414595,
      "loss": 0.3998,
      "step": 291
    },
    {
      "epoch": 0.1168467386954782,
      "grad_norm": 0.13170164823532104,
      "learning_rate": 0.0001769847634322374,
      "loss": 0.5786,
      "step": 292
    },
    {
      "epoch": 0.1172468987595038,
      "grad_norm": 0.12707193195819855,
      "learning_rate": 0.0001769045709703288,
      "loss": 0.5697,
      "step": 293
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.1291714608669281,
      "learning_rate": 0.00017682437850842023,
      "loss": 0.4786,
      "step": 294
    },
    {
      "epoch": 0.11804721888755502,
      "grad_norm": 0.11673704534769058,
      "learning_rate": 0.00017674418604651164,
      "loss": 0.4205,
      "step": 295
    },
    {
      "epoch": 0.11844737895158063,
      "grad_norm": 0.1406221091747284,
      "learning_rate": 0.00017666399358460305,
      "loss": 0.4725,
      "step": 296
    },
    {
      "epoch": 0.11884753901560624,
      "grad_norm": 0.10736479610204697,
      "learning_rate": 0.00017658380112269448,
      "loss": 0.4096,
      "step": 297
    },
    {
      "epoch": 0.11924769907963186,
      "grad_norm": 0.1359228491783142,
      "learning_rate": 0.0001765036086607859,
      "loss": 0.4967,
      "step": 298
    },
    {
      "epoch": 0.11964785914365746,
      "grad_norm": 0.10314254462718964,
      "learning_rate": 0.00017642341619887732,
      "loss": 0.3891,
      "step": 299
    },
    {
      "epoch": 0.12004801920768307,
      "grad_norm": 0.1572863757610321,
      "learning_rate": 0.00017634322373696873,
      "loss": 0.5153,
      "step": 300
    },
    {
      "epoch": 0.12044817927170869,
      "grad_norm": 0.11621096730232239,
      "learning_rate": 0.00017626303127506014,
      "loss": 0.4936,
      "step": 301
    },
    {
      "epoch": 0.1208483393357343,
      "grad_norm": 0.12380082160234451,
      "learning_rate": 0.00017618283881315158,
      "loss": 0.3939,
      "step": 302
    },
    {
      "epoch": 0.1212484993997599,
      "grad_norm": 0.13955582678318024,
      "learning_rate": 0.00017610264635124298,
      "loss": 0.521,
      "step": 303
    },
    {
      "epoch": 0.12164865946378552,
      "grad_norm": 0.12239433079957962,
      "learning_rate": 0.0001760224538893344,
      "loss": 0.497,
      "step": 304
    },
    {
      "epoch": 0.12204881952781113,
      "grad_norm": 0.11798510700464249,
      "learning_rate": 0.00017594226142742583,
      "loss": 0.4406,
      "step": 305
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 0.12374099344015121,
      "learning_rate": 0.00017586206896551723,
      "loss": 0.4515,
      "step": 306
    },
    {
      "epoch": 0.12284913965586235,
      "grad_norm": 0.09981679171323776,
      "learning_rate": 0.00017578187650360867,
      "loss": 0.3844,
      "step": 307
    },
    {
      "epoch": 0.12324929971988796,
      "grad_norm": 0.12497119605541229,
      "learning_rate": 0.00017570168404170008,
      "loss": 0.5248,
      "step": 308
    },
    {
      "epoch": 0.12364945978391356,
      "grad_norm": 0.11952691525220871,
      "learning_rate": 0.0001756214915797915,
      "loss": 0.5037,
      "step": 309
    },
    {
      "epoch": 0.12404961984793918,
      "grad_norm": 0.13054552674293518,
      "learning_rate": 0.00017554129911788292,
      "loss": 0.5707,
      "step": 310
    },
    {
      "epoch": 0.12444977991196479,
      "grad_norm": 0.11273949593305588,
      "learning_rate": 0.00017546110665597436,
      "loss": 0.4412,
      "step": 311
    },
    {
      "epoch": 0.12484993997599039,
      "grad_norm": 0.17011168599128723,
      "learning_rate": 0.00017538091419406576,
      "loss": 0.5804,
      "step": 312
    },
    {
      "epoch": 0.125250100040016,
      "grad_norm": 0.11116588115692139,
      "learning_rate": 0.0001753007217321572,
      "loss": 0.4479,
      "step": 313
    },
    {
      "epoch": 0.12565026010404162,
      "grad_norm": 0.14362746477127075,
      "learning_rate": 0.0001752205292702486,
      "loss": 0.5173,
      "step": 314
    },
    {
      "epoch": 0.12605042016806722,
      "grad_norm": 0.10549671947956085,
      "learning_rate": 0.00017514033680834004,
      "loss": 0.4456,
      "step": 315
    },
    {
      "epoch": 0.12645058023209282,
      "grad_norm": 0.12621377408504486,
      "learning_rate": 0.00017506014434643145,
      "loss": 0.5298,
      "step": 316
    },
    {
      "epoch": 0.12685074029611845,
      "grad_norm": 0.10526707023382187,
      "learning_rate": 0.00017497995188452286,
      "loss": 0.4735,
      "step": 317
    },
    {
      "epoch": 0.12725090036014405,
      "grad_norm": 0.12645408511161804,
      "learning_rate": 0.00017489975942261426,
      "loss": 0.4554,
      "step": 318
    },
    {
      "epoch": 0.12765106042416965,
      "grad_norm": 0.1174137145280838,
      "learning_rate": 0.0001748195669607057,
      "loss": 0.5211,
      "step": 319
    },
    {
      "epoch": 0.12805122048819528,
      "grad_norm": 0.1203165203332901,
      "learning_rate": 0.0001747393744987971,
      "loss": 0.4488,
      "step": 320
    },
    {
      "epoch": 0.12845138055222088,
      "grad_norm": 0.11035666614770889,
      "learning_rate": 0.00017465918203688854,
      "loss": 0.4815,
      "step": 321
    },
    {
      "epoch": 0.12885154061624648,
      "grad_norm": 0.12337521463632584,
      "learning_rate": 0.00017457898957497995,
      "loss": 0.4624,
      "step": 322
    },
    {
      "epoch": 0.1292517006802721,
      "grad_norm": 0.1361181139945984,
      "learning_rate": 0.00017449879711307139,
      "loss": 0.4383,
      "step": 323
    },
    {
      "epoch": 0.12965186074429771,
      "grad_norm": 0.11422773450613022,
      "learning_rate": 0.0001744186046511628,
      "loss": 0.4721,
      "step": 324
    },
    {
      "epoch": 0.13005202080832334,
      "grad_norm": 0.12968993186950684,
      "learning_rate": 0.00017433841218925423,
      "loss": 0.5627,
      "step": 325
    },
    {
      "epoch": 0.13045218087234894,
      "grad_norm": 0.13785965740680695,
      "learning_rate": 0.00017425821972734564,
      "loss": 0.5087,
      "step": 326
    },
    {
      "epoch": 0.13085234093637454,
      "grad_norm": 0.08839952945709229,
      "learning_rate": 0.00017417802726543707,
      "loss": 0.4361,
      "step": 327
    },
    {
      "epoch": 0.13125250100040017,
      "grad_norm": 0.12111512571573257,
      "learning_rate": 0.00017409783480352848,
      "loss": 0.5447,
      "step": 328
    },
    {
      "epoch": 0.13165266106442577,
      "grad_norm": 0.1203237771987915,
      "learning_rate": 0.00017401764234161991,
      "loss": 0.3946,
      "step": 329
    },
    {
      "epoch": 0.13205282112845138,
      "grad_norm": 0.1114087700843811,
      "learning_rate": 0.00017393744987971132,
      "loss": 0.4766,
      "step": 330
    },
    {
      "epoch": 0.132452981192477,
      "grad_norm": 0.1127285435795784,
      "learning_rate": 0.00017385725741780273,
      "loss": 0.4124,
      "step": 331
    },
    {
      "epoch": 0.1328531412565026,
      "grad_norm": 0.10884755849838257,
      "learning_rate": 0.00017377706495589417,
      "loss": 0.4588,
      "step": 332
    },
    {
      "epoch": 0.1332533013205282,
      "grad_norm": 0.10835112631320953,
      "learning_rate": 0.00017369687249398557,
      "loss": 0.4079,
      "step": 333
    },
    {
      "epoch": 0.13365346138455383,
      "grad_norm": 0.10405492037534714,
      "learning_rate": 0.00017361668003207698,
      "loss": 0.4032,
      "step": 334
    },
    {
      "epoch": 0.13405362144857944,
      "grad_norm": 0.14053793251514435,
      "learning_rate": 0.00017353648757016842,
      "loss": 0.5664,
      "step": 335
    },
    {
      "epoch": 0.13445378151260504,
      "grad_norm": 0.1297028511762619,
      "learning_rate": 0.00017345629510825982,
      "loss": 0.4983,
      "step": 336
    },
    {
      "epoch": 0.13485394157663066,
      "grad_norm": 0.1314513087272644,
      "learning_rate": 0.00017337610264635126,
      "loss": 0.4583,
      "step": 337
    },
    {
      "epoch": 0.13525410164065627,
      "grad_norm": 0.16063039004802704,
      "learning_rate": 0.00017329591018444267,
      "loss": 0.5617,
      "step": 338
    },
    {
      "epoch": 0.13565426170468187,
      "grad_norm": 0.13425534963607788,
      "learning_rate": 0.00017321571772253408,
      "loss": 0.4064,
      "step": 339
    },
    {
      "epoch": 0.1360544217687075,
      "grad_norm": 0.09534755349159241,
      "learning_rate": 0.0001731355252606255,
      "loss": 0.4619,
      "step": 340
    },
    {
      "epoch": 0.1364545818327331,
      "grad_norm": 0.12575189769268036,
      "learning_rate": 0.00017305533279871692,
      "loss": 0.4923,
      "step": 341
    },
    {
      "epoch": 0.1368547418967587,
      "grad_norm": 0.1369265913963318,
      "learning_rate": 0.00017297514033680835,
      "loss": 0.5129,
      "step": 342
    },
    {
      "epoch": 0.13725490196078433,
      "grad_norm": 0.11007128655910492,
      "learning_rate": 0.00017289494787489976,
      "loss": 0.4752,
      "step": 343
    },
    {
      "epoch": 0.13765506202480993,
      "grad_norm": 0.11221540719270706,
      "learning_rate": 0.0001728147554129912,
      "loss": 0.4917,
      "step": 344
    },
    {
      "epoch": 0.13805522208883553,
      "grad_norm": 0.11158888041973114,
      "learning_rate": 0.0001727345629510826,
      "loss": 0.5108,
      "step": 345
    },
    {
      "epoch": 0.13845538215286116,
      "grad_norm": 0.11136064678430557,
      "learning_rate": 0.00017265437048917404,
      "loss": 0.4805,
      "step": 346
    },
    {
      "epoch": 0.13885554221688676,
      "grad_norm": 0.1117139682173729,
      "learning_rate": 0.00017257417802726545,
      "loss": 0.4159,
      "step": 347
    },
    {
      "epoch": 0.13925570228091236,
      "grad_norm": 0.12443190813064575,
      "learning_rate": 0.00017249398556535688,
      "loss": 0.4529,
      "step": 348
    },
    {
      "epoch": 0.139655862344938,
      "grad_norm": 0.1258106380701065,
      "learning_rate": 0.00017241379310344826,
      "loss": 0.5022,
      "step": 349
    },
    {
      "epoch": 0.1400560224089636,
      "grad_norm": 0.11179228127002716,
      "learning_rate": 0.0001723336006415397,
      "loss": 0.4809,
      "step": 350
    },
    {
      "epoch": 0.1404561824729892,
      "grad_norm": 0.1076909527182579,
      "learning_rate": 0.0001722534081796311,
      "loss": 0.4603,
      "step": 351
    },
    {
      "epoch": 0.14085634253701482,
      "grad_norm": 0.11836329847574234,
      "learning_rate": 0.00017217321571772254,
      "loss": 0.4798,
      "step": 352
    },
    {
      "epoch": 0.14125650260104042,
      "grad_norm": 0.12494666874408722,
      "learning_rate": 0.00017209302325581395,
      "loss": 0.4897,
      "step": 353
    },
    {
      "epoch": 0.14165666266506602,
      "grad_norm": 0.13211901485919952,
      "learning_rate": 0.00017201283079390538,
      "loss": 0.5272,
      "step": 354
    },
    {
      "epoch": 0.14205682272909165,
      "grad_norm": 0.10374327003955841,
      "learning_rate": 0.0001719326383319968,
      "loss": 0.3856,
      "step": 355
    },
    {
      "epoch": 0.14245698279311725,
      "grad_norm": 0.10176452994346619,
      "learning_rate": 0.00017185244587008823,
      "loss": 0.4218,
      "step": 356
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.11028917878866196,
      "learning_rate": 0.00017177225340817963,
      "loss": 0.5246,
      "step": 357
    },
    {
      "epoch": 0.14325730292116848,
      "grad_norm": 0.09067730605602264,
      "learning_rate": 0.00017169206094627107,
      "loss": 0.3673,
      "step": 358
    },
    {
      "epoch": 0.14365746298519408,
      "grad_norm": 0.09684707969427109,
      "learning_rate": 0.00017161186848436248,
      "loss": 0.4441,
      "step": 359
    },
    {
      "epoch": 0.14405762304921968,
      "grad_norm": 0.1187102198600769,
      "learning_rate": 0.0001715316760224539,
      "loss": 0.4855,
      "step": 360
    },
    {
      "epoch": 0.1444577831132453,
      "grad_norm": 0.11125773936510086,
      "learning_rate": 0.00017145148356054532,
      "loss": 0.4581,
      "step": 361
    },
    {
      "epoch": 0.1448579431772709,
      "grad_norm": 0.13981030881404877,
      "learning_rate": 0.00017137129109863676,
      "loss": 0.5043,
      "step": 362
    },
    {
      "epoch": 0.1452581032412965,
      "grad_norm": 0.12813803553581238,
      "learning_rate": 0.00017129109863672816,
      "loss": 0.4848,
      "step": 363
    },
    {
      "epoch": 0.14565826330532214,
      "grad_norm": 0.08456263691186905,
      "learning_rate": 0.0001712109061748196,
      "loss": 0.3236,
      "step": 364
    },
    {
      "epoch": 0.14605842336934774,
      "grad_norm": 0.13357853889465332,
      "learning_rate": 0.00017113071371291098,
      "loss": 0.5086,
      "step": 365
    },
    {
      "epoch": 0.14645858343337334,
      "grad_norm": 0.11530838906764984,
      "learning_rate": 0.00017105052125100241,
      "loss": 0.5666,
      "step": 366
    },
    {
      "epoch": 0.14685874349739897,
      "grad_norm": 0.09979753196239471,
      "learning_rate": 0.00017097032878909382,
      "loss": 0.3689,
      "step": 367
    },
    {
      "epoch": 0.14725890356142457,
      "grad_norm": 0.1561024934053421,
      "learning_rate": 0.00017089013632718526,
      "loss": 0.4745,
      "step": 368
    },
    {
      "epoch": 0.14765906362545017,
      "grad_norm": 0.10678814351558685,
      "learning_rate": 0.00017080994386527666,
      "loss": 0.3846,
      "step": 369
    },
    {
      "epoch": 0.1480592236894758,
      "grad_norm": 0.0987798348069191,
      "learning_rate": 0.0001707297514033681,
      "loss": 0.3925,
      "step": 370
    },
    {
      "epoch": 0.1484593837535014,
      "grad_norm": 0.12061325460672379,
      "learning_rate": 0.0001706495589414595,
      "loss": 0.4914,
      "step": 371
    },
    {
      "epoch": 0.148859543817527,
      "grad_norm": 0.09760754555463791,
      "learning_rate": 0.00017056936647955094,
      "loss": 0.4775,
      "step": 372
    },
    {
      "epoch": 0.14925970388155263,
      "grad_norm": 0.09186648577451706,
      "learning_rate": 0.00017048917401764235,
      "loss": 0.4746,
      "step": 373
    },
    {
      "epoch": 0.14965986394557823,
      "grad_norm": 0.10475005954504013,
      "learning_rate": 0.00017040898155573376,
      "loss": 0.5157,
      "step": 374
    },
    {
      "epoch": 0.15006002400960383,
      "grad_norm": 0.11618597060441971,
      "learning_rate": 0.0001703287890938252,
      "loss": 0.5434,
      "step": 375
    },
    {
      "epoch": 0.15046018407362946,
      "grad_norm": 0.09750402718782425,
      "learning_rate": 0.0001702485966319166,
      "loss": 0.4448,
      "step": 376
    },
    {
      "epoch": 0.15086034413765506,
      "grad_norm": 0.09555304050445557,
      "learning_rate": 0.00017016840417000804,
      "loss": 0.484,
      "step": 377
    },
    {
      "epoch": 0.15126050420168066,
      "grad_norm": 0.10915882140398026,
      "learning_rate": 0.00017008821170809944,
      "loss": 0.4575,
      "step": 378
    },
    {
      "epoch": 0.1516606642657063,
      "grad_norm": 0.1095331460237503,
      "learning_rate": 0.00017000801924619088,
      "loss": 0.4696,
      "step": 379
    },
    {
      "epoch": 0.1520608243297319,
      "grad_norm": 0.10085665434598923,
      "learning_rate": 0.0001699278267842823,
      "loss": 0.4205,
      "step": 380
    },
    {
      "epoch": 0.1524609843937575,
      "grad_norm": 0.10788648575544357,
      "learning_rate": 0.0001698476343223737,
      "loss": 0.3818,
      "step": 381
    },
    {
      "epoch": 0.15286114445778312,
      "grad_norm": 0.12082894891500473,
      "learning_rate": 0.0001697674418604651,
      "loss": 0.5246,
      "step": 382
    },
    {
      "epoch": 0.15326130452180872,
      "grad_norm": 0.10244151949882507,
      "learning_rate": 0.00016968724939855654,
      "loss": 0.4348,
      "step": 383
    },
    {
      "epoch": 0.15366146458583432,
      "grad_norm": 0.09938404709100723,
      "learning_rate": 0.00016960705693664795,
      "loss": 0.4506,
      "step": 384
    },
    {
      "epoch": 0.15406162464985995,
      "grad_norm": 0.09856906533241272,
      "learning_rate": 0.00016952686447473938,
      "loss": 0.3809,
      "step": 385
    },
    {
      "epoch": 0.15446178471388555,
      "grad_norm": 0.12787072360515594,
      "learning_rate": 0.0001694466720128308,
      "loss": 0.4216,
      "step": 386
    },
    {
      "epoch": 0.15486194477791115,
      "grad_norm": 0.09817204624414444,
      "learning_rate": 0.00016936647955092222,
      "loss": 0.4613,
      "step": 387
    },
    {
      "epoch": 0.15526210484193678,
      "grad_norm": 0.12146846204996109,
      "learning_rate": 0.00016928628708901363,
      "loss": 0.4937,
      "step": 388
    },
    {
      "epoch": 0.15566226490596238,
      "grad_norm": 0.10243633389472961,
      "learning_rate": 0.00016920609462710507,
      "loss": 0.4093,
      "step": 389
    },
    {
      "epoch": 0.15606242496998798,
      "grad_norm": 0.13339482247829437,
      "learning_rate": 0.00016912590216519648,
      "loss": 0.5252,
      "step": 390
    },
    {
      "epoch": 0.1564625850340136,
      "grad_norm": 0.11929335445165634,
      "learning_rate": 0.0001690457097032879,
      "loss": 0.4622,
      "step": 391
    },
    {
      "epoch": 0.1568627450980392,
      "grad_norm": 0.10773039609193802,
      "learning_rate": 0.00016896551724137932,
      "loss": 0.5269,
      "step": 392
    },
    {
      "epoch": 0.15726290516206481,
      "grad_norm": 0.09540361166000366,
      "learning_rate": 0.00016888532477947075,
      "loss": 0.4192,
      "step": 393
    },
    {
      "epoch": 0.15766306522609044,
      "grad_norm": 0.10514029115438461,
      "learning_rate": 0.00016880513231756216,
      "loss": 0.4241,
      "step": 394
    },
    {
      "epoch": 0.15806322529011604,
      "grad_norm": 0.11016156524419785,
      "learning_rate": 0.0001687249398556536,
      "loss": 0.4529,
      "step": 395
    },
    {
      "epoch": 0.15846338535414164,
      "grad_norm": 0.10774089395999908,
      "learning_rate": 0.000168644747393745,
      "loss": 0.4657,
      "step": 396
    },
    {
      "epoch": 0.15886354541816727,
      "grad_norm": 0.1095195934176445,
      "learning_rate": 0.0001685645549318364,
      "loss": 0.4473,
      "step": 397
    },
    {
      "epoch": 0.15926370548219287,
      "grad_norm": 0.10802184045314789,
      "learning_rate": 0.00016848436246992782,
      "loss": 0.45,
      "step": 398
    },
    {
      "epoch": 0.15966386554621848,
      "grad_norm": 0.09583262354135513,
      "learning_rate": 0.00016840417000801925,
      "loss": 0.3974,
      "step": 399
    },
    {
      "epoch": 0.1600640256102441,
      "grad_norm": 0.11612385511398315,
      "learning_rate": 0.00016832397754611066,
      "loss": 0.4723,
      "step": 400
    },
    {
      "epoch": 0.1604641856742697,
      "grad_norm": 0.11543251574039459,
      "learning_rate": 0.0001682437850842021,
      "loss": 0.4373,
      "step": 401
    },
    {
      "epoch": 0.1608643457382953,
      "grad_norm": 0.10931137204170227,
      "learning_rate": 0.0001681635926222935,
      "loss": 0.5034,
      "step": 402
    },
    {
      "epoch": 0.16126450580232093,
      "grad_norm": 0.09334681183099747,
      "learning_rate": 0.00016808340016038494,
      "loss": 0.4197,
      "step": 403
    },
    {
      "epoch": 0.16166466586634654,
      "grad_norm": 0.11418827623128891,
      "learning_rate": 0.00016800320769847635,
      "loss": 0.5319,
      "step": 404
    },
    {
      "epoch": 0.16206482593037214,
      "grad_norm": 0.15250597894191742,
      "learning_rate": 0.00016792301523656778,
      "loss": 0.4587,
      "step": 405
    },
    {
      "epoch": 0.16246498599439776,
      "grad_norm": 0.08031617850065231,
      "learning_rate": 0.0001678428227746592,
      "loss": 0.3797,
      "step": 406
    },
    {
      "epoch": 0.16286514605842337,
      "grad_norm": 0.11269550025463104,
      "learning_rate": 0.0001677626303127506,
      "loss": 0.4888,
      "step": 407
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.1360541135072708,
      "learning_rate": 0.00016768243785084203,
      "loss": 0.4529,
      "step": 408
    },
    {
      "epoch": 0.1636654661864746,
      "grad_norm": 0.10395000129938126,
      "learning_rate": 0.00016760224538893344,
      "loss": 0.4864,
      "step": 409
    },
    {
      "epoch": 0.1640656262505002,
      "grad_norm": 0.12377123534679413,
      "learning_rate": 0.00016752205292702488,
      "loss": 0.4995,
      "step": 410
    },
    {
      "epoch": 0.1644657863145258,
      "grad_norm": 0.09497114270925522,
      "learning_rate": 0.00016744186046511629,
      "loss": 0.3642,
      "step": 411
    },
    {
      "epoch": 0.16486594637855143,
      "grad_norm": 0.12263194471597672,
      "learning_rate": 0.00016736166800320772,
      "loss": 0.4754,
      "step": 412
    },
    {
      "epoch": 0.16526610644257703,
      "grad_norm": 0.10228072106838226,
      "learning_rate": 0.00016728147554129913,
      "loss": 0.4232,
      "step": 413
    },
    {
      "epoch": 0.16566626650660263,
      "grad_norm": 0.11278390884399414,
      "learning_rate": 0.00016720128307939054,
      "loss": 0.3807,
      "step": 414
    },
    {
      "epoch": 0.16606642657062826,
      "grad_norm": 0.16033034026622772,
      "learning_rate": 0.00016712109061748194,
      "loss": 0.6185,
      "step": 415
    },
    {
      "epoch": 0.16646658663465386,
      "grad_norm": 0.1128154918551445,
      "learning_rate": 0.00016704089815557338,
      "loss": 0.4818,
      "step": 416
    },
    {
      "epoch": 0.16686674669867949,
      "grad_norm": 0.11332529783248901,
      "learning_rate": 0.0001669607056936648,
      "loss": 0.469,
      "step": 417
    },
    {
      "epoch": 0.1672669067627051,
      "grad_norm": 0.10386919230222702,
      "learning_rate": 0.00016688051323175622,
      "loss": 0.3989,
      "step": 418
    },
    {
      "epoch": 0.1676670668267307,
      "grad_norm": 0.11381728947162628,
      "learning_rate": 0.00016680032076984763,
      "loss": 0.4467,
      "step": 419
    },
    {
      "epoch": 0.16806722689075632,
      "grad_norm": 0.12440313398838043,
      "learning_rate": 0.00016672012830793906,
      "loss": 0.4507,
      "step": 420
    },
    {
      "epoch": 0.16846738695478192,
      "grad_norm": 0.09180522710084915,
      "learning_rate": 0.00016663993584603047,
      "loss": 0.4394,
      "step": 421
    },
    {
      "epoch": 0.16886754701880752,
      "grad_norm": 0.10706024616956711,
      "learning_rate": 0.0001665597433841219,
      "loss": 0.487,
      "step": 422
    },
    {
      "epoch": 0.16926770708283315,
      "grad_norm": 0.12211337685585022,
      "learning_rate": 0.00016647955092221332,
      "loss": 0.5312,
      "step": 423
    },
    {
      "epoch": 0.16966786714685875,
      "grad_norm": 0.11429573595523834,
      "learning_rate": 0.00016639935846030475,
      "loss": 0.4751,
      "step": 424
    },
    {
      "epoch": 0.17006802721088435,
      "grad_norm": 0.11027617007493973,
      "learning_rate": 0.00016631916599839616,
      "loss": 0.476,
      "step": 425
    },
    {
      "epoch": 0.17046818727490998,
      "grad_norm": 0.12389478832483292,
      "learning_rate": 0.0001662389735364876,
      "loss": 0.4632,
      "step": 426
    },
    {
      "epoch": 0.17086834733893558,
      "grad_norm": 0.12682238221168518,
      "learning_rate": 0.000166158781074579,
      "loss": 0.4672,
      "step": 427
    },
    {
      "epoch": 0.17126850740296118,
      "grad_norm": 0.09478241205215454,
      "learning_rate": 0.00016607858861267044,
      "loss": 0.4157,
      "step": 428
    },
    {
      "epoch": 0.1716686674669868,
      "grad_norm": 0.1543552726507187,
      "learning_rate": 0.00016599839615076182,
      "loss": 0.5299,
      "step": 429
    },
    {
      "epoch": 0.1720688275310124,
      "grad_norm": 0.10749566555023193,
      "learning_rate": 0.00016591820368885325,
      "loss": 0.4686,
      "step": 430
    },
    {
      "epoch": 0.172468987595038,
      "grad_norm": 0.13217896223068237,
      "learning_rate": 0.00016583801122694466,
      "loss": 0.5202,
      "step": 431
    },
    {
      "epoch": 0.17286914765906364,
      "grad_norm": 0.09995675832033157,
      "learning_rate": 0.0001657578187650361,
      "loss": 0.3972,
      "step": 432
    },
    {
      "epoch": 0.17326930772308924,
      "grad_norm": 0.099616639316082,
      "learning_rate": 0.0001656776263031275,
      "loss": 0.5,
      "step": 433
    },
    {
      "epoch": 0.17366946778711484,
      "grad_norm": 0.10888225585222244,
      "learning_rate": 0.00016559743384121894,
      "loss": 0.5139,
      "step": 434
    },
    {
      "epoch": 0.17406962785114047,
      "grad_norm": 0.10737816244363785,
      "learning_rate": 0.00016551724137931035,
      "loss": 0.4395,
      "step": 435
    },
    {
      "epoch": 0.17446978791516607,
      "grad_norm": 0.10688190162181854,
      "learning_rate": 0.00016543704891740178,
      "loss": 0.4147,
      "step": 436
    },
    {
      "epoch": 0.17486994797919167,
      "grad_norm": 0.09064700454473495,
      "learning_rate": 0.0001653568564554932,
      "loss": 0.4397,
      "step": 437
    },
    {
      "epoch": 0.1752701080432173,
      "grad_norm": 0.10624825209379196,
      "learning_rate": 0.00016527666399358462,
      "loss": 0.408,
      "step": 438
    },
    {
      "epoch": 0.1756702681072429,
      "grad_norm": 0.0889757052063942,
      "learning_rate": 0.00016519647153167603,
      "loss": 0.4418,
      "step": 439
    },
    {
      "epoch": 0.1760704281712685,
      "grad_norm": 0.12173134833574295,
      "learning_rate": 0.00016511627906976747,
      "loss": 0.5359,
      "step": 440
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.12943217158317566,
      "learning_rate": 0.00016503608660785888,
      "loss": 0.5427,
      "step": 441
    },
    {
      "epoch": 0.17687074829931973,
      "grad_norm": 0.09590510278940201,
      "learning_rate": 0.00016495589414595028,
      "loss": 0.4583,
      "step": 442
    },
    {
      "epoch": 0.17727090836334533,
      "grad_norm": 0.10100230574607849,
      "learning_rate": 0.00016487570168404172,
      "loss": 0.5098,
      "step": 443
    },
    {
      "epoch": 0.17767106842737096,
      "grad_norm": 0.1111992597579956,
      "learning_rate": 0.00016479550922213313,
      "loss": 0.4435,
      "step": 444
    },
    {
      "epoch": 0.17807122849139656,
      "grad_norm": 0.11293987184762955,
      "learning_rate": 0.00016471531676022453,
      "loss": 0.4399,
      "step": 445
    },
    {
      "epoch": 0.17847138855542216,
      "grad_norm": 0.13352513313293457,
      "learning_rate": 0.00016463512429831597,
      "loss": 0.4092,
      "step": 446
    },
    {
      "epoch": 0.1788715486194478,
      "grad_norm": 0.11819305270910263,
      "learning_rate": 0.00016455493183640738,
      "loss": 0.5256,
      "step": 447
    },
    {
      "epoch": 0.1792717086834734,
      "grad_norm": 0.09756582975387573,
      "learning_rate": 0.0001644747393744988,
      "loss": 0.3467,
      "step": 448
    },
    {
      "epoch": 0.179671868747499,
      "grad_norm": 0.10889743268489838,
      "learning_rate": 0.00016439454691259022,
      "loss": 0.5133,
      "step": 449
    },
    {
      "epoch": 0.18007202881152462,
      "grad_norm": 0.12313370406627655,
      "learning_rate": 0.00016431435445068163,
      "loss": 0.5422,
      "step": 450
    },
    {
      "epoch": 0.18047218887555022,
      "grad_norm": 0.13783803582191467,
      "learning_rate": 0.00016423416198877306,
      "loss": 0.4429,
      "step": 451
    },
    {
      "epoch": 0.18087234893957582,
      "grad_norm": 0.09798813611268997,
      "learning_rate": 0.00016415396952686447,
      "loss": 0.4477,
      "step": 452
    },
    {
      "epoch": 0.18127250900360145,
      "grad_norm": 0.11045589298009872,
      "learning_rate": 0.0001640737770649559,
      "loss": 0.4083,
      "step": 453
    },
    {
      "epoch": 0.18167266906762705,
      "grad_norm": 0.10568922758102417,
      "learning_rate": 0.0001639935846030473,
      "loss": 0.508,
      "step": 454
    },
    {
      "epoch": 0.18207282913165265,
      "grad_norm": 0.08680666983127594,
      "learning_rate": 0.00016391339214113875,
      "loss": 0.4307,
      "step": 455
    },
    {
      "epoch": 0.18247298919567828,
      "grad_norm": 0.10682111233472824,
      "learning_rate": 0.00016383319967923016,
      "loss": 0.4703,
      "step": 456
    },
    {
      "epoch": 0.18287314925970388,
      "grad_norm": 0.10381560027599335,
      "learning_rate": 0.0001637530072173216,
      "loss": 0.4458,
      "step": 457
    },
    {
      "epoch": 0.18327330932372948,
      "grad_norm": 0.09116524457931519,
      "learning_rate": 0.000163672814755413,
      "loss": 0.4521,
      "step": 458
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 0.10218220204114914,
      "learning_rate": 0.00016359262229350443,
      "loss": 0.3931,
      "step": 459
    },
    {
      "epoch": 0.1840736294517807,
      "grad_norm": 0.11209532618522644,
      "learning_rate": 0.00016351242983159584,
      "loss": 0.5067,
      "step": 460
    },
    {
      "epoch": 0.1844737895158063,
      "grad_norm": 0.11690597236156464,
      "learning_rate": 0.00016343223736968725,
      "loss": 0.4767,
      "step": 461
    },
    {
      "epoch": 0.18487394957983194,
      "grad_norm": 0.10228600353002548,
      "learning_rate": 0.00016335204490777866,
      "loss": 0.4961,
      "step": 462
    },
    {
      "epoch": 0.18527410964385754,
      "grad_norm": 0.10662172734737396,
      "learning_rate": 0.0001632718524458701,
      "loss": 0.4135,
      "step": 463
    },
    {
      "epoch": 0.18567426970788314,
      "grad_norm": 0.11126268655061722,
      "learning_rate": 0.0001631916599839615,
      "loss": 0.4438,
      "step": 464
    },
    {
      "epoch": 0.18607442977190877,
      "grad_norm": 0.1104714646935463,
      "learning_rate": 0.00016311146752205294,
      "loss": 0.4465,
      "step": 465
    },
    {
      "epoch": 0.18647458983593437,
      "grad_norm": 0.09419477730989456,
      "learning_rate": 0.00016303127506014434,
      "loss": 0.4064,
      "step": 466
    },
    {
      "epoch": 0.18687474989995997,
      "grad_norm": 0.11288101971149445,
      "learning_rate": 0.00016295108259823578,
      "loss": 0.4446,
      "step": 467
    },
    {
      "epoch": 0.1872749099639856,
      "grad_norm": 0.12478205561637878,
      "learning_rate": 0.0001628708901363272,
      "loss": 0.5268,
      "step": 468
    },
    {
      "epoch": 0.1876750700280112,
      "grad_norm": 0.12878870964050293,
      "learning_rate": 0.00016279069767441862,
      "loss": 0.5244,
      "step": 469
    },
    {
      "epoch": 0.1880752300920368,
      "grad_norm": 0.11776390671730042,
      "learning_rate": 0.00016271050521251003,
      "loss": 0.5274,
      "step": 470
    },
    {
      "epoch": 0.18847539015606243,
      "grad_norm": 0.13141551613807678,
      "learning_rate": 0.00016263031275060146,
      "loss": 0.5275,
      "step": 471
    },
    {
      "epoch": 0.18887555022008803,
      "grad_norm": 0.13568970561027527,
      "learning_rate": 0.00016255012028869287,
      "loss": 0.4357,
      "step": 472
    },
    {
      "epoch": 0.18927571028411364,
      "grad_norm": 0.14077135920524597,
      "learning_rate": 0.0001624699278267843,
      "loss": 0.5637,
      "step": 473
    },
    {
      "epoch": 0.18967587034813926,
      "grad_norm": 0.07844144105911255,
      "learning_rate": 0.00016238973536487572,
      "loss": 0.3779,
      "step": 474
    },
    {
      "epoch": 0.19007603041216486,
      "grad_norm": 0.1034446656703949,
      "learning_rate": 0.00016230954290296715,
      "loss": 0.4779,
      "step": 475
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.12724757194519043,
      "learning_rate": 0.00016222935044105856,
      "loss": 0.4757,
      "step": 476
    },
    {
      "epoch": 0.1908763505402161,
      "grad_norm": 0.1189434677362442,
      "learning_rate": 0.00016214915797914997,
      "loss": 0.488,
      "step": 477
    },
    {
      "epoch": 0.1912765106042417,
      "grad_norm": 0.1115308403968811,
      "learning_rate": 0.00016206896551724137,
      "loss": 0.5036,
      "step": 478
    },
    {
      "epoch": 0.1916766706682673,
      "grad_norm": 0.09522669017314911,
      "learning_rate": 0.0001619887730553328,
      "loss": 0.4545,
      "step": 479
    },
    {
      "epoch": 0.19207683073229292,
      "grad_norm": 0.108955018222332,
      "learning_rate": 0.00016190858059342422,
      "loss": 0.4825,
      "step": 480
    },
    {
      "epoch": 0.19247699079631853,
      "grad_norm": 0.12519310414791107,
      "learning_rate": 0.00016182838813151565,
      "loss": 0.57,
      "step": 481
    },
    {
      "epoch": 0.19287715086034413,
      "grad_norm": 0.14481234550476074,
      "learning_rate": 0.00016174819566960706,
      "loss": 0.4503,
      "step": 482
    },
    {
      "epoch": 0.19327731092436976,
      "grad_norm": 0.10824614018201828,
      "learning_rate": 0.0001616680032076985,
      "loss": 0.4371,
      "step": 483
    },
    {
      "epoch": 0.19367747098839536,
      "grad_norm": 0.10712645202875137,
      "learning_rate": 0.0001615878107457899,
      "loss": 0.4575,
      "step": 484
    },
    {
      "epoch": 0.19407763105242096,
      "grad_norm": 0.13719598948955536,
      "learning_rate": 0.0001615076182838813,
      "loss": 0.519,
      "step": 485
    },
    {
      "epoch": 0.19447779111644659,
      "grad_norm": 0.11803009361028671,
      "learning_rate": 0.00016142742582197275,
      "loss": 0.4316,
      "step": 486
    },
    {
      "epoch": 0.1948779511804722,
      "grad_norm": 0.1473352015018463,
      "learning_rate": 0.00016134723336006415,
      "loss": 0.535,
      "step": 487
    },
    {
      "epoch": 0.1952781112444978,
      "grad_norm": 0.10205501317977905,
      "learning_rate": 0.0001612670408981556,
      "loss": 0.4562,
      "step": 488
    },
    {
      "epoch": 0.19567827130852342,
      "grad_norm": 0.13764865696430206,
      "learning_rate": 0.000161186848436247,
      "loss": 0.5613,
      "step": 489
    },
    {
      "epoch": 0.19607843137254902,
      "grad_norm": 0.11124549061059952,
      "learning_rate": 0.00016110665597433843,
      "loss": 0.4485,
      "step": 490
    },
    {
      "epoch": 0.19647859143657462,
      "grad_norm": 0.11892858892679214,
      "learning_rate": 0.00016102646351242984,
      "loss": 0.558,
      "step": 491
    },
    {
      "epoch": 0.19687875150060025,
      "grad_norm": 0.11151297390460968,
      "learning_rate": 0.00016094627105052128,
      "loss": 0.4032,
      "step": 492
    },
    {
      "epoch": 0.19727891156462585,
      "grad_norm": 0.08672972023487091,
      "learning_rate": 0.00016086607858861266,
      "loss": 0.3667,
      "step": 493
    },
    {
      "epoch": 0.19767907162865145,
      "grad_norm": 0.0961608961224556,
      "learning_rate": 0.0001607858861267041,
      "loss": 0.4499,
      "step": 494
    },
    {
      "epoch": 0.19807923169267708,
      "grad_norm": 0.11347705125808716,
      "learning_rate": 0.0001607056936647955,
      "loss": 0.4791,
      "step": 495
    },
    {
      "epoch": 0.19847939175670268,
      "grad_norm": 0.11131388694047928,
      "learning_rate": 0.00016062550120288693,
      "loss": 0.5169,
      "step": 496
    },
    {
      "epoch": 0.19887955182072828,
      "grad_norm": 0.11383651196956635,
      "learning_rate": 0.00016054530874097834,
      "loss": 0.4702,
      "step": 497
    },
    {
      "epoch": 0.1992797118847539,
      "grad_norm": 0.1742691546678543,
      "learning_rate": 0.00016046511627906978,
      "loss": 0.5,
      "step": 498
    },
    {
      "epoch": 0.1996798719487795,
      "grad_norm": 0.1191936507821083,
      "learning_rate": 0.00016038492381716118,
      "loss": 0.4621,
      "step": 499
    },
    {
      "epoch": 0.2000800320128051,
      "grad_norm": 0.10533282905817032,
      "learning_rate": 0.00016030473135525262,
      "loss": 0.4328,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 2499,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.349648375175414e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
