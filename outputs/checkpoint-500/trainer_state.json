{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.01938454082868912,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 3.876908165737824e-05,
      "grad_norm": 3.076904535293579,
      "learning_rate": 4e-05,
      "loss": 3.4603,
      "step": 1
    },
    {
      "epoch": 7.753816331475648e-05,
      "grad_norm": 3.215231418609619,
      "learning_rate": 8e-05,
      "loss": 3.4399,
      "step": 2
    },
    {
      "epoch": 0.00011630724497213472,
      "grad_norm": 2.72213077545166,
      "learning_rate": 0.00012,
      "loss": 3.3332,
      "step": 3
    },
    {
      "epoch": 0.00015507632662951296,
      "grad_norm": 1.9455198049545288,
      "learning_rate": 0.00016,
      "loss": 3.2079,
      "step": 4
    },
    {
      "epoch": 0.0001938454082868912,
      "grad_norm": 1.9726125001907349,
      "learning_rate": 0.0002,
      "loss": 3.0827,
      "step": 5
    },
    {
      "epoch": 0.00023261448994426945,
      "grad_norm": 2.2453792095184326,
      "learning_rate": 0.00019999224445478518,
      "loss": 2.8358,
      "step": 6
    },
    {
      "epoch": 0.0002713835716016477,
      "grad_norm": 2.065769910812378,
      "learning_rate": 0.00019998448890957036,
      "loss": 2.6327,
      "step": 7
    },
    {
      "epoch": 0.00031015265325902593,
      "grad_norm": 1.7409061193466187,
      "learning_rate": 0.00019997673336435553,
      "loss": 2.4464,
      "step": 8
    },
    {
      "epoch": 0.0003489217349164042,
      "grad_norm": 1.476381778717041,
      "learning_rate": 0.0001999689778191407,
      "loss": 2.2423,
      "step": 9
    },
    {
      "epoch": 0.0003876908165737824,
      "grad_norm": 1.4034909009933472,
      "learning_rate": 0.00019996122227392587,
      "loss": 2.0961,
      "step": 10
    },
    {
      "epoch": 0.00042645989823116064,
      "grad_norm": 1.2606425285339355,
      "learning_rate": 0.00019995346672871105,
      "loss": 1.8487,
      "step": 11
    },
    {
      "epoch": 0.0004652289798885389,
      "grad_norm": 1.5401495695114136,
      "learning_rate": 0.00019994571118349622,
      "loss": 1.7758,
      "step": 12
    },
    {
      "epoch": 0.0005039980615459171,
      "grad_norm": 2.1655983924865723,
      "learning_rate": 0.00019993795563828136,
      "loss": 1.677,
      "step": 13
    },
    {
      "epoch": 0.0005427671432032953,
      "grad_norm": 1.12831711769104,
      "learning_rate": 0.00019993020009306656,
      "loss": 1.67,
      "step": 14
    },
    {
      "epoch": 0.0005815362248606736,
      "grad_norm": 2.7026541233062744,
      "learning_rate": 0.0001999224445478517,
      "loss": 1.5806,
      "step": 15
    },
    {
      "epoch": 0.0006203053065180519,
      "grad_norm": 1.7286372184753418,
      "learning_rate": 0.0001999146890026369,
      "loss": 1.3481,
      "step": 16
    },
    {
      "epoch": 0.0006590743881754301,
      "grad_norm": 1.5930252075195312,
      "learning_rate": 0.00019990693345742206,
      "loss": 1.3147,
      "step": 17
    },
    {
      "epoch": 0.0006978434698328084,
      "grad_norm": 2.443732976913452,
      "learning_rate": 0.00019989917791220726,
      "loss": 1.2903,
      "step": 18
    },
    {
      "epoch": 0.0007366125514901866,
      "grad_norm": 1.2027349472045898,
      "learning_rate": 0.0001998914223669924,
      "loss": 1.1465,
      "step": 19
    },
    {
      "epoch": 0.0007753816331475648,
      "grad_norm": 1.8845245838165283,
      "learning_rate": 0.00019988366682177757,
      "loss": 0.9872,
      "step": 20
    },
    {
      "epoch": 0.000814150714804943,
      "grad_norm": 1.4270838499069214,
      "learning_rate": 0.00019987591127656275,
      "loss": 0.9575,
      "step": 21
    },
    {
      "epoch": 0.0008529197964623213,
      "grad_norm": 1.5661834478378296,
      "learning_rate": 0.00019986815573134792,
      "loss": 0.9309,
      "step": 22
    },
    {
      "epoch": 0.0008916888781196995,
      "grad_norm": 1.03627347946167,
      "learning_rate": 0.0001998604001861331,
      "loss": 0.9188,
      "step": 23
    },
    {
      "epoch": 0.0009304579597770778,
      "grad_norm": 1.6426228284835815,
      "learning_rate": 0.00019985264464091827,
      "loss": 0.9318,
      "step": 24
    },
    {
      "epoch": 0.000969227041434456,
      "grad_norm": 0.7082459330558777,
      "learning_rate": 0.00019984488909570344,
      "loss": 0.8426,
      "step": 25
    },
    {
      "epoch": 0.0010079961230918342,
      "grad_norm": 0.5908604860305786,
      "learning_rate": 0.0001998371335504886,
      "loss": 0.9475,
      "step": 26
    },
    {
      "epoch": 0.0010467652047492124,
      "grad_norm": 0.7839014530181885,
      "learning_rate": 0.00019982937800527378,
      "loss": 0.8908,
      "step": 27
    },
    {
      "epoch": 0.0010855342864065907,
      "grad_norm": 0.5976871252059937,
      "learning_rate": 0.00019982162246005896,
      "loss": 0.8808,
      "step": 28
    },
    {
      "epoch": 0.001124303368063969,
      "grad_norm": 0.7599288821220398,
      "learning_rate": 0.00019981386691484413,
      "loss": 0.9041,
      "step": 29
    },
    {
      "epoch": 0.0011630724497213472,
      "grad_norm": 0.7777177095413208,
      "learning_rate": 0.0001998061113696293,
      "loss": 0.8954,
      "step": 30
    },
    {
      "epoch": 0.0012018415313787255,
      "grad_norm": 0.8363538980484009,
      "learning_rate": 0.00019979835582441448,
      "loss": 0.9483,
      "step": 31
    },
    {
      "epoch": 0.0012406106130361037,
      "grad_norm": 0.7017698884010315,
      "learning_rate": 0.00019979060027919965,
      "loss": 0.8142,
      "step": 32
    },
    {
      "epoch": 0.001279379694693482,
      "grad_norm": 0.9329237341880798,
      "learning_rate": 0.00019978284473398482,
      "loss": 0.9068,
      "step": 33
    },
    {
      "epoch": 0.0013181487763508602,
      "grad_norm": 0.5964627861976624,
      "learning_rate": 0.00019977508918876997,
      "loss": 0.8351,
      "step": 34
    },
    {
      "epoch": 0.0013569178580082385,
      "grad_norm": 1.2543880939483643,
      "learning_rate": 0.00019976733364355517,
      "loss": 0.8335,
      "step": 35
    },
    {
      "epoch": 0.0013956869396656167,
      "grad_norm": 0.9619218111038208,
      "learning_rate": 0.0001997595780983403,
      "loss": 0.9513,
      "step": 36
    },
    {
      "epoch": 0.001434456021322995,
      "grad_norm": 0.8074339628219604,
      "learning_rate": 0.0001997518225531255,
      "loss": 0.8216,
      "step": 37
    },
    {
      "epoch": 0.0014732251029803732,
      "grad_norm": 1.0775606632232666,
      "learning_rate": 0.00019974406700791066,
      "loss": 0.7653,
      "step": 38
    },
    {
      "epoch": 0.0015119941846377515,
      "grad_norm": 1.053296685218811,
      "learning_rate": 0.00019973631146269586,
      "loss": 0.7693,
      "step": 39
    },
    {
      "epoch": 0.0015507632662951295,
      "grad_norm": 1.6107959747314453,
      "learning_rate": 0.000199728555917481,
      "loss": 0.804,
      "step": 40
    },
    {
      "epoch": 0.0015895323479525078,
      "grad_norm": 1.4093143939971924,
      "learning_rate": 0.00019972080037226618,
      "loss": 0.8094,
      "step": 41
    },
    {
      "epoch": 0.001628301429609886,
      "grad_norm": 2.0524916648864746,
      "learning_rate": 0.00019971304482705135,
      "loss": 0.7175,
      "step": 42
    },
    {
      "epoch": 0.0016670705112672643,
      "grad_norm": 2.060410261154175,
      "learning_rate": 0.00019970528928183652,
      "loss": 0.703,
      "step": 43
    },
    {
      "epoch": 0.0017058395929246426,
      "grad_norm": 0.9922105073928833,
      "learning_rate": 0.0001996975337366217,
      "loss": 0.6411,
      "step": 44
    },
    {
      "epoch": 0.0017446086745820208,
      "grad_norm": 0.9802547097206116,
      "learning_rate": 0.00019968977819140687,
      "loss": 0.5638,
      "step": 45
    },
    {
      "epoch": 0.001783377756239399,
      "grad_norm": 1.1488854885101318,
      "learning_rate": 0.00019968202264619204,
      "loss": 0.6791,
      "step": 46
    },
    {
      "epoch": 0.0018221468378967773,
      "grad_norm": 2.2893660068511963,
      "learning_rate": 0.0001996742671009772,
      "loss": 0.6355,
      "step": 47
    },
    {
      "epoch": 0.0018609159195541556,
      "grad_norm": 2.081825017929077,
      "learning_rate": 0.00019966651155576236,
      "loss": 0.6547,
      "step": 48
    },
    {
      "epoch": 0.0018996850012115338,
      "grad_norm": 1.7704328298568726,
      "learning_rate": 0.00019965875601054756,
      "loss": 0.6987,
      "step": 49
    },
    {
      "epoch": 0.001938454082868912,
      "grad_norm": 1.9767711162567139,
      "learning_rate": 0.0001996510004653327,
      "loss": 0.5354,
      "step": 50
    },
    {
      "epoch": 0.0019772231645262903,
      "grad_norm": 1.3959672451019287,
      "learning_rate": 0.0001996432449201179,
      "loss": 0.5873,
      "step": 51
    },
    {
      "epoch": 0.0020159922461836684,
      "grad_norm": 1.3025474548339844,
      "learning_rate": 0.00019963548937490305,
      "loss": 0.6591,
      "step": 52
    },
    {
      "epoch": 0.002054761327841047,
      "grad_norm": 2.2802367210388184,
      "learning_rate": 0.00019962773382968825,
      "loss": 0.6478,
      "step": 53
    },
    {
      "epoch": 0.002093530409498425,
      "grad_norm": 1.4379206895828247,
      "learning_rate": 0.0001996199782844734,
      "loss": 0.494,
      "step": 54
    },
    {
      "epoch": 0.0021322994911558034,
      "grad_norm": 1.7023591995239258,
      "learning_rate": 0.00019961222273925857,
      "loss": 0.6367,
      "step": 55
    },
    {
      "epoch": 0.0021710685728131814,
      "grad_norm": 1.3177282810211182,
      "learning_rate": 0.00019960446719404374,
      "loss": 0.4629,
      "step": 56
    },
    {
      "epoch": 0.00220983765447056,
      "grad_norm": 1.016967535018921,
      "learning_rate": 0.00019959671164882891,
      "loss": 0.4934,
      "step": 57
    },
    {
      "epoch": 0.002248606736127938,
      "grad_norm": 1.5113985538482666,
      "learning_rate": 0.0001995889561036141,
      "loss": 0.3996,
      "step": 58
    },
    {
      "epoch": 0.0022873758177853164,
      "grad_norm": 0.8883568048477173,
      "learning_rate": 0.00019958120055839926,
      "loss": 0.5175,
      "step": 59
    },
    {
      "epoch": 0.0023261448994426944,
      "grad_norm": 1.4438486099243164,
      "learning_rate": 0.00019957344501318443,
      "loss": 0.56,
      "step": 60
    },
    {
      "epoch": 0.002364913981100073,
      "grad_norm": 0.9607509970664978,
      "learning_rate": 0.0001995656894679696,
      "loss": 0.5382,
      "step": 61
    },
    {
      "epoch": 0.002403683062757451,
      "grad_norm": 0.9949698448181152,
      "learning_rate": 0.00019955793392275478,
      "loss": 0.5117,
      "step": 62
    },
    {
      "epoch": 0.002442452144414829,
      "grad_norm": 1.3765926361083984,
      "learning_rate": 0.00019955017837753995,
      "loss": 0.4388,
      "step": 63
    },
    {
      "epoch": 0.0024812212260722074,
      "grad_norm": 0.8322263360023499,
      "learning_rate": 0.00019954242283232512,
      "loss": 0.5912,
      "step": 64
    },
    {
      "epoch": 0.0025199903077295855,
      "grad_norm": 0.7141783833503723,
      "learning_rate": 0.0001995346672871103,
      "loss": 0.4569,
      "step": 65
    },
    {
      "epoch": 0.002558759389386964,
      "grad_norm": 1.1710041761398315,
      "learning_rate": 0.00019952691174189547,
      "loss": 0.5295,
      "step": 66
    },
    {
      "epoch": 0.002597528471044342,
      "grad_norm": 0.7588098645210266,
      "learning_rate": 0.00019951915619668064,
      "loss": 0.3879,
      "step": 67
    },
    {
      "epoch": 0.0026362975527017204,
      "grad_norm": 0.39814552664756775,
      "learning_rate": 0.00019951140065146582,
      "loss": 0.4427,
      "step": 68
    },
    {
      "epoch": 0.0026750666343590985,
      "grad_norm": 0.6337335109710693,
      "learning_rate": 0.00019950364510625096,
      "loss": 0.4273,
      "step": 69
    },
    {
      "epoch": 0.002713835716016477,
      "grad_norm": 0.6543207764625549,
      "learning_rate": 0.00019949588956103616,
      "loss": 0.4187,
      "step": 70
    },
    {
      "epoch": 0.002752604797673855,
      "grad_norm": 0.42048418521881104,
      "learning_rate": 0.0001994881340158213,
      "loss": 0.4071,
      "step": 71
    },
    {
      "epoch": 0.0027913738793312335,
      "grad_norm": 0.380841463804245,
      "learning_rate": 0.0001994803784706065,
      "loss": 0.4538,
      "step": 72
    },
    {
      "epoch": 0.0028301429609886115,
      "grad_norm": 0.6037354469299316,
      "learning_rate": 0.00019947262292539165,
      "loss": 0.5582,
      "step": 73
    },
    {
      "epoch": 0.00286891204264599,
      "grad_norm": 0.5498448610305786,
      "learning_rate": 0.00019946486738017685,
      "loss": 0.4431,
      "step": 74
    },
    {
      "epoch": 0.002907681124303368,
      "grad_norm": 0.43996989727020264,
      "learning_rate": 0.000199457111834962,
      "loss": 0.3518,
      "step": 75
    },
    {
      "epoch": 0.0029464502059607465,
      "grad_norm": 0.37207508087158203,
      "learning_rate": 0.00019944935628974717,
      "loss": 0.4092,
      "step": 76
    },
    {
      "epoch": 0.0029852192876181245,
      "grad_norm": 0.48353052139282227,
      "learning_rate": 0.00019944160074453234,
      "loss": 0.3983,
      "step": 77
    },
    {
      "epoch": 0.003023988369275503,
      "grad_norm": 0.4166845381259918,
      "learning_rate": 0.00019943384519931752,
      "loss": 0.3752,
      "step": 78
    },
    {
      "epoch": 0.003062757450932881,
      "grad_norm": 0.46338117122650146,
      "learning_rate": 0.0001994260896541027,
      "loss": 0.4284,
      "step": 79
    },
    {
      "epoch": 0.003101526532590259,
      "grad_norm": 0.30757734179496765,
      "learning_rate": 0.00019941833410888786,
      "loss": 0.4026,
      "step": 80
    },
    {
      "epoch": 0.0031402956142476375,
      "grad_norm": 0.39001184701919556,
      "learning_rate": 0.00019941057856367303,
      "loss": 0.3386,
      "step": 81
    },
    {
      "epoch": 0.0031790646959050156,
      "grad_norm": 0.40398213267326355,
      "learning_rate": 0.0001994028230184582,
      "loss": 0.3646,
      "step": 82
    },
    {
      "epoch": 0.003217833777562394,
      "grad_norm": 0.31530216336250305,
      "learning_rate": 0.00019939506747324338,
      "loss": 0.3802,
      "step": 83
    },
    {
      "epoch": 0.003256602859219772,
      "grad_norm": 0.39986175298690796,
      "learning_rate": 0.00019938731192802855,
      "loss": 0.3554,
      "step": 84
    },
    {
      "epoch": 0.0032953719408771506,
      "grad_norm": 0.4107156991958618,
      "learning_rate": 0.00019937955638281373,
      "loss": 0.3778,
      "step": 85
    },
    {
      "epoch": 0.0033341410225345286,
      "grad_norm": 0.5386656522750854,
      "learning_rate": 0.0001993718008375989,
      "loss": 0.4716,
      "step": 86
    },
    {
      "epoch": 0.003372910104191907,
      "grad_norm": 0.3884955048561096,
      "learning_rate": 0.00019936404529238407,
      "loss": 0.4119,
      "step": 87
    },
    {
      "epoch": 0.003411679185849285,
      "grad_norm": 0.4117557406425476,
      "learning_rate": 0.00019935628974716924,
      "loss": 0.3993,
      "step": 88
    },
    {
      "epoch": 0.0034504482675066636,
      "grad_norm": 0.3828742206096649,
      "learning_rate": 0.00019934853420195442,
      "loss": 0.3883,
      "step": 89
    },
    {
      "epoch": 0.0034892173491640416,
      "grad_norm": 0.3464539647102356,
      "learning_rate": 0.00019934077865673956,
      "loss": 0.332,
      "step": 90
    },
    {
      "epoch": 0.00352798643082142,
      "grad_norm": 0.36263829469680786,
      "learning_rate": 0.00019933302311152476,
      "loss": 0.4075,
      "step": 91
    },
    {
      "epoch": 0.003566755512478798,
      "grad_norm": 0.3180046081542969,
      "learning_rate": 0.0001993252675663099,
      "loss": 0.3944,
      "step": 92
    },
    {
      "epoch": 0.0036055245941361766,
      "grad_norm": 0.43269050121307373,
      "learning_rate": 0.0001993175120210951,
      "loss": 0.4561,
      "step": 93
    },
    {
      "epoch": 0.0036442936757935546,
      "grad_norm": 0.3255857527256012,
      "learning_rate": 0.00019930975647588025,
      "loss": 0.351,
      "step": 94
    },
    {
      "epoch": 0.0036830627574509327,
      "grad_norm": 0.21789297461509705,
      "learning_rate": 0.00019930200093066545,
      "loss": 0.4543,
      "step": 95
    },
    {
      "epoch": 0.003721831839108311,
      "grad_norm": 0.22572259604930878,
      "learning_rate": 0.0001992942453854506,
      "loss": 0.3563,
      "step": 96
    },
    {
      "epoch": 0.003760600920765689,
      "grad_norm": 0.4792696535587311,
      "learning_rate": 0.00019928648984023577,
      "loss": 0.3664,
      "step": 97
    },
    {
      "epoch": 0.0037993700024230677,
      "grad_norm": 0.37555548548698425,
      "learning_rate": 0.00019927873429502095,
      "loss": 0.3325,
      "step": 98
    },
    {
      "epoch": 0.0038381390840804457,
      "grad_norm": 0.3820406198501587,
      "learning_rate": 0.00019927097874980612,
      "loss": 0.3665,
      "step": 99
    },
    {
      "epoch": 0.003876908165737824,
      "grad_norm": 0.3498466908931732,
      "learning_rate": 0.0001992632232045913,
      "loss": 0.4414,
      "step": 100
    },
    {
      "epoch": 0.003915677247395203,
      "grad_norm": 0.32542911171913147,
      "learning_rate": 0.00019925546765937646,
      "loss": 0.4803,
      "step": 101
    },
    {
      "epoch": 0.003954446329052581,
      "grad_norm": 0.27770984172821045,
      "learning_rate": 0.00019924771211416164,
      "loss": 0.3232,
      "step": 102
    },
    {
      "epoch": 0.003993215410709959,
      "grad_norm": 0.7336586117744446,
      "learning_rate": 0.0001992399565689468,
      "loss": 0.3708,
      "step": 103
    },
    {
      "epoch": 0.004031984492367337,
      "grad_norm": 0.4379984438419342,
      "learning_rate": 0.00019923220102373195,
      "loss": 0.326,
      "step": 104
    },
    {
      "epoch": 0.004070753574024716,
      "grad_norm": 0.3229488730430603,
      "learning_rate": 0.00019922444547851715,
      "loss": 0.3486,
      "step": 105
    },
    {
      "epoch": 0.004109522655682094,
      "grad_norm": 0.37352925539016724,
      "learning_rate": 0.00019921668993330233,
      "loss": 0.4103,
      "step": 106
    },
    {
      "epoch": 0.004148291737339472,
      "grad_norm": 0.31027740240097046,
      "learning_rate": 0.0001992089343880875,
      "loss": 0.3942,
      "step": 107
    },
    {
      "epoch": 0.00418706081899685,
      "grad_norm": 0.47671830654144287,
      "learning_rate": 0.00019920117884287267,
      "loss": 0.4303,
      "step": 108
    },
    {
      "epoch": 0.004225829900654229,
      "grad_norm": 0.2849827706813812,
      "learning_rate": 0.00019919342329765785,
      "loss": 0.3735,
      "step": 109
    },
    {
      "epoch": 0.004264598982311607,
      "grad_norm": 0.3720203936100006,
      "learning_rate": 0.00019918566775244302,
      "loss": 0.5271,
      "step": 110
    },
    {
      "epoch": 0.004303368063968985,
      "grad_norm": 0.3835037350654602,
      "learning_rate": 0.00019917791220722816,
      "loss": 0.3959,
      "step": 111
    },
    {
      "epoch": 0.004342137145626363,
      "grad_norm": 0.2934713363647461,
      "learning_rate": 0.00019917015666201336,
      "loss": 0.2994,
      "step": 112
    },
    {
      "epoch": 0.004380906227283741,
      "grad_norm": 0.28044673800468445,
      "learning_rate": 0.0001991624011167985,
      "loss": 0.3853,
      "step": 113
    },
    {
      "epoch": 0.00441967530894112,
      "grad_norm": 0.31564101576805115,
      "learning_rate": 0.0001991546455715837,
      "loss": 0.4622,
      "step": 114
    },
    {
      "epoch": 0.004458444390598498,
      "grad_norm": 0.3015250265598297,
      "learning_rate": 0.00019914689002636886,
      "loss": 0.4569,
      "step": 115
    },
    {
      "epoch": 0.004497213472255876,
      "grad_norm": 0.2700304388999939,
      "learning_rate": 0.00019913913448115406,
      "loss": 0.3658,
      "step": 116
    },
    {
      "epoch": 0.004535982553913254,
      "grad_norm": 0.23505519330501556,
      "learning_rate": 0.0001991313789359392,
      "loss": 0.3252,
      "step": 117
    },
    {
      "epoch": 0.004574751635570633,
      "grad_norm": 0.26610928773880005,
      "learning_rate": 0.00019912362339072437,
      "loss": 0.377,
      "step": 118
    },
    {
      "epoch": 0.004613520717228011,
      "grad_norm": 0.2529745399951935,
      "learning_rate": 0.00019911586784550955,
      "loss": 0.4238,
      "step": 119
    },
    {
      "epoch": 0.004652289798885389,
      "grad_norm": 0.37146490812301636,
      "learning_rate": 0.00019910811230029472,
      "loss": 0.3766,
      "step": 120
    },
    {
      "epoch": 0.004691058880542767,
      "grad_norm": 0.22810615599155426,
      "learning_rate": 0.0001991003567550799,
      "loss": 0.325,
      "step": 121
    },
    {
      "epoch": 0.004729827962200146,
      "grad_norm": 0.2723677456378937,
      "learning_rate": 0.00019909260120986507,
      "loss": 0.4377,
      "step": 122
    },
    {
      "epoch": 0.004768597043857524,
      "grad_norm": 0.26087960600852966,
      "learning_rate": 0.00019908484566465024,
      "loss": 0.3262,
      "step": 123
    },
    {
      "epoch": 0.004807366125514902,
      "grad_norm": 0.22962044179439545,
      "learning_rate": 0.0001990770901194354,
      "loss": 0.3561,
      "step": 124
    },
    {
      "epoch": 0.00484613520717228,
      "grad_norm": 0.18949374556541443,
      "learning_rate": 0.00019906933457422056,
      "loss": 0.296,
      "step": 125
    },
    {
      "epoch": 0.004884904288829658,
      "grad_norm": 0.3092156946659088,
      "learning_rate": 0.00019906157902900576,
      "loss": 0.4671,
      "step": 126
    },
    {
      "epoch": 0.004923673370487037,
      "grad_norm": 0.19221702218055725,
      "learning_rate": 0.0001990538234837909,
      "loss": 0.3193,
      "step": 127
    },
    {
      "epoch": 0.004962442452144415,
      "grad_norm": 0.25905564427375793,
      "learning_rate": 0.0001990460679385761,
      "loss": 0.3691,
      "step": 128
    },
    {
      "epoch": 0.005001211533801793,
      "grad_norm": 0.1957622617483139,
      "learning_rate": 0.00019903831239336125,
      "loss": 0.317,
      "step": 129
    },
    {
      "epoch": 0.005039980615459171,
      "grad_norm": 0.2595498263835907,
      "learning_rate": 0.00019903055684814645,
      "loss": 0.3453,
      "step": 130
    },
    {
      "epoch": 0.00507874969711655,
      "grad_norm": 0.2487931102514267,
      "learning_rate": 0.0001990228013029316,
      "loss": 0.3481,
      "step": 131
    },
    {
      "epoch": 0.005117518778773928,
      "grad_norm": 0.23804764449596405,
      "learning_rate": 0.00019901504575771677,
      "loss": 0.3483,
      "step": 132
    },
    {
      "epoch": 0.005156287860431306,
      "grad_norm": 0.23190927505493164,
      "learning_rate": 0.00019900729021250194,
      "loss": 0.3861,
      "step": 133
    },
    {
      "epoch": 0.005195056942088684,
      "grad_norm": 0.28612321615219116,
      "learning_rate": 0.0001989995346672871,
      "loss": 0.4202,
      "step": 134
    },
    {
      "epoch": 0.005233826023746063,
      "grad_norm": 0.21660266816616058,
      "learning_rate": 0.00019899177912207228,
      "loss": 0.3302,
      "step": 135
    },
    {
      "epoch": 0.005272595105403441,
      "grad_norm": 0.2306484580039978,
      "learning_rate": 0.00019898402357685746,
      "loss": 0.4023,
      "step": 136
    },
    {
      "epoch": 0.005311364187060819,
      "grad_norm": 0.24266718327999115,
      "learning_rate": 0.00019897626803164263,
      "loss": 0.3465,
      "step": 137
    },
    {
      "epoch": 0.005350133268718197,
      "grad_norm": 0.19707806408405304,
      "learning_rate": 0.0001989685124864278,
      "loss": 0.3858,
      "step": 138
    },
    {
      "epoch": 0.005388902350375576,
      "grad_norm": 0.41391465067863464,
      "learning_rate": 0.00019896075694121298,
      "loss": 0.339,
      "step": 139
    },
    {
      "epoch": 0.005427671432032954,
      "grad_norm": 0.2371853142976761,
      "learning_rate": 0.00019895300139599815,
      "loss": 0.3628,
      "step": 140
    },
    {
      "epoch": 0.005466440513690332,
      "grad_norm": 0.2575313448905945,
      "learning_rate": 0.00019894524585078332,
      "loss": 0.4077,
      "step": 141
    },
    {
      "epoch": 0.00550520959534771,
      "grad_norm": 0.3299817144870758,
      "learning_rate": 0.0001989374903055685,
      "loss": 0.3442,
      "step": 142
    },
    {
      "epoch": 0.005543978677005088,
      "grad_norm": 0.24534080922603607,
      "learning_rate": 0.00019892973476035367,
      "loss": 0.32,
      "step": 143
    },
    {
      "epoch": 0.005582747758662467,
      "grad_norm": 0.2236955165863037,
      "learning_rate": 0.00019892197921513884,
      "loss": 0.3568,
      "step": 144
    },
    {
      "epoch": 0.005621516840319845,
      "grad_norm": 0.23586390912532806,
      "learning_rate": 0.000198914223669924,
      "loss": 0.2916,
      "step": 145
    },
    {
      "epoch": 0.005660285921977223,
      "grad_norm": 0.2849571108818054,
      "learning_rate": 0.00019890646812470916,
      "loss": 0.3689,
      "step": 146
    },
    {
      "epoch": 0.005699055003634601,
      "grad_norm": 0.28808072209358215,
      "learning_rate": 0.00019889871257949436,
      "loss": 0.3548,
      "step": 147
    },
    {
      "epoch": 0.00573782408529198,
      "grad_norm": 0.19898295402526855,
      "learning_rate": 0.0001988909570342795,
      "loss": 0.328,
      "step": 148
    },
    {
      "epoch": 0.005776593166949358,
      "grad_norm": 0.26719728112220764,
      "learning_rate": 0.0001988832014890647,
      "loss": 0.3655,
      "step": 149
    },
    {
      "epoch": 0.005815362248606736,
      "grad_norm": 0.2787806987762451,
      "learning_rate": 0.00019887544594384985,
      "loss": 0.3641,
      "step": 150
    },
    {
      "epoch": 0.005854131330264114,
      "grad_norm": 0.216325581073761,
      "learning_rate": 0.00019886769039863505,
      "loss": 0.4156,
      "step": 151
    },
    {
      "epoch": 0.005892900411921493,
      "grad_norm": 0.2534346282482147,
      "learning_rate": 0.0001988599348534202,
      "loss": 0.4626,
      "step": 152
    },
    {
      "epoch": 0.005931669493578871,
      "grad_norm": 0.26094526052474976,
      "learning_rate": 0.00019885217930820537,
      "loss": 0.3627,
      "step": 153
    },
    {
      "epoch": 0.005970438575236249,
      "grad_norm": 0.2076389491558075,
      "learning_rate": 0.00019884442376299054,
      "loss": 0.3806,
      "step": 154
    },
    {
      "epoch": 0.006009207656893627,
      "grad_norm": 0.21460583806037903,
      "learning_rate": 0.00019883666821777571,
      "loss": 0.2809,
      "step": 155
    },
    {
      "epoch": 0.006047976738551006,
      "grad_norm": 0.21737274527549744,
      "learning_rate": 0.0001988289126725609,
      "loss": 0.3628,
      "step": 156
    },
    {
      "epoch": 0.006086745820208384,
      "grad_norm": 0.25624778866767883,
      "learning_rate": 0.00019882115712734606,
      "loss": 0.3904,
      "step": 157
    },
    {
      "epoch": 0.006125514901865762,
      "grad_norm": 0.2228907197713852,
      "learning_rate": 0.00019881340158213123,
      "loss": 0.3509,
      "step": 158
    },
    {
      "epoch": 0.00616428398352314,
      "grad_norm": 0.2858039438724518,
      "learning_rate": 0.0001988056460369164,
      "loss": 0.3582,
      "step": 159
    },
    {
      "epoch": 0.006203053065180518,
      "grad_norm": 0.25192990899086,
      "learning_rate": 0.00019879789049170158,
      "loss": 0.3491,
      "step": 160
    },
    {
      "epoch": 0.006241822146837897,
      "grad_norm": 0.29516175389289856,
      "learning_rate": 0.00019879013494648675,
      "loss": 0.3721,
      "step": 161
    },
    {
      "epoch": 0.006280591228495275,
      "grad_norm": 0.2907150685787201,
      "learning_rate": 0.00019878237940127192,
      "loss": 0.4412,
      "step": 162
    },
    {
      "epoch": 0.006319360310152653,
      "grad_norm": 0.25139757990837097,
      "learning_rate": 0.0001987746238560571,
      "loss": 0.3145,
      "step": 163
    },
    {
      "epoch": 0.006358129391810031,
      "grad_norm": 0.30777716636657715,
      "learning_rate": 0.00019876686831084227,
      "loss": 0.364,
      "step": 164
    },
    {
      "epoch": 0.00639689847346741,
      "grad_norm": 0.16985292732715607,
      "learning_rate": 0.00019875911276562744,
      "loss": 0.3066,
      "step": 165
    },
    {
      "epoch": 0.006435667555124788,
      "grad_norm": 0.2536247968673706,
      "learning_rate": 0.00019875135722041261,
      "loss": 0.4194,
      "step": 166
    },
    {
      "epoch": 0.006474436636782166,
      "grad_norm": 0.3053729832172394,
      "learning_rate": 0.00019874360167519776,
      "loss": 0.3576,
      "step": 167
    },
    {
      "epoch": 0.006513205718439544,
      "grad_norm": 0.23392751812934875,
      "learning_rate": 0.00019873584612998296,
      "loss": 0.3457,
      "step": 168
    },
    {
      "epoch": 0.006551974800096923,
      "grad_norm": 0.17018342018127441,
      "learning_rate": 0.0001987280905847681,
      "loss": 0.2684,
      "step": 169
    },
    {
      "epoch": 0.006590743881754301,
      "grad_norm": 0.22994540631771088,
      "learning_rate": 0.0001987203350395533,
      "loss": 0.2659,
      "step": 170
    },
    {
      "epoch": 0.006629512963411679,
      "grad_norm": 0.25113654136657715,
      "learning_rate": 0.00019871257949433845,
      "loss": 0.2883,
      "step": 171
    },
    {
      "epoch": 0.006668282045069057,
      "grad_norm": 0.24131953716278076,
      "learning_rate": 0.00019870482394912365,
      "loss": 0.346,
      "step": 172
    },
    {
      "epoch": 0.006707051126726436,
      "grad_norm": 0.22444583475589752,
      "learning_rate": 0.0001986970684039088,
      "loss": 0.3707,
      "step": 173
    },
    {
      "epoch": 0.006745820208383814,
      "grad_norm": 0.276846706867218,
      "learning_rate": 0.00019868931285869397,
      "loss": 0.4082,
      "step": 174
    },
    {
      "epoch": 0.006784589290041192,
      "grad_norm": 0.32546162605285645,
      "learning_rate": 0.00019868155731347914,
      "loss": 0.3567,
      "step": 175
    },
    {
      "epoch": 0.00682335837169857,
      "grad_norm": 0.21358749270439148,
      "learning_rate": 0.00019867380176826432,
      "loss": 0.309,
      "step": 176
    },
    {
      "epoch": 0.006862127453355948,
      "grad_norm": 0.2544545829296112,
      "learning_rate": 0.0001986660462230495,
      "loss": 0.322,
      "step": 177
    },
    {
      "epoch": 0.006900896535013327,
      "grad_norm": 0.28094881772994995,
      "learning_rate": 0.00019865829067783466,
      "loss": 0.3945,
      "step": 178
    },
    {
      "epoch": 0.006939665616670705,
      "grad_norm": 0.22359661757946014,
      "learning_rate": 0.00019865053513261983,
      "loss": 0.3348,
      "step": 179
    },
    {
      "epoch": 0.006978434698328083,
      "grad_norm": 0.23866857588291168,
      "learning_rate": 0.000198642779587405,
      "loss": 0.3771,
      "step": 180
    },
    {
      "epoch": 0.007017203779985461,
      "grad_norm": 0.23260502517223358,
      "learning_rate": 0.00019863502404219015,
      "loss": 0.3714,
      "step": 181
    },
    {
      "epoch": 0.00705597286164284,
      "grad_norm": 0.35842689871788025,
      "learning_rate": 0.00019862726849697535,
      "loss": 0.4361,
      "step": 182
    },
    {
      "epoch": 0.007094741943300218,
      "grad_norm": 0.279440701007843,
      "learning_rate": 0.0001986195129517605,
      "loss": 0.3489,
      "step": 183
    },
    {
      "epoch": 0.007133511024957596,
      "grad_norm": 0.23785582184791565,
      "learning_rate": 0.0001986117574065457,
      "loss": 0.3855,
      "step": 184
    },
    {
      "epoch": 0.007172280106614974,
      "grad_norm": 0.3093741834163666,
      "learning_rate": 0.00019860400186133084,
      "loss": 0.3674,
      "step": 185
    },
    {
      "epoch": 0.007211049188272353,
      "grad_norm": 0.25695836544036865,
      "learning_rate": 0.00019859624631611604,
      "loss": 0.3549,
      "step": 186
    },
    {
      "epoch": 0.007249818269929731,
      "grad_norm": 0.21849408745765686,
      "learning_rate": 0.00019858849077090122,
      "loss": 0.3152,
      "step": 187
    },
    {
      "epoch": 0.007288587351587109,
      "grad_norm": 0.21231992542743683,
      "learning_rate": 0.00019858073522568636,
      "loss": 0.3166,
      "step": 188
    },
    {
      "epoch": 0.007327356433244487,
      "grad_norm": 0.20371273159980774,
      "learning_rate": 0.00019857297968047156,
      "loss": 0.3646,
      "step": 189
    },
    {
      "epoch": 0.007366125514901865,
      "grad_norm": 0.21982692182064056,
      "learning_rate": 0.0001985652241352567,
      "loss": 0.3519,
      "step": 190
    },
    {
      "epoch": 0.007404894596559244,
      "grad_norm": 0.26631149649620056,
      "learning_rate": 0.0001985574685900419,
      "loss": 0.3604,
      "step": 191
    },
    {
      "epoch": 0.007443663678216622,
      "grad_norm": 0.15002907812595367,
      "learning_rate": 0.00019854971304482705,
      "loss": 0.3247,
      "step": 192
    },
    {
      "epoch": 0.007482432759874,
      "grad_norm": 0.23978418111801147,
      "learning_rate": 0.00019854195749961225,
      "loss": 0.3531,
      "step": 193
    },
    {
      "epoch": 0.007521201841531378,
      "grad_norm": 0.2265661358833313,
      "learning_rate": 0.0001985342019543974,
      "loss": 0.3235,
      "step": 194
    },
    {
      "epoch": 0.007559970923188757,
      "grad_norm": 0.24001309275627136,
      "learning_rate": 0.00019852644640918257,
      "loss": 0.4128,
      "step": 195
    },
    {
      "epoch": 0.007598740004846135,
      "grad_norm": 0.30206188559532166,
      "learning_rate": 0.00019851869086396774,
      "loss": 0.2674,
      "step": 196
    },
    {
      "epoch": 0.007637509086503513,
      "grad_norm": 0.3277266025543213,
      "learning_rate": 0.00019851093531875292,
      "loss": 0.4339,
      "step": 197
    },
    {
      "epoch": 0.007676278168160891,
      "grad_norm": 0.23538357019424438,
      "learning_rate": 0.0001985031797735381,
      "loss": 0.4001,
      "step": 198
    },
    {
      "epoch": 0.00771504724981827,
      "grad_norm": 0.2776458263397217,
      "learning_rate": 0.00019849542422832326,
      "loss": 0.4194,
      "step": 199
    },
    {
      "epoch": 0.007753816331475648,
      "grad_norm": 0.22483037412166595,
      "learning_rate": 0.00019848766868310844,
      "loss": 0.4102,
      "step": 200
    },
    {
      "epoch": 0.007792585413133026,
      "grad_norm": 0.23312336206436157,
      "learning_rate": 0.0001984799131378936,
      "loss": 0.4197,
      "step": 201
    },
    {
      "epoch": 0.007831354494790405,
      "grad_norm": 0.2425794005393982,
      "learning_rate": 0.00019847215759267875,
      "loss": 0.3582,
      "step": 202
    },
    {
      "epoch": 0.007870123576447783,
      "grad_norm": 0.2374963015317917,
      "learning_rate": 0.00019846440204746395,
      "loss": 0.427,
      "step": 203
    },
    {
      "epoch": 0.007908892658105161,
      "grad_norm": 0.2443149983882904,
      "learning_rate": 0.0001984566465022491,
      "loss": 0.384,
      "step": 204
    },
    {
      "epoch": 0.00794766173976254,
      "grad_norm": 0.17869865894317627,
      "learning_rate": 0.0001984488909570343,
      "loss": 0.3407,
      "step": 205
    },
    {
      "epoch": 0.007986430821419917,
      "grad_norm": 0.24580666422843933,
      "learning_rate": 0.00019844113541181945,
      "loss": 0.4399,
      "step": 206
    },
    {
      "epoch": 0.008025199903077295,
      "grad_norm": 0.19430327415466309,
      "learning_rate": 0.00019843337986660465,
      "loss": 0.3219,
      "step": 207
    },
    {
      "epoch": 0.008063968984734673,
      "grad_norm": 0.18826711177825928,
      "learning_rate": 0.0001984256243213898,
      "loss": 0.401,
      "step": 208
    },
    {
      "epoch": 0.008102738066392052,
      "grad_norm": 0.17811612784862518,
      "learning_rate": 0.00019841786877617496,
      "loss": 0.3484,
      "step": 209
    },
    {
      "epoch": 0.008141507148049431,
      "grad_norm": 0.21373820304870605,
      "learning_rate": 0.00019841011323096014,
      "loss": 0.3871,
      "step": 210
    },
    {
      "epoch": 0.00818027622970681,
      "grad_norm": 0.14814259111881256,
      "learning_rate": 0.0001984023576857453,
      "loss": 0.3096,
      "step": 211
    },
    {
      "epoch": 0.008219045311364187,
      "grad_norm": 0.1660691499710083,
      "learning_rate": 0.00019839460214053048,
      "loss": 0.3096,
      "step": 212
    },
    {
      "epoch": 0.008257814393021565,
      "grad_norm": 0.17767572402954102,
      "learning_rate": 0.00019838684659531566,
      "loss": 0.3553,
      "step": 213
    },
    {
      "epoch": 0.008296583474678943,
      "grad_norm": 0.19122640788555145,
      "learning_rate": 0.00019837909105010083,
      "loss": 0.3132,
      "step": 214
    },
    {
      "epoch": 0.008335352556336321,
      "grad_norm": 0.2131112515926361,
      "learning_rate": 0.000198371335504886,
      "loss": 0.3667,
      "step": 215
    },
    {
      "epoch": 0.0083741216379937,
      "grad_norm": 0.18761901557445526,
      "learning_rate": 0.00019836357995967117,
      "loss": 0.3492,
      "step": 216
    },
    {
      "epoch": 0.008412890719651078,
      "grad_norm": 0.15587353706359863,
      "learning_rate": 0.00019835582441445635,
      "loss": 0.2288,
      "step": 217
    },
    {
      "epoch": 0.008451659801308457,
      "grad_norm": 0.17419500648975372,
      "learning_rate": 0.00019834806886924152,
      "loss": 0.3957,
      "step": 218
    },
    {
      "epoch": 0.008490428882965835,
      "grad_norm": 0.1698499470949173,
      "learning_rate": 0.0001983403133240267,
      "loss": 0.3066,
      "step": 219
    },
    {
      "epoch": 0.008529197964623213,
      "grad_norm": 0.22016595304012299,
      "learning_rate": 0.00019833255777881186,
      "loss": 0.3841,
      "step": 220
    },
    {
      "epoch": 0.008567967046280591,
      "grad_norm": 0.19667862355709076,
      "learning_rate": 0.00019832480223359704,
      "loss": 0.3685,
      "step": 221
    },
    {
      "epoch": 0.00860673612793797,
      "grad_norm": 0.16683275997638702,
      "learning_rate": 0.0001983170466883822,
      "loss": 0.3112,
      "step": 222
    },
    {
      "epoch": 0.008645505209595348,
      "grad_norm": 0.18448972702026367,
      "learning_rate": 0.00019830929114316736,
      "loss": 0.3305,
      "step": 223
    },
    {
      "epoch": 0.008684274291252726,
      "grad_norm": 0.1848680078983307,
      "learning_rate": 0.00019830153559795256,
      "loss": 0.3277,
      "step": 224
    },
    {
      "epoch": 0.008723043372910104,
      "grad_norm": 0.2399527132511139,
      "learning_rate": 0.0001982937800527377,
      "loss": 0.35,
      "step": 225
    },
    {
      "epoch": 0.008761812454567482,
      "grad_norm": 0.15571805834770203,
      "learning_rate": 0.0001982860245075229,
      "loss": 0.3054,
      "step": 226
    },
    {
      "epoch": 0.008800581536224861,
      "grad_norm": 0.17402401566505432,
      "learning_rate": 0.00019827826896230805,
      "loss": 0.2746,
      "step": 227
    },
    {
      "epoch": 0.00883935061788224,
      "grad_norm": 0.17548836767673492,
      "learning_rate": 0.00019827051341709325,
      "loss": 0.3351,
      "step": 228
    },
    {
      "epoch": 0.008878119699539617,
      "grad_norm": 0.1942170113325119,
      "learning_rate": 0.0001982627578718784,
      "loss": 0.3577,
      "step": 229
    },
    {
      "epoch": 0.008916888781196996,
      "grad_norm": 0.1753177046775818,
      "learning_rate": 0.00019825500232666357,
      "loss": 0.3122,
      "step": 230
    },
    {
      "epoch": 0.008955657862854374,
      "grad_norm": 0.2553737461566925,
      "learning_rate": 0.00019824724678144874,
      "loss": 0.2775,
      "step": 231
    },
    {
      "epoch": 0.008994426944511752,
      "grad_norm": 0.3430439829826355,
      "learning_rate": 0.0001982394912362339,
      "loss": 0.3819,
      "step": 232
    },
    {
      "epoch": 0.00903319602616913,
      "grad_norm": 0.16468292474746704,
      "learning_rate": 0.00019823173569101908,
      "loss": 0.2915,
      "step": 233
    },
    {
      "epoch": 0.009071965107826508,
      "grad_norm": 0.27156931161880493,
      "learning_rate": 0.00019822398014580426,
      "loss": 0.3251,
      "step": 234
    },
    {
      "epoch": 0.009110734189483887,
      "grad_norm": 0.1652126908302307,
      "learning_rate": 0.00019821622460058943,
      "loss": 0.3116,
      "step": 235
    },
    {
      "epoch": 0.009149503271141265,
      "grad_norm": 0.19468088448047638,
      "learning_rate": 0.0001982084690553746,
      "loss": 0.3723,
      "step": 236
    },
    {
      "epoch": 0.009188272352798644,
      "grad_norm": 0.26629719138145447,
      "learning_rate": 0.00019820071351015978,
      "loss": 0.4613,
      "step": 237
    },
    {
      "epoch": 0.009227041434456022,
      "grad_norm": 0.2159719616174698,
      "learning_rate": 0.00019819295796494495,
      "loss": 0.3142,
      "step": 238
    },
    {
      "epoch": 0.0092658105161134,
      "grad_norm": 0.28781548142433167,
      "learning_rate": 0.00019818520241973012,
      "loss": 0.4023,
      "step": 239
    },
    {
      "epoch": 0.009304579597770778,
      "grad_norm": 0.1749735176563263,
      "learning_rate": 0.0001981774468745153,
      "loss": 0.373,
      "step": 240
    },
    {
      "epoch": 0.009343348679428156,
      "grad_norm": 0.19041107594966888,
      "learning_rate": 0.00019816969132930047,
      "loss": 0.38,
      "step": 241
    },
    {
      "epoch": 0.009382117761085534,
      "grad_norm": 0.16356168687343597,
      "learning_rate": 0.00019816193578408564,
      "loss": 0.2764,
      "step": 242
    },
    {
      "epoch": 0.009420886842742912,
      "grad_norm": 0.14309971034526825,
      "learning_rate": 0.0001981541802388708,
      "loss": 0.2802,
      "step": 243
    },
    {
      "epoch": 0.009459655924400292,
      "grad_norm": 0.19768203794956207,
      "learning_rate": 0.00019814642469365596,
      "loss": 0.3786,
      "step": 244
    },
    {
      "epoch": 0.00949842500605767,
      "grad_norm": 0.20835228264331818,
      "learning_rate": 0.00019813866914844116,
      "loss": 0.3543,
      "step": 245
    },
    {
      "epoch": 0.009537194087715048,
      "grad_norm": 0.1827169805765152,
      "learning_rate": 0.0001981309136032263,
      "loss": 0.3318,
      "step": 246
    },
    {
      "epoch": 0.009575963169372426,
      "grad_norm": 0.15357844531536102,
      "learning_rate": 0.0001981231580580115,
      "loss": 0.329,
      "step": 247
    },
    {
      "epoch": 0.009614732251029804,
      "grad_norm": 0.17357362806797028,
      "learning_rate": 0.00019811540251279665,
      "loss": 0.3469,
      "step": 248
    },
    {
      "epoch": 0.009653501332687182,
      "grad_norm": 0.1783611923456192,
      "learning_rate": 0.00019810764696758185,
      "loss": 0.335,
      "step": 249
    },
    {
      "epoch": 0.00969227041434456,
      "grad_norm": 0.14683598279953003,
      "learning_rate": 0.000198099891422367,
      "loss": 0.2943,
      "step": 250
    },
    {
      "epoch": 0.009731039496001938,
      "grad_norm": 0.22065375745296478,
      "learning_rate": 0.00019809213587715217,
      "loss": 0.3443,
      "step": 251
    },
    {
      "epoch": 0.009769808577659316,
      "grad_norm": 0.22206880152225494,
      "learning_rate": 0.00019808438033193734,
      "loss": 0.4172,
      "step": 252
    },
    {
      "epoch": 0.009808577659316696,
      "grad_norm": 0.175215944647789,
      "learning_rate": 0.0001980766247867225,
      "loss": 0.3026,
      "step": 253
    },
    {
      "epoch": 0.009847346740974074,
      "grad_norm": 0.1968924105167389,
      "learning_rate": 0.00019806886924150769,
      "loss": 0.4053,
      "step": 254
    },
    {
      "epoch": 0.009886115822631452,
      "grad_norm": 0.17728610336780548,
      "learning_rate": 0.00019806111369629286,
      "loss": 0.3231,
      "step": 255
    },
    {
      "epoch": 0.00992488490428883,
      "grad_norm": 0.18522633612155914,
      "learning_rate": 0.00019805335815107803,
      "loss": 0.4608,
      "step": 256
    },
    {
      "epoch": 0.009963653985946208,
      "grad_norm": 0.22844792902469635,
      "learning_rate": 0.0001980456026058632,
      "loss": 0.3442,
      "step": 257
    },
    {
      "epoch": 0.010002423067603586,
      "grad_norm": 0.17206160724163055,
      "learning_rate": 0.00019803784706064835,
      "loss": 0.2479,
      "step": 258
    },
    {
      "epoch": 0.010041192149260964,
      "grad_norm": 0.1747860163450241,
      "learning_rate": 0.00019803009151543355,
      "loss": 0.3317,
      "step": 259
    },
    {
      "epoch": 0.010079961230918342,
      "grad_norm": 0.27453184127807617,
      "learning_rate": 0.0001980223359702187,
      "loss": 0.3766,
      "step": 260
    },
    {
      "epoch": 0.010118730312575722,
      "grad_norm": 0.1655982881784439,
      "learning_rate": 0.0001980145804250039,
      "loss": 0.2834,
      "step": 261
    },
    {
      "epoch": 0.0101574993942331,
      "grad_norm": 0.20382310450077057,
      "learning_rate": 0.00019800682487978904,
      "loss": 0.2617,
      "step": 262
    },
    {
      "epoch": 0.010196268475890478,
      "grad_norm": 0.13375471532344818,
      "learning_rate": 0.00019799906933457424,
      "loss": 0.295,
      "step": 263
    },
    {
      "epoch": 0.010235037557547856,
      "grad_norm": 0.19845730066299438,
      "learning_rate": 0.0001979913137893594,
      "loss": 0.3892,
      "step": 264
    },
    {
      "epoch": 0.010273806639205234,
      "grad_norm": 0.234327495098114,
      "learning_rate": 0.00019798355824414456,
      "loss": 0.3243,
      "step": 265
    },
    {
      "epoch": 0.010312575720862612,
      "grad_norm": 0.15347817540168762,
      "learning_rate": 0.00019797580269892976,
      "loss": 0.3425,
      "step": 266
    },
    {
      "epoch": 0.01035134480251999,
      "grad_norm": 0.18633626401424408,
      "learning_rate": 0.0001979680471537149,
      "loss": 0.3278,
      "step": 267
    },
    {
      "epoch": 0.010390113884177368,
      "grad_norm": 0.17555655539035797,
      "learning_rate": 0.0001979602916085001,
      "loss": 0.3666,
      "step": 268
    },
    {
      "epoch": 0.010428882965834746,
      "grad_norm": 0.16634850203990936,
      "learning_rate": 0.00019795253606328525,
      "loss": 0.3496,
      "step": 269
    },
    {
      "epoch": 0.010467652047492126,
      "grad_norm": 0.17517060041427612,
      "learning_rate": 0.00019794478051807045,
      "loss": 0.3821,
      "step": 270
    },
    {
      "epoch": 0.010506421129149504,
      "grad_norm": 0.15766176581382751,
      "learning_rate": 0.0001979370249728556,
      "loss": 0.2971,
      "step": 271
    },
    {
      "epoch": 0.010545190210806882,
      "grad_norm": 0.21017912030220032,
      "learning_rate": 0.00019792926942764077,
      "loss": 0.3785,
      "step": 272
    },
    {
      "epoch": 0.01058395929246426,
      "grad_norm": 0.1562787890434265,
      "learning_rate": 0.00019792151388242594,
      "loss": 0.2753,
      "step": 273
    },
    {
      "epoch": 0.010622728374121638,
      "grad_norm": 0.187228262424469,
      "learning_rate": 0.00019791375833721112,
      "loss": 0.3797,
      "step": 274
    },
    {
      "epoch": 0.010661497455779016,
      "grad_norm": 0.15154555439949036,
      "learning_rate": 0.0001979060027919963,
      "loss": 0.3775,
      "step": 275
    },
    {
      "epoch": 0.010700266537436394,
      "grad_norm": 0.24230250716209412,
      "learning_rate": 0.00019789824724678146,
      "loss": 0.3792,
      "step": 276
    },
    {
      "epoch": 0.010739035619093772,
      "grad_norm": 0.1980646699666977,
      "learning_rate": 0.00019789049170156663,
      "loss": 0.3705,
      "step": 277
    },
    {
      "epoch": 0.010777804700751152,
      "grad_norm": 0.16102230548858643,
      "learning_rate": 0.0001978827361563518,
      "loss": 0.3982,
      "step": 278
    },
    {
      "epoch": 0.01081657378240853,
      "grad_norm": 0.16752837598323822,
      "learning_rate": 0.00019787498061113695,
      "loss": 0.2635,
      "step": 279
    },
    {
      "epoch": 0.010855342864065908,
      "grad_norm": 0.14931409060955048,
      "learning_rate": 0.00019786722506592215,
      "loss": 0.2641,
      "step": 280
    },
    {
      "epoch": 0.010894111945723286,
      "grad_norm": 0.18310342729091644,
      "learning_rate": 0.0001978594695207073,
      "loss": 0.3631,
      "step": 281
    },
    {
      "epoch": 0.010932881027380664,
      "grad_norm": 0.19142034649848938,
      "learning_rate": 0.0001978517139754925,
      "loss": 0.3426,
      "step": 282
    },
    {
      "epoch": 0.010971650109038042,
      "grad_norm": 0.22250211238861084,
      "learning_rate": 0.00019784395843027764,
      "loss": 0.4223,
      "step": 283
    },
    {
      "epoch": 0.01101041919069542,
      "grad_norm": 0.1315949708223343,
      "learning_rate": 0.00019783620288506284,
      "loss": 0.2805,
      "step": 284
    },
    {
      "epoch": 0.011049188272352798,
      "grad_norm": 0.1983024626970291,
      "learning_rate": 0.000197828447339848,
      "loss": 0.3607,
      "step": 285
    },
    {
      "epoch": 0.011087957354010176,
      "grad_norm": 0.18783748149871826,
      "learning_rate": 0.00019782069179463316,
      "loss": 0.3468,
      "step": 286
    },
    {
      "epoch": 0.011126726435667556,
      "grad_norm": 0.17191797494888306,
      "learning_rate": 0.00019781293624941833,
      "loss": 0.3353,
      "step": 287
    },
    {
      "epoch": 0.011165495517324934,
      "grad_norm": 0.1678626388311386,
      "learning_rate": 0.0001978051807042035,
      "loss": 0.3243,
      "step": 288
    },
    {
      "epoch": 0.011204264598982312,
      "grad_norm": 0.15798453986644745,
      "learning_rate": 0.00019779742515898868,
      "loss": 0.286,
      "step": 289
    },
    {
      "epoch": 0.01124303368063969,
      "grad_norm": 0.19910366833209991,
      "learning_rate": 0.00019778966961377385,
      "loss": 0.315,
      "step": 290
    },
    {
      "epoch": 0.011281802762297068,
      "grad_norm": 0.14746172726154327,
      "learning_rate": 0.00019778191406855903,
      "loss": 0.2821,
      "step": 291
    },
    {
      "epoch": 0.011320571843954446,
      "grad_norm": 0.17933209240436554,
      "learning_rate": 0.0001977741585233442,
      "loss": 0.2993,
      "step": 292
    },
    {
      "epoch": 0.011359340925611824,
      "grad_norm": 0.2179802805185318,
      "learning_rate": 0.00019776640297812937,
      "loss": 0.4491,
      "step": 293
    },
    {
      "epoch": 0.011398110007269202,
      "grad_norm": 0.2650682330131531,
      "learning_rate": 0.00019775864743291454,
      "loss": 0.3999,
      "step": 294
    },
    {
      "epoch": 0.011436879088926582,
      "grad_norm": 0.39750441908836365,
      "learning_rate": 0.00019775089188769972,
      "loss": 0.2923,
      "step": 295
    },
    {
      "epoch": 0.01147564817058396,
      "grad_norm": 0.2048538476228714,
      "learning_rate": 0.0001977431363424849,
      "loss": 0.3767,
      "step": 296
    },
    {
      "epoch": 0.011514417252241338,
      "grad_norm": 0.23770080506801605,
      "learning_rate": 0.00019773538079727006,
      "loss": 0.3324,
      "step": 297
    },
    {
      "epoch": 0.011553186333898716,
      "grad_norm": 0.2167208045721054,
      "learning_rate": 0.00019772762525205524,
      "loss": 0.387,
      "step": 298
    },
    {
      "epoch": 0.011591955415556094,
      "grad_norm": 0.21112513542175293,
      "learning_rate": 0.0001977198697068404,
      "loss": 0.3602,
      "step": 299
    },
    {
      "epoch": 0.011630724497213472,
      "grad_norm": 0.2109885811805725,
      "learning_rate": 0.00019771211416162555,
      "loss": 0.3423,
      "step": 300
    },
    {
      "epoch": 0.01166949357887085,
      "grad_norm": 0.16143456101417542,
      "learning_rate": 0.00019770435861641075,
      "loss": 0.2867,
      "step": 301
    },
    {
      "epoch": 0.011708262660528228,
      "grad_norm": 0.17057301104068756,
      "learning_rate": 0.0001976966030711959,
      "loss": 0.2888,
      "step": 302
    },
    {
      "epoch": 0.011747031742185606,
      "grad_norm": 0.14760105311870575,
      "learning_rate": 0.0001976888475259811,
      "loss": 0.2664,
      "step": 303
    },
    {
      "epoch": 0.011785800823842986,
      "grad_norm": 0.1633409559726715,
      "learning_rate": 0.00019768109198076625,
      "loss": 0.2907,
      "step": 304
    },
    {
      "epoch": 0.011824569905500364,
      "grad_norm": 0.17707787454128265,
      "learning_rate": 0.00019767333643555145,
      "loss": 0.3252,
      "step": 305
    },
    {
      "epoch": 0.011863338987157742,
      "grad_norm": 0.19688111543655396,
      "learning_rate": 0.0001976655808903366,
      "loss": 0.3371,
      "step": 306
    },
    {
      "epoch": 0.01190210806881512,
      "grad_norm": 0.19473734498023987,
      "learning_rate": 0.00019765782534512176,
      "loss": 0.3798,
      "step": 307
    },
    {
      "epoch": 0.011940877150472498,
      "grad_norm": 0.14888896048069,
      "learning_rate": 0.00019765006979990694,
      "loss": 0.2537,
      "step": 308
    },
    {
      "epoch": 0.011979646232129876,
      "grad_norm": 0.1824783831834793,
      "learning_rate": 0.0001976423142546921,
      "loss": 0.402,
      "step": 309
    },
    {
      "epoch": 0.012018415313787254,
      "grad_norm": 0.17040567100048065,
      "learning_rate": 0.00019763455870947728,
      "loss": 0.342,
      "step": 310
    },
    {
      "epoch": 0.012057184395444632,
      "grad_norm": 0.1792033314704895,
      "learning_rate": 0.00019762680316426245,
      "loss": 0.3741,
      "step": 311
    },
    {
      "epoch": 0.012095953477102012,
      "grad_norm": 0.14477096498012543,
      "learning_rate": 0.00019761904761904763,
      "loss": 0.3898,
      "step": 312
    },
    {
      "epoch": 0.01213472255875939,
      "grad_norm": 0.18197011947631836,
      "learning_rate": 0.0001976112920738328,
      "loss": 0.3575,
      "step": 313
    },
    {
      "epoch": 0.012173491640416768,
      "grad_norm": 0.25540998578071594,
      "learning_rate": 0.00019760353652861797,
      "loss": 0.4476,
      "step": 314
    },
    {
      "epoch": 0.012212260722074146,
      "grad_norm": 0.16906523704528809,
      "learning_rate": 0.00019759578098340315,
      "loss": 0.3852,
      "step": 315
    },
    {
      "epoch": 0.012251029803731524,
      "grad_norm": 0.1891743540763855,
      "learning_rate": 0.00019758802543818832,
      "loss": 0.3941,
      "step": 316
    },
    {
      "epoch": 0.012289798885388902,
      "grad_norm": 0.21310895681381226,
      "learning_rate": 0.0001975802698929735,
      "loss": 0.2448,
      "step": 317
    },
    {
      "epoch": 0.01232856796704628,
      "grad_norm": 0.18495100736618042,
      "learning_rate": 0.00019757251434775866,
      "loss": 0.2787,
      "step": 318
    },
    {
      "epoch": 0.012367337048703658,
      "grad_norm": 0.13902883231639862,
      "learning_rate": 0.00019756475880254384,
      "loss": 0.2582,
      "step": 319
    },
    {
      "epoch": 0.012406106130361036,
      "grad_norm": 0.23445065319538116,
      "learning_rate": 0.000197557003257329,
      "loss": 0.4025,
      "step": 320
    },
    {
      "epoch": 0.012444875212018416,
      "grad_norm": 0.15872922539710999,
      "learning_rate": 0.00019754924771211416,
      "loss": 0.302,
      "step": 321
    },
    {
      "epoch": 0.012483644293675794,
      "grad_norm": 0.17622597515583038,
      "learning_rate": 0.00019754149216689936,
      "loss": 0.371,
      "step": 322
    },
    {
      "epoch": 0.012522413375333172,
      "grad_norm": 0.1626409888267517,
      "learning_rate": 0.0001975337366216845,
      "loss": 0.3547,
      "step": 323
    },
    {
      "epoch": 0.01256118245699055,
      "grad_norm": 0.1293002814054489,
      "learning_rate": 0.0001975259810764697,
      "loss": 0.2844,
      "step": 324
    },
    {
      "epoch": 0.012599951538647928,
      "grad_norm": 0.16771991550922394,
      "learning_rate": 0.00019751822553125485,
      "loss": 0.2798,
      "step": 325
    },
    {
      "epoch": 0.012638720620305306,
      "grad_norm": 0.18062721192836761,
      "learning_rate": 0.00019751046998604005,
      "loss": 0.3056,
      "step": 326
    },
    {
      "epoch": 0.012677489701962684,
      "grad_norm": 0.1534145027399063,
      "learning_rate": 0.0001975027144408252,
      "loss": 0.3746,
      "step": 327
    },
    {
      "epoch": 0.012716258783620062,
      "grad_norm": 0.17321977019309998,
      "learning_rate": 0.00019749495889561037,
      "loss": 0.3724,
      "step": 328
    },
    {
      "epoch": 0.012755027865277442,
      "grad_norm": 0.13039974868297577,
      "learning_rate": 0.00019748720335039554,
      "loss": 0.2945,
      "step": 329
    },
    {
      "epoch": 0.01279379694693482,
      "grad_norm": 0.1825912743806839,
      "learning_rate": 0.0001974794478051807,
      "loss": 0.3845,
      "step": 330
    },
    {
      "epoch": 0.012832566028592198,
      "grad_norm": 0.15827584266662598,
      "learning_rate": 0.00019747169225996588,
      "loss": 0.3069,
      "step": 331
    },
    {
      "epoch": 0.012871335110249576,
      "grad_norm": 0.16109423339366913,
      "learning_rate": 0.00019746393671475106,
      "loss": 0.3511,
      "step": 332
    },
    {
      "epoch": 0.012910104191906954,
      "grad_norm": 0.15195219218730927,
      "learning_rate": 0.00019745618116953623,
      "loss": 0.3644,
      "step": 333
    },
    {
      "epoch": 0.012948873273564332,
      "grad_norm": 0.20042744278907776,
      "learning_rate": 0.0001974484256243214,
      "loss": 0.2948,
      "step": 334
    },
    {
      "epoch": 0.01298764235522171,
      "grad_norm": 0.1693444401025772,
      "learning_rate": 0.00019744067007910655,
      "loss": 0.3109,
      "step": 335
    },
    {
      "epoch": 0.013026411436879088,
      "grad_norm": 0.1740037053823471,
      "learning_rate": 0.00019743291453389175,
      "loss": 0.4469,
      "step": 336
    },
    {
      "epoch": 0.013065180518536466,
      "grad_norm": 0.15699641406536102,
      "learning_rate": 0.0001974251589886769,
      "loss": 0.3549,
      "step": 337
    },
    {
      "epoch": 0.013103949600193846,
      "grad_norm": 0.12952309846878052,
      "learning_rate": 0.0001974174034434621,
      "loss": 0.2609,
      "step": 338
    },
    {
      "epoch": 0.013142718681851224,
      "grad_norm": 0.15725642442703247,
      "learning_rate": 0.00019740964789824724,
      "loss": 0.2554,
      "step": 339
    },
    {
      "epoch": 0.013181487763508602,
      "grad_norm": 0.17892472445964813,
      "learning_rate": 0.00019740189235303244,
      "loss": 0.3593,
      "step": 340
    },
    {
      "epoch": 0.01322025684516598,
      "grad_norm": 0.10841938108205795,
      "learning_rate": 0.00019739413680781758,
      "loss": 0.2245,
      "step": 341
    },
    {
      "epoch": 0.013259025926823358,
      "grad_norm": 0.13952043652534485,
      "learning_rate": 0.00019738638126260276,
      "loss": 0.303,
      "step": 342
    },
    {
      "epoch": 0.013297795008480736,
      "grad_norm": 0.17459259927272797,
      "learning_rate": 0.00019737862571738793,
      "loss": 0.1979,
      "step": 343
    },
    {
      "epoch": 0.013336564090138114,
      "grad_norm": 0.14991183578968048,
      "learning_rate": 0.0001973708701721731,
      "loss": 0.3705,
      "step": 344
    },
    {
      "epoch": 0.013375333171795492,
      "grad_norm": 0.1718611717224121,
      "learning_rate": 0.0001973631146269583,
      "loss": 0.3491,
      "step": 345
    },
    {
      "epoch": 0.013414102253452872,
      "grad_norm": 0.16202306747436523,
      "learning_rate": 0.00019735535908174345,
      "loss": 0.3625,
      "step": 346
    },
    {
      "epoch": 0.01345287133511025,
      "grad_norm": 0.15790744125843048,
      "learning_rate": 0.00019734760353652865,
      "loss": 0.4113,
      "step": 347
    },
    {
      "epoch": 0.013491640416767628,
      "grad_norm": 0.17096927762031555,
      "learning_rate": 0.0001973398479913138,
      "loss": 0.254,
      "step": 348
    },
    {
      "epoch": 0.013530409498425006,
      "grad_norm": 0.1469568908214569,
      "learning_rate": 0.00019733209244609897,
      "loss": 0.3018,
      "step": 349
    },
    {
      "epoch": 0.013569178580082384,
      "grad_norm": 0.18193010985851288,
      "learning_rate": 0.00019732433690088414,
      "loss": 0.2983,
      "step": 350
    },
    {
      "epoch": 0.013607947661739762,
      "grad_norm": 0.171823188662529,
      "learning_rate": 0.0001973165813556693,
      "loss": 0.3478,
      "step": 351
    },
    {
      "epoch": 0.01364671674339714,
      "grad_norm": 0.17279936373233795,
      "learning_rate": 0.00019730882581045449,
      "loss": 0.3802,
      "step": 352
    },
    {
      "epoch": 0.013685485825054518,
      "grad_norm": 0.143559992313385,
      "learning_rate": 0.00019730107026523966,
      "loss": 0.4014,
      "step": 353
    },
    {
      "epoch": 0.013724254906711897,
      "grad_norm": 0.14849218726158142,
      "learning_rate": 0.00019729331472002483,
      "loss": 0.4164,
      "step": 354
    },
    {
      "epoch": 0.013763023988369276,
      "grad_norm": 0.18447205424308777,
      "learning_rate": 0.00019728555917481,
      "loss": 0.3852,
      "step": 355
    },
    {
      "epoch": 0.013801793070026654,
      "grad_norm": 0.13087542355060577,
      "learning_rate": 0.00019727780362959515,
      "loss": 0.2702,
      "step": 356
    },
    {
      "epoch": 0.013840562151684032,
      "grad_norm": 0.17141889035701752,
      "learning_rate": 0.00019727004808438035,
      "loss": 0.3041,
      "step": 357
    },
    {
      "epoch": 0.01387933123334141,
      "grad_norm": 0.1346399188041687,
      "learning_rate": 0.0001972622925391655,
      "loss": 0.3317,
      "step": 358
    },
    {
      "epoch": 0.013918100314998788,
      "grad_norm": 0.15137608349323273,
      "learning_rate": 0.0001972545369939507,
      "loss": 0.3391,
      "step": 359
    },
    {
      "epoch": 0.013956869396656166,
      "grad_norm": 0.17044253647327423,
      "learning_rate": 0.00019724678144873584,
      "loss": 0.34,
      "step": 360
    },
    {
      "epoch": 0.013995638478313545,
      "grad_norm": 0.13251839578151703,
      "learning_rate": 0.00019723902590352104,
      "loss": 0.3125,
      "step": 361
    },
    {
      "epoch": 0.014034407559970923,
      "grad_norm": 0.20663651823997498,
      "learning_rate": 0.0001972312703583062,
      "loss": 0.4761,
      "step": 362
    },
    {
      "epoch": 0.0140731766416283,
      "grad_norm": 0.14277100563049316,
      "learning_rate": 0.00019722351481309136,
      "loss": 0.3136,
      "step": 363
    },
    {
      "epoch": 0.01411194572328568,
      "grad_norm": 0.1627928465604782,
      "learning_rate": 0.00019721575926787653,
      "loss": 0.2542,
      "step": 364
    },
    {
      "epoch": 0.014150714804943058,
      "grad_norm": 0.13414818048477173,
      "learning_rate": 0.0001972080037226617,
      "loss": 0.3094,
      "step": 365
    },
    {
      "epoch": 0.014189483886600436,
      "grad_norm": 0.1463170051574707,
      "learning_rate": 0.00019720024817744688,
      "loss": 0.3428,
      "step": 366
    },
    {
      "epoch": 0.014228252968257814,
      "grad_norm": 0.1298328936100006,
      "learning_rate": 0.00019719249263223205,
      "loss": 0.297,
      "step": 367
    },
    {
      "epoch": 0.014267022049915193,
      "grad_norm": 0.18295900523662567,
      "learning_rate": 0.00019718473708701722,
      "loss": 0.3135,
      "step": 368
    },
    {
      "epoch": 0.01430579113157257,
      "grad_norm": 0.15160812437534332,
      "learning_rate": 0.0001971769815418024,
      "loss": 0.3059,
      "step": 369
    },
    {
      "epoch": 0.014344560213229949,
      "grad_norm": 0.4178626835346222,
      "learning_rate": 0.00019716922599658757,
      "loss": 0.3444,
      "step": 370
    },
    {
      "epoch": 0.014383329294887327,
      "grad_norm": 0.22701288759708405,
      "learning_rate": 0.00019716147045137274,
      "loss": 0.314,
      "step": 371
    },
    {
      "epoch": 0.014422098376544706,
      "grad_norm": 4.2072601318359375,
      "learning_rate": 0.00019715371490615791,
      "loss": 0.3212,
      "step": 372
    },
    {
      "epoch": 0.014460867458202084,
      "grad_norm": 0.1444479525089264,
      "learning_rate": 0.0001971459593609431,
      "loss": 0.3968,
      "step": 373
    },
    {
      "epoch": 0.014499636539859462,
      "grad_norm": 0.46109452843666077,
      "learning_rate": 0.00019713820381572826,
      "loss": 0.286,
      "step": 374
    },
    {
      "epoch": 0.01453840562151684,
      "grad_norm": 0.27657434344291687,
      "learning_rate": 0.00019713044827051343,
      "loss": 0.2882,
      "step": 375
    },
    {
      "epoch": 0.014577174703174219,
      "grad_norm": 0.1620926856994629,
      "learning_rate": 0.0001971226927252986,
      "loss": 0.2686,
      "step": 376
    },
    {
      "epoch": 0.014615943784831597,
      "grad_norm": 0.1863682121038437,
      "learning_rate": 0.00019711493718008375,
      "loss": 0.3335,
      "step": 377
    },
    {
      "epoch": 0.014654712866488975,
      "grad_norm": 0.3389407694339752,
      "learning_rate": 0.00019710718163486895,
      "loss": 0.3859,
      "step": 378
    },
    {
      "epoch": 0.014693481948146353,
      "grad_norm": 0.2953808903694153,
      "learning_rate": 0.0001970994260896541,
      "loss": 0.2865,
      "step": 379
    },
    {
      "epoch": 0.01473225102980373,
      "grad_norm": 0.1781793236732483,
      "learning_rate": 0.0001970916705444393,
      "loss": 0.2588,
      "step": 380
    },
    {
      "epoch": 0.01477102011146111,
      "grad_norm": 0.21816477179527283,
      "learning_rate": 0.00019708391499922444,
      "loss": 0.3586,
      "step": 381
    },
    {
      "epoch": 0.014809789193118489,
      "grad_norm": 0.1896308809518814,
      "learning_rate": 0.00019707615945400964,
      "loss": 0.2902,
      "step": 382
    },
    {
      "epoch": 0.014848558274775867,
      "grad_norm": 0.19732654094696045,
      "learning_rate": 0.0001970684039087948,
      "loss": 0.3138,
      "step": 383
    },
    {
      "epoch": 0.014887327356433245,
      "grad_norm": 0.1830669343471527,
      "learning_rate": 0.00019706064836357996,
      "loss": 0.305,
      "step": 384
    },
    {
      "epoch": 0.014926096438090623,
      "grad_norm": 0.15074564516544342,
      "learning_rate": 0.00019705289281836513,
      "loss": 0.3441,
      "step": 385
    },
    {
      "epoch": 0.014964865519748,
      "grad_norm": 0.1711883693933487,
      "learning_rate": 0.0001970451372731503,
      "loss": 0.3178,
      "step": 386
    },
    {
      "epoch": 0.015003634601405379,
      "grad_norm": 0.15485863387584686,
      "learning_rate": 0.00019703738172793548,
      "loss": 0.3019,
      "step": 387
    },
    {
      "epoch": 0.015042403683062757,
      "grad_norm": 0.1592789590358734,
      "learning_rate": 0.00019702962618272065,
      "loss": 0.2968,
      "step": 388
    },
    {
      "epoch": 0.015081172764720137,
      "grad_norm": 0.17003025114536285,
      "learning_rate": 0.00019702187063750583,
      "loss": 0.2776,
      "step": 389
    },
    {
      "epoch": 0.015119941846377515,
      "grad_norm": 0.18380650877952576,
      "learning_rate": 0.000197014115092291,
      "loss": 0.3752,
      "step": 390
    },
    {
      "epoch": 0.015158710928034893,
      "grad_norm": 0.12687113881111145,
      "learning_rate": 0.00019700635954707617,
      "loss": 0.267,
      "step": 391
    },
    {
      "epoch": 0.01519748000969227,
      "grad_norm": 0.18678773939609528,
      "learning_rate": 0.00019699860400186134,
      "loss": 0.3703,
      "step": 392
    },
    {
      "epoch": 0.015236249091349649,
      "grad_norm": 0.13310611248016357,
      "learning_rate": 0.00019699084845664652,
      "loss": 0.2528,
      "step": 393
    },
    {
      "epoch": 0.015275018173007027,
      "grad_norm": 0.14297664165496826,
      "learning_rate": 0.0001969830929114317,
      "loss": 0.2971,
      "step": 394
    },
    {
      "epoch": 0.015313787254664405,
      "grad_norm": 0.1365012526512146,
      "learning_rate": 0.00019697533736621686,
      "loss": 0.3633,
      "step": 395
    },
    {
      "epoch": 0.015352556336321783,
      "grad_norm": 0.11892976611852646,
      "learning_rate": 0.00019696758182100203,
      "loss": 0.2151,
      "step": 396
    },
    {
      "epoch": 0.01539132541797916,
      "grad_norm": 0.17364825308322906,
      "learning_rate": 0.0001969598262757872,
      "loss": 0.4132,
      "step": 397
    },
    {
      "epoch": 0.01543009449963654,
      "grad_norm": 0.15145424008369446,
      "learning_rate": 0.00019695207073057235,
      "loss": 0.3646,
      "step": 398
    },
    {
      "epoch": 0.015468863581293919,
      "grad_norm": 0.1389559805393219,
      "learning_rate": 0.00019694431518535755,
      "loss": 0.2742,
      "step": 399
    },
    {
      "epoch": 0.015507632662951297,
      "grad_norm": 0.20473559200763702,
      "learning_rate": 0.0001969365596401427,
      "loss": 0.2589,
      "step": 400
    },
    {
      "epoch": 0.015546401744608675,
      "grad_norm": 0.13872358202934265,
      "learning_rate": 0.0001969288040949279,
      "loss": 0.3118,
      "step": 401
    },
    {
      "epoch": 0.015585170826266053,
      "grad_norm": 0.14497703313827515,
      "learning_rate": 0.00019692104854971304,
      "loss": 0.356,
      "step": 402
    },
    {
      "epoch": 0.01562393990792343,
      "grad_norm": 0.15379808843135834,
      "learning_rate": 0.00019691329300449824,
      "loss": 0.3221,
      "step": 403
    },
    {
      "epoch": 0.01566270898958081,
      "grad_norm": 0.1287560611963272,
      "learning_rate": 0.0001969055374592834,
      "loss": 0.2963,
      "step": 404
    },
    {
      "epoch": 0.015701478071238187,
      "grad_norm": 0.13889673352241516,
      "learning_rate": 0.00019689778191406856,
      "loss": 0.2836,
      "step": 405
    },
    {
      "epoch": 0.015740247152895567,
      "grad_norm": 0.23568636178970337,
      "learning_rate": 0.00019689002636885374,
      "loss": 0.3633,
      "step": 406
    },
    {
      "epoch": 0.015779016234552943,
      "grad_norm": 0.20410463213920593,
      "learning_rate": 0.0001968822708236389,
      "loss": 0.4038,
      "step": 407
    },
    {
      "epoch": 0.015817785316210323,
      "grad_norm": 0.23576930165290833,
      "learning_rate": 0.00019687451527842408,
      "loss": 0.3234,
      "step": 408
    },
    {
      "epoch": 0.0158565543978677,
      "grad_norm": 0.14117656648159027,
      "learning_rate": 0.00019686675973320925,
      "loss": 0.2376,
      "step": 409
    },
    {
      "epoch": 0.01589532347952508,
      "grad_norm": 0.23222610354423523,
      "learning_rate": 0.00019685900418799443,
      "loss": 0.3052,
      "step": 410
    },
    {
      "epoch": 0.01593409256118246,
      "grad_norm": 0.13417376577854156,
      "learning_rate": 0.0001968512486427796,
      "loss": 0.3504,
      "step": 411
    },
    {
      "epoch": 0.015972861642839835,
      "grad_norm": 0.20743921399116516,
      "learning_rate": 0.00019684349309756475,
      "loss": 0.3812,
      "step": 412
    },
    {
      "epoch": 0.016011630724497215,
      "grad_norm": 0.152695432305336,
      "learning_rate": 0.00019683573755234995,
      "loss": 0.3319,
      "step": 413
    },
    {
      "epoch": 0.01605039980615459,
      "grad_norm": 0.1172674149274826,
      "learning_rate": 0.0001968279820071351,
      "loss": 0.3222,
      "step": 414
    },
    {
      "epoch": 0.01608916888781197,
      "grad_norm": 0.15249352157115936,
      "learning_rate": 0.0001968202264619203,
      "loss": 0.3393,
      "step": 415
    },
    {
      "epoch": 0.016127937969469347,
      "grad_norm": 0.17346139252185822,
      "learning_rate": 0.00019681247091670544,
      "loss": 0.3137,
      "step": 416
    },
    {
      "epoch": 0.016166707051126727,
      "grad_norm": 0.1215927004814148,
      "learning_rate": 0.00019680471537149064,
      "loss": 0.2773,
      "step": 417
    },
    {
      "epoch": 0.016205476132784103,
      "grad_norm": 0.1866036355495453,
      "learning_rate": 0.00019679695982627578,
      "loss": 0.3991,
      "step": 418
    },
    {
      "epoch": 0.016244245214441483,
      "grad_norm": 0.13450121879577637,
      "learning_rate": 0.00019678920428106096,
      "loss": 0.3733,
      "step": 419
    },
    {
      "epoch": 0.016283014296098863,
      "grad_norm": 0.14097674190998077,
      "learning_rate": 0.00019678144873584613,
      "loss": 0.3737,
      "step": 420
    },
    {
      "epoch": 0.01632178337775624,
      "grad_norm": 0.1487146019935608,
      "learning_rate": 0.0001967736931906313,
      "loss": 0.2737,
      "step": 421
    },
    {
      "epoch": 0.01636055245941362,
      "grad_norm": 0.15188582241535187,
      "learning_rate": 0.00019676593764541647,
      "loss": 0.3997,
      "step": 422
    },
    {
      "epoch": 0.016399321541070995,
      "grad_norm": 0.148170605301857,
      "learning_rate": 0.00019675818210020165,
      "loss": 0.411,
      "step": 423
    },
    {
      "epoch": 0.016438090622728375,
      "grad_norm": 0.16563136875629425,
      "learning_rate": 0.00019675042655498685,
      "loss": 0.3869,
      "step": 424
    },
    {
      "epoch": 0.01647685970438575,
      "grad_norm": 0.12495933473110199,
      "learning_rate": 0.000196742671009772,
      "loss": 0.3035,
      "step": 425
    },
    {
      "epoch": 0.01651562878604313,
      "grad_norm": 0.11250076442956924,
      "learning_rate": 0.00019673491546455716,
      "loss": 0.267,
      "step": 426
    },
    {
      "epoch": 0.016554397867700507,
      "grad_norm": 0.1906145066022873,
      "learning_rate": 0.00019672715991934234,
      "loss": 0.24,
      "step": 427
    },
    {
      "epoch": 0.016593166949357887,
      "grad_norm": 0.1340598315000534,
      "learning_rate": 0.0001967194043741275,
      "loss": 0.2978,
      "step": 428
    },
    {
      "epoch": 0.016631936031015267,
      "grad_norm": 0.17734038829803467,
      "learning_rate": 0.00019671164882891268,
      "loss": 0.3504,
      "step": 429
    },
    {
      "epoch": 0.016670705112672643,
      "grad_norm": 0.17504557967185974,
      "learning_rate": 0.00019670389328369786,
      "loss": 0.3364,
      "step": 430
    },
    {
      "epoch": 0.016709474194330023,
      "grad_norm": 0.15641652047634125,
      "learning_rate": 0.00019669613773848303,
      "loss": 0.3275,
      "step": 431
    },
    {
      "epoch": 0.0167482432759874,
      "grad_norm": 0.1359223872423172,
      "learning_rate": 0.0001966883821932682,
      "loss": 0.2901,
      "step": 432
    },
    {
      "epoch": 0.01678701235764478,
      "grad_norm": 0.1247914656996727,
      "learning_rate": 0.00019668062664805335,
      "loss": 0.3211,
      "step": 433
    },
    {
      "epoch": 0.016825781439302155,
      "grad_norm": 0.182841956615448,
      "learning_rate": 0.00019667287110283855,
      "loss": 0.2943,
      "step": 434
    },
    {
      "epoch": 0.016864550520959535,
      "grad_norm": 0.12591026723384857,
      "learning_rate": 0.0001966651155576237,
      "loss": 0.3056,
      "step": 435
    },
    {
      "epoch": 0.016903319602616915,
      "grad_norm": 0.17070595920085907,
      "learning_rate": 0.0001966573600124089,
      "loss": 0.3261,
      "step": 436
    },
    {
      "epoch": 0.01694208868427429,
      "grad_norm": 0.11679597198963165,
      "learning_rate": 0.00019664960446719404,
      "loss": 0.2997,
      "step": 437
    },
    {
      "epoch": 0.01698085776593167,
      "grad_norm": 0.1818968951702118,
      "learning_rate": 0.00019664184892197924,
      "loss": 0.3805,
      "step": 438
    },
    {
      "epoch": 0.017019626847589047,
      "grad_norm": 0.10652333498001099,
      "learning_rate": 0.00019663409337676438,
      "loss": 0.2274,
      "step": 439
    },
    {
      "epoch": 0.017058395929246427,
      "grad_norm": 0.13854193687438965,
      "learning_rate": 0.00019662633783154956,
      "loss": 0.3703,
      "step": 440
    },
    {
      "epoch": 0.017097165010903803,
      "grad_norm": 0.14380992949008942,
      "learning_rate": 0.00019661858228633473,
      "loss": 0.2861,
      "step": 441
    },
    {
      "epoch": 0.017135934092561183,
      "grad_norm": 0.1269422024488449,
      "learning_rate": 0.0001966108267411199,
      "loss": 0.3638,
      "step": 442
    },
    {
      "epoch": 0.01717470317421856,
      "grad_norm": 0.12065412104129791,
      "learning_rate": 0.00019660307119590508,
      "loss": 0.2765,
      "step": 443
    },
    {
      "epoch": 0.01721347225587594,
      "grad_norm": 0.11309611797332764,
      "learning_rate": 0.00019659531565069025,
      "loss": 0.2974,
      "step": 444
    },
    {
      "epoch": 0.01725224133753332,
      "grad_norm": 0.12600910663604736,
      "learning_rate": 0.00019658756010547542,
      "loss": 0.3662,
      "step": 445
    },
    {
      "epoch": 0.017291010419190695,
      "grad_norm": 0.11566796898841858,
      "learning_rate": 0.0001965798045602606,
      "loss": 0.3736,
      "step": 446
    },
    {
      "epoch": 0.017329779500848075,
      "grad_norm": 0.1661793291568756,
      "learning_rate": 0.00019657204901504577,
      "loss": 0.335,
      "step": 447
    },
    {
      "epoch": 0.01736854858250545,
      "grad_norm": 0.11099682748317719,
      "learning_rate": 0.00019656429346983094,
      "loss": 0.2729,
      "step": 448
    },
    {
      "epoch": 0.01740731766416283,
      "grad_norm": 0.16623780131340027,
      "learning_rate": 0.0001965565379246161,
      "loss": 0.3343,
      "step": 449
    },
    {
      "epoch": 0.017446086745820207,
      "grad_norm": 0.17180109024047852,
      "learning_rate": 0.00019654878237940129,
      "loss": 0.2921,
      "step": 450
    },
    {
      "epoch": 0.017484855827477587,
      "grad_norm": 0.1504889875650406,
      "learning_rate": 0.00019654102683418646,
      "loss": 0.2741,
      "step": 451
    },
    {
      "epoch": 0.017523624909134963,
      "grad_norm": 0.10422014445066452,
      "learning_rate": 0.00019653327128897163,
      "loss": 0.2226,
      "step": 452
    },
    {
      "epoch": 0.017562393990792343,
      "grad_norm": 0.14729394018650055,
      "learning_rate": 0.0001965255157437568,
      "loss": 0.3405,
      "step": 453
    },
    {
      "epoch": 0.017601163072449723,
      "grad_norm": 0.1776709109544754,
      "learning_rate": 0.00019651776019854195,
      "loss": 0.3777,
      "step": 454
    },
    {
      "epoch": 0.0176399321541071,
      "grad_norm": 0.13256187736988068,
      "learning_rate": 0.00019651000465332715,
      "loss": 0.3678,
      "step": 455
    },
    {
      "epoch": 0.01767870123576448,
      "grad_norm": 0.1357676386833191,
      "learning_rate": 0.0001965022491081123,
      "loss": 0.3796,
      "step": 456
    },
    {
      "epoch": 0.017717470317421855,
      "grad_norm": 0.15299804508686066,
      "learning_rate": 0.0001964944935628975,
      "loss": 0.3233,
      "step": 457
    },
    {
      "epoch": 0.017756239399079235,
      "grad_norm": 0.14166061580181122,
      "learning_rate": 0.00019648673801768264,
      "loss": 0.3091,
      "step": 458
    },
    {
      "epoch": 0.01779500848073661,
      "grad_norm": 0.15285156667232513,
      "learning_rate": 0.00019647898247246784,
      "loss": 0.3561,
      "step": 459
    },
    {
      "epoch": 0.01783377756239399,
      "grad_norm": 0.16991491615772247,
      "learning_rate": 0.00019647122692725299,
      "loss": 0.3442,
      "step": 460
    },
    {
      "epoch": 0.017872546644051367,
      "grad_norm": 0.17830587923526764,
      "learning_rate": 0.00019646347138203816,
      "loss": 0.3461,
      "step": 461
    },
    {
      "epoch": 0.017911315725708747,
      "grad_norm": 0.1583179235458374,
      "learning_rate": 0.00019645571583682333,
      "loss": 0.3149,
      "step": 462
    },
    {
      "epoch": 0.017950084807366127,
      "grad_norm": 0.2645764946937561,
      "learning_rate": 0.0001964479602916085,
      "loss": 0.365,
      "step": 463
    },
    {
      "epoch": 0.017988853889023503,
      "grad_norm": 0.18679235875606537,
      "learning_rate": 0.00019644020474639368,
      "loss": 0.3996,
      "step": 464
    },
    {
      "epoch": 0.018027622970680883,
      "grad_norm": 0.23870238661766052,
      "learning_rate": 0.00019643244920117885,
      "loss": 0.3848,
      "step": 465
    },
    {
      "epoch": 0.01806639205233826,
      "grad_norm": 0.20034120976924896,
      "learning_rate": 0.00019642469365596402,
      "loss": 0.28,
      "step": 466
    },
    {
      "epoch": 0.01810516113399564,
      "grad_norm": 0.12934046983718872,
      "learning_rate": 0.0001964169381107492,
      "loss": 0.3173,
      "step": 467
    },
    {
      "epoch": 0.018143930215653015,
      "grad_norm": 0.2263900190591812,
      "learning_rate": 0.00019640918256553437,
      "loss": 0.2826,
      "step": 468
    },
    {
      "epoch": 0.018182699297310395,
      "grad_norm": 0.17984196543693542,
      "learning_rate": 0.00019640142702031954,
      "loss": 0.3531,
      "step": 469
    },
    {
      "epoch": 0.018221468378967775,
      "grad_norm": 0.13668012619018555,
      "learning_rate": 0.00019639367147510471,
      "loss": 0.3074,
      "step": 470
    },
    {
      "epoch": 0.01826023746062515,
      "grad_norm": 0.15420718491077423,
      "learning_rate": 0.0001963859159298899,
      "loss": 0.3384,
      "step": 471
    },
    {
      "epoch": 0.01829900654228253,
      "grad_norm": 0.14365902543067932,
      "learning_rate": 0.00019637816038467506,
      "loss": 0.2748,
      "step": 472
    },
    {
      "epoch": 0.018337775623939907,
      "grad_norm": 0.1230478435754776,
      "learning_rate": 0.00019637040483946023,
      "loss": 0.2912,
      "step": 473
    },
    {
      "epoch": 0.018376544705597287,
      "grad_norm": 0.14745737612247467,
      "learning_rate": 0.0001963626492942454,
      "loss": 0.341,
      "step": 474
    },
    {
      "epoch": 0.018415313787254663,
      "grad_norm": 0.15114833414554596,
      "learning_rate": 0.00019635489374903058,
      "loss": 0.3764,
      "step": 475
    },
    {
      "epoch": 0.018454082868912043,
      "grad_norm": 0.14132431149482727,
      "learning_rate": 0.00019634713820381575,
      "loss": 0.3807,
      "step": 476
    },
    {
      "epoch": 0.01849285195056942,
      "grad_norm": 0.14662304520606995,
      "learning_rate": 0.0001963393826586009,
      "loss": 0.3174,
      "step": 477
    },
    {
      "epoch": 0.0185316210322268,
      "grad_norm": 0.1288323551416397,
      "learning_rate": 0.0001963316271133861,
      "loss": 0.3244,
      "step": 478
    },
    {
      "epoch": 0.01857039011388418,
      "grad_norm": 0.11655396968126297,
      "learning_rate": 0.00019632387156817124,
      "loss": 0.2948,
      "step": 479
    },
    {
      "epoch": 0.018609159195541555,
      "grad_norm": 0.13553081452846527,
      "learning_rate": 0.00019631611602295644,
      "loss": 0.3089,
      "step": 480
    },
    {
      "epoch": 0.018647928277198935,
      "grad_norm": 0.12978409230709076,
      "learning_rate": 0.0001963083604777416,
      "loss": 0.3753,
      "step": 481
    },
    {
      "epoch": 0.01868669735885631,
      "grad_norm": 0.12580548226833344,
      "learning_rate": 0.0001963006049325268,
      "loss": 0.35,
      "step": 482
    },
    {
      "epoch": 0.01872546644051369,
      "grad_norm": 0.1397409737110138,
      "learning_rate": 0.00019629284938731193,
      "loss": 0.4071,
      "step": 483
    },
    {
      "epoch": 0.018764235522171067,
      "grad_norm": 0.24354536831378937,
      "learning_rate": 0.0001962850938420971,
      "loss": 0.3744,
      "step": 484
    },
    {
      "epoch": 0.018803004603828447,
      "grad_norm": 0.12411569803953171,
      "learning_rate": 0.00019627733829688228,
      "loss": 0.4022,
      "step": 485
    },
    {
      "epoch": 0.018841773685485824,
      "grad_norm": 0.1271260678768158,
      "learning_rate": 0.00019626958275166745,
      "loss": 0.3319,
      "step": 486
    },
    {
      "epoch": 0.018880542767143203,
      "grad_norm": 0.14929701387882233,
      "learning_rate": 0.00019626182720645262,
      "loss": 0.3811,
      "step": 487
    },
    {
      "epoch": 0.018919311848800583,
      "grad_norm": 0.15954889357089996,
      "learning_rate": 0.0001962540716612378,
      "loss": 0.384,
      "step": 488
    },
    {
      "epoch": 0.01895808093045796,
      "grad_norm": 0.12126569449901581,
      "learning_rate": 0.00019624631611602297,
      "loss": 0.3006,
      "step": 489
    },
    {
      "epoch": 0.01899685001211534,
      "grad_norm": 0.16860656440258026,
      "learning_rate": 0.00019623856057080814,
      "loss": 0.3763,
      "step": 490
    },
    {
      "epoch": 0.019035619093772715,
      "grad_norm": 0.132612943649292,
      "learning_rate": 0.0001962308050255933,
      "loss": 0.2896,
      "step": 491
    },
    {
      "epoch": 0.019074388175430095,
      "grad_norm": 0.09728425741195679,
      "learning_rate": 0.0001962230494803785,
      "loss": 0.2022,
      "step": 492
    },
    {
      "epoch": 0.01911315725708747,
      "grad_norm": 0.12854249775409698,
      "learning_rate": 0.00019621529393516363,
      "loss": 0.2721,
      "step": 493
    },
    {
      "epoch": 0.01915192633874485,
      "grad_norm": 0.14845336973667145,
      "learning_rate": 0.00019620753838994883,
      "loss": 0.3557,
      "step": 494
    },
    {
      "epoch": 0.019190695420402228,
      "grad_norm": 0.1459975242614746,
      "learning_rate": 0.00019619978284473398,
      "loss": 0.359,
      "step": 495
    },
    {
      "epoch": 0.019229464502059607,
      "grad_norm": 0.1503349244594574,
      "learning_rate": 0.00019619202729951918,
      "loss": 0.3922,
      "step": 496
    },
    {
      "epoch": 0.019268233583716987,
      "grad_norm": 0.15641088783740997,
      "learning_rate": 0.00019618427175430433,
      "loss": 0.4299,
      "step": 497
    },
    {
      "epoch": 0.019307002665374363,
      "grad_norm": 0.12854236364364624,
      "learning_rate": 0.0001961765162090895,
      "loss": 0.4304,
      "step": 498
    },
    {
      "epoch": 0.019345771747031743,
      "grad_norm": 0.1404946744441986,
      "learning_rate": 0.00019616876066387467,
      "loss": 0.2577,
      "step": 499
    },
    {
      "epoch": 0.01938454082868912,
      "grad_norm": 0.11552102118730545,
      "learning_rate": 0.00019616100511865984,
      "loss": 0.3135,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 25793,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.56401286463488e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
