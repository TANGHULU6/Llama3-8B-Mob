{
  "best_metric": 0.3281901478767395,
  "best_model_checkpoint": "outputs/checkpoint-500",
  "epoch": 0.8333333333333334,
  "eval_steps": 100,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.0955612659454346,
      "learning_rate": 4e-05,
      "loss": 1.9448,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.291522264480591,
      "learning_rate": 8e-05,
      "loss": 1.8955,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.3751931190490723,
      "learning_rate": 0.00012,
      "loss": 1.9633,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.9432048797607422,
      "learning_rate": 0.00016,
      "loss": 1.7703,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4780776500701904,
      "learning_rate": 0.0002,
      "loss": 1.5581,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.2533655166625977,
      "learning_rate": 0.00019988857938718665,
      "loss": 1.3527,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.636291265487671,
      "learning_rate": 0.00019977715877437326,
      "loss": 1.0732,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.666510820388794,
      "learning_rate": 0.0001996657381615599,
      "loss": 1.2106,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.6698962450027466,
      "learning_rate": 0.00019955431754874653,
      "loss": 0.8514,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 2.109370708465576,
      "learning_rate": 0.00019944289693593316,
      "loss": 0.9609,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.8895906209945679,
      "learning_rate": 0.0001993314763231198,
      "loss": 0.6892,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8486143946647644,
      "learning_rate": 0.0001992200557103064,
      "loss": 0.4873,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.9126704931259155,
      "learning_rate": 0.00019910863509749305,
      "loss": 0.5162,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.47595322132110596,
      "learning_rate": 0.00019899721448467968,
      "loss": 0.6428,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.41951069235801697,
      "learning_rate": 0.0001988857938718663,
      "loss": 0.4553,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.49427658319473267,
      "learning_rate": 0.00019877437325905293,
      "loss": 0.6551,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.2967330813407898,
      "learning_rate": 0.00019866295264623957,
      "loss": 0.4602,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.2054484486579895,
      "learning_rate": 0.0001985515320334262,
      "loss": 0.4395,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.28449466824531555,
      "learning_rate": 0.00019844011142061284,
      "loss": 0.4793,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.34066563844680786,
      "learning_rate": 0.00019832869080779945,
      "loss": 0.5514,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2806191146373749,
      "learning_rate": 0.00019821727019498608,
      "loss": 0.5207,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.1904837042093277,
      "learning_rate": 0.00019810584958217272,
      "loss": 0.4974,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.16037176549434662,
      "learning_rate": 0.00019799442896935933,
      "loss": 0.393,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.163355752825737,
      "learning_rate": 0.00019788300835654597,
      "loss": 0.4312,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.1961064487695694,
      "learning_rate": 0.00019777158774373258,
      "loss": 0.4412,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.19962693750858307,
      "learning_rate": 0.00019766016713091924,
      "loss": 0.5202,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.1513756513595581,
      "learning_rate": 0.00019754874651810588,
      "loss": 0.4442,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.1553286612033844,
      "learning_rate": 0.00019743732590529249,
      "loss": 0.4703,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.12882000207901,
      "learning_rate": 0.00019732590529247912,
      "loss": 0.3949,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15362459421157837,
      "learning_rate": 0.00019721448467966573,
      "loss": 0.4067,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.16636455059051514,
      "learning_rate": 0.00019710306406685237,
      "loss": 0.4751,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10700325667858124,
      "learning_rate": 0.000196991643454039,
      "loss": 0.4901,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.10109906643629074,
      "learning_rate": 0.00019688022284122561,
      "loss": 0.2806,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.15828914940357208,
      "learning_rate": 0.00019676880222841228,
      "loss": 0.4163,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.1429501175880432,
      "learning_rate": 0.00019665738161559891,
      "loss": 0.3946,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11291559785604477,
      "learning_rate": 0.00019654596100278552,
      "loss": 0.4201,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.11685312539339066,
      "learning_rate": 0.00019643454038997216,
      "loss": 0.3055,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.11478576809167862,
      "learning_rate": 0.00019632311977715877,
      "loss": 0.3478,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.11220988631248474,
      "learning_rate": 0.0001962116991643454,
      "loss": 0.3454,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.10664703696966171,
      "learning_rate": 0.00019610027855153204,
      "loss": 0.3251,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.10532573610544205,
      "learning_rate": 0.00019598885793871865,
      "loss": 0.3798,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.09253589063882828,
      "learning_rate": 0.00019587743732590532,
      "loss": 0.3477,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.10197603702545166,
      "learning_rate": 0.00019576601671309192,
      "loss": 0.3672,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09146757423877716,
      "learning_rate": 0.00019565459610027856,
      "loss": 0.4197,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12180428951978683,
      "learning_rate": 0.0001955431754874652,
      "loss": 0.4203,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.11242137849330902,
      "learning_rate": 0.0001954317548746518,
      "loss": 0.3167,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.0971837043762207,
      "learning_rate": 0.00019532033426183844,
      "loss": 0.3995,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08444882929325104,
      "learning_rate": 0.00019520891364902508,
      "loss": 0.2908,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.09193655103445053,
      "learning_rate": 0.0001950974930362117,
      "loss": 0.3851,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.10707516968250275,
      "learning_rate": 0.00019498607242339835,
      "loss": 0.3579,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.08961743116378784,
      "learning_rate": 0.00019487465181058496,
      "loss": 0.3716,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.09596701711416245,
      "learning_rate": 0.0001947632311977716,
      "loss": 0.415,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.06666703522205353,
      "learning_rate": 0.00019465181058495824,
      "loss": 0.3567,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.11535973846912384,
      "learning_rate": 0.00019454038997214484,
      "loss": 0.4295,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.08375324308872223,
      "learning_rate": 0.00019442896935933148,
      "loss": 0.3019,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.10852738469839096,
      "learning_rate": 0.00019431754874651812,
      "loss": 0.3493,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.10233605653047562,
      "learning_rate": 0.00019420612813370473,
      "loss": 0.3831,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.13586613535881042,
      "learning_rate": 0.0001940947075208914,
      "loss": 0.4087,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.07681457698345184,
      "learning_rate": 0.000193983286908078,
      "loss": 0.3024,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.09336651116609573,
      "learning_rate": 0.00019387186629526464,
      "loss": 0.3808,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.06770402193069458,
      "learning_rate": 0.00019376044568245127,
      "loss": 0.2627,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.07679001986980438,
      "learning_rate": 0.00019364902506963788,
      "loss": 0.3938,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06754700839519501,
      "learning_rate": 0.00019353760445682452,
      "loss": 0.3716,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07708554714918137,
      "learning_rate": 0.00019342618384401116,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.08859172463417053,
      "learning_rate": 0.00019331476323119776,
      "loss": 0.3443,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.10104084014892578,
      "learning_rate": 0.00019320334261838443,
      "loss": 0.3372,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.07533049583435059,
      "learning_rate": 0.00019309192200557104,
      "loss": 0.3046,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.08598867803812027,
      "learning_rate": 0.00019298050139275767,
      "loss": 0.2944,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.12070752680301666,
      "learning_rate": 0.0001928690807799443,
      "loss": 0.3952,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.10526693612337112,
      "learning_rate": 0.00019275766016713092,
      "loss": 0.379,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.0892651155591011,
      "learning_rate": 0.00019264623955431756,
      "loss": 0.3193,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07309211045503616,
      "learning_rate": 0.0001925348189415042,
      "loss": 0.3395,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.06044447422027588,
      "learning_rate": 0.0001924233983286908,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.16050706803798676,
      "learning_rate": 0.00019231197771587747,
      "loss": 0.3868,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.07688120752573013,
      "learning_rate": 0.00019220055710306408,
      "loss": 0.325,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17312116920948029,
      "learning_rate": 0.0001920891364902507,
      "loss": 0.3973,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.09058317542076111,
      "learning_rate": 0.00019197771587743735,
      "loss": 0.3466,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.08493395149707794,
      "learning_rate": 0.00019186629526462396,
      "loss": 0.3251,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.11874042451381683,
      "learning_rate": 0.0001917548746518106,
      "loss": 0.3859,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08727477490901947,
      "learning_rate": 0.0001916434540389972,
      "loss": 0.3329,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.08828488737344742,
      "learning_rate": 0.00019153203342618384,
      "loss": 0.3498,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.0929422378540039,
      "learning_rate": 0.0001914206128133705,
      "loss": 0.3477,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.06502547860145569,
      "learning_rate": 0.0001913091922005571,
      "loss": 0.2619,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0826447606086731,
      "learning_rate": 0.00019119777158774375,
      "loss": 0.375,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.06894782930612564,
      "learning_rate": 0.00019108635097493039,
      "loss": 0.384,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.08264435827732086,
      "learning_rate": 0.000190974930362117,
      "loss": 0.3475,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.08043172955513,
      "learning_rate": 0.00019086350974930363,
      "loss": 0.2602,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08756858110427856,
      "learning_rate": 0.00019075208913649024,
      "loss": 0.3371,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.07516686618328094,
      "learning_rate": 0.00019064066852367688,
      "loss": 0.3206,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06845497339963913,
      "learning_rate": 0.00019052924791086354,
      "loss": 0.2438,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.16434063017368317,
      "learning_rate": 0.00019041782729805015,
      "loss": 0.3756,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08764452487230301,
      "learning_rate": 0.0001903064066852368,
      "loss": 0.364,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.09765373170375824,
      "learning_rate": 0.0001901949860724234,
      "loss": 0.336,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.12203431129455566,
      "learning_rate": 0.00019008356545961003,
      "loss": 0.3644,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.0851789191365242,
      "learning_rate": 0.00018997214484679667,
      "loss": 0.3209,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.0814284235239029,
      "learning_rate": 0.00018986072423398328,
      "loss": 0.3538,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.10511325299739838,
      "learning_rate": 0.00018974930362116992,
      "loss": 0.3208,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.06572487205266953,
      "learning_rate": 0.00018963788300835655,
      "loss": 0.3373,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.060700900852680206,
      "learning_rate": 0.0001895264623955432,
      "loss": 0.2413,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.09838373959064484,
      "learning_rate": 0.00018941504178272982,
      "loss": 0.4219,
      "step": 100
    },
    {
      "epoch": 0.16666666666666666,
      "eval_loss": 0.3487149178981781,
      "eval_runtime": 374.2756,
      "eval_samples_per_second": 0.267,
      "eval_steps_per_second": 0.267,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.07030830532312393,
      "learning_rate": 0.00018930362116991643,
      "loss": 0.2833,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.0894852876663208,
      "learning_rate": 0.00018919220055710307,
      "loss": 0.3532,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.08684878051280975,
      "learning_rate": 0.0001890807799442897,
      "loss": 0.346,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.06322134286165237,
      "learning_rate": 0.00018896935933147632,
      "loss": 0.2794,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.11456792056560516,
      "learning_rate": 0.00018885793871866295,
      "loss": 0.4254,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.08284913748502731,
      "learning_rate": 0.0001887465181058496,
      "loss": 0.3428,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.08064185827970505,
      "learning_rate": 0.00018863509749303623,
      "loss": 0.4691,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.07856238633394241,
      "learning_rate": 0.00018852367688022286,
      "loss": 0.402,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.11941109597682953,
      "learning_rate": 0.00018841225626740947,
      "loss": 0.3084,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.059156522154808044,
      "learning_rate": 0.0001883008356545961,
      "loss": 0.3936,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06889190524816513,
      "learning_rate": 0.00018818941504178274,
      "loss": 0.3664,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06391744315624237,
      "learning_rate": 0.00018807799442896935,
      "loss": 0.3049,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.07021020352840424,
      "learning_rate": 0.000187966573816156,
      "loss": 0.379,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.13152983784675598,
      "learning_rate": 0.00018785515320334263,
      "loss": 0.4045,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08291923254728317,
      "learning_rate": 0.00018774373259052926,
      "loss": 0.3675,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.12651987373828888,
      "learning_rate": 0.0001876323119777159,
      "loss": 0.3512,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.06338682025671005,
      "learning_rate": 0.0001875208913649025,
      "loss": 0.3052,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.06231988966464996,
      "learning_rate": 0.00018740947075208915,
      "loss": 0.3496,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.092839315533638,
      "learning_rate": 0.00018729805013927578,
      "loss": 0.3263,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05712943151593208,
      "learning_rate": 0.0001871866295264624,
      "loss": 0.2907,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.07494724541902542,
      "learning_rate": 0.00018707520891364903,
      "loss": 0.3746,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06764266639947891,
      "learning_rate": 0.00018696378830083566,
      "loss": 0.34,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05663382261991501,
      "learning_rate": 0.0001868523676880223,
      "loss": 0.2878,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06405160576105118,
      "learning_rate": 0.00018674094707520894,
      "loss": 0.3435,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.07559706270694733,
      "learning_rate": 0.00018662952646239555,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.07337447255849838,
      "learning_rate": 0.00018651810584958218,
      "loss": 0.351,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.07447608560323715,
      "learning_rate": 0.00018640668523676882,
      "loss": 0.3073,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06198855862021446,
      "learning_rate": 0.00018629526462395543,
      "loss": 0.2524,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.07246571034193039,
      "learning_rate": 0.00018618384401114207,
      "loss": 0.3869,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.09226162731647491,
      "learning_rate": 0.0001860724233983287,
      "loss": 0.4094,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09503162652254105,
      "learning_rate": 0.00018596100278551534,
      "loss": 0.4443,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07748464494943619,
      "learning_rate": 0.00018584958217270198,
      "loss": 0.3511,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.05017251893877983,
      "learning_rate": 0.00018573816155988858,
      "loss": 0.295,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05881548672914505,
      "learning_rate": 0.00018562674094707522,
      "loss": 0.3032,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.08307559788227081,
      "learning_rate": 0.00018551532033426183,
      "loss": 0.3153,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05294783040881157,
      "learning_rate": 0.00018540389972144847,
      "loss": 0.299,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.08568192273378372,
      "learning_rate": 0.0001852924791086351,
      "loss": 0.3543,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.056592926383018494,
      "learning_rate": 0.00018518105849582174,
      "loss": 0.375,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.07708677649497986,
      "learning_rate": 0.00018506963788300838,
      "loss": 0.3557,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.08158115297555923,
      "learning_rate": 0.000184958217270195,
      "loss": 0.2725,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.0755927711725235,
      "learning_rate": 0.00018484679665738162,
      "loss": 0.3757,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.05458508059382439,
      "learning_rate": 0.00018473537604456826,
      "loss": 0.2738,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.06590596586465836,
      "learning_rate": 0.00018462395543175487,
      "loss": 0.3337,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.08819787949323654,
      "learning_rate": 0.0001845125348189415,
      "loss": 0.3993,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.07023905217647552,
      "learning_rate": 0.00018440111420612814,
      "loss": 0.3453,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.08446215838193893,
      "learning_rate": 0.00018428969359331478,
      "loss": 0.3801,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.08325417339801788,
      "learning_rate": 0.00018417827298050141,
      "loss": 0.3376,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06384475529193878,
      "learning_rate": 0.00018406685236768802,
      "loss": 0.3087,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.09863974153995514,
      "learning_rate": 0.00018395543175487466,
      "loss": 0.3617,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.07659132778644562,
      "learning_rate": 0.0001838440111420613,
      "loss": 0.3664,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.07176845520734787,
      "learning_rate": 0.0001837325905292479,
      "loss": 0.3111,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06705939024686813,
      "learning_rate": 0.00018362116991643454,
      "loss": 0.2932,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09360035508871078,
      "learning_rate": 0.00018350974930362118,
      "loss": 0.4418,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08361534029245377,
      "learning_rate": 0.00018339832869080782,
      "loss": 0.361,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.07738277316093445,
      "learning_rate": 0.00018328690807799445,
      "loss": 0.3252,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06970240920782089,
      "learning_rate": 0.00018317548746518106,
      "loss": 0.3677,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.07010390609502792,
      "learning_rate": 0.0001830640668523677,
      "loss": 0.296,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.05854480341076851,
      "learning_rate": 0.00018295264623955433,
      "loss": 0.3228,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05452617257833481,
      "learning_rate": 0.00018284122562674094,
      "loss": 0.256,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.08169269561767578,
      "learning_rate": 0.00018272980501392758,
      "loss": 0.3414,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.06454888731241226,
      "learning_rate": 0.00018261838440111422,
      "loss": 0.3587,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.07409106940031052,
      "learning_rate": 0.00018250696378830085,
      "loss": 0.2825,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.058084022253751755,
      "learning_rate": 0.0001823955431754875,
      "loss": 0.2171,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.08975499123334885,
      "learning_rate": 0.0001822841225626741,
      "loss": 0.365,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.07376623153686523,
      "learning_rate": 0.00018217270194986074,
      "loss": 0.3336,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.06804781407117844,
      "learning_rate": 0.00018206128133704737,
      "loss": 0.3163,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.06362677365541458,
      "learning_rate": 0.00018194986072423398,
      "loss": 0.317,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06292019784450531,
      "learning_rate": 0.00018183844011142062,
      "loss": 0.2978,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.0647486001253128,
      "learning_rate": 0.00018172701949860725,
      "loss": 0.27,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.059582650661468506,
      "learning_rate": 0.00018161559888579386,
      "loss": 0.344,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.06418173015117645,
      "learning_rate": 0.00018150417827298053,
      "loss": 0.3143,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05272924154996872,
      "learning_rate": 0.00018139275766016714,
      "loss": 0.282,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.05326687917113304,
      "learning_rate": 0.00018128133704735377,
      "loss": 0.2529,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.08164666593074799,
      "learning_rate": 0.0001811699164345404,
      "loss": 0.3672,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.08568466454744339,
      "learning_rate": 0.00018105849582172702,
      "loss": 0.3995,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.06509491801261902,
      "learning_rate": 0.00018094707520891366,
      "loss": 0.2809,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.06712925434112549,
      "learning_rate": 0.0001808356545961003,
      "loss": 0.2367,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.10027410835027695,
      "learning_rate": 0.0001807242339832869,
      "loss": 0.3808,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.07324101775884628,
      "learning_rate": 0.00018061281337047356,
      "loss": 0.3234,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10830274969339371,
      "learning_rate": 0.00018050139275766017,
      "loss": 0.4434,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.08461984992027283,
      "learning_rate": 0.0001803899721448468,
      "loss": 0.3819,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06849800795316696,
      "learning_rate": 0.00018027855153203345,
      "loss": 0.3983,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.07148388773202896,
      "learning_rate": 0.00018016713091922006,
      "loss": 0.2975,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.06068180501461029,
      "learning_rate": 0.0001800557103064067,
      "loss": 0.3016,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.051959868520498276,
      "learning_rate": 0.0001799442896935933,
      "loss": 0.2419,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.07422163337469101,
      "learning_rate": 0.00017983286908077994,
      "loss": 0.2948,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.07162340730428696,
      "learning_rate": 0.0001797214484679666,
      "loss": 0.3059,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.06812291592359543,
      "learning_rate": 0.0001796100278551532,
      "loss": 0.3171,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.07255540788173676,
      "learning_rate": 0.00017949860724233985,
      "loss": 0.3323,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.0663914754986763,
      "learning_rate": 0.00017938718662952648,
      "loss": 0.2966,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.03991664573550224,
      "learning_rate": 0.0001792757660167131,
      "loss": 0.251,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05767596513032913,
      "learning_rate": 0.00017916434540389973,
      "loss": 0.3607,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.05921991914510727,
      "learning_rate": 0.00017905292479108634,
      "loss": 0.3541,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.05089111626148224,
      "learning_rate": 0.00017894150417827298,
      "loss": 0.2928,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.05411629378795624,
      "learning_rate": 0.00017883008356545964,
      "loss": 0.3113,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.08924172818660736,
      "learning_rate": 0.00017871866295264625,
      "loss": 0.3164,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.06599845737218857,
      "learning_rate": 0.00017860724233983289,
      "loss": 0.3165,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.06638164818286896,
      "learning_rate": 0.0001784958217270195,
      "loss": 0.39,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06859147548675537,
      "learning_rate": 0.00017838440111420613,
      "loss": 0.3435,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05851481482386589,
      "learning_rate": 0.00017827298050139277,
      "loss": 0.2616,
      "step": 200
    },
    {
      "epoch": 0.3333333333333333,
      "eval_loss": 0.3388613164424896,
      "eval_runtime": 373.5218,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.06528528034687042,
      "learning_rate": 0.00017816155988857938,
      "loss": 0.3377,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.07327250391244888,
      "learning_rate": 0.00017805013927576601,
      "loss": 0.3787,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.0866728201508522,
      "learning_rate": 0.00017793871866295265,
      "loss": 0.3318,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.08070702105760574,
      "learning_rate": 0.0001778272980501393,
      "loss": 0.272,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.07358337938785553,
      "learning_rate": 0.00017771587743732592,
      "loss": 0.2816,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.11622414737939835,
      "learning_rate": 0.00017760445682451253,
      "loss": 0.3935,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.07202191650867462,
      "learning_rate": 0.00017749303621169917,
      "loss": 0.3054,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.06922760605812073,
      "learning_rate": 0.0001773816155988858,
      "loss": 0.2836,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.040258318185806274,
      "learning_rate": 0.00017727019498607242,
      "loss": 0.251,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06730931252241135,
      "learning_rate": 0.00017715877437325905,
      "loss": 0.3381,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.08930881321430206,
      "learning_rate": 0.0001770473537604457,
      "loss": 0.3098,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.08481532335281372,
      "learning_rate": 0.00017693593314763232,
      "loss": 0.3761,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.059320684522390366,
      "learning_rate": 0.00017682451253481896,
      "loss": 0.2541,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06912222504615784,
      "learning_rate": 0.00017671309192200557,
      "loss": 0.2664,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.08226578682661057,
      "learning_rate": 0.0001766016713091922,
      "loss": 0.3138,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.04609953239560127,
      "learning_rate": 0.00017649025069637884,
      "loss": 0.2542,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.06391782313585281,
      "learning_rate": 0.00017637883008356545,
      "loss": 0.3264,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.05829302966594696,
      "learning_rate": 0.0001762674094707521,
      "loss": 0.2933,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06562326848506927,
      "learning_rate": 0.00017615598885793873,
      "loss": 0.2715,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.06454933434724808,
      "learning_rate": 0.00017604456824512536,
      "loss": 0.3263,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.05812375620007515,
      "learning_rate": 0.000175933147632312,
      "loss": 0.3219,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04714621976017952,
      "learning_rate": 0.0001758217270194986,
      "loss": 0.3001,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.058413684368133545,
      "learning_rate": 0.00017571030640668524,
      "loss": 0.3342,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.05853581801056862,
      "learning_rate": 0.00017559888579387188,
      "loss": 0.3559,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.06629899889230728,
      "learning_rate": 0.0001754874651810585,
      "loss": 0.3503,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.060716353356838226,
      "learning_rate": 0.00017537604456824513,
      "loss": 0.2432,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.0828184112906456,
      "learning_rate": 0.00017526462395543176,
      "loss": 0.3287,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09290502220392227,
      "learning_rate": 0.0001751532033426184,
      "loss": 0.4283,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.0796162560582161,
      "learning_rate": 0.00017504178272980504,
      "loss": 0.3038,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.059088610112667084,
      "learning_rate": 0.00017493036211699165,
      "loss": 0.2984,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06552764028310776,
      "learning_rate": 0.00017481894150417828,
      "loss": 0.28,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.07208864390850067,
      "learning_rate": 0.00017470752089136492,
      "loss": 0.3022,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.07033393532037735,
      "learning_rate": 0.00017459610027855153,
      "loss": 0.345,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.08407337963581085,
      "learning_rate": 0.00017448467966573816,
      "loss": 0.4327,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.037531256675720215,
      "learning_rate": 0.0001743732590529248,
      "loss": 0.2626,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.06134509667754173,
      "learning_rate": 0.00017426183844011144,
      "loss": 0.3523,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.07239819318056107,
      "learning_rate": 0.00017415041782729807,
      "loss": 0.3462,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.049917254596948624,
      "learning_rate": 0.00017403899721448468,
      "loss": 0.3035,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.06036359816789627,
      "learning_rate": 0.00017392757660167132,
      "loss": 0.2773,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07293415814638138,
      "learning_rate": 0.00017381615598885793,
      "loss": 0.2826,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.11930102854967117,
      "learning_rate": 0.00017370473537604457,
      "loss": 0.4427,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06569141149520874,
      "learning_rate": 0.0001735933147632312,
      "loss": 0.3097,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.06904499232769012,
      "learning_rate": 0.00017348189415041784,
      "loss": 0.2963,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.07275153696537018,
      "learning_rate": 0.00017337047353760448,
      "loss": 0.3557,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.0644286647439003,
      "learning_rate": 0.0001732590529247911,
      "loss": 0.2972,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.09861116111278534,
      "learning_rate": 0.00017314763231197772,
      "loss": 0.426,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.06782294809818268,
      "learning_rate": 0.00017303621169916436,
      "loss": 0.4074,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.055203475058078766,
      "learning_rate": 0.00017292479108635097,
      "loss": 0.3725,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.1059175655245781,
      "learning_rate": 0.0001728133704735376,
      "loss": 0.4315,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.08250673860311508,
      "learning_rate": 0.00017270194986072424,
      "loss": 0.3823,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.06672301888465881,
      "learning_rate": 0.00017259052924791088,
      "loss": 0.272,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.07668539136648178,
      "learning_rate": 0.0001724791086350975,
      "loss": 0.3585,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.09261240810155869,
      "learning_rate": 0.00017236768802228412,
      "loss": 0.3171,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.09300730377435684,
      "learning_rate": 0.00017225626740947076,
      "loss": 0.3526,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.0796714648604393,
      "learning_rate": 0.0001721448467966574,
      "loss": 0.3038,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.06646404415369034,
      "learning_rate": 0.000172033426183844,
      "loss": 0.2964,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.06485851854085922,
      "learning_rate": 0.00017192200557103064,
      "loss": 0.3135,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.06937497109174728,
      "learning_rate": 0.00017181058495821728,
      "loss": 0.3272,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07694260776042938,
      "learning_rate": 0.00017169916434540391,
      "loss": 0.3705,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.04648123309016228,
      "learning_rate": 0.00017158774373259055,
      "loss": 0.2886,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.056954897940158844,
      "learning_rate": 0.00017147632311977716,
      "loss": 0.3066,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04178736358880997,
      "learning_rate": 0.0001713649025069638,
      "loss": 0.2673,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.08387766778469086,
      "learning_rate": 0.00017125348189415043,
      "loss": 0.3245,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.06390725821256638,
      "learning_rate": 0.00017114206128133704,
      "loss": 0.3462,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07663051038980484,
      "learning_rate": 0.00017103064066852368,
      "loss": 0.371,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.06242416799068451,
      "learning_rate": 0.00017091922005571032,
      "loss": 0.3902,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.04411499202251434,
      "learning_rate": 0.00017080779944289695,
      "loss": 0.3499,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.048253074288368225,
      "learning_rate": 0.0001706963788300836,
      "loss": 0.3073,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.05330132693052292,
      "learning_rate": 0.0001705849582172702,
      "loss": 0.2782,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.09645810723304749,
      "learning_rate": 0.00017047353760445683,
      "loss": 0.3513,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.07553944736719131,
      "learning_rate": 0.00017036211699164347,
      "loss": 0.3487,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10264316946268082,
      "learning_rate": 0.00017025069637883008,
      "loss": 0.4436,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.05646580085158348,
      "learning_rate": 0.00017013927576601672,
      "loss": 0.3528,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.07130212336778641,
      "learning_rate": 0.00017002785515320335,
      "loss": 0.3375,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.1121247336268425,
      "learning_rate": 0.00016991643454039,
      "loss": 0.3786,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05661875754594803,
      "learning_rate": 0.00016980501392757663,
      "loss": 0.2849,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.06488969922065735,
      "learning_rate": 0.00016969359331476324,
      "loss": 0.3286,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.04874249920248985,
      "learning_rate": 0.00016958217270194987,
      "loss": 0.3338,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.049081817269325256,
      "learning_rate": 0.0001694707520891365,
      "loss": 0.3112,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06580985337495804,
      "learning_rate": 0.00016935933147632312,
      "loss": 0.3001,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.055983323603868484,
      "learning_rate": 0.00016924791086350975,
      "loss": 0.312,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05061831325292587,
      "learning_rate": 0.0001691364902506964,
      "loss": 0.315,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.07002280652523041,
      "learning_rate": 0.00016902506963788303,
      "loss": 0.3657,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05577494204044342,
      "learning_rate": 0.00016891364902506966,
      "loss": 0.3993,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.0922563299536705,
      "learning_rate": 0.00016880222841225627,
      "loss": 0.3746,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.10654503852128983,
      "learning_rate": 0.0001686908077994429,
      "loss": 0.3481,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.0896528884768486,
      "learning_rate": 0.00016857938718662955,
      "loss": 0.3915,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.11687684059143066,
      "learning_rate": 0.00016846796657381616,
      "loss": 0.4647,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.05887802690267563,
      "learning_rate": 0.0001683565459610028,
      "loss": 0.3566,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.09831524640321732,
      "learning_rate": 0.0001682451253481894,
      "loss": 0.4483,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.0771913006901741,
      "learning_rate": 0.00016813370473537606,
      "loss": 0.3773,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.05990441143512726,
      "learning_rate": 0.0001680222841225627,
      "loss": 0.3402,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07814719527959824,
      "learning_rate": 0.0001679108635097493,
      "loss": 0.4058,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05076230689883232,
      "learning_rate": 0.00016779944289693595,
      "loss": 0.3336,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.0958528146147728,
      "learning_rate": 0.00016768802228412256,
      "loss": 0.4436,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.060141295194625854,
      "learning_rate": 0.0001675766016713092,
      "loss": 0.4008,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.05116834118962288,
      "learning_rate": 0.00016746518105849583,
      "loss": 0.262,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.04933657869696617,
      "learning_rate": 0.00016735376044568244,
      "loss": 0.2721,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.07373249530792236,
      "learning_rate": 0.0001672423398328691,
      "loss": 0.355,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.044377684593200684,
      "learning_rate": 0.00016713091922005574,
      "loss": 0.2971,
      "step": 300
    },
    {
      "epoch": 0.5,
      "eval_loss": 0.33406704664230347,
      "eval_runtime": 373.8116,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.09329823404550552,
      "learning_rate": 0.00016701949860724235,
      "loss": 0.3936,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.05307592451572418,
      "learning_rate": 0.00016690807799442898,
      "loss": 0.2952,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.05269429087638855,
      "learning_rate": 0.0001667966573816156,
      "loss": 0.2697,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.07882646471261978,
      "learning_rate": 0.00016668523676880223,
      "loss": 0.3733,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.07247893512248993,
      "learning_rate": 0.00016657381615598887,
      "loss": 0.4261,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05371604114770889,
      "learning_rate": 0.00016646239554317548,
      "loss": 0.3111,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.05024495720863342,
      "learning_rate": 0.00016635097493036214,
      "loss": 0.2737,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06022166833281517,
      "learning_rate": 0.00016623955431754875,
      "loss": 0.2782,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.05292487516999245,
      "learning_rate": 0.00016612813370473539,
      "loss": 0.3065,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.056927524507045746,
      "learning_rate": 0.00016601671309192202,
      "loss": 0.3671,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.0786108672618866,
      "learning_rate": 0.00016590529247910863,
      "loss": 0.3963,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09610572457313538,
      "learning_rate": 0.00016579387186629527,
      "loss": 0.3851,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.05500410869717598,
      "learning_rate": 0.0001656824512534819,
      "loss": 0.3583,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.08582132309675217,
      "learning_rate": 0.00016557103064066851,
      "loss": 0.3898,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.061810631304979324,
      "learning_rate": 0.00016545961002785518,
      "loss": 0.2483,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07196531444787979,
      "learning_rate": 0.0001653481894150418,
      "loss": 0.4125,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.06049725040793419,
      "learning_rate": 0.00016523676880222842,
      "loss": 0.3356,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06734035164117813,
      "learning_rate": 0.00016512534818941506,
      "loss": 0.3966,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.05909775570034981,
      "learning_rate": 0.00016501392757660167,
      "loss": 0.2551,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09914226830005646,
      "learning_rate": 0.0001649025069637883,
      "loss": 0.3654,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.06723151355981827,
      "learning_rate": 0.00016479108635097494,
      "loss": 0.2901,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06519628316164017,
      "learning_rate": 0.00016467966573816155,
      "loss": 0.3625,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04443543031811714,
      "learning_rate": 0.00016456824512534822,
      "loss": 0.2689,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07195848226547241,
      "learning_rate": 0.00016445682451253482,
      "loss": 0.3749,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.05139874666929245,
      "learning_rate": 0.00016434540389972146,
      "loss": 0.3168,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.06149451434612274,
      "learning_rate": 0.0001642339832869081,
      "loss": 0.3361,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.059856116771698,
      "learning_rate": 0.0001641225626740947,
      "loss": 0.2833,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.060059476643800735,
      "learning_rate": 0.00016401114206128134,
      "loss": 0.3624,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.06225313991308212,
      "learning_rate": 0.00016389972144846798,
      "loss": 0.3619,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.0349864698946476,
      "learning_rate": 0.0001637883008356546,
      "loss": 0.209,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.05916865915060043,
      "learning_rate": 0.00016367688022284125,
      "loss": 0.336,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.043841492384672165,
      "learning_rate": 0.00016356545961002786,
      "loss": 0.3148,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.05800284445285797,
      "learning_rate": 0.0001634540389972145,
      "loss": 0.3651,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08612257242202759,
      "learning_rate": 0.00016334261838440114,
      "loss": 0.3809,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.05423552915453911,
      "learning_rate": 0.00016323119777158774,
      "loss": 0.3328,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.043566737323999405,
      "learning_rate": 0.00016311977715877438,
      "loss": 0.2274,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.0462743416428566,
      "learning_rate": 0.00016300835654596102,
      "loss": 0.312,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.06379109621047974,
      "learning_rate": 0.00016289693593314763,
      "loss": 0.3236,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.054661449044942856,
      "learning_rate": 0.00016278551532033426,
      "loss": 0.2697,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.044119060039520264,
      "learning_rate": 0.0001626740947075209,
      "loss": 0.3071,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.06531501561403275,
      "learning_rate": 0.00016256267409470754,
      "loss": 0.3056,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07126521319150925,
      "learning_rate": 0.00016245125348189417,
      "loss": 0.3725,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.062475211918354034,
      "learning_rate": 0.00016233983286908078,
      "loss": 0.3481,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.051287248730659485,
      "learning_rate": 0.00016222841225626742,
      "loss": 0.3282,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.0753796398639679,
      "learning_rate": 0.00016211699164345403,
      "loss": 0.4105,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.06075955927371979,
      "learning_rate": 0.00016200557103064066,
      "loss": 0.2847,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.049342215061187744,
      "learning_rate": 0.0001618941504178273,
      "loss": 0.319,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11758521944284439,
      "learning_rate": 0.00016178272980501394,
      "loss": 0.3552,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06062491238117218,
      "learning_rate": 0.00016167130919220057,
      "loss": 0.3879,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.053289785981178284,
      "learning_rate": 0.0001615598885793872,
      "loss": 0.3227,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.046300258487463,
      "learning_rate": 0.00016144846796657382,
      "loss": 0.278,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.042093485593795776,
      "learning_rate": 0.00016133704735376046,
      "loss": 0.2906,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.0622057169675827,
      "learning_rate": 0.00016122562674094707,
      "loss": 0.3826,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.046517372131347656,
      "learning_rate": 0.0001611142061281337,
      "loss": 0.2846,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.07477085292339325,
      "learning_rate": 0.00016100278551532034,
      "loss": 0.39,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.05725261569023132,
      "learning_rate": 0.00016089136490250698,
      "loss": 0.3105,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.047877974808216095,
      "learning_rate": 0.0001607799442896936,
      "loss": 0.2642,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.06068779155611992,
      "learning_rate": 0.00016066852367688022,
      "loss": 0.3415,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.06766151636838913,
      "learning_rate": 0.00016055710306406686,
      "loss": 0.3771,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04824470356106758,
      "learning_rate": 0.0001604456824512535,
      "loss": 0.3413,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.05535799637436867,
      "learning_rate": 0.0001603342618384401,
      "loss": 0.3053,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.07063216716051102,
      "learning_rate": 0.00016022284122562674,
      "loss": 0.3687,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03917135298252106,
      "learning_rate": 0.00016011142061281338,
      "loss": 0.2866,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.0627112165093422,
      "learning_rate": 0.00016,
      "loss": 0.2673,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.09194136410951614,
      "learning_rate": 0.00015988857938718665,
      "loss": 0.4174,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.07770700752735138,
      "learning_rate": 0.00015977715877437326,
      "loss": 0.3098,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.0638580247759819,
      "learning_rate": 0.0001596657381615599,
      "loss": 0.3463,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.06891981512308121,
      "learning_rate": 0.00015955431754874653,
      "loss": 0.3873,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.0637492835521698,
      "learning_rate": 0.00015944289693593314,
      "loss": 0.3125,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06685901433229446,
      "learning_rate": 0.00015933147632311978,
      "loss": 0.291,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.06276644021272659,
      "learning_rate": 0.00015922005571030641,
      "loss": 0.3353,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06120016798377037,
      "learning_rate": 0.00015910863509749305,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.06211041286587715,
      "learning_rate": 0.0001589972144846797,
      "loss": 0.3885,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.04900497943162918,
      "learning_rate": 0.0001588857938718663,
      "loss": 0.3883,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.07299406081438065,
      "learning_rate": 0.00015877437325905293,
      "loss": 0.2706,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.09120436757802963,
      "learning_rate": 0.00015866295264623957,
      "loss": 0.3739,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.07414333522319794,
      "learning_rate": 0.00015855153203342618,
      "loss": 0.3925,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.04861472547054291,
      "learning_rate": 0.00015844011142061282,
      "loss": 0.2753,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.10256479680538177,
      "learning_rate": 0.00015832869080779945,
      "loss": 0.3699,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.05784834176301956,
      "learning_rate": 0.0001582172701949861,
      "loss": 0.3029,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.05931473523378372,
      "learning_rate": 0.00015810584958217272,
      "loss": 0.2805,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.05126677453517914,
      "learning_rate": 0.00015799442896935933,
      "loss": 0.2928,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.09290463477373123,
      "learning_rate": 0.00015788300835654597,
      "loss": 0.3602,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10042349249124527,
      "learning_rate": 0.0001577715877437326,
      "loss": 0.4415,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.08114712685346603,
      "learning_rate": 0.00015766016713091922,
      "loss": 0.391,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.07912176847457886,
      "learning_rate": 0.00015754874651810585,
      "loss": 0.3671,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.06592220067977905,
      "learning_rate": 0.0001574373259052925,
      "loss": 0.3083,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05964716896414757,
      "learning_rate": 0.00015732590529247913,
      "loss": 0.2582,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.06917083263397217,
      "learning_rate": 0.00015721448467966576,
      "loss": 0.3186,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.05812530592083931,
      "learning_rate": 0.00015710306406685237,
      "loss": 0.3,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.06314097344875336,
      "learning_rate": 0.000156991643454039,
      "loss": 0.2926,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.06183494254946709,
      "learning_rate": 0.00015688022284122564,
      "loss": 0.2942,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.05879906937479973,
      "learning_rate": 0.00015676880222841225,
      "loss": 0.2761,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.07158152759075165,
      "learning_rate": 0.0001566573816155989,
      "loss": 0.4515,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.05752398073673248,
      "learning_rate": 0.0001565459610027855,
      "loss": 0.3788,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06474988907575607,
      "learning_rate": 0.00015643454038997216,
      "loss": 0.3496,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.07224076986312866,
      "learning_rate": 0.0001563231197771588,
      "loss": 0.3171,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.04606259614229202,
      "learning_rate": 0.0001562116991643454,
      "loss": 0.2679,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.06618712842464447,
      "learning_rate": 0.00015610027855153205,
      "loss": 0.3408,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0727199912071228,
      "learning_rate": 0.00015598885793871866,
      "loss": 0.3694,
      "step": 400
    },
    {
      "epoch": 0.6666666666666666,
      "eval_loss": 0.33038070797920227,
      "eval_runtime": 373.7813,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.049671657383441925,
      "learning_rate": 0.0001558774373259053,
      "loss": 0.3638,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.0751660093665123,
      "learning_rate": 0.00015576601671309193,
      "loss": 0.3246,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.051771342754364014,
      "learning_rate": 0.00015565459610027854,
      "loss": 0.2599,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.06960935890674591,
      "learning_rate": 0.0001555431754874652,
      "loss": 0.2855,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.0638466402888298,
      "learning_rate": 0.00015543175487465184,
      "loss": 0.3257,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.06104127690196037,
      "learning_rate": 0.00015532033426183845,
      "loss": 0.2893,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.09004420042037964,
      "learning_rate": 0.00015520891364902508,
      "loss": 0.4522,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.07697346806526184,
      "learning_rate": 0.0001550974930362117,
      "loss": 0.2782,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.07419420033693314,
      "learning_rate": 0.00015498607242339833,
      "loss": 0.3739,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.055936019867658615,
      "learning_rate": 0.00015487465181058497,
      "loss": 0.3843,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.047740768641233444,
      "learning_rate": 0.00015476323119777158,
      "loss": 0.2755,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.04831160977482796,
      "learning_rate": 0.00015465181058495824,
      "loss": 0.2952,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.06364057958126068,
      "learning_rate": 0.00015454038997214485,
      "loss": 0.3322,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.05456951633095741,
      "learning_rate": 0.00015442896935933148,
      "loss": 0.3343,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.03701818734407425,
      "learning_rate": 0.00015431754874651812,
      "loss": 0.1905,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.06792543083429337,
      "learning_rate": 0.00015420612813370473,
      "loss": 0.3592,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.08207648247480392,
      "learning_rate": 0.00015409470752089137,
      "loss": 0.3399,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.06876149773597717,
      "learning_rate": 0.000153983286908078,
      "loss": 0.374,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.06378063559532166,
      "learning_rate": 0.0001538718662952646,
      "loss": 0.3097,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04361330345273018,
      "learning_rate": 0.00015376044568245128,
      "loss": 0.3016,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.05422496423125267,
      "learning_rate": 0.00015364902506963789,
      "loss": 0.4201,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.06403195112943649,
      "learning_rate": 0.00015353760445682452,
      "loss": 0.3504,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.05734192952513695,
      "learning_rate": 0.00015342618384401116,
      "loss": 0.3443,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06866508722305298,
      "learning_rate": 0.00015331476323119777,
      "loss": 0.3852,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.04271905496716499,
      "learning_rate": 0.0001532033426183844,
      "loss": 0.3524,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.056068915873765945,
      "learning_rate": 0.00015309192200557104,
      "loss": 0.2999,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.043030448257923126,
      "learning_rate": 0.00015298050139275765,
      "loss": 0.2591,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.0719999372959137,
      "learning_rate": 0.00015286908077994431,
      "loss": 0.325,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.05486292019486427,
      "learning_rate": 0.00015275766016713092,
      "loss": 0.3018,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.05790204182267189,
      "learning_rate": 0.00015264623955431756,
      "loss": 0.3348,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.0518096461892128,
      "learning_rate": 0.0001525348189415042,
      "loss": 0.3198,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06278590857982635,
      "learning_rate": 0.0001524233983286908,
      "loss": 0.3884,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.04729745164513588,
      "learning_rate": 0.00015231197771587744,
      "loss": 0.2791,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.06511681526899338,
      "learning_rate": 0.00015220055710306408,
      "loss": 0.3217,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05852040275931358,
      "learning_rate": 0.0001520891364902507,
      "loss": 0.3039,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.05285286903381348,
      "learning_rate": 0.00015197771587743735,
      "loss": 0.3112,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.0563475526869297,
      "learning_rate": 0.00015186629526462396,
      "loss": 0.3308,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.04962017387151718,
      "learning_rate": 0.0001517548746518106,
      "loss": 0.3074,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.06965400278568268,
      "learning_rate": 0.00015164345403899723,
      "loss": 0.3453,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03574811667203903,
      "learning_rate": 0.00015153203342618384,
      "loss": 0.2622,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09636316448450089,
      "learning_rate": 0.00015142061281337048,
      "loss": 0.3886,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.059329334646463394,
      "learning_rate": 0.00015130919220055712,
      "loss": 0.3156,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06609758734703064,
      "learning_rate": 0.00015119777158774373,
      "loss": 0.2453,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04046764224767685,
      "learning_rate": 0.0001510863509749304,
      "loss": 0.2396,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.10643579065799713,
      "learning_rate": 0.000150974930362117,
      "loss": 0.4159,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06950733810663223,
      "learning_rate": 0.00015086350974930364,
      "loss": 0.3398,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.0766250416636467,
      "learning_rate": 0.00015075208913649027,
      "loss": 0.3328,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05973466485738754,
      "learning_rate": 0.00015064066852367688,
      "loss": 0.3382,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.04731937497854233,
      "learning_rate": 0.00015052924791086352,
      "loss": 0.2357,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.0641368106007576,
      "learning_rate": 0.00015041782729805013,
      "loss": 0.3222,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.08205905556678772,
      "learning_rate": 0.00015030640668523676,
      "loss": 0.3659,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06015029922127724,
      "learning_rate": 0.00015019498607242343,
      "loss": 0.28,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.049014464020729065,
      "learning_rate": 0.00015008356545961004,
      "loss": 0.2436,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06603681296110153,
      "learning_rate": 0.00014997214484679667,
      "loss": 0.3804,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07793335616588593,
      "learning_rate": 0.0001498607242339833,
      "loss": 0.294,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04767234995961189,
      "learning_rate": 0.00014974930362116992,
      "loss": 0.2779,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.056623298674821854,
      "learning_rate": 0.00014963788300835656,
      "loss": 0.3908,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.05769085884094238,
      "learning_rate": 0.00014952646239554316,
      "loss": 0.3466,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.04517456516623497,
      "learning_rate": 0.0001494150417827298,
      "loss": 0.2858,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.0452684722840786,
      "learning_rate": 0.00014930362116991646,
      "loss": 0.2877,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.053076423704624176,
      "learning_rate": 0.00014919220055710307,
      "loss": 0.3587,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.05435944348573685,
      "learning_rate": 0.0001490807799442897,
      "loss": 0.2822,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.05604596063494682,
      "learning_rate": 0.00014896935933147632,
      "loss": 0.3081,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.07164648920297623,
      "learning_rate": 0.00014885793871866296,
      "loss": 0.4099,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06206504628062248,
      "learning_rate": 0.0001487465181058496,
      "loss": 0.3222,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.04434536397457123,
      "learning_rate": 0.0001486350974930362,
      "loss": 0.2555,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.042083725333213806,
      "learning_rate": 0.00014852367688022284,
      "loss": 0.2848,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04734901338815689,
      "learning_rate": 0.00014841225626740948,
      "loss": 0.3472,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.07734812796115875,
      "learning_rate": 0.0001483008356545961,
      "loss": 0.3498,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.04673198238015175,
      "learning_rate": 0.00014818941504178275,
      "loss": 0.3031,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.04137273505330086,
      "learning_rate": 0.00014807799442896936,
      "loss": 0.2605,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04433909058570862,
      "learning_rate": 0.000147966573816156,
      "loss": 0.298,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.051636550575494766,
      "learning_rate": 0.00014785515320334263,
      "loss": 0.316,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.06208198517560959,
      "learning_rate": 0.00014774373259052924,
      "loss": 0.3718,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.05418902263045311,
      "learning_rate": 0.00014763231197771588,
      "loss": 0.3801,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05110190063714981,
      "learning_rate": 0.0001475208913649025,
      "loss": 0.3136,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.09889218956232071,
      "learning_rate": 0.00014740947075208915,
      "loss": 0.3812,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.05322788283228874,
      "learning_rate": 0.00014729805013927579,
      "loss": 0.2818,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05451244115829468,
      "learning_rate": 0.0001471866295264624,
      "loss": 0.3402,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06887216866016388,
      "learning_rate": 0.00014707520891364903,
      "loss": 0.3994,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.0672229751944542,
      "learning_rate": 0.00014696378830083567,
      "loss": 0.4048,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.03675975278019905,
      "learning_rate": 0.00014685236768802228,
      "loss": 0.2377,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04699351638555527,
      "learning_rate": 0.00014674094707520891,
      "loss": 0.2695,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.06801038235425949,
      "learning_rate": 0.00014662952646239555,
      "loss": 0.3598,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.06345156580209732,
      "learning_rate": 0.0001465181058495822,
      "loss": 0.3508,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05789051949977875,
      "learning_rate": 0.00014640668523676882,
      "loss": 0.32,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.04386863857507706,
      "learning_rate": 0.00014629526462395543,
      "loss": 0.2561,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.05822966247797012,
      "learning_rate": 0.00014618384401114207,
      "loss": 0.2997,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06740472465753555,
      "learning_rate": 0.0001460724233983287,
      "loss": 0.3412,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.04136982187628746,
      "learning_rate": 0.00014596100278551532,
      "loss": 0.2713,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.04848026484251022,
      "learning_rate": 0.00014584958217270195,
      "loss": 0.2985,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.0479574128985405,
      "learning_rate": 0.0001457381615598886,
      "loss": 0.3135,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.04878498986363411,
      "learning_rate": 0.00014562674094707522,
      "loss": 0.2961,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.06789479404687881,
      "learning_rate": 0.00014551532033426186,
      "loss": 0.3353,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.05490916594862938,
      "learning_rate": 0.00014540389972144847,
      "loss": 0.3405,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.0509500578045845,
      "learning_rate": 0.0001452924791086351,
      "loss": 0.2784,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.053391579538583755,
      "learning_rate": 0.00014518105849582174,
      "loss": 0.311,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.06824196875095367,
      "learning_rate": 0.00014506963788300835,
      "loss": 0.3328,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04643777385354042,
      "learning_rate": 0.000144958217270195,
      "loss": 0.2726,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.051668308675289154,
      "learning_rate": 0.0001448467966573816,
      "loss": 0.2921,
      "step": 500
    },
    {
      "epoch": 0.8333333333333334,
      "eval_loss": 0.3281901478767395,
      "eval_runtime": 373.7636,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6425543847874396e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
