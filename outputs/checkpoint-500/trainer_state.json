{
  "best_metric": 1.5547724962234497,
  "best_model_checkpoint": "outputs/checkpoint-500",
  "epoch": 1.0,
  "eval_steps": 100,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 0.07153336703777313,
      "learning_rate": 0.0004,
      "loss": 0.3265,
      "step": 1
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.07364838570356369,
      "learning_rate": 0.0008,
      "loss": 0.3142,
      "step": 2
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.12498562783002853,
      "learning_rate": 0.0012,
      "loss": 0.3848,
      "step": 3
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.18664319813251495,
      "learning_rate": 0.0016,
      "loss": 0.3473,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.0481288433074951,
      "learning_rate": 0.002,
      "loss": 0.5477,
      "step": 5
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.36579111218452454,
      "learning_rate": 0.0019999977920603196,
      "loss": 0.4836,
      "step": 6
    },
    {
      "epoch": 0.014,
      "grad_norm": 2.3086600303649902,
      "learning_rate": 0.0019999911682510277,
      "loss": 0.5702,
      "step": 7
    },
    {
      "epoch": 0.016,
      "grad_norm": 22.525005340576172,
      "learning_rate": 0.001999980128601375,
      "loss": 1.0408,
      "step": 8
    },
    {
      "epoch": 0.018,
      "grad_norm": 21.848602294921875,
      "learning_rate": 0.00199996467316011,
      "loss": 3.2526,
      "step": 9
    },
    {
      "epoch": 0.02,
      "grad_norm": 14.757415771484375,
      "learning_rate": 0.001999944801995484,
      "loss": 1.8052,
      "step": 10
    },
    {
      "epoch": 0.022,
      "grad_norm": 17.1826171875,
      "learning_rate": 0.0019999205151952438,
      "loss": 2.3941,
      "step": 11
    },
    {
      "epoch": 0.024,
      "grad_norm": 6.885984420776367,
      "learning_rate": 0.001999891812866638,
      "loss": 0.9802,
      "step": 12
    },
    {
      "epoch": 0.026,
      "grad_norm": 10.152630805969238,
      "learning_rate": 0.0019998586951364126,
      "loss": 1.7751,
      "step": 13
    },
    {
      "epoch": 0.028,
      "grad_norm": 7.794259548187256,
      "learning_rate": 0.0019998211621508107,
      "loss": 0.9923,
      "step": 14
    },
    {
      "epoch": 0.03,
      "grad_norm": 9.997300148010254,
      "learning_rate": 0.0019997792140755742,
      "loss": 1.235,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 6.218258380889893,
      "learning_rate": 0.0019997328510959413,
      "loss": 0.8716,
      "step": 16
    },
    {
      "epoch": 0.034,
      "grad_norm": 7.197497844696045,
      "learning_rate": 0.001999682073416644,
      "loss": 0.9204,
      "step": 17
    },
    {
      "epoch": 0.036,
      "grad_norm": 40.00090789794922,
      "learning_rate": 0.0019996268812619107,
      "loss": 2.4472,
      "step": 18
    },
    {
      "epoch": 0.038,
      "grad_norm": 5.918007850646973,
      "learning_rate": 0.001999567274875464,
      "loss": 0.8146,
      "step": 19
    },
    {
      "epoch": 0.04,
      "grad_norm": 14.861492156982422,
      "learning_rate": 0.0019995032545205176,
      "loss": 1.3966,
      "step": 20
    },
    {
      "epoch": 0.042,
      "grad_norm": 15.856351852416992,
      "learning_rate": 0.0019994348204797788,
      "loss": 0.982,
      "step": 21
    },
    {
      "epoch": 0.044,
      "grad_norm": 11.071501731872559,
      "learning_rate": 0.001999361973055443,
      "loss": 1.5479,
      "step": 22
    },
    {
      "epoch": 0.046,
      "grad_norm": 108.72428894042969,
      "learning_rate": 0.001999284712569196,
      "loss": 3.0887,
      "step": 23
    },
    {
      "epoch": 0.048,
      "grad_norm": 59.42506408691406,
      "learning_rate": 0.0019992030393622107,
      "loss": 3.3124,
      "step": 24
    },
    {
      "epoch": 0.05,
      "grad_norm": 5.313709735870361,
      "learning_rate": 0.0019991169537951466,
      "loss": 0.9667,
      "step": 25
    },
    {
      "epoch": 0.052,
      "grad_norm": 13.352042198181152,
      "learning_rate": 0.001999026456248147,
      "loss": 1.5735,
      "step": 26
    },
    {
      "epoch": 0.054,
      "grad_norm": 4.222155570983887,
      "learning_rate": 0.001998931547120838,
      "loss": 1.113,
      "step": 27
    },
    {
      "epoch": 0.056,
      "grad_norm": 8.537115097045898,
      "learning_rate": 0.0019988322268323267,
      "loss": 1.0243,
      "step": 28
    },
    {
      "epoch": 0.058,
      "grad_norm": 1.7223957777023315,
      "learning_rate": 0.0019987284958211996,
      "loss": 0.6193,
      "step": 29
    },
    {
      "epoch": 0.06,
      "grad_norm": 18.858154296875,
      "learning_rate": 0.0019986203545455205,
      "loss": 1.6633,
      "step": 30
    },
    {
      "epoch": 0.062,
      "grad_norm": 14.651820182800293,
      "learning_rate": 0.001998507803482828,
      "loss": 2.0824,
      "step": 31
    },
    {
      "epoch": 0.064,
      "grad_norm": 33.13444900512695,
      "learning_rate": 0.0019983908431301343,
      "loss": 2.237,
      "step": 32
    },
    {
      "epoch": 0.066,
      "grad_norm": 49.49964141845703,
      "learning_rate": 0.001998269474003922,
      "loss": 3.2682,
      "step": 33
    },
    {
      "epoch": 0.068,
      "grad_norm": 11.279411315917969,
      "learning_rate": 0.0019981436966401427,
      "loss": 1.8529,
      "step": 34
    },
    {
      "epoch": 0.07,
      "grad_norm": 11.400317192077637,
      "learning_rate": 0.0019980135115942135,
      "loss": 1.2975,
      "step": 35
    },
    {
      "epoch": 0.072,
      "grad_norm": 2.172196865081787,
      "learning_rate": 0.0019978789194410166,
      "loss": 0.667,
      "step": 36
    },
    {
      "epoch": 0.074,
      "grad_norm": 4.283565998077393,
      "learning_rate": 0.0019977399207748944,
      "loss": 0.7487,
      "step": 37
    },
    {
      "epoch": 0.076,
      "grad_norm": 16.038217544555664,
      "learning_rate": 0.0019975965162096483,
      "loss": 1.1877,
      "step": 38
    },
    {
      "epoch": 0.078,
      "grad_norm": 42.251346588134766,
      "learning_rate": 0.0019974487063785353,
      "loss": 1.4782,
      "step": 39
    },
    {
      "epoch": 0.08,
      "grad_norm": 38.56047058105469,
      "learning_rate": 0.0019972964919342663,
      "loss": 5.3519,
      "step": 40
    },
    {
      "epoch": 0.082,
      "grad_norm": 99.82169342041016,
      "learning_rate": 0.0019971398735490016,
      "loss": 13.7152,
      "step": 41
    },
    {
      "epoch": 0.084,
      "grad_norm": 53.84722900390625,
      "learning_rate": 0.001996978851914349,
      "loss": 14.451,
      "step": 42
    },
    {
      "epoch": 0.086,
      "grad_norm": 23.021757125854492,
      "learning_rate": 0.0019968134277413606,
      "loss": 10.9631,
      "step": 43
    },
    {
      "epoch": 0.088,
      "grad_norm": 30.860822677612305,
      "learning_rate": 0.0019966436017605296,
      "loss": 12.2249,
      "step": 44
    },
    {
      "epoch": 0.09,
      "grad_norm": 26.272480010986328,
      "learning_rate": 0.0019964693747217873,
      "loss": 10.3854,
      "step": 45
    },
    {
      "epoch": 0.092,
      "grad_norm": 25.899433135986328,
      "learning_rate": 0.0019962907473944995,
      "loss": 11.9361,
      "step": 46
    },
    {
      "epoch": 0.094,
      "grad_norm": 16.461830139160156,
      "learning_rate": 0.001996107720567462,
      "loss": 10.2164,
      "step": 47
    },
    {
      "epoch": 0.096,
      "grad_norm": 7.193535804748535,
      "learning_rate": 0.0019959202950489,
      "loss": 5.7087,
      "step": 48
    },
    {
      "epoch": 0.098,
      "grad_norm": 33.34125900268555,
      "learning_rate": 0.0019957284716664615,
      "loss": 11.7368,
      "step": 49
    },
    {
      "epoch": 0.1,
      "grad_norm": 24.017000198364258,
      "learning_rate": 0.001995532251267216,
      "loss": 5.1569,
      "step": 50
    },
    {
      "epoch": 0.102,
      "grad_norm": 54.824546813964844,
      "learning_rate": 0.0019953316347176486,
      "loss": 15.9276,
      "step": 51
    },
    {
      "epoch": 0.104,
      "grad_norm": 35.168212890625,
      "learning_rate": 0.001995126622903658,
      "loss": 12.9034,
      "step": 52
    },
    {
      "epoch": 0.106,
      "grad_norm": 22.955968856811523,
      "learning_rate": 0.0019949172167305516,
      "loss": 8.2095,
      "step": 53
    },
    {
      "epoch": 0.108,
      "grad_norm": 11.206243515014648,
      "learning_rate": 0.001994703417123042,
      "loss": 5.0177,
      "step": 54
    },
    {
      "epoch": 0.11,
      "grad_norm": 25.80901527404785,
      "learning_rate": 0.0019944852250252418,
      "loss": 10.7141,
      "step": 55
    },
    {
      "epoch": 0.112,
      "grad_norm": 26.53685760498047,
      "learning_rate": 0.0019942626414006614,
      "loss": 8.0554,
      "step": 56
    },
    {
      "epoch": 0.114,
      "grad_norm": 26.4266300201416,
      "learning_rate": 0.0019940356672322034,
      "loss": 8.1232,
      "step": 57
    },
    {
      "epoch": 0.116,
      "grad_norm": 27.160112380981445,
      "learning_rate": 0.0019938043035221584,
      "loss": 8.1943,
      "step": 58
    },
    {
      "epoch": 0.118,
      "grad_norm": 14.713798522949219,
      "learning_rate": 0.0019935685512922005,
      "loss": 6.0858,
      "step": 59
    },
    {
      "epoch": 0.12,
      "grad_norm": 37.84613800048828,
      "learning_rate": 0.0019933284115833828,
      "loss": 6.1043,
      "step": 60
    },
    {
      "epoch": 0.122,
      "grad_norm": 36.95431900024414,
      "learning_rate": 0.001993083885456134,
      "loss": 5.8995,
      "step": 61
    },
    {
      "epoch": 0.124,
      "grad_norm": 37.088035583496094,
      "learning_rate": 0.001992834973990251,
      "loss": 7.143,
      "step": 62
    },
    {
      "epoch": 0.126,
      "grad_norm": 39.99118423461914,
      "learning_rate": 0.0019925816782848976,
      "loss": 6.7677,
      "step": 63
    },
    {
      "epoch": 0.128,
      "grad_norm": 8.601700782775879,
      "learning_rate": 0.0019923239994585965,
      "loss": 4.354,
      "step": 64
    },
    {
      "epoch": 0.13,
      "grad_norm": 76.58305358886719,
      "learning_rate": 0.0019920619386492268,
      "loss": 8.6727,
      "step": 65
    },
    {
      "epoch": 0.132,
      "grad_norm": 215.24545288085938,
      "learning_rate": 0.0019917954970140174,
      "loss": 19.3494,
      "step": 66
    },
    {
      "epoch": 0.134,
      "grad_norm": 317.1894836425781,
      "learning_rate": 0.001991524675729542,
      "loss": 13.2993,
      "step": 67
    },
    {
      "epoch": 0.136,
      "grad_norm": 63.986480712890625,
      "learning_rate": 0.001991249475991715,
      "loss": 8.1727,
      "step": 68
    },
    {
      "epoch": 0.138,
      "grad_norm": 61.14423370361328,
      "learning_rate": 0.001990969899015785,
      "loss": 18.8724,
      "step": 69
    },
    {
      "epoch": 0.14,
      "grad_norm": 48.705894470214844,
      "learning_rate": 0.0019906859460363307,
      "loss": 18.8833,
      "step": 70
    },
    {
      "epoch": 0.142,
      "grad_norm": 44.769229888916016,
      "learning_rate": 0.001990397618307254,
      "loss": 9.431,
      "step": 71
    },
    {
      "epoch": 0.144,
      "grad_norm": 36.0777702331543,
      "learning_rate": 0.001990104917101775,
      "loss": 9.1747,
      "step": 72
    },
    {
      "epoch": 0.146,
      "grad_norm": 20.258756637573242,
      "learning_rate": 0.0019898078437124273,
      "loss": 9.9993,
      "step": 73
    },
    {
      "epoch": 0.148,
      "grad_norm": 19.348947525024414,
      "learning_rate": 0.001989506399451051,
      "loss": 8.0574,
      "step": 74
    },
    {
      "epoch": 0.15,
      "grad_norm": 12.677506446838379,
      "learning_rate": 0.0019892005856487877,
      "loss": 4.6704,
      "step": 75
    },
    {
      "epoch": 0.152,
      "grad_norm": 7.288967132568359,
      "learning_rate": 0.0019888904036560744,
      "loss": 4.3465,
      "step": 76
    },
    {
      "epoch": 0.154,
      "grad_norm": 25.731111526489258,
      "learning_rate": 0.0019885758548426366,
      "loss": 4.551,
      "step": 77
    },
    {
      "epoch": 0.156,
      "grad_norm": 10.50670051574707,
      "learning_rate": 0.001988256940597485,
      "loss": 4.0567,
      "step": 78
    },
    {
      "epoch": 0.158,
      "grad_norm": 38.44298553466797,
      "learning_rate": 0.0019879336623289056,
      "loss": 5.0879,
      "step": 79
    },
    {
      "epoch": 0.16,
      "grad_norm": 18.668872833251953,
      "learning_rate": 0.0019876060214644568,
      "loss": 5.8952,
      "step": 80
    },
    {
      "epoch": 0.162,
      "grad_norm": 10.860904693603516,
      "learning_rate": 0.0019872740194509606,
      "loss": 4.8303,
      "step": 81
    },
    {
      "epoch": 0.164,
      "grad_norm": 16.271303176879883,
      "learning_rate": 0.0019869376577544983,
      "loss": 4.5477,
      "step": 82
    },
    {
      "epoch": 0.166,
      "grad_norm": 16.538307189941406,
      "learning_rate": 0.001986596937860402,
      "loss": 4.1279,
      "step": 83
    },
    {
      "epoch": 0.168,
      "grad_norm": 10.17186450958252,
      "learning_rate": 0.0019862518612732503,
      "loss": 3.4794,
      "step": 84
    },
    {
      "epoch": 0.17,
      "grad_norm": 126.83295440673828,
      "learning_rate": 0.0019859024295168595,
      "loss": 3.9457,
      "step": 85
    },
    {
      "epoch": 0.172,
      "grad_norm": 12.776949882507324,
      "learning_rate": 0.001985548644134278,
      "loss": 4.4403,
      "step": 86
    },
    {
      "epoch": 0.174,
      "grad_norm": 17.375917434692383,
      "learning_rate": 0.0019851905066877794,
      "loss": 4.2164,
      "step": 87
    },
    {
      "epoch": 0.176,
      "grad_norm": 8.938773155212402,
      "learning_rate": 0.0019848280187588557,
      "loss": 3.6192,
      "step": 88
    },
    {
      "epoch": 0.178,
      "grad_norm": 5.6368088722229,
      "learning_rate": 0.0019844611819482094,
      "loss": 3.4678,
      "step": 89
    },
    {
      "epoch": 0.18,
      "grad_norm": 6.4656782150268555,
      "learning_rate": 0.0019840899978757483,
      "loss": 3.3958,
      "step": 90
    },
    {
      "epoch": 0.182,
      "grad_norm": 3.5705130100250244,
      "learning_rate": 0.0019837144681805756,
      "loss": 3.1315,
      "step": 91
    },
    {
      "epoch": 0.184,
      "grad_norm": 4.337969779968262,
      "learning_rate": 0.0019833345945209856,
      "loss": 3.2196,
      "step": 92
    },
    {
      "epoch": 0.186,
      "grad_norm": 4.525212287902832,
      "learning_rate": 0.001982950378574455,
      "loss": 3.3266,
      "step": 93
    },
    {
      "epoch": 0.188,
      "grad_norm": 10.478811264038086,
      "learning_rate": 0.0019825618220376344,
      "loss": 3.6558,
      "step": 94
    },
    {
      "epoch": 0.19,
      "grad_norm": 2.0926499366760254,
      "learning_rate": 0.0019821689266263424,
      "loss": 2.8955,
      "step": 95
    },
    {
      "epoch": 0.192,
      "grad_norm": 7.8299784660339355,
      "learning_rate": 0.0019817716940755585,
      "loss": 3.3969,
      "step": 96
    },
    {
      "epoch": 0.194,
      "grad_norm": 21.893238067626953,
      "learning_rate": 0.0019813701261394137,
      "loss": 6.187,
      "step": 97
    },
    {
      "epoch": 0.196,
      "grad_norm": 12.930903434753418,
      "learning_rate": 0.001980964224591183,
      "loss": 4.349,
      "step": 98
    },
    {
      "epoch": 0.198,
      "grad_norm": 6.7530388832092285,
      "learning_rate": 0.0019805539912232783,
      "loss": 3.2752,
      "step": 99
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.532876491546631,
      "learning_rate": 0.0019801394278472417,
      "loss": 3.7424,
      "step": 100
    },
    {
      "epoch": 0.2,
      "eval_loss": 3.5269076824188232,
      "eval_runtime": 228.953,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 100
    },
    {
      "epoch": 0.202,
      "grad_norm": 5.784553050994873,
      "learning_rate": 0.0019797205362937346,
      "loss": 3.3643,
      "step": 101
    },
    {
      "epoch": 0.204,
      "grad_norm": 9.127640724182129,
      "learning_rate": 0.0019792973184125317,
      "loss": 3.704,
      "step": 102
    },
    {
      "epoch": 0.206,
      "grad_norm": 11.89439868927002,
      "learning_rate": 0.001978869776072512,
      "loss": 3.7486,
      "step": 103
    },
    {
      "epoch": 0.208,
      "grad_norm": 3.653444528579712,
      "learning_rate": 0.0019784379111616505,
      "loss": 3.1518,
      "step": 104
    },
    {
      "epoch": 0.21,
      "grad_norm": 6.55711555480957,
      "learning_rate": 0.001978001725587011,
      "loss": 3.7818,
      "step": 105
    },
    {
      "epoch": 0.212,
      "grad_norm": 5.933317184448242,
      "learning_rate": 0.001977561221274737,
      "loss": 3.4243,
      "step": 106
    },
    {
      "epoch": 0.214,
      "grad_norm": 6.509677886962891,
      "learning_rate": 0.001977116400170041,
      "loss": 3.2879,
      "step": 107
    },
    {
      "epoch": 0.216,
      "grad_norm": 4.7551188468933105,
      "learning_rate": 0.0019766672642372,
      "loss": 3.221,
      "step": 108
    },
    {
      "epoch": 0.218,
      "grad_norm": 5.226152420043945,
      "learning_rate": 0.0019762138154595446,
      "loss": 3.4831,
      "step": 109
    },
    {
      "epoch": 0.22,
      "grad_norm": 5.412924289703369,
      "learning_rate": 0.0019757560558394493,
      "loss": 3.2454,
      "step": 110
    },
    {
      "epoch": 0.222,
      "grad_norm": 2.984659433364868,
      "learning_rate": 0.0019752939873983254,
      "loss": 3.0154,
      "step": 111
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.7997300624847412,
      "learning_rate": 0.0019748276121766117,
      "loss": 2.984,
      "step": 112
    },
    {
      "epoch": 0.226,
      "grad_norm": 4.594057083129883,
      "learning_rate": 0.001974356932233764,
      "loss": 2.9406,
      "step": 113
    },
    {
      "epoch": 0.228,
      "grad_norm": 3.41648268699646,
      "learning_rate": 0.0019738819496482496,
      "loss": 3.0348,
      "step": 114
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.4951449632644653,
      "learning_rate": 0.0019734026665175334,
      "loss": 2.7256,
      "step": 115
    },
    {
      "epoch": 0.232,
      "grad_norm": 1.9623335599899292,
      "learning_rate": 0.001972919084958072,
      "loss": 2.8238,
      "step": 116
    },
    {
      "epoch": 0.234,
      "grad_norm": 1.9828375577926636,
      "learning_rate": 0.001972431207105303,
      "loss": 2.6557,
      "step": 117
    },
    {
      "epoch": 0.236,
      "grad_norm": 2.3744359016418457,
      "learning_rate": 0.001971939035113636,
      "loss": 2.6103,
      "step": 118
    },
    {
      "epoch": 0.238,
      "grad_norm": 3.2531588077545166,
      "learning_rate": 0.0019714425711564445,
      "loss": 2.7168,
      "step": 119
    },
    {
      "epoch": 0.24,
      "grad_norm": 7.147645950317383,
      "learning_rate": 0.001970941817426052,
      "loss": 2.9605,
      "step": 120
    },
    {
      "epoch": 0.242,
      "grad_norm": 2.484334945678711,
      "learning_rate": 0.001970436776133727,
      "loss": 2.7385,
      "step": 121
    },
    {
      "epoch": 0.244,
      "grad_norm": 1.369204044342041,
      "learning_rate": 0.0019699274495096715,
      "loss": 2.569,
      "step": 122
    },
    {
      "epoch": 0.246,
      "grad_norm": 10.312801361083984,
      "learning_rate": 0.0019694138398030094,
      "loss": 3.4052,
      "step": 123
    },
    {
      "epoch": 0.248,
      "grad_norm": 9.634992599487305,
      "learning_rate": 0.00196889594928178,
      "loss": 3.2181,
      "step": 124
    },
    {
      "epoch": 0.25,
      "grad_norm": 6.4653096199035645,
      "learning_rate": 0.001968373780232924,
      "loss": 2.8706,
      "step": 125
    },
    {
      "epoch": 0.252,
      "grad_norm": 4.661166667938232,
      "learning_rate": 0.0019678473349622793,
      "loss": 2.9823,
      "step": 126
    },
    {
      "epoch": 0.254,
      "grad_norm": 11.554218292236328,
      "learning_rate": 0.0019673166157945627,
      "loss": 3.067,
      "step": 127
    },
    {
      "epoch": 0.256,
      "grad_norm": 12.641718864440918,
      "learning_rate": 0.001966781625073367,
      "loss": 2.8902,
      "step": 128
    },
    {
      "epoch": 0.258,
      "grad_norm": 2.1051597595214844,
      "learning_rate": 0.001966242365161146,
      "loss": 2.687,
      "step": 129
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.1415337324142456,
      "learning_rate": 0.0019656988384392075,
      "loss": 2.6062,
      "step": 130
    },
    {
      "epoch": 0.262,
      "grad_norm": 7.867302417755127,
      "learning_rate": 0.0019651510473076986,
      "loss": 3.0761,
      "step": 131
    },
    {
      "epoch": 0.264,
      "grad_norm": 8.365605354309082,
      "learning_rate": 0.0019645989941856,
      "loss": 3.1222,
      "step": 132
    },
    {
      "epoch": 0.266,
      "grad_norm": 2.648066282272339,
      "learning_rate": 0.0019640426815107108,
      "loss": 2.7301,
      "step": 133
    },
    {
      "epoch": 0.268,
      "grad_norm": 4.7227349281311035,
      "learning_rate": 0.001963482111739641,
      "loss": 2.7605,
      "step": 134
    },
    {
      "epoch": 0.27,
      "grad_norm": 3.2076096534729004,
      "learning_rate": 0.0019629172873477994,
      "loss": 2.7632,
      "step": 135
    },
    {
      "epoch": 0.272,
      "grad_norm": 2.9289326667785645,
      "learning_rate": 0.0019623482108293818,
      "loss": 2.6788,
      "step": 136
    },
    {
      "epoch": 0.274,
      "grad_norm": 3.5195915699005127,
      "learning_rate": 0.001961774884697362,
      "loss": 2.5345,
      "step": 137
    },
    {
      "epoch": 0.276,
      "grad_norm": 2.2036643028259277,
      "learning_rate": 0.001961197311483479,
      "loss": 2.614,
      "step": 138
    },
    {
      "epoch": 0.278,
      "grad_norm": 2.5068509578704834,
      "learning_rate": 0.001960615493738226,
      "loss": 2.5876,
      "step": 139
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.586123943328857,
      "learning_rate": 0.0019600294340308398,
      "loss": 2.5494,
      "step": 140
    },
    {
      "epoch": 0.282,
      "grad_norm": 2.2469773292541504,
      "learning_rate": 0.0019594391349492903,
      "loss": 2.4626,
      "step": 141
    },
    {
      "epoch": 0.284,
      "grad_norm": 3.114971399307251,
      "learning_rate": 0.001958844599100266,
      "loss": 2.5529,
      "step": 142
    },
    {
      "epoch": 0.286,
      "grad_norm": 3.3518009185791016,
      "learning_rate": 0.0019582458291091663,
      "loss": 2.4463,
      "step": 143
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.3204469680786133,
      "learning_rate": 0.001957642827620087,
      "loss": 2.3298,
      "step": 144
    },
    {
      "epoch": 0.29,
      "grad_norm": 2.791255235671997,
      "learning_rate": 0.00195703559729581,
      "loss": 2.4031,
      "step": 145
    },
    {
      "epoch": 0.292,
      "grad_norm": 2.419426918029785,
      "learning_rate": 0.00195642414081779,
      "loss": 2.2956,
      "step": 146
    },
    {
      "epoch": 0.294,
      "grad_norm": 3.620445728302002,
      "learning_rate": 0.0019558084608861472,
      "loss": 2.3149,
      "step": 147
    },
    {
      "epoch": 0.296,
      "grad_norm": 3.8125832080841064,
      "learning_rate": 0.001955188560219648,
      "loss": 2.3881,
      "step": 148
    },
    {
      "epoch": 0.298,
      "grad_norm": 5.389157772064209,
      "learning_rate": 0.0019545644415557,
      "loss": 2.4915,
      "step": 149
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.9504411220550537,
      "learning_rate": 0.001953936107650336,
      "loss": 2.3557,
      "step": 150
    },
    {
      "epoch": 0.302,
      "grad_norm": 4.520712375640869,
      "learning_rate": 0.001953303561278202,
      "loss": 2.4204,
      "step": 151
    },
    {
      "epoch": 0.304,
      "grad_norm": 2.5971944332122803,
      "learning_rate": 0.0019526668052325467,
      "loss": 2.277,
      "step": 152
    },
    {
      "epoch": 0.306,
      "grad_norm": 4.897330284118652,
      "learning_rate": 0.001952025842325208,
      "loss": 2.3802,
      "step": 153
    },
    {
      "epoch": 0.308,
      "grad_norm": 7.155513763427734,
      "learning_rate": 0.0019513806753866014,
      "loss": 2.5903,
      "step": 154
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.4044182300567627,
      "learning_rate": 0.0019507313072657055,
      "loss": 2.3653,
      "step": 155
    },
    {
      "epoch": 0.312,
      "grad_norm": 2.7513935565948486,
      "learning_rate": 0.0019500777408300517,
      "loss": 2.5413,
      "step": 156
    },
    {
      "epoch": 0.314,
      "grad_norm": 3.6750152111053467,
      "learning_rate": 0.001949419978965711,
      "loss": 2.4324,
      "step": 157
    },
    {
      "epoch": 0.316,
      "grad_norm": 2.800503730773926,
      "learning_rate": 0.00194875802457728,
      "loss": 2.4592,
      "step": 158
    },
    {
      "epoch": 0.318,
      "grad_norm": 1.5488383769989014,
      "learning_rate": 0.0019480918805878697,
      "loss": 2.5536,
      "step": 159
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.892378807067871,
      "learning_rate": 0.001947421549939091,
      "loss": 2.3528,
      "step": 160
    },
    {
      "epoch": 0.322,
      "grad_norm": 2.5410561561584473,
      "learning_rate": 0.0019467470355910438,
      "loss": 2.5435,
      "step": 161
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.5102311372756958,
      "learning_rate": 0.0019460683405223018,
      "loss": 2.3692,
      "step": 162
    },
    {
      "epoch": 0.326,
      "grad_norm": 1.8860889673233032,
      "learning_rate": 0.001945385467729901,
      "loss": 2.4393,
      "step": 163
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.9577900171279907,
      "learning_rate": 0.0019446984202293246,
      "loss": 2.6922,
      "step": 164
    },
    {
      "epoch": 0.33,
      "grad_norm": 2.702770948410034,
      "learning_rate": 0.0019440072010544918,
      "loss": 2.4289,
      "step": 165
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.7343031167984009,
      "learning_rate": 0.001943311813257743,
      "loss": 2.4545,
      "step": 166
    },
    {
      "epoch": 0.334,
      "grad_norm": 2.591649055480957,
      "learning_rate": 0.001942612259909827,
      "loss": 2.3311,
      "step": 167
    },
    {
      "epoch": 0.336,
      "grad_norm": 4.664798736572266,
      "learning_rate": 0.0019419085440998871,
      "loss": 2.3579,
      "step": 168
    },
    {
      "epoch": 0.338,
      "grad_norm": 2.376110792160034,
      "learning_rate": 0.0019412006689354469,
      "loss": 2.3356,
      "step": 169
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.4358927011489868,
      "learning_rate": 0.0019404886375423982,
      "loss": 2.5352,
      "step": 170
    },
    {
      "epoch": 0.342,
      "grad_norm": 5.066070556640625,
      "learning_rate": 0.0019397724530649857,
      "loss": 2.286,
      "step": 171
    },
    {
      "epoch": 0.344,
      "grad_norm": 3.3609206676483154,
      "learning_rate": 0.0019390521186657935,
      "loss": 2.4734,
      "step": 172
    },
    {
      "epoch": 0.346,
      "grad_norm": 2.534966230392456,
      "learning_rate": 0.001938327637525731,
      "loss": 2.3758,
      "step": 173
    },
    {
      "epoch": 0.348,
      "grad_norm": 3.235281467437744,
      "learning_rate": 0.0019375990128440205,
      "loss": 2.2871,
      "step": 174
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.8142403960227966,
      "learning_rate": 0.0019368662478381799,
      "loss": 2.3591,
      "step": 175
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.0178661346435547,
      "learning_rate": 0.001936129345744011,
      "loss": 2.2127,
      "step": 176
    },
    {
      "epoch": 0.354,
      "grad_norm": 1.2104151248931885,
      "learning_rate": 0.0019353883098155854,
      "loss": 2.3315,
      "step": 177
    },
    {
      "epoch": 0.356,
      "grad_norm": 1.7122215032577515,
      "learning_rate": 0.0019346431433252273,
      "loss": 2.2473,
      "step": 178
    },
    {
      "epoch": 0.358,
      "grad_norm": 0.9045205116271973,
      "learning_rate": 0.001933893849563503,
      "loss": 2.2175,
      "step": 179
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.8639039993286133,
      "learning_rate": 0.0019331404318392025,
      "loss": 2.0355,
      "step": 180
    },
    {
      "epoch": 0.362,
      "grad_norm": 0.5442056655883789,
      "learning_rate": 0.0019323828934793284,
      "loss": 2.1411,
      "step": 181
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.9039509296417236,
      "learning_rate": 0.0019316212378290782,
      "loss": 2.1988,
      "step": 182
    },
    {
      "epoch": 0.366,
      "grad_norm": 1.0469768047332764,
      "learning_rate": 0.0019308554682518312,
      "loss": 2.2151,
      "step": 183
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.0894259214401245,
      "learning_rate": 0.001930085588129134,
      "loss": 2.2926,
      "step": 184
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.074076771736145,
      "learning_rate": 0.0019293116008606837,
      "loss": 2.2157,
      "step": 185
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.7083254456520081,
      "learning_rate": 0.0019285335098643153,
      "loss": 2.0883,
      "step": 186
    },
    {
      "epoch": 0.374,
      "grad_norm": 0.7717963457107544,
      "learning_rate": 0.0019277513185759845,
      "loss": 2.0789,
      "step": 187
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.0702821016311646,
      "learning_rate": 0.001926965030449754,
      "loss": 2.0811,
      "step": 188
    },
    {
      "epoch": 0.378,
      "grad_norm": 1.4277511835098267,
      "learning_rate": 0.0019261746489577765,
      "loss": 2.0163,
      "step": 189
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.046269178390503,
      "learning_rate": 0.001925380177590282,
      "loss": 2.0642,
      "step": 190
    },
    {
      "epoch": 0.382,
      "grad_norm": 2.1891238689422607,
      "learning_rate": 0.0019245816198555604,
      "loss": 2.1662,
      "step": 191
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.068668842315674,
      "learning_rate": 0.0019237789792799457,
      "loss": 2.0996,
      "step": 192
    },
    {
      "epoch": 0.386,
      "grad_norm": 2.1617050170898438,
      "learning_rate": 0.001922972259407802,
      "loss": 2.11,
      "step": 193
    },
    {
      "epoch": 0.388,
      "grad_norm": 3.4916043281555176,
      "learning_rate": 0.0019221614638015075,
      "loss": 2.0878,
      "step": 194
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.109224319458008,
      "learning_rate": 0.001921346596041437,
      "loss": 2.1832,
      "step": 195
    },
    {
      "epoch": 0.392,
      "grad_norm": 4.173354625701904,
      "learning_rate": 0.0019205276597259483,
      "loss": 2.2866,
      "step": 196
    },
    {
      "epoch": 0.394,
      "grad_norm": 1.5933623313903809,
      "learning_rate": 0.0019197046584713661,
      "loss": 2.153,
      "step": 197
    },
    {
      "epoch": 0.396,
      "grad_norm": 1.0274534225463867,
      "learning_rate": 0.0019188775959119641,
      "loss": 1.9966,
      "step": 198
    },
    {
      "epoch": 0.398,
      "grad_norm": 1.349075436592102,
      "learning_rate": 0.0019180464756999509,
      "loss": 1.9761,
      "step": 199
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.169824481010437,
      "learning_rate": 0.001917211301505453,
      "loss": 2.3021,
      "step": 200
    },
    {
      "epoch": 0.4,
      "eval_loss": 2.087155342102051,
      "eval_runtime": 228.7401,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 200
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.9232484102249146,
      "learning_rate": 0.001916372077016499,
      "loss": 2.004,
      "step": 201
    },
    {
      "epoch": 0.404,
      "grad_norm": 4.446470260620117,
      "learning_rate": 0.0019155288059390027,
      "loss": 2.0448,
      "step": 202
    },
    {
      "epoch": 0.406,
      "grad_norm": 2.6597437858581543,
      "learning_rate": 0.0019146814919967481,
      "loss": 2.0506,
      "step": 203
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.728100538253784,
      "learning_rate": 0.0019138301389313708,
      "loss": 2.1125,
      "step": 204
    },
    {
      "epoch": 0.41,
      "grad_norm": 1.0705533027648926,
      "learning_rate": 0.0019129747505023437,
      "loss": 1.9811,
      "step": 205
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.311711311340332,
      "learning_rate": 0.0019121153304869584,
      "loss": 1.8992,
      "step": 206
    },
    {
      "epoch": 0.414,
      "grad_norm": 1.3047494888305664,
      "learning_rate": 0.0019112518826803098,
      "loss": 1.8774,
      "step": 207
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.7055065631866455,
      "learning_rate": 0.00191038441089528,
      "loss": 2.0167,
      "step": 208
    },
    {
      "epoch": 0.418,
      "grad_norm": 5.122485160827637,
      "learning_rate": 0.0019095129189625193,
      "loss": 2.3276,
      "step": 209
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.042346954345703,
      "learning_rate": 0.0019086374107304311,
      "loss": 1.9327,
      "step": 210
    },
    {
      "epoch": 0.422,
      "grad_norm": 2.1639699935913086,
      "learning_rate": 0.0019077578900651541,
      "loss": 2.1737,
      "step": 211
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.4633746147155762,
      "learning_rate": 0.0019068743608505454,
      "loss": 2.1236,
      "step": 212
    },
    {
      "epoch": 0.426,
      "grad_norm": 1.1507340669631958,
      "learning_rate": 0.0019059868269881636,
      "loss": 1.9466,
      "step": 213
    },
    {
      "epoch": 0.428,
      "grad_norm": 22.211172103881836,
      "learning_rate": 0.0019050952923972508,
      "loss": 2.2025,
      "step": 214
    },
    {
      "epoch": 0.43,
      "grad_norm": 12.042900085449219,
      "learning_rate": 0.0019041997610147166,
      "loss": 3.1032,
      "step": 215
    },
    {
      "epoch": 0.432,
      "grad_norm": 6.174982070922852,
      "learning_rate": 0.0019033002367951192,
      "loss": 2.6387,
      "step": 216
    },
    {
      "epoch": 0.434,
      "grad_norm": 1.4321067333221436,
      "learning_rate": 0.0019023967237106491,
      "loss": 2.1359,
      "step": 217
    },
    {
      "epoch": 0.436,
      "grad_norm": 8.086593627929688,
      "learning_rate": 0.0019014892257511117,
      "loss": 2.6089,
      "step": 218
    },
    {
      "epoch": 0.438,
      "grad_norm": 6.714529514312744,
      "learning_rate": 0.0019005777469239076,
      "loss": 2.895,
      "step": 219
    },
    {
      "epoch": 0.44,
      "grad_norm": 4.731200695037842,
      "learning_rate": 0.001899662291254018,
      "loss": 2.8271,
      "step": 220
    },
    {
      "epoch": 0.442,
      "grad_norm": 13.793353080749512,
      "learning_rate": 0.0018987428627839842,
      "loss": 2.4418,
      "step": 221
    },
    {
      "epoch": 0.444,
      "grad_norm": 2.278334617614746,
      "learning_rate": 0.0018978194655738915,
      "loss": 2.0087,
      "step": 222
    },
    {
      "epoch": 0.446,
      "grad_norm": 5.2318806648254395,
      "learning_rate": 0.0018968921037013512,
      "loss": 2.0781,
      "step": 223
    },
    {
      "epoch": 0.448,
      "grad_norm": 9.879586219787598,
      "learning_rate": 0.0018959607812614806,
      "loss": 2.2921,
      "step": 224
    },
    {
      "epoch": 0.45,
      "grad_norm": 3.0180296897888184,
      "learning_rate": 0.0018950255023668877,
      "loss": 2.1912,
      "step": 225
    },
    {
      "epoch": 0.452,
      "grad_norm": 2.880418300628662,
      "learning_rate": 0.0018940862711476511,
      "loss": 2.2629,
      "step": 226
    },
    {
      "epoch": 0.454,
      "grad_norm": 1.419576644897461,
      "learning_rate": 0.0018931430917513029,
      "loss": 2.0631,
      "step": 227
    },
    {
      "epoch": 0.456,
      "grad_norm": 1.007103681564331,
      "learning_rate": 0.001892195968342809,
      "loss": 1.9057,
      "step": 228
    },
    {
      "epoch": 0.458,
      "grad_norm": 1.1325498819351196,
      "learning_rate": 0.0018912449051045525,
      "loss": 1.9321,
      "step": 229
    },
    {
      "epoch": 0.46,
      "grad_norm": 2.100541114807129,
      "learning_rate": 0.0018902899062363141,
      "loss": 2.0121,
      "step": 230
    },
    {
      "epoch": 0.462,
      "grad_norm": 1.1221510171890259,
      "learning_rate": 0.0018893309759552529,
      "loss": 1.938,
      "step": 231
    },
    {
      "epoch": 0.464,
      "grad_norm": 1.1299188137054443,
      "learning_rate": 0.0018883681184958898,
      "loss": 2.1734,
      "step": 232
    },
    {
      "epoch": 0.466,
      "grad_norm": 1.4051908254623413,
      "learning_rate": 0.0018874013381100874,
      "loss": 1.846,
      "step": 233
    },
    {
      "epoch": 0.468,
      "grad_norm": 1.5859426259994507,
      "learning_rate": 0.0018864306390670308,
      "loss": 1.8241,
      "step": 234
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.8983497023582458,
      "learning_rate": 0.0018854560256532098,
      "loss": 1.8387,
      "step": 235
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.0259063243865967,
      "learning_rate": 0.0018844775021724003,
      "loss": 1.9667,
      "step": 236
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.7260911464691162,
      "learning_rate": 0.0018834950729456432,
      "loss": 1.9189,
      "step": 237
    },
    {
      "epoch": 0.476,
      "grad_norm": 1.650926947593689,
      "learning_rate": 0.001882508742311228,
      "loss": 1.7305,
      "step": 238
    },
    {
      "epoch": 0.478,
      "grad_norm": 1.172369360923767,
      "learning_rate": 0.0018815185146246716,
      "loss": 1.8847,
      "step": 239
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.6931760907173157,
      "learning_rate": 0.0018805243942587,
      "loss": 1.8689,
      "step": 240
    },
    {
      "epoch": 0.482,
      "grad_norm": 1.2832351922988892,
      "learning_rate": 0.0018795263856032287,
      "loss": 1.9057,
      "step": 241
    },
    {
      "epoch": 0.484,
      "grad_norm": 26.47721290588379,
      "learning_rate": 0.0018785244930653439,
      "loss": 1.9442,
      "step": 242
    },
    {
      "epoch": 0.486,
      "grad_norm": 0.9876003861427307,
      "learning_rate": 0.0018775187210692814,
      "loss": 1.8215,
      "step": 243
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.9097820520401001,
      "learning_rate": 0.0018765090740564098,
      "loss": 1.855,
      "step": 244
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.9427711963653564,
      "learning_rate": 0.001875495556485208,
      "loss": 1.9431,
      "step": 245
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.7460276484489441,
      "learning_rate": 0.001874478172831248,
      "loss": 1.8116,
      "step": 246
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.9198756814002991,
      "learning_rate": 0.0018734569275871726,
      "loss": 1.8921,
      "step": 247
    },
    {
      "epoch": 0.496,
      "grad_norm": 1.4212478399276733,
      "learning_rate": 0.0018724318252626776,
      "loss": 2.0372,
      "step": 248
    },
    {
      "epoch": 0.498,
      "grad_norm": 0.5720387697219849,
      "learning_rate": 0.0018714028703844914,
      "loss": 1.6899,
      "step": 249
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.8207013010978699,
      "learning_rate": 0.0018703700674963545,
      "loss": 1.8318,
      "step": 250
    },
    {
      "epoch": 0.502,
      "grad_norm": 1.0384647846221924,
      "learning_rate": 0.0018693334211590006,
      "loss": 1.8637,
      "step": 251
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.8766223192214966,
      "learning_rate": 0.0018682929359501337,
      "loss": 1.7961,
      "step": 252
    },
    {
      "epoch": 0.506,
      "grad_norm": 1.039932131767273,
      "learning_rate": 0.0018672486164644116,
      "loss": 1.7555,
      "step": 253
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.5402951836585999,
      "learning_rate": 0.0018662004673134231,
      "loss": 1.757,
      "step": 254
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.9596356749534607,
      "learning_rate": 0.0018651484931256684,
      "loss": 1.6867,
      "step": 255
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.8282240629196167,
      "learning_rate": 0.0018640926985465387,
      "loss": 1.6481,
      "step": 256
    },
    {
      "epoch": 0.514,
      "grad_norm": 1.3981173038482666,
      "learning_rate": 0.0018630330882382952,
      "loss": 1.8429,
      "step": 257
    },
    {
      "epoch": 0.516,
      "grad_norm": 1.1158251762390137,
      "learning_rate": 0.0018619696668800492,
      "loss": 1.8297,
      "step": 258
    },
    {
      "epoch": 0.518,
      "grad_norm": 1.6066783666610718,
      "learning_rate": 0.0018609024391677417,
      "loss": 1.7256,
      "step": 259
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.7062008380889893,
      "learning_rate": 0.0018598314098141207,
      "loss": 1.7927,
      "step": 260
    },
    {
      "epoch": 0.522,
      "grad_norm": 2.072417736053467,
      "learning_rate": 0.0018587565835487233,
      "loss": 1.8325,
      "step": 261
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.968880295753479,
      "learning_rate": 0.0018576779651178522,
      "loss": 1.771,
      "step": 262
    },
    {
      "epoch": 0.526,
      "grad_norm": 1.589495062828064,
      "learning_rate": 0.0018565955592845563,
      "loss": 1.8607,
      "step": 263
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.3808385133743286,
      "learning_rate": 0.0018555093708286093,
      "loss": 1.814,
      "step": 264
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.6895244717597961,
      "learning_rate": 0.0018544194045464887,
      "loss": 1.7371,
      "step": 265
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.941152572631836,
      "learning_rate": 0.0018533256652513534,
      "loss": 2.0481,
      "step": 266
    },
    {
      "epoch": 0.534,
      "grad_norm": 1.556852102279663,
      "learning_rate": 0.001852228157773025,
      "loss": 1.7803,
      "step": 267
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.5502138137817383,
      "learning_rate": 0.0018511268869579635,
      "loss": 1.7977,
      "step": 268
    },
    {
      "epoch": 0.538,
      "grad_norm": 11.519356727600098,
      "learning_rate": 0.001850021857669248,
      "loss": 1.8868,
      "step": 269
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.5086493492126465,
      "learning_rate": 0.0018489130747865548,
      "loss": 2.0024,
      "step": 270
    },
    {
      "epoch": 0.542,
      "grad_norm": 2.329739809036255,
      "learning_rate": 0.0018478005432061352,
      "loss": 1.6887,
      "step": 271
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.3066250085830688,
      "learning_rate": 0.0018466842678407944,
      "loss": 1.8537,
      "step": 272
    },
    {
      "epoch": 0.546,
      "grad_norm": 2.0700197219848633,
      "learning_rate": 0.00184556425361987,
      "loss": 2.0206,
      "step": 273
    },
    {
      "epoch": 0.548,
      "grad_norm": 2.1918694972991943,
      "learning_rate": 0.0018444405054892092,
      "loss": 1.7804,
      "step": 274
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9389934539794922,
      "learning_rate": 0.001843313028411149,
      "loss": 1.7287,
      "step": 275
    },
    {
      "epoch": 0.552,
      "grad_norm": 2.0404369831085205,
      "learning_rate": 0.0018421818273644912,
      "loss": 1.7987,
      "step": 276
    },
    {
      "epoch": 0.554,
      "grad_norm": 1.5645990371704102,
      "learning_rate": 0.001841046907344484,
      "loss": 1.7326,
      "step": 277
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.0867469310760498,
      "learning_rate": 0.0018399082733627965,
      "loss": 1.8908,
      "step": 278
    },
    {
      "epoch": 0.558,
      "grad_norm": 1.9863090515136719,
      "learning_rate": 0.0018387659304474994,
      "loss": 1.8284,
      "step": 279
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.9959880113601685,
      "learning_rate": 0.0018376198836430415,
      "loss": 1.7495,
      "step": 280
    },
    {
      "epoch": 0.562,
      "grad_norm": 1.2685647010803223,
      "learning_rate": 0.0018364701380102267,
      "loss": 1.5932,
      "step": 281
    },
    {
      "epoch": 0.564,
      "grad_norm": 3.379605770111084,
      "learning_rate": 0.0018353166986261936,
      "loss": 1.8427,
      "step": 282
    },
    {
      "epoch": 0.566,
      "grad_norm": 1.1339600086212158,
      "learning_rate": 0.0018341595705843906,
      "loss": 1.7817,
      "step": 283
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.6808093786239624,
      "learning_rate": 0.001832998758994556,
      "loss": 1.7777,
      "step": 284
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.0846030712127686,
      "learning_rate": 0.0018318342689826936,
      "loss": 1.5967,
      "step": 285
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.857460081577301,
      "learning_rate": 0.001830666105691051,
      "loss": 1.7492,
      "step": 286
    },
    {
      "epoch": 0.574,
      "grad_norm": 18.6268253326416,
      "learning_rate": 0.0018294942742780964,
      "loss": 2.3627,
      "step": 287
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.662473201751709,
      "learning_rate": 0.0018283187799184957,
      "loss": 1.8249,
      "step": 288
    },
    {
      "epoch": 0.578,
      "grad_norm": 1.8290801048278809,
      "learning_rate": 0.0018271396278030903,
      "loss": 1.81,
      "step": 289
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.5882046222686768,
      "learning_rate": 0.0018259568231388736,
      "loss": 2.0656,
      "step": 290
    },
    {
      "epoch": 0.582,
      "grad_norm": 2.7158992290496826,
      "learning_rate": 0.0018247703711489684,
      "loss": 1.7376,
      "step": 291
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.7360192537307739,
      "learning_rate": 0.0018235802770726038,
      "loss": 1.6592,
      "step": 292
    },
    {
      "epoch": 0.586,
      "grad_norm": 0.9611104726791382,
      "learning_rate": 0.0018223865461650913,
      "loss": 1.6154,
      "step": 293
    },
    {
      "epoch": 0.588,
      "grad_norm": 1.0467870235443115,
      "learning_rate": 0.0018211891836978028,
      "loss": 1.7841,
      "step": 294
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.962512731552124,
      "learning_rate": 0.001819988194958146,
      "loss": 1.794,
      "step": 295
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.1482343673706055,
      "learning_rate": 0.0018187835852495429,
      "loss": 1.7645,
      "step": 296
    },
    {
      "epoch": 0.594,
      "grad_norm": 0.7849602699279785,
      "learning_rate": 0.0018175753598914047,
      "loss": 1.82,
      "step": 297
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.269761562347412,
      "learning_rate": 0.0018163635242191083,
      "loss": 1.7728,
      "step": 298
    },
    {
      "epoch": 0.598,
      "grad_norm": 0.8502991199493408,
      "learning_rate": 0.0018151480835839743,
      "loss": 1.6464,
      "step": 299
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5836257934570312,
      "learning_rate": 0.0018139290433532413,
      "loss": 1.652,
      "step": 300
    },
    {
      "epoch": 0.6,
      "eval_loss": 1.7823143005371094,
      "eval_runtime": 228.8832,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 300
    },
    {
      "epoch": 0.602,
      "grad_norm": 1.5231127738952637,
      "learning_rate": 0.0018127064089100446,
      "loss": 1.8211,
      "step": 301
    },
    {
      "epoch": 0.604,
      "grad_norm": 1.0323700904846191,
      "learning_rate": 0.00181148018565339,
      "loss": 1.7974,
      "step": 302
    },
    {
      "epoch": 0.606,
      "grad_norm": 1.9231871366500854,
      "learning_rate": 0.001810250378998132,
      "loss": 1.8587,
      "step": 303
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.168794870376587,
      "learning_rate": 0.0018090169943749475,
      "loss": 1.7838,
      "step": 304
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.2404541969299316,
      "learning_rate": 0.001807780037230315,
      "loss": 1.8,
      "step": 305
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.8690732717514038,
      "learning_rate": 0.0018065395130264874,
      "loss": 1.6905,
      "step": 306
    },
    {
      "epoch": 0.614,
      "grad_norm": 0.57569420337677,
      "learning_rate": 0.0018052954272414707,
      "loss": 1.7044,
      "step": 307
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.8931528925895691,
      "learning_rate": 0.0018040477853689969,
      "loss": 1.6956,
      "step": 308
    },
    {
      "epoch": 0.618,
      "grad_norm": 0.9971257448196411,
      "learning_rate": 0.0018027965929185024,
      "loss": 1.8003,
      "step": 309
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.4838508367538452,
      "learning_rate": 0.0018015418554151023,
      "loss": 1.5543,
      "step": 310
    },
    {
      "epoch": 0.622,
      "grad_norm": 1.077484130859375,
      "learning_rate": 0.0018002835783995652,
      "loss": 1.8775,
      "step": 311
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.4516801834106445,
      "learning_rate": 0.0017990217674282915,
      "loss": 1.8355,
      "step": 312
    },
    {
      "epoch": 0.626,
      "grad_norm": 1.5076583623886108,
      "learning_rate": 0.001797756428073286,
      "loss": 1.6261,
      "step": 313
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.9125683903694153,
      "learning_rate": 0.0017964875659221343,
      "loss": 1.7385,
      "step": 314
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.3437045812606812,
      "learning_rate": 0.0017952151865779792,
      "loss": 1.5429,
      "step": 315
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.8631296157836914,
      "learning_rate": 0.0017939392956594932,
      "loss": 1.6427,
      "step": 316
    },
    {
      "epoch": 0.634,
      "grad_norm": 1.231150507926941,
      "learning_rate": 0.0017926598988008582,
      "loss": 1.6562,
      "step": 317
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.7557599544525146,
      "learning_rate": 0.0017913770016517354,
      "loss": 1.6754,
      "step": 318
    },
    {
      "epoch": 0.638,
      "grad_norm": 1.099472165107727,
      "learning_rate": 0.0017900906098772444,
      "loss": 1.6691,
      "step": 319
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.3493553400039673,
      "learning_rate": 0.0017888007291579355,
      "loss": 1.6849,
      "step": 320
    },
    {
      "epoch": 0.642,
      "grad_norm": 0.7711889743804932,
      "learning_rate": 0.001787507365189767,
      "loss": 1.702,
      "step": 321
    },
    {
      "epoch": 0.644,
      "grad_norm": 1.1909146308898926,
      "learning_rate": 0.0017862105236840775,
      "loss": 1.6445,
      "step": 322
    },
    {
      "epoch": 0.646,
      "grad_norm": 1.202271819114685,
      "learning_rate": 0.001784910210367563,
      "loss": 1.7829,
      "step": 323
    },
    {
      "epoch": 0.648,
      "grad_norm": 1.0791819095611572,
      "learning_rate": 0.0017836064309822502,
      "loss": 1.5777,
      "step": 324
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.8754224181175232,
      "learning_rate": 0.0017822991912854714,
      "loss": 1.6147,
      "step": 325
    },
    {
      "epoch": 0.652,
      "grad_norm": 1.19380521774292,
      "learning_rate": 0.0017809884970498395,
      "loss": 1.7346,
      "step": 326
    },
    {
      "epoch": 0.654,
      "grad_norm": 1.2211077213287354,
      "learning_rate": 0.0017796743540632223,
      "loss": 1.6115,
      "step": 327
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.0480109453201294,
      "learning_rate": 0.0017783567681287167,
      "loss": 1.6735,
      "step": 328
    },
    {
      "epoch": 0.658,
      "grad_norm": 0.9083231091499329,
      "learning_rate": 0.001777035745064623,
      "loss": 1.6116,
      "step": 329
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.7638611793518066,
      "learning_rate": 0.0017757112907044199,
      "loss": 1.6513,
      "step": 330
    },
    {
      "epoch": 0.662,
      "grad_norm": 1.1647852659225464,
      "learning_rate": 0.001774383410896738,
      "loss": 1.6887,
      "step": 331
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.099047064781189,
      "learning_rate": 0.001773052111505334,
      "loss": 1.7065,
      "step": 332
    },
    {
      "epoch": 0.666,
      "grad_norm": 1.370642900466919,
      "learning_rate": 0.0017717173984090658,
      "loss": 1.7755,
      "step": 333
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.6246787905693054,
      "learning_rate": 0.0017703792775018655,
      "loss": 1.577,
      "step": 334
    },
    {
      "epoch": 0.67,
      "grad_norm": 1.0504446029663086,
      "learning_rate": 0.0017690377546927133,
      "loss": 1.6766,
      "step": 335
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.7460998892784119,
      "learning_rate": 0.001767692835905612,
      "loss": 1.4748,
      "step": 336
    },
    {
      "epoch": 0.674,
      "grad_norm": 1.4882992506027222,
      "learning_rate": 0.001766344527079561,
      "loss": 1.695,
      "step": 337
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.9611985683441162,
      "learning_rate": 0.0017649928341685298,
      "loss": 1.7733,
      "step": 338
    },
    {
      "epoch": 0.678,
      "grad_norm": 1.264172077178955,
      "learning_rate": 0.0017636377631414302,
      "loss": 1.6448,
      "step": 339
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.1381560564041138,
      "learning_rate": 0.0017622793199820932,
      "loss": 1.6442,
      "step": 340
    },
    {
      "epoch": 0.682,
      "grad_norm": 0.9429733753204346,
      "learning_rate": 0.0017609175106892395,
      "loss": 1.597,
      "step": 341
    },
    {
      "epoch": 0.684,
      "grad_norm": 1.1724679470062256,
      "learning_rate": 0.001759552341276455,
      "loss": 1.6193,
      "step": 342
    },
    {
      "epoch": 0.686,
      "grad_norm": 0.9145501255989075,
      "learning_rate": 0.0017581838177721627,
      "loss": 1.6621,
      "step": 343
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.8912887573242188,
      "learning_rate": 0.0017568119462195977,
      "loss": 1.6034,
      "step": 344
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.8403961658477783,
      "learning_rate": 0.0017554367326767792,
      "loss": 1.5582,
      "step": 345
    },
    {
      "epoch": 0.692,
      "grad_norm": 5.842499256134033,
      "learning_rate": 0.0017540581832164838,
      "loss": 1.7362,
      "step": 346
    },
    {
      "epoch": 0.694,
      "grad_norm": 2.252243757247925,
      "learning_rate": 0.0017526763039262207,
      "loss": 1.6522,
      "step": 347
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.9733878970146179,
      "learning_rate": 0.0017512911009082012,
      "loss": 1.6934,
      "step": 348
    },
    {
      "epoch": 0.698,
      "grad_norm": 1.1161422729492188,
      "learning_rate": 0.0017499025802793146,
      "loss": 1.739,
      "step": 349
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.8673473596572876,
      "learning_rate": 0.001748510748171101,
      "loss": 1.5813,
      "step": 350
    },
    {
      "epoch": 0.702,
      "grad_norm": 0.8545043468475342,
      "learning_rate": 0.0017471156107297233,
      "loss": 1.6128,
      "step": 351
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.7536791563034058,
      "learning_rate": 0.0017457171741159395,
      "loss": 1.7518,
      "step": 352
    },
    {
      "epoch": 0.706,
      "grad_norm": 0.9054182171821594,
      "learning_rate": 0.0017443154445050772,
      "loss": 1.6668,
      "step": 353
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.9451432228088379,
      "learning_rate": 0.0017429104280870056,
      "loss": 1.5543,
      "step": 354
    },
    {
      "epoch": 0.71,
      "grad_norm": 4.472679138183594,
      "learning_rate": 0.001741502131066107,
      "loss": 1.7556,
      "step": 355
    },
    {
      "epoch": 0.712,
      "grad_norm": 1.3667792081832886,
      "learning_rate": 0.001740090559661252,
      "loss": 1.7082,
      "step": 356
    },
    {
      "epoch": 0.714,
      "grad_norm": 0.6175951957702637,
      "learning_rate": 0.001738675720105769,
      "loss": 1.593,
      "step": 357
    },
    {
      "epoch": 0.716,
      "grad_norm": 29.68411636352539,
      "learning_rate": 0.001737257618647419,
      "loss": 1.6585,
      "step": 358
    },
    {
      "epoch": 0.718,
      "grad_norm": 4.170158863067627,
      "learning_rate": 0.0017358362615483669,
      "loss": 1.838,
      "step": 359
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.307258129119873,
      "learning_rate": 0.0017344116550851542,
      "loss": 1.6608,
      "step": 360
    },
    {
      "epoch": 0.722,
      "grad_norm": 3.2212538719177246,
      "learning_rate": 0.0017329838055486716,
      "loss": 1.7082,
      "step": 361
    },
    {
      "epoch": 0.724,
      "grad_norm": 1.291123867034912,
      "learning_rate": 0.0017315527192441297,
      "loss": 1.5484,
      "step": 362
    },
    {
      "epoch": 0.726,
      "grad_norm": 0.9879775643348694,
      "learning_rate": 0.0017301184024910332,
      "loss": 1.6113,
      "step": 363
    },
    {
      "epoch": 0.728,
      "grad_norm": 2.1589083671569824,
      "learning_rate": 0.0017286808616231522,
      "loss": 1.5939,
      "step": 364
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.538110613822937,
      "learning_rate": 0.0017272401029884933,
      "loss": 1.6761,
      "step": 365
    },
    {
      "epoch": 0.732,
      "grad_norm": 1.4945731163024902,
      "learning_rate": 0.0017257961329492728,
      "loss": 1.5987,
      "step": 366
    },
    {
      "epoch": 0.734,
      "grad_norm": 1.0853922367095947,
      "learning_rate": 0.001724348957881889,
      "loss": 1.6008,
      "step": 367
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.185198426246643,
      "learning_rate": 0.0017228985841768914,
      "loss": 1.6473,
      "step": 368
    },
    {
      "epoch": 0.738,
      "grad_norm": 2.011244535446167,
      "learning_rate": 0.0017214450182389558,
      "loss": 1.702,
      "step": 369
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.352113127708435,
      "learning_rate": 0.0017199882664868538,
      "loss": 1.5394,
      "step": 370
    },
    {
      "epoch": 0.742,
      "grad_norm": 1.401039719581604,
      "learning_rate": 0.0017185283353534258,
      "loss": 1.6784,
      "step": 371
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.957292377948761,
      "learning_rate": 0.0017170652312855513,
      "loss": 1.5743,
      "step": 372
    },
    {
      "epoch": 0.746,
      "grad_norm": 1.1197599172592163,
      "learning_rate": 0.0017155989607441212,
      "loss": 1.6577,
      "step": 373
    },
    {
      "epoch": 0.748,
      "grad_norm": 1.1531860828399658,
      "learning_rate": 0.0017141295302040094,
      "loss": 1.5859,
      "step": 374
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.8067148327827454,
      "learning_rate": 0.0017126569461540441,
      "loss": 1.7011,
      "step": 375
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.7454786896705627,
      "learning_rate": 0.0017111812150969788,
      "loss": 1.6242,
      "step": 376
    },
    {
      "epoch": 0.754,
      "grad_norm": 0.8122573494911194,
      "learning_rate": 0.0017097023435494636,
      "loss": 1.65,
      "step": 377
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.8289828300476074,
      "learning_rate": 0.001708220338042017,
      "loss": 1.4458,
      "step": 378
    },
    {
      "epoch": 0.758,
      "grad_norm": 1.8177695274353027,
      "learning_rate": 0.0017067352051189967,
      "loss": 1.5271,
      "step": 379
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9217049479484558,
      "learning_rate": 0.0017052469513385699,
      "loss": 1.5631,
      "step": 380
    },
    {
      "epoch": 0.762,
      "grad_norm": 0.8236808180809021,
      "learning_rate": 0.0017037555832726864,
      "loss": 1.5464,
      "step": 381
    },
    {
      "epoch": 0.764,
      "grad_norm": 2.1468148231506348,
      "learning_rate": 0.0017022611075070474,
      "loss": 1.6201,
      "step": 382
    },
    {
      "epoch": 0.766,
      "grad_norm": 1.1715935468673706,
      "learning_rate": 0.0017007635306410774,
      "loss": 1.6249,
      "step": 383
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.9796546101570129,
      "learning_rate": 0.0016992628592878956,
      "loss": 1.4883,
      "step": 384
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.1490569114685059,
      "learning_rate": 0.0016977591000742853,
      "loss": 1.6485,
      "step": 385
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.8662044405937195,
      "learning_rate": 0.0016962522596406663,
      "loss": 1.4815,
      "step": 386
    },
    {
      "epoch": 0.774,
      "grad_norm": 1.4506090879440308,
      "learning_rate": 0.0016947423446410635,
      "loss": 1.6829,
      "step": 387
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.8883365392684937,
      "learning_rate": 0.0016932293617430796,
      "loss": 1.5468,
      "step": 388
    },
    {
      "epoch": 0.778,
      "grad_norm": 1.3998026847839355,
      "learning_rate": 0.0016917133176278648,
      "loss": 1.5558,
      "step": 389
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.281177282333374,
      "learning_rate": 0.0016901942189900866,
      "loss": 1.5969,
      "step": 390
    },
    {
      "epoch": 0.782,
      "grad_norm": 1.1527841091156006,
      "learning_rate": 0.001688672072537902,
      "loss": 1.5642,
      "step": 391
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.9075772166252136,
      "learning_rate": 0.0016871468849929253,
      "loss": 1.5742,
      "step": 392
    },
    {
      "epoch": 0.786,
      "grad_norm": 1.0418435335159302,
      "learning_rate": 0.0016856186630902013,
      "loss": 1.4203,
      "step": 393
    },
    {
      "epoch": 0.788,
      "grad_norm": 1.427114725112915,
      "learning_rate": 0.001684087413578173,
      "loss": 1.5263,
      "step": 394
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.8930515050888062,
      "learning_rate": 0.0016825531432186542,
      "loss": 1.5345,
      "step": 395
    },
    {
      "epoch": 0.792,
      "grad_norm": 2.4928700923919678,
      "learning_rate": 0.0016810158587867972,
      "loss": 1.438,
      "step": 396
    },
    {
      "epoch": 0.794,
      "grad_norm": 1.422499656677246,
      "learning_rate": 0.001679475567071065,
      "loss": 1.7737,
      "step": 397
    },
    {
      "epoch": 0.796,
      "grad_norm": 1.565308690071106,
      "learning_rate": 0.0016779322748731995,
      "loss": 1.608,
      "step": 398
    },
    {
      "epoch": 0.798,
      "grad_norm": 3.0083491802215576,
      "learning_rate": 0.001676385989008193,
      "loss": 1.5348,
      "step": 399
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.825927257537842,
      "learning_rate": 0.0016748367163042577,
      "loss": 1.894,
      "step": 400
    },
    {
      "epoch": 0.8,
      "eval_loss": 2.663140058517456,
      "eval_runtime": 228.5861,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 400
    },
    {
      "epoch": 0.802,
      "grad_norm": 49.084259033203125,
      "learning_rate": 0.0016732844636027945,
      "loss": 2.8966,
      "step": 401
    },
    {
      "epoch": 0.804,
      "grad_norm": 7.208386421203613,
      "learning_rate": 0.0016717292377583647,
      "loss": 2.3815,
      "step": 402
    },
    {
      "epoch": 0.806,
      "grad_norm": 11.825223922729492,
      "learning_rate": 0.0016701710456386572,
      "loss": 2.3087,
      "step": 403
    },
    {
      "epoch": 0.808,
      "grad_norm": 18.69350814819336,
      "learning_rate": 0.0016686098941244613,
      "loss": 2.1081,
      "step": 404
    },
    {
      "epoch": 0.81,
      "grad_norm": 4.945819854736328,
      "learning_rate": 0.0016670457901096327,
      "loss": 2.0841,
      "step": 405
    },
    {
      "epoch": 0.812,
      "grad_norm": 2.2382442951202393,
      "learning_rate": 0.001665478740501067,
      "loss": 1.9495,
      "step": 406
    },
    {
      "epoch": 0.814,
      "grad_norm": 3.30540132522583,
      "learning_rate": 0.0016639087522186658,
      "loss": 2.0836,
      "step": 407
    },
    {
      "epoch": 0.816,
      "grad_norm": 2.6894960403442383,
      "learning_rate": 0.0016623358321953077,
      "loss": 1.7824,
      "step": 408
    },
    {
      "epoch": 0.818,
      "grad_norm": 1.501424789428711,
      "learning_rate": 0.0016607599873768183,
      "loss": 1.7852,
      "step": 409
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.4573293924331665,
      "learning_rate": 0.0016591812247219377,
      "loss": 1.7624,
      "step": 410
    },
    {
      "epoch": 0.822,
      "grad_norm": 1.5187232494354248,
      "learning_rate": 0.0016575995512022922,
      "loss": 1.7677,
      "step": 411
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.3021838665008545,
      "learning_rate": 0.00165601497380236,
      "loss": 1.5918,
      "step": 412
    },
    {
      "epoch": 0.826,
      "grad_norm": 1.3647332191467285,
      "learning_rate": 0.0016544274995194447,
      "loss": 1.728,
      "step": 413
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.6052151918411255,
      "learning_rate": 0.0016528371353636408,
      "loss": 1.6632,
      "step": 414
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.3614898920059204,
      "learning_rate": 0.0016512438883578046,
      "loss": 1.7832,
      "step": 415
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.2418807744979858,
      "learning_rate": 0.0016496477655375227,
      "loss": 1.7268,
      "step": 416
    },
    {
      "epoch": 0.834,
      "grad_norm": 1.1295928955078125,
      "learning_rate": 0.0016480487739510808,
      "loss": 1.7145,
      "step": 417
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.997776448726654,
      "learning_rate": 0.0016464469206594332,
      "loss": 1.5929,
      "step": 418
    },
    {
      "epoch": 0.838,
      "grad_norm": 2.637593984603882,
      "learning_rate": 0.0016448422127361706,
      "loss": 1.5882,
      "step": 419
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.6778512001037598,
      "learning_rate": 0.0016432346572674897,
      "loss": 1.5691,
      "step": 420
    },
    {
      "epoch": 0.842,
      "grad_norm": 1.4109376668930054,
      "learning_rate": 0.001641624261352161,
      "loss": 1.7181,
      "step": 421
    },
    {
      "epoch": 0.844,
      "grad_norm": 2.1076979637145996,
      "learning_rate": 0.0016400110321014992,
      "loss": 1.5405,
      "step": 422
    },
    {
      "epoch": 0.846,
      "grad_norm": 1.2131354808807373,
      "learning_rate": 0.00163839497663933,
      "loss": 1.6033,
      "step": 423
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.8180492520332336,
      "learning_rate": 0.001636776102101959,
      "loss": 1.6174,
      "step": 424
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.8554771542549133,
      "learning_rate": 0.0016351544156381413,
      "loss": 1.6058,
      "step": 425
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.9781724214553833,
      "learning_rate": 0.0016335299244090479,
      "loss": 1.6724,
      "step": 426
    },
    {
      "epoch": 0.854,
      "grad_norm": 0.9397765398025513,
      "learning_rate": 0.001631902635588237,
      "loss": 1.667,
      "step": 427
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.7167592644691467,
      "learning_rate": 0.0016302725563616192,
      "loss": 1.7178,
      "step": 428
    },
    {
      "epoch": 0.858,
      "grad_norm": 1.1971646547317505,
      "learning_rate": 0.001628639693927428,
      "loss": 1.5612,
      "step": 429
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.537182331085205,
      "learning_rate": 0.0016270040554961867,
      "loss": 1.6573,
      "step": 430
    },
    {
      "epoch": 0.862,
      "grad_norm": 1.295730710029602,
      "learning_rate": 0.0016253656482906776,
      "loss": 1.8103,
      "step": 431
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.6355118155479431,
      "learning_rate": 0.0016237244795459084,
      "loss": 1.4355,
      "step": 432
    },
    {
      "epoch": 0.866,
      "grad_norm": 0.9493564367294312,
      "learning_rate": 0.0016220805565090837,
      "loss": 1.7143,
      "step": 433
    },
    {
      "epoch": 0.868,
      "grad_norm": 1.1721752882003784,
      "learning_rate": 0.0016204338864395681,
      "loss": 1.6398,
      "step": 434
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.8405128717422485,
      "learning_rate": 0.0016187844766088583,
      "loss": 1.4582,
      "step": 435
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.164881467819214,
      "learning_rate": 0.0016171323343005498,
      "loss": 1.7084,
      "step": 436
    },
    {
      "epoch": 0.874,
      "grad_norm": 2.0078232288360596,
      "learning_rate": 0.0016154774668103028,
      "loss": 1.668,
      "step": 437
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.6471284627914429,
      "learning_rate": 0.0016138198814458127,
      "loss": 1.5394,
      "step": 438
    },
    {
      "epoch": 0.878,
      "grad_norm": 257.2276306152344,
      "learning_rate": 0.0016121595855267765,
      "loss": 1.6876,
      "step": 439
    },
    {
      "epoch": 0.88,
      "grad_norm": 4.955711364746094,
      "learning_rate": 0.0016104965863848616,
      "loss": 1.8844,
      "step": 440
    },
    {
      "epoch": 0.882,
      "grad_norm": 2.479661226272583,
      "learning_rate": 0.0016088308913636703,
      "loss": 1.8095,
      "step": 441
    },
    {
      "epoch": 0.884,
      "grad_norm": 3.6681101322174072,
      "learning_rate": 0.0016071625078187112,
      "loss": 1.8407,
      "step": 442
    },
    {
      "epoch": 0.886,
      "grad_norm": 4.782470226287842,
      "learning_rate": 0.0016054914431173652,
      "loss": 1.8935,
      "step": 443
    },
    {
      "epoch": 0.888,
      "grad_norm": 30.53533172607422,
      "learning_rate": 0.0016038177046388523,
      "loss": 1.7284,
      "step": 444
    },
    {
      "epoch": 0.89,
      "grad_norm": 17.501195907592773,
      "learning_rate": 0.0016021412997741992,
      "loss": 1.7687,
      "step": 445
    },
    {
      "epoch": 0.892,
      "grad_norm": 3.643260955810547,
      "learning_rate": 0.0016004622359262085,
      "loss": 1.756,
      "step": 446
    },
    {
      "epoch": 0.894,
      "grad_norm": 8.269022941589355,
      "learning_rate": 0.0015987805205094226,
      "loss": 1.8604,
      "step": 447
    },
    {
      "epoch": 0.896,
      "grad_norm": 3.765413761138916,
      "learning_rate": 0.0015970961609500945,
      "loss": 1.7478,
      "step": 448
    },
    {
      "epoch": 0.898,
      "grad_norm": 1.3105733394622803,
      "learning_rate": 0.0015954091646861524,
      "loss": 1.6538,
      "step": 449
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.932565450668335,
      "learning_rate": 0.0015937195391671688,
      "loss": 1.6795,
      "step": 450
    },
    {
      "epoch": 0.902,
      "grad_norm": 2.3548834323883057,
      "learning_rate": 0.0015920272918543256,
      "loss": 1.7642,
      "step": 451
    },
    {
      "epoch": 0.904,
      "grad_norm": 1.330745816230774,
      "learning_rate": 0.0015903324302203835,
      "loss": 1.4647,
      "step": 452
    },
    {
      "epoch": 0.906,
      "grad_norm": 1.658000111579895,
      "learning_rate": 0.001588634961749646,
      "loss": 1.7556,
      "step": 453
    },
    {
      "epoch": 0.908,
      "grad_norm": 3.387324810028076,
      "learning_rate": 0.0015869348939379303,
      "loss": 1.6538,
      "step": 454
    },
    {
      "epoch": 0.91,
      "grad_norm": 152.97799682617188,
      "learning_rate": 0.0015852322342925294,
      "loss": 1.7838,
      "step": 455
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.4772089719772339,
      "learning_rate": 0.0015835269903321841,
      "loss": 1.6393,
      "step": 456
    },
    {
      "epoch": 0.914,
      "grad_norm": 1.7114837169647217,
      "learning_rate": 0.0015818191695870453,
      "loss": 1.7409,
      "step": 457
    },
    {
      "epoch": 0.916,
      "grad_norm": 1.6915321350097656,
      "learning_rate": 0.0015801087795986437,
      "loss": 1.7024,
      "step": 458
    },
    {
      "epoch": 0.918,
      "grad_norm": 2.0051369667053223,
      "learning_rate": 0.0015783958279198549,
      "loss": 1.7917,
      "step": 459
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.8461072444915771,
      "learning_rate": 0.0015766803221148673,
      "loss": 1.6529,
      "step": 460
    },
    {
      "epoch": 0.922,
      "grad_norm": 1.5127614736557007,
      "learning_rate": 0.001574962269759147,
      "loss": 1.6524,
      "step": 461
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.6698948740959167,
      "learning_rate": 0.0015732416784394064,
      "loss": 1.7163,
      "step": 462
    },
    {
      "epoch": 0.926,
      "grad_norm": 1.6204802989959717,
      "learning_rate": 0.0015715185557535689,
      "loss": 1.5534,
      "step": 463
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.6415321826934814,
      "learning_rate": 0.0015697929093107365,
      "loss": 1.6848,
      "step": 464
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.0659085512161255,
      "learning_rate": 0.0015680647467311557,
      "loss": 1.5771,
      "step": 465
    },
    {
      "epoch": 0.932,
      "grad_norm": 1.336102843284607,
      "learning_rate": 0.0015663340756461844,
      "loss": 1.566,
      "step": 466
    },
    {
      "epoch": 0.934,
      "grad_norm": 1.5215941667556763,
      "learning_rate": 0.0015646009036982566,
      "loss": 1.8299,
      "step": 467
    },
    {
      "epoch": 0.936,
      "grad_norm": 2.7674427032470703,
      "learning_rate": 0.0015628652385408508,
      "loss": 1.6396,
      "step": 468
    },
    {
      "epoch": 0.938,
      "grad_norm": 1.0575882196426392,
      "learning_rate": 0.001561127087838455,
      "loss": 1.4491,
      "step": 469
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.733551263809204,
      "learning_rate": 0.0015593864592665333,
      "loss": 1.8176,
      "step": 470
    },
    {
      "epoch": 0.942,
      "grad_norm": 1.4474126100540161,
      "learning_rate": 0.001557643360511491,
      "loss": 1.615,
      "step": 471
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.8422797322273254,
      "learning_rate": 0.0015558977992706424,
      "loss": 1.5498,
      "step": 472
    },
    {
      "epoch": 0.946,
      "grad_norm": 1.0337456464767456,
      "learning_rate": 0.001554149783252175,
      "loss": 1.6324,
      "step": 473
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.8075729608535767,
      "learning_rate": 0.0015523993201751166,
      "loss": 1.5321,
      "step": 474
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.2668739557266235,
      "learning_rate": 0.0015506464177693008,
      "loss": 1.7793,
      "step": 475
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.7997688055038452,
      "learning_rate": 0.001548891083775334,
      "loss": 1.7718,
      "step": 476
    },
    {
      "epoch": 0.954,
      "grad_norm": 1.2308053970336914,
      "learning_rate": 0.001547133325944559,
      "loss": 1.6295,
      "step": 477
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.8287737965583801,
      "learning_rate": 0.0015453731520390214,
      "loss": 1.582,
      "step": 478
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.7936168313026428,
      "learning_rate": 0.0015436105698314383,
      "loss": 1.6437,
      "step": 479
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.871144413948059,
      "learning_rate": 0.001541845587105159,
      "loss": 1.7393,
      "step": 480
    },
    {
      "epoch": 0.962,
      "grad_norm": 1.0065170526504517,
      "learning_rate": 0.001540078211654135,
      "loss": 1.5133,
      "step": 481
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.7463515996932983,
      "learning_rate": 0.0015383084512828825,
      "loss": 1.5742,
      "step": 482
    },
    {
      "epoch": 0.966,
      "grad_norm": 1.4624966382980347,
      "learning_rate": 0.0015365363138064498,
      "loss": 1.5693,
      "step": 483
    },
    {
      "epoch": 0.968,
      "grad_norm": 2.0347917079925537,
      "learning_rate": 0.0015347618070503826,
      "loss": 1.638,
      "step": 484
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.8479105234146118,
      "learning_rate": 0.0015329849388506886,
      "loss": 1.5787,
      "step": 485
    },
    {
      "epoch": 0.972,
      "grad_norm": 1.2716195583343506,
      "learning_rate": 0.0015312057170538034,
      "loss": 1.4339,
      "step": 486
    },
    {
      "epoch": 0.974,
      "grad_norm": 2.105614185333252,
      "learning_rate": 0.0015294241495165558,
      "loss": 1.6836,
      "step": 487
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.3805021047592163,
      "learning_rate": 0.0015276402441061327,
      "loss": 1.6813,
      "step": 488
    },
    {
      "epoch": 0.978,
      "grad_norm": 1.1508023738861084,
      "learning_rate": 0.0015258540087000458,
      "loss": 1.5458,
      "step": 489
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.9013211727142334,
      "learning_rate": 0.001524065451186095,
      "loss": 1.3457,
      "step": 490
    },
    {
      "epoch": 0.982,
      "grad_norm": 0.956143319606781,
      "learning_rate": 0.0015222745794623341,
      "loss": 1.601,
      "step": 491
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.786769688129425,
      "learning_rate": 0.0015204814014370372,
      "loss": 1.5378,
      "step": 492
    },
    {
      "epoch": 0.986,
      "grad_norm": 0.7278709411621094,
      "learning_rate": 0.0015186859250286616,
      "loss": 1.4091,
      "step": 493
    },
    {
      "epoch": 0.988,
      "grad_norm": 1.3844939470291138,
      "learning_rate": 0.0015168881581658147,
      "loss": 1.6194,
      "step": 494
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.487123727798462,
      "learning_rate": 0.0015150881087872183,
      "loss": 1.6873,
      "step": 495
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.6781864762306213,
      "learning_rate": 0.0015132857848416733,
      "loss": 1.5885,
      "step": 496
    },
    {
      "epoch": 0.994,
      "grad_norm": 1.135419487953186,
      "learning_rate": 0.0015114811942880243,
      "loss": 1.594,
      "step": 497
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.0823935270309448,
      "learning_rate": 0.0015096743450951258,
      "loss": 1.614,
      "step": 498
    },
    {
      "epoch": 0.998,
      "grad_norm": 0.9972948431968689,
      "learning_rate": 0.0015078652452418062,
      "loss": 1.4762,
      "step": 499
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7504764795303345,
      "learning_rate": 0.0015060539027168317,
      "loss": 1.4587,
      "step": 500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.5547724962234497,
      "eval_runtime": 228.9372,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1707075519492915e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
