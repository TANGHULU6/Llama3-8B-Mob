{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8333333333333334,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.5661721229553223,
      "learning_rate": 4e-05,
      "loss": 2.303,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.9344658851623535,
      "learning_rate": 8e-05,
      "loss": 2.2558,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.9262771606445312,
      "learning_rate": 0.00012,
      "loss": 2.3115,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.9615176916122437,
      "learning_rate": 0.00016,
      "loss": 2.1243,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.5024659633636475,
      "learning_rate": 0.0002,
      "loss": 1.9493,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.1196906566619873,
      "learning_rate": 0.0001996638655462185,
      "loss": 1.8691,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.502569317817688,
      "learning_rate": 0.00019932773109243698,
      "loss": 1.6319,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.2315130233764648,
      "learning_rate": 0.00019899159663865548,
      "loss": 1.801,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.36921226978302,
      "learning_rate": 0.00019865546218487395,
      "loss": 1.45,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 1.5720726251602173,
      "learning_rate": 0.00019831932773109245,
      "loss": 1.6808,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.0161380767822266,
      "learning_rate": 0.00019798319327731095,
      "loss": 1.3581,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8896417617797852,
      "learning_rate": 0.00019764705882352942,
      "loss": 1.1796,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 1.3818228244781494,
      "learning_rate": 0.00019731092436974792,
      "loss": 1.1899,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 1.3324708938598633,
      "learning_rate": 0.00019697478991596642,
      "loss": 1.3526,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.6876637935638428,
      "learning_rate": 0.00019663865546218486,
      "loss": 1.0908,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.9917048215866089,
      "learning_rate": 0.00019630252100840336,
      "loss": 1.2983,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 1.1974130868911743,
      "learning_rate": 0.00019596638655462186,
      "loss": 1.0695,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.1983751058578491,
      "learning_rate": 0.00019563025210084033,
      "loss": 1.0562,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 2.3961362838745117,
      "learning_rate": 0.00019529411764705883,
      "loss": 1.158,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.8209903836250305,
      "learning_rate": 0.0001949579831932773,
      "loss": 1.0805,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 1.3336471319198608,
      "learning_rate": 0.0001946218487394958,
      "loss": 1.0835,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.8282696604728699,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.9901,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 1.3399937152862549,
      "learning_rate": 0.00019394957983193278,
      "loss": 0.8165,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.330065131187439,
      "learning_rate": 0.00019361344537815127,
      "loss": 0.7991,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 1.2469419240951538,
      "learning_rate": 0.00019327731092436975,
      "loss": 0.8213,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 3.155519962310791,
      "learning_rate": 0.00019294117647058825,
      "loss": 0.8461,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 1.9440536499023438,
      "learning_rate": 0.00019260504201680674,
      "loss": 0.7308,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.8644464612007141,
      "learning_rate": 0.00019226890756302522,
      "loss": 0.694,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 1.0113990306854248,
      "learning_rate": 0.00019193277310924372,
      "loss": 0.5772,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.3092641830444336,
      "learning_rate": 0.00019159663865546221,
      "loss": 0.5946,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.7926218509674072,
      "learning_rate": 0.0001912605042016807,
      "loss": 0.7052,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 1.474076747894287,
      "learning_rate": 0.00019092436974789919,
      "loss": 0.6718,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.627723753452301,
      "learning_rate": 0.00019058823529411766,
      "loss": 0.4045,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.9614128470420837,
      "learning_rate": 0.00019025210084033613,
      "loss": 0.5765,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.5163346529006958,
      "learning_rate": 0.00018991596638655463,
      "loss": 0.5415,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.914383053779602,
      "learning_rate": 0.0001895798319327731,
      "loss": 0.5533,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.3781643807888031,
      "learning_rate": 0.0001892436974789916,
      "loss": 0.4234,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.4331152141094208,
      "learning_rate": 0.0001889075630252101,
      "loss": 0.4511,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.7180728912353516,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.476,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.24078796803951263,
      "learning_rate": 0.00018823529411764707,
      "loss": 0.407,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.8268648386001587,
      "learning_rate": 0.00018789915966386554,
      "loss": 0.4811,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.26039940118789673,
      "learning_rate": 0.00018756302521008404,
      "loss": 0.4329,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.2442910522222519,
      "learning_rate": 0.00018722689075630254,
      "loss": 0.4579,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.2849426567554474,
      "learning_rate": 0.000186890756302521,
      "loss": 0.5029,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.345053106546402,
      "learning_rate": 0.0001865546218487395,
      "loss": 0.5064,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.36598291993141174,
      "learning_rate": 0.000186218487394958,
      "loss": 0.3937,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.2468591332435608,
      "learning_rate": 0.00018588235294117648,
      "loss": 0.4701,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.2714437246322632,
      "learning_rate": 0.00018554621848739498,
      "loss": 0.3457,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.24427710473537445,
      "learning_rate": 0.00018521008403361345,
      "loss": 0.45,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.2622455358505249,
      "learning_rate": 0.00018487394957983195,
      "loss": 0.4306,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.22143912315368652,
      "learning_rate": 0.00018453781512605045,
      "loss": 0.4317,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.1908300668001175,
      "learning_rate": 0.0001842016806722689,
      "loss": 0.4688,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.20756910741329193,
      "learning_rate": 0.0001838655462184874,
      "loss": 0.4091,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.28145065903663635,
      "learning_rate": 0.0001835294117647059,
      "loss": 0.4974,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.17015068233013153,
      "learning_rate": 0.00018319327731092437,
      "loss": 0.3489,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.18336021900177002,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.3929,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.1724860519170761,
      "learning_rate": 0.00018252100840336134,
      "loss": 0.4335,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.2563060224056244,
      "learning_rate": 0.00018218487394957984,
      "loss": 0.4641,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.2617512345314026,
      "learning_rate": 0.00018184873949579833,
      "loss": 0.3416,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.21522939205169678,
      "learning_rate": 0.0001815126050420168,
      "loss": 0.4323,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.18363913893699646,
      "learning_rate": 0.0001811764705882353,
      "loss": 0.29,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.21534262597560883,
      "learning_rate": 0.0001808403361344538,
      "loss": 0.4408,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.1391594558954239,
      "learning_rate": 0.00018050420168067228,
      "loss": 0.4164,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.12781597673892975,
      "learning_rate": 0.00018016806722689078,
      "loss": 0.3648,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.1600930392742157,
      "learning_rate": 0.00017983193277310925,
      "loss": 0.3816,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.18819169700145721,
      "learning_rate": 0.00017949579831932775,
      "loss": 0.3782,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.13240452110767365,
      "learning_rate": 0.00017915966386554625,
      "loss": 0.3448,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.2708832323551178,
      "learning_rate": 0.00017882352941176472,
      "loss": 0.3251,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.2537456154823303,
      "learning_rate": 0.00017848739495798322,
      "loss": 0.4513,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.2284460961818695,
      "learning_rate": 0.0001781512605042017,
      "loss": 0.4188,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.18491916358470917,
      "learning_rate": 0.00017781512605042016,
      "loss": 0.351,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.15174488723278046,
      "learning_rate": 0.00017747899159663866,
      "loss": 0.3733,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.10689562559127808,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.2748,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.2130720317363739,
      "learning_rate": 0.00017680672268907563,
      "loss": 0.4282,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.1919727474451065,
      "learning_rate": 0.00017647058823529413,
      "loss": 0.3572,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.18564611673355103,
      "learning_rate": 0.0001761344537815126,
      "loss": 0.4321,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.1452057957649231,
      "learning_rate": 0.0001757983193277311,
      "loss": 0.3788,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.21927952766418457,
      "learning_rate": 0.0001754621848739496,
      "loss": 0.3649,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.17974980175495148,
      "learning_rate": 0.00017512605042016807,
      "loss": 0.4213,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.17333140969276428,
      "learning_rate": 0.00017478991596638657,
      "loss": 0.3607,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.19937582314014435,
      "learning_rate": 0.00017445378151260504,
      "loss": 0.3848,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.13749882578849792,
      "learning_rate": 0.00017411764705882354,
      "loss": 0.3758,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.12513850629329681,
      "learning_rate": 0.00017378151260504204,
      "loss": 0.2896,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.167961984872818,
      "learning_rate": 0.0001734453781512605,
      "loss": 0.4103,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.20158807933330536,
      "learning_rate": 0.000173109243697479,
      "loss": 0.4229,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.1862211525440216,
      "learning_rate": 0.00017277310924369748,
      "loss": 0.3825,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.17853553593158722,
      "learning_rate": 0.00017243697478991598,
      "loss": 0.2856,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.1831968128681183,
      "learning_rate": 0.00017210084033613448,
      "loss": 0.365,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.15256951749324799,
      "learning_rate": 0.00017176470588235293,
      "loss": 0.3472,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.13528244197368622,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.2659,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.28682950139045715,
      "learning_rate": 0.00017109243697478992,
      "loss": 0.4076,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.16988743841648102,
      "learning_rate": 0.0001707563025210084,
      "loss": 0.3907,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.179747074842453,
      "learning_rate": 0.0001704201680672269,
      "loss": 0.3596,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.19822461903095245,
      "learning_rate": 0.0001700840336134454,
      "loss": 0.3882,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.13598406314849854,
      "learning_rate": 0.00016974789915966387,
      "loss": 0.3405,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.259380966424942,
      "learning_rate": 0.00016941176470588237,
      "loss": 0.3852,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.13041788339614868,
      "learning_rate": 0.00016907563025210084,
      "loss": 0.3357,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.09985513240098953,
      "learning_rate": 0.00016873949579831934,
      "loss": 0.3537,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.1210433691740036,
      "learning_rate": 0.00016840336134453784,
      "loss": 0.2558,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.23290787637233734,
      "learning_rate": 0.0001680672268907563,
      "loss": 0.4515,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.11966464668512344,
      "learning_rate": 0.0001677310924369748,
      "loss": 0.2995,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.1664268672466278,
      "learning_rate": 0.00016739495798319328,
      "loss": 0.3785,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.18880346417427063,
      "learning_rate": 0.00016705882352941178,
      "loss": 0.3725,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.12860730290412903,
      "learning_rate": 0.00016672268907563028,
      "loss": 0.2955,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.26269689202308655,
      "learning_rate": 0.00016638655462184875,
      "loss": 0.4499,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.14706847071647644,
      "learning_rate": 0.00016605042016806725,
      "loss": 0.3597,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.12669004499912262,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.4903,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.16930922865867615,
      "learning_rate": 0.0001653781512605042,
      "loss": 0.4234,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.18980653584003448,
      "learning_rate": 0.0001650420168067227,
      "loss": 0.3248,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.18840400874614716,
      "learning_rate": 0.0001647058823529412,
      "loss": 0.4099,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.15826371312141418,
      "learning_rate": 0.00016436974789915966,
      "loss": 0.3824,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.08255958557128906,
      "learning_rate": 0.00016403361344537816,
      "loss": 0.3165,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.11719492822885513,
      "learning_rate": 0.00016369747899159663,
      "loss": 0.3964,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.19307219982147217,
      "learning_rate": 0.00016336134453781513,
      "loss": 0.4308,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.20991674065589905,
      "learning_rate": 0.00016302521008403363,
      "loss": 0.3866,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.16034284234046936,
      "learning_rate": 0.0001626890756302521,
      "loss": 0.3751,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.1102844625711441,
      "learning_rate": 0.0001623529411764706,
      "loss": 0.3187,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.14199577271938324,
      "learning_rate": 0.00016201680672268907,
      "loss": 0.366,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.1400391161441803,
      "learning_rate": 0.00016168067226890757,
      "loss": 0.3375,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.08833246678113937,
      "learning_rate": 0.00016134453781512607,
      "loss": 0.301,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.15444695949554443,
      "learning_rate": 0.00016100840336134454,
      "loss": 0.3947,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.13776631653308868,
      "learning_rate": 0.00016067226890756304,
      "loss": 0.3557,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.14852550625801086,
      "learning_rate": 0.00016033613445378154,
      "loss": 0.3035,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.1471501737833023,
      "learning_rate": 0.00016,
      "loss": 0.3604,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.15211179852485657,
      "learning_rate": 0.0001596638655462185,
      "loss": 0.3483,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.10873676091432571,
      "learning_rate": 0.00015932773109243698,
      "loss": 0.3653,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.14498455822467804,
      "learning_rate": 0.00015899159663865546,
      "loss": 0.3212,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.14836572110652924,
      "learning_rate": 0.00015865546218487396,
      "loss": 0.2641,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.1737600862979889,
      "learning_rate": 0.00015831932773109243,
      "loss": 0.406,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.13045647740364075,
      "learning_rate": 0.00015798319327731093,
      "loss": 0.4271,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.2413131296634674,
      "learning_rate": 0.00015764705882352943,
      "loss": 0.4625,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.1322043389081955,
      "learning_rate": 0.0001573109243697479,
      "loss": 0.3691,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.08616301417350769,
      "learning_rate": 0.0001569747899159664,
      "loss": 0.3059,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.10121957957744598,
      "learning_rate": 0.00015663865546218487,
      "loss": 0.3118,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.12280059605836868,
      "learning_rate": 0.00015630252100840337,
      "loss": 0.3264,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.11323551088571548,
      "learning_rate": 0.00015596638655462187,
      "loss": 0.3104,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.1459842473268509,
      "learning_rate": 0.00015563025210084034,
      "loss": 0.3741,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.11072827875614166,
      "learning_rate": 0.00015529411764705884,
      "loss": 0.3927,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.19146105647087097,
      "learning_rate": 0.00015495798319327734,
      "loss": 0.3722,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.11631489545106888,
      "learning_rate": 0.0001546218487394958,
      "loss": 0.2825,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.10704005509614944,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3903,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.11832943558692932,
      "learning_rate": 0.00015394957983193278,
      "loss": 0.2843,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.14827676117420197,
      "learning_rate": 0.00015361344537815128,
      "loss": 0.3517,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.15449492633342743,
      "learning_rate": 0.00015327731092436978,
      "loss": 0.4158,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.14493128657341003,
      "learning_rate": 0.00015294117647058822,
      "loss": 0.3603,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.10763043910264969,
      "learning_rate": 0.00015260504201680672,
      "loss": 0.3947,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.12138542532920837,
      "learning_rate": 0.00015226890756302522,
      "loss": 0.3526,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.12744510173797607,
      "learning_rate": 0.0001519327731092437,
      "loss": 0.3187,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.15138036012649536,
      "learning_rate": 0.0001515966386554622,
      "loss": 0.3791,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.1440071314573288,
      "learning_rate": 0.00015126050420168066,
      "loss": 0.3809,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.12910088896751404,
      "learning_rate": 0.00015092436974789916,
      "loss": 0.3245,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.137859508395195,
      "learning_rate": 0.00015058823529411766,
      "loss": 0.3026,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.20582152903079987,
      "learning_rate": 0.00015025210084033613,
      "loss": 0.4597,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.20389969646930695,
      "learning_rate": 0.00014991596638655463,
      "loss": 0.3809,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.1316036731004715,
      "learning_rate": 0.00014957983193277313,
      "loss": 0.3372,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.15139207243919373,
      "learning_rate": 0.0001492436974789916,
      "loss": 0.383,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.1484660506248474,
      "learning_rate": 0.0001489075630252101,
      "loss": 0.3082,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.11766514927148819,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3347,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.12610439956188202,
      "learning_rate": 0.00014823529411764707,
      "loss": 0.2682,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.11963679641485214,
      "learning_rate": 0.00014789915966386557,
      "loss": 0.3532,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.12619924545288086,
      "learning_rate": 0.00014756302521008404,
      "loss": 0.3731,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.12076064944267273,
      "learning_rate": 0.00014722689075630254,
      "loss": 0.2924,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.08992743492126465,
      "learning_rate": 0.00014689075630252101,
      "loss": 0.2243,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.1388910412788391,
      "learning_rate": 0.0001465546218487395,
      "loss": 0.3797,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.1263900101184845,
      "learning_rate": 0.00014621848739495799,
      "loss": 0.3439,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.11395574361085892,
      "learning_rate": 0.00014588235294117646,
      "loss": 0.3284,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.150899738073349,
      "learning_rate": 0.00014554621848739496,
      "loss": 0.3281,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.12106195837259293,
      "learning_rate": 0.00014521008403361346,
      "loss": 0.3067,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.0990961566567421,
      "learning_rate": 0.00014487394957983193,
      "loss": 0.2794,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.10286998003721237,
      "learning_rate": 0.00014453781512605043,
      "loss": 0.3559,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.09361008554697037,
      "learning_rate": 0.00014420168067226893,
      "loss": 0.3256,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.1089647188782692,
      "learning_rate": 0.0001438655462184874,
      "loss": 0.2933,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.10971757769584656,
      "learning_rate": 0.0001435294117647059,
      "loss": 0.2633,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.13637380301952362,
      "learning_rate": 0.00014319327731092437,
      "loss": 0.3819,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.16428828239440918,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.4139,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.1138189360499382,
      "learning_rate": 0.00014252100840336137,
      "loss": 0.2892,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.1671767681837082,
      "learning_rate": 0.00014218487394957984,
      "loss": 0.2481,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.2530444264411926,
      "learning_rate": 0.00014184873949579834,
      "loss": 0.4018,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.13007375597953796,
      "learning_rate": 0.0001415126050420168,
      "loss": 0.3383,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.3057572841644287,
      "learning_rate": 0.0001411764705882353,
      "loss": 0.4714,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.18439891934394836,
      "learning_rate": 0.0001408403361344538,
      "loss": 0.4037,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.14923419058322906,
      "learning_rate": 0.00014050420168067225,
      "loss": 0.4106,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.1420719027519226,
      "learning_rate": 0.00014016806722689075,
      "loss": 0.3069,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.1353847086429596,
      "learning_rate": 0.00013983193277310925,
      "loss": 0.3091,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.13371318578720093,
      "learning_rate": 0.00013949579831932772,
      "loss": 0.2504,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.16323885321617126,
      "learning_rate": 0.00013915966386554622,
      "loss": 0.306,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.1323823630809784,
      "learning_rate": 0.00013882352941176472,
      "loss": 0.319,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.12363748252391815,
      "learning_rate": 0.0001384873949579832,
      "loss": 0.3275,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.14408983290195465,
      "learning_rate": 0.0001381512605042017,
      "loss": 0.3442,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.13170164823532104,
      "learning_rate": 0.00013781512605042016,
      "loss": 0.3075,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.09659533947706223,
      "learning_rate": 0.00013747899159663866,
      "loss": 0.2581,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.11089111119508743,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.3754,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.10389801114797592,
      "learning_rate": 0.00013680672268907563,
      "loss": 0.3656,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.11276721954345703,
      "learning_rate": 0.00013647058823529413,
      "loss": 0.3032,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.10916987806558609,
      "learning_rate": 0.0001361344537815126,
      "loss": 0.3219,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.18485985696315765,
      "learning_rate": 0.0001357983193277311,
      "loss": 0.3333,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.13199597597122192,
      "learning_rate": 0.0001354621848739496,
      "loss": 0.3281,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.1355859786272049,
      "learning_rate": 0.00013512605042016807,
      "loss": 0.4023,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.13998642563819885,
      "learning_rate": 0.00013478991596638657,
      "loss": 0.3574,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.0962975025177002,
      "learning_rate": 0.00013445378151260507,
      "loss": 0.2701,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.08774573355913162,
      "learning_rate": 0.00013411764705882352,
      "loss": 0.3479,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.15924006700515747,
      "learning_rate": 0.00013378151260504202,
      "loss": 0.3957,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.12797050178050995,
      "learning_rate": 0.00013344537815126052,
      "loss": 0.3412,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.14511704444885254,
      "learning_rate": 0.000133109243697479,
      "loss": 0.2846,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.09221897274255753,
      "learning_rate": 0.0001327731092436975,
      "loss": 0.2888,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.16869789361953735,
      "learning_rate": 0.00013243697478991596,
      "loss": 0.41,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.1326804906129837,
      "learning_rate": 0.00013210084033613446,
      "loss": 0.3176,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.13668572902679443,
      "learning_rate": 0.00013176470588235296,
      "loss": 0.2941,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.11957073211669922,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.2608,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.1145225241780281,
      "learning_rate": 0.00013109243697478993,
      "loss": 0.3487,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.13720549643039703,
      "learning_rate": 0.0001307563025210084,
      "loss": 0.3174,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.20660945773124695,
      "learning_rate": 0.0001304201680672269,
      "loss": 0.398,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.09794674068689346,
      "learning_rate": 0.0001300840336134454,
      "loss": 0.2632,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.13741888105869293,
      "learning_rate": 0.00012974789915966387,
      "loss": 0.2739,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.12882089614868164,
      "learning_rate": 0.00012941176470588237,
      "loss": 0.3222,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.08571291714906693,
      "learning_rate": 0.00012907563025210087,
      "loss": 0.2612,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.10409200936555862,
      "learning_rate": 0.00012873949579831934,
      "loss": 0.3366,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.09040453284978867,
      "learning_rate": 0.00012840336134453784,
      "loss": 0.3024,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.10907662659883499,
      "learning_rate": 0.0001280672268907563,
      "loss": 0.2794,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.1320391297340393,
      "learning_rate": 0.00012773109243697478,
      "loss": 0.3378,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.09124366194009781,
      "learning_rate": 0.00012739495798319328,
      "loss": 0.3334,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.08708720654249191,
      "learning_rate": 0.00012705882352941175,
      "loss": 0.3112,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.08096510916948318,
      "learning_rate": 0.00012672268907563025,
      "loss": 0.3471,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.12317626178264618,
      "learning_rate": 0.00012638655462184875,
      "loss": 0.3713,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.13556599617004395,
      "learning_rate": 0.00012605042016806722,
      "loss": 0.3634,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.10459845513105392,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.2505,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.16166602075099945,
      "learning_rate": 0.0001253781512605042,
      "loss": 0.3386,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.16363009810447693,
      "learning_rate": 0.0001250420168067227,
      "loss": 0.4435,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.1284928172826767,
      "learning_rate": 0.0001247058823529412,
      "loss": 0.3142,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.08927808701992035,
      "learning_rate": 0.00012436974789915966,
      "loss": 0.3066,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.10749734938144684,
      "learning_rate": 0.00012403361344537816,
      "loss": 0.2897,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.11068788915872574,
      "learning_rate": 0.00012369747899159666,
      "loss": 0.3099,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.1265253871679306,
      "learning_rate": 0.00012336134453781513,
      "loss": 0.3572,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.18937262892723083,
      "learning_rate": 0.00012302521008403363,
      "loss": 0.4505,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.062203001230955124,
      "learning_rate": 0.0001226890756302521,
      "loss": 0.2679,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.11793375760316849,
      "learning_rate": 0.0001223529411764706,
      "loss": 0.366,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.11372873187065125,
      "learning_rate": 0.00012201680672268909,
      "loss": 0.3542,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.12156499922275543,
      "learning_rate": 0.00012168067226890756,
      "loss": 0.3176,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.13760161399841309,
      "learning_rate": 0.00012134453781512605,
      "loss": 0.2928,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.12507155537605286,
      "learning_rate": 0.00012100840336134453,
      "loss": 0.2899,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.2048012614250183,
      "learning_rate": 0.00012067226890756302,
      "loss": 0.4727,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.10739456862211227,
      "learning_rate": 0.00012033613445378152,
      "loss": 0.317,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.13229729235172272,
      "learning_rate": 0.00012,
      "loss": 0.3058,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.180500328540802,
      "learning_rate": 0.00011966386554621849,
      "loss": 0.3724,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.10554136335849762,
      "learning_rate": 0.00011932773109243697,
      "loss": 0.3059,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.16613169014453888,
      "learning_rate": 0.00011899159663865547,
      "loss": 0.4431,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.15018242597579956,
      "learning_rate": 0.00011865546218487396,
      "loss": 0.4226,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.1302986443042755,
      "learning_rate": 0.00011831932773109244,
      "loss": 0.3852,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.1614023745059967,
      "learning_rate": 0.00011798319327731093,
      "loss": 0.4554,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.1569850742816925,
      "learning_rate": 0.00011764705882352942,
      "loss": 0.395,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.15548023581504822,
      "learning_rate": 0.00011731092436974791,
      "loss": 0.2843,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.1349761188030243,
      "learning_rate": 0.0001169747899159664,
      "loss": 0.3672,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.14136190712451935,
      "learning_rate": 0.00011663865546218489,
      "loss": 0.3295,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.1916339099407196,
      "learning_rate": 0.00011630252100840337,
      "loss": 0.3654,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.20819373428821564,
      "learning_rate": 0.00011596638655462187,
      "loss": 0.3159,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.14098067581653595,
      "learning_rate": 0.00011563025210084036,
      "loss": 0.3062,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.1169615387916565,
      "learning_rate": 0.00011529411764705881,
      "loss": 0.3236,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.11915284395217896,
      "learning_rate": 0.00011495798319327731,
      "loss": 0.3362,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.14119099080562592,
      "learning_rate": 0.0001146218487394958,
      "loss": 0.3845,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.09672906994819641,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.2949,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.11942458897829056,
      "learning_rate": 0.00011394957983193277,
      "loss": 0.315,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.11637287586927414,
      "learning_rate": 0.00011361344537815127,
      "loss": 0.2782,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.17792975902557373,
      "learning_rate": 0.00011327731092436975,
      "loss": 0.3473,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.11681767553091049,
      "learning_rate": 0.00011294117647058824,
      "loss": 0.3548,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.12649434804916382,
      "learning_rate": 0.00011260504201680672,
      "loss": 0.3856,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.16143733263015747,
      "learning_rate": 0.00011226890756302521,
      "loss": 0.4087,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.16897828876972198,
      "learning_rate": 0.00011193277310924371,
      "loss": 0.3654,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.12101007997989655,
      "learning_rate": 0.0001115966386554622,
      "loss": 0.3162,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.1182578057050705,
      "learning_rate": 0.00011126050420168068,
      "loss": 0.2862,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.19354991614818573,
      "learning_rate": 0.00011092436974789917,
      "loss": 0.3679,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.16989506781101227,
      "learning_rate": 0.00011058823529411766,
      "loss": 0.3629,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.15549544990062714,
      "learning_rate": 0.00011025210084033615,
      "loss": 0.4643,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.11974342912435532,
      "learning_rate": 0.00010991596638655464,
      "loss": 0.3652,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.13562320172786713,
      "learning_rate": 0.00010957983193277312,
      "loss": 0.3495,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.16524024307727814,
      "learning_rate": 0.00010924369747899159,
      "loss": 0.4051,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.14718127250671387,
      "learning_rate": 0.00010890756302521008,
      "loss": 0.2975,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.11973083019256592,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.3422,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.14048489928245544,
      "learning_rate": 0.00010823529411764706,
      "loss": 0.3439,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.08022641390562057,
      "learning_rate": 0.00010789915966386555,
      "loss": 0.32,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.10361675918102264,
      "learning_rate": 0.00010756302521008403,
      "loss": 0.3117,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.12670700252056122,
      "learning_rate": 0.00010722689075630252,
      "loss": 0.3235,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.17819327116012573,
      "learning_rate": 0.000106890756302521,
      "loss": 0.3276,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.13250084221363068,
      "learning_rate": 0.0001065546218487395,
      "loss": 0.3773,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.1128392219543457,
      "learning_rate": 0.00010621848739495799,
      "loss": 0.4155,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.12015292048454285,
      "learning_rate": 0.00010588235294117647,
      "loss": 0.386,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.14978145062923431,
      "learning_rate": 0.00010554621848739496,
      "loss": 0.3692,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.1678314059972763,
      "learning_rate": 0.00010521008403361346,
      "loss": 0.4079,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.12566807866096497,
      "learning_rate": 0.00010487394957983194,
      "loss": 0.4829,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.1037873774766922,
      "learning_rate": 0.00010453781512605043,
      "loss": 0.3683,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.18455375730991364,
      "learning_rate": 0.00010420168067226892,
      "loss": 0.4665,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.11664123088121414,
      "learning_rate": 0.00010386554621848741,
      "loss": 0.3865,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.09979666769504547,
      "learning_rate": 0.0001035294117647059,
      "loss": 0.3489,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.1403149515390396,
      "learning_rate": 0.00010319327731092439,
      "loss": 0.4252,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.12225803732872009,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.3469,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.18264876306056976,
      "learning_rate": 0.00010252100840336134,
      "loss": 0.4704,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.09047933667898178,
      "learning_rate": 0.00010218487394957983,
      "loss": 0.4157,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.07575677335262299,
      "learning_rate": 0.00010184873949579831,
      "loss": 0.2675,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.09827939420938492,
      "learning_rate": 0.0001015126050420168,
      "loss": 0.2815,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.12948618829250336,
      "learning_rate": 0.0001011764705882353,
      "loss": 0.3684,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.11350984871387482,
      "learning_rate": 0.00010084033613445378,
      "loss": 0.3051,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.1417248249053955,
      "learning_rate": 0.00010050420168067227,
      "loss": 0.4167,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.1149488165974617,
      "learning_rate": 0.00010016806722689076,
      "loss": 0.3044,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.10077805072069168,
      "learning_rate": 9.983193277310925e-05,
      "loss": 0.2799,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.12139570713043213,
      "learning_rate": 9.949579831932774e-05,
      "loss": 0.3847,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.13837860524654388,
      "learning_rate": 9.915966386554623e-05,
      "loss": 0.4445,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.10939938575029373,
      "learning_rate": 9.882352941176471e-05,
      "loss": 0.3236,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.10013436526060104,
      "learning_rate": 9.848739495798321e-05,
      "loss": 0.2841,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.13061176240444183,
      "learning_rate": 9.815126050420168e-05,
      "loss": 0.2891,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.10955072939395905,
      "learning_rate": 9.781512605042017e-05,
      "loss": 0.3166,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.10593509674072266,
      "learning_rate": 9.747899159663865e-05,
      "loss": 0.3803,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.1227491945028305,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.4079,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.1276005059480667,
      "learning_rate": 9.680672268907564e-05,
      "loss": 0.4004,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.10220682621002197,
      "learning_rate": 9.647058823529412e-05,
      "loss": 0.3738,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.1052403599023819,
      "learning_rate": 9.613445378151261e-05,
      "loss": 0.4078,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.0826040655374527,
      "learning_rate": 9.579831932773111e-05,
      "loss": 0.2562,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.10370852798223495,
      "learning_rate": 9.546218487394959e-05,
      "loss": 0.4253,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.08919474482536316,
      "learning_rate": 9.512605042016806e-05,
      "loss": 0.3447,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.12922786176204681,
      "learning_rate": 9.478991596638655e-05,
      "loss": 0.4145,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.10960955172777176,
      "learning_rate": 9.445378151260505e-05,
      "loss": 0.2636,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.1628570705652237,
      "learning_rate": 9.411764705882353e-05,
      "loss": 0.3882,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.10847502201795578,
      "learning_rate": 9.378151260504202e-05,
      "loss": 0.2965,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.10454510897397995,
      "learning_rate": 9.34453781512605e-05,
      "loss": 0.3785,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.08569291979074478,
      "learning_rate": 9.3109243697479e-05,
      "loss": 0.2775,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.1522148698568344,
      "learning_rate": 9.277310924369749e-05,
      "loss": 0.3882,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.09580136090517044,
      "learning_rate": 9.243697478991598e-05,
      "loss": 0.3249,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.12600049376487732,
      "learning_rate": 9.210084033613445e-05,
      "loss": 0.3506,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.0768590122461319,
      "learning_rate": 9.176470588235295e-05,
      "loss": 0.292,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.10959033668041229,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.3776,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.11495716124773026,
      "learning_rate": 9.109243697478992e-05,
      "loss": 0.3755,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.07171259075403214,
      "learning_rate": 9.07563025210084e-05,
      "loss": 0.2135,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.11308588832616806,
      "learning_rate": 9.04201680672269e-05,
      "loss": 0.3474,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.08264511823654175,
      "learning_rate": 9.008403361344539e-05,
      "loss": 0.326,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.09971965849399567,
      "learning_rate": 8.974789915966387e-05,
      "loss": 0.3796,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.1448274403810501,
      "learning_rate": 8.941176470588236e-05,
      "loss": 0.3976,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.10503078997135162,
      "learning_rate": 8.907563025210084e-05,
      "loss": 0.3438,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.056681107729673386,
      "learning_rate": 8.873949579831933e-05,
      "loss": 0.2323,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.07996407151222229,
      "learning_rate": 8.840336134453782e-05,
      "loss": 0.3239,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.1481596678495407,
      "learning_rate": 8.80672268907563e-05,
      "loss": 0.3381,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.10167090594768524,
      "learning_rate": 8.77310924369748e-05,
      "loss": 0.2788,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.07799839228391647,
      "learning_rate": 8.739495798319329e-05,
      "loss": 0.3172,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.11156558245420456,
      "learning_rate": 8.705882352941177e-05,
      "loss": 0.3145,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.13759087026119232,
      "learning_rate": 8.672268907563026e-05,
      "loss": 0.385,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.12283437699079514,
      "learning_rate": 8.638655462184874e-05,
      "loss": 0.3617,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.08797206729650497,
      "learning_rate": 8.605042016806724e-05,
      "loss": 0.336,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.1120632216334343,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.4304,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.08397400379180908,
      "learning_rate": 8.53781512605042e-05,
      "loss": 0.2905,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.09093499928712845,
      "learning_rate": 8.50420168067227e-05,
      "loss": 0.3293,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.19216686487197876,
      "learning_rate": 8.470588235294118e-05,
      "loss": 0.3771,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.09882060438394547,
      "learning_rate": 8.436974789915967e-05,
      "loss": 0.4003,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.12947696447372437,
      "learning_rate": 8.403361344537815e-05,
      "loss": 0.3365,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.08534395694732666,
      "learning_rate": 8.369747899159664e-05,
      "loss": 0.2862,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.08245596289634705,
      "learning_rate": 8.336134453781514e-05,
      "loss": 0.3006,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.1203647032380104,
      "learning_rate": 8.302521008403362e-05,
      "loss": 0.3995,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.1005585566163063,
      "learning_rate": 8.26890756302521e-05,
      "loss": 0.2936,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.13701477646827698,
      "learning_rate": 8.23529411764706e-05,
      "loss": 0.4031,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.1039593294262886,
      "learning_rate": 8.201680672268908e-05,
      "loss": 0.3185,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.09403844177722931,
      "learning_rate": 8.168067226890757e-05,
      "loss": 0.272,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.10521123558282852,
      "learning_rate": 8.134453781512605e-05,
      "loss": 0.3546,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.0988626554608345,
      "learning_rate": 8.100840336134454e-05,
      "loss": 0.3896,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.08429711312055588,
      "learning_rate": 8.067226890756304e-05,
      "loss": 0.352,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.0950467512011528,
      "learning_rate": 8.033613445378152e-05,
      "loss": 0.3133,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.15286540985107422,
      "learning_rate": 8e-05,
      "loss": 0.3833,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.0968131422996521,
      "learning_rate": 7.966386554621849e-05,
      "loss": 0.2947,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.13076071441173553,
      "learning_rate": 7.932773109243698e-05,
      "loss": 0.2763,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.1694665551185608,
      "learning_rate": 7.899159663865546e-05,
      "loss": 0.4392,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.11438877880573273,
      "learning_rate": 7.865546218487395e-05,
      "loss": 0.3188,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.11193592101335526,
      "learning_rate": 7.831932773109243e-05,
      "loss": 0.3575,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.11389893293380737,
      "learning_rate": 7.798319327731093e-05,
      "loss": 0.4029,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.10082867741584778,
      "learning_rate": 7.764705882352942e-05,
      "loss": 0.3212,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.09000629931688309,
      "learning_rate": 7.73109243697479e-05,
      "loss": 0.2982,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.10828367620706558,
      "learning_rate": 7.697478991596639e-05,
      "loss": 0.3477,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.10433386266231537,
      "learning_rate": 7.663865546218489e-05,
      "loss": 0.3394,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.11150062829256058,
      "learning_rate": 7.630252100840336e-05,
      "loss": 0.3993,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.10840420424938202,
      "learning_rate": 7.596638655462185e-05,
      "loss": 0.4004,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.11551884561777115,
      "learning_rate": 7.563025210084033e-05,
      "loss": 0.2781,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.13691167533397675,
      "learning_rate": 7.529411764705883e-05,
      "loss": 0.3969,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.12778183817863464,
      "learning_rate": 7.495798319327732e-05,
      "loss": 0.4112,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.08980514109134674,
      "learning_rate": 7.46218487394958e-05,
      "loss": 0.2812,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.15179802477359772,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.3929,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.11207204312086105,
      "learning_rate": 7.394957983193279e-05,
      "loss": 0.3103,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.0928298756480217,
      "learning_rate": 7.361344537815127e-05,
      "loss": 0.2868,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.08236096054315567,
      "learning_rate": 7.327731092436974e-05,
      "loss": 0.3008,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.18606726825237274,
      "learning_rate": 7.294117647058823e-05,
      "loss": 0.3841,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.18041417002677917,
      "learning_rate": 7.260504201680673e-05,
      "loss": 0.4585,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.14742067456245422,
      "learning_rate": 7.226890756302521e-05,
      "loss": 0.4036,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.14400257170200348,
      "learning_rate": 7.19327731092437e-05,
      "loss": 0.3872,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.12637574970722198,
      "learning_rate": 7.159663865546218e-05,
      "loss": 0.3165,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.11668704450130463,
      "learning_rate": 7.126050420168068e-05,
      "loss": 0.2653,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.12285971641540527,
      "learning_rate": 7.092436974789917e-05,
      "loss": 0.3289,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.11196926981210709,
      "learning_rate": 7.058823529411765e-05,
      "loss": 0.3095,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.09041347354650497,
      "learning_rate": 7.025210084033613e-05,
      "loss": 0.2984,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.10527332872152328,
      "learning_rate": 6.991596638655463e-05,
      "loss": 0.3025,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.16055677831172943,
      "learning_rate": 6.957983193277311e-05,
      "loss": 0.2858,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.1576845496892929,
      "learning_rate": 6.92436974789916e-05,
      "loss": 0.4683,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.1144046038389206,
      "learning_rate": 6.890756302521008e-05,
      "loss": 0.39,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.10743904113769531,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.3638,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.10835056751966476,
      "learning_rate": 6.823529411764707e-05,
      "loss": 0.3251,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.09722775965929031,
      "learning_rate": 6.789915966386555e-05,
      "loss": 0.276,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.09646805375814438,
      "learning_rate": 6.756302521008404e-05,
      "loss": 0.3524,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.13759803771972656,
      "learning_rate": 6.722689075630254e-05,
      "loss": 0.3842,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.10092216730117798,
      "learning_rate": 6.689075630252101e-05,
      "loss": 0.3735,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.16481654345989227,
      "learning_rate": 6.65546218487395e-05,
      "loss": 0.3389,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.09888104349374771,
      "learning_rate": 6.621848739495798e-05,
      "loss": 0.2689,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.12685032188892365,
      "learning_rate": 6.588235294117648e-05,
      "loss": 0.3007,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.0978441834449768,
      "learning_rate": 6.554621848739496e-05,
      "loss": 0.3359,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.08911416679620743,
      "learning_rate": 6.521008403361345e-05,
      "loss": 0.2976,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.13985969126224518,
      "learning_rate": 6.487394957983193e-05,
      "loss": 0.4758,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.12232298403978348,
      "learning_rate": 6.453781512605043e-05,
      "loss": 0.2863,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.1309228241443634,
      "learning_rate": 6.420168067226892e-05,
      "loss": 0.381,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.11910846829414368,
      "learning_rate": 6.386554621848739e-05,
      "loss": 0.3986,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.0891137346625328,
      "learning_rate": 6.352941176470588e-05,
      "loss": 0.283,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.08935544639825821,
      "learning_rate": 6.319327731092438e-05,
      "loss": 0.3044,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.14007098972797394,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.3481,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.07649523764848709,
      "learning_rate": 6.252100840336135e-05,
      "loss": 0.3448,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.05483785644173622,
      "learning_rate": 6.218487394957983e-05,
      "loss": 0.1946,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.11057504266500473,
      "learning_rate": 6.184873949579833e-05,
      "loss": 0.3698,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.1595994085073471,
      "learning_rate": 6.151260504201682e-05,
      "loss": 0.3581,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.1078917384147644,
      "learning_rate": 6.11764705882353e-05,
      "loss": 0.3889,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.1057991236448288,
      "learning_rate": 6.084033613445378e-05,
      "loss": 0.3193,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.09442824125289917,
      "learning_rate": 6.0504201680672267e-05,
      "loss": 0.3084,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.10170813649892807,
      "learning_rate": 6.016806722689076e-05,
      "loss": 0.438,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.09551827609539032,
      "learning_rate": 5.9831932773109244e-05,
      "loss": 0.3636,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.10653682053089142,
      "learning_rate": 5.9495798319327737e-05,
      "loss": 0.3589,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.12225319445133209,
      "learning_rate": 5.915966386554622e-05,
      "loss": 0.4013,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.09497038275003433,
      "learning_rate": 5.882352941176471e-05,
      "loss": 0.3653,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.10578107833862305,
      "learning_rate": 5.84873949579832e-05,
      "loss": 0.3134,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.07702214270830154,
      "learning_rate": 5.8151260504201685e-05,
      "loss": 0.2675,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.10164794325828552,
      "learning_rate": 5.781512605042018e-05,
      "loss": 0.3366,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.0921027660369873,
      "learning_rate": 5.7478991596638656e-05,
      "loss": 0.3103,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.12087923288345337,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.3442,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.09149736166000366,
      "learning_rate": 5.6806722689075634e-05,
      "loss": 0.3288,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.15063077211380005,
      "learning_rate": 5.647058823529412e-05,
      "loss": 0.4035,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.12460055947303772,
      "learning_rate": 5.6134453781512605e-05,
      "loss": 0.288,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.10340480506420135,
      "learning_rate": 5.57983193277311e-05,
      "loss": 0.332,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.10224959999322891,
      "learning_rate": 5.546218487394958e-05,
      "loss": 0.3155,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.08550440520048141,
      "learning_rate": 5.5126050420168075e-05,
      "loss": 0.3167,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.0953272208571434,
      "learning_rate": 5.478991596638656e-05,
      "loss": 0.342,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.09616520255804062,
      "learning_rate": 5.445378151260504e-05,
      "loss": 0.3178,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.08046571165323257,
      "learning_rate": 5.411764705882353e-05,
      "loss": 0.353,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.06258095055818558,
      "learning_rate": 5.378151260504202e-05,
      "loss": 0.2698,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.19148798286914825,
      "learning_rate": 5.34453781512605e-05,
      "loss": 0.4071,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.10188302397727966,
      "learning_rate": 5.3109243697478995e-05,
      "loss": 0.3256,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.12908720970153809,
      "learning_rate": 5.277310924369748e-05,
      "loss": 0.2518,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.09061878174543381,
      "learning_rate": 5.243697478991597e-05,
      "loss": 0.2453,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.18224690854549408,
      "learning_rate": 5.210084033613446e-05,
      "loss": 0.4412,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.12186053395271301,
      "learning_rate": 5.176470588235295e-05,
      "loss": 0.3525,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.11850640177726746,
      "learning_rate": 5.142857142857143e-05,
      "loss": 0.3425,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.09012561291456223,
      "learning_rate": 5.1092436974789914e-05,
      "loss": 0.351,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.0900406762957573,
      "learning_rate": 5.07563025210084e-05,
      "loss": 0.2414,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.10211767256259918,
      "learning_rate": 5.042016806722689e-05,
      "loss": 0.3335,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.13130278885364532,
      "learning_rate": 5.008403361344538e-05,
      "loss": 0.3775,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.11433355510234833,
      "learning_rate": 4.974789915966387e-05,
      "loss": 0.2895,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.09765345603227615,
      "learning_rate": 4.9411764705882355e-05,
      "loss": 0.2518,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.08363477140665054,
      "learning_rate": 4.907563025210084e-05,
      "loss": 0.3905,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.12584145367145538,
      "learning_rate": 4.8739495798319326e-05,
      "loss": 0.3077,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.08727627247571945,
      "learning_rate": 4.840336134453782e-05,
      "loss": 0.2836,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.13647791743278503,
      "learning_rate": 4.8067226890756304e-05,
      "loss": 0.4053,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.10143575817346573,
      "learning_rate": 4.7731092436974796e-05,
      "loss": 0.3589,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.06846851855516434,
      "learning_rate": 4.7394957983193275e-05,
      "loss": 0.294,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.08480772376060486,
      "learning_rate": 4.705882352941177e-05,
      "loss": 0.2955,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.11227374523878098,
      "learning_rate": 4.672268907563025e-05,
      "loss": 0.3708,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.08493714779615402,
      "learning_rate": 4.6386554621848745e-05,
      "loss": 0.2914,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.08833222091197968,
      "learning_rate": 4.6050420168067224e-05,
      "loss": 0.3187,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.1488715261220932,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.4242,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.12306097149848938,
      "learning_rate": 4.53781512605042e-05,
      "loss": 0.3354,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.07416713237762451,
      "learning_rate": 4.5042016806722694e-05,
      "loss": 0.2628,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.09349291771650314,
      "learning_rate": 4.470588235294118e-05,
      "loss": 0.2933,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.09180624783039093,
      "learning_rate": 4.4369747899159665e-05,
      "loss": 0.3598,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.16108456254005432,
      "learning_rate": 4.403361344537815e-05,
      "loss": 0.3659,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.07473412156105042,
      "learning_rate": 4.369747899159664e-05,
      "loss": 0.3092,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.07307972013950348,
      "learning_rate": 4.336134453781513e-05,
      "loss": 0.2687,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.07967942953109741,
      "learning_rate": 4.302521008403362e-05,
      "loss": 0.3046,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.09844189137220383,
      "learning_rate": 4.26890756302521e-05,
      "loss": 0.3259,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.1430872678756714,
      "learning_rate": 4.235294117647059e-05,
      "loss": 0.3872,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.11556772887706757,
      "learning_rate": 4.201680672268908e-05,
      "loss": 0.394,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.10139476507902145,
      "learning_rate": 4.168067226890757e-05,
      "loss": 0.3262,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.16265824437141418,
      "learning_rate": 4.134453781512605e-05,
      "loss": 0.4026,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.10993698239326477,
      "learning_rate": 4.100840336134454e-05,
      "loss": 0.2925,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.1341819018125534,
      "learning_rate": 4.0672268907563026e-05,
      "loss": 0.3515,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.11913276463747025,
      "learning_rate": 4.033613445378152e-05,
      "loss": 0.4143,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.11281388252973557,
      "learning_rate": 4e-05,
      "loss": 0.4222,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.07794380933046341,
      "learning_rate": 3.966386554621849e-05,
      "loss": 0.2445,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.0729835256934166,
      "learning_rate": 3.9327731092436974e-05,
      "loss": 0.2762,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.11555670201778412,
      "learning_rate": 3.8991596638655467e-05,
      "loss": 0.3701,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.09313695132732391,
      "learning_rate": 3.865546218487395e-05,
      "loss": 0.3652,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.0830584466457367,
      "learning_rate": 3.8319327731092444e-05,
      "loss": 0.3281,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.08743768185377121,
      "learning_rate": 3.798319327731092e-05,
      "loss": 0.2622,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.07800552994012833,
      "learning_rate": 3.7647058823529415e-05,
      "loss": 0.3074,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.10117367655038834,
      "learning_rate": 3.73109243697479e-05,
      "loss": 0.3592,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.07467978447675705,
      "learning_rate": 3.697478991596639e-05,
      "loss": 0.2773,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.08551692217588425,
      "learning_rate": 3.663865546218487e-05,
      "loss": 0.3093,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.08348270505666733,
      "learning_rate": 3.6302521008403364e-05,
      "loss": 0.3231,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.07651393860578537,
      "learning_rate": 3.596638655462185e-05,
      "loss": 0.3034,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.10448411852121353,
      "learning_rate": 3.563025210084034e-05,
      "loss": 0.347,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.10084354132413864,
      "learning_rate": 3.529411764705883e-05,
      "loss": 0.3486,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.07860083132982254,
      "learning_rate": 3.495798319327731e-05,
      "loss": 0.2868,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.09833862632513046,
      "learning_rate": 3.46218487394958e-05,
      "loss": 0.318,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.12835939228534698,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.3488,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.07150580734014511,
      "learning_rate": 3.3949579831932776e-05,
      "loss": 0.2792,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.10504882782697678,
      "learning_rate": 3.361344537815127e-05,
      "loss": 0.3012,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 600,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6425543847874396e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
