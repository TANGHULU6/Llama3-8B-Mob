{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8333333333333334,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 1.4054195880889893,
      "learning_rate": 4e-05,
      "loss": 1.5417,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 1.555207371711731,
      "learning_rate": 8e-05,
      "loss": 1.456,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 1.583700180053711,
      "learning_rate": 0.00012,
      "loss": 1.4951,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.5464413166046143,
      "learning_rate": 0.00016,
      "loss": 1.3649,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4387598037719727,
      "learning_rate": 0.0002,
      "loss": 1.1689,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.2917715311050415,
      "learning_rate": 0.0001996638655462185,
      "loss": 0.8795,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 2.9485769271850586,
      "learning_rate": 0.00019932773109243698,
      "loss": 0.7246,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 4.266175746917725,
      "learning_rate": 0.00019899159663865548,
      "loss": 0.8987,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.9743868112564087,
      "learning_rate": 0.00019865546218487395,
      "loss": 0.6093,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.7634749412536621,
      "learning_rate": 0.00019831932773109245,
      "loss": 0.715,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.5358420610427856,
      "learning_rate": 0.00019798319327731095,
      "loss": 0.5139,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.3020939528942108,
      "learning_rate": 0.00019764705882352942,
      "loss": 0.3916,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.22278816998004913,
      "learning_rate": 0.00019731092436974792,
      "loss": 0.4379,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.22447381913661957,
      "learning_rate": 0.00019697478991596642,
      "loss": 0.5664,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.1617433726787567,
      "learning_rate": 0.00019663865546218486,
      "loss": 0.4026,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.26861968636512756,
      "learning_rate": 0.00019630252100840336,
      "loss": 0.5816,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.1962285041809082,
      "learning_rate": 0.00019596638655462186,
      "loss": 0.4217,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.1996934562921524,
      "learning_rate": 0.00019563025210084033,
      "loss": 0.4072,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.144467294216156,
      "learning_rate": 0.00019529411764705883,
      "loss": 0.4425,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.1699308156967163,
      "learning_rate": 0.0001949579831932773,
      "loss": 0.5102,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2577715814113617,
      "learning_rate": 0.0001946218487394958,
      "loss": 0.49,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.15910933911800385,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.4656,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.10941454768180847,
      "learning_rate": 0.00019394957983193278,
      "loss": 0.3699,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.12407840043306351,
      "learning_rate": 0.00019361344537815127,
      "loss": 0.3996,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.15468482673168182,
      "learning_rate": 0.00019327731092436975,
      "loss": 0.4058,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.16952231526374817,
      "learning_rate": 0.00019294117647058825,
      "loss": 0.4833,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.1182880625128746,
      "learning_rate": 0.00019260504201680674,
      "loss": 0.4123,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.11270896345376968,
      "learning_rate": 0.00019226890756302522,
      "loss": 0.4333,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.08923894166946411,
      "learning_rate": 0.00019193277310924372,
      "loss": 0.3699,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.11360733956098557,
      "learning_rate": 0.00019159663865546221,
      "loss": 0.3761,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.1734744757413864,
      "learning_rate": 0.0001912605042016807,
      "loss": 0.4353,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.09268668293952942,
      "learning_rate": 0.00019092436974789919,
      "loss": 0.462,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.0803622305393219,
      "learning_rate": 0.00019058823529411766,
      "loss": 0.2615,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.10047028958797455,
      "learning_rate": 0.00019025210084033613,
      "loss": 0.3894,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.12204628437757492,
      "learning_rate": 0.00018991596638655463,
      "loss": 0.37,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.09615829586982727,
      "learning_rate": 0.0001895798319327731,
      "loss": 0.4082,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.07114240527153015,
      "learning_rate": 0.0001892436974789916,
      "loss": 0.2888,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.07574242353439331,
      "learning_rate": 0.0001889075630252101,
      "loss": 0.3301,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.08202502131462097,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3303,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.07649857550859451,
      "learning_rate": 0.00018823529411764707,
      "loss": 0.3126,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.06298894435167313,
      "learning_rate": 0.00018789915966386554,
      "loss": 0.3644,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.06337515264749527,
      "learning_rate": 0.00018756302521008404,
      "loss": 0.337,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.06717579811811447,
      "learning_rate": 0.00018722689075630254,
      "loss": 0.3575,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.07841004431247711,
      "learning_rate": 0.000186890756302521,
      "loss": 0.4102,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.06586755067110062,
      "learning_rate": 0.0001865546218487395,
      "loss": 0.4096,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.07020412385463715,
      "learning_rate": 0.000186218487394958,
      "loss": 0.3107,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.07130230963230133,
      "learning_rate": 0.00018588235294117648,
      "loss": 0.3906,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.06041331961750984,
      "learning_rate": 0.00018554621848739498,
      "loss": 0.2874,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.06462864577770233,
      "learning_rate": 0.00018521008403361345,
      "loss": 0.3772,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.0742872878909111,
      "learning_rate": 0.00018487394957983195,
      "loss": 0.3493,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.09004637598991394,
      "learning_rate": 0.00018453781512605045,
      "loss": 0.365,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.12043625116348267,
      "learning_rate": 0.0001842016806722689,
      "loss": 0.4088,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.07429993897676468,
      "learning_rate": 0.0001838655462184874,
      "loss": 0.3508,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.09804980456829071,
      "learning_rate": 0.0001835294117647059,
      "loss": 0.4197,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.07881782948970795,
      "learning_rate": 0.00018319327731092437,
      "loss": 0.2969,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.095767080783844,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.3415,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.08579565584659576,
      "learning_rate": 0.00018252100840336134,
      "loss": 0.3769,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.11313845217227936,
      "learning_rate": 0.00018218487394957984,
      "loss": 0.4043,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.06345517188310623,
      "learning_rate": 0.00018184873949579833,
      "loss": 0.2977,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.06694050133228302,
      "learning_rate": 0.0001815126050420168,
      "loss": 0.3751,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.05596921220421791,
      "learning_rate": 0.0001811764705882353,
      "loss": 0.261,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.06289839744567871,
      "learning_rate": 0.0001808403361344538,
      "loss": 0.388,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06636751443147659,
      "learning_rate": 0.00018050420168067228,
      "loss": 0.3676,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.05795130506157875,
      "learning_rate": 0.00018016806722689078,
      "loss": 0.3278,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.06594518572092056,
      "learning_rate": 0.00017983193277310925,
      "loss": 0.3381,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.08598905801773071,
      "learning_rate": 0.00017949579831932775,
      "loss": 0.3308,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.05096037685871124,
      "learning_rate": 0.00017915966386554625,
      "loss": 0.2995,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.07714802026748657,
      "learning_rate": 0.00017882352941176472,
      "loss": 0.2915,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.08716674894094467,
      "learning_rate": 0.00017848739495798322,
      "loss": 0.3873,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.07820872962474823,
      "learning_rate": 0.0001781512605042017,
      "loss": 0.372,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.07242529839277267,
      "learning_rate": 0.00017781512605042016,
      "loss": 0.3141,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.060456015169620514,
      "learning_rate": 0.00017747899159663866,
      "loss": 0.3352,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.05380423367023468,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.12327950447797775,
      "learning_rate": 0.00017680672268907563,
      "loss": 0.3818,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.0510326512157917,
      "learning_rate": 0.00017647058823529413,
      "loss": 0.3252,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.10206542164087296,
      "learning_rate": 0.0001761344537815126,
      "loss": 0.3891,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.07850395143032074,
      "learning_rate": 0.0001757983193277311,
      "loss": 0.3443,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.07182018458843231,
      "learning_rate": 0.0001754621848739496,
      "loss": 0.3206,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.0670001283288002,
      "learning_rate": 0.00017512605042016807,
      "loss": 0.3794,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.051259152591228485,
      "learning_rate": 0.00017478991596638657,
      "loss": 0.3286,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.06767262518405914,
      "learning_rate": 0.00017445378151260504,
      "loss": 0.3443,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.08190235495567322,
      "learning_rate": 0.00017411764705882354,
      "loss": 0.3439,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.04378005862236023,
      "learning_rate": 0.00017378151260504204,
      "loss": 0.2588,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.06052009016275406,
      "learning_rate": 0.0001734453781512605,
      "loss": 0.3704,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.0639125257730484,
      "learning_rate": 0.000173109243697479,
      "loss": 0.3785,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.05897470936179161,
      "learning_rate": 0.00017277310924369748,
      "loss": 0.3416,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.06982028484344482,
      "learning_rate": 0.00017243697478991598,
      "loss": 0.2583,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.06428564339876175,
      "learning_rate": 0.00017210084033613448,
      "loss": 0.3358,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.06795814633369446,
      "learning_rate": 0.00017176470588235293,
      "loss": 0.3178,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.05635474994778633,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.2451,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.10237409174442291,
      "learning_rate": 0.00017109243697478992,
      "loss": 0.3666,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.06739139556884766,
      "learning_rate": 0.0001707563025210084,
      "loss": 0.3596,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.07285594195127487,
      "learning_rate": 0.0001704201680672269,
      "loss": 0.3316,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.07522281259298325,
      "learning_rate": 0.0001700840336134454,
      "loss": 0.3602,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.06685081869363785,
      "learning_rate": 0.00016974789915966387,
      "loss": 0.3172,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.06141424551606178,
      "learning_rate": 0.00016941176470588237,
      "loss": 0.3493,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.061691101640462875,
      "learning_rate": 0.00016907563025210084,
      "loss": 0.3162,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.0614585354924202,
      "learning_rate": 0.00016873949579831934,
      "loss": 0.3339,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.0496419258415699,
      "learning_rate": 0.00016840336134453784,
      "loss": 0.2413,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.08882880955934525,
      "learning_rate": 0.0001680672268907563,
      "loss": 0.4158,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.05547051504254341,
      "learning_rate": 0.0001677310924369748,
      "loss": 0.2798,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.08702944219112396,
      "learning_rate": 0.00016739495798319328,
      "loss": 0.3485,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.07315992563962936,
      "learning_rate": 0.00016705882352941178,
      "loss": 0.3388,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.057944681495428085,
      "learning_rate": 0.00016672268907563028,
      "loss": 0.2761,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.09500506520271301,
      "learning_rate": 0.00016638655462184875,
      "loss": 0.4238,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.06549602001905441,
      "learning_rate": 0.00016605042016806725,
      "loss": 0.3388,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.06731800734996796,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.4649,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.062973752617836,
      "learning_rate": 0.0001653781512605042,
      "loss": 0.3975,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.09443088620901108,
      "learning_rate": 0.0001650420168067227,
      "loss": 0.3037,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.06028984114527702,
      "learning_rate": 0.0001647058823529412,
      "loss": 0.39,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.0506599061191082,
      "learning_rate": 0.00016436974789915966,
      "loss": 0.3632,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.04244016483426094,
      "learning_rate": 0.00016403361344537816,
      "loss": 0.3007,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.054453182965517044,
      "learning_rate": 0.00016369747899159663,
      "loss": 0.3746,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.10073786228895187,
      "learning_rate": 0.00016336134453781513,
      "loss": 0.3991,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08028939366340637,
      "learning_rate": 0.00016302521008403363,
      "loss": 0.3642,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.09127162396907806,
      "learning_rate": 0.0001626890756302521,
      "loss": 0.3459,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.04770274832844734,
      "learning_rate": 0.0001623529411764706,
      "loss": 0.3012,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.05118678882718086,
      "learning_rate": 0.00016201680672268907,
      "loss": 0.3457,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.06769824773073196,
      "learning_rate": 0.00016168067226890757,
      "loss": 0.3223,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.049733683466911316,
      "learning_rate": 0.00016134453781512607,
      "loss": 0.2881,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.06353523582220078,
      "learning_rate": 0.00016100840336134454,
      "loss": 0.3708,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06050877273082733,
      "learning_rate": 0.00016067226890756304,
      "loss": 0.342,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.0509507842361927,
      "learning_rate": 0.00016033613445378154,
      "loss": 0.2883,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.04976369068026543,
      "learning_rate": 0.00016,
      "loss": 0.3401,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.058125175535678864,
      "learning_rate": 0.0001596638655462185,
      "loss": 0.3301,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.05522217974066734,
      "learning_rate": 0.00015932773109243698,
      "loss": 0.3465,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.06310960650444031,
      "learning_rate": 0.00015899159663865546,
      "loss": 0.3103,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.0535898432135582,
      "learning_rate": 0.00015865546218487396,
      "loss": 0.2505,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.06480037420988083,
      "learning_rate": 0.00015831932773109243,
      "loss": 0.3826,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.06223045662045479,
      "learning_rate": 0.00015798319327731093,
      "loss": 0.4096,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.08454132825136185,
      "learning_rate": 0.00015764705882352943,
      "loss": 0.441,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07112450152635574,
      "learning_rate": 0.0001573109243697479,
      "loss": 0.3554,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.04817084223031998,
      "learning_rate": 0.0001569747899159664,
      "loss": 0.2924,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05354689806699753,
      "learning_rate": 0.00015663865546218487,
      "loss": 0.3005,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.06967891752719879,
      "learning_rate": 0.00015630252100840337,
      "loss": 0.3125,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05279020220041275,
      "learning_rate": 0.00015596638655462187,
      "loss": 0.2968,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.06701767444610596,
      "learning_rate": 0.00015563025210084034,
      "loss": 0.3512,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.05608438327908516,
      "learning_rate": 0.00015529411764705884,
      "loss": 0.3714,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.06748303771018982,
      "learning_rate": 0.00015495798319327734,
      "loss": 0.353,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.06323929131031036,
      "learning_rate": 0.0001546218487394958,
      "loss": 0.27,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.06434617936611176,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3729,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.03903554007411003,
      "learning_rate": 0.00015394957983193278,
      "loss": 0.2708,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.051943857222795486,
      "learning_rate": 0.00015361344537815128,
      "loss": 0.3376,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.06847108155488968,
      "learning_rate": 0.00015327731092436978,
      "loss": 0.3976,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.06950753182172775,
      "learning_rate": 0.00015294117647058822,
      "loss": 0.3437,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.06255417317152023,
      "learning_rate": 0.00015260504201680672,
      "loss": 0.3762,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.07384087145328522,
      "learning_rate": 0.00015226890756302522,
      "loss": 0.3354,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.04565652459859848,
      "learning_rate": 0.0001519327731092437,
      "loss": 0.3062,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.06590331345796585,
      "learning_rate": 0.0001515966386554622,
      "loss": 0.3566,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.06040659174323082,
      "learning_rate": 0.00015126050420168066,
      "loss": 0.3636,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.05946071445941925,
      "learning_rate": 0.00015092436974789916,
      "loss": 0.3082,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.05283123999834061,
      "learning_rate": 0.00015058823529411766,
      "loss": 0.2904,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.07162099331617355,
      "learning_rate": 0.00015025210084033613,
      "loss": 0.4391,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08802147209644318,
      "learning_rate": 0.00014991596638655463,
      "loss": 0.3613,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.07149039953947067,
      "learning_rate": 0.00014957983193277313,
      "loss": 0.3224,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.05855373665690422,
      "learning_rate": 0.0001492436974789916,
      "loss": 0.3647,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.05240558087825775,
      "learning_rate": 0.0001489075630252101,
      "loss": 0.301,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.0463988222181797,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3196,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.04390238597989082,
      "learning_rate": 0.00014823529411764707,
      "loss": 0.254,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.05333559960126877,
      "learning_rate": 0.00014789915966386557,
      "loss": 0.3379,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.051743511110544205,
      "learning_rate": 0.00014756302521008404,
      "loss": 0.3559,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.0572541244328022,
      "learning_rate": 0.00014722689075630254,
      "loss": 0.2815,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.05247779190540314,
      "learning_rate": 0.00014689075630252101,
      "loss": 0.2159,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.06832840293645859,
      "learning_rate": 0.0001465546218487395,
      "loss": 0.3623,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.062487583607435226,
      "learning_rate": 0.00014621848739495799,
      "loss": 0.3312,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.05925987288355827,
      "learning_rate": 0.00014588235294117646,
      "loss": 0.3137,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.04899313673377037,
      "learning_rate": 0.00014554621848739496,
      "loss": 0.3187,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.049107812345027924,
      "learning_rate": 0.00014521008403361346,
      "loss": 0.296,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.04844258725643158,
      "learning_rate": 0.00014487394957983193,
      "loss": 0.2662,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.055069442838430405,
      "learning_rate": 0.00014453781512605043,
      "loss": 0.3424,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.042290933430194855,
      "learning_rate": 0.00014420168067226893,
      "loss": 0.3129,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.04312681779265404,
      "learning_rate": 0.0001438655462184874,
      "loss": 0.2872,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.052618540823459625,
      "learning_rate": 0.0001435294117647059,
      "loss": 0.2516,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.05352506414055824,
      "learning_rate": 0.00014319327731092437,
      "loss": 0.3631,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.06253834813833237,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3954,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.05486872047185898,
      "learning_rate": 0.00014252100840336137,
      "loss": 0.2789,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.048592984676361084,
      "learning_rate": 0.00014218487394957984,
      "loss": 0.2347,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.07634804397821426,
      "learning_rate": 0.00014184873949579834,
      "loss": 0.3769,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.048945989459753036,
      "learning_rate": 0.0001415126050420168,
      "loss": 0.3196,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.08780886977910995,
      "learning_rate": 0.0001411764705882353,
      "loss": 0.4398,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.06758235394954681,
      "learning_rate": 0.0001408403361344538,
      "loss": 0.3781,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06816046684980392,
      "learning_rate": 0.00014050420168067225,
      "loss": 0.3961,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.05613347142934799,
      "learning_rate": 0.00014016806722689075,
      "loss": 0.3011,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.05102626234292984,
      "learning_rate": 0.00013983193277310925,
      "loss": 0.2991,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.044207822531461716,
      "learning_rate": 0.00013949579831932772,
      "loss": 0.2405,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0554254874587059,
      "learning_rate": 0.00013915966386554622,
      "loss": 0.2919,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.07137871533632278,
      "learning_rate": 0.00013882352941176472,
      "loss": 0.3068,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.057278748601675034,
      "learning_rate": 0.0001384873949579832,
      "loss": 0.315,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.05776836350560188,
      "learning_rate": 0.0001381512605042017,
      "loss": 0.3302,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.05700752139091492,
      "learning_rate": 0.00013781512605042016,
      "loss": 0.2981,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.037222810089588165,
      "learning_rate": 0.00013747899159663866,
      "loss": 0.2499,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05884036049246788,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.3581,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.05409453064203262,
      "learning_rate": 0.00013680672268907563,
      "loss": 0.3513,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.0460929349064827,
      "learning_rate": 0.00013647058823529413,
      "loss": 0.2908,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.05062955245375633,
      "learning_rate": 0.0001361344537815126,
      "loss": 0.3096,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.07279154658317566,
      "learning_rate": 0.0001357983193277311,
      "loss": 0.3125,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.068229541182518,
      "learning_rate": 0.0001354621848739496,
      "loss": 0.3167,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.05755160376429558,
      "learning_rate": 0.00013512605042016807,
      "loss": 0.3879,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06103194132447243,
      "learning_rate": 0.00013478991596638657,
      "loss": 0.3424,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.04604163393378258,
      "learning_rate": 0.00013445378151260507,
      "loss": 0.2594,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.049634698778390884,
      "learning_rate": 0.00013411764705882352,
      "loss": 0.3355,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.0629403293132782,
      "learning_rate": 0.00013378151260504202,
      "loss": 0.3757,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.07044802606105804,
      "learning_rate": 0.00013344537815126052,
      "loss": 0.3294,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.0720302164554596,
      "learning_rate": 0.000133109243697479,
      "loss": 0.2683,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.06395585834980011,
      "learning_rate": 0.0001327731092436975,
      "loss": 0.28,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.1020650863647461,
      "learning_rate": 0.00013243697478991596,
      "loss": 0.3917,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.06108816713094711,
      "learning_rate": 0.00013210084033613446,
      "loss": 0.3021,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.05928527191281319,
      "learning_rate": 0.00013176470588235296,
      "loss": 0.2816,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.0422271192073822,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.2499,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.055980656296014786,
      "learning_rate": 0.00013109243697478993,
      "loss": 0.3359,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.06600435823202133,
      "learning_rate": 0.0001307563025210084,
      "loss": 0.3072,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.07877815514802933,
      "learning_rate": 0.0001304201680672269,
      "loss": 0.3727,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.047687701880931854,
      "learning_rate": 0.0001300840336134454,
      "loss": 0.2518,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.05535099655389786,
      "learning_rate": 0.00012974789915966387,
      "loss": 0.2665,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.06244836002588272,
      "learning_rate": 0.00012941176470588237,
      "loss": 0.3133,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.03867635130882263,
      "learning_rate": 0.00012907563025210087,
      "loss": 0.253,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.05889761820435524,
      "learning_rate": 0.00012873949579831934,
      "loss": 0.3247,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.05134006217122078,
      "learning_rate": 0.00012840336134453784,
      "loss": 0.297,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.047094035893678665,
      "learning_rate": 0.0001280672268907563,
      "loss": 0.2682,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.05849578231573105,
      "learning_rate": 0.00012773109243697478,
      "loss": 0.328,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.043181583285331726,
      "learning_rate": 0.00012739495798319328,
      "loss": 0.3216,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04264124855399132,
      "learning_rate": 0.00012705882352941175,
      "loss": 0.2984,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.055801089853048325,
      "learning_rate": 0.00012672268907563025,
      "loss": 0.3321,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.052933577448129654,
      "learning_rate": 0.00012638655462184875,
      "loss": 0.3531,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.06710308790206909,
      "learning_rate": 0.00012605042016806722,
      "loss": 0.3482,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.04160328581929207,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.2404,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.06991766393184662,
      "learning_rate": 0.0001253781512605042,
      "loss": 0.326,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.06989175826311111,
      "learning_rate": 0.0001250420168067227,
      "loss": 0.4254,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.0667327493429184,
      "learning_rate": 0.0001247058823529412,
      "loss": 0.3009,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.05139871686697006,
      "learning_rate": 0.00012436974789915966,
      "loss": 0.2976,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06273012608289719,
      "learning_rate": 0.00012403361344537816,
      "loss": 0.2785,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.0453084260225296,
      "learning_rate": 0.00012369747899159666,
      "loss": 0.2993,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.06316515803337097,
      "learning_rate": 0.00012336134453781513,
      "loss": 0.3423,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.07456739991903305,
      "learning_rate": 0.00012302521008403363,
      "loss": 0.4278,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.03533536568284035,
      "learning_rate": 0.0001226890756302521,
      "loss": 0.2615,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.049019090831279755,
      "learning_rate": 0.0001223529411764706,
      "loss": 0.3499,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.053170617669820786,
      "learning_rate": 0.00012201680672268909,
      "loss": 0.3448,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.047490738332271576,
      "learning_rate": 0.00012168067226890756,
      "loss": 0.3015,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.058823589235544205,
      "learning_rate": 0.00012134453781512605,
      "loss": 0.2764,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.04685772582888603,
      "learning_rate": 0.00012100840336134453,
      "loss": 0.2793,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.10277030616998672,
      "learning_rate": 0.00012067226890756302,
      "loss": 0.4389,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06533946096897125,
      "learning_rate": 0.00012033613445378152,
      "loss": 0.3087,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.04914902150630951,
      "learning_rate": 0.00012,
      "loss": 0.294,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.06026449054479599,
      "learning_rate": 0.00011966386554621849,
      "loss": 0.355,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.05269932746887207,
      "learning_rate": 0.00011932773109243697,
      "loss": 0.2956,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.08288493007421494,
      "learning_rate": 0.00011899159663865547,
      "loss": 0.4234,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.05238451808691025,
      "learning_rate": 0.00011865546218487396,
      "loss": 0.405,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.060434117913246155,
      "learning_rate": 0.00011831932773109244,
      "loss": 0.3709,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.08480890095233917,
      "learning_rate": 0.00011798319327731093,
      "loss": 0.43,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.06763534992933273,
      "learning_rate": 0.00011764705882352942,
      "loss": 0.3806,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.05371963605284691,
      "learning_rate": 0.00011731092436974791,
      "loss": 0.2699,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.06191708892583847,
      "learning_rate": 0.0001169747899159664,
      "loss": 0.3556,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.07632268220186234,
      "learning_rate": 0.00011663865546218489,
      "loss": 0.3141,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.07482656091451645,
      "learning_rate": 0.00011630252100840337,
      "loss": 0.3562,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.07352543622255325,
      "learning_rate": 0.00011596638655462187,
      "loss": 0.3023,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.054279886186122894,
      "learning_rate": 0.00011563025210084036,
      "loss": 0.2971,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.05944676324725151,
      "learning_rate": 0.00011529411764705881,
      "loss": 0.3135,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.05439133942127228,
      "learning_rate": 0.00011495798319327731,
      "loss": 0.3244,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07696704566478729,
      "learning_rate": 0.0001146218487394958,
      "loss": 0.3681,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.04498272016644478,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.2877,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.049900077283382416,
      "learning_rate": 0.00011394957983193277,
      "loss": 0.305,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04467645287513733,
      "learning_rate": 0.00011361344537815127,
      "loss": 0.2661,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.09342732280492783,
      "learning_rate": 0.00011327731092436975,
      "loss": 0.3242,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.05727897956967354,
      "learning_rate": 0.00011294117647058824,
      "loss": 0.3438,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07545483112335205,
      "learning_rate": 0.00011260504201680672,
      "loss": 0.3687,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.06797177344560623,
      "learning_rate": 0.00011226890756302521,
      "loss": 0.3893,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.046302735805511475,
      "learning_rate": 0.00011193277310924371,
      "loss": 0.3488,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.04388623684644699,
      "learning_rate": 0.0001115966386554622,
      "loss": 0.3058,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.05346697196364403,
      "learning_rate": 0.00011126050420168068,
      "loss": 0.2778,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.08327189832925797,
      "learning_rate": 0.00011092436974789917,
      "loss": 0.3487,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.06831448525190353,
      "learning_rate": 0.00011058823529411766,
      "loss": 0.3468,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.08248267322778702,
      "learning_rate": 0.00011025210084033615,
      "loss": 0.4418,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.0503322035074234,
      "learning_rate": 0.00010991596638655464,
      "loss": 0.3517,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.06279263645410538,
      "learning_rate": 0.00010957983193277312,
      "loss": 0.3355,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.09413671493530273,
      "learning_rate": 0.00010924369747899159,
      "loss": 0.3798,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05492383986711502,
      "learning_rate": 0.00010890756302521008,
      "loss": 0.2837,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.05990251898765564,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.3261,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.05129700526595116,
      "learning_rate": 0.00010823529411764706,
      "loss": 0.332,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.04264286532998085,
      "learning_rate": 0.00010789915966386555,
      "loss": 0.309,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.058379847556352615,
      "learning_rate": 0.00010756302521008403,
      "loss": 0.2978,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.046581488102674484,
      "learning_rate": 0.00010722689075630252,
      "loss": 0.3106,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05166913941502571,
      "learning_rate": 0.000106890756302521,
      "loss": 0.3142,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.06608784943819046,
      "learning_rate": 0.0001065546218487395,
      "loss": 0.3646,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.0547393299639225,
      "learning_rate": 0.00010621848739495799,
      "loss": 0.3983,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.05051540955901146,
      "learning_rate": 0.00010588235294117647,
      "loss": 0.3705,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.08552861213684082,
      "learning_rate": 0.00010554621848739496,
      "loss": 0.349,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.08468075096607208,
      "learning_rate": 0.00010521008403361346,
      "loss": 0.3902,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.07486693561077118,
      "learning_rate": 0.00010487394957983194,
      "loss": 0.4622,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.04992455989122391,
      "learning_rate": 0.00010453781512605043,
      "loss": 0.3547,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.07144948840141296,
      "learning_rate": 0.00010420168067226892,
      "loss": 0.4442,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.06500690430402756,
      "learning_rate": 0.00010386554621848741,
      "loss": 0.3798,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.0502140149474144,
      "learning_rate": 0.0001035294117647059,
      "loss": 0.338,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07363931834697723,
      "learning_rate": 0.00010319327731092439,
      "loss": 0.404,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.045051466673612595,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.332,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.09037106484174728,
      "learning_rate": 0.00010252100840336134,
      "loss": 0.443,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.058586981147527695,
      "learning_rate": 0.00010218487394957983,
      "loss": 0.3994,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.04479117691516876,
      "learning_rate": 0.00010184873949579831,
      "loss": 0.2597,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.04499774053692818,
      "learning_rate": 0.0001015126050420168,
      "loss": 0.2702,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.06712427735328674,
      "learning_rate": 0.0001011764705882353,
      "loss": 0.3535,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.042023319751024246,
      "learning_rate": 0.00010084033613445378,
      "loss": 0.2952,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.09416703879833221,
      "learning_rate": 0.00010050420168067227,
      "loss": 0.3926,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.04594302922487259,
      "learning_rate": 0.00010016806722689076,
      "loss": 0.2934,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.04654405266046524,
      "learning_rate": 9.983193277310925e-05,
      "loss": 0.2682,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0593428909778595,
      "learning_rate": 9.949579831932774e-05,
      "loss": 0.3704,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.053799912333488464,
      "learning_rate": 9.915966386554623e-05,
      "loss": 0.4236,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.053128644824028015,
      "learning_rate": 9.882352941176471e-05,
      "loss": 0.3094,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.04600797966122627,
      "learning_rate": 9.848739495798321e-05,
      "loss": 0.2756,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.047943565994501114,
      "learning_rate": 9.815126050420168e-05,
      "loss": 0.2803,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.04653657227754593,
      "learning_rate": 9.781512605042017e-05,
      "loss": 0.3053,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.053695205599069595,
      "learning_rate": 9.747899159663865e-05,
      "loss": 0.3647,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.062447067350149155,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.3932,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.07734744250774384,
      "learning_rate": 9.680672268907564e-05,
      "loss": 0.3806,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.055346641689538956,
      "learning_rate": 9.647058823529412e-05,
      "loss": 0.3625,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.06996474415063858,
      "learning_rate": 9.613445378151261e-05,
      "loss": 0.3875,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.05198107659816742,
      "learning_rate": 9.579831932773111e-05,
      "loss": 0.247,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.05685421824455261,
      "learning_rate": 9.546218487394959e-05,
      "loss": 0.4106,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.054194413125514984,
      "learning_rate": 9.512605042016806e-05,
      "loss": 0.3382,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06986108422279358,
      "learning_rate": 9.478991596638655e-05,
      "loss": 0.3959,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.063298299908638,
      "learning_rate": 9.445378151260505e-05,
      "loss": 0.255,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09130087494850159,
      "learning_rate": 9.411764705882353e-05,
      "loss": 0.3617,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.04506262019276619,
      "learning_rate": 9.378151260504202e-05,
      "loss": 0.2899,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06226065382361412,
      "learning_rate": 9.34453781512605e-05,
      "loss": 0.3602,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04722338169813156,
      "learning_rate": 9.3109243697479e-05,
      "loss": 0.2676,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.06286595016717911,
      "learning_rate": 9.277310924369749e-05,
      "loss": 0.3728,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.06137560307979584,
      "learning_rate": 9.243697478991598e-05,
      "loss": 0.316,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.05942685157060623,
      "learning_rate": 9.210084033613445e-05,
      "loss": 0.3347,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.042238421738147736,
      "learning_rate": 9.176470588235295e-05,
      "loss": 0.2817,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.050670452415943146,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.3601,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.05395469814538956,
      "learning_rate": 9.109243697478992e-05,
      "loss": 0.3616,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.036372095346450806,
      "learning_rate": 9.07563025210084e-05,
      "loss": 0.2113,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.05981580540537834,
      "learning_rate": 9.04201680672269e-05,
      "loss": 0.3345,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.040700532495975494,
      "learning_rate": 9.008403361344539e-05,
      "loss": 0.3139,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.056080397218465805,
      "learning_rate": 8.974789915966387e-05,
      "loss": 0.364,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08687462657690048,
      "learning_rate": 8.941176470588236e-05,
      "loss": 0.3801,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.052805062383413315,
      "learning_rate": 8.907563025210084e-05,
      "loss": 0.3326,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.035264752805233,
      "learning_rate": 8.873949579831933e-05,
      "loss": 0.2263,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.04310610890388489,
      "learning_rate": 8.840336134453782e-05,
      "loss": 0.3113,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.0653771236538887,
      "learning_rate": 8.80672268907563e-05,
      "loss": 0.322,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.050950076431035995,
      "learning_rate": 8.77310924369748e-05,
      "loss": 0.269,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.052316393703222275,
      "learning_rate": 8.739495798319329e-05,
      "loss": 0.3062,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.06515742838382721,
      "learning_rate": 8.705882352941177e-05,
      "loss": 0.3049,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07055295258760452,
      "learning_rate": 8.672268907563026e-05,
      "loss": 0.3713,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.05747872218489647,
      "learning_rate": 8.638655462184874e-05,
      "loss": 0.3463,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.04811479151248932,
      "learning_rate": 8.605042016806724e-05,
      "loss": 0.327,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.07811559736728668,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.4079,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.049428295344114304,
      "learning_rate": 8.53781512605042e-05,
      "loss": 0.2898,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.05632252246141434,
      "learning_rate": 8.50420168067227e-05,
      "loss": 0.3184,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11199343949556351,
      "learning_rate": 8.470588235294118e-05,
      "loss": 0.3527,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06073231250047684,
      "learning_rate": 8.436974789915967e-05,
      "loss": 0.3868,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.0562119297683239,
      "learning_rate": 8.403361344537815e-05,
      "loss": 0.3219,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.04508448764681816,
      "learning_rate": 8.369747899159664e-05,
      "loss": 0.2768,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.0510864332318306,
      "learning_rate": 8.336134453781514e-05,
      "loss": 0.2977,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.07318849861621857,
      "learning_rate": 8.302521008403362e-05,
      "loss": 0.3824,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.04570917785167694,
      "learning_rate": 8.26890756302521e-05,
      "loss": 0.2843,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.0589633472263813,
      "learning_rate": 8.23529411764706e-05,
      "loss": 0.3869,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.049250710755586624,
      "learning_rate": 8.201680672268908e-05,
      "loss": 0.3088,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.05447178706526756,
      "learning_rate": 8.168067226890757e-05,
      "loss": 0.2701,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.05135701596736908,
      "learning_rate": 8.134453781512605e-05,
      "loss": 0.3397,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.0670817419886589,
      "learning_rate": 8.100840336134454e-05,
      "loss": 0.3765,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04624961316585541,
      "learning_rate": 8.067226890756304e-05,
      "loss": 0.3404,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.055911146104335785,
      "learning_rate": 8.033613445378152e-05,
      "loss": 0.3037,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.0752808004617691,
      "learning_rate": 8e-05,
      "loss": 0.369,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.040648069232702255,
      "learning_rate": 7.966386554621849e-05,
      "loss": 0.2916,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.049986205995082855,
      "learning_rate": 7.932773109243698e-05,
      "loss": 0.2658,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.07278549671173096,
      "learning_rate": 7.899159663865546e-05,
      "loss": 0.4143,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.059924375265836716,
      "learning_rate": 7.865546218487395e-05,
      "loss": 0.3072,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.061904046684503555,
      "learning_rate": 7.831932773109243e-05,
      "loss": 0.3452,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.0663599893450737,
      "learning_rate": 7.798319327731093e-05,
      "loss": 0.3865,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.05303044617176056,
      "learning_rate": 7.764705882352942e-05,
      "loss": 0.315,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06036394089460373,
      "learning_rate": 7.73109243697479e-05,
      "loss": 0.2885,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.04806103929877281,
      "learning_rate": 7.697478991596639e-05,
      "loss": 0.333,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06291871517896652,
      "learning_rate": 7.663865546218489e-05,
      "loss": 0.3293,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.05391750857234001,
      "learning_rate": 7.630252100840336e-05,
      "loss": 0.3851,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.054906729608774185,
      "learning_rate": 7.596638655462185e-05,
      "loss": 0.3888,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.055975042283535004,
      "learning_rate": 7.563025210084033e-05,
      "loss": 0.2688,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.08423983305692673,
      "learning_rate": 7.529411764705883e-05,
      "loss": 0.3727,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.08204298466444016,
      "learning_rate": 7.495798319327732e-05,
      "loss": 0.3916,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.043738070875406265,
      "learning_rate": 7.46218487394958e-05,
      "loss": 0.2743,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.09955333918333054,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.3691,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.049914974719285965,
      "learning_rate": 7.394957983193279e-05,
      "loss": 0.3047,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.04672957584261894,
      "learning_rate": 7.361344537815127e-05,
      "loss": 0.2785,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.04791035130620003,
      "learning_rate": 7.327731092436974e-05,
      "loss": 0.2916,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.07797733694314957,
      "learning_rate": 7.294117647058823e-05,
      "loss": 0.3575,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0947900116443634,
      "learning_rate": 7.260504201680673e-05,
      "loss": 0.4383,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.07515981048345566,
      "learning_rate": 7.226890756302521e-05,
      "loss": 0.39,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.08153430372476578,
      "learning_rate": 7.19327731092437e-05,
      "loss": 0.3656,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.061755336821079254,
      "learning_rate": 7.159663865546218e-05,
      "loss": 0.3111,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05439639836549759,
      "learning_rate": 7.126050420168068e-05,
      "loss": 0.2604,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.060226891189813614,
      "learning_rate": 7.092436974789917e-05,
      "loss": 0.3161,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.04754548892378807,
      "learning_rate": 7.058823529411765e-05,
      "loss": 0.2979,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.04664352536201477,
      "learning_rate": 7.025210084033613e-05,
      "loss": 0.2901,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.05268900468945503,
      "learning_rate": 6.991596638655463e-05,
      "loss": 0.2928,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.08030102401971817,
      "learning_rate": 6.957983193277311e-05,
      "loss": 0.2757,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.09077029675245285,
      "learning_rate": 6.92436974789916e-05,
      "loss": 0.4494,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.05391593277454376,
      "learning_rate": 6.890756302521008e-05,
      "loss": 0.3767,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.07394105941057205,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.3474,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.05819035694003105,
      "learning_rate": 6.823529411764707e-05,
      "loss": 0.3141,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.038969580084085464,
      "learning_rate": 6.789915966386555e-05,
      "loss": 0.2662,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.05798739567399025,
      "learning_rate": 6.756302521008404e-05,
      "loss": 0.3386,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.095215804874897,
      "learning_rate": 6.722689075630254e-05,
      "loss": 0.3696,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.05970003083348274,
      "learning_rate": 6.689075630252101e-05,
      "loss": 0.3626,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.06249988451600075,
      "learning_rate": 6.65546218487395e-05,
      "loss": 0.3238,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.058753687888383865,
      "learning_rate": 6.621848739495798e-05,
      "loss": 0.2621,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.0809716209769249,
      "learning_rate": 6.588235294117648e-05,
      "loss": 0.2846,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.05206849426031113,
      "learning_rate": 6.554621848739496e-05,
      "loss": 0.324,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.049270205199718475,
      "learning_rate": 6.521008403361345e-05,
      "loss": 0.2872,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.07654781639575958,
      "learning_rate": 6.487394957983193e-05,
      "loss": 0.4506,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.049323972314596176,
      "learning_rate": 6.453781512605043e-05,
      "loss": 0.2816,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.07455730438232422,
      "learning_rate": 6.420168067226892e-05,
      "loss": 0.3738,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.07236874103546143,
      "learning_rate": 6.386554621848739e-05,
      "loss": 0.3834,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.03830499202013016,
      "learning_rate": 6.352941176470588e-05,
      "loss": 0.2775,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.044319406151771545,
      "learning_rate": 6.319327731092438e-05,
      "loss": 0.2942,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.06487616151571274,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.3301,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.042386073619127274,
      "learning_rate": 6.252100840336135e-05,
      "loss": 0.3341,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.03240629658102989,
      "learning_rate": 6.218487394957983e-05,
      "loss": 0.1897,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.0641627088189125,
      "learning_rate": 6.184873949579833e-05,
      "loss": 0.3569,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.07767748832702637,
      "learning_rate": 6.151260504201682e-05,
      "loss": 0.3379,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.058574434369802475,
      "learning_rate": 6.11764705882353e-05,
      "loss": 0.373,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.04828990250825882,
      "learning_rate": 6.084033613445378e-05,
      "loss": 0.3073,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04146663099527359,
      "learning_rate": 6.0504201680672267e-05,
      "loss": 0.3018,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.0607866533100605,
      "learning_rate": 6.016806722689076e-05,
      "loss": 0.4186,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.05587982386350632,
      "learning_rate": 5.9831932773109244e-05,
      "loss": 0.3477,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.05359126254916191,
      "learning_rate": 5.9495798319327737e-05,
      "loss": 0.3433,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.07327298074960709,
      "learning_rate": 5.915966386554622e-05,
      "loss": 0.3833,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.05458013713359833,
      "learning_rate": 5.882352941176471e-05,
      "loss": 0.3537,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.06186026707291603,
      "learning_rate": 5.84873949579832e-05,
      "loss": 0.2988,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.047881510108709335,
      "learning_rate": 5.8151260504201685e-05,
      "loss": 0.2596,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.052929360419511795,
      "learning_rate": 5.781512605042018e-05,
      "loss": 0.3231,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.04831072315573692,
      "learning_rate": 5.7478991596638656e-05,
      "loss": 0.3066,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.057545773684978485,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.3337,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.05169958621263504,
      "learning_rate": 5.6806722689075634e-05,
      "loss": 0.318,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.05658767372369766,
      "learning_rate": 5.647058823529412e-05,
      "loss": 0.3861,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.05335130915045738,
      "learning_rate": 5.6134453781512605e-05,
      "loss": 0.2783,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.04733116179704666,
      "learning_rate": 5.57983193277311e-05,
      "loss": 0.3198,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.04851739853620529,
      "learning_rate": 5.546218487394958e-05,
      "loss": 0.3018,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.04695788025856018,
      "learning_rate": 5.5126050420168075e-05,
      "loss": 0.3099,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.05151848495006561,
      "learning_rate": 5.478991596638656e-05,
      "loss": 0.3284,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.05261353775858879,
      "learning_rate": 5.445378151260504e-05,
      "loss": 0.3067,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.05186080187559128,
      "learning_rate": 5.411764705882353e-05,
      "loss": 0.3431,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.036770306527614594,
      "learning_rate": 5.378151260504202e-05,
      "loss": 0.2613,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.109068363904953,
      "learning_rate": 5.34453781512605e-05,
      "loss": 0.3866,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.05800795927643776,
      "learning_rate": 5.3109243697478995e-05,
      "loss": 0.3139,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.061238836497068405,
      "learning_rate": 5.277310924369748e-05,
      "loss": 0.2437,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.038940057158470154,
      "learning_rate": 5.243697478991597e-05,
      "loss": 0.2382,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.11143284291028976,
      "learning_rate": 5.210084033613446e-05,
      "loss": 0.4122,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06194454804062843,
      "learning_rate": 5.176470588235295e-05,
      "loss": 0.3369,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.05470440909266472,
      "learning_rate": 5.142857142857143e-05,
      "loss": 0.3309,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.0525054857134819,
      "learning_rate": 5.1092436974789914e-05,
      "loss": 0.3361,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.043374091386795044,
      "learning_rate": 5.07563025210084e-05,
      "loss": 0.2341,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.050612859427928925,
      "learning_rate": 5.042016806722689e-05,
      "loss": 0.3212,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.05894525721669197,
      "learning_rate": 5.008403361344538e-05,
      "loss": 0.3634,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06533082574605942,
      "learning_rate": 4.974789915966387e-05,
      "loss": 0.2791,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.04496507719159126,
      "learning_rate": 4.9411764705882355e-05,
      "loss": 0.2415,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06756517291069031,
      "learning_rate": 4.907563025210084e-05,
      "loss": 0.3783,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07010626792907715,
      "learning_rate": 4.8739495798319326e-05,
      "loss": 0.292,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.041916754096746445,
      "learning_rate": 4.840336134453782e-05,
      "loss": 0.2764,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.06727927178144455,
      "learning_rate": 4.8067226890756304e-05,
      "loss": 0.389,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.057726722210645676,
      "learning_rate": 4.7731092436974796e-05,
      "loss": 0.3448,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.041583411395549774,
      "learning_rate": 4.7394957983193275e-05,
      "loss": 0.2845,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.04244019091129303,
      "learning_rate": 4.705882352941177e-05,
      "loss": 0.286,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.05420730262994766,
      "learning_rate": 4.672268907563025e-05,
      "loss": 0.357,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.04895046353340149,
      "learning_rate": 4.6386554621848745e-05,
      "loss": 0.2817,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.06454316526651382,
      "learning_rate": 4.6050420168067224e-05,
      "loss": 0.3072,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.06195459142327309,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.4076,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06659147143363953,
      "learning_rate": 4.53781512605042e-05,
      "loss": 0.3201,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.03973660618066788,
      "learning_rate": 4.5042016806722694e-05,
      "loss": 0.2538,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.052275437861680984,
      "learning_rate": 4.470588235294118e-05,
      "loss": 0.2841,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.05418861284852028,
      "learning_rate": 4.4369747899159665e-05,
      "loss": 0.3466,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.0707176998257637,
      "learning_rate": 4.403361344537815e-05,
      "loss": 0.3478,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.03694941848516464,
      "learning_rate": 4.369747899159664e-05,
      "loss": 0.3041,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.04275348037481308,
      "learning_rate": 4.336134453781513e-05,
      "loss": 0.2596,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.046901848167181015,
      "learning_rate": 4.302521008403362e-05,
      "loss": 0.3006,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.04479426518082619,
      "learning_rate": 4.26890756302521e-05,
      "loss": 0.3141,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.06313494592905045,
      "learning_rate": 4.235294117647059e-05,
      "loss": 0.3704,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.06034884601831436,
      "learning_rate": 4.201680672268908e-05,
      "loss": 0.3804,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05495867133140564,
      "learning_rate": 4.168067226890757e-05,
      "loss": 0.318,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.08659520000219345,
      "learning_rate": 4.134453781512605e-05,
      "loss": 0.3797,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.05070212855935097,
      "learning_rate": 4.100840336134454e-05,
      "loss": 0.28,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05461186543107033,
      "learning_rate": 4.0672268907563026e-05,
      "loss": 0.3382,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06389494240283966,
      "learning_rate": 4.033613445378152e-05,
      "loss": 0.3987,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.07057099044322968,
      "learning_rate": 4e-05,
      "loss": 0.4052,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.03626135736703873,
      "learning_rate": 3.966386554621849e-05,
      "loss": 0.2385,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04597623646259308,
      "learning_rate": 3.9327731092436974e-05,
      "loss": 0.2712,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.055682823061943054,
      "learning_rate": 3.8991596638655467e-05,
      "loss": 0.3576,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.051777247339487076,
      "learning_rate": 3.865546218487395e-05,
      "loss": 0.3499,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.04432830214500427,
      "learning_rate": 3.8319327731092444e-05,
      "loss": 0.3176,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.036936260759830475,
      "learning_rate": 3.798319327731092e-05,
      "loss": 0.2544,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.044373396784067154,
      "learning_rate": 3.7647058823529415e-05,
      "loss": 0.2983,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.055560898035764694,
      "learning_rate": 3.73109243697479e-05,
      "loss": 0.34,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.043980132788419724,
      "learning_rate": 3.697478991596639e-05,
      "loss": 0.2726,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.04656039550900459,
      "learning_rate": 3.663865546218487e-05,
      "loss": 0.2975,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.046629469841718674,
      "learning_rate": 3.6302521008403364e-05,
      "loss": 0.3124,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.042263422161340714,
      "learning_rate": 3.596638655462185e-05,
      "loss": 0.2957,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.061386819928884506,
      "learning_rate": 3.563025210084034e-05,
      "loss": 0.333,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.052461352199316025,
      "learning_rate": 3.529411764705883e-05,
      "loss": 0.34,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.052588652819395065,
      "learning_rate": 3.495798319327731e-05,
      "loss": 0.2788,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.0460253544151783,
      "learning_rate": 3.46218487394958e-05,
      "loss": 0.3095,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.0740259513258934,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.3312,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04320492967963219,
      "learning_rate": 3.3949579831932776e-05,
      "loss": 0.2711,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.050913359969854355,
      "learning_rate": 3.361344537815127e-05,
      "loss": 0.2932,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 600,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6281058763653448e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
