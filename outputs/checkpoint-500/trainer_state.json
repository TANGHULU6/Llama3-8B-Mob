{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6666666666666666,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 2.723815679550171,
      "learning_rate": 4e-05,
      "loss": 2.5,
      "step": 1
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 2.9685046672821045,
      "learning_rate": 8e-05,
      "loss": 2.7807,
      "step": 2
    },
    {
      "epoch": 0.004,
      "grad_norm": 2.503380537033081,
      "learning_rate": 0.00012,
      "loss": 2.6875,
      "step": 3
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 1.475252628326416,
      "learning_rate": 0.00016,
      "loss": 2.4442,
      "step": 4
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 2.4813244342803955,
      "learning_rate": 0.0002,
      "loss": 2.2628,
      "step": 5
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.3976294994354248,
      "learning_rate": 0.0001997315436241611,
      "loss": 2.2489,
      "step": 6
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 1.9337061643600464,
      "learning_rate": 0.00019946308724832216,
      "loss": 2.0195,
      "step": 7
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 1.1687088012695312,
      "learning_rate": 0.00019919463087248322,
      "loss": 2.0437,
      "step": 8
    },
    {
      "epoch": 0.012,
      "grad_norm": 1.520156979560852,
      "learning_rate": 0.0001989261744966443,
      "loss": 1.9883,
      "step": 9
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.516724705696106,
      "learning_rate": 0.0001986577181208054,
      "loss": 1.7944,
      "step": 10
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 1.2711273431777954,
      "learning_rate": 0.00019838926174496645,
      "loss": 1.7214,
      "step": 11
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.0578079223632812,
      "learning_rate": 0.00019812080536912751,
      "loss": 1.6688,
      "step": 12
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 1.0328564643859863,
      "learning_rate": 0.0001978523489932886,
      "loss": 1.6812,
      "step": 13
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.894223153591156,
      "learning_rate": 0.00019758389261744966,
      "loss": 1.6134,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.6094439029693604,
      "learning_rate": 0.00019731543624161075,
      "loss": 1.4865,
      "step": 15
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.8949456810951233,
      "learning_rate": 0.00019704697986577184,
      "loss": 1.5357,
      "step": 16
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 1.1865921020507812,
      "learning_rate": 0.0001967785234899329,
      "loss": 1.4982,
      "step": 17
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.1493502855300903,
      "learning_rate": 0.00019651006711409396,
      "loss": 1.3657,
      "step": 18
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 1.2694082260131836,
      "learning_rate": 0.00019624161073825505,
      "loss": 1.3787,
      "step": 19
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 2.1344399452209473,
      "learning_rate": 0.00019597315436241613,
      "loss": 1.4,
      "step": 20
    },
    {
      "epoch": 0.028,
      "grad_norm": 1.982196569442749,
      "learning_rate": 0.0001957046979865772,
      "loss": 1.1898,
      "step": 21
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 3.2962048053741455,
      "learning_rate": 0.00019543624161073826,
      "loss": 1.2723,
      "step": 22
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 3.50461483001709,
      "learning_rate": 0.00019516778523489934,
      "loss": 1.08,
      "step": 23
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.2684801816940308,
      "learning_rate": 0.0001948993288590604,
      "loss": 1.0688,
      "step": 24
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 2.535876512527466,
      "learning_rate": 0.0001946308724832215,
      "loss": 0.9789,
      "step": 25
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 1.6896135807037354,
      "learning_rate": 0.00019436241610738255,
      "loss": 1.0022,
      "step": 26
    },
    {
      "epoch": 0.036,
      "grad_norm": 5.202516078948975,
      "learning_rate": 0.00019409395973154364,
      "loss": 0.9034,
      "step": 27
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 1.3576048612594604,
      "learning_rate": 0.0001938255033557047,
      "loss": 0.7942,
      "step": 28
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 2.8357460498809814,
      "learning_rate": 0.0001935570469798658,
      "loss": 0.8446,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.798033356666565,
      "learning_rate": 0.00019328859060402688,
      "loss": 0.835,
      "step": 30
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 0.9634085893630981,
      "learning_rate": 0.0001930201342281879,
      "loss": 0.8244,
      "step": 31
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 1.3095715045928955,
      "learning_rate": 0.000192751677852349,
      "loss": 0.7439,
      "step": 32
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.9432581067085266,
      "learning_rate": 0.00019248322147651008,
      "loss": 0.6712,
      "step": 33
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.5450010299682617,
      "learning_rate": 0.00019221476510067115,
      "loss": 0.6948,
      "step": 34
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 1.0913035869598389,
      "learning_rate": 0.0001919463087248322,
      "loss": 0.5956,
      "step": 35
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.6211374402046204,
      "learning_rate": 0.0001916778523489933,
      "loss": 0.7629,
      "step": 36
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 0.6069960594177246,
      "learning_rate": 0.00019140939597315438,
      "loss": 0.6064,
      "step": 37
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.585067093372345,
      "learning_rate": 0.00019114093959731544,
      "loss": 0.6258,
      "step": 38
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.3613353967666626,
      "learning_rate": 0.00019087248322147653,
      "loss": 0.5902,
      "step": 39
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.920565664768219,
      "learning_rate": 0.0001906040268456376,
      "loss": 0.6555,
      "step": 40
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 0.4131779670715332,
      "learning_rate": 0.00019033557046979865,
      "loss": 0.635,
      "step": 41
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.48718544840812683,
      "learning_rate": 0.00019006711409395974,
      "loss": 0.5781,
      "step": 42
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 0.30693942308425903,
      "learning_rate": 0.00018979865771812083,
      "loss": 0.67,
      "step": 43
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.36902666091918945,
      "learning_rate": 0.0001895302013422819,
      "loss": 0.6169,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.611107587814331,
      "learning_rate": 0.00018926174496644295,
      "loss": 0.6331,
      "step": 45
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.21919956803321838,
      "learning_rate": 0.00018899328859060404,
      "loss": 0.6224,
      "step": 46
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.3313750624656677,
      "learning_rate": 0.00018872483221476512,
      "loss": 0.5728,
      "step": 47
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.2681863009929657,
      "learning_rate": 0.00018845637583892618,
      "loss": 0.5841,
      "step": 48
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 0.26074013113975525,
      "learning_rate": 0.00018818791946308724,
      "loss": 0.5823,
      "step": 49
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.2604086101055145,
      "learning_rate": 0.00018791946308724833,
      "loss": 0.5761,
      "step": 50
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.262260764837265,
      "learning_rate": 0.0001876510067114094,
      "loss": 0.6528,
      "step": 51
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.2471352070569992,
      "learning_rate": 0.00018738255033557048,
      "loss": 0.5458,
      "step": 52
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 0.23585209250450134,
      "learning_rate": 0.00018711409395973157,
      "loss": 0.6306,
      "step": 53
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.23484039306640625,
      "learning_rate": 0.00018684563758389263,
      "loss": 0.5848,
      "step": 54
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.20840023458003998,
      "learning_rate": 0.0001865771812080537,
      "loss": 0.5683,
      "step": 55
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.18594132363796234,
      "learning_rate": 0.00018630872483221478,
      "loss": 0.5432,
      "step": 56
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.1860542744398117,
      "learning_rate": 0.00018604026845637586,
      "loss": 0.5763,
      "step": 57
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.24705623090267181,
      "learning_rate": 0.00018577181208053692,
      "loss": 0.6209,
      "step": 58
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 0.20100738108158112,
      "learning_rate": 0.00018550335570469799,
      "loss": 0.5886,
      "step": 59
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.19436311721801758,
      "learning_rate": 0.00018523489932885907,
      "loss": 0.5821,
      "step": 60
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 0.2535220980644226,
      "learning_rate": 0.00018496644295302016,
      "loss": 0.5629,
      "step": 61
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.22248795628547668,
      "learning_rate": 0.00018469798657718122,
      "loss": 0.7046,
      "step": 62
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.17545481026172638,
      "learning_rate": 0.00018442953020134228,
      "loss": 0.5356,
      "step": 63
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.2196282148361206,
      "learning_rate": 0.00018416107382550337,
      "loss": 0.6036,
      "step": 64
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.1907508820295334,
      "learning_rate": 0.00018389261744966443,
      "loss": 0.5671,
      "step": 65
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.2226567566394806,
      "learning_rate": 0.00018362416107382552,
      "loss": 0.5832,
      "step": 66
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.2007017433643341,
      "learning_rate": 0.0001833557046979866,
      "loss": 0.5627,
      "step": 67
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.15883277356624603,
      "learning_rate": 0.00018308724832214767,
      "loss": 0.5126,
      "step": 68
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.15589788556098938,
      "learning_rate": 0.00018281879194630873,
      "loss": 0.5139,
      "step": 69
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.17529423534870148,
      "learning_rate": 0.00018255033557046981,
      "loss": 0.531,
      "step": 70
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 0.1763117015361786,
      "learning_rate": 0.0001822818791946309,
      "loss": 0.5102,
      "step": 71
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.20358185470104218,
      "learning_rate": 0.00018201342281879194,
      "loss": 0.51,
      "step": 72
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 0.14628788828849792,
      "learning_rate": 0.00018174496644295302,
      "loss": 0.5773,
      "step": 73
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.10516517609357834,
      "learning_rate": 0.0001814765100671141,
      "loss": 0.4504,
      "step": 74
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.14257574081420898,
      "learning_rate": 0.00018120805369127517,
      "loss": 0.573,
      "step": 75
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.11481186747550964,
      "learning_rate": 0.00018093959731543626,
      "loss": 0.5022,
      "step": 76
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 0.14008845388889313,
      "learning_rate": 0.00018067114093959732,
      "loss": 0.5613,
      "step": 77
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.15420730412006378,
      "learning_rate": 0.0001804026845637584,
      "loss": 0.6127,
      "step": 78
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 0.12467651069164276,
      "learning_rate": 0.00018013422818791947,
      "loss": 0.5327,
      "step": 79
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.13095083832740784,
      "learning_rate": 0.00017986577181208056,
      "loss": 0.5599,
      "step": 80
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.12855251133441925,
      "learning_rate": 0.00017959731543624162,
      "loss": 0.5052,
      "step": 81
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.16475766897201538,
      "learning_rate": 0.00017932885906040268,
      "loss": 0.5484,
      "step": 82
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 0.15927277505397797,
      "learning_rate": 0.00017906040268456376,
      "loss": 0.5567,
      "step": 83
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.13724763691425323,
      "learning_rate": 0.00017879194630872485,
      "loss": 0.5935,
      "step": 84
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.17293532192707062,
      "learning_rate": 0.0001785234899328859,
      "loss": 0.525,
      "step": 85
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.17043158411979675,
      "learning_rate": 0.00017825503355704697,
      "loss": 0.5524,
      "step": 86
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.1365189105272293,
      "learning_rate": 0.00017798657718120806,
      "loss": 0.5507,
      "step": 87
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.18575674295425415,
      "learning_rate": 0.00017771812080536915,
      "loss": 0.587,
      "step": 88
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.16230738162994385,
      "learning_rate": 0.0001774496644295302,
      "loss": 0.5276,
      "step": 89
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.1287364810705185,
      "learning_rate": 0.0001771812080536913,
      "loss": 0.4772,
      "step": 90
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.3197518587112427,
      "learning_rate": 0.00017691275167785236,
      "loss": 0.6028,
      "step": 91
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.15457262098789215,
      "learning_rate": 0.00017664429530201342,
      "loss": 0.5184,
      "step": 92
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.21856310963630676,
      "learning_rate": 0.0001763758389261745,
      "loss": 0.5602,
      "step": 93
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.21847905218601227,
      "learning_rate": 0.0001761073825503356,
      "loss": 0.5601,
      "step": 94
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17406229674816132,
      "learning_rate": 0.00017583892617449665,
      "loss": 0.5055,
      "step": 95
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.22073408961296082,
      "learning_rate": 0.00017557046979865771,
      "loss": 0.598,
      "step": 96
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 0.11186487227678299,
      "learning_rate": 0.0001753020134228188,
      "loss": 0.4796,
      "step": 97
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.13610105216503143,
      "learning_rate": 0.0001750335570469799,
      "loss": 0.5273,
      "step": 98
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.21267354488372803,
      "learning_rate": 0.00017476510067114095,
      "loss": 0.542,
      "step": 99
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.176675483584404,
      "learning_rate": 0.000174496644295302,
      "loss": 0.5967,
      "step": 100
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 0.16401363909244537,
      "learning_rate": 0.0001742281879194631,
      "loss": 0.4625,
      "step": 101
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.19610834121704102,
      "learning_rate": 0.00017395973154362416,
      "loss": 0.6065,
      "step": 102
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 0.14259560406208038,
      "learning_rate": 0.00017369127516778525,
      "loss": 0.5486,
      "step": 103
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.1464507132768631,
      "learning_rate": 0.0001734228187919463,
      "loss": 0.5533,
      "step": 104
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.16915719211101532,
      "learning_rate": 0.0001731543624161074,
      "loss": 0.5468,
      "step": 105
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.12893734872341156,
      "learning_rate": 0.00017288590604026846,
      "loss": 0.4811,
      "step": 106
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 0.13757452368736267,
      "learning_rate": 0.00017261744966442954,
      "loss": 0.5782,
      "step": 107
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.24733436107635498,
      "learning_rate": 0.00017234899328859063,
      "loss": 0.5707,
      "step": 108
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 0.1555091291666031,
      "learning_rate": 0.00017208053691275166,
      "loss": 0.6154,
      "step": 109
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.1350300908088684,
      "learning_rate": 0.00017181208053691275,
      "loss": 0.5162,
      "step": 110
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.22905318439006805,
      "learning_rate": 0.00017154362416107384,
      "loss": 0.5565,
      "step": 111
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.1266145408153534,
      "learning_rate": 0.0001712751677852349,
      "loss": 0.5165,
      "step": 112
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.11315270513296127,
      "learning_rate": 0.000171006711409396,
      "loss": 0.5386,
      "step": 113
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.1719426065683365,
      "learning_rate": 0.00017073825503355705,
      "loss": 0.5285,
      "step": 114
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.159308522939682,
      "learning_rate": 0.00017046979865771814,
      "loss": 0.5242,
      "step": 115
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.13087989389896393,
      "learning_rate": 0.0001702013422818792,
      "loss": 0.4816,
      "step": 116
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.13954536616802216,
      "learning_rate": 0.00016993288590604028,
      "loss": 0.5184,
      "step": 117
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.18408606946468353,
      "learning_rate": 0.00016966442953020134,
      "loss": 0.5377,
      "step": 118
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.13542522490024567,
      "learning_rate": 0.00016939597315436243,
      "loss": 0.5875,
      "step": 119
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.1113203763961792,
      "learning_rate": 0.0001691275167785235,
      "loss": 0.5432,
      "step": 120
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 0.15531787276268005,
      "learning_rate": 0.00016885906040268458,
      "loss": 0.5715,
      "step": 121
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.1422499120235443,
      "learning_rate": 0.00016859060402684567,
      "loss": 0.5547,
      "step": 122
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.19247937202453613,
      "learning_rate": 0.0001683221476510067,
      "loss": 0.5454,
      "step": 123
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.16682404279708862,
      "learning_rate": 0.0001680536912751678,
      "loss": 0.5457,
      "step": 124
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.11881460249423981,
      "learning_rate": 0.00016778523489932888,
      "loss": 0.4518,
      "step": 125
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.1690768450498581,
      "learning_rate": 0.00016751677852348994,
      "loss": 0.5157,
      "step": 126
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 0.14804063737392426,
      "learning_rate": 0.000167248322147651,
      "loss": 0.5285,
      "step": 127
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.2141050547361374,
      "learning_rate": 0.00016697986577181209,
      "loss": 0.5797,
      "step": 128
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.11826096475124359,
      "learning_rate": 0.00016671140939597317,
      "loss": 0.5342,
      "step": 129
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.3085940182209015,
      "learning_rate": 0.00016644295302013423,
      "loss": 0.5408,
      "step": 130
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 0.12216658145189285,
      "learning_rate": 0.00016617449664429532,
      "loss": 0.4142,
      "step": 131
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.22591014206409454,
      "learning_rate": 0.00016590604026845638,
      "loss": 0.4962,
      "step": 132
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.2137034684419632,
      "learning_rate": 0.00016563758389261744,
      "loss": 0.4919,
      "step": 133
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.1448015719652176,
      "learning_rate": 0.00016536912751677853,
      "loss": 0.5696,
      "step": 134
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2940560281276703,
      "learning_rate": 0.00016510067114093962,
      "loss": 0.5481,
      "step": 135
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.1785200536251068,
      "learning_rate": 0.00016483221476510068,
      "loss": 0.4884,
      "step": 136
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.17212000489234924,
      "learning_rate": 0.00016456375838926174,
      "loss": 0.5242,
      "step": 137
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.23092731833457947,
      "learning_rate": 0.00016429530201342283,
      "loss": 0.5777,
      "step": 138
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.14297333359718323,
      "learning_rate": 0.00016402684563758391,
      "loss": 0.4621,
      "step": 139
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.19521552324295044,
      "learning_rate": 0.00016375838926174498,
      "loss": 0.5738,
      "step": 140
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.2101367861032486,
      "learning_rate": 0.00016348993288590604,
      "loss": 0.5723,
      "step": 141
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.16901490092277527,
      "learning_rate": 0.00016322147651006712,
      "loss": 0.5319,
      "step": 142
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.2041298896074295,
      "learning_rate": 0.00016295302013422818,
      "loss": 0.5126,
      "step": 143
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.1297338604927063,
      "learning_rate": 0.00016268456375838927,
      "loss": 0.4902,
      "step": 144
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.10514619946479797,
      "learning_rate": 0.00016241610738255036,
      "loss": 0.4463,
      "step": 145
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.18400543928146362,
      "learning_rate": 0.00016214765100671142,
      "loss": 0.4697,
      "step": 146
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.19909021258354187,
      "learning_rate": 0.00016187919463087248,
      "loss": 0.5446,
      "step": 147
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.1836933195590973,
      "learning_rate": 0.00016161073825503357,
      "loss": 0.6111,
      "step": 148
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.13329249620437622,
      "learning_rate": 0.00016134228187919466,
      "loss": 0.5266,
      "step": 149
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.13207976520061493,
      "learning_rate": 0.0001610738255033557,
      "loss": 0.5141,
      "step": 150
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.1317371428012848,
      "learning_rate": 0.00016080536912751678,
      "loss": 0.4441,
      "step": 151
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.18770557641983032,
      "learning_rate": 0.00016053691275167786,
      "loss": 0.5188,
      "step": 152
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.16575250029563904,
      "learning_rate": 0.00016026845637583893,
      "loss": 0.5723,
      "step": 153
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.2317441999912262,
      "learning_rate": 0.00016,
      "loss": 0.5879,
      "step": 154
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.12101209908723831,
      "learning_rate": 0.00015973154362416107,
      "loss": 0.4646,
      "step": 155
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.11907678842544556,
      "learning_rate": 0.00015946308724832216,
      "loss": 0.4631,
      "step": 156
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.12661537528038025,
      "learning_rate": 0.00015919463087248322,
      "loss": 0.5863,
      "step": 157
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.17173932492733002,
      "learning_rate": 0.0001589261744966443,
      "loss": 0.538,
      "step": 158
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.1152791976928711,
      "learning_rate": 0.0001586577181208054,
      "loss": 0.4921,
      "step": 159
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.16651783883571625,
      "learning_rate": 0.00015838926174496643,
      "loss": 0.5509,
      "step": 160
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 0.14353354275226593,
      "learning_rate": 0.00015812080536912752,
      "loss": 0.4843,
      "step": 161
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.11458922922611237,
      "learning_rate": 0.0001578523489932886,
      "loss": 0.4535,
      "step": 162
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.12745900452136993,
      "learning_rate": 0.00015758389261744967,
      "loss": 0.5501,
      "step": 163
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.16347217559814453,
      "learning_rate": 0.00015731543624161073,
      "loss": 0.507,
      "step": 164
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.166949063539505,
      "learning_rate": 0.00015704697986577181,
      "loss": 0.5829,
      "step": 165
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.12640833854675293,
      "learning_rate": 0.0001567785234899329,
      "loss": 0.5213,
      "step": 166
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.15547046065330505,
      "learning_rate": 0.00015651006711409396,
      "loss": 0.4741,
      "step": 167
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.12274295091629028,
      "learning_rate": 0.00015624161073825505,
      "loss": 0.4685,
      "step": 168
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 0.1545671671628952,
      "learning_rate": 0.0001559731543624161,
      "loss": 0.5563,
      "step": 169
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.1493377685546875,
      "learning_rate": 0.0001557046979865772,
      "loss": 0.5009,
      "step": 170
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.11334281414747238,
      "learning_rate": 0.00015543624161073826,
      "loss": 0.5266,
      "step": 171
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.18426550924777985,
      "learning_rate": 0.00015516778523489935,
      "loss": 0.5069,
      "step": 172
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 0.16292202472686768,
      "learning_rate": 0.0001548993288590604,
      "loss": 0.4956,
      "step": 173
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.2123706340789795,
      "learning_rate": 0.00015463087248322147,
      "loss": 0.5089,
      "step": 174
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.12738704681396484,
      "learning_rate": 0.00015436241610738256,
      "loss": 0.543,
      "step": 175
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.21198797225952148,
      "learning_rate": 0.00015409395973154364,
      "loss": 0.5315,
      "step": 176
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.2485591173171997,
      "learning_rate": 0.0001538255033557047,
      "loss": 0.5678,
      "step": 177
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.20750382542610168,
      "learning_rate": 0.00015355704697986576,
      "loss": 0.558,
      "step": 178
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 0.16961121559143066,
      "learning_rate": 0.00015328859060402685,
      "loss": 0.4542,
      "step": 179
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.17473088204860687,
      "learning_rate": 0.00015302013422818794,
      "loss": 0.526,
      "step": 180
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 0.1924571990966797,
      "learning_rate": 0.000152751677852349,
      "loss": 0.5113,
      "step": 181
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.19773225486278534,
      "learning_rate": 0.0001524832214765101,
      "loss": 0.6376,
      "step": 182
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.15570113062858582,
      "learning_rate": 0.00015221476510067115,
      "loss": 0.5606,
      "step": 183
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.13289795815944672,
      "learning_rate": 0.0001519463087248322,
      "loss": 0.5378,
      "step": 184
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.14042134582996368,
      "learning_rate": 0.0001516778523489933,
      "loss": 0.4207,
      "step": 185
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.1705239862203598,
      "learning_rate": 0.00015140939597315438,
      "loss": 0.4731,
      "step": 186
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.0967443510890007,
      "learning_rate": 0.00015114093959731545,
      "loss": 0.3908,
      "step": 187
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.1520439237356186,
      "learning_rate": 0.0001508724832214765,
      "loss": 0.4737,
      "step": 188
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.13675175607204437,
      "learning_rate": 0.0001506040268456376,
      "loss": 0.5051,
      "step": 189
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.12178727239370346,
      "learning_rate": 0.00015033557046979868,
      "loss": 0.5402,
      "step": 190
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 0.16326504945755005,
      "learning_rate": 0.00015006711409395974,
      "loss": 0.5537,
      "step": 191
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.12697133421897888,
      "learning_rate": 0.0001497986577181208,
      "loss": 0.4662,
      "step": 192
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.17699091136455536,
      "learning_rate": 0.0001495302013422819,
      "loss": 0.5508,
      "step": 193
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.12198177725076675,
      "learning_rate": 0.00014926174496644295,
      "loss": 0.5163,
      "step": 194
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.16242410242557526,
      "learning_rate": 0.00014899328859060404,
      "loss": 0.4613,
      "step": 195
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.16547176241874695,
      "learning_rate": 0.0001487248322147651,
      "loss": 0.5333,
      "step": 196
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.13119584321975708,
      "learning_rate": 0.0001484563758389262,
      "loss": 0.5554,
      "step": 197
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.15666402876377106,
      "learning_rate": 0.00014818791946308725,
      "loss": 0.4771,
      "step": 198
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.16213931143283844,
      "learning_rate": 0.00014791946308724834,
      "loss": 0.4967,
      "step": 199
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.12626579403877258,
      "learning_rate": 0.00014765100671140942,
      "loss": 0.5509,
      "step": 200
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.19205409288406372,
      "learning_rate": 0.00014738255033557046,
      "loss": 0.5586,
      "step": 201
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.14109189808368683,
      "learning_rate": 0.00014711409395973154,
      "loss": 0.4811,
      "step": 202
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 0.19084596633911133,
      "learning_rate": 0.00014684563758389263,
      "loss": 0.568,
      "step": 203
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.14348801970481873,
      "learning_rate": 0.0001465771812080537,
      "loss": 0.525,
      "step": 204
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.09870108217000961,
      "learning_rate": 0.00014630872483221478,
      "loss": 0.4714,
      "step": 205
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.16182178258895874,
      "learning_rate": 0.00014604026845637584,
      "loss": 0.533,
      "step": 206
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.10815033316612244,
      "learning_rate": 0.00014577181208053693,
      "loss": 0.5411,
      "step": 207
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.1114797443151474,
      "learning_rate": 0.000145503355704698,
      "loss": 0.4749,
      "step": 208
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.13052169978618622,
      "learning_rate": 0.00014523489932885908,
      "loss": 0.5888,
      "step": 209
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.11835075169801712,
      "learning_rate": 0.00014496644295302014,
      "loss": 0.3816,
      "step": 210
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.15940488874912262,
      "learning_rate": 0.0001446979865771812,
      "loss": 0.4469,
      "step": 211
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.12433143705129623,
      "learning_rate": 0.00014442953020134229,
      "loss": 0.5351,
      "step": 212
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.16542257368564606,
      "learning_rate": 0.00014416107382550337,
      "loss": 0.4984,
      "step": 213
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.19157303869724274,
      "learning_rate": 0.00014389261744966443,
      "loss": 0.4829,
      "step": 214
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.13222254812717438,
      "learning_rate": 0.0001436241610738255,
      "loss": 0.4607,
      "step": 215
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.1669374704360962,
      "learning_rate": 0.00014335570469798658,
      "loss": 0.4699,
      "step": 216
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 0.20329007506370544,
      "learning_rate": 0.00014308724832214767,
      "loss": 0.5829,
      "step": 217
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.2116711288690567,
      "learning_rate": 0.00014281879194630873,
      "loss": 0.5016,
      "step": 218
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.17332139611244202,
      "learning_rate": 0.0001425503355704698,
      "loss": 0.531,
      "step": 219
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.23578409850597382,
      "learning_rate": 0.00014228187919463088,
      "loss": 0.5186,
      "step": 220
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.14669781923294067,
      "learning_rate": 0.00014201342281879197,
      "loss": 0.5403,
      "step": 221
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.21124683320522308,
      "learning_rate": 0.00014174496644295303,
      "loss": 0.5485,
      "step": 222
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 0.19506660103797913,
      "learning_rate": 0.00014147651006711411,
      "loss": 0.5366,
      "step": 223
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.1819337010383606,
      "learning_rate": 0.00014120805369127517,
      "loss": 0.544,
      "step": 224
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.24627019464969635,
      "learning_rate": 0.00014093959731543624,
      "loss": 0.6,
      "step": 225
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.15113204717636108,
      "learning_rate": 0.00014067114093959732,
      "loss": 0.5414,
      "step": 226
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.2574674189090729,
      "learning_rate": 0.0001404026845637584,
      "loss": 0.4961,
      "step": 227
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.14765174686908722,
      "learning_rate": 0.00014013422818791947,
      "loss": 0.4605,
      "step": 228
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.2378591150045395,
      "learning_rate": 0.00013986577181208053,
      "loss": 0.5932,
      "step": 229
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.1745029091835022,
      "learning_rate": 0.00013959731543624162,
      "loss": 0.5582,
      "step": 230
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.1291014403104782,
      "learning_rate": 0.0001393288590604027,
      "loss": 0.4611,
      "step": 231
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.228955939412117,
      "learning_rate": 0.00013906040268456377,
      "loss": 0.5638,
      "step": 232
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 0.1565016210079193,
      "learning_rate": 0.00013879194630872483,
      "loss": 0.5027,
      "step": 233
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.17258284986019135,
      "learning_rate": 0.00013852348993288592,
      "loss": 0.5094,
      "step": 234
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.14579495787620544,
      "learning_rate": 0.00013825503355704698,
      "loss": 0.5339,
      "step": 235
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.11477144807577133,
      "learning_rate": 0.00013798657718120806,
      "loss": 0.4721,
      "step": 236
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.20525150001049042,
      "learning_rate": 0.00013771812080536915,
      "loss": 0.5588,
      "step": 237
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.15818831324577332,
      "learning_rate": 0.0001374496644295302,
      "loss": 0.5139,
      "step": 238
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.14261679351329803,
      "learning_rate": 0.00013718120805369127,
      "loss": 0.5449,
      "step": 239
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.1544056236743927,
      "learning_rate": 0.00013691275167785236,
      "loss": 0.5148,
      "step": 240
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.1575741469860077,
      "learning_rate": 0.00013664429530201345,
      "loss": 0.444,
      "step": 241
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.15699279308319092,
      "learning_rate": 0.00013637583892617448,
      "loss": 0.5229,
      "step": 242
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.17152732610702515,
      "learning_rate": 0.00013610738255033557,
      "loss": 0.6241,
      "step": 243
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.15719042718410492,
      "learning_rate": 0.00013583892617449666,
      "loss": 0.499,
      "step": 244
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.16021716594696045,
      "learning_rate": 0.00013557046979865772,
      "loss": 0.4537,
      "step": 245
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.24018986523151398,
      "learning_rate": 0.0001353020134228188,
      "loss": 0.6029,
      "step": 246
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.17560672760009766,
      "learning_rate": 0.00013503355704697987,
      "loss": 0.5075,
      "step": 247
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.21419429779052734,
      "learning_rate": 0.00013476510067114095,
      "loss": 0.504,
      "step": 248
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.12412555515766144,
      "learning_rate": 0.00013449664429530201,
      "loss": 0.5101,
      "step": 249
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.19209639728069305,
      "learning_rate": 0.0001342281879194631,
      "loss": 0.6021,
      "step": 250
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.12739244103431702,
      "learning_rate": 0.0001339597315436242,
      "loss": 0.5259,
      "step": 251
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.20407500863075256,
      "learning_rate": 0.00013369127516778522,
      "loss": 0.5027,
      "step": 252
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.15300291776657104,
      "learning_rate": 0.0001334228187919463,
      "loss": 0.5751,
      "step": 253
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.12715519964694977,
      "learning_rate": 0.0001331543624161074,
      "loss": 0.4769,
      "step": 254
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.18835191428661346,
      "learning_rate": 0.00013288590604026846,
      "loss": 0.4798,
      "step": 255
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.19658073782920837,
      "learning_rate": 0.00013261744966442952,
      "loss": 0.5722,
      "step": 256
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.13234597444534302,
      "learning_rate": 0.0001323489932885906,
      "loss": 0.5113,
      "step": 257
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.14168037474155426,
      "learning_rate": 0.0001320805369127517,
      "loss": 0.5566,
      "step": 258
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.11935974657535553,
      "learning_rate": 0.00013181208053691276,
      "loss": 0.5061,
      "step": 259
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.1198730543255806,
      "learning_rate": 0.00013154362416107384,
      "loss": 0.4604,
      "step": 260
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.12536586821079254,
      "learning_rate": 0.0001312751677852349,
      "loss": 0.5044,
      "step": 261
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.13916125893592834,
      "learning_rate": 0.00013100671140939596,
      "loss": 0.4915,
      "step": 262
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.10984010994434357,
      "learning_rate": 0.00013073825503355705,
      "loss": 0.4882,
      "step": 263
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.11817828565835953,
      "learning_rate": 0.00013046979865771814,
      "loss": 0.443,
      "step": 264
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.11657844483852386,
      "learning_rate": 0.0001302013422818792,
      "loss": 0.4979,
      "step": 265
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.12008772790431976,
      "learning_rate": 0.00012993288590604026,
      "loss": 0.478,
      "step": 266
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.11077264696359634,
      "learning_rate": 0.00012966442953020135,
      "loss": 0.5148,
      "step": 267
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.11317206174135208,
      "learning_rate": 0.00012939597315436244,
      "loss": 0.4588,
      "step": 268
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.10906358808279037,
      "learning_rate": 0.0001291275167785235,
      "loss": 0.4845,
      "step": 269
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.1262381225824356,
      "learning_rate": 0.00012885906040268456,
      "loss": 0.5302,
      "step": 270
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.11013565957546234,
      "learning_rate": 0.00012859060402684564,
      "loss": 0.5112,
      "step": 271
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.12144706398248672,
      "learning_rate": 0.00012832214765100673,
      "loss": 0.4981,
      "step": 272
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.1093512549996376,
      "learning_rate": 0.0001280536912751678,
      "loss": 0.5299,
      "step": 273
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.11240535974502563,
      "learning_rate": 0.00012778523489932888,
      "loss": 0.5355,
      "step": 274
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.13993413746356964,
      "learning_rate": 0.00012751677852348994,
      "loss": 0.5946,
      "step": 275
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.10582461208105087,
      "learning_rate": 0.000127248322147651,
      "loss": 0.5201,
      "step": 276
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 0.10677764564752579,
      "learning_rate": 0.0001269798657718121,
      "loss": 0.5348,
      "step": 277
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.11306474357843399,
      "learning_rate": 0.00012671140939597318,
      "loss": 0.486,
      "step": 278
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.1132398173213005,
      "learning_rate": 0.00012644295302013424,
      "loss": 0.565,
      "step": 279
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.11835923790931702,
      "learning_rate": 0.0001261744966442953,
      "loss": 0.5361,
      "step": 280
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.12976959347724915,
      "learning_rate": 0.00012590604026845639,
      "loss": 0.6001,
      "step": 281
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.13728998601436615,
      "learning_rate": 0.00012563758389261747,
      "loss": 0.5424,
      "step": 282
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.11163897067308426,
      "learning_rate": 0.00012536912751677853,
      "loss": 0.5153,
      "step": 283
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.12988892197608948,
      "learning_rate": 0.0001251006711409396,
      "loss": 0.552,
      "step": 284
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.13172639906406403,
      "learning_rate": 0.00012483221476510068,
      "loss": 0.4873,
      "step": 285
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.12484672665596008,
      "learning_rate": 0.00012456375838926174,
      "loss": 0.5926,
      "step": 286
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.1547287255525589,
      "learning_rate": 0.00012429530201342283,
      "loss": 0.4971,
      "step": 287
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.1467929482460022,
      "learning_rate": 0.0001240268456375839,
      "loss": 0.5214,
      "step": 288
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 0.12442576885223389,
      "learning_rate": 0.00012375838926174498,
      "loss": 0.493,
      "step": 289
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.1466689109802246,
      "learning_rate": 0.00012348993288590604,
      "loss": 0.544,
      "step": 290
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.09453045576810837,
      "learning_rate": 0.00012322147651006713,
      "loss": 0.4382,
      "step": 291
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.12244310975074768,
      "learning_rate": 0.00012295302013422821,
      "loss": 0.5067,
      "step": 292
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 0.13035942614078522,
      "learning_rate": 0.00012268456375838925,
      "loss": 0.4506,
      "step": 293
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.131382018327713,
      "learning_rate": 0.00012241610738255034,
      "loss": 0.5257,
      "step": 294
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.1214059442281723,
      "learning_rate": 0.00012214765100671142,
      "loss": 0.5262,
      "step": 295
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.12746579945087433,
      "learning_rate": 0.0001218791946308725,
      "loss": 0.5236,
      "step": 296
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.11619884520769119,
      "learning_rate": 0.00012161073825503357,
      "loss": 0.4643,
      "step": 297
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.12472264468669891,
      "learning_rate": 0.00012134228187919463,
      "loss": 0.461,
      "step": 298
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.11476145684719086,
      "learning_rate": 0.0001210738255033557,
      "loss": 0.5138,
      "step": 299
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.13937227427959442,
      "learning_rate": 0.0001208053691275168,
      "loss": 0.4559,
      "step": 300
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.14570608735084534,
      "learning_rate": 0.00012053691275167787,
      "loss": 0.4929,
      "step": 301
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.1552971452474594,
      "learning_rate": 0.00012026845637583893,
      "loss": 0.5521,
      "step": 302
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.1574195921421051,
      "learning_rate": 0.00012,
      "loss": 0.5008,
      "step": 303
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.14711837470531464,
      "learning_rate": 0.00011973154362416108,
      "loss": 0.5337,
      "step": 304
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.1429002583026886,
      "learning_rate": 0.00011946308724832216,
      "loss": 0.4517,
      "step": 305
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.09851177036762238,
      "learning_rate": 0.00011919463087248324,
      "loss": 0.4903,
      "step": 306
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.10107535868883133,
      "learning_rate": 0.0001189261744966443,
      "loss": 0.4717,
      "step": 307
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.09059703350067139,
      "learning_rate": 0.00011865771812080537,
      "loss": 0.4468,
      "step": 308
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.10391360521316528,
      "learning_rate": 0.00011838926174496645,
      "loss": 0.5168,
      "step": 309
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.12881220877170563,
      "learning_rate": 0.00011812080536912754,
      "loss": 0.5045,
      "step": 310
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.12810847163200378,
      "learning_rate": 0.00011785234899328858,
      "loss": 0.5681,
      "step": 311
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.10771075636148453,
      "learning_rate": 0.00011758389261744967,
      "loss": 0.5142,
      "step": 312
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 0.13995681703090668,
      "learning_rate": 0.00011731543624161074,
      "loss": 0.4918,
      "step": 313
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.15157566964626312,
      "learning_rate": 0.00011704697986577182,
      "loss": 0.4607,
      "step": 314
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.11302800476551056,
      "learning_rate": 0.0001167785234899329,
      "loss": 0.4623,
      "step": 315
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.22739292681217194,
      "learning_rate": 0.00011651006711409395,
      "loss": 0.5433,
      "step": 316
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 0.11274749785661697,
      "learning_rate": 0.00011624161073825504,
      "loss": 0.4871,
      "step": 317
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.1452726274728775,
      "learning_rate": 0.00011597315436241611,
      "loss": 0.4855,
      "step": 318
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.1467413306236267,
      "learning_rate": 0.00011570469798657719,
      "loss": 0.5035,
      "step": 319
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.1189451590180397,
      "learning_rate": 0.00011543624161073828,
      "loss": 0.5141,
      "step": 320
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.22904513776302338,
      "learning_rate": 0.00011516778523489932,
      "loss": 0.5524,
      "step": 321
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.1359783262014389,
      "learning_rate": 0.00011489932885906041,
      "loss": 0.5529,
      "step": 322
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.2063322365283966,
      "learning_rate": 0.00011463087248322149,
      "loss": 0.6234,
      "step": 323
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.11830462515354156,
      "learning_rate": 0.00011436241610738256,
      "loss": 0.5635,
      "step": 324
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.17097453773021698,
      "learning_rate": 0.00011409395973154362,
      "loss": 0.5084,
      "step": 325
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.1499367207288742,
      "learning_rate": 0.0001138255033557047,
      "loss": 0.5676,
      "step": 326
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.1331903487443924,
      "learning_rate": 0.00011355704697986578,
      "loss": 0.5304,
      "step": 327
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.14245599508285522,
      "learning_rate": 0.00011328859060402686,
      "loss": 0.5541,
      "step": 328
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 0.11444198340177536,
      "learning_rate": 0.00011302013422818793,
      "loss": 0.5048,
      "step": 329
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.09622320532798767,
      "learning_rate": 0.00011275167785234899,
      "loss": 0.5643,
      "step": 330
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 0.17601345479488373,
      "learning_rate": 0.00011248322147651006,
      "loss": 0.4555,
      "step": 331
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.12424257397651672,
      "learning_rate": 0.00011221476510067115,
      "loss": 0.5438,
      "step": 332
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.12403170764446259,
      "learning_rate": 0.00011194630872483223,
      "loss": 0.5005,
      "step": 333
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.16326113045215607,
      "learning_rate": 0.00011167785234899329,
      "loss": 0.4778,
      "step": 334
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.21255703270435333,
      "learning_rate": 0.00011140939597315436,
      "loss": 0.6153,
      "step": 335
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.12876038253307343,
      "learning_rate": 0.00011114093959731544,
      "loss": 0.5454,
      "step": 336
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.2243720144033432,
      "learning_rate": 0.00011087248322147652,
      "loss": 0.5282,
      "step": 337
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.15364325046539307,
      "learning_rate": 0.0001106040268456376,
      "loss": 0.4957,
      "step": 338
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.14804615080356598,
      "learning_rate": 0.00011033557046979866,
      "loss": 0.4967,
      "step": 339
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.13152191042900085,
      "learning_rate": 0.00011006711409395973,
      "loss": 0.4794,
      "step": 340
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.13506615161895752,
      "learning_rate": 0.0001097986577181208,
      "loss": 0.4502,
      "step": 341
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.16427068412303925,
      "learning_rate": 0.0001095302013422819,
      "loss": 0.4768,
      "step": 342
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 0.11988607794046402,
      "learning_rate": 0.00010926174496644297,
      "loss": 0.5478,
      "step": 343
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.2750554382801056,
      "learning_rate": 0.00010899328859060403,
      "loss": 0.5299,
      "step": 344
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.1299515813589096,
      "learning_rate": 0.0001087248322147651,
      "loss": 0.5901,
      "step": 345
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.09776704758405685,
      "learning_rate": 0.00010845637583892618,
      "loss": 0.458,
      "step": 346
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.17601116001605988,
      "learning_rate": 0.00010818791946308726,
      "loss": 0.5623,
      "step": 347
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.16389335691928864,
      "learning_rate": 0.00010791946308724831,
      "loss": 0.5126,
      "step": 348
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.1330176740884781,
      "learning_rate": 0.0001076510067114094,
      "loss": 0.5139,
      "step": 349
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.16010861098766327,
      "learning_rate": 0.00010738255033557047,
      "loss": 0.4954,
      "step": 350
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.0982598140835762,
      "learning_rate": 0.00010711409395973155,
      "loss": 0.4608,
      "step": 351
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.16528964042663574,
      "learning_rate": 0.00010684563758389264,
      "loss": 0.5258,
      "step": 352
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.12659575045108795,
      "learning_rate": 0.0001065771812080537,
      "loss": 0.5232,
      "step": 353
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.09136935323476791,
      "learning_rate": 0.00010630872483221477,
      "loss": 0.4931,
      "step": 354
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.15239731967449188,
      "learning_rate": 0.00010604026845637584,
      "loss": 0.4958,
      "step": 355
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.12044967710971832,
      "learning_rate": 0.00010577181208053693,
      "loss": 0.44,
      "step": 356
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.13787251710891724,
      "learning_rate": 0.00010550335570469798,
      "loss": 0.4849,
      "step": 357
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.1228780522942543,
      "learning_rate": 0.00010523489932885907,
      "loss": 0.5131,
      "step": 358
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 0.1341889351606369,
      "learning_rate": 0.00010496644295302014,
      "loss": 0.4845,
      "step": 359
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.14094920456409454,
      "learning_rate": 0.00010469798657718121,
      "loss": 0.5399,
      "step": 360
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.1421298384666443,
      "learning_rate": 0.0001044295302013423,
      "loss": 0.544,
      "step": 361
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.1081862822175026,
      "learning_rate": 0.00010416107382550335,
      "loss": 0.487,
      "step": 362
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.14071348309516907,
      "learning_rate": 0.00010389261744966444,
      "loss": 0.5703,
      "step": 363
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.12189878523349762,
      "learning_rate": 0.00010362416107382551,
      "loss": 0.4831,
      "step": 364
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.12419838458299637,
      "learning_rate": 0.00010335570469798659,
      "loss": 0.5051,
      "step": 365
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.10525435954332352,
      "learning_rate": 0.00010308724832214767,
      "loss": 0.5198,
      "step": 366
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.14943276345729828,
      "learning_rate": 0.00010281879194630872,
      "loss": 0.5722,
      "step": 367
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.14764931797981262,
      "learning_rate": 0.00010255033557046981,
      "loss": 0.5047,
      "step": 368
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.13344071805477142,
      "learning_rate": 0.00010228187919463088,
      "loss": 0.5297,
      "step": 369
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.14890243113040924,
      "learning_rate": 0.00010201342281879196,
      "loss": 0.5225,
      "step": 370
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.14297525584697723,
      "learning_rate": 0.00010174496644295302,
      "loss": 0.5321,
      "step": 371
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.12593573331832886,
      "learning_rate": 0.00010147651006711409,
      "loss": 0.5433,
      "step": 372
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.137226864695549,
      "learning_rate": 0.00010120805369127518,
      "loss": 0.5106,
      "step": 373
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.14991889894008636,
      "learning_rate": 0.00010093959731543625,
      "loss": 0.513,
      "step": 374
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.13540974259376526,
      "learning_rate": 0.00010067114093959733,
      "loss": 0.5588,
      "step": 375
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.17612580955028534,
      "learning_rate": 0.00010040268456375839,
      "loss": 0.4818,
      "step": 376
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.11235078424215317,
      "learning_rate": 0.00010013422818791946,
      "loss": 0.4607,
      "step": 377
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.14630775153636932,
      "learning_rate": 9.986577181208055e-05,
      "loss": 0.4945,
      "step": 378
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.17776870727539062,
      "learning_rate": 9.959731543624161e-05,
      "loss": 0.5478,
      "step": 379
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.15833646059036255,
      "learning_rate": 9.93288590604027e-05,
      "loss": 0.4892,
      "step": 380
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.15081846714019775,
      "learning_rate": 9.906040268456376e-05,
      "loss": 0.5222,
      "step": 381
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.11610275506973267,
      "learning_rate": 9.879194630872483e-05,
      "loss": 0.5052,
      "step": 382
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 0.17411361634731293,
      "learning_rate": 9.852348993288592e-05,
      "loss": 0.517,
      "step": 383
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.13529324531555176,
      "learning_rate": 9.825503355704698e-05,
      "loss": 0.5136,
      "step": 384
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.12368618696928024,
      "learning_rate": 9.798657718120807e-05,
      "loss": 0.4916,
      "step": 385
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.1376359909772873,
      "learning_rate": 9.771812080536913e-05,
      "loss": 0.4891,
      "step": 386
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.14586645364761353,
      "learning_rate": 9.74496644295302e-05,
      "loss": 0.4574,
      "step": 387
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.13392485678195953,
      "learning_rate": 9.718120805369128e-05,
      "loss": 0.5488,
      "step": 388
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.1384684145450592,
      "learning_rate": 9.691275167785235e-05,
      "loss": 0.4989,
      "step": 389
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.12202391028404236,
      "learning_rate": 9.664429530201344e-05,
      "loss": 0.4919,
      "step": 390
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 0.1363389939069748,
      "learning_rate": 9.63758389261745e-05,
      "loss": 0.4915,
      "step": 391
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.08643393218517303,
      "learning_rate": 9.610738255033557e-05,
      "loss": 0.4545,
      "step": 392
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.1437472254037857,
      "learning_rate": 9.583892617449665e-05,
      "loss": 0.5335,
      "step": 393
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.21450835466384888,
      "learning_rate": 9.557046979865772e-05,
      "loss": 0.5297,
      "step": 394
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.10569456964731216,
      "learning_rate": 9.53020134228188e-05,
      "loss": 0.5008,
      "step": 395
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.1381160169839859,
      "learning_rate": 9.503355704697987e-05,
      "loss": 0.5031,
      "step": 396
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 0.1586257964372635,
      "learning_rate": 9.476510067114094e-05,
      "loss": 0.4986,
      "step": 397
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.18046875298023224,
      "learning_rate": 9.449664429530202e-05,
      "loss": 0.5311,
      "step": 398
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.10972803086042404,
      "learning_rate": 9.422818791946309e-05,
      "loss": 0.5106,
      "step": 399
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.168837308883667,
      "learning_rate": 9.395973154362417e-05,
      "loss": 0.5302,
      "step": 400
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.12625263631343842,
      "learning_rate": 9.369127516778524e-05,
      "loss": 0.4966,
      "step": 401
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.1258689910173416,
      "learning_rate": 9.342281879194631e-05,
      "loss": 0.4902,
      "step": 402
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.14573025703430176,
      "learning_rate": 9.315436241610739e-05,
      "loss": 0.5595,
      "step": 403
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.08677715063095093,
      "learning_rate": 9.288590604026846e-05,
      "loss": 0.4551,
      "step": 404
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.11850690096616745,
      "learning_rate": 9.261744966442954e-05,
      "loss": 0.496,
      "step": 405
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.11914549767971039,
      "learning_rate": 9.234899328859061e-05,
      "loss": 0.5223,
      "step": 406
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.13783225417137146,
      "learning_rate": 9.208053691275168e-05,
      "loss": 0.5027,
      "step": 407
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.10206591337919235,
      "learning_rate": 9.181208053691276e-05,
      "loss": 0.4674,
      "step": 408
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 0.10678739100694656,
      "learning_rate": 9.154362416107383e-05,
      "loss": 0.4787,
      "step": 409
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.10575684905052185,
      "learning_rate": 9.127516778523491e-05,
      "loss": 0.5064,
      "step": 410
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.11742957681417465,
      "learning_rate": 9.100671140939597e-05,
      "loss": 0.4303,
      "step": 411
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.14177927374839783,
      "learning_rate": 9.073825503355706e-05,
      "loss": 0.5746,
      "step": 412
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.11582808941602707,
      "learning_rate": 9.046979865771813e-05,
      "loss": 0.5377,
      "step": 413
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.0904831513762474,
      "learning_rate": 9.02013422818792e-05,
      "loss": 0.4586,
      "step": 414
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.12081251293420792,
      "learning_rate": 8.993288590604028e-05,
      "loss": 0.516,
      "step": 415
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.10882513225078583,
      "learning_rate": 8.966442953020134e-05,
      "loss": 0.5241,
      "step": 416
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.11608737707138062,
      "learning_rate": 8.939597315436243e-05,
      "loss": 0.5138,
      "step": 417
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.1165337935090065,
      "learning_rate": 8.912751677852349e-05,
      "loss": 0.5646,
      "step": 418
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.10970687121152878,
      "learning_rate": 8.885906040268457e-05,
      "loss": 0.5108,
      "step": 419
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.11673825234174728,
      "learning_rate": 8.859060402684565e-05,
      "loss": 0.5426,
      "step": 420
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.1268654316663742,
      "learning_rate": 8.832214765100671e-05,
      "loss": 0.5365,
      "step": 421
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.1385861188173294,
      "learning_rate": 8.80536912751678e-05,
      "loss": 0.508,
      "step": 422
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.13843756914138794,
      "learning_rate": 8.778523489932886e-05,
      "loss": 0.4937,
      "step": 423
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.12318234145641327,
      "learning_rate": 8.751677852348994e-05,
      "loss": 0.5093,
      "step": 424
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.1258506327867508,
      "learning_rate": 8.7248322147651e-05,
      "loss": 0.5346,
      "step": 425
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.13029739260673523,
      "learning_rate": 8.697986577181208e-05,
      "loss": 0.4212,
      "step": 426
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 0.11432024091482162,
      "learning_rate": 8.671140939597315e-05,
      "loss": 0.5378,
      "step": 427
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.1009708046913147,
      "learning_rate": 8.644295302013423e-05,
      "loss": 0.4693,
      "step": 428
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.13719695806503296,
      "learning_rate": 8.617449664429532e-05,
      "loss": 0.5534,
      "step": 429
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.12394293397665024,
      "learning_rate": 8.590604026845638e-05,
      "loss": 0.5396,
      "step": 430
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 0.12633182108402252,
      "learning_rate": 8.563758389261745e-05,
      "loss": 0.5032,
      "step": 431
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.13319304585456848,
      "learning_rate": 8.536912751677852e-05,
      "loss": 0.5197,
      "step": 432
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.1446397751569748,
      "learning_rate": 8.51006711409396e-05,
      "loss": 0.5655,
      "step": 433
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.11442547291517258,
      "learning_rate": 8.483221476510067e-05,
      "loss": 0.469,
      "step": 434
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.13972727954387665,
      "learning_rate": 8.456375838926175e-05,
      "loss": 0.5104,
      "step": 435
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.1339639127254486,
      "learning_rate": 8.429530201342283e-05,
      "loss": 0.5074,
      "step": 436
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.14074550569057465,
      "learning_rate": 8.40268456375839e-05,
      "loss": 0.5777,
      "step": 437
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.10726700723171234,
      "learning_rate": 8.375838926174497e-05,
      "loss": 0.5207,
      "step": 438
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.1700795292854309,
      "learning_rate": 8.348993288590604e-05,
      "loss": 0.5554,
      "step": 439
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.12143436819314957,
      "learning_rate": 8.322147651006712e-05,
      "loss": 0.5323,
      "step": 440
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.10514678806066513,
      "learning_rate": 8.295302013422819e-05,
      "loss": 0.4728,
      "step": 441
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.11956241726875305,
      "learning_rate": 8.268456375838927e-05,
      "loss": 0.4929,
      "step": 442
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 0.13336734473705292,
      "learning_rate": 8.241610738255034e-05,
      "loss": 0.4968,
      "step": 443
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.13198284804821014,
      "learning_rate": 8.214765100671141e-05,
      "loss": 0.4315,
      "step": 444
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.12160442769527435,
      "learning_rate": 8.187919463087249e-05,
      "loss": 0.4633,
      "step": 445
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.10285260528326035,
      "learning_rate": 8.161073825503356e-05,
      "loss": 0.4962,
      "step": 446
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.15911969542503357,
      "learning_rate": 8.134228187919464e-05,
      "loss": 0.5401,
      "step": 447
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.09246931225061417,
      "learning_rate": 8.107382550335571e-05,
      "loss": 0.4087,
      "step": 448
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.13845773041248322,
      "learning_rate": 8.080536912751678e-05,
      "loss": 0.5631,
      "step": 449
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.18733376264572144,
      "learning_rate": 8.053691275167784e-05,
      "loss": 0.4445,
      "step": 450
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.10032053291797638,
      "learning_rate": 8.026845637583893e-05,
      "loss": 0.5023,
      "step": 451
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.12969724833965302,
      "learning_rate": 8e-05,
      "loss": 0.4827,
      "step": 452
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.11876823753118515,
      "learning_rate": 7.973154362416108e-05,
      "loss": 0.4541,
      "step": 453
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.10615856200456619,
      "learning_rate": 7.946308724832215e-05,
      "loss": 0.5237,
      "step": 454
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.10967779159545898,
      "learning_rate": 7.919463087248322e-05,
      "loss": 0.5273,
      "step": 455
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.0999264270067215,
      "learning_rate": 7.89261744966443e-05,
      "loss": 0.4847,
      "step": 456
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.21545551717281342,
      "learning_rate": 7.865771812080536e-05,
      "loss": 0.4958,
      "step": 457
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.14130298793315887,
      "learning_rate": 7.838926174496645e-05,
      "loss": 0.4928,
      "step": 458
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.16973553597927094,
      "learning_rate": 7.812080536912753e-05,
      "loss": 0.5064,
      "step": 459
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.20457835495471954,
      "learning_rate": 7.78523489932886e-05,
      "loss": 0.5416,
      "step": 460
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.1462077647447586,
      "learning_rate": 7.758389261744967e-05,
      "loss": 0.5436,
      "step": 461
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.12730465829372406,
      "learning_rate": 7.731543624161073e-05,
      "loss": 0.5275,
      "step": 462
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.1968497633934021,
      "learning_rate": 7.704697986577182e-05,
      "loss": 0.5332,
      "step": 463
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.13347235321998596,
      "learning_rate": 7.677852348993288e-05,
      "loss": 0.5061,
      "step": 464
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.12414992600679398,
      "learning_rate": 7.651006711409397e-05,
      "loss": 0.5015,
      "step": 465
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.11551976203918457,
      "learning_rate": 7.624161073825504e-05,
      "loss": 0.5143,
      "step": 466
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.1662350594997406,
      "learning_rate": 7.59731543624161e-05,
      "loss": 0.5465,
      "step": 467
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.1733846217393875,
      "learning_rate": 7.570469798657719e-05,
      "loss": 0.5389,
      "step": 468
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.10724364966154099,
      "learning_rate": 7.543624161073825e-05,
      "loss": 0.5058,
      "step": 469
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.14774106442928314,
      "learning_rate": 7.516778523489934e-05,
      "loss": 0.473,
      "step": 470
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.12069293856620789,
      "learning_rate": 7.48993288590604e-05,
      "loss": 0.4738,
      "step": 471
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.14744384586811066,
      "learning_rate": 7.463087248322148e-05,
      "loss": 0.5121,
      "step": 472
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 0.12881991267204285,
      "learning_rate": 7.436241610738255e-05,
      "loss": 0.485,
      "step": 473
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.1452915519475937,
      "learning_rate": 7.409395973154362e-05,
      "loss": 0.5628,
      "step": 474
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.17099301517009735,
      "learning_rate": 7.382550335570471e-05,
      "loss": 0.5731,
      "step": 475
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.12683796882629395,
      "learning_rate": 7.355704697986577e-05,
      "loss": 0.4888,
      "step": 476
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.1079927384853363,
      "learning_rate": 7.328859060402685e-05,
      "loss": 0.4399,
      "step": 477
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.10541452467441559,
      "learning_rate": 7.302013422818792e-05,
      "loss": 0.5083,
      "step": 478
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 0.136881485581398,
      "learning_rate": 7.2751677852349e-05,
      "loss": 0.5057,
      "step": 479
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.1318810135126114,
      "learning_rate": 7.248322147651007e-05,
      "loss": 0.482,
      "step": 480
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.148305743932724,
      "learning_rate": 7.221476510067114e-05,
      "loss": 0.5203,
      "step": 481
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.1048143208026886,
      "learning_rate": 7.194630872483222e-05,
      "loss": 0.4012,
      "step": 482
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.12890848517417908,
      "learning_rate": 7.167785234899329e-05,
      "loss": 0.4913,
      "step": 483
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.10563205927610397,
      "learning_rate": 7.140939597315436e-05,
      "loss": 0.5438,
      "step": 484
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.11557302623987198,
      "learning_rate": 7.114093959731544e-05,
      "loss": 0.471,
      "step": 485
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.12772585451602936,
      "learning_rate": 7.087248322147651e-05,
      "loss": 0.4459,
      "step": 486
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.15494950115680695,
      "learning_rate": 7.060402684563759e-05,
      "loss": 0.5365,
      "step": 487
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.1383446753025055,
      "learning_rate": 7.033557046979866e-05,
      "loss": 0.537,
      "step": 488
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.11445358395576477,
      "learning_rate": 7.006711409395974e-05,
      "loss": 0.4446,
      "step": 489
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.16382922232151031,
      "learning_rate": 6.979865771812081e-05,
      "loss": 0.5759,
      "step": 490
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 0.09466589987277985,
      "learning_rate": 6.953020134228188e-05,
      "loss": 0.4484,
      "step": 491
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.10677800327539444,
      "learning_rate": 6.926174496644296e-05,
      "loss": 0.4753,
      "step": 492
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.1479160189628601,
      "learning_rate": 6.899328859060403e-05,
      "loss": 0.475,
      "step": 493
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.10687734931707382,
      "learning_rate": 6.87248322147651e-05,
      "loss": 0.5193,
      "step": 494
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.12719112634658813,
      "learning_rate": 6.845637583892618e-05,
      "loss": 0.5906,
      "step": 495
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.10656489431858063,
      "learning_rate": 6.818791946308724e-05,
      "loss": 0.4571,
      "step": 496
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.12834610044956207,
      "learning_rate": 6.791946308724833e-05,
      "loss": 0.5932,
      "step": 497
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.09979817271232605,
      "learning_rate": 6.76510067114094e-05,
      "loss": 0.5217,
      "step": 498
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.11064504086971283,
      "learning_rate": 6.738255033557048e-05,
      "loss": 0.4497,
      "step": 499
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.14562785625457764,
      "learning_rate": 6.711409395973155e-05,
      "loss": 0.4787,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7017227936485868e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
