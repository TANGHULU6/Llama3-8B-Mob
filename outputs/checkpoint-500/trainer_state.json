{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8333333333333334,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.0956664085388184,
      "learning_rate": 4e-05,
      "loss": 1.9448,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.2918241024017334,
      "learning_rate": 8e-05,
      "loss": 1.8955,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.376105308532715,
      "learning_rate": 0.00012,
      "loss": 1.9635,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.942995309829712,
      "learning_rate": 0.00016,
      "loss": 1.7703,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4782493114471436,
      "learning_rate": 0.0002,
      "loss": 1.558,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.251857042312622,
      "learning_rate": 0.00019993322203672788,
      "loss": 1.3528,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.6353691816329956,
      "learning_rate": 0.00019986644407345576,
      "loss": 1.0731,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.6602898836135864,
      "learning_rate": 0.00019979966611018366,
      "loss": 1.2105,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.6666288375854492,
      "learning_rate": 0.00019973288814691153,
      "loss": 0.8511,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 2.097238302230835,
      "learning_rate": 0.0001996661101836394,
      "loss": 0.9601,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.886622428894043,
      "learning_rate": 0.00019959933222036728,
      "loss": 0.6888,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8481279015541077,
      "learning_rate": 0.00019953255425709515,
      "loss": 0.4869,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.9076886177062988,
      "learning_rate": 0.00019946577629382305,
      "loss": 0.5155,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.4742388427257538,
      "learning_rate": 0.00019939899833055092,
      "loss": 0.6427,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.42061689496040344,
      "learning_rate": 0.00019933222036727882,
      "loss": 0.4551,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.49660366773605347,
      "learning_rate": 0.0001992654424040067,
      "loss": 0.6552,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.29572218656539917,
      "learning_rate": 0.00019919866444073457,
      "loss": 0.46,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.20609429478645325,
      "learning_rate": 0.00019913188647746244,
      "loss": 0.4393,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.2833709120750427,
      "learning_rate": 0.00019906510851419034,
      "loss": 0.4794,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3383139669895172,
      "learning_rate": 0.00019899833055091822,
      "loss": 0.5512,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2814450263977051,
      "learning_rate": 0.0001989315525876461,
      "loss": 0.5206,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.18887953460216522,
      "learning_rate": 0.00019886477462437396,
      "loss": 0.4972,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.1577294021844864,
      "learning_rate": 0.00019879799666110183,
      "loss": 0.3929,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.16395266354084015,
      "learning_rate": 0.00019873121869782974,
      "loss": 0.4311,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.19629164040088654,
      "learning_rate": 0.0001986644407345576,
      "loss": 0.4407,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.19796745479106903,
      "learning_rate": 0.00019859766277128548,
      "loss": 0.5199,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.15026581287384033,
      "learning_rate": 0.00019853088480801335,
      "loss": 0.444,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.15527544915676117,
      "learning_rate": 0.00019846410684474123,
      "loss": 0.47,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.12846054136753082,
      "learning_rate": 0.00019839732888146913,
      "loss": 0.3947,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15415337681770325,
      "learning_rate": 0.000198330550918197,
      "loss": 0.4066,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.16547812521457672,
      "learning_rate": 0.00019826377295492487,
      "loss": 0.4745,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10649402439594269,
      "learning_rate": 0.00019819699499165277,
      "loss": 0.4897,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.1004398912191391,
      "learning_rate": 0.00019813021702838065,
      "loss": 0.2804,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.1579684317111969,
      "learning_rate": 0.00019806343906510852,
      "loss": 0.4162,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.14333078265190125,
      "learning_rate": 0.00019799666110183642,
      "loss": 0.3944,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11239796131849289,
      "learning_rate": 0.0001979298831385643,
      "loss": 0.4197,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.11745484918355942,
      "learning_rate": 0.00019786310517529217,
      "loss": 0.3054,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.11401297152042389,
      "learning_rate": 0.00019779632721202004,
      "loss": 0.3476,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.11132606118917465,
      "learning_rate": 0.00019772954924874791,
      "loss": 0.3451,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.10694670677185059,
      "learning_rate": 0.00019766277128547581,
      "loss": 0.325,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.10581845790147781,
      "learning_rate": 0.0001975959933222037,
      "loss": 0.3798,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.09086530655622482,
      "learning_rate": 0.00019752921535893156,
      "loss": 0.3476,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.10241302847862244,
      "learning_rate": 0.00019746243739565943,
      "loss": 0.3672,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09187440574169159,
      "learning_rate": 0.0001973956594323873,
      "loss": 0.4196,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12214738875627518,
      "learning_rate": 0.0001973288814691152,
      "loss": 0.4203,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.11255165189504623,
      "learning_rate": 0.00019726210350584308,
      "loss": 0.3168,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.09649581462144852,
      "learning_rate": 0.00019719532554257095,
      "loss": 0.3994,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08454272150993347,
      "learning_rate": 0.00019712854757929883,
      "loss": 0.2907,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.09278556704521179,
      "learning_rate": 0.00019706176961602673,
      "loss": 0.3851,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.10700076818466187,
      "learning_rate": 0.0001969949916527546,
      "loss": 0.3579,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.08786964416503906,
      "learning_rate": 0.0001969282136894825,
      "loss": 0.3716,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.09527817368507385,
      "learning_rate": 0.00019686143572621037,
      "loss": 0.4149,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.06768197566270828,
      "learning_rate": 0.00019679465776293825,
      "loss": 0.3567,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.11477317661046982,
      "learning_rate": 0.00019672787979966612,
      "loss": 0.4293,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.084235779941082,
      "learning_rate": 0.000196661101836394,
      "loss": 0.3019,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.10784310102462769,
      "learning_rate": 0.0001965943238731219,
      "loss": 0.3491,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.10310035198926926,
      "learning_rate": 0.00019652754590984977,
      "loss": 0.3831,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.1375250220298767,
      "learning_rate": 0.00019646076794657764,
      "loss": 0.4089,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.07553678750991821,
      "learning_rate": 0.0001963939899833055,
      "loss": 0.3023,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0941384807229042,
      "learning_rate": 0.00019632721202003339,
      "loss": 0.3806,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.06856777518987656,
      "learning_rate": 0.00019626043405676129,
      "loss": 0.2628,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.07682006061077118,
      "learning_rate": 0.00019619365609348916,
      "loss": 0.3936,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06746046990156174,
      "learning_rate": 0.00019612687813021703,
      "loss": 0.3716,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07772789150476456,
      "learning_rate": 0.0001960601001669449,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.08953963220119476,
      "learning_rate": 0.00019599332220367278,
      "loss": 0.3442,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.0995715856552124,
      "learning_rate": 0.00019592654424040068,
      "loss": 0.3373,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.07466694712638855,
      "learning_rate": 0.00019585976627712855,
      "loss": 0.3045,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.08509878069162369,
      "learning_rate": 0.00019579298831385645,
      "loss": 0.2943,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.12085497379302979,
      "learning_rate": 0.00019572621035058433,
      "loss": 0.3952,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.10573051869869232,
      "learning_rate": 0.0001956594323873122,
      "loss": 0.3791,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.09017354995012283,
      "learning_rate": 0.00019559265442404007,
      "loss": 0.3193,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0738191232085228,
      "learning_rate": 0.00019552587646076797,
      "loss": 0.3396,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.06006060913205147,
      "learning_rate": 0.00019545909849749584,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.15513241291046143,
      "learning_rate": 0.00019539232053422372,
      "loss": 0.3864,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.07705923914909363,
      "learning_rate": 0.0001953255425709516,
      "loss": 0.3251,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17013779282569885,
      "learning_rate": 0.00019525876460767946,
      "loss": 0.3971,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.08929529041051865,
      "learning_rate": 0.00019519198664440736,
      "loss": 0.3465,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.08618784695863724,
      "learning_rate": 0.00019512520868113524,
      "loss": 0.3251,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.11940111219882965,
      "learning_rate": 0.0001950584307178631,
      "loss": 0.386,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08647278696298599,
      "learning_rate": 0.00019499165275459098,
      "loss": 0.3328,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.08629181981086731,
      "learning_rate": 0.00019492487479131886,
      "loss": 0.3497,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.09361159801483154,
      "learning_rate": 0.00019485809682804673,
      "loss": 0.3479,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.06511078774929047,
      "learning_rate": 0.00019479131886477463,
      "loss": 0.262,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0810856968164444,
      "learning_rate": 0.0001947245409015025,
      "loss": 0.375,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.06852146983146667,
      "learning_rate": 0.0001946577629382304,
      "loss": 0.3839,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.08113353699445724,
      "learning_rate": 0.00019459098497495828,
      "loss": 0.3473,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.07892511785030365,
      "learning_rate": 0.00019452420701168615,
      "loss": 0.2601,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08675333112478256,
      "learning_rate": 0.00019445742904841405,
      "loss": 0.3368,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.07467499375343323,
      "learning_rate": 0.00019439065108514192,
      "loss": 0.3206,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06793464720249176,
      "learning_rate": 0.0001943238731218698,
      "loss": 0.2437,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.16194306313991547,
      "learning_rate": 0.00019425709515859767,
      "loss": 0.3755,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08678337186574936,
      "learning_rate": 0.00019419031719532554,
      "loss": 0.364,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.09742604196071625,
      "learning_rate": 0.00019412353923205344,
      "loss": 0.336,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.12137395888566971,
      "learning_rate": 0.00019405676126878132,
      "loss": 0.3643,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.08253426104784012,
      "learning_rate": 0.0001939899833055092,
      "loss": 0.3209,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.07981440424919128,
      "learning_rate": 0.00019392320534223706,
      "loss": 0.3538,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.10532163828611374,
      "learning_rate": 0.00019385642737896494,
      "loss": 0.3208,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.06535062938928604,
      "learning_rate": 0.0001937896494156928,
      "loss": 0.3372,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.05981944501399994,
      "learning_rate": 0.0001937228714524207,
      "loss": 0.2412,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.09706909954547882,
      "learning_rate": 0.00019365609348914858,
      "loss": 0.4219,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.06982459872961044,
      "learning_rate": 0.00019358931552587646,
      "loss": 0.2833,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.08719934523105621,
      "learning_rate": 0.00019352253756260436,
      "loss": 0.353,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.08656898885965347,
      "learning_rate": 0.00019345575959933223,
      "loss": 0.346,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.06393106281757355,
      "learning_rate": 0.00019338898163606013,
      "loss": 0.2793,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.11401592195034027,
      "learning_rate": 0.000193322203672788,
      "loss": 0.4255,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.08084436506032944,
      "learning_rate": 0.00019325542570951588,
      "loss": 0.3427,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.07798241823911667,
      "learning_rate": 0.00019318864774624375,
      "loss": 0.4689,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.08108892291784286,
      "learning_rate": 0.00019312186978297162,
      "loss": 0.402,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.11977798491716385,
      "learning_rate": 0.00019305509181969952,
      "loss": 0.3084,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.058830082416534424,
      "learning_rate": 0.0001929883138564274,
      "loss": 0.3936,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.07111568003892899,
      "learning_rate": 0.00019292153589315527,
      "loss": 0.3664,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06430325657129288,
      "learning_rate": 0.00019285475792988314,
      "loss": 0.3048,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.06825023144483566,
      "learning_rate": 0.00019278797996661101,
      "loss": 0.3785,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.12661273777484894,
      "learning_rate": 0.0001927212020033389,
      "loss": 0.4042,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08384962379932404,
      "learning_rate": 0.0001926544240400668,
      "loss": 0.3674,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.12418660521507263,
      "learning_rate": 0.00019258764607679466,
      "loss": 0.3506,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.061764538288116455,
      "learning_rate": 0.00019252086811352253,
      "loss": 0.305,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.06392883509397507,
      "learning_rate": 0.0001924540901502504,
      "loss": 0.3497,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.09458301216363907,
      "learning_rate": 0.0001923873121869783,
      "loss": 0.3263,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05542557314038277,
      "learning_rate": 0.00019232053422370618,
      "loss": 0.2906,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.0728297159075737,
      "learning_rate": 0.00019225375626043408,
      "loss": 0.3746,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06852685660123825,
      "learning_rate": 0.00019218697829716195,
      "loss": 0.34,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05804547667503357,
      "learning_rate": 0.00019212020033388983,
      "loss": 0.2878,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06328660249710083,
      "learning_rate": 0.0001920534223706177,
      "loss": 0.3435,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.07714178413152695,
      "learning_rate": 0.0001919866444073456,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.07320096343755722,
      "learning_rate": 0.00019191986644407347,
      "loss": 0.3508,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.07564099133014679,
      "learning_rate": 0.00019185308848080135,
      "loss": 0.3074,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06308815628290176,
      "learning_rate": 0.00019178631051752922,
      "loss": 0.2524,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.07419486343860626,
      "learning_rate": 0.0001917195325542571,
      "loss": 0.3869,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.09122984856367111,
      "learning_rate": 0.00019165275459098497,
      "loss": 0.4091,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09594853967428207,
      "learning_rate": 0.00019158597662771287,
      "loss": 0.4443,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07332075387239456,
      "learning_rate": 0.00019151919866444074,
      "loss": 0.351,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.05169367790222168,
      "learning_rate": 0.0001914524207011686,
      "loss": 0.2951,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05943819880485535,
      "learning_rate": 0.0001913856427378965,
      "loss": 0.3033,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.0833563581109047,
      "learning_rate": 0.0001913188647746244,
      "loss": 0.3152,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05343350023031235,
      "learning_rate": 0.00019125208681135226,
      "loss": 0.2989,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.0843648836016655,
      "learning_rate": 0.00019118530884808016,
      "loss": 0.354,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.056954026222229004,
      "learning_rate": 0.00019111853088480803,
      "loss": 0.3748,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.07683058083057404,
      "learning_rate": 0.0001910517529215359,
      "loss": 0.3557,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.08052007853984833,
      "learning_rate": 0.00019098497495826378,
      "loss": 0.2724,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.07334864139556885,
      "learning_rate": 0.00019091819699499168,
      "loss": 0.3755,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.05416643247008324,
      "learning_rate": 0.00019085141903171955,
      "loss": 0.2737,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.0647912248969078,
      "learning_rate": 0.00019078464106844743,
      "loss": 0.3337,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.08493472635746002,
      "learning_rate": 0.0001907178631051753,
      "loss": 0.3992,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.06978499889373779,
      "learning_rate": 0.00019065108514190317,
      "loss": 0.3452,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.08354634791612625,
      "learning_rate": 0.00019058430717863107,
      "loss": 0.3801,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.08108527958393097,
      "learning_rate": 0.00019051752921535895,
      "loss": 0.3377,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06336899101734161,
      "learning_rate": 0.00019045075125208682,
      "loss": 0.3086,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.09381486475467682,
      "learning_rate": 0.0001903839732888147,
      "loss": 0.3614,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.0781172513961792,
      "learning_rate": 0.00019031719532554257,
      "loss": 0.3664,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.07172472029924393,
      "learning_rate": 0.00019025041736227044,
      "loss": 0.3111,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06654836237430573,
      "learning_rate": 0.00019018363939899834,
      "loss": 0.2931,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09867861866950989,
      "learning_rate": 0.0001901168614357262,
      "loss": 0.4418,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08223884552717209,
      "learning_rate": 0.0001900500834724541,
      "loss": 0.3609,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.0780235156416893,
      "learning_rate": 0.00018998330550918199,
      "loss": 0.3251,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06921236962080002,
      "learning_rate": 0.00018991652754590986,
      "loss": 0.3675,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.06987825036048889,
      "learning_rate": 0.00018984974958263776,
      "loss": 0.296,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.05670154094696045,
      "learning_rate": 0.00018978297161936563,
      "loss": 0.3226,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05686641484498978,
      "learning_rate": 0.0001897161936560935,
      "loss": 0.2558,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.08335665613412857,
      "learning_rate": 0.00018964941569282138,
      "loss": 0.3413,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.061617542058229446,
      "learning_rate": 0.00018958263772954925,
      "loss": 0.3585,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.07397318631410599,
      "learning_rate": 0.00018951585976627715,
      "loss": 0.2825,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.07524124532938004,
      "learning_rate": 0.00018944908180300502,
      "loss": 0.2172,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.09329230338335037,
      "learning_rate": 0.0001893823038397329,
      "loss": 0.365,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.0736825168132782,
      "learning_rate": 0.00018931552587646077,
      "loss": 0.3335,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.06876420229673386,
      "learning_rate": 0.00018924874791318864,
      "loss": 0.3162,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.06728898733854294,
      "learning_rate": 0.00018918196994991652,
      "loss": 0.3171,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06478077173233032,
      "learning_rate": 0.00018911519198664442,
      "loss": 0.2978,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.06718949228525162,
      "learning_rate": 0.0001890484140233723,
      "loss": 0.2701,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.06102819740772247,
      "learning_rate": 0.00018898163606010016,
      "loss": 0.3441,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.0668511912226677,
      "learning_rate": 0.00018891485809682806,
      "loss": 0.3145,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05413564294576645,
      "learning_rate": 0.00018884808013355594,
      "loss": 0.2821,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.051658570766448975,
      "learning_rate": 0.00018878130217028384,
      "loss": 0.2528,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.08169940859079361,
      "learning_rate": 0.0001887145242070117,
      "loss": 0.3672,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.08725954592227936,
      "learning_rate": 0.00018864774624373958,
      "loss": 0.3994,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.06593123078346252,
      "learning_rate": 0.00018858096828046746,
      "loss": 0.281,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.06933767348527908,
      "learning_rate": 0.00018851419031719533,
      "loss": 0.2368,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.10475962609052658,
      "learning_rate": 0.00018844741235392323,
      "loss": 0.3811,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.07346801459789276,
      "learning_rate": 0.0001883806343906511,
      "loss": 0.3234,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10945428907871246,
      "learning_rate": 0.00018831385642737898,
      "loss": 0.4434,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.0850963145494461,
      "learning_rate": 0.00018824707846410685,
      "loss": 0.3818,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06807483732700348,
      "learning_rate": 0.00018818030050083472,
      "loss": 0.3981,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.07143398374319077,
      "learning_rate": 0.0001881135225375626,
      "loss": 0.2976,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.060877878218889236,
      "learning_rate": 0.0001880467445742905,
      "loss": 0.3016,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.05298999696969986,
      "learning_rate": 0.00018797996661101837,
      "loss": 0.242,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.07313723862171173,
      "learning_rate": 0.00018791318864774624,
      "loss": 0.2949,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.06976813077926636,
      "learning_rate": 0.00018784641068447412,
      "loss": 0.3056,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.06937415897846222,
      "learning_rate": 0.00018777963272120202,
      "loss": 0.3172,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.07303337752819061,
      "learning_rate": 0.0001877128547579299,
      "loss": 0.3322,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.06589815765619278,
      "learning_rate": 0.0001876460767946578,
      "loss": 0.2966,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.041662562638521194,
      "learning_rate": 0.00018757929883138566,
      "loss": 0.2511,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05949961021542549,
      "learning_rate": 0.00018751252086811354,
      "loss": 0.3607,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.057940538972616196,
      "learning_rate": 0.0001874457429048414,
      "loss": 0.3539,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.049378618597984314,
      "learning_rate": 0.0001873789649415693,
      "loss": 0.2927,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.053056396543979645,
      "learning_rate": 0.00018731218697829718,
      "loss": 0.3114,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.09249860048294067,
      "learning_rate": 0.00018724540901502506,
      "loss": 0.3164,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.06798094511032104,
      "learning_rate": 0.00018717863105175293,
      "loss": 0.3167,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.0637553483247757,
      "learning_rate": 0.0001871118530884808,
      "loss": 0.39,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06718450784683228,
      "learning_rate": 0.00018704507512520868,
      "loss": 0.3436,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05805756896734238,
      "learning_rate": 0.00018697829716193658,
      "loss": 0.2615,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.066567063331604,
      "learning_rate": 0.00018691151919866445,
      "loss": 0.3377,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.0723097026348114,
      "learning_rate": 0.00018684474123539232,
      "loss": 0.3788,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.084930919110775,
      "learning_rate": 0.0001867779632721202,
      "loss": 0.3318,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.07866982370615005,
      "learning_rate": 0.00018671118530884807,
      "loss": 0.2717,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.07142424583435059,
      "learning_rate": 0.00018664440734557597,
      "loss": 0.2816,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.11116000264883041,
      "learning_rate": 0.00018657762938230384,
      "loss": 0.3934,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.06793380528688431,
      "learning_rate": 0.00018651085141903174,
      "loss": 0.3052,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.067479707300663,
      "learning_rate": 0.00018644407345575962,
      "loss": 0.2836,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.04016229510307312,
      "learning_rate": 0.0001863772954924875,
      "loss": 0.2509,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06527184695005417,
      "learning_rate": 0.0001863105175292154,
      "loss": 0.3379,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.08602338284254074,
      "learning_rate": 0.00018624373956594326,
      "loss": 0.3096,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.08282919973134995,
      "learning_rate": 0.00018617696160267113,
      "loss": 0.376,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.057017698884010315,
      "learning_rate": 0.000186110183639399,
      "loss": 0.254,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06612373888492584,
      "learning_rate": 0.00018604340567612688,
      "loss": 0.2664,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.08113586157560349,
      "learning_rate": 0.00018597662771285475,
      "loss": 0.3139,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.045813411474227905,
      "learning_rate": 0.00018590984974958265,
      "loss": 0.2541,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.06276820600032806,
      "learning_rate": 0.00018584307178631053,
      "loss": 0.3263,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.056134093552827835,
      "learning_rate": 0.0001857762938230384,
      "loss": 0.2932,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06455931812524796,
      "learning_rate": 0.00018570951585976627,
      "loss": 0.2713,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.0641329288482666,
      "learning_rate": 0.00018564273789649415,
      "loss": 0.3262,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.057115621864795685,
      "learning_rate": 0.00018557595993322205,
      "loss": 0.3218,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04558887332677841,
      "learning_rate": 0.00018550918196994992,
      "loss": 0.2999,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.05689835548400879,
      "learning_rate": 0.0001854424040066778,
      "loss": 0.334,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.058476708829402924,
      "learning_rate": 0.0001853756260434057,
      "loss": 0.3557,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.0640471801161766,
      "learning_rate": 0.00018530884808013357,
      "loss": 0.3499,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.06061849743127823,
      "learning_rate": 0.00018524207011686147,
      "loss": 0.2432,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.07924827188253403,
      "learning_rate": 0.00018517529215358934,
      "loss": 0.3285,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09070103615522385,
      "learning_rate": 0.00018510851419031721,
      "loss": 0.4282,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.07663854956626892,
      "learning_rate": 0.0001850417362270451,
      "loss": 0.3036,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.058672137558460236,
      "learning_rate": 0.00018497495826377296,
      "loss": 0.2984,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06622264534235,
      "learning_rate": 0.00018490818030050083,
      "loss": 0.28,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.07077071815729141,
      "learning_rate": 0.00018484140233722873,
      "loss": 0.3022,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.06734973192214966,
      "learning_rate": 0.0001847746243739566,
      "loss": 0.3449,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.08291982859373093,
      "learning_rate": 0.00018470784641068448,
      "loss": 0.4326,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.03683653473854065,
      "learning_rate": 0.00018464106844741235,
      "loss": 0.2626,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.061428993940353394,
      "learning_rate": 0.00018457429048414023,
      "loss": 0.3523,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.07222747057676315,
      "learning_rate": 0.00018450751252086813,
      "loss": 0.3464,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.050284553319215775,
      "learning_rate": 0.000184440734557596,
      "loss": 0.3032,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.05952095612883568,
      "learning_rate": 0.00018437395659432387,
      "loss": 0.277,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07080469280481339,
      "learning_rate": 0.00018430717863105175,
      "loss": 0.2825,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.11495279520750046,
      "learning_rate": 0.00018424040066777965,
      "loss": 0.4424,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06323540955781937,
      "learning_rate": 0.00018417362270450752,
      "loss": 0.3093,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.0691780149936676,
      "learning_rate": 0.00018410684474123542,
      "loss": 0.2963,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.07082061469554901,
      "learning_rate": 0.0001840400667779633,
      "loss": 0.3557,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.06467094272375107,
      "learning_rate": 0.00018397328881469117,
      "loss": 0.2971,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.09653520584106445,
      "learning_rate": 0.00018390651085141904,
      "loss": 0.4259,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.0660548061132431,
      "learning_rate": 0.0001838397328881469,
      "loss": 0.4072,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.05308215692639351,
      "learning_rate": 0.0001837729549248748,
      "loss": 0.3724,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.10216723382472992,
      "learning_rate": 0.00018370617696160269,
      "loss": 0.431,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.08200223743915558,
      "learning_rate": 0.00018363939899833056,
      "loss": 0.3823,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.06216123327612877,
      "learning_rate": 0.00018357262103505843,
      "loss": 0.2717,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.07566752284765244,
      "learning_rate": 0.0001835058430717863,
      "loss": 0.3584,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.09035226702690125,
      "learning_rate": 0.0001834390651085142,
      "loss": 0.3171,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.08981560170650482,
      "learning_rate": 0.00018337228714524208,
      "loss": 0.3525,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.08013089746236801,
      "learning_rate": 0.00018330550918196995,
      "loss": 0.3037,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.06599704176187515,
      "learning_rate": 0.00018323873121869782,
      "loss": 0.2963,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.06157409027218819,
      "learning_rate": 0.0001831719532554257,
      "loss": 0.3132,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.06951967626810074,
      "learning_rate": 0.0001831051752921536,
      "loss": 0.3272,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07502143085002899,
      "learning_rate": 0.00018303839732888147,
      "loss": 0.3704,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.0474037267267704,
      "learning_rate": 0.00018297161936560937,
      "loss": 0.2887,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.05565059185028076,
      "learning_rate": 0.00018290484140233724,
      "loss": 0.3066,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04033610224723816,
      "learning_rate": 0.00018283806343906512,
      "loss": 0.2671,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.08553161472082138,
      "learning_rate": 0.000182771285475793,
      "loss": 0.325,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.06281795352697372,
      "learning_rate": 0.0001827045075125209,
      "loss": 0.3459,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07509568333625793,
      "learning_rate": 0.00018263772954924876,
      "loss": 0.3709,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.061399880796670914,
      "learning_rate": 0.00018257095158597664,
      "loss": 0.39,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.04432804137468338,
      "learning_rate": 0.0001825041736227045,
      "loss": 0.3498,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.044908903539180756,
      "learning_rate": 0.00018243739565943238,
      "loss": 0.3071,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.052189409732818604,
      "learning_rate": 0.00018237061769616028,
      "loss": 0.2782,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.09120634198188782,
      "learning_rate": 0.00018230383973288816,
      "loss": 0.351,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.07506801187992096,
      "learning_rate": 0.00018223706176961603,
      "loss": 0.3482,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10021274536848068,
      "learning_rate": 0.0001821702838063439,
      "loss": 0.4434,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.05500519275665283,
      "learning_rate": 0.00018210350584307178,
      "loss": 0.3526,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.0683271586894989,
      "learning_rate": 0.00018203672787979968,
      "loss": 0.3372,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.10096199810504913,
      "learning_rate": 0.00018196994991652755,
      "loss": 0.3778,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.054836444556713104,
      "learning_rate": 0.00018190317195325542,
      "loss": 0.2848,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.06321289390325546,
      "learning_rate": 0.00018183639398998332,
      "loss": 0.3287,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.047697845846414566,
      "learning_rate": 0.0001817696160267112,
      "loss": 0.3336,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.0485498309135437,
      "learning_rate": 0.0001817028380634391,
      "loss": 0.3112,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06390071660280228,
      "learning_rate": 0.00018163606010016697,
      "loss": 0.2998,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.05406942218542099,
      "learning_rate": 0.00018156928213689484,
      "loss": 0.3118,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05003362149000168,
      "learning_rate": 0.00018150250417362272,
      "loss": 0.3149,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.06629174947738647,
      "learning_rate": 0.0001814357262103506,
      "loss": 0.3656,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05630950629711151,
      "learning_rate": 0.00018136894824707846,
      "loss": 0.3993,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.08491452783346176,
      "learning_rate": 0.00018130217028380636,
      "loss": 0.3745,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.1000320240855217,
      "learning_rate": 0.00018123539232053424,
      "loss": 0.3475,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.088816799223423,
      "learning_rate": 0.0001811686143572621,
      "loss": 0.3918,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.12104816734790802,
      "learning_rate": 0.00018110183639398998,
      "loss": 0.4646,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.059016842395067215,
      "learning_rate": 0.00018103505843071786,
      "loss": 0.3566,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.09891337156295776,
      "learning_rate": 0.00018096828046744576,
      "loss": 0.4484,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.07177264988422394,
      "learning_rate": 0.00018090150250417363,
      "loss": 0.3773,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.06040855124592781,
      "learning_rate": 0.0001808347245409015,
      "loss": 0.3401,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07831306010484695,
      "learning_rate": 0.00018076794657762938,
      "loss": 0.4055,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05135171487927437,
      "learning_rate": 0.00018070116861435728,
      "loss": 0.3338,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.09665145725011826,
      "learning_rate": 0.00018063439065108515,
      "loss": 0.4434,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.061043549329042435,
      "learning_rate": 0.00018056761268781305,
      "loss": 0.4006,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.05078258365392685,
      "learning_rate": 0.00018050083472454092,
      "loss": 0.2619,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.047669392079114914,
      "learning_rate": 0.0001804340567612688,
      "loss": 0.272,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.07371781021356583,
      "learning_rate": 0.00018036727879799667,
      "loss": 0.3549,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.045762427151203156,
      "learning_rate": 0.00018030050083472454,
      "loss": 0.2972,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.0923825204372406,
      "learning_rate": 0.00018023372287145244,
      "loss": 0.3934,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.05545240268111229,
      "learning_rate": 0.00018016694490818031,
      "loss": 0.2954,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.05194549635052681,
      "learning_rate": 0.0001801001669449082,
      "loss": 0.2697,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0787268877029419,
      "learning_rate": 0.00018003338898163606,
      "loss": 0.3735,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.07389024645090103,
      "learning_rate": 0.00017996661101836393,
      "loss": 0.4262,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05247769504785538,
      "learning_rate": 0.00017989983305509183,
      "loss": 0.3109,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.05217419192194939,
      "learning_rate": 0.0001798330550918197,
      "loss": 0.2739,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06178995221853256,
      "learning_rate": 0.00017976627712854758,
      "loss": 0.2783,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.0531550757586956,
      "learning_rate": 0.00017969949916527545,
      "loss": 0.3064,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.05511941760778427,
      "learning_rate": 0.00017963272120200333,
      "loss": 0.3669,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.0790976732969284,
      "learning_rate": 0.00017956594323873123,
      "loss": 0.3963,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09130754321813583,
      "learning_rate": 0.0001794991652754591,
      "loss": 0.3848,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.054158858954906464,
      "learning_rate": 0.000179432387312187,
      "loss": 0.3583,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.08371222764253616,
      "learning_rate": 0.00017936560934891487,
      "loss": 0.3896,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.06208382919430733,
      "learning_rate": 0.00017929883138564275,
      "loss": 0.2482,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07005839049816132,
      "learning_rate": 0.00017923205342237062,
      "loss": 0.412,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.05783788487315178,
      "learning_rate": 0.00017916527545909852,
      "loss": 0.3352,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06575559824705124,
      "learning_rate": 0.0001790984974958264,
      "loss": 0.3964,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.05575578659772873,
      "learning_rate": 0.00017903171953255427,
      "loss": 0.2551,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09290974587202072,
      "learning_rate": 0.00017896494156928214,
      "loss": 0.3649,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.06782729923725128,
      "learning_rate": 0.00017889816360601,
      "loss": 0.2901,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06380290538072586,
      "learning_rate": 0.0001788313856427379,
      "loss": 0.362,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04423273354768753,
      "learning_rate": 0.0001787646076794658,
      "loss": 0.2689,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07054946571588516,
      "learning_rate": 0.00017869782971619366,
      "loss": 0.3748,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.05055522546172142,
      "learning_rate": 0.00017863105175292153,
      "loss": 0.3167,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.05999542027711868,
      "learning_rate": 0.0001785642737896494,
      "loss": 0.3359,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.05833917483687401,
      "learning_rate": 0.0001784974958263773,
      "loss": 0.2835,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.058605678379535675,
      "learning_rate": 0.00017843071786310518,
      "loss": 0.3623,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.060921862721443176,
      "learning_rate": 0.00017836393989983305,
      "loss": 0.3618,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.034921158105134964,
      "learning_rate": 0.00017829716193656095,
      "loss": 0.2091,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.06082122027873993,
      "learning_rate": 0.00017823038397328883,
      "loss": 0.3362,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.04426771029829979,
      "learning_rate": 0.0001781636060100167,
      "loss": 0.3148,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.056465454399585724,
      "learning_rate": 0.0001780968280467446,
      "loss": 0.3649,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08533026278018951,
      "learning_rate": 0.00017803005008347247,
      "loss": 0.3809,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.05331623554229736,
      "learning_rate": 0.00017796327212020035,
      "loss": 0.3329,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.04352397844195366,
      "learning_rate": 0.00017789649415692822,
      "loss": 0.2272,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.04538191854953766,
      "learning_rate": 0.0001778297161936561,
      "loss": 0.3121,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.06431352347135544,
      "learning_rate": 0.000177762938230384,
      "loss": 0.3235,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.05296257883310318,
      "learning_rate": 0.00017769616026711187,
      "loss": 0.2699,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.04389478638768196,
      "learning_rate": 0.00017762938230383974,
      "loss": 0.307,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.0654267817735672,
      "learning_rate": 0.0001775626043405676,
      "loss": 0.3058,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07108623534440994,
      "learning_rate": 0.00017749582637729548,
      "loss": 0.3726,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.06231032684445381,
      "learning_rate": 0.00017742904841402339,
      "loss": 0.348,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.05152330547571182,
      "learning_rate": 0.00017736227045075126,
      "loss": 0.328,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.07389311492443085,
      "learning_rate": 0.00017729549248747913,
      "loss": 0.4102,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.05757777392864227,
      "learning_rate": 0.000177228714524207,
      "loss": 0.2844,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.04819808527827263,
      "learning_rate": 0.0001771619365609349,
      "loss": 0.3189,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11943501979112625,
      "learning_rate": 0.00017709515859766278,
      "loss": 0.3542,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06013776361942291,
      "learning_rate": 0.00017702838063439068,
      "loss": 0.3877,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.05257827043533325,
      "learning_rate": 0.00017696160267111855,
      "loss": 0.3227,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.053413026034832,
      "learning_rate": 0.00017689482470784642,
      "loss": 0.2781,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.04171682894229889,
      "learning_rate": 0.0001768280467445743,
      "loss": 0.2906,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.05988014116883278,
      "learning_rate": 0.00017676126878130217,
      "loss": 0.3826,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.04357519745826721,
      "learning_rate": 0.00017669449081803007,
      "loss": 0.2846,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.07369498163461685,
      "learning_rate": 0.00017662771285475794,
      "loss": 0.3901,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.056979950517416,
      "learning_rate": 0.00017656093489148582,
      "loss": 0.3105,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.04863990843296051,
      "learning_rate": 0.0001764941569282137,
      "loss": 0.2644,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.0584864467382431,
      "learning_rate": 0.00017642737896494156,
      "loss": 0.3412,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.06396812200546265,
      "learning_rate": 0.00017636060100166946,
      "loss": 0.3769,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.048790302127599716,
      "learning_rate": 0.00017629382303839734,
      "loss": 0.3412,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.05513220652937889,
      "learning_rate": 0.0001762270450751252,
      "loss": 0.3053,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.06738697737455368,
      "learning_rate": 0.00017616026711185308,
      "loss": 0.3689,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03774310275912285,
      "learning_rate": 0.00017609348914858096,
      "loss": 0.2866,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.0639033168554306,
      "learning_rate": 0.00017602671118530886,
      "loss": 0.2677,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.08802369236946106,
      "learning_rate": 0.00017595993322203673,
      "loss": 0.417,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.07504976540803909,
      "learning_rate": 0.00017589315525876463,
      "loss": 0.31,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.06366459280252457,
      "learning_rate": 0.0001758263772954925,
      "loss": 0.3457,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.06799014657735825,
      "learning_rate": 0.00017575959933222038,
      "loss": 0.3874,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.061081353574991226,
      "learning_rate": 0.00017569282136894825,
      "loss": 0.3125,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06835927814245224,
      "learning_rate": 0.00017562604340567615,
      "loss": 0.2911,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.06039321795105934,
      "learning_rate": 0.00017555926544240402,
      "loss": 0.3353,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06088876351714134,
      "learning_rate": 0.0001754924874791319,
      "loss": 0.3306,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.06041053682565689,
      "learning_rate": 0.00017542570951585977,
      "loss": 0.3883,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.049776844680309296,
      "learning_rate": 0.00017535893155258764,
      "loss": 0.3883,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.07191675901412964,
      "learning_rate": 0.00017529215358931554,
      "loss": 0.2709,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.08683455735445023,
      "learning_rate": 0.00017522537562604342,
      "loss": 0.3736,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.08970895409584045,
      "learning_rate": 0.0001751585976627713,
      "loss": 0.3928,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.047020550817251205,
      "learning_rate": 0.00017509181969949916,
      "loss": 0.2753,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.09434638172388077,
      "learning_rate": 0.00017502504173622704,
      "loss": 0.3704,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.060877081006765366,
      "learning_rate": 0.0001749582637729549,
      "loss": 0.3033,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.05946670472621918,
      "learning_rate": 0.0001748914858096828,
      "loss": 0.2807,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.05076393485069275,
      "learning_rate": 0.0001748247078464107,
      "loss": 0.2931,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.08851009607315063,
      "learning_rate": 0.00017475792988313858,
      "loss": 0.3605,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10143689811229706,
      "learning_rate": 0.00017469115191986646,
      "loss": 0.4418,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.07623142749071121,
      "learning_rate": 0.00017462437395659433,
      "loss": 0.3911,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.07023727893829346,
      "learning_rate": 0.00017455759599332223,
      "loss": 0.3663,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.06017600744962692,
      "learning_rate": 0.0001744908180300501,
      "loss": 0.3083,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05826139077544212,
      "learning_rate": 0.00017442404006677798,
      "loss": 0.2582,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.06767719238996506,
      "learning_rate": 0.00017435726210350585,
      "loss": 0.3188,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.05285611003637314,
      "learning_rate": 0.00017429048414023372,
      "loss": 0.2998,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.056411128491163254,
      "learning_rate": 0.00017422370617696162,
      "loss": 0.2921,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.061185091733932495,
      "learning_rate": 0.0001741569282136895,
      "loss": 0.2943,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.05641420558094978,
      "learning_rate": 0.00017409015025041737,
      "loss": 0.2759,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.06851478666067123,
      "learning_rate": 0.00017402337228714524,
      "loss": 0.4513,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.0594760999083519,
      "learning_rate": 0.00017395659432387311,
      "loss": 0.379,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06426342576742172,
      "learning_rate": 0.00017388981636060101,
      "loss": 0.3499,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.0700017437338829,
      "learning_rate": 0.0001738230383973289,
      "loss": 0.317,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.04312475770711899,
      "learning_rate": 0.00017375626043405676,
      "loss": 0.2678,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.06582977622747421,
      "learning_rate": 0.00017368948247078466,
      "loss": 0.3406,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.07006170600652695,
      "learning_rate": 0.00017362270450751253,
      "loss": 0.369,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.05010639503598213,
      "learning_rate": 0.0001735559265442404,
      "loss": 0.3638,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.07488889247179031,
      "learning_rate": 0.0001734891485809683,
      "loss": 0.3247,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.05184513330459595,
      "learning_rate": 0.00017342237061769618,
      "loss": 0.2603,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.06638722866773605,
      "learning_rate": 0.00017335559265442405,
      "loss": 0.2854,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.06103762611746788,
      "learning_rate": 0.00017328881469115193,
      "loss": 0.3253,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.05783310532569885,
      "learning_rate": 0.0001732220367278798,
      "loss": 0.289,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.08919013291597366,
      "learning_rate": 0.0001731552587646077,
      "loss": 0.4523,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.08311869204044342,
      "learning_rate": 0.00017308848080133557,
      "loss": 0.2787,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.0692351907491684,
      "learning_rate": 0.00017302170283806345,
      "loss": 0.3734,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.05560993030667305,
      "learning_rate": 0.00017295492487479132,
      "loss": 0.3845,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.04939797893166542,
      "learning_rate": 0.0001728881469115192,
      "loss": 0.2759,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.04753990098834038,
      "learning_rate": 0.0001728213689482471,
      "loss": 0.2949,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.0605107918381691,
      "learning_rate": 0.00017275459098497497,
      "loss": 0.332,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.05432650446891785,
      "learning_rate": 0.00017268781302170284,
      "loss": 0.334,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.0383317731320858,
      "learning_rate": 0.0001726210350584307,
      "loss": 0.1907,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.06635861098766327,
      "learning_rate": 0.0001725542570951586,
      "loss": 0.3594,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.07990048080682755,
      "learning_rate": 0.0001724874791318865,
      "loss": 0.34,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.06931383162736893,
      "learning_rate": 0.0001724207011686144,
      "loss": 0.3743,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.06240297481417656,
      "learning_rate": 0.00017235392320534226,
      "loss": 0.3097,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04172668978571892,
      "learning_rate": 0.00017228714524207013,
      "loss": 0.3016,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.05873313546180725,
      "learning_rate": 0.000172220367278798,
      "loss": 0.4202,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.06333927810192108,
      "learning_rate": 0.00017215358931552588,
      "loss": 0.3504,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.054410386830568314,
      "learning_rate": 0.00017208681135225378,
      "loss": 0.3442,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06687209010124207,
      "learning_rate": 0.00017202003338898165,
      "loss": 0.3851,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.04169590398669243,
      "learning_rate": 0.00017195325542570953,
      "loss": 0.3524,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.05493532493710518,
      "learning_rate": 0.0001718864774624374,
      "loss": 0.2999,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.04102494567632675,
      "learning_rate": 0.00017181969949916527,
      "loss": 0.2589,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.06786254793405533,
      "learning_rate": 0.00017175292153589317,
      "loss": 0.3247,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.05260216444730759,
      "learning_rate": 0.00017168614357262105,
      "loss": 0.3017,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.051911525428295135,
      "learning_rate": 0.00017161936560934892,
      "loss": 0.3347,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.04882310703396797,
      "learning_rate": 0.0001715525876460768,
      "loss": 0.3196,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06122424080967903,
      "learning_rate": 0.00017148580968280467,
      "loss": 0.3887,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.04538717120885849,
      "learning_rate": 0.00017141903171953257,
      "loss": 0.279,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.06093759089708328,
      "learning_rate": 0.00017135225375626044,
      "loss": 0.3215,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05240204557776451,
      "learning_rate": 0.00017128547579298834,
      "loss": 0.3036,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.05205075070261955,
      "learning_rate": 0.0001712186978297162,
      "loss": 0.311,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.05608535557985306,
      "learning_rate": 0.00017115191986644409,
      "loss": 0.331,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.047444626688957214,
      "learning_rate": 0.00017108514190317196,
      "loss": 0.3073,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.06831686943769455,
      "learning_rate": 0.00017101836393989986,
      "loss": 0.3452,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03715347498655319,
      "learning_rate": 0.00017095158597662773,
      "loss": 0.2624,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09170711785554886,
      "learning_rate": 0.0001708848080133556,
      "loss": 0.3888,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.05762215703725815,
      "learning_rate": 0.00017081803005008348,
      "loss": 0.3158,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06637157499790192,
      "learning_rate": 0.00017075125208681135,
      "loss": 0.2453,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04048594832420349,
      "learning_rate": 0.00017068447412353925,
      "loss": 0.2397,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.10212234407663345,
      "learning_rate": 0.00017061769616026712,
      "loss": 0.4159,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06937846541404724,
      "learning_rate": 0.000170550918196995,
      "loss": 0.3396,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.07474826276302338,
      "learning_rate": 0.00017048414023372287,
      "loss": 0.3326,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05824249982833862,
      "learning_rate": 0.00017041736227045074,
      "loss": 0.3379,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.04704830422997475,
      "learning_rate": 0.00017035058430717862,
      "loss": 0.2354,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.06278561055660248,
      "learning_rate": 0.00017028380634390652,
      "loss": 0.3221,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.08273832499980927,
      "learning_rate": 0.0001702170283806344,
      "loss": 0.366,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.0600234791636467,
      "learning_rate": 0.0001701502504173623,
      "loss": 0.2801,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.04539366066455841,
      "learning_rate": 0.00017008347245409016,
      "loss": 0.2435,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06901519000530243,
      "learning_rate": 0.00017001669449081804,
      "loss": 0.3805,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07664025574922562,
      "learning_rate": 0.00016994991652754594,
      "loss": 0.2939,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.046238843351602554,
      "learning_rate": 0.0001698831385642738,
      "loss": 0.2781,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.06035570055246353,
      "learning_rate": 0.00016981636060100168,
      "loss": 0.3907,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.06277912855148315,
      "learning_rate": 0.00016974958263772956,
      "loss": 0.3467,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.044353120028972626,
      "learning_rate": 0.00016968280467445743,
      "loss": 0.2857,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.04181146249175072,
      "learning_rate": 0.00016961602671118533,
      "loss": 0.2875,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.05250684544444084,
      "learning_rate": 0.0001695492487479132,
      "loss": 0.3587,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.05013364553451538,
      "learning_rate": 0.00016948247078464108,
      "loss": 0.2818,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.051002535969018936,
      "learning_rate": 0.00016941569282136895,
      "loss": 0.3078,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.06919902563095093,
      "learning_rate": 0.00016934891485809682,
      "loss": 0.4099,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06153741106390953,
      "learning_rate": 0.0001692821368948247,
      "loss": 0.3219,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.0433584563434124,
      "learning_rate": 0.0001692153589315526,
      "loss": 0.2554,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.041832782328128815,
      "learning_rate": 0.00016914858096828047,
      "loss": 0.2847,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.045837342739105225,
      "learning_rate": 0.00016908180300500834,
      "loss": 0.347,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.07606692612171173,
      "learning_rate": 0.00016901502504173624,
      "loss": 0.3495,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.04513280466198921,
      "learning_rate": 0.00016894824707846412,
      "loss": 0.303,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.039144162088632584,
      "learning_rate": 0.00016888146911519202,
      "loss": 0.2603,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04442967474460602,
      "learning_rate": 0.0001688146911519199,
      "loss": 0.2981,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.04966007545590401,
      "learning_rate": 0.00016874791318864776,
      "loss": 0.3159,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.05852694436907768,
      "learning_rate": 0.00016868113522537564,
      "loss": 0.3716,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.053059473633766174,
      "learning_rate": 0.0001686143572621035,
      "loss": 0.3792,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05099688097834587,
      "learning_rate": 0.0001685475792988314,
      "loss": 0.3137,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.10189834982156754,
      "learning_rate": 0.00016848080133555928,
      "loss": 0.3818,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.0526309497654438,
      "learning_rate": 0.00016841402337228716,
      "loss": 0.2817,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05295218899846077,
      "learning_rate": 0.00016834724540901503,
      "loss": 0.3401,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06662885844707489,
      "learning_rate": 0.0001682804674457429,
      "loss": 0.3991,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.06841499358415604,
      "learning_rate": 0.00016821368948247077,
      "loss": 0.4051,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.0365263968706131,
      "learning_rate": 0.00016814691151919868,
      "loss": 0.2376,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04485613852739334,
      "learning_rate": 0.00016808013355592655,
      "loss": 0.2694,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.06939379125833511,
      "learning_rate": 0.00016801335559265442,
      "loss": 0.3597,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.06019312143325806,
      "learning_rate": 0.0001679465776293823,
      "loss": 0.3509,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05627303943037987,
      "learning_rate": 0.0001678797996661102,
      "loss": 0.32,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.04382266476750374,
      "learning_rate": 0.00016781302170283807,
      "loss": 0.2564,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.056689437478780746,
      "learning_rate": 0.00016774624373956597,
      "loss": 0.2995,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06798779219388962,
      "learning_rate": 0.00016767946577629384,
      "loss": 0.3409,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.04012368619441986,
      "learning_rate": 0.00016761268781302171,
      "loss": 0.2714,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.046074435114860535,
      "learning_rate": 0.0001675459098497496,
      "loss": 0.2982,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.047727275639772415,
      "learning_rate": 0.0001674791318864775,
      "loss": 0.3133,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.04751616716384888,
      "learning_rate": 0.00016741235392320536,
      "loss": 0.2959,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.06545601040124893,
      "learning_rate": 0.00016734557595993323,
      "loss": 0.335,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.05398217961192131,
      "learning_rate": 0.0001672787979966611,
      "loss": 0.3406,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.049862947314977646,
      "learning_rate": 0.00016721202003338898,
      "loss": 0.2785,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.051326289772987366,
      "learning_rate": 0.00016714524207011685,
      "loss": 0.3111,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.06404118984937668,
      "learning_rate": 0.00016707846410684475,
      "loss": 0.3326,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04448550567030907,
      "learning_rate": 0.00016701168614357263,
      "loss": 0.2722,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.04964333772659302,
      "learning_rate": 0.0001669449081803005,
      "loss": 0.2919,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6425543847874396e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
