{
  "best_metric": 0.36545103788375854,
  "best_model_checkpoint": "outputs/checkpoint-500",
  "epoch": 0.14705882352941177,
  "eval_steps": 100,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002941176470588235,
      "grad_norm": 2.183479070663452,
      "learning_rate": 4e-05,
      "loss": 2.0034,
      "step": 1
    },
    {
      "epoch": 0.000588235294117647,
      "grad_norm": 2.10685658454895,
      "learning_rate": 8e-05,
      "loss": 2.1112,
      "step": 2
    },
    {
      "epoch": 0.0008823529411764706,
      "grad_norm": 2.0220677852630615,
      "learning_rate": 0.00012,
      "loss": 2.0415,
      "step": 3
    },
    {
      "epoch": 0.001176470588235294,
      "grad_norm": 1.8765255212783813,
      "learning_rate": 0.00016,
      "loss": 2.0435,
      "step": 4
    },
    {
      "epoch": 0.0014705882352941176,
      "grad_norm": 1.5902093648910522,
      "learning_rate": 0.0002,
      "loss": 1.6744,
      "step": 5
    },
    {
      "epoch": 0.0017647058823529412,
      "grad_norm": 2.2842509746551514,
      "learning_rate": 0.00019994108983799707,
      "loss": 1.2759,
      "step": 6
    },
    {
      "epoch": 0.002058823529411765,
      "grad_norm": 1.3071730136871338,
      "learning_rate": 0.00019988217967599413,
      "loss": 1.2103,
      "step": 7
    },
    {
      "epoch": 0.002352941176470588,
      "grad_norm": 1.2924339771270752,
      "learning_rate": 0.00019982326951399116,
      "loss": 1.1982,
      "step": 8
    },
    {
      "epoch": 0.0026470588235294116,
      "grad_norm": 1.2307484149932861,
      "learning_rate": 0.00019976435935198822,
      "loss": 1.0957,
      "step": 9
    },
    {
      "epoch": 0.0029411764705882353,
      "grad_norm": 1.2179538011550903,
      "learning_rate": 0.00019970544918998528,
      "loss": 0.998,
      "step": 10
    },
    {
      "epoch": 0.003235294117647059,
      "grad_norm": 2.353794574737549,
      "learning_rate": 0.00019964653902798234,
      "loss": 0.7913,
      "step": 11
    },
    {
      "epoch": 0.0035294117647058825,
      "grad_norm": 1.0379599332809448,
      "learning_rate": 0.0001995876288659794,
      "loss": 0.7102,
      "step": 12
    },
    {
      "epoch": 0.003823529411764706,
      "grad_norm": 0.931348443031311,
      "learning_rate": 0.00019952871870397644,
      "loss": 0.8079,
      "step": 13
    },
    {
      "epoch": 0.00411764705882353,
      "grad_norm": 0.5651299357414246,
      "learning_rate": 0.0001994698085419735,
      "loss": 0.5726,
      "step": 14
    },
    {
      "epoch": 0.004411764705882353,
      "grad_norm": 0.4931602478027344,
      "learning_rate": 0.00019941089837997056,
      "loss": 0.59,
      "step": 15
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 0.48339712619781494,
      "learning_rate": 0.00019935198821796762,
      "loss": 0.6296,
      "step": 16
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.4681629240512848,
      "learning_rate": 0.00019929307805596468,
      "loss": 0.6948,
      "step": 17
    },
    {
      "epoch": 0.005294117647058823,
      "grad_norm": 0.371240496635437,
      "learning_rate": 0.0001992341678939617,
      "loss": 0.6883,
      "step": 18
    },
    {
      "epoch": 0.005588235294117647,
      "grad_norm": 0.25134366750717163,
      "learning_rate": 0.00019917525773195877,
      "loss": 0.4542,
      "step": 19
    },
    {
      "epoch": 0.0058823529411764705,
      "grad_norm": 0.27983060479164124,
      "learning_rate": 0.00019911634756995583,
      "loss": 0.5234,
      "step": 20
    },
    {
      "epoch": 0.006176470588235294,
      "grad_norm": 0.23852011561393738,
      "learning_rate": 0.0001990574374079529,
      "loss": 0.5364,
      "step": 21
    },
    {
      "epoch": 0.006470588235294118,
      "grad_norm": 0.18029136955738068,
      "learning_rate": 0.00019899852724594995,
      "loss": 0.4398,
      "step": 22
    },
    {
      "epoch": 0.006764705882352941,
      "grad_norm": 0.23648658394813538,
      "learning_rate": 0.00019893961708394698,
      "loss": 0.5708,
      "step": 23
    },
    {
      "epoch": 0.007058823529411765,
      "grad_norm": 0.1452111452817917,
      "learning_rate": 0.00019888070692194404,
      "loss": 0.4328,
      "step": 24
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 0.29196223616600037,
      "learning_rate": 0.0001988217967599411,
      "loss": 0.5203,
      "step": 25
    },
    {
      "epoch": 0.007647058823529412,
      "grad_norm": 0.20188039541244507,
      "learning_rate": 0.00019876288659793816,
      "loss": 0.5585,
      "step": 26
    },
    {
      "epoch": 0.007941176470588234,
      "grad_norm": 0.12955594062805176,
      "learning_rate": 0.00019870397643593522,
      "loss": 0.4612,
      "step": 27
    },
    {
      "epoch": 0.00823529411764706,
      "grad_norm": 0.1844465285539627,
      "learning_rate": 0.00019864506627393226,
      "loss": 0.4544,
      "step": 28
    },
    {
      "epoch": 0.008529411764705883,
      "grad_norm": 0.18567843735218048,
      "learning_rate": 0.00019858615611192932,
      "loss": 0.4765,
      "step": 29
    },
    {
      "epoch": 0.008823529411764706,
      "grad_norm": 0.17821620404720306,
      "learning_rate": 0.00019852724594992638,
      "loss": 0.3675,
      "step": 30
    },
    {
      "epoch": 0.009117647058823529,
      "grad_norm": 0.17739641666412354,
      "learning_rate": 0.00019846833578792344,
      "loss": 0.3991,
      "step": 31
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 0.16542911529541016,
      "learning_rate": 0.0001984094256259205,
      "loss": 0.414,
      "step": 32
    },
    {
      "epoch": 0.009705882352941177,
      "grad_norm": 0.1985095590353012,
      "learning_rate": 0.00019835051546391753,
      "loss": 0.5053,
      "step": 33
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1879136860370636,
      "learning_rate": 0.0001982916053019146,
      "loss": 0.4769,
      "step": 34
    },
    {
      "epoch": 0.010294117647058823,
      "grad_norm": 0.18819700181484222,
      "learning_rate": 0.00019823269513991165,
      "loss": 0.3618,
      "step": 35
    },
    {
      "epoch": 0.010588235294117647,
      "grad_norm": 0.12826715409755707,
      "learning_rate": 0.0001981737849779087,
      "loss": 0.4915,
      "step": 36
    },
    {
      "epoch": 0.01088235294117647,
      "grad_norm": 0.1465553343296051,
      "learning_rate": 0.00019811487481590577,
      "loss": 0.3617,
      "step": 37
    },
    {
      "epoch": 0.011176470588235295,
      "grad_norm": 0.1761365383863449,
      "learning_rate": 0.0001980559646539028,
      "loss": 0.4842,
      "step": 38
    },
    {
      "epoch": 0.011470588235294118,
      "grad_norm": 0.09384077042341232,
      "learning_rate": 0.00019799705449189987,
      "loss": 0.2294,
      "step": 39
    },
    {
      "epoch": 0.011764705882352941,
      "grad_norm": 0.11776022613048553,
      "learning_rate": 0.00019793814432989693,
      "loss": 0.3746,
      "step": 40
    },
    {
      "epoch": 0.012058823529411764,
      "grad_norm": 0.13776753842830658,
      "learning_rate": 0.00019787923416789399,
      "loss": 0.4129,
      "step": 41
    },
    {
      "epoch": 0.012352941176470587,
      "grad_norm": 0.08935388177633286,
      "learning_rate": 0.00019782032400589105,
      "loss": 0.3549,
      "step": 42
    },
    {
      "epoch": 0.012647058823529412,
      "grad_norm": 0.15252627432346344,
      "learning_rate": 0.00019776141384388808,
      "loss": 0.4398,
      "step": 43
    },
    {
      "epoch": 0.012941176470588235,
      "grad_norm": 0.10281182825565338,
      "learning_rate": 0.00019770250368188514,
      "loss": 0.3606,
      "step": 44
    },
    {
      "epoch": 0.013235294117647059,
      "grad_norm": 0.09776261448860168,
      "learning_rate": 0.0001976435935198822,
      "loss": 0.4772,
      "step": 45
    },
    {
      "epoch": 0.013529411764705882,
      "grad_norm": 0.10491859167814255,
      "learning_rate": 0.00019758468335787926,
      "loss": 0.3781,
      "step": 46
    },
    {
      "epoch": 0.013823529411764707,
      "grad_norm": 0.1012224331498146,
      "learning_rate": 0.00019752577319587632,
      "loss": 0.4278,
      "step": 47
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 0.11251673102378845,
      "learning_rate": 0.00019746686303387335,
      "loss": 0.4952,
      "step": 48
    },
    {
      "epoch": 0.014411764705882353,
      "grad_norm": 0.08367883414030075,
      "learning_rate": 0.0001974079528718704,
      "loss": 0.4161,
      "step": 49
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 0.09331709891557693,
      "learning_rate": 0.00019734904270986747,
      "loss": 0.4508,
      "step": 50
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.08101513236761093,
      "learning_rate": 0.00019729013254786453,
      "loss": 0.3236,
      "step": 51
    },
    {
      "epoch": 0.015294117647058824,
      "grad_norm": 0.10727065801620483,
      "learning_rate": 0.0001972312223858616,
      "loss": 0.3601,
      "step": 52
    },
    {
      "epoch": 0.015588235294117648,
      "grad_norm": 0.10416439920663834,
      "learning_rate": 0.00019717231222385863,
      "loss": 0.4145,
      "step": 53
    },
    {
      "epoch": 0.01588235294117647,
      "grad_norm": 0.0757676288485527,
      "learning_rate": 0.0001971134020618557,
      "loss": 0.3368,
      "step": 54
    },
    {
      "epoch": 0.016176470588235296,
      "grad_norm": 0.09751800447702408,
      "learning_rate": 0.00019705449189985275,
      "loss": 0.4249,
      "step": 55
    },
    {
      "epoch": 0.01647058823529412,
      "grad_norm": 0.07901518046855927,
      "learning_rate": 0.0001969955817378498,
      "loss": 0.3693,
      "step": 56
    },
    {
      "epoch": 0.016764705882352942,
      "grad_norm": 0.0967986211180687,
      "learning_rate": 0.00019693667157584687,
      "loss": 0.4862,
      "step": 57
    },
    {
      "epoch": 0.017058823529411765,
      "grad_norm": 0.09867838025093079,
      "learning_rate": 0.0001968777614138439,
      "loss": 0.3648,
      "step": 58
    },
    {
      "epoch": 0.01735294117647059,
      "grad_norm": 0.06541481614112854,
      "learning_rate": 0.00019681885125184093,
      "loss": 0.3701,
      "step": 59
    },
    {
      "epoch": 0.01764705882352941,
      "grad_norm": 0.09128256142139435,
      "learning_rate": 0.000196759941089838,
      "loss": 0.4507,
      "step": 60
    },
    {
      "epoch": 0.017941176470588235,
      "grad_norm": 0.09447271376848221,
      "learning_rate": 0.00019670103092783505,
      "loss": 0.3778,
      "step": 61
    },
    {
      "epoch": 0.018235294117647058,
      "grad_norm": 0.08395741879940033,
      "learning_rate": 0.00019664212076583211,
      "loss": 0.3372,
      "step": 62
    },
    {
      "epoch": 0.01852941176470588,
      "grad_norm": 0.07432801276445389,
      "learning_rate": 0.00019658321060382915,
      "loss": 0.3504,
      "step": 63
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 0.08092419803142548,
      "learning_rate": 0.0001965243004418262,
      "loss": 0.3852,
      "step": 64
    },
    {
      "epoch": 0.01911764705882353,
      "grad_norm": 0.07702334225177765,
      "learning_rate": 0.00019646539027982327,
      "loss": 0.3449,
      "step": 65
    },
    {
      "epoch": 0.019411764705882354,
      "grad_norm": 0.09544370323419571,
      "learning_rate": 0.00019640648011782033,
      "loss": 0.4421,
      "step": 66
    },
    {
      "epoch": 0.019705882352941177,
      "grad_norm": 0.079366534948349,
      "learning_rate": 0.0001963475699558174,
      "loss": 0.2845,
      "step": 67
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.08863312751054764,
      "learning_rate": 0.00019628865979381442,
      "loss": 0.3601,
      "step": 68
    },
    {
      "epoch": 0.020294117647058824,
      "grad_norm": 0.06770386546850204,
      "learning_rate": 0.00019622974963181148,
      "loss": 0.3229,
      "step": 69
    },
    {
      "epoch": 0.020588235294117647,
      "grad_norm": 0.08941573649644852,
      "learning_rate": 0.00019617083946980854,
      "loss": 0.2738,
      "step": 70
    },
    {
      "epoch": 0.02088235294117647,
      "grad_norm": 0.1214604452252388,
      "learning_rate": 0.0001961119293078056,
      "loss": 0.4371,
      "step": 71
    },
    {
      "epoch": 0.021176470588235293,
      "grad_norm": 0.07221110910177231,
      "learning_rate": 0.00019605301914580266,
      "loss": 0.3142,
      "step": 72
    },
    {
      "epoch": 0.021470588235294116,
      "grad_norm": 0.14565660059452057,
      "learning_rate": 0.0001959941089837997,
      "loss": 0.4053,
      "step": 73
    },
    {
      "epoch": 0.02176470588235294,
      "grad_norm": 0.09350073337554932,
      "learning_rate": 0.00019593519882179675,
      "loss": 0.4561,
      "step": 74
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 0.07255196571350098,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.4226,
      "step": 75
    },
    {
      "epoch": 0.02235294117647059,
      "grad_norm": 0.10246308892965317,
      "learning_rate": 0.00019581737849779087,
      "loss": 0.3564,
      "step": 76
    },
    {
      "epoch": 0.022647058823529412,
      "grad_norm": 0.09264907240867615,
      "learning_rate": 0.00019575846833578793,
      "loss": 0.3436,
      "step": 77
    },
    {
      "epoch": 0.022941176470588236,
      "grad_norm": 0.07179585099220276,
      "learning_rate": 0.00019569955817378497,
      "loss": 0.3345,
      "step": 78
    },
    {
      "epoch": 0.02323529411764706,
      "grad_norm": 0.07578697055578232,
      "learning_rate": 0.00019564064801178203,
      "loss": 0.3262,
      "step": 79
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 0.12121928483247757,
      "learning_rate": 0.0001955817378497791,
      "loss": 0.4263,
      "step": 80
    },
    {
      "epoch": 0.023823529411764705,
      "grad_norm": 0.09517159312963486,
      "learning_rate": 0.00019552282768777615,
      "loss": 0.3815,
      "step": 81
    },
    {
      "epoch": 0.02411764705882353,
      "grad_norm": 0.11090484261512756,
      "learning_rate": 0.0001954639175257732,
      "loss": 0.4297,
      "step": 82
    },
    {
      "epoch": 0.02441176470588235,
      "grad_norm": 0.15365923941135406,
      "learning_rate": 0.00019540500736377024,
      "loss": 0.4979,
      "step": 83
    },
    {
      "epoch": 0.024705882352941175,
      "grad_norm": 0.09497728943824768,
      "learning_rate": 0.0001953460972017673,
      "loss": 0.4349,
      "step": 84
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.08315237611532211,
      "learning_rate": 0.00019528718703976436,
      "loss": 0.2738,
      "step": 85
    },
    {
      "epoch": 0.025294117647058825,
      "grad_norm": 0.1224735751748085,
      "learning_rate": 0.00019522827687776142,
      "loss": 0.3724,
      "step": 86
    },
    {
      "epoch": 0.025588235294117648,
      "grad_norm": 0.11303207278251648,
      "learning_rate": 0.00019516936671575848,
      "loss": 0.4179,
      "step": 87
    },
    {
      "epoch": 0.02588235294117647,
      "grad_norm": 0.06620808690786362,
      "learning_rate": 0.00019511045655375552,
      "loss": 0.2997,
      "step": 88
    },
    {
      "epoch": 0.026176470588235294,
      "grad_norm": 0.07132145017385483,
      "learning_rate": 0.00019505154639175258,
      "loss": 0.3479,
      "step": 89
    },
    {
      "epoch": 0.026470588235294117,
      "grad_norm": 0.08084312826395035,
      "learning_rate": 0.00019499263622974964,
      "loss": 0.4186,
      "step": 90
    },
    {
      "epoch": 0.02676470588235294,
      "grad_norm": 0.09820722788572311,
      "learning_rate": 0.0001949337260677467,
      "loss": 0.4531,
      "step": 91
    },
    {
      "epoch": 0.027058823529411764,
      "grad_norm": 0.11209862679243088,
      "learning_rate": 0.00019487481590574376,
      "loss": 0.391,
      "step": 92
    },
    {
      "epoch": 0.027352941176470587,
      "grad_norm": 0.08833757042884827,
      "learning_rate": 0.0001948159057437408,
      "loss": 0.3612,
      "step": 93
    },
    {
      "epoch": 0.027647058823529413,
      "grad_norm": 0.07863058149814606,
      "learning_rate": 0.00019475699558173785,
      "loss": 0.3995,
      "step": 94
    },
    {
      "epoch": 0.027941176470588237,
      "grad_norm": 0.06074966490268707,
      "learning_rate": 0.0001946980854197349,
      "loss": 0.3514,
      "step": 95
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 0.08599737286567688,
      "learning_rate": 0.00019463917525773197,
      "loss": 0.3256,
      "step": 96
    },
    {
      "epoch": 0.028529411764705883,
      "grad_norm": 0.10413768887519836,
      "learning_rate": 0.00019458026509572903,
      "loss": 0.4315,
      "step": 97
    },
    {
      "epoch": 0.028823529411764706,
      "grad_norm": 0.08582927286624908,
      "learning_rate": 0.00019452135493372606,
      "loss": 0.4555,
      "step": 98
    },
    {
      "epoch": 0.02911764705882353,
      "grad_norm": 0.05906078964471817,
      "learning_rate": 0.00019446244477172312,
      "loss": 0.306,
      "step": 99
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.09747941046953201,
      "learning_rate": 0.00019440353460972018,
      "loss": 0.5009,
      "step": 100
    },
    {
      "epoch": 0.029411764705882353,
      "eval_loss": 0.3878893554210663,
      "eval_runtime": 214.9157,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 100
    },
    {
      "epoch": 0.029705882352941176,
      "grad_norm": 0.07448837161064148,
      "learning_rate": 0.00019434462444771724,
      "loss": 0.3754,
      "step": 101
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.0614958293735981,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.3578,
      "step": 102
    },
    {
      "epoch": 0.030294117647058822,
      "grad_norm": 0.06929472833871841,
      "learning_rate": 0.00019422680412371134,
      "loss": 0.3498,
      "step": 103
    },
    {
      "epoch": 0.03058823529411765,
      "grad_norm": 0.08569588512182236,
      "learning_rate": 0.0001941678939617084,
      "loss": 0.4205,
      "step": 104
    },
    {
      "epoch": 0.030882352941176472,
      "grad_norm": 0.0914229154586792,
      "learning_rate": 0.00019410898379970546,
      "loss": 0.38,
      "step": 105
    },
    {
      "epoch": 0.031176470588235295,
      "grad_norm": 0.07119389623403549,
      "learning_rate": 0.00019405007363770252,
      "loss": 0.4083,
      "step": 106
    },
    {
      "epoch": 0.03147058823529412,
      "grad_norm": 0.11529682576656342,
      "learning_rate": 0.00019399116347569958,
      "loss": 0.4223,
      "step": 107
    },
    {
      "epoch": 0.03176470588235294,
      "grad_norm": 0.09149236977100372,
      "learning_rate": 0.0001939322533136966,
      "loss": 0.3922,
      "step": 108
    },
    {
      "epoch": 0.032058823529411765,
      "grad_norm": 0.06520511209964752,
      "learning_rate": 0.00019387334315169367,
      "loss": 0.2844,
      "step": 109
    },
    {
      "epoch": 0.03235294117647059,
      "grad_norm": 0.10490236431360245,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.3964,
      "step": 110
    },
    {
      "epoch": 0.03264705882352941,
      "grad_norm": 0.07780566066503525,
      "learning_rate": 0.0001937555228276878,
      "loss": 0.373,
      "step": 111
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 0.09160245209932327,
      "learning_rate": 0.00019369661266568485,
      "loss": 0.4141,
      "step": 112
    },
    {
      "epoch": 0.03323529411764706,
      "grad_norm": 0.1157829761505127,
      "learning_rate": 0.00019363770250368188,
      "loss": 0.3645,
      "step": 113
    },
    {
      "epoch": 0.033529411764705884,
      "grad_norm": 0.08872844278812408,
      "learning_rate": 0.00019357879234167894,
      "loss": 0.3769,
      "step": 114
    },
    {
      "epoch": 0.033823529411764704,
      "grad_norm": 0.09189071506261826,
      "learning_rate": 0.000193519882179676,
      "loss": 0.415,
      "step": 115
    },
    {
      "epoch": 0.03411764705882353,
      "grad_norm": 0.12543384730815887,
      "learning_rate": 0.00019346097201767306,
      "loss": 0.3818,
      "step": 116
    },
    {
      "epoch": 0.03441176470588235,
      "grad_norm": 0.09957624971866608,
      "learning_rate": 0.00019340206185567012,
      "loss": 0.4251,
      "step": 117
    },
    {
      "epoch": 0.03470588235294118,
      "grad_norm": 0.08664538711309433,
      "learning_rate": 0.00019334315169366716,
      "loss": 0.4353,
      "step": 118
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.0905040055513382,
      "learning_rate": 0.00019328424153166422,
      "loss": 0.4325,
      "step": 119
    },
    {
      "epoch": 0.03529411764705882,
      "grad_norm": 0.07393920421600342,
      "learning_rate": 0.00019322533136966128,
      "loss": 0.4034,
      "step": 120
    },
    {
      "epoch": 0.03558823529411765,
      "grad_norm": 0.10218611359596252,
      "learning_rate": 0.00019316642120765834,
      "loss": 0.3409,
      "step": 121
    },
    {
      "epoch": 0.03588235294117647,
      "grad_norm": 0.10684023052453995,
      "learning_rate": 0.0001931075110456554,
      "loss": 0.4893,
      "step": 122
    },
    {
      "epoch": 0.036176470588235296,
      "grad_norm": 0.07973593473434448,
      "learning_rate": 0.00019304860088365243,
      "loss": 0.3938,
      "step": 123
    },
    {
      "epoch": 0.036470588235294116,
      "grad_norm": 0.08641645312309265,
      "learning_rate": 0.0001929896907216495,
      "loss": 0.4374,
      "step": 124
    },
    {
      "epoch": 0.03676470588235294,
      "grad_norm": 0.07073718309402466,
      "learning_rate": 0.00019293078055964655,
      "loss": 0.3082,
      "step": 125
    },
    {
      "epoch": 0.03705882352941176,
      "grad_norm": 0.09762880951166153,
      "learning_rate": 0.0001928718703976436,
      "loss": 0.4032,
      "step": 126
    },
    {
      "epoch": 0.03735294117647059,
      "grad_norm": 0.07529620826244354,
      "learning_rate": 0.00019281296023564067,
      "loss": 0.3605,
      "step": 127
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 0.05729182809591293,
      "learning_rate": 0.0001927540500736377,
      "loss": 0.3305,
      "step": 128
    },
    {
      "epoch": 0.037941176470588235,
      "grad_norm": 0.07892298698425293,
      "learning_rate": 0.00019269513991163477,
      "loss": 0.4032,
      "step": 129
    },
    {
      "epoch": 0.03823529411764706,
      "grad_norm": 0.07929697632789612,
      "learning_rate": 0.00019263622974963183,
      "loss": 0.3707,
      "step": 130
    },
    {
      "epoch": 0.03852941176470588,
      "grad_norm": 0.06248617172241211,
      "learning_rate": 0.00019257731958762889,
      "loss": 0.324,
      "step": 131
    },
    {
      "epoch": 0.03882352941176471,
      "grad_norm": 0.09426065534353256,
      "learning_rate": 0.00019251840942562595,
      "loss": 0.4326,
      "step": 132
    },
    {
      "epoch": 0.03911764705882353,
      "grad_norm": 0.0804290696978569,
      "learning_rate": 0.00019245949926362298,
      "loss": 0.2912,
      "step": 133
    },
    {
      "epoch": 0.039411764705882354,
      "grad_norm": 0.0701073482632637,
      "learning_rate": 0.00019240058910162004,
      "loss": 0.4114,
      "step": 134
    },
    {
      "epoch": 0.039705882352941174,
      "grad_norm": 0.08479296416044235,
      "learning_rate": 0.0001923416789396171,
      "loss": 0.402,
      "step": 135
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.09177654981613159,
      "learning_rate": 0.00019228276877761416,
      "loss": 0.4206,
      "step": 136
    },
    {
      "epoch": 0.04029411764705882,
      "grad_norm": 0.0995887815952301,
      "learning_rate": 0.00019222385861561122,
      "loss": 0.3542,
      "step": 137
    },
    {
      "epoch": 0.04058823529411765,
      "grad_norm": 0.0686226338148117,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.2912,
      "step": 138
    },
    {
      "epoch": 0.040882352941176474,
      "grad_norm": 0.09901077300310135,
      "learning_rate": 0.0001921060382916053,
      "loss": 0.4222,
      "step": 139
    },
    {
      "epoch": 0.041176470588235294,
      "grad_norm": 0.06634445488452911,
      "learning_rate": 0.00019204712812960237,
      "loss": 0.3625,
      "step": 140
    },
    {
      "epoch": 0.04147058823529412,
      "grad_norm": 0.09192539751529694,
      "learning_rate": 0.00019198821796759943,
      "loss": 0.4323,
      "step": 141
    },
    {
      "epoch": 0.04176470588235294,
      "grad_norm": 0.0802953690290451,
      "learning_rate": 0.0001919293078055965,
      "loss": 0.4253,
      "step": 142
    },
    {
      "epoch": 0.04205882352941177,
      "grad_norm": 0.07491888105869293,
      "learning_rate": 0.00019187039764359353,
      "loss": 0.3895,
      "step": 143
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 0.09178746491670609,
      "learning_rate": 0.0001918114874815906,
      "loss": 0.412,
      "step": 144
    },
    {
      "epoch": 0.04264705882352941,
      "grad_norm": 0.08292875438928604,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.3632,
      "step": 145
    },
    {
      "epoch": 0.04294117647058823,
      "grad_norm": 0.09996288269758224,
      "learning_rate": 0.0001916936671575847,
      "loss": 0.4135,
      "step": 146
    },
    {
      "epoch": 0.04323529411764706,
      "grad_norm": 0.06329604983329773,
      "learning_rate": 0.00019163475699558177,
      "loss": 0.3524,
      "step": 147
    },
    {
      "epoch": 0.04352941176470588,
      "grad_norm": 0.08010169863700867,
      "learning_rate": 0.0001915758468335788,
      "loss": 0.3706,
      "step": 148
    },
    {
      "epoch": 0.043823529411764706,
      "grad_norm": 0.09632746875286102,
      "learning_rate": 0.00019151693667157586,
      "loss": 0.4426,
      "step": 149
    },
    {
      "epoch": 0.04411764705882353,
      "grad_norm": 0.10381979495286942,
      "learning_rate": 0.00019145802650957292,
      "loss": 0.3951,
      "step": 150
    },
    {
      "epoch": 0.04441176470588235,
      "grad_norm": 0.05651373043656349,
      "learning_rate": 0.00019139911634756998,
      "loss": 0.324,
      "step": 151
    },
    {
      "epoch": 0.04470588235294118,
      "grad_norm": 0.07263705134391785,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.3196,
      "step": 152
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.08887629210948944,
      "learning_rate": 0.00019128129602356407,
      "loss": 0.4209,
      "step": 153
    },
    {
      "epoch": 0.045294117647058825,
      "grad_norm": 0.08438466489315033,
      "learning_rate": 0.00019122238586156113,
      "loss": 0.4128,
      "step": 154
    },
    {
      "epoch": 0.045588235294117645,
      "grad_norm": 0.07210016995668411,
      "learning_rate": 0.0001911634756995582,
      "loss": 0.3699,
      "step": 155
    },
    {
      "epoch": 0.04588235294117647,
      "grad_norm": 0.05374811962246895,
      "learning_rate": 0.00019110456553755525,
      "loss": 0.3022,
      "step": 156
    },
    {
      "epoch": 0.04617647058823529,
      "grad_norm": 0.0962301567196846,
      "learning_rate": 0.00019104565537555231,
      "loss": 0.4716,
      "step": 157
    },
    {
      "epoch": 0.04647058823529412,
      "grad_norm": 0.0778447687625885,
      "learning_rate": 0.00019098674521354935,
      "loss": 0.3786,
      "step": 158
    },
    {
      "epoch": 0.046764705882352944,
      "grad_norm": 0.10694622248411179,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.422,
      "step": 159
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 0.06638095527887344,
      "learning_rate": 0.00019086892488954347,
      "loss": 0.2732,
      "step": 160
    },
    {
      "epoch": 0.04735294117647059,
      "grad_norm": 0.09084460139274597,
      "learning_rate": 0.00019081001472754053,
      "loss": 0.3637,
      "step": 161
    },
    {
      "epoch": 0.04764705882352941,
      "grad_norm": 0.08099769055843353,
      "learning_rate": 0.0001907511045655376,
      "loss": 0.3694,
      "step": 162
    },
    {
      "epoch": 0.04794117647058824,
      "grad_norm": 0.09244579821825027,
      "learning_rate": 0.00019069219440353462,
      "loss": 0.3655,
      "step": 163
    },
    {
      "epoch": 0.04823529411764706,
      "grad_norm": 0.09207990020513535,
      "learning_rate": 0.00019063328424153168,
      "loss": 0.3535,
      "step": 164
    },
    {
      "epoch": 0.04852941176470588,
      "grad_norm": 0.09165927767753601,
      "learning_rate": 0.00019057437407952871,
      "loss": 0.3486,
      "step": 165
    },
    {
      "epoch": 0.0488235294117647,
      "grad_norm": 0.10693888366222382,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.4196,
      "step": 166
    },
    {
      "epoch": 0.04911764705882353,
      "grad_norm": 0.08076412975788116,
      "learning_rate": 0.00019045655375552283,
      "loss": 0.3039,
      "step": 167
    },
    {
      "epoch": 0.04941176470588235,
      "grad_norm": 0.08076328784227371,
      "learning_rate": 0.00019039764359351987,
      "loss": 0.3357,
      "step": 168
    },
    {
      "epoch": 0.049705882352941176,
      "grad_norm": 0.07852445542812347,
      "learning_rate": 0.00019033873343151693,
      "loss": 0.3743,
      "step": 169
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.07005435228347778,
      "learning_rate": 0.000190279823269514,
      "loss": 0.327,
      "step": 170
    },
    {
      "epoch": 0.05029411764705882,
      "grad_norm": 0.07748187333345413,
      "learning_rate": 0.00019022091310751105,
      "loss": 0.4139,
      "step": 171
    },
    {
      "epoch": 0.05058823529411765,
      "grad_norm": 0.08679607510566711,
      "learning_rate": 0.0001901620029455081,
      "loss": 0.3866,
      "step": 172
    },
    {
      "epoch": 0.05088235294117647,
      "grad_norm": 0.07484240084886551,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.3319,
      "step": 173
    },
    {
      "epoch": 0.051176470588235295,
      "grad_norm": 0.08253172785043716,
      "learning_rate": 0.0001900441826215022,
      "loss": 0.3339,
      "step": 174
    },
    {
      "epoch": 0.051470588235294115,
      "grad_norm": 0.09325040131807327,
      "learning_rate": 0.00018998527245949926,
      "loss": 0.4256,
      "step": 175
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 0.08114452660083771,
      "learning_rate": 0.00018992636229749632,
      "loss": 0.3255,
      "step": 176
    },
    {
      "epoch": 0.05205882352941176,
      "grad_norm": 0.11641375720500946,
      "learning_rate": 0.00018986745213549338,
      "loss": 0.3524,
      "step": 177
    },
    {
      "epoch": 0.05235294117647059,
      "grad_norm": 0.08202917873859406,
      "learning_rate": 0.00018980854197349042,
      "loss": 0.3387,
      "step": 178
    },
    {
      "epoch": 0.052647058823529415,
      "grad_norm": 0.07000445574522018,
      "learning_rate": 0.00018974963181148748,
      "loss": 0.3619,
      "step": 179
    },
    {
      "epoch": 0.052941176470588235,
      "grad_norm": 0.10430999100208282,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.3723,
      "step": 180
    },
    {
      "epoch": 0.05323529411764706,
      "grad_norm": 0.08108723163604736,
      "learning_rate": 0.0001896318114874816,
      "loss": 0.4293,
      "step": 181
    },
    {
      "epoch": 0.05352941176470588,
      "grad_norm": 0.07953565567731857,
      "learning_rate": 0.00018957290132547866,
      "loss": 0.3287,
      "step": 182
    },
    {
      "epoch": 0.05382352941176471,
      "grad_norm": 0.05877528712153435,
      "learning_rate": 0.0001895139911634757,
      "loss": 0.2636,
      "step": 183
    },
    {
      "epoch": 0.05411764705882353,
      "grad_norm": 0.0758645236492157,
      "learning_rate": 0.00018945508100147275,
      "loss": 0.367,
      "step": 184
    },
    {
      "epoch": 0.054411764705882354,
      "grad_norm": 0.07813616842031479,
      "learning_rate": 0.0001893961708394698,
      "loss": 0.3368,
      "step": 185
    },
    {
      "epoch": 0.054705882352941174,
      "grad_norm": 0.08332835137844086,
      "learning_rate": 0.00018933726067746687,
      "loss": 0.3504,
      "step": 186
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.11283726990222931,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.3934,
      "step": 187
    },
    {
      "epoch": 0.05529411764705883,
      "grad_norm": 0.059762921184301376,
      "learning_rate": 0.00018921944035346096,
      "loss": 0.3449,
      "step": 188
    },
    {
      "epoch": 0.05558823529411765,
      "grad_norm": 0.06143395975232124,
      "learning_rate": 0.00018916053019145802,
      "loss": 0.2979,
      "step": 189
    },
    {
      "epoch": 0.05588235294117647,
      "grad_norm": 0.08984668552875519,
      "learning_rate": 0.00018910162002945508,
      "loss": 0.4148,
      "step": 190
    },
    {
      "epoch": 0.05617647058823529,
      "grad_norm": 0.05973659083247185,
      "learning_rate": 0.00018904270986745214,
      "loss": 0.3196,
      "step": 191
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 0.06807804852724075,
      "learning_rate": 0.0001889837997054492,
      "loss": 0.4252,
      "step": 192
    },
    {
      "epoch": 0.05676470588235294,
      "grad_norm": 0.06614738702774048,
      "learning_rate": 0.00018892488954344624,
      "loss": 0.3819,
      "step": 193
    },
    {
      "epoch": 0.057058823529411766,
      "grad_norm": 0.07081478089094162,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.371,
      "step": 194
    },
    {
      "epoch": 0.057352941176470586,
      "grad_norm": 0.08730503916740417,
      "learning_rate": 0.00018880706921944036,
      "loss": 0.4878,
      "step": 195
    },
    {
      "epoch": 0.05764705882352941,
      "grad_norm": 0.08386127650737762,
      "learning_rate": 0.00018874815905743742,
      "loss": 0.4275,
      "step": 196
    },
    {
      "epoch": 0.05794117647058823,
      "grad_norm": 0.06872805953025818,
      "learning_rate": 0.00018868924889543448,
      "loss": 0.2907,
      "step": 197
    },
    {
      "epoch": 0.05823529411764706,
      "grad_norm": 0.07516340166330338,
      "learning_rate": 0.0001886303387334315,
      "loss": 0.3722,
      "step": 198
    },
    {
      "epoch": 0.058529411764705885,
      "grad_norm": 0.0641753226518631,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3229,
      "step": 199
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.06973642855882645,
      "learning_rate": 0.00018851251840942563,
      "loss": 0.3445,
      "step": 200
    },
    {
      "epoch": 0.058823529411764705,
      "eval_loss": 0.3767182528972626,
      "eval_runtime": 214.4602,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 200
    },
    {
      "epoch": 0.05911764705882353,
      "grad_norm": 0.08166845887899399,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.3866,
      "step": 201
    },
    {
      "epoch": 0.05941176470588235,
      "grad_norm": 0.062053874135017395,
      "learning_rate": 0.00018839469808541975,
      "loss": 0.3172,
      "step": 202
    },
    {
      "epoch": 0.05970588235294118,
      "grad_norm": 0.061366621404886246,
      "learning_rate": 0.00018833578792341678,
      "loss": 0.3834,
      "step": 203
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.053111426532268524,
      "learning_rate": 0.00018827687776141384,
      "loss": 0.298,
      "step": 204
    },
    {
      "epoch": 0.060294117647058824,
      "grad_norm": 0.099363312125206,
      "learning_rate": 0.0001882179675994109,
      "loss": 0.3964,
      "step": 205
    },
    {
      "epoch": 0.060588235294117644,
      "grad_norm": 0.05475326254963875,
      "learning_rate": 0.00018815905743740796,
      "loss": 0.3463,
      "step": 206
    },
    {
      "epoch": 0.06088235294117647,
      "grad_norm": 0.06388437747955322,
      "learning_rate": 0.00018810014727540502,
      "loss": 0.3271,
      "step": 207
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 0.06116502359509468,
      "learning_rate": 0.00018804123711340206,
      "loss": 0.343,
      "step": 208
    },
    {
      "epoch": 0.06147058823529412,
      "grad_norm": 0.08131694793701172,
      "learning_rate": 0.00018798232695139912,
      "loss": 0.3793,
      "step": 209
    },
    {
      "epoch": 0.061764705882352944,
      "grad_norm": 0.06262039393186569,
      "learning_rate": 0.00018792341678939618,
      "loss": 0.3383,
      "step": 210
    },
    {
      "epoch": 0.062058823529411763,
      "grad_norm": 0.0843704417347908,
      "learning_rate": 0.00018786450662739324,
      "loss": 0.4044,
      "step": 211
    },
    {
      "epoch": 0.06235294117647059,
      "grad_norm": 0.0662040263414383,
      "learning_rate": 0.0001878055964653903,
      "loss": 0.3542,
      "step": 212
    },
    {
      "epoch": 0.06264705882352942,
      "grad_norm": 0.06551224738359451,
      "learning_rate": 0.00018774668630338733,
      "loss": 0.384,
      "step": 213
    },
    {
      "epoch": 0.06294117647058824,
      "grad_norm": 0.06674382090568542,
      "learning_rate": 0.0001876877761413844,
      "loss": 0.3074,
      "step": 214
    },
    {
      "epoch": 0.06323529411764706,
      "grad_norm": 0.08320005983114243,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.3714,
      "step": 215
    },
    {
      "epoch": 0.06352941176470588,
      "grad_norm": 0.052629657089710236,
      "learning_rate": 0.0001875699558173785,
      "loss": 0.3204,
      "step": 216
    },
    {
      "epoch": 0.06382352941176471,
      "grad_norm": 0.06657926738262177,
      "learning_rate": 0.00018751104565537557,
      "loss": 0.2796,
      "step": 217
    },
    {
      "epoch": 0.06411764705882353,
      "grad_norm": 0.06517869234085083,
      "learning_rate": 0.0001874521354933726,
      "loss": 0.3483,
      "step": 218
    },
    {
      "epoch": 0.06441176470588235,
      "grad_norm": 0.09810834378004074,
      "learning_rate": 0.00018739322533136967,
      "loss": 0.3366,
      "step": 219
    },
    {
      "epoch": 0.06470588235294118,
      "grad_norm": 0.10006564855575562,
      "learning_rate": 0.00018733431516936673,
      "loss": 0.4012,
      "step": 220
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.09300980716943741,
      "learning_rate": 0.00018727540500736379,
      "loss": 0.3651,
      "step": 221
    },
    {
      "epoch": 0.06529411764705882,
      "grad_norm": 0.06971597671508789,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.3294,
      "step": 222
    },
    {
      "epoch": 0.06558823529411764,
      "grad_norm": 0.0776345282793045,
      "learning_rate": 0.00018715758468335788,
      "loss": 0.3345,
      "step": 223
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 0.0927155613899231,
      "learning_rate": 0.00018709867452135494,
      "loss": 0.3162,
      "step": 224
    },
    {
      "epoch": 0.0661764705882353,
      "grad_norm": 0.08812511712312698,
      "learning_rate": 0.000187039764359352,
      "loss": 0.3909,
      "step": 225
    },
    {
      "epoch": 0.06647058823529411,
      "grad_norm": 0.07460417598485947,
      "learning_rate": 0.00018698085419734906,
      "loss": 0.3995,
      "step": 226
    },
    {
      "epoch": 0.06676470588235293,
      "grad_norm": 0.057078417390584946,
      "learning_rate": 0.00018692194403534612,
      "loss": 0.3695,
      "step": 227
    },
    {
      "epoch": 0.06705882352941177,
      "grad_norm": 0.05906906723976135,
      "learning_rate": 0.00018686303387334315,
      "loss": 0.3419,
      "step": 228
    },
    {
      "epoch": 0.06735294117647059,
      "grad_norm": 0.06583355367183685,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.2909,
      "step": 229
    },
    {
      "epoch": 0.06764705882352941,
      "grad_norm": 0.07396094501018524,
      "learning_rate": 0.00018674521354933727,
      "loss": 0.3711,
      "step": 230
    },
    {
      "epoch": 0.06794117647058824,
      "grad_norm": 0.08158345520496368,
      "learning_rate": 0.00018668630338733433,
      "loss": 0.3674,
      "step": 231
    },
    {
      "epoch": 0.06823529411764706,
      "grad_norm": 0.0752912163734436,
      "learning_rate": 0.0001866273932253314,
      "loss": 0.4318,
      "step": 232
    },
    {
      "epoch": 0.06852941176470588,
      "grad_norm": 0.10488764941692352,
      "learning_rate": 0.00018656848306332843,
      "loss": 0.3866,
      "step": 233
    },
    {
      "epoch": 0.0688235294117647,
      "grad_norm": 0.07578109204769135,
      "learning_rate": 0.0001865095729013255,
      "loss": 0.384,
      "step": 234
    },
    {
      "epoch": 0.06911764705882353,
      "grad_norm": 0.052479587495326996,
      "learning_rate": 0.00018645066273932255,
      "loss": 0.3382,
      "step": 235
    },
    {
      "epoch": 0.06941176470588235,
      "grad_norm": 0.06620831787586212,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.3808,
      "step": 236
    },
    {
      "epoch": 0.06970588235294117,
      "grad_norm": 0.07575883716344833,
      "learning_rate": 0.00018633284241531667,
      "loss": 0.3497,
      "step": 237
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0871863141655922,
      "learning_rate": 0.0001862739322533137,
      "loss": 0.4954,
      "step": 238
    },
    {
      "epoch": 0.07029411764705883,
      "grad_norm": 0.0836196318268776,
      "learning_rate": 0.00018621502209131076,
      "loss": 0.3722,
      "step": 239
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 0.07479219883680344,
      "learning_rate": 0.00018615611192930782,
      "loss": 0.4152,
      "step": 240
    },
    {
      "epoch": 0.07088235294117647,
      "grad_norm": 0.07516150176525116,
      "learning_rate": 0.00018609720176730488,
      "loss": 0.3162,
      "step": 241
    },
    {
      "epoch": 0.0711764705882353,
      "grad_norm": 0.06700415164232254,
      "learning_rate": 0.00018603829160530194,
      "loss": 0.3604,
      "step": 242
    },
    {
      "epoch": 0.07147058823529412,
      "grad_norm": 0.0815596655011177,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.3925,
      "step": 243
    },
    {
      "epoch": 0.07176470588235294,
      "grad_norm": 0.06797503679990768,
      "learning_rate": 0.00018592047128129603,
      "loss": 0.3939,
      "step": 244
    },
    {
      "epoch": 0.07205882352941176,
      "grad_norm": 0.08037151396274567,
      "learning_rate": 0.0001858615611192931,
      "loss": 0.363,
      "step": 245
    },
    {
      "epoch": 0.07235294117647059,
      "grad_norm": 0.10852646082639694,
      "learning_rate": 0.00018580265095729015,
      "loss": 0.4394,
      "step": 246
    },
    {
      "epoch": 0.07264705882352941,
      "grad_norm": 0.10037321597337723,
      "learning_rate": 0.00018574374079528721,
      "loss": 0.4556,
      "step": 247
    },
    {
      "epoch": 0.07294117647058823,
      "grad_norm": 0.062211133539676666,
      "learning_rate": 0.00018568483063328425,
      "loss": 0.3554,
      "step": 248
    },
    {
      "epoch": 0.07323529411764707,
      "grad_norm": 0.09802661091089249,
      "learning_rate": 0.0001856259204712813,
      "loss": 0.3845,
      "step": 249
    },
    {
      "epoch": 0.07352941176470588,
      "grad_norm": 0.10403288155794144,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.4759,
      "step": 250
    },
    {
      "epoch": 0.0738235294117647,
      "grad_norm": 0.055067576467990875,
      "learning_rate": 0.00018550810014727543,
      "loss": 0.2973,
      "step": 251
    },
    {
      "epoch": 0.07411764705882352,
      "grad_norm": 0.06348162144422531,
      "learning_rate": 0.0001854491899852725,
      "loss": 0.3869,
      "step": 252
    },
    {
      "epoch": 0.07441176470588236,
      "grad_norm": 0.06961240619421005,
      "learning_rate": 0.00018539027982326952,
      "loss": 0.3115,
      "step": 253
    },
    {
      "epoch": 0.07470588235294118,
      "grad_norm": 0.06145700812339783,
      "learning_rate": 0.00018533136966126658,
      "loss": 0.3156,
      "step": 254
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.08133311569690704,
      "learning_rate": 0.00018527245949926364,
      "loss": 0.4542,
      "step": 255
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 0.0609547384083271,
      "learning_rate": 0.0001852135493372607,
      "loss": 0.3222,
      "step": 256
    },
    {
      "epoch": 0.07558823529411765,
      "grad_norm": 0.04856938123703003,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.297,
      "step": 257
    },
    {
      "epoch": 0.07588235294117647,
      "grad_norm": 0.07120084017515182,
      "learning_rate": 0.0001850957290132548,
      "loss": 0.3647,
      "step": 258
    },
    {
      "epoch": 0.07617647058823529,
      "grad_norm": 0.08879172056913376,
      "learning_rate": 0.00018503681885125186,
      "loss": 0.3811,
      "step": 259
    },
    {
      "epoch": 0.07647058823529412,
      "grad_norm": 0.06976604461669922,
      "learning_rate": 0.00018497790868924892,
      "loss": 0.4262,
      "step": 260
    },
    {
      "epoch": 0.07676470588235294,
      "grad_norm": 0.07280811667442322,
      "learning_rate": 0.00018491899852724598,
      "loss": 0.4456,
      "step": 261
    },
    {
      "epoch": 0.07705882352941176,
      "grad_norm": 0.09368153661489487,
      "learning_rate": 0.00018486008836524304,
      "loss": 0.4654,
      "step": 262
    },
    {
      "epoch": 0.07735294117647058,
      "grad_norm": 0.05659034103155136,
      "learning_rate": 0.00018480117820324007,
      "loss": 0.3351,
      "step": 263
    },
    {
      "epoch": 0.07764705882352942,
      "grad_norm": 0.0724044144153595,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.3908,
      "step": 264
    },
    {
      "epoch": 0.07794117647058824,
      "grad_norm": 0.08915983140468597,
      "learning_rate": 0.0001846833578792342,
      "loss": 0.3803,
      "step": 265
    },
    {
      "epoch": 0.07823529411764706,
      "grad_norm": 0.07771384716033936,
      "learning_rate": 0.00018462444771723125,
      "loss": 0.4127,
      "step": 266
    },
    {
      "epoch": 0.07852941176470589,
      "grad_norm": 0.0986986979842186,
      "learning_rate": 0.0001845655375552283,
      "loss": 0.4261,
      "step": 267
    },
    {
      "epoch": 0.07882352941176471,
      "grad_norm": 0.0804448127746582,
      "learning_rate": 0.00018450662739322534,
      "loss": 0.3962,
      "step": 268
    },
    {
      "epoch": 0.07911764705882353,
      "grad_norm": 0.07400243729352951,
      "learning_rate": 0.0001844477172312224,
      "loss": 0.3748,
      "step": 269
    },
    {
      "epoch": 0.07941176470588235,
      "grad_norm": 0.09537741541862488,
      "learning_rate": 0.00018438880706921946,
      "loss": 0.3819,
      "step": 270
    },
    {
      "epoch": 0.07970588235294118,
      "grad_norm": 0.08744517713785172,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.4168,
      "step": 271
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.0631398856639862,
      "learning_rate": 0.00018427098674521356,
      "loss": 0.2908,
      "step": 272
    },
    {
      "epoch": 0.08029411764705882,
      "grad_norm": 0.07990700751543045,
      "learning_rate": 0.0001842120765832106,
      "loss": 0.325,
      "step": 273
    },
    {
      "epoch": 0.08058823529411764,
      "grad_norm": 0.09226884692907333,
      "learning_rate": 0.00018415316642120765,
      "loss": 0.3699,
      "step": 274
    },
    {
      "epoch": 0.08088235294117647,
      "grad_norm": 0.07983876019716263,
      "learning_rate": 0.0001840942562592047,
      "loss": 0.3971,
      "step": 275
    },
    {
      "epoch": 0.0811764705882353,
      "grad_norm": 0.0801749899983406,
      "learning_rate": 0.00018403534609720177,
      "loss": 0.3373,
      "step": 276
    },
    {
      "epoch": 0.08147058823529411,
      "grad_norm": 0.07723645865917206,
      "learning_rate": 0.00018397643593519883,
      "loss": 0.398,
      "step": 277
    },
    {
      "epoch": 0.08176470588235295,
      "grad_norm": 0.06696151196956635,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.3898,
      "step": 278
    },
    {
      "epoch": 0.08205882352941177,
      "grad_norm": 0.07642757147550583,
      "learning_rate": 0.00018385861561119292,
      "loss": 0.4008,
      "step": 279
    },
    {
      "epoch": 0.08235294117647059,
      "grad_norm": 0.08622708916664124,
      "learning_rate": 0.00018379970544918998,
      "loss": 0.4354,
      "step": 280
    },
    {
      "epoch": 0.0826470588235294,
      "grad_norm": 0.08928098529577255,
      "learning_rate": 0.00018374079528718704,
      "loss": 0.4553,
      "step": 281
    },
    {
      "epoch": 0.08294117647058824,
      "grad_norm": 0.09323769062757492,
      "learning_rate": 0.0001836818851251841,
      "loss": 0.4542,
      "step": 282
    },
    {
      "epoch": 0.08323529411764706,
      "grad_norm": 0.06147265434265137,
      "learning_rate": 0.00018362297496318114,
      "loss": 0.3943,
      "step": 283
    },
    {
      "epoch": 0.08352941176470588,
      "grad_norm": 0.060861315578222275,
      "learning_rate": 0.0001835640648011782,
      "loss": 0.3185,
      "step": 284
    },
    {
      "epoch": 0.0838235294117647,
      "grad_norm": 0.0663820207118988,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.3383,
      "step": 285
    },
    {
      "epoch": 0.08411764705882353,
      "grad_norm": 0.055125344544649124,
      "learning_rate": 0.00018344624447717232,
      "loss": 0.3278,
      "step": 286
    },
    {
      "epoch": 0.08441176470588235,
      "grad_norm": 0.07817421108484268,
      "learning_rate": 0.00018338733431516938,
      "loss": 0.423,
      "step": 287
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 0.061867907643318176,
      "learning_rate": 0.0001833284241531664,
      "loss": 0.3475,
      "step": 288
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.06432729214429855,
      "learning_rate": 0.00018326951399116347,
      "loss": 0.4151,
      "step": 289
    },
    {
      "epoch": 0.08529411764705883,
      "grad_norm": 0.08231685310602188,
      "learning_rate": 0.00018321060382916053,
      "loss": 0.4428,
      "step": 290
    },
    {
      "epoch": 0.08558823529411765,
      "grad_norm": 0.05390845611691475,
      "learning_rate": 0.0001831516936671576,
      "loss": 0.2827,
      "step": 291
    },
    {
      "epoch": 0.08588235294117647,
      "grad_norm": 0.10428059101104736,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.4806,
      "step": 292
    },
    {
      "epoch": 0.0861764705882353,
      "grad_norm": 0.06025645509362221,
      "learning_rate": 0.00018303387334315168,
      "loss": 0.3857,
      "step": 293
    },
    {
      "epoch": 0.08647058823529412,
      "grad_norm": 0.06423747539520264,
      "learning_rate": 0.00018297496318114874,
      "loss": 0.3793,
      "step": 294
    },
    {
      "epoch": 0.08676470588235294,
      "grad_norm": 0.06492647528648376,
      "learning_rate": 0.0001829160530191458,
      "loss": 0.3688,
      "step": 295
    },
    {
      "epoch": 0.08705882352941176,
      "grad_norm": 0.06362714618444443,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.4156,
      "step": 296
    },
    {
      "epoch": 0.08735294117647059,
      "grad_norm": 0.06653653830289841,
      "learning_rate": 0.00018279823269513992,
      "loss": 0.3548,
      "step": 297
    },
    {
      "epoch": 0.08764705882352941,
      "grad_norm": 0.07362311333417892,
      "learning_rate": 0.00018273932253313696,
      "loss": 0.3489,
      "step": 298
    },
    {
      "epoch": 0.08794117647058823,
      "grad_norm": 0.06643049418926239,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.3615,
      "step": 299
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.07043516635894775,
      "learning_rate": 0.00018262150220913108,
      "loss": 0.4047,
      "step": 300
    },
    {
      "epoch": 0.08823529411764706,
      "eval_loss": 0.3704615533351898,
      "eval_runtime": 214.0093,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 300
    },
    {
      "epoch": 0.08852941176470588,
      "grad_norm": 0.05111408233642578,
      "learning_rate": 0.00018256259204712814,
      "loss": 0.2938,
      "step": 301
    },
    {
      "epoch": 0.0888235294117647,
      "grad_norm": 0.06887197494506836,
      "learning_rate": 0.0001825036818851252,
      "loss": 0.39,
      "step": 302
    },
    {
      "epoch": 0.08911764705882352,
      "grad_norm": 0.0789952427148819,
      "learning_rate": 0.00018244477172312223,
      "loss": 0.3863,
      "step": 303
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 0.06443461030721664,
      "learning_rate": 0.0001823858615611193,
      "loss": 0.3641,
      "step": 304
    },
    {
      "epoch": 0.08970588235294118,
      "grad_norm": 0.060883525758981705,
      "learning_rate": 0.00018232695139911635,
      "loss": 0.3922,
      "step": 305
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.05561806261539459,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.3209,
      "step": 306
    },
    {
      "epoch": 0.09029411764705883,
      "grad_norm": 0.07831435650587082,
      "learning_rate": 0.00018220913107511047,
      "loss": 0.3631,
      "step": 307
    },
    {
      "epoch": 0.09058823529411765,
      "grad_norm": 0.0717497318983078,
      "learning_rate": 0.0001821502209131075,
      "loss": 0.36,
      "step": 308
    },
    {
      "epoch": 0.09088235294117647,
      "grad_norm": 0.07044396549463272,
      "learning_rate": 0.00018209131075110457,
      "loss": 0.3772,
      "step": 309
    },
    {
      "epoch": 0.09117647058823529,
      "grad_norm": 0.05202275514602661,
      "learning_rate": 0.00018203240058910163,
      "loss": 0.3335,
      "step": 310
    },
    {
      "epoch": 0.09147058823529412,
      "grad_norm": 0.07764929533004761,
      "learning_rate": 0.00018197349042709869,
      "loss": 0.408,
      "step": 311
    },
    {
      "epoch": 0.09176470588235294,
      "grad_norm": 0.06273847818374634,
      "learning_rate": 0.00018191458026509575,
      "loss": 0.357,
      "step": 312
    },
    {
      "epoch": 0.09205882352941176,
      "grad_norm": 0.0746827945113182,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.402,
      "step": 313
    },
    {
      "epoch": 0.09235294117647058,
      "grad_norm": 0.05298242345452309,
      "learning_rate": 0.00018179675994108984,
      "loss": 0.3659,
      "step": 314
    },
    {
      "epoch": 0.09264705882352942,
      "grad_norm": 0.06540913879871368,
      "learning_rate": 0.0001817378497790869,
      "loss": 0.345,
      "step": 315
    },
    {
      "epoch": 0.09294117647058824,
      "grad_norm": 0.06143077835440636,
      "learning_rate": 0.00018167893961708396,
      "loss": 0.3885,
      "step": 316
    },
    {
      "epoch": 0.09323529411764706,
      "grad_norm": 0.0803089290857315,
      "learning_rate": 0.00018162002945508102,
      "loss": 0.2972,
      "step": 317
    },
    {
      "epoch": 0.09352941176470589,
      "grad_norm": 0.056733958423137665,
      "learning_rate": 0.00018156111929307805,
      "loss": 0.3107,
      "step": 318
    },
    {
      "epoch": 0.09382352941176471,
      "grad_norm": 0.0864543467760086,
      "learning_rate": 0.0001815022091310751,
      "loss": 0.4175,
      "step": 319
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.05277983099222183,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.3065,
      "step": 320
    },
    {
      "epoch": 0.09441176470588235,
      "grad_norm": 0.06177477166056633,
      "learning_rate": 0.00018138438880706923,
      "loss": 0.3307,
      "step": 321
    },
    {
      "epoch": 0.09470588235294118,
      "grad_norm": 0.0936097726225853,
      "learning_rate": 0.0001813254786450663,
      "loss": 0.444,
      "step": 322
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06528394669294357,
      "learning_rate": 0.00018126656848306333,
      "loss": 0.3443,
      "step": 323
    },
    {
      "epoch": 0.09529411764705882,
      "grad_norm": 0.06071492284536362,
      "learning_rate": 0.00018120765832106039,
      "loss": 0.348,
      "step": 324
    },
    {
      "epoch": 0.09558823529411764,
      "grad_norm": 0.07457941025495529,
      "learning_rate": 0.00018114874815905745,
      "loss": 0.3457,
      "step": 325
    },
    {
      "epoch": 0.09588235294117647,
      "grad_norm": 0.08464271575212479,
      "learning_rate": 0.0001810898379970545,
      "loss": 0.4089,
      "step": 326
    },
    {
      "epoch": 0.0961764705882353,
      "grad_norm": 0.06049331650137901,
      "learning_rate": 0.00018103092783505157,
      "loss": 0.3713,
      "step": 327
    },
    {
      "epoch": 0.09647058823529411,
      "grad_norm": 0.06371386349201202,
      "learning_rate": 0.0001809720176730486,
      "loss": 0.3219,
      "step": 328
    },
    {
      "epoch": 0.09676470588235295,
      "grad_norm": 0.06541358679533005,
      "learning_rate": 0.00018091310751104566,
      "loss": 0.3225,
      "step": 329
    },
    {
      "epoch": 0.09705882352941177,
      "grad_norm": 0.06573724001646042,
      "learning_rate": 0.00018085419734904272,
      "loss": 0.3102,
      "step": 330
    },
    {
      "epoch": 0.09735294117647059,
      "grad_norm": 0.0672830119729042,
      "learning_rate": 0.00018079528718703978,
      "loss": 0.3864,
      "step": 331
    },
    {
      "epoch": 0.0976470588235294,
      "grad_norm": 0.0658344179391861,
      "learning_rate": 0.00018073637702503684,
      "loss": 0.3578,
      "step": 332
    },
    {
      "epoch": 0.09794117647058824,
      "grad_norm": 0.05132212117314339,
      "learning_rate": 0.00018067746686303387,
      "loss": 0.3227,
      "step": 333
    },
    {
      "epoch": 0.09823529411764706,
      "grad_norm": 0.07412703335285187,
      "learning_rate": 0.00018061855670103093,
      "loss": 0.3921,
      "step": 334
    },
    {
      "epoch": 0.09852941176470588,
      "grad_norm": 0.06318201124668121,
      "learning_rate": 0.000180559646539028,
      "loss": 0.3685,
      "step": 335
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 0.06001734733581543,
      "learning_rate": 0.00018050073637702505,
      "loss": 0.3365,
      "step": 336
    },
    {
      "epoch": 0.09911764705882353,
      "grad_norm": 0.06040819361805916,
      "learning_rate": 0.00018044182621502211,
      "loss": 0.404,
      "step": 337
    },
    {
      "epoch": 0.09941176470588235,
      "grad_norm": 0.06483824551105499,
      "learning_rate": 0.00018038291605301915,
      "loss": 0.3489,
      "step": 338
    },
    {
      "epoch": 0.09970588235294117,
      "grad_norm": 0.049011338502168655,
      "learning_rate": 0.0001803240058910162,
      "loss": 0.315,
      "step": 339
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.07122957706451416,
      "learning_rate": 0.00018026509572901327,
      "loss": 0.412,
      "step": 340
    },
    {
      "epoch": 0.10029411764705883,
      "grad_norm": 0.06241445615887642,
      "learning_rate": 0.00018020618556701033,
      "loss": 0.3668,
      "step": 341
    },
    {
      "epoch": 0.10058823529411764,
      "grad_norm": 0.07471606880426407,
      "learning_rate": 0.0001801472754050074,
      "loss": 0.3731,
      "step": 342
    },
    {
      "epoch": 0.10088235294117646,
      "grad_norm": 0.06179254874587059,
      "learning_rate": 0.00018008836524300442,
      "loss": 0.3401,
      "step": 343
    },
    {
      "epoch": 0.1011764705882353,
      "grad_norm": 0.07204656302928925,
      "learning_rate": 0.00018002945508100148,
      "loss": 0.4009,
      "step": 344
    },
    {
      "epoch": 0.10147058823529412,
      "grad_norm": 0.0760396420955658,
      "learning_rate": 0.00017997054491899854,
      "loss": 0.3671,
      "step": 345
    },
    {
      "epoch": 0.10176470588235294,
      "grad_norm": 0.05884910002350807,
      "learning_rate": 0.0001799116347569956,
      "loss": 0.4125,
      "step": 346
    },
    {
      "epoch": 0.10205882352941177,
      "grad_norm": 0.06462839990854263,
      "learning_rate": 0.00017985272459499266,
      "loss": 0.3126,
      "step": 347
    },
    {
      "epoch": 0.10235294117647059,
      "grad_norm": 0.07201676815748215,
      "learning_rate": 0.0001797938144329897,
      "loss": 0.31,
      "step": 348
    },
    {
      "epoch": 0.10264705882352941,
      "grad_norm": 0.0700809508562088,
      "learning_rate": 0.00017973490427098675,
      "loss": 0.343,
      "step": 349
    },
    {
      "epoch": 0.10294117647058823,
      "grad_norm": 0.05343497171998024,
      "learning_rate": 0.00017967599410898382,
      "loss": 0.3251,
      "step": 350
    },
    {
      "epoch": 0.10323529411764706,
      "grad_norm": 0.05584115907549858,
      "learning_rate": 0.00017961708394698088,
      "loss": 0.3171,
      "step": 351
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 0.05746942758560181,
      "learning_rate": 0.00017955817378497794,
      "loss": 0.3416,
      "step": 352
    },
    {
      "epoch": 0.1038235294117647,
      "grad_norm": 0.07215959578752518,
      "learning_rate": 0.00017949926362297497,
      "loss": 0.3658,
      "step": 353
    },
    {
      "epoch": 0.10411764705882352,
      "grad_norm": 0.07397723942995071,
      "learning_rate": 0.00017944035346097203,
      "loss": 0.4487,
      "step": 354
    },
    {
      "epoch": 0.10441176470588236,
      "grad_norm": 0.07747973501682281,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4236,
      "step": 355
    },
    {
      "epoch": 0.10470588235294118,
      "grad_norm": 0.09172757714986801,
      "learning_rate": 0.00017932253313696615,
      "loss": 0.4821,
      "step": 356
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06078871712088585,
      "learning_rate": 0.0001792636229749632,
      "loss": 0.3594,
      "step": 357
    },
    {
      "epoch": 0.10529411764705883,
      "grad_norm": 0.058273136615753174,
      "learning_rate": 0.00017920471281296024,
      "loss": 0.355,
      "step": 358
    },
    {
      "epoch": 0.10558823529411765,
      "grad_norm": 0.0687500610947609,
      "learning_rate": 0.0001791458026509573,
      "loss": 0.428,
      "step": 359
    },
    {
      "epoch": 0.10588235294117647,
      "grad_norm": 0.06714075803756714,
      "learning_rate": 0.00017908689248895436,
      "loss": 0.432,
      "step": 360
    },
    {
      "epoch": 0.10617647058823529,
      "grad_norm": 0.07221890985965729,
      "learning_rate": 0.00017902798232695142,
      "loss": 0.4209,
      "step": 361
    },
    {
      "epoch": 0.10647058823529412,
      "grad_norm": 0.07603282481431961,
      "learning_rate": 0.00017896907216494848,
      "loss": 0.4382,
      "step": 362
    },
    {
      "epoch": 0.10676470588235294,
      "grad_norm": 0.07435841858386993,
      "learning_rate": 0.00017891016200294552,
      "loss": 0.3795,
      "step": 363
    },
    {
      "epoch": 0.10705882352941176,
      "grad_norm": 0.06485453248023987,
      "learning_rate": 0.00017885125184094258,
      "loss": 0.3296,
      "step": 364
    },
    {
      "epoch": 0.10735294117647058,
      "grad_norm": 0.05698046833276749,
      "learning_rate": 0.00017879234167893964,
      "loss": 0.3454,
      "step": 365
    },
    {
      "epoch": 0.10764705882352942,
      "grad_norm": 0.08076491951942444,
      "learning_rate": 0.0001787334315169367,
      "loss": 0.3827,
      "step": 366
    },
    {
      "epoch": 0.10794117647058823,
      "grad_norm": 0.06518575549125671,
      "learning_rate": 0.00017867452135493376,
      "loss": 0.334,
      "step": 367
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 0.08755042403936386,
      "learning_rate": 0.0001786156111929308,
      "loss": 0.4493,
      "step": 368
    },
    {
      "epoch": 0.10852941176470589,
      "grad_norm": 0.07141425460577011,
      "learning_rate": 0.00017855670103092785,
      "loss": 0.4216,
      "step": 369
    },
    {
      "epoch": 0.10882352941176471,
      "grad_norm": 0.07941947132349014,
      "learning_rate": 0.0001784977908689249,
      "loss": 0.4007,
      "step": 370
    },
    {
      "epoch": 0.10911764705882353,
      "grad_norm": 0.07058899849653244,
      "learning_rate": 0.00017843888070692197,
      "loss": 0.3645,
      "step": 371
    },
    {
      "epoch": 0.10941176470588235,
      "grad_norm": 0.07162158191204071,
      "learning_rate": 0.00017837997054491903,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.10970588235294118,
      "grad_norm": 0.0682276040315628,
      "learning_rate": 0.00017832106038291606,
      "loss": 0.3817,
      "step": 373
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.06322090327739716,
      "learning_rate": 0.00017826215022091312,
      "loss": 0.341,
      "step": 374
    },
    {
      "epoch": 0.11029411764705882,
      "grad_norm": 0.07450313121080399,
      "learning_rate": 0.00017820324005891018,
      "loss": 0.372,
      "step": 375
    },
    {
      "epoch": 0.11058823529411765,
      "grad_norm": 0.0451170913875103,
      "learning_rate": 0.00017814432989690724,
      "loss": 0.2582,
      "step": 376
    },
    {
      "epoch": 0.11088235294117647,
      "grad_norm": 0.05825483798980713,
      "learning_rate": 0.00017808541973490428,
      "loss": 0.3824,
      "step": 377
    },
    {
      "epoch": 0.1111764705882353,
      "grad_norm": 0.09508311003446579,
      "learning_rate": 0.0001780265095729013,
      "loss": 0.3936,
      "step": 378
    },
    {
      "epoch": 0.11147058823529411,
      "grad_norm": 0.07506752014160156,
      "learning_rate": 0.00017796759941089837,
      "loss": 0.3861,
      "step": 379
    },
    {
      "epoch": 0.11176470588235295,
      "grad_norm": 0.047860223799943924,
      "learning_rate": 0.00017790868924889543,
      "loss": 0.3173,
      "step": 380
    },
    {
      "epoch": 0.11205882352941177,
      "grad_norm": 0.05295055732131004,
      "learning_rate": 0.0001778497790868925,
      "loss": 0.3199,
      "step": 381
    },
    {
      "epoch": 0.11235294117647059,
      "grad_norm": 0.0679042711853981,
      "learning_rate": 0.00017779086892488955,
      "loss": 0.367,
      "step": 382
    },
    {
      "epoch": 0.1126470588235294,
      "grad_norm": 0.08049646019935608,
      "learning_rate": 0.00017773195876288658,
      "loss": 0.3803,
      "step": 383
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.059739384800195694,
      "learning_rate": 0.00017767304860088364,
      "loss": 0.3174,
      "step": 384
    },
    {
      "epoch": 0.11323529411764706,
      "grad_norm": 0.06677518039941788,
      "learning_rate": 0.0001776141384388807,
      "loss": 0.4545,
      "step": 385
    },
    {
      "epoch": 0.11352941176470588,
      "grad_norm": 0.0665544793009758,
      "learning_rate": 0.00017755522827687776,
      "loss": 0.3212,
      "step": 386
    },
    {
      "epoch": 0.11382352941176471,
      "grad_norm": 0.05580659210681915,
      "learning_rate": 0.00017749631811487482,
      "loss": 0.3195,
      "step": 387
    },
    {
      "epoch": 0.11411764705882353,
      "grad_norm": 0.06076865643262863,
      "learning_rate": 0.00017743740795287186,
      "loss": 0.3935,
      "step": 388
    },
    {
      "epoch": 0.11441176470588235,
      "grad_norm": 0.055945150554180145,
      "learning_rate": 0.00017737849779086892,
      "loss": 0.3228,
      "step": 389
    },
    {
      "epoch": 0.11470588235294117,
      "grad_norm": 0.05191303789615631,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.3207,
      "step": 390
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.050948645919561386,
      "learning_rate": 0.00017726067746686304,
      "loss": 0.3089,
      "step": 391
    },
    {
      "epoch": 0.11529411764705882,
      "grad_norm": 0.05824671685695648,
      "learning_rate": 0.0001772017673048601,
      "loss": 0.3184,
      "step": 392
    },
    {
      "epoch": 0.11558823529411764,
      "grad_norm": 0.06559012085199356,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3809,
      "step": 393
    },
    {
      "epoch": 0.11588235294117646,
      "grad_norm": 0.059053681790828705,
      "learning_rate": 0.0001770839469808542,
      "loss": 0.3867,
      "step": 394
    },
    {
      "epoch": 0.1161764705882353,
      "grad_norm": 0.08661926537752151,
      "learning_rate": 0.00017702503681885125,
      "loss": 0.4373,
      "step": 395
    },
    {
      "epoch": 0.11647058823529412,
      "grad_norm": 0.08061210811138153,
      "learning_rate": 0.0001769661266568483,
      "loss": 0.4645,
      "step": 396
    },
    {
      "epoch": 0.11676470588235294,
      "grad_norm": 0.05530845746397972,
      "learning_rate": 0.00017690721649484537,
      "loss": 0.3235,
      "step": 397
    },
    {
      "epoch": 0.11705882352941177,
      "grad_norm": 0.06628365814685822,
      "learning_rate": 0.0001768483063328424,
      "loss": 0.3643,
      "step": 398
    },
    {
      "epoch": 0.11735294117647059,
      "grad_norm": 0.08501642197370529,
      "learning_rate": 0.00017678939617083947,
      "loss": 0.3869,
      "step": 399
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.06242210045456886,
      "learning_rate": 0.00017673048600883653,
      "loss": 0.3576,
      "step": 400
    },
    {
      "epoch": 0.11764705882352941,
      "eval_loss": 0.3672531545162201,
      "eval_runtime": 213.7276,
      "eval_samples_per_second": 0.468,
      "eval_steps_per_second": 0.468,
      "step": 400
    },
    {
      "epoch": 0.11794117647058823,
      "grad_norm": 0.08263355493545532,
      "learning_rate": 0.00017667157584683359,
      "loss": 0.4616,
      "step": 401
    },
    {
      "epoch": 0.11823529411764706,
      "grad_norm": 0.07237403839826584,
      "learning_rate": 0.00017661266568483065,
      "loss": 0.474,
      "step": 402
    },
    {
      "epoch": 0.11852941176470588,
      "grad_norm": 0.07534389197826385,
      "learning_rate": 0.00017655375552282768,
      "loss": 0.3536,
      "step": 403
    },
    {
      "epoch": 0.1188235294117647,
      "grad_norm": 0.06584201753139496,
      "learning_rate": 0.00017649484536082474,
      "loss": 0.3358,
      "step": 404
    },
    {
      "epoch": 0.11911764705882352,
      "grad_norm": 0.06149284541606903,
      "learning_rate": 0.0001764359351988218,
      "loss": 0.3823,
      "step": 405
    },
    {
      "epoch": 0.11941176470588236,
      "grad_norm": 0.0789175033569336,
      "learning_rate": 0.00017637702503681886,
      "loss": 0.385,
      "step": 406
    },
    {
      "epoch": 0.11970588235294118,
      "grad_norm": 0.05479629337787628,
      "learning_rate": 0.00017631811487481592,
      "loss": 0.3309,
      "step": 407
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07517760246992111,
      "learning_rate": 0.00017625920471281295,
      "loss": 0.4345,
      "step": 408
    },
    {
      "epoch": 0.12029411764705883,
      "grad_norm": 0.06628546863794327,
      "learning_rate": 0.00017620029455081,
      "loss": 0.3628,
      "step": 409
    },
    {
      "epoch": 0.12058823529411765,
      "grad_norm": 0.07795121520757675,
      "learning_rate": 0.00017614138438880707,
      "loss": 0.4178,
      "step": 410
    },
    {
      "epoch": 0.12088235294117647,
      "grad_norm": 0.06767774373292923,
      "learning_rate": 0.00017608247422680413,
      "loss": 0.3899,
      "step": 411
    },
    {
      "epoch": 0.12117647058823529,
      "grad_norm": 0.059194840490818024,
      "learning_rate": 0.0001760235640648012,
      "loss": 0.3724,
      "step": 412
    },
    {
      "epoch": 0.12147058823529412,
      "grad_norm": 0.06003563106060028,
      "learning_rate": 0.00017596465390279823,
      "loss": 0.4076,
      "step": 413
    },
    {
      "epoch": 0.12176470588235294,
      "grad_norm": 0.04255178943276405,
      "learning_rate": 0.00017590574374079529,
      "loss": 0.3021,
      "step": 414
    },
    {
      "epoch": 0.12205882352941176,
      "grad_norm": 0.07247471064329147,
      "learning_rate": 0.00017584683357879235,
      "loss": 0.3898,
      "step": 415
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 0.05953656882047653,
      "learning_rate": 0.0001757879234167894,
      "loss": 0.3128,
      "step": 416
    },
    {
      "epoch": 0.12264705882352941,
      "grad_norm": 0.08154992759227753,
      "learning_rate": 0.00017572901325478647,
      "loss": 0.4183,
      "step": 417
    },
    {
      "epoch": 0.12294117647058823,
      "grad_norm": 0.05401672422885895,
      "learning_rate": 0.0001756701030927835,
      "loss": 0.3215,
      "step": 418
    },
    {
      "epoch": 0.12323529411764705,
      "grad_norm": 0.05628965422511101,
      "learning_rate": 0.00017561119293078056,
      "loss": 0.3281,
      "step": 419
    },
    {
      "epoch": 0.12352941176470589,
      "grad_norm": 0.09957556426525116,
      "learning_rate": 0.00017555228276877762,
      "loss": 0.3042,
      "step": 420
    },
    {
      "epoch": 0.12382352941176471,
      "grad_norm": 0.07639549672603607,
      "learning_rate": 0.00017549337260677468,
      "loss": 0.4341,
      "step": 421
    },
    {
      "epoch": 0.12411764705882353,
      "grad_norm": 0.05212264135479927,
      "learning_rate": 0.00017543446244477174,
      "loss": 0.316,
      "step": 422
    },
    {
      "epoch": 0.12441176470588235,
      "grad_norm": 0.08201335370540619,
      "learning_rate": 0.00017537555228276877,
      "loss": 0.3606,
      "step": 423
    },
    {
      "epoch": 0.12470588235294118,
      "grad_norm": 0.0841822400689125,
      "learning_rate": 0.00017531664212076583,
      "loss": 0.4169,
      "step": 424
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06840043514966965,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4096,
      "step": 425
    },
    {
      "epoch": 0.12529411764705883,
      "grad_norm": 0.07148951292037964,
      "learning_rate": 0.00017519882179675995,
      "loss": 0.4096,
      "step": 426
    },
    {
      "epoch": 0.12558823529411764,
      "grad_norm": 0.06686822324991226,
      "learning_rate": 0.00017513991163475701,
      "loss": 0.3671,
      "step": 427
    },
    {
      "epoch": 0.12588235294117647,
      "grad_norm": 0.05390598624944687,
      "learning_rate": 0.00017508100147275405,
      "loss": 0.3129,
      "step": 428
    },
    {
      "epoch": 0.1261764705882353,
      "grad_norm": 0.05109141767024994,
      "learning_rate": 0.0001750220913107511,
      "loss": 0.2997,
      "step": 429
    },
    {
      "epoch": 0.1264705882352941,
      "grad_norm": 0.07828373461961746,
      "learning_rate": 0.00017496318114874817,
      "loss": 0.3868,
      "step": 430
    },
    {
      "epoch": 0.12676470588235295,
      "grad_norm": 0.061607904732227325,
      "learning_rate": 0.00017490427098674523,
      "loss": 0.3277,
      "step": 431
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 0.07689444720745087,
      "learning_rate": 0.0001748453608247423,
      "loss": 0.4161,
      "step": 432
    },
    {
      "epoch": 0.12735294117647059,
      "grad_norm": 0.06176171079277992,
      "learning_rate": 0.00017478645066273932,
      "loss": 0.3204,
      "step": 433
    },
    {
      "epoch": 0.12764705882352942,
      "grad_norm": 0.05509386956691742,
      "learning_rate": 0.00017472754050073638,
      "loss": 0.3386,
      "step": 434
    },
    {
      "epoch": 0.12794117647058822,
      "grad_norm": 0.05754280090332031,
      "learning_rate": 0.00017466863033873344,
      "loss": 0.3538,
      "step": 435
    },
    {
      "epoch": 0.12823529411764706,
      "grad_norm": 0.08163037151098251,
      "learning_rate": 0.0001746097201767305,
      "loss": 0.4038,
      "step": 436
    },
    {
      "epoch": 0.1285294117647059,
      "grad_norm": 0.05647306516766548,
      "learning_rate": 0.00017455081001472756,
      "loss": 0.3729,
      "step": 437
    },
    {
      "epoch": 0.1288235294117647,
      "grad_norm": 0.07209846377372742,
      "learning_rate": 0.0001744918998527246,
      "loss": 0.4259,
      "step": 438
    },
    {
      "epoch": 0.12911764705882353,
      "grad_norm": 0.06568286567926407,
      "learning_rate": 0.00017443298969072165,
      "loss": 0.3943,
      "step": 439
    },
    {
      "epoch": 0.12941176470588237,
      "grad_norm": 0.061060767620801926,
      "learning_rate": 0.00017437407952871872,
      "loss": 0.3711,
      "step": 440
    },
    {
      "epoch": 0.12970588235294117,
      "grad_norm": 0.062304235994815826,
      "learning_rate": 0.00017431516936671578,
      "loss": 0.3468,
      "step": 441
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.06511593610048294,
      "learning_rate": 0.00017425625920471284,
      "loss": 0.3523,
      "step": 442
    },
    {
      "epoch": 0.1302941176470588,
      "grad_norm": 0.07485492527484894,
      "learning_rate": 0.00017419734904270987,
      "loss": 0.3686,
      "step": 443
    },
    {
      "epoch": 0.13058823529411764,
      "grad_norm": 0.059787947684526443,
      "learning_rate": 0.00017413843888070693,
      "loss": 0.3702,
      "step": 444
    },
    {
      "epoch": 0.13088235294117648,
      "grad_norm": 0.061845812946558,
      "learning_rate": 0.000174079528718704,
      "loss": 0.3248,
      "step": 445
    },
    {
      "epoch": 0.13117647058823528,
      "grad_norm": 0.073362335562706,
      "learning_rate": 0.00017402061855670105,
      "loss": 0.4019,
      "step": 446
    },
    {
      "epoch": 0.13147058823529412,
      "grad_norm": 0.057457905262708664,
      "learning_rate": 0.0001739617083946981,
      "loss": 0.3218,
      "step": 447
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 0.05499434843659401,
      "learning_rate": 0.00017390279823269514,
      "loss": 0.3147,
      "step": 448
    },
    {
      "epoch": 0.13205882352941176,
      "grad_norm": 0.074238620698452,
      "learning_rate": 0.0001738438880706922,
      "loss": 0.3675,
      "step": 449
    },
    {
      "epoch": 0.1323529411764706,
      "grad_norm": 0.06017538905143738,
      "learning_rate": 0.00017378497790868926,
      "loss": 0.3322,
      "step": 450
    },
    {
      "epoch": 0.13264705882352942,
      "grad_norm": 0.06091383099555969,
      "learning_rate": 0.00017372606774668632,
      "loss": 0.4063,
      "step": 451
    },
    {
      "epoch": 0.13294117647058823,
      "grad_norm": 0.048693880438804626,
      "learning_rate": 0.00017366715758468338,
      "loss": 0.2808,
      "step": 452
    },
    {
      "epoch": 0.13323529411764706,
      "grad_norm": 0.06318904459476471,
      "learning_rate": 0.00017360824742268042,
      "loss": 0.3655,
      "step": 453
    },
    {
      "epoch": 0.13352941176470587,
      "grad_norm": 0.053866591304540634,
      "learning_rate": 0.00017354933726067748,
      "loss": 0.3069,
      "step": 454
    },
    {
      "epoch": 0.1338235294117647,
      "grad_norm": 0.05395478755235672,
      "learning_rate": 0.00017349042709867454,
      "loss": 0.3273,
      "step": 455
    },
    {
      "epoch": 0.13411764705882354,
      "grad_norm": 0.06352989375591278,
      "learning_rate": 0.0001734315169366716,
      "loss": 0.3202,
      "step": 456
    },
    {
      "epoch": 0.13441176470588234,
      "grad_norm": 0.05848497152328491,
      "learning_rate": 0.00017337260677466866,
      "loss": 0.3035,
      "step": 457
    },
    {
      "epoch": 0.13470588235294118,
      "grad_norm": 0.06433206796646118,
      "learning_rate": 0.0001733136966126657,
      "loss": 0.3535,
      "step": 458
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.0535602830350399,
      "learning_rate": 0.00017325478645066275,
      "loss": 0.3542,
      "step": 459
    },
    {
      "epoch": 0.13529411764705881,
      "grad_norm": 0.05265054106712341,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.2825,
      "step": 460
    },
    {
      "epoch": 0.13558823529411765,
      "grad_norm": 0.0714925229549408,
      "learning_rate": 0.00017313696612665687,
      "loss": 0.3816,
      "step": 461
    },
    {
      "epoch": 0.13588235294117648,
      "grad_norm": 0.05201279744505882,
      "learning_rate": 0.00017307805596465393,
      "loss": 0.3201,
      "step": 462
    },
    {
      "epoch": 0.1361764705882353,
      "grad_norm": 0.06183996796607971,
      "learning_rate": 0.00017301914580265096,
      "loss": 0.307,
      "step": 463
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 0.06556003540754318,
      "learning_rate": 0.00017296023564064802,
      "loss": 0.34,
      "step": 464
    },
    {
      "epoch": 0.13676470588235295,
      "grad_norm": 0.05667487904429436,
      "learning_rate": 0.00017290132547864508,
      "loss": 0.3593,
      "step": 465
    },
    {
      "epoch": 0.13705882352941176,
      "grad_norm": 0.07059769332408905,
      "learning_rate": 0.00017284241531664214,
      "loss": 0.4527,
      "step": 466
    },
    {
      "epoch": 0.1373529411764706,
      "grad_norm": 0.05512050911784172,
      "learning_rate": 0.0001727835051546392,
      "loss": 0.3875,
      "step": 467
    },
    {
      "epoch": 0.1376470588235294,
      "grad_norm": 0.05575348809361458,
      "learning_rate": 0.00017272459499263624,
      "loss": 0.3994,
      "step": 468
    },
    {
      "epoch": 0.13794117647058823,
      "grad_norm": 0.07065296918153763,
      "learning_rate": 0.0001726656848306333,
      "loss": 0.3501,
      "step": 469
    },
    {
      "epoch": 0.13823529411764707,
      "grad_norm": 0.056007541716098785,
      "learning_rate": 0.00017260677466863036,
      "loss": 0.3254,
      "step": 470
    },
    {
      "epoch": 0.13852941176470587,
      "grad_norm": 0.04979860037565231,
      "learning_rate": 0.00017254786450662742,
      "loss": 0.3075,
      "step": 471
    },
    {
      "epoch": 0.1388235294117647,
      "grad_norm": 0.07049575448036194,
      "learning_rate": 0.00017248895434462448,
      "loss": 0.3535,
      "step": 472
    },
    {
      "epoch": 0.13911764705882354,
      "grad_norm": 0.057973578572273254,
      "learning_rate": 0.0001724300441826215,
      "loss": 0.2986,
      "step": 473
    },
    {
      "epoch": 0.13941176470588235,
      "grad_norm": 0.06545397639274597,
      "learning_rate": 0.00017237113402061857,
      "loss": 0.3819,
      "step": 474
    },
    {
      "epoch": 0.13970588235294118,
      "grad_norm": 0.056255120784044266,
      "learning_rate": 0.00017231222385861563,
      "loss": 0.3143,
      "step": 475
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.039822451770305634,
      "learning_rate": 0.0001722533136966127,
      "loss": 0.2702,
      "step": 476
    },
    {
      "epoch": 0.14029411764705882,
      "grad_norm": 0.05215808376669884,
      "learning_rate": 0.00017219440353460975,
      "loss": 0.4001,
      "step": 477
    },
    {
      "epoch": 0.14058823529411765,
      "grad_norm": 0.0583135187625885,
      "learning_rate": 0.00017213549337260678,
      "loss": 0.3764,
      "step": 478
    },
    {
      "epoch": 0.14088235294117646,
      "grad_norm": 0.06295765936374664,
      "learning_rate": 0.00017207658321060384,
      "loss": 0.3323,
      "step": 479
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 0.052351173013448715,
      "learning_rate": 0.0001720176730486009,
      "loss": 0.3386,
      "step": 480
    },
    {
      "epoch": 0.14147058823529413,
      "grad_norm": 0.07185954600572586,
      "learning_rate": 0.00017195876288659796,
      "loss": 0.3566,
      "step": 481
    },
    {
      "epoch": 0.14176470588235293,
      "grad_norm": 0.0632806196808815,
      "learning_rate": 0.00017189985272459503,
      "loss": 0.3787,
      "step": 482
    },
    {
      "epoch": 0.14205882352941177,
      "grad_norm": 0.06502800434827805,
      "learning_rate": 0.00017184094256259203,
      "loss": 0.3158,
      "step": 483
    },
    {
      "epoch": 0.1423529411764706,
      "grad_norm": 0.057220734655857086,
      "learning_rate": 0.0001717820324005891,
      "loss": 0.3649,
      "step": 484
    },
    {
      "epoch": 0.1426470588235294,
      "grad_norm": 0.05213073268532753,
      "learning_rate": 0.00017172312223858615,
      "loss": 0.3134,
      "step": 485
    },
    {
      "epoch": 0.14294117647058824,
      "grad_norm": 0.06543286144733429,
      "learning_rate": 0.0001716642120765832,
      "loss": 0.4079,
      "step": 486
    },
    {
      "epoch": 0.14323529411764707,
      "grad_norm": 0.05453117936849594,
      "learning_rate": 0.00017160530191458027,
      "loss": 0.3666,
      "step": 487
    },
    {
      "epoch": 0.14352941176470588,
      "grad_norm": 0.05235879123210907,
      "learning_rate": 0.0001715463917525773,
      "loss": 0.2937,
      "step": 488
    },
    {
      "epoch": 0.1438235294117647,
      "grad_norm": 0.05976506695151329,
      "learning_rate": 0.00017148748159057437,
      "loss": 0.385,
      "step": 489
    },
    {
      "epoch": 0.14411764705882352,
      "grad_norm": 0.07005610316991806,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4162,
      "step": 490
    },
    {
      "epoch": 0.14441176470588235,
      "grad_norm": 0.05024661868810654,
      "learning_rate": 0.00017136966126656849,
      "loss": 0.3276,
      "step": 491
    },
    {
      "epoch": 0.14470588235294118,
      "grad_norm": 0.07257386296987534,
      "learning_rate": 0.00017131075110456555,
      "loss": 0.3949,
      "step": 492
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05447471886873245,
      "learning_rate": 0.00017125184094256258,
      "loss": 0.3068,
      "step": 493
    },
    {
      "epoch": 0.14529411764705882,
      "grad_norm": 0.059542350471019745,
      "learning_rate": 0.00017119293078055964,
      "loss": 0.4006,
      "step": 494
    },
    {
      "epoch": 0.14558823529411766,
      "grad_norm": 0.0608154721558094,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.3795,
      "step": 495
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 0.044919852167367935,
      "learning_rate": 0.00017107511045655376,
      "loss": 0.2894,
      "step": 496
    },
    {
      "epoch": 0.1461764705882353,
      "grad_norm": 0.049503348767757416,
      "learning_rate": 0.00017101620029455082,
      "loss": 0.3292,
      "step": 497
    },
    {
      "epoch": 0.14647058823529413,
      "grad_norm": 0.07854627817869186,
      "learning_rate": 0.00017095729013254785,
      "loss": 0.3632,
      "step": 498
    },
    {
      "epoch": 0.14676470588235294,
      "grad_norm": 0.05168467015028,
      "learning_rate": 0.0001708983799705449,
      "loss": 0.3386,
      "step": 499
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.05691979452967644,
      "learning_rate": 0.00017083946980854197,
      "loss": 0.3579,
      "step": 500
    },
    {
      "epoch": 0.14705882352941177,
      "eval_loss": 0.36545103788375854,
      "eval_runtime": 213.9326,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0863377971750502e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
