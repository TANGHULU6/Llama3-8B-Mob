{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008,
      "grad_norm": 2.1347780227661133,
      "learning_rate": 4e-05,
      "loss": 1.5272,
      "step": 1
    },
    {
      "epoch": 0.0016,
      "grad_norm": 1.9836015701293945,
      "learning_rate": 8e-05,
      "loss": 1.5059,
      "step": 2
    },
    {
      "epoch": 0.0024,
      "grad_norm": 2.0839576721191406,
      "learning_rate": 0.00012,
      "loss": 1.5305,
      "step": 3
    },
    {
      "epoch": 0.0032,
      "grad_norm": 1.9773516654968262,
      "learning_rate": 0.00016,
      "loss": 1.3257,
      "step": 4
    },
    {
      "epoch": 0.004,
      "grad_norm": 1.5968767404556274,
      "learning_rate": 0.0002,
      "loss": 1.0726,
      "step": 5
    },
    {
      "epoch": 0.0048,
      "grad_norm": 2.5706734657287598,
      "learning_rate": 0.00019983935742971887,
      "loss": 0.953,
      "step": 6
    },
    {
      "epoch": 0.0056,
      "grad_norm": 1.5561612844467163,
      "learning_rate": 0.00019967871485943777,
      "loss": 0.9545,
      "step": 7
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.5294229388237,
      "learning_rate": 0.00019951807228915663,
      "loss": 0.7613,
      "step": 8
    },
    {
      "epoch": 0.0072,
      "grad_norm": 0.4604426622390747,
      "learning_rate": 0.00019935742971887552,
      "loss": 0.7275,
      "step": 9
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.28582048416137695,
      "learning_rate": 0.0001991967871485944,
      "loss": 0.6602,
      "step": 10
    },
    {
      "epoch": 0.0088,
      "grad_norm": 0.21004942059516907,
      "learning_rate": 0.00019903614457831325,
      "loss": 0.6515,
      "step": 11
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.22134442627429962,
      "learning_rate": 0.00019887550200803214,
      "loss": 0.6994,
      "step": 12
    },
    {
      "epoch": 0.0104,
      "grad_norm": 0.20244379341602325,
      "learning_rate": 0.000198714859437751,
      "loss": 0.6745,
      "step": 13
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.16944095492362976,
      "learning_rate": 0.0001985542168674699,
      "loss": 0.7192,
      "step": 14
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.16837897896766663,
      "learning_rate": 0.00019839357429718877,
      "loss": 0.7035,
      "step": 15
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.13038621842861176,
      "learning_rate": 0.00019823293172690763,
      "loss": 0.6408,
      "step": 16
    },
    {
      "epoch": 0.0136,
      "grad_norm": 0.14087414741516113,
      "learning_rate": 0.00019807228915662652,
      "loss": 0.7379,
      "step": 17
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.12386209517717361,
      "learning_rate": 0.0001979116465863454,
      "loss": 0.6188,
      "step": 18
    },
    {
      "epoch": 0.0152,
      "grad_norm": 0.12316779792308807,
      "learning_rate": 0.00019775100401606425,
      "loss": 0.6189,
      "step": 19
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.10036855936050415,
      "learning_rate": 0.00019759036144578314,
      "loss": 0.5264,
      "step": 20
    },
    {
      "epoch": 0.0168,
      "grad_norm": 0.12213204056024551,
      "learning_rate": 0.000197429718875502,
      "loss": 0.5988,
      "step": 21
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.11751913279294968,
      "learning_rate": 0.0001972690763052209,
      "loss": 0.6221,
      "step": 22
    },
    {
      "epoch": 0.0184,
      "grad_norm": 0.14861634373664856,
      "learning_rate": 0.00019710843373493977,
      "loss": 0.5886,
      "step": 23
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.11728625744581223,
      "learning_rate": 0.00019694779116465866,
      "loss": 0.5821,
      "step": 24
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.13090768456459045,
      "learning_rate": 0.00019678714859437752,
      "loss": 0.526,
      "step": 25
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.11452450603246689,
      "learning_rate": 0.00019662650602409642,
      "loss": 0.5544,
      "step": 26
    },
    {
      "epoch": 0.0216,
      "grad_norm": 0.10732513666152954,
      "learning_rate": 0.00019646586345381528,
      "loss": 0.5758,
      "step": 27
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.09670563787221909,
      "learning_rate": 0.00019630522088353415,
      "loss": 0.555,
      "step": 28
    },
    {
      "epoch": 0.0232,
      "grad_norm": 0.11975087970495224,
      "learning_rate": 0.000196144578313253,
      "loss": 0.6437,
      "step": 29
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.0895768329501152,
      "learning_rate": 0.0001959839357429719,
      "loss": 0.5843,
      "step": 30
    },
    {
      "epoch": 0.0248,
      "grad_norm": 0.09090537577867508,
      "learning_rate": 0.00019582329317269077,
      "loss": 0.5417,
      "step": 31
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.08725374191999435,
      "learning_rate": 0.00019566265060240966,
      "loss": 0.6009,
      "step": 32
    },
    {
      "epoch": 0.0264,
      "grad_norm": 0.07858794182538986,
      "learning_rate": 0.00019550200803212852,
      "loss": 0.5475,
      "step": 33
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.06744091212749481,
      "learning_rate": 0.00019534136546184742,
      "loss": 0.5692,
      "step": 34
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.05985536426305771,
      "learning_rate": 0.00019518072289156628,
      "loss": 0.4942,
      "step": 35
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.06994383782148361,
      "learning_rate": 0.00019502008032128517,
      "loss": 0.523,
      "step": 36
    },
    {
      "epoch": 0.0296,
      "grad_norm": 0.06843142211437225,
      "learning_rate": 0.000194859437751004,
      "loss": 0.4906,
      "step": 37
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.08211509883403778,
      "learning_rate": 0.0001946987951807229,
      "loss": 0.5686,
      "step": 38
    },
    {
      "epoch": 0.0312,
      "grad_norm": 0.08831407129764557,
      "learning_rate": 0.00019453815261044177,
      "loss": 0.5156,
      "step": 39
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.0778830349445343,
      "learning_rate": 0.00019437751004016066,
      "loss": 0.5215,
      "step": 40
    },
    {
      "epoch": 0.0328,
      "grad_norm": 0.09175307303667068,
      "learning_rate": 0.00019421686746987952,
      "loss": 0.6314,
      "step": 41
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.09310062974691391,
      "learning_rate": 0.00019405622489959842,
      "loss": 0.508,
      "step": 42
    },
    {
      "epoch": 0.0344,
      "grad_norm": 0.09307271242141724,
      "learning_rate": 0.00019389558232931728,
      "loss": 0.5746,
      "step": 43
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.10029983520507812,
      "learning_rate": 0.00019373493975903617,
      "loss": 0.5312,
      "step": 44
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.07840858399868011,
      "learning_rate": 0.00019357429718875504,
      "loss": 0.5715,
      "step": 45
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.07684183120727539,
      "learning_rate": 0.0001934136546184739,
      "loss": 0.5127,
      "step": 46
    },
    {
      "epoch": 0.0376,
      "grad_norm": 0.0633784607052803,
      "learning_rate": 0.00019325301204819277,
      "loss": 0.4791,
      "step": 47
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.0775885358452797,
      "learning_rate": 0.00019309236947791166,
      "loss": 0.5743,
      "step": 48
    },
    {
      "epoch": 0.0392,
      "grad_norm": 0.07309908419847488,
      "learning_rate": 0.00019293172690763052,
      "loss": 0.5298,
      "step": 49
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.060990288853645325,
      "learning_rate": 0.00019277108433734942,
      "loss": 0.5145,
      "step": 50
    },
    {
      "epoch": 0.0408,
      "grad_norm": 0.06224774196743965,
      "learning_rate": 0.00019261044176706828,
      "loss": 0.5266,
      "step": 51
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.06949962675571442,
      "learning_rate": 0.00019244979919678717,
      "loss": 0.495,
      "step": 52
    },
    {
      "epoch": 0.0424,
      "grad_norm": 0.07793927192687988,
      "learning_rate": 0.00019228915662650604,
      "loss": 0.4946,
      "step": 53
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.06882878392934799,
      "learning_rate": 0.00019212851405622493,
      "loss": 0.4983,
      "step": 54
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.0680503249168396,
      "learning_rate": 0.00019196787148594377,
      "loss": 0.5327,
      "step": 55
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.09238329529762268,
      "learning_rate": 0.00019180722891566266,
      "loss": 0.4929,
      "step": 56
    },
    {
      "epoch": 0.0456,
      "grad_norm": 0.05784634128212929,
      "learning_rate": 0.00019164658634538152,
      "loss": 0.4854,
      "step": 57
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.06709297001361847,
      "learning_rate": 0.00019148594377510042,
      "loss": 0.5238,
      "step": 58
    },
    {
      "epoch": 0.0472,
      "grad_norm": 0.05830276384949684,
      "learning_rate": 0.00019132530120481928,
      "loss": 0.49,
      "step": 59
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.08362855762243271,
      "learning_rate": 0.00019116465863453817,
      "loss": 0.4854,
      "step": 60
    },
    {
      "epoch": 0.0488,
      "grad_norm": 0.07083585858345032,
      "learning_rate": 0.00019100401606425704,
      "loss": 0.5047,
      "step": 61
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.057147182524204254,
      "learning_rate": 0.00019084337349397593,
      "loss": 0.5124,
      "step": 62
    },
    {
      "epoch": 0.0504,
      "grad_norm": 0.06513812392950058,
      "learning_rate": 0.0001906827309236948,
      "loss": 0.4895,
      "step": 63
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.0823916643857956,
      "learning_rate": 0.00019052208835341369,
      "loss": 0.4985,
      "step": 64
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.07407432794570923,
      "learning_rate": 0.00019036144578313252,
      "loss": 0.5398,
      "step": 65
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.07363937795162201,
      "learning_rate": 0.00019020080321285142,
      "loss": 0.4899,
      "step": 66
    },
    {
      "epoch": 0.0536,
      "grad_norm": 0.109267957508564,
      "learning_rate": 0.00019004016064257028,
      "loss": 0.6309,
      "step": 67
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.06288694590330124,
      "learning_rate": 0.00018987951807228917,
      "loss": 0.5411,
      "step": 68
    },
    {
      "epoch": 0.0552,
      "grad_norm": 0.08857997506856918,
      "learning_rate": 0.00018971887550200804,
      "loss": 0.4656,
      "step": 69
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.08155405521392822,
      "learning_rate": 0.00018955823293172693,
      "loss": 0.5531,
      "step": 70
    },
    {
      "epoch": 0.0568,
      "grad_norm": 0.05838432163000107,
      "learning_rate": 0.0001893975903614458,
      "loss": 0.4383,
      "step": 71
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.07726778835058212,
      "learning_rate": 0.00018923694779116469,
      "loss": 0.547,
      "step": 72
    },
    {
      "epoch": 0.0584,
      "grad_norm": 0.10900668799877167,
      "learning_rate": 0.00018907630522088355,
      "loss": 0.5797,
      "step": 73
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.05461811274290085,
      "learning_rate": 0.00018891566265060242,
      "loss": 0.5346,
      "step": 74
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.0707673728466034,
      "learning_rate": 0.00018875502008032128,
      "loss": 0.502,
      "step": 75
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.07554429024457932,
      "learning_rate": 0.00018859437751004017,
      "loss": 0.4876,
      "step": 76
    },
    {
      "epoch": 0.0616,
      "grad_norm": 0.06960006058216095,
      "learning_rate": 0.00018843373493975904,
      "loss": 0.5321,
      "step": 77
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.06240013241767883,
      "learning_rate": 0.00018827309236947793,
      "loss": 0.4814,
      "step": 78
    },
    {
      "epoch": 0.0632,
      "grad_norm": 0.0629945695400238,
      "learning_rate": 0.0001881124497991968,
      "loss": 0.5214,
      "step": 79
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.08035027980804443,
      "learning_rate": 0.00018795180722891569,
      "loss": 0.4964,
      "step": 80
    },
    {
      "epoch": 0.0648,
      "grad_norm": 0.07858648896217346,
      "learning_rate": 0.00018779116465863455,
      "loss": 0.5251,
      "step": 81
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.06235964596271515,
      "learning_rate": 0.00018763052208835344,
      "loss": 0.5278,
      "step": 82
    },
    {
      "epoch": 0.0664,
      "grad_norm": 0.06611556559801102,
      "learning_rate": 0.00018746987951807228,
      "loss": 0.4986,
      "step": 83
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.0752670094370842,
      "learning_rate": 0.00018730923694779117,
      "loss": 0.4702,
      "step": 84
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.08786909282207489,
      "learning_rate": 0.00018714859437751004,
      "loss": 0.5823,
      "step": 85
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.05722428858280182,
      "learning_rate": 0.00018698795180722893,
      "loss": 0.5271,
      "step": 86
    },
    {
      "epoch": 0.0696,
      "grad_norm": 0.05981328338384628,
      "learning_rate": 0.0001868273092369478,
      "loss": 0.4445,
      "step": 87
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.08743041008710861,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.5454,
      "step": 88
    },
    {
      "epoch": 0.0712,
      "grad_norm": 0.0496487058699131,
      "learning_rate": 0.00018650602409638555,
      "loss": 0.4552,
      "step": 89
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.08568675816059113,
      "learning_rate": 0.00018634538152610444,
      "loss": 0.5691,
      "step": 90
    },
    {
      "epoch": 0.0728,
      "grad_norm": 0.08056145906448364,
      "learning_rate": 0.0001861847389558233,
      "loss": 0.5273,
      "step": 91
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.07053664326667786,
      "learning_rate": 0.00018602409638554217,
      "loss": 0.4388,
      "step": 92
    },
    {
      "epoch": 0.0744,
      "grad_norm": 0.07541362196207047,
      "learning_rate": 0.00018586345381526104,
      "loss": 0.5415,
      "step": 93
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.06503012031316757,
      "learning_rate": 0.00018570281124497993,
      "loss": 0.4673,
      "step": 94
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.0771484225988388,
      "learning_rate": 0.0001855421686746988,
      "loss": 0.528,
      "step": 95
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.056726906448602676,
      "learning_rate": 0.0001853815261044177,
      "loss": 0.4041,
      "step": 96
    },
    {
      "epoch": 0.0776,
      "grad_norm": 0.0685051754117012,
      "learning_rate": 0.00018522088353413655,
      "loss": 0.4857,
      "step": 97
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.07529725879430771,
      "learning_rate": 0.00018506024096385544,
      "loss": 0.4908,
      "step": 98
    },
    {
      "epoch": 0.0792,
      "grad_norm": 0.05291372910141945,
      "learning_rate": 0.0001848995983935743,
      "loss": 0.4525,
      "step": 99
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.06566192954778671,
      "learning_rate": 0.0001847389558232932,
      "loss": 0.5108,
      "step": 100
    },
    {
      "epoch": 0.0808,
      "grad_norm": 0.07882264256477356,
      "learning_rate": 0.00018457831325301204,
      "loss": 0.5335,
      "step": 101
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.06889068335294724,
      "learning_rate": 0.00018441767068273093,
      "loss": 0.552,
      "step": 102
    },
    {
      "epoch": 0.0824,
      "grad_norm": 0.059857551008462906,
      "learning_rate": 0.0001842570281124498,
      "loss": 0.4817,
      "step": 103
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.05914648249745369,
      "learning_rate": 0.0001840963855421687,
      "loss": 0.5013,
      "step": 104
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.06123293936252594,
      "learning_rate": 0.00018393574297188755,
      "loss": 0.5052,
      "step": 105
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.0671229436993599,
      "learning_rate": 0.00018377510040160644,
      "loss": 0.4545,
      "step": 106
    },
    {
      "epoch": 0.0856,
      "grad_norm": 0.069180928170681,
      "learning_rate": 0.0001836144578313253,
      "loss": 0.6011,
      "step": 107
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.06726595014333725,
      "learning_rate": 0.0001834538152610442,
      "loss": 0.5384,
      "step": 108
    },
    {
      "epoch": 0.0872,
      "grad_norm": 0.0809977799654007,
      "learning_rate": 0.00018329317269076307,
      "loss": 0.4666,
      "step": 109
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.059094980359077454,
      "learning_rate": 0.00018313253012048193,
      "loss": 0.4406,
      "step": 110
    },
    {
      "epoch": 0.0888,
      "grad_norm": 0.05279257521033287,
      "learning_rate": 0.0001829718875502008,
      "loss": 0.4524,
      "step": 111
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.07719718664884567,
      "learning_rate": 0.0001828112449799197,
      "loss": 0.4582,
      "step": 112
    },
    {
      "epoch": 0.0904,
      "grad_norm": 0.06799697875976562,
      "learning_rate": 0.00018265060240963855,
      "loss": 0.5249,
      "step": 113
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.0698668584227562,
      "learning_rate": 0.00018248995983935744,
      "loss": 0.6056,
      "step": 114
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.05933667719364166,
      "learning_rate": 0.0001823293172690763,
      "loss": 0.5493,
      "step": 115
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.06582602113485336,
      "learning_rate": 0.0001821686746987952,
      "loss": 0.5132,
      "step": 116
    },
    {
      "epoch": 0.0936,
      "grad_norm": 0.07358702272176743,
      "learning_rate": 0.00018200803212851407,
      "loss": 0.4927,
      "step": 117
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.0671166181564331,
      "learning_rate": 0.00018184738955823296,
      "loss": 0.5074,
      "step": 118
    },
    {
      "epoch": 0.0952,
      "grad_norm": 0.05688825994729996,
      "learning_rate": 0.0001816867469879518,
      "loss": 0.472,
      "step": 119
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.07915422320365906,
      "learning_rate": 0.0001815261044176707,
      "loss": 0.4897,
      "step": 120
    },
    {
      "epoch": 0.0968,
      "grad_norm": 0.06279944628477097,
      "learning_rate": 0.00018136546184738955,
      "loss": 0.4908,
      "step": 121
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.06097010523080826,
      "learning_rate": 0.00018120481927710844,
      "loss": 0.5108,
      "step": 122
    },
    {
      "epoch": 0.0984,
      "grad_norm": 0.05787007510662079,
      "learning_rate": 0.0001810441767068273,
      "loss": 0.4711,
      "step": 123
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.12259437888860703,
      "learning_rate": 0.0001808835341365462,
      "loss": 0.5336,
      "step": 124
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.05863196402788162,
      "learning_rate": 0.00018072289156626507,
      "loss": 0.4876,
      "step": 125
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.059456717222929,
      "learning_rate": 0.00018056224899598396,
      "loss": 0.4697,
      "step": 126
    },
    {
      "epoch": 0.1016,
      "grad_norm": 0.12795314192771912,
      "learning_rate": 0.00018040160642570282,
      "loss": 0.5625,
      "step": 127
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.07247341424226761,
      "learning_rate": 0.0001802409638554217,
      "loss": 0.5489,
      "step": 128
    },
    {
      "epoch": 0.1032,
      "grad_norm": 0.0710255354642868,
      "learning_rate": 0.00018008032128514055,
      "loss": 0.5124,
      "step": 129
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.0826912596821785,
      "learning_rate": 0.00017991967871485944,
      "loss": 0.4902,
      "step": 130
    },
    {
      "epoch": 0.1048,
      "grad_norm": 0.08676572889089584,
      "learning_rate": 0.0001797590361445783,
      "loss": 0.4983,
      "step": 131
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.05807721987366676,
      "learning_rate": 0.0001795983935742972,
      "loss": 0.4908,
      "step": 132
    },
    {
      "epoch": 0.1064,
      "grad_norm": 0.06632667034864426,
      "learning_rate": 0.00017943775100401607,
      "loss": 0.4524,
      "step": 133
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.10262958705425262,
      "learning_rate": 0.00017927710843373496,
      "loss": 0.5519,
      "step": 134
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.06370515376329422,
      "learning_rate": 0.00017911646586345382,
      "loss": 0.5032,
      "step": 135
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.06694759428501129,
      "learning_rate": 0.00017895582329317271,
      "loss": 0.4669,
      "step": 136
    },
    {
      "epoch": 0.1096,
      "grad_norm": 0.07470206171274185,
      "learning_rate": 0.00017879518072289155,
      "loss": 0.5165,
      "step": 137
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.07905607670545578,
      "learning_rate": 0.00017863453815261044,
      "loss": 0.5423,
      "step": 138
    },
    {
      "epoch": 0.1112,
      "grad_norm": 0.05962207913398743,
      "learning_rate": 0.0001784738955823293,
      "loss": 0.4675,
      "step": 139
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.06447664648294449,
      "learning_rate": 0.0001783132530120482,
      "loss": 0.5003,
      "step": 140
    },
    {
      "epoch": 0.1128,
      "grad_norm": 0.05551251396536827,
      "learning_rate": 0.00017815261044176707,
      "loss": 0.4453,
      "step": 141
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.06052719056606293,
      "learning_rate": 0.00017799196787148596,
      "loss": 0.4511,
      "step": 142
    },
    {
      "epoch": 0.1144,
      "grad_norm": 0.06985758990049362,
      "learning_rate": 0.00017783132530120482,
      "loss": 0.4835,
      "step": 143
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.05087987333536148,
      "learning_rate": 0.00017767068273092371,
      "loss": 0.4485,
      "step": 144
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.056277789175510406,
      "learning_rate": 0.00017751004016064258,
      "loss": 0.4404,
      "step": 145
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.06233266741037369,
      "learning_rate": 0.00017734939759036144,
      "loss": 0.5014,
      "step": 146
    },
    {
      "epoch": 0.1176,
      "grad_norm": 0.0675848051905632,
      "learning_rate": 0.0001771887550200803,
      "loss": 0.5744,
      "step": 147
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.06425812095403671,
      "learning_rate": 0.0001770281124497992,
      "loss": 0.5287,
      "step": 148
    },
    {
      "epoch": 0.1192,
      "grad_norm": 0.06053530052304268,
      "learning_rate": 0.00017686746987951807,
      "loss": 0.4442,
      "step": 149
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.05577150732278824,
      "learning_rate": 0.00017670682730923696,
      "loss": 0.5191,
      "step": 150
    },
    {
      "epoch": 0.1208,
      "grad_norm": 0.05600070208311081,
      "learning_rate": 0.00017654618473895582,
      "loss": 0.4092,
      "step": 151
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.07803887873888016,
      "learning_rate": 0.00017638554216867471,
      "loss": 0.5205,
      "step": 152
    },
    {
      "epoch": 0.1224,
      "grad_norm": 0.05819672718644142,
      "learning_rate": 0.00017622489959839358,
      "loss": 0.5109,
      "step": 153
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.05448455736041069,
      "learning_rate": 0.00017606425702811247,
      "loss": 0.4064,
      "step": 154
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.05949678644537926,
      "learning_rate": 0.00017590361445783134,
      "loss": 0.5237,
      "step": 155
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.06190333142876625,
      "learning_rate": 0.0001757429718875502,
      "loss": 0.4866,
      "step": 156
    },
    {
      "epoch": 0.1256,
      "grad_norm": 0.054005686193704605,
      "learning_rate": 0.00017558232931726907,
      "loss": 0.4658,
      "step": 157
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.06831654161214828,
      "learning_rate": 0.00017542168674698796,
      "loss": 0.5746,
      "step": 158
    },
    {
      "epoch": 0.1272,
      "grad_norm": 0.05544079467654228,
      "learning_rate": 0.00017526104417670682,
      "loss": 0.4815,
      "step": 159
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.0802280530333519,
      "learning_rate": 0.00017510040160642571,
      "loss": 0.4847,
      "step": 160
    },
    {
      "epoch": 0.1288,
      "grad_norm": 0.08610692620277405,
      "learning_rate": 0.00017493975903614458,
      "loss": 0.5618,
      "step": 161
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.05789168179035187,
      "learning_rate": 0.00017477911646586347,
      "loss": 0.5108,
      "step": 162
    },
    {
      "epoch": 0.1304,
      "grad_norm": 0.08183213323354721,
      "learning_rate": 0.00017461847389558234,
      "loss": 0.4753,
      "step": 163
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.053463805466890335,
      "learning_rate": 0.00017445783132530123,
      "loss": 0.4347,
      "step": 164
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.06504719704389572,
      "learning_rate": 0.0001742971887550201,
      "loss": 0.6031,
      "step": 165
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.04906444996595383,
      "learning_rate": 0.00017413654618473896,
      "loss": 0.4883,
      "step": 166
    },
    {
      "epoch": 0.1336,
      "grad_norm": 0.06892620772123337,
      "learning_rate": 0.00017397590361445782,
      "loss": 0.4805,
      "step": 167
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.0623418465256691,
      "learning_rate": 0.00017381526104417671,
      "loss": 0.5658,
      "step": 168
    },
    {
      "epoch": 0.1352,
      "grad_norm": 0.05464158207178116,
      "learning_rate": 0.00017365461847389558,
      "loss": 0.4665,
      "step": 169
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.05788956582546234,
      "learning_rate": 0.00017349397590361447,
      "loss": 0.4541,
      "step": 170
    },
    {
      "epoch": 0.1368,
      "grad_norm": 0.05464113503694534,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4863,
      "step": 171
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.0669345110654831,
      "learning_rate": 0.00017317269076305223,
      "loss": 0.5045,
      "step": 172
    },
    {
      "epoch": 0.1384,
      "grad_norm": 0.07338683307170868,
      "learning_rate": 0.0001730120481927711,
      "loss": 0.4812,
      "step": 173
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.05940936505794525,
      "learning_rate": 0.00017285140562248996,
      "loss": 0.4962,
      "step": 174
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.053209222853183746,
      "learning_rate": 0.00017269076305220885,
      "loss": 0.5347,
      "step": 175
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.06280893087387085,
      "learning_rate": 0.00017253012048192771,
      "loss": 0.5699,
      "step": 176
    },
    {
      "epoch": 0.1416,
      "grad_norm": 0.0716436430811882,
      "learning_rate": 0.00017236947791164658,
      "loss": 0.5514,
      "step": 177
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.056861553341150284,
      "learning_rate": 0.00017220883534136547,
      "loss": 0.4601,
      "step": 178
    },
    {
      "epoch": 0.1432,
      "grad_norm": 0.056940555572509766,
      "learning_rate": 0.00017204819277108434,
      "loss": 0.5264,
      "step": 179
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.06461384892463684,
      "learning_rate": 0.00017188755020080323,
      "loss": 0.5388,
      "step": 180
    },
    {
      "epoch": 0.1448,
      "grad_norm": 0.0695658028125763,
      "learning_rate": 0.0001717269076305221,
      "loss": 0.4784,
      "step": 181
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.06939040124416351,
      "learning_rate": 0.00017156626506024099,
      "loss": 0.5195,
      "step": 182
    },
    {
      "epoch": 0.1464,
      "grad_norm": 0.04495857283473015,
      "learning_rate": 0.00017140562248995985,
      "loss": 0.4165,
      "step": 183
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.06293704360723495,
      "learning_rate": 0.00017124497991967871,
      "loss": 0.493,
      "step": 184
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.07269351184368134,
      "learning_rate": 0.0001710843373493976,
      "loss": 0.5137,
      "step": 185
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.06422314792871475,
      "learning_rate": 0.00017092369477911647,
      "loss": 0.5549,
      "step": 186
    },
    {
      "epoch": 0.1496,
      "grad_norm": 0.04643041267991066,
      "learning_rate": 0.00017076305220883536,
      "loss": 0.4773,
      "step": 187
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.058588139712810516,
      "learning_rate": 0.00017060240963855423,
      "loss": 0.4873,
      "step": 188
    },
    {
      "epoch": 0.1512,
      "grad_norm": 0.06310485303401947,
      "learning_rate": 0.0001704417670682731,
      "loss": 0.5011,
      "step": 189
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.05779886618256569,
      "learning_rate": 0.00017028112449799199,
      "loss": 0.4992,
      "step": 190
    },
    {
      "epoch": 0.1528,
      "grad_norm": 0.06585408747196198,
      "learning_rate": 0.00017012048192771085,
      "loss": 0.5186,
      "step": 191
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.05036412551999092,
      "learning_rate": 0.00016995983935742971,
      "loss": 0.4261,
      "step": 192
    },
    {
      "epoch": 0.1544,
      "grad_norm": 0.053989049047231674,
      "learning_rate": 0.0001697991967871486,
      "loss": 0.5042,
      "step": 193
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.063383087515831,
      "learning_rate": 0.00016963855421686747,
      "loss": 0.5123,
      "step": 194
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.06761002540588379,
      "learning_rate": 0.00016947791164658636,
      "loss": 0.4865,
      "step": 195
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.060508791357278824,
      "learning_rate": 0.00016931726907630523,
      "loss": 0.5375,
      "step": 196
    },
    {
      "epoch": 0.1576,
      "grad_norm": 0.05776913836598396,
      "learning_rate": 0.00016915662650602412,
      "loss": 0.5196,
      "step": 197
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.04957139864563942,
      "learning_rate": 0.00016899598393574299,
      "loss": 0.4434,
      "step": 198
    },
    {
      "epoch": 0.1592,
      "grad_norm": 0.07992851734161377,
      "learning_rate": 0.00016883534136546185,
      "loss": 0.5721,
      "step": 199
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.0448601096868515,
      "learning_rate": 0.00016867469879518074,
      "loss": 0.471,
      "step": 200
    },
    {
      "epoch": 0.1608,
      "grad_norm": 0.045748692005872726,
      "learning_rate": 0.0001685140562248996,
      "loss": 0.4156,
      "step": 201
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.0480528324842453,
      "learning_rate": 0.00016835341365461847,
      "loss": 0.41,
      "step": 202
    },
    {
      "epoch": 0.1624,
      "grad_norm": 0.05730884149670601,
      "learning_rate": 0.00016819277108433736,
      "loss": 0.5258,
      "step": 203
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.053701385855674744,
      "learning_rate": 0.00016803212851405623,
      "loss": 0.4771,
      "step": 204
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.05747315660119057,
      "learning_rate": 0.00016787148594377512,
      "loss": 0.4931,
      "step": 205
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.04820830002427101,
      "learning_rate": 0.00016771084337349399,
      "loss": 0.4223,
      "step": 206
    },
    {
      "epoch": 0.1656,
      "grad_norm": 0.06655223667621613,
      "learning_rate": 0.00016755020080321288,
      "loss": 0.5291,
      "step": 207
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.06322818994522095,
      "learning_rate": 0.00016738955823293174,
      "loss": 0.4875,
      "step": 208
    },
    {
      "epoch": 0.1672,
      "grad_norm": 0.06005417928099632,
      "learning_rate": 0.0001672289156626506,
      "loss": 0.5317,
      "step": 209
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.053677525371313095,
      "learning_rate": 0.00016706827309236947,
      "loss": 0.4322,
      "step": 210
    },
    {
      "epoch": 0.1688,
      "grad_norm": 0.07085507363080978,
      "learning_rate": 0.00016690763052208836,
      "loss": 0.5027,
      "step": 211
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.060437288135290146,
      "learning_rate": 0.00016674698795180723,
      "loss": 0.4239,
      "step": 212
    },
    {
      "epoch": 0.1704,
      "grad_norm": 0.06214321032166481,
      "learning_rate": 0.00016658634538152612,
      "loss": 0.4959,
      "step": 213
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.07835772633552551,
      "learning_rate": 0.00016642570281124499,
      "loss": 0.4863,
      "step": 214
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.06811828911304474,
      "learning_rate": 0.00016626506024096388,
      "loss": 0.5317,
      "step": 215
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.050230637192726135,
      "learning_rate": 0.00016610441767068274,
      "loss": 0.4439,
      "step": 216
    },
    {
      "epoch": 0.1736,
      "grad_norm": 0.06684186309576035,
      "learning_rate": 0.00016594377510040163,
      "loss": 0.5367,
      "step": 217
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.06147449463605881,
      "learning_rate": 0.0001657831325301205,
      "loss": 0.4869,
      "step": 218
    },
    {
      "epoch": 0.1752,
      "grad_norm": 0.05261843651533127,
      "learning_rate": 0.00016562248995983936,
      "loss": 0.4485,
      "step": 219
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.055768344551324844,
      "learning_rate": 0.00016546184738955823,
      "loss": 0.4728,
      "step": 220
    },
    {
      "epoch": 0.1768,
      "grad_norm": 0.06616415083408356,
      "learning_rate": 0.00016530120481927712,
      "loss": 0.4515,
      "step": 221
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.06046942248940468,
      "learning_rate": 0.00016514056224899599,
      "loss": 0.4765,
      "step": 222
    },
    {
      "epoch": 0.1784,
      "grad_norm": 0.05828780308365822,
      "learning_rate": 0.00016497991967871488,
      "loss": 0.5043,
      "step": 223
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.06070037558674812,
      "learning_rate": 0.00016481927710843374,
      "loss": 0.5022,
      "step": 224
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.057593390345573425,
      "learning_rate": 0.00016465863453815263,
      "loss": 0.5826,
      "step": 225
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.04914085566997528,
      "learning_rate": 0.0001644979919678715,
      "loss": 0.4892,
      "step": 226
    },
    {
      "epoch": 0.1816,
      "grad_norm": 0.05689353123307228,
      "learning_rate": 0.0001643373493975904,
      "loss": 0.5021,
      "step": 227
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.05147885903716087,
      "learning_rate": 0.00016417670682730923,
      "loss": 0.4947,
      "step": 228
    },
    {
      "epoch": 0.1832,
      "grad_norm": 0.049064844846725464,
      "learning_rate": 0.00016401606425702812,
      "loss": 0.4674,
      "step": 229
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.0596543550491333,
      "learning_rate": 0.00016385542168674699,
      "loss": 0.5439,
      "step": 230
    },
    {
      "epoch": 0.1848,
      "grad_norm": 0.055820390582084656,
      "learning_rate": 0.00016369477911646588,
      "loss": 0.5667,
      "step": 231
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.052124664187431335,
      "learning_rate": 0.00016353413654618474,
      "loss": 0.5325,
      "step": 232
    },
    {
      "epoch": 0.1864,
      "grad_norm": 0.05359750986099243,
      "learning_rate": 0.00016337349397590363,
      "loss": 0.4445,
      "step": 233
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.06003478541970253,
      "learning_rate": 0.0001632128514056225,
      "loss": 0.5402,
      "step": 234
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.06366219371557236,
      "learning_rate": 0.0001630522088353414,
      "loss": 0.5638,
      "step": 235
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.0604616142809391,
      "learning_rate": 0.00016289156626506026,
      "loss": 0.5495,
      "step": 236
    },
    {
      "epoch": 0.1896,
      "grad_norm": 0.058994196355342865,
      "learning_rate": 0.00016273092369477912,
      "loss": 0.569,
      "step": 237
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.055926743894815445,
      "learning_rate": 0.00016257028112449799,
      "loss": 0.486,
      "step": 238
    },
    {
      "epoch": 0.1912,
      "grad_norm": 0.05703776329755783,
      "learning_rate": 0.00016240963855421688,
      "loss": 0.5152,
      "step": 239
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.0579754076898098,
      "learning_rate": 0.00016224899598393574,
      "loss": 0.4797,
      "step": 240
    },
    {
      "epoch": 0.1928,
      "grad_norm": 0.052077993750572205,
      "learning_rate": 0.00016208835341365463,
      "loss": 0.4702,
      "step": 241
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.05391334369778633,
      "learning_rate": 0.0001619277108433735,
      "loss": 0.4721,
      "step": 242
    },
    {
      "epoch": 0.1944,
      "grad_norm": 0.05541302263736725,
      "learning_rate": 0.0001617670682730924,
      "loss": 0.4723,
      "step": 243
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.06302434206008911,
      "learning_rate": 0.00016160642570281126,
      "loss": 0.553,
      "step": 244
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.060641251504421234,
      "learning_rate": 0.00016144578313253015,
      "loss": 0.5916,
      "step": 245
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.05734767019748688,
      "learning_rate": 0.00016128514056224899,
      "loss": 0.4518,
      "step": 246
    },
    {
      "epoch": 0.1976,
      "grad_norm": 0.05207259953022003,
      "learning_rate": 0.00016112449799196788,
      "loss": 0.4732,
      "step": 247
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.06917824596166611,
      "learning_rate": 0.00016096385542168674,
      "loss": 0.5048,
      "step": 248
    },
    {
      "epoch": 0.1992,
      "grad_norm": 0.053998906165361404,
      "learning_rate": 0.00016080321285140563,
      "loss": 0.4686,
      "step": 249
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.08358516544103622,
      "learning_rate": 0.0001606425702811245,
      "loss": 0.6262,
      "step": 250
    },
    {
      "epoch": 0.2008,
      "grad_norm": 0.052449677139520645,
      "learning_rate": 0.0001604819277108434,
      "loss": 0.4631,
      "step": 251
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.05791386589407921,
      "learning_rate": 0.00016032128514056226,
      "loss": 0.4828,
      "step": 252
    },
    {
      "epoch": 0.2024,
      "grad_norm": 0.06077329441905022,
      "learning_rate": 0.00016016064257028115,
      "loss": 0.5648,
      "step": 253
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.06597800552845001,
      "learning_rate": 0.00016,
      "loss": 0.4782,
      "step": 254
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.0506383441388607,
      "learning_rate": 0.00015983935742971888,
      "loss": 0.4613,
      "step": 255
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.05768336355686188,
      "learning_rate": 0.00015967871485943774,
      "loss": 0.5358,
      "step": 256
    },
    {
      "epoch": 0.2056,
      "grad_norm": 0.06127495318651199,
      "learning_rate": 0.00015951807228915663,
      "loss": 0.5254,
      "step": 257
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.06919392943382263,
      "learning_rate": 0.0001593574297188755,
      "loss": 0.4814,
      "step": 258
    },
    {
      "epoch": 0.2072,
      "grad_norm": 0.06447075307369232,
      "learning_rate": 0.0001591967871485944,
      "loss": 0.5174,
      "step": 259
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.06820085644721985,
      "learning_rate": 0.00015903614457831326,
      "loss": 0.4889,
      "step": 260
    },
    {
      "epoch": 0.2088,
      "grad_norm": 0.068202905356884,
      "learning_rate": 0.00015887550200803215,
      "loss": 0.4834,
      "step": 261
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.061375413089990616,
      "learning_rate": 0.000158714859437751,
      "loss": 0.4898,
      "step": 262
    },
    {
      "epoch": 0.2104,
      "grad_norm": 0.05234726518392563,
      "learning_rate": 0.0001585542168674699,
      "loss": 0.5012,
      "step": 263
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.06541641056537628,
      "learning_rate": 0.00015839357429718874,
      "loss": 0.4926,
      "step": 264
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.06164320558309555,
      "learning_rate": 0.00015823293172690763,
      "loss": 0.541,
      "step": 265
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.06678148359060287,
      "learning_rate": 0.0001580722891566265,
      "loss": 0.5073,
      "step": 266
    },
    {
      "epoch": 0.2136,
      "grad_norm": 0.07577947527170181,
      "learning_rate": 0.0001579116465863454,
      "loss": 0.5857,
      "step": 267
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.0631055235862732,
      "learning_rate": 0.00015775100401606426,
      "loss": 0.4796,
      "step": 268
    },
    {
      "epoch": 0.2152,
      "grad_norm": 0.06542311608791351,
      "learning_rate": 0.00015759036144578315,
      "loss": 0.536,
      "step": 269
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.05937357619404793,
      "learning_rate": 0.000157429718875502,
      "loss": 0.5704,
      "step": 270
    },
    {
      "epoch": 0.2168,
      "grad_norm": 0.06638732552528381,
      "learning_rate": 0.0001572690763052209,
      "loss": 0.5395,
      "step": 271
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.06162670999765396,
      "learning_rate": 0.00015710843373493977,
      "loss": 0.529,
      "step": 272
    },
    {
      "epoch": 0.2184,
      "grad_norm": 0.046842657029628754,
      "learning_rate": 0.00015694779116465866,
      "loss": 0.4418,
      "step": 273
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.05582407861948013,
      "learning_rate": 0.0001567871485943775,
      "loss": 0.4448,
      "step": 274
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07717180252075195,
      "learning_rate": 0.0001566265060240964,
      "loss": 0.5182,
      "step": 275
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.06025996804237366,
      "learning_rate": 0.00015646586345381526,
      "loss": 0.5133,
      "step": 276
    },
    {
      "epoch": 0.2216,
      "grad_norm": 0.05558518320322037,
      "learning_rate": 0.00015630522088353415,
      "loss": 0.4605,
      "step": 277
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.05458090081810951,
      "learning_rate": 0.000156144578313253,
      "loss": 0.4406,
      "step": 278
    },
    {
      "epoch": 0.2232,
      "grad_norm": 0.059976957738399506,
      "learning_rate": 0.0001559839357429719,
      "loss": 0.5075,
      "step": 279
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.06781185418367386,
      "learning_rate": 0.00015582329317269077,
      "loss": 0.4323,
      "step": 280
    },
    {
      "epoch": 0.2248,
      "grad_norm": 0.057508062571287155,
      "learning_rate": 0.00015566265060240966,
      "loss": 0.5067,
      "step": 281
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.05725878104567528,
      "learning_rate": 0.00015550200803212853,
      "loss": 0.5342,
      "step": 282
    },
    {
      "epoch": 0.2264,
      "grad_norm": 0.049051638692617416,
      "learning_rate": 0.0001553413654618474,
      "loss": 0.471,
      "step": 283
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.05667721852660179,
      "learning_rate": 0.00015518072289156626,
      "loss": 0.4722,
      "step": 284
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.050575509667396545,
      "learning_rate": 0.00015502008032128515,
      "loss": 0.4919,
      "step": 285
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.07044538855552673,
      "learning_rate": 0.000154859437751004,
      "loss": 0.5367,
      "step": 286
    },
    {
      "epoch": 0.2296,
      "grad_norm": 0.056515179574489594,
      "learning_rate": 0.0001546987951807229,
      "loss": 0.5248,
      "step": 287
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.07039755582809448,
      "learning_rate": 0.00015453815261044177,
      "loss": 0.5601,
      "step": 288
    },
    {
      "epoch": 0.2312,
      "grad_norm": 0.05840161070227623,
      "learning_rate": 0.00015437751004016066,
      "loss": 0.4817,
      "step": 289
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.07280672341585159,
      "learning_rate": 0.00015421686746987953,
      "loss": 0.5942,
      "step": 290
    },
    {
      "epoch": 0.2328,
      "grad_norm": 0.06532258540391922,
      "learning_rate": 0.00015405622489959842,
      "loss": 0.5986,
      "step": 291
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.08185930550098419,
      "learning_rate": 0.00015389558232931726,
      "loss": 0.5159,
      "step": 292
    },
    {
      "epoch": 0.2344,
      "grad_norm": 0.08250583708286285,
      "learning_rate": 0.00015373493975903615,
      "loss": 0.5081,
      "step": 293
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.07862396538257599,
      "learning_rate": 0.00015357429718875501,
      "loss": 0.5795,
      "step": 294
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.06650818884372711,
      "learning_rate": 0.0001534136546184739,
      "loss": 0.472,
      "step": 295
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.07352012395858765,
      "learning_rate": 0.00015325301204819277,
      "loss": 0.5173,
      "step": 296
    },
    {
      "epoch": 0.2376,
      "grad_norm": 0.05550243705511093,
      "learning_rate": 0.00015309236947791166,
      "loss": 0.4823,
      "step": 297
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.06328247487545013,
      "learning_rate": 0.00015293172690763053,
      "loss": 0.528,
      "step": 298
    },
    {
      "epoch": 0.2392,
      "grad_norm": 0.0758652538061142,
      "learning_rate": 0.00015277108433734942,
      "loss": 0.5232,
      "step": 299
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.053618889302015305,
      "learning_rate": 0.00015261044176706828,
      "loss": 0.535,
      "step": 300
    },
    {
      "epoch": 0.2408,
      "grad_norm": 0.0661269873380661,
      "learning_rate": 0.00015244979919678715,
      "loss": 0.5101,
      "step": 301
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.05801964923739433,
      "learning_rate": 0.00015228915662650601,
      "loss": 0.4757,
      "step": 302
    },
    {
      "epoch": 0.2424,
      "grad_norm": 0.05565347149968147,
      "learning_rate": 0.0001521285140562249,
      "loss": 0.4612,
      "step": 303
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.057156581431627274,
      "learning_rate": 0.00015196787148594377,
      "loss": 0.5007,
      "step": 304
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.05187027156352997,
      "learning_rate": 0.00015180722891566266,
      "loss": 0.5143,
      "step": 305
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.05799998342990875,
      "learning_rate": 0.00015164658634538153,
      "loss": 0.4646,
      "step": 306
    },
    {
      "epoch": 0.2456,
      "grad_norm": 0.05542176961898804,
      "learning_rate": 0.00015148594377510042,
      "loss": 0.4729,
      "step": 307
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.05676762014627457,
      "learning_rate": 0.00015132530120481928,
      "loss": 0.4738,
      "step": 308
    },
    {
      "epoch": 0.2472,
      "grad_norm": 0.07417292892932892,
      "learning_rate": 0.00015116465863453818,
      "loss": 0.5128,
      "step": 309
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.06742720305919647,
      "learning_rate": 0.00015100401606425701,
      "loss": 0.4786,
      "step": 310
    },
    {
      "epoch": 0.2488,
      "grad_norm": 0.04495292529463768,
      "learning_rate": 0.0001508433734939759,
      "loss": 0.4361,
      "step": 311
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.06478895992040634,
      "learning_rate": 0.00015068273092369477,
      "loss": 0.4668,
      "step": 312
    },
    {
      "epoch": 0.2504,
      "grad_norm": 0.05785438045859337,
      "learning_rate": 0.00015052208835341366,
      "loss": 0.4927,
      "step": 313
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.052143167704343796,
      "learning_rate": 0.00015036144578313253,
      "loss": 0.4873,
      "step": 314
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.056123800575733185,
      "learning_rate": 0.00015020080321285142,
      "loss": 0.4599,
      "step": 315
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.05802905187010765,
      "learning_rate": 0.00015004016064257028,
      "loss": 0.4857,
      "step": 316
    },
    {
      "epoch": 0.2536,
      "grad_norm": 0.05660131201148033,
      "learning_rate": 0.00014987951807228918,
      "loss": 0.5093,
      "step": 317
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.05642184242606163,
      "learning_rate": 0.00014971887550200804,
      "loss": 0.4633,
      "step": 318
    },
    {
      "epoch": 0.2552,
      "grad_norm": 0.05438413470983505,
      "learning_rate": 0.0001495582329317269,
      "loss": 0.5061,
      "step": 319
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.055322177708148956,
      "learning_rate": 0.00014939759036144577,
      "loss": 0.4761,
      "step": 320
    },
    {
      "epoch": 0.2568,
      "grad_norm": 0.04925363510847092,
      "learning_rate": 0.00014923694779116466,
      "loss": 0.4713,
      "step": 321
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.05919613689184189,
      "learning_rate": 0.00014907630522088353,
      "loss": 0.5584,
      "step": 322
    },
    {
      "epoch": 0.2584,
      "grad_norm": 0.06550600379705429,
      "learning_rate": 0.00014891566265060242,
      "loss": 0.4612,
      "step": 323
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.05468013137578964,
      "learning_rate": 0.00014875502008032128,
      "loss": 0.5003,
      "step": 324
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.053922127932310104,
      "learning_rate": 0.00014859437751004018,
      "loss": 0.5175,
      "step": 325
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.06139649823307991,
      "learning_rate": 0.00014843373493975904,
      "loss": 0.5108,
      "step": 326
    },
    {
      "epoch": 0.2616,
      "grad_norm": 0.0648142546415329,
      "learning_rate": 0.00014827309236947793,
      "loss": 0.4963,
      "step": 327
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.05207137390971184,
      "learning_rate": 0.00014811244979919677,
      "loss": 0.4754,
      "step": 328
    },
    {
      "epoch": 0.2632,
      "grad_norm": 0.056041307747364044,
      "learning_rate": 0.00014795180722891566,
      "loss": 0.5105,
      "step": 329
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.05113578215241432,
      "learning_rate": 0.00014779116465863453,
      "loss": 0.4508,
      "step": 330
    },
    {
      "epoch": 0.2648,
      "grad_norm": 0.04581137374043465,
      "learning_rate": 0.00014763052208835342,
      "loss": 0.4824,
      "step": 331
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.05955459177494049,
      "learning_rate": 0.00014746987951807228,
      "loss": 0.4593,
      "step": 332
    },
    {
      "epoch": 0.2664,
      "grad_norm": 0.06032082065939903,
      "learning_rate": 0.00014730923694779118,
      "loss": 0.5204,
      "step": 333
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.06349150836467743,
      "learning_rate": 0.00014714859437751004,
      "loss": 0.4899,
      "step": 334
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.06525403261184692,
      "learning_rate": 0.00014698795180722893,
      "loss": 0.4976,
      "step": 335
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.05974062532186508,
      "learning_rate": 0.0001468273092369478,
      "loss": 0.5576,
      "step": 336
    },
    {
      "epoch": 0.2696,
      "grad_norm": 0.07713156193494797,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.5598,
      "step": 337
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.05496792867779732,
      "learning_rate": 0.00014650602409638555,
      "loss": 0.483,
      "step": 338
    },
    {
      "epoch": 0.2712,
      "grad_norm": 0.0517636314034462,
      "learning_rate": 0.00014634538152610442,
      "loss": 0.4445,
      "step": 339
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.05178803950548172,
      "learning_rate": 0.00014618473895582328,
      "loss": 0.4245,
      "step": 340
    },
    {
      "epoch": 0.2728,
      "grad_norm": 0.056997284293174744,
      "learning_rate": 0.00014602409638554218,
      "loss": 0.4445,
      "step": 341
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.057540882378816605,
      "learning_rate": 0.00014586345381526104,
      "loss": 0.4971,
      "step": 342
    },
    {
      "epoch": 0.2744,
      "grad_norm": 0.056900326162576675,
      "learning_rate": 0.00014570281124497993,
      "loss": 0.4597,
      "step": 343
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.05678516626358032,
      "learning_rate": 0.0001455421686746988,
      "loss": 0.5106,
      "step": 344
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.0692368596792221,
      "learning_rate": 0.0001453815261044177,
      "loss": 0.5064,
      "step": 345
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.05989290028810501,
      "learning_rate": 0.00014522088353413655,
      "loss": 0.537,
      "step": 346
    },
    {
      "epoch": 0.2776,
      "grad_norm": 0.07197991758584976,
      "learning_rate": 0.00014506024096385542,
      "loss": 0.5113,
      "step": 347
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.05578117072582245,
      "learning_rate": 0.0001448995983935743,
      "loss": 0.4354,
      "step": 348
    },
    {
      "epoch": 0.2792,
      "grad_norm": 0.05489293485879898,
      "learning_rate": 0.00014473895582329318,
      "loss": 0.4765,
      "step": 349
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.054207298904657364,
      "learning_rate": 0.00014457831325301204,
      "loss": 0.4891,
      "step": 350
    },
    {
      "epoch": 0.2808,
      "grad_norm": 0.06417633593082428,
      "learning_rate": 0.00014441767068273093,
      "loss": 0.5094,
      "step": 351
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.058629635721445084,
      "learning_rate": 0.0001442570281124498,
      "loss": 0.4629,
      "step": 352
    },
    {
      "epoch": 0.2824,
      "grad_norm": 0.07095636427402496,
      "learning_rate": 0.0001440963855421687,
      "loss": 0.539,
      "step": 353
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.04812699556350708,
      "learning_rate": 0.00014393574297188756,
      "loss": 0.3925,
      "step": 354
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.06330052763223648,
      "learning_rate": 0.00014377510040160642,
      "loss": 0.4998,
      "step": 355
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.06363329291343689,
      "learning_rate": 0.0001436144578313253,
      "loss": 0.4957,
      "step": 356
    },
    {
      "epoch": 0.2856,
      "grad_norm": 0.05427331104874611,
      "learning_rate": 0.00014345381526104418,
      "loss": 0.4987,
      "step": 357
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.05488278344273567,
      "learning_rate": 0.00014329317269076307,
      "loss": 0.5066,
      "step": 358
    },
    {
      "epoch": 0.2872,
      "grad_norm": 0.06829617917537689,
      "learning_rate": 0.00014313253012048193,
      "loss": 0.5245,
      "step": 359
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.06082294136285782,
      "learning_rate": 0.0001429718875502008,
      "loss": 0.484,
      "step": 360
    },
    {
      "epoch": 0.2888,
      "grad_norm": 0.06442826986312866,
      "learning_rate": 0.0001428112449799197,
      "loss": 0.456,
      "step": 361
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.06122709438204765,
      "learning_rate": 0.00014265060240963856,
      "loss": 0.4209,
      "step": 362
    },
    {
      "epoch": 0.2904,
      "grad_norm": 0.07608819007873535,
      "learning_rate": 0.00014248995983935745,
      "loss": 0.5494,
      "step": 363
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.06352595239877701,
      "learning_rate": 0.0001423293172690763,
      "loss": 0.507,
      "step": 364
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.06391178071498871,
      "learning_rate": 0.00014216867469879518,
      "loss": 0.4376,
      "step": 365
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.05828239023685455,
      "learning_rate": 0.00014200803212851407,
      "loss": 0.4944,
      "step": 366
    },
    {
      "epoch": 0.2936,
      "grad_norm": 0.06337838619947433,
      "learning_rate": 0.00014184738955823293,
      "loss": 0.5224,
      "step": 367
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.0573074109852314,
      "learning_rate": 0.00014168674698795183,
      "loss": 0.5206,
      "step": 368
    },
    {
      "epoch": 0.2952,
      "grad_norm": 0.06257432699203491,
      "learning_rate": 0.0001415261044176707,
      "loss": 0.5459,
      "step": 369
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.057934682816267014,
      "learning_rate": 0.00014136546184738956,
      "loss": 0.497,
      "step": 370
    },
    {
      "epoch": 0.2968,
      "grad_norm": 0.05557722970843315,
      "learning_rate": 0.00014120481927710845,
      "loss": 0.472,
      "step": 371
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.06969202309846878,
      "learning_rate": 0.0001410441767068273,
      "loss": 0.5259,
      "step": 372
    },
    {
      "epoch": 0.2984,
      "grad_norm": 0.05599045753479004,
      "learning_rate": 0.0001408835341365462,
      "loss": 0.4364,
      "step": 373
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.06640414893627167,
      "learning_rate": 0.00014072289156626507,
      "loss": 0.5022,
      "step": 374
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.07096552103757858,
      "learning_rate": 0.00014056224899598393,
      "loss": 0.5296,
      "step": 375
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.05378240346908569,
      "learning_rate": 0.00014040160642570283,
      "loss": 0.4377,
      "step": 376
    },
    {
      "epoch": 0.3016,
      "grad_norm": 0.06347481161355972,
      "learning_rate": 0.0001402409638554217,
      "loss": 0.4774,
      "step": 377
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.06324858218431473,
      "learning_rate": 0.00014008032128514058,
      "loss": 0.466,
      "step": 378
    },
    {
      "epoch": 0.3032,
      "grad_norm": 0.05364953726530075,
      "learning_rate": 0.00013991967871485945,
      "loss": 0.4674,
      "step": 379
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.06157214567065239,
      "learning_rate": 0.00013975903614457834,
      "loss": 0.5033,
      "step": 380
    },
    {
      "epoch": 0.3048,
      "grad_norm": 0.04696663096547127,
      "learning_rate": 0.0001395983935742972,
      "loss": 0.4101,
      "step": 381
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.0688188448548317,
      "learning_rate": 0.00013943775100401607,
      "loss": 0.5132,
      "step": 382
    },
    {
      "epoch": 0.3064,
      "grad_norm": 0.05361160635948181,
      "learning_rate": 0.00013927710843373493,
      "loss": 0.5013,
      "step": 383
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.06219669058918953,
      "learning_rate": 0.00013911646586345383,
      "loss": 0.4513,
      "step": 384
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.07813066989183426,
      "learning_rate": 0.0001389558232931727,
      "loss": 0.5595,
      "step": 385
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.054729703813791275,
      "learning_rate": 0.00013879518072289158,
      "loss": 0.5249,
      "step": 386
    },
    {
      "epoch": 0.3096,
      "grad_norm": 0.05281206965446472,
      "learning_rate": 0.00013863453815261045,
      "loss": 0.442,
      "step": 387
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.07297005504369736,
      "learning_rate": 0.00013847389558232934,
      "loss": 0.4833,
      "step": 388
    },
    {
      "epoch": 0.3112,
      "grad_norm": 0.05194693058729172,
      "learning_rate": 0.0001383132530120482,
      "loss": 0.5109,
      "step": 389
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.057716891169548035,
      "learning_rate": 0.0001381526104417671,
      "loss": 0.4826,
      "step": 390
    },
    {
      "epoch": 0.3128,
      "grad_norm": 0.06078902259469032,
      "learning_rate": 0.00013799196787148596,
      "loss": 0.4228,
      "step": 391
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.07093769311904907,
      "learning_rate": 0.00013783132530120483,
      "loss": 0.4545,
      "step": 392
    },
    {
      "epoch": 0.3144,
      "grad_norm": 0.05292607843875885,
      "learning_rate": 0.0001376706827309237,
      "loss": 0.5121,
      "step": 393
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.054326705634593964,
      "learning_rate": 0.00013751004016064258,
      "loss": 0.4711,
      "step": 394
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.08149890601634979,
      "learning_rate": 0.00013734939759036145,
      "loss": 0.5089,
      "step": 395
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.05587958171963692,
      "learning_rate": 0.00013718875502008034,
      "loss": 0.4899,
      "step": 396
    },
    {
      "epoch": 0.3176,
      "grad_norm": 0.050678007304668427,
      "learning_rate": 0.0001370281124497992,
      "loss": 0.4091,
      "step": 397
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.06825815141201019,
      "learning_rate": 0.0001368674698795181,
      "loss": 0.4517,
      "step": 398
    },
    {
      "epoch": 0.3192,
      "grad_norm": 0.05731218308210373,
      "learning_rate": 0.00013670682730923696,
      "loss": 0.4203,
      "step": 399
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05230037868022919,
      "learning_rate": 0.00013654618473895585,
      "loss": 0.4364,
      "step": 400
    },
    {
      "epoch": 0.3208,
      "grad_norm": 0.06093864142894745,
      "learning_rate": 0.0001363855421686747,
      "loss": 0.5388,
      "step": 401
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.06075981259346008,
      "learning_rate": 0.00013622489959839358,
      "loss": 0.4726,
      "step": 402
    },
    {
      "epoch": 0.3224,
      "grad_norm": 0.08810392767190933,
      "learning_rate": 0.00013606425702811245,
      "loss": 0.5519,
      "step": 403
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.05071055144071579,
      "learning_rate": 0.00013590361445783134,
      "loss": 0.4142,
      "step": 404
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.06834553927183151,
      "learning_rate": 0.0001357429718875502,
      "loss": 0.5668,
      "step": 405
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.07264396548271179,
      "learning_rate": 0.0001355823293172691,
      "loss": 0.4712,
      "step": 406
    },
    {
      "epoch": 0.3256,
      "grad_norm": 0.06174252927303314,
      "learning_rate": 0.00013542168674698796,
      "loss": 0.5126,
      "step": 407
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.07592079788446426,
      "learning_rate": 0.00013526104417670685,
      "loss": 0.5521,
      "step": 408
    },
    {
      "epoch": 0.3272,
      "grad_norm": 0.055605437606573105,
      "learning_rate": 0.00013510040160642572,
      "loss": 0.4525,
      "step": 409
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.07659612596035004,
      "learning_rate": 0.00013493975903614458,
      "loss": 0.5484,
      "step": 410
    },
    {
      "epoch": 0.3288,
      "grad_norm": 0.06287281960248947,
      "learning_rate": 0.00013477911646586345,
      "loss": 0.5625,
      "step": 411
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.05448196455836296,
      "learning_rate": 0.00013461847389558234,
      "loss": 0.522,
      "step": 412
    },
    {
      "epoch": 0.3304,
      "grad_norm": 0.06676631420850754,
      "learning_rate": 0.0001344578313253012,
      "loss": 0.5256,
      "step": 413
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.07343973219394684,
      "learning_rate": 0.0001342971887550201,
      "loss": 0.6007,
      "step": 414
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.052249349653720856,
      "learning_rate": 0.00013413654618473896,
      "loss": 0.5223,
      "step": 415
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.062311287969350815,
      "learning_rate": 0.00013397590361445785,
      "loss": 0.4412,
      "step": 416
    },
    {
      "epoch": 0.3336,
      "grad_norm": 0.06626175343990326,
      "learning_rate": 0.00013381526104417672,
      "loss": 0.5898,
      "step": 417
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.05446753278374672,
      "learning_rate": 0.0001336546184738956,
      "loss": 0.5408,
      "step": 418
    },
    {
      "epoch": 0.3352,
      "grad_norm": 0.05663812905550003,
      "learning_rate": 0.00013349397590361445,
      "loss": 0.4891,
      "step": 419
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.055140767246484756,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.471,
      "step": 420
    },
    {
      "epoch": 0.3368,
      "grad_norm": 0.055410176515579224,
      "learning_rate": 0.0001331726907630522,
      "loss": 0.4608,
      "step": 421
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.0540957935154438,
      "learning_rate": 0.0001330120481927711,
      "loss": 0.5089,
      "step": 422
    },
    {
      "epoch": 0.3384,
      "grad_norm": 0.06181781738996506,
      "learning_rate": 0.00013285140562248996,
      "loss": 0.4908,
      "step": 423
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.05122995749115944,
      "learning_rate": 0.00013269076305220885,
      "loss": 0.4284,
      "step": 424
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.052731990814208984,
      "learning_rate": 0.00013253012048192772,
      "loss": 0.4835,
      "step": 425
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.05220864340662956,
      "learning_rate": 0.0001323694779116466,
      "loss": 0.4653,
      "step": 426
    },
    {
      "epoch": 0.3416,
      "grad_norm": 0.06401536613702774,
      "learning_rate": 0.00013220883534136547,
      "loss": 0.5304,
      "step": 427
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.05454602465033531,
      "learning_rate": 0.00013204819277108434,
      "loss": 0.4602,
      "step": 428
    },
    {
      "epoch": 0.3432,
      "grad_norm": 0.05134376883506775,
      "learning_rate": 0.0001318875502008032,
      "loss": 0.4473,
      "step": 429
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.05788581818342209,
      "learning_rate": 0.0001317269076305221,
      "loss": 0.4569,
      "step": 430
    },
    {
      "epoch": 0.3448,
      "grad_norm": 0.06563182920217514,
      "learning_rate": 0.00013156626506024096,
      "loss": 0.538,
      "step": 431
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.05506248027086258,
      "learning_rate": 0.00013140562248995985,
      "loss": 0.4322,
      "step": 432
    },
    {
      "epoch": 0.3464,
      "grad_norm": 0.07079292833805084,
      "learning_rate": 0.00013124497991967872,
      "loss": 0.5062,
      "step": 433
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.04234445095062256,
      "learning_rate": 0.0001310843373493976,
      "loss": 0.3823,
      "step": 434
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.05661578103899956,
      "learning_rate": 0.00013092369477911648,
      "loss": 0.464,
      "step": 435
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.07355768978595734,
      "learning_rate": 0.00013076305220883537,
      "loss": 0.5273,
      "step": 436
    },
    {
      "epoch": 0.3496,
      "grad_norm": 0.05723718926310539,
      "learning_rate": 0.0001306024096385542,
      "loss": 0.4817,
      "step": 437
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.06345789134502411,
      "learning_rate": 0.0001304417670682731,
      "loss": 0.5363,
      "step": 438
    },
    {
      "epoch": 0.3512,
      "grad_norm": 0.07079284638166428,
      "learning_rate": 0.00013028112449799196,
      "loss": 0.5669,
      "step": 439
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.07909895479679108,
      "learning_rate": 0.00013012048192771085,
      "loss": 0.5619,
      "step": 440
    },
    {
      "epoch": 0.3528,
      "grad_norm": 0.05766876041889191,
      "learning_rate": 0.00012995983935742972,
      "loss": 0.4934,
      "step": 441
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.07871949672698975,
      "learning_rate": 0.0001297991967871486,
      "loss": 0.4525,
      "step": 442
    },
    {
      "epoch": 0.3544,
      "grad_norm": 0.058916278183460236,
      "learning_rate": 0.00012963855421686748,
      "loss": 0.5074,
      "step": 443
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.04921337589621544,
      "learning_rate": 0.00012947791164658637,
      "loss": 0.4681,
      "step": 444
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.06704752147197723,
      "learning_rate": 0.00012931726907630523,
      "loss": 0.4909,
      "step": 445
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.05707016959786415,
      "learning_rate": 0.0001291566265060241,
      "loss": 0.4946,
      "step": 446
    },
    {
      "epoch": 0.3576,
      "grad_norm": 0.06007932126522064,
      "learning_rate": 0.00012899598393574296,
      "loss": 0.4781,
      "step": 447
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.05554605647921562,
      "learning_rate": 0.00012883534136546185,
      "loss": 0.4926,
      "step": 448
    },
    {
      "epoch": 0.3592,
      "grad_norm": 0.06595340371131897,
      "learning_rate": 0.00012867469879518072,
      "loss": 0.5032,
      "step": 449
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.0665116012096405,
      "learning_rate": 0.0001285140562248996,
      "loss": 0.4666,
      "step": 450
    },
    {
      "epoch": 0.3608,
      "grad_norm": 0.05589277297258377,
      "learning_rate": 0.00012835341365461848,
      "loss": 0.5063,
      "step": 451
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.07175843417644501,
      "learning_rate": 0.00012819277108433737,
      "loss": 0.517,
      "step": 452
    },
    {
      "epoch": 0.3624,
      "grad_norm": 0.06059408187866211,
      "learning_rate": 0.00012803212851405623,
      "loss": 0.4568,
      "step": 453
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.05687959864735603,
      "learning_rate": 0.00012787148594377512,
      "loss": 0.5086,
      "step": 454
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.06156369671225548,
      "learning_rate": 0.00012771084337349396,
      "loss": 0.4716,
      "step": 455
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.07128911465406418,
      "learning_rate": 0.00012755020080321285,
      "loss": 0.549,
      "step": 456
    },
    {
      "epoch": 0.3656,
      "grad_norm": 0.05486135557293892,
      "learning_rate": 0.00012738955823293172,
      "loss": 0.5486,
      "step": 457
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.05548640340566635,
      "learning_rate": 0.0001272289156626506,
      "loss": 0.5031,
      "step": 458
    },
    {
      "epoch": 0.3672,
      "grad_norm": 0.08020593971014023,
      "learning_rate": 0.00012706827309236948,
      "loss": 0.5732,
      "step": 459
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.04977188631892204,
      "learning_rate": 0.00012690763052208837,
      "loss": 0.4193,
      "step": 460
    },
    {
      "epoch": 0.3688,
      "grad_norm": 0.06595908850431442,
      "learning_rate": 0.00012674698795180723,
      "loss": 0.5544,
      "step": 461
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.05653262138366699,
      "learning_rate": 0.00012658634538152612,
      "loss": 0.467,
      "step": 462
    },
    {
      "epoch": 0.3704,
      "grad_norm": 0.054772865027189255,
      "learning_rate": 0.000126425702811245,
      "loss": 0.4955,
      "step": 463
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.055880799889564514,
      "learning_rate": 0.00012626506024096385,
      "loss": 0.4852,
      "step": 464
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.05453401431441307,
      "learning_rate": 0.00012610441767068272,
      "loss": 0.5052,
      "step": 465
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.05233475938439369,
      "learning_rate": 0.0001259437751004016,
      "loss": 0.4984,
      "step": 466
    },
    {
      "epoch": 0.3736,
      "grad_norm": 0.0515948049724102,
      "learning_rate": 0.00012578313253012048,
      "loss": 0.4604,
      "step": 467
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.05828465148806572,
      "learning_rate": 0.00012562248995983937,
      "loss": 0.5242,
      "step": 468
    },
    {
      "epoch": 0.3752,
      "grad_norm": 0.0537337101995945,
      "learning_rate": 0.00012546184738955823,
      "loss": 0.4792,
      "step": 469
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.06594223529100418,
      "learning_rate": 0.00012530120481927712,
      "loss": 0.5491,
      "step": 470
    },
    {
      "epoch": 0.3768,
      "grad_norm": 0.0487217903137207,
      "learning_rate": 0.000125140562248996,
      "loss": 0.4926,
      "step": 471
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.06236279755830765,
      "learning_rate": 0.00012497991967871488,
      "loss": 0.542,
      "step": 472
    },
    {
      "epoch": 0.3784,
      "grad_norm": 0.053711190819740295,
      "learning_rate": 0.00012481927710843375,
      "loss": 0.4352,
      "step": 473
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.0807608813047409,
      "learning_rate": 0.0001246586345381526,
      "loss": 0.5213,
      "step": 474
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.06633628159761429,
      "learning_rate": 0.00012449799196787148,
      "loss": 0.5202,
      "step": 475
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.0577068030834198,
      "learning_rate": 0.00012433734939759037,
      "loss": 0.4725,
      "step": 476
    },
    {
      "epoch": 0.3816,
      "grad_norm": 0.05926555022597313,
      "learning_rate": 0.00012417670682730923,
      "loss": 0.4935,
      "step": 477
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.05999190732836723,
      "learning_rate": 0.00012401606425702812,
      "loss": 0.4788,
      "step": 478
    },
    {
      "epoch": 0.3832,
      "grad_norm": 0.0567362941801548,
      "learning_rate": 0.000123855421686747,
      "loss": 0.4852,
      "step": 479
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.04527538642287254,
      "learning_rate": 0.00012369477911646588,
      "loss": 0.4743,
      "step": 480
    },
    {
      "epoch": 0.3848,
      "grad_norm": 0.06124204397201538,
      "learning_rate": 0.00012353413654618475,
      "loss": 0.5541,
      "step": 481
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.050088778138160706,
      "learning_rate": 0.00012337349397590364,
      "loss": 0.4922,
      "step": 482
    },
    {
      "epoch": 0.3864,
      "grad_norm": 0.05866168811917305,
      "learning_rate": 0.00012321285140562248,
      "loss": 0.4912,
      "step": 483
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.05328784137964249,
      "learning_rate": 0.00012305220883534137,
      "loss": 0.4636,
      "step": 484
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.052802857011556625,
      "learning_rate": 0.00012289156626506023,
      "loss": 0.4402,
      "step": 485
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.05273902043700218,
      "learning_rate": 0.00012273092369477912,
      "loss": 0.4376,
      "step": 486
    },
    {
      "epoch": 0.3896,
      "grad_norm": 0.05735865235328674,
      "learning_rate": 0.000122570281124498,
      "loss": 0.4582,
      "step": 487
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.0664205253124237,
      "learning_rate": 0.00012240963855421688,
      "loss": 0.4851,
      "step": 488
    },
    {
      "epoch": 0.3912,
      "grad_norm": 0.06231449544429779,
      "learning_rate": 0.00012224899598393575,
      "loss": 0.5746,
      "step": 489
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.04749613627791405,
      "learning_rate": 0.00012208835341365464,
      "loss": 0.4456,
      "step": 490
    },
    {
      "epoch": 0.3928,
      "grad_norm": 0.056306615471839905,
      "learning_rate": 0.00012192771084337352,
      "loss": 0.4592,
      "step": 491
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.05557025969028473,
      "learning_rate": 0.00012176706827309237,
      "loss": 0.4942,
      "step": 492
    },
    {
      "epoch": 0.3944,
      "grad_norm": 0.058227650821208954,
      "learning_rate": 0.00012160642570281125,
      "loss": 0.5204,
      "step": 493
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.05260312929749489,
      "learning_rate": 0.00012144578313253012,
      "loss": 0.4889,
      "step": 494
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.06555879861116409,
      "learning_rate": 0.000121285140562249,
      "loss": 0.5631,
      "step": 495
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.06317988783121109,
      "learning_rate": 0.00012112449799196788,
      "loss": 0.4927,
      "step": 496
    },
    {
      "epoch": 0.3976,
      "grad_norm": 0.0633046105504036,
      "learning_rate": 0.00012096385542168676,
      "loss": 0.5328,
      "step": 497
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.059579864144325256,
      "learning_rate": 0.00012080321285140564,
      "loss": 0.5292,
      "step": 498
    },
    {
      "epoch": 0.3992,
      "grad_norm": 0.05962629243731499,
      "learning_rate": 0.00012064257028112452,
      "loss": 0.4502,
      "step": 499
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.05274531617760658,
      "learning_rate": 0.0001204819277108434,
      "loss": 0.4699,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6992642052728422e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
