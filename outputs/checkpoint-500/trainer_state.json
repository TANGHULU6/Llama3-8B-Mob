{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.14705882352941177,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002941176470588235,
      "grad_norm": 1.9017901420593262,
      "learning_rate": 4e-05,
      "loss": 1.8347,
      "step": 1
    },
    {
      "epoch": 0.000588235294117647,
      "grad_norm": 1.8029898405075073,
      "learning_rate": 8e-05,
      "loss": 1.9589,
      "step": 2
    },
    {
      "epoch": 0.0008823529411764706,
      "grad_norm": 1.828238844871521,
      "learning_rate": 0.00012,
      "loss": 1.8937,
      "step": 3
    },
    {
      "epoch": 0.001176470588235294,
      "grad_norm": 1.7710850238800049,
      "learning_rate": 0.00016,
      "loss": 1.9004,
      "step": 4
    },
    {
      "epoch": 0.0014705882352941176,
      "grad_norm": 1.2679718732833862,
      "learning_rate": 0.0002,
      "loss": 1.4987,
      "step": 5
    },
    {
      "epoch": 0.0017647058823529412,
      "grad_norm": 2.1268582344055176,
      "learning_rate": 0.00019994108983799707,
      "loss": 1.1144,
      "step": 6
    },
    {
      "epoch": 0.002058823529411765,
      "grad_norm": 1.4259804487228394,
      "learning_rate": 0.00019988217967599413,
      "loss": 1.0524,
      "step": 7
    },
    {
      "epoch": 0.002352941176470588,
      "grad_norm": 1.2433640956878662,
      "learning_rate": 0.00019982326951399116,
      "loss": 1.0174,
      "step": 8
    },
    {
      "epoch": 0.0026470588235294116,
      "grad_norm": 1.1211179494857788,
      "learning_rate": 0.00019976435935198822,
      "loss": 0.9121,
      "step": 9
    },
    {
      "epoch": 0.0029411764705882353,
      "grad_norm": 1.2409859895706177,
      "learning_rate": 0.00019970544918998528,
      "loss": 0.8452,
      "step": 10
    },
    {
      "epoch": 0.003235294117647059,
      "grad_norm": 1.1146550178527832,
      "learning_rate": 0.00019964653902798234,
      "loss": 0.651,
      "step": 11
    },
    {
      "epoch": 0.0035294117647058825,
      "grad_norm": 0.5345523357391357,
      "learning_rate": 0.0001995876288659794,
      "loss": 0.6296,
      "step": 12
    },
    {
      "epoch": 0.003823529411764706,
      "grad_norm": 0.4835131764411926,
      "learning_rate": 0.00019952871870397644,
      "loss": 0.7068,
      "step": 13
    },
    {
      "epoch": 0.00411764705882353,
      "grad_norm": 0.35805419087409973,
      "learning_rate": 0.0001994698085419735,
      "loss": 0.5234,
      "step": 14
    },
    {
      "epoch": 0.004411764705882353,
      "grad_norm": 0.46082064509391785,
      "learning_rate": 0.00019941089837997056,
      "loss": 0.5471,
      "step": 15
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 0.35709115862846375,
      "learning_rate": 0.00019935198821796762,
      "loss": 0.5731,
      "step": 16
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.3679710328578949,
      "learning_rate": 0.00019929307805596468,
      "loss": 0.6501,
      "step": 17
    },
    {
      "epoch": 0.005294117647058823,
      "grad_norm": 0.41414979100227356,
      "learning_rate": 0.0001992341678939617,
      "loss": 0.6627,
      "step": 18
    },
    {
      "epoch": 0.005588235294117647,
      "grad_norm": 0.21425144374370575,
      "learning_rate": 0.00019917525773195877,
      "loss": 0.4337,
      "step": 19
    },
    {
      "epoch": 0.0058823529411764705,
      "grad_norm": 0.2038000077009201,
      "learning_rate": 0.00019911634756995583,
      "loss": 0.5012,
      "step": 20
    },
    {
      "epoch": 0.006176470588235294,
      "grad_norm": 0.2082575261592865,
      "learning_rate": 0.0001990574374079529,
      "loss": 0.5084,
      "step": 21
    },
    {
      "epoch": 0.006470588235294118,
      "grad_norm": 0.17989258468151093,
      "learning_rate": 0.00019899852724594995,
      "loss": 0.4244,
      "step": 22
    },
    {
      "epoch": 0.006764705882352941,
      "grad_norm": 0.20391178131103516,
      "learning_rate": 0.00019893961708394698,
      "loss": 0.5454,
      "step": 23
    },
    {
      "epoch": 0.007058823529411765,
      "grad_norm": 0.1495966911315918,
      "learning_rate": 0.00019888070692194404,
      "loss": 0.4167,
      "step": 24
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 0.2706458270549774,
      "learning_rate": 0.0001988217967599411,
      "loss": 0.4934,
      "step": 25
    },
    {
      "epoch": 0.007647058823529412,
      "grad_norm": 0.20677486062049866,
      "learning_rate": 0.00019876288659793816,
      "loss": 0.5307,
      "step": 26
    },
    {
      "epoch": 0.007941176470588234,
      "grad_norm": 0.1531270444393158,
      "learning_rate": 0.00019870397643593522,
      "loss": 0.4436,
      "step": 27
    },
    {
      "epoch": 0.00823529411764706,
      "grad_norm": 0.12485053390264511,
      "learning_rate": 0.00019864506627393226,
      "loss": 0.432,
      "step": 28
    },
    {
      "epoch": 0.008529411764705883,
      "grad_norm": 0.17757835984230042,
      "learning_rate": 0.00019858615611192932,
      "loss": 0.4469,
      "step": 29
    },
    {
      "epoch": 0.008823529411764706,
      "grad_norm": 0.1437951624393463,
      "learning_rate": 0.00019852724594992638,
      "loss": 0.348,
      "step": 30
    },
    {
      "epoch": 0.009117647058823529,
      "grad_norm": 0.15121173858642578,
      "learning_rate": 0.00019846833578792344,
      "loss": 0.3762,
      "step": 31
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 0.12074103206396103,
      "learning_rate": 0.0001984094256259205,
      "loss": 0.3952,
      "step": 32
    },
    {
      "epoch": 0.009705882352941177,
      "grad_norm": 0.14854411780834198,
      "learning_rate": 0.00019835051546391753,
      "loss": 0.4861,
      "step": 33
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1415865570306778,
      "learning_rate": 0.0001982916053019146,
      "loss": 0.4645,
      "step": 34
    },
    {
      "epoch": 0.010294117647058823,
      "grad_norm": 0.09063605219125748,
      "learning_rate": 0.00019823269513991165,
      "loss": 0.3487,
      "step": 35
    },
    {
      "epoch": 0.010588235294117647,
      "grad_norm": 0.08852742612361908,
      "learning_rate": 0.0001981737849779087,
      "loss": 0.4835,
      "step": 36
    },
    {
      "epoch": 0.01088235294117647,
      "grad_norm": 0.11155688762664795,
      "learning_rate": 0.00019811487481590577,
      "loss": 0.3558,
      "step": 37
    },
    {
      "epoch": 0.011176470588235295,
      "grad_norm": 0.1143687292933464,
      "learning_rate": 0.0001980559646539028,
      "loss": 0.4766,
      "step": 38
    },
    {
      "epoch": 0.011470588235294118,
      "grad_norm": 0.07191305607557297,
      "learning_rate": 0.00019799705449189987,
      "loss": 0.2243,
      "step": 39
    },
    {
      "epoch": 0.011764705882352941,
      "grad_norm": 0.0937204584479332,
      "learning_rate": 0.00019793814432989693,
      "loss": 0.3691,
      "step": 40
    },
    {
      "epoch": 0.012058823529411764,
      "grad_norm": 0.11187702417373657,
      "learning_rate": 0.00019787923416789399,
      "loss": 0.4092,
      "step": 41
    },
    {
      "epoch": 0.012352941176470587,
      "grad_norm": 0.07088126987218857,
      "learning_rate": 0.00019782032400589105,
      "loss": 0.3516,
      "step": 42
    },
    {
      "epoch": 0.012647058823529412,
      "grad_norm": 0.09653101861476898,
      "learning_rate": 0.00019776141384388808,
      "loss": 0.4344,
      "step": 43
    },
    {
      "epoch": 0.012941176470588235,
      "grad_norm": 0.0715731680393219,
      "learning_rate": 0.00019770250368188514,
      "loss": 0.3558,
      "step": 44
    },
    {
      "epoch": 0.013235294117647059,
      "grad_norm": 0.08587374538183212,
      "learning_rate": 0.0001976435935198822,
      "loss": 0.4738,
      "step": 45
    },
    {
      "epoch": 0.013529411764705882,
      "grad_norm": 0.07828318327665329,
      "learning_rate": 0.00019758468335787926,
      "loss": 0.3747,
      "step": 46
    },
    {
      "epoch": 0.013823529411764707,
      "grad_norm": 0.08546391129493713,
      "learning_rate": 0.00019752577319587632,
      "loss": 0.4248,
      "step": 47
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 0.08674682676792145,
      "learning_rate": 0.00019746686303387335,
      "loss": 0.4896,
      "step": 48
    },
    {
      "epoch": 0.014411764705882353,
      "grad_norm": 0.07425059378147125,
      "learning_rate": 0.0001974079528718704,
      "loss": 0.4127,
      "step": 49
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 0.08759815245866776,
      "learning_rate": 0.00019734904270986747,
      "loss": 0.4475,
      "step": 50
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.06706732511520386,
      "learning_rate": 0.00019729013254786453,
      "loss": 0.3203,
      "step": 51
    },
    {
      "epoch": 0.015294117647058824,
      "grad_norm": 0.08208151161670685,
      "learning_rate": 0.0001972312223858616,
      "loss": 0.356,
      "step": 52
    },
    {
      "epoch": 0.015588235294117648,
      "grad_norm": 0.09781312942504883,
      "learning_rate": 0.00019717231222385863,
      "loss": 0.4132,
      "step": 53
    },
    {
      "epoch": 0.01588235294117647,
      "grad_norm": 0.07092918455600739,
      "learning_rate": 0.0001971134020618557,
      "loss": 0.335,
      "step": 54
    },
    {
      "epoch": 0.016176470588235296,
      "grad_norm": 0.09368723630905151,
      "learning_rate": 0.00019705449189985275,
      "loss": 0.4217,
      "step": 55
    },
    {
      "epoch": 0.01647058823529412,
      "grad_norm": 0.07235271483659744,
      "learning_rate": 0.0001969955817378498,
      "loss": 0.3666,
      "step": 56
    },
    {
      "epoch": 0.016764705882352942,
      "grad_norm": 0.07903876900672913,
      "learning_rate": 0.00019693667157584687,
      "loss": 0.4831,
      "step": 57
    },
    {
      "epoch": 0.017058823529411765,
      "grad_norm": 0.08748850226402283,
      "learning_rate": 0.0001968777614138439,
      "loss": 0.3627,
      "step": 58
    },
    {
      "epoch": 0.01735294117647059,
      "grad_norm": 0.05902159586548805,
      "learning_rate": 0.00019681885125184093,
      "loss": 0.3672,
      "step": 59
    },
    {
      "epoch": 0.01764705882352941,
      "grad_norm": 0.09208288788795471,
      "learning_rate": 0.000196759941089838,
      "loss": 0.4468,
      "step": 60
    },
    {
      "epoch": 0.017941176470588235,
      "grad_norm": 0.07281191647052765,
      "learning_rate": 0.00019670103092783505,
      "loss": 0.3752,
      "step": 61
    },
    {
      "epoch": 0.018235294117647058,
      "grad_norm": 0.07336322218179703,
      "learning_rate": 0.00019664212076583211,
      "loss": 0.3351,
      "step": 62
    },
    {
      "epoch": 0.01852941176470588,
      "grad_norm": 0.0857594832777977,
      "learning_rate": 0.00019658321060382915,
      "loss": 0.3486,
      "step": 63
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 0.07910019904375076,
      "learning_rate": 0.0001965243004418262,
      "loss": 0.3821,
      "step": 64
    },
    {
      "epoch": 0.01911764705882353,
      "grad_norm": 0.06599901616573334,
      "learning_rate": 0.00019646539027982327,
      "loss": 0.3421,
      "step": 65
    },
    {
      "epoch": 0.019411764705882354,
      "grad_norm": 0.13408559560775757,
      "learning_rate": 0.00019640648011782033,
      "loss": 0.4391,
      "step": 66
    },
    {
      "epoch": 0.019705882352941177,
      "grad_norm": 0.09280645102262497,
      "learning_rate": 0.0001963475699558174,
      "loss": 0.2835,
      "step": 67
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.12123672664165497,
      "learning_rate": 0.00019628865979381442,
      "loss": 0.3594,
      "step": 68
    },
    {
      "epoch": 0.020294117647058824,
      "grad_norm": 0.05456243455410004,
      "learning_rate": 0.00019622974963181148,
      "loss": 0.3202,
      "step": 69
    },
    {
      "epoch": 0.020588235294117647,
      "grad_norm": 0.08300095051527023,
      "learning_rate": 0.00019617083946980854,
      "loss": 0.2711,
      "step": 70
    },
    {
      "epoch": 0.02088235294117647,
      "grad_norm": 0.11253716796636581,
      "learning_rate": 0.0001961119293078056,
      "loss": 0.434,
      "step": 71
    },
    {
      "epoch": 0.021176470588235293,
      "grad_norm": 0.08026277273893356,
      "learning_rate": 0.00019605301914580266,
      "loss": 0.3116,
      "step": 72
    },
    {
      "epoch": 0.021470588235294116,
      "grad_norm": 0.10930692404508591,
      "learning_rate": 0.0001959941089837997,
      "loss": 0.4016,
      "step": 73
    },
    {
      "epoch": 0.02176470588235294,
      "grad_norm": 0.10134784132242203,
      "learning_rate": 0.00019593519882179675,
      "loss": 0.4533,
      "step": 74
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 0.06525098532438278,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.42,
      "step": 75
    },
    {
      "epoch": 0.02235294117647059,
      "grad_norm": 0.08386800438165665,
      "learning_rate": 0.00019581737849779087,
      "loss": 0.3532,
      "step": 76
    },
    {
      "epoch": 0.022647058823529412,
      "grad_norm": 0.09660832583904266,
      "learning_rate": 0.00019575846833578793,
      "loss": 0.342,
      "step": 77
    },
    {
      "epoch": 0.022941176470588236,
      "grad_norm": 0.067966029047966,
      "learning_rate": 0.00019569955817378497,
      "loss": 0.3326,
      "step": 78
    },
    {
      "epoch": 0.02323529411764706,
      "grad_norm": 0.05596631020307541,
      "learning_rate": 0.00019564064801178203,
      "loss": 0.3234,
      "step": 79
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 0.09456225484609604,
      "learning_rate": 0.0001955817378497791,
      "loss": 0.4232,
      "step": 80
    },
    {
      "epoch": 0.023823529411764705,
      "grad_norm": 0.08949293196201324,
      "learning_rate": 0.00019552282768777615,
      "loss": 0.3799,
      "step": 81
    },
    {
      "epoch": 0.02411764705882353,
      "grad_norm": 0.10172037780284882,
      "learning_rate": 0.0001954639175257732,
      "loss": 0.4274,
      "step": 82
    },
    {
      "epoch": 0.02441176470588235,
      "grad_norm": 0.12107711285352707,
      "learning_rate": 0.00019540500736377024,
      "loss": 0.4931,
      "step": 83
    },
    {
      "epoch": 0.024705882352941175,
      "grad_norm": 0.08407244831323624,
      "learning_rate": 0.0001953460972017673,
      "loss": 0.4311,
      "step": 84
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.09880246967077255,
      "learning_rate": 0.00019528718703976436,
      "loss": 0.2732,
      "step": 85
    },
    {
      "epoch": 0.025294117647058825,
      "grad_norm": 0.10875432193279266,
      "learning_rate": 0.00019522827687776142,
      "loss": 0.3692,
      "step": 86
    },
    {
      "epoch": 0.025588235294117648,
      "grad_norm": 0.09373142570257187,
      "learning_rate": 0.00019516936671575848,
      "loss": 0.4146,
      "step": 87
    },
    {
      "epoch": 0.02588235294117647,
      "grad_norm": 0.06063848361372948,
      "learning_rate": 0.00019511045655375552,
      "loss": 0.2976,
      "step": 88
    },
    {
      "epoch": 0.026176470588235294,
      "grad_norm": 0.05426608398556709,
      "learning_rate": 0.00019505154639175258,
      "loss": 0.3455,
      "step": 89
    },
    {
      "epoch": 0.026470588235294117,
      "grad_norm": 0.09773965179920197,
      "learning_rate": 0.00019499263622974964,
      "loss": 0.4176,
      "step": 90
    },
    {
      "epoch": 0.02676470588235294,
      "grad_norm": 0.07388195395469666,
      "learning_rate": 0.0001949337260677467,
      "loss": 0.4503,
      "step": 91
    },
    {
      "epoch": 0.027058823529411764,
      "grad_norm": 0.09933801740407944,
      "learning_rate": 0.00019487481590574376,
      "loss": 0.3875,
      "step": 92
    },
    {
      "epoch": 0.027352941176470587,
      "grad_norm": 0.08318546414375305,
      "learning_rate": 0.0001948159057437408,
      "loss": 0.3592,
      "step": 93
    },
    {
      "epoch": 0.027647058823529413,
      "grad_norm": 0.07178034633398056,
      "learning_rate": 0.00019475699558173785,
      "loss": 0.3976,
      "step": 94
    },
    {
      "epoch": 0.027941176470588237,
      "grad_norm": 0.06823879480361938,
      "learning_rate": 0.0001946980854197349,
      "loss": 0.3504,
      "step": 95
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 0.0746409222483635,
      "learning_rate": 0.00019463917525773197,
      "loss": 0.3232,
      "step": 96
    },
    {
      "epoch": 0.028529411764705883,
      "grad_norm": 0.09007870405912399,
      "learning_rate": 0.00019458026509572903,
      "loss": 0.4303,
      "step": 97
    },
    {
      "epoch": 0.028823529411764706,
      "grad_norm": 0.08346293866634369,
      "learning_rate": 0.00019452135493372606,
      "loss": 0.454,
      "step": 98
    },
    {
      "epoch": 0.02911764705882353,
      "grad_norm": 0.05117262527346611,
      "learning_rate": 0.00019446244477172312,
      "loss": 0.3047,
      "step": 99
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.08778347074985504,
      "learning_rate": 0.00019440353460972018,
      "loss": 0.4976,
      "step": 100
    },
    {
      "epoch": 0.029705882352941176,
      "grad_norm": 0.07785649597644806,
      "learning_rate": 0.00019434462444771724,
      "loss": 0.3745,
      "step": 101
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.05458449572324753,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.3561,
      "step": 102
    },
    {
      "epoch": 0.030294117647058822,
      "grad_norm": 0.06850040704011917,
      "learning_rate": 0.00019422680412371134,
      "loss": 0.3486,
      "step": 103
    },
    {
      "epoch": 0.03058823529411765,
      "grad_norm": 0.07516885548830032,
      "learning_rate": 0.0001941678939617084,
      "loss": 0.4176,
      "step": 104
    },
    {
      "epoch": 0.030882352941176472,
      "grad_norm": 0.07857518643140793,
      "learning_rate": 0.00019410898379970546,
      "loss": 0.3769,
      "step": 105
    },
    {
      "epoch": 0.031176470588235295,
      "grad_norm": 0.07482893019914627,
      "learning_rate": 0.00019405007363770252,
      "loss": 0.4067,
      "step": 106
    },
    {
      "epoch": 0.03147058823529412,
      "grad_norm": 0.09830916672945023,
      "learning_rate": 0.00019399116347569958,
      "loss": 0.4196,
      "step": 107
    },
    {
      "epoch": 0.03176470588235294,
      "grad_norm": 0.07468336075544357,
      "learning_rate": 0.0001939322533136966,
      "loss": 0.3891,
      "step": 108
    },
    {
      "epoch": 0.032058823529411765,
      "grad_norm": 0.0617087185382843,
      "learning_rate": 0.00019387334315169367,
      "loss": 0.2831,
      "step": 109
    },
    {
      "epoch": 0.03235294117647059,
      "grad_norm": 0.08578453958034515,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.3942,
      "step": 110
    },
    {
      "epoch": 0.03264705882352941,
      "grad_norm": 0.0785159096121788,
      "learning_rate": 0.0001937555228276878,
      "loss": 0.3718,
      "step": 111
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 0.07825743407011032,
      "learning_rate": 0.00019369661266568485,
      "loss": 0.4118,
      "step": 112
    },
    {
      "epoch": 0.03323529411764706,
      "grad_norm": 0.0976356640458107,
      "learning_rate": 0.00019363770250368188,
      "loss": 0.3632,
      "step": 113
    },
    {
      "epoch": 0.033529411764705884,
      "grad_norm": 0.08399389684200287,
      "learning_rate": 0.00019357879234167894,
      "loss": 0.3752,
      "step": 114
    },
    {
      "epoch": 0.033823529411764704,
      "grad_norm": 0.08931345492601395,
      "learning_rate": 0.000193519882179676,
      "loss": 0.4144,
      "step": 115
    },
    {
      "epoch": 0.03411764705882353,
      "grad_norm": 0.09492973238229752,
      "learning_rate": 0.00019346097201767306,
      "loss": 0.3792,
      "step": 116
    },
    {
      "epoch": 0.03441176470588235,
      "grad_norm": 0.09350302815437317,
      "learning_rate": 0.00019340206185567012,
      "loss": 0.4238,
      "step": 117
    },
    {
      "epoch": 0.03470588235294118,
      "grad_norm": 0.0785617008805275,
      "learning_rate": 0.00019334315169366716,
      "loss": 0.4334,
      "step": 118
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.08060774207115173,
      "learning_rate": 0.00019328424153166422,
      "loss": 0.4303,
      "step": 119
    },
    {
      "epoch": 0.03529411764705882,
      "grad_norm": 0.07216943055391312,
      "learning_rate": 0.00019322533136966128,
      "loss": 0.4022,
      "step": 120
    },
    {
      "epoch": 0.03558823529411765,
      "grad_norm": 0.08611080795526505,
      "learning_rate": 0.00019316642120765834,
      "loss": 0.3385,
      "step": 121
    },
    {
      "epoch": 0.03588235294117647,
      "grad_norm": 0.09069265425205231,
      "learning_rate": 0.0001931075110456554,
      "loss": 0.4865,
      "step": 122
    },
    {
      "epoch": 0.036176470588235296,
      "grad_norm": 0.06891902536153793,
      "learning_rate": 0.00019304860088365243,
      "loss": 0.3918,
      "step": 123
    },
    {
      "epoch": 0.036470588235294116,
      "grad_norm": 0.07725144922733307,
      "learning_rate": 0.0001929896907216495,
      "loss": 0.4358,
      "step": 124
    },
    {
      "epoch": 0.03676470588235294,
      "grad_norm": 0.06405150890350342,
      "learning_rate": 0.00019293078055964655,
      "loss": 0.307,
      "step": 125
    },
    {
      "epoch": 0.03705882352941176,
      "grad_norm": 0.09536789357662201,
      "learning_rate": 0.0001928718703976436,
      "loss": 0.4015,
      "step": 126
    },
    {
      "epoch": 0.03735294117647059,
      "grad_norm": 0.07194528728723526,
      "learning_rate": 0.00019281296023564067,
      "loss": 0.3593,
      "step": 127
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 0.053252458572387695,
      "learning_rate": 0.0001927540500736377,
      "loss": 0.3293,
      "step": 128
    },
    {
      "epoch": 0.037941176470588235,
      "grad_norm": 0.0684155523777008,
      "learning_rate": 0.00019269513991163477,
      "loss": 0.4011,
      "step": 129
    },
    {
      "epoch": 0.03823529411764706,
      "grad_norm": 0.07344681769609451,
      "learning_rate": 0.00019263622974963183,
      "loss": 0.3686,
      "step": 130
    },
    {
      "epoch": 0.03852941176470588,
      "grad_norm": 0.051469989120960236,
      "learning_rate": 0.00019257731958762889,
      "loss": 0.323,
      "step": 131
    },
    {
      "epoch": 0.03882352941176471,
      "grad_norm": 0.08393733203411102,
      "learning_rate": 0.00019251840942562595,
      "loss": 0.4296,
      "step": 132
    },
    {
      "epoch": 0.03911764705882353,
      "grad_norm": 0.05931030958890915,
      "learning_rate": 0.00019245949926362298,
      "loss": 0.2892,
      "step": 133
    },
    {
      "epoch": 0.039411764705882354,
      "grad_norm": 0.07138442993164062,
      "learning_rate": 0.00019240058910162004,
      "loss": 0.4097,
      "step": 134
    },
    {
      "epoch": 0.039705882352941174,
      "grad_norm": 0.0805647075176239,
      "learning_rate": 0.0001923416789396171,
      "loss": 0.4004,
      "step": 135
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.07254071533679962,
      "learning_rate": 0.00019228276877761416,
      "loss": 0.4183,
      "step": 136
    },
    {
      "epoch": 0.04029411764705882,
      "grad_norm": 0.07440889626741409,
      "learning_rate": 0.00019222385861561122,
      "loss": 0.3521,
      "step": 137
    },
    {
      "epoch": 0.04058823529411765,
      "grad_norm": 0.06398832052946091,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.2904,
      "step": 138
    },
    {
      "epoch": 0.040882352941176474,
      "grad_norm": 0.10812432318925858,
      "learning_rate": 0.0001921060382916053,
      "loss": 0.421,
      "step": 139
    },
    {
      "epoch": 0.041176470588235294,
      "grad_norm": 0.05943019315600395,
      "learning_rate": 0.00019204712812960237,
      "loss": 0.3607,
      "step": 140
    },
    {
      "epoch": 0.04147058823529412,
      "grad_norm": 0.07914503663778305,
      "learning_rate": 0.00019198821796759943,
      "loss": 0.4313,
      "step": 141
    },
    {
      "epoch": 0.04176470588235294,
      "grad_norm": 0.07476179301738739,
      "learning_rate": 0.0001919293078055965,
      "loss": 0.423,
      "step": 142
    },
    {
      "epoch": 0.04205882352941177,
      "grad_norm": 0.06569647043943405,
      "learning_rate": 0.00019187039764359353,
      "loss": 0.3882,
      "step": 143
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 0.0693218782544136,
      "learning_rate": 0.0001918114874815906,
      "loss": 0.4107,
      "step": 144
    },
    {
      "epoch": 0.04264705882352941,
      "grad_norm": 0.08186395466327667,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.3618,
      "step": 145
    },
    {
      "epoch": 0.04294117647058823,
      "grad_norm": 0.08952964842319489,
      "learning_rate": 0.0001916936671575847,
      "loss": 0.4112,
      "step": 146
    },
    {
      "epoch": 0.04323529411764706,
      "grad_norm": 0.07004809379577637,
      "learning_rate": 0.00019163475699558177,
      "loss": 0.3516,
      "step": 147
    },
    {
      "epoch": 0.04352941176470588,
      "grad_norm": 0.06543531268835068,
      "learning_rate": 0.0001915758468335788,
      "loss": 0.3691,
      "step": 148
    },
    {
      "epoch": 0.043823529411764706,
      "grad_norm": 0.0856979712843895,
      "learning_rate": 0.00019151693667157586,
      "loss": 0.4403,
      "step": 149
    },
    {
      "epoch": 0.04411764705882353,
      "grad_norm": 0.08877045661211014,
      "learning_rate": 0.00019145802650957292,
      "loss": 0.3939,
      "step": 150
    },
    {
      "epoch": 0.04441176470588235,
      "grad_norm": 0.05518195405602455,
      "learning_rate": 0.00019139911634756998,
      "loss": 0.3232,
      "step": 151
    },
    {
      "epoch": 0.04470588235294118,
      "grad_norm": 0.06716704368591309,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.3184,
      "step": 152
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.08146704733371735,
      "learning_rate": 0.00019128129602356407,
      "loss": 0.4199,
      "step": 153
    },
    {
      "epoch": 0.045294117647058825,
      "grad_norm": 0.07697336375713348,
      "learning_rate": 0.00019122238586156113,
      "loss": 0.4123,
      "step": 154
    },
    {
      "epoch": 0.045588235294117645,
      "grad_norm": 0.0758376270532608,
      "learning_rate": 0.0001911634756995582,
      "loss": 0.3689,
      "step": 155
    },
    {
      "epoch": 0.04588235294117647,
      "grad_norm": 0.052515652030706406,
      "learning_rate": 0.00019110456553755525,
      "loss": 0.3013,
      "step": 156
    },
    {
      "epoch": 0.04617647058823529,
      "grad_norm": 0.09197742491960526,
      "learning_rate": 0.00019104565537555231,
      "loss": 0.4684,
      "step": 157
    },
    {
      "epoch": 0.04647058823529412,
      "grad_norm": 0.06481204181909561,
      "learning_rate": 0.00019098674521354935,
      "loss": 0.3764,
      "step": 158
    },
    {
      "epoch": 0.046764705882352944,
      "grad_norm": 0.11131249368190765,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.4198,
      "step": 159
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 0.06052528694272041,
      "learning_rate": 0.00019086892488954347,
      "loss": 0.2726,
      "step": 160
    },
    {
      "epoch": 0.04735294117647059,
      "grad_norm": 0.0812031552195549,
      "learning_rate": 0.00019081001472754053,
      "loss": 0.3611,
      "step": 161
    },
    {
      "epoch": 0.04764705882352941,
      "grad_norm": 0.08201907575130463,
      "learning_rate": 0.0001907511045655376,
      "loss": 0.3686,
      "step": 162
    },
    {
      "epoch": 0.04794117647058824,
      "grad_norm": 0.08419091999530792,
      "learning_rate": 0.00019069219440353462,
      "loss": 0.3642,
      "step": 163
    },
    {
      "epoch": 0.04823529411764706,
      "grad_norm": 0.07594932615756989,
      "learning_rate": 0.00019063328424153168,
      "loss": 0.3521,
      "step": 164
    },
    {
      "epoch": 0.04852941176470588,
      "grad_norm": 0.08616840094327927,
      "learning_rate": 0.00019057437407952871,
      "loss": 0.348,
      "step": 165
    },
    {
      "epoch": 0.0488235294117647,
      "grad_norm": 0.09599340707063675,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.4184,
      "step": 166
    },
    {
      "epoch": 0.04911764705882353,
      "grad_norm": 0.06443074345588684,
      "learning_rate": 0.00019045655375552283,
      "loss": 0.3023,
      "step": 167
    },
    {
      "epoch": 0.04941176470588235,
      "grad_norm": 0.07103496789932251,
      "learning_rate": 0.00019039764359351987,
      "loss": 0.3338,
      "step": 168
    },
    {
      "epoch": 0.049705882352941176,
      "grad_norm": 0.07142699509859085,
      "learning_rate": 0.00019033873343151693,
      "loss": 0.3735,
      "step": 169
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.06523020565509796,
      "learning_rate": 0.000190279823269514,
      "loss": 0.3259,
      "step": 170
    },
    {
      "epoch": 0.05029411764705882,
      "grad_norm": 0.06932653486728668,
      "learning_rate": 0.00019022091310751105,
      "loss": 0.4123,
      "step": 171
    },
    {
      "epoch": 0.05058823529411765,
      "grad_norm": 0.07477002590894699,
      "learning_rate": 0.0001901620029455081,
      "loss": 0.3851,
      "step": 172
    },
    {
      "epoch": 0.05088235294117647,
      "grad_norm": 0.06321363896131516,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.3298,
      "step": 173
    },
    {
      "epoch": 0.051176470588235295,
      "grad_norm": 0.06770181655883789,
      "learning_rate": 0.0001900441826215022,
      "loss": 0.3327,
      "step": 174
    },
    {
      "epoch": 0.051470588235294115,
      "grad_norm": 0.08581835776567459,
      "learning_rate": 0.00018998527245949926,
      "loss": 0.4237,
      "step": 175
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 0.08285832405090332,
      "learning_rate": 0.00018992636229749632,
      "loss": 0.3257,
      "step": 176
    },
    {
      "epoch": 0.05205882352941176,
      "grad_norm": 0.0945327952504158,
      "learning_rate": 0.00018986745213549338,
      "loss": 0.3499,
      "step": 177
    },
    {
      "epoch": 0.05235294117647059,
      "grad_norm": 0.07117632776498795,
      "learning_rate": 0.00018980854197349042,
      "loss": 0.3369,
      "step": 178
    },
    {
      "epoch": 0.052647058823529415,
      "grad_norm": 0.0673501193523407,
      "learning_rate": 0.00018974963181148748,
      "loss": 0.3608,
      "step": 179
    },
    {
      "epoch": 0.052941176470588235,
      "grad_norm": 0.08254967629909515,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.3696,
      "step": 180
    },
    {
      "epoch": 0.05323529411764706,
      "grad_norm": 0.06940854340791702,
      "learning_rate": 0.0001896318114874816,
      "loss": 0.4267,
      "step": 181
    },
    {
      "epoch": 0.05352941176470588,
      "grad_norm": 0.07577857375144958,
      "learning_rate": 0.00018957290132547866,
      "loss": 0.3282,
      "step": 182
    },
    {
      "epoch": 0.05382352941176471,
      "grad_norm": 0.05162256956100464,
      "learning_rate": 0.0001895139911634757,
      "loss": 0.2629,
      "step": 183
    },
    {
      "epoch": 0.05411764705882353,
      "grad_norm": 0.06116514280438423,
      "learning_rate": 0.00018945508100147275,
      "loss": 0.3658,
      "step": 184
    },
    {
      "epoch": 0.054411764705882354,
      "grad_norm": 0.07126085460186005,
      "learning_rate": 0.0001893961708394698,
      "loss": 0.3358,
      "step": 185
    },
    {
      "epoch": 0.054705882352941174,
      "grad_norm": 0.07707571983337402,
      "learning_rate": 0.00018933726067746687,
      "loss": 0.3492,
      "step": 186
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.10023003071546555,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.3914,
      "step": 187
    },
    {
      "epoch": 0.05529411764705883,
      "grad_norm": 0.058111727237701416,
      "learning_rate": 0.00018921944035346096,
      "loss": 0.344,
      "step": 188
    },
    {
      "epoch": 0.05558823529411765,
      "grad_norm": 0.05905693396925926,
      "learning_rate": 0.00018916053019145802,
      "loss": 0.2973,
      "step": 189
    },
    {
      "epoch": 0.05588235294117647,
      "grad_norm": 0.07258603721857071,
      "learning_rate": 0.00018910162002945508,
      "loss": 0.4136,
      "step": 190
    },
    {
      "epoch": 0.05617647058823529,
      "grad_norm": 0.056615542620420456,
      "learning_rate": 0.00018904270986745214,
      "loss": 0.3189,
      "step": 191
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 0.06405438482761383,
      "learning_rate": 0.0001889837997054492,
      "loss": 0.4238,
      "step": 192
    },
    {
      "epoch": 0.05676470588235294,
      "grad_norm": 0.059019383043050766,
      "learning_rate": 0.00018892488954344624,
      "loss": 0.3804,
      "step": 193
    },
    {
      "epoch": 0.057058823529411766,
      "grad_norm": 0.0648026242852211,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.3693,
      "step": 194
    },
    {
      "epoch": 0.057352941176470586,
      "grad_norm": 0.08629299700260162,
      "learning_rate": 0.00018880706921944036,
      "loss": 0.4859,
      "step": 195
    },
    {
      "epoch": 0.05764705882352941,
      "grad_norm": 0.08178982138633728,
      "learning_rate": 0.00018874815905743742,
      "loss": 0.4264,
      "step": 196
    },
    {
      "epoch": 0.05794117647058823,
      "grad_norm": 0.06669653207063675,
      "learning_rate": 0.00018868924889543448,
      "loss": 0.29,
      "step": 197
    },
    {
      "epoch": 0.05823529411764706,
      "grad_norm": 0.07139337062835693,
      "learning_rate": 0.0001886303387334315,
      "loss": 0.3713,
      "step": 198
    },
    {
      "epoch": 0.058529411764705885,
      "grad_norm": 0.06619426608085632,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3224,
      "step": 199
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.07070176303386688,
      "learning_rate": 0.00018851251840942563,
      "loss": 0.3441,
      "step": 200
    },
    {
      "epoch": 0.05911764705882353,
      "grad_norm": 0.08892921358346939,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.3867,
      "step": 201
    },
    {
      "epoch": 0.05941176470588235,
      "grad_norm": 0.06082486733794212,
      "learning_rate": 0.00018839469808541975,
      "loss": 0.3161,
      "step": 202
    },
    {
      "epoch": 0.05970588235294118,
      "grad_norm": 0.05711718276143074,
      "learning_rate": 0.00018833578792341678,
      "loss": 0.3822,
      "step": 203
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.05581040307879448,
      "learning_rate": 0.00018827687776141384,
      "loss": 0.2972,
      "step": 204
    },
    {
      "epoch": 0.060294117647058824,
      "grad_norm": 0.09559269994497299,
      "learning_rate": 0.0001882179675994109,
      "loss": 0.396,
      "step": 205
    },
    {
      "epoch": 0.060588235294117644,
      "grad_norm": 0.05928659066557884,
      "learning_rate": 0.00018815905743740796,
      "loss": 0.3453,
      "step": 206
    },
    {
      "epoch": 0.06088235294117647,
      "grad_norm": 0.06149083375930786,
      "learning_rate": 0.00018810014727540502,
      "loss": 0.3259,
      "step": 207
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 0.06094400957226753,
      "learning_rate": 0.00018804123711340206,
      "loss": 0.3424,
      "step": 208
    },
    {
      "epoch": 0.06147058823529412,
      "grad_norm": 0.0803094357252121,
      "learning_rate": 0.00018798232695139912,
      "loss": 0.378,
      "step": 209
    },
    {
      "epoch": 0.061764705882352944,
      "grad_norm": 0.05801349878311157,
      "learning_rate": 0.00018792341678939618,
      "loss": 0.3371,
      "step": 210
    },
    {
      "epoch": 0.062058823529411763,
      "grad_norm": 0.0828152596950531,
      "learning_rate": 0.00018786450662739324,
      "loss": 0.4039,
      "step": 211
    },
    {
      "epoch": 0.06235294117647059,
      "grad_norm": 0.05826396495103836,
      "learning_rate": 0.0001878055964653903,
      "loss": 0.353,
      "step": 212
    },
    {
      "epoch": 0.06264705882352942,
      "grad_norm": 0.06452875584363937,
      "learning_rate": 0.00018774668630338733,
      "loss": 0.3836,
      "step": 213
    },
    {
      "epoch": 0.06294117647058824,
      "grad_norm": 0.05847480520606041,
      "learning_rate": 0.0001876877761413844,
      "loss": 0.3062,
      "step": 214
    },
    {
      "epoch": 0.06323529411764706,
      "grad_norm": 0.06894835084676743,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.3715,
      "step": 215
    },
    {
      "epoch": 0.06352941176470588,
      "grad_norm": 0.06043218821287155,
      "learning_rate": 0.0001875699558173785,
      "loss": 0.3197,
      "step": 216
    },
    {
      "epoch": 0.06382352941176471,
      "grad_norm": 0.060165759176015854,
      "learning_rate": 0.00018751104565537557,
      "loss": 0.2788,
      "step": 217
    },
    {
      "epoch": 0.06411764705882353,
      "grad_norm": 0.057238586246967316,
      "learning_rate": 0.0001874521354933726,
      "loss": 0.3481,
      "step": 218
    },
    {
      "epoch": 0.06441176470588235,
      "grad_norm": 0.08119262009859085,
      "learning_rate": 0.00018739322533136967,
      "loss": 0.3352,
      "step": 219
    },
    {
      "epoch": 0.06470588235294118,
      "grad_norm": 0.08880080282688141,
      "learning_rate": 0.00018733431516936673,
      "loss": 0.4009,
      "step": 220
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.06980836391448975,
      "learning_rate": 0.00018727540500736379,
      "loss": 0.3645,
      "step": 221
    },
    {
      "epoch": 0.06529411764705882,
      "grad_norm": 0.05632235109806061,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.328,
      "step": 222
    },
    {
      "epoch": 0.06558823529411764,
      "grad_norm": 0.04948994517326355,
      "learning_rate": 0.00018715758468335788,
      "loss": 0.3328,
      "step": 223
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 0.07953855395317078,
      "learning_rate": 0.00018709867452135494,
      "loss": 0.3155,
      "step": 224
    },
    {
      "epoch": 0.0661764705882353,
      "grad_norm": 0.07696998119354248,
      "learning_rate": 0.000187039764359352,
      "loss": 0.389,
      "step": 225
    },
    {
      "epoch": 0.06647058823529411,
      "grad_norm": 0.07052081823348999,
      "learning_rate": 0.00018698085419734906,
      "loss": 0.3984,
      "step": 226
    },
    {
      "epoch": 0.06676470588235293,
      "grad_norm": 0.05734168365597725,
      "learning_rate": 0.00018692194403534612,
      "loss": 0.3694,
      "step": 227
    },
    {
      "epoch": 0.06705882352941177,
      "grad_norm": 0.05877940356731415,
      "learning_rate": 0.00018686303387334315,
      "loss": 0.3408,
      "step": 228
    },
    {
      "epoch": 0.06735294117647059,
      "grad_norm": 0.060689233243465424,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.29,
      "step": 229
    },
    {
      "epoch": 0.06764705882352941,
      "grad_norm": 0.05925682932138443,
      "learning_rate": 0.00018674521354933727,
      "loss": 0.3687,
      "step": 230
    },
    {
      "epoch": 0.06794117647058824,
      "grad_norm": 0.07324173301458359,
      "learning_rate": 0.00018668630338733433,
      "loss": 0.3666,
      "step": 231
    },
    {
      "epoch": 0.06823529411764706,
      "grad_norm": 0.07756275683641434,
      "learning_rate": 0.0001866273932253314,
      "loss": 0.4314,
      "step": 232
    },
    {
      "epoch": 0.06852941176470588,
      "grad_norm": 0.07440493255853653,
      "learning_rate": 0.00018656848306332843,
      "loss": 0.3846,
      "step": 233
    },
    {
      "epoch": 0.0688235294117647,
      "grad_norm": 0.06739978492259979,
      "learning_rate": 0.0001865095729013255,
      "loss": 0.382,
      "step": 234
    },
    {
      "epoch": 0.06911764705882353,
      "grad_norm": 0.05041012167930603,
      "learning_rate": 0.00018645066273932255,
      "loss": 0.3368,
      "step": 235
    },
    {
      "epoch": 0.06941176470588235,
      "grad_norm": 0.066290944814682,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.3793,
      "step": 236
    },
    {
      "epoch": 0.06970588235294117,
      "grad_norm": 0.06832732260227203,
      "learning_rate": 0.00018633284241531667,
      "loss": 0.3489,
      "step": 237
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0740690529346466,
      "learning_rate": 0.0001862739322533137,
      "loss": 0.4941,
      "step": 238
    },
    {
      "epoch": 0.07029411764705883,
      "grad_norm": 0.07410252094268799,
      "learning_rate": 0.00018621502209131076,
      "loss": 0.3707,
      "step": 239
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 0.07068129628896713,
      "learning_rate": 0.00018615611192930782,
      "loss": 0.4141,
      "step": 240
    },
    {
      "epoch": 0.07088235294117647,
      "grad_norm": 0.06870078295469284,
      "learning_rate": 0.00018609720176730488,
      "loss": 0.3152,
      "step": 241
    },
    {
      "epoch": 0.0711764705882353,
      "grad_norm": 0.06046896427869797,
      "learning_rate": 0.00018603829160530194,
      "loss": 0.3597,
      "step": 242
    },
    {
      "epoch": 0.07147058823529412,
      "grad_norm": 0.08149320632219315,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.3901,
      "step": 243
    },
    {
      "epoch": 0.07176470588235294,
      "grad_norm": 0.06869029998779297,
      "learning_rate": 0.00018592047128129603,
      "loss": 0.3938,
      "step": 244
    },
    {
      "epoch": 0.07205882352941176,
      "grad_norm": 0.08220042288303375,
      "learning_rate": 0.0001858615611192931,
      "loss": 0.362,
      "step": 245
    },
    {
      "epoch": 0.07235294117647059,
      "grad_norm": 0.09427651017904282,
      "learning_rate": 0.00018580265095729015,
      "loss": 0.4375,
      "step": 246
    },
    {
      "epoch": 0.07264705882352941,
      "grad_norm": 0.09413967281579971,
      "learning_rate": 0.00018574374079528721,
      "loss": 0.4539,
      "step": 247
    },
    {
      "epoch": 0.07294117647058823,
      "grad_norm": 0.06712189316749573,
      "learning_rate": 0.00018568483063328425,
      "loss": 0.3547,
      "step": 248
    },
    {
      "epoch": 0.07323529411764707,
      "grad_norm": 0.08505844324827194,
      "learning_rate": 0.0001856259204712813,
      "loss": 0.3818,
      "step": 249
    },
    {
      "epoch": 0.07352941176470588,
      "grad_norm": 0.09507601708173752,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.4742,
      "step": 250
    },
    {
      "epoch": 0.0738235294117647,
      "grad_norm": 0.05248608440160751,
      "learning_rate": 0.00018550810014727543,
      "loss": 0.2966,
      "step": 251
    },
    {
      "epoch": 0.07411764705882352,
      "grad_norm": 0.06461775302886963,
      "learning_rate": 0.0001854491899852725,
      "loss": 0.3861,
      "step": 252
    },
    {
      "epoch": 0.07441176470588236,
      "grad_norm": 0.06287456303834915,
      "learning_rate": 0.00018539027982326952,
      "loss": 0.3106,
      "step": 253
    },
    {
      "epoch": 0.07470588235294118,
      "grad_norm": 0.055949170142412186,
      "learning_rate": 0.00018533136966126658,
      "loss": 0.3145,
      "step": 254
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.07702046632766724,
      "learning_rate": 0.00018527245949926364,
      "loss": 0.4529,
      "step": 255
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 0.05366233363747597,
      "learning_rate": 0.0001852135493372607,
      "loss": 0.3212,
      "step": 256
    },
    {
      "epoch": 0.07558823529411765,
      "grad_norm": 0.04590430483222008,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.2962,
      "step": 257
    },
    {
      "epoch": 0.07588235294117647,
      "grad_norm": 0.06390290707349777,
      "learning_rate": 0.0001850957290132548,
      "loss": 0.3636,
      "step": 258
    },
    {
      "epoch": 0.07617647058823529,
      "grad_norm": 0.079315684735775,
      "learning_rate": 0.00018503681885125186,
      "loss": 0.3795,
      "step": 259
    },
    {
      "epoch": 0.07647058823529412,
      "grad_norm": 0.068527452647686,
      "learning_rate": 0.00018497790868924892,
      "loss": 0.4243,
      "step": 260
    },
    {
      "epoch": 0.07676470588235294,
      "grad_norm": 0.07096641510725021,
      "learning_rate": 0.00018491899852724598,
      "loss": 0.4449,
      "step": 261
    },
    {
      "epoch": 0.07705882352941176,
      "grad_norm": 0.07984885573387146,
      "learning_rate": 0.00018486008836524304,
      "loss": 0.4637,
      "step": 262
    },
    {
      "epoch": 0.07735294117647058,
      "grad_norm": 0.05733319744467735,
      "learning_rate": 0.00018480117820324007,
      "loss": 0.3345,
      "step": 263
    },
    {
      "epoch": 0.07764705882352942,
      "grad_norm": 0.07460702210664749,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.3905,
      "step": 264
    },
    {
      "epoch": 0.07794117647058824,
      "grad_norm": 0.08981731534004211,
      "learning_rate": 0.0001846833578792342,
      "loss": 0.3799,
      "step": 265
    },
    {
      "epoch": 0.07823529411764706,
      "grad_norm": 0.07102034240961075,
      "learning_rate": 0.00018462444771723125,
      "loss": 0.4121,
      "step": 266
    },
    {
      "epoch": 0.07852941176470589,
      "grad_norm": 0.09098925441503525,
      "learning_rate": 0.0001845655375552283,
      "loss": 0.4233,
      "step": 267
    },
    {
      "epoch": 0.07882352941176471,
      "grad_norm": 0.0705140233039856,
      "learning_rate": 0.00018450662739322534,
      "loss": 0.3948,
      "step": 268
    },
    {
      "epoch": 0.07911764705882353,
      "grad_norm": 0.05768349766731262,
      "learning_rate": 0.0001844477172312224,
      "loss": 0.3722,
      "step": 269
    },
    {
      "epoch": 0.07941176470588235,
      "grad_norm": 0.0816139355301857,
      "learning_rate": 0.00018438880706921946,
      "loss": 0.3795,
      "step": 270
    },
    {
      "epoch": 0.07970588235294118,
      "grad_norm": 0.06616862863302231,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.415,
      "step": 271
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.05865853652358055,
      "learning_rate": 0.00018427098674521356,
      "loss": 0.2895,
      "step": 272
    },
    {
      "epoch": 0.08029411764705882,
      "grad_norm": 0.0715838149189949,
      "learning_rate": 0.0001842120765832106,
      "loss": 0.3233,
      "step": 273
    },
    {
      "epoch": 0.08058823529411764,
      "grad_norm": 0.07817074656486511,
      "learning_rate": 0.00018415316642120765,
      "loss": 0.3683,
      "step": 274
    },
    {
      "epoch": 0.08088235294117647,
      "grad_norm": 0.06610382348299026,
      "learning_rate": 0.0001840942562592047,
      "loss": 0.3951,
      "step": 275
    },
    {
      "epoch": 0.0811764705882353,
      "grad_norm": 0.07598253339529037,
      "learning_rate": 0.00018403534609720177,
      "loss": 0.3353,
      "step": 276
    },
    {
      "epoch": 0.08147058823529411,
      "grad_norm": 0.07075203210115433,
      "learning_rate": 0.00018397643593519883,
      "loss": 0.3959,
      "step": 277
    },
    {
      "epoch": 0.08176470588235295,
      "grad_norm": 0.06762167066335678,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.3882,
      "step": 278
    },
    {
      "epoch": 0.08205882352941177,
      "grad_norm": 0.07061689347028732,
      "learning_rate": 0.00018385861561119292,
      "loss": 0.4008,
      "step": 279
    },
    {
      "epoch": 0.08235294117647059,
      "grad_norm": 0.08138107508420944,
      "learning_rate": 0.00018379970544918998,
      "loss": 0.4333,
      "step": 280
    },
    {
      "epoch": 0.0826470588235294,
      "grad_norm": 0.07874414324760437,
      "learning_rate": 0.00018374079528718704,
      "loss": 0.4537,
      "step": 281
    },
    {
      "epoch": 0.08294117647058824,
      "grad_norm": 0.0925433561205864,
      "learning_rate": 0.0001836818851251841,
      "loss": 0.4508,
      "step": 282
    },
    {
      "epoch": 0.08323529411764706,
      "grad_norm": 0.05465120077133179,
      "learning_rate": 0.00018362297496318114,
      "loss": 0.3921,
      "step": 283
    },
    {
      "epoch": 0.08352941176470588,
      "grad_norm": 0.059858329594135284,
      "learning_rate": 0.0001835640648011782,
      "loss": 0.3174,
      "step": 284
    },
    {
      "epoch": 0.0838235294117647,
      "grad_norm": 0.05330105498433113,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.336,
      "step": 285
    },
    {
      "epoch": 0.08411764705882353,
      "grad_norm": 0.06698829680681229,
      "learning_rate": 0.00018344624447717232,
      "loss": 0.3269,
      "step": 286
    },
    {
      "epoch": 0.08441176470588235,
      "grad_norm": 0.08343657106161118,
      "learning_rate": 0.00018338733431516938,
      "loss": 0.4219,
      "step": 287
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 0.060324955731630325,
      "learning_rate": 0.0001833284241531664,
      "loss": 0.3466,
      "step": 288
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.057091135531663895,
      "learning_rate": 0.00018326951399116347,
      "loss": 0.4132,
      "step": 289
    },
    {
      "epoch": 0.08529411764705883,
      "grad_norm": 0.08447415381669998,
      "learning_rate": 0.00018321060382916053,
      "loss": 0.4419,
      "step": 290
    },
    {
      "epoch": 0.08558823529411765,
      "grad_norm": 0.049560029059648514,
      "learning_rate": 0.0001831516936671576,
      "loss": 0.2815,
      "step": 291
    },
    {
      "epoch": 0.08588235294117647,
      "grad_norm": 0.12339417636394501,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.4795,
      "step": 292
    },
    {
      "epoch": 0.0861764705882353,
      "grad_norm": 0.0648915022611618,
      "learning_rate": 0.00018303387334315168,
      "loss": 0.3854,
      "step": 293
    },
    {
      "epoch": 0.08647058823529412,
      "grad_norm": 0.06920251995325089,
      "learning_rate": 0.00018297496318114874,
      "loss": 0.3784,
      "step": 294
    },
    {
      "epoch": 0.08676470588235294,
      "grad_norm": 0.06751497834920883,
      "learning_rate": 0.0001829160530191458,
      "loss": 0.3685,
      "step": 295
    },
    {
      "epoch": 0.08705882352941176,
      "grad_norm": 0.06494586169719696,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.4147,
      "step": 296
    },
    {
      "epoch": 0.08735294117647059,
      "grad_norm": 0.0658063217997551,
      "learning_rate": 0.00018279823269513992,
      "loss": 0.3549,
      "step": 297
    },
    {
      "epoch": 0.08764705882352941,
      "grad_norm": 0.07966029644012451,
      "learning_rate": 0.00018273932253313696,
      "loss": 0.3485,
      "step": 298
    },
    {
      "epoch": 0.08794117647058823,
      "grad_norm": 0.06242798641324043,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.3608,
      "step": 299
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.06966325640678406,
      "learning_rate": 0.00018262150220913108,
      "loss": 0.4039,
      "step": 300
    },
    {
      "epoch": 0.08852941176470588,
      "grad_norm": 0.046506062150001526,
      "learning_rate": 0.00018256259204712814,
      "loss": 0.2942,
      "step": 301
    },
    {
      "epoch": 0.0888235294117647,
      "grad_norm": 0.06489913165569305,
      "learning_rate": 0.0001825036818851252,
      "loss": 0.3899,
      "step": 302
    },
    {
      "epoch": 0.08911764705882352,
      "grad_norm": 0.07311385869979858,
      "learning_rate": 0.00018244477172312223,
      "loss": 0.3858,
      "step": 303
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 0.0623113214969635,
      "learning_rate": 0.0001823858615611193,
      "loss": 0.363,
      "step": 304
    },
    {
      "epoch": 0.08970588235294118,
      "grad_norm": 0.058694787323474884,
      "learning_rate": 0.00018232695139911635,
      "loss": 0.3917,
      "step": 305
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.049128107726573944,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.3205,
      "step": 306
    },
    {
      "epoch": 0.09029411764705883,
      "grad_norm": 0.07686536014080048,
      "learning_rate": 0.00018220913107511047,
      "loss": 0.3624,
      "step": 307
    },
    {
      "epoch": 0.09058823529411765,
      "grad_norm": 0.06679993122816086,
      "learning_rate": 0.0001821502209131075,
      "loss": 0.3594,
      "step": 308
    },
    {
      "epoch": 0.09088235294117647,
      "grad_norm": 0.06321009248495102,
      "learning_rate": 0.00018209131075110457,
      "loss": 0.3755,
      "step": 309
    },
    {
      "epoch": 0.09117647058823529,
      "grad_norm": 0.05005679279565811,
      "learning_rate": 0.00018203240058910163,
      "loss": 0.3325,
      "step": 310
    },
    {
      "epoch": 0.09147058823529412,
      "grad_norm": 0.07738867402076721,
      "learning_rate": 0.00018197349042709869,
      "loss": 0.4073,
      "step": 311
    },
    {
      "epoch": 0.09176470588235294,
      "grad_norm": 0.05306961387395859,
      "learning_rate": 0.00018191458026509575,
      "loss": 0.3559,
      "step": 312
    },
    {
      "epoch": 0.09205882352941176,
      "grad_norm": 0.07010605186223984,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.4019,
      "step": 313
    },
    {
      "epoch": 0.09235294117647058,
      "grad_norm": 0.0561184287071228,
      "learning_rate": 0.00018179675994108984,
      "loss": 0.3662,
      "step": 314
    },
    {
      "epoch": 0.09264705882352942,
      "grad_norm": 0.06064090132713318,
      "learning_rate": 0.0001817378497790869,
      "loss": 0.3442,
      "step": 315
    },
    {
      "epoch": 0.09294117647058824,
      "grad_norm": 0.055273476988077164,
      "learning_rate": 0.00018167893961708396,
      "loss": 0.3867,
      "step": 316
    },
    {
      "epoch": 0.09323529411764706,
      "grad_norm": 0.06528283655643463,
      "learning_rate": 0.00018162002945508102,
      "loss": 0.2962,
      "step": 317
    },
    {
      "epoch": 0.09352941176470589,
      "grad_norm": 0.054356832057237625,
      "learning_rate": 0.00018156111929307805,
      "loss": 0.3098,
      "step": 318
    },
    {
      "epoch": 0.09382352941176471,
      "grad_norm": 0.0716046467423439,
      "learning_rate": 0.0001815022091310751,
      "loss": 0.416,
      "step": 319
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.048138394951820374,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.3053,
      "step": 320
    },
    {
      "epoch": 0.09441176470588235,
      "grad_norm": 0.059518963098526,
      "learning_rate": 0.00018138438880706923,
      "loss": 0.3306,
      "step": 321
    },
    {
      "epoch": 0.09470588235294118,
      "grad_norm": 0.08200190216302872,
      "learning_rate": 0.0001813254786450663,
      "loss": 0.4431,
      "step": 322
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06480855494737625,
      "learning_rate": 0.00018126656848306333,
      "loss": 0.3429,
      "step": 323
    },
    {
      "epoch": 0.09529411764705882,
      "grad_norm": 0.05147107318043709,
      "learning_rate": 0.00018120765832106039,
      "loss": 0.347,
      "step": 324
    },
    {
      "epoch": 0.09558823529411764,
      "grad_norm": 0.07055281102657318,
      "learning_rate": 0.00018114874815905745,
      "loss": 0.3451,
      "step": 325
    },
    {
      "epoch": 0.09588235294117647,
      "grad_norm": 0.07833453267812729,
      "learning_rate": 0.0001810898379970545,
      "loss": 0.4076,
      "step": 326
    },
    {
      "epoch": 0.0961764705882353,
      "grad_norm": 0.05751001834869385,
      "learning_rate": 0.00018103092783505157,
      "loss": 0.3703,
      "step": 327
    },
    {
      "epoch": 0.09647058823529411,
      "grad_norm": 0.05488632619380951,
      "learning_rate": 0.0001809720176730486,
      "loss": 0.3214,
      "step": 328
    },
    {
      "epoch": 0.09676470588235295,
      "grad_norm": 0.059503521770238876,
      "learning_rate": 0.00018091310751104566,
      "loss": 0.3217,
      "step": 329
    },
    {
      "epoch": 0.09705882352941177,
      "grad_norm": 0.0577617809176445,
      "learning_rate": 0.00018085419734904272,
      "loss": 0.3086,
      "step": 330
    },
    {
      "epoch": 0.09735294117647059,
      "grad_norm": 0.0629652738571167,
      "learning_rate": 0.00018079528718703978,
      "loss": 0.3844,
      "step": 331
    },
    {
      "epoch": 0.0976470588235294,
      "grad_norm": 0.06608890742063522,
      "learning_rate": 0.00018073637702503684,
      "loss": 0.3571,
      "step": 332
    },
    {
      "epoch": 0.09794117647058824,
      "grad_norm": 0.05386192724108696,
      "learning_rate": 0.00018067746686303387,
      "loss": 0.3223,
      "step": 333
    },
    {
      "epoch": 0.09823529411764706,
      "grad_norm": 0.07279947400093079,
      "learning_rate": 0.00018061855670103093,
      "loss": 0.3918,
      "step": 334
    },
    {
      "epoch": 0.09852941176470588,
      "grad_norm": 0.06606367975473404,
      "learning_rate": 0.000180559646539028,
      "loss": 0.3674,
      "step": 335
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 0.0569785013794899,
      "learning_rate": 0.00018050073637702505,
      "loss": 0.3349,
      "step": 336
    },
    {
      "epoch": 0.09911764705882353,
      "grad_norm": 0.056245751678943634,
      "learning_rate": 0.00018044182621502211,
      "loss": 0.4027,
      "step": 337
    },
    {
      "epoch": 0.09941176470588235,
      "grad_norm": 0.06317412853240967,
      "learning_rate": 0.00018038291605301915,
      "loss": 0.3482,
      "step": 338
    },
    {
      "epoch": 0.09970588235294117,
      "grad_norm": 0.048747770488262177,
      "learning_rate": 0.0001803240058910162,
      "loss": 0.3146,
      "step": 339
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.06825648248195648,
      "learning_rate": 0.00018026509572901327,
      "loss": 0.4104,
      "step": 340
    },
    {
      "epoch": 0.10029411764705883,
      "grad_norm": 0.05543675646185875,
      "learning_rate": 0.00018020618556701033,
      "loss": 0.3656,
      "step": 341
    },
    {
      "epoch": 0.10058823529411764,
      "grad_norm": 0.0682123601436615,
      "learning_rate": 0.0001801472754050074,
      "loss": 0.3719,
      "step": 342
    },
    {
      "epoch": 0.10088235294117646,
      "grad_norm": 0.06396583467721939,
      "learning_rate": 0.00018008836524300442,
      "loss": 0.3392,
      "step": 343
    },
    {
      "epoch": 0.1011764705882353,
      "grad_norm": 0.06525035947561264,
      "learning_rate": 0.00018002945508100148,
      "loss": 0.4001,
      "step": 344
    },
    {
      "epoch": 0.10147058823529412,
      "grad_norm": 0.06932403892278671,
      "learning_rate": 0.00017997054491899854,
      "loss": 0.3657,
      "step": 345
    },
    {
      "epoch": 0.10176470588235294,
      "grad_norm": 0.05467165634036064,
      "learning_rate": 0.0001799116347569956,
      "loss": 0.4113,
      "step": 346
    },
    {
      "epoch": 0.10205882352941177,
      "grad_norm": 0.06200922280550003,
      "learning_rate": 0.00017985272459499266,
      "loss": 0.3121,
      "step": 347
    },
    {
      "epoch": 0.10235294117647059,
      "grad_norm": 0.061485063284635544,
      "learning_rate": 0.0001797938144329897,
      "loss": 0.3098,
      "step": 348
    },
    {
      "epoch": 0.10264705882352941,
      "grad_norm": 0.0642416849732399,
      "learning_rate": 0.00017973490427098675,
      "loss": 0.3411,
      "step": 349
    },
    {
      "epoch": 0.10294117647058823,
      "grad_norm": 0.04680927097797394,
      "learning_rate": 0.00017967599410898382,
      "loss": 0.324,
      "step": 350
    },
    {
      "epoch": 0.10323529411764706,
      "grad_norm": 0.05197561904788017,
      "learning_rate": 0.00017961708394698088,
      "loss": 0.3162,
      "step": 351
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 0.054913997650146484,
      "learning_rate": 0.00017955817378497794,
      "loss": 0.3408,
      "step": 352
    },
    {
      "epoch": 0.1038235294117647,
      "grad_norm": 0.07395322620868683,
      "learning_rate": 0.00017949926362297497,
      "loss": 0.3652,
      "step": 353
    },
    {
      "epoch": 0.10411764705882352,
      "grad_norm": 0.06962316483259201,
      "learning_rate": 0.00017944035346097203,
      "loss": 0.4478,
      "step": 354
    },
    {
      "epoch": 0.10441176470588236,
      "grad_norm": 0.06380102783441544,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4222,
      "step": 355
    },
    {
      "epoch": 0.10470588235294118,
      "grad_norm": 0.08563566952943802,
      "learning_rate": 0.00017932253313696615,
      "loss": 0.4801,
      "step": 356
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.05784589424729347,
      "learning_rate": 0.0001792636229749632,
      "loss": 0.3583,
      "step": 357
    },
    {
      "epoch": 0.10529411764705883,
      "grad_norm": 0.05640096217393875,
      "learning_rate": 0.00017920471281296024,
      "loss": 0.3538,
      "step": 358
    },
    {
      "epoch": 0.10558823529411765,
      "grad_norm": 0.06621186435222626,
      "learning_rate": 0.0001791458026509573,
      "loss": 0.4273,
      "step": 359
    },
    {
      "epoch": 0.10588235294117647,
      "grad_norm": 0.06506894528865814,
      "learning_rate": 0.00017908689248895436,
      "loss": 0.431,
      "step": 360
    },
    {
      "epoch": 0.10617647058823529,
      "grad_norm": 0.07250071316957474,
      "learning_rate": 0.00017902798232695142,
      "loss": 0.42,
      "step": 361
    },
    {
      "epoch": 0.10647058823529412,
      "grad_norm": 0.06515748053789139,
      "learning_rate": 0.00017896907216494848,
      "loss": 0.4368,
      "step": 362
    },
    {
      "epoch": 0.10676470588235294,
      "grad_norm": 0.06887184083461761,
      "learning_rate": 0.00017891016200294552,
      "loss": 0.3783,
      "step": 363
    },
    {
      "epoch": 0.10705882352941176,
      "grad_norm": 0.06759919226169586,
      "learning_rate": 0.00017885125184094258,
      "loss": 0.3283,
      "step": 364
    },
    {
      "epoch": 0.10735294117647058,
      "grad_norm": 0.058880142867565155,
      "learning_rate": 0.00017879234167893964,
      "loss": 0.3454,
      "step": 365
    },
    {
      "epoch": 0.10764705882352942,
      "grad_norm": 0.07714717835187912,
      "learning_rate": 0.0001787334315169367,
      "loss": 0.3812,
      "step": 366
    },
    {
      "epoch": 0.10794117647058823,
      "grad_norm": 0.06342080980539322,
      "learning_rate": 0.00017867452135493376,
      "loss": 0.3334,
      "step": 367
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 0.08185877650976181,
      "learning_rate": 0.0001786156111929308,
      "loss": 0.4489,
      "step": 368
    },
    {
      "epoch": 0.10852941176470589,
      "grad_norm": 0.06441660970449448,
      "learning_rate": 0.00017855670103092785,
      "loss": 0.4196,
      "step": 369
    },
    {
      "epoch": 0.10882352941176471,
      "grad_norm": 0.0684155523777008,
      "learning_rate": 0.0001784977908689249,
      "loss": 0.3998,
      "step": 370
    },
    {
      "epoch": 0.10911764705882353,
      "grad_norm": 0.06541825085878372,
      "learning_rate": 0.00017843888070692197,
      "loss": 0.3635,
      "step": 371
    },
    {
      "epoch": 0.10941176470588235,
      "grad_norm": 0.06810446083545685,
      "learning_rate": 0.00017837997054491903,
      "loss": 0.3301,
      "step": 372
    },
    {
      "epoch": 0.10970588235294118,
      "grad_norm": 0.06217741221189499,
      "learning_rate": 0.00017832106038291606,
      "loss": 0.3809,
      "step": 373
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.05610036849975586,
      "learning_rate": 0.00017826215022091312,
      "loss": 0.3398,
      "step": 374
    },
    {
      "epoch": 0.11029411764705882,
      "grad_norm": 0.06394899636507034,
      "learning_rate": 0.00017820324005891018,
      "loss": 0.3713,
      "step": 375
    },
    {
      "epoch": 0.11058823529411765,
      "grad_norm": 0.04780898615717888,
      "learning_rate": 0.00017814432989690724,
      "loss": 0.2578,
      "step": 376
    },
    {
      "epoch": 0.11088235294117647,
      "grad_norm": 0.055820561945438385,
      "learning_rate": 0.00017808541973490428,
      "loss": 0.3816,
      "step": 377
    },
    {
      "epoch": 0.1111764705882353,
      "grad_norm": 0.08161716163158417,
      "learning_rate": 0.0001780265095729013,
      "loss": 0.3929,
      "step": 378
    },
    {
      "epoch": 0.11147058823529411,
      "grad_norm": 0.06336405873298645,
      "learning_rate": 0.00017796759941089837,
      "loss": 0.3856,
      "step": 379
    },
    {
      "epoch": 0.11176470588235295,
      "grad_norm": 0.0461711585521698,
      "learning_rate": 0.00017790868924889543,
      "loss": 0.3168,
      "step": 380
    },
    {
      "epoch": 0.11205882352941177,
      "grad_norm": 0.05394534766674042,
      "learning_rate": 0.0001778497790868925,
      "loss": 0.3194,
      "step": 381
    },
    {
      "epoch": 0.11235294117647059,
      "grad_norm": 0.060035914182662964,
      "learning_rate": 0.00017779086892488955,
      "loss": 0.3661,
      "step": 382
    },
    {
      "epoch": 0.1126470588235294,
      "grad_norm": 0.06650905311107635,
      "learning_rate": 0.00017773195876288658,
      "loss": 0.3788,
      "step": 383
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.0511661060154438,
      "learning_rate": 0.00017767304860088364,
      "loss": 0.3154,
      "step": 384
    },
    {
      "epoch": 0.11323529411764706,
      "grad_norm": 0.06222601607441902,
      "learning_rate": 0.0001776141384388807,
      "loss": 0.4536,
      "step": 385
    },
    {
      "epoch": 0.11352941176470588,
      "grad_norm": 0.05904239043593407,
      "learning_rate": 0.00017755522827687776,
      "loss": 0.3199,
      "step": 386
    },
    {
      "epoch": 0.11382352941176471,
      "grad_norm": 0.05111019313335419,
      "learning_rate": 0.00017749631811487482,
      "loss": 0.318,
      "step": 387
    },
    {
      "epoch": 0.11411764705882353,
      "grad_norm": 0.06134296953678131,
      "learning_rate": 0.00017743740795287186,
      "loss": 0.3932,
      "step": 388
    },
    {
      "epoch": 0.11441176470588235,
      "grad_norm": 0.04546453803777695,
      "learning_rate": 0.00017737849779086892,
      "loss": 0.3217,
      "step": 389
    },
    {
      "epoch": 0.11470588235294117,
      "grad_norm": 0.044524893164634705,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.3199,
      "step": 390
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.04633469134569168,
      "learning_rate": 0.00017726067746686304,
      "loss": 0.3078,
      "step": 391
    },
    {
      "epoch": 0.11529411764705882,
      "grad_norm": 0.0478605292737484,
      "learning_rate": 0.0001772017673048601,
      "loss": 0.317,
      "step": 392
    },
    {
      "epoch": 0.11558823529411764,
      "grad_norm": 0.06444822251796722,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3799,
      "step": 393
    },
    {
      "epoch": 0.11588235294117646,
      "grad_norm": 0.05480935424566269,
      "learning_rate": 0.0001770839469808542,
      "loss": 0.3854,
      "step": 394
    },
    {
      "epoch": 0.1161764705882353,
      "grad_norm": 0.07725254446268082,
      "learning_rate": 0.00017702503681885125,
      "loss": 0.4353,
      "step": 395
    },
    {
      "epoch": 0.11647058823529412,
      "grad_norm": 0.07547573745250702,
      "learning_rate": 0.0001769661266568483,
      "loss": 0.4639,
      "step": 396
    },
    {
      "epoch": 0.11676470588235294,
      "grad_norm": 0.05252469331026077,
      "learning_rate": 0.00017690721649484537,
      "loss": 0.3224,
      "step": 397
    },
    {
      "epoch": 0.11705882352941177,
      "grad_norm": 0.06379898637533188,
      "learning_rate": 0.0001768483063328424,
      "loss": 0.3626,
      "step": 398
    },
    {
      "epoch": 0.11735294117647059,
      "grad_norm": 0.07324875146150589,
      "learning_rate": 0.00017678939617083947,
      "loss": 0.3845,
      "step": 399
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.07202717661857605,
      "learning_rate": 0.00017673048600883653,
      "loss": 0.3564,
      "step": 400
    },
    {
      "epoch": 0.11794117647058823,
      "grad_norm": 0.07221133261919022,
      "learning_rate": 0.00017667157584683359,
      "loss": 0.459,
      "step": 401
    },
    {
      "epoch": 0.11823529411764706,
      "grad_norm": 0.06054593622684479,
      "learning_rate": 0.00017661266568483065,
      "loss": 0.4718,
      "step": 402
    },
    {
      "epoch": 0.11852941176470588,
      "grad_norm": 0.0724506825208664,
      "learning_rate": 0.00017655375552282768,
      "loss": 0.3528,
      "step": 403
    },
    {
      "epoch": 0.1188235294117647,
      "grad_norm": 0.06451458483934402,
      "learning_rate": 0.00017649484536082474,
      "loss": 0.3355,
      "step": 404
    },
    {
      "epoch": 0.11911764705882352,
      "grad_norm": 0.057649191468954086,
      "learning_rate": 0.0001764359351988218,
      "loss": 0.3808,
      "step": 405
    },
    {
      "epoch": 0.11941176470588236,
      "grad_norm": 0.06574881821870804,
      "learning_rate": 0.00017637702503681886,
      "loss": 0.3834,
      "step": 406
    },
    {
      "epoch": 0.11970588235294118,
      "grad_norm": 0.05234568566083908,
      "learning_rate": 0.00017631811487481592,
      "loss": 0.3308,
      "step": 407
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07136711478233337,
      "learning_rate": 0.00017625920471281295,
      "loss": 0.4331,
      "step": 408
    },
    {
      "epoch": 0.12029411764705883,
      "grad_norm": 0.0593692921102047,
      "learning_rate": 0.00017620029455081,
      "loss": 0.3605,
      "step": 409
    },
    {
      "epoch": 0.12058823529411765,
      "grad_norm": 0.061242297291755676,
      "learning_rate": 0.00017614138438880707,
      "loss": 0.4166,
      "step": 410
    },
    {
      "epoch": 0.12088235294117647,
      "grad_norm": 0.06117153540253639,
      "learning_rate": 0.00017608247422680413,
      "loss": 0.388,
      "step": 411
    },
    {
      "epoch": 0.12117647058823529,
      "grad_norm": 0.05603696033358574,
      "learning_rate": 0.0001760235640648012,
      "loss": 0.3718,
      "step": 412
    },
    {
      "epoch": 0.12147058823529412,
      "grad_norm": 0.06118542328476906,
      "learning_rate": 0.00017596465390279823,
      "loss": 0.4067,
      "step": 413
    },
    {
      "epoch": 0.12176470588235294,
      "grad_norm": 0.039172861725091934,
      "learning_rate": 0.00017590574374079529,
      "loss": 0.3015,
      "step": 414
    },
    {
      "epoch": 0.12205882352941176,
      "grad_norm": 0.06444255262613297,
      "learning_rate": 0.00017584683357879235,
      "loss": 0.3892,
      "step": 415
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 0.05588488280773163,
      "learning_rate": 0.0001757879234167894,
      "loss": 0.3118,
      "step": 416
    },
    {
      "epoch": 0.12264705882352941,
      "grad_norm": 0.07427003979682922,
      "learning_rate": 0.00017572901325478647,
      "loss": 0.4167,
      "step": 417
    },
    {
      "epoch": 0.12294117647058823,
      "grad_norm": 0.0465058796107769,
      "learning_rate": 0.0001756701030927835,
      "loss": 0.3195,
      "step": 418
    },
    {
      "epoch": 0.12323529411764705,
      "grad_norm": 0.050864074379205704,
      "learning_rate": 0.00017561119293078056,
      "loss": 0.3268,
      "step": 419
    },
    {
      "epoch": 0.12352941176470589,
      "grad_norm": 0.09469547122716904,
      "learning_rate": 0.00017555228276877762,
      "loss": 0.3025,
      "step": 420
    },
    {
      "epoch": 0.12382352941176471,
      "grad_norm": 0.07172059267759323,
      "learning_rate": 0.00017549337260677468,
      "loss": 0.4325,
      "step": 421
    },
    {
      "epoch": 0.12411764705882353,
      "grad_norm": 0.051121532917022705,
      "learning_rate": 0.00017543446244477174,
      "loss": 0.3146,
      "step": 422
    },
    {
      "epoch": 0.12441176470588235,
      "grad_norm": 0.07183513045310974,
      "learning_rate": 0.00017537555228276877,
      "loss": 0.3588,
      "step": 423
    },
    {
      "epoch": 0.12470588235294118,
      "grad_norm": 0.0905645564198494,
      "learning_rate": 0.00017531664212076583,
      "loss": 0.4159,
      "step": 424
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06450112164020538,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4084,
      "step": 425
    },
    {
      "epoch": 0.12529411764705883,
      "grad_norm": 0.06768307834863663,
      "learning_rate": 0.00017519882179675995,
      "loss": 0.4074,
      "step": 426
    },
    {
      "epoch": 0.12558823529411764,
      "grad_norm": 0.07054144144058228,
      "learning_rate": 0.00017513991163475701,
      "loss": 0.3664,
      "step": 427
    },
    {
      "epoch": 0.12588235294117647,
      "grad_norm": 0.05521046370267868,
      "learning_rate": 0.00017508100147275405,
      "loss": 0.3123,
      "step": 428
    },
    {
      "epoch": 0.1261764705882353,
      "grad_norm": 0.049050744622945786,
      "learning_rate": 0.0001750220913107511,
      "loss": 0.2982,
      "step": 429
    },
    {
      "epoch": 0.1264705882352941,
      "grad_norm": 0.07248954474925995,
      "learning_rate": 0.00017496318114874817,
      "loss": 0.3856,
      "step": 430
    },
    {
      "epoch": 0.12676470588235295,
      "grad_norm": 0.05912009999155998,
      "learning_rate": 0.00017490427098674523,
      "loss": 0.3263,
      "step": 431
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 0.08046949654817581,
      "learning_rate": 0.0001748453608247423,
      "loss": 0.4143,
      "step": 432
    },
    {
      "epoch": 0.12735294117647059,
      "grad_norm": 0.05608232691884041,
      "learning_rate": 0.00017478645066273932,
      "loss": 0.3189,
      "step": 433
    },
    {
      "epoch": 0.12764705882352942,
      "grad_norm": 0.05293964222073555,
      "learning_rate": 0.00017472754050073638,
      "loss": 0.3381,
      "step": 434
    },
    {
      "epoch": 0.12794117647058822,
      "grad_norm": 0.05264270305633545,
      "learning_rate": 0.00017466863033873344,
      "loss": 0.3527,
      "step": 435
    },
    {
      "epoch": 0.12823529411764706,
      "grad_norm": 0.06921862810850143,
      "learning_rate": 0.0001746097201767305,
      "loss": 0.4024,
      "step": 436
    },
    {
      "epoch": 0.1285294117647059,
      "grad_norm": 0.05247751623392105,
      "learning_rate": 0.00017455081001472756,
      "loss": 0.3725,
      "step": 437
    },
    {
      "epoch": 0.1288235294117647,
      "grad_norm": 0.06483626365661621,
      "learning_rate": 0.0001744918998527246,
      "loss": 0.4246,
      "step": 438
    },
    {
      "epoch": 0.12911764705882353,
      "grad_norm": 0.05579894781112671,
      "learning_rate": 0.00017443298969072165,
      "loss": 0.3929,
      "step": 439
    },
    {
      "epoch": 0.12941176470588237,
      "grad_norm": 0.05639004707336426,
      "learning_rate": 0.00017437407952871872,
      "loss": 0.3702,
      "step": 440
    },
    {
      "epoch": 0.12970588235294117,
      "grad_norm": 0.05637900531291962,
      "learning_rate": 0.00017431516936671578,
      "loss": 0.3456,
      "step": 441
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.058856867253780365,
      "learning_rate": 0.00017425625920471284,
      "loss": 0.3508,
      "step": 442
    },
    {
      "epoch": 0.1302941176470588,
      "grad_norm": 0.06624807417392731,
      "learning_rate": 0.00017419734904270987,
      "loss": 0.367,
      "step": 443
    },
    {
      "epoch": 0.13058823529411764,
      "grad_norm": 0.05654902011156082,
      "learning_rate": 0.00017413843888070693,
      "loss": 0.3695,
      "step": 444
    },
    {
      "epoch": 0.13088235294117648,
      "grad_norm": 0.05620846897363663,
      "learning_rate": 0.000174079528718704,
      "loss": 0.3235,
      "step": 445
    },
    {
      "epoch": 0.13117647058823528,
      "grad_norm": 0.07250788062810898,
      "learning_rate": 0.00017402061855670105,
      "loss": 0.4003,
      "step": 446
    },
    {
      "epoch": 0.13147058823529412,
      "grad_norm": 0.0533524826169014,
      "learning_rate": 0.0001739617083946981,
      "loss": 0.321,
      "step": 447
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 0.04872708022594452,
      "learning_rate": 0.00017390279823269514,
      "loss": 0.3145,
      "step": 448
    },
    {
      "epoch": 0.13205882352941176,
      "grad_norm": 0.0677766501903534,
      "learning_rate": 0.0001738438880706922,
      "loss": 0.3661,
      "step": 449
    },
    {
      "epoch": 0.1323529411764706,
      "grad_norm": 0.06011781096458435,
      "learning_rate": 0.00017378497790868926,
      "loss": 0.3321,
      "step": 450
    },
    {
      "epoch": 0.13264705882352942,
      "grad_norm": 0.059614550322294235,
      "learning_rate": 0.00017372606774668632,
      "loss": 0.4049,
      "step": 451
    },
    {
      "epoch": 0.13294117647058823,
      "grad_norm": 0.04442768916487694,
      "learning_rate": 0.00017366715758468338,
      "loss": 0.28,
      "step": 452
    },
    {
      "epoch": 0.13323529411764706,
      "grad_norm": 0.05528303235769272,
      "learning_rate": 0.00017360824742268042,
      "loss": 0.3638,
      "step": 453
    },
    {
      "epoch": 0.13352941176470587,
      "grad_norm": 0.05034511163830757,
      "learning_rate": 0.00017354933726067748,
      "loss": 0.3053,
      "step": 454
    },
    {
      "epoch": 0.1338235294117647,
      "grad_norm": 0.05702619254589081,
      "learning_rate": 0.00017349042709867454,
      "loss": 0.3265,
      "step": 455
    },
    {
      "epoch": 0.13411764705882354,
      "grad_norm": 0.06102799251675606,
      "learning_rate": 0.0001734315169366716,
      "loss": 0.3186,
      "step": 456
    },
    {
      "epoch": 0.13441176470588234,
      "grad_norm": 0.053947534412145615,
      "learning_rate": 0.00017337260677466866,
      "loss": 0.3026,
      "step": 457
    },
    {
      "epoch": 0.13470588235294118,
      "grad_norm": 0.05989567190408707,
      "learning_rate": 0.0001733136966126657,
      "loss": 0.3526,
      "step": 458
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.048722539097070694,
      "learning_rate": 0.00017325478645066275,
      "loss": 0.353,
      "step": 459
    },
    {
      "epoch": 0.13529411764705881,
      "grad_norm": 0.04778992384672165,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.2815,
      "step": 460
    },
    {
      "epoch": 0.13558823529411765,
      "grad_norm": 0.06761613488197327,
      "learning_rate": 0.00017313696612665687,
      "loss": 0.3809,
      "step": 461
    },
    {
      "epoch": 0.13588235294117648,
      "grad_norm": 0.050742797553539276,
      "learning_rate": 0.00017307805596465393,
      "loss": 0.32,
      "step": 462
    },
    {
      "epoch": 0.1361764705882353,
      "grad_norm": 0.049926646053791046,
      "learning_rate": 0.00017301914580265096,
      "loss": 0.3056,
      "step": 463
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 0.11792955547571182,
      "learning_rate": 0.00017296023564064802,
      "loss": 0.3385,
      "step": 464
    },
    {
      "epoch": 0.13676470588235295,
      "grad_norm": 0.04991514980792999,
      "learning_rate": 0.00017290132547864508,
      "loss": 0.3578,
      "step": 465
    },
    {
      "epoch": 0.13705882352941176,
      "grad_norm": 0.07346171885728836,
      "learning_rate": 0.00017284241531664214,
      "loss": 0.4517,
      "step": 466
    },
    {
      "epoch": 0.1373529411764706,
      "grad_norm": 0.05691985785961151,
      "learning_rate": 0.0001727835051546392,
      "loss": 0.3876,
      "step": 467
    },
    {
      "epoch": 0.1376470588235294,
      "grad_norm": 0.053480759263038635,
      "learning_rate": 0.00017272459499263624,
      "loss": 0.3981,
      "step": 468
    },
    {
      "epoch": 0.13794117647058823,
      "grad_norm": 0.0681968703866005,
      "learning_rate": 0.0001726656848306333,
      "loss": 0.3489,
      "step": 469
    },
    {
      "epoch": 0.13823529411764707,
      "grad_norm": 0.05734647065401077,
      "learning_rate": 0.00017260677466863036,
      "loss": 0.3242,
      "step": 470
    },
    {
      "epoch": 0.13852941176470587,
      "grad_norm": 0.05011032149195671,
      "learning_rate": 0.00017254786450662742,
      "loss": 0.3068,
      "step": 471
    },
    {
      "epoch": 0.1388235294117647,
      "grad_norm": 0.05899882689118385,
      "learning_rate": 0.00017248895434462448,
      "loss": 0.3522,
      "step": 472
    },
    {
      "epoch": 0.13911764705882354,
      "grad_norm": 0.06030746549367905,
      "learning_rate": 0.0001724300441826215,
      "loss": 0.2981,
      "step": 473
    },
    {
      "epoch": 0.13941176470588235,
      "grad_norm": 0.06484637409448624,
      "learning_rate": 0.00017237113402061857,
      "loss": 0.3812,
      "step": 474
    },
    {
      "epoch": 0.13970588235294118,
      "grad_norm": 0.04798899218440056,
      "learning_rate": 0.00017231222385861563,
      "loss": 0.3134,
      "step": 475
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0365891270339489,
      "learning_rate": 0.0001722533136966127,
      "loss": 0.2699,
      "step": 476
    },
    {
      "epoch": 0.14029411764705882,
      "grad_norm": 0.04788472130894661,
      "learning_rate": 0.00017219440353460975,
      "loss": 0.3992,
      "step": 477
    },
    {
      "epoch": 0.14058823529411765,
      "grad_norm": 0.05779784545302391,
      "learning_rate": 0.00017213549337260678,
      "loss": 0.3754,
      "step": 478
    },
    {
      "epoch": 0.14088235294117646,
      "grad_norm": 0.05887613445520401,
      "learning_rate": 0.00017207658321060384,
      "loss": 0.3312,
      "step": 479
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 0.050651371479034424,
      "learning_rate": 0.0001720176730486009,
      "loss": 0.3383,
      "step": 480
    },
    {
      "epoch": 0.14147058823529413,
      "grad_norm": 0.06277495622634888,
      "learning_rate": 0.00017195876288659796,
      "loss": 0.355,
      "step": 481
    },
    {
      "epoch": 0.14176470588235293,
      "grad_norm": 0.05530104786157608,
      "learning_rate": 0.00017189985272459503,
      "loss": 0.3773,
      "step": 482
    },
    {
      "epoch": 0.14205882352941177,
      "grad_norm": 0.06142742931842804,
      "learning_rate": 0.00017184094256259203,
      "loss": 0.3146,
      "step": 483
    },
    {
      "epoch": 0.1423529411764706,
      "grad_norm": 0.05781945958733559,
      "learning_rate": 0.0001717820324005891,
      "loss": 0.3646,
      "step": 484
    },
    {
      "epoch": 0.1426470588235294,
      "grad_norm": 0.04569896683096886,
      "learning_rate": 0.00017172312223858615,
      "loss": 0.312,
      "step": 485
    },
    {
      "epoch": 0.14294117647058824,
      "grad_norm": 0.06400693207979202,
      "learning_rate": 0.0001716642120765832,
      "loss": 0.4075,
      "step": 486
    },
    {
      "epoch": 0.14323529411764707,
      "grad_norm": 0.05434136092662811,
      "learning_rate": 0.00017160530191458027,
      "loss": 0.3659,
      "step": 487
    },
    {
      "epoch": 0.14352941176470588,
      "grad_norm": 0.049584534019231796,
      "learning_rate": 0.0001715463917525773,
      "loss": 0.2925,
      "step": 488
    },
    {
      "epoch": 0.1438235294117647,
      "grad_norm": 0.06350846588611603,
      "learning_rate": 0.00017148748159057437,
      "loss": 0.3843,
      "step": 489
    },
    {
      "epoch": 0.14411764705882352,
      "grad_norm": 0.07016438245773315,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4143,
      "step": 490
    },
    {
      "epoch": 0.14441176470588235,
      "grad_norm": 0.045964185148477554,
      "learning_rate": 0.00017136966126656849,
      "loss": 0.3264,
      "step": 491
    },
    {
      "epoch": 0.14470588235294118,
      "grad_norm": 0.06972756236791611,
      "learning_rate": 0.00017131075110456555,
      "loss": 0.3941,
      "step": 492
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05731245130300522,
      "learning_rate": 0.00017125184094256258,
      "loss": 0.306,
      "step": 493
    },
    {
      "epoch": 0.14529411764705882,
      "grad_norm": 0.055932413786649704,
      "learning_rate": 0.00017119293078055964,
      "loss": 0.4001,
      "step": 494
    },
    {
      "epoch": 0.14558823529411766,
      "grad_norm": 0.05855575576424599,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.3782,
      "step": 495
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 0.043309926986694336,
      "learning_rate": 0.00017107511045655376,
      "loss": 0.2886,
      "step": 496
    },
    {
      "epoch": 0.1461764705882353,
      "grad_norm": 0.048510197550058365,
      "learning_rate": 0.00017101620029455082,
      "loss": 0.3289,
      "step": 497
    },
    {
      "epoch": 0.14647058823529413,
      "grad_norm": 0.08199948817491531,
      "learning_rate": 0.00017095729013254785,
      "loss": 0.3627,
      "step": 498
    },
    {
      "epoch": 0.14676470588235294,
      "grad_norm": 0.04825655370950699,
      "learning_rate": 0.0001708983799705449,
      "loss": 0.3373,
      "step": 499
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.04910390079021454,
      "learning_rate": 0.00017083946980854197,
      "loss": 0.3566,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0863377971750502e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
