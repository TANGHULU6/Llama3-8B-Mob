{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.5,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.0956664085388184,
      "learning_rate": 4e-05,
      "loss": 1.9448,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.2918241024017334,
      "learning_rate": 8e-05,
      "loss": 1.8955,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.376105308532715,
      "learning_rate": 0.00012,
      "loss": 1.9635,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.942995309829712,
      "learning_rate": 0.00016,
      "loss": 1.7703,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4782493114471436,
      "learning_rate": 0.0002,
      "loss": 1.558,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.251857042312622,
      "learning_rate": 0.00019993322203672788,
      "loss": 1.3528,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.6353691816329956,
      "learning_rate": 0.00019986644407345576,
      "loss": 1.0731,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.6602898836135864,
      "learning_rate": 0.00019979966611018366,
      "loss": 1.2105,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.6666288375854492,
      "learning_rate": 0.00019973288814691153,
      "loss": 0.8511,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 2.097238302230835,
      "learning_rate": 0.0001996661101836394,
      "loss": 0.9601,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.886622428894043,
      "learning_rate": 0.00019959933222036728,
      "loss": 0.6888,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8481279015541077,
      "learning_rate": 0.00019953255425709515,
      "loss": 0.4869,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.9076886177062988,
      "learning_rate": 0.00019946577629382305,
      "loss": 0.5155,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.4742388427257538,
      "learning_rate": 0.00019939899833055092,
      "loss": 0.6427,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.42061689496040344,
      "learning_rate": 0.00019933222036727882,
      "loss": 0.4551,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.49660366773605347,
      "learning_rate": 0.0001992654424040067,
      "loss": 0.6552,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.29572218656539917,
      "learning_rate": 0.00019919866444073457,
      "loss": 0.46,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.20609429478645325,
      "learning_rate": 0.00019913188647746244,
      "loss": 0.4393,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.2833709120750427,
      "learning_rate": 0.00019906510851419034,
      "loss": 0.4794,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3383139669895172,
      "learning_rate": 0.00019899833055091822,
      "loss": 0.5512,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2814450263977051,
      "learning_rate": 0.0001989315525876461,
      "loss": 0.5206,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.18887953460216522,
      "learning_rate": 0.00019886477462437396,
      "loss": 0.4972,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.1577294021844864,
      "learning_rate": 0.00019879799666110183,
      "loss": 0.3929,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.16395266354084015,
      "learning_rate": 0.00019873121869782974,
      "loss": 0.4311,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.19629164040088654,
      "learning_rate": 0.0001986644407345576,
      "loss": 0.4407,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.19796745479106903,
      "learning_rate": 0.00019859766277128548,
      "loss": 0.5199,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.15026581287384033,
      "learning_rate": 0.00019853088480801335,
      "loss": 0.444,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.15527544915676117,
      "learning_rate": 0.00019846410684474123,
      "loss": 0.47,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.12846054136753082,
      "learning_rate": 0.00019839732888146913,
      "loss": 0.3947,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15415337681770325,
      "learning_rate": 0.000198330550918197,
      "loss": 0.4066,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.16547812521457672,
      "learning_rate": 0.00019826377295492487,
      "loss": 0.4745,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10649402439594269,
      "learning_rate": 0.00019819699499165277,
      "loss": 0.4897,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.1004398912191391,
      "learning_rate": 0.00019813021702838065,
      "loss": 0.2804,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.1579684317111969,
      "learning_rate": 0.00019806343906510852,
      "loss": 0.4162,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.14333078265190125,
      "learning_rate": 0.00019799666110183642,
      "loss": 0.3944,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11239796131849289,
      "learning_rate": 0.0001979298831385643,
      "loss": 0.4197,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.11745484918355942,
      "learning_rate": 0.00019786310517529217,
      "loss": 0.3054,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.11401297152042389,
      "learning_rate": 0.00019779632721202004,
      "loss": 0.3476,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.11132606118917465,
      "learning_rate": 0.00019772954924874791,
      "loss": 0.3451,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.10694670677185059,
      "learning_rate": 0.00019766277128547581,
      "loss": 0.325,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.10581845790147781,
      "learning_rate": 0.0001975959933222037,
      "loss": 0.3798,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.09086530655622482,
      "learning_rate": 0.00019752921535893156,
      "loss": 0.3476,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.10241302847862244,
      "learning_rate": 0.00019746243739565943,
      "loss": 0.3672,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09187440574169159,
      "learning_rate": 0.0001973956594323873,
      "loss": 0.4196,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12214738875627518,
      "learning_rate": 0.0001973288814691152,
      "loss": 0.4203,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.11255165189504623,
      "learning_rate": 0.00019726210350584308,
      "loss": 0.3168,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.09649581462144852,
      "learning_rate": 0.00019719532554257095,
      "loss": 0.3994,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08454272150993347,
      "learning_rate": 0.00019712854757929883,
      "loss": 0.2907,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.09278556704521179,
      "learning_rate": 0.00019706176961602673,
      "loss": 0.3851,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.10700076818466187,
      "learning_rate": 0.0001969949916527546,
      "loss": 0.3579,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.08786964416503906,
      "learning_rate": 0.0001969282136894825,
      "loss": 0.3716,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.09527817368507385,
      "learning_rate": 0.00019686143572621037,
      "loss": 0.4149,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.06768197566270828,
      "learning_rate": 0.00019679465776293825,
      "loss": 0.3567,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.11477317661046982,
      "learning_rate": 0.00019672787979966612,
      "loss": 0.4293,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.084235779941082,
      "learning_rate": 0.000196661101836394,
      "loss": 0.3019,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.10784310102462769,
      "learning_rate": 0.0001965943238731219,
      "loss": 0.3491,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.10310035198926926,
      "learning_rate": 0.00019652754590984977,
      "loss": 0.3831,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.1375250220298767,
      "learning_rate": 0.00019646076794657764,
      "loss": 0.4089,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.07553678750991821,
      "learning_rate": 0.0001963939899833055,
      "loss": 0.3023,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0941384807229042,
      "learning_rate": 0.00019632721202003339,
      "loss": 0.3806,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.06856777518987656,
      "learning_rate": 0.00019626043405676129,
      "loss": 0.2628,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.07682006061077118,
      "learning_rate": 0.00019619365609348916,
      "loss": 0.3936,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06746046990156174,
      "learning_rate": 0.00019612687813021703,
      "loss": 0.3716,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07772789150476456,
      "learning_rate": 0.0001960601001669449,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.08953963220119476,
      "learning_rate": 0.00019599332220367278,
      "loss": 0.3442,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.0995715856552124,
      "learning_rate": 0.00019592654424040068,
      "loss": 0.3373,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.07466694712638855,
      "learning_rate": 0.00019585976627712855,
      "loss": 0.3045,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.08509878069162369,
      "learning_rate": 0.00019579298831385645,
      "loss": 0.2943,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.12085497379302979,
      "learning_rate": 0.00019572621035058433,
      "loss": 0.3952,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.10573051869869232,
      "learning_rate": 0.0001956594323873122,
      "loss": 0.3791,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.09017354995012283,
      "learning_rate": 0.00019559265442404007,
      "loss": 0.3193,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0738191232085228,
      "learning_rate": 0.00019552587646076797,
      "loss": 0.3396,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.06006060913205147,
      "learning_rate": 0.00019545909849749584,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.15513241291046143,
      "learning_rate": 0.00019539232053422372,
      "loss": 0.3864,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.07705923914909363,
      "learning_rate": 0.0001953255425709516,
      "loss": 0.3251,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17013779282569885,
      "learning_rate": 0.00019525876460767946,
      "loss": 0.3971,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.08929529041051865,
      "learning_rate": 0.00019519198664440736,
      "loss": 0.3465,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.08618784695863724,
      "learning_rate": 0.00019512520868113524,
      "loss": 0.3251,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.11940111219882965,
      "learning_rate": 0.0001950584307178631,
      "loss": 0.386,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08647278696298599,
      "learning_rate": 0.00019499165275459098,
      "loss": 0.3328,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.08629181981086731,
      "learning_rate": 0.00019492487479131886,
      "loss": 0.3497,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.09361159801483154,
      "learning_rate": 0.00019485809682804673,
      "loss": 0.3479,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.06511078774929047,
      "learning_rate": 0.00019479131886477463,
      "loss": 0.262,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0810856968164444,
      "learning_rate": 0.0001947245409015025,
      "loss": 0.375,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.06852146983146667,
      "learning_rate": 0.0001946577629382304,
      "loss": 0.3839,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.08113353699445724,
      "learning_rate": 0.00019459098497495828,
      "loss": 0.3473,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.07892511785030365,
      "learning_rate": 0.00019452420701168615,
      "loss": 0.2601,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08675333112478256,
      "learning_rate": 0.00019445742904841405,
      "loss": 0.3368,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.07467499375343323,
      "learning_rate": 0.00019439065108514192,
      "loss": 0.3206,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06793464720249176,
      "learning_rate": 0.0001943238731218698,
      "loss": 0.2437,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.16194306313991547,
      "learning_rate": 0.00019425709515859767,
      "loss": 0.3755,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08678337186574936,
      "learning_rate": 0.00019419031719532554,
      "loss": 0.364,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.09742604196071625,
      "learning_rate": 0.00019412353923205344,
      "loss": 0.336,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.12137395888566971,
      "learning_rate": 0.00019405676126878132,
      "loss": 0.3643,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.08253426104784012,
      "learning_rate": 0.0001939899833055092,
      "loss": 0.3209,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.07981440424919128,
      "learning_rate": 0.00019392320534223706,
      "loss": 0.3538,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.10532163828611374,
      "learning_rate": 0.00019385642737896494,
      "loss": 0.3208,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.06535062938928604,
      "learning_rate": 0.0001937896494156928,
      "loss": 0.3372,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.05981944501399994,
      "learning_rate": 0.0001937228714524207,
      "loss": 0.2412,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.09706909954547882,
      "learning_rate": 0.00019365609348914858,
      "loss": 0.4219,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.06982459872961044,
      "learning_rate": 0.00019358931552587646,
      "loss": 0.2833,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.08719934523105621,
      "learning_rate": 0.00019352253756260436,
      "loss": 0.353,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.08656898885965347,
      "learning_rate": 0.00019345575959933223,
      "loss": 0.346,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.06393106281757355,
      "learning_rate": 0.00019338898163606013,
      "loss": 0.2793,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.11401592195034027,
      "learning_rate": 0.000193322203672788,
      "loss": 0.4255,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.08084436506032944,
      "learning_rate": 0.00019325542570951588,
      "loss": 0.3427,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.07798241823911667,
      "learning_rate": 0.00019318864774624375,
      "loss": 0.4689,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.08108892291784286,
      "learning_rate": 0.00019312186978297162,
      "loss": 0.402,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.11977798491716385,
      "learning_rate": 0.00019305509181969952,
      "loss": 0.3084,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.058830082416534424,
      "learning_rate": 0.0001929883138564274,
      "loss": 0.3936,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.07111568003892899,
      "learning_rate": 0.00019292153589315527,
      "loss": 0.3664,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06430325657129288,
      "learning_rate": 0.00019285475792988314,
      "loss": 0.3048,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.06825023144483566,
      "learning_rate": 0.00019278797996661101,
      "loss": 0.3785,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.12661273777484894,
      "learning_rate": 0.0001927212020033389,
      "loss": 0.4042,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08384962379932404,
      "learning_rate": 0.0001926544240400668,
      "loss": 0.3674,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.12418660521507263,
      "learning_rate": 0.00019258764607679466,
      "loss": 0.3506,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.061764538288116455,
      "learning_rate": 0.00019252086811352253,
      "loss": 0.305,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.06392883509397507,
      "learning_rate": 0.0001924540901502504,
      "loss": 0.3497,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.09458301216363907,
      "learning_rate": 0.0001923873121869783,
      "loss": 0.3263,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05542557314038277,
      "learning_rate": 0.00019232053422370618,
      "loss": 0.2906,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.0728297159075737,
      "learning_rate": 0.00019225375626043408,
      "loss": 0.3746,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06852685660123825,
      "learning_rate": 0.00019218697829716195,
      "loss": 0.34,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05804547667503357,
      "learning_rate": 0.00019212020033388983,
      "loss": 0.2878,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06328660249710083,
      "learning_rate": 0.0001920534223706177,
      "loss": 0.3435,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.07714178413152695,
      "learning_rate": 0.0001919866444073456,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.07320096343755722,
      "learning_rate": 0.00019191986644407347,
      "loss": 0.3508,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.07564099133014679,
      "learning_rate": 0.00019185308848080135,
      "loss": 0.3074,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06308815628290176,
      "learning_rate": 0.00019178631051752922,
      "loss": 0.2524,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.07419486343860626,
      "learning_rate": 0.0001917195325542571,
      "loss": 0.3869,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.09122984856367111,
      "learning_rate": 0.00019165275459098497,
      "loss": 0.4091,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09594853967428207,
      "learning_rate": 0.00019158597662771287,
      "loss": 0.4443,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07332075387239456,
      "learning_rate": 0.00019151919866444074,
      "loss": 0.351,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.05169367790222168,
      "learning_rate": 0.0001914524207011686,
      "loss": 0.2951,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05943819880485535,
      "learning_rate": 0.0001913856427378965,
      "loss": 0.3033,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.0833563581109047,
      "learning_rate": 0.0001913188647746244,
      "loss": 0.3152,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05343350023031235,
      "learning_rate": 0.00019125208681135226,
      "loss": 0.2989,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.0843648836016655,
      "learning_rate": 0.00019118530884808016,
      "loss": 0.354,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.056954026222229004,
      "learning_rate": 0.00019111853088480803,
      "loss": 0.3748,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.07683058083057404,
      "learning_rate": 0.0001910517529215359,
      "loss": 0.3557,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.08052007853984833,
      "learning_rate": 0.00019098497495826378,
      "loss": 0.2724,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.07334864139556885,
      "learning_rate": 0.00019091819699499168,
      "loss": 0.3755,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.05416643247008324,
      "learning_rate": 0.00019085141903171955,
      "loss": 0.2737,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.0647912248969078,
      "learning_rate": 0.00019078464106844743,
      "loss": 0.3337,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.08493472635746002,
      "learning_rate": 0.0001907178631051753,
      "loss": 0.3992,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.06978499889373779,
      "learning_rate": 0.00019065108514190317,
      "loss": 0.3452,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.08354634791612625,
      "learning_rate": 0.00019058430717863107,
      "loss": 0.3801,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.08108527958393097,
      "learning_rate": 0.00019051752921535895,
      "loss": 0.3377,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06336899101734161,
      "learning_rate": 0.00019045075125208682,
      "loss": 0.3086,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.09381486475467682,
      "learning_rate": 0.0001903839732888147,
      "loss": 0.3614,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.0781172513961792,
      "learning_rate": 0.00019031719532554257,
      "loss": 0.3664,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.07172472029924393,
      "learning_rate": 0.00019025041736227044,
      "loss": 0.3111,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06654836237430573,
      "learning_rate": 0.00019018363939899834,
      "loss": 0.2931,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09867861866950989,
      "learning_rate": 0.0001901168614357262,
      "loss": 0.4418,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08223884552717209,
      "learning_rate": 0.0001900500834724541,
      "loss": 0.3609,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.0780235156416893,
      "learning_rate": 0.00018998330550918199,
      "loss": 0.3251,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06921236962080002,
      "learning_rate": 0.00018991652754590986,
      "loss": 0.3675,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.06987825036048889,
      "learning_rate": 0.00018984974958263776,
      "loss": 0.296,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.05670154094696045,
      "learning_rate": 0.00018978297161936563,
      "loss": 0.3226,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05686641484498978,
      "learning_rate": 0.0001897161936560935,
      "loss": 0.2558,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.08335665613412857,
      "learning_rate": 0.00018964941569282138,
      "loss": 0.3413,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.061617542058229446,
      "learning_rate": 0.00018958263772954925,
      "loss": 0.3585,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.07397318631410599,
      "learning_rate": 0.00018951585976627715,
      "loss": 0.2825,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.07524124532938004,
      "learning_rate": 0.00018944908180300502,
      "loss": 0.2172,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.09329230338335037,
      "learning_rate": 0.0001893823038397329,
      "loss": 0.365,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.0736825168132782,
      "learning_rate": 0.00018931552587646077,
      "loss": 0.3335,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.06876420229673386,
      "learning_rate": 0.00018924874791318864,
      "loss": 0.3162,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.06728898733854294,
      "learning_rate": 0.00018918196994991652,
      "loss": 0.3171,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06478077173233032,
      "learning_rate": 0.00018911519198664442,
      "loss": 0.2978,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.06718949228525162,
      "learning_rate": 0.0001890484140233723,
      "loss": 0.2701,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.06102819740772247,
      "learning_rate": 0.00018898163606010016,
      "loss": 0.3441,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.0668511912226677,
      "learning_rate": 0.00018891485809682806,
      "loss": 0.3145,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05413564294576645,
      "learning_rate": 0.00018884808013355594,
      "loss": 0.2821,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.051658570766448975,
      "learning_rate": 0.00018878130217028384,
      "loss": 0.2528,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.08169940859079361,
      "learning_rate": 0.0001887145242070117,
      "loss": 0.3672,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.08725954592227936,
      "learning_rate": 0.00018864774624373958,
      "loss": 0.3994,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.06593123078346252,
      "learning_rate": 0.00018858096828046746,
      "loss": 0.281,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.06933767348527908,
      "learning_rate": 0.00018851419031719533,
      "loss": 0.2368,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.10475962609052658,
      "learning_rate": 0.00018844741235392323,
      "loss": 0.3811,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.07346801459789276,
      "learning_rate": 0.0001883806343906511,
      "loss": 0.3234,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10945428907871246,
      "learning_rate": 0.00018831385642737898,
      "loss": 0.4434,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.0850963145494461,
      "learning_rate": 0.00018824707846410685,
      "loss": 0.3818,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06807483732700348,
      "learning_rate": 0.00018818030050083472,
      "loss": 0.3981,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.07143398374319077,
      "learning_rate": 0.0001881135225375626,
      "loss": 0.2976,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.060877878218889236,
      "learning_rate": 0.0001880467445742905,
      "loss": 0.3016,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.05298999696969986,
      "learning_rate": 0.00018797996661101837,
      "loss": 0.242,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.07313723862171173,
      "learning_rate": 0.00018791318864774624,
      "loss": 0.2949,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.06976813077926636,
      "learning_rate": 0.00018784641068447412,
      "loss": 0.3056,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.06937415897846222,
      "learning_rate": 0.00018777963272120202,
      "loss": 0.3172,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.07303337752819061,
      "learning_rate": 0.0001877128547579299,
      "loss": 0.3322,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.06589815765619278,
      "learning_rate": 0.0001876460767946578,
      "loss": 0.2966,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.041662562638521194,
      "learning_rate": 0.00018757929883138566,
      "loss": 0.2511,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05949961021542549,
      "learning_rate": 0.00018751252086811354,
      "loss": 0.3607,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.057940538972616196,
      "learning_rate": 0.0001874457429048414,
      "loss": 0.3539,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.049378618597984314,
      "learning_rate": 0.0001873789649415693,
      "loss": 0.2927,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.053056396543979645,
      "learning_rate": 0.00018731218697829718,
      "loss": 0.3114,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.09249860048294067,
      "learning_rate": 0.00018724540901502506,
      "loss": 0.3164,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.06798094511032104,
      "learning_rate": 0.00018717863105175293,
      "loss": 0.3167,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.0637553483247757,
      "learning_rate": 0.0001871118530884808,
      "loss": 0.39,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06718450784683228,
      "learning_rate": 0.00018704507512520868,
      "loss": 0.3436,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05805756896734238,
      "learning_rate": 0.00018697829716193658,
      "loss": 0.2615,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.066567063331604,
      "learning_rate": 0.00018691151919866445,
      "loss": 0.3377,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.0723097026348114,
      "learning_rate": 0.00018684474123539232,
      "loss": 0.3788,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.084930919110775,
      "learning_rate": 0.0001867779632721202,
      "loss": 0.3318,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.07866982370615005,
      "learning_rate": 0.00018671118530884807,
      "loss": 0.2717,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.07142424583435059,
      "learning_rate": 0.00018664440734557597,
      "loss": 0.2816,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.11116000264883041,
      "learning_rate": 0.00018657762938230384,
      "loss": 0.3934,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.06793380528688431,
      "learning_rate": 0.00018651085141903174,
      "loss": 0.3052,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.067479707300663,
      "learning_rate": 0.00018644407345575962,
      "loss": 0.2836,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.04016229510307312,
      "learning_rate": 0.0001863772954924875,
      "loss": 0.2509,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06527184695005417,
      "learning_rate": 0.0001863105175292154,
      "loss": 0.3379,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.08602338284254074,
      "learning_rate": 0.00018624373956594326,
      "loss": 0.3096,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.08282919973134995,
      "learning_rate": 0.00018617696160267113,
      "loss": 0.376,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.057017698884010315,
      "learning_rate": 0.000186110183639399,
      "loss": 0.254,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06612373888492584,
      "learning_rate": 0.00018604340567612688,
      "loss": 0.2664,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.08113586157560349,
      "learning_rate": 0.00018597662771285475,
      "loss": 0.3139,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.045813411474227905,
      "learning_rate": 0.00018590984974958265,
      "loss": 0.2541,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.06276820600032806,
      "learning_rate": 0.00018584307178631053,
      "loss": 0.3263,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.056134093552827835,
      "learning_rate": 0.0001857762938230384,
      "loss": 0.2932,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06455931812524796,
      "learning_rate": 0.00018570951585976627,
      "loss": 0.2713,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.0641329288482666,
      "learning_rate": 0.00018564273789649415,
      "loss": 0.3262,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.057115621864795685,
      "learning_rate": 0.00018557595993322205,
      "loss": 0.3218,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04558887332677841,
      "learning_rate": 0.00018550918196994992,
      "loss": 0.2999,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.05689835548400879,
      "learning_rate": 0.0001854424040066778,
      "loss": 0.334,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.058476708829402924,
      "learning_rate": 0.0001853756260434057,
      "loss": 0.3557,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.0640471801161766,
      "learning_rate": 0.00018530884808013357,
      "loss": 0.3499,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.06061849743127823,
      "learning_rate": 0.00018524207011686147,
      "loss": 0.2432,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.07924827188253403,
      "learning_rate": 0.00018517529215358934,
      "loss": 0.3285,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09070103615522385,
      "learning_rate": 0.00018510851419031721,
      "loss": 0.4282,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.07663854956626892,
      "learning_rate": 0.0001850417362270451,
      "loss": 0.3036,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.058672137558460236,
      "learning_rate": 0.00018497495826377296,
      "loss": 0.2984,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06622264534235,
      "learning_rate": 0.00018490818030050083,
      "loss": 0.28,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.07077071815729141,
      "learning_rate": 0.00018484140233722873,
      "loss": 0.3022,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.06734973192214966,
      "learning_rate": 0.0001847746243739566,
      "loss": 0.3449,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.08291982859373093,
      "learning_rate": 0.00018470784641068448,
      "loss": 0.4326,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.03683653473854065,
      "learning_rate": 0.00018464106844741235,
      "loss": 0.2626,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.061428993940353394,
      "learning_rate": 0.00018457429048414023,
      "loss": 0.3523,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.07222747057676315,
      "learning_rate": 0.00018450751252086813,
      "loss": 0.3464,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.050284553319215775,
      "learning_rate": 0.000184440734557596,
      "loss": 0.3032,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.05952095612883568,
      "learning_rate": 0.00018437395659432387,
      "loss": 0.277,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07080469280481339,
      "learning_rate": 0.00018430717863105175,
      "loss": 0.2825,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.11495279520750046,
      "learning_rate": 0.00018424040066777965,
      "loss": 0.4424,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06323540955781937,
      "learning_rate": 0.00018417362270450752,
      "loss": 0.3093,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.0691780149936676,
      "learning_rate": 0.00018410684474123542,
      "loss": 0.2963,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.07082061469554901,
      "learning_rate": 0.0001840400667779633,
      "loss": 0.3557,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.06467094272375107,
      "learning_rate": 0.00018397328881469117,
      "loss": 0.2971,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.09653520584106445,
      "learning_rate": 0.00018390651085141904,
      "loss": 0.4259,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.0660548061132431,
      "learning_rate": 0.0001838397328881469,
      "loss": 0.4072,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.05308215692639351,
      "learning_rate": 0.0001837729549248748,
      "loss": 0.3724,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.10216723382472992,
      "learning_rate": 0.00018370617696160269,
      "loss": 0.431,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.08200223743915558,
      "learning_rate": 0.00018363939899833056,
      "loss": 0.3823,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.06216123327612877,
      "learning_rate": 0.00018357262103505843,
      "loss": 0.2717,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.07566752284765244,
      "learning_rate": 0.0001835058430717863,
      "loss": 0.3584,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.09035226702690125,
      "learning_rate": 0.0001834390651085142,
      "loss": 0.3171,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.08981560170650482,
      "learning_rate": 0.00018337228714524208,
      "loss": 0.3525,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.08013089746236801,
      "learning_rate": 0.00018330550918196995,
      "loss": 0.3037,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.06599704176187515,
      "learning_rate": 0.00018323873121869782,
      "loss": 0.2963,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.06157409027218819,
      "learning_rate": 0.0001831719532554257,
      "loss": 0.3132,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.06951967626810074,
      "learning_rate": 0.0001831051752921536,
      "loss": 0.3272,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07502143085002899,
      "learning_rate": 0.00018303839732888147,
      "loss": 0.3704,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.0474037267267704,
      "learning_rate": 0.00018297161936560937,
      "loss": 0.2887,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.05565059185028076,
      "learning_rate": 0.00018290484140233724,
      "loss": 0.3066,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04033610224723816,
      "learning_rate": 0.00018283806343906512,
      "loss": 0.2671,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.08553161472082138,
      "learning_rate": 0.000182771285475793,
      "loss": 0.325,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.06281795352697372,
      "learning_rate": 0.0001827045075125209,
      "loss": 0.3459,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07509568333625793,
      "learning_rate": 0.00018263772954924876,
      "loss": 0.3709,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.061399880796670914,
      "learning_rate": 0.00018257095158597664,
      "loss": 0.39,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.04432804137468338,
      "learning_rate": 0.0001825041736227045,
      "loss": 0.3498,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.044908903539180756,
      "learning_rate": 0.00018243739565943238,
      "loss": 0.3071,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.052189409732818604,
      "learning_rate": 0.00018237061769616028,
      "loss": 0.2782,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.09120634198188782,
      "learning_rate": 0.00018230383973288816,
      "loss": 0.351,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.07506801187992096,
      "learning_rate": 0.00018223706176961603,
      "loss": 0.3482,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10021274536848068,
      "learning_rate": 0.0001821702838063439,
      "loss": 0.4434,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.05500519275665283,
      "learning_rate": 0.00018210350584307178,
      "loss": 0.3526,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.0683271586894989,
      "learning_rate": 0.00018203672787979968,
      "loss": 0.3372,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.10096199810504913,
      "learning_rate": 0.00018196994991652755,
      "loss": 0.3778,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.054836444556713104,
      "learning_rate": 0.00018190317195325542,
      "loss": 0.2848,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.06321289390325546,
      "learning_rate": 0.00018183639398998332,
      "loss": 0.3287,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.047697845846414566,
      "learning_rate": 0.0001817696160267112,
      "loss": 0.3336,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.0485498309135437,
      "learning_rate": 0.0001817028380634391,
      "loss": 0.3112,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06390071660280228,
      "learning_rate": 0.00018163606010016697,
      "loss": 0.2998,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.05406942218542099,
      "learning_rate": 0.00018156928213689484,
      "loss": 0.3118,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05003362149000168,
      "learning_rate": 0.00018150250417362272,
      "loss": 0.3149,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.06629174947738647,
      "learning_rate": 0.0001814357262103506,
      "loss": 0.3656,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05630950629711151,
      "learning_rate": 0.00018136894824707846,
      "loss": 0.3993,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.08491452783346176,
      "learning_rate": 0.00018130217028380636,
      "loss": 0.3745,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.1000320240855217,
      "learning_rate": 0.00018123539232053424,
      "loss": 0.3475,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.088816799223423,
      "learning_rate": 0.0001811686143572621,
      "loss": 0.3918,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.12104816734790802,
      "learning_rate": 0.00018110183639398998,
      "loss": 0.4646,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.059016842395067215,
      "learning_rate": 0.00018103505843071786,
      "loss": 0.3566,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.09891337156295776,
      "learning_rate": 0.00018096828046744576,
      "loss": 0.4484,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.07177264988422394,
      "learning_rate": 0.00018090150250417363,
      "loss": 0.3773,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.06040855124592781,
      "learning_rate": 0.0001808347245409015,
      "loss": 0.3401,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07831306010484695,
      "learning_rate": 0.00018076794657762938,
      "loss": 0.4055,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05135171487927437,
      "learning_rate": 0.00018070116861435728,
      "loss": 0.3338,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.09665145725011826,
      "learning_rate": 0.00018063439065108515,
      "loss": 0.4434,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.061043549329042435,
      "learning_rate": 0.00018056761268781305,
      "loss": 0.4006,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.05078258365392685,
      "learning_rate": 0.00018050083472454092,
      "loss": 0.2619,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.047669392079114914,
      "learning_rate": 0.0001804340567612688,
      "loss": 0.272,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.07371781021356583,
      "learning_rate": 0.00018036727879799667,
      "loss": 0.3549,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.045762427151203156,
      "learning_rate": 0.00018030050083472454,
      "loss": 0.2972,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.0923825204372406,
      "learning_rate": 0.00018023372287145244,
      "loss": 0.3934,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.05545240268111229,
      "learning_rate": 0.00018016694490818031,
      "loss": 0.2954,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.05194549635052681,
      "learning_rate": 0.0001801001669449082,
      "loss": 0.2697,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0787268877029419,
      "learning_rate": 0.00018003338898163606,
      "loss": 0.3735,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.07389024645090103,
      "learning_rate": 0.00017996661101836393,
      "loss": 0.4262,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05247769504785538,
      "learning_rate": 0.00017989983305509183,
      "loss": 0.3109,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.05217419192194939,
      "learning_rate": 0.0001798330550918197,
      "loss": 0.2739,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06178995221853256,
      "learning_rate": 0.00017976627712854758,
      "loss": 0.2783,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.0531550757586956,
      "learning_rate": 0.00017969949916527545,
      "loss": 0.3064,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.05511941760778427,
      "learning_rate": 0.00017963272120200333,
      "loss": 0.3669,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.0790976732969284,
      "learning_rate": 0.00017956594323873123,
      "loss": 0.3963,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09130754321813583,
      "learning_rate": 0.0001794991652754591,
      "loss": 0.3848,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.054158858954906464,
      "learning_rate": 0.000179432387312187,
      "loss": 0.3583,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.08371222764253616,
      "learning_rate": 0.00017936560934891487,
      "loss": 0.3896,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.06208382919430733,
      "learning_rate": 0.00017929883138564275,
      "loss": 0.2482,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07005839049816132,
      "learning_rate": 0.00017923205342237062,
      "loss": 0.412,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.05783788487315178,
      "learning_rate": 0.00017916527545909852,
      "loss": 0.3352,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06575559824705124,
      "learning_rate": 0.0001790984974958264,
      "loss": 0.3964,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.05575578659772873,
      "learning_rate": 0.00017903171953255427,
      "loss": 0.2551,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09290974587202072,
      "learning_rate": 0.00017896494156928214,
      "loss": 0.3649,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.06782729923725128,
      "learning_rate": 0.00017889816360601,
      "loss": 0.2901,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06380290538072586,
      "learning_rate": 0.0001788313856427379,
      "loss": 0.362,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04423273354768753,
      "learning_rate": 0.0001787646076794658,
      "loss": 0.2689,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07054946571588516,
      "learning_rate": 0.00017869782971619366,
      "loss": 0.3748,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.05055522546172142,
      "learning_rate": 0.00017863105175292153,
      "loss": 0.3167,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.05999542027711868,
      "learning_rate": 0.0001785642737896494,
      "loss": 0.3359,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.05833917483687401,
      "learning_rate": 0.0001784974958263773,
      "loss": 0.2835,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.058605678379535675,
      "learning_rate": 0.00017843071786310518,
      "loss": 0.3623,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.060921862721443176,
      "learning_rate": 0.00017836393989983305,
      "loss": 0.3618,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.034921158105134964,
      "learning_rate": 0.00017829716193656095,
      "loss": 0.2091,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.06082122027873993,
      "learning_rate": 0.00017823038397328883,
      "loss": 0.3362,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.04426771029829979,
      "learning_rate": 0.0001781636060100167,
      "loss": 0.3148,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.056465454399585724,
      "learning_rate": 0.0001780968280467446,
      "loss": 0.3649,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08533026278018951,
      "learning_rate": 0.00017803005008347247,
      "loss": 0.3809,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.05331623554229736,
      "learning_rate": 0.00017796327212020035,
      "loss": 0.3329,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.04352397844195366,
      "learning_rate": 0.00017789649415692822,
      "loss": 0.2272,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.04538191854953766,
      "learning_rate": 0.0001778297161936561,
      "loss": 0.3121,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.06431352347135544,
      "learning_rate": 0.000177762938230384,
      "loss": 0.3235,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.05296257883310318,
      "learning_rate": 0.00017769616026711187,
      "loss": 0.2699,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.04389478638768196,
      "learning_rate": 0.00017762938230383974,
      "loss": 0.307,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.0654267817735672,
      "learning_rate": 0.0001775626043405676,
      "loss": 0.3058,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07108623534440994,
      "learning_rate": 0.00017749582637729548,
      "loss": 0.3726,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.06231032684445381,
      "learning_rate": 0.00017742904841402339,
      "loss": 0.348,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.05152330547571182,
      "learning_rate": 0.00017736227045075126,
      "loss": 0.328,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.07389311492443085,
      "learning_rate": 0.00017729549248747913,
      "loss": 0.4102,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.05757777392864227,
      "learning_rate": 0.000177228714524207,
      "loss": 0.2844,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.04819808527827263,
      "learning_rate": 0.0001771619365609349,
      "loss": 0.3189,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11943501979112625,
      "learning_rate": 0.00017709515859766278,
      "loss": 0.3542,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06013776361942291,
      "learning_rate": 0.00017702838063439068,
      "loss": 0.3877,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.05257827043533325,
      "learning_rate": 0.00017696160267111855,
      "loss": 0.3227,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.053413026034832,
      "learning_rate": 0.00017689482470784642,
      "loss": 0.2781,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.04171682894229889,
      "learning_rate": 0.0001768280467445743,
      "loss": 0.2906,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.05988014116883278,
      "learning_rate": 0.00017676126878130217,
      "loss": 0.3826,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.04357519745826721,
      "learning_rate": 0.00017669449081803007,
      "loss": 0.2846,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.07369498163461685,
      "learning_rate": 0.00017662771285475794,
      "loss": 0.3901,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.056979950517416,
      "learning_rate": 0.00017656093489148582,
      "loss": 0.3105,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.04863990843296051,
      "learning_rate": 0.0001764941569282137,
      "loss": 0.2644,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.0584864467382431,
      "learning_rate": 0.00017642737896494156,
      "loss": 0.3412,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.06396812200546265,
      "learning_rate": 0.00017636060100166946,
      "loss": 0.3769,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.048790302127599716,
      "learning_rate": 0.00017629382303839734,
      "loss": 0.3412,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.05513220652937889,
      "learning_rate": 0.0001762270450751252,
      "loss": 0.3053,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.06738697737455368,
      "learning_rate": 0.00017616026711185308,
      "loss": 0.3689,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03774310275912285,
      "learning_rate": 0.00017609348914858096,
      "loss": 0.2866,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.0639033168554306,
      "learning_rate": 0.00017602671118530886,
      "loss": 0.2677,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.08802369236946106,
      "learning_rate": 0.00017595993322203673,
      "loss": 0.417,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.07504976540803909,
      "learning_rate": 0.00017589315525876463,
      "loss": 0.31,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.06366459280252457,
      "learning_rate": 0.0001758263772954925,
      "loss": 0.3457,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.06799014657735825,
      "learning_rate": 0.00017575959933222038,
      "loss": 0.3874,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.061081353574991226,
      "learning_rate": 0.00017569282136894825,
      "loss": 0.3125,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06835927814245224,
      "learning_rate": 0.00017562604340567615,
      "loss": 0.2911,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.06039321795105934,
      "learning_rate": 0.00017555926544240402,
      "loss": 0.3353,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06088876351714134,
      "learning_rate": 0.0001754924874791319,
      "loss": 0.3306,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.06041053682565689,
      "learning_rate": 0.00017542570951585977,
      "loss": 0.3883,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.049776844680309296,
      "learning_rate": 0.00017535893155258764,
      "loss": 0.3883,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.07191675901412964,
      "learning_rate": 0.00017529215358931554,
      "loss": 0.2709,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.08683455735445023,
      "learning_rate": 0.00017522537562604342,
      "loss": 0.3736,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.08970895409584045,
      "learning_rate": 0.0001751585976627713,
      "loss": 0.3928,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.047020550817251205,
      "learning_rate": 0.00017509181969949916,
      "loss": 0.2753,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.09434638172388077,
      "learning_rate": 0.00017502504173622704,
      "loss": 0.3704,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.060877081006765366,
      "learning_rate": 0.0001749582637729549,
      "loss": 0.3033,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.05946670472621918,
      "learning_rate": 0.0001748914858096828,
      "loss": 0.2807,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.05076393485069275,
      "learning_rate": 0.0001748247078464107,
      "loss": 0.2931,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.08851009607315063,
      "learning_rate": 0.00017475792988313858,
      "loss": 0.3605,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10143689811229706,
      "learning_rate": 0.00017469115191986646,
      "loss": 0.4418,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.07623142749071121,
      "learning_rate": 0.00017462437395659433,
      "loss": 0.3911,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.07023727893829346,
      "learning_rate": 0.00017455759599332223,
      "loss": 0.3663,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.06017600744962692,
      "learning_rate": 0.0001744908180300501,
      "loss": 0.3083,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05826139077544212,
      "learning_rate": 0.00017442404006677798,
      "loss": 0.2582,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.06767719238996506,
      "learning_rate": 0.00017435726210350585,
      "loss": 0.3188,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.05285611003637314,
      "learning_rate": 0.00017429048414023372,
      "loss": 0.2998,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.056411128491163254,
      "learning_rate": 0.00017422370617696162,
      "loss": 0.2921,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.061185091733932495,
      "learning_rate": 0.0001741569282136895,
      "loss": 0.2943,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.05641420558094978,
      "learning_rate": 0.00017409015025041737,
      "loss": 0.2759,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.06851478666067123,
      "learning_rate": 0.00017402337228714524,
      "loss": 0.4513,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.0594760999083519,
      "learning_rate": 0.00017395659432387311,
      "loss": 0.379,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06426342576742172,
      "learning_rate": 0.00017388981636060101,
      "loss": 0.3499,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.0700017437338829,
      "learning_rate": 0.0001738230383973289,
      "loss": 0.317,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.04312475770711899,
      "learning_rate": 0.00017375626043405676,
      "loss": 0.2678,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.06582977622747421,
      "learning_rate": 0.00017368948247078466,
      "loss": 0.3406,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.07006170600652695,
      "learning_rate": 0.00017362270450751253,
      "loss": 0.369,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.05010639503598213,
      "learning_rate": 0.0001735559265442404,
      "loss": 0.3638,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.07488889247179031,
      "learning_rate": 0.0001734891485809683,
      "loss": 0.3247,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.05184513330459595,
      "learning_rate": 0.00017342237061769618,
      "loss": 0.2603,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.06638722866773605,
      "learning_rate": 0.00017335559265442405,
      "loss": 0.2854,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.06103762611746788,
      "learning_rate": 0.00017328881469115193,
      "loss": 0.3253,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.05783310532569885,
      "learning_rate": 0.0001732220367278798,
      "loss": 0.289,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.08919013291597366,
      "learning_rate": 0.0001731552587646077,
      "loss": 0.4523,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.08311869204044342,
      "learning_rate": 0.00017308848080133557,
      "loss": 0.2787,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.0692351907491684,
      "learning_rate": 0.00017302170283806345,
      "loss": 0.3734,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.05560993030667305,
      "learning_rate": 0.00017295492487479132,
      "loss": 0.3845,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.04939797893166542,
      "learning_rate": 0.0001728881469115192,
      "loss": 0.2759,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.04753990098834038,
      "learning_rate": 0.0001728213689482471,
      "loss": 0.2949,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.0605107918381691,
      "learning_rate": 0.00017275459098497497,
      "loss": 0.332,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.05432650446891785,
      "learning_rate": 0.00017268781302170284,
      "loss": 0.334,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.0383317731320858,
      "learning_rate": 0.0001726210350584307,
      "loss": 0.1907,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.06635861098766327,
      "learning_rate": 0.0001725542570951586,
      "loss": 0.3594,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.07990048080682755,
      "learning_rate": 0.0001724874791318865,
      "loss": 0.34,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.06931383162736893,
      "learning_rate": 0.0001724207011686144,
      "loss": 0.3743,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.06240297481417656,
      "learning_rate": 0.00017235392320534226,
      "loss": 0.3097,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04172668978571892,
      "learning_rate": 0.00017228714524207013,
      "loss": 0.3016,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.05873313546180725,
      "learning_rate": 0.000172220367278798,
      "loss": 0.4202,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.06333927810192108,
      "learning_rate": 0.00017215358931552588,
      "loss": 0.3504,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.054410386830568314,
      "learning_rate": 0.00017208681135225378,
      "loss": 0.3442,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06687209010124207,
      "learning_rate": 0.00017202003338898165,
      "loss": 0.3851,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.04169590398669243,
      "learning_rate": 0.00017195325542570953,
      "loss": 0.3524,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.05493532493710518,
      "learning_rate": 0.0001718864774624374,
      "loss": 0.2999,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.04102494567632675,
      "learning_rate": 0.00017181969949916527,
      "loss": 0.2589,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.06786254793405533,
      "learning_rate": 0.00017175292153589317,
      "loss": 0.3247,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.05260216444730759,
      "learning_rate": 0.00017168614357262105,
      "loss": 0.3017,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.051911525428295135,
      "learning_rate": 0.00017161936560934892,
      "loss": 0.3347,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.04882310703396797,
      "learning_rate": 0.0001715525876460768,
      "loss": 0.3196,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06122424080967903,
      "learning_rate": 0.00017148580968280467,
      "loss": 0.3887,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.04538717120885849,
      "learning_rate": 0.00017141903171953257,
      "loss": 0.279,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.06093759089708328,
      "learning_rate": 0.00017135225375626044,
      "loss": 0.3215,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05240204557776451,
      "learning_rate": 0.00017128547579298834,
      "loss": 0.3036,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.05205075070261955,
      "learning_rate": 0.0001712186978297162,
      "loss": 0.311,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.05608535557985306,
      "learning_rate": 0.00017115191986644409,
      "loss": 0.331,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.047444626688957214,
      "learning_rate": 0.00017108514190317196,
      "loss": 0.3073,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.06831686943769455,
      "learning_rate": 0.00017101836393989986,
      "loss": 0.3452,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03715347498655319,
      "learning_rate": 0.00017095158597662773,
      "loss": 0.2624,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09170711785554886,
      "learning_rate": 0.0001708848080133556,
      "loss": 0.3888,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.05762215703725815,
      "learning_rate": 0.00017081803005008348,
      "loss": 0.3158,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06637157499790192,
      "learning_rate": 0.00017075125208681135,
      "loss": 0.2453,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04048594832420349,
      "learning_rate": 0.00017068447412353925,
      "loss": 0.2397,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.10212234407663345,
      "learning_rate": 0.00017061769616026712,
      "loss": 0.4159,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06937846541404724,
      "learning_rate": 0.000170550918196995,
      "loss": 0.3396,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.07474826276302338,
      "learning_rate": 0.00017048414023372287,
      "loss": 0.3326,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05824249982833862,
      "learning_rate": 0.00017041736227045074,
      "loss": 0.3379,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.04704830422997475,
      "learning_rate": 0.00017035058430717862,
      "loss": 0.2354,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.06278561055660248,
      "learning_rate": 0.00017028380634390652,
      "loss": 0.3221,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.08273832499980927,
      "learning_rate": 0.0001702170283806344,
      "loss": 0.366,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.0600234791636467,
      "learning_rate": 0.0001701502504173623,
      "loss": 0.2801,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.04539366066455841,
      "learning_rate": 0.00017008347245409016,
      "loss": 0.2435,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06901519000530243,
      "learning_rate": 0.00017001669449081804,
      "loss": 0.3805,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07664025574922562,
      "learning_rate": 0.00016994991652754594,
      "loss": 0.2939,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.046238843351602554,
      "learning_rate": 0.0001698831385642738,
      "loss": 0.2781,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.06035570055246353,
      "learning_rate": 0.00016981636060100168,
      "loss": 0.3907,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.06277912855148315,
      "learning_rate": 0.00016974958263772956,
      "loss": 0.3467,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.044353120028972626,
      "learning_rate": 0.00016968280467445743,
      "loss": 0.2857,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.04181146249175072,
      "learning_rate": 0.00016961602671118533,
      "loss": 0.2875,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.05250684544444084,
      "learning_rate": 0.0001695492487479132,
      "loss": 0.3587,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.05013364553451538,
      "learning_rate": 0.00016948247078464108,
      "loss": 0.2818,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.051002535969018936,
      "learning_rate": 0.00016941569282136895,
      "loss": 0.3078,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.06919902563095093,
      "learning_rate": 0.00016934891485809682,
      "loss": 0.4099,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06153741106390953,
      "learning_rate": 0.0001692821368948247,
      "loss": 0.3219,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.0433584563434124,
      "learning_rate": 0.0001692153589315526,
      "loss": 0.2554,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.041832782328128815,
      "learning_rate": 0.00016914858096828047,
      "loss": 0.2847,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.045837342739105225,
      "learning_rate": 0.00016908180300500834,
      "loss": 0.347,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.07606692612171173,
      "learning_rate": 0.00016901502504173624,
      "loss": 0.3495,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.04513280466198921,
      "learning_rate": 0.00016894824707846412,
      "loss": 0.303,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.039144162088632584,
      "learning_rate": 0.00016888146911519202,
      "loss": 0.2603,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04442967474460602,
      "learning_rate": 0.0001688146911519199,
      "loss": 0.2981,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.04966007545590401,
      "learning_rate": 0.00016874791318864776,
      "loss": 0.3159,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.05852694436907768,
      "learning_rate": 0.00016868113522537564,
      "loss": 0.3716,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.053059473633766174,
      "learning_rate": 0.0001686143572621035,
      "loss": 0.3792,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05099688097834587,
      "learning_rate": 0.0001685475792988314,
      "loss": 0.3137,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.10189834982156754,
      "learning_rate": 0.00016848080133555928,
      "loss": 0.3818,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.0526309497654438,
      "learning_rate": 0.00016841402337228716,
      "loss": 0.2817,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05295218899846077,
      "learning_rate": 0.00016834724540901503,
      "loss": 0.3401,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06662885844707489,
      "learning_rate": 0.0001682804674457429,
      "loss": 0.3991,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.06841499358415604,
      "learning_rate": 0.00016821368948247077,
      "loss": 0.4051,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.0365263968706131,
      "learning_rate": 0.00016814691151919868,
      "loss": 0.2376,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04485613852739334,
      "learning_rate": 0.00016808013355592655,
      "loss": 0.2694,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.06939379125833511,
      "learning_rate": 0.00016801335559265442,
      "loss": 0.3597,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.06019312143325806,
      "learning_rate": 0.0001679465776293823,
      "loss": 0.3509,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05627303943037987,
      "learning_rate": 0.0001678797996661102,
      "loss": 0.32,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.04382266476750374,
      "learning_rate": 0.00016781302170283807,
      "loss": 0.2564,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.056689437478780746,
      "learning_rate": 0.00016774624373956597,
      "loss": 0.2995,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06798779219388962,
      "learning_rate": 0.00016767946577629384,
      "loss": 0.3409,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.04012368619441986,
      "learning_rate": 0.00016761268781302171,
      "loss": 0.2714,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.046074435114860535,
      "learning_rate": 0.0001675459098497496,
      "loss": 0.2982,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.047727275639772415,
      "learning_rate": 0.0001674791318864775,
      "loss": 0.3133,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.04751616716384888,
      "learning_rate": 0.00016741235392320536,
      "loss": 0.2959,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.06545601040124893,
      "learning_rate": 0.00016734557595993323,
      "loss": 0.335,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.05398217961192131,
      "learning_rate": 0.0001672787979966611,
      "loss": 0.3406,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.049862947314977646,
      "learning_rate": 0.00016721202003338898,
      "loss": 0.2785,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.051326289772987366,
      "learning_rate": 0.00016714524207011685,
      "loss": 0.3111,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.06404118984937668,
      "learning_rate": 0.00016707846410684475,
      "loss": 0.3326,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04448550567030907,
      "learning_rate": 0.00016701168614357263,
      "loss": 0.2722,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.04964333772659302,
      "learning_rate": 0.0001669449081803005,
      "loss": 0.2919,
      "step": 500
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.05675473436713219,
      "learning_rate": 0.00016687813021702837,
      "loss": 0.293,
      "step": 501
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.059033967554569244,
      "learning_rate": 0.00016681135225375625,
      "loss": 0.3704,
      "step": 502
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.07478541135787964,
      "learning_rate": 0.00016674457429048415,
      "loss": 0.3317,
      "step": 503
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.05344531685113907,
      "learning_rate": 0.00016667779632721202,
      "loss": 0.2982,
      "step": 504
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.07415473461151123,
      "learning_rate": 0.00016661101836393992,
      "loss": 0.3207,
      "step": 505
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.09510181099176407,
      "learning_rate": 0.0001665442404006678,
      "loss": 0.3774,
      "step": 506
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.03938121721148491,
      "learning_rate": 0.00016647746243739567,
      "loss": 0.2367,
      "step": 507
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.09157609939575195,
      "learning_rate": 0.00016641068447412357,
      "loss": 0.4259,
      "step": 508
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.03961868956685066,
      "learning_rate": 0.00016634390651085144,
      "loss": 0.2935,
      "step": 509
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.04324951022863388,
      "learning_rate": 0.0001662771285475793,
      "loss": 0.2709,
      "step": 510
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 0.0446760393679142,
      "learning_rate": 0.00016621035058430719,
      "loss": 0.2964,
      "step": 511
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.03857719153165817,
      "learning_rate": 0.00016614357262103506,
      "loss": 0.2571,
      "step": 512
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.06435337662696838,
      "learning_rate": 0.00016607679465776293,
      "loss": 0.3262,
      "step": 513
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.06182745471596718,
      "learning_rate": 0.00016601001669449083,
      "loss": 0.3382,
      "step": 514
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.0427301824092865,
      "learning_rate": 0.0001659432387312187,
      "loss": 0.2971,
      "step": 515
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.05086801201105118,
      "learning_rate": 0.00016587646076794658,
      "loss": 0.3126,
      "step": 516
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.06715759634971619,
      "learning_rate": 0.00016580968280467445,
      "loss": 0.3453,
      "step": 517
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.07618245482444763,
      "learning_rate": 0.00016574290484140233,
      "loss": 0.3537,
      "step": 518
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.04840880632400513,
      "learning_rate": 0.00016567612687813023,
      "loss": 0.3,
      "step": 519
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04332226142287254,
      "learning_rate": 0.0001656093489148581,
      "loss": 0.2733,
      "step": 520
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 0.06146964803338051,
      "learning_rate": 0.00016554257095158597,
      "loss": 0.3326,
      "step": 521
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.04399601370096207,
      "learning_rate": 0.00016547579298831387,
      "loss": 0.2826,
      "step": 522
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 0.06812360137701035,
      "learning_rate": 0.00016540901502504175,
      "loss": 0.3761,
      "step": 523
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.05370856449007988,
      "learning_rate": 0.00016534223706176965,
      "loss": 0.3308,
      "step": 524
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.044260215014219284,
      "learning_rate": 0.00016527545909849752,
      "loss": 0.2686,
      "step": 525
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.05853787064552307,
      "learning_rate": 0.0001652086811352254,
      "loss": 0.3793,
      "step": 526
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.0512453131377697,
      "learning_rate": 0.00016514190317195327,
      "loss": 0.2853,
      "step": 527
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.07850892096757889,
      "learning_rate": 0.00016507512520868114,
      "loss": 0.4188,
      "step": 528
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 0.04495372250676155,
      "learning_rate": 0.00016500834724540904,
      "loss": 0.2759,
      "step": 529
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.06586789339780807,
      "learning_rate": 0.0001649415692821369,
      "loss": 0.4153,
      "step": 530
    },
    {
      "epoch": 0.885,
      "grad_norm": 0.056059613823890686,
      "learning_rate": 0.00016487479131886478,
      "loss": 0.3336,
      "step": 531
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.08161654323339462,
      "learning_rate": 0.00016480801335559266,
      "loss": 0.3384,
      "step": 532
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 0.03978919982910156,
      "learning_rate": 0.00016474123539232053,
      "loss": 0.2742,
      "step": 533
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.04851942136883736,
      "learning_rate": 0.0001646744574290484,
      "loss": 0.2634,
      "step": 534
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.05090683326125145,
      "learning_rate": 0.0001646076794657763,
      "loss": 0.3193,
      "step": 535
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.07465936988592148,
      "learning_rate": 0.00016454090150250418,
      "loss": 0.3146,
      "step": 536
    },
    {
      "epoch": 0.895,
      "grad_norm": 0.06789000332355499,
      "learning_rate": 0.00016447412353923205,
      "loss": 0.3492,
      "step": 537
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.10537659376859665,
      "learning_rate": 0.00016440734557595992,
      "loss": 0.426,
      "step": 538
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.053664591163396835,
      "learning_rate": 0.00016434056761268782,
      "loss": 0.3543,
      "step": 539
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.08032732456922531,
      "learning_rate": 0.0001642737896494157,
      "loss": 0.3495,
      "step": 540
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.07147842645645142,
      "learning_rate": 0.0001642070116861436,
      "loss": 0.3444,
      "step": 541
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.04289628565311432,
      "learning_rate": 0.00016414023372287147,
      "loss": 0.2781,
      "step": 542
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.030786065384745598,
      "learning_rate": 0.00016407345575959934,
      "loss": 0.2176,
      "step": 543
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.06521321833133698,
      "learning_rate": 0.00016400667779632722,
      "loss": 0.2598,
      "step": 544
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.055258918553590775,
      "learning_rate": 0.00016393989983305512,
      "loss": 0.2933,
      "step": 545
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.051709070801734924,
      "learning_rate": 0.000163873121869783,
      "loss": 0.2903,
      "step": 546
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 0.05646602064371109,
      "learning_rate": 0.00016380634390651086,
      "loss": 0.3041,
      "step": 547
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.0457528792321682,
      "learning_rate": 0.00016373956594323874,
      "loss": 0.3229,
      "step": 548
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.05021786317229271,
      "learning_rate": 0.0001636727879799666,
      "loss": 0.2456,
      "step": 549
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.06800035387277603,
      "learning_rate": 0.00016360601001669448,
      "loss": 0.3065,
      "step": 550
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.056317973881959915,
      "learning_rate": 0.00016353923205342238,
      "loss": 0.3476,
      "step": 551
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.05203660577535629,
      "learning_rate": 0.00016347245409015026,
      "loss": 0.2806,
      "step": 552
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 0.0463302880525589,
      "learning_rate": 0.00016340567612687813,
      "loss": 0.2319,
      "step": 553
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 0.049490366131067276,
      "learning_rate": 0.000163338898163606,
      "loss": 0.2979,
      "step": 554
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.03928279131650925,
      "learning_rate": 0.00016327212020033388,
      "loss": 0.2375,
      "step": 555
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04550032317638397,
      "learning_rate": 0.00016320534223706178,
      "loss": 0.2814,
      "step": 556
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.08014900237321854,
      "learning_rate": 0.00016313856427378965,
      "loss": 0.3969,
      "step": 557
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.047231823205947876,
      "learning_rate": 0.00016307178631051755,
      "loss": 0.2863,
      "step": 558
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 0.03892596811056137,
      "learning_rate": 0.00016300500834724542,
      "loss": 0.2684,
      "step": 559
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.07393954694271088,
      "learning_rate": 0.0001629382303839733,
      "loss": 0.332,
      "step": 560
    },
    {
      "epoch": 0.935,
      "grad_norm": 0.05307886749505997,
      "learning_rate": 0.0001628714524207012,
      "loss": 0.3283,
      "step": 561
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.04826061800122261,
      "learning_rate": 0.00016280467445742907,
      "loss": 0.2215,
      "step": 562
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.044075049459934235,
      "learning_rate": 0.00016273789649415694,
      "loss": 0.3149,
      "step": 563
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.09960328042507172,
      "learning_rate": 0.00016267111853088482,
      "loss": 0.3516,
      "step": 564
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.051804590970277786,
      "learning_rate": 0.0001626043405676127,
      "loss": 0.3135,
      "step": 565
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.05039425194263458,
      "learning_rate": 0.00016253756260434056,
      "loss": 0.2858,
      "step": 566
    },
    {
      "epoch": 0.945,
      "grad_norm": 0.05222024768590927,
      "learning_rate": 0.00016247078464106846,
      "loss": 0.3118,
      "step": 567
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.04534272849559784,
      "learning_rate": 0.00016240400667779634,
      "loss": 0.258,
      "step": 568
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 0.04662933945655823,
      "learning_rate": 0.0001623372287145242,
      "loss": 0.2613,
      "step": 569
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.0396605059504509,
      "learning_rate": 0.00016227045075125208,
      "loss": 0.277,
      "step": 570
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.06038658693432808,
      "learning_rate": 0.00016220367278797996,
      "loss": 0.3882,
      "step": 571
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.052110228687524796,
      "learning_rate": 0.00016213689482470786,
      "loss": 0.3054,
      "step": 572
    },
    {
      "epoch": 0.955,
      "grad_norm": 0.06867527216672897,
      "learning_rate": 0.00016207011686143573,
      "loss": 0.3896,
      "step": 573
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.03911750018596649,
      "learning_rate": 0.0001620033388981636,
      "loss": 0.2944,
      "step": 574
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.05596785619854927,
      "learning_rate": 0.0001619365609348915,
      "loss": 0.3148,
      "step": 575
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.05006161704659462,
      "learning_rate": 0.00016186978297161938,
      "loss": 0.3285,
      "step": 576
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.04391731321811676,
      "learning_rate": 0.00016180300500834728,
      "loss": 0.3006,
      "step": 577
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.039790429174900055,
      "learning_rate": 0.00016173622704507515,
      "loss": 0.3033,
      "step": 578
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.04407166317105293,
      "learning_rate": 0.00016166944908180302,
      "loss": 0.2928,
      "step": 579
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.05337825044989586,
      "learning_rate": 0.0001616026711185309,
      "loss": 0.269,
      "step": 580
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.04339572414755821,
      "learning_rate": 0.00016153589315525877,
      "loss": 0.3427,
      "step": 581
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.03984799608588219,
      "learning_rate": 0.00016146911519198664,
      "loss": 0.2478,
      "step": 582
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.04794854298233986,
      "learning_rate": 0.00016140233722871454,
      "loss": 0.2847,
      "step": 583
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.046956200152635574,
      "learning_rate": 0.00016133555926544241,
      "loss": 0.3028,
      "step": 584
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.04814032465219498,
      "learning_rate": 0.0001612687813021703,
      "loss": 0.3513,
      "step": 585
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.0567287839949131,
      "learning_rate": 0.00016120200333889816,
      "loss": 0.3163,
      "step": 586
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 0.05966684967279434,
      "learning_rate": 0.00016113522537562603,
      "loss": 0.3096,
      "step": 587
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.038798265159130096,
      "learning_rate": 0.00016106844741235393,
      "loss": 0.2708,
      "step": 588
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.05440191924571991,
      "learning_rate": 0.0001610016694490818,
      "loss": 0.3676,
      "step": 589
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.04472116008400917,
      "learning_rate": 0.00016093489148580968,
      "loss": 0.3371,
      "step": 590
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.03645741194486618,
      "learning_rate": 0.00016086811352253755,
      "loss": 0.2207,
      "step": 591
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08760751783847809,
      "learning_rate": 0.00016080133555926545,
      "loss": 0.3986,
      "step": 592
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.052007872611284256,
      "learning_rate": 0.00016073455759599333,
      "loss": 0.3025,
      "step": 593
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.0768626481294632,
      "learning_rate": 0.00016066777963272123,
      "loss": 0.2732,
      "step": 594
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.06116440147161484,
      "learning_rate": 0.0001606010016694491,
      "loss": 0.3004,
      "step": 595
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.0482056699693203,
      "learning_rate": 0.00016053422370617697,
      "loss": 0.3117,
      "step": 596
    },
    {
      "epoch": 0.995,
      "grad_norm": 0.0809585228562355,
      "learning_rate": 0.00016046744574290485,
      "loss": 0.3999,
      "step": 597
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.06876283884048462,
      "learning_rate": 0.00016040066777963272,
      "loss": 0.3137,
      "step": 598
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.06914673000574112,
      "learning_rate": 0.00016033388981636062,
      "loss": 0.3975,
      "step": 599
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.052446719259023666,
      "learning_rate": 0.0001602671118530885,
      "loss": 0.3224,
      "step": 600
    },
    {
      "epoch": 1.0016666666666667,
      "grad_norm": 0.038091398775577545,
      "learning_rate": 0.00016020033388981637,
      "loss": 0.2446,
      "step": 601
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.07895876467227936,
      "learning_rate": 0.00016013355592654424,
      "loss": 0.3709,
      "step": 602
    },
    {
      "epoch": 1.005,
      "grad_norm": 0.05968007072806358,
      "learning_rate": 0.0001600667779632721,
      "loss": 0.3438,
      "step": 603
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.040867894887924194,
      "learning_rate": 0.00016,
      "loss": 0.2978,
      "step": 604
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.04802035540342331,
      "learning_rate": 0.00015993322203672789,
      "loss": 0.2758,
      "step": 605
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.06557966023683548,
      "learning_rate": 0.00015986644407345576,
      "loss": 0.3414,
      "step": 606
    },
    {
      "epoch": 1.0116666666666667,
      "grad_norm": 0.0787474736571312,
      "learning_rate": 0.00015979966611018363,
      "loss": 0.4171,
      "step": 607
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.06755445897579193,
      "learning_rate": 0.0001597328881469115,
      "loss": 0.3587,
      "step": 608
    },
    {
      "epoch": 1.015,
      "grad_norm": 0.04977213591337204,
      "learning_rate": 0.0001596661101836394,
      "loss": 0.3133,
      "step": 609
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.04914654791355133,
      "learning_rate": 0.00015959933222036728,
      "loss": 0.2566,
      "step": 610
    },
    {
      "epoch": 1.0183333333333333,
      "grad_norm": 0.07955732196569443,
      "learning_rate": 0.00015953255425709518,
      "loss": 0.3409,
      "step": 611
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.05477627366781235,
      "learning_rate": 0.00015946577629382305,
      "loss": 0.3098,
      "step": 612
    },
    {
      "epoch": 1.0216666666666667,
      "grad_norm": 0.058321762830019,
      "learning_rate": 0.00015939899833055093,
      "loss": 0.3374,
      "step": 613
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.061733197420835495,
      "learning_rate": 0.0001593322203672788,
      "loss": 0.3389,
      "step": 614
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.06176535412669182,
      "learning_rate": 0.0001592654424040067,
      "loss": 0.3596,
      "step": 615
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.04223475605249405,
      "learning_rate": 0.00015919866444073457,
      "loss": 0.2665,
      "step": 616
    },
    {
      "epoch": 1.0283333333333333,
      "grad_norm": 0.057717952877283096,
      "learning_rate": 0.00015913188647746245,
      "loss": 0.3805,
      "step": 617
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.05137455463409424,
      "learning_rate": 0.00015906510851419032,
      "loss": 0.3496,
      "step": 618
    },
    {
      "epoch": 1.0316666666666667,
      "grad_norm": 0.059790827333927155,
      "learning_rate": 0.0001589983305509182,
      "loss": 0.3471,
      "step": 619
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.053921446204185486,
      "learning_rate": 0.0001589315525876461,
      "loss": 0.2848,
      "step": 620
    },
    {
      "epoch": 1.035,
      "grad_norm": 0.0980955958366394,
      "learning_rate": 0.00015886477462437397,
      "loss": 0.3987,
      "step": 621
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.04493315517902374,
      "learning_rate": 0.00015879799666110184,
      "loss": 0.2832,
      "step": 622
    },
    {
      "epoch": 1.0383333333333333,
      "grad_norm": 0.04871201887726784,
      "learning_rate": 0.0001587312186978297,
      "loss": 0.2994,
      "step": 623
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.07018174231052399,
      "learning_rate": 0.00015866444073455758,
      "loss": 0.3451,
      "step": 624
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.04685669019818306,
      "learning_rate": 0.00015859766277128548,
      "loss": 0.3145,
      "step": 625
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.05067994445562363,
      "learning_rate": 0.00015853088480801336,
      "loss": 0.2814,
      "step": 626
    },
    {
      "epoch": 1.045,
      "grad_norm": 0.047015778720378876,
      "learning_rate": 0.00015846410684474123,
      "loss": 0.2981,
      "step": 627
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.05327262356877327,
      "learning_rate": 0.00015839732888146913,
      "loss": 0.3616,
      "step": 628
    },
    {
      "epoch": 1.0483333333333333,
      "grad_norm": 0.04395569860935211,
      "learning_rate": 0.000158330550918197,
      "loss": 0.226,
      "step": 629
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.07606248557567596,
      "learning_rate": 0.00015826377295492488,
      "loss": 0.3494,
      "step": 630
    },
    {
      "epoch": 1.0516666666666667,
      "grad_norm": 0.04616096243262291,
      "learning_rate": 0.00015819699499165278,
      "loss": 0.2862,
      "step": 631
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.04835774004459381,
      "learning_rate": 0.00015813021702838065,
      "loss": 0.3222,
      "step": 632
    },
    {
      "epoch": 1.055,
      "grad_norm": 0.042904309928417206,
      "learning_rate": 0.00015806343906510852,
      "loss": 0.2884,
      "step": 633
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.04336833953857422,
      "learning_rate": 0.0001579966611018364,
      "loss": 0.2648,
      "step": 634
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.050038766115903854,
      "learning_rate": 0.00015792988313856427,
      "loss": 0.3104,
      "step": 635
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.034569013863801956,
      "learning_rate": 0.00015786310517529217,
      "loss": 0.2428,
      "step": 636
    },
    {
      "epoch": 1.0616666666666668,
      "grad_norm": 0.059664610773324966,
      "learning_rate": 0.00015779632721202004,
      "loss": 0.3434,
      "step": 637
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.05424325913190842,
      "learning_rate": 0.00015772954924874792,
      "loss": 0.3253,
      "step": 638
    },
    {
      "epoch": 1.065,
      "grad_norm": 0.05082716792821884,
      "learning_rate": 0.0001576627712854758,
      "loss": 0.2674,
      "step": 639
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.06522243469953537,
      "learning_rate": 0.00015759599332220366,
      "loss": 0.333,
      "step": 640
    },
    {
      "epoch": 1.0683333333333334,
      "grad_norm": 0.04958084598183632,
      "learning_rate": 0.00015752921535893156,
      "loss": 0.3349,
      "step": 641
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.047340359538793564,
      "learning_rate": 0.00015746243739565944,
      "loss": 0.2916,
      "step": 642
    },
    {
      "epoch": 1.0716666666666668,
      "grad_norm": 0.03549574688076973,
      "learning_rate": 0.0001573956594323873,
      "loss": 0.231,
      "step": 643
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.05005121976137161,
      "learning_rate": 0.0001573288814691152,
      "loss": 0.265,
      "step": 644
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.042196810245513916,
      "learning_rate": 0.00015726210350584308,
      "loss": 0.2566,
      "step": 645
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.04615529999136925,
      "learning_rate": 0.00015719532554257096,
      "loss": 0.3151,
      "step": 646
    },
    {
      "epoch": 1.0783333333333334,
      "grad_norm": 0.048391491174697876,
      "learning_rate": 0.00015712854757929886,
      "loss": 0.3205,
      "step": 647
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.04352809861302376,
      "learning_rate": 0.00015706176961602673,
      "loss": 0.3041,
      "step": 648
    },
    {
      "epoch": 1.0816666666666666,
      "grad_norm": 0.04534140229225159,
      "learning_rate": 0.0001569949916527546,
      "loss": 0.2781,
      "step": 649
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.057619065046310425,
      "learning_rate": 0.00015692821368948248,
      "loss": 0.3847,
      "step": 650
    },
    {
      "epoch": 1.085,
      "grad_norm": 0.045412901788949966,
      "learning_rate": 0.00015686143572621035,
      "loss": 0.3125,
      "step": 651
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.04086766391992569,
      "learning_rate": 0.00015679465776293825,
      "loss": 0.2781,
      "step": 652
    },
    {
      "epoch": 1.0883333333333334,
      "grad_norm": 0.042553484439849854,
      "learning_rate": 0.00015672787979966612,
      "loss": 0.31,
      "step": 653
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.07585950940847397,
      "learning_rate": 0.000156661101836394,
      "loss": 0.2926,
      "step": 654
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.050615083426237106,
      "learning_rate": 0.00015659432387312187,
      "loss": 0.285,
      "step": 655
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.049835097044706345,
      "learning_rate": 0.00015652754590984974,
      "loss": 0.2963,
      "step": 656
    },
    {
      "epoch": 1.095,
      "grad_norm": 0.06484062224626541,
      "learning_rate": 0.00015646076794657764,
      "loss": 0.3872,
      "step": 657
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.0384904108941555,
      "learning_rate": 0.00015639398998330552,
      "loss": 0.2242,
      "step": 658
    },
    {
      "epoch": 1.0983333333333334,
      "grad_norm": 0.04326695203781128,
      "learning_rate": 0.0001563272120200334,
      "loss": 0.3243,
      "step": 659
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.05285787582397461,
      "learning_rate": 0.00015626043405676126,
      "loss": 0.3104,
      "step": 660
    },
    {
      "epoch": 1.1016666666666666,
      "grad_norm": 0.04762762039899826,
      "learning_rate": 0.00015619365609348916,
      "loss": 0.2879,
      "step": 661
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.04077906161546707,
      "learning_rate": 0.00015612687813021704,
      "loss": 0.262,
      "step": 662
    },
    {
      "epoch": 1.105,
      "grad_norm": 0.05733682960271835,
      "learning_rate": 0.00015606010016694494,
      "loss": 0.351,
      "step": 663
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.04401100054383278,
      "learning_rate": 0.0001559933222036728,
      "loss": 0.3192,
      "step": 664
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.04534782096743584,
      "learning_rate": 0.00015592654424040068,
      "loss": 0.2772,
      "step": 665
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.04895781725645065,
      "learning_rate": 0.00015585976627712856,
      "loss": 0.313,
      "step": 666
    },
    {
      "epoch": 1.1116666666666666,
      "grad_norm": 0.062344253063201904,
      "learning_rate": 0.00015579298831385643,
      "loss": 0.3909,
      "step": 667
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.03933556750416756,
      "learning_rate": 0.00015572621035058433,
      "loss": 0.2868,
      "step": 668
    },
    {
      "epoch": 1.115,
      "grad_norm": 0.07237284630537033,
      "learning_rate": 0.0001556594323873122,
      "loss": 0.3845,
      "step": 669
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.06220970302820206,
      "learning_rate": 0.00015559265442404007,
      "loss": 0.386,
      "step": 670
    },
    {
      "epoch": 1.1183333333333334,
      "grad_norm": 0.044580187648534775,
      "learning_rate": 0.00015552587646076795,
      "loss": 0.2842,
      "step": 671
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.045256610959768295,
      "learning_rate": 0.00015545909849749582,
      "loss": 0.3183,
      "step": 672
    },
    {
      "epoch": 1.1216666666666666,
      "grad_norm": 0.04900319501757622,
      "learning_rate": 0.00015539232053422372,
      "loss": 0.3716,
      "step": 673
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.044318221509456635,
      "learning_rate": 0.0001553255425709516,
      "loss": 0.3414,
      "step": 674
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.05138834938406944,
      "learning_rate": 0.00015525876460767947,
      "loss": 0.2722,
      "step": 675
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.05407276377081871,
      "learning_rate": 0.00015519198664440734,
      "loss": 0.3588,
      "step": 676
    },
    {
      "epoch": 1.1283333333333334,
      "grad_norm": 0.03867404907941818,
      "learning_rate": 0.00015512520868113521,
      "loss": 0.2555,
      "step": 677
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.03920826315879822,
      "learning_rate": 0.00015505843071786311,
      "loss": 0.267,
      "step": 678
    },
    {
      "epoch": 1.1316666666666666,
      "grad_norm": 0.04791778698563576,
      "learning_rate": 0.000154991652754591,
      "loss": 0.3647,
      "step": 679
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.03733225166797638,
      "learning_rate": 0.0001549248747913189,
      "loss": 0.2255,
      "step": 680
    },
    {
      "epoch": 1.135,
      "grad_norm": 0.07614464312791824,
      "learning_rate": 0.00015485809682804676,
      "loss": 0.3266,
      "step": 681
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.04570731520652771,
      "learning_rate": 0.00015479131886477463,
      "loss": 0.2948,
      "step": 682
    },
    {
      "epoch": 1.1383333333333334,
      "grad_norm": 0.042178552597761154,
      "learning_rate": 0.0001547245409015025,
      "loss": 0.2722,
      "step": 683
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.05690876021981239,
      "learning_rate": 0.0001546577629382304,
      "loss": 0.3318,
      "step": 684
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.05208662152290344,
      "learning_rate": 0.00015459098497495828,
      "loss": 0.307,
      "step": 685
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 0.06061986833810806,
      "learning_rate": 0.00015452420701168615,
      "loss": 0.3622,
      "step": 686
    },
    {
      "epoch": 1.145,
      "grad_norm": 0.040480148047208786,
      "learning_rate": 0.00015445742904841403,
      "loss": 0.2865,
      "step": 687
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.04853244498372078,
      "learning_rate": 0.0001543906510851419,
      "loss": 0.3098,
      "step": 688
    },
    {
      "epoch": 1.1483333333333334,
      "grad_norm": 0.04154907166957855,
      "learning_rate": 0.0001543238731218698,
      "loss": 0.2638,
      "step": 689
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.06354950368404388,
      "learning_rate": 0.00015425709515859767,
      "loss": 0.3932,
      "step": 690
    },
    {
      "epoch": 1.1516666666666666,
      "grad_norm": 0.044019024819135666,
      "learning_rate": 0.00015419031719532555,
      "loss": 0.2719,
      "step": 691
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.0442047156393528,
      "learning_rate": 0.00015412353923205342,
      "loss": 0.268,
      "step": 692
    },
    {
      "epoch": 1.155,
      "grad_norm": 0.05394209176301956,
      "learning_rate": 0.0001540567612687813,
      "loss": 0.358,
      "step": 693
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.03976394608616829,
      "learning_rate": 0.0001539899833055092,
      "loss": 0.2444,
      "step": 694
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.041658367961645126,
      "learning_rate": 0.00015392320534223707,
      "loss": 0.2377,
      "step": 695
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.050059638917446136,
      "learning_rate": 0.00015385642737896494,
      "loss": 0.3108,
      "step": 696
    },
    {
      "epoch": 1.1616666666666666,
      "grad_norm": 0.049862418323755264,
      "learning_rate": 0.00015378964941569284,
      "loss": 0.3045,
      "step": 697
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.04192636162042618,
      "learning_rate": 0.0001537228714524207,
      "loss": 0.3207,
      "step": 698
    },
    {
      "epoch": 1.165,
      "grad_norm": 0.03869447484612465,
      "learning_rate": 0.00015365609348914859,
      "loss": 0.2999,
      "step": 699
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.05222214758396149,
      "learning_rate": 0.00015358931552587649,
      "loss": 0.2939,
      "step": 700
    },
    {
      "epoch": 1.1683333333333334,
      "grad_norm": 0.04712746664881706,
      "learning_rate": 0.00015352253756260436,
      "loss": 0.3363,
      "step": 701
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.04344676062464714,
      "learning_rate": 0.00015345575959933223,
      "loss": 0.2947,
      "step": 702
    },
    {
      "epoch": 1.1716666666666666,
      "grad_norm": 0.046429477632045746,
      "learning_rate": 0.0001533889816360601,
      "loss": 0.2927,
      "step": 703
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.044384684413671494,
      "learning_rate": 0.00015332220367278798,
      "loss": 0.2591,
      "step": 704
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.06318604201078415,
      "learning_rate": 0.00015325542570951588,
      "loss": 0.337,
      "step": 705
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.053392697125673294,
      "learning_rate": 0.00015318864774624375,
      "loss": 0.3509,
      "step": 706
    },
    {
      "epoch": 1.1783333333333332,
      "grad_norm": 0.03701367974281311,
      "learning_rate": 0.00015312186978297163,
      "loss": 0.2529,
      "step": 707
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.05953879654407501,
      "learning_rate": 0.0001530550918196995,
      "loss": 0.3747,
      "step": 708
    },
    {
      "epoch": 1.1816666666666666,
      "grad_norm": 0.058472611010074615,
      "learning_rate": 0.00015298831385642737,
      "loss": 0.3097,
      "step": 709
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.07187087833881378,
      "learning_rate": 0.00015292153589315527,
      "loss": 0.3592,
      "step": 710
    },
    {
      "epoch": 1.185,
      "grad_norm": 0.053101684898138046,
      "learning_rate": 0.00015285475792988315,
      "loss": 0.3345,
      "step": 711
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.050619129091501236,
      "learning_rate": 0.00015278797996661102,
      "loss": 0.3245,
      "step": 712
    },
    {
      "epoch": 1.1883333333333332,
      "grad_norm": 0.050112009048461914,
      "learning_rate": 0.0001527212020033389,
      "loss": 0.312,
      "step": 713
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.04445197060704231,
      "learning_rate": 0.0001526544240400668,
      "loss": 0.3328,
      "step": 714
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.04975873976945877,
      "learning_rate": 0.00015258764607679466,
      "loss": 0.3136,
      "step": 715
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.054833248257637024,
      "learning_rate": 0.00015252086811352257,
      "loss": 0.3345,
      "step": 716
    },
    {
      "epoch": 1.195,
      "grad_norm": 0.06073689088225365,
      "learning_rate": 0.00015245409015025044,
      "loss": 0.3557,
      "step": 717
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.048695940524339676,
      "learning_rate": 0.0001523873121869783,
      "loss": 0.3164,
      "step": 718
    },
    {
      "epoch": 1.1983333333333333,
      "grad_norm": 0.040197037160396576,
      "learning_rate": 0.00015232053422370618,
      "loss": 0.3443,
      "step": 719
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.0488058477640152,
      "learning_rate": 0.00015225375626043406,
      "loss": 0.3092,
      "step": 720
    },
    {
      "epoch": 1.2016666666666667,
      "grad_norm": 0.05434321239590645,
      "learning_rate": 0.00015218697829716196,
      "loss": 0.3358,
      "step": 721
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.057552069425582886,
      "learning_rate": 0.00015212020033388983,
      "loss": 0.3508,
      "step": 722
    },
    {
      "epoch": 1.205,
      "grad_norm": 0.06062345579266548,
      "learning_rate": 0.0001520534223706177,
      "loss": 0.3614,
      "step": 723
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.03837205469608307,
      "learning_rate": 0.00015198664440734558,
      "loss": 0.2514,
      "step": 724
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.05871514976024628,
      "learning_rate": 0.00015191986644407345,
      "loss": 0.3135,
      "step": 725
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.034936025738716125,
      "learning_rate": 0.00015185308848080135,
      "loss": 0.296,
      "step": 726
    },
    {
      "epoch": 1.2116666666666667,
      "grad_norm": 0.03742704913020134,
      "learning_rate": 0.00015178631051752922,
      "loss": 0.2351,
      "step": 727
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.07162391394376755,
      "learning_rate": 0.0001517195325542571,
      "loss": 0.3195,
      "step": 728
    },
    {
      "epoch": 1.215,
      "grad_norm": 0.043744172900915146,
      "learning_rate": 0.00015165275459098497,
      "loss": 0.2899,
      "step": 729
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.03992181643843651,
      "learning_rate": 0.00015158597662771284,
      "loss": 0.2675,
      "step": 730
    },
    {
      "epoch": 1.2183333333333333,
      "grad_norm": 0.05414244160056114,
      "learning_rate": 0.00015151919866444074,
      "loss": 0.3045,
      "step": 731
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.038938771933317184,
      "learning_rate": 0.00015145242070116862,
      "loss": 0.2826,
      "step": 732
    },
    {
      "epoch": 1.2216666666666667,
      "grad_norm": 0.06355774402618408,
      "learning_rate": 0.00015138564273789652,
      "loss": 0.3749,
      "step": 733
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.047413140535354614,
      "learning_rate": 0.0001513188647746244,
      "loss": 0.3002,
      "step": 734
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.04913008213043213,
      "learning_rate": 0.00015125208681135226,
      "loss": 0.3374,
      "step": 735
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.05188795179128647,
      "learning_rate": 0.00015118530884808014,
      "loss": 0.3244,
      "step": 736
    },
    {
      "epoch": 1.2283333333333333,
      "grad_norm": 0.06035169959068298,
      "learning_rate": 0.00015111853088480804,
      "loss": 0.3113,
      "step": 737
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.06492582708597183,
      "learning_rate": 0.0001510517529215359,
      "loss": 0.3551,
      "step": 738
    },
    {
      "epoch": 1.2316666666666667,
      "grad_norm": 0.0496777780354023,
      "learning_rate": 0.00015098497495826378,
      "loss": 0.2973,
      "step": 739
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.08312207460403442,
      "learning_rate": 0.00015091819699499166,
      "loss": 0.3171,
      "step": 740
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 0.055813297629356384,
      "learning_rate": 0.00015085141903171953,
      "loss": 0.3916,
      "step": 741
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 0.040186915546655655,
      "learning_rate": 0.00015078464106844743,
      "loss": 0.2706,
      "step": 742
    },
    {
      "epoch": 1.2383333333333333,
      "grad_norm": 0.05272645130753517,
      "learning_rate": 0.0001507178631051753,
      "loss": 0.3069,
      "step": 743
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.052250053733587265,
      "learning_rate": 0.00015065108514190318,
      "loss": 0.3607,
      "step": 744
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.059457927942276,
      "learning_rate": 0.00015058430717863105,
      "loss": 0.3299,
      "step": 745
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.04478736221790314,
      "learning_rate": 0.00015051752921535892,
      "loss": 0.317,
      "step": 746
    },
    {
      "epoch": 1.245,
      "grad_norm": 0.05439458787441254,
      "learning_rate": 0.0001504507512520868,
      "loss": 0.3492,
      "step": 747
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.04258972033858299,
      "learning_rate": 0.0001503839732888147,
      "loss": 0.2921,
      "step": 748
    },
    {
      "epoch": 1.2483333333333333,
      "grad_norm": 0.041337139904499054,
      "learning_rate": 0.00015031719532554257,
      "loss": 0.2635,
      "step": 749
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.08207235485315323,
      "learning_rate": 0.00015025041736227047,
      "loss": 0.4193,
      "step": 750
    },
    {
      "epoch": 1.2516666666666667,
      "grad_norm": 0.049493446946144104,
      "learning_rate": 0.00015018363939899834,
      "loss": 0.3559,
      "step": 751
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.04776306077837944,
      "learning_rate": 0.00015011686143572622,
      "loss": 0.3486,
      "step": 752
    },
    {
      "epoch": 1.255,
      "grad_norm": 0.04559142887592316,
      "learning_rate": 0.00015005008347245412,
      "loss": 0.2761,
      "step": 753
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.04893052950501442,
      "learning_rate": 0.000149983305509182,
      "loss": 0.3207,
      "step": 754
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.04281974956393242,
      "learning_rate": 0.00014991652754590986,
      "loss": 0.3255,
      "step": 755
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.04556006193161011,
      "learning_rate": 0.00014984974958263774,
      "loss": 0.2962,
      "step": 756
    },
    {
      "epoch": 1.2616666666666667,
      "grad_norm": 0.041101813316345215,
      "learning_rate": 0.0001497829716193656,
      "loss": 0.3404,
      "step": 757
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.052992042154073715,
      "learning_rate": 0.0001497161936560935,
      "loss": 0.3587,
      "step": 758
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 0.08488159626722336,
      "learning_rate": 0.00014964941569282138,
      "loss": 0.3768,
      "step": 759
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.05371224135160446,
      "learning_rate": 0.00014958263772954926,
      "loss": 0.3184,
      "step": 760
    },
    {
      "epoch": 1.2683333333333333,
      "grad_norm": 0.04836156219244003,
      "learning_rate": 0.00014951585976627713,
      "loss": 0.3076,
      "step": 761
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.04885965213179588,
      "learning_rate": 0.000149449081803005,
      "loss": 0.3324,
      "step": 762
    },
    {
      "epoch": 1.2716666666666667,
      "grad_norm": 0.04085482284426689,
      "learning_rate": 0.00014938230383973287,
      "loss": 0.3045,
      "step": 763
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.03918344900012016,
      "learning_rate": 0.00014931552587646077,
      "loss": 0.2725,
      "step": 764
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.053932107985019684,
      "learning_rate": 0.00014924874791318865,
      "loss": 0.3445,
      "step": 765
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.05748683959245682,
      "learning_rate": 0.00014918196994991652,
      "loss": 0.3655,
      "step": 766
    },
    {
      "epoch": 1.2783333333333333,
      "grad_norm": 0.03813568502664566,
      "learning_rate": 0.00014911519198664442,
      "loss": 0.2512,
      "step": 767
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.04042002931237221,
      "learning_rate": 0.0001490484140233723,
      "loss": 0.3028,
      "step": 768
    },
    {
      "epoch": 1.2816666666666667,
      "grad_norm": 0.036622658371925354,
      "learning_rate": 0.0001489816360601002,
      "loss": 0.2734,
      "step": 769
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.0436050221323967,
      "learning_rate": 0.00014891485809682807,
      "loss": 0.2971,
      "step": 770
    },
    {
      "epoch": 1.285,
      "grad_norm": 0.04095073044300079,
      "learning_rate": 0.00014884808013355594,
      "loss": 0.3093,
      "step": 771
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.054695144295692444,
      "learning_rate": 0.00014878130217028381,
      "loss": 0.3601,
      "step": 772
    },
    {
      "epoch": 1.2883333333333333,
      "grad_norm": 0.043455757200717926,
      "learning_rate": 0.0001487145242070117,
      "loss": 0.3178,
      "step": 773
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.04684720188379288,
      "learning_rate": 0.0001486477462437396,
      "loss": 0.2841,
      "step": 774
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.04243561998009682,
      "learning_rate": 0.00014858096828046746,
      "loss": 0.289,
      "step": 775
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.06325976550579071,
      "learning_rate": 0.00014851419031719533,
      "loss": 0.4244,
      "step": 776
    },
    {
      "epoch": 1.295,
      "grad_norm": 0.054326944053173065,
      "learning_rate": 0.0001484474123539232,
      "loss": 0.3071,
      "step": 777
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.04747593775391579,
      "learning_rate": 0.00014838063439065108,
      "loss": 0.366,
      "step": 778
    },
    {
      "epoch": 1.2983333333333333,
      "grad_norm": 0.04907405003905296,
      "learning_rate": 0.00014831385642737895,
      "loss": 0.3028,
      "step": 779
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.05792691558599472,
      "learning_rate": 0.00014824707846410685,
      "loss": 0.4048,
      "step": 780
    },
    {
      "epoch": 1.3016666666666667,
      "grad_norm": 0.05227367952466011,
      "learning_rate": 0.00014818030050083473,
      "loss": 0.3683,
      "step": 781
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.06667546182870865,
      "learning_rate": 0.0001481135225375626,
      "loss": 0.3757,
      "step": 782
    },
    {
      "epoch": 1.305,
      "grad_norm": 0.08497359603643417,
      "learning_rate": 0.00014804674457429047,
      "loss": 0.3357,
      "step": 783
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.046052612364292145,
      "learning_rate": 0.00014797996661101837,
      "loss": 0.231,
      "step": 784
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.05356629565358162,
      "learning_rate": 0.00014791318864774625,
      "loss": 0.3138,
      "step": 785
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.05236159265041351,
      "learning_rate": 0.00014784641068447415,
      "loss": 0.2895,
      "step": 786
    },
    {
      "epoch": 1.3116666666666665,
      "grad_norm": 0.04707653820514679,
      "learning_rate": 0.00014777963272120202,
      "loss": 0.2847,
      "step": 787
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.05783150717616081,
      "learning_rate": 0.0001477128547579299,
      "loss": 0.31,
      "step": 788
    },
    {
      "epoch": 1.315,
      "grad_norm": 0.05115664750337601,
      "learning_rate": 0.00014764607679465777,
      "loss": 0.3265,
      "step": 789
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.03781493380665779,
      "learning_rate": 0.00014757929883138567,
      "loss": 0.2625,
      "step": 790
    },
    {
      "epoch": 1.3183333333333334,
      "grad_norm": 0.06874945014715195,
      "learning_rate": 0.00014751252086811354,
      "loss": 0.3726,
      "step": 791
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.04975422844290733,
      "learning_rate": 0.0001474457429048414,
      "loss": 0.3299,
      "step": 792
    },
    {
      "epoch": 1.3216666666666668,
      "grad_norm": 0.03655277565121651,
      "learning_rate": 0.00014737896494156929,
      "loss": 0.2485,
      "step": 793
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.041052982211112976,
      "learning_rate": 0.00014731218697829716,
      "loss": 0.2951,
      "step": 794
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.08416272699832916,
      "learning_rate": 0.00014724540901502506,
      "loss": 0.3598,
      "step": 795
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.04242365062236786,
      "learning_rate": 0.00014717863105175293,
      "loss": 0.278,
      "step": 796
    },
    {
      "epoch": 1.3283333333333334,
      "grad_norm": 0.05480702593922615,
      "learning_rate": 0.0001471118530884808,
      "loss": 0.3561,
      "step": 797
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.07098352164030075,
      "learning_rate": 0.00014704507512520868,
      "loss": 0.3212,
      "step": 798
    },
    {
      "epoch": 1.3316666666666666,
      "grad_norm": 0.0520385205745697,
      "learning_rate": 0.00014697829716193655,
      "loss": 0.3883,
      "step": 799
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.038162343204021454,
      "learning_rate": 0.00014691151919866443,
      "loss": 0.2688,
      "step": 800
    },
    {
      "epoch": 1.335,
      "grad_norm": 0.05466436967253685,
      "learning_rate": 0.00014684474123539233,
      "loss": 0.3429,
      "step": 801
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.0466240718960762,
      "learning_rate": 0.0001467779632721202,
      "loss": 0.2699,
      "step": 802
    },
    {
      "epoch": 1.3383333333333334,
      "grad_norm": 0.04729887843132019,
      "learning_rate": 0.0001467111853088481,
      "loss": 0.3133,
      "step": 803
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.07128466665744781,
      "learning_rate": 0.00014664440734557597,
      "loss": 0.2994,
      "step": 804
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.04227706417441368,
      "learning_rate": 0.00014657762938230385,
      "loss": 0.2985,
      "step": 805
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.07957490533590317,
      "learning_rate": 0.00014651085141903175,
      "loss": 0.3257,
      "step": 806
    },
    {
      "epoch": 1.345,
      "grad_norm": 0.06762976944446564,
      "learning_rate": 0.00014644407345575962,
      "loss": 0.3764,
      "step": 807
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.061814580112695694,
      "learning_rate": 0.0001463772954924875,
      "loss": 0.3156,
      "step": 808
    },
    {
      "epoch": 1.3483333333333334,
      "grad_norm": 0.039647918194532394,
      "learning_rate": 0.00014631051752921536,
      "loss": 0.2927,
      "step": 809
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.042904578149318695,
      "learning_rate": 0.00014624373956594324,
      "loss": 0.3298,
      "step": 810
    },
    {
      "epoch": 1.3516666666666666,
      "grad_norm": 0.05725293606519699,
      "learning_rate": 0.00014617696160267114,
      "loss": 0.3172,
      "step": 811
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.05383605509996414,
      "learning_rate": 0.000146110183639399,
      "loss": 0.3196,
      "step": 812
    },
    {
      "epoch": 1.355,
      "grad_norm": 0.03727524355053902,
      "learning_rate": 0.00014604340567612688,
      "loss": 0.294,
      "step": 813
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 0.04082087054848671,
      "learning_rate": 0.00014597662771285476,
      "loss": 0.3092,
      "step": 814
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.053316593170166016,
      "learning_rate": 0.00014590984974958263,
      "loss": 0.3455,
      "step": 815
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.04215686023235321,
      "learning_rate": 0.0001458430717863105,
      "loss": 0.2891,
      "step": 816
    },
    {
      "epoch": 1.3616666666666668,
      "grad_norm": 0.04883541166782379,
      "learning_rate": 0.0001457762938230384,
      "loss": 0.3054,
      "step": 817
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.04452184960246086,
      "learning_rate": 0.00014570951585976628,
      "loss": 0.3251,
      "step": 818
    },
    {
      "epoch": 1.365,
      "grad_norm": 0.036817487329244614,
      "learning_rate": 0.00014564273789649415,
      "loss": 0.2761,
      "step": 819
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.05689091607928276,
      "learning_rate": 0.00014557595993322205,
      "loss": 0.3213,
      "step": 820
    },
    {
      "epoch": 1.3683333333333334,
      "grad_norm": 0.057630062103271484,
      "learning_rate": 0.00014550918196994992,
      "loss": 0.3112,
      "step": 821
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.04787573218345642,
      "learning_rate": 0.00014544240400667782,
      "loss": 0.3238,
      "step": 822
    },
    {
      "epoch": 1.3716666666666666,
      "grad_norm": 0.04322456568479538,
      "learning_rate": 0.0001453756260434057,
      "loss": 0.2784,
      "step": 823
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.050416603684425354,
      "learning_rate": 0.00014530884808013357,
      "loss": 0.317,
      "step": 824
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.07771429419517517,
      "learning_rate": 0.00014524207011686144,
      "loss": 0.3936,
      "step": 825
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.04893088340759277,
      "learning_rate": 0.00014517529215358932,
      "loss": 0.3228,
      "step": 826
    },
    {
      "epoch": 1.3783333333333334,
      "grad_norm": 0.0668465793132782,
      "learning_rate": 0.00014510851419031722,
      "loss": 0.3677,
      "step": 827
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.05439887195825577,
      "learning_rate": 0.0001450417362270451,
      "loss": 0.3399,
      "step": 828
    },
    {
      "epoch": 1.3816666666666666,
      "grad_norm": 0.04045936465263367,
      "learning_rate": 0.00014497495826377296,
      "loss": 0.2706,
      "step": 829
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.034796737134456635,
      "learning_rate": 0.00014490818030050084,
      "loss": 0.2297,
      "step": 830
    },
    {
      "epoch": 1.385,
      "grad_norm": 0.07324884831905365,
      "learning_rate": 0.0001448414023372287,
      "loss": 0.422,
      "step": 831
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.04674148187041283,
      "learning_rate": 0.00014477462437395658,
      "loss": 0.2802,
      "step": 832
    },
    {
      "epoch": 1.3883333333333332,
      "grad_norm": 0.03933430835604668,
      "learning_rate": 0.00014470784641068448,
      "loss": 0.2456,
      "step": 833
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.06884127855300903,
      "learning_rate": 0.00014464106844741236,
      "loss": 0.382,
      "step": 834
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.04725079983472824,
      "learning_rate": 0.00014457429048414023,
      "loss": 0.2561,
      "step": 835
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.039300739765167236,
      "learning_rate": 0.0001445075125208681,
      "loss": 0.2601,
      "step": 836
    },
    {
      "epoch": 1.395,
      "grad_norm": 0.07430903613567352,
      "learning_rate": 0.000144440734557596,
      "loss": 0.3424,
      "step": 837
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.0780021995306015,
      "learning_rate": 0.00014437395659432388,
      "loss": 0.3018,
      "step": 838
    },
    {
      "epoch": 1.3983333333333334,
      "grad_norm": 0.03458559140563011,
      "learning_rate": 0.00014430717863105178,
      "loss": 0.2922,
      "step": 839
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.03931190446019173,
      "learning_rate": 0.00014424040066777965,
      "loss": 0.2629,
      "step": 840
    },
    {
      "epoch": 1.4016666666666666,
      "grad_norm": 0.046334754675626755,
      "learning_rate": 0.00014417362270450752,
      "loss": 0.3368,
      "step": 841
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.06603558361530304,
      "learning_rate": 0.0001441068447412354,
      "loss": 0.3293,
      "step": 842
    },
    {
      "epoch": 1.405,
      "grad_norm": 0.042271047830581665,
      "learning_rate": 0.0001440400667779633,
      "loss": 0.3043,
      "step": 843
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.04367033392190933,
      "learning_rate": 0.00014397328881469117,
      "loss": 0.317,
      "step": 844
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.0427095927298069,
      "learning_rate": 0.00014390651085141904,
      "loss": 0.3325,
      "step": 845
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.04340110719203949,
      "learning_rate": 0.00014383973288814692,
      "loss": 0.3019,
      "step": 846
    },
    {
      "epoch": 1.4116666666666666,
      "grad_norm": 0.0474495030939579,
      "learning_rate": 0.0001437729549248748,
      "loss": 0.2916,
      "step": 847
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.057868655771017075,
      "learning_rate": 0.00014370617696160266,
      "loss": 0.2678,
      "step": 848
    },
    {
      "epoch": 1.415,
      "grad_norm": 0.05852421000599861,
      "learning_rate": 0.00014363939899833056,
      "loss": 0.346,
      "step": 849
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.03947846591472626,
      "learning_rate": 0.00014357262103505844,
      "loss": 0.2712,
      "step": 850
    },
    {
      "epoch": 1.4183333333333334,
      "grad_norm": 0.04878981411457062,
      "learning_rate": 0.0001435058430717863,
      "loss": 0.2994,
      "step": 851
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.040267035365104675,
      "learning_rate": 0.00014343906510851418,
      "loss": 0.3163,
      "step": 852
    },
    {
      "epoch": 1.4216666666666666,
      "grad_norm": 0.03372003138065338,
      "learning_rate": 0.00014337228714524205,
      "loss": 0.2364,
      "step": 853
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.046071574091911316,
      "learning_rate": 0.00014330550918196995,
      "loss": 0.3244,
      "step": 854
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.07180870324373245,
      "learning_rate": 0.00014323873121869783,
      "loss": 0.4465,
      "step": 855
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.04414349049329758,
      "learning_rate": 0.00014317195325542573,
      "loss": 0.3168,
      "step": 856
    },
    {
      "epoch": 1.4283333333333332,
      "grad_norm": 0.046280134469270706,
      "learning_rate": 0.0001431051752921536,
      "loss": 0.2685,
      "step": 857
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.06045612692832947,
      "learning_rate": 0.00014303839732888147,
      "loss": 0.3512,
      "step": 858
    },
    {
      "epoch": 1.4316666666666666,
      "grad_norm": 0.06843306124210358,
      "learning_rate": 0.00014297161936560937,
      "loss": 0.3599,
      "step": 859
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.04307536408305168,
      "learning_rate": 0.00014290484140233725,
      "loss": 0.2431,
      "step": 860
    },
    {
      "epoch": 1.435,
      "grad_norm": 0.04636266827583313,
      "learning_rate": 0.00014283806343906512,
      "loss": 0.3456,
      "step": 861
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.06852729618549347,
      "learning_rate": 0.000142771285475793,
      "loss": 0.3909,
      "step": 862
    },
    {
      "epoch": 1.4383333333333335,
      "grad_norm": 0.049584850668907166,
      "learning_rate": 0.00014270450751252087,
      "loss": 0.3395,
      "step": 863
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.04812886193394661,
      "learning_rate": 0.00014263772954924874,
      "loss": 0.3646,
      "step": 864
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.04151337966322899,
      "learning_rate": 0.00014257095158597664,
      "loss": 0.3284,
      "step": 865
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 0.03223811835050583,
      "learning_rate": 0.00014250417362270451,
      "loss": 0.2744,
      "step": 866
    },
    {
      "epoch": 1.445,
      "grad_norm": 0.04047216475009918,
      "learning_rate": 0.0001424373956594324,
      "loss": 0.2553,
      "step": 867
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.043886639177799225,
      "learning_rate": 0.00014237061769616026,
      "loss": 0.2993,
      "step": 868
    },
    {
      "epoch": 1.4483333333333333,
      "grad_norm": 0.05120173469185829,
      "learning_rate": 0.00014230383973288813,
      "loss": 0.3368,
      "step": 869
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.04859189689159393,
      "learning_rate": 0.00014223706176961603,
      "loss": 0.3288,
      "step": 870
    },
    {
      "epoch": 1.4516666666666667,
      "grad_norm": 0.04985019192099571,
      "learning_rate": 0.0001421702838063439,
      "loss": 0.3306,
      "step": 871
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.045380111783742905,
      "learning_rate": 0.00014210350584307178,
      "loss": 0.3145,
      "step": 872
    },
    {
      "epoch": 1.455,
      "grad_norm": 0.06482722610235214,
      "learning_rate": 0.00014203672787979968,
      "loss": 0.3764,
      "step": 873
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.07491414994001389,
      "learning_rate": 0.00014196994991652755,
      "loss": 0.313,
      "step": 874
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.046655621379613876,
      "learning_rate": 0.00014190317195325545,
      "loss": 0.3219,
      "step": 875
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.0647640973329544,
      "learning_rate": 0.00014183639398998333,
      "loss": 0.3129,
      "step": 876
    },
    {
      "epoch": 1.4616666666666667,
      "grad_norm": 0.0445483922958374,
      "learning_rate": 0.0001417696160267112,
      "loss": 0.3285,
      "step": 877
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.046954866498708725,
      "learning_rate": 0.00014170283806343907,
      "loss": 0.3179,
      "step": 878
    },
    {
      "epoch": 1.465,
      "grad_norm": 0.04769301414489746,
      "learning_rate": 0.00014163606010016695,
      "loss": 0.2987,
      "step": 879
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.045295119285583496,
      "learning_rate": 0.00014156928213689482,
      "loss": 0.287,
      "step": 880
    },
    {
      "epoch": 1.4683333333333333,
      "grad_norm": 0.04727170988917351,
      "learning_rate": 0.00014150250417362272,
      "loss": 0.3451,
      "step": 881
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.045966386795043945,
      "learning_rate": 0.0001414357262103506,
      "loss": 0.2792,
      "step": 882
    },
    {
      "epoch": 1.4716666666666667,
      "grad_norm": 0.05901378393173218,
      "learning_rate": 0.00014136894824707847,
      "loss": 0.3172,
      "step": 883
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.05820878967642784,
      "learning_rate": 0.00014130217028380634,
      "loss": 0.2884,
      "step": 884
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.07530195266008377,
      "learning_rate": 0.0001412353923205342,
      "loss": 0.4522,
      "step": 885
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.06534326076507568,
      "learning_rate": 0.0001411686143572621,
      "loss": 0.313,
      "step": 886
    },
    {
      "epoch": 1.4783333333333333,
      "grad_norm": 0.0510396845638752,
      "learning_rate": 0.00014110183639398999,
      "loss": 0.3392,
      "step": 887
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.03537813201546669,
      "learning_rate": 0.00014103505843071786,
      "loss": 0.2205,
      "step": 888
    },
    {
      "epoch": 1.4816666666666667,
      "grad_norm": 0.0505184605717659,
      "learning_rate": 0.00014096828046744576,
      "loss": 0.3194,
      "step": 889
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.04588684067130089,
      "learning_rate": 0.00014090150250417363,
      "loss": 0.3088,
      "step": 890
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 0.05029069259762764,
      "learning_rate": 0.00014083472454090153,
      "loss": 0.3679,
      "step": 891
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.042831696569919586,
      "learning_rate": 0.0001407679465776294,
      "loss": 0.3141,
      "step": 892
    },
    {
      "epoch": 1.4883333333333333,
      "grad_norm": 0.05055160075426102,
      "learning_rate": 0.00014070116861435728,
      "loss": 0.3077,
      "step": 893
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.0531141497194767,
      "learning_rate": 0.00014063439065108515,
      "loss": 0.2193,
      "step": 894
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.04216708242893219,
      "learning_rate": 0.00014056761268781303,
      "loss": 0.292,
      "step": 895
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.04327073320746422,
      "learning_rate": 0.0001405008347245409,
      "loss": 0.2168,
      "step": 896
    },
    {
      "epoch": 1.495,
      "grad_norm": 0.04799102246761322,
      "learning_rate": 0.0001404340567612688,
      "loss": 0.3308,
      "step": 897
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.06827569007873535,
      "learning_rate": 0.00014036727879799667,
      "loss": 0.3433,
      "step": 898
    },
    {
      "epoch": 1.4983333333333333,
      "grad_norm": 0.05772053450345993,
      "learning_rate": 0.00014030050083472454,
      "loss": 0.3023,
      "step": 899
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.032591111958026886,
      "learning_rate": 0.00014023372287145242,
      "loss": 0.228,
      "step": 900
    },
    {
      "epoch": 1.5016666666666667,
      "grad_norm": 0.08502249419689178,
      "learning_rate": 0.0001401669449081803,
      "loss": 0.3983,
      "step": 901
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.04612479731440544,
      "learning_rate": 0.0001401001669449082,
      "loss": 0.2727,
      "step": 902
    },
    {
      "epoch": 1.505,
      "grad_norm": 0.0466214157640934,
      "learning_rate": 0.00014003338898163606,
      "loss": 0.3075,
      "step": 903
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.06671636551618576,
      "learning_rate": 0.00013996661101836394,
      "loss": 0.415,
      "step": 904
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.05804523453116417,
      "learning_rate": 0.0001398998330550918,
      "loss": 0.295,
      "step": 905
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.050621435046195984,
      "learning_rate": 0.0001398330550918197,
      "loss": 0.355,
      "step": 906
    },
    {
      "epoch": 1.5116666666666667,
      "grad_norm": 0.05810292810201645,
      "learning_rate": 0.00013976627712854758,
      "loss": 0.3699,
      "step": 907
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.06873875856399536,
      "learning_rate": 0.00013969949916527548,
      "loss": 0.4122,
      "step": 908
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.03874743729829788,
      "learning_rate": 0.00013963272120200336,
      "loss": 0.2893,
      "step": 909
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.055813420563936234,
      "learning_rate": 0.00013956594323873123,
      "loss": 0.3964,
      "step": 910
    },
    {
      "epoch": 1.5183333333333333,
      "grad_norm": 0.07139451056718826,
      "learning_rate": 0.0001394991652754591,
      "loss": 0.2981,
      "step": 911
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.044913649559020996,
      "learning_rate": 0.00013943238731218698,
      "loss": 0.3106,
      "step": 912
    },
    {
      "epoch": 1.5216666666666665,
      "grad_norm": 0.04596996307373047,
      "learning_rate": 0.00013936560934891488,
      "loss": 0.3384,
      "step": 913
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.05136718600988388,
      "learning_rate": 0.00013929883138564275,
      "loss": 0.3476,
      "step": 914
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.0747559443116188,
      "learning_rate": 0.00013923205342237062,
      "loss": 0.3849,
      "step": 915
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.05316942185163498,
      "learning_rate": 0.0001391652754590985,
      "loss": 0.2951,
      "step": 916
    },
    {
      "epoch": 1.5283333333333333,
      "grad_norm": 0.061886005103588104,
      "learning_rate": 0.00013909849749582637,
      "loss": 0.3757,
      "step": 917
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.04856758564710617,
      "learning_rate": 0.00013903171953255427,
      "loss": 0.3222,
      "step": 918
    },
    {
      "epoch": 1.5316666666666667,
      "grad_norm": 0.07645608484745026,
      "learning_rate": 0.00013896494156928214,
      "loss": 0.3277,
      "step": 919
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.06772875785827637,
      "learning_rate": 0.00013889816360601002,
      "loss": 0.4009,
      "step": 920
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 0.0757509395480156,
      "learning_rate": 0.0001388313856427379,
      "loss": 0.4035,
      "step": 921
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.05501822009682655,
      "learning_rate": 0.00013876460767946576,
      "loss": 0.3308,
      "step": 922
    },
    {
      "epoch": 1.5383333333333333,
      "grad_norm": 0.043375641107559204,
      "learning_rate": 0.00013869782971619366,
      "loss": 0.3036,
      "step": 923
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.07038883119821548,
      "learning_rate": 0.00013863105175292154,
      "loss": 0.3607,
      "step": 924
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.058588892221450806,
      "learning_rate": 0.00013856427378964944,
      "loss": 0.338,
      "step": 925
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 0.05313384532928467,
      "learning_rate": 0.0001384974958263773,
      "loss": 0.3626,
      "step": 926
    },
    {
      "epoch": 1.545,
      "grad_norm": 0.055786944925785065,
      "learning_rate": 0.00013843071786310518,
      "loss": 0.3001,
      "step": 927
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.09075192362070084,
      "learning_rate": 0.00013836393989983308,
      "loss": 0.3753,
      "step": 928
    },
    {
      "epoch": 1.5483333333333333,
      "grad_norm": 0.04591156542301178,
      "learning_rate": 0.00013829716193656096,
      "loss": 0.3354,
      "step": 929
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.040177710354328156,
      "learning_rate": 0.00013823038397328883,
      "loss": 0.2631,
      "step": 930
    },
    {
      "epoch": 1.5516666666666667,
      "grad_norm": 0.05105746164917946,
      "learning_rate": 0.0001381636060100167,
      "loss": 0.3158,
      "step": 931
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.0577065646648407,
      "learning_rate": 0.00013809682804674458,
      "loss": 0.3233,
      "step": 932
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 0.04383842274546623,
      "learning_rate": 0.00013803005008347245,
      "loss": 0.2954,
      "step": 933
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.04711417853832245,
      "learning_rate": 0.00013796327212020035,
      "loss": 0.3059,
      "step": 934
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.0515235997736454,
      "learning_rate": 0.00013789649415692822,
      "loss": 0.3368,
      "step": 935
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05888603627681732,
      "learning_rate": 0.0001378297161936561,
      "loss": 0.3381,
      "step": 936
    },
    {
      "epoch": 1.5616666666666665,
      "grad_norm": 0.05350120738148689,
      "learning_rate": 0.00013776293823038397,
      "loss": 0.3531,
      "step": 937
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.04944382235407829,
      "learning_rate": 0.00013769616026711184,
      "loss": 0.3821,
      "step": 938
    },
    {
      "epoch": 1.565,
      "grad_norm": 0.04268326237797737,
      "learning_rate": 0.00013762938230383974,
      "loss": 0.3095,
      "step": 939
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.05493771657347679,
      "learning_rate": 0.00013756260434056762,
      "loss": 0.3839,
      "step": 940
    },
    {
      "epoch": 1.5683333333333334,
      "grad_norm": 0.07032311707735062,
      "learning_rate": 0.0001374958263772955,
      "loss": 0.3824,
      "step": 941
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.05139169842004776,
      "learning_rate": 0.0001374290484140234,
      "loss": 0.2681,
      "step": 942
    },
    {
      "epoch": 1.5716666666666668,
      "grad_norm": 0.05202993005514145,
      "learning_rate": 0.00013736227045075126,
      "loss": 0.3322,
      "step": 943
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.04159674420952797,
      "learning_rate": 0.00013729549248747916,
      "loss": 0.2975,
      "step": 944
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.06989212334156036,
      "learning_rate": 0.00013722871452420704,
      "loss": 0.3638,
      "step": 945
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.05473538860678673,
      "learning_rate": 0.0001371619365609349,
      "loss": 0.2648,
      "step": 946
    },
    {
      "epoch": 1.5783333333333334,
      "grad_norm": 0.0342022143304348,
      "learning_rate": 0.00013709515859766278,
      "loss": 0.2865,
      "step": 947
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.042526617646217346,
      "learning_rate": 0.00013702838063439065,
      "loss": 0.2357,
      "step": 948
    },
    {
      "epoch": 1.5816666666666666,
      "grad_norm": 0.049015387892723083,
      "learning_rate": 0.00013696160267111853,
      "loss": 0.3125,
      "step": 949
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.030944352969527245,
      "learning_rate": 0.00013689482470784643,
      "loss": 0.2528,
      "step": 950
    },
    {
      "epoch": 1.585,
      "grad_norm": 0.04372309148311615,
      "learning_rate": 0.0001368280467445743,
      "loss": 0.3103,
      "step": 951
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.04487812891602516,
      "learning_rate": 0.00013676126878130217,
      "loss": 0.3224,
      "step": 952
    },
    {
      "epoch": 1.5883333333333334,
      "grad_norm": 0.0446169339120388,
      "learning_rate": 0.00013669449081803005,
      "loss": 0.3361,
      "step": 953
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.048597756773233414,
      "learning_rate": 0.00013662771285475792,
      "loss": 0.3359,
      "step": 954
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.044693704694509506,
      "learning_rate": 0.00013656093489148582,
      "loss": 0.3239,
      "step": 955
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.041537608951330185,
      "learning_rate": 0.0001364941569282137,
      "loss": 0.2646,
      "step": 956
    },
    {
      "epoch": 1.595,
      "grad_norm": 0.042723800987005234,
      "learning_rate": 0.00013642737896494157,
      "loss": 0.3339,
      "step": 957
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.05692213773727417,
      "learning_rate": 0.00013636060100166944,
      "loss": 0.3254,
      "step": 958
    },
    {
      "epoch": 1.5983333333333334,
      "grad_norm": 0.04255099222064018,
      "learning_rate": 0.00013629382303839734,
      "loss": 0.3515,
      "step": 959
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.06511254608631134,
      "learning_rate": 0.00013622704507512521,
      "loss": 0.3308,
      "step": 960
    },
    {
      "epoch": 1.6016666666666666,
      "grad_norm": 0.09750804305076599,
      "learning_rate": 0.00013616026711185311,
      "loss": 0.3096,
      "step": 961
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.05046100914478302,
      "learning_rate": 0.000136093489148581,
      "loss": 0.3334,
      "step": 962
    },
    {
      "epoch": 1.605,
      "grad_norm": 0.04997367039322853,
      "learning_rate": 0.00013602671118530886,
      "loss": 0.3465,
      "step": 963
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.03947768732905388,
      "learning_rate": 0.00013595993322203673,
      "loss": 0.2854,
      "step": 964
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.05549127236008644,
      "learning_rate": 0.0001358931552587646,
      "loss": 0.3037,
      "step": 965
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.04163867607712746,
      "learning_rate": 0.0001358263772954925,
      "loss": 0.3209,
      "step": 966
    },
    {
      "epoch": 1.6116666666666668,
      "grad_norm": 0.05413821339607239,
      "learning_rate": 0.00013575959933222038,
      "loss": 0.3436,
      "step": 967
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.07838261872529984,
      "learning_rate": 0.00013569282136894825,
      "loss": 0.3963,
      "step": 968
    },
    {
      "epoch": 1.615,
      "grad_norm": 0.04193643108010292,
      "learning_rate": 0.00013562604340567613,
      "loss": 0.2631,
      "step": 969
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.048738691955804825,
      "learning_rate": 0.000135559265442404,
      "loss": 0.2688,
      "step": 970
    },
    {
      "epoch": 1.6183333333333332,
      "grad_norm": 0.0459895133972168,
      "learning_rate": 0.0001354924874791319,
      "loss": 0.3084,
      "step": 971
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.062209729105234146,
      "learning_rate": 0.00013542570951585977,
      "loss": 0.3037,
      "step": 972
    },
    {
      "epoch": 1.6216666666666666,
      "grad_norm": 0.04098627343773842,
      "learning_rate": 0.00013535893155258765,
      "loss": 0.2495,
      "step": 973
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.06052949279546738,
      "learning_rate": 0.00013529215358931552,
      "loss": 0.3883,
      "step": 974
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.058028850704431534,
      "learning_rate": 0.0001352253756260434,
      "loss": 0.3131,
      "step": 975
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.037967611104249954,
      "learning_rate": 0.0001351585976627713,
      "loss": 0.258,
      "step": 976
    },
    {
      "epoch": 1.6283333333333334,
      "grad_norm": 0.04722531884908676,
      "learning_rate": 0.00013509181969949917,
      "loss": 0.2778,
      "step": 977
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.044648781418800354,
      "learning_rate": 0.00013502504173622707,
      "loss": 0.3343,
      "step": 978
    },
    {
      "epoch": 1.6316666666666668,
      "grad_norm": 0.03678503260016441,
      "learning_rate": 0.00013495826377295494,
      "loss": 0.2565,
      "step": 979
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.039121415466070175,
      "learning_rate": 0.0001348914858096828,
      "loss": 0.2623,
      "step": 980
    },
    {
      "epoch": 1.635,
      "grad_norm": 0.04092656075954437,
      "learning_rate": 0.00013482470784641069,
      "loss": 0.3092,
      "step": 981
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.051251523196697235,
      "learning_rate": 0.00013475792988313859,
      "loss": 0.346,
      "step": 982
    },
    {
      "epoch": 1.6383333333333332,
      "grad_norm": 0.04310860484838486,
      "learning_rate": 0.00013469115191986646,
      "loss": 0.3074,
      "step": 983
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.047086965292692184,
      "learning_rate": 0.00013462437395659433,
      "loss": 0.3179,
      "step": 984
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.0584566593170166,
      "learning_rate": 0.0001345575959933222,
      "loss": 0.3475,
      "step": 985
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.05166751146316528,
      "learning_rate": 0.00013449081803005008,
      "loss": 0.35,
      "step": 986
    },
    {
      "epoch": 1.645,
      "grad_norm": 0.04174710810184479,
      "learning_rate": 0.00013442404006677798,
      "loss": 0.3101,
      "step": 987
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.06651616096496582,
      "learning_rate": 0.00013435726210350585,
      "loss": 0.3081,
      "step": 988
    },
    {
      "epoch": 1.6483333333333334,
      "grad_norm": 0.04170955717563629,
      "learning_rate": 0.00013429048414023373,
      "loss": 0.2392,
      "step": 989
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.04214433580636978,
      "learning_rate": 0.0001342237061769616,
      "loss": 0.2838,
      "step": 990
    },
    {
      "epoch": 1.6516666666666666,
      "grad_norm": 0.05202667787671089,
      "learning_rate": 0.00013415692821368947,
      "loss": 0.3577,
      "step": 991
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.04032661393284798,
      "learning_rate": 0.00013409015025041737,
      "loss": 0.3128,
      "step": 992
    },
    {
      "epoch": 1.655,
      "grad_norm": 0.051851995289325714,
      "learning_rate": 0.00013402337228714524,
      "loss": 0.3386,
      "step": 993
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.0496482290327549,
      "learning_rate": 0.00013395659432387312,
      "loss": 0.3024,
      "step": 994
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.0371108278632164,
      "learning_rate": 0.00013388981636060102,
      "loss": 0.2869,
      "step": 995
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.03606151416897774,
      "learning_rate": 0.0001338230383973289,
      "loss": 0.2459,
      "step": 996
    },
    {
      "epoch": 1.6616666666666666,
      "grad_norm": 0.07311677932739258,
      "learning_rate": 0.00013375626043405676,
      "loss": 0.3692,
      "step": 997
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.03846399486064911,
      "learning_rate": 0.00013368948247078466,
      "loss": 0.2719,
      "step": 998
    },
    {
      "epoch": 1.665,
      "grad_norm": 0.04948924109339714,
      "learning_rate": 0.00013362270450751254,
      "loss": 0.3157,
      "step": 999
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.037923675030469894,
      "learning_rate": 0.0001335559265442404,
      "loss": 0.276,
      "step": 1000
    },
    {
      "epoch": 1.6683333333333334,
      "grad_norm": 0.06729485839605331,
      "learning_rate": 0.00013348914858096828,
      "loss": 0.3583,
      "step": 1001
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.0628759115934372,
      "learning_rate": 0.00013342237061769616,
      "loss": 0.3913,
      "step": 1002
    },
    {
      "epoch": 1.6716666666666666,
      "grad_norm": 0.053583502769470215,
      "learning_rate": 0.00013335559265442406,
      "loss": 0.3705,
      "step": 1003
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.046253591775894165,
      "learning_rate": 0.00013328881469115193,
      "loss": 0.3445,
      "step": 1004
    },
    {
      "epoch": 1.675,
      "grad_norm": 0.08141642808914185,
      "learning_rate": 0.0001332220367278798,
      "loss": 0.3728,
      "step": 1005
    },
    {
      "epoch": 1.6766666666666667,
      "grad_norm": 0.07604476064443588,
      "learning_rate": 0.00013315525876460768,
      "loss": 0.3716,
      "step": 1006
    },
    {
      "epoch": 1.6783333333333332,
      "grad_norm": 0.03778963163495064,
      "learning_rate": 0.00013308848080133555,
      "loss": 0.2796,
      "step": 1007
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.042032450437545776,
      "learning_rate": 0.00013302170283806345,
      "loss": 0.2925,
      "step": 1008
    },
    {
      "epoch": 1.6816666666666666,
      "grad_norm": 0.04012667387723923,
      "learning_rate": 0.00013295492487479132,
      "loss": 0.2715,
      "step": 1009
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.05352889746427536,
      "learning_rate": 0.0001328881469115192,
      "loss": 0.3461,
      "step": 1010
    },
    {
      "epoch": 1.685,
      "grad_norm": 0.044235631823539734,
      "learning_rate": 0.00013282136894824707,
      "loss": 0.3187,
      "step": 1011
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.04060358554124832,
      "learning_rate": 0.00013275459098497497,
      "loss": 0.2852,
      "step": 1012
    },
    {
      "epoch": 1.6883333333333335,
      "grad_norm": 0.04608463495969772,
      "learning_rate": 0.00013268781302170284,
      "loss": 0.3632,
      "step": 1013
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.05349704623222351,
      "learning_rate": 0.00013262103505843074,
      "loss": 0.3759,
      "step": 1014
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.05008327215909958,
      "learning_rate": 0.00013255425709515862,
      "loss": 0.3212,
      "step": 1015
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.028942150995135307,
      "learning_rate": 0.0001324874791318865,
      "loss": 0.2125,
      "step": 1016
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 0.05627622455358505,
      "learning_rate": 0.00013242070116861436,
      "loss": 0.2966,
      "step": 1017
    },
    {
      "epoch": 1.6966666666666668,
      "grad_norm": 0.03976047784090042,
      "learning_rate": 0.00013235392320534224,
      "loss": 0.2623,
      "step": 1018
    },
    {
      "epoch": 1.6983333333333333,
      "grad_norm": 0.04030706360936165,
      "learning_rate": 0.00013228714524207014,
      "loss": 0.2414,
      "step": 1019
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.05688543990254402,
      "learning_rate": 0.000132220367278798,
      "loss": 0.3208,
      "step": 1020
    },
    {
      "epoch": 1.7016666666666667,
      "grad_norm": 0.047575611621141434,
      "learning_rate": 0.00013215358931552588,
      "loss": 0.2781,
      "step": 1021
    },
    {
      "epoch": 1.7033333333333334,
      "grad_norm": 0.05373598635196686,
      "learning_rate": 0.00013208681135225376,
      "loss": 0.3775,
      "step": 1022
    },
    {
      "epoch": 1.705,
      "grad_norm": 0.04273916780948639,
      "learning_rate": 0.00013202003338898163,
      "loss": 0.2865,
      "step": 1023
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.05813917517662048,
      "learning_rate": 0.00013195325542570953,
      "loss": 0.3427,
      "step": 1024
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.06394652277231216,
      "learning_rate": 0.0001318864774624374,
      "loss": 0.3542,
      "step": 1025
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.04026031866669655,
      "learning_rate": 0.00013181969949916528,
      "loss": 0.3074,
      "step": 1026
    },
    {
      "epoch": 1.7116666666666667,
      "grad_norm": 0.03589266166090965,
      "learning_rate": 0.00013175292153589315,
      "loss": 0.2771,
      "step": 1027
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.049929939210414886,
      "learning_rate": 0.00013168614357262102,
      "loss": 0.3866,
      "step": 1028
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 0.04077513515949249,
      "learning_rate": 0.00013161936560934892,
      "loss": 0.2904,
      "step": 1029
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.0371658131480217,
      "learning_rate": 0.0001315525876460768,
      "loss": 0.2015,
      "step": 1030
    },
    {
      "epoch": 1.7183333333333333,
      "grad_norm": 0.046947360038757324,
      "learning_rate": 0.0001314858096828047,
      "loss": 0.2857,
      "step": 1031
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.06437688320875168,
      "learning_rate": 0.00013141903171953257,
      "loss": 0.3645,
      "step": 1032
    },
    {
      "epoch": 1.7216666666666667,
      "grad_norm": 0.053786635398864746,
      "learning_rate": 0.00013135225375626044,
      "loss": 0.3295,
      "step": 1033
    },
    {
      "epoch": 1.7233333333333334,
      "grad_norm": 0.06229240447282791,
      "learning_rate": 0.00013128547579298832,
      "loss": 0.3459,
      "step": 1034
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.04503043740987778,
      "learning_rate": 0.00013121869782971622,
      "loss": 0.3144,
      "step": 1035
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.0631893202662468,
      "learning_rate": 0.0001311519198664441,
      "loss": 0.3412,
      "step": 1036
    },
    {
      "epoch": 1.7283333333333335,
      "grad_norm": 0.054712895303964615,
      "learning_rate": 0.00013108514190317196,
      "loss": 0.348,
      "step": 1037
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.05043491721153259,
      "learning_rate": 0.00013101836393989983,
      "loss": 0.2999,
      "step": 1038
    },
    {
      "epoch": 1.7316666666666667,
      "grad_norm": 0.0500413253903389,
      "learning_rate": 0.0001309515859766277,
      "loss": 0.3012,
      "step": 1039
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.08589386940002441,
      "learning_rate": 0.0001308848080133556,
      "loss": 0.393,
      "step": 1040
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 0.05146476998925209,
      "learning_rate": 0.00013081803005008348,
      "loss": 0.3549,
      "step": 1041
    },
    {
      "epoch": 1.7366666666666668,
      "grad_norm": 0.052840061485767365,
      "learning_rate": 0.00013075125208681135,
      "loss": 0.345,
      "step": 1042
    },
    {
      "epoch": 1.7383333333333333,
      "grad_norm": 0.05032000318169594,
      "learning_rate": 0.00013068447412353923,
      "loss": 0.2806,
      "step": 1043
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.03866669535636902,
      "learning_rate": 0.0001306176961602671,
      "loss": 0.2678,
      "step": 1044
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 0.04533272981643677,
      "learning_rate": 0.00013055091819699497,
      "loss": 0.2913,
      "step": 1045
    },
    {
      "epoch": 1.7433333333333332,
      "grad_norm": 0.03810100629925728,
      "learning_rate": 0.00013048414023372287,
      "loss": 0.3111,
      "step": 1046
    },
    {
      "epoch": 1.745,
      "grad_norm": 0.05846405401825905,
      "learning_rate": 0.00013041736227045075,
      "loss": 0.3332,
      "step": 1047
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.07434669882059097,
      "learning_rate": 0.00013035058430717865,
      "loss": 0.3787,
      "step": 1048
    },
    {
      "epoch": 1.7483333333333333,
      "grad_norm": 0.053505972027778625,
      "learning_rate": 0.00013028380634390652,
      "loss": 0.3141,
      "step": 1049
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.03644069656729698,
      "learning_rate": 0.0001302170283806344,
      "loss": 0.2824,
      "step": 1050
    },
    {
      "epoch": 1.7516666666666667,
      "grad_norm": 0.06271888315677643,
      "learning_rate": 0.0001301502504173623,
      "loss": 0.3049,
      "step": 1051
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.04181337729096413,
      "learning_rate": 0.00013008347245409017,
      "loss": 0.2881,
      "step": 1052
    },
    {
      "epoch": 1.755,
      "grad_norm": 0.037730623036623,
      "learning_rate": 0.00013001669449081804,
      "loss": 0.2675,
      "step": 1053
    },
    {
      "epoch": 1.7566666666666668,
      "grad_norm": 0.08262412995100021,
      "learning_rate": 0.00012994991652754591,
      "loss": 0.3231,
      "step": 1054
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.04140356555581093,
      "learning_rate": 0.0001298831385642738,
      "loss": 0.267,
      "step": 1055
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.05467144772410393,
      "learning_rate": 0.0001298163606010017,
      "loss": 0.3021,
      "step": 1056
    },
    {
      "epoch": 1.7616666666666667,
      "grad_norm": 0.05465537682175636,
      "learning_rate": 0.00012974958263772956,
      "loss": 0.2948,
      "step": 1057
    },
    {
      "epoch": 1.7633333333333332,
      "grad_norm": 0.05109381675720215,
      "learning_rate": 0.00012968280467445743,
      "loss": 0.2796,
      "step": 1058
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 0.04681461676955223,
      "learning_rate": 0.0001296160267111853,
      "loss": 0.3565,
      "step": 1059
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.050567008554935455,
      "learning_rate": 0.00012954924874791318,
      "loss": 0.3976,
      "step": 1060
    },
    {
      "epoch": 1.7683333333333333,
      "grad_norm": 0.058165788650512695,
      "learning_rate": 0.00012948247078464108,
      "loss": 0.3255,
      "step": 1061
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.039803605526685715,
      "learning_rate": 0.00012941569282136895,
      "loss": 0.317,
      "step": 1062
    },
    {
      "epoch": 1.7716666666666665,
      "grad_norm": 0.042155761271715164,
      "learning_rate": 0.00012934891485809683,
      "loss": 0.3175,
      "step": 1063
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.037175532430410385,
      "learning_rate": 0.0001292821368948247,
      "loss": 0.2673,
      "step": 1064
    },
    {
      "epoch": 1.775,
      "grad_norm": 0.07222988456487656,
      "learning_rate": 0.0001292153589315526,
      "loss": 0.3578,
      "step": 1065
    },
    {
      "epoch": 1.7766666666666666,
      "grad_norm": 0.0403539314866066,
      "learning_rate": 0.00012914858096828047,
      "loss": 0.2678,
      "step": 1066
    },
    {
      "epoch": 1.7783333333333333,
      "grad_norm": 0.05393616110086441,
      "learning_rate": 0.00012908180300500837,
      "loss": 0.3511,
      "step": 1067
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.046882618218660355,
      "learning_rate": 0.00012901502504173625,
      "loss": 0.33,
      "step": 1068
    },
    {
      "epoch": 1.7816666666666667,
      "grad_norm": 0.03750748932361603,
      "learning_rate": 0.00012894824707846412,
      "loss": 0.2687,
      "step": 1069
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.03617999330163002,
      "learning_rate": 0.000128881469115192,
      "loss": 0.2617,
      "step": 1070
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 0.03936118632555008,
      "learning_rate": 0.00012881469115191987,
      "loss": 0.2933,
      "step": 1071
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.04972649738192558,
      "learning_rate": 0.00012874791318864777,
      "loss": 0.3582,
      "step": 1072
    },
    {
      "epoch": 1.7883333333333333,
      "grad_norm": 0.062223099172115326,
      "learning_rate": 0.00012868113522537564,
      "loss": 0.3844,
      "step": 1073
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.060599926859140396,
      "learning_rate": 0.0001286143572621035,
      "loss": 0.4029,
      "step": 1074
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.051546867936849594,
      "learning_rate": 0.00012854757929883139,
      "loss": 0.2903,
      "step": 1075
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.06320391595363617,
      "learning_rate": 0.00012848080133555926,
      "loss": 0.3899,
      "step": 1076
    },
    {
      "epoch": 1.795,
      "grad_norm": 0.033998556435108185,
      "learning_rate": 0.00012841402337228716,
      "loss": 0.244,
      "step": 1077
    },
    {
      "epoch": 1.7966666666666666,
      "grad_norm": 0.04409806430339813,
      "learning_rate": 0.00012834724540901503,
      "loss": 0.3124,
      "step": 1078
    },
    {
      "epoch": 1.7983333333333333,
      "grad_norm": 0.05397401377558708,
      "learning_rate": 0.0001282804674457429,
      "loss": 0.3262,
      "step": 1079
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.04816342890262604,
      "learning_rate": 0.00012821368948247078,
      "loss": 0.309,
      "step": 1080
    },
    {
      "epoch": 1.8016666666666667,
      "grad_norm": 0.03964848443865776,
      "learning_rate": 0.00012814691151919865,
      "loss": 0.3079,
      "step": 1081
    },
    {
      "epoch": 1.8033333333333332,
      "grad_norm": 0.041097044944763184,
      "learning_rate": 0.00012808013355592655,
      "loss": 0.2459,
      "step": 1082
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 0.055256959050893784,
      "learning_rate": 0.00012801335559265442,
      "loss": 0.3729,
      "step": 1083
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.04444698989391327,
      "learning_rate": 0.00012794657762938233,
      "loss": 0.3352,
      "step": 1084
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 0.0473179928958416,
      "learning_rate": 0.0001278797996661102,
      "loss": 0.3565,
      "step": 1085
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.060201678425073624,
      "learning_rate": 0.00012781302170283807,
      "loss": 0.3616,
      "step": 1086
    },
    {
      "epoch": 1.8116666666666665,
      "grad_norm": 0.046176470816135406,
      "learning_rate": 0.00012774624373956594,
      "loss": 0.3273,
      "step": 1087
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.04041766747832298,
      "learning_rate": 0.00012767946577629384,
      "loss": 0.2688,
      "step": 1088
    },
    {
      "epoch": 1.815,
      "grad_norm": 0.04602207988500595,
      "learning_rate": 0.00012761268781302172,
      "loss": 0.302,
      "step": 1089
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.0424186997115612,
      "learning_rate": 0.0001275459098497496,
      "loss": 0.3271,
      "step": 1090
    },
    {
      "epoch": 1.8183333333333334,
      "grad_norm": 0.045363474637269974,
      "learning_rate": 0.00012747913188647746,
      "loss": 0.3105,
      "step": 1091
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.05695577710866928,
      "learning_rate": 0.00012741235392320534,
      "loss": 0.3415,
      "step": 1092
    },
    {
      "epoch": 1.8216666666666668,
      "grad_norm": 0.03469161316752434,
      "learning_rate": 0.00012734557595993324,
      "loss": 0.2925,
      "step": 1093
    },
    {
      "epoch": 1.8233333333333333,
      "grad_norm": 0.0432279035449028,
      "learning_rate": 0.0001272787979966611,
      "loss": 0.3286,
      "step": 1094
    },
    {
      "epoch": 1.825,
      "grad_norm": 0.04450453445315361,
      "learning_rate": 0.00012721202003338898,
      "loss": 0.2256,
      "step": 1095
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.05200564116239548,
      "learning_rate": 0.00012714524207011686,
      "loss": 0.2749,
      "step": 1096
    },
    {
      "epoch": 1.8283333333333334,
      "grad_norm": 0.051189400255680084,
      "learning_rate": 0.00012707846410684473,
      "loss": 0.3486,
      "step": 1097
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.036846961826086044,
      "learning_rate": 0.0001270116861435726,
      "loss": 0.2347,
      "step": 1098
    },
    {
      "epoch": 1.8316666666666666,
      "grad_norm": 0.03655676916241646,
      "learning_rate": 0.0001269449081803005,
      "loss": 0.2953,
      "step": 1099
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.04654020816087723,
      "learning_rate": 0.00012687813021702838,
      "loss": 0.3467,
      "step": 1100
    },
    {
      "epoch": 1.835,
      "grad_norm": 0.0403098464012146,
      "learning_rate": 0.00012681135225375628,
      "loss": 0.2688,
      "step": 1101
    },
    {
      "epoch": 1.8366666666666667,
      "grad_norm": 0.050502561032772064,
      "learning_rate": 0.00012674457429048415,
      "loss": 0.3155,
      "step": 1102
    },
    {
      "epoch": 1.8383333333333334,
      "grad_norm": 0.06417331099510193,
      "learning_rate": 0.00012667779632721202,
      "loss": 0.3282,
      "step": 1103
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.06599336117506027,
      "learning_rate": 0.00012661101836393992,
      "loss": 0.4203,
      "step": 1104
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 0.05034000426530838,
      "learning_rate": 0.0001265442404006678,
      "loss": 0.3062,
      "step": 1105
    },
    {
      "epoch": 1.8433333333333333,
      "grad_norm": 0.052412789314985275,
      "learning_rate": 0.00012647746243739567,
      "loss": 0.3026,
      "step": 1106
    },
    {
      "epoch": 1.845,
      "grad_norm": 0.058007050305604935,
      "learning_rate": 0.00012641068447412354,
      "loss": 0.2552,
      "step": 1107
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.03514641150832176,
      "learning_rate": 0.00012634390651085142,
      "loss": 0.2337,
      "step": 1108
    },
    {
      "epoch": 1.8483333333333334,
      "grad_norm": 0.06032254546880722,
      "learning_rate": 0.00012627712854757932,
      "loss": 0.402,
      "step": 1109
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.038448065519332886,
      "learning_rate": 0.0001262103505843072,
      "loss": 0.276,
      "step": 1110
    },
    {
      "epoch": 1.8516666666666666,
      "grad_norm": 0.05786294490098953,
      "learning_rate": 0.00012614357262103506,
      "loss": 0.3557,
      "step": 1111
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.05790349841117859,
      "learning_rate": 0.00012607679465776294,
      "loss": 0.3282,
      "step": 1112
    },
    {
      "epoch": 1.855,
      "grad_norm": 0.04294292628765106,
      "learning_rate": 0.0001260100166944908,
      "loss": 0.2792,
      "step": 1113
    },
    {
      "epoch": 1.8566666666666667,
      "grad_norm": 0.05003895238041878,
      "learning_rate": 0.00012594323873121868,
      "loss": 0.3719,
      "step": 1114
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.03888378664851189,
      "learning_rate": 0.00012587646076794658,
      "loss": 0.2378,
      "step": 1115
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.05043719336390495,
      "learning_rate": 0.00012580968280467446,
      "loss": 0.303,
      "step": 1116
    },
    {
      "epoch": 1.8616666666666668,
      "grad_norm": 0.03904039412736893,
      "learning_rate": 0.00012574290484140233,
      "loss": 0.2895,
      "step": 1117
    },
    {
      "epoch": 1.8633333333333333,
      "grad_norm": 0.04182562232017517,
      "learning_rate": 0.00012567612687813023,
      "loss": 0.2637,
      "step": 1118
    },
    {
      "epoch": 1.865,
      "grad_norm": 0.04074758663773537,
      "learning_rate": 0.0001256093489148581,
      "loss": 0.2843,
      "step": 1119
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.02902400493621826,
      "learning_rate": 0.000125542570951586,
      "loss": 0.2089,
      "step": 1120
    },
    {
      "epoch": 1.8683333333333332,
      "grad_norm": 0.07069988548755646,
      "learning_rate": 0.00012547579298831388,
      "loss": 0.3593,
      "step": 1121
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.056055884808301926,
      "learning_rate": 0.00012540901502504175,
      "loss": 0.3478,
      "step": 1122
    },
    {
      "epoch": 1.8716666666666666,
      "grad_norm": 0.046870555728673935,
      "learning_rate": 0.00012534223706176962,
      "loss": 0.3217,
      "step": 1123
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.03961705043911934,
      "learning_rate": 0.0001252754590984975,
      "loss": 0.272,
      "step": 1124
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.04048635810613632,
      "learning_rate": 0.0001252086811352254,
      "loss": 0.2811,
      "step": 1125
    },
    {
      "epoch": 1.8766666666666667,
      "grad_norm": 0.047017309814691544,
      "learning_rate": 0.00012514190317195327,
      "loss": 0.304,
      "step": 1126
    },
    {
      "epoch": 1.8783333333333334,
      "grad_norm": 0.05114813148975372,
      "learning_rate": 0.00012507512520868114,
      "loss": 0.3211,
      "step": 1127
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.03841552510857582,
      "learning_rate": 0.00012500834724540902,
      "loss": 0.3041,
      "step": 1128
    },
    {
      "epoch": 1.8816666666666668,
      "grad_norm": 0.03635908663272858,
      "learning_rate": 0.0001249415692821369,
      "loss": 0.2734,
      "step": 1129
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.04458603635430336,
      "learning_rate": 0.00012487479131886476,
      "loss": 0.3067,
      "step": 1130
    },
    {
      "epoch": 1.885,
      "grad_norm": 0.04333452507853508,
      "learning_rate": 0.00012480801335559266,
      "loss": 0.2842,
      "step": 1131
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.02883961983025074,
      "learning_rate": 0.00012474123539232053,
      "loss": 0.2504,
      "step": 1132
    },
    {
      "epoch": 1.8883333333333332,
      "grad_norm": 0.04479227960109711,
      "learning_rate": 0.0001246744574290484,
      "loss": 0.3086,
      "step": 1133
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.05305534228682518,
      "learning_rate": 0.0001246076794657763,
      "loss": 0.3315,
      "step": 1134
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.0718669518828392,
      "learning_rate": 0.00012454090150250418,
      "loss": 0.4454,
      "step": 1135
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.05058874934911728,
      "learning_rate": 0.00012447412353923208,
      "loss": 0.3696,
      "step": 1136
    },
    {
      "epoch": 1.895,
      "grad_norm": 0.05493483319878578,
      "learning_rate": 0.00012440734557595995,
      "loss": 0.3405,
      "step": 1137
    },
    {
      "epoch": 1.8966666666666665,
      "grad_norm": 0.047928810119628906,
      "learning_rate": 0.00012434056761268783,
      "loss": 0.3121,
      "step": 1138
    },
    {
      "epoch": 1.8983333333333334,
      "grad_norm": 0.04084394872188568,
      "learning_rate": 0.0001242737896494157,
      "loss": 0.2626,
      "step": 1139
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.033703576773405075,
      "learning_rate": 0.00012420701168614357,
      "loss": 0.2532,
      "step": 1140
    },
    {
      "epoch": 1.9016666666666666,
      "grad_norm": 0.054389838129282,
      "learning_rate": 0.00012414023372287147,
      "loss": 0.3116,
      "step": 1141
    },
    {
      "epoch": 1.9033333333333333,
      "grad_norm": 0.06424488872289658,
      "learning_rate": 0.00012407345575959935,
      "loss": 0.2854,
      "step": 1142
    },
    {
      "epoch": 1.905,
      "grad_norm": 0.05081469938158989,
      "learning_rate": 0.00012400667779632722,
      "loss": 0.2871,
      "step": 1143
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.048271387815475464,
      "learning_rate": 0.0001239398998330551,
      "loss": 0.2618,
      "step": 1144
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 0.0423254556953907,
      "learning_rate": 0.00012387312186978297,
      "loss": 0.2979,
      "step": 1145
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.04164205491542816,
      "learning_rate": 0.00012380634390651084,
      "loss": 0.2951,
      "step": 1146
    },
    {
      "epoch": 1.9116666666666666,
      "grad_norm": 0.059152450412511826,
      "learning_rate": 0.00012373956594323874,
      "loss": 0.3737,
      "step": 1147
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.05186520516872406,
      "learning_rate": 0.00012367278797996661,
      "loss": 0.3757,
      "step": 1148
    },
    {
      "epoch": 1.915,
      "grad_norm": 0.05937480553984642,
      "learning_rate": 0.0001236060100166945,
      "loss": 0.3671,
      "step": 1149
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.03685033693909645,
      "learning_rate": 0.00012353923205342236,
      "loss": 0.2617,
      "step": 1150
    },
    {
      "epoch": 1.9183333333333334,
      "grad_norm": 0.03587542846798897,
      "learning_rate": 0.00012347245409015026,
      "loss": 0.2756,
      "step": 1151
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.05459021404385567,
      "learning_rate": 0.00012340567612687813,
      "loss": 0.3597,
      "step": 1152
    },
    {
      "epoch": 1.9216666666666666,
      "grad_norm": 0.050952520221471786,
      "learning_rate": 0.00012333889816360603,
      "loss": 0.2844,
      "step": 1153
    },
    {
      "epoch": 1.9233333333333333,
      "grad_norm": 0.06575113534927368,
      "learning_rate": 0.0001232721202003339,
      "loss": 0.2916,
      "step": 1154
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.050047725439071655,
      "learning_rate": 0.00012320534223706178,
      "loss": 0.3743,
      "step": 1155
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.0698772743344307,
      "learning_rate": 0.00012313856427378965,
      "loss": 0.3497,
      "step": 1156
    },
    {
      "epoch": 1.9283333333333332,
      "grad_norm": 0.05200282111763954,
      "learning_rate": 0.00012307178631051755,
      "loss": 0.3345,
      "step": 1157
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.05544343218207359,
      "learning_rate": 0.00012300500834724543,
      "loss": 0.3249,
      "step": 1158
    },
    {
      "epoch": 1.9316666666666666,
      "grad_norm": 0.04666020721197128,
      "learning_rate": 0.0001229382303839733,
      "loss": 0.3174,
      "step": 1159
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.06507077813148499,
      "learning_rate": 0.00012287145242070117,
      "loss": 0.3865,
      "step": 1160
    },
    {
      "epoch": 1.935,
      "grad_norm": 0.04382528364658356,
      "learning_rate": 0.00012280467445742905,
      "loss": 0.3343,
      "step": 1161
    },
    {
      "epoch": 1.9366666666666665,
      "grad_norm": 0.041210055351257324,
      "learning_rate": 0.00012273789649415692,
      "loss": 0.3005,
      "step": 1162
    },
    {
      "epoch": 1.9383333333333335,
      "grad_norm": 0.046642765402793884,
      "learning_rate": 0.00012267111853088482,
      "loss": 0.3086,
      "step": 1163
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.05490682274103165,
      "learning_rate": 0.0001226043405676127,
      "loss": 0.4,
      "step": 1164
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 0.04329514503479004,
      "learning_rate": 0.00012253756260434057,
      "loss": 0.3348,
      "step": 1165
    },
    {
      "epoch": 1.9433333333333334,
      "grad_norm": 0.05037299543619156,
      "learning_rate": 0.00012247078464106844,
      "loss": 0.2956,
      "step": 1166
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 0.02435540407896042,
      "learning_rate": 0.0001224040066777963,
      "loss": 0.1945,
      "step": 1167
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.05931873992085457,
      "learning_rate": 0.0001223372287145242,
      "loss": 0.3262,
      "step": 1168
    },
    {
      "epoch": 1.9483333333333333,
      "grad_norm": 0.03564140945672989,
      "learning_rate": 0.00012227045075125209,
      "loss": 0.2655,
      "step": 1169
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.04192393273115158,
      "learning_rate": 0.00012220367278797999,
      "loss": 0.2884,
      "step": 1170
    },
    {
      "epoch": 1.9516666666666667,
      "grad_norm": 0.07567590475082397,
      "learning_rate": 0.00012213689482470786,
      "loss": 0.3339,
      "step": 1171
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.04032636433839798,
      "learning_rate": 0.00012207011686143572,
      "loss": 0.3242,
      "step": 1172
    },
    {
      "epoch": 1.955,
      "grad_norm": 0.03501078113913536,
      "learning_rate": 0.00012200333889816362,
      "loss": 0.2415,
      "step": 1173
    },
    {
      "epoch": 1.9566666666666666,
      "grad_norm": 0.03502175956964493,
      "learning_rate": 0.00012193656093489149,
      "loss": 0.2791,
      "step": 1174
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.04669326916337013,
      "learning_rate": 0.00012186978297161938,
      "loss": 0.3513,
      "step": 1175
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.03518512472510338,
      "learning_rate": 0.00012180300500834725,
      "loss": 0.2499,
      "step": 1176
    },
    {
      "epoch": 1.9616666666666667,
      "grad_norm": 0.03784247860312462,
      "learning_rate": 0.00012173622704507512,
      "loss": 0.2596,
      "step": 1177
    },
    {
      "epoch": 1.9633333333333334,
      "grad_norm": 0.07321161776781082,
      "learning_rate": 0.00012166944908180303,
      "loss": 0.4059,
      "step": 1178
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 0.07648066431283951,
      "learning_rate": 0.0001216026711185309,
      "loss": 0.3981,
      "step": 1179
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.04671837389469147,
      "learning_rate": 0.00012153589315525877,
      "loss": 0.3058,
      "step": 1180
    },
    {
      "epoch": 1.9683333333333333,
      "grad_norm": 0.05132994428277016,
      "learning_rate": 0.00012146911519198664,
      "loss": 0.3086,
      "step": 1181
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.039940882474184036,
      "learning_rate": 0.00012140233722871452,
      "loss": 0.2736,
      "step": 1182
    },
    {
      "epoch": 1.9716666666666667,
      "grad_norm": 0.04608076065778732,
      "learning_rate": 0.0001213355592654424,
      "loss": 0.342,
      "step": 1183
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.039566028863191605,
      "learning_rate": 0.00012126878130217029,
      "loss": 0.2808,
      "step": 1184
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.03335586562752724,
      "learning_rate": 0.00012120200333889818,
      "loss": 0.2124,
      "step": 1185
    },
    {
      "epoch": 1.9766666666666666,
      "grad_norm": 0.03265528008341789,
      "learning_rate": 0.00012113522537562605,
      "loss": 0.2382,
      "step": 1186
    },
    {
      "epoch": 1.9783333333333335,
      "grad_norm": 0.048312555998563766,
      "learning_rate": 0.00012106844741235392,
      "loss": 0.3313,
      "step": 1187
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.0679377019405365,
      "learning_rate": 0.0001210016694490818,
      "loss": 0.3322,
      "step": 1188
    },
    {
      "epoch": 1.9816666666666667,
      "grad_norm": 0.054133232682943344,
      "learning_rate": 0.0001209348914858097,
      "loss": 0.2452,
      "step": 1189
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.06654620170593262,
      "learning_rate": 0.00012086811352253757,
      "loss": 0.3705,
      "step": 1190
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 0.057237282395362854,
      "learning_rate": 0.00012080133555926544,
      "loss": 0.3936,
      "step": 1191
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.05323536694049835,
      "learning_rate": 0.00012073455759599333,
      "loss": 0.3878,
      "step": 1192
    },
    {
      "epoch": 1.9883333333333333,
      "grad_norm": 0.053447891026735306,
      "learning_rate": 0.0001206677796327212,
      "loss": 0.3099,
      "step": 1193
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.05408481881022453,
      "learning_rate": 0.0001206010016694491,
      "loss": 0.3395,
      "step": 1194
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 0.04290888085961342,
      "learning_rate": 0.00012053422370617698,
      "loss": 0.2569,
      "step": 1195
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.05431116744875908,
      "learning_rate": 0.00012046744574290485,
      "loss": 0.323,
      "step": 1196
    },
    {
      "epoch": 1.995,
      "grad_norm": 0.04820726811885834,
      "learning_rate": 0.00012040066777963272,
      "loss": 0.2842,
      "step": 1197
    },
    {
      "epoch": 1.9966666666666666,
      "grad_norm": 0.043807029724121094,
      "learning_rate": 0.0001203338898163606,
      "loss": 0.3166,
      "step": 1198
    },
    {
      "epoch": 1.9983333333333333,
      "grad_norm": 0.0364585816860199,
      "learning_rate": 0.00012026711185308848,
      "loss": 0.2895,
      "step": 1199
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.0497480109333992,
      "learning_rate": 0.00012020033388981637,
      "loss": 0.3132,
      "step": 1200
    },
    {
      "epoch": 2.0016666666666665,
      "grad_norm": 0.04301776364445686,
      "learning_rate": 0.00012013355592654426,
      "loss": 0.2962,
      "step": 1201
    },
    {
      "epoch": 2.0033333333333334,
      "grad_norm": 0.04695621132850647,
      "learning_rate": 0.00012006677796327213,
      "loss": 0.3224,
      "step": 1202
    },
    {
      "epoch": 2.005,
      "grad_norm": 0.03328459709882736,
      "learning_rate": 0.00012,
      "loss": 0.2763,
      "step": 1203
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.04892897605895996,
      "learning_rate": 0.00011993322203672788,
      "loss": 0.2917,
      "step": 1204
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.09201797097921371,
      "learning_rate": 0.00011986644407345578,
      "loss": 0.4178,
      "step": 1205
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.07487814873456955,
      "learning_rate": 0.00011979966611018365,
      "loss": 0.3638,
      "step": 1206
    },
    {
      "epoch": 2.0116666666666667,
      "grad_norm": 0.03581713140010834,
      "learning_rate": 0.00011973288814691152,
      "loss": 0.2592,
      "step": 1207
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.04913928732275963,
      "learning_rate": 0.0001196661101836394,
      "loss": 0.286,
      "step": 1208
    },
    {
      "epoch": 2.015,
      "grad_norm": 0.05776914954185486,
      "learning_rate": 0.00011959933222036728,
      "loss": 0.3143,
      "step": 1209
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.05267548933625221,
      "learning_rate": 0.00011953255425709517,
      "loss": 0.2611,
      "step": 1210
    },
    {
      "epoch": 2.0183333333333335,
      "grad_norm": 0.046605128794908524,
      "learning_rate": 0.00011946577629382306,
      "loss": 0.268,
      "step": 1211
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.04790177568793297,
      "learning_rate": 0.00011939899833055093,
      "loss": 0.2693,
      "step": 1212
    },
    {
      "epoch": 2.0216666666666665,
      "grad_norm": 0.06168558448553085,
      "learning_rate": 0.0001193322203672788,
      "loss": 0.3532,
      "step": 1213
    },
    {
      "epoch": 2.0233333333333334,
      "grad_norm": 0.04422501474618912,
      "learning_rate": 0.00011926544240400668,
      "loss": 0.2868,
      "step": 1214
    },
    {
      "epoch": 2.025,
      "grad_norm": 0.04268654063344002,
      "learning_rate": 0.00011919866444073455,
      "loss": 0.3197,
      "step": 1215
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.050761036574840546,
      "learning_rate": 0.00011913188647746245,
      "loss": 0.3228,
      "step": 1216
    },
    {
      "epoch": 2.0283333333333333,
      "grad_norm": 0.06652382016181946,
      "learning_rate": 0.00011906510851419032,
      "loss": 0.3488,
      "step": 1217
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.061333417892456055,
      "learning_rate": 0.00011899833055091821,
      "loss": 0.3277,
      "step": 1218
    },
    {
      "epoch": 2.0316666666666667,
      "grad_norm": 0.053160157054662704,
      "learning_rate": 0.00011893155258764608,
      "loss": 0.2835,
      "step": 1219
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.0709669291973114,
      "learning_rate": 0.00011886477462437396,
      "loss": 0.3788,
      "step": 1220
    },
    {
      "epoch": 2.035,
      "grad_norm": 0.05168885737657547,
      "learning_rate": 0.00011879799666110186,
      "loss": 0.2915,
      "step": 1221
    },
    {
      "epoch": 2.0366666666666666,
      "grad_norm": 0.05158694460988045,
      "learning_rate": 0.00011873121869782973,
      "loss": 0.2606,
      "step": 1222
    },
    {
      "epoch": 2.038333333333333,
      "grad_norm": 0.05717436969280243,
      "learning_rate": 0.0001186644407345576,
      "loss": 0.3099,
      "step": 1223
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.05401004105806351,
      "learning_rate": 0.00011859766277128547,
      "loss": 0.2868,
      "step": 1224
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 0.05693448707461357,
      "learning_rate": 0.00011853088480801335,
      "loss": 0.2986,
      "step": 1225
    },
    {
      "epoch": 2.0433333333333334,
      "grad_norm": 0.06464631855487823,
      "learning_rate": 0.00011846410684474125,
      "loss": 0.338,
      "step": 1226
    },
    {
      "epoch": 2.045,
      "grad_norm": 0.04655960947275162,
      "learning_rate": 0.00011839732888146912,
      "loss": 0.3065,
      "step": 1227
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.051158301532268524,
      "learning_rate": 0.00011833055091819701,
      "loss": 0.2891,
      "step": 1228
    },
    {
      "epoch": 2.0483333333333333,
      "grad_norm": 0.04498038813471794,
      "learning_rate": 0.00011826377295492488,
      "loss": 0.3505,
      "step": 1229
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.041308749467134476,
      "learning_rate": 0.00011819699499165275,
      "loss": 0.2738,
      "step": 1230
    },
    {
      "epoch": 2.0516666666666667,
      "grad_norm": 0.036096833646297455,
      "learning_rate": 0.00011813021702838063,
      "loss": 0.2898,
      "step": 1231
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.05366828665137291,
      "learning_rate": 0.00011806343906510853,
      "loss": 0.338,
      "step": 1232
    },
    {
      "epoch": 2.055,
      "grad_norm": 0.069493867456913,
      "learning_rate": 0.0001179966611018364,
      "loss": 0.4065,
      "step": 1233
    },
    {
      "epoch": 2.0566666666666666,
      "grad_norm": 0.06753870844841003,
      "learning_rate": 0.00011792988313856427,
      "loss": 0.3368,
      "step": 1234
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.0371033139526844,
      "learning_rate": 0.00011786310517529216,
      "loss": 0.2654,
      "step": 1235
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.03463153541088104,
      "learning_rate": 0.00011779632721202003,
      "loss": 0.264,
      "step": 1236
    },
    {
      "epoch": 2.0616666666666665,
      "grad_norm": 0.055887047201395035,
      "learning_rate": 0.00011772954924874793,
      "loss": 0.3996,
      "step": 1237
    },
    {
      "epoch": 2.0633333333333335,
      "grad_norm": 0.04600432515144348,
      "learning_rate": 0.00011766277128547581,
      "loss": 0.2363,
      "step": 1238
    },
    {
      "epoch": 2.065,
      "grad_norm": 0.04478434473276138,
      "learning_rate": 0.00011759599332220368,
      "loss": 0.2813,
      "step": 1239
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.07549832761287689,
      "learning_rate": 0.00011752921535893155,
      "loss": 0.3249,
      "step": 1240
    },
    {
      "epoch": 2.0683333333333334,
      "grad_norm": 0.060893427580595016,
      "learning_rate": 0.00011746243739565943,
      "loss": 0.3581,
      "step": 1241
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.03600490093231201,
      "learning_rate": 0.00011739565943238733,
      "loss": 0.2735,
      "step": 1242
    },
    {
      "epoch": 2.0716666666666668,
      "grad_norm": 0.06484731286764145,
      "learning_rate": 0.0001173288814691152,
      "loss": 0.3156,
      "step": 1243
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.047721635550260544,
      "learning_rate": 0.00011726210350584307,
      "loss": 0.2556,
      "step": 1244
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.04867493361234665,
      "learning_rate": 0.00011719532554257096,
      "loss": 0.2812,
      "step": 1245
    },
    {
      "epoch": 2.0766666666666667,
      "grad_norm": 0.043749306350946426,
      "learning_rate": 0.00011712854757929883,
      "loss": 0.2899,
      "step": 1246
    },
    {
      "epoch": 2.078333333333333,
      "grad_norm": 0.07606405764818192,
      "learning_rate": 0.0001170617696160267,
      "loss": 0.434,
      "step": 1247
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.03311292082071304,
      "learning_rate": 0.0001169949916527546,
      "loss": 0.2225,
      "step": 1248
    },
    {
      "epoch": 2.0816666666666666,
      "grad_norm": 0.04637237638235092,
      "learning_rate": 0.00011692821368948248,
      "loss": 0.2598,
      "step": 1249
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.07954106479883194,
      "learning_rate": 0.00011686143572621035,
      "loss": 0.2734,
      "step": 1250
    },
    {
      "epoch": 2.085,
      "grad_norm": 0.03784738853573799,
      "learning_rate": 0.00011679465776293823,
      "loss": 0.28,
      "step": 1251
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.04352603852748871,
      "learning_rate": 0.00011672787979966611,
      "loss": 0.2822,
      "step": 1252
    },
    {
      "epoch": 2.0883333333333334,
      "grad_norm": 0.04250387102365494,
      "learning_rate": 0.000116661101836394,
      "loss": 0.2837,
      "step": 1253
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.04520426690578461,
      "learning_rate": 0.00011659432387312189,
      "loss": 0.2885,
      "step": 1254
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 0.05342428386211395,
      "learning_rate": 0.00011652754590984976,
      "loss": 0.3319,
      "step": 1255
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.053849637508392334,
      "learning_rate": 0.00011646076794657763,
      "loss": 0.3392,
      "step": 1256
    },
    {
      "epoch": 2.095,
      "grad_norm": 0.05088827759027481,
      "learning_rate": 0.0001163939899833055,
      "loss": 0.3065,
      "step": 1257
    },
    {
      "epoch": 2.0966666666666667,
      "grad_norm": 0.036561690270900726,
      "learning_rate": 0.0001163272120200334,
      "loss": 0.2581,
      "step": 1258
    },
    {
      "epoch": 2.098333333333333,
      "grad_norm": 0.05425052344799042,
      "learning_rate": 0.00011626043405676128,
      "loss": 0.394,
      "step": 1259
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.05010334029793739,
      "learning_rate": 0.00011619365609348915,
      "loss": 0.3283,
      "step": 1260
    },
    {
      "epoch": 2.1016666666666666,
      "grad_norm": 0.04656018316745758,
      "learning_rate": 0.00011612687813021703,
      "loss": 0.3648,
      "step": 1261
    },
    {
      "epoch": 2.1033333333333335,
      "grad_norm": 0.04638374596834183,
      "learning_rate": 0.00011606010016694491,
      "loss": 0.3413,
      "step": 1262
    },
    {
      "epoch": 2.105,
      "grad_norm": 0.05373930558562279,
      "learning_rate": 0.00011599332220367279,
      "loss": 0.3145,
      "step": 1263
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.04815629869699478,
      "learning_rate": 0.00011592654424040069,
      "loss": 0.2571,
      "step": 1264
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.03963738679885864,
      "learning_rate": 0.00011585976627712856,
      "loss": 0.2162,
      "step": 1265
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.05108199641108513,
      "learning_rate": 0.00011579298831385643,
      "loss": 0.2871,
      "step": 1266
    },
    {
      "epoch": 2.111666666666667,
      "grad_norm": 0.045757319778203964,
      "learning_rate": 0.0001157262103505843,
      "loss": 0.294,
      "step": 1267
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.07438202202320099,
      "learning_rate": 0.00011565943238731218,
      "loss": 0.3619,
      "step": 1268
    },
    {
      "epoch": 2.115,
      "grad_norm": 0.03945685923099518,
      "learning_rate": 0.00011559265442404008,
      "loss": 0.2387,
      "step": 1269
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.043085549026727676,
      "learning_rate": 0.00011552587646076795,
      "loss": 0.3168,
      "step": 1270
    },
    {
      "epoch": 2.118333333333333,
      "grad_norm": 0.043303411453962326,
      "learning_rate": 0.00011545909849749584,
      "loss": 0.3265,
      "step": 1271
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.037291768938302994,
      "learning_rate": 0.00011539232053422371,
      "loss": 0.2302,
      "step": 1272
    },
    {
      "epoch": 2.1216666666666666,
      "grad_norm": 0.04229063168168068,
      "learning_rate": 0.00011532554257095158,
      "loss": 0.2627,
      "step": 1273
    },
    {
      "epoch": 2.1233333333333335,
      "grad_norm": 0.04173721373081207,
      "learning_rate": 0.00011525876460767948,
      "loss": 0.2962,
      "step": 1274
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.09908606112003326,
      "learning_rate": 0.00011519198664440736,
      "loss": 0.3874,
      "step": 1275
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.0525374710559845,
      "learning_rate": 0.00011512520868113523,
      "loss": 0.3574,
      "step": 1276
    },
    {
      "epoch": 2.1283333333333334,
      "grad_norm": 0.047327496111392975,
      "learning_rate": 0.0001150584307178631,
      "loss": 0.3074,
      "step": 1277
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.03906570002436638,
      "learning_rate": 0.00011499165275459098,
      "loss": 0.2517,
      "step": 1278
    },
    {
      "epoch": 2.131666666666667,
      "grad_norm": 0.05297704413533211,
      "learning_rate": 0.00011492487479131886,
      "loss": 0.3171,
      "step": 1279
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.05640590935945511,
      "learning_rate": 0.00011485809682804675,
      "loss": 0.3066,
      "step": 1280
    },
    {
      "epoch": 2.135,
      "grad_norm": 0.03975578024983406,
      "learning_rate": 0.00011479131886477464,
      "loss": 0.2516,
      "step": 1281
    },
    {
      "epoch": 2.1366666666666667,
      "grad_norm": 0.045066531747579575,
      "learning_rate": 0.00011472454090150251,
      "loss": 0.3042,
      "step": 1282
    },
    {
      "epoch": 2.138333333333333,
      "grad_norm": 0.04243871569633484,
      "learning_rate": 0.00011465776293823038,
      "loss": 0.2633,
      "step": 1283
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.03583915904164314,
      "learning_rate": 0.00011459098497495826,
      "loss": 0.2419,
      "step": 1284
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 0.03844531625509262,
      "learning_rate": 0.00011452420701168616,
      "loss": 0.293,
      "step": 1285
    },
    {
      "epoch": 2.1433333333333335,
      "grad_norm": 0.04643364995718002,
      "learning_rate": 0.00011445742904841403,
      "loss": 0.3394,
      "step": 1286
    },
    {
      "epoch": 2.145,
      "grad_norm": 0.06798799335956573,
      "learning_rate": 0.0001143906510851419,
      "loss": 0.3206,
      "step": 1287
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.04110465571284294,
      "learning_rate": 0.00011432387312186979,
      "loss": 0.2962,
      "step": 1288
    },
    {
      "epoch": 2.1483333333333334,
      "grad_norm": 0.059502556920051575,
      "learning_rate": 0.00011425709515859766,
      "loss": 0.3362,
      "step": 1289
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.05724635720252991,
      "learning_rate": 0.00011419031719532556,
      "loss": 0.3601,
      "step": 1290
    },
    {
      "epoch": 2.151666666666667,
      "grad_norm": 0.07895616441965103,
      "learning_rate": 0.00011412353923205344,
      "loss": 0.3575,
      "step": 1291
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.04843952879309654,
      "learning_rate": 0.00011405676126878131,
      "loss": 0.2698,
      "step": 1292
    },
    {
      "epoch": 2.155,
      "grad_norm": 0.04575173929333687,
      "learning_rate": 0.00011398998330550918,
      "loss": 0.2741,
      "step": 1293
    },
    {
      "epoch": 2.1566666666666667,
      "grad_norm": 0.03620077669620514,
      "learning_rate": 0.00011392320534223706,
      "loss": 0.3065,
      "step": 1294
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 0.04040117561817169,
      "learning_rate": 0.00011385642737896493,
      "loss": 0.3059,
      "step": 1295
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.041057880967855453,
      "learning_rate": 0.00011378964941569283,
      "loss": 0.2541,
      "step": 1296
    },
    {
      "epoch": 2.1616666666666666,
      "grad_norm": 0.041201863437891006,
      "learning_rate": 0.0001137228714524207,
      "loss": 0.2531,
      "step": 1297
    },
    {
      "epoch": 2.163333333333333,
      "grad_norm": 0.04538583755493164,
      "learning_rate": 0.00011365609348914859,
      "loss": 0.3106,
      "step": 1298
    },
    {
      "epoch": 2.165,
      "grad_norm": 0.048856984823942184,
      "learning_rate": 0.00011358931552587646,
      "loss": 0.3562,
      "step": 1299
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.03312812000513077,
      "learning_rate": 0.00011352253756260434,
      "loss": 0.2841,
      "step": 1300
    },
    {
      "epoch": 2.1683333333333334,
      "grad_norm": 0.03758879378437996,
      "learning_rate": 0.00011345575959933224,
      "loss": 0.285,
      "step": 1301
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.04096600413322449,
      "learning_rate": 0.00011338898163606011,
      "loss": 0.2328,
      "step": 1302
    },
    {
      "epoch": 2.171666666666667,
      "grad_norm": 0.04577379673719406,
      "learning_rate": 0.00011332220367278798,
      "loss": 0.3275,
      "step": 1303
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.05625300481915474,
      "learning_rate": 0.00011325542570951586,
      "loss": 0.31,
      "step": 1304
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.04204051196575165,
      "learning_rate": 0.00011318864774624374,
      "loss": 0.3193,
      "step": 1305
    },
    {
      "epoch": 2.1766666666666667,
      "grad_norm": 0.03669551759958267,
      "learning_rate": 0.00011312186978297163,
      "loss": 0.2619,
      "step": 1306
    },
    {
      "epoch": 2.1783333333333332,
      "grad_norm": 0.0501430407166481,
      "learning_rate": 0.00011305509181969952,
      "loss": 0.3343,
      "step": 1307
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.050863105803728104,
      "learning_rate": 0.00011298831385642739,
      "loss": 0.3702,
      "step": 1308
    },
    {
      "epoch": 2.1816666666666666,
      "grad_norm": 0.03904383257031441,
      "learning_rate": 0.00011292153589315526,
      "loss": 0.3204,
      "step": 1309
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.06822840869426727,
      "learning_rate": 0.00011285475792988314,
      "loss": 0.3521,
      "step": 1310
    },
    {
      "epoch": 2.185,
      "grad_norm": 0.0567891001701355,
      "learning_rate": 0.00011278797996661104,
      "loss": 0.3248,
      "step": 1311
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.04745042696595192,
      "learning_rate": 0.00011272120200333891,
      "loss": 0.3027,
      "step": 1312
    },
    {
      "epoch": 2.1883333333333335,
      "grad_norm": 0.025709282606840134,
      "learning_rate": 0.00011265442404006678,
      "loss": 0.2223,
      "step": 1313
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.06083907559514046,
      "learning_rate": 0.00011258764607679465,
      "loss": 0.3983,
      "step": 1314
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 0.06139684468507767,
      "learning_rate": 0.00011252086811352254,
      "loss": 0.4065,
      "step": 1315
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.06708298623561859,
      "learning_rate": 0.00011245409015025041,
      "loss": 0.3596,
      "step": 1316
    },
    {
      "epoch": 2.195,
      "grad_norm": 0.04532661661505699,
      "learning_rate": 0.00011238731218697832,
      "loss": 0.2637,
      "step": 1317
    },
    {
      "epoch": 2.1966666666666668,
      "grad_norm": 0.03970969840884209,
      "learning_rate": 0.00011232053422370619,
      "loss": 0.3214,
      "step": 1318
    },
    {
      "epoch": 2.1983333333333333,
      "grad_norm": 0.06071338430047035,
      "learning_rate": 0.00011225375626043406,
      "loss": 0.3532,
      "step": 1319
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.046101249754428864,
      "learning_rate": 0.00011218697829716193,
      "loss": 0.3271,
      "step": 1320
    },
    {
      "epoch": 2.2016666666666667,
      "grad_norm": 0.048069532960653305,
      "learning_rate": 0.00011212020033388981,
      "loss": 0.3596,
      "step": 1321
    },
    {
      "epoch": 2.203333333333333,
      "grad_norm": 0.037619784474372864,
      "learning_rate": 0.00011205342237061771,
      "loss": 0.301,
      "step": 1322
    },
    {
      "epoch": 2.205,
      "grad_norm": 0.052347343415021896,
      "learning_rate": 0.00011198664440734558,
      "loss": 0.3647,
      "step": 1323
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.0492757223546505,
      "learning_rate": 0.00011191986644407347,
      "loss": 0.3142,
      "step": 1324
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 0.06685268133878708,
      "learning_rate": 0.00011185308848080134,
      "loss": 0.4283,
      "step": 1325
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.0422087237238884,
      "learning_rate": 0.00011178631051752921,
      "loss": 0.3026,
      "step": 1326
    },
    {
      "epoch": 2.211666666666667,
      "grad_norm": 0.0540628507733345,
      "learning_rate": 0.00011171953255425711,
      "loss": 0.3743,
      "step": 1327
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.0715484470129013,
      "learning_rate": 0.00011165275459098499,
      "loss": 0.3723,
      "step": 1328
    },
    {
      "epoch": 2.215,
      "grad_norm": 0.04975099489092827,
      "learning_rate": 0.00011158597662771286,
      "loss": 0.3596,
      "step": 1329
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.07029929012060165,
      "learning_rate": 0.00011151919866444073,
      "loss": 0.3037,
      "step": 1330
    },
    {
      "epoch": 2.2183333333333333,
      "grad_norm": 0.05714070051908493,
      "learning_rate": 0.00011145242070116862,
      "loss": 0.3412,
      "step": 1331
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.032669927924871445,
      "learning_rate": 0.0001113856427378965,
      "loss": 0.2463,
      "step": 1332
    },
    {
      "epoch": 2.2216666666666667,
      "grad_norm": 0.04589973762631416,
      "learning_rate": 0.0001113188647746244,
      "loss": 0.2883,
      "step": 1333
    },
    {
      "epoch": 2.223333333333333,
      "grad_norm": 0.050850581377744675,
      "learning_rate": 0.00011125208681135227,
      "loss": 0.3393,
      "step": 1334
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.04126717895269394,
      "learning_rate": 0.00011118530884808014,
      "loss": 0.3097,
      "step": 1335
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.108832947909832,
      "learning_rate": 0.00011111853088480801,
      "loss": 0.4379,
      "step": 1336
    },
    {
      "epoch": 2.2283333333333335,
      "grad_norm": 0.05090248957276344,
      "learning_rate": 0.00011105175292153589,
      "loss": 0.3108,
      "step": 1337
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.04152049869298935,
      "learning_rate": 0.00011098497495826379,
      "loss": 0.3456,
      "step": 1338
    },
    {
      "epoch": 2.2316666666666665,
      "grad_norm": 0.052308954298496246,
      "learning_rate": 0.00011091819699499166,
      "loss": 0.3684,
      "step": 1339
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.04100535809993744,
      "learning_rate": 0.00011085141903171953,
      "loss": 0.2687,
      "step": 1340
    },
    {
      "epoch": 2.235,
      "grad_norm": 0.04624568670988083,
      "learning_rate": 0.00011078464106844742,
      "loss": 0.2804,
      "step": 1341
    },
    {
      "epoch": 2.236666666666667,
      "grad_norm": 0.05360019579529762,
      "learning_rate": 0.00011071786310517529,
      "loss": 0.3219,
      "step": 1342
    },
    {
      "epoch": 2.2383333333333333,
      "grad_norm": 0.06889837980270386,
      "learning_rate": 0.0001106510851419032,
      "loss": 0.343,
      "step": 1343
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.04372263699769974,
      "learning_rate": 0.00011058430717863107,
      "loss": 0.2988,
      "step": 1344
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 0.03760122135281563,
      "learning_rate": 0.00011051752921535894,
      "loss": 0.2645,
      "step": 1345
    },
    {
      "epoch": 2.243333333333333,
      "grad_norm": 0.043109696358442307,
      "learning_rate": 0.00011045075125208681,
      "loss": 0.3019,
      "step": 1346
    },
    {
      "epoch": 2.245,
      "grad_norm": 0.04281935095787048,
      "learning_rate": 0.00011038397328881469,
      "loss": 0.3023,
      "step": 1347
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.050939857959747314,
      "learning_rate": 0.00011031719532554257,
      "loss": 0.3137,
      "step": 1348
    },
    {
      "epoch": 2.2483333333333335,
      "grad_norm": 0.047550491988658905,
      "learning_rate": 0.00011025041736227046,
      "loss": 0.306,
      "step": 1349
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.03763650730252266,
      "learning_rate": 0.00011018363939899835,
      "loss": 0.273,
      "step": 1350
    },
    {
      "epoch": 2.2516666666666665,
      "grad_norm": 0.05680291727185249,
      "learning_rate": 0.00011011686143572622,
      "loss": 0.3371,
      "step": 1351
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.04880351573228836,
      "learning_rate": 0.00011005008347245409,
      "loss": 0.2968,
      "step": 1352
    },
    {
      "epoch": 2.255,
      "grad_norm": 0.041535984724760056,
      "learning_rate": 0.00010998330550918197,
      "loss": 0.2605,
      "step": 1353
    },
    {
      "epoch": 2.256666666666667,
      "grad_norm": 0.040757179260253906,
      "learning_rate": 0.00010991652754590987,
      "loss": 0.2877,
      "step": 1354
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.04649787396192551,
      "learning_rate": 0.00010984974958263774,
      "loss": 0.2928,
      "step": 1355
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.04432113841176033,
      "learning_rate": 0.00010978297161936561,
      "loss": 0.3453,
      "step": 1356
    },
    {
      "epoch": 2.2616666666666667,
      "grad_norm": 0.057204846292734146,
      "learning_rate": 0.00010971619365609349,
      "loss": 0.361,
      "step": 1357
    },
    {
      "epoch": 2.263333333333333,
      "grad_norm": 0.04873208701610565,
      "learning_rate": 0.00010964941569282137,
      "loss": 0.3309,
      "step": 1358
    },
    {
      "epoch": 2.265,
      "grad_norm": 0.0430399626493454,
      "learning_rate": 0.00010958263772954926,
      "loss": 0.3367,
      "step": 1359
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.04419350624084473,
      "learning_rate": 0.00010951585976627715,
      "loss": 0.3069,
      "step": 1360
    },
    {
      "epoch": 2.2683333333333335,
      "grad_norm": 0.04080675169825554,
      "learning_rate": 0.00010944908180300502,
      "loss": 0.3404,
      "step": 1361
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.029385805130004883,
      "learning_rate": 0.00010938230383973289,
      "loss": 0.2315,
      "step": 1362
    },
    {
      "epoch": 2.2716666666666665,
      "grad_norm": 0.05341643467545509,
      "learning_rate": 0.00010931552587646076,
      "loss": 0.347,
      "step": 1363
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.04181624576449394,
      "learning_rate": 0.00010924874791318864,
      "loss": 0.2952,
      "step": 1364
    },
    {
      "epoch": 2.275,
      "grad_norm": 0.055085428059101105,
      "learning_rate": 0.00010918196994991654,
      "loss": 0.3287,
      "step": 1365
    },
    {
      "epoch": 2.276666666666667,
      "grad_norm": 0.056234534829854965,
      "learning_rate": 0.00010911519198664441,
      "loss": 0.3361,
      "step": 1366
    },
    {
      "epoch": 2.2783333333333333,
      "grad_norm": 0.04931879788637161,
      "learning_rate": 0.0001090484140233723,
      "loss": 0.3286,
      "step": 1367
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.043716371059417725,
      "learning_rate": 0.00010898163606010017,
      "loss": 0.3393,
      "step": 1368
    },
    {
      "epoch": 2.2816666666666667,
      "grad_norm": 0.03245168551802635,
      "learning_rate": 0.00010891485809682804,
      "loss": 0.2302,
      "step": 1369
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.04572686180472374,
      "learning_rate": 0.00010884808013355594,
      "loss": 0.3107,
      "step": 1370
    },
    {
      "epoch": 2.285,
      "grad_norm": 0.04401659592986107,
      "learning_rate": 0.00010878130217028382,
      "loss": 0.3246,
      "step": 1371
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.043931297957897186,
      "learning_rate": 0.00010871452420701169,
      "loss": 0.3358,
      "step": 1372
    },
    {
      "epoch": 2.288333333333333,
      "grad_norm": 0.05000155046582222,
      "learning_rate": 0.00010864774624373956,
      "loss": 0.3867,
      "step": 1373
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.04099830612540245,
      "learning_rate": 0.00010858096828046744,
      "loss": 0.3148,
      "step": 1374
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.04383559897542,
      "learning_rate": 0.00010851419031719534,
      "loss": 0.3317,
      "step": 1375
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.04229459911584854,
      "learning_rate": 0.00010844741235392321,
      "loss": 0.327,
      "step": 1376
    },
    {
      "epoch": 2.295,
      "grad_norm": 0.03466473147273064,
      "learning_rate": 0.0001083806343906511,
      "loss": 0.2609,
      "step": 1377
    },
    {
      "epoch": 2.296666666666667,
      "grad_norm": 0.0394781120121479,
      "learning_rate": 0.00010831385642737897,
      "loss": 0.2967,
      "step": 1378
    },
    {
      "epoch": 2.2983333333333333,
      "grad_norm": 0.05149355158209801,
      "learning_rate": 0.00010824707846410684,
      "loss": 0.3836,
      "step": 1379
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.04335353523492813,
      "learning_rate": 0.00010818030050083472,
      "loss": 0.351,
      "step": 1380
    },
    {
      "epoch": 2.3016666666666667,
      "grad_norm": 0.04405440017580986,
      "learning_rate": 0.00010811352253756262,
      "loss": 0.3123,
      "step": 1381
    },
    {
      "epoch": 2.3033333333333332,
      "grad_norm": 0.038893867284059525,
      "learning_rate": 0.00010804674457429049,
      "loss": 0.289,
      "step": 1382
    },
    {
      "epoch": 2.305,
      "grad_norm": 0.08164285123348236,
      "learning_rate": 0.00010797996661101836,
      "loss": 0.3821,
      "step": 1383
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.05829804763197899,
      "learning_rate": 0.00010791318864774625,
      "loss": 0.3847,
      "step": 1384
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 0.04495077207684517,
      "learning_rate": 0.00010784641068447412,
      "loss": 0.379,
      "step": 1385
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.037379465997219086,
      "learning_rate": 0.00010777963272120202,
      "loss": 0.3035,
      "step": 1386
    },
    {
      "epoch": 2.3116666666666665,
      "grad_norm": 0.06868374347686768,
      "learning_rate": 0.0001077128547579299,
      "loss": 0.3282,
      "step": 1387
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.04402051866054535,
      "learning_rate": 0.00010764607679465777,
      "loss": 0.3685,
      "step": 1388
    },
    {
      "epoch": 2.315,
      "grad_norm": 0.045365747064352036,
      "learning_rate": 0.00010757929883138564,
      "loss": 0.3306,
      "step": 1389
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.05386698991060257,
      "learning_rate": 0.00010751252086811352,
      "loss": 0.295,
      "step": 1390
    },
    {
      "epoch": 2.3183333333333334,
      "grad_norm": 0.032515693455934525,
      "learning_rate": 0.00010744574290484142,
      "loss": 0.2196,
      "step": 1391
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.03688321262598038,
      "learning_rate": 0.00010737896494156929,
      "loss": 0.2676,
      "step": 1392
    },
    {
      "epoch": 2.3216666666666668,
      "grad_norm": 0.04910999536514282,
      "learning_rate": 0.00010731218697829716,
      "loss": 0.2799,
      "step": 1393
    },
    {
      "epoch": 2.3233333333333333,
      "grad_norm": 0.04248809069395065,
      "learning_rate": 0.00010724540901502505,
      "loss": 0.2644,
      "step": 1394
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.04116111993789673,
      "learning_rate": 0.00010717863105175292,
      "loss": 0.3013,
      "step": 1395
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.062009792774915695,
      "learning_rate": 0.0001071118530884808,
      "loss": 0.2973,
      "step": 1396
    },
    {
      "epoch": 2.328333333333333,
      "grad_norm": 0.036329109221696854,
      "learning_rate": 0.0001070450751252087,
      "loss": 0.2141,
      "step": 1397
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.0481041744351387,
      "learning_rate": 0.00010697829716193657,
      "loss": 0.3501,
      "step": 1398
    },
    {
      "epoch": 2.3316666666666666,
      "grad_norm": 0.03341355919837952,
      "learning_rate": 0.00010691151919866444,
      "loss": 0.25,
      "step": 1399
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.0625496357679367,
      "learning_rate": 0.00010684474123539232,
      "loss": 0.3125,
      "step": 1400
    },
    {
      "epoch": 2.335,
      "grad_norm": 0.04510776698589325,
      "learning_rate": 0.0001067779632721202,
      "loss": 0.2956,
      "step": 1401
    },
    {
      "epoch": 2.336666666666667,
      "grad_norm": 0.04404119774699211,
      "learning_rate": 0.00010671118530884809,
      "loss": 0.2968,
      "step": 1402
    },
    {
      "epoch": 2.3383333333333334,
      "grad_norm": 0.050552189350128174,
      "learning_rate": 0.00010664440734557598,
      "loss": 0.3081,
      "step": 1403
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.03290446847677231,
      "learning_rate": 0.00010657762938230385,
      "loss": 0.2827,
      "step": 1404
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.04212913289666176,
      "learning_rate": 0.00010651085141903172,
      "loss": 0.3014,
      "step": 1405
    },
    {
      "epoch": 2.3433333333333333,
      "grad_norm": 0.049999527633190155,
      "learning_rate": 0.0001064440734557596,
      "loss": 0.3589,
      "step": 1406
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 0.04403725266456604,
      "learning_rate": 0.0001063772954924875,
      "loss": 0.3754,
      "step": 1407
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.03893263638019562,
      "learning_rate": 0.00010631051752921537,
      "loss": 0.273,
      "step": 1408
    },
    {
      "epoch": 2.348333333333333,
      "grad_norm": 0.03134392946958542,
      "learning_rate": 0.00010624373956594324,
      "loss": 0.2499,
      "step": 1409
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.05040867626667023,
      "learning_rate": 0.00010617696160267111,
      "loss": 0.3533,
      "step": 1410
    },
    {
      "epoch": 2.3516666666666666,
      "grad_norm": 0.05000928416848183,
      "learning_rate": 0.000106110183639399,
      "loss": 0.3373,
      "step": 1411
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.061851173639297485,
      "learning_rate": 0.00010604340567612687,
      "loss": 0.3079,
      "step": 1412
    },
    {
      "epoch": 2.355,
      "grad_norm": 0.03486660495400429,
      "learning_rate": 0.00010597662771285477,
      "loss": 0.2064,
      "step": 1413
    },
    {
      "epoch": 2.3566666666666665,
      "grad_norm": 0.09169895201921463,
      "learning_rate": 0.00010590984974958265,
      "loss": 0.3719,
      "step": 1414
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 0.03266946226358414,
      "learning_rate": 0.00010584307178631052,
      "loss": 0.2807,
      "step": 1415
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.04826796427369118,
      "learning_rate": 0.0001057762938230384,
      "loss": 0.3029,
      "step": 1416
    },
    {
      "epoch": 2.361666666666667,
      "grad_norm": 0.03472161293029785,
      "learning_rate": 0.00010570951585976627,
      "loss": 0.2631,
      "step": 1417
    },
    {
      "epoch": 2.3633333333333333,
      "grad_norm": 0.04982312396168709,
      "learning_rate": 0.00010564273789649417,
      "loss": 0.3509,
      "step": 1418
    },
    {
      "epoch": 2.365,
      "grad_norm": 0.044585008174180984,
      "learning_rate": 0.00010557595993322204,
      "loss": 0.3208,
      "step": 1419
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.0367768369615078,
      "learning_rate": 0.00010550918196994993,
      "loss": 0.2658,
      "step": 1420
    },
    {
      "epoch": 2.368333333333333,
      "grad_norm": 0.04895355924963951,
      "learning_rate": 0.0001054424040066778,
      "loss": 0.2705,
      "step": 1421
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.03240306302905083,
      "learning_rate": 0.00010537562604340567,
      "loss": 0.2677,
      "step": 1422
    },
    {
      "epoch": 2.3716666666666666,
      "grad_norm": 0.0411730594933033,
      "learning_rate": 0.00010530884808013357,
      "loss": 0.2537,
      "step": 1423
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.044723402708768845,
      "learning_rate": 0.00010524207011686145,
      "loss": 0.2805,
      "step": 1424
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.039580296725034714,
      "learning_rate": 0.00010517529215358932,
      "loss": 0.2558,
      "step": 1425
    },
    {
      "epoch": 2.3766666666666665,
      "grad_norm": 0.0568104088306427,
      "learning_rate": 0.0001051085141903172,
      "loss": 0.3363,
      "step": 1426
    },
    {
      "epoch": 2.3783333333333334,
      "grad_norm": 0.0723712220788002,
      "learning_rate": 0.00010504173622704507,
      "loss": 0.3595,
      "step": 1427
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.0555865503847599,
      "learning_rate": 0.00010497495826377295,
      "loss": 0.3461,
      "step": 1428
    },
    {
      "epoch": 2.381666666666667,
      "grad_norm": 0.04186144843697548,
      "learning_rate": 0.00010490818030050084,
      "loss": 0.2923,
      "step": 1429
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.05412086099386215,
      "learning_rate": 0.00010484140233722873,
      "loss": 0.347,
      "step": 1430
    },
    {
      "epoch": 2.385,
      "grad_norm": 0.05489248037338257,
      "learning_rate": 0.0001047746243739566,
      "loss": 0.3451,
      "step": 1431
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.040147196501493454,
      "learning_rate": 0.00010470784641068447,
      "loss": 0.3164,
      "step": 1432
    },
    {
      "epoch": 2.388333333333333,
      "grad_norm": 0.04177836328744888,
      "learning_rate": 0.00010464106844741235,
      "loss": 0.2573,
      "step": 1433
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.08215593546628952,
      "learning_rate": 0.00010457429048414025,
      "loss": 0.334,
      "step": 1434
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.07419685274362564,
      "learning_rate": 0.00010450751252086812,
      "loss": 0.3534,
      "step": 1435
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.039621878415346146,
      "learning_rate": 0.00010444073455759599,
      "loss": 0.2554,
      "step": 1436
    },
    {
      "epoch": 2.395,
      "grad_norm": 0.05845452472567558,
      "learning_rate": 0.00010437395659432388,
      "loss": 0.3355,
      "step": 1437
    },
    {
      "epoch": 2.3966666666666665,
      "grad_norm": 0.038017723709344864,
      "learning_rate": 0.00010430717863105175,
      "loss": 0.2635,
      "step": 1438
    },
    {
      "epoch": 2.3983333333333334,
      "grad_norm": 0.034781862050294876,
      "learning_rate": 0.00010424040066777965,
      "loss": 0.2561,
      "step": 1439
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.04372202605009079,
      "learning_rate": 0.00010417362270450753,
      "loss": 0.3515,
      "step": 1440
    },
    {
      "epoch": 2.401666666666667,
      "grad_norm": 0.04032072052359581,
      "learning_rate": 0.0001041068447412354,
      "loss": 0.3116,
      "step": 1441
    },
    {
      "epoch": 2.4033333333333333,
      "grad_norm": 0.03662048652768135,
      "learning_rate": 0.00010404006677796327,
      "loss": 0.2558,
      "step": 1442
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 0.03352939710021019,
      "learning_rate": 0.00010397328881469115,
      "loss": 0.228,
      "step": 1443
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.037490297108888626,
      "learning_rate": 0.00010390651085141905,
      "loss": 0.2497,
      "step": 1444
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.03344340994954109,
      "learning_rate": 0.00010383973288814692,
      "loss": 0.256,
      "step": 1445
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.03682905063033104,
      "learning_rate": 0.0001037729549248748,
      "loss": 0.2844,
      "step": 1446
    },
    {
      "epoch": 2.4116666666666666,
      "grad_norm": 0.046089161187410355,
      "learning_rate": 0.00010370617696160268,
      "loss": 0.3211,
      "step": 1447
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.031198538839817047,
      "learning_rate": 0.00010363939899833055,
      "loss": 0.2192,
      "step": 1448
    },
    {
      "epoch": 2.415,
      "grad_norm": 0.037006739526987076,
      "learning_rate": 0.00010357262103505843,
      "loss": 0.2511,
      "step": 1449
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.027640122920274734,
      "learning_rate": 0.00010350584307178633,
      "loss": 0.1915,
      "step": 1450
    },
    {
      "epoch": 2.4183333333333334,
      "grad_norm": 0.036473535001277924,
      "learning_rate": 0.0001034390651085142,
      "loss": 0.2545,
      "step": 1451
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.07211489230394363,
      "learning_rate": 0.00010337228714524207,
      "loss": 0.3303,
      "step": 1452
    },
    {
      "epoch": 2.421666666666667,
      "grad_norm": 0.03753171116113663,
      "learning_rate": 0.00010330550918196994,
      "loss": 0.2867,
      "step": 1453
    },
    {
      "epoch": 2.4233333333333333,
      "grad_norm": 0.04831143096089363,
      "learning_rate": 0.00010323873121869783,
      "loss": 0.332,
      "step": 1454
    },
    {
      "epoch": 2.425,
      "grad_norm": 0.043362703174352646,
      "learning_rate": 0.00010317195325542572,
      "loss": 0.3117,
      "step": 1455
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.04973810166120529,
      "learning_rate": 0.0001031051752921536,
      "loss": 0.3035,
      "step": 1456
    },
    {
      "epoch": 2.4283333333333332,
      "grad_norm": 0.045074813067913055,
      "learning_rate": 0.00010303839732888148,
      "loss": 0.3453,
      "step": 1457
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.03210065886378288,
      "learning_rate": 0.00010297161936560935,
      "loss": 0.2499,
      "step": 1458
    },
    {
      "epoch": 2.4316666666666666,
      "grad_norm": 0.051591772586107254,
      "learning_rate": 0.00010290484140233722,
      "loss": 0.2978,
      "step": 1459
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.042086780071258545,
      "learning_rate": 0.00010283806343906512,
      "loss": 0.3373,
      "step": 1460
    },
    {
      "epoch": 2.435,
      "grad_norm": 0.05760788172483444,
      "learning_rate": 0.000102771285475793,
      "loss": 0.3067,
      "step": 1461
    },
    {
      "epoch": 2.4366666666666665,
      "grad_norm": 0.050000838935375214,
      "learning_rate": 0.00010270450751252087,
      "loss": 0.3388,
      "step": 1462
    },
    {
      "epoch": 2.4383333333333335,
      "grad_norm": 0.043961744755506516,
      "learning_rate": 0.00010263772954924876,
      "loss": 0.3272,
      "step": 1463
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.039009422063827515,
      "learning_rate": 0.00010257095158597663,
      "loss": 0.3006,
      "step": 1464
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 0.04189685359597206,
      "learning_rate": 0.0001025041736227045,
      "loss": 0.2712,
      "step": 1465
    },
    {
      "epoch": 2.4433333333333334,
      "grad_norm": 0.06212511286139488,
      "learning_rate": 0.0001024373956594324,
      "loss": 0.3555,
      "step": 1466
    },
    {
      "epoch": 2.445,
      "grad_norm": 0.05445551127195358,
      "learning_rate": 0.00010237061769616028,
      "loss": 0.3327,
      "step": 1467
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.04559718817472458,
      "learning_rate": 0.00010230383973288815,
      "loss": 0.2876,
      "step": 1468
    },
    {
      "epoch": 2.4483333333333333,
      "grad_norm": 0.04328901320695877,
      "learning_rate": 0.00010223706176961602,
      "loss": 0.3143,
      "step": 1469
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.0399332158267498,
      "learning_rate": 0.0001021702838063439,
      "loss": 0.2802,
      "step": 1470
    },
    {
      "epoch": 2.4516666666666667,
      "grad_norm": 0.053718775510787964,
      "learning_rate": 0.0001021035058430718,
      "loss": 0.353,
      "step": 1471
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.06225457414984703,
      "learning_rate": 0.00010203672787979967,
      "loss": 0.4134,
      "step": 1472
    },
    {
      "epoch": 2.455,
      "grad_norm": 0.055224280804395676,
      "learning_rate": 0.00010196994991652756,
      "loss": 0.3934,
      "step": 1473
    },
    {
      "epoch": 2.4566666666666666,
      "grad_norm": 0.0434880368411541,
      "learning_rate": 0.00010190317195325543,
      "loss": 0.3333,
      "step": 1474
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.04586184769868851,
      "learning_rate": 0.0001018363939899833,
      "loss": 0.3338,
      "step": 1475
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.05217510089278221,
      "learning_rate": 0.0001017696160267112,
      "loss": 0.3305,
      "step": 1476
    },
    {
      "epoch": 2.461666666666667,
      "grad_norm": 0.05205001309514046,
      "learning_rate": 0.00010170283806343908,
      "loss": 0.3079,
      "step": 1477
    },
    {
      "epoch": 2.4633333333333334,
      "grad_norm": 0.04915407672524452,
      "learning_rate": 0.00010163606010016695,
      "loss": 0.2451,
      "step": 1478
    },
    {
      "epoch": 2.465,
      "grad_norm": 0.0557168647646904,
      "learning_rate": 0.00010156928213689482,
      "loss": 0.3579,
      "step": 1479
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.048031434416770935,
      "learning_rate": 0.00010150250417362271,
      "loss": 0.3294,
      "step": 1480
    },
    {
      "epoch": 2.4683333333333333,
      "grad_norm": 0.036160025745630264,
      "learning_rate": 0.00010143572621035058,
      "loss": 0.2485,
      "step": 1481
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.04580971226096153,
      "learning_rate": 0.00010136894824707848,
      "loss": 0.3042,
      "step": 1482
    },
    {
      "epoch": 2.4716666666666667,
      "grad_norm": 0.04843103513121605,
      "learning_rate": 0.00010130217028380636,
      "loss": 0.3334,
      "step": 1483
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.04426259547472,
      "learning_rate": 0.00010123539232053423,
      "loss": 0.245,
      "step": 1484
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.05204618349671364,
      "learning_rate": 0.0001011686143572621,
      "loss": 0.3016,
      "step": 1485
    },
    {
      "epoch": 2.4766666666666666,
      "grad_norm": 0.04717372730374336,
      "learning_rate": 0.00010110183639398998,
      "loss": 0.3077,
      "step": 1486
    },
    {
      "epoch": 2.4783333333333335,
      "grad_norm": 0.04665202647447586,
      "learning_rate": 0.00010103505843071788,
      "loss": 0.3075,
      "step": 1487
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.05995183810591698,
      "learning_rate": 0.00010096828046744575,
      "loss": 0.371,
      "step": 1488
    },
    {
      "epoch": 2.4816666666666665,
      "grad_norm": 0.056211378425359726,
      "learning_rate": 0.00010090150250417362,
      "loss": 0.351,
      "step": 1489
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.04900602251291275,
      "learning_rate": 0.00010083472454090151,
      "loss": 0.3293,
      "step": 1490
    },
    {
      "epoch": 2.485,
      "grad_norm": 0.04777010902762413,
      "learning_rate": 0.00010076794657762938,
      "loss": 0.3365,
      "step": 1491
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.05941714346408844,
      "learning_rate": 0.00010070116861435728,
      "loss": 0.3405,
      "step": 1492
    },
    {
      "epoch": 2.4883333333333333,
      "grad_norm": 0.0757373794913292,
      "learning_rate": 0.00010063439065108516,
      "loss": 0.3159,
      "step": 1493
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.06918811798095703,
      "learning_rate": 0.00010056761268781303,
      "loss": 0.3212,
      "step": 1494
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 0.05530046671628952,
      "learning_rate": 0.0001005008347245409,
      "loss": 0.3859,
      "step": 1495
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.05100347101688385,
      "learning_rate": 0.00010043405676126878,
      "loss": 0.3435,
      "step": 1496
    },
    {
      "epoch": 2.495,
      "grad_norm": 0.04422896355390549,
      "learning_rate": 0.00010036727879799666,
      "loss": 0.2547,
      "step": 1497
    },
    {
      "epoch": 2.4966666666666666,
      "grad_norm": 0.07904937863349915,
      "learning_rate": 0.00010030050083472455,
      "loss": 0.4235,
      "step": 1498
    },
    {
      "epoch": 2.4983333333333335,
      "grad_norm": 0.046593986451625824,
      "learning_rate": 0.00010023372287145244,
      "loss": 0.3389,
      "step": 1499
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.05924468860030174,
      "learning_rate": 0.00010016694490818031,
      "loss": 0.3851,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.941564484687823e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
