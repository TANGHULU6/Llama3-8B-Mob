{
  "best_metric": 0.3294380307197571,
  "best_model_checkpoint": "outputs/checkpoint-1500",
  "epoch": 0.3409090909090909,
  "eval_steps": 100,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00022727272727272727,
      "grad_norm": 0.25020626187324524,
      "learning_rate": 4e-05,
      "loss": 0.451,
      "step": 1
    },
    {
      "epoch": 0.00045454545454545455,
      "grad_norm": 0.1242343857884407,
      "learning_rate": 8e-05,
      "loss": 0.2596,
      "step": 2
    },
    {
      "epoch": 0.0006818181818181819,
      "grad_norm": 0.0740601047873497,
      "learning_rate": 0.00012,
      "loss": 0.2844,
      "step": 3
    },
    {
      "epoch": 0.0009090909090909091,
      "grad_norm": 0.0860307514667511,
      "learning_rate": 0.00016,
      "loss": 0.2927,
      "step": 4
    },
    {
      "epoch": 0.0011363636363636363,
      "grad_norm": 0.07858523726463318,
      "learning_rate": 0.0002,
      "loss": 0.3179,
      "step": 5
    },
    {
      "epoch": 0.0013636363636363637,
      "grad_norm": 0.10723748058080673,
      "learning_rate": 0.00019995449374288966,
      "loss": 0.3657,
      "step": 6
    },
    {
      "epoch": 0.001590909090909091,
      "grad_norm": 0.14084665477275848,
      "learning_rate": 0.00019990898748577932,
      "loss": 0.3446,
      "step": 7
    },
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 0.12993307411670685,
      "learning_rate": 0.00019986348122866897,
      "loss": 0.317,
      "step": 8
    },
    {
      "epoch": 0.0020454545454545456,
      "grad_norm": 0.2617608904838562,
      "learning_rate": 0.00019981797497155862,
      "loss": 0.41,
      "step": 9
    },
    {
      "epoch": 0.0022727272727272726,
      "grad_norm": 0.3081091642379761,
      "learning_rate": 0.00019977246871444825,
      "loss": 0.3026,
      "step": 10
    },
    {
      "epoch": 0.0025,
      "grad_norm": 0.14921945333480835,
      "learning_rate": 0.00019972696245733787,
      "loss": 0.4456,
      "step": 11
    },
    {
      "epoch": 0.0027272727272727275,
      "grad_norm": 0.1487351655960083,
      "learning_rate": 0.00019968145620022753,
      "loss": 0.4063,
      "step": 12
    },
    {
      "epoch": 0.0029545454545454545,
      "grad_norm": 0.08904249221086502,
      "learning_rate": 0.00019963594994311718,
      "loss": 0.3346,
      "step": 13
    },
    {
      "epoch": 0.003181818181818182,
      "grad_norm": 0.08903719484806061,
      "learning_rate": 0.00019959044368600683,
      "loss": 0.3562,
      "step": 14
    },
    {
      "epoch": 0.003409090909090909,
      "grad_norm": 0.08778191357851028,
      "learning_rate": 0.00019954493742889648,
      "loss": 0.3233,
      "step": 15
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 0.08045671880245209,
      "learning_rate": 0.00019949943117178614,
      "loss": 0.3253,
      "step": 16
    },
    {
      "epoch": 0.003863636363636364,
      "grad_norm": 0.0733303502202034,
      "learning_rate": 0.0001994539249146758,
      "loss": 0.3385,
      "step": 17
    },
    {
      "epoch": 0.004090909090909091,
      "grad_norm": 0.08564203977584839,
      "learning_rate": 0.00019940841865756544,
      "loss": 0.3934,
      "step": 18
    },
    {
      "epoch": 0.004318181818181818,
      "grad_norm": 0.07626289874315262,
      "learning_rate": 0.0001993629124004551,
      "loss": 0.3643,
      "step": 19
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 0.07446857541799545,
      "learning_rate": 0.00019931740614334472,
      "loss": 0.3571,
      "step": 20
    },
    {
      "epoch": 0.004772727272727273,
      "grad_norm": 0.0966043546795845,
      "learning_rate": 0.00019927189988623435,
      "loss": 0.394,
      "step": 21
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.08619073033332825,
      "learning_rate": 0.000199226393629124,
      "loss": 0.3264,
      "step": 22
    },
    {
      "epoch": 0.005227272727272727,
      "grad_norm": 0.07027238607406616,
      "learning_rate": 0.00019918088737201365,
      "loss": 0.3904,
      "step": 23
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 0.053751252591609955,
      "learning_rate": 0.0001991353811149033,
      "loss": 0.269,
      "step": 24
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.07503972202539444,
      "learning_rate": 0.00019908987485779296,
      "loss": 0.3637,
      "step": 25
    },
    {
      "epoch": 0.005909090909090909,
      "grad_norm": 0.07268328219652176,
      "learning_rate": 0.0001990443686006826,
      "loss": 0.3787,
      "step": 26
    },
    {
      "epoch": 0.006136363636363636,
      "grad_norm": 0.05804140120744705,
      "learning_rate": 0.00019899886234357226,
      "loss": 0.3302,
      "step": 27
    },
    {
      "epoch": 0.006363636363636364,
      "grad_norm": 0.07492166012525558,
      "learning_rate": 0.00019895335608646192,
      "loss": 0.4267,
      "step": 28
    },
    {
      "epoch": 0.006590909090909091,
      "grad_norm": 0.07164125144481659,
      "learning_rate": 0.00019890784982935157,
      "loss": 0.3739,
      "step": 29
    },
    {
      "epoch": 0.006818181818181818,
      "grad_norm": 0.0751582682132721,
      "learning_rate": 0.0001988623435722412,
      "loss": 0.4001,
      "step": 30
    },
    {
      "epoch": 0.007045454545454546,
      "grad_norm": 0.059325575828552246,
      "learning_rate": 0.00019881683731513082,
      "loss": 0.3762,
      "step": 31
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 0.06740505993366241,
      "learning_rate": 0.00019877133105802047,
      "loss": 0.3903,
      "step": 32
    },
    {
      "epoch": 0.0075,
      "grad_norm": 0.05763225257396698,
      "learning_rate": 0.00019872582480091013,
      "loss": 0.3028,
      "step": 33
    },
    {
      "epoch": 0.007727272727272728,
      "grad_norm": 0.07344617694616318,
      "learning_rate": 0.00019868031854379978,
      "loss": 0.3345,
      "step": 34
    },
    {
      "epoch": 0.007954545454545454,
      "grad_norm": 0.07512003183364868,
      "learning_rate": 0.00019863481228668943,
      "loss": 0.3622,
      "step": 35
    },
    {
      "epoch": 0.008181818181818182,
      "grad_norm": 0.08397770673036575,
      "learning_rate": 0.00019858930602957908,
      "loss": 0.3697,
      "step": 36
    },
    {
      "epoch": 0.00840909090909091,
      "grad_norm": 0.0820905938744545,
      "learning_rate": 0.00019854379977246874,
      "loss": 0.3351,
      "step": 37
    },
    {
      "epoch": 0.008636363636363636,
      "grad_norm": 0.07418496161699295,
      "learning_rate": 0.0001984982935153584,
      "loss": 0.3132,
      "step": 38
    },
    {
      "epoch": 0.008863636363636363,
      "grad_norm": 0.07003125548362732,
      "learning_rate": 0.00019845278725824804,
      "loss": 0.3058,
      "step": 39
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.05843110382556915,
      "learning_rate": 0.00019840728100113767,
      "loss": 0.337,
      "step": 40
    },
    {
      "epoch": 0.009318181818181817,
      "grad_norm": 0.06167495995759964,
      "learning_rate": 0.0001983617747440273,
      "loss": 0.3894,
      "step": 41
    },
    {
      "epoch": 0.009545454545454546,
      "grad_norm": 0.07494774460792542,
      "learning_rate": 0.00019831626848691695,
      "loss": 0.3511,
      "step": 42
    },
    {
      "epoch": 0.009772727272727273,
      "grad_norm": 0.06796998530626297,
      "learning_rate": 0.0001982707622298066,
      "loss": 0.3297,
      "step": 43
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.07079331576824188,
      "learning_rate": 0.00019822525597269625,
      "loss": 0.4007,
      "step": 44
    },
    {
      "epoch": 0.010227272727272727,
      "grad_norm": 0.06379951536655426,
      "learning_rate": 0.0001981797497155859,
      "loss": 0.3592,
      "step": 45
    },
    {
      "epoch": 0.010454545454545454,
      "grad_norm": 0.09263726323843002,
      "learning_rate": 0.00019813424345847556,
      "loss": 0.4318,
      "step": 46
    },
    {
      "epoch": 0.010681818181818181,
      "grad_norm": 0.08466766029596329,
      "learning_rate": 0.0001980887372013652,
      "loss": 0.3356,
      "step": 47
    },
    {
      "epoch": 0.01090909090909091,
      "grad_norm": 0.07781442254781723,
      "learning_rate": 0.00019804323094425486,
      "loss": 0.3143,
      "step": 48
    },
    {
      "epoch": 0.011136363636363637,
      "grad_norm": 0.060328125953674316,
      "learning_rate": 0.0001979977246871445,
      "loss": 0.3215,
      "step": 49
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.0904291644692421,
      "learning_rate": 0.00019795221843003414,
      "loss": 0.3652,
      "step": 50
    },
    {
      "epoch": 0.011590909090909091,
      "grad_norm": 0.0865832194685936,
      "learning_rate": 0.00019790671217292377,
      "loss": 0.3993,
      "step": 51
    },
    {
      "epoch": 0.011818181818181818,
      "grad_norm": 0.062298763543367386,
      "learning_rate": 0.00019786120591581342,
      "loss": 0.3584,
      "step": 52
    },
    {
      "epoch": 0.012045454545454545,
      "grad_norm": 0.10263701528310776,
      "learning_rate": 0.00019781569965870307,
      "loss": 0.4766,
      "step": 53
    },
    {
      "epoch": 0.012272727272727272,
      "grad_norm": 0.08348086476325989,
      "learning_rate": 0.00019777019340159273,
      "loss": 0.3912,
      "step": 54
    },
    {
      "epoch": 0.0125,
      "grad_norm": 0.053798988461494446,
      "learning_rate": 0.00019772468714448238,
      "loss": 0.3373,
      "step": 55
    },
    {
      "epoch": 0.012727272727272728,
      "grad_norm": 0.1146434098482132,
      "learning_rate": 0.00019767918088737203,
      "loss": 0.3962,
      "step": 56
    },
    {
      "epoch": 0.012954545454545455,
      "grad_norm": 0.07339777052402496,
      "learning_rate": 0.00019763367463026169,
      "loss": 0.4095,
      "step": 57
    },
    {
      "epoch": 0.013181818181818182,
      "grad_norm": 0.07138066738843918,
      "learning_rate": 0.00019758816837315134,
      "loss": 0.3176,
      "step": 58
    },
    {
      "epoch": 0.013409090909090909,
      "grad_norm": 0.0775488018989563,
      "learning_rate": 0.00019754266211604096,
      "loss": 0.3841,
      "step": 59
    },
    {
      "epoch": 0.013636363636363636,
      "grad_norm": 0.06653862446546555,
      "learning_rate": 0.00019749715585893062,
      "loss": 0.3424,
      "step": 60
    },
    {
      "epoch": 0.013863636363636364,
      "grad_norm": 0.08365213871002197,
      "learning_rate": 0.00019745164960182024,
      "loss": 0.2929,
      "step": 61
    },
    {
      "epoch": 0.014090909090909091,
      "grad_norm": 0.07876574248075485,
      "learning_rate": 0.0001974061433447099,
      "loss": 0.3558,
      "step": 62
    },
    {
      "epoch": 0.014318181818181818,
      "grad_norm": 0.08063793182373047,
      "learning_rate": 0.00019736063708759955,
      "loss": 0.334,
      "step": 63
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 0.059023141860961914,
      "learning_rate": 0.0001973151308304892,
      "loss": 0.3095,
      "step": 64
    },
    {
      "epoch": 0.014772727272727272,
      "grad_norm": 0.06496645510196686,
      "learning_rate": 0.00019726962457337885,
      "loss": 0.3054,
      "step": 65
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.08576630800962448,
      "learning_rate": 0.0001972241183162685,
      "loss": 0.3679,
      "step": 66
    },
    {
      "epoch": 0.015227272727272726,
      "grad_norm": 0.0797283872961998,
      "learning_rate": 0.00019717861205915816,
      "loss": 0.3178,
      "step": 67
    },
    {
      "epoch": 0.015454545454545455,
      "grad_norm": 0.06985502690076828,
      "learning_rate": 0.0001971331058020478,
      "loss": 0.3515,
      "step": 68
    },
    {
      "epoch": 0.015681818181818182,
      "grad_norm": 0.08788812160491943,
      "learning_rate": 0.00019708759954493744,
      "loss": 0.4028,
      "step": 69
    },
    {
      "epoch": 0.015909090909090907,
      "grad_norm": 0.09745489805936813,
      "learning_rate": 0.0001970420932878271,
      "loss": 0.3597,
      "step": 70
    },
    {
      "epoch": 0.016136363636363636,
      "grad_norm": 0.0603397935628891,
      "learning_rate": 0.00019699658703071672,
      "loss": 0.3396,
      "step": 71
    },
    {
      "epoch": 0.016363636363636365,
      "grad_norm": 0.061473678797483444,
      "learning_rate": 0.00019695108077360637,
      "loss": 0.3635,
      "step": 72
    },
    {
      "epoch": 0.01659090909090909,
      "grad_norm": 0.07115209847688675,
      "learning_rate": 0.00019690557451649602,
      "loss": 0.3402,
      "step": 73
    },
    {
      "epoch": 0.01681818181818182,
      "grad_norm": 0.06805097311735153,
      "learning_rate": 0.00019686006825938567,
      "loss": 0.3619,
      "step": 74
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.07518858462572098,
      "learning_rate": 0.00019681456200227533,
      "loss": 0.3901,
      "step": 75
    },
    {
      "epoch": 0.017272727272727273,
      "grad_norm": 0.07054062932729721,
      "learning_rate": 0.00019676905574516498,
      "loss": 0.4366,
      "step": 76
    },
    {
      "epoch": 0.0175,
      "grad_norm": 0.067987821996212,
      "learning_rate": 0.00019672354948805463,
      "loss": 0.33,
      "step": 77
    },
    {
      "epoch": 0.017727272727272727,
      "grad_norm": 0.06659303605556488,
      "learning_rate": 0.00019667804323094426,
      "loss": 0.3764,
      "step": 78
    },
    {
      "epoch": 0.017954545454545456,
      "grad_norm": 0.07198026776313782,
      "learning_rate": 0.0001966325369738339,
      "loss": 0.3384,
      "step": 79
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 0.06810051947832108,
      "learning_rate": 0.00019658703071672356,
      "loss": 0.3622,
      "step": 80
    },
    {
      "epoch": 0.01840909090909091,
      "grad_norm": 0.08115657418966293,
      "learning_rate": 0.0001965415244596132,
      "loss": 0.3549,
      "step": 81
    },
    {
      "epoch": 0.018636363636363635,
      "grad_norm": 0.0819796472787857,
      "learning_rate": 0.00019649601820250284,
      "loss": 0.3222,
      "step": 82
    },
    {
      "epoch": 0.018863636363636364,
      "grad_norm": 0.0599668063223362,
      "learning_rate": 0.0001964505119453925,
      "loss": 0.3215,
      "step": 83
    },
    {
      "epoch": 0.019090909090909092,
      "grad_norm": 0.05372064188122749,
      "learning_rate": 0.00019640500568828215,
      "loss": 0.3305,
      "step": 84
    },
    {
      "epoch": 0.019318181818181818,
      "grad_norm": 0.06788784265518188,
      "learning_rate": 0.0001963594994311718,
      "loss": 0.3071,
      "step": 85
    },
    {
      "epoch": 0.019545454545454546,
      "grad_norm": 0.08167719841003418,
      "learning_rate": 0.00019631399317406145,
      "loss": 0.3826,
      "step": 86
    },
    {
      "epoch": 0.01977272727272727,
      "grad_norm": 0.08595576882362366,
      "learning_rate": 0.0001962684869169511,
      "loss": 0.427,
      "step": 87
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.058351967483758926,
      "learning_rate": 0.00019622298065984073,
      "loss": 0.3087,
      "step": 88
    },
    {
      "epoch": 0.020227272727272726,
      "grad_norm": 0.05115113779902458,
      "learning_rate": 0.00019617747440273039,
      "loss": 0.3093,
      "step": 89
    },
    {
      "epoch": 0.020454545454545454,
      "grad_norm": 0.08103372901678085,
      "learning_rate": 0.00019613196814562004,
      "loss": 0.3591,
      "step": 90
    },
    {
      "epoch": 0.020681818181818183,
      "grad_norm": 0.06732356548309326,
      "learning_rate": 0.00019608646188850966,
      "loss": 0.3157,
      "step": 91
    },
    {
      "epoch": 0.02090909090909091,
      "grad_norm": 0.07818669825792313,
      "learning_rate": 0.00019604095563139932,
      "loss": 0.3787,
      "step": 92
    },
    {
      "epoch": 0.021136363636363637,
      "grad_norm": 0.07566282153129578,
      "learning_rate": 0.00019599544937428897,
      "loss": 0.3088,
      "step": 93
    },
    {
      "epoch": 0.021363636363636362,
      "grad_norm": 0.06960880011320114,
      "learning_rate": 0.00019594994311717862,
      "loss": 0.3343,
      "step": 94
    },
    {
      "epoch": 0.02159090909090909,
      "grad_norm": 0.059884823858737946,
      "learning_rate": 0.00019590443686006828,
      "loss": 0.3123,
      "step": 95
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 0.05784144625067711,
      "learning_rate": 0.00019585893060295793,
      "loss": 0.295,
      "step": 96
    },
    {
      "epoch": 0.022045454545454545,
      "grad_norm": 0.06917823851108551,
      "learning_rate": 0.00019581342434584758,
      "loss": 0.3764,
      "step": 97
    },
    {
      "epoch": 0.022272727272727274,
      "grad_norm": 0.04899419844150543,
      "learning_rate": 0.0001957679180887372,
      "loss": 0.3065,
      "step": 98
    },
    {
      "epoch": 0.0225,
      "grad_norm": 0.056973475962877274,
      "learning_rate": 0.00019572241183162686,
      "loss": 0.3494,
      "step": 99
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.06409335136413574,
      "learning_rate": 0.0001956769055745165,
      "loss": 0.3169,
      "step": 100
    },
    {
      "epoch": 0.022727272727272728,
      "eval_loss": 0.3440212905406952,
      "eval_runtime": 222.0266,
      "eval_samples_per_second": 0.45,
      "eval_steps_per_second": 0.45,
      "step": 100
    },
    {
      "epoch": 0.022954545454545453,
      "grad_norm": 0.0684981718659401,
      "learning_rate": 0.00019563139931740614,
      "loss": 0.3479,
      "step": 101
    },
    {
      "epoch": 0.023181818181818182,
      "grad_norm": 0.06219605356454849,
      "learning_rate": 0.0001955858930602958,
      "loss": 0.3915,
      "step": 102
    },
    {
      "epoch": 0.02340909090909091,
      "grad_norm": 0.07370982319116592,
      "learning_rate": 0.00019554038680318544,
      "loss": 0.3876,
      "step": 103
    },
    {
      "epoch": 0.023636363636363636,
      "grad_norm": 0.06118752807378769,
      "learning_rate": 0.0001954948805460751,
      "loss": 0.2629,
      "step": 104
    },
    {
      "epoch": 0.023863636363636365,
      "grad_norm": 0.07970228791236877,
      "learning_rate": 0.00019544937428896475,
      "loss": 0.3384,
      "step": 105
    },
    {
      "epoch": 0.02409090909090909,
      "grad_norm": 0.07450418174266815,
      "learning_rate": 0.0001954038680318544,
      "loss": 0.3655,
      "step": 106
    },
    {
      "epoch": 0.02431818181818182,
      "grad_norm": 0.06886322796344757,
      "learning_rate": 0.00019535836177474406,
      "loss": 0.3521,
      "step": 107
    },
    {
      "epoch": 0.024545454545454544,
      "grad_norm": 0.0750531405210495,
      "learning_rate": 0.00019531285551763368,
      "loss": 0.3743,
      "step": 108
    },
    {
      "epoch": 0.024772727272727273,
      "grad_norm": 0.08278155326843262,
      "learning_rate": 0.00019526734926052333,
      "loss": 0.3149,
      "step": 109
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.08016758412122726,
      "learning_rate": 0.00019522184300341299,
      "loss": 0.3857,
      "step": 110
    },
    {
      "epoch": 0.025227272727272727,
      "grad_norm": 0.06231500953435898,
      "learning_rate": 0.0001951763367463026,
      "loss": 0.3324,
      "step": 111
    },
    {
      "epoch": 0.025454545454545455,
      "grad_norm": 0.08114475756883621,
      "learning_rate": 0.00019513083048919226,
      "loss": 0.3779,
      "step": 112
    },
    {
      "epoch": 0.02568181818181818,
      "grad_norm": 0.07329140603542328,
      "learning_rate": 0.00019508532423208192,
      "loss": 0.4122,
      "step": 113
    },
    {
      "epoch": 0.02590909090909091,
      "grad_norm": 0.067377008497715,
      "learning_rate": 0.00019503981797497157,
      "loss": 0.3857,
      "step": 114
    },
    {
      "epoch": 0.026136363636363635,
      "grad_norm": 0.07714938372373581,
      "learning_rate": 0.00019499431171786122,
      "loss": 0.3055,
      "step": 115
    },
    {
      "epoch": 0.026363636363636363,
      "grad_norm": 0.06590323895215988,
      "learning_rate": 0.00019494880546075088,
      "loss": 0.3662,
      "step": 116
    },
    {
      "epoch": 0.026590909090909092,
      "grad_norm": 0.05692821368575096,
      "learning_rate": 0.0001949032992036405,
      "loss": 0.2856,
      "step": 117
    },
    {
      "epoch": 0.026818181818181817,
      "grad_norm": 0.0659884363412857,
      "learning_rate": 0.00019485779294653015,
      "loss": 0.286,
      "step": 118
    },
    {
      "epoch": 0.027045454545454546,
      "grad_norm": 0.0598343200981617,
      "learning_rate": 0.0001948122866894198,
      "loss": 0.3123,
      "step": 119
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 0.07671663910150528,
      "learning_rate": 0.00019476678043230946,
      "loss": 0.3781,
      "step": 120
    },
    {
      "epoch": 0.0275,
      "grad_norm": 0.07296498119831085,
      "learning_rate": 0.00019472127417519909,
      "loss": 0.3601,
      "step": 121
    },
    {
      "epoch": 0.02772727272727273,
      "grad_norm": 0.052076417952775955,
      "learning_rate": 0.00019467576791808874,
      "loss": 0.3217,
      "step": 122
    },
    {
      "epoch": 0.027954545454545454,
      "grad_norm": 0.06639982759952545,
      "learning_rate": 0.0001946302616609784,
      "loss": 0.3342,
      "step": 123
    },
    {
      "epoch": 0.028181818181818183,
      "grad_norm": 0.05712908133864403,
      "learning_rate": 0.00019458475540386804,
      "loss": 0.2983,
      "step": 124
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.06791418045759201,
      "learning_rate": 0.0001945392491467577,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.028636363636363637,
      "grad_norm": 0.06489987671375275,
      "learning_rate": 0.00019449374288964735,
      "loss": 0.3758,
      "step": 126
    },
    {
      "epoch": 0.028863636363636362,
      "grad_norm": 0.07328926771879196,
      "learning_rate": 0.00019444823663253698,
      "loss": 0.3673,
      "step": 127
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 0.06999117136001587,
      "learning_rate": 0.00019440273037542663,
      "loss": 0.3512,
      "step": 128
    },
    {
      "epoch": 0.02931818181818182,
      "grad_norm": 0.05733741819858551,
      "learning_rate": 0.00019435722411831628,
      "loss": 0.3551,
      "step": 129
    },
    {
      "epoch": 0.029545454545454545,
      "grad_norm": 0.07569628208875656,
      "learning_rate": 0.00019431171786120593,
      "loss": 0.2757,
      "step": 130
    },
    {
      "epoch": 0.029772727272727274,
      "grad_norm": 0.05405205860733986,
      "learning_rate": 0.00019426621160409556,
      "loss": 0.3409,
      "step": 131
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.06941142678260803,
      "learning_rate": 0.0001942207053469852,
      "loss": 0.3213,
      "step": 132
    },
    {
      "epoch": 0.030227272727272728,
      "grad_norm": 0.05390465632081032,
      "learning_rate": 0.00019417519908987487,
      "loss": 0.2409,
      "step": 133
    },
    {
      "epoch": 0.030454545454545453,
      "grad_norm": 0.060169950127601624,
      "learning_rate": 0.00019412969283276452,
      "loss": 0.3492,
      "step": 134
    },
    {
      "epoch": 0.03068181818181818,
      "grad_norm": 0.06582920253276825,
      "learning_rate": 0.00019408418657565417,
      "loss": 0.339,
      "step": 135
    },
    {
      "epoch": 0.03090909090909091,
      "grad_norm": 0.06849858164787292,
      "learning_rate": 0.00019403868031854382,
      "loss": 0.296,
      "step": 136
    },
    {
      "epoch": 0.031136363636363636,
      "grad_norm": 0.06799591332674026,
      "learning_rate": 0.00019399317406143345,
      "loss": 0.3568,
      "step": 137
    },
    {
      "epoch": 0.031363636363636364,
      "grad_norm": 0.07181922346353531,
      "learning_rate": 0.0001939476678043231,
      "loss": 0.3948,
      "step": 138
    },
    {
      "epoch": 0.03159090909090909,
      "grad_norm": 0.06596127897500992,
      "learning_rate": 0.00019390216154721276,
      "loss": 0.3253,
      "step": 139
    },
    {
      "epoch": 0.031818181818181815,
      "grad_norm": 0.07256683707237244,
      "learning_rate": 0.0001938566552901024,
      "loss": 0.3628,
      "step": 140
    },
    {
      "epoch": 0.032045454545454544,
      "grad_norm": 0.06957809627056122,
      "learning_rate": 0.00019381114903299203,
      "loss": 0.32,
      "step": 141
    },
    {
      "epoch": 0.03227272727272727,
      "grad_norm": 0.05213717743754387,
      "learning_rate": 0.0001937656427758817,
      "loss": 0.2804,
      "step": 142
    },
    {
      "epoch": 0.0325,
      "grad_norm": 0.08225961774587631,
      "learning_rate": 0.00019372013651877134,
      "loss": 0.4252,
      "step": 143
    },
    {
      "epoch": 0.03272727272727273,
      "grad_norm": 0.05746239051222801,
      "learning_rate": 0.000193674630261661,
      "loss": 0.3026,
      "step": 144
    },
    {
      "epoch": 0.03295454545454545,
      "grad_norm": 0.07988333702087402,
      "learning_rate": 0.00019362912400455065,
      "loss": 0.3306,
      "step": 145
    },
    {
      "epoch": 0.03318181818181818,
      "grad_norm": 0.05592226982116699,
      "learning_rate": 0.00019358361774744027,
      "loss": 0.2534,
      "step": 146
    },
    {
      "epoch": 0.03340909090909091,
      "grad_norm": 0.08577007055282593,
      "learning_rate": 0.00019353811149032992,
      "loss": 0.3862,
      "step": 147
    },
    {
      "epoch": 0.03363636363636364,
      "grad_norm": 0.08133303374052048,
      "learning_rate": 0.00019349260523321958,
      "loss": 0.3851,
      "step": 148
    },
    {
      "epoch": 0.03386363636363637,
      "grad_norm": 0.0591951347887516,
      "learning_rate": 0.00019344709897610923,
      "loss": 0.3468,
      "step": 149
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.06228388100862503,
      "learning_rate": 0.00019340159271899888,
      "loss": 0.3152,
      "step": 150
    },
    {
      "epoch": 0.03431818181818182,
      "grad_norm": 0.06655502319335938,
      "learning_rate": 0.0001933560864618885,
      "loss": 0.3685,
      "step": 151
    },
    {
      "epoch": 0.034545454545454546,
      "grad_norm": 0.06506670266389847,
      "learning_rate": 0.00019331058020477816,
      "loss": 0.3336,
      "step": 152
    },
    {
      "epoch": 0.034772727272727275,
      "grad_norm": 0.05744832381606102,
      "learning_rate": 0.00019326507394766781,
      "loss": 0.3752,
      "step": 153
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.06742896884679794,
      "learning_rate": 0.00019321956769055747,
      "loss": 0.3581,
      "step": 154
    },
    {
      "epoch": 0.035227272727272725,
      "grad_norm": 0.08552875369787216,
      "learning_rate": 0.00019317406143344712,
      "loss": 0.3876,
      "step": 155
    },
    {
      "epoch": 0.035454545454545454,
      "grad_norm": 0.05776657164096832,
      "learning_rate": 0.00019312855517633675,
      "loss": 0.3163,
      "step": 156
    },
    {
      "epoch": 0.03568181818181818,
      "grad_norm": 0.06120475009083748,
      "learning_rate": 0.0001930830489192264,
      "loss": 0.382,
      "step": 157
    },
    {
      "epoch": 0.03590909090909091,
      "grad_norm": 0.06853175908327103,
      "learning_rate": 0.00019303754266211605,
      "loss": 0.3524,
      "step": 158
    },
    {
      "epoch": 0.03613636363636363,
      "grad_norm": 0.05829175189137459,
      "learning_rate": 0.0001929920364050057,
      "loss": 0.3342,
      "step": 159
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.06022721156477928,
      "learning_rate": 0.00019294653014789536,
      "loss": 0.326,
      "step": 160
    },
    {
      "epoch": 0.03659090909090909,
      "grad_norm": 0.06351108103990555,
      "learning_rate": 0.00019290102389078498,
      "loss": 0.3186,
      "step": 161
    },
    {
      "epoch": 0.03681818181818182,
      "grad_norm": 0.07363327592611313,
      "learning_rate": 0.00019285551763367463,
      "loss": 0.3835,
      "step": 162
    },
    {
      "epoch": 0.03704545454545455,
      "grad_norm": 0.07200946658849716,
      "learning_rate": 0.0001928100113765643,
      "loss": 0.3486,
      "step": 163
    },
    {
      "epoch": 0.03727272727272727,
      "grad_norm": 0.08438070863485336,
      "learning_rate": 0.00019276450511945394,
      "loss": 0.3871,
      "step": 164
    },
    {
      "epoch": 0.0375,
      "grad_norm": 0.0557725727558136,
      "learning_rate": 0.0001927189988623436,
      "loss": 0.2841,
      "step": 165
    },
    {
      "epoch": 0.03772727272727273,
      "grad_norm": 0.05706772953271866,
      "learning_rate": 0.00019267349260523322,
      "loss": 0.3094,
      "step": 166
    },
    {
      "epoch": 0.037954545454545456,
      "grad_norm": 0.06938499212265015,
      "learning_rate": 0.00019262798634812287,
      "loss": 0.3526,
      "step": 167
    },
    {
      "epoch": 0.038181818181818185,
      "grad_norm": 0.060556311160326004,
      "learning_rate": 0.00019258248009101252,
      "loss": 0.3434,
      "step": 168
    },
    {
      "epoch": 0.03840909090909091,
      "grad_norm": 0.08228754997253418,
      "learning_rate": 0.00019253697383390218,
      "loss": 0.3105,
      "step": 169
    },
    {
      "epoch": 0.038636363636363635,
      "grad_norm": 0.05237473547458649,
      "learning_rate": 0.00019249146757679183,
      "loss": 0.319,
      "step": 170
    },
    {
      "epoch": 0.038863636363636364,
      "grad_norm": 0.05773526057600975,
      "learning_rate": 0.00019244596131968146,
      "loss": 0.3586,
      "step": 171
    },
    {
      "epoch": 0.03909090909090909,
      "grad_norm": 0.052996791899204254,
      "learning_rate": 0.0001924004550625711,
      "loss": 0.3834,
      "step": 172
    },
    {
      "epoch": 0.03931818181818182,
      "grad_norm": 0.0445987768471241,
      "learning_rate": 0.00019235494880546076,
      "loss": 0.2378,
      "step": 173
    },
    {
      "epoch": 0.03954545454545454,
      "grad_norm": 0.06939978152513504,
      "learning_rate": 0.00019230944254835041,
      "loss": 0.303,
      "step": 174
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.0514724999666214,
      "learning_rate": 0.00019226393629124007,
      "loss": 0.2889,
      "step": 175
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.07608234882354736,
      "learning_rate": 0.0001922184300341297,
      "loss": 0.3059,
      "step": 176
    },
    {
      "epoch": 0.04022727272727273,
      "grad_norm": 0.07490229606628418,
      "learning_rate": 0.00019217292377701935,
      "loss": 0.4129,
      "step": 177
    },
    {
      "epoch": 0.04045454545454545,
      "grad_norm": 0.06505292654037476,
      "learning_rate": 0.000192127417519909,
      "loss": 0.3625,
      "step": 178
    },
    {
      "epoch": 0.04068181818181818,
      "grad_norm": 0.07077926397323608,
      "learning_rate": 0.00019208191126279865,
      "loss": 0.3207,
      "step": 179
    },
    {
      "epoch": 0.04090909090909091,
      "grad_norm": 0.05384380742907524,
      "learning_rate": 0.0001920364050056883,
      "loss": 0.2465,
      "step": 180
    },
    {
      "epoch": 0.04113636363636364,
      "grad_norm": 0.05173149332404137,
      "learning_rate": 0.00019199089874857793,
      "loss": 0.3096,
      "step": 181
    },
    {
      "epoch": 0.041363636363636366,
      "grad_norm": 0.06955502182245255,
      "learning_rate": 0.00019194539249146758,
      "loss": 0.3216,
      "step": 182
    },
    {
      "epoch": 0.04159090909090909,
      "grad_norm": 0.06838874518871307,
      "learning_rate": 0.00019189988623435724,
      "loss": 0.3548,
      "step": 183
    },
    {
      "epoch": 0.04181818181818182,
      "grad_norm": 0.08407984673976898,
      "learning_rate": 0.0001918543799772469,
      "loss": 0.3955,
      "step": 184
    },
    {
      "epoch": 0.042045454545454546,
      "grad_norm": 0.066097691655159,
      "learning_rate": 0.00019180887372013651,
      "loss": 0.3048,
      "step": 185
    },
    {
      "epoch": 0.042272727272727274,
      "grad_norm": 0.05891042575240135,
      "learning_rate": 0.00019176336746302617,
      "loss": 0.3576,
      "step": 186
    },
    {
      "epoch": 0.0425,
      "grad_norm": 0.07977075129747391,
      "learning_rate": 0.00019171786120591582,
      "loss": 0.3749,
      "step": 187
    },
    {
      "epoch": 0.042727272727272725,
      "grad_norm": 0.04728803038597107,
      "learning_rate": 0.00019167235494880547,
      "loss": 0.2262,
      "step": 188
    },
    {
      "epoch": 0.042954545454545454,
      "grad_norm": 0.06967733055353165,
      "learning_rate": 0.00019162684869169513,
      "loss": 0.3083,
      "step": 189
    },
    {
      "epoch": 0.04318181818181818,
      "grad_norm": 0.05192410573363304,
      "learning_rate": 0.00019158134243458478,
      "loss": 0.3228,
      "step": 190
    },
    {
      "epoch": 0.04340909090909091,
      "grad_norm": 0.059386227279901505,
      "learning_rate": 0.0001915358361774744,
      "loss": 0.3348,
      "step": 191
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.08361666649580002,
      "learning_rate": 0.00019149032992036406,
      "loss": 0.3743,
      "step": 192
    },
    {
      "epoch": 0.04386363636363636,
      "grad_norm": 0.05023857578635216,
      "learning_rate": 0.0001914448236632537,
      "loss": 0.2784,
      "step": 193
    },
    {
      "epoch": 0.04409090909090909,
      "grad_norm": 0.05396678298711777,
      "learning_rate": 0.00019139931740614336,
      "loss": 0.2428,
      "step": 194
    },
    {
      "epoch": 0.04431818181818182,
      "grad_norm": 0.082494355738163,
      "learning_rate": 0.000191353811149033,
      "loss": 0.4046,
      "step": 195
    },
    {
      "epoch": 0.04454545454545455,
      "grad_norm": 0.058040227741003036,
      "learning_rate": 0.00019130830489192264,
      "loss": 0.3246,
      "step": 196
    },
    {
      "epoch": 0.04477272727272727,
      "grad_norm": 0.05817261338233948,
      "learning_rate": 0.0001912627986348123,
      "loss": 0.3449,
      "step": 197
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.07164663076400757,
      "learning_rate": 0.00019121729237770195,
      "loss": 0.3995,
      "step": 198
    },
    {
      "epoch": 0.04522727272727273,
      "grad_norm": 0.049850042909383774,
      "learning_rate": 0.0001911717861205916,
      "loss": 0.2929,
      "step": 199
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.07068168371915817,
      "learning_rate": 0.00019112627986348125,
      "loss": 0.3346,
      "step": 200
    },
    {
      "epoch": 0.045454545454545456,
      "eval_loss": 0.33973070979118347,
      "eval_runtime": 228.601,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 200
    },
    {
      "epoch": 0.045681818181818185,
      "grad_norm": 0.06637563556432724,
      "learning_rate": 0.00019108077360637088,
      "loss": 0.361,
      "step": 201
    },
    {
      "epoch": 0.045909090909090906,
      "grad_norm": 0.05545880272984505,
      "learning_rate": 0.00019103526734926053,
      "loss": 0.2913,
      "step": 202
    },
    {
      "epoch": 0.046136363636363635,
      "grad_norm": 0.07532505691051483,
      "learning_rate": 0.00019098976109215018,
      "loss": 0.3566,
      "step": 203
    },
    {
      "epoch": 0.046363636363636364,
      "grad_norm": 0.06428946554660797,
      "learning_rate": 0.00019094425483503984,
      "loss": 0.3837,
      "step": 204
    },
    {
      "epoch": 0.04659090909090909,
      "grad_norm": 0.09043516218662262,
      "learning_rate": 0.00019089874857792946,
      "loss": 0.3475,
      "step": 205
    },
    {
      "epoch": 0.04681818181818182,
      "grad_norm": 0.05737529695034027,
      "learning_rate": 0.00019085324232081911,
      "loss": 0.3406,
      "step": 206
    },
    {
      "epoch": 0.04704545454545454,
      "grad_norm": 0.06333868205547333,
      "learning_rate": 0.00019080773606370877,
      "loss": 0.3251,
      "step": 207
    },
    {
      "epoch": 0.04727272727272727,
      "grad_norm": 0.068020761013031,
      "learning_rate": 0.00019076222980659842,
      "loss": 0.3461,
      "step": 208
    },
    {
      "epoch": 0.0475,
      "grad_norm": 0.059210874140262604,
      "learning_rate": 0.00019071672354948807,
      "loss": 0.3649,
      "step": 209
    },
    {
      "epoch": 0.04772727272727273,
      "grad_norm": 0.053291529417037964,
      "learning_rate": 0.00019067121729237773,
      "loss": 0.301,
      "step": 210
    },
    {
      "epoch": 0.04795454545454545,
      "grad_norm": 0.07752056419849396,
      "learning_rate": 0.00019062571103526735,
      "loss": 0.3242,
      "step": 211
    },
    {
      "epoch": 0.04818181818181818,
      "grad_norm": 0.06355318427085876,
      "learning_rate": 0.000190580204778157,
      "loss": 0.3832,
      "step": 212
    },
    {
      "epoch": 0.04840909090909091,
      "grad_norm": 0.07631457597017288,
      "learning_rate": 0.00019053469852104666,
      "loss": 0.3288,
      "step": 213
    },
    {
      "epoch": 0.04863636363636364,
      "grad_norm": 0.053499236702919006,
      "learning_rate": 0.0001904891922639363,
      "loss": 0.3128,
      "step": 214
    },
    {
      "epoch": 0.048863636363636366,
      "grad_norm": 0.062307581305503845,
      "learning_rate": 0.00019044368600682594,
      "loss": 0.3337,
      "step": 215
    },
    {
      "epoch": 0.04909090909090909,
      "grad_norm": 0.07349566370248795,
      "learning_rate": 0.0001903981797497156,
      "loss": 0.3753,
      "step": 216
    },
    {
      "epoch": 0.04931818181818182,
      "grad_norm": 0.0729663074016571,
      "learning_rate": 0.00019035267349260524,
      "loss": 0.3941,
      "step": 217
    },
    {
      "epoch": 0.049545454545454545,
      "grad_norm": 0.06423742324113846,
      "learning_rate": 0.0001903071672354949,
      "loss": 0.2792,
      "step": 218
    },
    {
      "epoch": 0.049772727272727274,
      "grad_norm": 0.06739667803049088,
      "learning_rate": 0.00019026166097838455,
      "loss": 0.3365,
      "step": 219
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.07511761784553528,
      "learning_rate": 0.0001902161547212742,
      "loss": 0.3509,
      "step": 220
    },
    {
      "epoch": 0.050227272727272725,
      "grad_norm": 0.06530986726284027,
      "learning_rate": 0.00019017064846416383,
      "loss": 0.3271,
      "step": 221
    },
    {
      "epoch": 0.05045454545454545,
      "grad_norm": 0.08769334107637405,
      "learning_rate": 0.00019012514220705348,
      "loss": 0.3635,
      "step": 222
    },
    {
      "epoch": 0.05068181818181818,
      "grad_norm": 0.07046125829219818,
      "learning_rate": 0.00019007963594994313,
      "loss": 0.3703,
      "step": 223
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.10029996931552887,
      "learning_rate": 0.00019003412969283276,
      "loss": 0.4375,
      "step": 224
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.062044549733400345,
      "learning_rate": 0.0001899886234357224,
      "loss": 0.3275,
      "step": 225
    },
    {
      "epoch": 0.05136363636363636,
      "grad_norm": 0.07206036150455475,
      "learning_rate": 0.00018994311717861206,
      "loss": 0.3883,
      "step": 226
    },
    {
      "epoch": 0.05159090909090909,
      "grad_norm": 0.056764598935842514,
      "learning_rate": 0.00018989761092150172,
      "loss": 0.3293,
      "step": 227
    },
    {
      "epoch": 0.05181818181818182,
      "grad_norm": 0.06265449523925781,
      "learning_rate": 0.00018985210466439137,
      "loss": 0.3591,
      "step": 228
    },
    {
      "epoch": 0.05204545454545455,
      "grad_norm": 0.0704827755689621,
      "learning_rate": 0.00018980659840728102,
      "loss": 0.3863,
      "step": 229
    },
    {
      "epoch": 0.05227272727272727,
      "grad_norm": 0.07427499443292618,
      "learning_rate": 0.00018976109215017067,
      "loss": 0.3605,
      "step": 230
    },
    {
      "epoch": 0.0525,
      "grad_norm": 0.04532312601804733,
      "learning_rate": 0.0001897155858930603,
      "loss": 0.2759,
      "step": 231
    },
    {
      "epoch": 0.05272727272727273,
      "grad_norm": 0.08714520931243896,
      "learning_rate": 0.00018967007963594995,
      "loss": 0.3799,
      "step": 232
    },
    {
      "epoch": 0.052954545454545456,
      "grad_norm": 0.05569176375865936,
      "learning_rate": 0.0001896245733788396,
      "loss": 0.3028,
      "step": 233
    },
    {
      "epoch": 0.053181818181818184,
      "grad_norm": 0.060443442314863205,
      "learning_rate": 0.00018957906712172923,
      "loss": 0.3251,
      "step": 234
    },
    {
      "epoch": 0.053409090909090906,
      "grad_norm": 0.08791851252317429,
      "learning_rate": 0.00018953356086461888,
      "loss": 0.4099,
      "step": 235
    },
    {
      "epoch": 0.053636363636363635,
      "grad_norm": 0.05636463314294815,
      "learning_rate": 0.00018948805460750854,
      "loss": 0.3495,
      "step": 236
    },
    {
      "epoch": 0.053863636363636364,
      "grad_norm": 0.05781586468219757,
      "learning_rate": 0.0001894425483503982,
      "loss": 0.3114,
      "step": 237
    },
    {
      "epoch": 0.05409090909090909,
      "grad_norm": 0.054775647819042206,
      "learning_rate": 0.00018939704209328784,
      "loss": 0.3294,
      "step": 238
    },
    {
      "epoch": 0.05431818181818182,
      "grad_norm": 0.06332335621118546,
      "learning_rate": 0.0001893515358361775,
      "loss": 0.3052,
      "step": 239
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.0523117259144783,
      "learning_rate": 0.00018930602957906712,
      "loss": 0.3068,
      "step": 240
    },
    {
      "epoch": 0.05477272727272727,
      "grad_norm": 0.06116436421871185,
      "learning_rate": 0.00018926052332195677,
      "loss": 0.3135,
      "step": 241
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.07384780049324036,
      "learning_rate": 0.00018921501706484643,
      "loss": 0.3478,
      "step": 242
    },
    {
      "epoch": 0.05522727272727273,
      "grad_norm": 0.05761607363820076,
      "learning_rate": 0.00018916951080773608,
      "loss": 0.3679,
      "step": 243
    },
    {
      "epoch": 0.05545454545454546,
      "grad_norm": 0.06219494715332985,
      "learning_rate": 0.0001891240045506257,
      "loss": 0.3274,
      "step": 244
    },
    {
      "epoch": 0.05568181818181818,
      "grad_norm": 0.056931450963020325,
      "learning_rate": 0.00018907849829351536,
      "loss": 0.3373,
      "step": 245
    },
    {
      "epoch": 0.05590909090909091,
      "grad_norm": 0.0499444417655468,
      "learning_rate": 0.000189032992036405,
      "loss": 0.3176,
      "step": 246
    },
    {
      "epoch": 0.05613636363636364,
      "grad_norm": 0.05455170199275017,
      "learning_rate": 0.00018898748577929466,
      "loss": 0.3243,
      "step": 247
    },
    {
      "epoch": 0.056363636363636366,
      "grad_norm": 0.06345746666193008,
      "learning_rate": 0.00018894197952218432,
      "loss": 0.3589,
      "step": 248
    },
    {
      "epoch": 0.05659090909090909,
      "grad_norm": 0.059976834803819656,
      "learning_rate": 0.00018889647326507397,
      "loss": 0.3336,
      "step": 249
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.0618450865149498,
      "learning_rate": 0.0001888509670079636,
      "loss": 0.3185,
      "step": 250
    },
    {
      "epoch": 0.057045454545454545,
      "grad_norm": 0.07273529469966888,
      "learning_rate": 0.00018880546075085325,
      "loss": 0.3639,
      "step": 251
    },
    {
      "epoch": 0.057272727272727274,
      "grad_norm": 0.06493639945983887,
      "learning_rate": 0.0001887599544937429,
      "loss": 0.3126,
      "step": 252
    },
    {
      "epoch": 0.0575,
      "grad_norm": 0.05789375305175781,
      "learning_rate": 0.00018871444823663253,
      "loss": 0.3349,
      "step": 253
    },
    {
      "epoch": 0.057727272727272724,
      "grad_norm": 0.06730552017688751,
      "learning_rate": 0.00018866894197952218,
      "loss": 0.3564,
      "step": 254
    },
    {
      "epoch": 0.05795454545454545,
      "grad_norm": 0.07241714000701904,
      "learning_rate": 0.00018862343572241183,
      "loss": 0.3508,
      "step": 255
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.07435200363397598,
      "learning_rate": 0.00018857792946530148,
      "loss": 0.3991,
      "step": 256
    },
    {
      "epoch": 0.05840909090909091,
      "grad_norm": 0.04984204098582268,
      "learning_rate": 0.00018853242320819114,
      "loss": 0.3127,
      "step": 257
    },
    {
      "epoch": 0.05863636363636364,
      "grad_norm": 0.06494864821434021,
      "learning_rate": 0.0001884869169510808,
      "loss": 0.3411,
      "step": 258
    },
    {
      "epoch": 0.05886363636363636,
      "grad_norm": 0.05709389969706535,
      "learning_rate": 0.00018844141069397044,
      "loss": 0.2936,
      "step": 259
    },
    {
      "epoch": 0.05909090909090909,
      "grad_norm": 0.07324212044477463,
      "learning_rate": 0.00018839590443686007,
      "loss": 0.3425,
      "step": 260
    },
    {
      "epoch": 0.05931818181818182,
      "grad_norm": 0.05414817854762077,
      "learning_rate": 0.00018835039817974972,
      "loss": 0.2874,
      "step": 261
    },
    {
      "epoch": 0.05954545454545455,
      "grad_norm": 0.06195894628763199,
      "learning_rate": 0.00018830489192263937,
      "loss": 0.3769,
      "step": 262
    },
    {
      "epoch": 0.059772727272727276,
      "grad_norm": 0.06658520549535751,
      "learning_rate": 0.000188259385665529,
      "loss": 0.3183,
      "step": 263
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.08125341683626175,
      "learning_rate": 0.00018821387940841865,
      "loss": 0.3718,
      "step": 264
    },
    {
      "epoch": 0.060227272727272727,
      "grad_norm": 0.06189883127808571,
      "learning_rate": 0.0001881683731513083,
      "loss": 0.3218,
      "step": 265
    },
    {
      "epoch": 0.060454545454545455,
      "grad_norm": 0.08256998658180237,
      "learning_rate": 0.00018812286689419796,
      "loss": 0.3623,
      "step": 266
    },
    {
      "epoch": 0.060681818181818184,
      "grad_norm": 0.0667322501540184,
      "learning_rate": 0.0001880773606370876,
      "loss": 0.3889,
      "step": 267
    },
    {
      "epoch": 0.060909090909090906,
      "grad_norm": 0.05645238608121872,
      "learning_rate": 0.00018803185437997726,
      "loss": 0.3343,
      "step": 268
    },
    {
      "epoch": 0.061136363636363635,
      "grad_norm": 0.07936195284128189,
      "learning_rate": 0.00018798634812286692,
      "loss": 0.4129,
      "step": 269
    },
    {
      "epoch": 0.06136363636363636,
      "grad_norm": 0.05208612233400345,
      "learning_rate": 0.00018794084186575654,
      "loss": 0.3341,
      "step": 270
    },
    {
      "epoch": 0.06159090909090909,
      "grad_norm": 0.05376911908388138,
      "learning_rate": 0.0001878953356086462,
      "loss": 0.2715,
      "step": 271
    },
    {
      "epoch": 0.06181818181818182,
      "grad_norm": 0.05768753960728645,
      "learning_rate": 0.00018784982935153585,
      "loss": 0.3503,
      "step": 272
    },
    {
      "epoch": 0.06204545454545454,
      "grad_norm": 0.06343735009431839,
      "learning_rate": 0.00018780432309442547,
      "loss": 0.3599,
      "step": 273
    },
    {
      "epoch": 0.06227272727272727,
      "grad_norm": 0.07808777689933777,
      "learning_rate": 0.00018775881683731513,
      "loss": 0.3817,
      "step": 274
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.06478074938058853,
      "learning_rate": 0.00018771331058020478,
      "loss": 0.3096,
      "step": 275
    },
    {
      "epoch": 0.06272727272727273,
      "grad_norm": 0.06855427473783493,
      "learning_rate": 0.00018766780432309443,
      "loss": 0.3398,
      "step": 276
    },
    {
      "epoch": 0.06295454545454546,
      "grad_norm": 0.07006140798330307,
      "learning_rate": 0.00018762229806598409,
      "loss": 0.3881,
      "step": 277
    },
    {
      "epoch": 0.06318181818181819,
      "grad_norm": 0.05006023496389389,
      "learning_rate": 0.00018757679180887374,
      "loss": 0.3031,
      "step": 278
    },
    {
      "epoch": 0.06340909090909091,
      "grad_norm": 0.06360261887311935,
      "learning_rate": 0.0001875312855517634,
      "loss": 0.3712,
      "step": 279
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.06049923226237297,
      "learning_rate": 0.00018748577929465302,
      "loss": 0.3447,
      "step": 280
    },
    {
      "epoch": 0.06386363636363636,
      "grad_norm": 0.05943867191672325,
      "learning_rate": 0.00018744027303754267,
      "loss": 0.3487,
      "step": 281
    },
    {
      "epoch": 0.06409090909090909,
      "grad_norm": 0.07768157124519348,
      "learning_rate": 0.00018739476678043232,
      "loss": 0.4098,
      "step": 282
    },
    {
      "epoch": 0.06431818181818182,
      "grad_norm": 0.059326086193323135,
      "learning_rate": 0.00018734926052332195,
      "loss": 0.365,
      "step": 283
    },
    {
      "epoch": 0.06454545454545454,
      "grad_norm": 0.05652749165892601,
      "learning_rate": 0.0001873037542662116,
      "loss": 0.3398,
      "step": 284
    },
    {
      "epoch": 0.06477272727272727,
      "grad_norm": 0.06126794219017029,
      "learning_rate": 0.00018725824800910125,
      "loss": 0.3596,
      "step": 285
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.060622937977313995,
      "learning_rate": 0.0001872127417519909,
      "loss": 0.3126,
      "step": 286
    },
    {
      "epoch": 0.06522727272727273,
      "grad_norm": 0.05987560749053955,
      "learning_rate": 0.00018716723549488056,
      "loss": 0.3453,
      "step": 287
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.07210557162761688,
      "learning_rate": 0.0001871217292377702,
      "loss": 0.4111,
      "step": 288
    },
    {
      "epoch": 0.06568181818181819,
      "grad_norm": 0.05392724275588989,
      "learning_rate": 0.00018707622298065987,
      "loss": 0.2772,
      "step": 289
    },
    {
      "epoch": 0.0659090909090909,
      "grad_norm": 0.06297224760055542,
      "learning_rate": 0.0001870307167235495,
      "loss": 0.3487,
      "step": 290
    },
    {
      "epoch": 0.06613636363636363,
      "grad_norm": 0.05719798430800438,
      "learning_rate": 0.00018698521046643914,
      "loss": 0.315,
      "step": 291
    },
    {
      "epoch": 0.06636363636363636,
      "grad_norm": 0.057106442749500275,
      "learning_rate": 0.00018693970420932877,
      "loss": 0.3845,
      "step": 292
    },
    {
      "epoch": 0.06659090909090909,
      "grad_norm": 0.07257503271102905,
      "learning_rate": 0.00018689419795221842,
      "loss": 0.398,
      "step": 293
    },
    {
      "epoch": 0.06681818181818182,
      "grad_norm": 0.044861674308776855,
      "learning_rate": 0.00018684869169510808,
      "loss": 0.2694,
      "step": 294
    },
    {
      "epoch": 0.06704545454545455,
      "grad_norm": 0.05067869648337364,
      "learning_rate": 0.00018680318543799773,
      "loss": 0.2981,
      "step": 295
    },
    {
      "epoch": 0.06727272727272728,
      "grad_norm": 0.04672825708985329,
      "learning_rate": 0.00018675767918088738,
      "loss": 0.3237,
      "step": 296
    },
    {
      "epoch": 0.0675,
      "grad_norm": 0.07013896852731705,
      "learning_rate": 0.00018671217292377703,
      "loss": 0.3704,
      "step": 297
    },
    {
      "epoch": 0.06772727272727273,
      "grad_norm": 0.058799296617507935,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.3214,
      "step": 298
    },
    {
      "epoch": 0.06795454545454545,
      "grad_norm": 0.06500910967588425,
      "learning_rate": 0.00018662116040955634,
      "loss": 0.3596,
      "step": 299
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.04998491704463959,
      "learning_rate": 0.00018657565415244596,
      "loss": 0.3603,
      "step": 300
    },
    {
      "epoch": 0.06818181818181818,
      "eval_loss": 0.3375626802444458,
      "eval_runtime": 229.2994,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 300
    },
    {
      "epoch": 0.0684090909090909,
      "grad_norm": 0.05571478605270386,
      "learning_rate": 0.00018653014789533562,
      "loss": 0.3529,
      "step": 301
    },
    {
      "epoch": 0.06863636363636363,
      "grad_norm": 0.05011240020394325,
      "learning_rate": 0.00018648464163822524,
      "loss": 0.2967,
      "step": 302
    },
    {
      "epoch": 0.06886363636363636,
      "grad_norm": 0.0551641620695591,
      "learning_rate": 0.0001864391353811149,
      "loss": 0.3481,
      "step": 303
    },
    {
      "epoch": 0.06909090909090909,
      "grad_norm": 0.07351671904325485,
      "learning_rate": 0.00018639362912400455,
      "loss": 0.401,
      "step": 304
    },
    {
      "epoch": 0.06931818181818182,
      "grad_norm": 0.05675261840224266,
      "learning_rate": 0.0001863481228668942,
      "loss": 0.3342,
      "step": 305
    },
    {
      "epoch": 0.06954545454545455,
      "grad_norm": 0.05562445521354675,
      "learning_rate": 0.00018630261660978385,
      "loss": 0.3231,
      "step": 306
    },
    {
      "epoch": 0.06977272727272728,
      "grad_norm": 0.06512128561735153,
      "learning_rate": 0.0001862571103526735,
      "loss": 0.2756,
      "step": 307
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.06847364455461502,
      "learning_rate": 0.00018621160409556316,
      "loss": 0.3583,
      "step": 308
    },
    {
      "epoch": 0.07022727272727272,
      "grad_norm": 0.07113979011774063,
      "learning_rate": 0.0001861660978384528,
      "loss": 0.3277,
      "step": 309
    },
    {
      "epoch": 0.07045454545454545,
      "grad_norm": 0.0561877004802227,
      "learning_rate": 0.00018612059158134244,
      "loss": 0.3825,
      "step": 310
    },
    {
      "epoch": 0.07068181818181818,
      "grad_norm": 0.07877157628536224,
      "learning_rate": 0.0001860750853242321,
      "loss": 0.375,
      "step": 311
    },
    {
      "epoch": 0.07090909090909091,
      "grad_norm": 0.0588800311088562,
      "learning_rate": 0.00018602957906712172,
      "loss": 0.3235,
      "step": 312
    },
    {
      "epoch": 0.07113636363636364,
      "grad_norm": 0.07455543428659439,
      "learning_rate": 0.00018598407281001137,
      "loss": 0.3314,
      "step": 313
    },
    {
      "epoch": 0.07136363636363637,
      "grad_norm": 0.05918717011809349,
      "learning_rate": 0.00018593856655290102,
      "loss": 0.3965,
      "step": 314
    },
    {
      "epoch": 0.0715909090909091,
      "grad_norm": 0.05986466258764267,
      "learning_rate": 0.00018589306029579068,
      "loss": 0.3199,
      "step": 315
    },
    {
      "epoch": 0.07181818181818182,
      "grad_norm": 0.06119471788406372,
      "learning_rate": 0.00018584755403868033,
      "loss": 0.2907,
      "step": 316
    },
    {
      "epoch": 0.07204545454545455,
      "grad_norm": 0.07025151699781418,
      "learning_rate": 0.00018580204778156998,
      "loss": 0.383,
      "step": 317
    },
    {
      "epoch": 0.07227272727272727,
      "grad_norm": 0.05500084161758423,
      "learning_rate": 0.00018575654152445963,
      "loss": 0.3445,
      "step": 318
    },
    {
      "epoch": 0.0725,
      "grad_norm": 0.06230686977505684,
      "learning_rate": 0.0001857110352673493,
      "loss": 0.3579,
      "step": 319
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.059757255017757416,
      "learning_rate": 0.0001856655290102389,
      "loss": 0.3572,
      "step": 320
    },
    {
      "epoch": 0.07295454545454545,
      "grad_norm": 0.055172260850667953,
      "learning_rate": 0.00018562002275312857,
      "loss": 0.3257,
      "step": 321
    },
    {
      "epoch": 0.07318181818181818,
      "grad_norm": 0.06141284853219986,
      "learning_rate": 0.0001855745164960182,
      "loss": 0.3259,
      "step": 322
    },
    {
      "epoch": 0.07340909090909091,
      "grad_norm": 0.062204401940107346,
      "learning_rate": 0.00018552901023890784,
      "loss": 0.3327,
      "step": 323
    },
    {
      "epoch": 0.07363636363636364,
      "grad_norm": 0.06782051920890808,
      "learning_rate": 0.0001854835039817975,
      "loss": 0.3765,
      "step": 324
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.04632611572742462,
      "learning_rate": 0.00018543799772468715,
      "loss": 0.3089,
      "step": 325
    },
    {
      "epoch": 0.0740909090909091,
      "grad_norm": 0.06859161704778671,
      "learning_rate": 0.0001853924914675768,
      "loss": 0.3482,
      "step": 326
    },
    {
      "epoch": 0.07431818181818182,
      "grad_norm": 0.0684117004275322,
      "learning_rate": 0.00018534698521046646,
      "loss": 0.316,
      "step": 327
    },
    {
      "epoch": 0.07454545454545454,
      "grad_norm": 0.051302239298820496,
      "learning_rate": 0.0001853014789533561,
      "loss": 0.2783,
      "step": 328
    },
    {
      "epoch": 0.07477272727272727,
      "grad_norm": 0.04913968965411186,
      "learning_rate": 0.00018525597269624576,
      "loss": 0.3127,
      "step": 329
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.07029266655445099,
      "learning_rate": 0.0001852104664391354,
      "loss": 0.4024,
      "step": 330
    },
    {
      "epoch": 0.07522727272727273,
      "grad_norm": 0.06327605992555618,
      "learning_rate": 0.00018516496018202504,
      "loss": 0.3884,
      "step": 331
    },
    {
      "epoch": 0.07545454545454545,
      "grad_norm": 0.05399687960743904,
      "learning_rate": 0.00018511945392491467,
      "loss": 0.3589,
      "step": 332
    },
    {
      "epoch": 0.07568181818181818,
      "grad_norm": 0.06942550837993622,
      "learning_rate": 0.00018507394766780432,
      "loss": 0.3727,
      "step": 333
    },
    {
      "epoch": 0.07590909090909091,
      "grad_norm": 0.05158386006951332,
      "learning_rate": 0.00018502844141069397,
      "loss": 0.3051,
      "step": 334
    },
    {
      "epoch": 0.07613636363636364,
      "grad_norm": 0.07687368988990784,
      "learning_rate": 0.00018498293515358362,
      "loss": 0.3778,
      "step": 335
    },
    {
      "epoch": 0.07636363636363637,
      "grad_norm": 0.06145583838224411,
      "learning_rate": 0.00018493742889647328,
      "loss": 0.3295,
      "step": 336
    },
    {
      "epoch": 0.07659090909090908,
      "grad_norm": 0.052066825330257416,
      "learning_rate": 0.00018489192263936293,
      "loss": 0.3501,
      "step": 337
    },
    {
      "epoch": 0.07681818181818181,
      "grad_norm": 0.054428424686193466,
      "learning_rate": 0.00018484641638225258,
      "loss": 0.3296,
      "step": 338
    },
    {
      "epoch": 0.07704545454545454,
      "grad_norm": 0.06750671565532684,
      "learning_rate": 0.00018480091012514224,
      "loss": 0.3751,
      "step": 339
    },
    {
      "epoch": 0.07727272727272727,
      "grad_norm": 0.08502378314733505,
      "learning_rate": 0.00018475540386803186,
      "loss": 0.3231,
      "step": 340
    },
    {
      "epoch": 0.0775,
      "grad_norm": 0.06974463909864426,
      "learning_rate": 0.00018470989761092151,
      "loss": 0.3802,
      "step": 341
    },
    {
      "epoch": 0.07772727272727273,
      "grad_norm": 0.05236705765128136,
      "learning_rate": 0.00018466439135381114,
      "loss": 0.3156,
      "step": 342
    },
    {
      "epoch": 0.07795454545454546,
      "grad_norm": 0.05520901083946228,
      "learning_rate": 0.0001846188850967008,
      "loss": 0.3265,
      "step": 343
    },
    {
      "epoch": 0.07818181818181819,
      "grad_norm": 0.07235579192638397,
      "learning_rate": 0.00018457337883959045,
      "loss": 0.3868,
      "step": 344
    },
    {
      "epoch": 0.07840909090909091,
      "grad_norm": 0.05018214136362076,
      "learning_rate": 0.0001845278725824801,
      "loss": 0.3003,
      "step": 345
    },
    {
      "epoch": 0.07863636363636364,
      "grad_norm": 0.06049244478344917,
      "learning_rate": 0.00018448236632536975,
      "loss": 0.2899,
      "step": 346
    },
    {
      "epoch": 0.07886363636363636,
      "grad_norm": 0.06062636896967888,
      "learning_rate": 0.0001844368600682594,
      "loss": 0.3545,
      "step": 347
    },
    {
      "epoch": 0.07909090909090909,
      "grad_norm": 0.06538060307502747,
      "learning_rate": 0.00018439135381114906,
      "loss": 0.3279,
      "step": 348
    },
    {
      "epoch": 0.07931818181818182,
      "grad_norm": 0.0950709879398346,
      "learning_rate": 0.0001843458475540387,
      "loss": 0.372,
      "step": 349
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.08758344501256943,
      "learning_rate": 0.00018430034129692833,
      "loss": 0.365,
      "step": 350
    },
    {
      "epoch": 0.07977272727272727,
      "grad_norm": 0.04499778524041176,
      "learning_rate": 0.000184254835039818,
      "loss": 0.266,
      "step": 351
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.06203042343258858,
      "learning_rate": 0.0001842093287827076,
      "loss": 0.3804,
      "step": 352
    },
    {
      "epoch": 0.08022727272727273,
      "grad_norm": 0.05408722162246704,
      "learning_rate": 0.00018416382252559727,
      "loss": 0.3052,
      "step": 353
    },
    {
      "epoch": 0.08045454545454546,
      "grad_norm": 0.0438285693526268,
      "learning_rate": 0.00018411831626848692,
      "loss": 0.263,
      "step": 354
    },
    {
      "epoch": 0.08068181818181819,
      "grad_norm": 0.04283115640282631,
      "learning_rate": 0.00018407281001137657,
      "loss": 0.2892,
      "step": 355
    },
    {
      "epoch": 0.0809090909090909,
      "grad_norm": 0.054086439311504364,
      "learning_rate": 0.00018402730375426622,
      "loss": 0.329,
      "step": 356
    },
    {
      "epoch": 0.08113636363636363,
      "grad_norm": 0.07436028122901917,
      "learning_rate": 0.00018398179749715588,
      "loss": 0.3918,
      "step": 357
    },
    {
      "epoch": 0.08136363636363636,
      "grad_norm": 0.05793753266334534,
      "learning_rate": 0.00018393629124004553,
      "loss": 0.3674,
      "step": 358
    },
    {
      "epoch": 0.08159090909090909,
      "grad_norm": 0.0623105987906456,
      "learning_rate": 0.00018389078498293518,
      "loss": 0.3542,
      "step": 359
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.07052865624427795,
      "learning_rate": 0.0001838452787258248,
      "loss": 0.3785,
      "step": 360
    },
    {
      "epoch": 0.08204545454545455,
      "grad_norm": 0.05484484136104584,
      "learning_rate": 0.00018379977246871446,
      "loss": 0.3268,
      "step": 361
    },
    {
      "epoch": 0.08227272727272728,
      "grad_norm": 0.059357356280088425,
      "learning_rate": 0.0001837542662116041,
      "loss": 0.3082,
      "step": 362
    },
    {
      "epoch": 0.0825,
      "grad_norm": 0.05056121572852135,
      "learning_rate": 0.00018370875995449374,
      "loss": 0.2903,
      "step": 363
    },
    {
      "epoch": 0.08272727272727273,
      "grad_norm": 0.054701998829841614,
      "learning_rate": 0.0001836632536973834,
      "loss": 0.3219,
      "step": 364
    },
    {
      "epoch": 0.08295454545454546,
      "grad_norm": 0.04846586659550667,
      "learning_rate": 0.00018361774744027305,
      "loss": 0.2904,
      "step": 365
    },
    {
      "epoch": 0.08318181818181818,
      "grad_norm": 0.07360218465328217,
      "learning_rate": 0.0001835722411831627,
      "loss": 0.4299,
      "step": 366
    },
    {
      "epoch": 0.0834090909090909,
      "grad_norm": 0.04738742113113403,
      "learning_rate": 0.00018352673492605235,
      "loss": 0.317,
      "step": 367
    },
    {
      "epoch": 0.08363636363636363,
      "grad_norm": 0.04903309792280197,
      "learning_rate": 0.000183481228668942,
      "loss": 0.2866,
      "step": 368
    },
    {
      "epoch": 0.08386363636363636,
      "grad_norm": 0.0833115503191948,
      "learning_rate": 0.00018343572241183166,
      "loss": 0.3785,
      "step": 369
    },
    {
      "epoch": 0.08409090909090909,
      "grad_norm": 0.06808403134346008,
      "learning_rate": 0.00018339021615472128,
      "loss": 0.368,
      "step": 370
    },
    {
      "epoch": 0.08431818181818182,
      "grad_norm": 0.05991891026496887,
      "learning_rate": 0.00018334470989761094,
      "loss": 0.3482,
      "step": 371
    },
    {
      "epoch": 0.08454545454545455,
      "grad_norm": 0.07018113136291504,
      "learning_rate": 0.00018329920364050056,
      "loss": 0.3371,
      "step": 372
    },
    {
      "epoch": 0.08477272727272728,
      "grad_norm": 0.06757903099060059,
      "learning_rate": 0.00018325369738339021,
      "loss": 0.3395,
      "step": 373
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.07070954889059067,
      "learning_rate": 0.00018320819112627987,
      "loss": 0.3901,
      "step": 374
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.04612259566783905,
      "learning_rate": 0.00018316268486916952,
      "loss": 0.2776,
      "step": 375
    },
    {
      "epoch": 0.08545454545454545,
      "grad_norm": 0.061091482639312744,
      "learning_rate": 0.00018311717861205917,
      "loss": 0.3192,
      "step": 376
    },
    {
      "epoch": 0.08568181818181818,
      "grad_norm": 0.06999938189983368,
      "learning_rate": 0.00018307167235494883,
      "loss": 0.3459,
      "step": 377
    },
    {
      "epoch": 0.08590909090909091,
      "grad_norm": 0.061206694692373276,
      "learning_rate": 0.00018302616609783848,
      "loss": 0.3338,
      "step": 378
    },
    {
      "epoch": 0.08613636363636364,
      "grad_norm": 0.05018576234579086,
      "learning_rate": 0.00018298065984072813,
      "loss": 0.3062,
      "step": 379
    },
    {
      "epoch": 0.08636363636363636,
      "grad_norm": 0.04792008921504021,
      "learning_rate": 0.00018293515358361776,
      "loss": 0.2362,
      "step": 380
    },
    {
      "epoch": 0.0865909090909091,
      "grad_norm": 0.058117084205150604,
      "learning_rate": 0.0001828896473265074,
      "loss": 0.3454,
      "step": 381
    },
    {
      "epoch": 0.08681818181818182,
      "grad_norm": 0.05418876186013222,
      "learning_rate": 0.00018284414106939704,
      "loss": 0.3581,
      "step": 382
    },
    {
      "epoch": 0.08704545454545455,
      "grad_norm": 0.05510317161679268,
      "learning_rate": 0.0001827986348122867,
      "loss": 0.3217,
      "step": 383
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.06461760401725769,
      "learning_rate": 0.00018275312855517634,
      "loss": 0.3507,
      "step": 384
    },
    {
      "epoch": 0.0875,
      "grad_norm": 0.04381901025772095,
      "learning_rate": 0.000182707622298066,
      "loss": 0.309,
      "step": 385
    },
    {
      "epoch": 0.08772727272727272,
      "grad_norm": 0.04516829550266266,
      "learning_rate": 0.00018266211604095565,
      "loss": 0.2841,
      "step": 386
    },
    {
      "epoch": 0.08795454545454545,
      "grad_norm": 0.08336053788661957,
      "learning_rate": 0.0001826166097838453,
      "loss": 0.333,
      "step": 387
    },
    {
      "epoch": 0.08818181818181818,
      "grad_norm": 0.0661243200302124,
      "learning_rate": 0.00018257110352673495,
      "loss": 0.3221,
      "step": 388
    },
    {
      "epoch": 0.08840909090909091,
      "grad_norm": 0.06861138343811035,
      "learning_rate": 0.0001825255972696246,
      "loss": 0.3496,
      "step": 389
    },
    {
      "epoch": 0.08863636363636364,
      "grad_norm": 0.0420398972928524,
      "learning_rate": 0.00018248009101251423,
      "loss": 0.2254,
      "step": 390
    },
    {
      "epoch": 0.08886363636363637,
      "grad_norm": 0.06301550567150116,
      "learning_rate": 0.00018243458475540386,
      "loss": 0.3174,
      "step": 391
    },
    {
      "epoch": 0.0890909090909091,
      "grad_norm": 0.07083197683095932,
      "learning_rate": 0.0001823890784982935,
      "loss": 0.3824,
      "step": 392
    },
    {
      "epoch": 0.08931818181818182,
      "grad_norm": 0.043263990432024,
      "learning_rate": 0.00018234357224118316,
      "loss": 0.2702,
      "step": 393
    },
    {
      "epoch": 0.08954545454545454,
      "grad_norm": 0.06810455769300461,
      "learning_rate": 0.00018229806598407282,
      "loss": 0.3569,
      "step": 394
    },
    {
      "epoch": 0.08977272727272727,
      "grad_norm": 0.04648266360163689,
      "learning_rate": 0.00018225255972696247,
      "loss": 0.2519,
      "step": 395
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.05640213191509247,
      "learning_rate": 0.00018220705346985212,
      "loss": 0.3573,
      "step": 396
    },
    {
      "epoch": 0.09022727272727273,
      "grad_norm": 0.050358474254608154,
      "learning_rate": 0.00018216154721274177,
      "loss": 0.2516,
      "step": 397
    },
    {
      "epoch": 0.09045454545454545,
      "grad_norm": 0.06371146440505981,
      "learning_rate": 0.00018211604095563143,
      "loss": 0.405,
      "step": 398
    },
    {
      "epoch": 0.09068181818181818,
      "grad_norm": 0.049988098442554474,
      "learning_rate": 0.00018207053469852105,
      "loss": 0.3192,
      "step": 399
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.06314140558242798,
      "learning_rate": 0.0001820250284414107,
      "loss": 0.3701,
      "step": 400
    },
    {
      "epoch": 0.09090909090909091,
      "eval_loss": 0.3364519774913788,
      "eval_runtime": 229.3813,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 400
    },
    {
      "epoch": 0.09113636363636364,
      "grad_norm": 0.07614454627037048,
      "learning_rate": 0.00018197952218430033,
      "loss": 0.3144,
      "step": 401
    },
    {
      "epoch": 0.09136363636363637,
      "grad_norm": 0.0628618448972702,
      "learning_rate": 0.00018193401592718998,
      "loss": 0.3436,
      "step": 402
    },
    {
      "epoch": 0.0915909090909091,
      "grad_norm": 0.062169600278139114,
      "learning_rate": 0.00018188850967007964,
      "loss": 0.3672,
      "step": 403
    },
    {
      "epoch": 0.09181818181818181,
      "grad_norm": 0.04231170564889908,
      "learning_rate": 0.0001818430034129693,
      "loss": 0.2981,
      "step": 404
    },
    {
      "epoch": 0.09204545454545454,
      "grad_norm": 0.06226668134331703,
      "learning_rate": 0.00018179749715585894,
      "loss": 0.332,
      "step": 405
    },
    {
      "epoch": 0.09227272727272727,
      "grad_norm": 0.06952092051506042,
      "learning_rate": 0.0001817519908987486,
      "loss": 0.3889,
      "step": 406
    },
    {
      "epoch": 0.0925,
      "grad_norm": 0.062365975230932236,
      "learning_rate": 0.00018170648464163825,
      "loss": 0.2873,
      "step": 407
    },
    {
      "epoch": 0.09272727272727273,
      "grad_norm": 0.054913077503442764,
      "learning_rate": 0.0001816609783845279,
      "loss": 0.3515,
      "step": 408
    },
    {
      "epoch": 0.09295454545454546,
      "grad_norm": 0.07111149281263351,
      "learning_rate": 0.00018161547212741753,
      "loss": 0.3718,
      "step": 409
    },
    {
      "epoch": 0.09318181818181819,
      "grad_norm": 0.07363881915807724,
      "learning_rate": 0.00018156996587030718,
      "loss": 0.3746,
      "step": 410
    },
    {
      "epoch": 0.09340909090909091,
      "grad_norm": 0.06373968720436096,
      "learning_rate": 0.0001815244596131968,
      "loss": 0.4089,
      "step": 411
    },
    {
      "epoch": 0.09363636363636364,
      "grad_norm": 0.04143640771508217,
      "learning_rate": 0.00018147895335608646,
      "loss": 0.2755,
      "step": 412
    },
    {
      "epoch": 0.09386363636363636,
      "grad_norm": 0.06671814620494843,
      "learning_rate": 0.0001814334470989761,
      "loss": 0.3125,
      "step": 413
    },
    {
      "epoch": 0.09409090909090909,
      "grad_norm": 0.07465078681707382,
      "learning_rate": 0.00018138794084186576,
      "loss": 0.3329,
      "step": 414
    },
    {
      "epoch": 0.09431818181818181,
      "grad_norm": 0.07469790428876877,
      "learning_rate": 0.00018134243458475542,
      "loss": 0.362,
      "step": 415
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.05462029203772545,
      "learning_rate": 0.00018129692832764507,
      "loss": 0.2811,
      "step": 416
    },
    {
      "epoch": 0.09477272727272727,
      "grad_norm": 0.05136135593056679,
      "learning_rate": 0.00018125142207053472,
      "loss": 0.2809,
      "step": 417
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06241116672754288,
      "learning_rate": 0.00018120591581342437,
      "loss": 0.3075,
      "step": 418
    },
    {
      "epoch": 0.09522727272727273,
      "grad_norm": 0.04923901706933975,
      "learning_rate": 0.000181160409556314,
      "loss": 0.2606,
      "step": 419
    },
    {
      "epoch": 0.09545454545454546,
      "grad_norm": 0.05547000840306282,
      "learning_rate": 0.00018111490329920365,
      "loss": 0.3287,
      "step": 420
    },
    {
      "epoch": 0.09568181818181819,
      "grad_norm": 0.06978916376829147,
      "learning_rate": 0.00018106939704209328,
      "loss": 0.3983,
      "step": 421
    },
    {
      "epoch": 0.0959090909090909,
      "grad_norm": 0.06304024159908295,
      "learning_rate": 0.00018102389078498293,
      "loss": 0.3582,
      "step": 422
    },
    {
      "epoch": 0.09613636363636363,
      "grad_norm": 0.07052987068891525,
      "learning_rate": 0.00018097838452787258,
      "loss": 0.3479,
      "step": 423
    },
    {
      "epoch": 0.09636363636363636,
      "grad_norm": 0.05885699391365051,
      "learning_rate": 0.00018093287827076224,
      "loss": 0.2777,
      "step": 424
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.06951218843460083,
      "learning_rate": 0.0001808873720136519,
      "loss": 0.3127,
      "step": 425
    },
    {
      "epoch": 0.09681818181818182,
      "grad_norm": 0.06593619287014008,
      "learning_rate": 0.00018084186575654154,
      "loss": 0.3738,
      "step": 426
    },
    {
      "epoch": 0.09704545454545455,
      "grad_norm": 0.09853368252515793,
      "learning_rate": 0.0001807963594994312,
      "loss": 0.3467,
      "step": 427
    },
    {
      "epoch": 0.09727272727272727,
      "grad_norm": 0.05188799649477005,
      "learning_rate": 0.00018075085324232082,
      "loss": 0.2697,
      "step": 428
    },
    {
      "epoch": 0.0975,
      "grad_norm": 0.0518181212246418,
      "learning_rate": 0.00018070534698521047,
      "loss": 0.2852,
      "step": 429
    },
    {
      "epoch": 0.09772727272727273,
      "grad_norm": 0.05629769340157509,
      "learning_rate": 0.00018065984072810013,
      "loss": 0.3176,
      "step": 430
    },
    {
      "epoch": 0.09795454545454546,
      "grad_norm": 0.05179151892662048,
      "learning_rate": 0.00018061433447098975,
      "loss": 0.3277,
      "step": 431
    },
    {
      "epoch": 0.09818181818181818,
      "grad_norm": 0.049845680594444275,
      "learning_rate": 0.0001805688282138794,
      "loss": 0.3002,
      "step": 432
    },
    {
      "epoch": 0.0984090909090909,
      "grad_norm": 0.072137750685215,
      "learning_rate": 0.00018052332195676906,
      "loss": 0.3563,
      "step": 433
    },
    {
      "epoch": 0.09863636363636363,
      "grad_norm": 0.05695284903049469,
      "learning_rate": 0.0001804778156996587,
      "loss": 0.2988,
      "step": 434
    },
    {
      "epoch": 0.09886363636363636,
      "grad_norm": 0.06448507308959961,
      "learning_rate": 0.00018043230944254836,
      "loss": 0.3424,
      "step": 435
    },
    {
      "epoch": 0.09909090909090909,
      "grad_norm": 0.05255685746669769,
      "learning_rate": 0.00018038680318543802,
      "loss": 0.3139,
      "step": 436
    },
    {
      "epoch": 0.09931818181818182,
      "grad_norm": 0.04958260804414749,
      "learning_rate": 0.00018034129692832767,
      "loss": 0.3164,
      "step": 437
    },
    {
      "epoch": 0.09954545454545455,
      "grad_norm": 0.05611756816506386,
      "learning_rate": 0.0001802957906712173,
      "loss": 0.3488,
      "step": 438
    },
    {
      "epoch": 0.09977272727272728,
      "grad_norm": 0.0713486522436142,
      "learning_rate": 0.00018025028441410695,
      "loss": 0.3958,
      "step": 439
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.05434608832001686,
      "learning_rate": 0.0001802047781569966,
      "loss": 0.3172,
      "step": 440
    },
    {
      "epoch": 0.10022727272727272,
      "grad_norm": 0.047852639108896255,
      "learning_rate": 0.00018015927189988623,
      "loss": 0.2686,
      "step": 441
    },
    {
      "epoch": 0.10045454545454545,
      "grad_norm": 0.0675477460026741,
      "learning_rate": 0.00018011376564277588,
      "loss": 0.3863,
      "step": 442
    },
    {
      "epoch": 0.10068181818181818,
      "grad_norm": 0.06973353773355484,
      "learning_rate": 0.00018006825938566553,
      "loss": 0.3015,
      "step": 443
    },
    {
      "epoch": 0.1009090909090909,
      "grad_norm": 0.05300756171345711,
      "learning_rate": 0.00018002275312855518,
      "loss": 0.3133,
      "step": 444
    },
    {
      "epoch": 0.10113636363636364,
      "grad_norm": 0.04301581159234047,
      "learning_rate": 0.00017997724687144484,
      "loss": 0.2521,
      "step": 445
    },
    {
      "epoch": 0.10136363636363636,
      "grad_norm": 0.048425380140542984,
      "learning_rate": 0.0001799317406143345,
      "loss": 0.3171,
      "step": 446
    },
    {
      "epoch": 0.10159090909090909,
      "grad_norm": 0.049164433032274246,
      "learning_rate": 0.00017988623435722414,
      "loss": 0.2791,
      "step": 447
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.05639597773551941,
      "learning_rate": 0.00017984072810011377,
      "loss": 0.3244,
      "step": 448
    },
    {
      "epoch": 0.10204545454545455,
      "grad_norm": 0.05769642814993858,
      "learning_rate": 0.00017979522184300342,
      "loss": 0.3196,
      "step": 449
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.058185551315546036,
      "learning_rate": 0.00017974971558589307,
      "loss": 0.3362,
      "step": 450
    },
    {
      "epoch": 0.1025,
      "grad_norm": 0.06437792629003525,
      "learning_rate": 0.0001797042093287827,
      "loss": 0.4056,
      "step": 451
    },
    {
      "epoch": 0.10272727272727272,
      "grad_norm": 0.06978847086429596,
      "learning_rate": 0.00017965870307167235,
      "loss": 0.367,
      "step": 452
    },
    {
      "epoch": 0.10295454545454545,
      "grad_norm": 0.05250142514705658,
      "learning_rate": 0.000179613196814562,
      "loss": 0.3064,
      "step": 453
    },
    {
      "epoch": 0.10318181818181818,
      "grad_norm": 0.051788974553346634,
      "learning_rate": 0.00017956769055745166,
      "loss": 0.3138,
      "step": 454
    },
    {
      "epoch": 0.10340909090909091,
      "grad_norm": 0.07662665843963623,
      "learning_rate": 0.0001795221843003413,
      "loss": 0.4119,
      "step": 455
    },
    {
      "epoch": 0.10363636363636364,
      "grad_norm": 0.05637098848819733,
      "learning_rate": 0.00017947667804323096,
      "loss": 0.3776,
      "step": 456
    },
    {
      "epoch": 0.10386363636363637,
      "grad_norm": 0.05243312194943428,
      "learning_rate": 0.00017943117178612062,
      "loss": 0.3195,
      "step": 457
    },
    {
      "epoch": 0.1040909090909091,
      "grad_norm": 0.05853841081261635,
      "learning_rate": 0.00017938566552901024,
      "loss": 0.2956,
      "step": 458
    },
    {
      "epoch": 0.10431818181818182,
      "grad_norm": 0.052953075617551804,
      "learning_rate": 0.0001793401592718999,
      "loss": 0.301,
      "step": 459
    },
    {
      "epoch": 0.10454545454545454,
      "grad_norm": 0.05661046504974365,
      "learning_rate": 0.00017929465301478955,
      "loss": 0.2892,
      "step": 460
    },
    {
      "epoch": 0.10477272727272727,
      "grad_norm": 0.061243750154972076,
      "learning_rate": 0.00017924914675767917,
      "loss": 0.3173,
      "step": 461
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.055729176849126816,
      "learning_rate": 0.00017920364050056883,
      "loss": 0.3309,
      "step": 462
    },
    {
      "epoch": 0.10522727272727272,
      "grad_norm": 0.04795679822564125,
      "learning_rate": 0.00017915813424345848,
      "loss": 0.3099,
      "step": 463
    },
    {
      "epoch": 0.10545454545454545,
      "grad_norm": 0.046669576317071915,
      "learning_rate": 0.00017911262798634813,
      "loss": 0.3006,
      "step": 464
    },
    {
      "epoch": 0.10568181818181818,
      "grad_norm": 0.08013086020946503,
      "learning_rate": 0.00017906712172923779,
      "loss": 0.3785,
      "step": 465
    },
    {
      "epoch": 0.10590909090909091,
      "grad_norm": 0.04876035451889038,
      "learning_rate": 0.00017902161547212744,
      "loss": 0.3186,
      "step": 466
    },
    {
      "epoch": 0.10613636363636364,
      "grad_norm": 0.0654255673289299,
      "learning_rate": 0.00017897610921501706,
      "loss": 0.3373,
      "step": 467
    },
    {
      "epoch": 0.10636363636363637,
      "grad_norm": 0.06645885109901428,
      "learning_rate": 0.00017893060295790672,
      "loss": 0.3651,
      "step": 468
    },
    {
      "epoch": 0.1065909090909091,
      "grad_norm": 0.046544890850782394,
      "learning_rate": 0.00017888509670079637,
      "loss": 0.268,
      "step": 469
    },
    {
      "epoch": 0.10681818181818181,
      "grad_norm": 0.057280898094177246,
      "learning_rate": 0.00017883959044368602,
      "loss": 0.3091,
      "step": 470
    },
    {
      "epoch": 0.10704545454545454,
      "grad_norm": 0.0473831370472908,
      "learning_rate": 0.00017879408418657565,
      "loss": 0.2871,
      "step": 471
    },
    {
      "epoch": 0.10727272727272727,
      "grad_norm": 0.05136201158165932,
      "learning_rate": 0.0001787485779294653,
      "loss": 0.2855,
      "step": 472
    },
    {
      "epoch": 0.1075,
      "grad_norm": 0.06771073490381241,
      "learning_rate": 0.00017870307167235495,
      "loss": 0.3682,
      "step": 473
    },
    {
      "epoch": 0.10772727272727273,
      "grad_norm": 0.05622688680887222,
      "learning_rate": 0.0001786575654152446,
      "loss": 0.3006,
      "step": 474
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.06537683308124542,
      "learning_rate": 0.00017861205915813426,
      "loss": 0.3683,
      "step": 475
    },
    {
      "epoch": 0.10818181818181818,
      "grad_norm": 0.051876869052648544,
      "learning_rate": 0.0001785665529010239,
      "loss": 0.2918,
      "step": 476
    },
    {
      "epoch": 0.10840909090909091,
      "grad_norm": 0.06505049020051956,
      "learning_rate": 0.00017852104664391354,
      "loss": 0.3799,
      "step": 477
    },
    {
      "epoch": 0.10863636363636364,
      "grad_norm": 0.07585334032773972,
      "learning_rate": 0.0001784755403868032,
      "loss": 0.392,
      "step": 478
    },
    {
      "epoch": 0.10886363636363636,
      "grad_norm": 0.06538118422031403,
      "learning_rate": 0.00017843003412969284,
      "loss": 0.3449,
      "step": 479
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.06496328860521317,
      "learning_rate": 0.0001783845278725825,
      "loss": 0.3728,
      "step": 480
    },
    {
      "epoch": 0.10931818181818181,
      "grad_norm": 0.04781965911388397,
      "learning_rate": 0.00017833902161547212,
      "loss": 0.3156,
      "step": 481
    },
    {
      "epoch": 0.10954545454545454,
      "grad_norm": 0.09350293129682541,
      "learning_rate": 0.00017829351535836178,
      "loss": 0.3973,
      "step": 482
    },
    {
      "epoch": 0.10977272727272727,
      "grad_norm": 0.05953672155737877,
      "learning_rate": 0.00017824800910125143,
      "loss": 0.3638,
      "step": 483
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.056874167174100876,
      "learning_rate": 0.00017820250284414108,
      "loss": 0.3469,
      "step": 484
    },
    {
      "epoch": 0.11022727272727273,
      "grad_norm": 0.05511213839054108,
      "learning_rate": 0.00017815699658703073,
      "loss": 0.3432,
      "step": 485
    },
    {
      "epoch": 0.11045454545454546,
      "grad_norm": 0.07507922500371933,
      "learning_rate": 0.0001781114903299204,
      "loss": 0.3948,
      "step": 486
    },
    {
      "epoch": 0.11068181818181819,
      "grad_norm": 0.04522980377078056,
      "learning_rate": 0.00017806598407281,
      "loss": 0.2917,
      "step": 487
    },
    {
      "epoch": 0.11090909090909092,
      "grad_norm": 0.04748627170920372,
      "learning_rate": 0.00017802047781569967,
      "loss": 0.2896,
      "step": 488
    },
    {
      "epoch": 0.11113636363636363,
      "grad_norm": 0.05316555127501488,
      "learning_rate": 0.00017797497155858932,
      "loss": 0.3342,
      "step": 489
    },
    {
      "epoch": 0.11136363636363636,
      "grad_norm": 0.050108835101127625,
      "learning_rate": 0.00017792946530147897,
      "loss": 0.3058,
      "step": 490
    },
    {
      "epoch": 0.11159090909090909,
      "grad_norm": 0.05937526747584343,
      "learning_rate": 0.0001778839590443686,
      "loss": 0.3556,
      "step": 491
    },
    {
      "epoch": 0.11181818181818182,
      "grad_norm": 0.06405888497829437,
      "learning_rate": 0.00017783845278725825,
      "loss": 0.3814,
      "step": 492
    },
    {
      "epoch": 0.11204545454545455,
      "grad_norm": 0.06423214077949524,
      "learning_rate": 0.0001777929465301479,
      "loss": 0.399,
      "step": 493
    },
    {
      "epoch": 0.11227272727272727,
      "grad_norm": 0.057938117533922195,
      "learning_rate": 0.00017774744027303755,
      "loss": 0.3271,
      "step": 494
    },
    {
      "epoch": 0.1125,
      "grad_norm": 0.05632242560386658,
      "learning_rate": 0.0001777019340159272,
      "loss": 0.3017,
      "step": 495
    },
    {
      "epoch": 0.11272727272727273,
      "grad_norm": 0.047210998833179474,
      "learning_rate": 0.00017765642775881686,
      "loss": 0.3195,
      "step": 496
    },
    {
      "epoch": 0.11295454545454546,
      "grad_norm": 0.044692493975162506,
      "learning_rate": 0.00017761092150170649,
      "loss": 0.3009,
      "step": 497
    },
    {
      "epoch": 0.11318181818181818,
      "grad_norm": 0.052671849727630615,
      "learning_rate": 0.00017756541524459614,
      "loss": 0.376,
      "step": 498
    },
    {
      "epoch": 0.1134090909090909,
      "grad_norm": 0.055584441870450974,
      "learning_rate": 0.0001775199089874858,
      "loss": 0.3627,
      "step": 499
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.03603094071149826,
      "learning_rate": 0.00017747440273037544,
      "loss": 0.2348,
      "step": 500
    },
    {
      "epoch": 0.11363636363636363,
      "eval_loss": 0.3352252244949341,
      "eval_runtime": 229.3356,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 500
    },
    {
      "epoch": 0.11386363636363636,
      "grad_norm": 0.048993512988090515,
      "learning_rate": 0.00017742889647326507,
      "loss": 0.2902,
      "step": 501
    },
    {
      "epoch": 0.11409090909090909,
      "grad_norm": 0.048808611929416656,
      "learning_rate": 0.00017738339021615472,
      "loss": 0.3665,
      "step": 502
    },
    {
      "epoch": 0.11431818181818182,
      "grad_norm": 0.05437523126602173,
      "learning_rate": 0.00017733788395904438,
      "loss": 0.3493,
      "step": 503
    },
    {
      "epoch": 0.11454545454545455,
      "grad_norm": 0.057084858417510986,
      "learning_rate": 0.00017729237770193403,
      "loss": 0.3595,
      "step": 504
    },
    {
      "epoch": 0.11477272727272728,
      "grad_norm": 0.060430858284235,
      "learning_rate": 0.00017724687144482368,
      "loss": 0.4006,
      "step": 505
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.05483154579997063,
      "learning_rate": 0.0001772013651877133,
      "loss": 0.3505,
      "step": 506
    },
    {
      "epoch": 0.11522727272727273,
      "grad_norm": 0.05411570891737938,
      "learning_rate": 0.00017715585893060296,
      "loss": 0.3356,
      "step": 507
    },
    {
      "epoch": 0.11545454545454545,
      "grad_norm": 0.06525540351867676,
      "learning_rate": 0.0001771103526734926,
      "loss": 0.3997,
      "step": 508
    },
    {
      "epoch": 0.11568181818181818,
      "grad_norm": 0.06886537373065948,
      "learning_rate": 0.00017706484641638227,
      "loss": 0.3821,
      "step": 509
    },
    {
      "epoch": 0.1159090909090909,
      "grad_norm": 0.05847842991352081,
      "learning_rate": 0.00017701934015927192,
      "loss": 0.3881,
      "step": 510
    },
    {
      "epoch": 0.11613636363636363,
      "grad_norm": 0.04758916050195694,
      "learning_rate": 0.00017697383390216154,
      "loss": 0.2962,
      "step": 511
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.048142097890377045,
      "learning_rate": 0.0001769283276450512,
      "loss": 0.2558,
      "step": 512
    },
    {
      "epoch": 0.11659090909090909,
      "grad_norm": 0.06142638251185417,
      "learning_rate": 0.00017688282138794085,
      "loss": 0.3543,
      "step": 513
    },
    {
      "epoch": 0.11681818181818182,
      "grad_norm": 0.05205705761909485,
      "learning_rate": 0.0001768373151308305,
      "loss": 0.338,
      "step": 514
    },
    {
      "epoch": 0.11704545454545455,
      "grad_norm": 0.04839332029223442,
      "learning_rate": 0.00017679180887372016,
      "loss": 0.336,
      "step": 515
    },
    {
      "epoch": 0.11727272727272728,
      "grad_norm": 0.06375140696763992,
      "learning_rate": 0.00017674630261660978,
      "loss": 0.3561,
      "step": 516
    },
    {
      "epoch": 0.1175,
      "grad_norm": 0.043640006333589554,
      "learning_rate": 0.00017670079635949943,
      "loss": 0.2883,
      "step": 517
    },
    {
      "epoch": 0.11772727272727272,
      "grad_norm": 0.06426126509904861,
      "learning_rate": 0.0001766552901023891,
      "loss": 0.3525,
      "step": 518
    },
    {
      "epoch": 0.11795454545454545,
      "grad_norm": 0.06798187643289566,
      "learning_rate": 0.00017660978384527874,
      "loss": 0.3808,
      "step": 519
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 0.05307812616229057,
      "learning_rate": 0.0001765642775881684,
      "loss": 0.3255,
      "step": 520
    },
    {
      "epoch": 0.11840909090909091,
      "grad_norm": 0.046551983803510666,
      "learning_rate": 0.00017651877133105802,
      "loss": 0.2987,
      "step": 521
    },
    {
      "epoch": 0.11863636363636364,
      "grad_norm": 0.047288887202739716,
      "learning_rate": 0.00017647326507394767,
      "loss": 0.3106,
      "step": 522
    },
    {
      "epoch": 0.11886363636363637,
      "grad_norm": 0.04344639927148819,
      "learning_rate": 0.00017642775881683732,
      "loss": 0.2921,
      "step": 523
    },
    {
      "epoch": 0.1190909090909091,
      "grad_norm": 0.04615866765379906,
      "learning_rate": 0.00017638225255972698,
      "loss": 0.3171,
      "step": 524
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.04903727024793625,
      "learning_rate": 0.00017633674630261663,
      "loss": 0.3377,
      "step": 525
    },
    {
      "epoch": 0.11954545454545455,
      "grad_norm": 0.04798099771142006,
      "learning_rate": 0.00017629124004550626,
      "loss": 0.3493,
      "step": 526
    },
    {
      "epoch": 0.11977272727272727,
      "grad_norm": 0.06974482536315918,
      "learning_rate": 0.0001762457337883959,
      "loss": 0.3937,
      "step": 527
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.052319880574941635,
      "learning_rate": 0.00017620022753128556,
      "loss": 0.3275,
      "step": 528
    },
    {
      "epoch": 0.12022727272727272,
      "grad_norm": 0.06406069546937943,
      "learning_rate": 0.00017615472127417521,
      "loss": 0.3802,
      "step": 529
    },
    {
      "epoch": 0.12045454545454545,
      "grad_norm": 0.05667225643992424,
      "learning_rate": 0.00017610921501706487,
      "loss": 0.2996,
      "step": 530
    },
    {
      "epoch": 0.12068181818181818,
      "grad_norm": 0.048895567655563354,
      "learning_rate": 0.0001760637087599545,
      "loss": 0.3107,
      "step": 531
    },
    {
      "epoch": 0.12090909090909091,
      "grad_norm": 0.06562640517950058,
      "learning_rate": 0.00017601820250284415,
      "loss": 0.3485,
      "step": 532
    },
    {
      "epoch": 0.12113636363636364,
      "grad_norm": 0.052542079240083694,
      "learning_rate": 0.0001759726962457338,
      "loss": 0.3064,
      "step": 533
    },
    {
      "epoch": 0.12136363636363637,
      "grad_norm": 0.07144320756196976,
      "learning_rate": 0.00017592718998862345,
      "loss": 0.3612,
      "step": 534
    },
    {
      "epoch": 0.1215909090909091,
      "grad_norm": 0.04809005931019783,
      "learning_rate": 0.00017588168373151308,
      "loss": 0.3161,
      "step": 535
    },
    {
      "epoch": 0.12181818181818181,
      "grad_norm": 0.0640719011425972,
      "learning_rate": 0.00017583617747440273,
      "loss": 0.411,
      "step": 536
    },
    {
      "epoch": 0.12204545454545454,
      "grad_norm": 0.060218580067157745,
      "learning_rate": 0.00017579067121729238,
      "loss": 0.3271,
      "step": 537
    },
    {
      "epoch": 0.12227272727272727,
      "grad_norm": 0.05714570730924606,
      "learning_rate": 0.00017574516496018203,
      "loss": 0.3044,
      "step": 538
    },
    {
      "epoch": 0.1225,
      "grad_norm": 0.05448334664106369,
      "learning_rate": 0.0001756996587030717,
      "loss": 0.2965,
      "step": 539
    },
    {
      "epoch": 0.12272727272727273,
      "grad_norm": 0.03829410299658775,
      "learning_rate": 0.00017565415244596134,
      "loss": 0.2699,
      "step": 540
    },
    {
      "epoch": 0.12295454545454546,
      "grad_norm": 0.0695779100060463,
      "learning_rate": 0.00017560864618885097,
      "loss": 0.365,
      "step": 541
    },
    {
      "epoch": 0.12318181818181818,
      "grad_norm": 0.06481578946113586,
      "learning_rate": 0.00017556313993174062,
      "loss": 0.3232,
      "step": 542
    },
    {
      "epoch": 0.12340909090909091,
      "grad_norm": 0.04715026170015335,
      "learning_rate": 0.00017551763367463027,
      "loss": 0.3014,
      "step": 543
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.05785340070724487,
      "learning_rate": 0.00017547212741751992,
      "loss": 0.3497,
      "step": 544
    },
    {
      "epoch": 0.12386363636363637,
      "grad_norm": 0.056276965886354446,
      "learning_rate": 0.00017542662116040955,
      "loss": 0.3346,
      "step": 545
    },
    {
      "epoch": 0.12409090909090909,
      "grad_norm": 0.0649939477443695,
      "learning_rate": 0.0001753811149032992,
      "loss": 0.3322,
      "step": 546
    },
    {
      "epoch": 0.12431818181818181,
      "grad_norm": 0.0663546547293663,
      "learning_rate": 0.00017533560864618886,
      "loss": 0.3326,
      "step": 547
    },
    {
      "epoch": 0.12454545454545454,
      "grad_norm": 0.0654093325138092,
      "learning_rate": 0.0001752901023890785,
      "loss": 0.4097,
      "step": 548
    },
    {
      "epoch": 0.12477272727272727,
      "grad_norm": 0.06352446228265762,
      "learning_rate": 0.00017524459613196816,
      "loss": 0.4086,
      "step": 549
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.05300815775990486,
      "learning_rate": 0.00017519908987485781,
      "loss": 0.3424,
      "step": 550
    },
    {
      "epoch": 0.12522727272727271,
      "grad_norm": 0.04001250118017197,
      "learning_rate": 0.00017515358361774744,
      "loss": 0.2727,
      "step": 551
    },
    {
      "epoch": 0.12545454545454546,
      "grad_norm": 0.07137420773506165,
      "learning_rate": 0.0001751080773606371,
      "loss": 0.3887,
      "step": 552
    },
    {
      "epoch": 0.12568181818181817,
      "grad_norm": 0.06737621873617172,
      "learning_rate": 0.00017506257110352675,
      "loss": 0.4098,
      "step": 553
    },
    {
      "epoch": 0.12590909090909091,
      "grad_norm": 0.04923174902796745,
      "learning_rate": 0.0001750170648464164,
      "loss": 0.3486,
      "step": 554
    },
    {
      "epoch": 0.12613636363636363,
      "grad_norm": 0.05270671844482422,
      "learning_rate": 0.00017497155858930602,
      "loss": 0.3273,
      "step": 555
    },
    {
      "epoch": 0.12636363636363637,
      "grad_norm": 0.08793140947818756,
      "learning_rate": 0.00017492605233219568,
      "loss": 0.3147,
      "step": 556
    },
    {
      "epoch": 0.1265909090909091,
      "grad_norm": 0.04154791682958603,
      "learning_rate": 0.00017488054607508533,
      "loss": 0.2868,
      "step": 557
    },
    {
      "epoch": 0.12681818181818183,
      "grad_norm": 0.059143681079149246,
      "learning_rate": 0.00017483503981797498,
      "loss": 0.3775,
      "step": 558
    },
    {
      "epoch": 0.12704545454545454,
      "grad_norm": 0.047602422535419464,
      "learning_rate": 0.00017478953356086464,
      "loss": 0.3068,
      "step": 559
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.05725899711251259,
      "learning_rate": 0.0001747440273037543,
      "loss": 0.3087,
      "step": 560
    },
    {
      "epoch": 0.1275,
      "grad_norm": 0.05410975217819214,
      "learning_rate": 0.00017469852104664391,
      "loss": 0.3144,
      "step": 561
    },
    {
      "epoch": 0.12772727272727272,
      "grad_norm": 0.06257391721010208,
      "learning_rate": 0.00017465301478953357,
      "loss": 0.3768,
      "step": 562
    },
    {
      "epoch": 0.12795454545454546,
      "grad_norm": 0.05953197181224823,
      "learning_rate": 0.00017460750853242322,
      "loss": 0.3534,
      "step": 563
    },
    {
      "epoch": 0.12818181818181817,
      "grad_norm": 0.07276472449302673,
      "learning_rate": 0.00017456200227531287,
      "loss": 0.3959,
      "step": 564
    },
    {
      "epoch": 0.12840909090909092,
      "grad_norm": 0.05736171081662178,
      "learning_rate": 0.0001745164960182025,
      "loss": 0.3311,
      "step": 565
    },
    {
      "epoch": 0.12863636363636363,
      "grad_norm": 0.042416349053382874,
      "learning_rate": 0.00017447098976109215,
      "loss": 0.3228,
      "step": 566
    },
    {
      "epoch": 0.12886363636363637,
      "grad_norm": 0.04784190654754639,
      "learning_rate": 0.0001744254835039818,
      "loss": 0.3323,
      "step": 567
    },
    {
      "epoch": 0.1290909090909091,
      "grad_norm": 0.04407854750752449,
      "learning_rate": 0.00017437997724687146,
      "loss": 0.2614,
      "step": 568
    },
    {
      "epoch": 0.1293181818181818,
      "grad_norm": 0.043174393475055695,
      "learning_rate": 0.0001743344709897611,
      "loss": 0.2798,
      "step": 569
    },
    {
      "epoch": 0.12954545454545455,
      "grad_norm": 0.06062325835227966,
      "learning_rate": 0.00017428896473265076,
      "loss": 0.3406,
      "step": 570
    },
    {
      "epoch": 0.12977272727272726,
      "grad_norm": 0.04567984491586685,
      "learning_rate": 0.0001742434584755404,
      "loss": 0.3165,
      "step": 571
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.049220796674489975,
      "learning_rate": 0.00017419795221843004,
      "loss": 0.294,
      "step": 572
    },
    {
      "epoch": 0.13022727272727272,
      "grad_norm": 0.06136693060398102,
      "learning_rate": 0.0001741524459613197,
      "loss": 0.3113,
      "step": 573
    },
    {
      "epoch": 0.13045454545454546,
      "grad_norm": 0.05443638935685158,
      "learning_rate": 0.00017410693970420932,
      "loss": 0.3398,
      "step": 574
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.056909650564193726,
      "learning_rate": 0.00017406143344709897,
      "loss": 0.3573,
      "step": 575
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.07627102732658386,
      "learning_rate": 0.00017401592718998863,
      "loss": 0.3148,
      "step": 576
    },
    {
      "epoch": 0.13113636363636363,
      "grad_norm": 0.06365431100130081,
      "learning_rate": 0.00017397042093287828,
      "loss": 0.2844,
      "step": 577
    },
    {
      "epoch": 0.13136363636363638,
      "grad_norm": 0.08596453815698624,
      "learning_rate": 0.00017392491467576793,
      "loss": 0.4272,
      "step": 578
    },
    {
      "epoch": 0.1315909090909091,
      "grad_norm": 0.046767156571149826,
      "learning_rate": 0.00017387940841865758,
      "loss": 0.3174,
      "step": 579
    },
    {
      "epoch": 0.1318181818181818,
      "grad_norm": 0.0861961841583252,
      "learning_rate": 0.00017383390216154724,
      "loss": 0.4094,
      "step": 580
    },
    {
      "epoch": 0.13204545454545455,
      "grad_norm": 0.06277924031019211,
      "learning_rate": 0.00017378839590443686,
      "loss": 0.3553,
      "step": 581
    },
    {
      "epoch": 0.13227272727272726,
      "grad_norm": 0.056563932448625565,
      "learning_rate": 0.00017374288964732652,
      "loss": 0.3232,
      "step": 582
    },
    {
      "epoch": 0.1325,
      "grad_norm": 0.05662654712796211,
      "learning_rate": 0.00017369738339021617,
      "loss": 0.3206,
      "step": 583
    },
    {
      "epoch": 0.13272727272727272,
      "grad_norm": 0.06364743411540985,
      "learning_rate": 0.0001736518771331058,
      "loss": 0.3632,
      "step": 584
    },
    {
      "epoch": 0.13295454545454546,
      "grad_norm": 0.07031646370887756,
      "learning_rate": 0.00017360637087599545,
      "loss": 0.4141,
      "step": 585
    },
    {
      "epoch": 0.13318181818181818,
      "grad_norm": 0.05460287630558014,
      "learning_rate": 0.0001735608646188851,
      "loss": 0.3616,
      "step": 586
    },
    {
      "epoch": 0.13340909090909092,
      "grad_norm": 0.053226374089717865,
      "learning_rate": 0.00017351535836177475,
      "loss": 0.3468,
      "step": 587
    },
    {
      "epoch": 0.13363636363636364,
      "grad_norm": 0.07543289661407471,
      "learning_rate": 0.0001734698521046644,
      "loss": 0.3386,
      "step": 588
    },
    {
      "epoch": 0.13386363636363635,
      "grad_norm": 0.06184103339910507,
      "learning_rate": 0.00017342434584755406,
      "loss": 0.3848,
      "step": 589
    },
    {
      "epoch": 0.1340909090909091,
      "grad_norm": 0.0436197929084301,
      "learning_rate": 0.0001733788395904437,
      "loss": 0.2887,
      "step": 590
    },
    {
      "epoch": 0.1343181818181818,
      "grad_norm": 0.07337699085474014,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4001,
      "step": 591
    },
    {
      "epoch": 0.13454545454545455,
      "grad_norm": 0.055682893842458725,
      "learning_rate": 0.000173287827076223,
      "loss": 0.3664,
      "step": 592
    },
    {
      "epoch": 0.13477272727272727,
      "grad_norm": 0.05953022465109825,
      "learning_rate": 0.00017324232081911264,
      "loss": 0.3645,
      "step": 593
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.04237101599574089,
      "learning_rate": 0.00017319681456200227,
      "loss": 0.3118,
      "step": 594
    },
    {
      "epoch": 0.13522727272727272,
      "grad_norm": 0.06847155839204788,
      "learning_rate": 0.00017315130830489192,
      "loss": 0.2903,
      "step": 595
    },
    {
      "epoch": 0.13545454545454547,
      "grad_norm": 0.08457653969526291,
      "learning_rate": 0.00017310580204778157,
      "loss": 0.4473,
      "step": 596
    },
    {
      "epoch": 0.13568181818181818,
      "grad_norm": 0.06464230269193649,
      "learning_rate": 0.00017306029579067123,
      "loss": 0.3941,
      "step": 597
    },
    {
      "epoch": 0.1359090909090909,
      "grad_norm": 0.06191185861825943,
      "learning_rate": 0.00017301478953356088,
      "loss": 0.3994,
      "step": 598
    },
    {
      "epoch": 0.13613636363636364,
      "grad_norm": 0.059096671640872955,
      "learning_rate": 0.00017296928327645053,
      "loss": 0.3428,
      "step": 599
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.06673092395067215,
      "learning_rate": 0.00017292377701934018,
      "loss": 0.383,
      "step": 600
    },
    {
      "epoch": 0.13636363636363635,
      "eval_loss": 0.33472996950149536,
      "eval_runtime": 229.3214,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 600
    },
    {
      "epoch": 0.1365909090909091,
      "grad_norm": 0.04738491401076317,
      "learning_rate": 0.0001728782707622298,
      "loss": 0.3126,
      "step": 601
    },
    {
      "epoch": 0.1368181818181818,
      "grad_norm": 0.04566465690732002,
      "learning_rate": 0.00017283276450511946,
      "loss": 0.3118,
      "step": 602
    },
    {
      "epoch": 0.13704545454545455,
      "grad_norm": 0.051928404718637466,
      "learning_rate": 0.0001727872582480091,
      "loss": 0.3256,
      "step": 603
    },
    {
      "epoch": 0.13727272727272727,
      "grad_norm": 0.05326681584119797,
      "learning_rate": 0.00017274175199089874,
      "loss": 0.3167,
      "step": 604
    },
    {
      "epoch": 0.1375,
      "grad_norm": 0.04972149804234505,
      "learning_rate": 0.0001726962457337884,
      "loss": 0.2909,
      "step": 605
    },
    {
      "epoch": 0.13772727272727273,
      "grad_norm": 0.05248430371284485,
      "learning_rate": 0.00017265073947667805,
      "loss": 0.3448,
      "step": 606
    },
    {
      "epoch": 0.13795454545454544,
      "grad_norm": 0.043776270002126694,
      "learning_rate": 0.0001726052332195677,
      "loss": 0.3143,
      "step": 607
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.05582286790013313,
      "learning_rate": 0.00017255972696245735,
      "loss": 0.3674,
      "step": 608
    },
    {
      "epoch": 0.1384090909090909,
      "grad_norm": 0.041494179517030716,
      "learning_rate": 0.000172514220705347,
      "loss": 0.3223,
      "step": 609
    },
    {
      "epoch": 0.13863636363636364,
      "grad_norm": 0.05239845812320709,
      "learning_rate": 0.00017246871444823666,
      "loss": 0.3327,
      "step": 610
    },
    {
      "epoch": 0.13886363636363636,
      "grad_norm": 0.04838741198182106,
      "learning_rate": 0.00017242320819112628,
      "loss": 0.3273,
      "step": 611
    },
    {
      "epoch": 0.1390909090909091,
      "grad_norm": 0.05662813410162926,
      "learning_rate": 0.00017237770193401594,
      "loss": 0.3158,
      "step": 612
    },
    {
      "epoch": 0.1393181818181818,
      "grad_norm": 0.051660630851984024,
      "learning_rate": 0.00017233219567690556,
      "loss": 0.3107,
      "step": 613
    },
    {
      "epoch": 0.13954545454545456,
      "grad_norm": 0.039754875004291534,
      "learning_rate": 0.00017228668941979522,
      "loss": 0.3017,
      "step": 614
    },
    {
      "epoch": 0.13977272727272727,
      "grad_norm": 0.05477316305041313,
      "learning_rate": 0.00017224118316268487,
      "loss": 0.324,
      "step": 615
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0537070669233799,
      "learning_rate": 0.00017219567690557452,
      "loss": 0.2926,
      "step": 616
    },
    {
      "epoch": 0.14022727272727273,
      "grad_norm": 0.04596412181854248,
      "learning_rate": 0.00017215017064846417,
      "loss": 0.3008,
      "step": 617
    },
    {
      "epoch": 0.14045454545454544,
      "grad_norm": 0.06524081528186798,
      "learning_rate": 0.00017210466439135383,
      "loss": 0.3454,
      "step": 618
    },
    {
      "epoch": 0.14068181818181819,
      "grad_norm": 0.05915672704577446,
      "learning_rate": 0.00017205915813424348,
      "loss": 0.3382,
      "step": 619
    },
    {
      "epoch": 0.1409090909090909,
      "grad_norm": 0.05417058616876602,
      "learning_rate": 0.00017201365187713313,
      "loss": 0.3264,
      "step": 620
    },
    {
      "epoch": 0.14113636363636364,
      "grad_norm": 0.04184246063232422,
      "learning_rate": 0.00017196814562002276,
      "loss": 0.3058,
      "step": 621
    },
    {
      "epoch": 0.14136363636363636,
      "grad_norm": 0.05668943375349045,
      "learning_rate": 0.0001719226393629124,
      "loss": 0.377,
      "step": 622
    },
    {
      "epoch": 0.1415909090909091,
      "grad_norm": 0.05874263867735863,
      "learning_rate": 0.00017187713310580204,
      "loss": 0.3587,
      "step": 623
    },
    {
      "epoch": 0.14181818181818182,
      "grad_norm": 0.044492170214653015,
      "learning_rate": 0.0001718316268486917,
      "loss": 0.3087,
      "step": 624
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 0.04513342306017876,
      "learning_rate": 0.00017178612059158134,
      "loss": 0.2761,
      "step": 625
    },
    {
      "epoch": 0.14227272727272727,
      "grad_norm": 0.05415678024291992,
      "learning_rate": 0.000171740614334471,
      "loss": 0.3367,
      "step": 626
    },
    {
      "epoch": 0.1425,
      "grad_norm": 0.07208278775215149,
      "learning_rate": 0.00017169510807736065,
      "loss": 0.3972,
      "step": 627
    },
    {
      "epoch": 0.14272727272727273,
      "grad_norm": 0.059340689331293106,
      "learning_rate": 0.0001716496018202503,
      "loss": 0.3251,
      "step": 628
    },
    {
      "epoch": 0.14295454545454545,
      "grad_norm": 0.04605596885085106,
      "learning_rate": 0.00017160409556313995,
      "loss": 0.3091,
      "step": 629
    },
    {
      "epoch": 0.1431818181818182,
      "grad_norm": 0.05487842485308647,
      "learning_rate": 0.0001715585893060296,
      "loss": 0.3255,
      "step": 630
    },
    {
      "epoch": 0.1434090909090909,
      "grad_norm": 0.05609817057847977,
      "learning_rate": 0.00017151308304891923,
      "loss": 0.4043,
      "step": 631
    },
    {
      "epoch": 0.14363636363636365,
      "grad_norm": 0.05741423740983009,
      "learning_rate": 0.00017146757679180888,
      "loss": 0.3622,
      "step": 632
    },
    {
      "epoch": 0.14386363636363636,
      "grad_norm": 0.05011988803744316,
      "learning_rate": 0.0001714220705346985,
      "loss": 0.319,
      "step": 633
    },
    {
      "epoch": 0.1440909090909091,
      "grad_norm": 0.053392939269542694,
      "learning_rate": 0.00017137656427758816,
      "loss": 0.3591,
      "step": 634
    },
    {
      "epoch": 0.14431818181818182,
      "grad_norm": 0.05203476920723915,
      "learning_rate": 0.00017133105802047782,
      "loss": 0.3128,
      "step": 635
    },
    {
      "epoch": 0.14454545454545453,
      "grad_norm": 0.05436680093407631,
      "learning_rate": 0.00017128555176336747,
      "loss": 0.3365,
      "step": 636
    },
    {
      "epoch": 0.14477272727272728,
      "grad_norm": 0.0510670505464077,
      "learning_rate": 0.00017124004550625712,
      "loss": 0.3149,
      "step": 637
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05075068771839142,
      "learning_rate": 0.00017119453924914677,
      "loss": 0.3262,
      "step": 638
    },
    {
      "epoch": 0.14522727272727273,
      "grad_norm": 0.04838799312710762,
      "learning_rate": 0.00017114903299203643,
      "loss": 0.3229,
      "step": 639
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.04740465059876442,
      "learning_rate": 0.00017110352673492608,
      "loss": 0.2605,
      "step": 640
    },
    {
      "epoch": 0.1456818181818182,
      "grad_norm": 0.04061247780919075,
      "learning_rate": 0.0001710580204778157,
      "loss": 0.315,
      "step": 641
    },
    {
      "epoch": 0.1459090909090909,
      "grad_norm": 0.050047993659973145,
      "learning_rate": 0.00017101251422070533,
      "loss": 0.3045,
      "step": 642
    },
    {
      "epoch": 0.14613636363636365,
      "grad_norm": 0.060488082468509674,
      "learning_rate": 0.00017096700796359498,
      "loss": 0.3507,
      "step": 643
    },
    {
      "epoch": 0.14636363636363636,
      "grad_norm": 0.059354495257139206,
      "learning_rate": 0.00017092150170648464,
      "loss": 0.3322,
      "step": 644
    },
    {
      "epoch": 0.14659090909090908,
      "grad_norm": 0.06399247795343399,
      "learning_rate": 0.0001708759954493743,
      "loss": 0.366,
      "step": 645
    },
    {
      "epoch": 0.14681818181818182,
      "grad_norm": 0.06558898836374283,
      "learning_rate": 0.00017083048919226394,
      "loss": 0.376,
      "step": 646
    },
    {
      "epoch": 0.14704545454545453,
      "grad_norm": 0.050755683332681656,
      "learning_rate": 0.0001707849829351536,
      "loss": 0.3083,
      "step": 647
    },
    {
      "epoch": 0.14727272727272728,
      "grad_norm": 0.05352776125073433,
      "learning_rate": 0.00017073947667804325,
      "loss": 0.2962,
      "step": 648
    },
    {
      "epoch": 0.1475,
      "grad_norm": 0.05026084557175636,
      "learning_rate": 0.0001706939704209329,
      "loss": 0.316,
      "step": 649
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.0688771903514862,
      "learning_rate": 0.00017064846416382255,
      "loss": 0.354,
      "step": 650
    },
    {
      "epoch": 0.14795454545454545,
      "grad_norm": 0.0613066665828228,
      "learning_rate": 0.00017060295790671218,
      "loss": 0.3669,
      "step": 651
    },
    {
      "epoch": 0.1481818181818182,
      "grad_norm": 0.07777025550603867,
      "learning_rate": 0.0001705574516496018,
      "loss": 0.3515,
      "step": 652
    },
    {
      "epoch": 0.1484090909090909,
      "grad_norm": 0.06700615584850311,
      "learning_rate": 0.00017051194539249146,
      "loss": 0.4031,
      "step": 653
    },
    {
      "epoch": 0.14863636363636365,
      "grad_norm": 0.06569758802652359,
      "learning_rate": 0.0001704664391353811,
      "loss": 0.3067,
      "step": 654
    },
    {
      "epoch": 0.14886363636363636,
      "grad_norm": 0.05566924810409546,
      "learning_rate": 0.00017042093287827076,
      "loss": 0.3737,
      "step": 655
    },
    {
      "epoch": 0.14909090909090908,
      "grad_norm": 0.05358699709177017,
      "learning_rate": 0.00017037542662116042,
      "loss": 0.3601,
      "step": 656
    },
    {
      "epoch": 0.14931818181818182,
      "grad_norm": 0.05153760686516762,
      "learning_rate": 0.00017032992036405007,
      "loss": 0.2773,
      "step": 657
    },
    {
      "epoch": 0.14954545454545454,
      "grad_norm": 0.05710677430033684,
      "learning_rate": 0.00017028441410693972,
      "loss": 0.3684,
      "step": 658
    },
    {
      "epoch": 0.14977272727272728,
      "grad_norm": 0.051318343728780746,
      "learning_rate": 0.00017023890784982938,
      "loss": 0.3001,
      "step": 659
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.04830048978328705,
      "learning_rate": 0.00017019340159271903,
      "loss": 0.3223,
      "step": 660
    },
    {
      "epoch": 0.15022727272727274,
      "grad_norm": 0.057458195835351944,
      "learning_rate": 0.00017014789533560865,
      "loss": 0.3138,
      "step": 661
    },
    {
      "epoch": 0.15045454545454545,
      "grad_norm": 0.04582609236240387,
      "learning_rate": 0.00017010238907849828,
      "loss": 0.3197,
      "step": 662
    },
    {
      "epoch": 0.1506818181818182,
      "grad_norm": 0.05384613201022148,
      "learning_rate": 0.00017005688282138793,
      "loss": 0.3667,
      "step": 663
    },
    {
      "epoch": 0.1509090909090909,
      "grad_norm": 0.05001474916934967,
      "learning_rate": 0.00017001137656427759,
      "loss": 0.3125,
      "step": 664
    },
    {
      "epoch": 0.15113636363636362,
      "grad_norm": 0.06549757719039917,
      "learning_rate": 0.00016996587030716724,
      "loss": 0.3495,
      "step": 665
    },
    {
      "epoch": 0.15136363636363637,
      "grad_norm": 0.06820278614759445,
      "learning_rate": 0.0001699203640500569,
      "loss": 0.3856,
      "step": 666
    },
    {
      "epoch": 0.15159090909090908,
      "grad_norm": 0.050752971321344376,
      "learning_rate": 0.00016987485779294654,
      "loss": 0.3441,
      "step": 667
    },
    {
      "epoch": 0.15181818181818182,
      "grad_norm": 0.04665445163846016,
      "learning_rate": 0.0001698293515358362,
      "loss": 0.3301,
      "step": 668
    },
    {
      "epoch": 0.15204545454545454,
      "grad_norm": 0.059678394347429276,
      "learning_rate": 0.00016978384527872585,
      "loss": 0.3594,
      "step": 669
    },
    {
      "epoch": 0.15227272727272728,
      "grad_norm": 0.06929796189069748,
      "learning_rate": 0.0001697383390216155,
      "loss": 0.3721,
      "step": 670
    },
    {
      "epoch": 0.1525,
      "grad_norm": 0.04546508193016052,
      "learning_rate": 0.00016969283276450513,
      "loss": 0.3084,
      "step": 671
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.05654168128967285,
      "learning_rate": 0.00016964732650739475,
      "loss": 0.321,
      "step": 672
    },
    {
      "epoch": 0.15295454545454545,
      "grad_norm": 0.04854639247059822,
      "learning_rate": 0.0001696018202502844,
      "loss": 0.3276,
      "step": 673
    },
    {
      "epoch": 0.15318181818181817,
      "grad_norm": 0.04841850325465202,
      "learning_rate": 0.00016955631399317406,
      "loss": 0.2884,
      "step": 674
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.06682108342647552,
      "learning_rate": 0.0001695108077360637,
      "loss": 0.4011,
      "step": 675
    },
    {
      "epoch": 0.15363636363636363,
      "grad_norm": 0.049984194338321686,
      "learning_rate": 0.00016946530147895337,
      "loss": 0.3745,
      "step": 676
    },
    {
      "epoch": 0.15386363636363637,
      "grad_norm": 0.06764756888151169,
      "learning_rate": 0.00016941979522184302,
      "loss": 0.3894,
      "step": 677
    },
    {
      "epoch": 0.15409090909090908,
      "grad_norm": 0.0538211464881897,
      "learning_rate": 0.00016937428896473267,
      "loss": 0.3628,
      "step": 678
    },
    {
      "epoch": 0.15431818181818183,
      "grad_norm": 0.053610485047101974,
      "learning_rate": 0.00016932878270762232,
      "loss": 0.3121,
      "step": 679
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.05851612240076065,
      "learning_rate": 0.00016928327645051198,
      "loss": 0.299,
      "step": 680
    },
    {
      "epoch": 0.15477272727272728,
      "grad_norm": 0.0545702688395977,
      "learning_rate": 0.0001692377701934016,
      "loss": 0.3184,
      "step": 681
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.051561154425144196,
      "learning_rate": 0.00016919226393629123,
      "loss": 0.3321,
      "step": 682
    },
    {
      "epoch": 0.1552272727272727,
      "grad_norm": 0.05096243694424629,
      "learning_rate": 0.00016914675767918088,
      "loss": 0.2951,
      "step": 683
    },
    {
      "epoch": 0.15545454545454546,
      "grad_norm": 0.048616327345371246,
      "learning_rate": 0.00016910125142207053,
      "loss": 0.3409,
      "step": 684
    },
    {
      "epoch": 0.15568181818181817,
      "grad_norm": 0.06647318601608276,
      "learning_rate": 0.00016905574516496019,
      "loss": 0.364,
      "step": 685
    },
    {
      "epoch": 0.1559090909090909,
      "grad_norm": 0.0586124062538147,
      "learning_rate": 0.00016901023890784984,
      "loss": 0.3792,
      "step": 686
    },
    {
      "epoch": 0.15613636363636363,
      "grad_norm": 0.059999823570251465,
      "learning_rate": 0.0001689647326507395,
      "loss": 0.3475,
      "step": 687
    },
    {
      "epoch": 0.15636363636363637,
      "grad_norm": 0.05281908065080643,
      "learning_rate": 0.00016891922639362914,
      "loss": 0.35,
      "step": 688
    },
    {
      "epoch": 0.1565909090909091,
      "grad_norm": 0.0680927112698555,
      "learning_rate": 0.0001688737201365188,
      "loss": 0.381,
      "step": 689
    },
    {
      "epoch": 0.15681818181818183,
      "grad_norm": 0.06640768051147461,
      "learning_rate": 0.00016882821387940842,
      "loss": 0.3457,
      "step": 690
    },
    {
      "epoch": 0.15704545454545454,
      "grad_norm": 0.04555423557758331,
      "learning_rate": 0.00016878270762229808,
      "loss": 0.2749,
      "step": 691
    },
    {
      "epoch": 0.1572727272727273,
      "grad_norm": 0.04428906738758087,
      "learning_rate": 0.0001687372013651877,
      "loss": 0.2984,
      "step": 692
    },
    {
      "epoch": 0.1575,
      "grad_norm": 0.050630901008844376,
      "learning_rate": 0.00016869169510807735,
      "loss": 0.3528,
      "step": 693
    },
    {
      "epoch": 0.15772727272727272,
      "grad_norm": 0.04924161359667778,
      "learning_rate": 0.000168646188850967,
      "loss": 0.2982,
      "step": 694
    },
    {
      "epoch": 0.15795454545454546,
      "grad_norm": 0.05314767733216286,
      "learning_rate": 0.00016860068259385666,
      "loss": 0.3464,
      "step": 695
    },
    {
      "epoch": 0.15818181818181817,
      "grad_norm": 0.055611830204725266,
      "learning_rate": 0.0001685551763367463,
      "loss": 0.3804,
      "step": 696
    },
    {
      "epoch": 0.15840909090909092,
      "grad_norm": 0.055942460894584656,
      "learning_rate": 0.00016850967007963597,
      "loss": 0.3539,
      "step": 697
    },
    {
      "epoch": 0.15863636363636363,
      "grad_norm": 0.047500692307949066,
      "learning_rate": 0.00016846416382252562,
      "loss": 0.3576,
      "step": 698
    },
    {
      "epoch": 0.15886363636363637,
      "grad_norm": 0.0696183443069458,
      "learning_rate": 0.00016841865756541527,
      "loss": 0.342,
      "step": 699
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.0619642436504364,
      "learning_rate": 0.0001683731513083049,
      "loss": 0.3671,
      "step": 700
    },
    {
      "epoch": 0.1590909090909091,
      "eval_loss": 0.3337444067001343,
      "eval_runtime": 229.4139,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 700
    },
    {
      "epoch": 0.15931818181818183,
      "grad_norm": 0.05031069740653038,
      "learning_rate": 0.00016832764505119455,
      "loss": 0.3438,
      "step": 701
    },
    {
      "epoch": 0.15954545454545455,
      "grad_norm": 0.05003191530704498,
      "learning_rate": 0.00016828213879408418,
      "loss": 0.3313,
      "step": 702
    },
    {
      "epoch": 0.15977272727272726,
      "grad_norm": 0.04869591072201729,
      "learning_rate": 0.00016823663253697383,
      "loss": 0.3415,
      "step": 703
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.04381396621465683,
      "learning_rate": 0.00016819112627986348,
      "loss": 0.3011,
      "step": 704
    },
    {
      "epoch": 0.16022727272727272,
      "grad_norm": 0.04753773659467697,
      "learning_rate": 0.00016814562002275313,
      "loss": 0.3789,
      "step": 705
    },
    {
      "epoch": 0.16045454545454546,
      "grad_norm": 0.05539868399500847,
      "learning_rate": 0.0001681001137656428,
      "loss": 0.3592,
      "step": 706
    },
    {
      "epoch": 0.16068181818181818,
      "grad_norm": 0.03965640068054199,
      "learning_rate": 0.00016805460750853244,
      "loss": 0.2958,
      "step": 707
    },
    {
      "epoch": 0.16090909090909092,
      "grad_norm": 0.04675867035984993,
      "learning_rate": 0.0001680091012514221,
      "loss": 0.3033,
      "step": 708
    },
    {
      "epoch": 0.16113636363636363,
      "grad_norm": 0.06440422683954239,
      "learning_rate": 0.00016796359499431175,
      "loss": 0.3948,
      "step": 709
    },
    {
      "epoch": 0.16136363636363638,
      "grad_norm": 0.0569709911942482,
      "learning_rate": 0.00016791808873720137,
      "loss": 0.432,
      "step": 710
    },
    {
      "epoch": 0.1615909090909091,
      "grad_norm": 0.05606742575764656,
      "learning_rate": 0.00016787258248009102,
      "loss": 0.3625,
      "step": 711
    },
    {
      "epoch": 0.1618181818181818,
      "grad_norm": 0.05952315405011177,
      "learning_rate": 0.00016782707622298065,
      "loss": 0.3354,
      "step": 712
    },
    {
      "epoch": 0.16204545454545455,
      "grad_norm": 0.04997987300157547,
      "learning_rate": 0.0001677815699658703,
      "loss": 0.3203,
      "step": 713
    },
    {
      "epoch": 0.16227272727272726,
      "grad_norm": 0.04627012461423874,
      "learning_rate": 0.00016773606370875996,
      "loss": 0.3263,
      "step": 714
    },
    {
      "epoch": 0.1625,
      "grad_norm": 0.05343112349510193,
      "learning_rate": 0.0001676905574516496,
      "loss": 0.328,
      "step": 715
    },
    {
      "epoch": 0.16272727272727272,
      "grad_norm": 0.043251268565654755,
      "learning_rate": 0.00016764505119453926,
      "loss": 0.2897,
      "step": 716
    },
    {
      "epoch": 0.16295454545454546,
      "grad_norm": 0.05253908783197403,
      "learning_rate": 0.00016759954493742891,
      "loss": 0.313,
      "step": 717
    },
    {
      "epoch": 0.16318181818181818,
      "grad_norm": 0.05924048274755478,
      "learning_rate": 0.00016755403868031857,
      "loss": 0.3547,
      "step": 718
    },
    {
      "epoch": 0.16340909090909092,
      "grad_norm": 0.05599510297179222,
      "learning_rate": 0.00016750853242320822,
      "loss": 0.3558,
      "step": 719
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.05936156213283539,
      "learning_rate": 0.00016746302616609785,
      "loss": 0.387,
      "step": 720
    },
    {
      "epoch": 0.16386363636363635,
      "grad_norm": 0.0686301738023758,
      "learning_rate": 0.0001674175199089875,
      "loss": 0.4013,
      "step": 721
    },
    {
      "epoch": 0.1640909090909091,
      "grad_norm": 0.0538618303835392,
      "learning_rate": 0.00016737201365187712,
      "loss": 0.3191,
      "step": 722
    },
    {
      "epoch": 0.1643181818181818,
      "grad_norm": 0.046235013753175735,
      "learning_rate": 0.00016732650739476678,
      "loss": 0.2976,
      "step": 723
    },
    {
      "epoch": 0.16454545454545455,
      "grad_norm": 0.06156256049871445,
      "learning_rate": 0.00016728100113765643,
      "loss": 0.3026,
      "step": 724
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.07434730976819992,
      "learning_rate": 0.00016723549488054608,
      "loss": 0.4191,
      "step": 725
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.06272217631340027,
      "learning_rate": 0.00016718998862343573,
      "loss": 0.3818,
      "step": 726
    },
    {
      "epoch": 0.16522727272727272,
      "grad_norm": 0.05213889107108116,
      "learning_rate": 0.0001671444823663254,
      "loss": 0.3242,
      "step": 727
    },
    {
      "epoch": 0.16545454545454547,
      "grad_norm": 0.05072067677974701,
      "learning_rate": 0.00016709897610921504,
      "loss": 0.2874,
      "step": 728
    },
    {
      "epoch": 0.16568181818181818,
      "grad_norm": 0.0599062405526638,
      "learning_rate": 0.0001670534698521047,
      "loss": 0.366,
      "step": 729
    },
    {
      "epoch": 0.16590909090909092,
      "grad_norm": 0.05920661240816116,
      "learning_rate": 0.00016700796359499432,
      "loss": 0.2855,
      "step": 730
    },
    {
      "epoch": 0.16613636363636364,
      "grad_norm": 0.06645669788122177,
      "learning_rate": 0.00016696245733788397,
      "loss": 0.4039,
      "step": 731
    },
    {
      "epoch": 0.16636363636363635,
      "grad_norm": 0.0464085191488266,
      "learning_rate": 0.0001669169510807736,
      "loss": 0.2523,
      "step": 732
    },
    {
      "epoch": 0.1665909090909091,
      "grad_norm": 0.0577140748500824,
      "learning_rate": 0.00016687144482366325,
      "loss": 0.3501,
      "step": 733
    },
    {
      "epoch": 0.1668181818181818,
      "grad_norm": 0.052513621747493744,
      "learning_rate": 0.0001668259385665529,
      "loss": 0.3184,
      "step": 734
    },
    {
      "epoch": 0.16704545454545455,
      "grad_norm": 0.06191881373524666,
      "learning_rate": 0.00016678043230944256,
      "loss": 0.3724,
      "step": 735
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.08234657347202301,
      "learning_rate": 0.0001667349260523322,
      "loss": 0.3967,
      "step": 736
    },
    {
      "epoch": 0.1675,
      "grad_norm": 0.0535292886197567,
      "learning_rate": 0.00016668941979522186,
      "loss": 0.3454,
      "step": 737
    },
    {
      "epoch": 0.16772727272727272,
      "grad_norm": 0.0392642505466938,
      "learning_rate": 0.00016664391353811151,
      "loss": 0.2749,
      "step": 738
    },
    {
      "epoch": 0.16795454545454547,
      "grad_norm": 0.04829739034175873,
      "learning_rate": 0.00016659840728100117,
      "loss": 0.2909,
      "step": 739
    },
    {
      "epoch": 0.16818181818181818,
      "grad_norm": 0.053749166429042816,
      "learning_rate": 0.0001665529010238908,
      "loss": 0.3169,
      "step": 740
    },
    {
      "epoch": 0.1684090909090909,
      "grad_norm": 0.0548819862306118,
      "learning_rate": 0.00016650739476678045,
      "loss": 0.2933,
      "step": 741
    },
    {
      "epoch": 0.16863636363636364,
      "grad_norm": 0.06855711340904236,
      "learning_rate": 0.00016646188850967007,
      "loss": 0.414,
      "step": 742
    },
    {
      "epoch": 0.16886363636363635,
      "grad_norm": 0.049845028668642044,
      "learning_rate": 0.00016641638225255972,
      "loss": 0.304,
      "step": 743
    },
    {
      "epoch": 0.1690909090909091,
      "grad_norm": 0.05847739800810814,
      "learning_rate": 0.00016637087599544938,
      "loss": 0.3407,
      "step": 744
    },
    {
      "epoch": 0.1693181818181818,
      "grad_norm": 0.050523292273283005,
      "learning_rate": 0.00016632536973833903,
      "loss": 0.3272,
      "step": 745
    },
    {
      "epoch": 0.16954545454545455,
      "grad_norm": 0.044471655040979385,
      "learning_rate": 0.00016627986348122868,
      "loss": 0.2879,
      "step": 746
    },
    {
      "epoch": 0.16977272727272727,
      "grad_norm": 0.06936908513307571,
      "learning_rate": 0.00016623435722411834,
      "loss": 0.3644,
      "step": 747
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.07089009135961533,
      "learning_rate": 0.000166188850967008,
      "loss": 0.335,
      "step": 748
    },
    {
      "epoch": 0.17022727272727273,
      "grad_norm": 0.044378720223903656,
      "learning_rate": 0.00016614334470989761,
      "loss": 0.3333,
      "step": 749
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 0.061794936656951904,
      "learning_rate": 0.00016609783845278727,
      "loss": 0.3293,
      "step": 750
    },
    {
      "epoch": 0.17068181818181818,
      "grad_norm": 0.05797794833779335,
      "learning_rate": 0.00016605233219567692,
      "loss": 0.3549,
      "step": 751
    },
    {
      "epoch": 0.1709090909090909,
      "grad_norm": 0.04787864536046982,
      "learning_rate": 0.00016600682593856655,
      "loss": 0.359,
      "step": 752
    },
    {
      "epoch": 0.17113636363636364,
      "grad_norm": 0.06404341757297516,
      "learning_rate": 0.0001659613196814562,
      "loss": 0.3642,
      "step": 753
    },
    {
      "epoch": 0.17136363636363636,
      "grad_norm": 0.05190613493323326,
      "learning_rate": 0.00016591581342434585,
      "loss": 0.2942,
      "step": 754
    },
    {
      "epoch": 0.1715909090909091,
      "grad_norm": 0.0518302321434021,
      "learning_rate": 0.0001658703071672355,
      "loss": 0.3662,
      "step": 755
    },
    {
      "epoch": 0.17181818181818181,
      "grad_norm": 0.051222916692495346,
      "learning_rate": 0.00016582480091012516,
      "loss": 0.3337,
      "step": 756
    },
    {
      "epoch": 0.17204545454545456,
      "grad_norm": 0.06060122326016426,
      "learning_rate": 0.0001657792946530148,
      "loss": 0.3488,
      "step": 757
    },
    {
      "epoch": 0.17227272727272727,
      "grad_norm": 0.055247046053409576,
      "learning_rate": 0.00016573378839590446,
      "loss": 0.357,
      "step": 758
    },
    {
      "epoch": 0.1725,
      "grad_norm": 0.05634928494691849,
      "learning_rate": 0.0001656882821387941,
      "loss": 0.3452,
      "step": 759
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 0.03799140453338623,
      "learning_rate": 0.00016564277588168374,
      "loss": 0.2913,
      "step": 760
    },
    {
      "epoch": 0.17295454545454544,
      "grad_norm": 0.05777553841471672,
      "learning_rate": 0.0001655972696245734,
      "loss": 0.3538,
      "step": 761
    },
    {
      "epoch": 0.1731818181818182,
      "grad_norm": 0.045753225684165955,
      "learning_rate": 0.00016555176336746302,
      "loss": 0.3536,
      "step": 762
    },
    {
      "epoch": 0.1734090909090909,
      "grad_norm": 0.056784797459840775,
      "learning_rate": 0.00016550625711035267,
      "loss": 0.3249,
      "step": 763
    },
    {
      "epoch": 0.17363636363636364,
      "grad_norm": 0.049416497349739075,
      "learning_rate": 0.00016546075085324233,
      "loss": 0.3147,
      "step": 764
    },
    {
      "epoch": 0.17386363636363636,
      "grad_norm": 0.044397998601198196,
      "learning_rate": 0.00016541524459613198,
      "loss": 0.2949,
      "step": 765
    },
    {
      "epoch": 0.1740909090909091,
      "grad_norm": 0.044296279549598694,
      "learning_rate": 0.00016536973833902163,
      "loss": 0.3205,
      "step": 766
    },
    {
      "epoch": 0.17431818181818182,
      "grad_norm": 0.05732559412717819,
      "learning_rate": 0.00016532423208191128,
      "loss": 0.3049,
      "step": 767
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.0542488731443882,
      "learning_rate": 0.00016527872582480094,
      "loss": 0.3203,
      "step": 768
    },
    {
      "epoch": 0.17477272727272727,
      "grad_norm": 0.05080803111195564,
      "learning_rate": 0.00016523321956769056,
      "loss": 0.2933,
      "step": 769
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.056925490498542786,
      "learning_rate": 0.00016518771331058022,
      "loss": 0.3698,
      "step": 770
    },
    {
      "epoch": 0.17522727272727273,
      "grad_norm": 0.04548017308115959,
      "learning_rate": 0.00016514220705346987,
      "loss": 0.3039,
      "step": 771
    },
    {
      "epoch": 0.17545454545454545,
      "grad_norm": 0.05020653456449509,
      "learning_rate": 0.0001650967007963595,
      "loss": 0.3439,
      "step": 772
    },
    {
      "epoch": 0.1756818181818182,
      "grad_norm": 0.052820853888988495,
      "learning_rate": 0.00016505119453924915,
      "loss": 0.3589,
      "step": 773
    },
    {
      "epoch": 0.1759090909090909,
      "grad_norm": 0.05321275070309639,
      "learning_rate": 0.0001650056882821388,
      "loss": 0.3528,
      "step": 774
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.06271153688430786,
      "learning_rate": 0.00016496018202502845,
      "loss": 0.2751,
      "step": 775
    },
    {
      "epoch": 0.17636363636363636,
      "grad_norm": 0.052587978541851044,
      "learning_rate": 0.0001649146757679181,
      "loss": 0.3725,
      "step": 776
    },
    {
      "epoch": 0.1765909090909091,
      "grad_norm": 0.05814914032816887,
      "learning_rate": 0.00016486916951080776,
      "loss": 0.2956,
      "step": 777
    },
    {
      "epoch": 0.17681818181818182,
      "grad_norm": 0.05365045741200447,
      "learning_rate": 0.0001648236632536974,
      "loss": 0.3178,
      "step": 778
    },
    {
      "epoch": 0.17704545454545453,
      "grad_norm": 0.0575525164604187,
      "learning_rate": 0.00016477815699658704,
      "loss": 0.3444,
      "step": 779
    },
    {
      "epoch": 0.17727272727272728,
      "grad_norm": 0.039610862731933594,
      "learning_rate": 0.0001647326507394767,
      "loss": 0.2817,
      "step": 780
    },
    {
      "epoch": 0.1775,
      "grad_norm": 0.06255912035703659,
      "learning_rate": 0.00016468714448236634,
      "loss": 0.3512,
      "step": 781
    },
    {
      "epoch": 0.17772727272727273,
      "grad_norm": 0.04771072790026665,
      "learning_rate": 0.00016464163822525597,
      "loss": 0.3352,
      "step": 782
    },
    {
      "epoch": 0.17795454545454545,
      "grad_norm": 0.05879053473472595,
      "learning_rate": 0.00016459613196814562,
      "loss": 0.3376,
      "step": 783
    },
    {
      "epoch": 0.1781818181818182,
      "grad_norm": 0.05386488884687424,
      "learning_rate": 0.00016455062571103527,
      "loss": 0.3895,
      "step": 784
    },
    {
      "epoch": 0.1784090909090909,
      "grad_norm": 0.043497733771800995,
      "learning_rate": 0.00016450511945392493,
      "loss": 0.2669,
      "step": 785
    },
    {
      "epoch": 0.17863636363636365,
      "grad_norm": 0.045100755989551544,
      "learning_rate": 0.00016445961319681458,
      "loss": 0.2692,
      "step": 786
    },
    {
      "epoch": 0.17886363636363636,
      "grad_norm": 0.06767617166042328,
      "learning_rate": 0.00016441410693970423,
      "loss": 0.2858,
      "step": 787
    },
    {
      "epoch": 0.17909090909090908,
      "grad_norm": 0.06791232526302338,
      "learning_rate": 0.00016436860068259386,
      "loss": 0.3328,
      "step": 788
    },
    {
      "epoch": 0.17931818181818182,
      "grad_norm": 0.05492717772722244,
      "learning_rate": 0.0001643230944254835,
      "loss": 0.3511,
      "step": 789
    },
    {
      "epoch": 0.17954545454545454,
      "grad_norm": 0.055285703390836716,
      "learning_rate": 0.00016427758816837316,
      "loss": 0.3464,
      "step": 790
    },
    {
      "epoch": 0.17977272727272728,
      "grad_norm": 0.055951785296201706,
      "learning_rate": 0.00016423208191126282,
      "loss": 0.2829,
      "step": 791
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.05712466686964035,
      "learning_rate": 0.00016418657565415244,
      "loss": 0.3373,
      "step": 792
    },
    {
      "epoch": 0.18022727272727274,
      "grad_norm": 0.07806084305047989,
      "learning_rate": 0.0001641410693970421,
      "loss": 0.371,
      "step": 793
    },
    {
      "epoch": 0.18045454545454545,
      "grad_norm": 0.03392500430345535,
      "learning_rate": 0.00016409556313993175,
      "loss": 0.2509,
      "step": 794
    },
    {
      "epoch": 0.1806818181818182,
      "grad_norm": 0.06580079346895218,
      "learning_rate": 0.0001640500568828214,
      "loss": 0.4037,
      "step": 795
    },
    {
      "epoch": 0.1809090909090909,
      "grad_norm": 0.05216424539685249,
      "learning_rate": 0.00016400455062571105,
      "loss": 0.3362,
      "step": 796
    },
    {
      "epoch": 0.18113636363636362,
      "grad_norm": 0.06282731890678406,
      "learning_rate": 0.0001639590443686007,
      "loss": 0.3286,
      "step": 797
    },
    {
      "epoch": 0.18136363636363637,
      "grad_norm": 0.0634308010339737,
      "learning_rate": 0.00016391353811149033,
      "loss": 0.324,
      "step": 798
    },
    {
      "epoch": 0.18159090909090908,
      "grad_norm": 0.05070288106799126,
      "learning_rate": 0.00016386803185437998,
      "loss": 0.353,
      "step": 799
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.03863520547747612,
      "learning_rate": 0.00016382252559726964,
      "loss": 0.3048,
      "step": 800
    },
    {
      "epoch": 0.18181818181818182,
      "eval_loss": 0.3330596685409546,
      "eval_runtime": 229.3731,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 800
    },
    {
      "epoch": 0.18204545454545454,
      "grad_norm": 0.05932573974132538,
      "learning_rate": 0.0001637770193401593,
      "loss": 0.3786,
      "step": 801
    },
    {
      "epoch": 0.18227272727272728,
      "grad_norm": 0.04240293800830841,
      "learning_rate": 0.00016373151308304892,
      "loss": 0.3296,
      "step": 802
    },
    {
      "epoch": 0.1825,
      "grad_norm": 0.048736512660980225,
      "learning_rate": 0.00016368600682593857,
      "loss": 0.271,
      "step": 803
    },
    {
      "epoch": 0.18272727272727274,
      "grad_norm": 0.050076063722372055,
      "learning_rate": 0.00016364050056882822,
      "loss": 0.3599,
      "step": 804
    },
    {
      "epoch": 0.18295454545454545,
      "grad_norm": 0.04267081245779991,
      "learning_rate": 0.00016359499431171787,
      "loss": 0.2823,
      "step": 805
    },
    {
      "epoch": 0.1831818181818182,
      "grad_norm": 0.04671337828040123,
      "learning_rate": 0.00016354948805460753,
      "loss": 0.2925,
      "step": 806
    },
    {
      "epoch": 0.1834090909090909,
      "grad_norm": 0.054650336503982544,
      "learning_rate": 0.00016350398179749718,
      "loss": 0.3545,
      "step": 807
    },
    {
      "epoch": 0.18363636363636363,
      "grad_norm": 0.06265321373939514,
      "learning_rate": 0.0001634584755403868,
      "loss": 0.3172,
      "step": 808
    },
    {
      "epoch": 0.18386363636363637,
      "grad_norm": 0.07253297418355942,
      "learning_rate": 0.00016341296928327646,
      "loss": 0.4647,
      "step": 809
    },
    {
      "epoch": 0.18409090909090908,
      "grad_norm": 0.05101130157709122,
      "learning_rate": 0.0001633674630261661,
      "loss": 0.3247,
      "step": 810
    },
    {
      "epoch": 0.18431818181818183,
      "grad_norm": 0.061289235949516296,
      "learning_rate": 0.00016332195676905576,
      "loss": 0.3788,
      "step": 811
    },
    {
      "epoch": 0.18454545454545454,
      "grad_norm": 0.04423833638429642,
      "learning_rate": 0.0001632764505119454,
      "loss": 0.3033,
      "step": 812
    },
    {
      "epoch": 0.18477272727272728,
      "grad_norm": 0.05852806568145752,
      "learning_rate": 0.00016323094425483504,
      "loss": 0.3921,
      "step": 813
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.03541342914104462,
      "learning_rate": 0.0001631854379977247,
      "loss": 0.264,
      "step": 814
    },
    {
      "epoch": 0.18522727272727274,
      "grad_norm": 0.05769820138812065,
      "learning_rate": 0.00016313993174061435,
      "loss": 0.3935,
      "step": 815
    },
    {
      "epoch": 0.18545454545454546,
      "grad_norm": 0.04576795548200607,
      "learning_rate": 0.000163094425483504,
      "loss": 0.3392,
      "step": 816
    },
    {
      "epoch": 0.18568181818181817,
      "grad_norm": 0.04998869076371193,
      "learning_rate": 0.00016304891922639363,
      "loss": 0.2846,
      "step": 817
    },
    {
      "epoch": 0.1859090909090909,
      "grad_norm": 0.05961315706372261,
      "learning_rate": 0.00016300341296928328,
      "loss": 0.3326,
      "step": 818
    },
    {
      "epoch": 0.18613636363636363,
      "grad_norm": 0.0414855033159256,
      "learning_rate": 0.00016295790671217293,
      "loss": 0.2909,
      "step": 819
    },
    {
      "epoch": 0.18636363636363637,
      "grad_norm": 0.05042455717921257,
      "learning_rate": 0.00016291240045506259,
      "loss": 0.2716,
      "step": 820
    },
    {
      "epoch": 0.18659090909090909,
      "grad_norm": 0.05515671148896217,
      "learning_rate": 0.00016286689419795224,
      "loss": 0.3624,
      "step": 821
    },
    {
      "epoch": 0.18681818181818183,
      "grad_norm": 0.05346252769231796,
      "learning_rate": 0.00016282138794084186,
      "loss": 0.3581,
      "step": 822
    },
    {
      "epoch": 0.18704545454545454,
      "grad_norm": 0.055983591824769974,
      "learning_rate": 0.00016277588168373152,
      "loss": 0.3745,
      "step": 823
    },
    {
      "epoch": 0.18727272727272729,
      "grad_norm": 0.056287430226802826,
      "learning_rate": 0.00016273037542662117,
      "loss": 0.339,
      "step": 824
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.05359283462166786,
      "learning_rate": 0.00016268486916951082,
      "loss": 0.3515,
      "step": 825
    },
    {
      "epoch": 0.18772727272727271,
      "grad_norm": 0.047302473336458206,
      "learning_rate": 0.00016263936291240047,
      "loss": 0.3575,
      "step": 826
    },
    {
      "epoch": 0.18795454545454546,
      "grad_norm": 0.05793061852455139,
      "learning_rate": 0.0001625938566552901,
      "loss": 0.396,
      "step": 827
    },
    {
      "epoch": 0.18818181818181817,
      "grad_norm": 0.040294162929058075,
      "learning_rate": 0.00016254835039817975,
      "loss": 0.2525,
      "step": 828
    },
    {
      "epoch": 0.18840909090909091,
      "grad_norm": 0.05527367442846298,
      "learning_rate": 0.0001625028441410694,
      "loss": 0.3406,
      "step": 829
    },
    {
      "epoch": 0.18863636363636363,
      "grad_norm": 0.06367190182209015,
      "learning_rate": 0.00016245733788395906,
      "loss": 0.3831,
      "step": 830
    },
    {
      "epoch": 0.18886363636363637,
      "grad_norm": 0.046535998582839966,
      "learning_rate": 0.0001624118316268487,
      "loss": 0.3241,
      "step": 831
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.037906963378190994,
      "learning_rate": 0.00016236632536973834,
      "loss": 0.2881,
      "step": 832
    },
    {
      "epoch": 0.18931818181818183,
      "grad_norm": 0.06987036764621735,
      "learning_rate": 0.000162320819112628,
      "loss": 0.3367,
      "step": 833
    },
    {
      "epoch": 0.18954545454545454,
      "grad_norm": 0.053624797612428665,
      "learning_rate": 0.00016227531285551764,
      "loss": 0.3343,
      "step": 834
    },
    {
      "epoch": 0.18977272727272726,
      "grad_norm": 0.0473659448325634,
      "learning_rate": 0.0001622298065984073,
      "loss": 0.3096,
      "step": 835
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.055072031915187836,
      "learning_rate": 0.00016218430034129695,
      "loss": 0.3732,
      "step": 836
    },
    {
      "epoch": 0.19022727272727272,
      "grad_norm": 0.06420871615409851,
      "learning_rate": 0.00016213879408418657,
      "loss": 0.4004,
      "step": 837
    },
    {
      "epoch": 0.19045454545454546,
      "grad_norm": 0.046907320618629456,
      "learning_rate": 0.00016209328782707623,
      "loss": 0.2889,
      "step": 838
    },
    {
      "epoch": 0.19068181818181817,
      "grad_norm": 0.047694411128759384,
      "learning_rate": 0.00016204778156996588,
      "loss": 0.3501,
      "step": 839
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.04775398597121239,
      "learning_rate": 0.00016200227531285553,
      "loss": 0.2985,
      "step": 840
    },
    {
      "epoch": 0.19113636363636363,
      "grad_norm": 0.05445437878370285,
      "learning_rate": 0.00016195676905574516,
      "loss": 0.3329,
      "step": 841
    },
    {
      "epoch": 0.19136363636363637,
      "grad_norm": 0.04980569705367088,
      "learning_rate": 0.0001619112627986348,
      "loss": 0.2938,
      "step": 842
    },
    {
      "epoch": 0.1915909090909091,
      "grad_norm": 0.06329376995563507,
      "learning_rate": 0.00016186575654152446,
      "loss": 0.4207,
      "step": 843
    },
    {
      "epoch": 0.1918181818181818,
      "grad_norm": 0.047716788947582245,
      "learning_rate": 0.00016182025028441412,
      "loss": 0.2771,
      "step": 844
    },
    {
      "epoch": 0.19204545454545455,
      "grad_norm": 0.0415276475250721,
      "learning_rate": 0.00016177474402730377,
      "loss": 0.2752,
      "step": 845
    },
    {
      "epoch": 0.19227272727272726,
      "grad_norm": 0.04117187485098839,
      "learning_rate": 0.00016172923777019342,
      "loss": 0.3011,
      "step": 846
    },
    {
      "epoch": 0.1925,
      "grad_norm": 0.042383983731269836,
      "learning_rate": 0.00016168373151308305,
      "loss": 0.2893,
      "step": 847
    },
    {
      "epoch": 0.19272727272727272,
      "grad_norm": 0.07497818768024445,
      "learning_rate": 0.0001616382252559727,
      "loss": 0.3269,
      "step": 848
    },
    {
      "epoch": 0.19295454545454546,
      "grad_norm": 0.06295420974493027,
      "learning_rate": 0.00016159271899886235,
      "loss": 0.327,
      "step": 849
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.04964681342244148,
      "learning_rate": 0.000161547212741752,
      "loss": 0.3656,
      "step": 850
    },
    {
      "epoch": 0.19340909090909092,
      "grad_norm": 0.06107087805867195,
      "learning_rate": 0.00016150170648464163,
      "loss": 0.4104,
      "step": 851
    },
    {
      "epoch": 0.19363636363636363,
      "grad_norm": 0.04575595259666443,
      "learning_rate": 0.00016145620022753129,
      "loss": 0.3163,
      "step": 852
    },
    {
      "epoch": 0.19386363636363638,
      "grad_norm": 0.05172577500343323,
      "learning_rate": 0.00016141069397042094,
      "loss": 0.3735,
      "step": 853
    },
    {
      "epoch": 0.1940909090909091,
      "grad_norm": 0.035774871706962585,
      "learning_rate": 0.0001613651877133106,
      "loss": 0.271,
      "step": 854
    },
    {
      "epoch": 0.1943181818181818,
      "grad_norm": 0.04518185555934906,
      "learning_rate": 0.00016131968145620024,
      "loss": 0.281,
      "step": 855
    },
    {
      "epoch": 0.19454545454545455,
      "grad_norm": 0.049833718687295914,
      "learning_rate": 0.00016127417519908987,
      "loss": 0.2992,
      "step": 856
    },
    {
      "epoch": 0.19477272727272726,
      "grad_norm": 0.048714473843574524,
      "learning_rate": 0.00016122866894197952,
      "loss": 0.3227,
      "step": 857
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.053737085312604904,
      "learning_rate": 0.00016118316268486918,
      "loss": 0.2881,
      "step": 858
    },
    {
      "epoch": 0.19522727272727272,
      "grad_norm": 0.05495188757777214,
      "learning_rate": 0.00016113765642775883,
      "loss": 0.3426,
      "step": 859
    },
    {
      "epoch": 0.19545454545454546,
      "grad_norm": 0.04725024104118347,
      "learning_rate": 0.00016109215017064848,
      "loss": 0.3208,
      "step": 860
    },
    {
      "epoch": 0.19568181818181818,
      "grad_norm": 0.04938608780503273,
      "learning_rate": 0.0001610466439135381,
      "loss": 0.3178,
      "step": 861
    },
    {
      "epoch": 0.19590909090909092,
      "grad_norm": 0.05755525082349777,
      "learning_rate": 0.00016100113765642776,
      "loss": 0.3784,
      "step": 862
    },
    {
      "epoch": 0.19613636363636364,
      "grad_norm": 0.04885268211364746,
      "learning_rate": 0.0001609556313993174,
      "loss": 0.3894,
      "step": 863
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.04894084483385086,
      "learning_rate": 0.00016091012514220707,
      "loss": 0.3466,
      "step": 864
    },
    {
      "epoch": 0.1965909090909091,
      "grad_norm": 0.05836297199130058,
      "learning_rate": 0.00016086461888509672,
      "loss": 0.3742,
      "step": 865
    },
    {
      "epoch": 0.1968181818181818,
      "grad_norm": 0.05372440069913864,
      "learning_rate": 0.00016081911262798634,
      "loss": 0.329,
      "step": 866
    },
    {
      "epoch": 0.19704545454545455,
      "grad_norm": 0.04808910936117172,
      "learning_rate": 0.000160773606370876,
      "loss": 0.324,
      "step": 867
    },
    {
      "epoch": 0.19727272727272727,
      "grad_norm": 0.03632548823952675,
      "learning_rate": 0.00016072810011376565,
      "loss": 0.285,
      "step": 868
    },
    {
      "epoch": 0.1975,
      "grad_norm": 0.0744185522198677,
      "learning_rate": 0.0001606825938566553,
      "loss": 0.385,
      "step": 869
    },
    {
      "epoch": 0.19772727272727272,
      "grad_norm": 0.052928633987903595,
      "learning_rate": 0.00016063708759954495,
      "loss": 0.3388,
      "step": 870
    },
    {
      "epoch": 0.19795454545454547,
      "grad_norm": 0.06002967432141304,
      "learning_rate": 0.00016059158134243458,
      "loss": 0.34,
      "step": 871
    },
    {
      "epoch": 0.19818181818181818,
      "grad_norm": 0.05045570433139801,
      "learning_rate": 0.00016054607508532423,
      "loss": 0.366,
      "step": 872
    },
    {
      "epoch": 0.1984090909090909,
      "grad_norm": 0.05367381125688553,
      "learning_rate": 0.00016050056882821389,
      "loss": 0.3474,
      "step": 873
    },
    {
      "epoch": 0.19863636363636364,
      "grad_norm": 0.04444172605872154,
      "learning_rate": 0.00016045506257110354,
      "loss": 0.3237,
      "step": 874
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.05417361855506897,
      "learning_rate": 0.0001604095563139932,
      "loss": 0.2878,
      "step": 875
    },
    {
      "epoch": 0.1990909090909091,
      "grad_norm": 0.03338078036904335,
      "learning_rate": 0.00016036405005688282,
      "loss": 0.2246,
      "step": 876
    },
    {
      "epoch": 0.1993181818181818,
      "grad_norm": 0.06651124358177185,
      "learning_rate": 0.00016031854379977247,
      "loss": 0.3802,
      "step": 877
    },
    {
      "epoch": 0.19954545454545455,
      "grad_norm": 0.04922497645020485,
      "learning_rate": 0.00016027303754266212,
      "loss": 0.2909,
      "step": 878
    },
    {
      "epoch": 0.19977272727272727,
      "grad_norm": 0.055885326117277145,
      "learning_rate": 0.00016022753128555178,
      "loss": 0.3379,
      "step": 879
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.039438627660274506,
      "learning_rate": 0.00016018202502844143,
      "loss": 0.3006,
      "step": 880
    },
    {
      "epoch": 0.20022727272727273,
      "grad_norm": 0.04501638188958168,
      "learning_rate": 0.00016013651877133105,
      "loss": 0.2768,
      "step": 881
    },
    {
      "epoch": 0.20045454545454544,
      "grad_norm": 0.052563827484846115,
      "learning_rate": 0.0001600910125142207,
      "loss": 0.3185,
      "step": 882
    },
    {
      "epoch": 0.20068181818181818,
      "grad_norm": 0.060115326195955276,
      "learning_rate": 0.00016004550625711036,
      "loss": 0.4239,
      "step": 883
    },
    {
      "epoch": 0.2009090909090909,
      "grad_norm": 0.043318890035152435,
      "learning_rate": 0.00016,
      "loss": 0.2989,
      "step": 884
    },
    {
      "epoch": 0.20113636363636364,
      "grad_norm": 0.06332533806562424,
      "learning_rate": 0.00015995449374288964,
      "loss": 0.3785,
      "step": 885
    },
    {
      "epoch": 0.20136363636363636,
      "grad_norm": 0.06068994104862213,
      "learning_rate": 0.0001599089874857793,
      "loss": 0.2888,
      "step": 886
    },
    {
      "epoch": 0.2015909090909091,
      "grad_norm": 0.046582892537117004,
      "learning_rate": 0.00015986348122866894,
      "loss": 0.3373,
      "step": 887
    },
    {
      "epoch": 0.2018181818181818,
      "grad_norm": 0.044036615639925,
      "learning_rate": 0.0001598179749715586,
      "loss": 0.3247,
      "step": 888
    },
    {
      "epoch": 0.20204545454545456,
      "grad_norm": 0.05628526583313942,
      "learning_rate": 0.00015977246871444825,
      "loss": 0.3307,
      "step": 889
    },
    {
      "epoch": 0.20227272727272727,
      "grad_norm": 0.0598624125123024,
      "learning_rate": 0.0001597269624573379,
      "loss": 0.3538,
      "step": 890
    },
    {
      "epoch": 0.2025,
      "grad_norm": 0.05933700501918793,
      "learning_rate": 0.00015968145620022753,
      "loss": 0.3154,
      "step": 891
    },
    {
      "epoch": 0.20272727272727273,
      "grad_norm": 0.047488827258348465,
      "learning_rate": 0.00015963594994311718,
      "loss": 0.3428,
      "step": 892
    },
    {
      "epoch": 0.20295454545454544,
      "grad_norm": 0.05641252547502518,
      "learning_rate": 0.00015959044368600683,
      "loss": 0.3025,
      "step": 893
    },
    {
      "epoch": 0.20318181818181819,
      "grad_norm": 0.06375031173229218,
      "learning_rate": 0.0001595449374288965,
      "loss": 0.3908,
      "step": 894
    },
    {
      "epoch": 0.2034090909090909,
      "grad_norm": 0.059242282062768936,
      "learning_rate": 0.0001594994311717861,
      "loss": 0.3457,
      "step": 895
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.06867381185293198,
      "learning_rate": 0.00015945392491467577,
      "loss": 0.4355,
      "step": 896
    },
    {
      "epoch": 0.20386363636363636,
      "grad_norm": 0.04033256322145462,
      "learning_rate": 0.00015940841865756542,
      "loss": 0.3067,
      "step": 897
    },
    {
      "epoch": 0.2040909090909091,
      "grad_norm": 0.03185495361685753,
      "learning_rate": 0.00015936291240045507,
      "loss": 0.2087,
      "step": 898
    },
    {
      "epoch": 0.20431818181818182,
      "grad_norm": 0.050169937312603,
      "learning_rate": 0.00015931740614334472,
      "loss": 0.3311,
      "step": 899
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.06333417445421219,
      "learning_rate": 0.00015927189988623438,
      "loss": 0.3655,
      "step": 900
    },
    {
      "epoch": 0.20454545454545456,
      "eval_loss": 0.3323257863521576,
      "eval_runtime": 229.2813,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 900
    },
    {
      "epoch": 0.20477272727272727,
      "grad_norm": 0.06027643755078316,
      "learning_rate": 0.000159226393629124,
      "loss": 0.3461,
      "step": 901
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05398332327604294,
      "learning_rate": 0.00015918088737201366,
      "loss": 0.3867,
      "step": 902
    },
    {
      "epoch": 0.20522727272727273,
      "grad_norm": 0.03360999748110771,
      "learning_rate": 0.0001591353811149033,
      "loss": 0.2569,
      "step": 903
    },
    {
      "epoch": 0.20545454545454545,
      "grad_norm": 0.05404633283615112,
      "learning_rate": 0.00015908987485779296,
      "loss": 0.3726,
      "step": 904
    },
    {
      "epoch": 0.2056818181818182,
      "grad_norm": 0.05945470556616783,
      "learning_rate": 0.0001590443686006826,
      "loss": 0.3922,
      "step": 905
    },
    {
      "epoch": 0.2059090909090909,
      "grad_norm": 0.057736217975616455,
      "learning_rate": 0.00015899886234357224,
      "loss": 0.3884,
      "step": 906
    },
    {
      "epoch": 0.20613636363636365,
      "grad_norm": 0.04737751930952072,
      "learning_rate": 0.0001589533560864619,
      "loss": 0.3409,
      "step": 907
    },
    {
      "epoch": 0.20636363636363636,
      "grad_norm": 0.06676290184259415,
      "learning_rate": 0.00015890784982935155,
      "loss": 0.3887,
      "step": 908
    },
    {
      "epoch": 0.2065909090909091,
      "grad_norm": 0.05697585642337799,
      "learning_rate": 0.0001588623435722412,
      "loss": 0.3292,
      "step": 909
    },
    {
      "epoch": 0.20681818181818182,
      "grad_norm": 0.045891717076301575,
      "learning_rate": 0.00015881683731513085,
      "loss": 0.3019,
      "step": 910
    },
    {
      "epoch": 0.20704545454545453,
      "grad_norm": 0.04906604066491127,
      "learning_rate": 0.00015877133105802048,
      "loss": 0.3613,
      "step": 911
    },
    {
      "epoch": 0.20727272727272728,
      "grad_norm": 0.05326637625694275,
      "learning_rate": 0.00015872582480091013,
      "loss": 0.3447,
      "step": 912
    },
    {
      "epoch": 0.2075,
      "grad_norm": 0.052884556353092194,
      "learning_rate": 0.00015868031854379978,
      "loss": 0.3655,
      "step": 913
    },
    {
      "epoch": 0.20772727272727273,
      "grad_norm": 0.0553097166121006,
      "learning_rate": 0.00015863481228668944,
      "loss": 0.3663,
      "step": 914
    },
    {
      "epoch": 0.20795454545454545,
      "grad_norm": 0.048181354999542236,
      "learning_rate": 0.00015858930602957906,
      "loss": 0.2969,
      "step": 915
    },
    {
      "epoch": 0.2081818181818182,
      "grad_norm": 0.0501255989074707,
      "learning_rate": 0.00015854379977246871,
      "loss": 0.3195,
      "step": 916
    },
    {
      "epoch": 0.2084090909090909,
      "grad_norm": 0.05489646643400192,
      "learning_rate": 0.00015849829351535837,
      "loss": 0.2925,
      "step": 917
    },
    {
      "epoch": 0.20863636363636365,
      "grad_norm": 0.04424931854009628,
      "learning_rate": 0.00015845278725824802,
      "loss": 0.3044,
      "step": 918
    },
    {
      "epoch": 0.20886363636363636,
      "grad_norm": 0.036270298063755035,
      "learning_rate": 0.00015840728100113767,
      "loss": 0.2893,
      "step": 919
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.04697820544242859,
      "learning_rate": 0.00015836177474402732,
      "loss": 0.3044,
      "step": 920
    },
    {
      "epoch": 0.20931818181818182,
      "grad_norm": 0.06114225089550018,
      "learning_rate": 0.00015831626848691695,
      "loss": 0.3794,
      "step": 921
    },
    {
      "epoch": 0.20954545454545453,
      "grad_norm": 0.05736915394663811,
      "learning_rate": 0.0001582707622298066,
      "loss": 0.3617,
      "step": 922
    },
    {
      "epoch": 0.20977272727272728,
      "grad_norm": 0.046799421310424805,
      "learning_rate": 0.00015822525597269626,
      "loss": 0.3043,
      "step": 923
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.06569277495145798,
      "learning_rate": 0.00015817974971558588,
      "loss": 0.4124,
      "step": 924
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.04514444246888161,
      "learning_rate": 0.00015813424345847553,
      "loss": 0.2863,
      "step": 925
    },
    {
      "epoch": 0.21045454545454545,
      "grad_norm": 0.044851310551166534,
      "learning_rate": 0.0001580887372013652,
      "loss": 0.3543,
      "step": 926
    },
    {
      "epoch": 0.2106818181818182,
      "grad_norm": 0.04550354182720184,
      "learning_rate": 0.00015804323094425484,
      "loss": 0.2704,
      "step": 927
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.05176810920238495,
      "learning_rate": 0.0001579977246871445,
      "loss": 0.3206,
      "step": 928
    },
    {
      "epoch": 0.21113636363636365,
      "grad_norm": 0.053780697286129,
      "learning_rate": 0.00015795221843003415,
      "loss": 0.3492,
      "step": 929
    },
    {
      "epoch": 0.21136363636363636,
      "grad_norm": 0.048803746700286865,
      "learning_rate": 0.0001579067121729238,
      "loss": 0.2904,
      "step": 930
    },
    {
      "epoch": 0.21159090909090908,
      "grad_norm": 0.06276576966047287,
      "learning_rate": 0.00015786120591581342,
      "loss": 0.3388,
      "step": 931
    },
    {
      "epoch": 0.21181818181818182,
      "grad_norm": 0.06337109208106995,
      "learning_rate": 0.00015781569965870308,
      "loss": 0.3909,
      "step": 932
    },
    {
      "epoch": 0.21204545454545454,
      "grad_norm": 0.05565481632947922,
      "learning_rate": 0.00015777019340159273,
      "loss": 0.3388,
      "step": 933
    },
    {
      "epoch": 0.21227272727272728,
      "grad_norm": 0.04928908497095108,
      "learning_rate": 0.00015772468714448236,
      "loss": 0.34,
      "step": 934
    },
    {
      "epoch": 0.2125,
      "grad_norm": 0.04929244518280029,
      "learning_rate": 0.000157679180887372,
      "loss": 0.3424,
      "step": 935
    },
    {
      "epoch": 0.21272727272727274,
      "grad_norm": 0.05078747868537903,
      "learning_rate": 0.00015763367463026166,
      "loss": 0.3226,
      "step": 936
    },
    {
      "epoch": 0.21295454545454545,
      "grad_norm": 0.05584730952978134,
      "learning_rate": 0.00015758816837315131,
      "loss": 0.3303,
      "step": 937
    },
    {
      "epoch": 0.2131818181818182,
      "grad_norm": 0.0591261200606823,
      "learning_rate": 0.00015754266211604097,
      "loss": 0.364,
      "step": 938
    },
    {
      "epoch": 0.2134090909090909,
      "grad_norm": 0.04561871662735939,
      "learning_rate": 0.00015749715585893062,
      "loss": 0.2767,
      "step": 939
    },
    {
      "epoch": 0.21363636363636362,
      "grad_norm": 0.04790268465876579,
      "learning_rate": 0.00015745164960182027,
      "loss": 0.3328,
      "step": 940
    },
    {
      "epoch": 0.21386363636363637,
      "grad_norm": 0.06352808326482773,
      "learning_rate": 0.0001574061433447099,
      "loss": 0.334,
      "step": 941
    },
    {
      "epoch": 0.21409090909090908,
      "grad_norm": 0.04957524314522743,
      "learning_rate": 0.00015736063708759955,
      "loss": 0.3639,
      "step": 942
    },
    {
      "epoch": 0.21431818181818182,
      "grad_norm": 0.037071362137794495,
      "learning_rate": 0.0001573151308304892,
      "loss": 0.2787,
      "step": 943
    },
    {
      "epoch": 0.21454545454545454,
      "grad_norm": 0.048409152776002884,
      "learning_rate": 0.00015726962457337883,
      "loss": 0.2968,
      "step": 944
    },
    {
      "epoch": 0.21477272727272728,
      "grad_norm": 0.13755154609680176,
      "learning_rate": 0.00015722411831626848,
      "loss": 0.289,
      "step": 945
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.062197037041187286,
      "learning_rate": 0.00015717861205915814,
      "loss": 0.3847,
      "step": 946
    },
    {
      "epoch": 0.21522727272727274,
      "grad_norm": 0.059862252324819565,
      "learning_rate": 0.0001571331058020478,
      "loss": 0.2944,
      "step": 947
    },
    {
      "epoch": 0.21545454545454545,
      "grad_norm": 0.06950781494379044,
      "learning_rate": 0.00015708759954493744,
      "loss": 0.3077,
      "step": 948
    },
    {
      "epoch": 0.21568181818181817,
      "grad_norm": 0.06300850957632065,
      "learning_rate": 0.0001570420932878271,
      "loss": 0.3449,
      "step": 949
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.05319245904684067,
      "learning_rate": 0.00015699658703071675,
      "loss": 0.3311,
      "step": 950
    },
    {
      "epoch": 0.21613636363636363,
      "grad_norm": 0.05568677932024002,
      "learning_rate": 0.00015695108077360637,
      "loss": 0.3334,
      "step": 951
    },
    {
      "epoch": 0.21636363636363637,
      "grad_norm": 0.05818092077970505,
      "learning_rate": 0.00015690557451649603,
      "loss": 0.352,
      "step": 952
    },
    {
      "epoch": 0.21659090909090908,
      "grad_norm": 0.04304342344403267,
      "learning_rate": 0.00015686006825938568,
      "loss": 0.2964,
      "step": 953
    },
    {
      "epoch": 0.21681818181818183,
      "grad_norm": 0.04736172780394554,
      "learning_rate": 0.0001568145620022753,
      "loss": 0.3278,
      "step": 954
    },
    {
      "epoch": 0.21704545454545454,
      "grad_norm": 0.03804318979382515,
      "learning_rate": 0.00015676905574516496,
      "loss": 0.2579,
      "step": 955
    },
    {
      "epoch": 0.21727272727272728,
      "grad_norm": 0.058216553181409836,
      "learning_rate": 0.0001567235494880546,
      "loss": 0.3807,
      "step": 956
    },
    {
      "epoch": 0.2175,
      "grad_norm": 0.05030987039208412,
      "learning_rate": 0.00015667804323094426,
      "loss": 0.3284,
      "step": 957
    },
    {
      "epoch": 0.2177272727272727,
      "grad_norm": 0.05720171332359314,
      "learning_rate": 0.00015663253697383392,
      "loss": 0.3363,
      "step": 958
    },
    {
      "epoch": 0.21795454545454546,
      "grad_norm": 0.04861003905534744,
      "learning_rate": 0.00015658703071672357,
      "loss": 0.3509,
      "step": 959
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.04961790889501572,
      "learning_rate": 0.00015654152445961322,
      "loss": 0.3452,
      "step": 960
    },
    {
      "epoch": 0.2184090909090909,
      "grad_norm": 0.037270091474056244,
      "learning_rate": 0.00015649601820250285,
      "loss": 0.2695,
      "step": 961
    },
    {
      "epoch": 0.21863636363636363,
      "grad_norm": 0.05660165846347809,
      "learning_rate": 0.0001564505119453925,
      "loss": 0.3636,
      "step": 962
    },
    {
      "epoch": 0.21886363636363637,
      "grad_norm": 0.04948577284812927,
      "learning_rate": 0.00015640500568828213,
      "loss": 0.3623,
      "step": 963
    },
    {
      "epoch": 0.2190909090909091,
      "grad_norm": 0.04485844075679779,
      "learning_rate": 0.00015635949943117178,
      "loss": 0.2922,
      "step": 964
    },
    {
      "epoch": 0.21931818181818183,
      "grad_norm": 0.046199724078178406,
      "learning_rate": 0.00015631399317406143,
      "loss": 0.311,
      "step": 965
    },
    {
      "epoch": 0.21954545454545454,
      "grad_norm": 0.060162197798490524,
      "learning_rate": 0.00015626848691695108,
      "loss": 0.3808,
      "step": 966
    },
    {
      "epoch": 0.2197727272727273,
      "grad_norm": 0.06211453676223755,
      "learning_rate": 0.00015622298065984074,
      "loss": 0.3444,
      "step": 967
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.05383169278502464,
      "learning_rate": 0.0001561774744027304,
      "loss": 0.366,
      "step": 968
    },
    {
      "epoch": 0.22022727272727272,
      "grad_norm": 0.05771714076399803,
      "learning_rate": 0.00015613196814562004,
      "loss": 0.3611,
      "step": 969
    },
    {
      "epoch": 0.22045454545454546,
      "grad_norm": 0.04731052368879318,
      "learning_rate": 0.0001560864618885097,
      "loss": 0.2862,
      "step": 970
    },
    {
      "epoch": 0.22068181818181817,
      "grad_norm": 0.057989317923784256,
      "learning_rate": 0.00015604095563139932,
      "loss": 0.3993,
      "step": 971
    },
    {
      "epoch": 0.22090909090909092,
      "grad_norm": 0.045404914766550064,
      "learning_rate": 0.00015599544937428897,
      "loss": 0.3033,
      "step": 972
    },
    {
      "epoch": 0.22113636363636363,
      "grad_norm": 0.046366021037101746,
      "learning_rate": 0.0001559499431171786,
      "loss": 0.3319,
      "step": 973
    },
    {
      "epoch": 0.22136363636363637,
      "grad_norm": 0.03387761116027832,
      "learning_rate": 0.00015590443686006825,
      "loss": 0.2695,
      "step": 974
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.0515180379152298,
      "learning_rate": 0.0001558589306029579,
      "loss": 0.2872,
      "step": 975
    },
    {
      "epoch": 0.22181818181818183,
      "grad_norm": 0.04741411656141281,
      "learning_rate": 0.00015581342434584756,
      "loss": 0.2917,
      "step": 976
    },
    {
      "epoch": 0.22204545454545455,
      "grad_norm": 0.053102046251297,
      "learning_rate": 0.0001557679180887372,
      "loss": 0.3383,
      "step": 977
    },
    {
      "epoch": 0.22227272727272726,
      "grad_norm": 0.05528470128774643,
      "learning_rate": 0.00015572241183162686,
      "loss": 0.3836,
      "step": 978
    },
    {
      "epoch": 0.2225,
      "grad_norm": 0.04649611562490463,
      "learning_rate": 0.00015567690557451652,
      "loss": 0.3508,
      "step": 979
    },
    {
      "epoch": 0.22272727272727272,
      "grad_norm": 0.060122448951005936,
      "learning_rate": 0.00015563139931740617,
      "loss": 0.3619,
      "step": 980
    },
    {
      "epoch": 0.22295454545454546,
      "grad_norm": 0.05226031318306923,
      "learning_rate": 0.0001555858930602958,
      "loss": 0.2792,
      "step": 981
    },
    {
      "epoch": 0.22318181818181818,
      "grad_norm": 0.03991086408495903,
      "learning_rate": 0.00015554038680318545,
      "loss": 0.3085,
      "step": 982
    },
    {
      "epoch": 0.22340909090909092,
      "grad_norm": 0.05245048552751541,
      "learning_rate": 0.00015549488054607507,
      "loss": 0.3402,
      "step": 983
    },
    {
      "epoch": 0.22363636363636363,
      "grad_norm": 0.05785524472594261,
      "learning_rate": 0.00015544937428896473,
      "loss": 0.3607,
      "step": 984
    },
    {
      "epoch": 0.22386363636363638,
      "grad_norm": 0.07723948359489441,
      "learning_rate": 0.00015540386803185438,
      "loss": 0.3161,
      "step": 985
    },
    {
      "epoch": 0.2240909090909091,
      "grad_norm": 0.07592964172363281,
      "learning_rate": 0.00015535836177474403,
      "loss": 0.3636,
      "step": 986
    },
    {
      "epoch": 0.2243181818181818,
      "grad_norm": 0.060008253902196884,
      "learning_rate": 0.00015531285551763368,
      "loss": 0.3587,
      "step": 987
    },
    {
      "epoch": 0.22454545454545455,
      "grad_norm": 0.06084801256656647,
      "learning_rate": 0.00015526734926052334,
      "loss": 0.3368,
      "step": 988
    },
    {
      "epoch": 0.22477272727272726,
      "grad_norm": 0.0601930245757103,
      "learning_rate": 0.000155221843003413,
      "loss": 0.2731,
      "step": 989
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.05252881348133087,
      "learning_rate": 0.00015517633674630264,
      "loss": 0.3462,
      "step": 990
    },
    {
      "epoch": 0.22522727272727272,
      "grad_norm": 0.052654363214969635,
      "learning_rate": 0.00015513083048919227,
      "loss": 0.3179,
      "step": 991
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.05268614739179611,
      "learning_rate": 0.00015508532423208192,
      "loss": 0.3198,
      "step": 992
    },
    {
      "epoch": 0.22568181818181818,
      "grad_norm": 0.04893970489501953,
      "learning_rate": 0.00015503981797497155,
      "loss": 0.3528,
      "step": 993
    },
    {
      "epoch": 0.22590909090909092,
      "grad_norm": 0.04649427905678749,
      "learning_rate": 0.0001549943117178612,
      "loss": 0.3151,
      "step": 994
    },
    {
      "epoch": 0.22613636363636364,
      "grad_norm": 0.043444063514471054,
      "learning_rate": 0.00015494880546075085,
      "loss": 0.3081,
      "step": 995
    },
    {
      "epoch": 0.22636363636363635,
      "grad_norm": 0.048189450055360794,
      "learning_rate": 0.0001549032992036405,
      "loss": 0.3033,
      "step": 996
    },
    {
      "epoch": 0.2265909090909091,
      "grad_norm": 0.05448952317237854,
      "learning_rate": 0.00015485779294653016,
      "loss": 0.3685,
      "step": 997
    },
    {
      "epoch": 0.2268181818181818,
      "grad_norm": 0.04511737823486328,
      "learning_rate": 0.0001548122866894198,
      "loss": 0.2547,
      "step": 998
    },
    {
      "epoch": 0.22704545454545455,
      "grad_norm": 0.050684235990047455,
      "learning_rate": 0.00015476678043230946,
      "loss": 0.3393,
      "step": 999
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.05115203559398651,
      "learning_rate": 0.00015472127417519912,
      "loss": 0.3319,
      "step": 1000
    },
    {
      "epoch": 0.22727272727272727,
      "eval_loss": 0.331823468208313,
      "eval_runtime": 229.3128,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 1000
    },
    {
      "epoch": 0.2275,
      "grad_norm": 0.04511171206831932,
      "learning_rate": 0.00015467576791808874,
      "loss": 0.3053,
      "step": 1001
    },
    {
      "epoch": 0.22772727272727272,
      "grad_norm": 0.0458403155207634,
      "learning_rate": 0.00015463026166097837,
      "loss": 0.3374,
      "step": 1002
    },
    {
      "epoch": 0.22795454545454547,
      "grad_norm": 0.05618106946349144,
      "learning_rate": 0.00015458475540386802,
      "loss": 0.3291,
      "step": 1003
    },
    {
      "epoch": 0.22818181818181818,
      "grad_norm": 0.052053917199373245,
      "learning_rate": 0.00015453924914675767,
      "loss": 0.3771,
      "step": 1004
    },
    {
      "epoch": 0.22840909090909092,
      "grad_norm": 0.05206175893545151,
      "learning_rate": 0.00015449374288964733,
      "loss": 0.3883,
      "step": 1005
    },
    {
      "epoch": 0.22863636363636364,
      "grad_norm": 0.0434260368347168,
      "learning_rate": 0.00015444823663253698,
      "loss": 0.2592,
      "step": 1006
    },
    {
      "epoch": 0.22886363636363635,
      "grad_norm": 0.04519876465201378,
      "learning_rate": 0.00015440273037542663,
      "loss": 0.3309,
      "step": 1007
    },
    {
      "epoch": 0.2290909090909091,
      "grad_norm": 0.04300442337989807,
      "learning_rate": 0.00015435722411831629,
      "loss": 0.3,
      "step": 1008
    },
    {
      "epoch": 0.2293181818181818,
      "grad_norm": 0.05119091272354126,
      "learning_rate": 0.00015431171786120594,
      "loss": 0.3662,
      "step": 1009
    },
    {
      "epoch": 0.22954545454545455,
      "grad_norm": 0.05389774963259697,
      "learning_rate": 0.0001542662116040956,
      "loss": 0.3843,
      "step": 1010
    },
    {
      "epoch": 0.22977272727272727,
      "grad_norm": 0.04790669307112694,
      "learning_rate": 0.00015422070534698522,
      "loss": 0.3397,
      "step": 1011
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.051795437932014465,
      "learning_rate": 0.00015417519908987484,
      "loss": 0.3106,
      "step": 1012
    },
    {
      "epoch": 0.23022727272727272,
      "grad_norm": 0.059135980904102325,
      "learning_rate": 0.0001541296928327645,
      "loss": 0.3989,
      "step": 1013
    },
    {
      "epoch": 0.23045454545454547,
      "grad_norm": 0.07616477459669113,
      "learning_rate": 0.00015408418657565415,
      "loss": 0.403,
      "step": 1014
    },
    {
      "epoch": 0.23068181818181818,
      "grad_norm": 0.053701795637607574,
      "learning_rate": 0.0001540386803185438,
      "loss": 0.3672,
      "step": 1015
    },
    {
      "epoch": 0.2309090909090909,
      "grad_norm": 0.05527748912572861,
      "learning_rate": 0.00015399317406143345,
      "loss": 0.3126,
      "step": 1016
    },
    {
      "epoch": 0.23113636363636364,
      "grad_norm": 0.05491790175437927,
      "learning_rate": 0.0001539476678043231,
      "loss": 0.3436,
      "step": 1017
    },
    {
      "epoch": 0.23136363636363635,
      "grad_norm": 0.06455349922180176,
      "learning_rate": 0.00015390216154721276,
      "loss": 0.3958,
      "step": 1018
    },
    {
      "epoch": 0.2315909090909091,
      "grad_norm": 0.06747454404830933,
      "learning_rate": 0.0001538566552901024,
      "loss": 0.4214,
      "step": 1019
    },
    {
      "epoch": 0.2318181818181818,
      "grad_norm": 0.052145883440971375,
      "learning_rate": 0.00015381114903299206,
      "loss": 0.3569,
      "step": 1020
    },
    {
      "epoch": 0.23204545454545455,
      "grad_norm": 0.0474623367190361,
      "learning_rate": 0.0001537656427758817,
      "loss": 0.3297,
      "step": 1021
    },
    {
      "epoch": 0.23227272727272727,
      "grad_norm": 0.04916525259613991,
      "learning_rate": 0.00015372013651877132,
      "loss": 0.309,
      "step": 1022
    },
    {
      "epoch": 0.2325,
      "grad_norm": 0.07896045595407486,
      "learning_rate": 0.00015367463026166097,
      "loss": 0.4131,
      "step": 1023
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.06342114508152008,
      "learning_rate": 0.00015362912400455062,
      "loss": 0.308,
      "step": 1024
    },
    {
      "epoch": 0.23295454545454544,
      "grad_norm": 0.0508996807038784,
      "learning_rate": 0.00015358361774744027,
      "loss": 0.3251,
      "step": 1025
    },
    {
      "epoch": 0.23318181818181818,
      "grad_norm": 0.044722747057676315,
      "learning_rate": 0.00015353811149032993,
      "loss": 0.2605,
      "step": 1026
    },
    {
      "epoch": 0.2334090909090909,
      "grad_norm": 0.06341841071844101,
      "learning_rate": 0.00015349260523321958,
      "loss": 0.3849,
      "step": 1027
    },
    {
      "epoch": 0.23363636363636364,
      "grad_norm": 0.058940161019563675,
      "learning_rate": 0.00015344709897610923,
      "loss": 0.3204,
      "step": 1028
    },
    {
      "epoch": 0.23386363636363636,
      "grad_norm": 0.06943965703248978,
      "learning_rate": 0.00015340159271899889,
      "loss": 0.3839,
      "step": 1029
    },
    {
      "epoch": 0.2340909090909091,
      "grad_norm": 0.06753513962030411,
      "learning_rate": 0.00015335608646188854,
      "loss": 0.33,
      "step": 1030
    },
    {
      "epoch": 0.23431818181818181,
      "grad_norm": 0.05369272828102112,
      "learning_rate": 0.00015331058020477816,
      "loss": 0.2889,
      "step": 1031
    },
    {
      "epoch": 0.23454545454545456,
      "grad_norm": 0.047009747475385666,
      "learning_rate": 0.0001532650739476678,
      "loss": 0.3408,
      "step": 1032
    },
    {
      "epoch": 0.23477272727272727,
      "grad_norm": 0.060543231666088104,
      "learning_rate": 0.00015321956769055744,
      "loss": 0.3781,
      "step": 1033
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.06912152469158173,
      "learning_rate": 0.0001531740614334471,
      "loss": 0.3475,
      "step": 1034
    },
    {
      "epoch": 0.23522727272727273,
      "grad_norm": 0.05955905094742775,
      "learning_rate": 0.00015312855517633675,
      "loss": 0.3516,
      "step": 1035
    },
    {
      "epoch": 0.23545454545454544,
      "grad_norm": 0.056576795876026154,
      "learning_rate": 0.0001530830489192264,
      "loss": 0.3786,
      "step": 1036
    },
    {
      "epoch": 0.2356818181818182,
      "grad_norm": 0.046830929815769196,
      "learning_rate": 0.00015303754266211605,
      "loss": 0.2654,
      "step": 1037
    },
    {
      "epoch": 0.2359090909090909,
      "grad_norm": 0.0415363535284996,
      "learning_rate": 0.0001529920364050057,
      "loss": 0.3238,
      "step": 1038
    },
    {
      "epoch": 0.23613636363636364,
      "grad_norm": 0.041946712881326675,
      "learning_rate": 0.00015294653014789536,
      "loss": 0.3156,
      "step": 1039
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.05423014983534813,
      "learning_rate": 0.000152901023890785,
      "loss": 0.2969,
      "step": 1040
    },
    {
      "epoch": 0.2365909090909091,
      "grad_norm": 0.04844198003411293,
      "learning_rate": 0.00015285551763367464,
      "loss": 0.3517,
      "step": 1041
    },
    {
      "epoch": 0.23681818181818182,
      "grad_norm": 0.05782593786716461,
      "learning_rate": 0.00015281001137656426,
      "loss": 0.3853,
      "step": 1042
    },
    {
      "epoch": 0.23704545454545456,
      "grad_norm": 0.06382521241903305,
      "learning_rate": 0.00015276450511945392,
      "loss": 0.3721,
      "step": 1043
    },
    {
      "epoch": 0.23727272727272727,
      "grad_norm": 0.04419766366481781,
      "learning_rate": 0.00015271899886234357,
      "loss": 0.2609,
      "step": 1044
    },
    {
      "epoch": 0.2375,
      "grad_norm": 0.03819282352924347,
      "learning_rate": 0.00015267349260523322,
      "loss": 0.2884,
      "step": 1045
    },
    {
      "epoch": 0.23772727272727273,
      "grad_norm": 0.0570245124399662,
      "learning_rate": 0.00015262798634812288,
      "loss": 0.3892,
      "step": 1046
    },
    {
      "epoch": 0.23795454545454545,
      "grad_norm": 0.06282849609851837,
      "learning_rate": 0.00015258248009101253,
      "loss": 0.3663,
      "step": 1047
    },
    {
      "epoch": 0.2381818181818182,
      "grad_norm": 0.05643841624259949,
      "learning_rate": 0.00015253697383390218,
      "loss": 0.2901,
      "step": 1048
    },
    {
      "epoch": 0.2384090909090909,
      "grad_norm": 0.054376810789108276,
      "learning_rate": 0.00015249146757679183,
      "loss": 0.3142,
      "step": 1049
    },
    {
      "epoch": 0.23863636363636365,
      "grad_norm": 0.047991931438446045,
      "learning_rate": 0.0001524459613196815,
      "loss": 0.3124,
      "step": 1050
    },
    {
      "epoch": 0.23886363636363636,
      "grad_norm": 0.05460795760154724,
      "learning_rate": 0.0001524004550625711,
      "loss": 0.3397,
      "step": 1051
    },
    {
      "epoch": 0.2390909090909091,
      "grad_norm": 0.0406067855656147,
      "learning_rate": 0.00015235494880546074,
      "loss": 0.2932,
      "step": 1052
    },
    {
      "epoch": 0.23931818181818182,
      "grad_norm": 0.051131512969732285,
      "learning_rate": 0.0001523094425483504,
      "loss": 0.3157,
      "step": 1053
    },
    {
      "epoch": 0.23954545454545453,
      "grad_norm": 0.0677962377667427,
      "learning_rate": 0.00015226393629124004,
      "loss": 0.3224,
      "step": 1054
    },
    {
      "epoch": 0.23977272727272728,
      "grad_norm": 0.07541634142398834,
      "learning_rate": 0.0001522184300341297,
      "loss": 0.364,
      "step": 1055
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.05672210082411766,
      "learning_rate": 0.00015217292377701935,
      "loss": 0.386,
      "step": 1056
    },
    {
      "epoch": 0.24022727272727273,
      "grad_norm": 0.05216042324900627,
      "learning_rate": 0.000152127417519909,
      "loss": 0.3009,
      "step": 1057
    },
    {
      "epoch": 0.24045454545454545,
      "grad_norm": 0.0399102047085762,
      "learning_rate": 0.00015208191126279865,
      "loss": 0.3143,
      "step": 1058
    },
    {
      "epoch": 0.2406818181818182,
      "grad_norm": 0.054353851824998856,
      "learning_rate": 0.0001520364050056883,
      "loss": 0.3237,
      "step": 1059
    },
    {
      "epoch": 0.2409090909090909,
      "grad_norm": 0.04945641756057739,
      "learning_rate": 0.00015199089874857793,
      "loss": 0.3355,
      "step": 1060
    },
    {
      "epoch": 0.24113636363636365,
      "grad_norm": 0.06163453683257103,
      "learning_rate": 0.00015194539249146759,
      "loss": 0.3739,
      "step": 1061
    },
    {
      "epoch": 0.24136363636363636,
      "grad_norm": 0.06069303676486015,
      "learning_rate": 0.0001518998862343572,
      "loss": 0.3522,
      "step": 1062
    },
    {
      "epoch": 0.24159090909090908,
      "grad_norm": 0.033159688115119934,
      "learning_rate": 0.00015185437997724686,
      "loss": 0.2328,
      "step": 1063
    },
    {
      "epoch": 0.24181818181818182,
      "grad_norm": 0.04982698708772659,
      "learning_rate": 0.00015180887372013652,
      "loss": 0.3607,
      "step": 1064
    },
    {
      "epoch": 0.24204545454545454,
      "grad_norm": 0.04895196855068207,
      "learning_rate": 0.00015176336746302617,
      "loss": 0.2937,
      "step": 1065
    },
    {
      "epoch": 0.24227272727272728,
      "grad_norm": 0.05328463762998581,
      "learning_rate": 0.00015171786120591582,
      "loss": 0.3348,
      "step": 1066
    },
    {
      "epoch": 0.2425,
      "grad_norm": 0.05823932960629463,
      "learning_rate": 0.00015167235494880548,
      "loss": 0.361,
      "step": 1067
    },
    {
      "epoch": 0.24272727272727274,
      "grad_norm": 0.048432327806949615,
      "learning_rate": 0.00015162684869169513,
      "loss": 0.3272,
      "step": 1068
    },
    {
      "epoch": 0.24295454545454545,
      "grad_norm": 0.07310608774423599,
      "learning_rate": 0.00015158134243458478,
      "loss": 0.4229,
      "step": 1069
    },
    {
      "epoch": 0.2431818181818182,
      "grad_norm": 0.06111777946352959,
      "learning_rate": 0.0001515358361774744,
      "loss": 0.3711,
      "step": 1070
    },
    {
      "epoch": 0.2434090909090909,
      "grad_norm": 0.04843452200293541,
      "learning_rate": 0.00015149032992036406,
      "loss": 0.3164,
      "step": 1071
    },
    {
      "epoch": 0.24363636363636362,
      "grad_norm": 0.056004978716373444,
      "learning_rate": 0.00015144482366325369,
      "loss": 0.3424,
      "step": 1072
    },
    {
      "epoch": 0.24386363636363637,
      "grad_norm": 0.04489365592598915,
      "learning_rate": 0.00015139931740614334,
      "loss": 0.317,
      "step": 1073
    },
    {
      "epoch": 0.24409090909090908,
      "grad_norm": 0.04958093911409378,
      "learning_rate": 0.000151353811149033,
      "loss": 0.3566,
      "step": 1074
    },
    {
      "epoch": 0.24431818181818182,
      "grad_norm": 0.04443785548210144,
      "learning_rate": 0.00015130830489192264,
      "loss": 0.2884,
      "step": 1075
    },
    {
      "epoch": 0.24454545454545454,
      "grad_norm": 0.03636796772480011,
      "learning_rate": 0.0001512627986348123,
      "loss": 0.2567,
      "step": 1076
    },
    {
      "epoch": 0.24477272727272728,
      "grad_norm": 0.046512339264154434,
      "learning_rate": 0.00015121729237770195,
      "loss": 0.2725,
      "step": 1077
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.053937651216983795,
      "learning_rate": 0.0001511717861205916,
      "loss": 0.324,
      "step": 1078
    },
    {
      "epoch": 0.24522727272727274,
      "grad_norm": 0.0569784939289093,
      "learning_rate": 0.00015112627986348126,
      "loss": 0.3332,
      "step": 1079
    },
    {
      "epoch": 0.24545454545454545,
      "grad_norm": 0.0836014673113823,
      "learning_rate": 0.00015108077360637088,
      "loss": 0.4662,
      "step": 1080
    },
    {
      "epoch": 0.2456818181818182,
      "grad_norm": 0.04731796681880951,
      "learning_rate": 0.00015103526734926053,
      "loss": 0.325,
      "step": 1081
    },
    {
      "epoch": 0.2459090909090909,
      "grad_norm": 0.05464126542210579,
      "learning_rate": 0.00015098976109215016,
      "loss": 0.3391,
      "step": 1082
    },
    {
      "epoch": 0.24613636363636363,
      "grad_norm": 0.055599894374608994,
      "learning_rate": 0.0001509442548350398,
      "loss": 0.4015,
      "step": 1083
    },
    {
      "epoch": 0.24636363636363637,
      "grad_norm": 0.04622800275683403,
      "learning_rate": 0.00015089874857792947,
      "loss": 0.3322,
      "step": 1084
    },
    {
      "epoch": 0.24659090909090908,
      "grad_norm": 0.05085434764623642,
      "learning_rate": 0.00015085324232081912,
      "loss": 0.3556,
      "step": 1085
    },
    {
      "epoch": 0.24681818181818183,
      "grad_norm": 0.05081764981150627,
      "learning_rate": 0.00015080773606370877,
      "loss": 0.3856,
      "step": 1086
    },
    {
      "epoch": 0.24704545454545454,
      "grad_norm": 0.04821329191327095,
      "learning_rate": 0.00015076222980659842,
      "loss": 0.3454,
      "step": 1087
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 0.04972919076681137,
      "learning_rate": 0.00015071672354948808,
      "loss": 0.3454,
      "step": 1088
    },
    {
      "epoch": 0.2475,
      "grad_norm": 0.08733667433261871,
      "learning_rate": 0.00015067121729237773,
      "loss": 0.3774,
      "step": 1089
    },
    {
      "epoch": 0.24772727272727274,
      "grad_norm": 0.056246355175971985,
      "learning_rate": 0.00015062571103526736,
      "loss": 0.3638,
      "step": 1090
    },
    {
      "epoch": 0.24795454545454546,
      "grad_norm": 0.06038060784339905,
      "learning_rate": 0.000150580204778157,
      "loss": 0.3455,
      "step": 1091
    },
    {
      "epoch": 0.24818181818181817,
      "grad_norm": 0.07061759382486343,
      "learning_rate": 0.00015053469852104663,
      "loss": 0.3606,
      "step": 1092
    },
    {
      "epoch": 0.2484090909090909,
      "grad_norm": 0.05063381791114807,
      "learning_rate": 0.0001504891922639363,
      "loss": 0.3073,
      "step": 1093
    },
    {
      "epoch": 0.24863636363636363,
      "grad_norm": 0.05295050889253616,
      "learning_rate": 0.00015044368600682594,
      "loss": 0.3079,
      "step": 1094
    },
    {
      "epoch": 0.24886363636363637,
      "grad_norm": 0.06078290194272995,
      "learning_rate": 0.0001503981797497156,
      "loss": 0.3453,
      "step": 1095
    },
    {
      "epoch": 0.24909090909090909,
      "grad_norm": 0.05462127923965454,
      "learning_rate": 0.00015035267349260525,
      "loss": 0.3441,
      "step": 1096
    },
    {
      "epoch": 0.24931818181818183,
      "grad_norm": 0.06143038719892502,
      "learning_rate": 0.0001503071672354949,
      "loss": 0.3689,
      "step": 1097
    },
    {
      "epoch": 0.24954545454545454,
      "grad_norm": 0.04666893184185028,
      "learning_rate": 0.00015026166097838455,
      "loss": 0.33,
      "step": 1098
    },
    {
      "epoch": 0.24977272727272729,
      "grad_norm": 0.05831179395318031,
      "learning_rate": 0.00015021615472127418,
      "loss": 0.3863,
      "step": 1099
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.04852968454360962,
      "learning_rate": 0.00015017064846416383,
      "loss": 0.2775,
      "step": 1100
    },
    {
      "epoch": 0.25,
      "eval_loss": 0.33133426308631897,
      "eval_runtime": 229.3487,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 1100
    },
    {
      "epoch": 0.25022727272727274,
      "grad_norm": 0.04205863177776337,
      "learning_rate": 0.00015012514220705348,
      "loss": 0.3435,
      "step": 1101
    },
    {
      "epoch": 0.25045454545454543,
      "grad_norm": 0.05326538532972336,
      "learning_rate": 0.0001500796359499431,
      "loss": 0.3814,
      "step": 1102
    },
    {
      "epoch": 0.2506818181818182,
      "grad_norm": 0.038775742053985596,
      "learning_rate": 0.00015003412969283276,
      "loss": 0.316,
      "step": 1103
    },
    {
      "epoch": 0.2509090909090909,
      "grad_norm": 0.04705614969134331,
      "learning_rate": 0.00014998862343572241,
      "loss": 0.3176,
      "step": 1104
    },
    {
      "epoch": 0.25113636363636366,
      "grad_norm": 0.0546359159052372,
      "learning_rate": 0.00014994311717861207,
      "loss": 0.3799,
      "step": 1105
    },
    {
      "epoch": 0.25136363636363634,
      "grad_norm": 0.05903181433677673,
      "learning_rate": 0.00014989761092150172,
      "loss": 0.3562,
      "step": 1106
    },
    {
      "epoch": 0.2515909090909091,
      "grad_norm": 0.04068180173635483,
      "learning_rate": 0.00014985210466439137,
      "loss": 0.3085,
      "step": 1107
    },
    {
      "epoch": 0.25181818181818183,
      "grad_norm": 0.03165753185749054,
      "learning_rate": 0.00014980659840728102,
      "loss": 0.2881,
      "step": 1108
    },
    {
      "epoch": 0.2520454545454546,
      "grad_norm": 0.044155023992061615,
      "learning_rate": 0.00014976109215017065,
      "loss": 0.3132,
      "step": 1109
    },
    {
      "epoch": 0.25227272727272726,
      "grad_norm": 0.0425611287355423,
      "learning_rate": 0.0001497155858930603,
      "loss": 0.3165,
      "step": 1110
    },
    {
      "epoch": 0.2525,
      "grad_norm": 0.047695010900497437,
      "learning_rate": 0.00014967007963594996,
      "loss": 0.2905,
      "step": 1111
    },
    {
      "epoch": 0.25272727272727274,
      "grad_norm": 0.03499195724725723,
      "learning_rate": 0.00014962457337883958,
      "loss": 0.2453,
      "step": 1112
    },
    {
      "epoch": 0.25295454545454543,
      "grad_norm": 0.04715557023882866,
      "learning_rate": 0.00014957906712172923,
      "loss": 0.3637,
      "step": 1113
    },
    {
      "epoch": 0.2531818181818182,
      "grad_norm": 0.042314253747463226,
      "learning_rate": 0.0001495335608646189,
      "loss": 0.2854,
      "step": 1114
    },
    {
      "epoch": 0.2534090909090909,
      "grad_norm": 0.04165984317660332,
      "learning_rate": 0.00014948805460750854,
      "loss": 0.3337,
      "step": 1115
    },
    {
      "epoch": 0.25363636363636366,
      "grad_norm": 0.05039948225021362,
      "learning_rate": 0.0001494425483503982,
      "loss": 0.2876,
      "step": 1116
    },
    {
      "epoch": 0.25386363636363635,
      "grad_norm": 0.04404287785291672,
      "learning_rate": 0.00014939704209328785,
      "loss": 0.3552,
      "step": 1117
    },
    {
      "epoch": 0.2540909090909091,
      "grad_norm": 0.04303093999624252,
      "learning_rate": 0.0001493515358361775,
      "loss": 0.347,
      "step": 1118
    },
    {
      "epoch": 0.25431818181818183,
      "grad_norm": 0.04573512822389603,
      "learning_rate": 0.00014930602957906712,
      "loss": 0.3256,
      "step": 1119
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.04703407362103462,
      "learning_rate": 0.00014926052332195678,
      "loss": 0.3193,
      "step": 1120
    },
    {
      "epoch": 0.25477272727272726,
      "grad_norm": 0.056471578776836395,
      "learning_rate": 0.00014921501706484643,
      "loss": 0.3636,
      "step": 1121
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.06362798064947128,
      "learning_rate": 0.00014916951080773606,
      "loss": 0.3806,
      "step": 1122
    },
    {
      "epoch": 0.25522727272727275,
      "grad_norm": 0.05315073952078819,
      "learning_rate": 0.0001491240045506257,
      "loss": 0.3608,
      "step": 1123
    },
    {
      "epoch": 0.25545454545454543,
      "grad_norm": 0.040895916521549225,
      "learning_rate": 0.00014907849829351536,
      "loss": 0.2673,
      "step": 1124
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 0.05192380025982857,
      "learning_rate": 0.00014903299203640501,
      "loss": 0.3089,
      "step": 1125
    },
    {
      "epoch": 0.2559090909090909,
      "grad_norm": 0.047105494886636734,
      "learning_rate": 0.00014898748577929467,
      "loss": 0.2753,
      "step": 1126
    },
    {
      "epoch": 0.25613636363636366,
      "grad_norm": 0.05464392155408859,
      "learning_rate": 0.00014894197952218432,
      "loss": 0.3606,
      "step": 1127
    },
    {
      "epoch": 0.25636363636363635,
      "grad_norm": 0.05578113719820976,
      "learning_rate": 0.00014889647326507397,
      "loss": 0.404,
      "step": 1128
    },
    {
      "epoch": 0.2565909090909091,
      "grad_norm": 0.05354190245270729,
      "learning_rate": 0.0001488509670079636,
      "loss": 0.3679,
      "step": 1129
    },
    {
      "epoch": 0.25681818181818183,
      "grad_norm": 0.044533390551805496,
      "learning_rate": 0.00014880546075085325,
      "loss": 0.3485,
      "step": 1130
    },
    {
      "epoch": 0.2570454545454545,
      "grad_norm": 0.05831330642104149,
      "learning_rate": 0.0001487599544937429,
      "loss": 0.312,
      "step": 1131
    },
    {
      "epoch": 0.25727272727272726,
      "grad_norm": 0.05037958547472954,
      "learning_rate": 0.00014871444823663253,
      "loss": 0.366,
      "step": 1132
    },
    {
      "epoch": 0.2575,
      "grad_norm": 0.03976152464747429,
      "learning_rate": 0.00014866894197952218,
      "loss": 0.2893,
      "step": 1133
    },
    {
      "epoch": 0.25772727272727275,
      "grad_norm": 0.03657212480902672,
      "learning_rate": 0.00014862343572241184,
      "loss": 0.2845,
      "step": 1134
    },
    {
      "epoch": 0.25795454545454544,
      "grad_norm": 0.05045069381594658,
      "learning_rate": 0.0001485779294653015,
      "loss": 0.381,
      "step": 1135
    },
    {
      "epoch": 0.2581818181818182,
      "grad_norm": 0.06468813121318817,
      "learning_rate": 0.00014853242320819114,
      "loss": 0.3724,
      "step": 1136
    },
    {
      "epoch": 0.2584090909090909,
      "grad_norm": 0.060822274535894394,
      "learning_rate": 0.0001484869169510808,
      "loss": 0.2979,
      "step": 1137
    },
    {
      "epoch": 0.2586363636363636,
      "grad_norm": 0.040404561907052994,
      "learning_rate": 0.00014844141069397042,
      "loss": 0.2939,
      "step": 1138
    },
    {
      "epoch": 0.25886363636363635,
      "grad_norm": 0.06204746663570404,
      "learning_rate": 0.00014839590443686007,
      "loss": 0.4424,
      "step": 1139
    },
    {
      "epoch": 0.2590909090909091,
      "grad_norm": 0.041403282433748245,
      "learning_rate": 0.00014835039817974973,
      "loss": 0.3205,
      "step": 1140
    },
    {
      "epoch": 0.25931818181818184,
      "grad_norm": 0.056964319199323654,
      "learning_rate": 0.00014830489192263938,
      "loss": 0.365,
      "step": 1141
    },
    {
      "epoch": 0.2595454545454545,
      "grad_norm": 0.06143239140510559,
      "learning_rate": 0.000148259385665529,
      "loss": 0.3829,
      "step": 1142
    },
    {
      "epoch": 0.25977272727272727,
      "grad_norm": 0.05815288797020912,
      "learning_rate": 0.00014821387940841866,
      "loss": 0.3985,
      "step": 1143
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.07187265902757645,
      "learning_rate": 0.0001481683731513083,
      "loss": 0.4046,
      "step": 1144
    },
    {
      "epoch": 0.26022727272727275,
      "grad_norm": 0.044001441448926926,
      "learning_rate": 0.00014812286689419796,
      "loss": 0.2753,
      "step": 1145
    },
    {
      "epoch": 0.26045454545454544,
      "grad_norm": 0.06416987627744675,
      "learning_rate": 0.00014807736063708762,
      "loss": 0.3324,
      "step": 1146
    },
    {
      "epoch": 0.2606818181818182,
      "grad_norm": 0.045992352068424225,
      "learning_rate": 0.00014803185437997727,
      "loss": 0.3124,
      "step": 1147
    },
    {
      "epoch": 0.2609090909090909,
      "grad_norm": 0.044488757848739624,
      "learning_rate": 0.0001479863481228669,
      "loss": 0.298,
      "step": 1148
    },
    {
      "epoch": 0.2611363636363636,
      "grad_norm": 0.05312880501151085,
      "learning_rate": 0.00014794084186575655,
      "loss": 0.3431,
      "step": 1149
    },
    {
      "epoch": 0.26136363636363635,
      "grad_norm": 0.05543075501918793,
      "learning_rate": 0.0001478953356086462,
      "loss": 0.3313,
      "step": 1150
    },
    {
      "epoch": 0.2615909090909091,
      "grad_norm": 0.047903794795274734,
      "learning_rate": 0.00014784982935153585,
      "loss": 0.3041,
      "step": 1151
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.045167140662670135,
      "learning_rate": 0.00014780432309442548,
      "loss": 0.316,
      "step": 1152
    },
    {
      "epoch": 0.2620454545454545,
      "grad_norm": 0.03390093520283699,
      "learning_rate": 0.00014775881683731513,
      "loss": 0.2901,
      "step": 1153
    },
    {
      "epoch": 0.26227272727272727,
      "grad_norm": 0.049773525446653366,
      "learning_rate": 0.00014771331058020478,
      "loss": 0.342,
      "step": 1154
    },
    {
      "epoch": 0.2625,
      "grad_norm": 0.05264001339673996,
      "learning_rate": 0.00014766780432309444,
      "loss": 0.3156,
      "step": 1155
    },
    {
      "epoch": 0.26272727272727275,
      "grad_norm": 0.05416679382324219,
      "learning_rate": 0.0001476222980659841,
      "loss": 0.3428,
      "step": 1156
    },
    {
      "epoch": 0.26295454545454544,
      "grad_norm": 0.04788075387477875,
      "learning_rate": 0.00014757679180887374,
      "loss": 0.33,
      "step": 1157
    },
    {
      "epoch": 0.2631818181818182,
      "grad_norm": 0.04276937618851662,
      "learning_rate": 0.00014753128555176337,
      "loss": 0.3139,
      "step": 1158
    },
    {
      "epoch": 0.2634090909090909,
      "grad_norm": 0.046497032046318054,
      "learning_rate": 0.00014748577929465302,
      "loss": 0.3123,
      "step": 1159
    },
    {
      "epoch": 0.2636363636363636,
      "grad_norm": 0.040435340255498886,
      "learning_rate": 0.00014744027303754267,
      "loss": 0.2789,
      "step": 1160
    },
    {
      "epoch": 0.26386363636363636,
      "grad_norm": 0.03134801611304283,
      "learning_rate": 0.00014739476678043233,
      "loss": 0.254,
      "step": 1161
    },
    {
      "epoch": 0.2640909090909091,
      "grad_norm": 0.041716258972883224,
      "learning_rate": 0.00014734926052332195,
      "loss": 0.3033,
      "step": 1162
    },
    {
      "epoch": 0.26431818181818184,
      "grad_norm": 0.03821742534637451,
      "learning_rate": 0.0001473037542662116,
      "loss": 0.2833,
      "step": 1163
    },
    {
      "epoch": 0.26454545454545453,
      "grad_norm": 0.04722358658909798,
      "learning_rate": 0.00014725824800910126,
      "loss": 0.2688,
      "step": 1164
    },
    {
      "epoch": 0.26477272727272727,
      "grad_norm": 0.05031273141503334,
      "learning_rate": 0.0001472127417519909,
      "loss": 0.3307,
      "step": 1165
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.0474446639418602,
      "learning_rate": 0.00014716723549488056,
      "loss": 0.336,
      "step": 1166
    },
    {
      "epoch": 0.2652272727272727,
      "grad_norm": 0.05260849371552467,
      "learning_rate": 0.0001471217292377702,
      "loss": 0.3381,
      "step": 1167
    },
    {
      "epoch": 0.26545454545454544,
      "grad_norm": 0.050716500729322433,
      "learning_rate": 0.00014707622298065984,
      "loss": 0.3192,
      "step": 1168
    },
    {
      "epoch": 0.2656818181818182,
      "grad_norm": 0.05433684214949608,
      "learning_rate": 0.0001470307167235495,
      "loss": 0.3779,
      "step": 1169
    },
    {
      "epoch": 0.26590909090909093,
      "grad_norm": 0.04270331934094429,
      "learning_rate": 0.00014698521046643915,
      "loss": 0.3355,
      "step": 1170
    },
    {
      "epoch": 0.2661363636363636,
      "grad_norm": 0.05339471995830536,
      "learning_rate": 0.0001469397042093288,
      "loss": 0.3649,
      "step": 1171
    },
    {
      "epoch": 0.26636363636363636,
      "grad_norm": 0.043934449553489685,
      "learning_rate": 0.00014689419795221843,
      "loss": 0.2493,
      "step": 1172
    },
    {
      "epoch": 0.2665909090909091,
      "grad_norm": 0.0494636669754982,
      "learning_rate": 0.00014684869169510808,
      "loss": 0.3384,
      "step": 1173
    },
    {
      "epoch": 0.26681818181818184,
      "grad_norm": 0.0446130633354187,
      "learning_rate": 0.00014680318543799773,
      "loss": 0.3157,
      "step": 1174
    },
    {
      "epoch": 0.26704545454545453,
      "grad_norm": 0.04581707343459129,
      "learning_rate": 0.00014675767918088738,
      "loss": 0.324,
      "step": 1175
    },
    {
      "epoch": 0.2672727272727273,
      "grad_norm": 0.06040152534842491,
      "learning_rate": 0.00014671217292377704,
      "loss": 0.3132,
      "step": 1176
    },
    {
      "epoch": 0.2675,
      "grad_norm": 0.039652302861213684,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.2708,
      "step": 1177
    },
    {
      "epoch": 0.2677272727272727,
      "grad_norm": 0.04939841479063034,
      "learning_rate": 0.00014662116040955632,
      "loss": 0.3709,
      "step": 1178
    },
    {
      "epoch": 0.26795454545454545,
      "grad_norm": 0.04627012461423874,
      "learning_rate": 0.00014657565415244597,
      "loss": 0.2895,
      "step": 1179
    },
    {
      "epoch": 0.2681818181818182,
      "grad_norm": 0.03830360993742943,
      "learning_rate": 0.00014653014789533562,
      "loss": 0.3051,
      "step": 1180
    },
    {
      "epoch": 0.26840909090909093,
      "grad_norm": 0.049997881054878235,
      "learning_rate": 0.00014648464163822527,
      "loss": 0.331,
      "step": 1181
    },
    {
      "epoch": 0.2686363636363636,
      "grad_norm": 0.05083766207098961,
      "learning_rate": 0.0001464391353811149,
      "loss": 0.376,
      "step": 1182
    },
    {
      "epoch": 0.26886363636363636,
      "grad_norm": 0.04967617616057396,
      "learning_rate": 0.00014639362912400455,
      "loss": 0.3066,
      "step": 1183
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.057524122297763824,
      "learning_rate": 0.0001463481228668942,
      "loss": 0.3893,
      "step": 1184
    },
    {
      "epoch": 0.26931818181818185,
      "grad_norm": 0.058582939207553864,
      "learning_rate": 0.00014630261660978386,
      "loss": 0.345,
      "step": 1185
    },
    {
      "epoch": 0.26954545454545453,
      "grad_norm": 0.04949444904923439,
      "learning_rate": 0.0001462571103526735,
      "loss": 0.3344,
      "step": 1186
    },
    {
      "epoch": 0.2697727272727273,
      "grad_norm": 0.04698460176587105,
      "learning_rate": 0.00014621160409556314,
      "loss": 0.3014,
      "step": 1187
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.0532037615776062,
      "learning_rate": 0.0001461660978384528,
      "loss": 0.3592,
      "step": 1188
    },
    {
      "epoch": 0.2702272727272727,
      "grad_norm": 0.05112134665250778,
      "learning_rate": 0.00014612059158134244,
      "loss": 0.3883,
      "step": 1189
    },
    {
      "epoch": 0.27045454545454545,
      "grad_norm": 0.05592411383986473,
      "learning_rate": 0.0001460750853242321,
      "loss": 0.3595,
      "step": 1190
    },
    {
      "epoch": 0.2706818181818182,
      "grad_norm": 0.06161395460367203,
      "learning_rate": 0.00014602957906712175,
      "loss": 0.3532,
      "step": 1191
    },
    {
      "epoch": 0.27090909090909093,
      "grad_norm": 0.05738344043493271,
      "learning_rate": 0.00014598407281001137,
      "loss": 0.4002,
      "step": 1192
    },
    {
      "epoch": 0.2711363636363636,
      "grad_norm": 0.03907431662082672,
      "learning_rate": 0.00014593856655290103,
      "loss": 0.2585,
      "step": 1193
    },
    {
      "epoch": 0.27136363636363636,
      "grad_norm": 0.06277745217084885,
      "learning_rate": 0.00014589306029579068,
      "loss": 0.4125,
      "step": 1194
    },
    {
      "epoch": 0.2715909090909091,
      "grad_norm": 0.052891865372657776,
      "learning_rate": 0.00014584755403868033,
      "loss": 0.3511,
      "step": 1195
    },
    {
      "epoch": 0.2718181818181818,
      "grad_norm": 0.05498052015900612,
      "learning_rate": 0.00014580204778156999,
      "loss": 0.3813,
      "step": 1196
    },
    {
      "epoch": 0.27204545454545453,
      "grad_norm": 0.04488985985517502,
      "learning_rate": 0.0001457565415244596,
      "loss": 0.3052,
      "step": 1197
    },
    {
      "epoch": 0.2722727272727273,
      "grad_norm": 0.041063740849494934,
      "learning_rate": 0.00014571103526734926,
      "loss": 0.308,
      "step": 1198
    },
    {
      "epoch": 0.2725,
      "grad_norm": 0.06013093516230583,
      "learning_rate": 0.00014566552901023892,
      "loss": 0.3902,
      "step": 1199
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.04415111988782883,
      "learning_rate": 0.00014562002275312857,
      "loss": 0.3449,
      "step": 1200
    },
    {
      "epoch": 0.2727272727272727,
      "eval_loss": 0.33043941855430603,
      "eval_runtime": 229.2375,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 1200
    },
    {
      "epoch": 0.27295454545454545,
      "grad_norm": 0.04304991289973259,
      "learning_rate": 0.00014557451649601822,
      "loss": 0.3549,
      "step": 1201
    },
    {
      "epoch": 0.2731818181818182,
      "grad_norm": 0.05586371198296547,
      "learning_rate": 0.00014552901023890785,
      "loss": 0.3582,
      "step": 1202
    },
    {
      "epoch": 0.27340909090909093,
      "grad_norm": 0.05615265294909477,
      "learning_rate": 0.0001454835039817975,
      "loss": 0.4053,
      "step": 1203
    },
    {
      "epoch": 0.2736363636363636,
      "grad_norm": 0.05064411088824272,
      "learning_rate": 0.00014543799772468715,
      "loss": 0.299,
      "step": 1204
    },
    {
      "epoch": 0.27386363636363636,
      "grad_norm": 0.06764739006757736,
      "learning_rate": 0.0001453924914675768,
      "loss": 0.37,
      "step": 1205
    },
    {
      "epoch": 0.2740909090909091,
      "grad_norm": 0.05392075702548027,
      "learning_rate": 0.00014534698521046643,
      "loss": 0.3377,
      "step": 1206
    },
    {
      "epoch": 0.2743181818181818,
      "grad_norm": 0.05544262006878853,
      "learning_rate": 0.00014530147895335608,
      "loss": 0.3785,
      "step": 1207
    },
    {
      "epoch": 0.27454545454545454,
      "grad_norm": 0.054434459656476974,
      "learning_rate": 0.00014525597269624574,
      "loss": 0.3457,
      "step": 1208
    },
    {
      "epoch": 0.2747727272727273,
      "grad_norm": 0.06185436248779297,
      "learning_rate": 0.0001452104664391354,
      "loss": 0.3888,
      "step": 1209
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.0400557816028595,
      "learning_rate": 0.00014516496018202504,
      "loss": 0.3177,
      "step": 1210
    },
    {
      "epoch": 0.2752272727272727,
      "grad_norm": 0.04741767793893814,
      "learning_rate": 0.0001451194539249147,
      "loss": 0.3349,
      "step": 1211
    },
    {
      "epoch": 0.27545454545454545,
      "grad_norm": 0.038488347083330154,
      "learning_rate": 0.00014507394766780432,
      "loss": 0.2795,
      "step": 1212
    },
    {
      "epoch": 0.2756818181818182,
      "grad_norm": 0.04277696833014488,
      "learning_rate": 0.00014502844141069397,
      "loss": 0.3175,
      "step": 1213
    },
    {
      "epoch": 0.2759090909090909,
      "grad_norm": 0.04161834716796875,
      "learning_rate": 0.00014498293515358363,
      "loss": 0.3181,
      "step": 1214
    },
    {
      "epoch": 0.2761363636363636,
      "grad_norm": 0.050898097455501556,
      "learning_rate": 0.00014493742889647328,
      "loss": 0.3514,
      "step": 1215
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.05069752037525177,
      "learning_rate": 0.0001448919226393629,
      "loss": 0.3859,
      "step": 1216
    },
    {
      "epoch": 0.2765909090909091,
      "grad_norm": 0.04379932954907417,
      "learning_rate": 0.00014484641638225256,
      "loss": 0.35,
      "step": 1217
    },
    {
      "epoch": 0.2768181818181818,
      "grad_norm": 0.04685954377055168,
      "learning_rate": 0.0001448009101251422,
      "loss": 0.3398,
      "step": 1218
    },
    {
      "epoch": 0.27704545454545454,
      "grad_norm": 0.05163025110960007,
      "learning_rate": 0.00014475540386803186,
      "loss": 0.3357,
      "step": 1219
    },
    {
      "epoch": 0.2772727272727273,
      "grad_norm": 0.05634411796927452,
      "learning_rate": 0.00014470989761092152,
      "loss": 0.3526,
      "step": 1220
    },
    {
      "epoch": 0.2775,
      "grad_norm": 0.0424923337996006,
      "learning_rate": 0.00014466439135381117,
      "loss": 0.2788,
      "step": 1221
    },
    {
      "epoch": 0.2777272727272727,
      "grad_norm": 0.04081907495856285,
      "learning_rate": 0.0001446188850967008,
      "loss": 0.3048,
      "step": 1222
    },
    {
      "epoch": 0.27795454545454545,
      "grad_norm": 0.048628706485033035,
      "learning_rate": 0.00014457337883959045,
      "loss": 0.3423,
      "step": 1223
    },
    {
      "epoch": 0.2781818181818182,
      "grad_norm": 0.04585585370659828,
      "learning_rate": 0.0001445278725824801,
      "loss": 0.3742,
      "step": 1224
    },
    {
      "epoch": 0.2784090909090909,
      "grad_norm": 0.04737446829676628,
      "learning_rate": 0.00014448236632536975,
      "loss": 0.2855,
      "step": 1225
    },
    {
      "epoch": 0.2786363636363636,
      "grad_norm": 0.048425059765577316,
      "learning_rate": 0.00014443686006825938,
      "loss": 0.3321,
      "step": 1226
    },
    {
      "epoch": 0.27886363636363637,
      "grad_norm": 0.06230924651026726,
      "learning_rate": 0.00014439135381114903,
      "loss": 0.3531,
      "step": 1227
    },
    {
      "epoch": 0.2790909090909091,
      "grad_norm": 0.051634255796670914,
      "learning_rate": 0.00014434584755403869,
      "loss": 0.3689,
      "step": 1228
    },
    {
      "epoch": 0.2793181818181818,
      "grad_norm": 0.06497350335121155,
      "learning_rate": 0.00014430034129692834,
      "loss": 0.424,
      "step": 1229
    },
    {
      "epoch": 0.27954545454545454,
      "grad_norm": 0.04167044162750244,
      "learning_rate": 0.000144254835039818,
      "loss": 0.304,
      "step": 1230
    },
    {
      "epoch": 0.2797727272727273,
      "grad_norm": 0.04381164163351059,
      "learning_rate": 0.00014420932878270764,
      "loss": 0.2949,
      "step": 1231
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.056305866688489914,
      "learning_rate": 0.00014416382252559727,
      "loss": 0.3411,
      "step": 1232
    },
    {
      "epoch": 0.2802272727272727,
      "grad_norm": 0.04169166088104248,
      "learning_rate": 0.00014411831626848692,
      "loss": 0.3384,
      "step": 1233
    },
    {
      "epoch": 0.28045454545454546,
      "grad_norm": 0.060474004596471786,
      "learning_rate": 0.00014407281001137658,
      "loss": 0.3509,
      "step": 1234
    },
    {
      "epoch": 0.2806818181818182,
      "grad_norm": 0.04719403386116028,
      "learning_rate": 0.00014402730375426623,
      "loss": 0.3073,
      "step": 1235
    },
    {
      "epoch": 0.2809090909090909,
      "grad_norm": 0.050782784819602966,
      "learning_rate": 0.00014398179749715585,
      "loss": 0.3411,
      "step": 1236
    },
    {
      "epoch": 0.28113636363636363,
      "grad_norm": 0.04677595943212509,
      "learning_rate": 0.0001439362912400455,
      "loss": 0.347,
      "step": 1237
    },
    {
      "epoch": 0.28136363636363637,
      "grad_norm": 0.034909360110759735,
      "learning_rate": 0.00014389078498293516,
      "loss": 0.2575,
      "step": 1238
    },
    {
      "epoch": 0.2815909090909091,
      "grad_norm": 0.044134221971035004,
      "learning_rate": 0.0001438452787258248,
      "loss": 0.321,
      "step": 1239
    },
    {
      "epoch": 0.2818181818181818,
      "grad_norm": 0.0468129962682724,
      "learning_rate": 0.00014379977246871447,
      "loss": 0.3633,
      "step": 1240
    },
    {
      "epoch": 0.28204545454545454,
      "grad_norm": 0.05651023983955383,
      "learning_rate": 0.00014375426621160412,
      "loss": 0.3367,
      "step": 1241
    },
    {
      "epoch": 0.2822727272727273,
      "grad_norm": 0.04683932662010193,
      "learning_rate": 0.00014370875995449374,
      "loss": 0.3389,
      "step": 1242
    },
    {
      "epoch": 0.2825,
      "grad_norm": 0.059975337237119675,
      "learning_rate": 0.0001436632536973834,
      "loss": 0.3757,
      "step": 1243
    },
    {
      "epoch": 0.2827272727272727,
      "grad_norm": 0.045507680624723434,
      "learning_rate": 0.00014361774744027305,
      "loss": 0.3204,
      "step": 1244
    },
    {
      "epoch": 0.28295454545454546,
      "grad_norm": 0.0719316229224205,
      "learning_rate": 0.00014357224118316268,
      "loss": 0.4308,
      "step": 1245
    },
    {
      "epoch": 0.2831818181818182,
      "grad_norm": 0.05558788403868675,
      "learning_rate": 0.00014352673492605233,
      "loss": 0.3641,
      "step": 1246
    },
    {
      "epoch": 0.2834090909090909,
      "grad_norm": 0.07860013097524643,
      "learning_rate": 0.00014348122866894198,
      "loss": 0.4378,
      "step": 1247
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.045213621109724045,
      "learning_rate": 0.00014343572241183163,
      "loss": 0.3692,
      "step": 1248
    },
    {
      "epoch": 0.2838636363636364,
      "grad_norm": 0.04740380123257637,
      "learning_rate": 0.00014339021615472129,
      "loss": 0.3313,
      "step": 1249
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 0.05324666202068329,
      "learning_rate": 0.00014334470989761094,
      "loss": 0.3659,
      "step": 1250
    },
    {
      "epoch": 0.2843181818181818,
      "grad_norm": 0.04617813229560852,
      "learning_rate": 0.0001432992036405006,
      "loss": 0.3617,
      "step": 1251
    },
    {
      "epoch": 0.28454545454545455,
      "grad_norm": 0.04193606972694397,
      "learning_rate": 0.00014325369738339022,
      "loss": 0.3328,
      "step": 1252
    },
    {
      "epoch": 0.2847727272727273,
      "grad_norm": 0.04328213632106781,
      "learning_rate": 0.00014320819112627987,
      "loss": 0.3317,
      "step": 1253
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.053812526166439056,
      "learning_rate": 0.00014316268486916952,
      "loss": 0.3072,
      "step": 1254
    },
    {
      "epoch": 0.2852272727272727,
      "grad_norm": 0.04336068779230118,
      "learning_rate": 0.00014311717861205915,
      "loss": 0.3362,
      "step": 1255
    },
    {
      "epoch": 0.28545454545454546,
      "grad_norm": 0.050459880381822586,
      "learning_rate": 0.0001430716723549488,
      "loss": 0.3471,
      "step": 1256
    },
    {
      "epoch": 0.2856818181818182,
      "grad_norm": 0.04962955787777901,
      "learning_rate": 0.00014302616609783845,
      "loss": 0.3076,
      "step": 1257
    },
    {
      "epoch": 0.2859090909090909,
      "grad_norm": 0.06513509899377823,
      "learning_rate": 0.0001429806598407281,
      "loss": 0.4056,
      "step": 1258
    },
    {
      "epoch": 0.28613636363636363,
      "grad_norm": 0.041151344776153564,
      "learning_rate": 0.00014293515358361776,
      "loss": 0.3469,
      "step": 1259
    },
    {
      "epoch": 0.2863636363636364,
      "grad_norm": 0.054435014724731445,
      "learning_rate": 0.0001428896473265074,
      "loss": 0.3282,
      "step": 1260
    },
    {
      "epoch": 0.2865909090909091,
      "grad_norm": 0.04991231858730316,
      "learning_rate": 0.00014284414106939707,
      "loss": 0.3249,
      "step": 1261
    },
    {
      "epoch": 0.2868181818181818,
      "grad_norm": 0.04756641760468483,
      "learning_rate": 0.0001427986348122867,
      "loss": 0.3335,
      "step": 1262
    },
    {
      "epoch": 0.28704545454545455,
      "grad_norm": 0.04503866657614708,
      "learning_rate": 0.00014275312855517634,
      "loss": 0.3441,
      "step": 1263
    },
    {
      "epoch": 0.2872727272727273,
      "grad_norm": 0.04764321818947792,
      "learning_rate": 0.000142707622298066,
      "loss": 0.2966,
      "step": 1264
    },
    {
      "epoch": 0.2875,
      "grad_norm": 0.08047059923410416,
      "learning_rate": 0.00014266211604095562,
      "loss": 0.3548,
      "step": 1265
    },
    {
      "epoch": 0.2877272727272727,
      "grad_norm": 0.04108092561364174,
      "learning_rate": 0.00014261660978384528,
      "loss": 0.272,
      "step": 1266
    },
    {
      "epoch": 0.28795454545454546,
      "grad_norm": 0.04893287271261215,
      "learning_rate": 0.00014257110352673493,
      "loss": 0.2934,
      "step": 1267
    },
    {
      "epoch": 0.2881818181818182,
      "grad_norm": 0.05533614382147789,
      "learning_rate": 0.00014252559726962458,
      "loss": 0.3537,
      "step": 1268
    },
    {
      "epoch": 0.2884090909090909,
      "grad_norm": 0.07748442888259888,
      "learning_rate": 0.00014248009101251423,
      "loss": 0.4041,
      "step": 1269
    },
    {
      "epoch": 0.28863636363636364,
      "grad_norm": 0.04318932816386223,
      "learning_rate": 0.0001424345847554039,
      "loss": 0.3291,
      "step": 1270
    },
    {
      "epoch": 0.2888636363636364,
      "grad_norm": 0.04892052337527275,
      "learning_rate": 0.00014238907849829354,
      "loss": 0.3682,
      "step": 1271
    },
    {
      "epoch": 0.28909090909090907,
      "grad_norm": 0.05392800271511078,
      "learning_rate": 0.00014234357224118317,
      "loss": 0.3665,
      "step": 1272
    },
    {
      "epoch": 0.2893181818181818,
      "grad_norm": 0.04956365004181862,
      "learning_rate": 0.00014229806598407282,
      "loss": 0.2984,
      "step": 1273
    },
    {
      "epoch": 0.28954545454545455,
      "grad_norm": 0.053515493869781494,
      "learning_rate": 0.00014225255972696244,
      "loss": 0.3784,
      "step": 1274
    },
    {
      "epoch": 0.2897727272727273,
      "grad_norm": 0.05237402766942978,
      "learning_rate": 0.0001422070534698521,
      "loss": 0.3496,
      "step": 1275
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.03881418704986572,
      "learning_rate": 0.00014216154721274175,
      "loss": 0.3184,
      "step": 1276
    },
    {
      "epoch": 0.2902272727272727,
      "grad_norm": 0.05029431730508804,
      "learning_rate": 0.0001421160409556314,
      "loss": 0.2881,
      "step": 1277
    },
    {
      "epoch": 0.29045454545454547,
      "grad_norm": 0.04730900377035141,
      "learning_rate": 0.00014207053469852106,
      "loss": 0.2985,
      "step": 1278
    },
    {
      "epoch": 0.2906818181818182,
      "grad_norm": 0.037401266396045685,
      "learning_rate": 0.0001420250284414107,
      "loss": 0.2612,
      "step": 1279
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.0553450882434845,
      "learning_rate": 0.00014197952218430036,
      "loss": 0.3354,
      "step": 1280
    },
    {
      "epoch": 0.29113636363636364,
      "grad_norm": 0.06592367589473724,
      "learning_rate": 0.00014193401592719001,
      "loss": 0.3341,
      "step": 1281
    },
    {
      "epoch": 0.2913636363636364,
      "grad_norm": 0.05352753773331642,
      "learning_rate": 0.00014188850967007964,
      "loss": 0.3617,
      "step": 1282
    },
    {
      "epoch": 0.29159090909090907,
      "grad_norm": 0.05028379335999489,
      "learning_rate": 0.0001418430034129693,
      "loss": 0.3421,
      "step": 1283
    },
    {
      "epoch": 0.2918181818181818,
      "grad_norm": 0.06256330758333206,
      "learning_rate": 0.00014179749715585892,
      "loss": 0.3528,
      "step": 1284
    },
    {
      "epoch": 0.29204545454545455,
      "grad_norm": 0.042956192046403885,
      "learning_rate": 0.00014175199089874857,
      "loss": 0.329,
      "step": 1285
    },
    {
      "epoch": 0.2922727272727273,
      "grad_norm": 0.05192193388938904,
      "learning_rate": 0.00014170648464163822,
      "loss": 0.3687,
      "step": 1286
    },
    {
      "epoch": 0.2925,
      "grad_norm": 0.039565641433000565,
      "learning_rate": 0.00014166097838452788,
      "loss": 0.2781,
      "step": 1287
    },
    {
      "epoch": 0.2927272727272727,
      "grad_norm": 0.04677452892065048,
      "learning_rate": 0.00014161547212741753,
      "loss": 0.3009,
      "step": 1288
    },
    {
      "epoch": 0.29295454545454547,
      "grad_norm": 0.05355577543377876,
      "learning_rate": 0.00014156996587030718,
      "loss": 0.3733,
      "step": 1289
    },
    {
      "epoch": 0.29318181818181815,
      "grad_norm": 0.056913942098617554,
      "learning_rate": 0.00014152445961319684,
      "loss": 0.3449,
      "step": 1290
    },
    {
      "epoch": 0.2934090909090909,
      "grad_norm": 0.046705327928066254,
      "learning_rate": 0.00014147895335608646,
      "loss": 0.3304,
      "step": 1291
    },
    {
      "epoch": 0.29363636363636364,
      "grad_norm": 0.05710286274552345,
      "learning_rate": 0.00014143344709897611,
      "loss": 0.363,
      "step": 1292
    },
    {
      "epoch": 0.2938636363636364,
      "grad_norm": 0.05042421072721481,
      "learning_rate": 0.00014138794084186577,
      "loss": 0.33,
      "step": 1293
    },
    {
      "epoch": 0.29409090909090907,
      "grad_norm": 0.03885959833860397,
      "learning_rate": 0.0001413424345847554,
      "loss": 0.3106,
      "step": 1294
    },
    {
      "epoch": 0.2943181818181818,
      "grad_norm": 0.04620445892214775,
      "learning_rate": 0.00014129692832764505,
      "loss": 0.3772,
      "step": 1295
    },
    {
      "epoch": 0.29454545454545455,
      "grad_norm": 0.0461198054254055,
      "learning_rate": 0.0001412514220705347,
      "loss": 0.3344,
      "step": 1296
    },
    {
      "epoch": 0.2947727272727273,
      "grad_norm": 0.03987489268183708,
      "learning_rate": 0.00014120591581342435,
      "loss": 0.2887,
      "step": 1297
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.04275065287947655,
      "learning_rate": 0.000141160409556314,
      "loss": 0.3,
      "step": 1298
    },
    {
      "epoch": 0.2952272727272727,
      "grad_norm": 0.048732560127973557,
      "learning_rate": 0.00014111490329920366,
      "loss": 0.3137,
      "step": 1299
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.05384733900427818,
      "learning_rate": 0.0001410693970420933,
      "loss": 0.3401,
      "step": 1300
    },
    {
      "epoch": 0.29545454545454547,
      "eval_loss": 0.3305182158946991,
      "eval_runtime": 229.2945,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 1300
    },
    {
      "epoch": 0.29568181818181816,
      "grad_norm": 0.05397981032729149,
      "learning_rate": 0.00014102389078498293,
      "loss": 0.3171,
      "step": 1301
    },
    {
      "epoch": 0.2959090909090909,
      "grad_norm": 0.04543634504079819,
      "learning_rate": 0.0001409783845278726,
      "loss": 0.3151,
      "step": 1302
    },
    {
      "epoch": 0.29613636363636364,
      "grad_norm": 0.05461485683917999,
      "learning_rate": 0.00014093287827076224,
      "loss": 0.3282,
      "step": 1303
    },
    {
      "epoch": 0.2963636363636364,
      "grad_norm": 0.06801389902830124,
      "learning_rate": 0.00014088737201365187,
      "loss": 0.3335,
      "step": 1304
    },
    {
      "epoch": 0.29659090909090907,
      "grad_norm": 0.04758753627538681,
      "learning_rate": 0.00014084186575654152,
      "loss": 0.334,
      "step": 1305
    },
    {
      "epoch": 0.2968181818181818,
      "grad_norm": 0.060284968465566635,
      "learning_rate": 0.00014079635949943117,
      "loss": 0.3752,
      "step": 1306
    },
    {
      "epoch": 0.29704545454545456,
      "grad_norm": 0.04190224036574364,
      "learning_rate": 0.00014075085324232082,
      "loss": 0.3179,
      "step": 1307
    },
    {
      "epoch": 0.2972727272727273,
      "grad_norm": 0.05067184194922447,
      "learning_rate": 0.00014070534698521048,
      "loss": 0.3496,
      "step": 1308
    },
    {
      "epoch": 0.2975,
      "grad_norm": 0.051137011498212814,
      "learning_rate": 0.00014065984072810013,
      "loss": 0.3828,
      "step": 1309
    },
    {
      "epoch": 0.29772727272727273,
      "grad_norm": 0.04708222299814224,
      "learning_rate": 0.00014061433447098978,
      "loss": 0.32,
      "step": 1310
    },
    {
      "epoch": 0.29795454545454547,
      "grad_norm": 0.045512523502111435,
      "learning_rate": 0.0001405688282138794,
      "loss": 0.3419,
      "step": 1311
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 0.03364904969930649,
      "learning_rate": 0.00014052332195676906,
      "loss": 0.2779,
      "step": 1312
    },
    {
      "epoch": 0.2984090909090909,
      "grad_norm": 0.048970963805913925,
      "learning_rate": 0.0001404778156996587,
      "loss": 0.297,
      "step": 1313
    },
    {
      "epoch": 0.29863636363636364,
      "grad_norm": 0.04789530113339424,
      "learning_rate": 0.00014043230944254834,
      "loss": 0.3122,
      "step": 1314
    },
    {
      "epoch": 0.2988636363636364,
      "grad_norm": 0.03890833631157875,
      "learning_rate": 0.000140386803185438,
      "loss": 0.3165,
      "step": 1315
    },
    {
      "epoch": 0.2990909090909091,
      "grad_norm": 0.04477661848068237,
      "learning_rate": 0.00014034129692832765,
      "loss": 0.32,
      "step": 1316
    },
    {
      "epoch": 0.2993181818181818,
      "grad_norm": 0.05105625465512276,
      "learning_rate": 0.0001402957906712173,
      "loss": 0.3187,
      "step": 1317
    },
    {
      "epoch": 0.29954545454545456,
      "grad_norm": 0.0618894062936306,
      "learning_rate": 0.00014025028441410695,
      "loss": 0.3922,
      "step": 1318
    },
    {
      "epoch": 0.29977272727272725,
      "grad_norm": 0.05201754719018936,
      "learning_rate": 0.0001402047781569966,
      "loss": 0.3527,
      "step": 1319
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.044761911034584045,
      "learning_rate": 0.00014015927189988626,
      "loss": 0.3667,
      "step": 1320
    },
    {
      "epoch": 0.30022727272727273,
      "grad_norm": 0.054434459656476974,
      "learning_rate": 0.00014011376564277588,
      "loss": 0.2922,
      "step": 1321
    },
    {
      "epoch": 0.3004545454545455,
      "grad_norm": 0.06007065251469612,
      "learning_rate": 0.00014006825938566554,
      "loss": 0.3188,
      "step": 1322
    },
    {
      "epoch": 0.30068181818181816,
      "grad_norm": 0.055087462067604065,
      "learning_rate": 0.00014002275312855516,
      "loss": 0.3298,
      "step": 1323
    },
    {
      "epoch": 0.3009090909090909,
      "grad_norm": 0.05209232494235039,
      "learning_rate": 0.00013997724687144481,
      "loss": 0.3271,
      "step": 1324
    },
    {
      "epoch": 0.30113636363636365,
      "grad_norm": 0.04841476306319237,
      "learning_rate": 0.00013993174061433447,
      "loss": 0.3593,
      "step": 1325
    },
    {
      "epoch": 0.3013636363636364,
      "grad_norm": 0.05232228711247444,
      "learning_rate": 0.00013988623435722412,
      "loss": 0.3392,
      "step": 1326
    },
    {
      "epoch": 0.3015909090909091,
      "grad_norm": 0.04774969816207886,
      "learning_rate": 0.00013984072810011377,
      "loss": 0.2843,
      "step": 1327
    },
    {
      "epoch": 0.3018181818181818,
      "grad_norm": 0.039079684764146805,
      "learning_rate": 0.00013979522184300343,
      "loss": 0.2793,
      "step": 1328
    },
    {
      "epoch": 0.30204545454545456,
      "grad_norm": 0.04027460515499115,
      "learning_rate": 0.00013974971558589308,
      "loss": 0.2962,
      "step": 1329
    },
    {
      "epoch": 0.30227272727272725,
      "grad_norm": 0.05203616991639137,
      "learning_rate": 0.00013970420932878273,
      "loss": 0.3656,
      "step": 1330
    },
    {
      "epoch": 0.3025,
      "grad_norm": 0.0619187131524086,
      "learning_rate": 0.00013965870307167236,
      "loss": 0.381,
      "step": 1331
    },
    {
      "epoch": 0.30272727272727273,
      "grad_norm": 0.03973874822258949,
      "learning_rate": 0.000139613196814562,
      "loss": 0.3182,
      "step": 1332
    },
    {
      "epoch": 0.3029545454545455,
      "grad_norm": 0.04101702943444252,
      "learning_rate": 0.00013956769055745164,
      "loss": 0.3328,
      "step": 1333
    },
    {
      "epoch": 0.30318181818181816,
      "grad_norm": 0.03616252914071083,
      "learning_rate": 0.0001395221843003413,
      "loss": 0.3091,
      "step": 1334
    },
    {
      "epoch": 0.3034090909090909,
      "grad_norm": 0.05429059639573097,
      "learning_rate": 0.00013947667804323094,
      "loss": 0.3123,
      "step": 1335
    },
    {
      "epoch": 0.30363636363636365,
      "grad_norm": 0.04573073983192444,
      "learning_rate": 0.0001394311717861206,
      "loss": 0.3293,
      "step": 1336
    },
    {
      "epoch": 0.3038636363636364,
      "grad_norm": 0.044254228472709656,
      "learning_rate": 0.00013938566552901025,
      "loss": 0.2439,
      "step": 1337
    },
    {
      "epoch": 0.3040909090909091,
      "grad_norm": 0.05234650894999504,
      "learning_rate": 0.0001393401592718999,
      "loss": 0.3618,
      "step": 1338
    },
    {
      "epoch": 0.3043181818181818,
      "grad_norm": 0.040239181369543076,
      "learning_rate": 0.00013929465301478955,
      "loss": 0.2719,
      "step": 1339
    },
    {
      "epoch": 0.30454545454545456,
      "grad_norm": 0.04117975011467934,
      "learning_rate": 0.0001392491467576792,
      "loss": 0.3014,
      "step": 1340
    },
    {
      "epoch": 0.30477272727272725,
      "grad_norm": 0.06716938316822052,
      "learning_rate": 0.00013920364050056883,
      "loss": 0.3399,
      "step": 1341
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.056887973099946976,
      "learning_rate": 0.00013915813424345848,
      "loss": 0.3598,
      "step": 1342
    },
    {
      "epoch": 0.30522727272727274,
      "grad_norm": 0.04029204696416855,
      "learning_rate": 0.0001391126279863481,
      "loss": 0.321,
      "step": 1343
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 0.043911732733249664,
      "learning_rate": 0.00013906712172923776,
      "loss": 0.3432,
      "step": 1344
    },
    {
      "epoch": 0.30568181818181817,
      "grad_norm": 0.0640377327799797,
      "learning_rate": 0.00013902161547212741,
      "loss": 0.4346,
      "step": 1345
    },
    {
      "epoch": 0.3059090909090909,
      "grad_norm": 0.052597157657146454,
      "learning_rate": 0.00013897610921501707,
      "loss": 0.3503,
      "step": 1346
    },
    {
      "epoch": 0.30613636363636365,
      "grad_norm": 0.042717691510915756,
      "learning_rate": 0.00013893060295790672,
      "loss": 0.3333,
      "step": 1347
    },
    {
      "epoch": 0.30636363636363634,
      "grad_norm": 0.05153455212712288,
      "learning_rate": 0.00013888509670079637,
      "loss": 0.3442,
      "step": 1348
    },
    {
      "epoch": 0.3065909090909091,
      "grad_norm": 0.059743497520685196,
      "learning_rate": 0.00013883959044368603,
      "loss": 0.3205,
      "step": 1349
    },
    {
      "epoch": 0.3068181818181818,
      "grad_norm": 0.03926306962966919,
      "learning_rate": 0.00013879408418657568,
      "loss": 0.2958,
      "step": 1350
    },
    {
      "epoch": 0.30704545454545457,
      "grad_norm": 0.04305121302604675,
      "learning_rate": 0.0001387485779294653,
      "loss": 0.3564,
      "step": 1351
    },
    {
      "epoch": 0.30727272727272725,
      "grad_norm": 0.0485406331717968,
      "learning_rate": 0.00013870307167235496,
      "loss": 0.3208,
      "step": 1352
    },
    {
      "epoch": 0.3075,
      "grad_norm": 0.04845687001943588,
      "learning_rate": 0.00013865756541524458,
      "loss": 0.3261,
      "step": 1353
    },
    {
      "epoch": 0.30772727272727274,
      "grad_norm": 0.04887187108397484,
      "learning_rate": 0.00013861205915813424,
      "loss": 0.3109,
      "step": 1354
    },
    {
      "epoch": 0.3079545454545455,
      "grad_norm": 0.041136305779218674,
      "learning_rate": 0.0001385665529010239,
      "loss": 0.315,
      "step": 1355
    },
    {
      "epoch": 0.30818181818181817,
      "grad_norm": 0.04728426784276962,
      "learning_rate": 0.00013852104664391354,
      "loss": 0.3406,
      "step": 1356
    },
    {
      "epoch": 0.3084090909090909,
      "grad_norm": 0.047949112951755524,
      "learning_rate": 0.0001384755403868032,
      "loss": 0.3339,
      "step": 1357
    },
    {
      "epoch": 0.30863636363636365,
      "grad_norm": 0.040480755269527435,
      "learning_rate": 0.00013843003412969285,
      "loss": 0.3193,
      "step": 1358
    },
    {
      "epoch": 0.30886363636363634,
      "grad_norm": 0.05163423344492912,
      "learning_rate": 0.0001383845278725825,
      "loss": 0.367,
      "step": 1359
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.047682471573352814,
      "learning_rate": 0.00013833902161547215,
      "loss": 0.2939,
      "step": 1360
    },
    {
      "epoch": 0.3093181818181818,
      "grad_norm": 0.05391494557261467,
      "learning_rate": 0.00013829351535836178,
      "loss": 0.3515,
      "step": 1361
    },
    {
      "epoch": 0.30954545454545457,
      "grad_norm": 0.048269499093294144,
      "learning_rate": 0.00013824800910125143,
      "loss": 0.3504,
      "step": 1362
    },
    {
      "epoch": 0.30977272727272726,
      "grad_norm": 0.04825950041413307,
      "learning_rate": 0.00013820250284414106,
      "loss": 0.332,
      "step": 1363
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.047047875821590424,
      "learning_rate": 0.0001381569965870307,
      "loss": 0.3385,
      "step": 1364
    },
    {
      "epoch": 0.31022727272727274,
      "grad_norm": 0.047247257083654404,
      "learning_rate": 0.00013811149032992036,
      "loss": 0.391,
      "step": 1365
    },
    {
      "epoch": 0.3104545454545454,
      "grad_norm": 0.05212361365556717,
      "learning_rate": 0.00013806598407281002,
      "loss": 0.3345,
      "step": 1366
    },
    {
      "epoch": 0.31068181818181817,
      "grad_norm": 0.06347082555294037,
      "learning_rate": 0.00013802047781569967,
      "loss": 0.4127,
      "step": 1367
    },
    {
      "epoch": 0.3109090909090909,
      "grad_norm": 0.0456659197807312,
      "learning_rate": 0.00013797497155858932,
      "loss": 0.3244,
      "step": 1368
    },
    {
      "epoch": 0.31113636363636366,
      "grad_norm": 0.042937327176332474,
      "learning_rate": 0.00013792946530147897,
      "loss": 0.3396,
      "step": 1369
    },
    {
      "epoch": 0.31136363636363634,
      "grad_norm": 0.06032780557870865,
      "learning_rate": 0.00013788395904436863,
      "loss": 0.3592,
      "step": 1370
    },
    {
      "epoch": 0.3115909090909091,
      "grad_norm": 0.05086006969213486,
      "learning_rate": 0.00013783845278725825,
      "loss": 0.3556,
      "step": 1371
    },
    {
      "epoch": 0.3118181818181818,
      "grad_norm": 0.0409012995660305,
      "learning_rate": 0.0001377929465301479,
      "loss": 0.3341,
      "step": 1372
    },
    {
      "epoch": 0.31204545454545457,
      "grad_norm": 0.04219282045960426,
      "learning_rate": 0.00013774744027303753,
      "loss": 0.284,
      "step": 1373
    },
    {
      "epoch": 0.31227272727272726,
      "grad_norm": 0.040268756449222565,
      "learning_rate": 0.00013770193401592718,
      "loss": 0.319,
      "step": 1374
    },
    {
      "epoch": 0.3125,
      "grad_norm": 0.048524159938097,
      "learning_rate": 0.00013765642775881684,
      "loss": 0.3127,
      "step": 1375
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.055633559823036194,
      "learning_rate": 0.0001376109215017065,
      "loss": 0.3808,
      "step": 1376
    },
    {
      "epoch": 0.31295454545454543,
      "grad_norm": 0.0552314929664135,
      "learning_rate": 0.00013756541524459614,
      "loss": 0.3716,
      "step": 1377
    },
    {
      "epoch": 0.3131818181818182,
      "grad_norm": 0.048245660960674286,
      "learning_rate": 0.0001375199089874858,
      "loss": 0.3741,
      "step": 1378
    },
    {
      "epoch": 0.3134090909090909,
      "grad_norm": 0.06173437088727951,
      "learning_rate": 0.00013747440273037545,
      "loss": 0.3604,
      "step": 1379
    },
    {
      "epoch": 0.31363636363636366,
      "grad_norm": 0.04480734467506409,
      "learning_rate": 0.0001374288964732651,
      "loss": 0.3453,
      "step": 1380
    },
    {
      "epoch": 0.31386363636363634,
      "grad_norm": 0.05253433808684349,
      "learning_rate": 0.00013738339021615473,
      "loss": 0.3305,
      "step": 1381
    },
    {
      "epoch": 0.3140909090909091,
      "grad_norm": 0.040674254298210144,
      "learning_rate": 0.00013733788395904438,
      "loss": 0.3071,
      "step": 1382
    },
    {
      "epoch": 0.31431818181818183,
      "grad_norm": 0.04234464466571808,
      "learning_rate": 0.000137292377701934,
      "loss": 0.3311,
      "step": 1383
    },
    {
      "epoch": 0.3145454545454546,
      "grad_norm": 0.05974142625927925,
      "learning_rate": 0.00013724687144482366,
      "loss": 0.3349,
      "step": 1384
    },
    {
      "epoch": 0.31477272727272726,
      "grad_norm": 0.049749087542295456,
      "learning_rate": 0.0001372013651877133,
      "loss": 0.3492,
      "step": 1385
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.057627562433481216,
      "learning_rate": 0.00013715585893060296,
      "loss": 0.3631,
      "step": 1386
    },
    {
      "epoch": 0.31522727272727274,
      "grad_norm": 0.07339830696582794,
      "learning_rate": 0.00013711035267349262,
      "loss": 0.4343,
      "step": 1387
    },
    {
      "epoch": 0.31545454545454543,
      "grad_norm": 0.04207697510719299,
      "learning_rate": 0.00013706484641638227,
      "loss": 0.3066,
      "step": 1388
    },
    {
      "epoch": 0.3156818181818182,
      "grad_norm": 0.04021163657307625,
      "learning_rate": 0.00013701934015927192,
      "loss": 0.3085,
      "step": 1389
    },
    {
      "epoch": 0.3159090909090909,
      "grad_norm": 0.051045652478933334,
      "learning_rate": 0.00013697383390216157,
      "loss": 0.3338,
      "step": 1390
    },
    {
      "epoch": 0.31613636363636366,
      "grad_norm": 0.05129280686378479,
      "learning_rate": 0.0001369283276450512,
      "loss": 0.368,
      "step": 1391
    },
    {
      "epoch": 0.31636363636363635,
      "grad_norm": 0.043576519936323166,
      "learning_rate": 0.00013688282138794085,
      "loss": 0.3356,
      "step": 1392
    },
    {
      "epoch": 0.3165909090909091,
      "grad_norm": 0.03732229396700859,
      "learning_rate": 0.00013683731513083048,
      "loss": 0.2883,
      "step": 1393
    },
    {
      "epoch": 0.31681818181818183,
      "grad_norm": 0.03554561734199524,
      "learning_rate": 0.00013679180887372013,
      "loss": 0.2796,
      "step": 1394
    },
    {
      "epoch": 0.3170454545454545,
      "grad_norm": 0.04059968143701553,
      "learning_rate": 0.00013674630261660978,
      "loss": 0.3639,
      "step": 1395
    },
    {
      "epoch": 0.31727272727272726,
      "grad_norm": 0.036209918558597565,
      "learning_rate": 0.00013670079635949944,
      "loss": 0.2833,
      "step": 1396
    },
    {
      "epoch": 0.3175,
      "grad_norm": 0.04259771108627319,
      "learning_rate": 0.0001366552901023891,
      "loss": 0.3075,
      "step": 1397
    },
    {
      "epoch": 0.31772727272727275,
      "grad_norm": 0.04181980341672897,
      "learning_rate": 0.00013660978384527874,
      "loss": 0.3156,
      "step": 1398
    },
    {
      "epoch": 0.31795454545454543,
      "grad_norm": 0.059392381459474564,
      "learning_rate": 0.0001365642775881684,
      "loss": 0.4006,
      "step": 1399
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.06096465513110161,
      "learning_rate": 0.00013651877133105805,
      "loss": 0.373,
      "step": 1400
    },
    {
      "epoch": 0.3181818181818182,
      "eval_loss": 0.32971814274787903,
      "eval_runtime": 229.338,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 1400
    },
    {
      "epoch": 0.3184090909090909,
      "grad_norm": 0.046068813651800156,
      "learning_rate": 0.00013647326507394767,
      "loss": 0.3466,
      "step": 1401
    },
    {
      "epoch": 0.31863636363636366,
      "grad_norm": 0.05155574902892113,
      "learning_rate": 0.00013642775881683733,
      "loss": 0.3097,
      "step": 1402
    },
    {
      "epoch": 0.31886363636363635,
      "grad_norm": 0.04614706337451935,
      "learning_rate": 0.00013638225255972695,
      "loss": 0.3293,
      "step": 1403
    },
    {
      "epoch": 0.3190909090909091,
      "grad_norm": 0.047738492488861084,
      "learning_rate": 0.0001363367463026166,
      "loss": 0.3697,
      "step": 1404
    },
    {
      "epoch": 0.31931818181818183,
      "grad_norm": 0.04379991069436073,
      "learning_rate": 0.00013629124004550626,
      "loss": 0.3315,
      "step": 1405
    },
    {
      "epoch": 0.3195454545454545,
      "grad_norm": 0.04501351714134216,
      "learning_rate": 0.0001362457337883959,
      "loss": 0.2527,
      "step": 1406
    },
    {
      "epoch": 0.31977272727272726,
      "grad_norm": 0.051076244562864304,
      "learning_rate": 0.00013620022753128556,
      "loss": 0.3409,
      "step": 1407
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05227390676736832,
      "learning_rate": 0.00013615472127417522,
      "loss": 0.3397,
      "step": 1408
    },
    {
      "epoch": 0.32022727272727275,
      "grad_norm": 0.055505216121673584,
      "learning_rate": 0.00013610921501706487,
      "loss": 0.3793,
      "step": 1409
    },
    {
      "epoch": 0.32045454545454544,
      "grad_norm": 0.03542032465338707,
      "learning_rate": 0.00013606370875995452,
      "loss": 0.269,
      "step": 1410
    },
    {
      "epoch": 0.3206818181818182,
      "grad_norm": 0.05164864659309387,
      "learning_rate": 0.00013601820250284415,
      "loss": 0.3953,
      "step": 1411
    },
    {
      "epoch": 0.3209090909090909,
      "grad_norm": 0.04779777303338051,
      "learning_rate": 0.0001359726962457338,
      "loss": 0.3479,
      "step": 1412
    },
    {
      "epoch": 0.3211363636363636,
      "grad_norm": 0.047745876014232635,
      "learning_rate": 0.00013592718998862343,
      "loss": 0.3624,
      "step": 1413
    },
    {
      "epoch": 0.32136363636363635,
      "grad_norm": 0.04514792561531067,
      "learning_rate": 0.00013588168373151308,
      "loss": 0.3017,
      "step": 1414
    },
    {
      "epoch": 0.3215909090909091,
      "grad_norm": 0.060877151787281036,
      "learning_rate": 0.00013583617747440273,
      "loss": 0.405,
      "step": 1415
    },
    {
      "epoch": 0.32181818181818184,
      "grad_norm": 0.06000758334994316,
      "learning_rate": 0.00013579067121729239,
      "loss": 0.4336,
      "step": 1416
    },
    {
      "epoch": 0.3220454545454545,
      "grad_norm": 0.03934982419013977,
      "learning_rate": 0.00013574516496018204,
      "loss": 0.2861,
      "step": 1417
    },
    {
      "epoch": 0.32227272727272727,
      "grad_norm": 0.04169408604502678,
      "learning_rate": 0.0001356996587030717,
      "loss": 0.3269,
      "step": 1418
    },
    {
      "epoch": 0.3225,
      "grad_norm": 0.0720454528927803,
      "learning_rate": 0.00013565415244596134,
      "loss": 0.4314,
      "step": 1419
    },
    {
      "epoch": 0.32272727272727275,
      "grad_norm": 0.0435335710644722,
      "learning_rate": 0.00013560864618885097,
      "loss": 0.3198,
      "step": 1420
    },
    {
      "epoch": 0.32295454545454544,
      "grad_norm": 0.05036294460296631,
      "learning_rate": 0.00013556313993174062,
      "loss": 0.3355,
      "step": 1421
    },
    {
      "epoch": 0.3231818181818182,
      "grad_norm": 0.06113824248313904,
      "learning_rate": 0.00013551763367463028,
      "loss": 0.3508,
      "step": 1422
    },
    {
      "epoch": 0.3234090909090909,
      "grad_norm": 0.055914077907800674,
      "learning_rate": 0.0001354721274175199,
      "loss": 0.3499,
      "step": 1423
    },
    {
      "epoch": 0.3236363636363636,
      "grad_norm": 0.03983946517109871,
      "learning_rate": 0.00013542662116040955,
      "loss": 0.287,
      "step": 1424
    },
    {
      "epoch": 0.32386363636363635,
      "grad_norm": 0.04727116972208023,
      "learning_rate": 0.0001353811149032992,
      "loss": 0.3357,
      "step": 1425
    },
    {
      "epoch": 0.3240909090909091,
      "grad_norm": 0.05184141546487808,
      "learning_rate": 0.00013533560864618886,
      "loss": 0.3256,
      "step": 1426
    },
    {
      "epoch": 0.32431818181818184,
      "grad_norm": 0.05747624486684799,
      "learning_rate": 0.0001352901023890785,
      "loss": 0.3964,
      "step": 1427
    },
    {
      "epoch": 0.3245454545454545,
      "grad_norm": 0.054889094084501266,
      "learning_rate": 0.00013524459613196817,
      "loss": 0.3818,
      "step": 1428
    },
    {
      "epoch": 0.32477272727272727,
      "grad_norm": 0.05991828441619873,
      "learning_rate": 0.00013519908987485782,
      "loss": 0.3723,
      "step": 1429
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.048181917518377304,
      "learning_rate": 0.00013515358361774744,
      "loss": 0.3458,
      "step": 1430
    },
    {
      "epoch": 0.32522727272727275,
      "grad_norm": 0.05707196146249771,
      "learning_rate": 0.0001351080773606371,
      "loss": 0.3903,
      "step": 1431
    },
    {
      "epoch": 0.32545454545454544,
      "grad_norm": 0.0525994636118412,
      "learning_rate": 0.00013506257110352675,
      "loss": 0.364,
      "step": 1432
    },
    {
      "epoch": 0.3256818181818182,
      "grad_norm": 0.046676233410835266,
      "learning_rate": 0.00013501706484641638,
      "loss": 0.3581,
      "step": 1433
    },
    {
      "epoch": 0.3259090909090909,
      "grad_norm": 0.04936059191823006,
      "learning_rate": 0.00013497155858930603,
      "loss": 0.3487,
      "step": 1434
    },
    {
      "epoch": 0.3261363636363636,
      "grad_norm": 0.05137406662106514,
      "learning_rate": 0.00013492605233219568,
      "loss": 0.3408,
      "step": 1435
    },
    {
      "epoch": 0.32636363636363636,
      "grad_norm": 0.04484165459871292,
      "learning_rate": 0.00013488054607508533,
      "loss": 0.2801,
      "step": 1436
    },
    {
      "epoch": 0.3265909090909091,
      "grad_norm": 0.05055408552289009,
      "learning_rate": 0.00013483503981797499,
      "loss": 0.3708,
      "step": 1437
    },
    {
      "epoch": 0.32681818181818184,
      "grad_norm": 0.04007905721664429,
      "learning_rate": 0.00013478953356086464,
      "loss": 0.2918,
      "step": 1438
    },
    {
      "epoch": 0.32704545454545453,
      "grad_norm": 0.05486602336168289,
      "learning_rate": 0.0001347440273037543,
      "loss": 0.3343,
      "step": 1439
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.03401597961783409,
      "learning_rate": 0.00013469852104664392,
      "loss": 0.2697,
      "step": 1440
    },
    {
      "epoch": 0.3275,
      "grad_norm": 0.038727451115846634,
      "learning_rate": 0.00013465301478953357,
      "loss": 0.3068,
      "step": 1441
    },
    {
      "epoch": 0.3277272727272727,
      "grad_norm": 0.038824670016765594,
      "learning_rate": 0.00013460750853242322,
      "loss": 0.3565,
      "step": 1442
    },
    {
      "epoch": 0.32795454545454544,
      "grad_norm": 0.039456989616155624,
      "learning_rate": 0.00013456200227531285,
      "loss": 0.2667,
      "step": 1443
    },
    {
      "epoch": 0.3281818181818182,
      "grad_norm": 0.04146895930171013,
      "learning_rate": 0.0001345164960182025,
      "loss": 0.3084,
      "step": 1444
    },
    {
      "epoch": 0.32840909090909093,
      "grad_norm": 0.05581154674291611,
      "learning_rate": 0.00013447098976109215,
      "loss": 0.4037,
      "step": 1445
    },
    {
      "epoch": 0.3286363636363636,
      "grad_norm": 0.053247593343257904,
      "learning_rate": 0.0001344254835039818,
      "loss": 0.3129,
      "step": 1446
    },
    {
      "epoch": 0.32886363636363636,
      "grad_norm": 0.049694642424583435,
      "learning_rate": 0.00013437997724687146,
      "loss": 0.3975,
      "step": 1447
    },
    {
      "epoch": 0.3290909090909091,
      "grad_norm": 0.04653910920023918,
      "learning_rate": 0.0001343344709897611,
      "loss": 0.3182,
      "step": 1448
    },
    {
      "epoch": 0.32931818181818184,
      "grad_norm": 0.05268602445721626,
      "learning_rate": 0.00013428896473265074,
      "loss": 0.3457,
      "step": 1449
    },
    {
      "epoch": 0.32954545454545453,
      "grad_norm": 0.05995626747608185,
      "learning_rate": 0.0001342434584755404,
      "loss": 0.391,
      "step": 1450
    },
    {
      "epoch": 0.3297727272727273,
      "grad_norm": 0.046224500983953476,
      "learning_rate": 0.00013419795221843004,
      "loss": 0.3406,
      "step": 1451
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.03934966400265694,
      "learning_rate": 0.00013415244596131967,
      "loss": 0.2627,
      "step": 1452
    },
    {
      "epoch": 0.3302272727272727,
      "grad_norm": 0.040346574038267136,
      "learning_rate": 0.00013410693970420932,
      "loss": 0.311,
      "step": 1453
    },
    {
      "epoch": 0.33045454545454545,
      "grad_norm": 0.04980709031224251,
      "learning_rate": 0.00013406143344709898,
      "loss": 0.3308,
      "step": 1454
    },
    {
      "epoch": 0.3306818181818182,
      "grad_norm": 0.05047526955604553,
      "learning_rate": 0.00013401592718998863,
      "loss": 0.3697,
      "step": 1455
    },
    {
      "epoch": 0.33090909090909093,
      "grad_norm": 0.04772527888417244,
      "learning_rate": 0.00013397042093287828,
      "loss": 0.3412,
      "step": 1456
    },
    {
      "epoch": 0.3311363636363636,
      "grad_norm": 0.045175209641456604,
      "learning_rate": 0.00013392491467576793,
      "loss": 0.3293,
      "step": 1457
    },
    {
      "epoch": 0.33136363636363636,
      "grad_norm": 0.04065091162919998,
      "learning_rate": 0.0001338794084186576,
      "loss": 0.338,
      "step": 1458
    },
    {
      "epoch": 0.3315909090909091,
      "grad_norm": 0.04153355956077576,
      "learning_rate": 0.0001338339021615472,
      "loss": 0.3118,
      "step": 1459
    },
    {
      "epoch": 0.33181818181818185,
      "grad_norm": 0.04435184597969055,
      "learning_rate": 0.00013378839590443687,
      "loss": 0.2888,
      "step": 1460
    },
    {
      "epoch": 0.33204545454545453,
      "grad_norm": 0.056729309260845184,
      "learning_rate": 0.00013374288964732652,
      "loss": 0.2939,
      "step": 1461
    },
    {
      "epoch": 0.3322727272727273,
      "grad_norm": 0.06953783333301544,
      "learning_rate": 0.00013369738339021614,
      "loss": 0.4023,
      "step": 1462
    },
    {
      "epoch": 0.3325,
      "grad_norm": 0.05660589039325714,
      "learning_rate": 0.0001336518771331058,
      "loss": 0.3388,
      "step": 1463
    },
    {
      "epoch": 0.3327272727272727,
      "grad_norm": 0.05612346529960632,
      "learning_rate": 0.00013360637087599545,
      "loss": 0.3611,
      "step": 1464
    },
    {
      "epoch": 0.33295454545454545,
      "grad_norm": 0.04588257521390915,
      "learning_rate": 0.0001335608646188851,
      "loss": 0.311,
      "step": 1465
    },
    {
      "epoch": 0.3331818181818182,
      "grad_norm": 0.04790062829852104,
      "learning_rate": 0.00013351535836177476,
      "loss": 0.3574,
      "step": 1466
    },
    {
      "epoch": 0.33340909090909093,
      "grad_norm": 0.05157661810517311,
      "learning_rate": 0.0001334698521046644,
      "loss": 0.3319,
      "step": 1467
    },
    {
      "epoch": 0.3336363636363636,
      "grad_norm": 0.04269709810614586,
      "learning_rate": 0.00013342434584755406,
      "loss": 0.2929,
      "step": 1468
    },
    {
      "epoch": 0.33386363636363636,
      "grad_norm": 0.05292622372508049,
      "learning_rate": 0.0001333788395904437,
      "loss": 0.3773,
      "step": 1469
    },
    {
      "epoch": 0.3340909090909091,
      "grad_norm": 0.053467005491256714,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.33,
      "step": 1470
    },
    {
      "epoch": 0.3343181818181818,
      "grad_norm": 0.04477321729063988,
      "learning_rate": 0.000133287827076223,
      "loss": 0.3764,
      "step": 1471
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 0.0491141676902771,
      "learning_rate": 0.00013324232081911262,
      "loss": 0.3472,
      "step": 1472
    },
    {
      "epoch": 0.3347727272727273,
      "grad_norm": 0.042974743992090225,
      "learning_rate": 0.00013319681456200227,
      "loss": 0.3325,
      "step": 1473
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.041139714419841766,
      "learning_rate": 0.00013315130830489192,
      "loss": 0.3296,
      "step": 1474
    },
    {
      "epoch": 0.3352272727272727,
      "grad_norm": 0.045341163873672485,
      "learning_rate": 0.00013310580204778158,
      "loss": 0.3527,
      "step": 1475
    },
    {
      "epoch": 0.33545454545454545,
      "grad_norm": 0.03984177112579346,
      "learning_rate": 0.00013306029579067123,
      "loss": 0.2965,
      "step": 1476
    },
    {
      "epoch": 0.3356818181818182,
      "grad_norm": 0.0474960058927536,
      "learning_rate": 0.00013301478953356088,
      "loss": 0.3129,
      "step": 1477
    },
    {
      "epoch": 0.33590909090909093,
      "grad_norm": 0.04492652788758278,
      "learning_rate": 0.00013296928327645054,
      "loss": 0.3469,
      "step": 1478
    },
    {
      "epoch": 0.3361363636363636,
      "grad_norm": 0.04584619030356407,
      "learning_rate": 0.00013292377701934016,
      "loss": 0.3091,
      "step": 1479
    },
    {
      "epoch": 0.33636363636363636,
      "grad_norm": 0.06457406282424927,
      "learning_rate": 0.00013287827076222981,
      "loss": 0.346,
      "step": 1480
    },
    {
      "epoch": 0.3365909090909091,
      "grad_norm": 0.044650208204984665,
      "learning_rate": 0.00013283276450511947,
      "loss": 0.3394,
      "step": 1481
    },
    {
      "epoch": 0.3368181818181818,
      "grad_norm": 0.06513964384794235,
      "learning_rate": 0.0001327872582480091,
      "loss": 0.4257,
      "step": 1482
    },
    {
      "epoch": 0.33704545454545454,
      "grad_norm": 0.0565243735909462,
      "learning_rate": 0.00013274175199089875,
      "loss": 0.3672,
      "step": 1483
    },
    {
      "epoch": 0.3372727272727273,
      "grad_norm": 0.03692331537604332,
      "learning_rate": 0.0001326962457337884,
      "loss": 0.2144,
      "step": 1484
    },
    {
      "epoch": 0.3375,
      "grad_norm": 0.045292455703020096,
      "learning_rate": 0.00013265073947667805,
      "loss": 0.3362,
      "step": 1485
    },
    {
      "epoch": 0.3377272727272727,
      "grad_norm": 0.04202472046017647,
      "learning_rate": 0.0001326052332195677,
      "loss": 0.3187,
      "step": 1486
    },
    {
      "epoch": 0.33795454545454545,
      "grad_norm": 0.05235357955098152,
      "learning_rate": 0.00013255972696245736,
      "loss": 0.2995,
      "step": 1487
    },
    {
      "epoch": 0.3381818181818182,
      "grad_norm": 0.0394180491566658,
      "learning_rate": 0.00013251422070534698,
      "loss": 0.2978,
      "step": 1488
    },
    {
      "epoch": 0.3384090909090909,
      "grad_norm": 0.0392129048705101,
      "learning_rate": 0.00013246871444823663,
      "loss": 0.3293,
      "step": 1489
    },
    {
      "epoch": 0.3386363636363636,
      "grad_norm": 0.045842476189136505,
      "learning_rate": 0.0001324232081911263,
      "loss": 0.3102,
      "step": 1490
    },
    {
      "epoch": 0.33886363636363637,
      "grad_norm": 0.0496147945523262,
      "learning_rate": 0.00013237770193401594,
      "loss": 0.3186,
      "step": 1491
    },
    {
      "epoch": 0.3390909090909091,
      "grad_norm": 0.05277946963906288,
      "learning_rate": 0.00013233219567690557,
      "loss": 0.3334,
      "step": 1492
    },
    {
      "epoch": 0.3393181818181818,
      "grad_norm": 0.03587063401937485,
      "learning_rate": 0.00013228668941979522,
      "loss": 0.2667,
      "step": 1493
    },
    {
      "epoch": 0.33954545454545454,
      "grad_norm": 0.06695375591516495,
      "learning_rate": 0.00013224118316268487,
      "loss": 0.3715,
      "step": 1494
    },
    {
      "epoch": 0.3397727272727273,
      "grad_norm": 0.037383947521448135,
      "learning_rate": 0.00013219567690557452,
      "loss": 0.23,
      "step": 1495
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.0404624305665493,
      "learning_rate": 0.00013215017064846418,
      "loss": 0.3105,
      "step": 1496
    },
    {
      "epoch": 0.3402272727272727,
      "grad_norm": 0.03645749390125275,
      "learning_rate": 0.00013210466439135383,
      "loss": 0.2983,
      "step": 1497
    },
    {
      "epoch": 0.34045454545454545,
      "grad_norm": 0.05780673772096634,
      "learning_rate": 0.00013205915813424346,
      "loss": 0.3149,
      "step": 1498
    },
    {
      "epoch": 0.3406818181818182,
      "grad_norm": 0.03889412805438042,
      "learning_rate": 0.0001320136518771331,
      "loss": 0.3151,
      "step": 1499
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.04475686699151993,
      "learning_rate": 0.00013196814562002276,
      "loss": 0.3364,
      "step": 1500
    },
    {
      "epoch": 0.3409090909090909,
      "eval_loss": 0.3294380307197571,
      "eval_runtime": 229.3716,
      "eval_samples_per_second": 0.436,
      "eval_steps_per_second": 0.436,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 4400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.485203250406408e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
