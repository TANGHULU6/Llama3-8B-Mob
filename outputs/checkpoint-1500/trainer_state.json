{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4411764705882353,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002941176470588235,
      "grad_norm": 2.1835074424743652,
      "learning_rate": 4e-05,
      "loss": 2.0034,
      "step": 1
    },
    {
      "epoch": 0.000588235294117647,
      "grad_norm": 2.1066360473632812,
      "learning_rate": 8e-05,
      "loss": 2.1112,
      "step": 2
    },
    {
      "epoch": 0.0008823529411764706,
      "grad_norm": 2.020095109939575,
      "learning_rate": 0.00012,
      "loss": 2.0412,
      "step": 3
    },
    {
      "epoch": 0.001176470588235294,
      "grad_norm": 1.8769174814224243,
      "learning_rate": 0.00016,
      "loss": 2.0435,
      "step": 4
    },
    {
      "epoch": 0.0014705882352941176,
      "grad_norm": 1.5894464254379272,
      "learning_rate": 0.0002,
      "loss": 1.6745,
      "step": 5
    },
    {
      "epoch": 0.0017647058823529412,
      "grad_norm": 2.2829883098602295,
      "learning_rate": 0.00019994108983799707,
      "loss": 1.2758,
      "step": 6
    },
    {
      "epoch": 0.002058823529411765,
      "grad_norm": 1.3070324659347534,
      "learning_rate": 0.00019988217967599413,
      "loss": 1.2102,
      "step": 7
    },
    {
      "epoch": 0.002352941176470588,
      "grad_norm": 1.2940396070480347,
      "learning_rate": 0.00019982326951399116,
      "loss": 1.198,
      "step": 8
    },
    {
      "epoch": 0.0026470588235294116,
      "grad_norm": 1.2325717210769653,
      "learning_rate": 0.00019976435935198822,
      "loss": 1.0959,
      "step": 9
    },
    {
      "epoch": 0.0029411764705882353,
      "grad_norm": 1.2202820777893066,
      "learning_rate": 0.00019970544918998528,
      "loss": 0.998,
      "step": 10
    },
    {
      "epoch": 0.003235294117647059,
      "grad_norm": 2.3062727451324463,
      "learning_rate": 0.00019964653902798234,
      "loss": 0.7899,
      "step": 11
    },
    {
      "epoch": 0.0035294117647058825,
      "grad_norm": 1.056572437286377,
      "learning_rate": 0.0001995876288659794,
      "loss": 0.7102,
      "step": 12
    },
    {
      "epoch": 0.003823529411764706,
      "grad_norm": 0.9499183297157288,
      "learning_rate": 0.00019952871870397644,
      "loss": 0.8086,
      "step": 13
    },
    {
      "epoch": 0.00411764705882353,
      "grad_norm": 0.5703122615814209,
      "learning_rate": 0.0001994698085419735,
      "loss": 0.5728,
      "step": 14
    },
    {
      "epoch": 0.004411764705882353,
      "grad_norm": 0.4960967004299164,
      "learning_rate": 0.00019941089837997056,
      "loss": 0.5901,
      "step": 15
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 0.4866180121898651,
      "learning_rate": 0.00019935198821796762,
      "loss": 0.63,
      "step": 16
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.471566766500473,
      "learning_rate": 0.00019929307805596468,
      "loss": 0.6949,
      "step": 17
    },
    {
      "epoch": 0.005294117647058823,
      "grad_norm": 0.3700290620326996,
      "learning_rate": 0.0001992341678939617,
      "loss": 0.6886,
      "step": 18
    },
    {
      "epoch": 0.005588235294117647,
      "grad_norm": 0.252212792634964,
      "learning_rate": 0.00019917525773195877,
      "loss": 0.4542,
      "step": 19
    },
    {
      "epoch": 0.0058823529411764705,
      "grad_norm": 0.2795506417751312,
      "learning_rate": 0.00019911634756995583,
      "loss": 0.5234,
      "step": 20
    },
    {
      "epoch": 0.006176470588235294,
      "grad_norm": 0.23773442208766937,
      "learning_rate": 0.0001990574374079529,
      "loss": 0.5363,
      "step": 21
    },
    {
      "epoch": 0.006470588235294118,
      "grad_norm": 0.17992019653320312,
      "learning_rate": 0.00019899852724594995,
      "loss": 0.44,
      "step": 22
    },
    {
      "epoch": 0.006764705882352941,
      "grad_norm": 0.23571519553661346,
      "learning_rate": 0.00019893961708394698,
      "loss": 0.5706,
      "step": 23
    },
    {
      "epoch": 0.007058823529411765,
      "grad_norm": 0.14562827348709106,
      "learning_rate": 0.00019888070692194404,
      "loss": 0.4329,
      "step": 24
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 0.29210659861564636,
      "learning_rate": 0.0001988217967599411,
      "loss": 0.5207,
      "step": 25
    },
    {
      "epoch": 0.007647058823529412,
      "grad_norm": 0.2006864696741104,
      "learning_rate": 0.00019876288659793816,
      "loss": 0.5589,
      "step": 26
    },
    {
      "epoch": 0.007941176470588234,
      "grad_norm": 0.12910480797290802,
      "learning_rate": 0.00019870397643593522,
      "loss": 0.4613,
      "step": 27
    },
    {
      "epoch": 0.00823529411764706,
      "grad_norm": 0.18537572026252747,
      "learning_rate": 0.00019864506627393226,
      "loss": 0.4548,
      "step": 28
    },
    {
      "epoch": 0.008529411764705883,
      "grad_norm": 0.18503913283348083,
      "learning_rate": 0.00019858615611192932,
      "loss": 0.4766,
      "step": 29
    },
    {
      "epoch": 0.008823529411764706,
      "grad_norm": 0.17886650562286377,
      "learning_rate": 0.00019852724594992638,
      "loss": 0.3675,
      "step": 30
    },
    {
      "epoch": 0.009117647058823529,
      "grad_norm": 0.1789274662733078,
      "learning_rate": 0.00019846833578792344,
      "loss": 0.3992,
      "step": 31
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 0.16663184762001038,
      "learning_rate": 0.0001984094256259205,
      "loss": 0.4142,
      "step": 32
    },
    {
      "epoch": 0.009705882352941177,
      "grad_norm": 0.19921065866947174,
      "learning_rate": 0.00019835051546391753,
      "loss": 0.5053,
      "step": 33
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1876063495874405,
      "learning_rate": 0.0001982916053019146,
      "loss": 0.477,
      "step": 34
    },
    {
      "epoch": 0.010294117647058823,
      "grad_norm": 0.18909965455532074,
      "learning_rate": 0.00019823269513991165,
      "loss": 0.3618,
      "step": 35
    },
    {
      "epoch": 0.010588235294117647,
      "grad_norm": 0.12914979457855225,
      "learning_rate": 0.0001981737849779087,
      "loss": 0.4915,
      "step": 36
    },
    {
      "epoch": 0.01088235294117647,
      "grad_norm": 0.14478152990341187,
      "learning_rate": 0.00019811487481590577,
      "loss": 0.3616,
      "step": 37
    },
    {
      "epoch": 0.011176470588235295,
      "grad_norm": 0.174483984708786,
      "learning_rate": 0.0001980559646539028,
      "loss": 0.4842,
      "step": 38
    },
    {
      "epoch": 0.011470588235294118,
      "grad_norm": 0.09472692012786865,
      "learning_rate": 0.00019799705449189987,
      "loss": 0.2295,
      "step": 39
    },
    {
      "epoch": 0.011764705882352941,
      "grad_norm": 0.11488152295351028,
      "learning_rate": 0.00019793814432989693,
      "loss": 0.3744,
      "step": 40
    },
    {
      "epoch": 0.012058823529411764,
      "grad_norm": 0.13654370605945587,
      "learning_rate": 0.00019787923416789399,
      "loss": 0.4129,
      "step": 41
    },
    {
      "epoch": 0.012352941176470587,
      "grad_norm": 0.08820246160030365,
      "learning_rate": 0.00019782032400589105,
      "loss": 0.3549,
      "step": 42
    },
    {
      "epoch": 0.012647058823529412,
      "grad_norm": 0.15094943344593048,
      "learning_rate": 0.00019776141384388808,
      "loss": 0.4397,
      "step": 43
    },
    {
      "epoch": 0.012941176470588235,
      "grad_norm": 0.10602625459432602,
      "learning_rate": 0.00019770250368188514,
      "loss": 0.3604,
      "step": 44
    },
    {
      "epoch": 0.013235294117647059,
      "grad_norm": 0.0990748330950737,
      "learning_rate": 0.0001976435935198822,
      "loss": 0.4773,
      "step": 45
    },
    {
      "epoch": 0.013529411764705882,
      "grad_norm": 0.10455132275819778,
      "learning_rate": 0.00019758468335787926,
      "loss": 0.3782,
      "step": 46
    },
    {
      "epoch": 0.013823529411764707,
      "grad_norm": 0.10086803883314133,
      "learning_rate": 0.00019752577319587632,
      "loss": 0.4278,
      "step": 47
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 0.11410357058048248,
      "learning_rate": 0.00019746686303387335,
      "loss": 0.4953,
      "step": 48
    },
    {
      "epoch": 0.014411764705882353,
      "grad_norm": 0.08383175730705261,
      "learning_rate": 0.0001974079528718704,
      "loss": 0.4163,
      "step": 49
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 0.09381704777479172,
      "learning_rate": 0.00019734904270986747,
      "loss": 0.4508,
      "step": 50
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.08147246390581131,
      "learning_rate": 0.00019729013254786453,
      "loss": 0.3236,
      "step": 51
    },
    {
      "epoch": 0.015294117647058824,
      "grad_norm": 0.10762112587690353,
      "learning_rate": 0.0001972312223858616,
      "loss": 0.3602,
      "step": 52
    },
    {
      "epoch": 0.015588235294117648,
      "grad_norm": 0.10376492887735367,
      "learning_rate": 0.00019717231222385863,
      "loss": 0.4145,
      "step": 53
    },
    {
      "epoch": 0.01588235294117647,
      "grad_norm": 0.07615689188241959,
      "learning_rate": 0.0001971134020618557,
      "loss": 0.337,
      "step": 54
    },
    {
      "epoch": 0.016176470588235296,
      "grad_norm": 0.09692107886075974,
      "learning_rate": 0.00019705449189985275,
      "loss": 0.4248,
      "step": 55
    },
    {
      "epoch": 0.01647058823529412,
      "grad_norm": 0.0790897086262703,
      "learning_rate": 0.0001969955817378498,
      "loss": 0.3692,
      "step": 56
    },
    {
      "epoch": 0.016764705882352942,
      "grad_norm": 0.09807276725769043,
      "learning_rate": 0.00019693667157584687,
      "loss": 0.4863,
      "step": 57
    },
    {
      "epoch": 0.017058823529411765,
      "grad_norm": 0.0984250158071518,
      "learning_rate": 0.0001968777614138439,
      "loss": 0.3647,
      "step": 58
    },
    {
      "epoch": 0.01735294117647059,
      "grad_norm": 0.0653030276298523,
      "learning_rate": 0.00019681885125184093,
      "loss": 0.3701,
      "step": 59
    },
    {
      "epoch": 0.01764705882352941,
      "grad_norm": 0.09172077476978302,
      "learning_rate": 0.000196759941089838,
      "loss": 0.4507,
      "step": 60
    },
    {
      "epoch": 0.017941176470588235,
      "grad_norm": 0.07702208310365677,
      "learning_rate": 0.00019670103092783505,
      "loss": 0.3775,
      "step": 61
    },
    {
      "epoch": 0.018235294117647058,
      "grad_norm": 0.08453193306922913,
      "learning_rate": 0.00019664212076583211,
      "loss": 0.3371,
      "step": 62
    },
    {
      "epoch": 0.01852941176470588,
      "grad_norm": 0.07407738268375397,
      "learning_rate": 0.00019658321060382915,
      "loss": 0.3505,
      "step": 63
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 0.07670024037361145,
      "learning_rate": 0.0001965243004418262,
      "loss": 0.3849,
      "step": 64
    },
    {
      "epoch": 0.01911764705882353,
      "grad_norm": 0.07443025708198547,
      "learning_rate": 0.00019646539027982327,
      "loss": 0.3448,
      "step": 65
    },
    {
      "epoch": 0.019411764705882354,
      "grad_norm": 0.09393195062875748,
      "learning_rate": 0.00019640648011782033,
      "loss": 0.4421,
      "step": 66
    },
    {
      "epoch": 0.019705882352941177,
      "grad_norm": 0.08082257211208344,
      "learning_rate": 0.0001963475699558174,
      "loss": 0.2845,
      "step": 67
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.0882955864071846,
      "learning_rate": 0.00019628865979381442,
      "loss": 0.3601,
      "step": 68
    },
    {
      "epoch": 0.020294117647058824,
      "grad_norm": 0.06807062774896622,
      "learning_rate": 0.00019622974963181148,
      "loss": 0.3227,
      "step": 69
    },
    {
      "epoch": 0.020588235294117647,
      "grad_norm": 0.08838649839162827,
      "learning_rate": 0.00019617083946980854,
      "loss": 0.2737,
      "step": 70
    },
    {
      "epoch": 0.02088235294117647,
      "grad_norm": 0.12643128633499146,
      "learning_rate": 0.0001961119293078056,
      "loss": 0.4371,
      "step": 71
    },
    {
      "epoch": 0.021176470588235293,
      "grad_norm": 0.07378862053155899,
      "learning_rate": 0.00019605301914580266,
      "loss": 0.314,
      "step": 72
    },
    {
      "epoch": 0.021470588235294116,
      "grad_norm": 0.148187518119812,
      "learning_rate": 0.0001959941089837997,
      "loss": 0.4053,
      "step": 73
    },
    {
      "epoch": 0.02176470588235294,
      "grad_norm": 0.09213563799858093,
      "learning_rate": 0.00019593519882179675,
      "loss": 0.4561,
      "step": 74
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 0.07311000674962997,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.4227,
      "step": 75
    },
    {
      "epoch": 0.02235294117647059,
      "grad_norm": 0.10356037318706512,
      "learning_rate": 0.00019581737849779087,
      "loss": 0.3563,
      "step": 76
    },
    {
      "epoch": 0.022647058823529412,
      "grad_norm": 0.09420822560787201,
      "learning_rate": 0.00019575846833578793,
      "loss": 0.3436,
      "step": 77
    },
    {
      "epoch": 0.022941176470588236,
      "grad_norm": 0.06972385942935944,
      "learning_rate": 0.00019569955817378497,
      "loss": 0.3347,
      "step": 78
    },
    {
      "epoch": 0.02323529411764706,
      "grad_norm": 0.07533767819404602,
      "learning_rate": 0.00019564064801178203,
      "loss": 0.3263,
      "step": 79
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 0.12221214920282364,
      "learning_rate": 0.0001955817378497791,
      "loss": 0.4261,
      "step": 80
    },
    {
      "epoch": 0.023823529411764705,
      "grad_norm": 0.09389033168554306,
      "learning_rate": 0.00019552282768777615,
      "loss": 0.3815,
      "step": 81
    },
    {
      "epoch": 0.02411764705882353,
      "grad_norm": 0.11122535914182663,
      "learning_rate": 0.0001954639175257732,
      "loss": 0.43,
      "step": 82
    },
    {
      "epoch": 0.02441176470588235,
      "grad_norm": 0.15265360474586487,
      "learning_rate": 0.00019540500736377024,
      "loss": 0.498,
      "step": 83
    },
    {
      "epoch": 0.024705882352941175,
      "grad_norm": 0.09435665607452393,
      "learning_rate": 0.0001953460972017673,
      "loss": 0.4346,
      "step": 84
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.0835433080792427,
      "learning_rate": 0.00019528718703976436,
      "loss": 0.2738,
      "step": 85
    },
    {
      "epoch": 0.025294117647058825,
      "grad_norm": 0.12333851307630539,
      "learning_rate": 0.00019522827687776142,
      "loss": 0.3724,
      "step": 86
    },
    {
      "epoch": 0.025588235294117648,
      "grad_norm": 0.11394748836755753,
      "learning_rate": 0.00019516936671575848,
      "loss": 0.4179,
      "step": 87
    },
    {
      "epoch": 0.02588235294117647,
      "grad_norm": 0.0668557807803154,
      "learning_rate": 0.00019511045655375552,
      "loss": 0.2998,
      "step": 88
    },
    {
      "epoch": 0.026176470588235294,
      "grad_norm": 0.0722629576921463,
      "learning_rate": 0.00019505154639175258,
      "loss": 0.3479,
      "step": 89
    },
    {
      "epoch": 0.026470588235294117,
      "grad_norm": 0.08276353776454926,
      "learning_rate": 0.00019499263622974964,
      "loss": 0.4187,
      "step": 90
    },
    {
      "epoch": 0.02676470588235294,
      "grad_norm": 0.09815466403961182,
      "learning_rate": 0.0001949337260677467,
      "loss": 0.453,
      "step": 91
    },
    {
      "epoch": 0.027058823529411764,
      "grad_norm": 0.1115587130188942,
      "learning_rate": 0.00019487481590574376,
      "loss": 0.391,
      "step": 92
    },
    {
      "epoch": 0.027352941176470587,
      "grad_norm": 0.08895131945610046,
      "learning_rate": 0.0001948159057437408,
      "loss": 0.3613,
      "step": 93
    },
    {
      "epoch": 0.027647058823529413,
      "grad_norm": 0.07907140254974365,
      "learning_rate": 0.00019475699558173785,
      "loss": 0.3996,
      "step": 94
    },
    {
      "epoch": 0.027941176470588237,
      "grad_norm": 0.060466546565294266,
      "learning_rate": 0.0001946980854197349,
      "loss": 0.3514,
      "step": 95
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 0.08669271320104599,
      "learning_rate": 0.00019463917525773197,
      "loss": 0.3257,
      "step": 96
    },
    {
      "epoch": 0.028529411764705883,
      "grad_norm": 0.10394621640443802,
      "learning_rate": 0.00019458026509572903,
      "loss": 0.4313,
      "step": 97
    },
    {
      "epoch": 0.028823529411764706,
      "grad_norm": 0.08653022348880768,
      "learning_rate": 0.00019452135493372606,
      "loss": 0.4557,
      "step": 98
    },
    {
      "epoch": 0.02911764705882353,
      "grad_norm": 0.05869050323963165,
      "learning_rate": 0.00019446244477172312,
      "loss": 0.3059,
      "step": 99
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.09673353284597397,
      "learning_rate": 0.00019440353460972018,
      "loss": 0.5006,
      "step": 100
    },
    {
      "epoch": 0.029705882352941176,
      "grad_norm": 0.07488714158535004,
      "learning_rate": 0.00019434462444771724,
      "loss": 0.3754,
      "step": 101
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.06179310753941536,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.3578,
      "step": 102
    },
    {
      "epoch": 0.030294117647058822,
      "grad_norm": 0.07002272456884384,
      "learning_rate": 0.00019422680412371134,
      "loss": 0.3498,
      "step": 103
    },
    {
      "epoch": 0.03058823529411765,
      "grad_norm": 0.08683742582798004,
      "learning_rate": 0.0001941678939617084,
      "loss": 0.4209,
      "step": 104
    },
    {
      "epoch": 0.030882352941176472,
      "grad_norm": 0.09129704535007477,
      "learning_rate": 0.00019410898379970546,
      "loss": 0.38,
      "step": 105
    },
    {
      "epoch": 0.031176470588235295,
      "grad_norm": 0.0711800754070282,
      "learning_rate": 0.00019405007363770252,
      "loss": 0.4083,
      "step": 106
    },
    {
      "epoch": 0.03147058823529412,
      "grad_norm": 0.11384622752666473,
      "learning_rate": 0.00019399116347569958,
      "loss": 0.4224,
      "step": 107
    },
    {
      "epoch": 0.03176470588235294,
      "grad_norm": 0.0907897874712944,
      "learning_rate": 0.0001939322533136966,
      "loss": 0.3922,
      "step": 108
    },
    {
      "epoch": 0.032058823529411765,
      "grad_norm": 0.06573566794395447,
      "learning_rate": 0.00019387334315169367,
      "loss": 0.2844,
      "step": 109
    },
    {
      "epoch": 0.03235294117647059,
      "grad_norm": 0.10348077118396759,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.3964,
      "step": 110
    },
    {
      "epoch": 0.03264705882352941,
      "grad_norm": 0.0762903019785881,
      "learning_rate": 0.0001937555228276878,
      "loss": 0.3731,
      "step": 111
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 0.0904083326458931,
      "learning_rate": 0.00019369661266568485,
      "loss": 0.4141,
      "step": 112
    },
    {
      "epoch": 0.03323529411764706,
      "grad_norm": 0.1139480322599411,
      "learning_rate": 0.00019363770250368188,
      "loss": 0.3646,
      "step": 113
    },
    {
      "epoch": 0.033529411764705884,
      "grad_norm": 0.08844795823097229,
      "learning_rate": 0.00019357879234167894,
      "loss": 0.3768,
      "step": 114
    },
    {
      "epoch": 0.033823529411764704,
      "grad_norm": 0.08932732790708542,
      "learning_rate": 0.000193519882179676,
      "loss": 0.415,
      "step": 115
    },
    {
      "epoch": 0.03411764705882353,
      "grad_norm": 0.1254056990146637,
      "learning_rate": 0.00019346097201767306,
      "loss": 0.3819,
      "step": 116
    },
    {
      "epoch": 0.03441176470588235,
      "grad_norm": 0.09746512025594711,
      "learning_rate": 0.00019340206185567012,
      "loss": 0.425,
      "step": 117
    },
    {
      "epoch": 0.03470588235294118,
      "grad_norm": 0.08445445448160172,
      "learning_rate": 0.00019334315169366716,
      "loss": 0.4351,
      "step": 118
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.09242141991853714,
      "learning_rate": 0.00019328424153166422,
      "loss": 0.4325,
      "step": 119
    },
    {
      "epoch": 0.03529411764705882,
      "grad_norm": 0.07406548410654068,
      "learning_rate": 0.00019322533136966128,
      "loss": 0.4032,
      "step": 120
    },
    {
      "epoch": 0.03558823529411765,
      "grad_norm": 0.10077258199453354,
      "learning_rate": 0.00019316642120765834,
      "loss": 0.3409,
      "step": 121
    },
    {
      "epoch": 0.03588235294117647,
      "grad_norm": 0.10805519670248032,
      "learning_rate": 0.0001931075110456554,
      "loss": 0.489,
      "step": 122
    },
    {
      "epoch": 0.036176470588235296,
      "grad_norm": 0.08050020039081573,
      "learning_rate": 0.00019304860088365243,
      "loss": 0.3938,
      "step": 123
    },
    {
      "epoch": 0.036470588235294116,
      "grad_norm": 0.08660823851823807,
      "learning_rate": 0.0001929896907216495,
      "loss": 0.4373,
      "step": 124
    },
    {
      "epoch": 0.03676470588235294,
      "grad_norm": 0.0706796795129776,
      "learning_rate": 0.00019293078055964655,
      "loss": 0.3082,
      "step": 125
    },
    {
      "epoch": 0.03705882352941176,
      "grad_norm": 0.09656774252653122,
      "learning_rate": 0.0001928718703976436,
      "loss": 0.4031,
      "step": 126
    },
    {
      "epoch": 0.03735294117647059,
      "grad_norm": 0.07571390271186829,
      "learning_rate": 0.00019281296023564067,
      "loss": 0.3605,
      "step": 127
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 0.0586065910756588,
      "learning_rate": 0.0001927540500736377,
      "loss": 0.3305,
      "step": 128
    },
    {
      "epoch": 0.037941176470588235,
      "grad_norm": 0.07895198464393616,
      "learning_rate": 0.00019269513991163477,
      "loss": 0.4031,
      "step": 129
    },
    {
      "epoch": 0.03823529411764706,
      "grad_norm": 0.07876359671354294,
      "learning_rate": 0.00019263622974963183,
      "loss": 0.3708,
      "step": 130
    },
    {
      "epoch": 0.03852941176470588,
      "grad_norm": 0.06287344545125961,
      "learning_rate": 0.00019257731958762889,
      "loss": 0.3241,
      "step": 131
    },
    {
      "epoch": 0.03882352941176471,
      "grad_norm": 0.09601379185914993,
      "learning_rate": 0.00019251840942562595,
      "loss": 0.4325,
      "step": 132
    },
    {
      "epoch": 0.03911764705882353,
      "grad_norm": 0.07955898344516754,
      "learning_rate": 0.00019245949926362298,
      "loss": 0.2913,
      "step": 133
    },
    {
      "epoch": 0.039411764705882354,
      "grad_norm": 0.0715600773692131,
      "learning_rate": 0.00019240058910162004,
      "loss": 0.4114,
      "step": 134
    },
    {
      "epoch": 0.039705882352941174,
      "grad_norm": 0.08470238000154495,
      "learning_rate": 0.0001923416789396171,
      "loss": 0.4022,
      "step": 135
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.09018309414386749,
      "learning_rate": 0.00019228276877761416,
      "loss": 0.4207,
      "step": 136
    },
    {
      "epoch": 0.04029411764705882,
      "grad_norm": 0.08381707221269608,
      "learning_rate": 0.00019222385861561122,
      "loss": 0.3539,
      "step": 137
    },
    {
      "epoch": 0.04058823529411765,
      "grad_norm": 0.07111643254756927,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.2911,
      "step": 138
    },
    {
      "epoch": 0.040882352941176474,
      "grad_norm": 0.09789180010557175,
      "learning_rate": 0.0001921060382916053,
      "loss": 0.4222,
      "step": 139
    },
    {
      "epoch": 0.041176470588235294,
      "grad_norm": 0.06659652292728424,
      "learning_rate": 0.00019204712812960237,
      "loss": 0.3623,
      "step": 140
    },
    {
      "epoch": 0.04147058823529412,
      "grad_norm": 0.09430304914712906,
      "learning_rate": 0.00019198821796759943,
      "loss": 0.4324,
      "step": 141
    },
    {
      "epoch": 0.04176470588235294,
      "grad_norm": 0.079689621925354,
      "learning_rate": 0.0001919293078055965,
      "loss": 0.425,
      "step": 142
    },
    {
      "epoch": 0.04205882352941177,
      "grad_norm": 0.07443461567163467,
      "learning_rate": 0.00019187039764359353,
      "loss": 0.3895,
      "step": 143
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 0.09401646256446838,
      "learning_rate": 0.0001918114874815906,
      "loss": 0.4119,
      "step": 144
    },
    {
      "epoch": 0.04264705882352941,
      "grad_norm": 0.0865236446261406,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.3633,
      "step": 145
    },
    {
      "epoch": 0.04294117647058823,
      "grad_norm": 0.099636971950531,
      "learning_rate": 0.0001916936671575847,
      "loss": 0.4135,
      "step": 146
    },
    {
      "epoch": 0.04323529411764706,
      "grad_norm": 0.06416217237710953,
      "learning_rate": 0.00019163475699558177,
      "loss": 0.3523,
      "step": 147
    },
    {
      "epoch": 0.04352941176470588,
      "grad_norm": 0.0789947584271431,
      "learning_rate": 0.0001915758468335788,
      "loss": 0.3704,
      "step": 148
    },
    {
      "epoch": 0.043823529411764706,
      "grad_norm": 0.09879098832607269,
      "learning_rate": 0.00019151693667157586,
      "loss": 0.4428,
      "step": 149
    },
    {
      "epoch": 0.04411764705882353,
      "grad_norm": 0.11308111250400543,
      "learning_rate": 0.00019145802650957292,
      "loss": 0.3952,
      "step": 150
    },
    {
      "epoch": 0.04441176470588235,
      "grad_norm": 0.057182349264621735,
      "learning_rate": 0.00019139911634756998,
      "loss": 0.3241,
      "step": 151
    },
    {
      "epoch": 0.04470588235294118,
      "grad_norm": 0.07456338405609131,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.3198,
      "step": 152
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.08957011252641678,
      "learning_rate": 0.00019128129602356407,
      "loss": 0.421,
      "step": 153
    },
    {
      "epoch": 0.045294117647058825,
      "grad_norm": 0.08492514491081238,
      "learning_rate": 0.00019122238586156113,
      "loss": 0.4126,
      "step": 154
    },
    {
      "epoch": 0.045588235294117645,
      "grad_norm": 0.07395157963037491,
      "learning_rate": 0.0001911634756995582,
      "loss": 0.37,
      "step": 155
    },
    {
      "epoch": 0.04588235294117647,
      "grad_norm": 0.052728813141584396,
      "learning_rate": 0.00019110456553755525,
      "loss": 0.3022,
      "step": 156
    },
    {
      "epoch": 0.04617647058823529,
      "grad_norm": 0.09481725841760635,
      "learning_rate": 0.00019104565537555231,
      "loss": 0.472,
      "step": 157
    },
    {
      "epoch": 0.04647058823529412,
      "grad_norm": 0.07792573422193527,
      "learning_rate": 0.00019098674521354935,
      "loss": 0.3787,
      "step": 158
    },
    {
      "epoch": 0.046764705882352944,
      "grad_norm": 0.10479330271482468,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.4218,
      "step": 159
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 0.0658794417977333,
      "learning_rate": 0.00019086892488954347,
      "loss": 0.2732,
      "step": 160
    },
    {
      "epoch": 0.04735294117647059,
      "grad_norm": 0.09008630365133286,
      "learning_rate": 0.00019081001472754053,
      "loss": 0.3636,
      "step": 161
    },
    {
      "epoch": 0.04764705882352941,
      "grad_norm": 0.08040700852870941,
      "learning_rate": 0.0001907511045655376,
      "loss": 0.3693,
      "step": 162
    },
    {
      "epoch": 0.04794117647058824,
      "grad_norm": 0.09139419347047806,
      "learning_rate": 0.00019069219440353462,
      "loss": 0.3655,
      "step": 163
    },
    {
      "epoch": 0.04823529411764706,
      "grad_norm": 0.09087029099464417,
      "learning_rate": 0.00019063328424153168,
      "loss": 0.3532,
      "step": 164
    },
    {
      "epoch": 0.04852941176470588,
      "grad_norm": 0.08937810361385345,
      "learning_rate": 0.00019057437407952871,
      "loss": 0.3484,
      "step": 165
    },
    {
      "epoch": 0.0488235294117647,
      "grad_norm": 0.10684864968061447,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.4195,
      "step": 166
    },
    {
      "epoch": 0.04911764705882353,
      "grad_norm": 0.07988648861646652,
      "learning_rate": 0.00019045655375552283,
      "loss": 0.3038,
      "step": 167
    },
    {
      "epoch": 0.04941176470588235,
      "grad_norm": 0.07970356941223145,
      "learning_rate": 0.00019039764359351987,
      "loss": 0.3355,
      "step": 168
    },
    {
      "epoch": 0.049705882352941176,
      "grad_norm": 0.07659484446048737,
      "learning_rate": 0.00019033873343151693,
      "loss": 0.3742,
      "step": 169
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.07002292573451996,
      "learning_rate": 0.000190279823269514,
      "loss": 0.3269,
      "step": 170
    },
    {
      "epoch": 0.05029411764705882,
      "grad_norm": 0.07644231617450714,
      "learning_rate": 0.00019022091310751105,
      "loss": 0.4137,
      "step": 171
    },
    {
      "epoch": 0.05058823529411765,
      "grad_norm": 0.08522560447454453,
      "learning_rate": 0.0001901620029455081,
      "loss": 0.3864,
      "step": 172
    },
    {
      "epoch": 0.05088235294117647,
      "grad_norm": 0.07500582933425903,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.3319,
      "step": 173
    },
    {
      "epoch": 0.051176470588235295,
      "grad_norm": 0.08164885640144348,
      "learning_rate": 0.0001900441826215022,
      "loss": 0.3337,
      "step": 174
    },
    {
      "epoch": 0.051470588235294115,
      "grad_norm": 0.09285160154104233,
      "learning_rate": 0.00018998527245949926,
      "loss": 0.4254,
      "step": 175
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 0.0819367915391922,
      "learning_rate": 0.00018992636229749632,
      "loss": 0.3257,
      "step": 176
    },
    {
      "epoch": 0.05205882352941176,
      "grad_norm": 0.12490145862102509,
      "learning_rate": 0.00018986745213549338,
      "loss": 0.3524,
      "step": 177
    },
    {
      "epoch": 0.05235294117647059,
      "grad_norm": 0.0866042822599411,
      "learning_rate": 0.00018980854197349042,
      "loss": 0.3386,
      "step": 178
    },
    {
      "epoch": 0.052647058823529415,
      "grad_norm": 0.07165395468473434,
      "learning_rate": 0.00018974963181148748,
      "loss": 0.3618,
      "step": 179
    },
    {
      "epoch": 0.052941176470588235,
      "grad_norm": 0.10758268088102341,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.3723,
      "step": 180
    },
    {
      "epoch": 0.05323529411764706,
      "grad_norm": 0.08238274604082108,
      "learning_rate": 0.0001896318114874816,
      "loss": 0.4296,
      "step": 181
    },
    {
      "epoch": 0.05352941176470588,
      "grad_norm": 0.07832297682762146,
      "learning_rate": 0.00018957290132547866,
      "loss": 0.3288,
      "step": 182
    },
    {
      "epoch": 0.05382352941176471,
      "grad_norm": 0.058846212923526764,
      "learning_rate": 0.0001895139911634757,
      "loss": 0.2637,
      "step": 183
    },
    {
      "epoch": 0.05411764705882353,
      "grad_norm": 0.07765717804431915,
      "learning_rate": 0.00018945508100147275,
      "loss": 0.3672,
      "step": 184
    },
    {
      "epoch": 0.054411764705882354,
      "grad_norm": 0.07941259443759918,
      "learning_rate": 0.0001893961708394698,
      "loss": 0.3371,
      "step": 185
    },
    {
      "epoch": 0.054705882352941174,
      "grad_norm": 0.08304019272327423,
      "learning_rate": 0.00018933726067746687,
      "loss": 0.3507,
      "step": 186
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.11205962300300598,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.3935,
      "step": 187
    },
    {
      "epoch": 0.05529411764705883,
      "grad_norm": 0.05942642316222191,
      "learning_rate": 0.00018921944035346096,
      "loss": 0.3451,
      "step": 188
    },
    {
      "epoch": 0.05558823529411765,
      "grad_norm": 0.06165124475955963,
      "learning_rate": 0.00018916053019145802,
      "loss": 0.2979,
      "step": 189
    },
    {
      "epoch": 0.05588235294117647,
      "grad_norm": 0.08855830132961273,
      "learning_rate": 0.00018910162002945508,
      "loss": 0.415,
      "step": 190
    },
    {
      "epoch": 0.05617647058823529,
      "grad_norm": 0.05826304852962494,
      "learning_rate": 0.00018904270986745214,
      "loss": 0.3196,
      "step": 191
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 0.06903307884931564,
      "learning_rate": 0.0001889837997054492,
      "loss": 0.4254,
      "step": 192
    },
    {
      "epoch": 0.05676470588235294,
      "grad_norm": 0.06660143285989761,
      "learning_rate": 0.00018892488954344624,
      "loss": 0.3819,
      "step": 193
    },
    {
      "epoch": 0.057058823529411766,
      "grad_norm": 0.06879907101392746,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.3708,
      "step": 194
    },
    {
      "epoch": 0.057352941176470586,
      "grad_norm": 0.0858893170952797,
      "learning_rate": 0.00018880706921944036,
      "loss": 0.488,
      "step": 195
    },
    {
      "epoch": 0.05764705882352941,
      "grad_norm": 0.08478183299303055,
      "learning_rate": 0.00018874815905743742,
      "loss": 0.4276,
      "step": 196
    },
    {
      "epoch": 0.05794117647058823,
      "grad_norm": 0.06848134100437164,
      "learning_rate": 0.00018868924889543448,
      "loss": 0.2908,
      "step": 197
    },
    {
      "epoch": 0.05823529411764706,
      "grad_norm": 0.0728461891412735,
      "learning_rate": 0.0001886303387334315,
      "loss": 0.372,
      "step": 198
    },
    {
      "epoch": 0.058529411764705885,
      "grad_norm": 0.06572681665420532,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3229,
      "step": 199
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.06704764813184738,
      "learning_rate": 0.00018851251840942563,
      "loss": 0.3441,
      "step": 200
    },
    {
      "epoch": 0.05911764705882353,
      "grad_norm": 0.08072269707918167,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.3864,
      "step": 201
    },
    {
      "epoch": 0.05941176470588235,
      "grad_norm": 0.06302585452795029,
      "learning_rate": 0.00018839469808541975,
      "loss": 0.3172,
      "step": 202
    },
    {
      "epoch": 0.05970588235294118,
      "grad_norm": 0.0621432401239872,
      "learning_rate": 0.00018833578792341678,
      "loss": 0.3834,
      "step": 203
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.052341435104608536,
      "learning_rate": 0.00018827687776141384,
      "loss": 0.2979,
      "step": 204
    },
    {
      "epoch": 0.060294117647058824,
      "grad_norm": 0.09799778461456299,
      "learning_rate": 0.0001882179675994109,
      "loss": 0.3965,
      "step": 205
    },
    {
      "epoch": 0.060588235294117644,
      "grad_norm": 0.054619040340185165,
      "learning_rate": 0.00018815905743740796,
      "loss": 0.3463,
      "step": 206
    },
    {
      "epoch": 0.06088235294117647,
      "grad_norm": 0.06466399878263474,
      "learning_rate": 0.00018810014727540502,
      "loss": 0.3271,
      "step": 207
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 0.0610072985291481,
      "learning_rate": 0.00018804123711340206,
      "loss": 0.3431,
      "step": 208
    },
    {
      "epoch": 0.06147058823529412,
      "grad_norm": 0.084291972219944,
      "learning_rate": 0.00018798232695139912,
      "loss": 0.3796,
      "step": 209
    },
    {
      "epoch": 0.061764705882352944,
      "grad_norm": 0.06195719167590141,
      "learning_rate": 0.00018792341678939618,
      "loss": 0.3384,
      "step": 210
    },
    {
      "epoch": 0.062058823529411763,
      "grad_norm": 0.08425376564264297,
      "learning_rate": 0.00018786450662739324,
      "loss": 0.4044,
      "step": 211
    },
    {
      "epoch": 0.06235294117647059,
      "grad_norm": 0.06337657570838928,
      "learning_rate": 0.0001878055964653903,
      "loss": 0.354,
      "step": 212
    },
    {
      "epoch": 0.06264705882352942,
      "grad_norm": 0.06619176268577576,
      "learning_rate": 0.00018774668630338733,
      "loss": 0.3842,
      "step": 213
    },
    {
      "epoch": 0.06294117647058824,
      "grad_norm": 0.0671001449227333,
      "learning_rate": 0.0001876877761413844,
      "loss": 0.3074,
      "step": 214
    },
    {
      "epoch": 0.06323529411764706,
      "grad_norm": 0.07938998937606812,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.3713,
      "step": 215
    },
    {
      "epoch": 0.06352941176470588,
      "grad_norm": 0.05267553776502609,
      "learning_rate": 0.0001875699558173785,
      "loss": 0.3206,
      "step": 216
    },
    {
      "epoch": 0.06382352941176471,
      "grad_norm": 0.06867070496082306,
      "learning_rate": 0.00018751104565537557,
      "loss": 0.2797,
      "step": 217
    },
    {
      "epoch": 0.06411764705882353,
      "grad_norm": 0.0672331377863884,
      "learning_rate": 0.0001874521354933726,
      "loss": 0.3487,
      "step": 218
    },
    {
      "epoch": 0.06441176470588235,
      "grad_norm": 0.0979737862944603,
      "learning_rate": 0.00018739322533136967,
      "loss": 0.3366,
      "step": 219
    },
    {
      "epoch": 0.06470588235294118,
      "grad_norm": 0.09713222831487656,
      "learning_rate": 0.00018733431516936673,
      "loss": 0.4014,
      "step": 220
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.09628748893737793,
      "learning_rate": 0.00018727540500736379,
      "loss": 0.3651,
      "step": 221
    },
    {
      "epoch": 0.06529411764705882,
      "grad_norm": 0.07174662500619888,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.3295,
      "step": 222
    },
    {
      "epoch": 0.06558823529411764,
      "grad_norm": 0.09166757762432098,
      "learning_rate": 0.00018715758468335788,
      "loss": 0.335,
      "step": 223
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 0.08839033544063568,
      "learning_rate": 0.00018709867452135494,
      "loss": 0.3159,
      "step": 224
    },
    {
      "epoch": 0.0661764705882353,
      "grad_norm": 0.08679421991109848,
      "learning_rate": 0.000187039764359352,
      "loss": 0.3906,
      "step": 225
    },
    {
      "epoch": 0.06647058823529411,
      "grad_norm": 0.07508322596549988,
      "learning_rate": 0.00018698085419734906,
      "loss": 0.3996,
      "step": 226
    },
    {
      "epoch": 0.06676470588235293,
      "grad_norm": 0.056456372141838074,
      "learning_rate": 0.00018692194403534612,
      "loss": 0.3694,
      "step": 227
    },
    {
      "epoch": 0.06705882352941177,
      "grad_norm": 0.05808485671877861,
      "learning_rate": 0.00018686303387334315,
      "loss": 0.3419,
      "step": 228
    },
    {
      "epoch": 0.06735294117647059,
      "grad_norm": 0.06492408365011215,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.2908,
      "step": 229
    },
    {
      "epoch": 0.06764705882352941,
      "grad_norm": 0.07471184432506561,
      "learning_rate": 0.00018674521354933727,
      "loss": 0.3712,
      "step": 230
    },
    {
      "epoch": 0.06794117647058824,
      "grad_norm": 0.08175772428512573,
      "learning_rate": 0.00018668630338733433,
      "loss": 0.3676,
      "step": 231
    },
    {
      "epoch": 0.06823529411764706,
      "grad_norm": 0.07734062522649765,
      "learning_rate": 0.0001866273932253314,
      "loss": 0.432,
      "step": 232
    },
    {
      "epoch": 0.06852941176470588,
      "grad_norm": 0.11309321969747543,
      "learning_rate": 0.00018656848306332843,
      "loss": 0.3868,
      "step": 233
    },
    {
      "epoch": 0.0688235294117647,
      "grad_norm": 0.07719901204109192,
      "learning_rate": 0.0001865095729013255,
      "loss": 0.3841,
      "step": 234
    },
    {
      "epoch": 0.06911764705882353,
      "grad_norm": 0.052105437964200974,
      "learning_rate": 0.00018645066273932255,
      "loss": 0.3381,
      "step": 235
    },
    {
      "epoch": 0.06941176470588235,
      "grad_norm": 0.0661512091755867,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.3807,
      "step": 236
    },
    {
      "epoch": 0.06970588235294117,
      "grad_norm": 0.07502448558807373,
      "learning_rate": 0.00018633284241531667,
      "loss": 0.3497,
      "step": 237
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.08837555348873138,
      "learning_rate": 0.0001862739322533137,
      "loss": 0.4959,
      "step": 238
    },
    {
      "epoch": 0.07029411764705883,
      "grad_norm": 0.08164649456739426,
      "learning_rate": 0.00018621502209131076,
      "loss": 0.372,
      "step": 239
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 0.07557537406682968,
      "learning_rate": 0.00018615611192930782,
      "loss": 0.4156,
      "step": 240
    },
    {
      "epoch": 0.07088235294117647,
      "grad_norm": 0.0747814029455185,
      "learning_rate": 0.00018609720176730488,
      "loss": 0.316,
      "step": 241
    },
    {
      "epoch": 0.0711764705882353,
      "grad_norm": 0.06791973859071732,
      "learning_rate": 0.00018603829160530194,
      "loss": 0.3607,
      "step": 242
    },
    {
      "epoch": 0.07147058823529412,
      "grad_norm": 0.08322137594223022,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.3926,
      "step": 243
    },
    {
      "epoch": 0.07176470588235294,
      "grad_norm": 0.067705437541008,
      "learning_rate": 0.00018592047128129603,
      "loss": 0.394,
      "step": 244
    },
    {
      "epoch": 0.07205882352941176,
      "grad_norm": 0.08154399693012238,
      "learning_rate": 0.0001858615611192931,
      "loss": 0.3632,
      "step": 245
    },
    {
      "epoch": 0.07235294117647059,
      "grad_norm": 0.10853835195302963,
      "learning_rate": 0.00018580265095729015,
      "loss": 0.4394,
      "step": 246
    },
    {
      "epoch": 0.07264705882352941,
      "grad_norm": 0.09938590973615646,
      "learning_rate": 0.00018574374079528721,
      "loss": 0.4557,
      "step": 247
    },
    {
      "epoch": 0.07294117647058823,
      "grad_norm": 0.06192362681031227,
      "learning_rate": 0.00018568483063328425,
      "loss": 0.3553,
      "step": 248
    },
    {
      "epoch": 0.07323529411764707,
      "grad_norm": 0.09583700448274612,
      "learning_rate": 0.0001856259204712813,
      "loss": 0.3844,
      "step": 249
    },
    {
      "epoch": 0.07352941176470588,
      "grad_norm": 0.10403881222009659,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.4756,
      "step": 250
    },
    {
      "epoch": 0.0738235294117647,
      "grad_norm": 0.05601095035672188,
      "learning_rate": 0.00018550810014727543,
      "loss": 0.2973,
      "step": 251
    },
    {
      "epoch": 0.07411764705882352,
      "grad_norm": 0.06452985852956772,
      "learning_rate": 0.0001854491899852725,
      "loss": 0.3872,
      "step": 252
    },
    {
      "epoch": 0.07441176470588236,
      "grad_norm": 0.06883833557367325,
      "learning_rate": 0.00018539027982326952,
      "loss": 0.3115,
      "step": 253
    },
    {
      "epoch": 0.07470588235294118,
      "grad_norm": 0.06155490130186081,
      "learning_rate": 0.00018533136966126658,
      "loss": 0.3156,
      "step": 254
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.08306905627250671,
      "learning_rate": 0.00018527245949926364,
      "loss": 0.4543,
      "step": 255
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 0.06061279773712158,
      "learning_rate": 0.0001852135493372607,
      "loss": 0.3221,
      "step": 256
    },
    {
      "epoch": 0.07558823529411765,
      "grad_norm": 0.04823104664683342,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.2969,
      "step": 257
    },
    {
      "epoch": 0.07588235294117647,
      "grad_norm": 0.07310818135738373,
      "learning_rate": 0.0001850957290132548,
      "loss": 0.365,
      "step": 258
    },
    {
      "epoch": 0.07617647058823529,
      "grad_norm": 0.08841561526060104,
      "learning_rate": 0.00018503681885125186,
      "loss": 0.381,
      "step": 259
    },
    {
      "epoch": 0.07647058823529412,
      "grad_norm": 0.06944283097982407,
      "learning_rate": 0.00018497790868924892,
      "loss": 0.4261,
      "step": 260
    },
    {
      "epoch": 0.07676470588235294,
      "grad_norm": 0.07298435270786285,
      "learning_rate": 0.00018491899852724598,
      "loss": 0.4459,
      "step": 261
    },
    {
      "epoch": 0.07705882352941176,
      "grad_norm": 0.09518639743328094,
      "learning_rate": 0.00018486008836524304,
      "loss": 0.4657,
      "step": 262
    },
    {
      "epoch": 0.07735294117647058,
      "grad_norm": 0.05629313364624977,
      "learning_rate": 0.00018480117820324007,
      "loss": 0.3351,
      "step": 263
    },
    {
      "epoch": 0.07764705882352942,
      "grad_norm": 0.07238731533288956,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.3908,
      "step": 264
    },
    {
      "epoch": 0.07794117647058824,
      "grad_norm": 0.08914436399936676,
      "learning_rate": 0.0001846833578792342,
      "loss": 0.3806,
      "step": 265
    },
    {
      "epoch": 0.07823529411764706,
      "grad_norm": 0.0776228979229927,
      "learning_rate": 0.00018462444771723125,
      "loss": 0.4126,
      "step": 266
    },
    {
      "epoch": 0.07852941176470589,
      "grad_norm": 0.09864117205142975,
      "learning_rate": 0.0001845655375552283,
      "loss": 0.4262,
      "step": 267
    },
    {
      "epoch": 0.07882352941176471,
      "grad_norm": 0.08066605776548386,
      "learning_rate": 0.00018450662739322534,
      "loss": 0.3962,
      "step": 268
    },
    {
      "epoch": 0.07911764705882353,
      "grad_norm": 0.07323722541332245,
      "learning_rate": 0.0001844477172312224,
      "loss": 0.3747,
      "step": 269
    },
    {
      "epoch": 0.07941176470588235,
      "grad_norm": 0.09612224251031876,
      "learning_rate": 0.00018438880706921946,
      "loss": 0.382,
      "step": 270
    },
    {
      "epoch": 0.07970588235294118,
      "grad_norm": 0.08757799118757248,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.4167,
      "step": 271
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.06094774603843689,
      "learning_rate": 0.00018427098674521356,
      "loss": 0.2908,
      "step": 272
    },
    {
      "epoch": 0.08029411764705882,
      "grad_norm": 0.07896409183740616,
      "learning_rate": 0.0001842120765832106,
      "loss": 0.3249,
      "step": 273
    },
    {
      "epoch": 0.08058823529411764,
      "grad_norm": 0.09363578259944916,
      "learning_rate": 0.00018415316642120765,
      "loss": 0.3697,
      "step": 274
    },
    {
      "epoch": 0.08088235294117647,
      "grad_norm": 0.07927625626325607,
      "learning_rate": 0.0001840942562592047,
      "loss": 0.3971,
      "step": 275
    },
    {
      "epoch": 0.0811764705882353,
      "grad_norm": 0.08030544966459274,
      "learning_rate": 0.00018403534609720177,
      "loss": 0.3374,
      "step": 276
    },
    {
      "epoch": 0.08147058823529411,
      "grad_norm": 0.07672452181577682,
      "learning_rate": 0.00018397643593519883,
      "loss": 0.3978,
      "step": 277
    },
    {
      "epoch": 0.08176470588235295,
      "grad_norm": 0.06783337891101837,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.3899,
      "step": 278
    },
    {
      "epoch": 0.08205882352941177,
      "grad_norm": 0.07583273202180862,
      "learning_rate": 0.00018385861561119292,
      "loss": 0.4009,
      "step": 279
    },
    {
      "epoch": 0.08235294117647059,
      "grad_norm": 0.08617544919252396,
      "learning_rate": 0.00018379970544918998,
      "loss": 0.4353,
      "step": 280
    },
    {
      "epoch": 0.0826470588235294,
      "grad_norm": 0.09015573561191559,
      "learning_rate": 0.00018374079528718704,
      "loss": 0.4554,
      "step": 281
    },
    {
      "epoch": 0.08294117647058824,
      "grad_norm": 0.09381821006536484,
      "learning_rate": 0.0001836818851251841,
      "loss": 0.4542,
      "step": 282
    },
    {
      "epoch": 0.08323529411764706,
      "grad_norm": 0.059960540384054184,
      "learning_rate": 0.00018362297496318114,
      "loss": 0.394,
      "step": 283
    },
    {
      "epoch": 0.08352941176470588,
      "grad_norm": 0.05992317572236061,
      "learning_rate": 0.0001835640648011782,
      "loss": 0.3182,
      "step": 284
    },
    {
      "epoch": 0.0838235294117647,
      "grad_norm": 0.06755014508962631,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.3384,
      "step": 285
    },
    {
      "epoch": 0.08411764705882353,
      "grad_norm": 0.05742734670639038,
      "learning_rate": 0.00018344624447717232,
      "loss": 0.328,
      "step": 286
    },
    {
      "epoch": 0.08441176470588235,
      "grad_norm": 0.07704940438270569,
      "learning_rate": 0.00018338733431516938,
      "loss": 0.4231,
      "step": 287
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 0.06226291507482529,
      "learning_rate": 0.0001833284241531664,
      "loss": 0.3478,
      "step": 288
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.06497034430503845,
      "learning_rate": 0.00018326951399116347,
      "loss": 0.4152,
      "step": 289
    },
    {
      "epoch": 0.08529411764705883,
      "grad_norm": 0.0818169042468071,
      "learning_rate": 0.00018321060382916053,
      "loss": 0.4429,
      "step": 290
    },
    {
      "epoch": 0.08558823529411765,
      "grad_norm": 0.053118690848350525,
      "learning_rate": 0.0001831516936671576,
      "loss": 0.2825,
      "step": 291
    },
    {
      "epoch": 0.08588235294117647,
      "grad_norm": 0.10339044779539108,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.4807,
      "step": 292
    },
    {
      "epoch": 0.0861764705882353,
      "grad_norm": 0.06001389026641846,
      "learning_rate": 0.00018303387334315168,
      "loss": 0.3856,
      "step": 293
    },
    {
      "epoch": 0.08647058823529412,
      "grad_norm": 0.0635848417878151,
      "learning_rate": 0.00018297496318114874,
      "loss": 0.3795,
      "step": 294
    },
    {
      "epoch": 0.08676470588235294,
      "grad_norm": 0.06537123769521713,
      "learning_rate": 0.0001829160530191458,
      "loss": 0.3689,
      "step": 295
    },
    {
      "epoch": 0.08705882352941176,
      "grad_norm": 0.06368153542280197,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.4154,
      "step": 296
    },
    {
      "epoch": 0.08735294117647059,
      "grad_norm": 0.06571908295154572,
      "learning_rate": 0.00018279823269513992,
      "loss": 0.3549,
      "step": 297
    },
    {
      "epoch": 0.08764705882352941,
      "grad_norm": 0.07359504699707031,
      "learning_rate": 0.00018273932253313696,
      "loss": 0.3488,
      "step": 298
    },
    {
      "epoch": 0.08794117647058823,
      "grad_norm": 0.06670806556940079,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.3614,
      "step": 299
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.06990577280521393,
      "learning_rate": 0.00018262150220913108,
      "loss": 0.4047,
      "step": 300
    },
    {
      "epoch": 0.08852941176470588,
      "grad_norm": 0.05055892840027809,
      "learning_rate": 0.00018256259204712814,
      "loss": 0.294,
      "step": 301
    },
    {
      "epoch": 0.0888235294117647,
      "grad_norm": 0.06855972856283188,
      "learning_rate": 0.0001825036818851252,
      "loss": 0.3903,
      "step": 302
    },
    {
      "epoch": 0.08911764705882352,
      "grad_norm": 0.07813625782728195,
      "learning_rate": 0.00018244477172312223,
      "loss": 0.3862,
      "step": 303
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 0.0653594508767128,
      "learning_rate": 0.0001823858615611193,
      "loss": 0.3646,
      "step": 304
    },
    {
      "epoch": 0.08970588235294118,
      "grad_norm": 0.06089194864034653,
      "learning_rate": 0.00018232695139911635,
      "loss": 0.3923,
      "step": 305
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.053997382521629333,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.3208,
      "step": 306
    },
    {
      "epoch": 0.09029411764705883,
      "grad_norm": 0.07740602642297745,
      "learning_rate": 0.00018220913107511047,
      "loss": 0.3631,
      "step": 307
    },
    {
      "epoch": 0.09058823529411765,
      "grad_norm": 0.07099450379610062,
      "learning_rate": 0.0001821502209131075,
      "loss": 0.3599,
      "step": 308
    },
    {
      "epoch": 0.09088235294117647,
      "grad_norm": 0.07136023789644241,
      "learning_rate": 0.00018209131075110457,
      "loss": 0.3771,
      "step": 309
    },
    {
      "epoch": 0.09117647058823529,
      "grad_norm": 0.05170866474509239,
      "learning_rate": 0.00018203240058910163,
      "loss": 0.3335,
      "step": 310
    },
    {
      "epoch": 0.09147058823529412,
      "grad_norm": 0.07615814357995987,
      "learning_rate": 0.00018197349042709869,
      "loss": 0.4079,
      "step": 311
    },
    {
      "epoch": 0.09176470588235294,
      "grad_norm": 0.06258656084537506,
      "learning_rate": 0.00018191458026509575,
      "loss": 0.3571,
      "step": 312
    },
    {
      "epoch": 0.09205882352941176,
      "grad_norm": 0.07436079531908035,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.402,
      "step": 313
    },
    {
      "epoch": 0.09235294117647058,
      "grad_norm": 0.053074728697538376,
      "learning_rate": 0.00018179675994108984,
      "loss": 0.3659,
      "step": 314
    },
    {
      "epoch": 0.09264705882352942,
      "grad_norm": 0.06488879024982452,
      "learning_rate": 0.0001817378497790869,
      "loss": 0.345,
      "step": 315
    },
    {
      "epoch": 0.09294117647058824,
      "grad_norm": 0.060727816075086594,
      "learning_rate": 0.00018167893961708396,
      "loss": 0.3886,
      "step": 316
    },
    {
      "epoch": 0.09323529411764706,
      "grad_norm": 0.08009360730648041,
      "learning_rate": 0.00018162002945508102,
      "loss": 0.2971,
      "step": 317
    },
    {
      "epoch": 0.09352941176470589,
      "grad_norm": 0.05685359612107277,
      "learning_rate": 0.00018156111929307805,
      "loss": 0.3106,
      "step": 318
    },
    {
      "epoch": 0.09382352941176471,
      "grad_norm": 0.08681752532720566,
      "learning_rate": 0.0001815022091310751,
      "loss": 0.4174,
      "step": 319
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.05234995111823082,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.3066,
      "step": 320
    },
    {
      "epoch": 0.09441176470588235,
      "grad_norm": 0.06165998801589012,
      "learning_rate": 0.00018138438880706923,
      "loss": 0.3306,
      "step": 321
    },
    {
      "epoch": 0.09470588235294118,
      "grad_norm": 0.09200310707092285,
      "learning_rate": 0.0001813254786450663,
      "loss": 0.4439,
      "step": 322
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06489979475736618,
      "learning_rate": 0.00018126656848306333,
      "loss": 0.3442,
      "step": 323
    },
    {
      "epoch": 0.09529411764705882,
      "grad_norm": 0.05849449336528778,
      "learning_rate": 0.00018120765832106039,
      "loss": 0.348,
      "step": 324
    },
    {
      "epoch": 0.09558823529411764,
      "grad_norm": 0.07316594570875168,
      "learning_rate": 0.00018114874815905745,
      "loss": 0.3457,
      "step": 325
    },
    {
      "epoch": 0.09588235294117647,
      "grad_norm": 0.08434084802865982,
      "learning_rate": 0.0001810898379970545,
      "loss": 0.4089,
      "step": 326
    },
    {
      "epoch": 0.0961764705882353,
      "grad_norm": 0.061086054891347885,
      "learning_rate": 0.00018103092783505157,
      "loss": 0.3714,
      "step": 327
    },
    {
      "epoch": 0.09647058823529411,
      "grad_norm": 0.0624801442027092,
      "learning_rate": 0.0001809720176730486,
      "loss": 0.3219,
      "step": 328
    },
    {
      "epoch": 0.09676470588235295,
      "grad_norm": 0.06434611231088638,
      "learning_rate": 0.00018091310751104566,
      "loss": 0.3222,
      "step": 329
    },
    {
      "epoch": 0.09705882352941177,
      "grad_norm": 0.06464701145887375,
      "learning_rate": 0.00018085419734904272,
      "loss": 0.3103,
      "step": 330
    },
    {
      "epoch": 0.09735294117647059,
      "grad_norm": 0.06848642975091934,
      "learning_rate": 0.00018079528718703978,
      "loss": 0.3865,
      "step": 331
    },
    {
      "epoch": 0.0976470588235294,
      "grad_norm": 0.06561613827943802,
      "learning_rate": 0.00018073637702503684,
      "loss": 0.3578,
      "step": 332
    },
    {
      "epoch": 0.09794117647058824,
      "grad_norm": 0.05055467411875725,
      "learning_rate": 0.00018067746686303387,
      "loss": 0.3225,
      "step": 333
    },
    {
      "epoch": 0.09823529411764706,
      "grad_norm": 0.07373936474323273,
      "learning_rate": 0.00018061855670103093,
      "loss": 0.3922,
      "step": 334
    },
    {
      "epoch": 0.09852941176470588,
      "grad_norm": 0.06367476284503937,
      "learning_rate": 0.000180559646539028,
      "loss": 0.3685,
      "step": 335
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 0.058183781802654266,
      "learning_rate": 0.00018050073637702505,
      "loss": 0.3361,
      "step": 336
    },
    {
      "epoch": 0.09911764705882353,
      "grad_norm": 0.061347659677267075,
      "learning_rate": 0.00018044182621502211,
      "loss": 0.4039,
      "step": 337
    },
    {
      "epoch": 0.09941176470588235,
      "grad_norm": 0.06506085395812988,
      "learning_rate": 0.00018038291605301915,
      "loss": 0.3489,
      "step": 338
    },
    {
      "epoch": 0.09970588235294117,
      "grad_norm": 0.04899275302886963,
      "learning_rate": 0.0001803240058910162,
      "loss": 0.3151,
      "step": 339
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.07107077538967133,
      "learning_rate": 0.00018026509572901327,
      "loss": 0.4117,
      "step": 340
    },
    {
      "epoch": 0.10029411764705883,
      "grad_norm": 0.06344206631183624,
      "learning_rate": 0.00018020618556701033,
      "loss": 0.3668,
      "step": 341
    },
    {
      "epoch": 0.10058823529411764,
      "grad_norm": 0.07463090121746063,
      "learning_rate": 0.0001801472754050074,
      "loss": 0.3728,
      "step": 342
    },
    {
      "epoch": 0.10088235294117646,
      "grad_norm": 0.06196259707212448,
      "learning_rate": 0.00018008836524300442,
      "loss": 0.3402,
      "step": 343
    },
    {
      "epoch": 0.1011764705882353,
      "grad_norm": 0.07231795787811279,
      "learning_rate": 0.00018002945508100148,
      "loss": 0.4009,
      "step": 344
    },
    {
      "epoch": 0.10147058823529412,
      "grad_norm": 0.07656300067901611,
      "learning_rate": 0.00017997054491899854,
      "loss": 0.3672,
      "step": 345
    },
    {
      "epoch": 0.10176470588235294,
      "grad_norm": 0.0582057349383831,
      "learning_rate": 0.0001799116347569956,
      "loss": 0.4122,
      "step": 346
    },
    {
      "epoch": 0.10205882352941177,
      "grad_norm": 0.06444188952445984,
      "learning_rate": 0.00017985272459499266,
      "loss": 0.3125,
      "step": 347
    },
    {
      "epoch": 0.10235294117647059,
      "grad_norm": 0.07201991975307465,
      "learning_rate": 0.0001797938144329897,
      "loss": 0.3101,
      "step": 348
    },
    {
      "epoch": 0.10264705882352941,
      "grad_norm": 0.06943051517009735,
      "learning_rate": 0.00017973490427098675,
      "loss": 0.3428,
      "step": 349
    },
    {
      "epoch": 0.10294117647058823,
      "grad_norm": 0.05349080264568329,
      "learning_rate": 0.00017967599410898382,
      "loss": 0.3251,
      "step": 350
    },
    {
      "epoch": 0.10323529411764706,
      "grad_norm": 0.05618695169687271,
      "learning_rate": 0.00017961708394698088,
      "loss": 0.317,
      "step": 351
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 0.05786282941699028,
      "learning_rate": 0.00017955817378497794,
      "loss": 0.3418,
      "step": 352
    },
    {
      "epoch": 0.1038235294117647,
      "grad_norm": 0.07092132419347763,
      "learning_rate": 0.00017949926362297497,
      "loss": 0.3658,
      "step": 353
    },
    {
      "epoch": 0.10411764705882352,
      "grad_norm": 0.07410195469856262,
      "learning_rate": 0.00017944035346097203,
      "loss": 0.4483,
      "step": 354
    },
    {
      "epoch": 0.10441176470588236,
      "grad_norm": 0.07642927020788193,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4236,
      "step": 355
    },
    {
      "epoch": 0.10470588235294118,
      "grad_norm": 0.09109827876091003,
      "learning_rate": 0.00017932253313696615,
      "loss": 0.4819,
      "step": 356
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06083108112215996,
      "learning_rate": 0.0001792636229749632,
      "loss": 0.3593,
      "step": 357
    },
    {
      "epoch": 0.10529411764705883,
      "grad_norm": 0.05710162967443466,
      "learning_rate": 0.00017920471281296024,
      "loss": 0.3548,
      "step": 358
    },
    {
      "epoch": 0.10558823529411765,
      "grad_norm": 0.06898236274719238,
      "learning_rate": 0.0001791458026509573,
      "loss": 0.4279,
      "step": 359
    },
    {
      "epoch": 0.10588235294117647,
      "grad_norm": 0.06828377395868301,
      "learning_rate": 0.00017908689248895436,
      "loss": 0.4319,
      "step": 360
    },
    {
      "epoch": 0.10617647058823529,
      "grad_norm": 0.07263288646936417,
      "learning_rate": 0.00017902798232695142,
      "loss": 0.4207,
      "step": 361
    },
    {
      "epoch": 0.10647058823529412,
      "grad_norm": 0.07407941669225693,
      "learning_rate": 0.00017896907216494848,
      "loss": 0.4381,
      "step": 362
    },
    {
      "epoch": 0.10676470588235294,
      "grad_norm": 0.07479868084192276,
      "learning_rate": 0.00017891016200294552,
      "loss": 0.3792,
      "step": 363
    },
    {
      "epoch": 0.10705882352941176,
      "grad_norm": 0.06474834680557251,
      "learning_rate": 0.00017885125184094258,
      "loss": 0.3294,
      "step": 364
    },
    {
      "epoch": 0.10735294117647058,
      "grad_norm": 0.05806325748562813,
      "learning_rate": 0.00017879234167893964,
      "loss": 0.3455,
      "step": 365
    },
    {
      "epoch": 0.10764705882352942,
      "grad_norm": 0.08173426985740662,
      "learning_rate": 0.0001787334315169367,
      "loss": 0.3826,
      "step": 366
    },
    {
      "epoch": 0.10794117647058823,
      "grad_norm": 0.06613181531429291,
      "learning_rate": 0.00017867452135493376,
      "loss": 0.3342,
      "step": 367
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 0.08808574080467224,
      "learning_rate": 0.0001786156111929308,
      "loss": 0.4492,
      "step": 368
    },
    {
      "epoch": 0.10852941176470589,
      "grad_norm": 0.06901829689741135,
      "learning_rate": 0.00017855670103092785,
      "loss": 0.4216,
      "step": 369
    },
    {
      "epoch": 0.10882352941176471,
      "grad_norm": 0.07953732460737228,
      "learning_rate": 0.0001784977908689249,
      "loss": 0.4006,
      "step": 370
    },
    {
      "epoch": 0.10911764705882353,
      "grad_norm": 0.07308214157819748,
      "learning_rate": 0.00017843888070692197,
      "loss": 0.3644,
      "step": 371
    },
    {
      "epoch": 0.10941176470588235,
      "grad_norm": 0.0722666010260582,
      "learning_rate": 0.00017837997054491903,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.10970588235294118,
      "grad_norm": 0.06988727301359177,
      "learning_rate": 0.00017832106038291606,
      "loss": 0.3816,
      "step": 373
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.062480855733156204,
      "learning_rate": 0.00017826215022091312,
      "loss": 0.3408,
      "step": 374
    },
    {
      "epoch": 0.11029411764705882,
      "grad_norm": 0.07396873086690903,
      "learning_rate": 0.00017820324005891018,
      "loss": 0.3718,
      "step": 375
    },
    {
      "epoch": 0.11058823529411765,
      "grad_norm": 0.045270275324583054,
      "learning_rate": 0.00017814432989690724,
      "loss": 0.2581,
      "step": 376
    },
    {
      "epoch": 0.11088235294117647,
      "grad_norm": 0.05783374607563019,
      "learning_rate": 0.00017808541973490428,
      "loss": 0.3824,
      "step": 377
    },
    {
      "epoch": 0.1111764705882353,
      "grad_norm": 0.09262153506278992,
      "learning_rate": 0.0001780265095729013,
      "loss": 0.3936,
      "step": 378
    },
    {
      "epoch": 0.11147058823529411,
      "grad_norm": 0.07748464494943619,
      "learning_rate": 0.00017796759941089837,
      "loss": 0.3862,
      "step": 379
    },
    {
      "epoch": 0.11176470588235295,
      "grad_norm": 0.047957275062799454,
      "learning_rate": 0.00017790868924889543,
      "loss": 0.3173,
      "step": 380
    },
    {
      "epoch": 0.11205882352941177,
      "grad_norm": 0.05351791903376579,
      "learning_rate": 0.0001778497790868925,
      "loss": 0.3197,
      "step": 381
    },
    {
      "epoch": 0.11235294117647059,
      "grad_norm": 0.06672481447458267,
      "learning_rate": 0.00017779086892488955,
      "loss": 0.367,
      "step": 382
    },
    {
      "epoch": 0.1126470588235294,
      "grad_norm": 0.0817033052444458,
      "learning_rate": 0.00017773195876288658,
      "loss": 0.3801,
      "step": 383
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.05897071585059166,
      "learning_rate": 0.00017767304860088364,
      "loss": 0.3172,
      "step": 384
    },
    {
      "epoch": 0.11323529411764706,
      "grad_norm": 0.06713476777076721,
      "learning_rate": 0.0001776141384388807,
      "loss": 0.4544,
      "step": 385
    },
    {
      "epoch": 0.11352941176470588,
      "grad_norm": 0.06568536907434464,
      "learning_rate": 0.00017755522827687776,
      "loss": 0.321,
      "step": 386
    },
    {
      "epoch": 0.11382352941176471,
      "grad_norm": 0.05591333657503128,
      "learning_rate": 0.00017749631811487482,
      "loss": 0.3195,
      "step": 387
    },
    {
      "epoch": 0.11411764705882353,
      "grad_norm": 0.06133725121617317,
      "learning_rate": 0.00017743740795287186,
      "loss": 0.3934,
      "step": 388
    },
    {
      "epoch": 0.11441176470588235,
      "grad_norm": 0.05600662901997566,
      "learning_rate": 0.00017737849779086892,
      "loss": 0.3227,
      "step": 389
    },
    {
      "epoch": 0.11470588235294117,
      "grad_norm": 0.052532583475112915,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.3207,
      "step": 390
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.0512576550245285,
      "learning_rate": 0.00017726067746686304,
      "loss": 0.3087,
      "step": 391
    },
    {
      "epoch": 0.11529411764705882,
      "grad_norm": 0.05669146031141281,
      "learning_rate": 0.0001772017673048601,
      "loss": 0.3183,
      "step": 392
    },
    {
      "epoch": 0.11558823529411764,
      "grad_norm": 0.06597939878702164,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3809,
      "step": 393
    },
    {
      "epoch": 0.11588235294117646,
      "grad_norm": 0.059711527079343796,
      "learning_rate": 0.0001770839469808542,
      "loss": 0.3867,
      "step": 394
    },
    {
      "epoch": 0.1161764705882353,
      "grad_norm": 0.08614617586135864,
      "learning_rate": 0.00017702503681885125,
      "loss": 0.4372,
      "step": 395
    },
    {
      "epoch": 0.11647058823529412,
      "grad_norm": 0.08026904612779617,
      "learning_rate": 0.0001769661266568483,
      "loss": 0.4646,
      "step": 396
    },
    {
      "epoch": 0.11676470588235294,
      "grad_norm": 0.05460386350750923,
      "learning_rate": 0.00017690721649484537,
      "loss": 0.3232,
      "step": 397
    },
    {
      "epoch": 0.11705882352941177,
      "grad_norm": 0.06595972180366516,
      "learning_rate": 0.0001768483063328424,
      "loss": 0.3641,
      "step": 398
    },
    {
      "epoch": 0.11735294117647059,
      "grad_norm": 0.0833452045917511,
      "learning_rate": 0.00017678939617083947,
      "loss": 0.3866,
      "step": 399
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.062220629304647446,
      "learning_rate": 0.00017673048600883653,
      "loss": 0.3576,
      "step": 400
    },
    {
      "epoch": 0.11794117647058823,
      "grad_norm": 0.08257558196783066,
      "learning_rate": 0.00017667157584683359,
      "loss": 0.4615,
      "step": 401
    },
    {
      "epoch": 0.11823529411764706,
      "grad_norm": 0.07315663993358612,
      "learning_rate": 0.00017661266568483065,
      "loss": 0.4741,
      "step": 402
    },
    {
      "epoch": 0.11852941176470588,
      "grad_norm": 0.07738703489303589,
      "learning_rate": 0.00017655375552282768,
      "loss": 0.3537,
      "step": 403
    },
    {
      "epoch": 0.1188235294117647,
      "grad_norm": 0.06754371523857117,
      "learning_rate": 0.00017649484536082474,
      "loss": 0.336,
      "step": 404
    },
    {
      "epoch": 0.11911764705882352,
      "grad_norm": 0.06247233599424362,
      "learning_rate": 0.0001764359351988218,
      "loss": 0.3823,
      "step": 405
    },
    {
      "epoch": 0.11941176470588236,
      "grad_norm": 0.077308289706707,
      "learning_rate": 0.00017637702503681886,
      "loss": 0.3849,
      "step": 406
    },
    {
      "epoch": 0.11970588235294118,
      "grad_norm": 0.05560912936925888,
      "learning_rate": 0.00017631811487481592,
      "loss": 0.331,
      "step": 407
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07612839341163635,
      "learning_rate": 0.00017625920471281295,
      "loss": 0.4348,
      "step": 408
    },
    {
      "epoch": 0.12029411764705883,
      "grad_norm": 0.0655772015452385,
      "learning_rate": 0.00017620029455081,
      "loss": 0.3628,
      "step": 409
    },
    {
      "epoch": 0.12058823529411765,
      "grad_norm": 0.0807056576013565,
      "learning_rate": 0.00017614138438880707,
      "loss": 0.4177,
      "step": 410
    },
    {
      "epoch": 0.12088235294117647,
      "grad_norm": 0.06848520785570145,
      "learning_rate": 0.00017608247422680413,
      "loss": 0.39,
      "step": 411
    },
    {
      "epoch": 0.12117647058823529,
      "grad_norm": 0.05728285014629364,
      "learning_rate": 0.0001760235640648012,
      "loss": 0.3723,
      "step": 412
    },
    {
      "epoch": 0.12147058823529412,
      "grad_norm": 0.05980508774518967,
      "learning_rate": 0.00017596465390279823,
      "loss": 0.4077,
      "step": 413
    },
    {
      "epoch": 0.12176470588235294,
      "grad_norm": 0.04307478293776512,
      "learning_rate": 0.00017590574374079529,
      "loss": 0.3021,
      "step": 414
    },
    {
      "epoch": 0.12205882352941176,
      "grad_norm": 0.07224758714437485,
      "learning_rate": 0.00017584683357879235,
      "loss": 0.3899,
      "step": 415
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 0.060470569878816605,
      "learning_rate": 0.0001757879234167894,
      "loss": 0.313,
      "step": 416
    },
    {
      "epoch": 0.12264705882352941,
      "grad_norm": 0.08347459137439728,
      "learning_rate": 0.00017572901325478647,
      "loss": 0.4183,
      "step": 417
    },
    {
      "epoch": 0.12294117647058823,
      "grad_norm": 0.05439579114317894,
      "learning_rate": 0.0001756701030927835,
      "loss": 0.3214,
      "step": 418
    },
    {
      "epoch": 0.12323529411764705,
      "grad_norm": 0.056708924472332,
      "learning_rate": 0.00017561119293078056,
      "loss": 0.3282,
      "step": 419
    },
    {
      "epoch": 0.12352941176470589,
      "grad_norm": 0.0986727923154831,
      "learning_rate": 0.00017555228276877762,
      "loss": 0.3043,
      "step": 420
    },
    {
      "epoch": 0.12382352941176471,
      "grad_norm": 0.0913943499326706,
      "learning_rate": 0.00017549337260677468,
      "loss": 0.4343,
      "step": 421
    },
    {
      "epoch": 0.12411764705882353,
      "grad_norm": 0.05140068382024765,
      "learning_rate": 0.00017543446244477174,
      "loss": 0.3159,
      "step": 422
    },
    {
      "epoch": 0.12441176470588235,
      "grad_norm": 0.08153297007083893,
      "learning_rate": 0.00017537555228276877,
      "loss": 0.3605,
      "step": 423
    },
    {
      "epoch": 0.12470588235294118,
      "grad_norm": 0.08590778708457947,
      "learning_rate": 0.00017531664212076583,
      "loss": 0.4171,
      "step": 424
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06797812134027481,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4094,
      "step": 425
    },
    {
      "epoch": 0.12529411764705883,
      "grad_norm": 0.07034395635128021,
      "learning_rate": 0.00017519882179675995,
      "loss": 0.4094,
      "step": 426
    },
    {
      "epoch": 0.12558823529411764,
      "grad_norm": 0.06863990426063538,
      "learning_rate": 0.00017513991163475701,
      "loss": 0.3671,
      "step": 427
    },
    {
      "epoch": 0.12588235294117647,
      "grad_norm": 0.05765393748879433,
      "learning_rate": 0.00017508100147275405,
      "loss": 0.3131,
      "step": 428
    },
    {
      "epoch": 0.1261764705882353,
      "grad_norm": 0.05362609773874283,
      "learning_rate": 0.0001750220913107511,
      "loss": 0.2999,
      "step": 429
    },
    {
      "epoch": 0.1264705882352941,
      "grad_norm": 0.08066284656524658,
      "learning_rate": 0.00017496318114874817,
      "loss": 0.3869,
      "step": 430
    },
    {
      "epoch": 0.12676470588235295,
      "grad_norm": 0.06270390748977661,
      "learning_rate": 0.00017490427098674523,
      "loss": 0.3278,
      "step": 431
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 0.07776700705289841,
      "learning_rate": 0.0001748453608247423,
      "loss": 0.4157,
      "step": 432
    },
    {
      "epoch": 0.12735294117647059,
      "grad_norm": 0.06161027401685715,
      "learning_rate": 0.00017478645066273932,
      "loss": 0.3204,
      "step": 433
    },
    {
      "epoch": 0.12764705882352942,
      "grad_norm": 0.054306965321302414,
      "learning_rate": 0.00017472754050073638,
      "loss": 0.3386,
      "step": 434
    },
    {
      "epoch": 0.12794117647058822,
      "grad_norm": 0.05765855684876442,
      "learning_rate": 0.00017466863033873344,
      "loss": 0.3537,
      "step": 435
    },
    {
      "epoch": 0.12823529411764706,
      "grad_norm": 0.08346139639616013,
      "learning_rate": 0.0001746097201767305,
      "loss": 0.4039,
      "step": 436
    },
    {
      "epoch": 0.1285294117647059,
      "grad_norm": 0.056448858231306076,
      "learning_rate": 0.00017455081001472756,
      "loss": 0.3729,
      "step": 437
    },
    {
      "epoch": 0.1288235294117647,
      "grad_norm": 0.07123912870883942,
      "learning_rate": 0.0001744918998527246,
      "loss": 0.4258,
      "step": 438
    },
    {
      "epoch": 0.12911764705882353,
      "grad_norm": 0.06362074613571167,
      "learning_rate": 0.00017443298969072165,
      "loss": 0.3943,
      "step": 439
    },
    {
      "epoch": 0.12941176470588237,
      "grad_norm": 0.06075656786561012,
      "learning_rate": 0.00017437407952871872,
      "loss": 0.371,
      "step": 440
    },
    {
      "epoch": 0.12970588235294117,
      "grad_norm": 0.06329082697629929,
      "learning_rate": 0.00017431516936671578,
      "loss": 0.347,
      "step": 441
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.06610801815986633,
      "learning_rate": 0.00017425625920471284,
      "loss": 0.3524,
      "step": 442
    },
    {
      "epoch": 0.1302941176470588,
      "grad_norm": 0.07422363013029099,
      "learning_rate": 0.00017419734904270987,
      "loss": 0.3686,
      "step": 443
    },
    {
      "epoch": 0.13058823529411764,
      "grad_norm": 0.0583348348736763,
      "learning_rate": 0.00017413843888070693,
      "loss": 0.37,
      "step": 444
    },
    {
      "epoch": 0.13088235294117648,
      "grad_norm": 0.06199157238006592,
      "learning_rate": 0.000174079528718704,
      "loss": 0.3247,
      "step": 445
    },
    {
      "epoch": 0.13117647058823528,
      "grad_norm": 0.07494378089904785,
      "learning_rate": 0.00017402061855670105,
      "loss": 0.4019,
      "step": 446
    },
    {
      "epoch": 0.13147058823529412,
      "grad_norm": 0.05664242058992386,
      "learning_rate": 0.0001739617083946981,
      "loss": 0.3218,
      "step": 447
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 0.05308847874403,
      "learning_rate": 0.00017390279823269514,
      "loss": 0.3145,
      "step": 448
    },
    {
      "epoch": 0.13205882352941176,
      "grad_norm": 0.07317924499511719,
      "learning_rate": 0.0001738438880706922,
      "loss": 0.3673,
      "step": 449
    },
    {
      "epoch": 0.1323529411764706,
      "grad_norm": 0.06949937343597412,
      "learning_rate": 0.00017378497790868926,
      "loss": 0.3324,
      "step": 450
    },
    {
      "epoch": 0.13264705882352942,
      "grad_norm": 0.05919482558965683,
      "learning_rate": 0.00017372606774668632,
      "loss": 0.4061,
      "step": 451
    },
    {
      "epoch": 0.13294117647058823,
      "grad_norm": 0.04733148589730263,
      "learning_rate": 0.00017366715758468338,
      "loss": 0.2809,
      "step": 452
    },
    {
      "epoch": 0.13323529411764706,
      "grad_norm": 0.06390117108821869,
      "learning_rate": 0.00017360824742268042,
      "loss": 0.3655,
      "step": 453
    },
    {
      "epoch": 0.13352941176470587,
      "grad_norm": 0.05269750580191612,
      "learning_rate": 0.00017354933726067748,
      "loss": 0.3068,
      "step": 454
    },
    {
      "epoch": 0.1338235294117647,
      "grad_norm": 0.05385202169418335,
      "learning_rate": 0.00017349042709867454,
      "loss": 0.3273,
      "step": 455
    },
    {
      "epoch": 0.13411764705882354,
      "grad_norm": 0.061493147164583206,
      "learning_rate": 0.0001734315169366716,
      "loss": 0.3199,
      "step": 456
    },
    {
      "epoch": 0.13441176470588234,
      "grad_norm": 0.05856568366289139,
      "learning_rate": 0.00017337260677466866,
      "loss": 0.3036,
      "step": 457
    },
    {
      "epoch": 0.13470588235294118,
      "grad_norm": 0.06415963172912598,
      "learning_rate": 0.0001733136966126657,
      "loss": 0.3535,
      "step": 458
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.05263753607869148,
      "learning_rate": 0.00017325478645066275,
      "loss": 0.354,
      "step": 459
    },
    {
      "epoch": 0.13529411764705881,
      "grad_norm": 0.052405644208192825,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.2826,
      "step": 460
    },
    {
      "epoch": 0.13558823529411765,
      "grad_norm": 0.0717768445611,
      "learning_rate": 0.00017313696612665687,
      "loss": 0.3819,
      "step": 461
    },
    {
      "epoch": 0.13588235294117648,
      "grad_norm": 0.05164726451039314,
      "learning_rate": 0.00017307805596465393,
      "loss": 0.3202,
      "step": 462
    },
    {
      "epoch": 0.1361764705882353,
      "grad_norm": 0.06342334300279617,
      "learning_rate": 0.00017301914580265096,
      "loss": 0.3069,
      "step": 463
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 0.0639723390340805,
      "learning_rate": 0.00017296023564064802,
      "loss": 0.34,
      "step": 464
    },
    {
      "epoch": 0.13676470588235295,
      "grad_norm": 0.057234782725572586,
      "learning_rate": 0.00017290132547864508,
      "loss": 0.3596,
      "step": 465
    },
    {
      "epoch": 0.13705882352941176,
      "grad_norm": 0.0710613876581192,
      "learning_rate": 0.00017284241531664214,
      "loss": 0.4529,
      "step": 466
    },
    {
      "epoch": 0.1373529411764706,
      "grad_norm": 0.05788923054933548,
      "learning_rate": 0.0001727835051546392,
      "loss": 0.3876,
      "step": 467
    },
    {
      "epoch": 0.1376470588235294,
      "grad_norm": 0.05585173889994621,
      "learning_rate": 0.00017272459499263624,
      "loss": 0.3995,
      "step": 468
    },
    {
      "epoch": 0.13794117647058823,
      "grad_norm": 0.0703703761100769,
      "learning_rate": 0.0001726656848306333,
      "loss": 0.3501,
      "step": 469
    },
    {
      "epoch": 0.13823529411764707,
      "grad_norm": 0.05603313073515892,
      "learning_rate": 0.00017260677466863036,
      "loss": 0.3252,
      "step": 470
    },
    {
      "epoch": 0.13852941176470587,
      "grad_norm": 0.049182724207639694,
      "learning_rate": 0.00017254786450662742,
      "loss": 0.3075,
      "step": 471
    },
    {
      "epoch": 0.1388235294117647,
      "grad_norm": 0.07177457213401794,
      "learning_rate": 0.00017248895434462448,
      "loss": 0.3536,
      "step": 472
    },
    {
      "epoch": 0.13911764705882354,
      "grad_norm": 0.05876198038458824,
      "learning_rate": 0.0001724300441826215,
      "loss": 0.2987,
      "step": 473
    },
    {
      "epoch": 0.13941176470588235,
      "grad_norm": 0.0633619874715805,
      "learning_rate": 0.00017237113402061857,
      "loss": 0.3818,
      "step": 474
    },
    {
      "epoch": 0.13970588235294118,
      "grad_norm": 0.055080026388168335,
      "learning_rate": 0.00017231222385861563,
      "loss": 0.314,
      "step": 475
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.04072139039635658,
      "learning_rate": 0.0001722533136966127,
      "loss": 0.2703,
      "step": 476
    },
    {
      "epoch": 0.14029411764705882,
      "grad_norm": 0.05345906317234039,
      "learning_rate": 0.00017219440353460975,
      "loss": 0.4003,
      "step": 477
    },
    {
      "epoch": 0.14058823529411765,
      "grad_norm": 0.0584883950650692,
      "learning_rate": 0.00017213549337260678,
      "loss": 0.3767,
      "step": 478
    },
    {
      "epoch": 0.14088235294117646,
      "grad_norm": 0.06285300105810165,
      "learning_rate": 0.00017207658321060384,
      "loss": 0.3324,
      "step": 479
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 0.049121711403131485,
      "learning_rate": 0.0001720176730486009,
      "loss": 0.3386,
      "step": 480
    },
    {
      "epoch": 0.14147058823529413,
      "grad_norm": 0.07132626324892044,
      "learning_rate": 0.00017195876288659796,
      "loss": 0.3565,
      "step": 481
    },
    {
      "epoch": 0.14176470588235293,
      "grad_norm": 0.06294410675764084,
      "learning_rate": 0.00017189985272459503,
      "loss": 0.3789,
      "step": 482
    },
    {
      "epoch": 0.14205882352941177,
      "grad_norm": 0.07501737773418427,
      "learning_rate": 0.00017184094256259203,
      "loss": 0.316,
      "step": 483
    },
    {
      "epoch": 0.1423529411764706,
      "grad_norm": 0.058059290051460266,
      "learning_rate": 0.0001717820324005891,
      "loss": 0.3649,
      "step": 484
    },
    {
      "epoch": 0.1426470588235294,
      "grad_norm": 0.051437921822071075,
      "learning_rate": 0.00017172312223858615,
      "loss": 0.3134,
      "step": 485
    },
    {
      "epoch": 0.14294117647058824,
      "grad_norm": 0.06578783690929413,
      "learning_rate": 0.0001716642120765832,
      "loss": 0.408,
      "step": 486
    },
    {
      "epoch": 0.14323529411764707,
      "grad_norm": 0.054990254342556,
      "learning_rate": 0.00017160530191458027,
      "loss": 0.3665,
      "step": 487
    },
    {
      "epoch": 0.14352941176470588,
      "grad_norm": 0.05266501009464264,
      "learning_rate": 0.0001715463917525773,
      "loss": 0.2937,
      "step": 488
    },
    {
      "epoch": 0.1438235294117647,
      "grad_norm": 0.06444081664085388,
      "learning_rate": 0.00017148748159057437,
      "loss": 0.3852,
      "step": 489
    },
    {
      "epoch": 0.14411764705882352,
      "grad_norm": 0.07051171362400055,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4163,
      "step": 490
    },
    {
      "epoch": 0.14441176470588235,
      "grad_norm": 0.04941039904952049,
      "learning_rate": 0.00017136966126656849,
      "loss": 0.3275,
      "step": 491
    },
    {
      "epoch": 0.14470588235294118,
      "grad_norm": 0.07058579474687576,
      "learning_rate": 0.00017131075110456555,
      "loss": 0.3945,
      "step": 492
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05393172800540924,
      "learning_rate": 0.00017125184094256258,
      "loss": 0.3068,
      "step": 493
    },
    {
      "epoch": 0.14529411764705882,
      "grad_norm": 0.05968550592660904,
      "learning_rate": 0.00017119293078055964,
      "loss": 0.4008,
      "step": 494
    },
    {
      "epoch": 0.14558823529411766,
      "grad_norm": 0.06070926785469055,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.3796,
      "step": 495
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 0.04709979146718979,
      "learning_rate": 0.00017107511045655376,
      "loss": 0.2895,
      "step": 496
    },
    {
      "epoch": 0.1461764705882353,
      "grad_norm": 0.050172340124845505,
      "learning_rate": 0.00017101620029455082,
      "loss": 0.3294,
      "step": 497
    },
    {
      "epoch": 0.14647058823529413,
      "grad_norm": 0.07811614871025085,
      "learning_rate": 0.00017095729013254785,
      "loss": 0.3634,
      "step": 498
    },
    {
      "epoch": 0.14676470588235294,
      "grad_norm": 0.052022531628608704,
      "learning_rate": 0.0001708983799705449,
      "loss": 0.3388,
      "step": 499
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.057600993663072586,
      "learning_rate": 0.00017083946980854197,
      "loss": 0.3581,
      "step": 500
    },
    {
      "epoch": 0.14735294117647058,
      "grad_norm": 0.06193580478429794,
      "learning_rate": 0.00017078055964653903,
      "loss": 0.3617,
      "step": 501
    },
    {
      "epoch": 0.1476470588235294,
      "grad_norm": 0.07039792090654373,
      "learning_rate": 0.0001707216494845361,
      "loss": 0.4368,
      "step": 502
    },
    {
      "epoch": 0.14794117647058824,
      "grad_norm": 0.05649368837475777,
      "learning_rate": 0.00017066273932253313,
      "loss": 0.2813,
      "step": 503
    },
    {
      "epoch": 0.14823529411764705,
      "grad_norm": 0.07238007336854935,
      "learning_rate": 0.00017060382916053019,
      "loss": 0.3657,
      "step": 504
    },
    {
      "epoch": 0.14852941176470588,
      "grad_norm": 0.06057054549455643,
      "learning_rate": 0.00017054491899852725,
      "loss": 0.3245,
      "step": 505
    },
    {
      "epoch": 0.14882352941176472,
      "grad_norm": 0.06513822078704834,
      "learning_rate": 0.0001704860088365243,
      "loss": 0.4206,
      "step": 506
    },
    {
      "epoch": 0.14911764705882352,
      "grad_norm": 0.05669257044792175,
      "learning_rate": 0.00017042709867452137,
      "loss": 0.3914,
      "step": 507
    },
    {
      "epoch": 0.14941176470588236,
      "grad_norm": 0.06805407255887985,
      "learning_rate": 0.0001703681885125184,
      "loss": 0.4091,
      "step": 508
    },
    {
      "epoch": 0.1497058823529412,
      "grad_norm": 0.06307753920555115,
      "learning_rate": 0.00017030927835051546,
      "loss": 0.3465,
      "step": 509
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06840942800045013,
      "learning_rate": 0.00017025036818851252,
      "loss": 0.3658,
      "step": 510
    },
    {
      "epoch": 0.15029411764705883,
      "grad_norm": 0.053714569658041,
      "learning_rate": 0.00017019145802650958,
      "loss": 0.3499,
      "step": 511
    },
    {
      "epoch": 0.15058823529411763,
      "grad_norm": 0.040315892547369,
      "learning_rate": 0.00017013254786450664,
      "loss": 0.2971,
      "step": 512
    },
    {
      "epoch": 0.15088235294117647,
      "grad_norm": 0.05545332655310631,
      "learning_rate": 0.00017007363770250367,
      "loss": 0.3312,
      "step": 513
    },
    {
      "epoch": 0.1511764705882353,
      "grad_norm": 0.05611592158675194,
      "learning_rate": 0.00017001472754050073,
      "loss": 0.3433,
      "step": 514
    },
    {
      "epoch": 0.1514705882352941,
      "grad_norm": 0.05403720587491989,
      "learning_rate": 0.0001699558173784978,
      "loss": 0.3296,
      "step": 515
    },
    {
      "epoch": 0.15176470588235294,
      "grad_norm": 0.06694722175598145,
      "learning_rate": 0.00016989690721649485,
      "loss": 0.3829,
      "step": 516
    },
    {
      "epoch": 0.15205882352941177,
      "grad_norm": 0.06600271910429001,
      "learning_rate": 0.00016983799705449191,
      "loss": 0.3239,
      "step": 517
    },
    {
      "epoch": 0.15235294117647058,
      "grad_norm": 0.07736946642398834,
      "learning_rate": 0.00016977908689248895,
      "loss": 0.3627,
      "step": 518
    },
    {
      "epoch": 0.1526470588235294,
      "grad_norm": 0.05466568470001221,
      "learning_rate": 0.000169720176730486,
      "loss": 0.2987,
      "step": 519
    },
    {
      "epoch": 0.15294117647058825,
      "grad_norm": 0.0472368523478508,
      "learning_rate": 0.00016966126656848307,
      "loss": 0.2589,
      "step": 520
    },
    {
      "epoch": 0.15323529411764705,
      "grad_norm": 0.08170114457607269,
      "learning_rate": 0.00016960235640648013,
      "loss": 0.3619,
      "step": 521
    },
    {
      "epoch": 0.1535294117647059,
      "grad_norm": 0.07004242390394211,
      "learning_rate": 0.0001695434462444772,
      "loss": 0.397,
      "step": 522
    },
    {
      "epoch": 0.1538235294117647,
      "grad_norm": 0.04679474979639053,
      "learning_rate": 0.00016948453608247422,
      "loss": 0.2954,
      "step": 523
    },
    {
      "epoch": 0.15411764705882353,
      "grad_norm": 0.04191538691520691,
      "learning_rate": 0.00016942562592047128,
      "loss": 0.3184,
      "step": 524
    },
    {
      "epoch": 0.15441176470588236,
      "grad_norm": 0.05352380871772766,
      "learning_rate": 0.00016936671575846834,
      "loss": 0.3021,
      "step": 525
    },
    {
      "epoch": 0.15470588235294117,
      "grad_norm": 0.06191958487033844,
      "learning_rate": 0.0001693078055964654,
      "loss": 0.2907,
      "step": 526
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.06200692802667618,
      "learning_rate": 0.00016924889543446246,
      "loss": 0.3711,
      "step": 527
    },
    {
      "epoch": 0.15529411764705883,
      "grad_norm": 0.05208279937505722,
      "learning_rate": 0.0001691899852724595,
      "loss": 0.3267,
      "step": 528
    },
    {
      "epoch": 0.15558823529411764,
      "grad_norm": 0.07539403438568115,
      "learning_rate": 0.00016913107511045655,
      "loss": 0.4333,
      "step": 529
    },
    {
      "epoch": 0.15588235294117647,
      "grad_norm": 0.05377708747982979,
      "learning_rate": 0.00016907216494845361,
      "loss": 0.2867,
      "step": 530
    },
    {
      "epoch": 0.1561764705882353,
      "grad_norm": 0.051238998770713806,
      "learning_rate": 0.00016901325478645068,
      "loss": 0.3736,
      "step": 531
    },
    {
      "epoch": 0.1564705882352941,
      "grad_norm": 0.05820276588201523,
      "learning_rate": 0.00016895434462444774,
      "loss": 0.3972,
      "step": 532
    },
    {
      "epoch": 0.15676470588235294,
      "grad_norm": 0.047747302800416946,
      "learning_rate": 0.00016889543446244477,
      "loss": 0.3116,
      "step": 533
    },
    {
      "epoch": 0.15705882352941178,
      "grad_norm": 0.07083246856927872,
      "learning_rate": 0.00016883652430044183,
      "loss": 0.3625,
      "step": 534
    },
    {
      "epoch": 0.15735294117647058,
      "grad_norm": 0.058713871985673904,
      "learning_rate": 0.0001687776141384389,
      "loss": 0.3754,
      "step": 535
    },
    {
      "epoch": 0.15764705882352942,
      "grad_norm": 0.05031213164329529,
      "learning_rate": 0.00016871870397643595,
      "loss": 0.3209,
      "step": 536
    },
    {
      "epoch": 0.15794117647058822,
      "grad_norm": 0.05241355672478676,
      "learning_rate": 0.000168659793814433,
      "loss": 0.356,
      "step": 537
    },
    {
      "epoch": 0.15823529411764706,
      "grad_norm": 0.049185171723365784,
      "learning_rate": 0.00016860088365243004,
      "loss": 0.3436,
      "step": 538
    },
    {
      "epoch": 0.1585294117647059,
      "grad_norm": 0.05663933604955673,
      "learning_rate": 0.0001685419734904271,
      "loss": 0.3484,
      "step": 539
    },
    {
      "epoch": 0.1588235294117647,
      "grad_norm": 0.054598014801740646,
      "learning_rate": 0.00016848306332842416,
      "loss": 0.3277,
      "step": 540
    },
    {
      "epoch": 0.15911764705882353,
      "grad_norm": 0.07492633163928986,
      "learning_rate": 0.00016842415316642122,
      "loss": 0.3594,
      "step": 541
    },
    {
      "epoch": 0.15941176470588236,
      "grad_norm": 0.06319060176610947,
      "learning_rate": 0.00016836524300441828,
      "loss": 0.271,
      "step": 542
    },
    {
      "epoch": 0.15970588235294117,
      "grad_norm": 0.06293650716543198,
      "learning_rate": 0.00016830633284241532,
      "loss": 0.4016,
      "step": 543
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.060195211321115494,
      "learning_rate": 0.00016824742268041238,
      "loss": 0.3704,
      "step": 544
    },
    {
      "epoch": 0.16029411764705884,
      "grad_norm": 0.04301571846008301,
      "learning_rate": 0.00016818851251840944,
      "loss": 0.2679,
      "step": 545
    },
    {
      "epoch": 0.16058823529411764,
      "grad_norm": 0.06011172756552696,
      "learning_rate": 0.0001681296023564065,
      "loss": 0.3569,
      "step": 546
    },
    {
      "epoch": 0.16088235294117648,
      "grad_norm": 0.045250967144966125,
      "learning_rate": 0.00016807069219440356,
      "loss": 0.2533,
      "step": 547
    },
    {
      "epoch": 0.16117647058823528,
      "grad_norm": 0.056782059371471405,
      "learning_rate": 0.0001680117820324006,
      "loss": 0.2964,
      "step": 548
    },
    {
      "epoch": 0.16147058823529412,
      "grad_norm": 0.058335352689027786,
      "learning_rate": 0.00016795287187039765,
      "loss": 0.3577,
      "step": 549
    },
    {
      "epoch": 0.16176470588235295,
      "grad_norm": 0.05302882567048073,
      "learning_rate": 0.0001678939617083947,
      "loss": 0.3866,
      "step": 550
    },
    {
      "epoch": 0.16205882352941176,
      "grad_norm": 0.045930467545986176,
      "learning_rate": 0.00016783505154639177,
      "loss": 0.341,
      "step": 551
    },
    {
      "epoch": 0.1623529411764706,
      "grad_norm": 0.04835527762770653,
      "learning_rate": 0.00016777614138438883,
      "loss": 0.3106,
      "step": 552
    },
    {
      "epoch": 0.16264705882352942,
      "grad_norm": 0.06390727311372757,
      "learning_rate": 0.00016771723122238586,
      "loss": 0.268,
      "step": 553
    },
    {
      "epoch": 0.16294117647058823,
      "grad_norm": 0.0707143023610115,
      "learning_rate": 0.00016765832106038292,
      "loss": 0.4432,
      "step": 554
    },
    {
      "epoch": 0.16323529411764706,
      "grad_norm": 0.07104025781154633,
      "learning_rate": 0.00016759941089837998,
      "loss": 0.4227,
      "step": 555
    },
    {
      "epoch": 0.1635294117647059,
      "grad_norm": 0.05053001269698143,
      "learning_rate": 0.00016754050073637704,
      "loss": 0.3172,
      "step": 556
    },
    {
      "epoch": 0.1638235294117647,
      "grad_norm": 0.06678225100040436,
      "learning_rate": 0.0001674815905743741,
      "loss": 0.3237,
      "step": 557
    },
    {
      "epoch": 0.16411764705882353,
      "grad_norm": 0.06146559491753578,
      "learning_rate": 0.00016742268041237114,
      "loss": 0.3779,
      "step": 558
    },
    {
      "epoch": 0.16441176470588234,
      "grad_norm": 0.06342355161905289,
      "learning_rate": 0.0001673637702503682,
      "loss": 0.3702,
      "step": 559
    },
    {
      "epoch": 0.16470588235294117,
      "grad_norm": 0.06218024343252182,
      "learning_rate": 0.00016730486008836526,
      "loss": 0.4087,
      "step": 560
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.06960968673229218,
      "learning_rate": 0.00016724594992636232,
      "loss": 0.3562,
      "step": 561
    },
    {
      "epoch": 0.1652941176470588,
      "grad_norm": 0.07093697786331177,
      "learning_rate": 0.00016718703976435938,
      "loss": 0.3583,
      "step": 562
    },
    {
      "epoch": 0.16558823529411765,
      "grad_norm": 0.0660216435790062,
      "learning_rate": 0.0001671281296023564,
      "loss": 0.4231,
      "step": 563
    },
    {
      "epoch": 0.16588235294117648,
      "grad_norm": 0.0505535863339901,
      "learning_rate": 0.00016706921944035347,
      "loss": 0.3371,
      "step": 564
    },
    {
      "epoch": 0.1661764705882353,
      "grad_norm": 0.052712347358465195,
      "learning_rate": 0.00016701030927835053,
      "loss": 0.3381,
      "step": 565
    },
    {
      "epoch": 0.16647058823529412,
      "grad_norm": 0.06701494753360748,
      "learning_rate": 0.0001669513991163476,
      "loss": 0.4006,
      "step": 566
    },
    {
      "epoch": 0.16676470588235295,
      "grad_norm": 0.06080479919910431,
      "learning_rate": 0.00016689248895434465,
      "loss": 0.3621,
      "step": 567
    },
    {
      "epoch": 0.16705882352941176,
      "grad_norm": 0.05409641191363335,
      "learning_rate": 0.00016683357879234168,
      "loss": 0.3243,
      "step": 568
    },
    {
      "epoch": 0.1673529411764706,
      "grad_norm": 0.07134998589754105,
      "learning_rate": 0.00016677466863033874,
      "loss": 0.3008,
      "step": 569
    },
    {
      "epoch": 0.1676470588235294,
      "grad_norm": 0.07482819259166718,
      "learning_rate": 0.0001667157584683358,
      "loss": 0.3207,
      "step": 570
    },
    {
      "epoch": 0.16794117647058823,
      "grad_norm": 0.06479887664318085,
      "learning_rate": 0.00016665684830633286,
      "loss": 0.412,
      "step": 571
    },
    {
      "epoch": 0.16823529411764707,
      "grad_norm": 0.061265431344509125,
      "learning_rate": 0.00016659793814432993,
      "loss": 0.3652,
      "step": 572
    },
    {
      "epoch": 0.16852941176470587,
      "grad_norm": 0.0703388974070549,
      "learning_rate": 0.00016653902798232696,
      "loss": 0.3805,
      "step": 573
    },
    {
      "epoch": 0.1688235294117647,
      "grad_norm": 0.052610062062740326,
      "learning_rate": 0.00016648011782032402,
      "loss": 0.3312,
      "step": 574
    },
    {
      "epoch": 0.16911764705882354,
      "grad_norm": 0.06696663051843643,
      "learning_rate": 0.00016642120765832108,
      "loss": 0.4088,
      "step": 575
    },
    {
      "epoch": 0.16941176470588235,
      "grad_norm": 0.06395287066698074,
      "learning_rate": 0.00016636229749631814,
      "loss": 0.3971,
      "step": 576
    },
    {
      "epoch": 0.16970588235294118,
      "grad_norm": 0.062129609286785126,
      "learning_rate": 0.0001663033873343152,
      "loss": 0.4399,
      "step": 577
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.04644171521067619,
      "learning_rate": 0.00016624447717231223,
      "loss": 0.27,
      "step": 578
    },
    {
      "epoch": 0.17029411764705882,
      "grad_norm": 0.05524536594748497,
      "learning_rate": 0.0001661855670103093,
      "loss": 0.4055,
      "step": 579
    },
    {
      "epoch": 0.17058823529411765,
      "grad_norm": 0.05079932138323784,
      "learning_rate": 0.00016612665684830635,
      "loss": 0.3562,
      "step": 580
    },
    {
      "epoch": 0.17088235294117646,
      "grad_norm": 0.04984373226761818,
      "learning_rate": 0.0001660677466863034,
      "loss": 0.2906,
      "step": 581
    },
    {
      "epoch": 0.1711764705882353,
      "grad_norm": 0.04655125364661217,
      "learning_rate": 0.00016600883652430047,
      "loss": 0.3388,
      "step": 582
    },
    {
      "epoch": 0.17147058823529412,
      "grad_norm": 0.056734804064035416,
      "learning_rate": 0.0001659499263622975,
      "loss": 0.352,
      "step": 583
    },
    {
      "epoch": 0.17176470588235293,
      "grad_norm": 0.061462558805942535,
      "learning_rate": 0.00016589101620029457,
      "loss": 0.3824,
      "step": 584
    },
    {
      "epoch": 0.17205882352941176,
      "grad_norm": 0.05228470265865326,
      "learning_rate": 0.00016583210603829163,
      "loss": 0.3607,
      "step": 585
    },
    {
      "epoch": 0.1723529411764706,
      "grad_norm": 0.058265455067157745,
      "learning_rate": 0.00016577319587628869,
      "loss": 0.4351,
      "step": 586
    },
    {
      "epoch": 0.1726470588235294,
      "grad_norm": 0.06106986850500107,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.39,
      "step": 587
    },
    {
      "epoch": 0.17294117647058824,
      "grad_norm": 0.038869597017765045,
      "learning_rate": 0.00016565537555228278,
      "loss": 0.2904,
      "step": 588
    },
    {
      "epoch": 0.17323529411764707,
      "grad_norm": 0.05712001025676727,
      "learning_rate": 0.0001655964653902798,
      "loss": 0.4209,
      "step": 589
    },
    {
      "epoch": 0.17352941176470588,
      "grad_norm": 0.047472815960645676,
      "learning_rate": 0.00016553755522827687,
      "loss": 0.2935,
      "step": 590
    },
    {
      "epoch": 0.1738235294117647,
      "grad_norm": 0.04458276182413101,
      "learning_rate": 0.00016547864506627393,
      "loss": 0.2885,
      "step": 591
    },
    {
      "epoch": 0.17411764705882352,
      "grad_norm": 0.049312155693769455,
      "learning_rate": 0.000165419734904271,
      "loss": 0.3604,
      "step": 592
    },
    {
      "epoch": 0.17441176470588235,
      "grad_norm": 0.05668745934963226,
      "learning_rate": 0.00016536082474226803,
      "loss": 0.367,
      "step": 593
    },
    {
      "epoch": 0.17470588235294118,
      "grad_norm": 0.06275175511837006,
      "learning_rate": 0.00016530191458026509,
      "loss": 0.3404,
      "step": 594
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.050478558987379074,
      "learning_rate": 0.00016524300441826215,
      "loss": 0.4078,
      "step": 595
    },
    {
      "epoch": 0.17529411764705882,
      "grad_norm": 0.06471184641122818,
      "learning_rate": 0.0001651840942562592,
      "loss": 0.3823,
      "step": 596
    },
    {
      "epoch": 0.17558823529411766,
      "grad_norm": 0.058347951620817184,
      "learning_rate": 0.00016512518409425627,
      "loss": 0.3815,
      "step": 597
    },
    {
      "epoch": 0.17588235294117646,
      "grad_norm": 0.06391522288322449,
      "learning_rate": 0.0001650662739322533,
      "loss": 0.3636,
      "step": 598
    },
    {
      "epoch": 0.1761764705882353,
      "grad_norm": 0.06048767268657684,
      "learning_rate": 0.00016500736377025036,
      "loss": 0.3604,
      "step": 599
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.07474303245544434,
      "learning_rate": 0.00016494845360824742,
      "loss": 0.3756,
      "step": 600
    },
    {
      "epoch": 0.17676470588235293,
      "grad_norm": 0.06107667461037636,
      "learning_rate": 0.00016488954344624448,
      "loss": 0.4202,
      "step": 601
    },
    {
      "epoch": 0.17705882352941177,
      "grad_norm": 0.07629653811454773,
      "learning_rate": 0.00016483063328424154,
      "loss": 0.4249,
      "step": 602
    },
    {
      "epoch": 0.1773529411764706,
      "grad_norm": 0.05330305173993111,
      "learning_rate": 0.00016477172312223857,
      "loss": 0.3425,
      "step": 603
    },
    {
      "epoch": 0.1776470588235294,
      "grad_norm": 0.05535074695944786,
      "learning_rate": 0.00016471281296023563,
      "loss": 0.3534,
      "step": 604
    },
    {
      "epoch": 0.17794117647058824,
      "grad_norm": 0.061677485704422,
      "learning_rate": 0.0001646539027982327,
      "loss": 0.3499,
      "step": 605
    },
    {
      "epoch": 0.17823529411764705,
      "grad_norm": 0.06373181194067001,
      "learning_rate": 0.00016459499263622975,
      "loss": 0.4169,
      "step": 606
    },
    {
      "epoch": 0.17852941176470588,
      "grad_norm": 0.052286386489868164,
      "learning_rate": 0.00016453608247422681,
      "loss": 0.4158,
      "step": 607
    },
    {
      "epoch": 0.17882352941176471,
      "grad_norm": 0.06523198634386063,
      "learning_rate": 0.00016447717231222385,
      "loss": 0.3998,
      "step": 608
    },
    {
      "epoch": 0.17911764705882352,
      "grad_norm": 0.04262848198413849,
      "learning_rate": 0.0001644182621502209,
      "loss": 0.2542,
      "step": 609
    },
    {
      "epoch": 0.17941176470588235,
      "grad_norm": 0.0697278305888176,
      "learning_rate": 0.00016435935198821797,
      "loss": 0.3778,
      "step": 610
    },
    {
      "epoch": 0.1797058823529412,
      "grad_norm": 0.05354809761047363,
      "learning_rate": 0.00016430044182621503,
      "loss": 0.2937,
      "step": 611
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.05750763788819313,
      "learning_rate": 0.0001642415316642121,
      "loss": 0.3341,
      "step": 612
    },
    {
      "epoch": 0.18029411764705883,
      "grad_norm": 0.07634463906288147,
      "learning_rate": 0.00016418262150220912,
      "loss": 0.4056,
      "step": 613
    },
    {
      "epoch": 0.18058823529411766,
      "grad_norm": 0.05245326831936836,
      "learning_rate": 0.00016412371134020618,
      "loss": 0.3809,
      "step": 614
    },
    {
      "epoch": 0.18088235294117647,
      "grad_norm": 0.062018051743507385,
      "learning_rate": 0.00016406480117820324,
      "loss": 0.4161,
      "step": 615
    },
    {
      "epoch": 0.1811764705882353,
      "grad_norm": 0.071085624396801,
      "learning_rate": 0.0001640058910162003,
      "loss": 0.3459,
      "step": 616
    },
    {
      "epoch": 0.1814705882352941,
      "grad_norm": 0.06508353352546692,
      "learning_rate": 0.00016394698085419736,
      "loss": 0.3872,
      "step": 617
    },
    {
      "epoch": 0.18176470588235294,
      "grad_norm": 0.07106375694274902,
      "learning_rate": 0.0001638880706921944,
      "loss": 0.3966,
      "step": 618
    },
    {
      "epoch": 0.18205882352941177,
      "grad_norm": 0.048792023211717606,
      "learning_rate": 0.00016382916053019145,
      "loss": 0.3052,
      "step": 619
    },
    {
      "epoch": 0.18235294117647058,
      "grad_norm": 0.058426886796951294,
      "learning_rate": 0.00016377025036818851,
      "loss": 0.379,
      "step": 620
    },
    {
      "epoch": 0.1826470588235294,
      "grad_norm": 0.04340818524360657,
      "learning_rate": 0.00016371134020618558,
      "loss": 0.3102,
      "step": 621
    },
    {
      "epoch": 0.18294117647058825,
      "grad_norm": 0.05881083011627197,
      "learning_rate": 0.00016365243004418264,
      "loss": 0.3973,
      "step": 622
    },
    {
      "epoch": 0.18323529411764705,
      "grad_norm": 0.0563063845038414,
      "learning_rate": 0.00016359351988217967,
      "loss": 0.3502,
      "step": 623
    },
    {
      "epoch": 0.18352941176470589,
      "grad_norm": 0.06756271421909332,
      "learning_rate": 0.00016353460972017673,
      "loss": 0.372,
      "step": 624
    },
    {
      "epoch": 0.18382352941176472,
      "grad_norm": 0.04933758080005646,
      "learning_rate": 0.0001634756995581738,
      "loss": 0.3729,
      "step": 625
    },
    {
      "epoch": 0.18411764705882352,
      "grad_norm": 0.06471831351518631,
      "learning_rate": 0.00016341678939617085,
      "loss": 0.3797,
      "step": 626
    },
    {
      "epoch": 0.18441176470588236,
      "grad_norm": 0.07643914222717285,
      "learning_rate": 0.0001633578792341679,
      "loss": 0.3482,
      "step": 627
    },
    {
      "epoch": 0.18470588235294116,
      "grad_norm": 0.05957276001572609,
      "learning_rate": 0.00016329896907216494,
      "loss": 0.3729,
      "step": 628
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06735341250896454,
      "learning_rate": 0.000163240058910162,
      "loss": 0.3757,
      "step": 629
    },
    {
      "epoch": 0.18529411764705883,
      "grad_norm": 0.06439851969480515,
      "learning_rate": 0.00016318114874815906,
      "loss": 0.416,
      "step": 630
    },
    {
      "epoch": 0.18558823529411764,
      "grad_norm": 0.04577559977769852,
      "learning_rate": 0.00016312223858615612,
      "loss": 0.2814,
      "step": 631
    },
    {
      "epoch": 0.18588235294117647,
      "grad_norm": 0.04966415837407112,
      "learning_rate": 0.00016306332842415318,
      "loss": 0.3306,
      "step": 632
    },
    {
      "epoch": 0.1861764705882353,
      "grad_norm": 0.06001998111605644,
      "learning_rate": 0.00016300441826215022,
      "loss": 0.3347,
      "step": 633
    },
    {
      "epoch": 0.1864705882352941,
      "grad_norm": 0.04580844193696976,
      "learning_rate": 0.00016294550810014728,
      "loss": 0.3085,
      "step": 634
    },
    {
      "epoch": 0.18676470588235294,
      "grad_norm": 0.04229709878563881,
      "learning_rate": 0.00016288659793814434,
      "loss": 0.2584,
      "step": 635
    },
    {
      "epoch": 0.18705882352941178,
      "grad_norm": 0.06391051411628723,
      "learning_rate": 0.0001628276877761414,
      "loss": 0.3692,
      "step": 636
    },
    {
      "epoch": 0.18735294117647058,
      "grad_norm": 0.09518606960773468,
      "learning_rate": 0.00016276877761413846,
      "loss": 0.4263,
      "step": 637
    },
    {
      "epoch": 0.18764705882352942,
      "grad_norm": 0.05876845866441727,
      "learning_rate": 0.0001627098674521355,
      "loss": 0.3343,
      "step": 638
    },
    {
      "epoch": 0.18794117647058822,
      "grad_norm": 0.05391281843185425,
      "learning_rate": 0.00016265095729013255,
      "loss": 0.2743,
      "step": 639
    },
    {
      "epoch": 0.18823529411764706,
      "grad_norm": 0.07848921418190002,
      "learning_rate": 0.0001625920471281296,
      "loss": 0.3967,
      "step": 640
    },
    {
      "epoch": 0.1885294117647059,
      "grad_norm": 0.06307537108659744,
      "learning_rate": 0.00016253313696612667,
      "loss": 0.3708,
      "step": 641
    },
    {
      "epoch": 0.1888235294117647,
      "grad_norm": 0.06231262534856796,
      "learning_rate": 0.00016247422680412373,
      "loss": 0.4179,
      "step": 642
    },
    {
      "epoch": 0.18911764705882353,
      "grad_norm": 0.08004824817180634,
      "learning_rate": 0.00016241531664212076,
      "loss": 0.3985,
      "step": 643
    },
    {
      "epoch": 0.18941176470588236,
      "grad_norm": 0.05552724748849869,
      "learning_rate": 0.00016235640648011782,
      "loss": 0.3166,
      "step": 644
    },
    {
      "epoch": 0.18970588235294117,
      "grad_norm": 0.08370701223611832,
      "learning_rate": 0.00016229749631811488,
      "loss": 0.3741,
      "step": 645
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.060701947659254074,
      "learning_rate": 0.00016223858615611194,
      "loss": 0.361,
      "step": 646
    },
    {
      "epoch": 0.19029411764705884,
      "grad_norm": 0.07230162620544434,
      "learning_rate": 0.000162179675994109,
      "loss": 0.4247,
      "step": 647
    },
    {
      "epoch": 0.19058823529411764,
      "grad_norm": 0.07973267883062363,
      "learning_rate": 0.00016212076583210604,
      "loss": 0.3993,
      "step": 648
    },
    {
      "epoch": 0.19088235294117648,
      "grad_norm": 0.06663484871387482,
      "learning_rate": 0.0001620618556701031,
      "loss": 0.3584,
      "step": 649
    },
    {
      "epoch": 0.19117647058823528,
      "grad_norm": 0.05127593129873276,
      "learning_rate": 0.00016200294550810016,
      "loss": 0.3668,
      "step": 650
    },
    {
      "epoch": 0.19147058823529411,
      "grad_norm": 0.0807151049375534,
      "learning_rate": 0.00016194403534609722,
      "loss": 0.3889,
      "step": 651
    },
    {
      "epoch": 0.19176470588235295,
      "grad_norm": 0.07703541964292526,
      "learning_rate": 0.00016188512518409428,
      "loss": 0.3761,
      "step": 652
    },
    {
      "epoch": 0.19205882352941175,
      "grad_norm": 0.06200851500034332,
      "learning_rate": 0.0001618262150220913,
      "loss": 0.3669,
      "step": 653
    },
    {
      "epoch": 0.1923529411764706,
      "grad_norm": 0.06126273795962334,
      "learning_rate": 0.00016176730486008837,
      "loss": 0.3658,
      "step": 654
    },
    {
      "epoch": 0.19264705882352942,
      "grad_norm": 0.05351157486438751,
      "learning_rate": 0.00016170839469808543,
      "loss": 0.3293,
      "step": 655
    },
    {
      "epoch": 0.19294117647058823,
      "grad_norm": 0.03253978118300438,
      "learning_rate": 0.0001616494845360825,
      "loss": 0.1934,
      "step": 656
    },
    {
      "epoch": 0.19323529411764706,
      "grad_norm": 0.06235553324222565,
      "learning_rate": 0.00016159057437407955,
      "loss": 0.3295,
      "step": 657
    },
    {
      "epoch": 0.1935294117647059,
      "grad_norm": 0.0877184271812439,
      "learning_rate": 0.00016153166421207658,
      "loss": 0.3787,
      "step": 658
    },
    {
      "epoch": 0.1938235294117647,
      "grad_norm": 0.06164029613137245,
      "learning_rate": 0.00016147275405007364,
      "loss": 0.3811,
      "step": 659
    },
    {
      "epoch": 0.19411764705882353,
      "grad_norm": 0.055699754506349564,
      "learning_rate": 0.0001614138438880707,
      "loss": 0.3545,
      "step": 660
    },
    {
      "epoch": 0.19441176470588234,
      "grad_norm": 0.07298396527767181,
      "learning_rate": 0.00016135493372606776,
      "loss": 0.3992,
      "step": 661
    },
    {
      "epoch": 0.19470588235294117,
      "grad_norm": 0.06951436400413513,
      "learning_rate": 0.00016129602356406482,
      "loss": 0.2868,
      "step": 662
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.07633385807275772,
      "learning_rate": 0.00016123711340206186,
      "loss": 0.3997,
      "step": 663
    },
    {
      "epoch": 0.1952941176470588,
      "grad_norm": 0.0616174153983593,
      "learning_rate": 0.00016117820324005892,
      "loss": 0.3655,
      "step": 664
    },
    {
      "epoch": 0.19558823529411765,
      "grad_norm": 0.052812717854976654,
      "learning_rate": 0.00016111929307805598,
      "loss": 0.3561,
      "step": 665
    },
    {
      "epoch": 0.19588235294117648,
      "grad_norm": 0.06318505108356476,
      "learning_rate": 0.00016106038291605304,
      "loss": 0.3282,
      "step": 666
    },
    {
      "epoch": 0.19617647058823529,
      "grad_norm": 0.0728445053100586,
      "learning_rate": 0.0001610014727540501,
      "loss": 0.3631,
      "step": 667
    },
    {
      "epoch": 0.19647058823529412,
      "grad_norm": 0.07882007211446762,
      "learning_rate": 0.00016094256259204713,
      "loss": 0.3988,
      "step": 668
    },
    {
      "epoch": 0.19676470588235295,
      "grad_norm": 0.06059527024626732,
      "learning_rate": 0.0001608836524300442,
      "loss": 0.3657,
      "step": 669
    },
    {
      "epoch": 0.19705882352941176,
      "grad_norm": 0.042961303144693375,
      "learning_rate": 0.00016082474226804125,
      "loss": 0.3083,
      "step": 670
    },
    {
      "epoch": 0.1973529411764706,
      "grad_norm": 0.056660596281290054,
      "learning_rate": 0.0001607658321060383,
      "loss": 0.3595,
      "step": 671
    },
    {
      "epoch": 0.1976470588235294,
      "grad_norm": 0.07983566075563431,
      "learning_rate": 0.00016070692194403537,
      "loss": 0.3338,
      "step": 672
    },
    {
      "epoch": 0.19794117647058823,
      "grad_norm": 0.057897549122571945,
      "learning_rate": 0.0001606480117820324,
      "loss": 0.3452,
      "step": 673
    },
    {
      "epoch": 0.19823529411764707,
      "grad_norm": 0.04862534627318382,
      "learning_rate": 0.00016058910162002947,
      "loss": 0.3764,
      "step": 674
    },
    {
      "epoch": 0.19852941176470587,
      "grad_norm": 0.06794068217277527,
      "learning_rate": 0.00016053019145802653,
      "loss": 0.412,
      "step": 675
    },
    {
      "epoch": 0.1988235294117647,
      "grad_norm": 0.07110040634870529,
      "learning_rate": 0.00016047128129602359,
      "loss": 0.4105,
      "step": 676
    },
    {
      "epoch": 0.19911764705882354,
      "grad_norm": 0.04872092232108116,
      "learning_rate": 0.00016041237113402065,
      "loss": 0.3397,
      "step": 677
    },
    {
      "epoch": 0.19941176470588234,
      "grad_norm": 0.04470030963420868,
      "learning_rate": 0.00016035346097201768,
      "loss": 0.3192,
      "step": 678
    },
    {
      "epoch": 0.19970588235294118,
      "grad_norm": 0.05858094245195389,
      "learning_rate": 0.00016029455081001474,
      "loss": 0.4129,
      "step": 679
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.06117147207260132,
      "learning_rate": 0.0001602356406480118,
      "loss": 0.3452,
      "step": 680
    },
    {
      "epoch": 0.20029411764705882,
      "grad_norm": 0.07692009210586548,
      "learning_rate": 0.00016017673048600886,
      "loss": 0.4101,
      "step": 681
    },
    {
      "epoch": 0.20058823529411765,
      "grad_norm": 0.04911952465772629,
      "learning_rate": 0.00016011782032400592,
      "loss": 0.3679,
      "step": 682
    },
    {
      "epoch": 0.20088235294117648,
      "grad_norm": 0.04291140288114548,
      "learning_rate": 0.00016005891016200295,
      "loss": 0.3574,
      "step": 683
    },
    {
      "epoch": 0.2011764705882353,
      "grad_norm": 0.053893450647592545,
      "learning_rate": 0.00016,
      "loss": 0.2949,
      "step": 684
    },
    {
      "epoch": 0.20147058823529412,
      "grad_norm": 0.05680907145142555,
      "learning_rate": 0.00015994108983799707,
      "loss": 0.2984,
      "step": 685
    },
    {
      "epoch": 0.20176470588235293,
      "grad_norm": 0.060923878103494644,
      "learning_rate": 0.00015988217967599413,
      "loss": 0.3785,
      "step": 686
    },
    {
      "epoch": 0.20205882352941176,
      "grad_norm": 0.040785662829875946,
      "learning_rate": 0.0001598232695139912,
      "loss": 0.325,
      "step": 687
    },
    {
      "epoch": 0.2023529411764706,
      "grad_norm": 0.0461050346493721,
      "learning_rate": 0.00015976435935198823,
      "loss": 0.2981,
      "step": 688
    },
    {
      "epoch": 0.2026470588235294,
      "grad_norm": 0.04446212947368622,
      "learning_rate": 0.0001597054491899853,
      "loss": 0.3564,
      "step": 689
    },
    {
      "epoch": 0.20294117647058824,
      "grad_norm": 0.04444172978401184,
      "learning_rate": 0.00015964653902798235,
      "loss": 0.2802,
      "step": 690
    },
    {
      "epoch": 0.20323529411764707,
      "grad_norm": 0.048090510070323944,
      "learning_rate": 0.0001595876288659794,
      "loss": 0.2993,
      "step": 691
    },
    {
      "epoch": 0.20352941176470588,
      "grad_norm": 0.06313694268465042,
      "learning_rate": 0.00015952871870397647,
      "loss": 0.3867,
      "step": 692
    },
    {
      "epoch": 0.2038235294117647,
      "grad_norm": 0.059502724558115005,
      "learning_rate": 0.0001594698085419735,
      "loss": 0.4048,
      "step": 693
    },
    {
      "epoch": 0.20411764705882354,
      "grad_norm": 0.06442804634571075,
      "learning_rate": 0.00015941089837997056,
      "loss": 0.3585,
      "step": 694
    },
    {
      "epoch": 0.20441176470588235,
      "grad_norm": 0.050439152866601944,
      "learning_rate": 0.0001593519882179676,
      "loss": 0.3517,
      "step": 695
    },
    {
      "epoch": 0.20470588235294118,
      "grad_norm": 0.07076825946569443,
      "learning_rate": 0.00015929307805596465,
      "loss": 0.3776,
      "step": 696
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.06291080266237259,
      "learning_rate": 0.00015923416789396171,
      "loss": 0.4041,
      "step": 697
    },
    {
      "epoch": 0.20529411764705882,
      "grad_norm": 0.04894523322582245,
      "learning_rate": 0.00015917525773195875,
      "loss": 0.339,
      "step": 698
    },
    {
      "epoch": 0.20558823529411765,
      "grad_norm": 0.05032701790332794,
      "learning_rate": 0.0001591163475699558,
      "loss": 0.3302,
      "step": 699
    },
    {
      "epoch": 0.20588235294117646,
      "grad_norm": 0.056529004126787186,
      "learning_rate": 0.00015905743740795287,
      "loss": 0.3743,
      "step": 700
    },
    {
      "epoch": 0.2061764705882353,
      "grad_norm": 0.058404359966516495,
      "learning_rate": 0.00015899852724594993,
      "loss": 0.3456,
      "step": 701
    },
    {
      "epoch": 0.20647058823529413,
      "grad_norm": 0.05290975049138069,
      "learning_rate": 0.000158939617083947,
      "loss": 0.348,
      "step": 702
    },
    {
      "epoch": 0.20676470588235293,
      "grad_norm": 0.061785612255334854,
      "learning_rate": 0.00015888070692194402,
      "loss": 0.3425,
      "step": 703
    },
    {
      "epoch": 0.20705882352941177,
      "grad_norm": 0.053690794855356216,
      "learning_rate": 0.00015882179675994108,
      "loss": 0.3287,
      "step": 704
    },
    {
      "epoch": 0.2073529411764706,
      "grad_norm": 0.07636330276727676,
      "learning_rate": 0.00015876288659793814,
      "loss": 0.4783,
      "step": 705
    },
    {
      "epoch": 0.2076470588235294,
      "grad_norm": 0.06201598420739174,
      "learning_rate": 0.0001587039764359352,
      "loss": 0.3889,
      "step": 706
    },
    {
      "epoch": 0.20794117647058824,
      "grad_norm": 0.055612195283174515,
      "learning_rate": 0.00015864506627393226,
      "loss": 0.3632,
      "step": 707
    },
    {
      "epoch": 0.20823529411764705,
      "grad_norm": 0.04832616448402405,
      "learning_rate": 0.0001585861561119293,
      "loss": 0.2707,
      "step": 708
    },
    {
      "epoch": 0.20852941176470588,
      "grad_norm": 0.06464774906635284,
      "learning_rate": 0.00015852724594992635,
      "loss": 0.4143,
      "step": 709
    },
    {
      "epoch": 0.2088235294117647,
      "grad_norm": 0.04137435927987099,
      "learning_rate": 0.00015846833578792341,
      "loss": 0.2947,
      "step": 710
    },
    {
      "epoch": 0.20911764705882352,
      "grad_norm": 0.0571475476026535,
      "learning_rate": 0.00015840942562592047,
      "loss": 0.3184,
      "step": 711
    },
    {
      "epoch": 0.20941176470588235,
      "grad_norm": 0.06978932023048401,
      "learning_rate": 0.00015835051546391754,
      "loss": 0.3849,
      "step": 712
    },
    {
      "epoch": 0.2097058823529412,
      "grad_norm": 0.04857069253921509,
      "learning_rate": 0.00015829160530191457,
      "loss": 0.3517,
      "step": 713
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.04477962478995323,
      "learning_rate": 0.00015823269513991163,
      "loss": 0.331,
      "step": 714
    },
    {
      "epoch": 0.21029411764705883,
      "grad_norm": 0.05800754204392433,
      "learning_rate": 0.0001581737849779087,
      "loss": 0.307,
      "step": 715
    },
    {
      "epoch": 0.21058823529411766,
      "grad_norm": 0.058337271213531494,
      "learning_rate": 0.00015811487481590575,
      "loss": 0.3521,
      "step": 716
    },
    {
      "epoch": 0.21088235294117647,
      "grad_norm": 0.0661049485206604,
      "learning_rate": 0.0001580559646539028,
      "loss": 0.3996,
      "step": 717
    },
    {
      "epoch": 0.2111764705882353,
      "grad_norm": 0.05764153599739075,
      "learning_rate": 0.00015799705449189984,
      "loss": 0.4067,
      "step": 718
    },
    {
      "epoch": 0.2114705882352941,
      "grad_norm": 0.04720641300082207,
      "learning_rate": 0.0001579381443298969,
      "loss": 0.3394,
      "step": 719
    },
    {
      "epoch": 0.21176470588235294,
      "grad_norm": 0.060476917773485184,
      "learning_rate": 0.00015787923416789396,
      "loss": 0.3574,
      "step": 720
    },
    {
      "epoch": 0.21205882352941177,
      "grad_norm": 0.04659063741564751,
      "learning_rate": 0.00015782032400589102,
      "loss": 0.2939,
      "step": 721
    },
    {
      "epoch": 0.21235294117647058,
      "grad_norm": 0.0629427507519722,
      "learning_rate": 0.00015776141384388808,
      "loss": 0.35,
      "step": 722
    },
    {
      "epoch": 0.2126470588235294,
      "grad_norm": 0.06532291322946548,
      "learning_rate": 0.00015770250368188512,
      "loss": 0.3916,
      "step": 723
    },
    {
      "epoch": 0.21294117647058824,
      "grad_norm": 0.05802358686923981,
      "learning_rate": 0.00015764359351988218,
      "loss": 0.3201,
      "step": 724
    },
    {
      "epoch": 0.21323529411764705,
      "grad_norm": 0.047698717564344406,
      "learning_rate": 0.00015758468335787924,
      "loss": 0.3007,
      "step": 725
    },
    {
      "epoch": 0.21352941176470588,
      "grad_norm": 0.06573429703712463,
      "learning_rate": 0.0001575257731958763,
      "loss": 0.2949,
      "step": 726
    },
    {
      "epoch": 0.21382352941176472,
      "grad_norm": 0.06530513614416122,
      "learning_rate": 0.00015746686303387336,
      "loss": 0.3977,
      "step": 727
    },
    {
      "epoch": 0.21411764705882352,
      "grad_norm": 0.05426618084311485,
      "learning_rate": 0.0001574079528718704,
      "loss": 0.3868,
      "step": 728
    },
    {
      "epoch": 0.21441176470588236,
      "grad_norm": 0.06537091732025146,
      "learning_rate": 0.00015734904270986745,
      "loss": 0.4129,
      "step": 729
    },
    {
      "epoch": 0.21470588235294116,
      "grad_norm": 0.06139171123504639,
      "learning_rate": 0.0001572901325478645,
      "loss": 0.4059,
      "step": 730
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.0537787601351738,
      "learning_rate": 0.00015723122238586157,
      "loss": 0.3616,
      "step": 731
    },
    {
      "epoch": 0.21529411764705883,
      "grad_norm": 0.0435417965054512,
      "learning_rate": 0.00015717231222385863,
      "loss": 0.2964,
      "step": 732
    },
    {
      "epoch": 0.21558823529411764,
      "grad_norm": 0.07378406077623367,
      "learning_rate": 0.00015711340206185566,
      "loss": 0.4187,
      "step": 733
    },
    {
      "epoch": 0.21588235294117647,
      "grad_norm": 0.0531664676964283,
      "learning_rate": 0.00015705449189985272,
      "loss": 0.3912,
      "step": 734
    },
    {
      "epoch": 0.2161764705882353,
      "grad_norm": 0.047783706337213516,
      "learning_rate": 0.00015699558173784978,
      "loss": 0.3298,
      "step": 735
    },
    {
      "epoch": 0.2164705882352941,
      "grad_norm": 0.05372120440006256,
      "learning_rate": 0.00015693667157584684,
      "loss": 0.354,
      "step": 736
    },
    {
      "epoch": 0.21676470588235294,
      "grad_norm": 0.05178256705403328,
      "learning_rate": 0.0001568777614138439,
      "loss": 0.3821,
      "step": 737
    },
    {
      "epoch": 0.21705882352941178,
      "grad_norm": 0.05051824077963829,
      "learning_rate": 0.00015681885125184094,
      "loss": 0.3936,
      "step": 738
    },
    {
      "epoch": 0.21735294117647058,
      "grad_norm": 0.04216437041759491,
      "learning_rate": 0.000156759941089838,
      "loss": 0.3298,
      "step": 739
    },
    {
      "epoch": 0.21764705882352942,
      "grad_norm": 0.038164228200912476,
      "learning_rate": 0.00015670103092783506,
      "loss": 0.2802,
      "step": 740
    },
    {
      "epoch": 0.21794117647058822,
      "grad_norm": 0.031239116564393044,
      "learning_rate": 0.00015664212076583212,
      "loss": 0.2477,
      "step": 741
    },
    {
      "epoch": 0.21823529411764706,
      "grad_norm": 0.06569403409957886,
      "learning_rate": 0.00015658321060382918,
      "loss": 0.4356,
      "step": 742
    },
    {
      "epoch": 0.2185294117647059,
      "grad_norm": 0.03764916956424713,
      "learning_rate": 0.0001565243004418262,
      "loss": 0.2876,
      "step": 743
    },
    {
      "epoch": 0.2188235294117647,
      "grad_norm": 0.05332646891474724,
      "learning_rate": 0.00015646539027982327,
      "loss": 0.3227,
      "step": 744
    },
    {
      "epoch": 0.21911764705882353,
      "grad_norm": 0.0500241294503212,
      "learning_rate": 0.00015640648011782033,
      "loss": 0.3591,
      "step": 745
    },
    {
      "epoch": 0.21941176470588236,
      "grad_norm": 0.059934645891189575,
      "learning_rate": 0.0001563475699558174,
      "loss": 0.4106,
      "step": 746
    },
    {
      "epoch": 0.21970588235294117,
      "grad_norm": 0.04889747127890587,
      "learning_rate": 0.00015628865979381445,
      "loss": 0.3166,
      "step": 747
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.048640813678503036,
      "learning_rate": 0.00015622974963181148,
      "loss": 0.3461,
      "step": 748
    },
    {
      "epoch": 0.22029411764705883,
      "grad_norm": 0.039814915508031845,
      "learning_rate": 0.00015617083946980854,
      "loss": 0.2832,
      "step": 749
    },
    {
      "epoch": 0.22058823529411764,
      "grad_norm": 0.059623461216688156,
      "learning_rate": 0.0001561119293078056,
      "loss": 0.3801,
      "step": 750
    },
    {
      "epoch": 0.22088235294117647,
      "grad_norm": 0.044037219136953354,
      "learning_rate": 0.00015605301914580266,
      "loss": 0.3507,
      "step": 751
    },
    {
      "epoch": 0.2211764705882353,
      "grad_norm": 0.078094482421875,
      "learning_rate": 0.00015599410898379972,
      "loss": 0.4416,
      "step": 752
    },
    {
      "epoch": 0.2214705882352941,
      "grad_norm": 0.05183517932891846,
      "learning_rate": 0.00015593519882179676,
      "loss": 0.3962,
      "step": 753
    },
    {
      "epoch": 0.22176470588235295,
      "grad_norm": 0.049515582621097565,
      "learning_rate": 0.00015587628865979382,
      "loss": 0.3595,
      "step": 754
    },
    {
      "epoch": 0.22205882352941175,
      "grad_norm": 0.05179493501782417,
      "learning_rate": 0.00015581737849779088,
      "loss": 0.3304,
      "step": 755
    },
    {
      "epoch": 0.2223529411764706,
      "grad_norm": 0.04011007770895958,
      "learning_rate": 0.00015575846833578794,
      "loss": 0.3337,
      "step": 756
    },
    {
      "epoch": 0.22264705882352942,
      "grad_norm": 0.07643067836761475,
      "learning_rate": 0.000155699558173785,
      "loss": 0.3598,
      "step": 757
    },
    {
      "epoch": 0.22294117647058823,
      "grad_norm": 0.05183534696698189,
      "learning_rate": 0.00015564064801178203,
      "loss": 0.3348,
      "step": 758
    },
    {
      "epoch": 0.22323529411764706,
      "grad_norm": 0.05867390334606171,
      "learning_rate": 0.0001555817378497791,
      "loss": 0.3426,
      "step": 759
    },
    {
      "epoch": 0.2235294117647059,
      "grad_norm": 0.045140914618968964,
      "learning_rate": 0.00015552282768777615,
      "loss": 0.3419,
      "step": 760
    },
    {
      "epoch": 0.2238235294117647,
      "grad_norm": 0.06231553107500076,
      "learning_rate": 0.0001554639175257732,
      "loss": 0.3605,
      "step": 761
    },
    {
      "epoch": 0.22411764705882353,
      "grad_norm": 0.041879162192344666,
      "learning_rate": 0.00015540500736377027,
      "loss": 0.3444,
      "step": 762
    },
    {
      "epoch": 0.22441176470588237,
      "grad_norm": 0.0464022234082222,
      "learning_rate": 0.0001553460972017673,
      "loss": 0.3409,
      "step": 763
    },
    {
      "epoch": 0.22470588235294117,
      "grad_norm": 0.05099009349942207,
      "learning_rate": 0.00015528718703976437,
      "loss": 0.3948,
      "step": 764
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.04478596895933151,
      "learning_rate": 0.00015522827687776143,
      "loss": 0.3557,
      "step": 765
    },
    {
      "epoch": 0.2252941176470588,
      "grad_norm": 0.048449981957674026,
      "learning_rate": 0.00015516936671575849,
      "loss": 0.3608,
      "step": 766
    },
    {
      "epoch": 0.22558823529411764,
      "grad_norm": 0.08497659862041473,
      "learning_rate": 0.00015511045655375555,
      "loss": 0.4206,
      "step": 767
    },
    {
      "epoch": 0.22588235294117648,
      "grad_norm": 0.061473604291677475,
      "learning_rate": 0.00015505154639175258,
      "loss": 0.3059,
      "step": 768
    },
    {
      "epoch": 0.22617647058823528,
      "grad_norm": 0.05752651020884514,
      "learning_rate": 0.00015499263622974964,
      "loss": 0.3629,
      "step": 769
    },
    {
      "epoch": 0.22647058823529412,
      "grad_norm": 0.06633611023426056,
      "learning_rate": 0.0001549337260677467,
      "loss": 0.3843,
      "step": 770
    },
    {
      "epoch": 0.22676470588235295,
      "grad_norm": 0.05157380923628807,
      "learning_rate": 0.00015487481590574376,
      "loss": 0.3584,
      "step": 771
    },
    {
      "epoch": 0.22705882352941176,
      "grad_norm": 0.051166702061891556,
      "learning_rate": 0.00015481590574374082,
      "loss": 0.301,
      "step": 772
    },
    {
      "epoch": 0.2273529411764706,
      "grad_norm": 0.042449697852134705,
      "learning_rate": 0.00015475699558173785,
      "loss": 0.2995,
      "step": 773
    },
    {
      "epoch": 0.22764705882352942,
      "grad_norm": 0.04250197485089302,
      "learning_rate": 0.0001546980854197349,
      "loss": 0.3692,
      "step": 774
    },
    {
      "epoch": 0.22794117647058823,
      "grad_norm": 0.053548119962215424,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.4043,
      "step": 775
    },
    {
      "epoch": 0.22823529411764706,
      "grad_norm": 0.04838399589061737,
      "learning_rate": 0.00015458026509572903,
      "loss": 0.3087,
      "step": 776
    },
    {
      "epoch": 0.22852941176470587,
      "grad_norm": 0.0504065677523613,
      "learning_rate": 0.0001545213549337261,
      "loss": 0.3544,
      "step": 777
    },
    {
      "epoch": 0.2288235294117647,
      "grad_norm": 0.04909535124897957,
      "learning_rate": 0.00015446244477172313,
      "loss": 0.2677,
      "step": 778
    },
    {
      "epoch": 0.22911764705882354,
      "grad_norm": 0.0523778572678566,
      "learning_rate": 0.0001544035346097202,
      "loss": 0.3244,
      "step": 779
    },
    {
      "epoch": 0.22941176470588234,
      "grad_norm": 0.04403742030262947,
      "learning_rate": 0.00015434462444771725,
      "loss": 0.265,
      "step": 780
    },
    {
      "epoch": 0.22970588235294118,
      "grad_norm": 0.04736926779150963,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3697,
      "step": 781
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.05434809625148773,
      "learning_rate": 0.00015422680412371137,
      "loss": 0.3761,
      "step": 782
    },
    {
      "epoch": 0.23029411764705882,
      "grad_norm": 0.05977502837777138,
      "learning_rate": 0.0001541678939617084,
      "loss": 0.4036,
      "step": 783
    },
    {
      "epoch": 0.23058823529411765,
      "grad_norm": 0.051600657403469086,
      "learning_rate": 0.00015410898379970546,
      "loss": 0.3338,
      "step": 784
    },
    {
      "epoch": 0.23088235294117648,
      "grad_norm": 0.05443185567855835,
      "learning_rate": 0.00015405007363770252,
      "loss": 0.367,
      "step": 785
    },
    {
      "epoch": 0.2311764705882353,
      "grad_norm": 0.04886242002248764,
      "learning_rate": 0.00015399116347569958,
      "loss": 0.3254,
      "step": 786
    },
    {
      "epoch": 0.23147058823529412,
      "grad_norm": 0.0525943823158741,
      "learning_rate": 0.00015393225331369664,
      "loss": 0.3555,
      "step": 787
    },
    {
      "epoch": 0.23176470588235293,
      "grad_norm": 0.06001102179288864,
      "learning_rate": 0.00015387334315169367,
      "loss": 0.3846,
      "step": 788
    },
    {
      "epoch": 0.23205882352941176,
      "grad_norm": 0.05627163127064705,
      "learning_rate": 0.00015381443298969073,
      "loss": 0.4,
      "step": 789
    },
    {
      "epoch": 0.2323529411764706,
      "grad_norm": 0.05016671121120453,
      "learning_rate": 0.0001537555228276878,
      "loss": 0.4136,
      "step": 790
    },
    {
      "epoch": 0.2326470588235294,
      "grad_norm": 0.0548783503472805,
      "learning_rate": 0.00015369661266568485,
      "loss": 0.3869,
      "step": 791
    },
    {
      "epoch": 0.23294117647058823,
      "grad_norm": 0.05256401002407074,
      "learning_rate": 0.00015363770250368191,
      "loss": 0.3168,
      "step": 792
    },
    {
      "epoch": 0.23323529411764707,
      "grad_norm": 0.0504455491900444,
      "learning_rate": 0.00015357879234167895,
      "loss": 0.387,
      "step": 793
    },
    {
      "epoch": 0.23352941176470587,
      "grad_norm": 0.056336719542741776,
      "learning_rate": 0.000153519882179676,
      "loss": 0.3525,
      "step": 794
    },
    {
      "epoch": 0.2338235294117647,
      "grad_norm": 0.052811261266469955,
      "learning_rate": 0.00015346097201767307,
      "loss": 0.349,
      "step": 795
    },
    {
      "epoch": 0.23411764705882354,
      "grad_norm": 0.06716068089008331,
      "learning_rate": 0.00015340206185567013,
      "loss": 0.3651,
      "step": 796
    },
    {
      "epoch": 0.23441176470588235,
      "grad_norm": 0.05453069880604744,
      "learning_rate": 0.0001533431516936672,
      "loss": 0.3116,
      "step": 797
    },
    {
      "epoch": 0.23470588235294118,
      "grad_norm": 0.03846844285726547,
      "learning_rate": 0.00015328424153166422,
      "loss": 0.2707,
      "step": 798
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.05943136662244797,
      "learning_rate": 0.00015322533136966128,
      "loss": 0.3408,
      "step": 799
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.04608723521232605,
      "learning_rate": 0.00015316642120765834,
      "loss": 0.3243,
      "step": 800
    },
    {
      "epoch": 0.23558823529411765,
      "grad_norm": 0.07181533426046371,
      "learning_rate": 0.00015310751104565537,
      "loss": 0.3715,
      "step": 801
    },
    {
      "epoch": 0.23588235294117646,
      "grad_norm": 0.04271401837468147,
      "learning_rate": 0.00015304860088365243,
      "loss": 0.3159,
      "step": 802
    },
    {
      "epoch": 0.2361764705882353,
      "grad_norm": 0.04295480623841286,
      "learning_rate": 0.00015298969072164947,
      "loss": 0.3343,
      "step": 803
    },
    {
      "epoch": 0.23647058823529413,
      "grad_norm": 0.08767161518335342,
      "learning_rate": 0.00015293078055964653,
      "loss": 0.4023,
      "step": 804
    },
    {
      "epoch": 0.23676470588235293,
      "grad_norm": 0.05766723304986954,
      "learning_rate": 0.0001528718703976436,
      "loss": 0.38,
      "step": 805
    },
    {
      "epoch": 0.23705882352941177,
      "grad_norm": 0.06087141111493111,
      "learning_rate": 0.00015281296023564065,
      "loss": 0.3855,
      "step": 806
    },
    {
      "epoch": 0.2373529411764706,
      "grad_norm": 0.07504145801067352,
      "learning_rate": 0.0001527540500736377,
      "loss": 0.4296,
      "step": 807
    },
    {
      "epoch": 0.2376470588235294,
      "grad_norm": 0.059098973870277405,
      "learning_rate": 0.00015269513991163474,
      "loss": 0.3334,
      "step": 808
    },
    {
      "epoch": 0.23794117647058824,
      "grad_norm": 0.054836634546518326,
      "learning_rate": 0.0001526362297496318,
      "loss": 0.3732,
      "step": 809
    },
    {
      "epoch": 0.23823529411764705,
      "grad_norm": 0.054932720959186554,
      "learning_rate": 0.00015257731958762886,
      "loss": 0.3643,
      "step": 810
    },
    {
      "epoch": 0.23852941176470588,
      "grad_norm": 0.056128401309251785,
      "learning_rate": 0.00015251840942562592,
      "loss": 0.4055,
      "step": 811
    },
    {
      "epoch": 0.2388235294117647,
      "grad_norm": 0.06307770311832428,
      "learning_rate": 0.00015245949926362298,
      "loss": 0.3228,
      "step": 812
    },
    {
      "epoch": 0.23911764705882352,
      "grad_norm": 0.054991524666547775,
      "learning_rate": 0.00015240058910162002,
      "loss": 0.3676,
      "step": 813
    },
    {
      "epoch": 0.23941176470588235,
      "grad_norm": 0.04872496798634529,
      "learning_rate": 0.00015234167893961708,
      "loss": 0.3724,
      "step": 814
    },
    {
      "epoch": 0.23970588235294119,
      "grad_norm": 0.04811226204037666,
      "learning_rate": 0.00015228276877761414,
      "loss": 0.3281,
      "step": 815
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.05121970549225807,
      "learning_rate": 0.0001522238586156112,
      "loss": 0.3566,
      "step": 816
    },
    {
      "epoch": 0.24029411764705882,
      "grad_norm": 0.057808730751276016,
      "learning_rate": 0.00015216494845360826,
      "loss": 0.3152,
      "step": 817
    },
    {
      "epoch": 0.24058823529411766,
      "grad_norm": 0.06310531497001648,
      "learning_rate": 0.0001521060382916053,
      "loss": 0.3562,
      "step": 818
    },
    {
      "epoch": 0.24088235294117646,
      "grad_norm": 0.04868234694004059,
      "learning_rate": 0.00015204712812960235,
      "loss": 0.3329,
      "step": 819
    },
    {
      "epoch": 0.2411764705882353,
      "grad_norm": 0.06056853011250496,
      "learning_rate": 0.0001519882179675994,
      "loss": 0.3859,
      "step": 820
    },
    {
      "epoch": 0.24147058823529413,
      "grad_norm": 0.06528452038764954,
      "learning_rate": 0.00015192930780559647,
      "loss": 0.3617,
      "step": 821
    },
    {
      "epoch": 0.24176470588235294,
      "grad_norm": 0.05230215564370155,
      "learning_rate": 0.00015187039764359353,
      "loss": 0.2949,
      "step": 822
    },
    {
      "epoch": 0.24205882352941177,
      "grad_norm": 0.06731203198432922,
      "learning_rate": 0.00015181148748159056,
      "loss": 0.3657,
      "step": 823
    },
    {
      "epoch": 0.24235294117647058,
      "grad_norm": 0.056145813316106796,
      "learning_rate": 0.00015175257731958762,
      "loss": 0.3566,
      "step": 824
    },
    {
      "epoch": 0.2426470588235294,
      "grad_norm": 0.06409990042448044,
      "learning_rate": 0.00015169366715758468,
      "loss": 0.3839,
      "step": 825
    },
    {
      "epoch": 0.24294117647058824,
      "grad_norm": 0.06945058703422546,
      "learning_rate": 0.00015163475699558174,
      "loss": 0.4816,
      "step": 826
    },
    {
      "epoch": 0.24323529411764705,
      "grad_norm": 0.06122128292918205,
      "learning_rate": 0.0001515758468335788,
      "loss": 0.3622,
      "step": 827
    },
    {
      "epoch": 0.24352941176470588,
      "grad_norm": 0.05165794491767883,
      "learning_rate": 0.00015151693667157584,
      "loss": 0.3853,
      "step": 828
    },
    {
      "epoch": 0.24382352941176472,
      "grad_norm": 0.04984120652079582,
      "learning_rate": 0.0001514580265095729,
      "loss": 0.3838,
      "step": 829
    },
    {
      "epoch": 0.24411764705882352,
      "grad_norm": 0.04035786911845207,
      "learning_rate": 0.00015139911634756996,
      "loss": 0.2741,
      "step": 830
    },
    {
      "epoch": 0.24441176470588236,
      "grad_norm": 0.0698532834649086,
      "learning_rate": 0.00015134020618556702,
      "loss": 0.4192,
      "step": 831
    },
    {
      "epoch": 0.2447058823529412,
      "grad_norm": 0.05118882283568382,
      "learning_rate": 0.00015128129602356408,
      "loss": 0.3658,
      "step": 832
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.07093042880296707,
      "learning_rate": 0.0001512223858615611,
      "loss": 0.3019,
      "step": 833
    },
    {
      "epoch": 0.24529411764705883,
      "grad_norm": 0.05597876384854317,
      "learning_rate": 0.00015116347569955817,
      "loss": 0.3496,
      "step": 834
    },
    {
      "epoch": 0.24558823529411763,
      "grad_norm": 0.06814400851726532,
      "learning_rate": 0.00015110456553755523,
      "loss": 0.3652,
      "step": 835
    },
    {
      "epoch": 0.24588235294117647,
      "grad_norm": 0.06781720370054245,
      "learning_rate": 0.0001510456553755523,
      "loss": 0.3442,
      "step": 836
    },
    {
      "epoch": 0.2461764705882353,
      "grad_norm": 0.042514536529779434,
      "learning_rate": 0.00015098674521354935,
      "loss": 0.3874,
      "step": 837
    },
    {
      "epoch": 0.2464705882352941,
      "grad_norm": 0.057626936584711075,
      "learning_rate": 0.00015092783505154638,
      "loss": 0.3738,
      "step": 838
    },
    {
      "epoch": 0.24676470588235294,
      "grad_norm": 0.054062873125076294,
      "learning_rate": 0.00015086892488954344,
      "loss": 0.3188,
      "step": 839
    },
    {
      "epoch": 0.24705882352941178,
      "grad_norm": 0.059099771082401276,
      "learning_rate": 0.0001508100147275405,
      "loss": 0.3226,
      "step": 840
    },
    {
      "epoch": 0.24735294117647058,
      "grad_norm": 0.063271664083004,
      "learning_rate": 0.00015075110456553756,
      "loss": 0.4139,
      "step": 841
    },
    {
      "epoch": 0.24764705882352941,
      "grad_norm": 0.06892070919275284,
      "learning_rate": 0.00015069219440353462,
      "loss": 0.3332,
      "step": 842
    },
    {
      "epoch": 0.24794117647058825,
      "grad_norm": 0.05576075240969658,
      "learning_rate": 0.00015063328424153166,
      "loss": 0.3454,
      "step": 843
    },
    {
      "epoch": 0.24823529411764705,
      "grad_norm": 0.05657346546649933,
      "learning_rate": 0.00015057437407952872,
      "loss": 0.2995,
      "step": 844
    },
    {
      "epoch": 0.2485294117647059,
      "grad_norm": 0.06410253047943115,
      "learning_rate": 0.00015051546391752578,
      "loss": 0.3424,
      "step": 845
    },
    {
      "epoch": 0.2488235294117647,
      "grad_norm": 0.06051955372095108,
      "learning_rate": 0.00015045655375552284,
      "loss": 0.335,
      "step": 846
    },
    {
      "epoch": 0.24911764705882353,
      "grad_norm": 0.05353469029068947,
      "learning_rate": 0.0001503976435935199,
      "loss": 0.3578,
      "step": 847
    },
    {
      "epoch": 0.24941176470588236,
      "grad_norm": 0.06305938959121704,
      "learning_rate": 0.00015033873343151693,
      "loss": 0.402,
      "step": 848
    },
    {
      "epoch": 0.24970588235294117,
      "grad_norm": 0.053310081362724304,
      "learning_rate": 0.000150279823269514,
      "loss": 0.3336,
      "step": 849
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.0641208216547966,
      "learning_rate": 0.00015022091310751105,
      "loss": 0.4387,
      "step": 850
    },
    {
      "epoch": 0.25029411764705883,
      "grad_norm": 0.05950336158275604,
      "learning_rate": 0.0001501620029455081,
      "loss": 0.3822,
      "step": 851
    },
    {
      "epoch": 0.25058823529411767,
      "grad_norm": 0.060848936438560486,
      "learning_rate": 0.00015010309278350517,
      "loss": 0.389,
      "step": 852
    },
    {
      "epoch": 0.25088235294117645,
      "grad_norm": 0.04849682003259659,
      "learning_rate": 0.0001500441826215022,
      "loss": 0.3936,
      "step": 853
    },
    {
      "epoch": 0.2511764705882353,
      "grad_norm": 0.05510273575782776,
      "learning_rate": 0.00014998527245949927,
      "loss": 0.3608,
      "step": 854
    },
    {
      "epoch": 0.2514705882352941,
      "grad_norm": 0.06138743832707405,
      "learning_rate": 0.00014992636229749633,
      "loss": 0.3604,
      "step": 855
    },
    {
      "epoch": 0.25176470588235295,
      "grad_norm": 0.05221571773290634,
      "learning_rate": 0.00014986745213549339,
      "loss": 0.3548,
      "step": 856
    },
    {
      "epoch": 0.2520588235294118,
      "grad_norm": 0.06734069436788559,
      "learning_rate": 0.00014980854197349045,
      "loss": 0.4444,
      "step": 857
    },
    {
      "epoch": 0.2523529411764706,
      "grad_norm": 0.06240047886967659,
      "learning_rate": 0.00014974963181148748,
      "loss": 0.3968,
      "step": 858
    },
    {
      "epoch": 0.2526470588235294,
      "grad_norm": 0.05789473280310631,
      "learning_rate": 0.00014969072164948454,
      "loss": 0.383,
      "step": 859
    },
    {
      "epoch": 0.2529411764705882,
      "grad_norm": 0.05539305880665779,
      "learning_rate": 0.0001496318114874816,
      "loss": 0.3355,
      "step": 860
    },
    {
      "epoch": 0.25323529411764706,
      "grad_norm": 0.04203849658370018,
      "learning_rate": 0.00014957290132547866,
      "loss": 0.2664,
      "step": 861
    },
    {
      "epoch": 0.2535294117647059,
      "grad_norm": 0.054852575063705444,
      "learning_rate": 0.00014951399116347572,
      "loss": 0.3417,
      "step": 862
    },
    {
      "epoch": 0.2538235294117647,
      "grad_norm": 0.05410647764801979,
      "learning_rate": 0.00014945508100147275,
      "loss": 0.3605,
      "step": 863
    },
    {
      "epoch": 0.2541176470588235,
      "grad_norm": 0.043885912746191025,
      "learning_rate": 0.0001493961708394698,
      "loss": 0.3569,
      "step": 864
    },
    {
      "epoch": 0.25441176470588234,
      "grad_norm": 0.05074778571724892,
      "learning_rate": 0.00014933726067746687,
      "loss": 0.4279,
      "step": 865
    },
    {
      "epoch": 0.25470588235294117,
      "grad_norm": 0.03978065773844719,
      "learning_rate": 0.00014927835051546393,
      "loss": 0.3081,
      "step": 866
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.05362002179026604,
      "learning_rate": 0.000149219440353461,
      "loss": 0.3496,
      "step": 867
    },
    {
      "epoch": 0.25529411764705884,
      "grad_norm": 0.058729711920022964,
      "learning_rate": 0.00014916053019145803,
      "loss": 0.4101,
      "step": 868
    },
    {
      "epoch": 0.25558823529411767,
      "grad_norm": 0.052416399121284485,
      "learning_rate": 0.0001491016200294551,
      "loss": 0.3301,
      "step": 869
    },
    {
      "epoch": 0.25588235294117645,
      "grad_norm": 0.05703870579600334,
      "learning_rate": 0.00014904270986745215,
      "loss": 0.3518,
      "step": 870
    },
    {
      "epoch": 0.2561764705882353,
      "grad_norm": 0.055231329053640366,
      "learning_rate": 0.0001489837997054492,
      "loss": 0.3545,
      "step": 871
    },
    {
      "epoch": 0.2564705882352941,
      "grad_norm": 0.06382600218057632,
      "learning_rate": 0.00014892488954344627,
      "loss": 0.3608,
      "step": 872
    },
    {
      "epoch": 0.25676470588235295,
      "grad_norm": 0.06119081750512123,
      "learning_rate": 0.0001488659793814433,
      "loss": 0.36,
      "step": 873
    },
    {
      "epoch": 0.2570588235294118,
      "grad_norm": 0.08137867599725723,
      "learning_rate": 0.00014880706921944036,
      "loss": 0.4034,
      "step": 874
    },
    {
      "epoch": 0.25735294117647056,
      "grad_norm": 0.07120561599731445,
      "learning_rate": 0.00014874815905743742,
      "loss": 0.4204,
      "step": 875
    },
    {
      "epoch": 0.2576470588235294,
      "grad_norm": 0.055597659200429916,
      "learning_rate": 0.00014868924889543448,
      "loss": 0.3608,
      "step": 876
    },
    {
      "epoch": 0.25794117647058823,
      "grad_norm": 0.046641796827316284,
      "learning_rate": 0.00014863033873343154,
      "loss": 0.3481,
      "step": 877
    },
    {
      "epoch": 0.25823529411764706,
      "grad_norm": 0.041125234216451645,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3112,
      "step": 878
    },
    {
      "epoch": 0.2585294117647059,
      "grad_norm": 0.036888379603624344,
      "learning_rate": 0.00014851251840942563,
      "loss": 0.2504,
      "step": 879
    },
    {
      "epoch": 0.25882352941176473,
      "grad_norm": 0.05571471527218819,
      "learning_rate": 0.0001484536082474227,
      "loss": 0.3481,
      "step": 880
    },
    {
      "epoch": 0.2591176470588235,
      "grad_norm": 0.04781299829483032,
      "learning_rate": 0.00014839469808541975,
      "loss": 0.3333,
      "step": 881
    },
    {
      "epoch": 0.25941176470588234,
      "grad_norm": 0.059225380420684814,
      "learning_rate": 0.00014833578792341681,
      "loss": 0.3813,
      "step": 882
    },
    {
      "epoch": 0.2597058823529412,
      "grad_norm": 0.06043194234371185,
      "learning_rate": 0.00014827687776141385,
      "loss": 0.4207,
      "step": 883
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.05455294996500015,
      "learning_rate": 0.0001482179675994109,
      "loss": 0.3446,
      "step": 884
    },
    {
      "epoch": 0.26029411764705884,
      "grad_norm": 0.053884029388427734,
      "learning_rate": 0.00014815905743740797,
      "loss": 0.3898,
      "step": 885
    },
    {
      "epoch": 0.2605882352941176,
      "grad_norm": 0.0551350973546505,
      "learning_rate": 0.00014810014727540503,
      "loss": 0.376,
      "step": 886
    },
    {
      "epoch": 0.26088235294117645,
      "grad_norm": 0.061660684645175934,
      "learning_rate": 0.0001480412371134021,
      "loss": 0.4514,
      "step": 887
    },
    {
      "epoch": 0.2611764705882353,
      "grad_norm": 0.06455861032009125,
      "learning_rate": 0.00014798232695139912,
      "loss": 0.3512,
      "step": 888
    },
    {
      "epoch": 0.2614705882352941,
      "grad_norm": 0.05554740130901337,
      "learning_rate": 0.00014792341678939618,
      "loss": 0.3482,
      "step": 889
    },
    {
      "epoch": 0.26176470588235295,
      "grad_norm": 0.054420437663793564,
      "learning_rate": 0.00014786450662739324,
      "loss": 0.2929,
      "step": 890
    },
    {
      "epoch": 0.2620588235294118,
      "grad_norm": 0.06252085417509079,
      "learning_rate": 0.0001478055964653903,
      "loss": 0.3738,
      "step": 891
    },
    {
      "epoch": 0.26235294117647057,
      "grad_norm": 0.05034801736474037,
      "learning_rate": 0.00014774668630338736,
      "loss": 0.297,
      "step": 892
    },
    {
      "epoch": 0.2626470588235294,
      "grad_norm": 0.060217149555683136,
      "learning_rate": 0.0001476877761413844,
      "loss": 0.3661,
      "step": 893
    },
    {
      "epoch": 0.26294117647058823,
      "grad_norm": 0.06295526027679443,
      "learning_rate": 0.00014762886597938146,
      "loss": 0.3382,
      "step": 894
    },
    {
      "epoch": 0.26323529411764707,
      "grad_norm": 0.050394248217344284,
      "learning_rate": 0.00014756995581737852,
      "loss": 0.3734,
      "step": 895
    },
    {
      "epoch": 0.2635294117647059,
      "grad_norm": 0.05018913373351097,
      "learning_rate": 0.00014751104565537558,
      "loss": 0.3047,
      "step": 896
    },
    {
      "epoch": 0.2638235294117647,
      "grad_norm": 0.05402011796832085,
      "learning_rate": 0.00014745213549337264,
      "loss": 0.3515,
      "step": 897
    },
    {
      "epoch": 0.2641176470588235,
      "grad_norm": 0.047407541424036026,
      "learning_rate": 0.00014739322533136967,
      "loss": 0.3666,
      "step": 898
    },
    {
      "epoch": 0.26441176470588235,
      "grad_norm": 0.05710144340991974,
      "learning_rate": 0.00014733431516936673,
      "loss": 0.3625,
      "step": 899
    },
    {
      "epoch": 0.2647058823529412,
      "grad_norm": 0.06017865613102913,
      "learning_rate": 0.0001472754050073638,
      "loss": 0.3585,
      "step": 900
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05810526758432388,
      "learning_rate": 0.00014721649484536085,
      "loss": 0.3747,
      "step": 901
    },
    {
      "epoch": 0.26529411764705885,
      "grad_norm": 0.047773025929927826,
      "learning_rate": 0.0001471575846833579,
      "loss": 0.3426,
      "step": 902
    },
    {
      "epoch": 0.2655882352941176,
      "grad_norm": 0.08043684810400009,
      "learning_rate": 0.00014709867452135494,
      "loss": 0.3977,
      "step": 903
    },
    {
      "epoch": 0.26588235294117646,
      "grad_norm": 0.04550379887223244,
      "learning_rate": 0.000147039764359352,
      "loss": 0.3415,
      "step": 904
    },
    {
      "epoch": 0.2661764705882353,
      "grad_norm": 0.052217159420251846,
      "learning_rate": 0.00014698085419734906,
      "loss": 0.3295,
      "step": 905
    },
    {
      "epoch": 0.2664705882352941,
      "grad_norm": 0.05069698765873909,
      "learning_rate": 0.00014692194403534612,
      "loss": 0.4378,
      "step": 906
    },
    {
      "epoch": 0.26676470588235296,
      "grad_norm": 0.052270691841840744,
      "learning_rate": 0.00014686303387334316,
      "loss": 0.3739,
      "step": 907
    },
    {
      "epoch": 0.26705882352941174,
      "grad_norm": 0.05404457077383995,
      "learning_rate": 0.0001468041237113402,
      "loss": 0.375,
      "step": 908
    },
    {
      "epoch": 0.26735294117647057,
      "grad_norm": 0.06109911575913429,
      "learning_rate": 0.00014674521354933725,
      "loss": 0.4206,
      "step": 909
    },
    {
      "epoch": 0.2676470588235294,
      "grad_norm": 0.05253685638308525,
      "learning_rate": 0.0001466863033873343,
      "loss": 0.3255,
      "step": 910
    },
    {
      "epoch": 0.26794117647058824,
      "grad_norm": 0.047936927527189255,
      "learning_rate": 0.00014662739322533137,
      "loss": 0.3056,
      "step": 911
    },
    {
      "epoch": 0.26823529411764707,
      "grad_norm": 0.04639742523431778,
      "learning_rate": 0.00014656848306332843,
      "loss": 0.305,
      "step": 912
    },
    {
      "epoch": 0.2685294117647059,
      "grad_norm": 0.057180773466825485,
      "learning_rate": 0.00014650957290132546,
      "loss": 0.328,
      "step": 913
    },
    {
      "epoch": 0.2688235294117647,
      "grad_norm": 0.08240971714258194,
      "learning_rate": 0.00014645066273932252,
      "loss": 0.3702,
      "step": 914
    },
    {
      "epoch": 0.2691176470588235,
      "grad_norm": 0.05134551599621773,
      "learning_rate": 0.00014639175257731958,
      "loss": 0.3378,
      "step": 915
    },
    {
      "epoch": 0.26941176470588235,
      "grad_norm": 0.054945845156908035,
      "learning_rate": 0.00014633284241531664,
      "loss": 0.3412,
      "step": 916
    },
    {
      "epoch": 0.2697058823529412,
      "grad_norm": 0.0538768470287323,
      "learning_rate": 0.0001462739322533137,
      "loss": 0.3156,
      "step": 917
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.047760844230651855,
      "learning_rate": 0.00014621502209131074,
      "loss": 0.3007,
      "step": 918
    },
    {
      "epoch": 0.2702941176470588,
      "grad_norm": 0.0563683845102787,
      "learning_rate": 0.0001461561119293078,
      "loss": 0.3462,
      "step": 919
    },
    {
      "epoch": 0.27058823529411763,
      "grad_norm": 0.05150555446743965,
      "learning_rate": 0.00014609720176730486,
      "loss": 0.3679,
      "step": 920
    },
    {
      "epoch": 0.27088235294117646,
      "grad_norm": 0.043114855885505676,
      "learning_rate": 0.00014603829160530192,
      "loss": 0.3599,
      "step": 921
    },
    {
      "epoch": 0.2711764705882353,
      "grad_norm": 0.0499865859746933,
      "learning_rate": 0.00014597938144329898,
      "loss": 0.3404,
      "step": 922
    },
    {
      "epoch": 0.27147058823529413,
      "grad_norm": 0.05895785242319107,
      "learning_rate": 0.000145920471281296,
      "loss": 0.3441,
      "step": 923
    },
    {
      "epoch": 0.27176470588235296,
      "grad_norm": 0.05856414884328842,
      "learning_rate": 0.00014586156111929307,
      "loss": 0.4088,
      "step": 924
    },
    {
      "epoch": 0.27205882352941174,
      "grad_norm": 0.04538234323263168,
      "learning_rate": 0.00014580265095729013,
      "loss": 0.3354,
      "step": 925
    },
    {
      "epoch": 0.2723529411764706,
      "grad_norm": 0.06002802029252052,
      "learning_rate": 0.0001457437407952872,
      "loss": 0.4296,
      "step": 926
    },
    {
      "epoch": 0.2726470588235294,
      "grad_norm": 0.06993532180786133,
      "learning_rate": 0.00014568483063328425,
      "loss": 0.3951,
      "step": 927
    },
    {
      "epoch": 0.27294117647058824,
      "grad_norm": 0.054085832089185715,
      "learning_rate": 0.00014562592047128128,
      "loss": 0.3192,
      "step": 928
    },
    {
      "epoch": 0.2732352941176471,
      "grad_norm": 0.05395392328500748,
      "learning_rate": 0.00014556701030927834,
      "loss": 0.3681,
      "step": 929
    },
    {
      "epoch": 0.2735294117647059,
      "grad_norm": 0.05485706031322479,
      "learning_rate": 0.0001455081001472754,
      "loss": 0.3372,
      "step": 930
    },
    {
      "epoch": 0.2738235294117647,
      "grad_norm": 0.04228930547833443,
      "learning_rate": 0.00014544918998527246,
      "loss": 0.3249,
      "step": 931
    },
    {
      "epoch": 0.2741176470588235,
      "grad_norm": 0.05523638054728508,
      "learning_rate": 0.00014539027982326952,
      "loss": 0.3669,
      "step": 932
    },
    {
      "epoch": 0.27441176470588236,
      "grad_norm": 0.059681788086891174,
      "learning_rate": 0.00014533136966126656,
      "loss": 0.3452,
      "step": 933
    },
    {
      "epoch": 0.2747058823529412,
      "grad_norm": 0.05565102398395538,
      "learning_rate": 0.00014527245949926362,
      "loss": 0.3277,
      "step": 934
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.05444546416401863,
      "learning_rate": 0.00014521354933726068,
      "loss": 0.3573,
      "step": 935
    },
    {
      "epoch": 0.2752941176470588,
      "grad_norm": 0.0540597066283226,
      "learning_rate": 0.00014515463917525774,
      "loss": 0.4115,
      "step": 936
    },
    {
      "epoch": 0.27558823529411763,
      "grad_norm": 0.04078114777803421,
      "learning_rate": 0.0001450957290132548,
      "loss": 0.3223,
      "step": 937
    },
    {
      "epoch": 0.27588235294117647,
      "grad_norm": 0.055918723344802856,
      "learning_rate": 0.00014503681885125183,
      "loss": 0.3346,
      "step": 938
    },
    {
      "epoch": 0.2761764705882353,
      "grad_norm": 0.04926730692386627,
      "learning_rate": 0.0001449779086892489,
      "loss": 0.398,
      "step": 939
    },
    {
      "epoch": 0.27647058823529413,
      "grad_norm": 0.055656615644693375,
      "learning_rate": 0.00014491899852724595,
      "loss": 0.3776,
      "step": 940
    },
    {
      "epoch": 0.27676470588235297,
      "grad_norm": 0.04632708057761192,
      "learning_rate": 0.000144860088365243,
      "loss": 0.3798,
      "step": 941
    },
    {
      "epoch": 0.27705882352941175,
      "grad_norm": 0.05229625850915909,
      "learning_rate": 0.00014480117820324007,
      "loss": 0.3391,
      "step": 942
    },
    {
      "epoch": 0.2773529411764706,
      "grad_norm": 0.0403810478746891,
      "learning_rate": 0.0001447422680412371,
      "loss": 0.2661,
      "step": 943
    },
    {
      "epoch": 0.2776470588235294,
      "grad_norm": 0.062443945556879044,
      "learning_rate": 0.00014468335787923417,
      "loss": 0.4191,
      "step": 944
    },
    {
      "epoch": 0.27794117647058825,
      "grad_norm": 0.037968434393405914,
      "learning_rate": 0.00014462444771723123,
      "loss": 0.2812,
      "step": 945
    },
    {
      "epoch": 0.2782352941176471,
      "grad_norm": 0.05579185485839844,
      "learning_rate": 0.00014456553755522829,
      "loss": 0.3569,
      "step": 946
    },
    {
      "epoch": 0.27852941176470586,
      "grad_norm": 0.06327848881483078,
      "learning_rate": 0.00014450662739322535,
      "loss": 0.371,
      "step": 947
    },
    {
      "epoch": 0.2788235294117647,
      "grad_norm": 0.07567940652370453,
      "learning_rate": 0.00014444771723122238,
      "loss": 0.4381,
      "step": 948
    },
    {
      "epoch": 0.2791176470588235,
      "grad_norm": 0.050390057265758514,
      "learning_rate": 0.00014438880706921944,
      "loss": 0.3432,
      "step": 949
    },
    {
      "epoch": 0.27941176470588236,
      "grad_norm": 0.0642416924238205,
      "learning_rate": 0.0001443298969072165,
      "loss": 0.3706,
      "step": 950
    },
    {
      "epoch": 0.2797058823529412,
      "grad_norm": 0.06598974764347076,
      "learning_rate": 0.00014427098674521356,
      "loss": 0.4069,
      "step": 951
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06499747186899185,
      "learning_rate": 0.00014421207658321062,
      "loss": 0.4388,
      "step": 952
    },
    {
      "epoch": 0.2802941176470588,
      "grad_norm": 0.047016117721796036,
      "learning_rate": 0.00014415316642120765,
      "loss": 0.3563,
      "step": 953
    },
    {
      "epoch": 0.28058823529411764,
      "grad_norm": 0.06675603985786438,
      "learning_rate": 0.0001440942562592047,
      "loss": 0.4261,
      "step": 954
    },
    {
      "epoch": 0.28088235294117647,
      "grad_norm": 0.04983808845281601,
      "learning_rate": 0.00014403534609720177,
      "loss": 0.3007,
      "step": 955
    },
    {
      "epoch": 0.2811764705882353,
      "grad_norm": 0.043522849678993225,
      "learning_rate": 0.00014397643593519883,
      "loss": 0.4003,
      "step": 956
    },
    {
      "epoch": 0.28147058823529414,
      "grad_norm": 0.04677477851510048,
      "learning_rate": 0.0001439175257731959,
      "loss": 0.3196,
      "step": 957
    },
    {
      "epoch": 0.2817647058823529,
      "grad_norm": 0.04128250852227211,
      "learning_rate": 0.00014385861561119293,
      "loss": 0.2781,
      "step": 958
    },
    {
      "epoch": 0.28205882352941175,
      "grad_norm": 0.06443094462156296,
      "learning_rate": 0.00014379970544919,
      "loss": 0.3697,
      "step": 959
    },
    {
      "epoch": 0.2823529411764706,
      "grad_norm": 0.049895379692316055,
      "learning_rate": 0.00014374079528718705,
      "loss": 0.4171,
      "step": 960
    },
    {
      "epoch": 0.2826470588235294,
      "grad_norm": 0.04852476343512535,
      "learning_rate": 0.0001436818851251841,
      "loss": 0.3555,
      "step": 961
    },
    {
      "epoch": 0.28294117647058825,
      "grad_norm": 0.055776163935661316,
      "learning_rate": 0.00014362297496318117,
      "loss": 0.401,
      "step": 962
    },
    {
      "epoch": 0.2832352941176471,
      "grad_norm": 0.043341152369976044,
      "learning_rate": 0.0001435640648011782,
      "loss": 0.3427,
      "step": 963
    },
    {
      "epoch": 0.28352941176470586,
      "grad_norm": 0.0474889799952507,
      "learning_rate": 0.00014350515463917526,
      "loss": 0.3752,
      "step": 964
    },
    {
      "epoch": 0.2838235294117647,
      "grad_norm": 0.05166321247816086,
      "learning_rate": 0.00014344624447717232,
      "loss": 0.3732,
      "step": 965
    },
    {
      "epoch": 0.28411764705882353,
      "grad_norm": 0.04808064550161362,
      "learning_rate": 0.00014338733431516938,
      "loss": 0.3496,
      "step": 966
    },
    {
      "epoch": 0.28441176470588236,
      "grad_norm": 0.07296455651521683,
      "learning_rate": 0.00014332842415316644,
      "loss": 0.4219,
      "step": 967
    },
    {
      "epoch": 0.2847058823529412,
      "grad_norm": 0.04337478056550026,
      "learning_rate": 0.00014326951399116347,
      "loss": 0.2493,
      "step": 968
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.05178951099514961,
      "learning_rate": 0.00014321060382916053,
      "loss": 0.3127,
      "step": 969
    },
    {
      "epoch": 0.2852941176470588,
      "grad_norm": 0.057643283158540726,
      "learning_rate": 0.0001431516936671576,
      "loss": 0.3866,
      "step": 970
    },
    {
      "epoch": 0.28558823529411764,
      "grad_norm": 0.07185952365398407,
      "learning_rate": 0.00014309278350515465,
      "loss": 0.4367,
      "step": 971
    },
    {
      "epoch": 0.2858823529411765,
      "grad_norm": 0.06244342401623726,
      "learning_rate": 0.00014303387334315171,
      "loss": 0.4253,
      "step": 972
    },
    {
      "epoch": 0.2861764705882353,
      "grad_norm": 0.06102823466062546,
      "learning_rate": 0.00014297496318114875,
      "loss": 0.3888,
      "step": 973
    },
    {
      "epoch": 0.28647058823529414,
      "grad_norm": 0.04761918634176254,
      "learning_rate": 0.0001429160530191458,
      "loss": 0.3148,
      "step": 974
    },
    {
      "epoch": 0.2867647058823529,
      "grad_norm": 0.061437785625457764,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3687,
      "step": 975
    },
    {
      "epoch": 0.28705882352941176,
      "grad_norm": 0.05294808745384216,
      "learning_rate": 0.00014279823269513993,
      "loss": 0.3527,
      "step": 976
    },
    {
      "epoch": 0.2873529411764706,
      "grad_norm": 0.0563492514193058,
      "learning_rate": 0.000142739322533137,
      "loss": 0.4083,
      "step": 977
    },
    {
      "epoch": 0.2876470588235294,
      "grad_norm": 0.04850958287715912,
      "learning_rate": 0.00014268041237113402,
      "loss": 0.3412,
      "step": 978
    },
    {
      "epoch": 0.28794117647058826,
      "grad_norm": 0.06221327558159828,
      "learning_rate": 0.00014262150220913108,
      "loss": 0.3826,
      "step": 979
    },
    {
      "epoch": 0.28823529411764703,
      "grad_norm": 0.05060657486319542,
      "learning_rate": 0.00014256259204712814,
      "loss": 0.3531,
      "step": 980
    },
    {
      "epoch": 0.28852941176470587,
      "grad_norm": 0.06314856559038162,
      "learning_rate": 0.0001425036818851252,
      "loss": 0.3446,
      "step": 981
    },
    {
      "epoch": 0.2888235294117647,
      "grad_norm": 0.05264134705066681,
      "learning_rate": 0.00014244477172312226,
      "loss": 0.4282,
      "step": 982
    },
    {
      "epoch": 0.28911764705882353,
      "grad_norm": 0.05170394852757454,
      "learning_rate": 0.0001423858615611193,
      "loss": 0.3716,
      "step": 983
    },
    {
      "epoch": 0.28941176470588237,
      "grad_norm": 0.0440501905977726,
      "learning_rate": 0.00014232695139911636,
      "loss": 0.3877,
      "step": 984
    },
    {
      "epoch": 0.2897058823529412,
      "grad_norm": 0.06964879482984543,
      "learning_rate": 0.00014226804123711342,
      "loss": 0.4354,
      "step": 985
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.0427592471241951,
      "learning_rate": 0.00014220913107511048,
      "loss": 0.318,
      "step": 986
    },
    {
      "epoch": 0.2902941176470588,
      "grad_norm": 0.04687510058283806,
      "learning_rate": 0.00014215022091310754,
      "loss": 0.3432,
      "step": 987
    },
    {
      "epoch": 0.29058823529411765,
      "grad_norm": 0.06756463646888733,
      "learning_rate": 0.00014209131075110457,
      "loss": 0.3905,
      "step": 988
    },
    {
      "epoch": 0.2908823529411765,
      "grad_norm": 0.04211856424808502,
      "learning_rate": 0.00014203240058910163,
      "loss": 0.3422,
      "step": 989
    },
    {
      "epoch": 0.2911764705882353,
      "grad_norm": 0.04699687659740448,
      "learning_rate": 0.0001419734904270987,
      "loss": 0.3223,
      "step": 990
    },
    {
      "epoch": 0.2914705882352941,
      "grad_norm": 0.0592491552233696,
      "learning_rate": 0.00014191458026509575,
      "loss": 0.4158,
      "step": 991
    },
    {
      "epoch": 0.2917647058823529,
      "grad_norm": 0.04505949094891548,
      "learning_rate": 0.0001418556701030928,
      "loss": 0.3708,
      "step": 992
    },
    {
      "epoch": 0.29205882352941176,
      "grad_norm": 0.04811772331595421,
      "learning_rate": 0.00014179675994108984,
      "loss": 0.2678,
      "step": 993
    },
    {
      "epoch": 0.2923529411764706,
      "grad_norm": 0.048653293401002884,
      "learning_rate": 0.0001417378497790869,
      "loss": 0.3853,
      "step": 994
    },
    {
      "epoch": 0.2926470588235294,
      "grad_norm": 0.05546145513653755,
      "learning_rate": 0.00014167893961708396,
      "loss": 0.3563,
      "step": 995
    },
    {
      "epoch": 0.29294117647058826,
      "grad_norm": 0.04412589594721794,
      "learning_rate": 0.00014162002945508102,
      "loss": 0.3256,
      "step": 996
    },
    {
      "epoch": 0.29323529411764704,
      "grad_norm": 0.04751283675432205,
      "learning_rate": 0.00014156111929307808,
      "loss": 0.3829,
      "step": 997
    },
    {
      "epoch": 0.29352941176470587,
      "grad_norm": 0.04826300963759422,
      "learning_rate": 0.00014150220913107512,
      "loss": 0.331,
      "step": 998
    },
    {
      "epoch": 0.2938235294117647,
      "grad_norm": 0.05251488462090492,
      "learning_rate": 0.00014144329896907218,
      "loss": 0.3192,
      "step": 999
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.042043812572956085,
      "learning_rate": 0.00014138438880706924,
      "loss": 0.3131,
      "step": 1000
    },
    {
      "epoch": 0.2944117647058824,
      "grad_norm": 0.051341280341148376,
      "learning_rate": 0.0001413254786450663,
      "loss": 0.3874,
      "step": 1001
    },
    {
      "epoch": 0.29470588235294115,
      "grad_norm": 0.05732183903455734,
      "learning_rate": 0.00014126656848306336,
      "loss": 0.4081,
      "step": 1002
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.052939433604478836,
      "learning_rate": 0.0001412076583210604,
      "loss": 0.293,
      "step": 1003
    },
    {
      "epoch": 0.2952941176470588,
      "grad_norm": 0.048597078770399094,
      "learning_rate": 0.00014114874815905745,
      "loss": 0.3731,
      "step": 1004
    },
    {
      "epoch": 0.29558823529411765,
      "grad_norm": 0.05382390320301056,
      "learning_rate": 0.0001410898379970545,
      "loss": 0.339,
      "step": 1005
    },
    {
      "epoch": 0.2958823529411765,
      "grad_norm": 0.0557575523853302,
      "learning_rate": 0.00014103092783505157,
      "loss": 0.3071,
      "step": 1006
    },
    {
      "epoch": 0.2961764705882353,
      "grad_norm": 0.04721474274992943,
      "learning_rate": 0.00014097201767304863,
      "loss": 0.3034,
      "step": 1007
    },
    {
      "epoch": 0.2964705882352941,
      "grad_norm": 0.0453617237508297,
      "learning_rate": 0.00014091310751104566,
      "loss": 0.2703,
      "step": 1008
    },
    {
      "epoch": 0.29676470588235293,
      "grad_norm": 0.06340935826301575,
      "learning_rate": 0.00014085419734904272,
      "loss": 0.3545,
      "step": 1009
    },
    {
      "epoch": 0.29705882352941176,
      "grad_norm": 0.0745542123913765,
      "learning_rate": 0.00014079528718703978,
      "loss": 0.4097,
      "step": 1010
    },
    {
      "epoch": 0.2973529411764706,
      "grad_norm": 0.07125820219516754,
      "learning_rate": 0.00014073637702503684,
      "loss": 0.3752,
      "step": 1011
    },
    {
      "epoch": 0.29764705882352943,
      "grad_norm": 0.04367093741893768,
      "learning_rate": 0.0001406774668630339,
      "loss": 0.3059,
      "step": 1012
    },
    {
      "epoch": 0.2979411764705882,
      "grad_norm": 0.05852298066020012,
      "learning_rate": 0.0001406185567010309,
      "loss": 0.3381,
      "step": 1013
    },
    {
      "epoch": 0.29823529411764704,
      "grad_norm": 0.04852944239974022,
      "learning_rate": 0.00014055964653902797,
      "loss": 0.362,
      "step": 1014
    },
    {
      "epoch": 0.2985294117647059,
      "grad_norm": 0.05690501257777214,
      "learning_rate": 0.00014050073637702503,
      "loss": 0.3834,
      "step": 1015
    },
    {
      "epoch": 0.2988235294117647,
      "grad_norm": 0.04620125889778137,
      "learning_rate": 0.0001404418262150221,
      "loss": 0.3322,
      "step": 1016
    },
    {
      "epoch": 0.29911764705882354,
      "grad_norm": 0.0536544993519783,
      "learning_rate": 0.00014038291605301915,
      "loss": 0.3707,
      "step": 1017
    },
    {
      "epoch": 0.2994117647058824,
      "grad_norm": 0.04953986778855324,
      "learning_rate": 0.00014032400589101618,
      "loss": 0.3647,
      "step": 1018
    },
    {
      "epoch": 0.29970588235294116,
      "grad_norm": 0.06503067910671234,
      "learning_rate": 0.00014026509572901324,
      "loss": 0.3413,
      "step": 1019
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.052080508321523666,
      "learning_rate": 0.0001402061855670103,
      "loss": 0.3581,
      "step": 1020
    },
    {
      "epoch": 0.3002941176470588,
      "grad_norm": 0.05582733079791069,
      "learning_rate": 0.00014014727540500736,
      "loss": 0.3339,
      "step": 1021
    },
    {
      "epoch": 0.30058823529411766,
      "grad_norm": 0.05468344688415527,
      "learning_rate": 0.00014008836524300442,
      "loss": 0.3562,
      "step": 1022
    },
    {
      "epoch": 0.3008823529411765,
      "grad_norm": 0.050474394112825394,
      "learning_rate": 0.00014002945508100146,
      "loss": 0.3662,
      "step": 1023
    },
    {
      "epoch": 0.30117647058823527,
      "grad_norm": 0.0433373898267746,
      "learning_rate": 0.00013997054491899852,
      "loss": 0.2897,
      "step": 1024
    },
    {
      "epoch": 0.3014705882352941,
      "grad_norm": 0.04819948226213455,
      "learning_rate": 0.00013991163475699558,
      "loss": 0.4062,
      "step": 1025
    },
    {
      "epoch": 0.30176470588235293,
      "grad_norm": 0.05419018119573593,
      "learning_rate": 0.00013985272459499264,
      "loss": 0.3724,
      "step": 1026
    },
    {
      "epoch": 0.30205882352941177,
      "grad_norm": 0.05859893187880516,
      "learning_rate": 0.0001397938144329897,
      "loss": 0.3657,
      "step": 1027
    },
    {
      "epoch": 0.3023529411764706,
      "grad_norm": 0.06144881621003151,
      "learning_rate": 0.00013973490427098673,
      "loss": 0.3932,
      "step": 1028
    },
    {
      "epoch": 0.30264705882352944,
      "grad_norm": 0.05366535484790802,
      "learning_rate": 0.0001396759941089838,
      "loss": 0.4074,
      "step": 1029
    },
    {
      "epoch": 0.3029411764705882,
      "grad_norm": 0.06277600675821304,
      "learning_rate": 0.00013961708394698085,
      "loss": 0.3787,
      "step": 1030
    },
    {
      "epoch": 0.30323529411764705,
      "grad_norm": 0.03793596476316452,
      "learning_rate": 0.0001395581737849779,
      "loss": 0.2742,
      "step": 1031
    },
    {
      "epoch": 0.3035294117647059,
      "grad_norm": 0.04548404738306999,
      "learning_rate": 0.00013949926362297497,
      "loss": 0.2931,
      "step": 1032
    },
    {
      "epoch": 0.3038235294117647,
      "grad_norm": 0.06761139631271362,
      "learning_rate": 0.000139440353460972,
      "loss": 0.3707,
      "step": 1033
    },
    {
      "epoch": 0.30411764705882355,
      "grad_norm": 0.05042809620499611,
      "learning_rate": 0.00013938144329896907,
      "loss": 0.328,
      "step": 1034
    },
    {
      "epoch": 0.3044117647058823,
      "grad_norm": 0.049906518310308456,
      "learning_rate": 0.00013932253313696613,
      "loss": 0.3673,
      "step": 1035
    },
    {
      "epoch": 0.30470588235294116,
      "grad_norm": 0.06521006673574448,
      "learning_rate": 0.00013926362297496319,
      "loss": 0.3747,
      "step": 1036
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.04304023087024689,
      "learning_rate": 0.00013920471281296025,
      "loss": 0.2898,
      "step": 1037
    },
    {
      "epoch": 0.3052941176470588,
      "grad_norm": 0.031785622239112854,
      "learning_rate": 0.00013914580265095728,
      "loss": 0.2269,
      "step": 1038
    },
    {
      "epoch": 0.30558823529411766,
      "grad_norm": 0.051136963069438934,
      "learning_rate": 0.00013908689248895434,
      "loss": 0.3903,
      "step": 1039
    },
    {
      "epoch": 0.3058823529411765,
      "grad_norm": 0.04732788726687431,
      "learning_rate": 0.0001390279823269514,
      "loss": 0.3354,
      "step": 1040
    },
    {
      "epoch": 0.30617647058823527,
      "grad_norm": 0.0638386458158493,
      "learning_rate": 0.00013896907216494846,
      "loss": 0.4203,
      "step": 1041
    },
    {
      "epoch": 0.3064705882352941,
      "grad_norm": 0.049837905913591385,
      "learning_rate": 0.00013891016200294552,
      "loss": 0.3673,
      "step": 1042
    },
    {
      "epoch": 0.30676470588235294,
      "grad_norm": 0.044602878391742706,
      "learning_rate": 0.00013885125184094255,
      "loss": 0.2887,
      "step": 1043
    },
    {
      "epoch": 0.3070588235294118,
      "grad_norm": 0.048335302621126175,
      "learning_rate": 0.0001387923416789396,
      "loss": 0.3258,
      "step": 1044
    },
    {
      "epoch": 0.3073529411764706,
      "grad_norm": 0.06201092526316643,
      "learning_rate": 0.00013873343151693667,
      "loss": 0.4375,
      "step": 1045
    },
    {
      "epoch": 0.3076470588235294,
      "grad_norm": 0.05788855254650116,
      "learning_rate": 0.00013867452135493373,
      "loss": 0.3867,
      "step": 1046
    },
    {
      "epoch": 0.3079411764705882,
      "grad_norm": 0.0427359901368618,
      "learning_rate": 0.0001386156111929308,
      "loss": 0.3694,
      "step": 1047
    },
    {
      "epoch": 0.30823529411764705,
      "grad_norm": 0.07275224477052689,
      "learning_rate": 0.00013855670103092783,
      "loss": 0.4088,
      "step": 1048
    },
    {
      "epoch": 0.3085294117647059,
      "grad_norm": 0.06267794221639633,
      "learning_rate": 0.00013849779086892489,
      "loss": 0.3288,
      "step": 1049
    },
    {
      "epoch": 0.3088235294117647,
      "grad_norm": 0.05001003295183182,
      "learning_rate": 0.00013843888070692195,
      "loss": 0.3324,
      "step": 1050
    },
    {
      "epoch": 0.30911764705882355,
      "grad_norm": 0.045970428735017776,
      "learning_rate": 0.000138379970544919,
      "loss": 0.2827,
      "step": 1051
    },
    {
      "epoch": 0.30941176470588233,
      "grad_norm": 0.05254971608519554,
      "learning_rate": 0.00013832106038291607,
      "loss": 0.3431,
      "step": 1052
    },
    {
      "epoch": 0.30970588235294116,
      "grad_norm": 0.0602114200592041,
      "learning_rate": 0.0001382621502209131,
      "loss": 0.352,
      "step": 1053
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0577702671289444,
      "learning_rate": 0.00013820324005891016,
      "loss": 0.4206,
      "step": 1054
    },
    {
      "epoch": 0.31029411764705883,
      "grad_norm": 0.0564039908349514,
      "learning_rate": 0.00013814432989690722,
      "loss": 0.3579,
      "step": 1055
    },
    {
      "epoch": 0.31058823529411766,
      "grad_norm": 0.05527668073773384,
      "learning_rate": 0.00013808541973490428,
      "loss": 0.4295,
      "step": 1056
    },
    {
      "epoch": 0.31088235294117644,
      "grad_norm": 0.05100282281637192,
      "learning_rate": 0.00013802650957290134,
      "loss": 0.384,
      "step": 1057
    },
    {
      "epoch": 0.3111764705882353,
      "grad_norm": 0.05363363027572632,
      "learning_rate": 0.00013796759941089837,
      "loss": 0.38,
      "step": 1058
    },
    {
      "epoch": 0.3114705882352941,
      "grad_norm": 0.0552857480943203,
      "learning_rate": 0.00013790868924889543,
      "loss": 0.3679,
      "step": 1059
    },
    {
      "epoch": 0.31176470588235294,
      "grad_norm": 0.05524974688887596,
      "learning_rate": 0.0001378497790868925,
      "loss": 0.3647,
      "step": 1060
    },
    {
      "epoch": 0.3120588235294118,
      "grad_norm": 0.048499077558517456,
      "learning_rate": 0.00013779086892488955,
      "loss": 0.3439,
      "step": 1061
    },
    {
      "epoch": 0.3123529411764706,
      "grad_norm": 0.046894680708646774,
      "learning_rate": 0.00013773195876288661,
      "loss": 0.3264,
      "step": 1062
    },
    {
      "epoch": 0.3126470588235294,
      "grad_norm": 0.04811553657054901,
      "learning_rate": 0.00013767304860088365,
      "loss": 0.3262,
      "step": 1063
    },
    {
      "epoch": 0.3129411764705882,
      "grad_norm": 0.04047423601150513,
      "learning_rate": 0.0001376141384388807,
      "loss": 0.3056,
      "step": 1064
    },
    {
      "epoch": 0.31323529411764706,
      "grad_norm": 0.051817793399095535,
      "learning_rate": 0.00013755522827687777,
      "loss": 0.3703,
      "step": 1065
    },
    {
      "epoch": 0.3135294117647059,
      "grad_norm": 0.059142518788576126,
      "learning_rate": 0.00013749631811487483,
      "loss": 0.3725,
      "step": 1066
    },
    {
      "epoch": 0.3138235294117647,
      "grad_norm": 0.04984457418322563,
      "learning_rate": 0.0001374374079528719,
      "loss": 0.3744,
      "step": 1067
    },
    {
      "epoch": 0.31411764705882356,
      "grad_norm": 0.04663741588592529,
      "learning_rate": 0.00013737849779086892,
      "loss": 0.3308,
      "step": 1068
    },
    {
      "epoch": 0.31441176470588234,
      "grad_norm": 0.044646967202425,
      "learning_rate": 0.00013731958762886598,
      "loss": 0.3521,
      "step": 1069
    },
    {
      "epoch": 0.31470588235294117,
      "grad_norm": 0.041974812746047974,
      "learning_rate": 0.00013726067746686304,
      "loss": 0.2823,
      "step": 1070
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.04352371767163277,
      "learning_rate": 0.0001372017673048601,
      "loss": 0.3461,
      "step": 1071
    },
    {
      "epoch": 0.31529411764705884,
      "grad_norm": 0.055350739508867264,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.3612,
      "step": 1072
    },
    {
      "epoch": 0.31558823529411767,
      "grad_norm": 0.06913478672504425,
      "learning_rate": 0.0001370839469808542,
      "loss": 0.3735,
      "step": 1073
    },
    {
      "epoch": 0.31588235294117645,
      "grad_norm": 0.041391290724277496,
      "learning_rate": 0.00013702503681885126,
      "loss": 0.3416,
      "step": 1074
    },
    {
      "epoch": 0.3161764705882353,
      "grad_norm": 0.05816817283630371,
      "learning_rate": 0.00013696612665684832,
      "loss": 0.3962,
      "step": 1075
    },
    {
      "epoch": 0.3164705882352941,
      "grad_norm": 0.048165369778871536,
      "learning_rate": 0.00013690721649484538,
      "loss": 0.3569,
      "step": 1076
    },
    {
      "epoch": 0.31676470588235295,
      "grad_norm": 0.05057896301150322,
      "learning_rate": 0.00013684830633284244,
      "loss": 0.3312,
      "step": 1077
    },
    {
      "epoch": 0.3170588235294118,
      "grad_norm": 0.05162261426448822,
      "learning_rate": 0.00013678939617083947,
      "loss": 0.3552,
      "step": 1078
    },
    {
      "epoch": 0.3173529411764706,
      "grad_norm": 0.05308175086975098,
      "learning_rate": 0.00013673048600883653,
      "loss": 0.2831,
      "step": 1079
    },
    {
      "epoch": 0.3176470588235294,
      "grad_norm": 0.05311012640595436,
      "learning_rate": 0.0001366715758468336,
      "loss": 0.4058,
      "step": 1080
    },
    {
      "epoch": 0.3179411764705882,
      "grad_norm": 0.0663798376917839,
      "learning_rate": 0.00013661266568483065,
      "loss": 0.391,
      "step": 1081
    },
    {
      "epoch": 0.31823529411764706,
      "grad_norm": 0.05489886552095413,
      "learning_rate": 0.0001365537555228277,
      "loss": 0.2937,
      "step": 1082
    },
    {
      "epoch": 0.3185294117647059,
      "grad_norm": 0.048546139150857925,
      "learning_rate": 0.00013649484536082474,
      "loss": 0.3674,
      "step": 1083
    },
    {
      "epoch": 0.31882352941176473,
      "grad_norm": 0.055234961211681366,
      "learning_rate": 0.0001364359351988218,
      "loss": 0.3731,
      "step": 1084
    },
    {
      "epoch": 0.3191176470588235,
      "grad_norm": 0.04562889039516449,
      "learning_rate": 0.00013637702503681886,
      "loss": 0.2734,
      "step": 1085
    },
    {
      "epoch": 0.31941176470588234,
      "grad_norm": 0.05898446589708328,
      "learning_rate": 0.00013631811487481592,
      "loss": 0.3617,
      "step": 1086
    },
    {
      "epoch": 0.3197058823529412,
      "grad_norm": 0.046660155057907104,
      "learning_rate": 0.00013625920471281298,
      "loss": 0.2847,
      "step": 1087
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.046587660908699036,
      "learning_rate": 0.00013620029455081002,
      "loss": 0.3655,
      "step": 1088
    },
    {
      "epoch": 0.32029411764705884,
      "grad_norm": 0.05368533357977867,
      "learning_rate": 0.00013614138438880708,
      "loss": 0.3793,
      "step": 1089
    },
    {
      "epoch": 0.3205882352941177,
      "grad_norm": 0.05707770213484764,
      "learning_rate": 0.00013608247422680414,
      "loss": 0.3495,
      "step": 1090
    },
    {
      "epoch": 0.32088235294117645,
      "grad_norm": 0.04955393821001053,
      "learning_rate": 0.0001360235640648012,
      "loss": 0.3648,
      "step": 1091
    },
    {
      "epoch": 0.3211764705882353,
      "grad_norm": 0.04677427187561989,
      "learning_rate": 0.00013596465390279826,
      "loss": 0.3031,
      "step": 1092
    },
    {
      "epoch": 0.3214705882352941,
      "grad_norm": 0.05691395699977875,
      "learning_rate": 0.0001359057437407953,
      "loss": 0.3468,
      "step": 1093
    },
    {
      "epoch": 0.32176470588235295,
      "grad_norm": 0.05948138236999512,
      "learning_rate": 0.00013584683357879235,
      "loss": 0.4187,
      "step": 1094
    },
    {
      "epoch": 0.3220588235294118,
      "grad_norm": 0.06208048015832901,
      "learning_rate": 0.0001357879234167894,
      "loss": 0.3769,
      "step": 1095
    },
    {
      "epoch": 0.32235294117647056,
      "grad_norm": 0.05681874230504036,
      "learning_rate": 0.00013572901325478647,
      "loss": 0.4021,
      "step": 1096
    },
    {
      "epoch": 0.3226470588235294,
      "grad_norm": 0.056921616196632385,
      "learning_rate": 0.00013567010309278353,
      "loss": 0.3689,
      "step": 1097
    },
    {
      "epoch": 0.32294117647058823,
      "grad_norm": 0.05650492012500763,
      "learning_rate": 0.00013561119293078056,
      "loss": 0.3981,
      "step": 1098
    },
    {
      "epoch": 0.32323529411764707,
      "grad_norm": 0.04958132654428482,
      "learning_rate": 0.00013555228276877762,
      "loss": 0.3189,
      "step": 1099
    },
    {
      "epoch": 0.3235294117647059,
      "grad_norm": 0.05411536991596222,
      "learning_rate": 0.00013549337260677468,
      "loss": 0.3653,
      "step": 1100
    },
    {
      "epoch": 0.32382352941176473,
      "grad_norm": 0.060905225574970245,
      "learning_rate": 0.00013543446244477174,
      "loss": 0.2597,
      "step": 1101
    },
    {
      "epoch": 0.3241176470588235,
      "grad_norm": 0.058901071548461914,
      "learning_rate": 0.0001353755522827688,
      "loss": 0.293,
      "step": 1102
    },
    {
      "epoch": 0.32441176470588234,
      "grad_norm": 0.05032173916697502,
      "learning_rate": 0.00013531664212076584,
      "loss": 0.3299,
      "step": 1103
    },
    {
      "epoch": 0.3247058823529412,
      "grad_norm": 0.05952264368534088,
      "learning_rate": 0.0001352577319587629,
      "loss": 0.3491,
      "step": 1104
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.041966840624809265,
      "learning_rate": 0.00013519882179675996,
      "loss": 0.2822,
      "step": 1105
    },
    {
      "epoch": 0.32529411764705884,
      "grad_norm": 0.054049789905548096,
      "learning_rate": 0.00013513991163475702,
      "loss": 0.3297,
      "step": 1106
    },
    {
      "epoch": 0.3255882352941176,
      "grad_norm": 0.05360212177038193,
      "learning_rate": 0.00013508100147275408,
      "loss": 0.3517,
      "step": 1107
    },
    {
      "epoch": 0.32588235294117646,
      "grad_norm": 0.04166169464588165,
      "learning_rate": 0.0001350220913107511,
      "loss": 0.3206,
      "step": 1108
    },
    {
      "epoch": 0.3261764705882353,
      "grad_norm": 0.05082499608397484,
      "learning_rate": 0.00013496318114874817,
      "loss": 0.3961,
      "step": 1109
    },
    {
      "epoch": 0.3264705882352941,
      "grad_norm": 0.0545523576438427,
      "learning_rate": 0.00013490427098674523,
      "loss": 0.3646,
      "step": 1110
    },
    {
      "epoch": 0.32676470588235296,
      "grad_norm": 0.06231502443552017,
      "learning_rate": 0.0001348453608247423,
      "loss": 0.3833,
      "step": 1111
    },
    {
      "epoch": 0.3270588235294118,
      "grad_norm": 0.05166138708591461,
      "learning_rate": 0.00013478645066273935,
      "loss": 0.365,
      "step": 1112
    },
    {
      "epoch": 0.32735294117647057,
      "grad_norm": 0.04634623974561691,
      "learning_rate": 0.00013472754050073638,
      "loss": 0.3292,
      "step": 1113
    },
    {
      "epoch": 0.3276470588235294,
      "grad_norm": 0.05647117644548416,
      "learning_rate": 0.00013466863033873344,
      "loss": 0.3138,
      "step": 1114
    },
    {
      "epoch": 0.32794117647058824,
      "grad_norm": 0.08821556717157364,
      "learning_rate": 0.0001346097201767305,
      "loss": 0.4855,
      "step": 1115
    },
    {
      "epoch": 0.32823529411764707,
      "grad_norm": 0.05965130776166916,
      "learning_rate": 0.00013455081001472757,
      "loss": 0.3736,
      "step": 1116
    },
    {
      "epoch": 0.3285294117647059,
      "grad_norm": 0.053009383380413055,
      "learning_rate": 0.00013449189985272463,
      "loss": 0.3389,
      "step": 1117
    },
    {
      "epoch": 0.3288235294117647,
      "grad_norm": 0.05358622223138809,
      "learning_rate": 0.00013443298969072166,
      "loss": 0.3801,
      "step": 1118
    },
    {
      "epoch": 0.3291176470588235,
      "grad_norm": 0.06522226333618164,
      "learning_rate": 0.0001343740795287187,
      "loss": 0.334,
      "step": 1119
    },
    {
      "epoch": 0.32941176470588235,
      "grad_norm": 0.0511314794421196,
      "learning_rate": 0.00013431516936671575,
      "loss": 0.3786,
      "step": 1120
    },
    {
      "epoch": 0.3297058823529412,
      "grad_norm": 0.050858501344919205,
      "learning_rate": 0.0001342562592047128,
      "loss": 0.311,
      "step": 1121
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.056546714156866074,
      "learning_rate": 0.00013419734904270987,
      "loss": 0.3584,
      "step": 1122
    },
    {
      "epoch": 0.33029411764705885,
      "grad_norm": 0.056931186467409134,
      "learning_rate": 0.0001341384388807069,
      "loss": 0.3409,
      "step": 1123
    },
    {
      "epoch": 0.3305882352941176,
      "grad_norm": 0.047702353447675705,
      "learning_rate": 0.00013407952871870397,
      "loss": 0.3111,
      "step": 1124
    },
    {
      "epoch": 0.33088235294117646,
      "grad_norm": 0.04180747643113136,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.32,
      "step": 1125
    },
    {
      "epoch": 0.3311764705882353,
      "grad_norm": 0.03771497681736946,
      "learning_rate": 0.00013396170839469809,
      "loss": 0.3429,
      "step": 1126
    },
    {
      "epoch": 0.33147058823529413,
      "grad_norm": 0.06501225382089615,
      "learning_rate": 0.00013390279823269515,
      "loss": 0.4667,
      "step": 1127
    },
    {
      "epoch": 0.33176470588235296,
      "grad_norm": 0.04210295528173447,
      "learning_rate": 0.00013384388807069218,
      "loss": 0.2947,
      "step": 1128
    },
    {
      "epoch": 0.33205882352941174,
      "grad_norm": 0.0507059320807457,
      "learning_rate": 0.00013378497790868924,
      "loss": 0.3017,
      "step": 1129
    },
    {
      "epoch": 0.3323529411764706,
      "grad_norm": 0.046720776706933975,
      "learning_rate": 0.0001337260677466863,
      "loss": 0.3464,
      "step": 1130
    },
    {
      "epoch": 0.3326470588235294,
      "grad_norm": 0.06956089287996292,
      "learning_rate": 0.00013366715758468336,
      "loss": 0.4392,
      "step": 1131
    },
    {
      "epoch": 0.33294117647058824,
      "grad_norm": 0.0410098172724247,
      "learning_rate": 0.00013360824742268042,
      "loss": 0.3748,
      "step": 1132
    },
    {
      "epoch": 0.3332352941176471,
      "grad_norm": 0.04849325865507126,
      "learning_rate": 0.00013354933726067745,
      "loss": 0.3575,
      "step": 1133
    },
    {
      "epoch": 0.3335294117647059,
      "grad_norm": 0.03790104389190674,
      "learning_rate": 0.0001334904270986745,
      "loss": 0.3219,
      "step": 1134
    },
    {
      "epoch": 0.3338235294117647,
      "grad_norm": 0.05562128871679306,
      "learning_rate": 0.00013343151693667157,
      "loss": 0.3955,
      "step": 1135
    },
    {
      "epoch": 0.3341176470588235,
      "grad_norm": 0.049508705735206604,
      "learning_rate": 0.00013337260677466863,
      "loss": 0.3246,
      "step": 1136
    },
    {
      "epoch": 0.33441176470588235,
      "grad_norm": 0.05327296629548073,
      "learning_rate": 0.0001333136966126657,
      "loss": 0.3621,
      "step": 1137
    },
    {
      "epoch": 0.3347058823529412,
      "grad_norm": 0.04058036208152771,
      "learning_rate": 0.00013325478645066273,
      "loss": 0.2758,
      "step": 1138
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.05807316675782204,
      "learning_rate": 0.00013319587628865979,
      "loss": 0.4075,
      "step": 1139
    },
    {
      "epoch": 0.3352941176470588,
      "grad_norm": 0.06362071633338928,
      "learning_rate": 0.00013313696612665685,
      "loss": 0.4215,
      "step": 1140
    },
    {
      "epoch": 0.33558823529411763,
      "grad_norm": 0.05364774912595749,
      "learning_rate": 0.0001330780559646539,
      "loss": 0.3759,
      "step": 1141
    },
    {
      "epoch": 0.33588235294117647,
      "grad_norm": 0.053076982498168945,
      "learning_rate": 0.00013301914580265097,
      "loss": 0.3309,
      "step": 1142
    },
    {
      "epoch": 0.3361764705882353,
      "grad_norm": 0.042229343205690384,
      "learning_rate": 0.000132960235640648,
      "loss": 0.337,
      "step": 1143
    },
    {
      "epoch": 0.33647058823529413,
      "grad_norm": 0.0443028099834919,
      "learning_rate": 0.00013290132547864506,
      "loss": 0.3441,
      "step": 1144
    },
    {
      "epoch": 0.33676470588235297,
      "grad_norm": 0.048171836882829666,
      "learning_rate": 0.00013284241531664212,
      "loss": 0.3796,
      "step": 1145
    },
    {
      "epoch": 0.33705882352941174,
      "grad_norm": 0.04862277954816818,
      "learning_rate": 0.00013278350515463918,
      "loss": 0.3878,
      "step": 1146
    },
    {
      "epoch": 0.3373529411764706,
      "grad_norm": 0.04794380068778992,
      "learning_rate": 0.00013272459499263624,
      "loss": 0.2884,
      "step": 1147
    },
    {
      "epoch": 0.3376470588235294,
      "grad_norm": 0.045239683240652084,
      "learning_rate": 0.00013266568483063327,
      "loss": 0.3435,
      "step": 1148
    },
    {
      "epoch": 0.33794117647058824,
      "grad_norm": 0.054516080766916275,
      "learning_rate": 0.00013260677466863033,
      "loss": 0.3278,
      "step": 1149
    },
    {
      "epoch": 0.3382352941176471,
      "grad_norm": 0.06573491543531418,
      "learning_rate": 0.0001325478645066274,
      "loss": 0.358,
      "step": 1150
    },
    {
      "epoch": 0.33852941176470586,
      "grad_norm": 0.05373070389032364,
      "learning_rate": 0.00013248895434462445,
      "loss": 0.3672,
      "step": 1151
    },
    {
      "epoch": 0.3388235294117647,
      "grad_norm": 0.06397560983896255,
      "learning_rate": 0.00013243004418262151,
      "loss": 0.3491,
      "step": 1152
    },
    {
      "epoch": 0.3391176470588235,
      "grad_norm": 0.051888830959796906,
      "learning_rate": 0.00013237113402061855,
      "loss": 0.3513,
      "step": 1153
    },
    {
      "epoch": 0.33941176470588236,
      "grad_norm": 0.07812928408384323,
      "learning_rate": 0.0001323122238586156,
      "loss": 0.3966,
      "step": 1154
    },
    {
      "epoch": 0.3397058823529412,
      "grad_norm": 0.05271434411406517,
      "learning_rate": 0.00013225331369661267,
      "loss": 0.3744,
      "step": 1155
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.03349430114030838,
      "learning_rate": 0.00013219440353460973,
      "loss": 0.2378,
      "step": 1156
    },
    {
      "epoch": 0.3402941176470588,
      "grad_norm": 0.04840904101729393,
      "learning_rate": 0.0001321354933726068,
      "loss": 0.3039,
      "step": 1157
    },
    {
      "epoch": 0.34058823529411764,
      "grad_norm": 0.051185380667448044,
      "learning_rate": 0.00013207658321060382,
      "loss": 0.3322,
      "step": 1158
    },
    {
      "epoch": 0.34088235294117647,
      "grad_norm": 0.06270311772823334,
      "learning_rate": 0.00013201767304860088,
      "loss": 0.4098,
      "step": 1159
    },
    {
      "epoch": 0.3411764705882353,
      "grad_norm": 0.062127821147441864,
      "learning_rate": 0.00013195876288659794,
      "loss": 0.3761,
      "step": 1160
    },
    {
      "epoch": 0.34147058823529414,
      "grad_norm": 0.0544779971241951,
      "learning_rate": 0.000131899852724595,
      "loss": 0.3686,
      "step": 1161
    },
    {
      "epoch": 0.3417647058823529,
      "grad_norm": 0.04909919202327728,
      "learning_rate": 0.00013184094256259206,
      "loss": 0.3304,
      "step": 1162
    },
    {
      "epoch": 0.34205882352941175,
      "grad_norm": 0.0571029856801033,
      "learning_rate": 0.0001317820324005891,
      "loss": 0.3775,
      "step": 1163
    },
    {
      "epoch": 0.3423529411764706,
      "grad_norm": 0.0465298593044281,
      "learning_rate": 0.00013172312223858615,
      "loss": 0.3383,
      "step": 1164
    },
    {
      "epoch": 0.3426470588235294,
      "grad_norm": 0.046719081699848175,
      "learning_rate": 0.00013166421207658322,
      "loss": 0.3436,
      "step": 1165
    },
    {
      "epoch": 0.34294117647058825,
      "grad_norm": 0.038665104657411575,
      "learning_rate": 0.00013160530191458028,
      "loss": 0.2566,
      "step": 1166
    },
    {
      "epoch": 0.3432352941176471,
      "grad_norm": 0.05060292035341263,
      "learning_rate": 0.00013154639175257734,
      "loss": 0.3664,
      "step": 1167
    },
    {
      "epoch": 0.34352941176470586,
      "grad_norm": 0.04308950528502464,
      "learning_rate": 0.00013148748159057437,
      "loss": 0.3255,
      "step": 1168
    },
    {
      "epoch": 0.3438235294117647,
      "grad_norm": 0.045976053923368454,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.3549,
      "step": 1169
    },
    {
      "epoch": 0.34411764705882353,
      "grad_norm": 0.0441671758890152,
      "learning_rate": 0.0001313696612665685,
      "loss": 0.3246,
      "step": 1170
    },
    {
      "epoch": 0.34441176470588236,
      "grad_norm": 0.047823529690504074,
      "learning_rate": 0.00013131075110456555,
      "loss": 0.3289,
      "step": 1171
    },
    {
      "epoch": 0.3447058823529412,
      "grad_norm": 0.04683883488178253,
      "learning_rate": 0.0001312518409425626,
      "loss": 0.3269,
      "step": 1172
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.050591904670000076,
      "learning_rate": 0.00013119293078055964,
      "loss": 0.3581,
      "step": 1173
    },
    {
      "epoch": 0.3452941176470588,
      "grad_norm": 0.050069548189640045,
      "learning_rate": 0.0001311340206185567,
      "loss": 0.3422,
      "step": 1174
    },
    {
      "epoch": 0.34558823529411764,
      "grad_norm": 0.0477614589035511,
      "learning_rate": 0.00013107511045655376,
      "loss": 0.3696,
      "step": 1175
    },
    {
      "epoch": 0.3458823529411765,
      "grad_norm": 0.06405249238014221,
      "learning_rate": 0.00013101620029455082,
      "loss": 0.3718,
      "step": 1176
    },
    {
      "epoch": 0.3461764705882353,
      "grad_norm": 0.03844161331653595,
      "learning_rate": 0.00013095729013254788,
      "loss": 0.2802,
      "step": 1177
    },
    {
      "epoch": 0.34647058823529414,
      "grad_norm": 0.0393957756459713,
      "learning_rate": 0.00013089837997054492,
      "loss": 0.2905,
      "step": 1178
    },
    {
      "epoch": 0.3467647058823529,
      "grad_norm": 0.06549356132745743,
      "learning_rate": 0.00013083946980854198,
      "loss": 0.3622,
      "step": 1179
    },
    {
      "epoch": 0.34705882352941175,
      "grad_norm": 0.067227803170681,
      "learning_rate": 0.00013078055964653904,
      "loss": 0.3598,
      "step": 1180
    },
    {
      "epoch": 0.3473529411764706,
      "grad_norm": 0.050884101539850235,
      "learning_rate": 0.0001307216494845361,
      "loss": 0.3306,
      "step": 1181
    },
    {
      "epoch": 0.3476470588235294,
      "grad_norm": 0.054385289549827576,
      "learning_rate": 0.00013066273932253316,
      "loss": 0.4175,
      "step": 1182
    },
    {
      "epoch": 0.34794117647058825,
      "grad_norm": 0.04653196781873703,
      "learning_rate": 0.0001306038291605302,
      "loss": 0.3671,
      "step": 1183
    },
    {
      "epoch": 0.34823529411764703,
      "grad_norm": 0.053195320069789886,
      "learning_rate": 0.00013054491899852725,
      "loss": 0.3586,
      "step": 1184
    },
    {
      "epoch": 0.34852941176470587,
      "grad_norm": 0.05439235270023346,
      "learning_rate": 0.0001304860088365243,
      "loss": 0.377,
      "step": 1185
    },
    {
      "epoch": 0.3488235294117647,
      "grad_norm": 0.056144874542951584,
      "learning_rate": 0.00013042709867452137,
      "loss": 0.3729,
      "step": 1186
    },
    {
      "epoch": 0.34911764705882353,
      "grad_norm": 0.06505059450864792,
      "learning_rate": 0.00013036818851251843,
      "loss": 0.4422,
      "step": 1187
    },
    {
      "epoch": 0.34941176470588237,
      "grad_norm": 0.048418764024972916,
      "learning_rate": 0.00013030927835051546,
      "loss": 0.3321,
      "step": 1188
    },
    {
      "epoch": 0.3497058823529412,
      "grad_norm": 0.04322925582528114,
      "learning_rate": 0.00013025036818851252,
      "loss": 0.2927,
      "step": 1189
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06102242320775986,
      "learning_rate": 0.00013019145802650958,
      "loss": 0.3811,
      "step": 1190
    },
    {
      "epoch": 0.3502941176470588,
      "grad_norm": 0.04398060962557793,
      "learning_rate": 0.00013013254786450664,
      "loss": 0.3183,
      "step": 1191
    },
    {
      "epoch": 0.35058823529411764,
      "grad_norm": 0.05748866870999336,
      "learning_rate": 0.0001300736377025037,
      "loss": 0.3801,
      "step": 1192
    },
    {
      "epoch": 0.3508823529411765,
      "grad_norm": 0.06515807658433914,
      "learning_rate": 0.00013001472754050074,
      "loss": 0.4093,
      "step": 1193
    },
    {
      "epoch": 0.3511764705882353,
      "grad_norm": 0.053015001118183136,
      "learning_rate": 0.0001299558173784978,
      "loss": 0.3875,
      "step": 1194
    },
    {
      "epoch": 0.3514705882352941,
      "grad_norm": 0.04905546084046364,
      "learning_rate": 0.00012989690721649486,
      "loss": 0.3678,
      "step": 1195
    },
    {
      "epoch": 0.3517647058823529,
      "grad_norm": 0.059127405285835266,
      "learning_rate": 0.00012983799705449192,
      "loss": 0.3067,
      "step": 1196
    },
    {
      "epoch": 0.35205882352941176,
      "grad_norm": 0.047860611230134964,
      "learning_rate": 0.00012977908689248898,
      "loss": 0.3325,
      "step": 1197
    },
    {
      "epoch": 0.3523529411764706,
      "grad_norm": 0.053043536841869354,
      "learning_rate": 0.000129720176730486,
      "loss": 0.3653,
      "step": 1198
    },
    {
      "epoch": 0.3526470588235294,
      "grad_norm": 0.05277305468916893,
      "learning_rate": 0.00012966126656848307,
      "loss": 0.3189,
      "step": 1199
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.05675249546766281,
      "learning_rate": 0.00012960235640648013,
      "loss": 0.387,
      "step": 1200
    },
    {
      "epoch": 0.35323529411764704,
      "grad_norm": 0.05718355253338814,
      "learning_rate": 0.0001295434462444772,
      "loss": 0.3462,
      "step": 1201
    },
    {
      "epoch": 0.35352941176470587,
      "grad_norm": 0.05512603372335434,
      "learning_rate": 0.00012948453608247425,
      "loss": 0.3875,
      "step": 1202
    },
    {
      "epoch": 0.3538235294117647,
      "grad_norm": 0.05182097107172012,
      "learning_rate": 0.00012942562592047128,
      "loss": 0.3338,
      "step": 1203
    },
    {
      "epoch": 0.35411764705882354,
      "grad_norm": 0.057482410222291946,
      "learning_rate": 0.00012936671575846834,
      "loss": 0.3243,
      "step": 1204
    },
    {
      "epoch": 0.35441176470588237,
      "grad_norm": 0.038084618747234344,
      "learning_rate": 0.0001293078055964654,
      "loss": 0.2387,
      "step": 1205
    },
    {
      "epoch": 0.3547058823529412,
      "grad_norm": 0.05351848527789116,
      "learning_rate": 0.00012924889543446247,
      "loss": 0.3644,
      "step": 1206
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.04926329478621483,
      "learning_rate": 0.00012918998527245953,
      "loss": 0.3252,
      "step": 1207
    },
    {
      "epoch": 0.3552941176470588,
      "grad_norm": 0.05077064037322998,
      "learning_rate": 0.00012913107511045656,
      "loss": 0.367,
      "step": 1208
    },
    {
      "epoch": 0.35558823529411765,
      "grad_norm": 0.051585786044597626,
      "learning_rate": 0.00012907216494845362,
      "loss": 0.3343,
      "step": 1209
    },
    {
      "epoch": 0.3558823529411765,
      "grad_norm": 0.055382419377565384,
      "learning_rate": 0.00012901325478645068,
      "loss": 0.4434,
      "step": 1210
    },
    {
      "epoch": 0.3561764705882353,
      "grad_norm": 0.06065521761775017,
      "learning_rate": 0.00012895434462444774,
      "loss": 0.3631,
      "step": 1211
    },
    {
      "epoch": 0.3564705882352941,
      "grad_norm": 0.04082208499312401,
      "learning_rate": 0.0001288954344624448,
      "loss": 0.2923,
      "step": 1212
    },
    {
      "epoch": 0.35676470588235293,
      "grad_norm": 0.06574767082929611,
      "learning_rate": 0.00012883652430044183,
      "loss": 0.3859,
      "step": 1213
    },
    {
      "epoch": 0.35705882352941176,
      "grad_norm": 0.05407517030835152,
      "learning_rate": 0.0001287776141384389,
      "loss": 0.3507,
      "step": 1214
    },
    {
      "epoch": 0.3573529411764706,
      "grad_norm": 0.07608309388160706,
      "learning_rate": 0.00012871870397643595,
      "loss": 0.4222,
      "step": 1215
    },
    {
      "epoch": 0.35764705882352943,
      "grad_norm": 0.052025239914655685,
      "learning_rate": 0.000128659793814433,
      "loss": 0.3405,
      "step": 1216
    },
    {
      "epoch": 0.35794117647058826,
      "grad_norm": 0.04819073528051376,
      "learning_rate": 0.00012860088365243007,
      "loss": 0.3266,
      "step": 1217
    },
    {
      "epoch": 0.35823529411764704,
      "grad_norm": 0.0502382293343544,
      "learning_rate": 0.0001285419734904271,
      "loss": 0.3672,
      "step": 1218
    },
    {
      "epoch": 0.3585294117647059,
      "grad_norm": 0.06028193235397339,
      "learning_rate": 0.00012848306332842417,
      "loss": 0.3983,
      "step": 1219
    },
    {
      "epoch": 0.3588235294117647,
      "grad_norm": 0.05038086324930191,
      "learning_rate": 0.00012842415316642123,
      "loss": 0.3409,
      "step": 1220
    },
    {
      "epoch": 0.35911764705882354,
      "grad_norm": 0.05125132575631142,
      "learning_rate": 0.00012836524300441829,
      "loss": 0.4019,
      "step": 1221
    },
    {
      "epoch": 0.3594117647058824,
      "grad_norm": 0.04419030621647835,
      "learning_rate": 0.00012830633284241535,
      "loss": 0.3667,
      "step": 1222
    },
    {
      "epoch": 0.35970588235294115,
      "grad_norm": 0.057859521359205246,
      "learning_rate": 0.00012824742268041238,
      "loss": 0.3581,
      "step": 1223
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.04242267087101936,
      "learning_rate": 0.00012818851251840944,
      "loss": 0.3345,
      "step": 1224
    },
    {
      "epoch": 0.3602941176470588,
      "grad_norm": 0.0687156543135643,
      "learning_rate": 0.0001281296023564065,
      "loss": 0.3811,
      "step": 1225
    },
    {
      "epoch": 0.36058823529411765,
      "grad_norm": 0.040801309049129486,
      "learning_rate": 0.00012807069219440353,
      "loss": 0.2779,
      "step": 1226
    },
    {
      "epoch": 0.3608823529411765,
      "grad_norm": 0.05003528296947479,
      "learning_rate": 0.0001280117820324006,
      "loss": 0.3619,
      "step": 1227
    },
    {
      "epoch": 0.3611764705882353,
      "grad_norm": 0.05927104875445366,
      "learning_rate": 0.00012795287187039763,
      "loss": 0.3638,
      "step": 1228
    },
    {
      "epoch": 0.3614705882352941,
      "grad_norm": 0.04665049910545349,
      "learning_rate": 0.00012789396170839469,
      "loss": 0.3266,
      "step": 1229
    },
    {
      "epoch": 0.36176470588235293,
      "grad_norm": 0.053482670336961746,
      "learning_rate": 0.00012783505154639175,
      "loss": 0.3751,
      "step": 1230
    },
    {
      "epoch": 0.36205882352941177,
      "grad_norm": 0.042950551956892014,
      "learning_rate": 0.0001277761413843888,
      "loss": 0.3171,
      "step": 1231
    },
    {
      "epoch": 0.3623529411764706,
      "grad_norm": 0.03970734030008316,
      "learning_rate": 0.00012771723122238587,
      "loss": 0.3433,
      "step": 1232
    },
    {
      "epoch": 0.36264705882352943,
      "grad_norm": 0.04591294750571251,
      "learning_rate": 0.0001276583210603829,
      "loss": 0.2926,
      "step": 1233
    },
    {
      "epoch": 0.3629411764705882,
      "grad_norm": 0.06312287598848343,
      "learning_rate": 0.00012759941089837996,
      "loss": 0.4164,
      "step": 1234
    },
    {
      "epoch": 0.36323529411764705,
      "grad_norm": 0.042269881814718246,
      "learning_rate": 0.00012754050073637702,
      "loss": 0.3201,
      "step": 1235
    },
    {
      "epoch": 0.3635294117647059,
      "grad_norm": 0.04923565313220024,
      "learning_rate": 0.00012748159057437408,
      "loss": 0.4111,
      "step": 1236
    },
    {
      "epoch": 0.3638235294117647,
      "grad_norm": 0.05416258051991463,
      "learning_rate": 0.00012742268041237114,
      "loss": 0.3528,
      "step": 1237
    },
    {
      "epoch": 0.36411764705882355,
      "grad_norm": 0.041663143783807755,
      "learning_rate": 0.00012736377025036817,
      "loss": 0.2794,
      "step": 1238
    },
    {
      "epoch": 0.3644117647058824,
      "grad_norm": 0.05009074509143829,
      "learning_rate": 0.00012730486008836523,
      "loss": 0.3265,
      "step": 1239
    },
    {
      "epoch": 0.36470588235294116,
      "grad_norm": 0.050338003784418106,
      "learning_rate": 0.0001272459499263623,
      "loss": 0.3747,
      "step": 1240
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06102761998772621,
      "learning_rate": 0.00012718703976435935,
      "loss": 0.4242,
      "step": 1241
    },
    {
      "epoch": 0.3652941176470588,
      "grad_norm": 0.047139983624219894,
      "learning_rate": 0.00012712812960235641,
      "loss": 0.3892,
      "step": 1242
    },
    {
      "epoch": 0.36558823529411766,
      "grad_norm": 0.04694826155900955,
      "learning_rate": 0.00012706921944035345,
      "loss": 0.303,
      "step": 1243
    },
    {
      "epoch": 0.3658823529411765,
      "grad_norm": 0.06911609321832657,
      "learning_rate": 0.0001270103092783505,
      "loss": 0.3601,
      "step": 1244
    },
    {
      "epoch": 0.36617647058823527,
      "grad_norm": 0.062218792736530304,
      "learning_rate": 0.00012695139911634757,
      "loss": 0.3421,
      "step": 1245
    },
    {
      "epoch": 0.3664705882352941,
      "grad_norm": 0.05655054375529289,
      "learning_rate": 0.00012689248895434463,
      "loss": 0.3996,
      "step": 1246
    },
    {
      "epoch": 0.36676470588235294,
      "grad_norm": 0.07099892199039459,
      "learning_rate": 0.0001268335787923417,
      "loss": 0.4487,
      "step": 1247
    },
    {
      "epoch": 0.36705882352941177,
      "grad_norm": 0.07421479374170303,
      "learning_rate": 0.00012677466863033872,
      "loss": 0.4268,
      "step": 1248
    },
    {
      "epoch": 0.3673529411764706,
      "grad_norm": 0.06206342205405235,
      "learning_rate": 0.00012671575846833578,
      "loss": 0.374,
      "step": 1249
    },
    {
      "epoch": 0.36764705882352944,
      "grad_norm": 0.052368368953466415,
      "learning_rate": 0.00012665684830633284,
      "loss": 0.3717,
      "step": 1250
    },
    {
      "epoch": 0.3679411764705882,
      "grad_norm": 0.06473471224308014,
      "learning_rate": 0.0001265979381443299,
      "loss": 0.3914,
      "step": 1251
    },
    {
      "epoch": 0.36823529411764705,
      "grad_norm": 0.047993823885917664,
      "learning_rate": 0.00012653902798232696,
      "loss": 0.3249,
      "step": 1252
    },
    {
      "epoch": 0.3685294117647059,
      "grad_norm": 0.04610491916537285,
      "learning_rate": 0.000126480117820324,
      "loss": 0.2667,
      "step": 1253
    },
    {
      "epoch": 0.3688235294117647,
      "grad_norm": 0.05358102545142174,
      "learning_rate": 0.00012642120765832105,
      "loss": 0.3577,
      "step": 1254
    },
    {
      "epoch": 0.36911764705882355,
      "grad_norm": 0.05526606738567352,
      "learning_rate": 0.00012636229749631812,
      "loss": 0.3335,
      "step": 1255
    },
    {
      "epoch": 0.36941176470588233,
      "grad_norm": 0.04813884198665619,
      "learning_rate": 0.00012630338733431518,
      "loss": 0.3455,
      "step": 1256
    },
    {
      "epoch": 0.36970588235294116,
      "grad_norm": 0.05519367754459381,
      "learning_rate": 0.00012624447717231224,
      "loss": 0.3557,
      "step": 1257
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04282229393720627,
      "learning_rate": 0.00012618556701030927,
      "loss": 0.2827,
      "step": 1258
    },
    {
      "epoch": 0.37029411764705883,
      "grad_norm": 0.04396507889032364,
      "learning_rate": 0.00012612665684830633,
      "loss": 0.2994,
      "step": 1259
    },
    {
      "epoch": 0.37058823529411766,
      "grad_norm": 0.05511938035488129,
      "learning_rate": 0.0001260677466863034,
      "loss": 0.3551,
      "step": 1260
    },
    {
      "epoch": 0.3708823529411765,
      "grad_norm": 0.0768919438123703,
      "learning_rate": 0.00012600883652430045,
      "loss": 0.4302,
      "step": 1261
    },
    {
      "epoch": 0.3711764705882353,
      "grad_norm": 0.07389368861913681,
      "learning_rate": 0.0001259499263622975,
      "loss": 0.3785,
      "step": 1262
    },
    {
      "epoch": 0.3714705882352941,
      "grad_norm": 0.04722702130675316,
      "learning_rate": 0.00012589101620029454,
      "loss": 0.335,
      "step": 1263
    },
    {
      "epoch": 0.37176470588235294,
      "grad_norm": 0.06630875170230865,
      "learning_rate": 0.0001258321060382916,
      "loss": 0.348,
      "step": 1264
    },
    {
      "epoch": 0.3720588235294118,
      "grad_norm": 0.04876803606748581,
      "learning_rate": 0.00012577319587628866,
      "loss": 0.3731,
      "step": 1265
    },
    {
      "epoch": 0.3723529411764706,
      "grad_norm": 0.05568288639187813,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.3978,
      "step": 1266
    },
    {
      "epoch": 0.3726470588235294,
      "grad_norm": 0.06753812730312347,
      "learning_rate": 0.00012565537555228278,
      "loss": 0.4195,
      "step": 1267
    },
    {
      "epoch": 0.3729411764705882,
      "grad_norm": 0.04759494587779045,
      "learning_rate": 0.00012559646539027982,
      "loss": 0.3162,
      "step": 1268
    },
    {
      "epoch": 0.37323529411764705,
      "grad_norm": 0.049286358058452606,
      "learning_rate": 0.00012553755522827688,
      "loss": 0.3572,
      "step": 1269
    },
    {
      "epoch": 0.3735294117647059,
      "grad_norm": 0.05771895498037338,
      "learning_rate": 0.00012547864506627394,
      "loss": 0.3607,
      "step": 1270
    },
    {
      "epoch": 0.3738235294117647,
      "grad_norm": 0.055867891758680344,
      "learning_rate": 0.000125419734904271,
      "loss": 0.3535,
      "step": 1271
    },
    {
      "epoch": 0.37411764705882355,
      "grad_norm": 0.05273571237921715,
      "learning_rate": 0.00012536082474226806,
      "loss": 0.3678,
      "step": 1272
    },
    {
      "epoch": 0.37441176470588233,
      "grad_norm": 0.04210754483938217,
      "learning_rate": 0.0001253019145802651,
      "loss": 0.3302,
      "step": 1273
    },
    {
      "epoch": 0.37470588235294117,
      "grad_norm": 0.04759346321225166,
      "learning_rate": 0.00012524300441826215,
      "loss": 0.3719,
      "step": 1274
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.044636648148298264,
      "learning_rate": 0.0001251840942562592,
      "loss": 0.3158,
      "step": 1275
    },
    {
      "epoch": 0.37529411764705883,
      "grad_norm": 0.060443103313446045,
      "learning_rate": 0.00012512518409425627,
      "loss": 0.4525,
      "step": 1276
    },
    {
      "epoch": 0.37558823529411767,
      "grad_norm": 0.04425424709916115,
      "learning_rate": 0.00012506627393225333,
      "loss": 0.3295,
      "step": 1277
    },
    {
      "epoch": 0.37588235294117645,
      "grad_norm": 0.04747970402240753,
      "learning_rate": 0.00012500736377025036,
      "loss": 0.3899,
      "step": 1278
    },
    {
      "epoch": 0.3761764705882353,
      "grad_norm": 0.04468274116516113,
      "learning_rate": 0.00012494845360824742,
      "loss": 0.3071,
      "step": 1279
    },
    {
      "epoch": 0.3764705882352941,
      "grad_norm": 0.05490390583872795,
      "learning_rate": 0.00012488954344624448,
      "loss": 0.3782,
      "step": 1280
    },
    {
      "epoch": 0.37676470588235295,
      "grad_norm": 0.05540602281689644,
      "learning_rate": 0.00012483063328424154,
      "loss": 0.3591,
      "step": 1281
    },
    {
      "epoch": 0.3770588235294118,
      "grad_norm": 0.04729980230331421,
      "learning_rate": 0.0001247717231222386,
      "loss": 0.3363,
      "step": 1282
    },
    {
      "epoch": 0.3773529411764706,
      "grad_norm": 0.04645152390003204,
      "learning_rate": 0.00012471281296023564,
      "loss": 0.3546,
      "step": 1283
    },
    {
      "epoch": 0.3776470588235294,
      "grad_norm": 0.05511489510536194,
      "learning_rate": 0.0001246539027982327,
      "loss": 0.4294,
      "step": 1284
    },
    {
      "epoch": 0.3779411764705882,
      "grad_norm": 0.04356254264712334,
      "learning_rate": 0.00012459499263622976,
      "loss": 0.3119,
      "step": 1285
    },
    {
      "epoch": 0.37823529411764706,
      "grad_norm": 0.064662866294384,
      "learning_rate": 0.00012453608247422682,
      "loss": 0.3804,
      "step": 1286
    },
    {
      "epoch": 0.3785294117647059,
      "grad_norm": 0.03797849267721176,
      "learning_rate": 0.00012447717231222388,
      "loss": 0.2851,
      "step": 1287
    },
    {
      "epoch": 0.3788235294117647,
      "grad_norm": 0.059722378849983215,
      "learning_rate": 0.0001244182621502209,
      "loss": 0.3578,
      "step": 1288
    },
    {
      "epoch": 0.3791176470588235,
      "grad_norm": 0.05306950956583023,
      "learning_rate": 0.00012435935198821797,
      "loss": 0.3818,
      "step": 1289
    },
    {
      "epoch": 0.37941176470588234,
      "grad_norm": 0.0426027774810791,
      "learning_rate": 0.00012430044182621503,
      "loss": 0.3543,
      "step": 1290
    },
    {
      "epoch": 0.37970588235294117,
      "grad_norm": 0.04577934741973877,
      "learning_rate": 0.0001242415316642121,
      "loss": 0.3139,
      "step": 1291
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.050159208476543427,
      "learning_rate": 0.00012418262150220915,
      "loss": 0.3748,
      "step": 1292
    },
    {
      "epoch": 0.38029411764705884,
      "grad_norm": 0.058271512389183044,
      "learning_rate": 0.00012412371134020618,
      "loss": 0.4219,
      "step": 1293
    },
    {
      "epoch": 0.38058823529411767,
      "grad_norm": 0.04483126848936081,
      "learning_rate": 0.00012406480117820324,
      "loss": 0.324,
      "step": 1294
    },
    {
      "epoch": 0.38088235294117645,
      "grad_norm": 0.04657711461186409,
      "learning_rate": 0.0001240058910162003,
      "loss": 0.3071,
      "step": 1295
    },
    {
      "epoch": 0.3811764705882353,
      "grad_norm": 0.03605169057846069,
      "learning_rate": 0.00012394698085419736,
      "loss": 0.2929,
      "step": 1296
    },
    {
      "epoch": 0.3814705882352941,
      "grad_norm": 0.042276110500097275,
      "learning_rate": 0.00012388807069219443,
      "loss": 0.3455,
      "step": 1297
    },
    {
      "epoch": 0.38176470588235295,
      "grad_norm": 0.0625295639038086,
      "learning_rate": 0.00012382916053019146,
      "loss": 0.3889,
      "step": 1298
    },
    {
      "epoch": 0.3820588235294118,
      "grad_norm": 0.03907632455229759,
      "learning_rate": 0.00012377025036818852,
      "loss": 0.3408,
      "step": 1299
    },
    {
      "epoch": 0.38235294117647056,
      "grad_norm": 0.041001755744218826,
      "learning_rate": 0.00012371134020618558,
      "loss": 0.3384,
      "step": 1300
    },
    {
      "epoch": 0.3826470588235294,
      "grad_norm": 0.053751252591609955,
      "learning_rate": 0.00012365243004418264,
      "loss": 0.3946,
      "step": 1301
    },
    {
      "epoch": 0.38294117647058823,
      "grad_norm": 0.054638054221868515,
      "learning_rate": 0.0001235935198821797,
      "loss": 0.3628,
      "step": 1302
    },
    {
      "epoch": 0.38323529411764706,
      "grad_norm": 0.05607336387038231,
      "learning_rate": 0.00012353460972017673,
      "loss": 0.344,
      "step": 1303
    },
    {
      "epoch": 0.3835294117647059,
      "grad_norm": 0.053780119866132736,
      "learning_rate": 0.0001234756995581738,
      "loss": 0.3947,
      "step": 1304
    },
    {
      "epoch": 0.38382352941176473,
      "grad_norm": 0.06745556741952896,
      "learning_rate": 0.00012341678939617085,
      "loss": 0.4397,
      "step": 1305
    },
    {
      "epoch": 0.3841176470588235,
      "grad_norm": 0.061755623668432236,
      "learning_rate": 0.0001233578792341679,
      "loss": 0.3992,
      "step": 1306
    },
    {
      "epoch": 0.38441176470588234,
      "grad_norm": 0.03516651317477226,
      "learning_rate": 0.00012329896907216497,
      "loss": 0.2784,
      "step": 1307
    },
    {
      "epoch": 0.3847058823529412,
      "grad_norm": 0.039529815316200256,
      "learning_rate": 0.000123240058910162,
      "loss": 0.2914,
      "step": 1308
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.058236125856637955,
      "learning_rate": 0.00012318114874815907,
      "loss": 0.3438,
      "step": 1309
    },
    {
      "epoch": 0.38529411764705884,
      "grad_norm": 0.04956669360399246,
      "learning_rate": 0.00012312223858615613,
      "loss": 0.3844,
      "step": 1310
    },
    {
      "epoch": 0.3855882352941176,
      "grad_norm": 0.03252106159925461,
      "learning_rate": 0.00012306332842415319,
      "loss": 0.258,
      "step": 1311
    },
    {
      "epoch": 0.38588235294117645,
      "grad_norm": 0.05745253339409828,
      "learning_rate": 0.00012300441826215025,
      "loss": 0.3595,
      "step": 1312
    },
    {
      "epoch": 0.3861764705882353,
      "grad_norm": 0.046317920088768005,
      "learning_rate": 0.00012294550810014728,
      "loss": 0.3206,
      "step": 1313
    },
    {
      "epoch": 0.3864705882352941,
      "grad_norm": 0.04663233831524849,
      "learning_rate": 0.00012288659793814434,
      "loss": 0.3575,
      "step": 1314
    },
    {
      "epoch": 0.38676470588235295,
      "grad_norm": 0.05239938199520111,
      "learning_rate": 0.0001228276877761414,
      "loss": 0.3665,
      "step": 1315
    },
    {
      "epoch": 0.3870588235294118,
      "grad_norm": 0.047448012977838516,
      "learning_rate": 0.00012276877761413846,
      "loss": 0.3681,
      "step": 1316
    },
    {
      "epoch": 0.38735294117647057,
      "grad_norm": 0.05752386897802353,
      "learning_rate": 0.00012270986745213552,
      "loss": 0.4177,
      "step": 1317
    },
    {
      "epoch": 0.3876470588235294,
      "grad_norm": 0.04953908175230026,
      "learning_rate": 0.00012265095729013255,
      "loss": 0.3596,
      "step": 1318
    },
    {
      "epoch": 0.38794117647058823,
      "grad_norm": 0.047495074570178986,
      "learning_rate": 0.0001225920471281296,
      "loss": 0.3751,
      "step": 1319
    },
    {
      "epoch": 0.38823529411764707,
      "grad_norm": 0.05075526610016823,
      "learning_rate": 0.00012253313696612667,
      "loss": 0.3756,
      "step": 1320
    },
    {
      "epoch": 0.3885294117647059,
      "grad_norm": 0.055516067892313004,
      "learning_rate": 0.00012247422680412373,
      "loss": 0.3861,
      "step": 1321
    },
    {
      "epoch": 0.3888235294117647,
      "grad_norm": 0.06265439838171005,
      "learning_rate": 0.0001224153166421208,
      "loss": 0.3668,
      "step": 1322
    },
    {
      "epoch": 0.3891176470588235,
      "grad_norm": 0.04859693720936775,
      "learning_rate": 0.00012235640648011783,
      "loss": 0.3575,
      "step": 1323
    },
    {
      "epoch": 0.38941176470588235,
      "grad_norm": 0.0557103231549263,
      "learning_rate": 0.0001222974963181149,
      "loss": 0.363,
      "step": 1324
    },
    {
      "epoch": 0.3897058823529412,
      "grad_norm": 0.06495865434408188,
      "learning_rate": 0.00012223858615611195,
      "loss": 0.3823,
      "step": 1325
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.043825916945934296,
      "learning_rate": 0.000122179675994109,
      "loss": 0.3139,
      "step": 1326
    },
    {
      "epoch": 0.39029411764705885,
      "grad_norm": 0.0545281358063221,
      "learning_rate": 0.00012212076583210607,
      "loss": 0.4027,
      "step": 1327
    },
    {
      "epoch": 0.3905882352941176,
      "grad_norm": 0.04920879378914833,
      "learning_rate": 0.00012206185567010311,
      "loss": 0.3857,
      "step": 1328
    },
    {
      "epoch": 0.39088235294117646,
      "grad_norm": 0.050975970923900604,
      "learning_rate": 0.00012200294550810016,
      "loss": 0.3449,
      "step": 1329
    },
    {
      "epoch": 0.3911764705882353,
      "grad_norm": 0.07928720116615295,
      "learning_rate": 0.00012194403534609722,
      "loss": 0.4392,
      "step": 1330
    },
    {
      "epoch": 0.3914705882352941,
      "grad_norm": 0.04282214120030403,
      "learning_rate": 0.00012188512518409427,
      "loss": 0.3444,
      "step": 1331
    },
    {
      "epoch": 0.39176470588235296,
      "grad_norm": 0.05043970048427582,
      "learning_rate": 0.0001218262150220913,
      "loss": 0.3694,
      "step": 1332
    },
    {
      "epoch": 0.39205882352941174,
      "grad_norm": 0.051021769642829895,
      "learning_rate": 0.00012176730486008836,
      "loss": 0.3612,
      "step": 1333
    },
    {
      "epoch": 0.39235294117647057,
      "grad_norm": 0.049550630152225494,
      "learning_rate": 0.00012170839469808542,
      "loss": 0.3859,
      "step": 1334
    },
    {
      "epoch": 0.3926470588235294,
      "grad_norm": 0.051163844764232635,
      "learning_rate": 0.00012164948453608247,
      "loss": 0.4093,
      "step": 1335
    },
    {
      "epoch": 0.39294117647058824,
      "grad_norm": 0.0495951883494854,
      "learning_rate": 0.00012159057437407953,
      "loss": 0.3864,
      "step": 1336
    },
    {
      "epoch": 0.39323529411764707,
      "grad_norm": 0.04082044959068298,
      "learning_rate": 0.00012153166421207657,
      "loss": 0.3191,
      "step": 1337
    },
    {
      "epoch": 0.3935294117647059,
      "grad_norm": 0.06964438408613205,
      "learning_rate": 0.00012147275405007363,
      "loss": 0.4417,
      "step": 1338
    },
    {
      "epoch": 0.3938235294117647,
      "grad_norm": 0.06520950794219971,
      "learning_rate": 0.0001214138438880707,
      "loss": 0.4494,
      "step": 1339
    },
    {
      "epoch": 0.3941176470588235,
      "grad_norm": 0.04573240876197815,
      "learning_rate": 0.00012135493372606774,
      "loss": 0.33,
      "step": 1340
    },
    {
      "epoch": 0.39441176470588235,
      "grad_norm": 0.03914204239845276,
      "learning_rate": 0.0001212960235640648,
      "loss": 0.2753,
      "step": 1341
    },
    {
      "epoch": 0.3947058823529412,
      "grad_norm": 0.06401056051254272,
      "learning_rate": 0.00012123711340206185,
      "loss": 0.3679,
      "step": 1342
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.049247968941926956,
      "learning_rate": 0.00012117820324005891,
      "loss": 0.3236,
      "step": 1343
    },
    {
      "epoch": 0.3952941176470588,
      "grad_norm": 0.058798640966415405,
      "learning_rate": 0.00012111929307805597,
      "loss": 0.3597,
      "step": 1344
    },
    {
      "epoch": 0.39558823529411763,
      "grad_norm": 0.06224774941802025,
      "learning_rate": 0.00012106038291605301,
      "loss": 0.3943,
      "step": 1345
    },
    {
      "epoch": 0.39588235294117646,
      "grad_norm": 0.03800506889820099,
      "learning_rate": 0.00012100147275405008,
      "loss": 0.2893,
      "step": 1346
    },
    {
      "epoch": 0.3961764705882353,
      "grad_norm": 0.06172817572951317,
      "learning_rate": 0.00012094256259204712,
      "loss": 0.4119,
      "step": 1347
    },
    {
      "epoch": 0.39647058823529413,
      "grad_norm": 0.0794820785522461,
      "learning_rate": 0.00012088365243004418,
      "loss": 0.3972,
      "step": 1348
    },
    {
      "epoch": 0.39676470588235296,
      "grad_norm": 0.050745267421007156,
      "learning_rate": 0.00012082474226804124,
      "loss": 0.3603,
      "step": 1349
    },
    {
      "epoch": 0.39705882352941174,
      "grad_norm": 0.046682968735694885,
      "learning_rate": 0.00012076583210603829,
      "loss": 0.4045,
      "step": 1350
    },
    {
      "epoch": 0.3973529411764706,
      "grad_norm": 0.04742817580699921,
      "learning_rate": 0.00012070692194403535,
      "loss": 0.3202,
      "step": 1351
    },
    {
      "epoch": 0.3976470588235294,
      "grad_norm": 0.042382266372442245,
      "learning_rate": 0.0001206480117820324,
      "loss": 0.3541,
      "step": 1352
    },
    {
      "epoch": 0.39794117647058824,
      "grad_norm": 0.04189004376530647,
      "learning_rate": 0.00012058910162002946,
      "loss": 0.2823,
      "step": 1353
    },
    {
      "epoch": 0.3982352941176471,
      "grad_norm": 0.042059335857629776,
      "learning_rate": 0.00012053019145802652,
      "loss": 0.3408,
      "step": 1354
    },
    {
      "epoch": 0.3985294117647059,
      "grad_norm": 0.04467776045203209,
      "learning_rate": 0.00012047128129602356,
      "loss": 0.2704,
      "step": 1355
    },
    {
      "epoch": 0.3988235294117647,
      "grad_norm": 0.05947007238864899,
      "learning_rate": 0.00012041237113402062,
      "loss": 0.4138,
      "step": 1356
    },
    {
      "epoch": 0.3991176470588235,
      "grad_norm": 0.05069107562303543,
      "learning_rate": 0.00012035346097201767,
      "loss": 0.291,
      "step": 1357
    },
    {
      "epoch": 0.39941176470588236,
      "grad_norm": 0.051622193306684494,
      "learning_rate": 0.00012029455081001473,
      "loss": 0.3358,
      "step": 1358
    },
    {
      "epoch": 0.3997058823529412,
      "grad_norm": 0.04652887582778931,
      "learning_rate": 0.00012023564064801178,
      "loss": 0.3386,
      "step": 1359
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.05218986049294472,
      "learning_rate": 0.00012017673048600884,
      "loss": 0.3683,
      "step": 1360
    },
    {
      "epoch": 0.4002941176470588,
      "grad_norm": 0.04599303752183914,
      "learning_rate": 0.0001201178203240059,
      "loss": 0.3488,
      "step": 1361
    },
    {
      "epoch": 0.40058823529411763,
      "grad_norm": 0.051949143409729004,
      "learning_rate": 0.00012005891016200294,
      "loss": 0.4233,
      "step": 1362
    },
    {
      "epoch": 0.40088235294117647,
      "grad_norm": 0.06342252343893051,
      "learning_rate": 0.00012,
      "loss": 0.4162,
      "step": 1363
    },
    {
      "epoch": 0.4011764705882353,
      "grad_norm": 0.04974079132080078,
      "learning_rate": 0.00011994108983799705,
      "loss": 0.2892,
      "step": 1364
    },
    {
      "epoch": 0.40147058823529413,
      "grad_norm": 0.054079294204711914,
      "learning_rate": 0.00011988217967599411,
      "loss": 0.3676,
      "step": 1365
    },
    {
      "epoch": 0.40176470588235297,
      "grad_norm": 0.05098779499530792,
      "learning_rate": 0.00011982326951399117,
      "loss": 0.3998,
      "step": 1366
    },
    {
      "epoch": 0.40205882352941175,
      "grad_norm": 0.04928348958492279,
      "learning_rate": 0.00011976435935198822,
      "loss": 0.3154,
      "step": 1367
    },
    {
      "epoch": 0.4023529411764706,
      "grad_norm": 0.04938960820436478,
      "learning_rate": 0.00011970544918998528,
      "loss": 0.3316,
      "step": 1368
    },
    {
      "epoch": 0.4026470588235294,
      "grad_norm": 0.0444144569337368,
      "learning_rate": 0.00011964653902798232,
      "loss": 0.324,
      "step": 1369
    },
    {
      "epoch": 0.40294117647058825,
      "grad_norm": 0.05739104747772217,
      "learning_rate": 0.00011958762886597938,
      "loss": 0.398,
      "step": 1370
    },
    {
      "epoch": 0.4032352941176471,
      "grad_norm": 0.041217319667339325,
      "learning_rate": 0.00011952871870397644,
      "loss": 0.3219,
      "step": 1371
    },
    {
      "epoch": 0.40352941176470586,
      "grad_norm": 0.03784308210015297,
      "learning_rate": 0.00011946980854197349,
      "loss": 0.3319,
      "step": 1372
    },
    {
      "epoch": 0.4038235294117647,
      "grad_norm": 0.06552661210298538,
      "learning_rate": 0.00011941089837997055,
      "loss": 0.4132,
      "step": 1373
    },
    {
      "epoch": 0.4041176470588235,
      "grad_norm": 0.06006975099444389,
      "learning_rate": 0.0001193519882179676,
      "loss": 0.3654,
      "step": 1374
    },
    {
      "epoch": 0.40441176470588236,
      "grad_norm": 0.06632746756076813,
      "learning_rate": 0.00011929307805596466,
      "loss": 0.4114,
      "step": 1375
    },
    {
      "epoch": 0.4047058823529412,
      "grad_norm": 0.053175587207078934,
      "learning_rate": 0.00011923416789396172,
      "loss": 0.3593,
      "step": 1376
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.0450337752699852,
      "learning_rate": 0.00011917525773195876,
      "loss": 0.3261,
      "step": 1377
    },
    {
      "epoch": 0.4052941176470588,
      "grad_norm": 0.04300384223461151,
      "learning_rate": 0.00011911634756995582,
      "loss": 0.3361,
      "step": 1378
    },
    {
      "epoch": 0.40558823529411764,
      "grad_norm": 0.043306466192007065,
      "learning_rate": 0.00011905743740795287,
      "loss": 0.316,
      "step": 1379
    },
    {
      "epoch": 0.40588235294117647,
      "grad_norm": 0.06758464127779007,
      "learning_rate": 0.00011899852724594993,
      "loss": 0.4155,
      "step": 1380
    },
    {
      "epoch": 0.4061764705882353,
      "grad_norm": 0.0488818921148777,
      "learning_rate": 0.00011893961708394699,
      "loss": 0.3613,
      "step": 1381
    },
    {
      "epoch": 0.40647058823529414,
      "grad_norm": 0.04967258870601654,
      "learning_rate": 0.00011888070692194404,
      "loss": 0.3739,
      "step": 1382
    },
    {
      "epoch": 0.4067647058823529,
      "grad_norm": 0.04391469061374664,
      "learning_rate": 0.0001188217967599411,
      "loss": 0.3287,
      "step": 1383
    },
    {
      "epoch": 0.40705882352941175,
      "grad_norm": 0.06117546185851097,
      "learning_rate": 0.00011876288659793814,
      "loss": 0.4001,
      "step": 1384
    },
    {
      "epoch": 0.4073529411764706,
      "grad_norm": 0.05095063894987106,
      "learning_rate": 0.0001187039764359352,
      "loss": 0.3601,
      "step": 1385
    },
    {
      "epoch": 0.4076470588235294,
      "grad_norm": 0.04802936315536499,
      "learning_rate": 0.00011864506627393226,
      "loss": 0.3265,
      "step": 1386
    },
    {
      "epoch": 0.40794117647058825,
      "grad_norm": 0.04463087394833565,
      "learning_rate": 0.00011858615611192931,
      "loss": 0.3093,
      "step": 1387
    },
    {
      "epoch": 0.4082352941176471,
      "grad_norm": 0.055778104811906815,
      "learning_rate": 0.00011852724594992637,
      "loss": 0.3636,
      "step": 1388
    },
    {
      "epoch": 0.40852941176470586,
      "grad_norm": 0.044072072952985764,
      "learning_rate": 0.00011846833578792342,
      "loss": 0.3297,
      "step": 1389
    },
    {
      "epoch": 0.4088235294117647,
      "grad_norm": 0.051623936742544174,
      "learning_rate": 0.00011840942562592048,
      "loss": 0.3949,
      "step": 1390
    },
    {
      "epoch": 0.40911764705882353,
      "grad_norm": 0.05663737654685974,
      "learning_rate": 0.00011835051546391754,
      "loss": 0.369,
      "step": 1391
    },
    {
      "epoch": 0.40941176470588236,
      "grad_norm": 0.05134459584951401,
      "learning_rate": 0.00011829160530191459,
      "loss": 0.3253,
      "step": 1392
    },
    {
      "epoch": 0.4097058823529412,
      "grad_norm": 0.05714389681816101,
      "learning_rate": 0.00011823269513991165,
      "loss": 0.4311,
      "step": 1393
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.04258742555975914,
      "learning_rate": 0.00011817378497790869,
      "loss": 0.3503,
      "step": 1394
    },
    {
      "epoch": 0.4102941176470588,
      "grad_norm": 0.056830912828445435,
      "learning_rate": 0.00011811487481590575,
      "loss": 0.3877,
      "step": 1395
    },
    {
      "epoch": 0.41058823529411764,
      "grad_norm": 0.05259817838668823,
      "learning_rate": 0.00011805596465390281,
      "loss": 0.3977,
      "step": 1396
    },
    {
      "epoch": 0.4108823529411765,
      "grad_norm": 0.050232723355293274,
      "learning_rate": 0.00011799705449189986,
      "loss": 0.3841,
      "step": 1397
    },
    {
      "epoch": 0.4111764705882353,
      "grad_norm": 0.04241369664669037,
      "learning_rate": 0.00011793814432989692,
      "loss": 0.3039,
      "step": 1398
    },
    {
      "epoch": 0.41147058823529414,
      "grad_norm": 0.03655943274497986,
      "learning_rate": 0.00011787923416789397,
      "loss": 0.2639,
      "step": 1399
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.03909563273191452,
      "learning_rate": 0.00011782032400589103,
      "loss": 0.274,
      "step": 1400
    },
    {
      "epoch": 0.41205882352941176,
      "grad_norm": 0.07460065931081772,
      "learning_rate": 0.00011776141384388809,
      "loss": 0.3828,
      "step": 1401
    },
    {
      "epoch": 0.4123529411764706,
      "grad_norm": 0.04392150044441223,
      "learning_rate": 0.00011770250368188513,
      "loss": 0.3196,
      "step": 1402
    },
    {
      "epoch": 0.4126470588235294,
      "grad_norm": 0.053774643689394,
      "learning_rate": 0.00011764359351988219,
      "loss": 0.3753,
      "step": 1403
    },
    {
      "epoch": 0.41294117647058826,
      "grad_norm": 0.05775910243391991,
      "learning_rate": 0.00011758468335787924,
      "loss": 0.3431,
      "step": 1404
    },
    {
      "epoch": 0.41323529411764703,
      "grad_norm": 0.057249050587415695,
      "learning_rate": 0.0001175257731958763,
      "loss": 0.323,
      "step": 1405
    },
    {
      "epoch": 0.41352941176470587,
      "grad_norm": 0.05422135815024376,
      "learning_rate": 0.00011746686303387336,
      "loss": 0.3726,
      "step": 1406
    },
    {
      "epoch": 0.4138235294117647,
      "grad_norm": 0.068003349006176,
      "learning_rate": 0.0001174079528718704,
      "loss": 0.4008,
      "step": 1407
    },
    {
      "epoch": 0.41411764705882353,
      "grad_norm": 0.06354010105133057,
      "learning_rate": 0.00011734904270986747,
      "loss": 0.3895,
      "step": 1408
    },
    {
      "epoch": 0.41441176470588237,
      "grad_norm": 0.048743490129709244,
      "learning_rate": 0.00011729013254786451,
      "loss": 0.3584,
      "step": 1409
    },
    {
      "epoch": 0.4147058823529412,
      "grad_norm": 0.03658454492688179,
      "learning_rate": 0.00011723122238586157,
      "loss": 0.3123,
      "step": 1410
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.04793798550963402,
      "learning_rate": 0.00011717231222385863,
      "loss": 0.3576,
      "step": 1411
    },
    {
      "epoch": 0.4152941176470588,
      "grad_norm": 0.03904734551906586,
      "learning_rate": 0.00011711340206185568,
      "loss": 0.2857,
      "step": 1412
    },
    {
      "epoch": 0.41558823529411765,
      "grad_norm": 0.052268434315919876,
      "learning_rate": 0.00011705449189985274,
      "loss": 0.3849,
      "step": 1413
    },
    {
      "epoch": 0.4158823529411765,
      "grad_norm": 0.05369556322693825,
      "learning_rate": 0.00011699558173784979,
      "loss": 0.4046,
      "step": 1414
    },
    {
      "epoch": 0.4161764705882353,
      "grad_norm": 0.04974417760968208,
      "learning_rate": 0.00011693667157584685,
      "loss": 0.3595,
      "step": 1415
    },
    {
      "epoch": 0.4164705882352941,
      "grad_norm": 0.05012394115328789,
      "learning_rate": 0.0001168777614138439,
      "loss": 0.3995,
      "step": 1416
    },
    {
      "epoch": 0.4167647058823529,
      "grad_norm": 0.055952947586774826,
      "learning_rate": 0.00011681885125184095,
      "loss": 0.3384,
      "step": 1417
    },
    {
      "epoch": 0.41705882352941176,
      "grad_norm": 0.04726039990782738,
      "learning_rate": 0.00011675994108983801,
      "loss": 0.3713,
      "step": 1418
    },
    {
      "epoch": 0.4173529411764706,
      "grad_norm": 0.048330146819353104,
      "learning_rate": 0.00011670103092783506,
      "loss": 0.3836,
      "step": 1419
    },
    {
      "epoch": 0.4176470588235294,
      "grad_norm": 0.04266269877552986,
      "learning_rate": 0.00011664212076583212,
      "loss": 0.3235,
      "step": 1420
    },
    {
      "epoch": 0.41794117647058826,
      "grad_norm": 0.05757750943303108,
      "learning_rate": 0.00011658321060382917,
      "loss": 0.4101,
      "step": 1421
    },
    {
      "epoch": 0.41823529411764704,
      "grad_norm": 0.045642271637916565,
      "learning_rate": 0.00011652430044182623,
      "loss": 0.3659,
      "step": 1422
    },
    {
      "epoch": 0.41852941176470587,
      "grad_norm": 0.04070141166448593,
      "learning_rate": 0.00011646539027982329,
      "loss": 0.3492,
      "step": 1423
    },
    {
      "epoch": 0.4188235294117647,
      "grad_norm": 0.042854733765125275,
      "learning_rate": 0.00011640648011782033,
      "loss": 0.2532,
      "step": 1424
    },
    {
      "epoch": 0.41911764705882354,
      "grad_norm": 0.051709409803152084,
      "learning_rate": 0.0001163475699558174,
      "loss": 0.4325,
      "step": 1425
    },
    {
      "epoch": 0.4194117647058824,
      "grad_norm": 0.05406966432929039,
      "learning_rate": 0.00011628865979381444,
      "loss": 0.4008,
      "step": 1426
    },
    {
      "epoch": 0.41970588235294115,
      "grad_norm": 0.04792850837111473,
      "learning_rate": 0.0001162297496318115,
      "loss": 0.3322,
      "step": 1427
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.04542122036218643,
      "learning_rate": 0.00011617083946980856,
      "loss": 0.3124,
      "step": 1428
    },
    {
      "epoch": 0.4202941176470588,
      "grad_norm": 0.06228867545723915,
      "learning_rate": 0.00011611192930780561,
      "loss": 0.3382,
      "step": 1429
    },
    {
      "epoch": 0.42058823529411765,
      "grad_norm": 0.04599594324827194,
      "learning_rate": 0.00011605301914580267,
      "loss": 0.3762,
      "step": 1430
    },
    {
      "epoch": 0.4208823529411765,
      "grad_norm": 0.044481392949819565,
      "learning_rate": 0.00011599410898379971,
      "loss": 0.3516,
      "step": 1431
    },
    {
      "epoch": 0.4211764705882353,
      "grad_norm": 0.043788935989141464,
      "learning_rate": 0.00011593519882179677,
      "loss": 0.2687,
      "step": 1432
    },
    {
      "epoch": 0.4214705882352941,
      "grad_norm": 0.05554361641407013,
      "learning_rate": 0.00011587628865979384,
      "loss": 0.3385,
      "step": 1433
    },
    {
      "epoch": 0.42176470588235293,
      "grad_norm": 0.04629557952284813,
      "learning_rate": 0.00011581737849779088,
      "loss": 0.3375,
      "step": 1434
    },
    {
      "epoch": 0.42205882352941176,
      "grad_norm": 0.0548151396214962,
      "learning_rate": 0.00011575846833578794,
      "loss": 0.3856,
      "step": 1435
    },
    {
      "epoch": 0.4223529411764706,
      "grad_norm": 0.05201254412531853,
      "learning_rate": 0.00011569955817378499,
      "loss": 0.3599,
      "step": 1436
    },
    {
      "epoch": 0.42264705882352943,
      "grad_norm": 0.049818918108940125,
      "learning_rate": 0.00011564064801178205,
      "loss": 0.2902,
      "step": 1437
    },
    {
      "epoch": 0.4229411764705882,
      "grad_norm": 0.04683271795511246,
      "learning_rate": 0.00011558173784977908,
      "loss": 0.3647,
      "step": 1438
    },
    {
      "epoch": 0.42323529411764704,
      "grad_norm": 0.04776962101459503,
      "learning_rate": 0.00011552282768777614,
      "loss": 0.3513,
      "step": 1439
    },
    {
      "epoch": 0.4235294117647059,
      "grad_norm": 0.04836948588490486,
      "learning_rate": 0.00011546391752577319,
      "loss": 0.3291,
      "step": 1440
    },
    {
      "epoch": 0.4238235294117647,
      "grad_norm": 0.04423702135682106,
      "learning_rate": 0.00011540500736377025,
      "loss": 0.358,
      "step": 1441
    },
    {
      "epoch": 0.42411764705882354,
      "grad_norm": 0.03998124971985817,
      "learning_rate": 0.0001153460972017673,
      "loss": 0.2979,
      "step": 1442
    },
    {
      "epoch": 0.4244117647058824,
      "grad_norm": 0.0550394281744957,
      "learning_rate": 0.00011528718703976436,
      "loss": 0.4217,
      "step": 1443
    },
    {
      "epoch": 0.42470588235294116,
      "grad_norm": 0.05399560183286667,
      "learning_rate": 0.00011522827687776142,
      "loss": 0.3838,
      "step": 1444
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.0430891215801239,
      "learning_rate": 0.00011516936671575846,
      "loss": 0.3429,
      "step": 1445
    },
    {
      "epoch": 0.4252941176470588,
      "grad_norm": 0.035114020109176636,
      "learning_rate": 0.00011511045655375552,
      "loss": 0.274,
      "step": 1446
    },
    {
      "epoch": 0.42558823529411766,
      "grad_norm": 0.04181864485144615,
      "learning_rate": 0.00011505154639175257,
      "loss": 0.2732,
      "step": 1447
    },
    {
      "epoch": 0.4258823529411765,
      "grad_norm": 0.03943372517824173,
      "learning_rate": 0.00011499263622974963,
      "loss": 0.3427,
      "step": 1448
    },
    {
      "epoch": 0.42617647058823527,
      "grad_norm": 0.041472163051366806,
      "learning_rate": 0.00011493372606774669,
      "loss": 0.3155,
      "step": 1449
    },
    {
      "epoch": 0.4264705882352941,
      "grad_norm": 0.05455680191516876,
      "learning_rate": 0.00011487481590574374,
      "loss": 0.3761,
      "step": 1450
    },
    {
      "epoch": 0.42676470588235293,
      "grad_norm": 0.03510666266083717,
      "learning_rate": 0.0001148159057437408,
      "loss": 0.2656,
      "step": 1451
    },
    {
      "epoch": 0.42705882352941177,
      "grad_norm": 0.0478341169655323,
      "learning_rate": 0.00011475699558173784,
      "loss": 0.3046,
      "step": 1452
    },
    {
      "epoch": 0.4273529411764706,
      "grad_norm": 0.05802810192108154,
      "learning_rate": 0.0001146980854197349,
      "loss": 0.3269,
      "step": 1453
    },
    {
      "epoch": 0.42764705882352944,
      "grad_norm": 0.0640670657157898,
      "learning_rate": 0.00011463917525773196,
      "loss": 0.3757,
      "step": 1454
    },
    {
      "epoch": 0.4279411764705882,
      "grad_norm": 0.05871502682566643,
      "learning_rate": 0.00011458026509572901,
      "loss": 0.4113,
      "step": 1455
    },
    {
      "epoch": 0.42823529411764705,
      "grad_norm": 0.0450851172208786,
      "learning_rate": 0.00011452135493372607,
      "loss": 0.3557,
      "step": 1456
    },
    {
      "epoch": 0.4285294117647059,
      "grad_norm": 0.04450637102127075,
      "learning_rate": 0.00011446244477172312,
      "loss": 0.303,
      "step": 1457
    },
    {
      "epoch": 0.4288235294117647,
      "grad_norm": 0.06504804641008377,
      "learning_rate": 0.00011440353460972018,
      "loss": 0.3945,
      "step": 1458
    },
    {
      "epoch": 0.42911764705882355,
      "grad_norm": 0.05218217894434929,
      "learning_rate": 0.00011434462444771724,
      "loss": 0.3469,
      "step": 1459
    },
    {
      "epoch": 0.4294117647058823,
      "grad_norm": 0.05035050958395004,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.3679,
      "step": 1460
    },
    {
      "epoch": 0.42970588235294116,
      "grad_norm": 0.050837110728025436,
      "learning_rate": 0.00011422680412371134,
      "loss": 0.4008,
      "step": 1461
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.0503595694899559,
      "learning_rate": 0.00011416789396170839,
      "loss": 0.3999,
      "step": 1462
    },
    {
      "epoch": 0.4302941176470588,
      "grad_norm": 0.05921599268913269,
      "learning_rate": 0.00011410898379970545,
      "loss": 0.389,
      "step": 1463
    },
    {
      "epoch": 0.43058823529411766,
      "grad_norm": 0.05030768737196922,
      "learning_rate": 0.0001140500736377025,
      "loss": 0.3489,
      "step": 1464
    },
    {
      "epoch": 0.4308823529411765,
      "grad_norm": 0.04163459688425064,
      "learning_rate": 0.00011399116347569956,
      "loss": 0.2671,
      "step": 1465
    },
    {
      "epoch": 0.43117647058823527,
      "grad_norm": 0.038342759013175964,
      "learning_rate": 0.00011393225331369662,
      "loss": 0.3334,
      "step": 1466
    },
    {
      "epoch": 0.4314705882352941,
      "grad_norm": 0.04202922061085701,
      "learning_rate": 0.00011387334315169366,
      "loss": 0.3569,
      "step": 1467
    },
    {
      "epoch": 0.43176470588235294,
      "grad_norm": 0.04468262940645218,
      "learning_rate": 0.00011381443298969072,
      "loss": 0.3695,
      "step": 1468
    },
    {
      "epoch": 0.4320588235294118,
      "grad_norm": 0.03938781097531319,
      "learning_rate": 0.00011375552282768777,
      "loss": 0.2995,
      "step": 1469
    },
    {
      "epoch": 0.4323529411764706,
      "grad_norm": 0.04766286909580231,
      "learning_rate": 0.00011369661266568483,
      "loss": 0.3381,
      "step": 1470
    },
    {
      "epoch": 0.4326470588235294,
      "grad_norm": 0.05229419469833374,
      "learning_rate": 0.00011363770250368189,
      "loss": 0.4105,
      "step": 1471
    },
    {
      "epoch": 0.4329411764705882,
      "grad_norm": 0.04983612149953842,
      "learning_rate": 0.00011357879234167894,
      "loss": 0.3589,
      "step": 1472
    },
    {
      "epoch": 0.43323529411764705,
      "grad_norm": 0.043541260063648224,
      "learning_rate": 0.000113519882179676,
      "loss": 0.2984,
      "step": 1473
    },
    {
      "epoch": 0.4335294117647059,
      "grad_norm": 0.0470004603266716,
      "learning_rate": 0.00011346097201767304,
      "loss": 0.3411,
      "step": 1474
    },
    {
      "epoch": 0.4338235294117647,
      "grad_norm": 0.0457620844244957,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.3593,
      "step": 1475
    },
    {
      "epoch": 0.43411764705882355,
      "grad_norm": 0.04757905378937721,
      "learning_rate": 0.00011334315169366716,
      "loss": 0.3644,
      "step": 1476
    },
    {
      "epoch": 0.43441176470588233,
      "grad_norm": 0.05284321680665016,
      "learning_rate": 0.00011328424153166421,
      "loss": 0.3303,
      "step": 1477
    },
    {
      "epoch": 0.43470588235294116,
      "grad_norm": 0.04926764592528343,
      "learning_rate": 0.00011322533136966127,
      "loss": 0.3788,
      "step": 1478
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.039164453744888306,
      "learning_rate": 0.00011316642120765832,
      "loss": 0.2723,
      "step": 1479
    },
    {
      "epoch": 0.43529411764705883,
      "grad_norm": 0.04571625217795372,
      "learning_rate": 0.00011310751104565538,
      "loss": 0.3715,
      "step": 1480
    },
    {
      "epoch": 0.43558823529411766,
      "grad_norm": 0.04187869653105736,
      "learning_rate": 0.00011304860088365244,
      "loss": 0.3268,
      "step": 1481
    },
    {
      "epoch": 0.43588235294117644,
      "grad_norm": 0.04321970045566559,
      "learning_rate": 0.00011298969072164949,
      "loss": 0.3474,
      "step": 1482
    },
    {
      "epoch": 0.4361764705882353,
      "grad_norm": 0.04268364608287811,
      "learning_rate": 0.00011293078055964655,
      "loss": 0.3419,
      "step": 1483
    },
    {
      "epoch": 0.4364705882352941,
      "grad_norm": 0.038879282772541046,
      "learning_rate": 0.00011287187039764359,
      "loss": 0.3315,
      "step": 1484
    },
    {
      "epoch": 0.43676470588235294,
      "grad_norm": 0.05065784975886345,
      "learning_rate": 0.00011281296023564065,
      "loss": 0.3524,
      "step": 1485
    },
    {
      "epoch": 0.4370588235294118,
      "grad_norm": 0.046091243624687195,
      "learning_rate": 0.00011275405007363771,
      "loss": 0.4012,
      "step": 1486
    },
    {
      "epoch": 0.4373529411764706,
      "grad_norm": 0.06165005639195442,
      "learning_rate": 0.00011269513991163476,
      "loss": 0.4577,
      "step": 1487
    },
    {
      "epoch": 0.4376470588235294,
      "grad_norm": 0.04834842309355736,
      "learning_rate": 0.00011263622974963182,
      "loss": 0.3805,
      "step": 1488
    },
    {
      "epoch": 0.4379411764705882,
      "grad_norm": 0.04201754182577133,
      "learning_rate": 0.00011257731958762887,
      "loss": 0.3225,
      "step": 1489
    },
    {
      "epoch": 0.43823529411764706,
      "grad_norm": 0.05125832557678223,
      "learning_rate": 0.00011251840942562593,
      "loss": 0.3816,
      "step": 1490
    },
    {
      "epoch": 0.4385294117647059,
      "grad_norm": 0.048723846673965454,
      "learning_rate": 0.00011245949926362299,
      "loss": 0.3476,
      "step": 1491
    },
    {
      "epoch": 0.4388235294117647,
      "grad_norm": 0.04789407551288605,
      "learning_rate": 0.00011240058910162003,
      "loss": 0.375,
      "step": 1492
    },
    {
      "epoch": 0.43911764705882356,
      "grad_norm": 0.058249033987522125,
      "learning_rate": 0.00011234167893961709,
      "loss": 0.3821,
      "step": 1493
    },
    {
      "epoch": 0.43941176470588234,
      "grad_norm": 0.03490755334496498,
      "learning_rate": 0.00011228276877761414,
      "loss": 0.3257,
      "step": 1494
    },
    {
      "epoch": 0.43970588235294117,
      "grad_norm": 0.04989989846944809,
      "learning_rate": 0.0001122238586156112,
      "loss": 0.4336,
      "step": 1495
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.04563220962882042,
      "learning_rate": 0.00011216494845360826,
      "loss": 0.3639,
      "step": 1496
    },
    {
      "epoch": 0.44029411764705884,
      "grad_norm": 0.0425448939204216,
      "learning_rate": 0.0001121060382916053,
      "loss": 0.291,
      "step": 1497
    },
    {
      "epoch": 0.44058823529411767,
      "grad_norm": 0.05256984010338783,
      "learning_rate": 0.00011204712812960237,
      "loss": 0.3574,
      "step": 1498
    },
    {
      "epoch": 0.44088235294117645,
      "grad_norm": 0.04514993354678154,
      "learning_rate": 0.00011198821796759941,
      "loss": 0.354,
      "step": 1499
    },
    {
      "epoch": 0.4411764705882353,
      "grad_norm": 0.04611453041434288,
      "learning_rate": 0.00011192930780559647,
      "loss": 0.3127,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.2928565650928435e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
