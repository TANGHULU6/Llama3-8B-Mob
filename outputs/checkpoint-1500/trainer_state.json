{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.05815362248606736,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 3.876908165737824e-05,
      "grad_norm": 3.076904535293579,
      "learning_rate": 4e-05,
      "loss": 3.4603,
      "step": 1
    },
    {
      "epoch": 7.753816331475648e-05,
      "grad_norm": 3.215231418609619,
      "learning_rate": 8e-05,
      "loss": 3.4399,
      "step": 2
    },
    {
      "epoch": 0.00011630724497213472,
      "grad_norm": 2.72213077545166,
      "learning_rate": 0.00012,
      "loss": 3.3332,
      "step": 3
    },
    {
      "epoch": 0.00015507632662951296,
      "grad_norm": 1.9455198049545288,
      "learning_rate": 0.00016,
      "loss": 3.2079,
      "step": 4
    },
    {
      "epoch": 0.0001938454082868912,
      "grad_norm": 1.9726125001907349,
      "learning_rate": 0.0002,
      "loss": 3.0827,
      "step": 5
    },
    {
      "epoch": 0.00023261448994426945,
      "grad_norm": 2.2453792095184326,
      "learning_rate": 0.00019999224445478518,
      "loss": 2.8358,
      "step": 6
    },
    {
      "epoch": 0.0002713835716016477,
      "grad_norm": 2.065769910812378,
      "learning_rate": 0.00019998448890957036,
      "loss": 2.6327,
      "step": 7
    },
    {
      "epoch": 0.00031015265325902593,
      "grad_norm": 1.7409061193466187,
      "learning_rate": 0.00019997673336435553,
      "loss": 2.4464,
      "step": 8
    },
    {
      "epoch": 0.0003489217349164042,
      "grad_norm": 1.476381778717041,
      "learning_rate": 0.0001999689778191407,
      "loss": 2.2423,
      "step": 9
    },
    {
      "epoch": 0.0003876908165737824,
      "grad_norm": 1.4034909009933472,
      "learning_rate": 0.00019996122227392587,
      "loss": 2.0961,
      "step": 10
    },
    {
      "epoch": 0.00042645989823116064,
      "grad_norm": 1.2606425285339355,
      "learning_rate": 0.00019995346672871105,
      "loss": 1.8487,
      "step": 11
    },
    {
      "epoch": 0.0004652289798885389,
      "grad_norm": 1.5401495695114136,
      "learning_rate": 0.00019994571118349622,
      "loss": 1.7758,
      "step": 12
    },
    {
      "epoch": 0.0005039980615459171,
      "grad_norm": 2.1655983924865723,
      "learning_rate": 0.00019993795563828136,
      "loss": 1.677,
      "step": 13
    },
    {
      "epoch": 0.0005427671432032953,
      "grad_norm": 1.12831711769104,
      "learning_rate": 0.00019993020009306656,
      "loss": 1.67,
      "step": 14
    },
    {
      "epoch": 0.0005815362248606736,
      "grad_norm": 2.7026541233062744,
      "learning_rate": 0.0001999224445478517,
      "loss": 1.5806,
      "step": 15
    },
    {
      "epoch": 0.0006203053065180519,
      "grad_norm": 1.7286372184753418,
      "learning_rate": 0.0001999146890026369,
      "loss": 1.3481,
      "step": 16
    },
    {
      "epoch": 0.0006590743881754301,
      "grad_norm": 1.5930252075195312,
      "learning_rate": 0.00019990693345742206,
      "loss": 1.3147,
      "step": 17
    },
    {
      "epoch": 0.0006978434698328084,
      "grad_norm": 2.443732976913452,
      "learning_rate": 0.00019989917791220726,
      "loss": 1.2903,
      "step": 18
    },
    {
      "epoch": 0.0007366125514901866,
      "grad_norm": 1.2027349472045898,
      "learning_rate": 0.0001998914223669924,
      "loss": 1.1465,
      "step": 19
    },
    {
      "epoch": 0.0007753816331475648,
      "grad_norm": 1.8845245838165283,
      "learning_rate": 0.00019988366682177757,
      "loss": 0.9872,
      "step": 20
    },
    {
      "epoch": 0.000814150714804943,
      "grad_norm": 1.4270838499069214,
      "learning_rate": 0.00019987591127656275,
      "loss": 0.9575,
      "step": 21
    },
    {
      "epoch": 0.0008529197964623213,
      "grad_norm": 1.5661834478378296,
      "learning_rate": 0.00019986815573134792,
      "loss": 0.9309,
      "step": 22
    },
    {
      "epoch": 0.0008916888781196995,
      "grad_norm": 1.03627347946167,
      "learning_rate": 0.0001998604001861331,
      "loss": 0.9188,
      "step": 23
    },
    {
      "epoch": 0.0009304579597770778,
      "grad_norm": 1.6426228284835815,
      "learning_rate": 0.00019985264464091827,
      "loss": 0.9318,
      "step": 24
    },
    {
      "epoch": 0.000969227041434456,
      "grad_norm": 0.7082459330558777,
      "learning_rate": 0.00019984488909570344,
      "loss": 0.8426,
      "step": 25
    },
    {
      "epoch": 0.0010079961230918342,
      "grad_norm": 0.5908604860305786,
      "learning_rate": 0.0001998371335504886,
      "loss": 0.9475,
      "step": 26
    },
    {
      "epoch": 0.0010467652047492124,
      "grad_norm": 0.7839014530181885,
      "learning_rate": 0.00019982937800527378,
      "loss": 0.8908,
      "step": 27
    },
    {
      "epoch": 0.0010855342864065907,
      "grad_norm": 0.5976871252059937,
      "learning_rate": 0.00019982162246005896,
      "loss": 0.8808,
      "step": 28
    },
    {
      "epoch": 0.001124303368063969,
      "grad_norm": 0.7599288821220398,
      "learning_rate": 0.00019981386691484413,
      "loss": 0.9041,
      "step": 29
    },
    {
      "epoch": 0.0011630724497213472,
      "grad_norm": 0.7777177095413208,
      "learning_rate": 0.0001998061113696293,
      "loss": 0.8954,
      "step": 30
    },
    {
      "epoch": 0.0012018415313787255,
      "grad_norm": 0.8363538980484009,
      "learning_rate": 0.00019979835582441448,
      "loss": 0.9483,
      "step": 31
    },
    {
      "epoch": 0.0012406106130361037,
      "grad_norm": 0.7017698884010315,
      "learning_rate": 0.00019979060027919965,
      "loss": 0.8142,
      "step": 32
    },
    {
      "epoch": 0.001279379694693482,
      "grad_norm": 0.9329237341880798,
      "learning_rate": 0.00019978284473398482,
      "loss": 0.9068,
      "step": 33
    },
    {
      "epoch": 0.0013181487763508602,
      "grad_norm": 0.5964627861976624,
      "learning_rate": 0.00019977508918876997,
      "loss": 0.8351,
      "step": 34
    },
    {
      "epoch": 0.0013569178580082385,
      "grad_norm": 1.2543880939483643,
      "learning_rate": 0.00019976733364355517,
      "loss": 0.8335,
      "step": 35
    },
    {
      "epoch": 0.0013956869396656167,
      "grad_norm": 0.9619218111038208,
      "learning_rate": 0.0001997595780983403,
      "loss": 0.9513,
      "step": 36
    },
    {
      "epoch": 0.001434456021322995,
      "grad_norm": 0.8074339628219604,
      "learning_rate": 0.0001997518225531255,
      "loss": 0.8216,
      "step": 37
    },
    {
      "epoch": 0.0014732251029803732,
      "grad_norm": 1.0775606632232666,
      "learning_rate": 0.00019974406700791066,
      "loss": 0.7653,
      "step": 38
    },
    {
      "epoch": 0.0015119941846377515,
      "grad_norm": 1.053296685218811,
      "learning_rate": 0.00019973631146269586,
      "loss": 0.7693,
      "step": 39
    },
    {
      "epoch": 0.0015507632662951295,
      "grad_norm": 1.6107959747314453,
      "learning_rate": 0.000199728555917481,
      "loss": 0.804,
      "step": 40
    },
    {
      "epoch": 0.0015895323479525078,
      "grad_norm": 1.4093143939971924,
      "learning_rate": 0.00019972080037226618,
      "loss": 0.8094,
      "step": 41
    },
    {
      "epoch": 0.001628301429609886,
      "grad_norm": 2.0524916648864746,
      "learning_rate": 0.00019971304482705135,
      "loss": 0.7175,
      "step": 42
    },
    {
      "epoch": 0.0016670705112672643,
      "grad_norm": 2.060410261154175,
      "learning_rate": 0.00019970528928183652,
      "loss": 0.703,
      "step": 43
    },
    {
      "epoch": 0.0017058395929246426,
      "grad_norm": 0.9922105073928833,
      "learning_rate": 0.0001996975337366217,
      "loss": 0.6411,
      "step": 44
    },
    {
      "epoch": 0.0017446086745820208,
      "grad_norm": 0.9802547097206116,
      "learning_rate": 0.00019968977819140687,
      "loss": 0.5638,
      "step": 45
    },
    {
      "epoch": 0.001783377756239399,
      "grad_norm": 1.1488854885101318,
      "learning_rate": 0.00019968202264619204,
      "loss": 0.6791,
      "step": 46
    },
    {
      "epoch": 0.0018221468378967773,
      "grad_norm": 2.2893660068511963,
      "learning_rate": 0.0001996742671009772,
      "loss": 0.6355,
      "step": 47
    },
    {
      "epoch": 0.0018609159195541556,
      "grad_norm": 2.081825017929077,
      "learning_rate": 0.00019966651155576236,
      "loss": 0.6547,
      "step": 48
    },
    {
      "epoch": 0.0018996850012115338,
      "grad_norm": 1.7704328298568726,
      "learning_rate": 0.00019965875601054756,
      "loss": 0.6987,
      "step": 49
    },
    {
      "epoch": 0.001938454082868912,
      "grad_norm": 1.9767711162567139,
      "learning_rate": 0.0001996510004653327,
      "loss": 0.5354,
      "step": 50
    },
    {
      "epoch": 0.0019772231645262903,
      "grad_norm": 1.3959672451019287,
      "learning_rate": 0.0001996432449201179,
      "loss": 0.5873,
      "step": 51
    },
    {
      "epoch": 0.0020159922461836684,
      "grad_norm": 1.3025474548339844,
      "learning_rate": 0.00019963548937490305,
      "loss": 0.6591,
      "step": 52
    },
    {
      "epoch": 0.002054761327841047,
      "grad_norm": 2.2802367210388184,
      "learning_rate": 0.00019962773382968825,
      "loss": 0.6478,
      "step": 53
    },
    {
      "epoch": 0.002093530409498425,
      "grad_norm": 1.4379206895828247,
      "learning_rate": 0.0001996199782844734,
      "loss": 0.494,
      "step": 54
    },
    {
      "epoch": 0.0021322994911558034,
      "grad_norm": 1.7023591995239258,
      "learning_rate": 0.00019961222273925857,
      "loss": 0.6367,
      "step": 55
    },
    {
      "epoch": 0.0021710685728131814,
      "grad_norm": 1.3177282810211182,
      "learning_rate": 0.00019960446719404374,
      "loss": 0.4629,
      "step": 56
    },
    {
      "epoch": 0.00220983765447056,
      "grad_norm": 1.016967535018921,
      "learning_rate": 0.00019959671164882891,
      "loss": 0.4934,
      "step": 57
    },
    {
      "epoch": 0.002248606736127938,
      "grad_norm": 1.5113985538482666,
      "learning_rate": 0.0001995889561036141,
      "loss": 0.3996,
      "step": 58
    },
    {
      "epoch": 0.0022873758177853164,
      "grad_norm": 0.8883568048477173,
      "learning_rate": 0.00019958120055839926,
      "loss": 0.5175,
      "step": 59
    },
    {
      "epoch": 0.0023261448994426944,
      "grad_norm": 1.4438486099243164,
      "learning_rate": 0.00019957344501318443,
      "loss": 0.56,
      "step": 60
    },
    {
      "epoch": 0.002364913981100073,
      "grad_norm": 0.9607509970664978,
      "learning_rate": 0.0001995656894679696,
      "loss": 0.5382,
      "step": 61
    },
    {
      "epoch": 0.002403683062757451,
      "grad_norm": 0.9949698448181152,
      "learning_rate": 0.00019955793392275478,
      "loss": 0.5117,
      "step": 62
    },
    {
      "epoch": 0.002442452144414829,
      "grad_norm": 1.3765926361083984,
      "learning_rate": 0.00019955017837753995,
      "loss": 0.4388,
      "step": 63
    },
    {
      "epoch": 0.0024812212260722074,
      "grad_norm": 0.8322263360023499,
      "learning_rate": 0.00019954242283232512,
      "loss": 0.5912,
      "step": 64
    },
    {
      "epoch": 0.0025199903077295855,
      "grad_norm": 0.7141783833503723,
      "learning_rate": 0.0001995346672871103,
      "loss": 0.4569,
      "step": 65
    },
    {
      "epoch": 0.002558759389386964,
      "grad_norm": 1.1710041761398315,
      "learning_rate": 0.00019952691174189547,
      "loss": 0.5295,
      "step": 66
    },
    {
      "epoch": 0.002597528471044342,
      "grad_norm": 0.7588098645210266,
      "learning_rate": 0.00019951915619668064,
      "loss": 0.3879,
      "step": 67
    },
    {
      "epoch": 0.0026362975527017204,
      "grad_norm": 0.39814552664756775,
      "learning_rate": 0.00019951140065146582,
      "loss": 0.4427,
      "step": 68
    },
    {
      "epoch": 0.0026750666343590985,
      "grad_norm": 0.6337335109710693,
      "learning_rate": 0.00019950364510625096,
      "loss": 0.4273,
      "step": 69
    },
    {
      "epoch": 0.002713835716016477,
      "grad_norm": 0.6543207764625549,
      "learning_rate": 0.00019949588956103616,
      "loss": 0.4187,
      "step": 70
    },
    {
      "epoch": 0.002752604797673855,
      "grad_norm": 0.42048418521881104,
      "learning_rate": 0.0001994881340158213,
      "loss": 0.4071,
      "step": 71
    },
    {
      "epoch": 0.0027913738793312335,
      "grad_norm": 0.380841463804245,
      "learning_rate": 0.0001994803784706065,
      "loss": 0.4538,
      "step": 72
    },
    {
      "epoch": 0.0028301429609886115,
      "grad_norm": 0.6037354469299316,
      "learning_rate": 0.00019947262292539165,
      "loss": 0.5582,
      "step": 73
    },
    {
      "epoch": 0.00286891204264599,
      "grad_norm": 0.5498448610305786,
      "learning_rate": 0.00019946486738017685,
      "loss": 0.4431,
      "step": 74
    },
    {
      "epoch": 0.002907681124303368,
      "grad_norm": 0.43996989727020264,
      "learning_rate": 0.000199457111834962,
      "loss": 0.3518,
      "step": 75
    },
    {
      "epoch": 0.0029464502059607465,
      "grad_norm": 0.37207508087158203,
      "learning_rate": 0.00019944935628974717,
      "loss": 0.4092,
      "step": 76
    },
    {
      "epoch": 0.0029852192876181245,
      "grad_norm": 0.48353052139282227,
      "learning_rate": 0.00019944160074453234,
      "loss": 0.3983,
      "step": 77
    },
    {
      "epoch": 0.003023988369275503,
      "grad_norm": 0.4166845381259918,
      "learning_rate": 0.00019943384519931752,
      "loss": 0.3752,
      "step": 78
    },
    {
      "epoch": 0.003062757450932881,
      "grad_norm": 0.46338117122650146,
      "learning_rate": 0.0001994260896541027,
      "loss": 0.4284,
      "step": 79
    },
    {
      "epoch": 0.003101526532590259,
      "grad_norm": 0.30757734179496765,
      "learning_rate": 0.00019941833410888786,
      "loss": 0.4026,
      "step": 80
    },
    {
      "epoch": 0.0031402956142476375,
      "grad_norm": 0.39001184701919556,
      "learning_rate": 0.00019941057856367303,
      "loss": 0.3386,
      "step": 81
    },
    {
      "epoch": 0.0031790646959050156,
      "grad_norm": 0.40398213267326355,
      "learning_rate": 0.0001994028230184582,
      "loss": 0.3646,
      "step": 82
    },
    {
      "epoch": 0.003217833777562394,
      "grad_norm": 0.31530216336250305,
      "learning_rate": 0.00019939506747324338,
      "loss": 0.3802,
      "step": 83
    },
    {
      "epoch": 0.003256602859219772,
      "grad_norm": 0.39986175298690796,
      "learning_rate": 0.00019938731192802855,
      "loss": 0.3554,
      "step": 84
    },
    {
      "epoch": 0.0032953719408771506,
      "grad_norm": 0.4107156991958618,
      "learning_rate": 0.00019937955638281373,
      "loss": 0.3778,
      "step": 85
    },
    {
      "epoch": 0.0033341410225345286,
      "grad_norm": 0.5386656522750854,
      "learning_rate": 0.0001993718008375989,
      "loss": 0.4716,
      "step": 86
    },
    {
      "epoch": 0.003372910104191907,
      "grad_norm": 0.3884955048561096,
      "learning_rate": 0.00019936404529238407,
      "loss": 0.4119,
      "step": 87
    },
    {
      "epoch": 0.003411679185849285,
      "grad_norm": 0.4117557406425476,
      "learning_rate": 0.00019935628974716924,
      "loss": 0.3993,
      "step": 88
    },
    {
      "epoch": 0.0034504482675066636,
      "grad_norm": 0.3828742206096649,
      "learning_rate": 0.00019934853420195442,
      "loss": 0.3883,
      "step": 89
    },
    {
      "epoch": 0.0034892173491640416,
      "grad_norm": 0.3464539647102356,
      "learning_rate": 0.00019934077865673956,
      "loss": 0.332,
      "step": 90
    },
    {
      "epoch": 0.00352798643082142,
      "grad_norm": 0.36263829469680786,
      "learning_rate": 0.00019933302311152476,
      "loss": 0.4075,
      "step": 91
    },
    {
      "epoch": 0.003566755512478798,
      "grad_norm": 0.3180046081542969,
      "learning_rate": 0.0001993252675663099,
      "loss": 0.3944,
      "step": 92
    },
    {
      "epoch": 0.0036055245941361766,
      "grad_norm": 0.43269050121307373,
      "learning_rate": 0.0001993175120210951,
      "loss": 0.4561,
      "step": 93
    },
    {
      "epoch": 0.0036442936757935546,
      "grad_norm": 0.3255857527256012,
      "learning_rate": 0.00019930975647588025,
      "loss": 0.351,
      "step": 94
    },
    {
      "epoch": 0.0036830627574509327,
      "grad_norm": 0.21789297461509705,
      "learning_rate": 0.00019930200093066545,
      "loss": 0.4543,
      "step": 95
    },
    {
      "epoch": 0.003721831839108311,
      "grad_norm": 0.22572259604930878,
      "learning_rate": 0.0001992942453854506,
      "loss": 0.3563,
      "step": 96
    },
    {
      "epoch": 0.003760600920765689,
      "grad_norm": 0.4792696535587311,
      "learning_rate": 0.00019928648984023577,
      "loss": 0.3664,
      "step": 97
    },
    {
      "epoch": 0.0037993700024230677,
      "grad_norm": 0.37555548548698425,
      "learning_rate": 0.00019927873429502095,
      "loss": 0.3325,
      "step": 98
    },
    {
      "epoch": 0.0038381390840804457,
      "grad_norm": 0.3820406198501587,
      "learning_rate": 0.00019927097874980612,
      "loss": 0.3665,
      "step": 99
    },
    {
      "epoch": 0.003876908165737824,
      "grad_norm": 0.3498466908931732,
      "learning_rate": 0.0001992632232045913,
      "loss": 0.4414,
      "step": 100
    },
    {
      "epoch": 0.003915677247395203,
      "grad_norm": 0.32542911171913147,
      "learning_rate": 0.00019925546765937646,
      "loss": 0.4803,
      "step": 101
    },
    {
      "epoch": 0.003954446329052581,
      "grad_norm": 0.27770984172821045,
      "learning_rate": 0.00019924771211416164,
      "loss": 0.3232,
      "step": 102
    },
    {
      "epoch": 0.003993215410709959,
      "grad_norm": 0.7336586117744446,
      "learning_rate": 0.0001992399565689468,
      "loss": 0.3708,
      "step": 103
    },
    {
      "epoch": 0.004031984492367337,
      "grad_norm": 0.4379984438419342,
      "learning_rate": 0.00019923220102373195,
      "loss": 0.326,
      "step": 104
    },
    {
      "epoch": 0.004070753574024716,
      "grad_norm": 0.3229488730430603,
      "learning_rate": 0.00019922444547851715,
      "loss": 0.3486,
      "step": 105
    },
    {
      "epoch": 0.004109522655682094,
      "grad_norm": 0.37352925539016724,
      "learning_rate": 0.00019921668993330233,
      "loss": 0.4103,
      "step": 106
    },
    {
      "epoch": 0.004148291737339472,
      "grad_norm": 0.31027740240097046,
      "learning_rate": 0.0001992089343880875,
      "loss": 0.3942,
      "step": 107
    },
    {
      "epoch": 0.00418706081899685,
      "grad_norm": 0.47671830654144287,
      "learning_rate": 0.00019920117884287267,
      "loss": 0.4303,
      "step": 108
    },
    {
      "epoch": 0.004225829900654229,
      "grad_norm": 0.2849827706813812,
      "learning_rate": 0.00019919342329765785,
      "loss": 0.3735,
      "step": 109
    },
    {
      "epoch": 0.004264598982311607,
      "grad_norm": 0.3720203936100006,
      "learning_rate": 0.00019918566775244302,
      "loss": 0.5271,
      "step": 110
    },
    {
      "epoch": 0.004303368063968985,
      "grad_norm": 0.3835037350654602,
      "learning_rate": 0.00019917791220722816,
      "loss": 0.3959,
      "step": 111
    },
    {
      "epoch": 0.004342137145626363,
      "grad_norm": 0.2934713363647461,
      "learning_rate": 0.00019917015666201336,
      "loss": 0.2994,
      "step": 112
    },
    {
      "epoch": 0.004380906227283741,
      "grad_norm": 0.28044673800468445,
      "learning_rate": 0.0001991624011167985,
      "loss": 0.3853,
      "step": 113
    },
    {
      "epoch": 0.00441967530894112,
      "grad_norm": 0.31564101576805115,
      "learning_rate": 0.0001991546455715837,
      "loss": 0.4622,
      "step": 114
    },
    {
      "epoch": 0.004458444390598498,
      "grad_norm": 0.3015250265598297,
      "learning_rate": 0.00019914689002636886,
      "loss": 0.4569,
      "step": 115
    },
    {
      "epoch": 0.004497213472255876,
      "grad_norm": 0.2700304388999939,
      "learning_rate": 0.00019913913448115406,
      "loss": 0.3658,
      "step": 116
    },
    {
      "epoch": 0.004535982553913254,
      "grad_norm": 0.23505519330501556,
      "learning_rate": 0.0001991313789359392,
      "loss": 0.3252,
      "step": 117
    },
    {
      "epoch": 0.004574751635570633,
      "grad_norm": 0.26610928773880005,
      "learning_rate": 0.00019912362339072437,
      "loss": 0.377,
      "step": 118
    },
    {
      "epoch": 0.004613520717228011,
      "grad_norm": 0.2529745399951935,
      "learning_rate": 0.00019911586784550955,
      "loss": 0.4238,
      "step": 119
    },
    {
      "epoch": 0.004652289798885389,
      "grad_norm": 0.37146490812301636,
      "learning_rate": 0.00019910811230029472,
      "loss": 0.3766,
      "step": 120
    },
    {
      "epoch": 0.004691058880542767,
      "grad_norm": 0.22810615599155426,
      "learning_rate": 0.0001991003567550799,
      "loss": 0.325,
      "step": 121
    },
    {
      "epoch": 0.004729827962200146,
      "grad_norm": 0.2723677456378937,
      "learning_rate": 0.00019909260120986507,
      "loss": 0.4377,
      "step": 122
    },
    {
      "epoch": 0.004768597043857524,
      "grad_norm": 0.26087960600852966,
      "learning_rate": 0.00019908484566465024,
      "loss": 0.3262,
      "step": 123
    },
    {
      "epoch": 0.004807366125514902,
      "grad_norm": 0.22962044179439545,
      "learning_rate": 0.0001990770901194354,
      "loss": 0.3561,
      "step": 124
    },
    {
      "epoch": 0.00484613520717228,
      "grad_norm": 0.18949374556541443,
      "learning_rate": 0.00019906933457422056,
      "loss": 0.296,
      "step": 125
    },
    {
      "epoch": 0.004884904288829658,
      "grad_norm": 0.3092156946659088,
      "learning_rate": 0.00019906157902900576,
      "loss": 0.4671,
      "step": 126
    },
    {
      "epoch": 0.004923673370487037,
      "grad_norm": 0.19221702218055725,
      "learning_rate": 0.0001990538234837909,
      "loss": 0.3193,
      "step": 127
    },
    {
      "epoch": 0.004962442452144415,
      "grad_norm": 0.25905564427375793,
      "learning_rate": 0.0001990460679385761,
      "loss": 0.3691,
      "step": 128
    },
    {
      "epoch": 0.005001211533801793,
      "grad_norm": 0.1957622617483139,
      "learning_rate": 0.00019903831239336125,
      "loss": 0.317,
      "step": 129
    },
    {
      "epoch": 0.005039980615459171,
      "grad_norm": 0.2595498263835907,
      "learning_rate": 0.00019903055684814645,
      "loss": 0.3453,
      "step": 130
    },
    {
      "epoch": 0.00507874969711655,
      "grad_norm": 0.2487931102514267,
      "learning_rate": 0.0001990228013029316,
      "loss": 0.3481,
      "step": 131
    },
    {
      "epoch": 0.005117518778773928,
      "grad_norm": 0.23804764449596405,
      "learning_rate": 0.00019901504575771677,
      "loss": 0.3483,
      "step": 132
    },
    {
      "epoch": 0.005156287860431306,
      "grad_norm": 0.23190927505493164,
      "learning_rate": 0.00019900729021250194,
      "loss": 0.3861,
      "step": 133
    },
    {
      "epoch": 0.005195056942088684,
      "grad_norm": 0.28612321615219116,
      "learning_rate": 0.0001989995346672871,
      "loss": 0.4202,
      "step": 134
    },
    {
      "epoch": 0.005233826023746063,
      "grad_norm": 0.21660266816616058,
      "learning_rate": 0.00019899177912207228,
      "loss": 0.3302,
      "step": 135
    },
    {
      "epoch": 0.005272595105403441,
      "grad_norm": 0.2306484580039978,
      "learning_rate": 0.00019898402357685746,
      "loss": 0.4023,
      "step": 136
    },
    {
      "epoch": 0.005311364187060819,
      "grad_norm": 0.24266718327999115,
      "learning_rate": 0.00019897626803164263,
      "loss": 0.3465,
      "step": 137
    },
    {
      "epoch": 0.005350133268718197,
      "grad_norm": 0.19707806408405304,
      "learning_rate": 0.0001989685124864278,
      "loss": 0.3858,
      "step": 138
    },
    {
      "epoch": 0.005388902350375576,
      "grad_norm": 0.41391465067863464,
      "learning_rate": 0.00019896075694121298,
      "loss": 0.339,
      "step": 139
    },
    {
      "epoch": 0.005427671432032954,
      "grad_norm": 0.2371853142976761,
      "learning_rate": 0.00019895300139599815,
      "loss": 0.3628,
      "step": 140
    },
    {
      "epoch": 0.005466440513690332,
      "grad_norm": 0.2575313448905945,
      "learning_rate": 0.00019894524585078332,
      "loss": 0.4077,
      "step": 141
    },
    {
      "epoch": 0.00550520959534771,
      "grad_norm": 0.3299817144870758,
      "learning_rate": 0.0001989374903055685,
      "loss": 0.3442,
      "step": 142
    },
    {
      "epoch": 0.005543978677005088,
      "grad_norm": 0.24534080922603607,
      "learning_rate": 0.00019892973476035367,
      "loss": 0.32,
      "step": 143
    },
    {
      "epoch": 0.005582747758662467,
      "grad_norm": 0.2236955165863037,
      "learning_rate": 0.00019892197921513884,
      "loss": 0.3568,
      "step": 144
    },
    {
      "epoch": 0.005621516840319845,
      "grad_norm": 0.23586390912532806,
      "learning_rate": 0.000198914223669924,
      "loss": 0.2916,
      "step": 145
    },
    {
      "epoch": 0.005660285921977223,
      "grad_norm": 0.2849571108818054,
      "learning_rate": 0.00019890646812470916,
      "loss": 0.3689,
      "step": 146
    },
    {
      "epoch": 0.005699055003634601,
      "grad_norm": 0.28808072209358215,
      "learning_rate": 0.00019889871257949436,
      "loss": 0.3548,
      "step": 147
    },
    {
      "epoch": 0.00573782408529198,
      "grad_norm": 0.19898295402526855,
      "learning_rate": 0.0001988909570342795,
      "loss": 0.328,
      "step": 148
    },
    {
      "epoch": 0.005776593166949358,
      "grad_norm": 0.26719728112220764,
      "learning_rate": 0.0001988832014890647,
      "loss": 0.3655,
      "step": 149
    },
    {
      "epoch": 0.005815362248606736,
      "grad_norm": 0.2787806987762451,
      "learning_rate": 0.00019887544594384985,
      "loss": 0.3641,
      "step": 150
    },
    {
      "epoch": 0.005854131330264114,
      "grad_norm": 0.216325581073761,
      "learning_rate": 0.00019886769039863505,
      "loss": 0.4156,
      "step": 151
    },
    {
      "epoch": 0.005892900411921493,
      "grad_norm": 0.2534346282482147,
      "learning_rate": 0.0001988599348534202,
      "loss": 0.4626,
      "step": 152
    },
    {
      "epoch": 0.005931669493578871,
      "grad_norm": 0.26094526052474976,
      "learning_rate": 0.00019885217930820537,
      "loss": 0.3627,
      "step": 153
    },
    {
      "epoch": 0.005970438575236249,
      "grad_norm": 0.2076389491558075,
      "learning_rate": 0.00019884442376299054,
      "loss": 0.3806,
      "step": 154
    },
    {
      "epoch": 0.006009207656893627,
      "grad_norm": 0.21460583806037903,
      "learning_rate": 0.00019883666821777571,
      "loss": 0.2809,
      "step": 155
    },
    {
      "epoch": 0.006047976738551006,
      "grad_norm": 0.21737274527549744,
      "learning_rate": 0.0001988289126725609,
      "loss": 0.3628,
      "step": 156
    },
    {
      "epoch": 0.006086745820208384,
      "grad_norm": 0.25624778866767883,
      "learning_rate": 0.00019882115712734606,
      "loss": 0.3904,
      "step": 157
    },
    {
      "epoch": 0.006125514901865762,
      "grad_norm": 0.2228907197713852,
      "learning_rate": 0.00019881340158213123,
      "loss": 0.3509,
      "step": 158
    },
    {
      "epoch": 0.00616428398352314,
      "grad_norm": 0.2858039438724518,
      "learning_rate": 0.0001988056460369164,
      "loss": 0.3582,
      "step": 159
    },
    {
      "epoch": 0.006203053065180518,
      "grad_norm": 0.25192990899086,
      "learning_rate": 0.00019879789049170158,
      "loss": 0.3491,
      "step": 160
    },
    {
      "epoch": 0.006241822146837897,
      "grad_norm": 0.29516175389289856,
      "learning_rate": 0.00019879013494648675,
      "loss": 0.3721,
      "step": 161
    },
    {
      "epoch": 0.006280591228495275,
      "grad_norm": 0.2907150685787201,
      "learning_rate": 0.00019878237940127192,
      "loss": 0.4412,
      "step": 162
    },
    {
      "epoch": 0.006319360310152653,
      "grad_norm": 0.25139757990837097,
      "learning_rate": 0.0001987746238560571,
      "loss": 0.3145,
      "step": 163
    },
    {
      "epoch": 0.006358129391810031,
      "grad_norm": 0.30777716636657715,
      "learning_rate": 0.00019876686831084227,
      "loss": 0.364,
      "step": 164
    },
    {
      "epoch": 0.00639689847346741,
      "grad_norm": 0.16985292732715607,
      "learning_rate": 0.00019875911276562744,
      "loss": 0.3066,
      "step": 165
    },
    {
      "epoch": 0.006435667555124788,
      "grad_norm": 0.2536247968673706,
      "learning_rate": 0.00019875135722041261,
      "loss": 0.4194,
      "step": 166
    },
    {
      "epoch": 0.006474436636782166,
      "grad_norm": 0.3053729832172394,
      "learning_rate": 0.00019874360167519776,
      "loss": 0.3576,
      "step": 167
    },
    {
      "epoch": 0.006513205718439544,
      "grad_norm": 0.23392751812934875,
      "learning_rate": 0.00019873584612998296,
      "loss": 0.3457,
      "step": 168
    },
    {
      "epoch": 0.006551974800096923,
      "grad_norm": 0.17018342018127441,
      "learning_rate": 0.0001987280905847681,
      "loss": 0.2684,
      "step": 169
    },
    {
      "epoch": 0.006590743881754301,
      "grad_norm": 0.22994540631771088,
      "learning_rate": 0.0001987203350395533,
      "loss": 0.2659,
      "step": 170
    },
    {
      "epoch": 0.006629512963411679,
      "grad_norm": 0.25113654136657715,
      "learning_rate": 0.00019871257949433845,
      "loss": 0.2883,
      "step": 171
    },
    {
      "epoch": 0.006668282045069057,
      "grad_norm": 0.24131953716278076,
      "learning_rate": 0.00019870482394912365,
      "loss": 0.346,
      "step": 172
    },
    {
      "epoch": 0.006707051126726436,
      "grad_norm": 0.22444583475589752,
      "learning_rate": 0.0001986970684039088,
      "loss": 0.3707,
      "step": 173
    },
    {
      "epoch": 0.006745820208383814,
      "grad_norm": 0.276846706867218,
      "learning_rate": 0.00019868931285869397,
      "loss": 0.4082,
      "step": 174
    },
    {
      "epoch": 0.006784589290041192,
      "grad_norm": 0.32546162605285645,
      "learning_rate": 0.00019868155731347914,
      "loss": 0.3567,
      "step": 175
    },
    {
      "epoch": 0.00682335837169857,
      "grad_norm": 0.21358749270439148,
      "learning_rate": 0.00019867380176826432,
      "loss": 0.309,
      "step": 176
    },
    {
      "epoch": 0.006862127453355948,
      "grad_norm": 0.2544545829296112,
      "learning_rate": 0.0001986660462230495,
      "loss": 0.322,
      "step": 177
    },
    {
      "epoch": 0.006900896535013327,
      "grad_norm": 0.28094881772994995,
      "learning_rate": 0.00019865829067783466,
      "loss": 0.3945,
      "step": 178
    },
    {
      "epoch": 0.006939665616670705,
      "grad_norm": 0.22359661757946014,
      "learning_rate": 0.00019865053513261983,
      "loss": 0.3348,
      "step": 179
    },
    {
      "epoch": 0.006978434698328083,
      "grad_norm": 0.23866857588291168,
      "learning_rate": 0.000198642779587405,
      "loss": 0.3771,
      "step": 180
    },
    {
      "epoch": 0.007017203779985461,
      "grad_norm": 0.23260502517223358,
      "learning_rate": 0.00019863502404219015,
      "loss": 0.3714,
      "step": 181
    },
    {
      "epoch": 0.00705597286164284,
      "grad_norm": 0.35842689871788025,
      "learning_rate": 0.00019862726849697535,
      "loss": 0.4361,
      "step": 182
    },
    {
      "epoch": 0.007094741943300218,
      "grad_norm": 0.279440701007843,
      "learning_rate": 0.0001986195129517605,
      "loss": 0.3489,
      "step": 183
    },
    {
      "epoch": 0.007133511024957596,
      "grad_norm": 0.23785582184791565,
      "learning_rate": 0.0001986117574065457,
      "loss": 0.3855,
      "step": 184
    },
    {
      "epoch": 0.007172280106614974,
      "grad_norm": 0.3093741834163666,
      "learning_rate": 0.00019860400186133084,
      "loss": 0.3674,
      "step": 185
    },
    {
      "epoch": 0.007211049188272353,
      "grad_norm": 0.25695836544036865,
      "learning_rate": 0.00019859624631611604,
      "loss": 0.3549,
      "step": 186
    },
    {
      "epoch": 0.007249818269929731,
      "grad_norm": 0.21849408745765686,
      "learning_rate": 0.00019858849077090122,
      "loss": 0.3152,
      "step": 187
    },
    {
      "epoch": 0.007288587351587109,
      "grad_norm": 0.21231992542743683,
      "learning_rate": 0.00019858073522568636,
      "loss": 0.3166,
      "step": 188
    },
    {
      "epoch": 0.007327356433244487,
      "grad_norm": 0.20371273159980774,
      "learning_rate": 0.00019857297968047156,
      "loss": 0.3646,
      "step": 189
    },
    {
      "epoch": 0.007366125514901865,
      "grad_norm": 0.21982692182064056,
      "learning_rate": 0.0001985652241352567,
      "loss": 0.3519,
      "step": 190
    },
    {
      "epoch": 0.007404894596559244,
      "grad_norm": 0.26631149649620056,
      "learning_rate": 0.0001985574685900419,
      "loss": 0.3604,
      "step": 191
    },
    {
      "epoch": 0.007443663678216622,
      "grad_norm": 0.15002907812595367,
      "learning_rate": 0.00019854971304482705,
      "loss": 0.3247,
      "step": 192
    },
    {
      "epoch": 0.007482432759874,
      "grad_norm": 0.23978418111801147,
      "learning_rate": 0.00019854195749961225,
      "loss": 0.3531,
      "step": 193
    },
    {
      "epoch": 0.007521201841531378,
      "grad_norm": 0.2265661358833313,
      "learning_rate": 0.0001985342019543974,
      "loss": 0.3235,
      "step": 194
    },
    {
      "epoch": 0.007559970923188757,
      "grad_norm": 0.24001309275627136,
      "learning_rate": 0.00019852644640918257,
      "loss": 0.4128,
      "step": 195
    },
    {
      "epoch": 0.007598740004846135,
      "grad_norm": 0.30206188559532166,
      "learning_rate": 0.00019851869086396774,
      "loss": 0.2674,
      "step": 196
    },
    {
      "epoch": 0.007637509086503513,
      "grad_norm": 0.3277266025543213,
      "learning_rate": 0.00019851093531875292,
      "loss": 0.4339,
      "step": 197
    },
    {
      "epoch": 0.007676278168160891,
      "grad_norm": 0.23538357019424438,
      "learning_rate": 0.0001985031797735381,
      "loss": 0.4001,
      "step": 198
    },
    {
      "epoch": 0.00771504724981827,
      "grad_norm": 0.2776458263397217,
      "learning_rate": 0.00019849542422832326,
      "loss": 0.4194,
      "step": 199
    },
    {
      "epoch": 0.007753816331475648,
      "grad_norm": 0.22483037412166595,
      "learning_rate": 0.00019848766868310844,
      "loss": 0.4102,
      "step": 200
    },
    {
      "epoch": 0.007792585413133026,
      "grad_norm": 0.23312336206436157,
      "learning_rate": 0.0001984799131378936,
      "loss": 0.4197,
      "step": 201
    },
    {
      "epoch": 0.007831354494790405,
      "grad_norm": 0.2425794005393982,
      "learning_rate": 0.00019847215759267875,
      "loss": 0.3582,
      "step": 202
    },
    {
      "epoch": 0.007870123576447783,
      "grad_norm": 0.2374963015317917,
      "learning_rate": 0.00019846440204746395,
      "loss": 0.427,
      "step": 203
    },
    {
      "epoch": 0.007908892658105161,
      "grad_norm": 0.2443149983882904,
      "learning_rate": 0.0001984566465022491,
      "loss": 0.384,
      "step": 204
    },
    {
      "epoch": 0.00794766173976254,
      "grad_norm": 0.17869865894317627,
      "learning_rate": 0.0001984488909570343,
      "loss": 0.3407,
      "step": 205
    },
    {
      "epoch": 0.007986430821419917,
      "grad_norm": 0.24580666422843933,
      "learning_rate": 0.00019844113541181945,
      "loss": 0.4399,
      "step": 206
    },
    {
      "epoch": 0.008025199903077295,
      "grad_norm": 0.19430327415466309,
      "learning_rate": 0.00019843337986660465,
      "loss": 0.3219,
      "step": 207
    },
    {
      "epoch": 0.008063968984734673,
      "grad_norm": 0.18826711177825928,
      "learning_rate": 0.0001984256243213898,
      "loss": 0.401,
      "step": 208
    },
    {
      "epoch": 0.008102738066392052,
      "grad_norm": 0.17811612784862518,
      "learning_rate": 0.00019841786877617496,
      "loss": 0.3484,
      "step": 209
    },
    {
      "epoch": 0.008141507148049431,
      "grad_norm": 0.21373820304870605,
      "learning_rate": 0.00019841011323096014,
      "loss": 0.3871,
      "step": 210
    },
    {
      "epoch": 0.00818027622970681,
      "grad_norm": 0.14814259111881256,
      "learning_rate": 0.0001984023576857453,
      "loss": 0.3096,
      "step": 211
    },
    {
      "epoch": 0.008219045311364187,
      "grad_norm": 0.1660691499710083,
      "learning_rate": 0.00019839460214053048,
      "loss": 0.3096,
      "step": 212
    },
    {
      "epoch": 0.008257814393021565,
      "grad_norm": 0.17767572402954102,
      "learning_rate": 0.00019838684659531566,
      "loss": 0.3553,
      "step": 213
    },
    {
      "epoch": 0.008296583474678943,
      "grad_norm": 0.19122640788555145,
      "learning_rate": 0.00019837909105010083,
      "loss": 0.3132,
      "step": 214
    },
    {
      "epoch": 0.008335352556336321,
      "grad_norm": 0.2131112515926361,
      "learning_rate": 0.000198371335504886,
      "loss": 0.3667,
      "step": 215
    },
    {
      "epoch": 0.0083741216379937,
      "grad_norm": 0.18761901557445526,
      "learning_rate": 0.00019836357995967117,
      "loss": 0.3492,
      "step": 216
    },
    {
      "epoch": 0.008412890719651078,
      "grad_norm": 0.15587353706359863,
      "learning_rate": 0.00019835582441445635,
      "loss": 0.2288,
      "step": 217
    },
    {
      "epoch": 0.008451659801308457,
      "grad_norm": 0.17419500648975372,
      "learning_rate": 0.00019834806886924152,
      "loss": 0.3957,
      "step": 218
    },
    {
      "epoch": 0.008490428882965835,
      "grad_norm": 0.1698499470949173,
      "learning_rate": 0.0001983403133240267,
      "loss": 0.3066,
      "step": 219
    },
    {
      "epoch": 0.008529197964623213,
      "grad_norm": 0.22016595304012299,
      "learning_rate": 0.00019833255777881186,
      "loss": 0.3841,
      "step": 220
    },
    {
      "epoch": 0.008567967046280591,
      "grad_norm": 0.19667862355709076,
      "learning_rate": 0.00019832480223359704,
      "loss": 0.3685,
      "step": 221
    },
    {
      "epoch": 0.00860673612793797,
      "grad_norm": 0.16683275997638702,
      "learning_rate": 0.0001983170466883822,
      "loss": 0.3112,
      "step": 222
    },
    {
      "epoch": 0.008645505209595348,
      "grad_norm": 0.18448972702026367,
      "learning_rate": 0.00019830929114316736,
      "loss": 0.3305,
      "step": 223
    },
    {
      "epoch": 0.008684274291252726,
      "grad_norm": 0.1848680078983307,
      "learning_rate": 0.00019830153559795256,
      "loss": 0.3277,
      "step": 224
    },
    {
      "epoch": 0.008723043372910104,
      "grad_norm": 0.2399527132511139,
      "learning_rate": 0.0001982937800527377,
      "loss": 0.35,
      "step": 225
    },
    {
      "epoch": 0.008761812454567482,
      "grad_norm": 0.15571805834770203,
      "learning_rate": 0.0001982860245075229,
      "loss": 0.3054,
      "step": 226
    },
    {
      "epoch": 0.008800581536224861,
      "grad_norm": 0.17402401566505432,
      "learning_rate": 0.00019827826896230805,
      "loss": 0.2746,
      "step": 227
    },
    {
      "epoch": 0.00883935061788224,
      "grad_norm": 0.17548836767673492,
      "learning_rate": 0.00019827051341709325,
      "loss": 0.3351,
      "step": 228
    },
    {
      "epoch": 0.008878119699539617,
      "grad_norm": 0.1942170113325119,
      "learning_rate": 0.0001982627578718784,
      "loss": 0.3577,
      "step": 229
    },
    {
      "epoch": 0.008916888781196996,
      "grad_norm": 0.1753177046775818,
      "learning_rate": 0.00019825500232666357,
      "loss": 0.3122,
      "step": 230
    },
    {
      "epoch": 0.008955657862854374,
      "grad_norm": 0.2553737461566925,
      "learning_rate": 0.00019824724678144874,
      "loss": 0.2775,
      "step": 231
    },
    {
      "epoch": 0.008994426944511752,
      "grad_norm": 0.3430439829826355,
      "learning_rate": 0.0001982394912362339,
      "loss": 0.3819,
      "step": 232
    },
    {
      "epoch": 0.00903319602616913,
      "grad_norm": 0.16468292474746704,
      "learning_rate": 0.00019823173569101908,
      "loss": 0.2915,
      "step": 233
    },
    {
      "epoch": 0.009071965107826508,
      "grad_norm": 0.27156931161880493,
      "learning_rate": 0.00019822398014580426,
      "loss": 0.3251,
      "step": 234
    },
    {
      "epoch": 0.009110734189483887,
      "grad_norm": 0.1652126908302307,
      "learning_rate": 0.00019821622460058943,
      "loss": 0.3116,
      "step": 235
    },
    {
      "epoch": 0.009149503271141265,
      "grad_norm": 0.19468088448047638,
      "learning_rate": 0.0001982084690553746,
      "loss": 0.3723,
      "step": 236
    },
    {
      "epoch": 0.009188272352798644,
      "grad_norm": 0.26629719138145447,
      "learning_rate": 0.00019820071351015978,
      "loss": 0.4613,
      "step": 237
    },
    {
      "epoch": 0.009227041434456022,
      "grad_norm": 0.2159719616174698,
      "learning_rate": 0.00019819295796494495,
      "loss": 0.3142,
      "step": 238
    },
    {
      "epoch": 0.0092658105161134,
      "grad_norm": 0.28781548142433167,
      "learning_rate": 0.00019818520241973012,
      "loss": 0.4023,
      "step": 239
    },
    {
      "epoch": 0.009304579597770778,
      "grad_norm": 0.1749735176563263,
      "learning_rate": 0.0001981774468745153,
      "loss": 0.373,
      "step": 240
    },
    {
      "epoch": 0.009343348679428156,
      "grad_norm": 0.19041107594966888,
      "learning_rate": 0.00019816969132930047,
      "loss": 0.38,
      "step": 241
    },
    {
      "epoch": 0.009382117761085534,
      "grad_norm": 0.16356168687343597,
      "learning_rate": 0.00019816193578408564,
      "loss": 0.2764,
      "step": 242
    },
    {
      "epoch": 0.009420886842742912,
      "grad_norm": 0.14309971034526825,
      "learning_rate": 0.0001981541802388708,
      "loss": 0.2802,
      "step": 243
    },
    {
      "epoch": 0.009459655924400292,
      "grad_norm": 0.19768203794956207,
      "learning_rate": 0.00019814642469365596,
      "loss": 0.3786,
      "step": 244
    },
    {
      "epoch": 0.00949842500605767,
      "grad_norm": 0.20835228264331818,
      "learning_rate": 0.00019813866914844116,
      "loss": 0.3543,
      "step": 245
    },
    {
      "epoch": 0.009537194087715048,
      "grad_norm": 0.1827169805765152,
      "learning_rate": 0.0001981309136032263,
      "loss": 0.3318,
      "step": 246
    },
    {
      "epoch": 0.009575963169372426,
      "grad_norm": 0.15357844531536102,
      "learning_rate": 0.0001981231580580115,
      "loss": 0.329,
      "step": 247
    },
    {
      "epoch": 0.009614732251029804,
      "grad_norm": 0.17357362806797028,
      "learning_rate": 0.00019811540251279665,
      "loss": 0.3469,
      "step": 248
    },
    {
      "epoch": 0.009653501332687182,
      "grad_norm": 0.1783611923456192,
      "learning_rate": 0.00019810764696758185,
      "loss": 0.335,
      "step": 249
    },
    {
      "epoch": 0.00969227041434456,
      "grad_norm": 0.14683598279953003,
      "learning_rate": 0.000198099891422367,
      "loss": 0.2943,
      "step": 250
    },
    {
      "epoch": 0.009731039496001938,
      "grad_norm": 0.22065375745296478,
      "learning_rate": 0.00019809213587715217,
      "loss": 0.3443,
      "step": 251
    },
    {
      "epoch": 0.009769808577659316,
      "grad_norm": 0.22206880152225494,
      "learning_rate": 0.00019808438033193734,
      "loss": 0.4172,
      "step": 252
    },
    {
      "epoch": 0.009808577659316696,
      "grad_norm": 0.175215944647789,
      "learning_rate": 0.0001980766247867225,
      "loss": 0.3026,
      "step": 253
    },
    {
      "epoch": 0.009847346740974074,
      "grad_norm": 0.1968924105167389,
      "learning_rate": 0.00019806886924150769,
      "loss": 0.4053,
      "step": 254
    },
    {
      "epoch": 0.009886115822631452,
      "grad_norm": 0.17728610336780548,
      "learning_rate": 0.00019806111369629286,
      "loss": 0.3231,
      "step": 255
    },
    {
      "epoch": 0.00992488490428883,
      "grad_norm": 0.18522633612155914,
      "learning_rate": 0.00019805335815107803,
      "loss": 0.4608,
      "step": 256
    },
    {
      "epoch": 0.009963653985946208,
      "grad_norm": 0.22844792902469635,
      "learning_rate": 0.0001980456026058632,
      "loss": 0.3442,
      "step": 257
    },
    {
      "epoch": 0.010002423067603586,
      "grad_norm": 0.17206160724163055,
      "learning_rate": 0.00019803784706064835,
      "loss": 0.2479,
      "step": 258
    },
    {
      "epoch": 0.010041192149260964,
      "grad_norm": 0.1747860163450241,
      "learning_rate": 0.00019803009151543355,
      "loss": 0.3317,
      "step": 259
    },
    {
      "epoch": 0.010079961230918342,
      "grad_norm": 0.27453184127807617,
      "learning_rate": 0.0001980223359702187,
      "loss": 0.3766,
      "step": 260
    },
    {
      "epoch": 0.010118730312575722,
      "grad_norm": 0.1655982881784439,
      "learning_rate": 0.0001980145804250039,
      "loss": 0.2834,
      "step": 261
    },
    {
      "epoch": 0.0101574993942331,
      "grad_norm": 0.20382310450077057,
      "learning_rate": 0.00019800682487978904,
      "loss": 0.2617,
      "step": 262
    },
    {
      "epoch": 0.010196268475890478,
      "grad_norm": 0.13375471532344818,
      "learning_rate": 0.00019799906933457424,
      "loss": 0.295,
      "step": 263
    },
    {
      "epoch": 0.010235037557547856,
      "grad_norm": 0.19845730066299438,
      "learning_rate": 0.0001979913137893594,
      "loss": 0.3892,
      "step": 264
    },
    {
      "epoch": 0.010273806639205234,
      "grad_norm": 0.234327495098114,
      "learning_rate": 0.00019798355824414456,
      "loss": 0.3243,
      "step": 265
    },
    {
      "epoch": 0.010312575720862612,
      "grad_norm": 0.15347817540168762,
      "learning_rate": 0.00019797580269892976,
      "loss": 0.3425,
      "step": 266
    },
    {
      "epoch": 0.01035134480251999,
      "grad_norm": 0.18633626401424408,
      "learning_rate": 0.0001979680471537149,
      "loss": 0.3278,
      "step": 267
    },
    {
      "epoch": 0.010390113884177368,
      "grad_norm": 0.17555655539035797,
      "learning_rate": 0.0001979602916085001,
      "loss": 0.3666,
      "step": 268
    },
    {
      "epoch": 0.010428882965834746,
      "grad_norm": 0.16634850203990936,
      "learning_rate": 0.00019795253606328525,
      "loss": 0.3496,
      "step": 269
    },
    {
      "epoch": 0.010467652047492126,
      "grad_norm": 0.17517060041427612,
      "learning_rate": 0.00019794478051807045,
      "loss": 0.3821,
      "step": 270
    },
    {
      "epoch": 0.010506421129149504,
      "grad_norm": 0.15766176581382751,
      "learning_rate": 0.0001979370249728556,
      "loss": 0.2971,
      "step": 271
    },
    {
      "epoch": 0.010545190210806882,
      "grad_norm": 0.21017912030220032,
      "learning_rate": 0.00019792926942764077,
      "loss": 0.3785,
      "step": 272
    },
    {
      "epoch": 0.01058395929246426,
      "grad_norm": 0.1562787890434265,
      "learning_rate": 0.00019792151388242594,
      "loss": 0.2753,
      "step": 273
    },
    {
      "epoch": 0.010622728374121638,
      "grad_norm": 0.187228262424469,
      "learning_rate": 0.00019791375833721112,
      "loss": 0.3797,
      "step": 274
    },
    {
      "epoch": 0.010661497455779016,
      "grad_norm": 0.15154555439949036,
      "learning_rate": 0.0001979060027919963,
      "loss": 0.3775,
      "step": 275
    },
    {
      "epoch": 0.010700266537436394,
      "grad_norm": 0.24230250716209412,
      "learning_rate": 0.00019789824724678146,
      "loss": 0.3792,
      "step": 276
    },
    {
      "epoch": 0.010739035619093772,
      "grad_norm": 0.1980646699666977,
      "learning_rate": 0.00019789049170156663,
      "loss": 0.3705,
      "step": 277
    },
    {
      "epoch": 0.010777804700751152,
      "grad_norm": 0.16102230548858643,
      "learning_rate": 0.0001978827361563518,
      "loss": 0.3982,
      "step": 278
    },
    {
      "epoch": 0.01081657378240853,
      "grad_norm": 0.16752837598323822,
      "learning_rate": 0.00019787498061113695,
      "loss": 0.2635,
      "step": 279
    },
    {
      "epoch": 0.010855342864065908,
      "grad_norm": 0.14931409060955048,
      "learning_rate": 0.00019786722506592215,
      "loss": 0.2641,
      "step": 280
    },
    {
      "epoch": 0.010894111945723286,
      "grad_norm": 0.18310342729091644,
      "learning_rate": 0.0001978594695207073,
      "loss": 0.3631,
      "step": 281
    },
    {
      "epoch": 0.010932881027380664,
      "grad_norm": 0.19142034649848938,
      "learning_rate": 0.0001978517139754925,
      "loss": 0.3426,
      "step": 282
    },
    {
      "epoch": 0.010971650109038042,
      "grad_norm": 0.22250211238861084,
      "learning_rate": 0.00019784395843027764,
      "loss": 0.4223,
      "step": 283
    },
    {
      "epoch": 0.01101041919069542,
      "grad_norm": 0.1315949708223343,
      "learning_rate": 0.00019783620288506284,
      "loss": 0.2805,
      "step": 284
    },
    {
      "epoch": 0.011049188272352798,
      "grad_norm": 0.1983024626970291,
      "learning_rate": 0.000197828447339848,
      "loss": 0.3607,
      "step": 285
    },
    {
      "epoch": 0.011087957354010176,
      "grad_norm": 0.18783748149871826,
      "learning_rate": 0.00019782069179463316,
      "loss": 0.3468,
      "step": 286
    },
    {
      "epoch": 0.011126726435667556,
      "grad_norm": 0.17191797494888306,
      "learning_rate": 0.00019781293624941833,
      "loss": 0.3353,
      "step": 287
    },
    {
      "epoch": 0.011165495517324934,
      "grad_norm": 0.1678626388311386,
      "learning_rate": 0.0001978051807042035,
      "loss": 0.3243,
      "step": 288
    },
    {
      "epoch": 0.011204264598982312,
      "grad_norm": 0.15798453986644745,
      "learning_rate": 0.00019779742515898868,
      "loss": 0.286,
      "step": 289
    },
    {
      "epoch": 0.01124303368063969,
      "grad_norm": 0.19910366833209991,
      "learning_rate": 0.00019778966961377385,
      "loss": 0.315,
      "step": 290
    },
    {
      "epoch": 0.011281802762297068,
      "grad_norm": 0.14746172726154327,
      "learning_rate": 0.00019778191406855903,
      "loss": 0.2821,
      "step": 291
    },
    {
      "epoch": 0.011320571843954446,
      "grad_norm": 0.17933209240436554,
      "learning_rate": 0.0001977741585233442,
      "loss": 0.2993,
      "step": 292
    },
    {
      "epoch": 0.011359340925611824,
      "grad_norm": 0.2179802805185318,
      "learning_rate": 0.00019776640297812937,
      "loss": 0.4491,
      "step": 293
    },
    {
      "epoch": 0.011398110007269202,
      "grad_norm": 0.2650682330131531,
      "learning_rate": 0.00019775864743291454,
      "loss": 0.3999,
      "step": 294
    },
    {
      "epoch": 0.011436879088926582,
      "grad_norm": 0.39750441908836365,
      "learning_rate": 0.00019775089188769972,
      "loss": 0.2923,
      "step": 295
    },
    {
      "epoch": 0.01147564817058396,
      "grad_norm": 0.2048538476228714,
      "learning_rate": 0.0001977431363424849,
      "loss": 0.3767,
      "step": 296
    },
    {
      "epoch": 0.011514417252241338,
      "grad_norm": 0.23770080506801605,
      "learning_rate": 0.00019773538079727006,
      "loss": 0.3324,
      "step": 297
    },
    {
      "epoch": 0.011553186333898716,
      "grad_norm": 0.2167208045721054,
      "learning_rate": 0.00019772762525205524,
      "loss": 0.387,
      "step": 298
    },
    {
      "epoch": 0.011591955415556094,
      "grad_norm": 0.21112513542175293,
      "learning_rate": 0.0001977198697068404,
      "loss": 0.3602,
      "step": 299
    },
    {
      "epoch": 0.011630724497213472,
      "grad_norm": 0.2109885811805725,
      "learning_rate": 0.00019771211416162555,
      "loss": 0.3423,
      "step": 300
    },
    {
      "epoch": 0.01166949357887085,
      "grad_norm": 0.16143456101417542,
      "learning_rate": 0.00019770435861641075,
      "loss": 0.2867,
      "step": 301
    },
    {
      "epoch": 0.011708262660528228,
      "grad_norm": 0.17057301104068756,
      "learning_rate": 0.0001976966030711959,
      "loss": 0.2888,
      "step": 302
    },
    {
      "epoch": 0.011747031742185606,
      "grad_norm": 0.14760105311870575,
      "learning_rate": 0.0001976888475259811,
      "loss": 0.2664,
      "step": 303
    },
    {
      "epoch": 0.011785800823842986,
      "grad_norm": 0.1633409559726715,
      "learning_rate": 0.00019768109198076625,
      "loss": 0.2907,
      "step": 304
    },
    {
      "epoch": 0.011824569905500364,
      "grad_norm": 0.17707787454128265,
      "learning_rate": 0.00019767333643555145,
      "loss": 0.3252,
      "step": 305
    },
    {
      "epoch": 0.011863338987157742,
      "grad_norm": 0.19688111543655396,
      "learning_rate": 0.0001976655808903366,
      "loss": 0.3371,
      "step": 306
    },
    {
      "epoch": 0.01190210806881512,
      "grad_norm": 0.19473734498023987,
      "learning_rate": 0.00019765782534512176,
      "loss": 0.3798,
      "step": 307
    },
    {
      "epoch": 0.011940877150472498,
      "grad_norm": 0.14888896048069,
      "learning_rate": 0.00019765006979990694,
      "loss": 0.2537,
      "step": 308
    },
    {
      "epoch": 0.011979646232129876,
      "grad_norm": 0.1824783831834793,
      "learning_rate": 0.0001976423142546921,
      "loss": 0.402,
      "step": 309
    },
    {
      "epoch": 0.012018415313787254,
      "grad_norm": 0.17040567100048065,
      "learning_rate": 0.00019763455870947728,
      "loss": 0.342,
      "step": 310
    },
    {
      "epoch": 0.012057184395444632,
      "grad_norm": 0.1792033314704895,
      "learning_rate": 0.00019762680316426245,
      "loss": 0.3741,
      "step": 311
    },
    {
      "epoch": 0.012095953477102012,
      "grad_norm": 0.14477096498012543,
      "learning_rate": 0.00019761904761904763,
      "loss": 0.3898,
      "step": 312
    },
    {
      "epoch": 0.01213472255875939,
      "grad_norm": 0.18197011947631836,
      "learning_rate": 0.0001976112920738328,
      "loss": 0.3575,
      "step": 313
    },
    {
      "epoch": 0.012173491640416768,
      "grad_norm": 0.25540998578071594,
      "learning_rate": 0.00019760353652861797,
      "loss": 0.4476,
      "step": 314
    },
    {
      "epoch": 0.012212260722074146,
      "grad_norm": 0.16906523704528809,
      "learning_rate": 0.00019759578098340315,
      "loss": 0.3852,
      "step": 315
    },
    {
      "epoch": 0.012251029803731524,
      "grad_norm": 0.1891743540763855,
      "learning_rate": 0.00019758802543818832,
      "loss": 0.3941,
      "step": 316
    },
    {
      "epoch": 0.012289798885388902,
      "grad_norm": 0.21310895681381226,
      "learning_rate": 0.0001975802698929735,
      "loss": 0.2448,
      "step": 317
    },
    {
      "epoch": 0.01232856796704628,
      "grad_norm": 0.18495100736618042,
      "learning_rate": 0.00019757251434775866,
      "loss": 0.2787,
      "step": 318
    },
    {
      "epoch": 0.012367337048703658,
      "grad_norm": 0.13902883231639862,
      "learning_rate": 0.00019756475880254384,
      "loss": 0.2582,
      "step": 319
    },
    {
      "epoch": 0.012406106130361036,
      "grad_norm": 0.23445065319538116,
      "learning_rate": 0.000197557003257329,
      "loss": 0.4025,
      "step": 320
    },
    {
      "epoch": 0.012444875212018416,
      "grad_norm": 0.15872922539710999,
      "learning_rate": 0.00019754924771211416,
      "loss": 0.302,
      "step": 321
    },
    {
      "epoch": 0.012483644293675794,
      "grad_norm": 0.17622597515583038,
      "learning_rate": 0.00019754149216689936,
      "loss": 0.371,
      "step": 322
    },
    {
      "epoch": 0.012522413375333172,
      "grad_norm": 0.1626409888267517,
      "learning_rate": 0.0001975337366216845,
      "loss": 0.3547,
      "step": 323
    },
    {
      "epoch": 0.01256118245699055,
      "grad_norm": 0.1293002814054489,
      "learning_rate": 0.0001975259810764697,
      "loss": 0.2844,
      "step": 324
    },
    {
      "epoch": 0.012599951538647928,
      "grad_norm": 0.16771991550922394,
      "learning_rate": 0.00019751822553125485,
      "loss": 0.2798,
      "step": 325
    },
    {
      "epoch": 0.012638720620305306,
      "grad_norm": 0.18062721192836761,
      "learning_rate": 0.00019751046998604005,
      "loss": 0.3056,
      "step": 326
    },
    {
      "epoch": 0.012677489701962684,
      "grad_norm": 0.1534145027399063,
      "learning_rate": 0.0001975027144408252,
      "loss": 0.3746,
      "step": 327
    },
    {
      "epoch": 0.012716258783620062,
      "grad_norm": 0.17321977019309998,
      "learning_rate": 0.00019749495889561037,
      "loss": 0.3724,
      "step": 328
    },
    {
      "epoch": 0.012755027865277442,
      "grad_norm": 0.13039974868297577,
      "learning_rate": 0.00019748720335039554,
      "loss": 0.2945,
      "step": 329
    },
    {
      "epoch": 0.01279379694693482,
      "grad_norm": 0.1825912743806839,
      "learning_rate": 0.0001974794478051807,
      "loss": 0.3845,
      "step": 330
    },
    {
      "epoch": 0.012832566028592198,
      "grad_norm": 0.15827584266662598,
      "learning_rate": 0.00019747169225996588,
      "loss": 0.3069,
      "step": 331
    },
    {
      "epoch": 0.012871335110249576,
      "grad_norm": 0.16109423339366913,
      "learning_rate": 0.00019746393671475106,
      "loss": 0.3511,
      "step": 332
    },
    {
      "epoch": 0.012910104191906954,
      "grad_norm": 0.15195219218730927,
      "learning_rate": 0.00019745618116953623,
      "loss": 0.3644,
      "step": 333
    },
    {
      "epoch": 0.012948873273564332,
      "grad_norm": 0.20042744278907776,
      "learning_rate": 0.0001974484256243214,
      "loss": 0.2948,
      "step": 334
    },
    {
      "epoch": 0.01298764235522171,
      "grad_norm": 0.1693444401025772,
      "learning_rate": 0.00019744067007910655,
      "loss": 0.3109,
      "step": 335
    },
    {
      "epoch": 0.013026411436879088,
      "grad_norm": 0.1740037053823471,
      "learning_rate": 0.00019743291453389175,
      "loss": 0.4469,
      "step": 336
    },
    {
      "epoch": 0.013065180518536466,
      "grad_norm": 0.15699641406536102,
      "learning_rate": 0.0001974251589886769,
      "loss": 0.3549,
      "step": 337
    },
    {
      "epoch": 0.013103949600193846,
      "grad_norm": 0.12952309846878052,
      "learning_rate": 0.0001974174034434621,
      "loss": 0.2609,
      "step": 338
    },
    {
      "epoch": 0.013142718681851224,
      "grad_norm": 0.15725642442703247,
      "learning_rate": 0.00019740964789824724,
      "loss": 0.2554,
      "step": 339
    },
    {
      "epoch": 0.013181487763508602,
      "grad_norm": 0.17892472445964813,
      "learning_rate": 0.00019740189235303244,
      "loss": 0.3593,
      "step": 340
    },
    {
      "epoch": 0.01322025684516598,
      "grad_norm": 0.10841938108205795,
      "learning_rate": 0.00019739413680781758,
      "loss": 0.2245,
      "step": 341
    },
    {
      "epoch": 0.013259025926823358,
      "grad_norm": 0.13952043652534485,
      "learning_rate": 0.00019738638126260276,
      "loss": 0.303,
      "step": 342
    },
    {
      "epoch": 0.013297795008480736,
      "grad_norm": 0.17459259927272797,
      "learning_rate": 0.00019737862571738793,
      "loss": 0.1979,
      "step": 343
    },
    {
      "epoch": 0.013336564090138114,
      "grad_norm": 0.14991183578968048,
      "learning_rate": 0.0001973708701721731,
      "loss": 0.3705,
      "step": 344
    },
    {
      "epoch": 0.013375333171795492,
      "grad_norm": 0.1718611717224121,
      "learning_rate": 0.0001973631146269583,
      "loss": 0.3491,
      "step": 345
    },
    {
      "epoch": 0.013414102253452872,
      "grad_norm": 0.16202306747436523,
      "learning_rate": 0.00019735535908174345,
      "loss": 0.3625,
      "step": 346
    },
    {
      "epoch": 0.01345287133511025,
      "grad_norm": 0.15790744125843048,
      "learning_rate": 0.00019734760353652865,
      "loss": 0.4113,
      "step": 347
    },
    {
      "epoch": 0.013491640416767628,
      "grad_norm": 0.17096927762031555,
      "learning_rate": 0.0001973398479913138,
      "loss": 0.254,
      "step": 348
    },
    {
      "epoch": 0.013530409498425006,
      "grad_norm": 0.1469568908214569,
      "learning_rate": 0.00019733209244609897,
      "loss": 0.3018,
      "step": 349
    },
    {
      "epoch": 0.013569178580082384,
      "grad_norm": 0.18193010985851288,
      "learning_rate": 0.00019732433690088414,
      "loss": 0.2983,
      "step": 350
    },
    {
      "epoch": 0.013607947661739762,
      "grad_norm": 0.171823188662529,
      "learning_rate": 0.0001973165813556693,
      "loss": 0.3478,
      "step": 351
    },
    {
      "epoch": 0.01364671674339714,
      "grad_norm": 0.17279936373233795,
      "learning_rate": 0.00019730882581045449,
      "loss": 0.3802,
      "step": 352
    },
    {
      "epoch": 0.013685485825054518,
      "grad_norm": 0.143559992313385,
      "learning_rate": 0.00019730107026523966,
      "loss": 0.4014,
      "step": 353
    },
    {
      "epoch": 0.013724254906711897,
      "grad_norm": 0.14849218726158142,
      "learning_rate": 0.00019729331472002483,
      "loss": 0.4164,
      "step": 354
    },
    {
      "epoch": 0.013763023988369276,
      "grad_norm": 0.18447205424308777,
      "learning_rate": 0.00019728555917481,
      "loss": 0.3852,
      "step": 355
    },
    {
      "epoch": 0.013801793070026654,
      "grad_norm": 0.13087542355060577,
      "learning_rate": 0.00019727780362959515,
      "loss": 0.2702,
      "step": 356
    },
    {
      "epoch": 0.013840562151684032,
      "grad_norm": 0.17141889035701752,
      "learning_rate": 0.00019727004808438035,
      "loss": 0.3041,
      "step": 357
    },
    {
      "epoch": 0.01387933123334141,
      "grad_norm": 0.1346399188041687,
      "learning_rate": 0.0001972622925391655,
      "loss": 0.3317,
      "step": 358
    },
    {
      "epoch": 0.013918100314998788,
      "grad_norm": 0.15137608349323273,
      "learning_rate": 0.0001972545369939507,
      "loss": 0.3391,
      "step": 359
    },
    {
      "epoch": 0.013956869396656166,
      "grad_norm": 0.17044253647327423,
      "learning_rate": 0.00019724678144873584,
      "loss": 0.34,
      "step": 360
    },
    {
      "epoch": 0.013995638478313545,
      "grad_norm": 0.13251839578151703,
      "learning_rate": 0.00019723902590352104,
      "loss": 0.3125,
      "step": 361
    },
    {
      "epoch": 0.014034407559970923,
      "grad_norm": 0.20663651823997498,
      "learning_rate": 0.0001972312703583062,
      "loss": 0.4761,
      "step": 362
    },
    {
      "epoch": 0.0140731766416283,
      "grad_norm": 0.14277100563049316,
      "learning_rate": 0.00019722351481309136,
      "loss": 0.3136,
      "step": 363
    },
    {
      "epoch": 0.01411194572328568,
      "grad_norm": 0.1627928465604782,
      "learning_rate": 0.00019721575926787653,
      "loss": 0.2542,
      "step": 364
    },
    {
      "epoch": 0.014150714804943058,
      "grad_norm": 0.13414818048477173,
      "learning_rate": 0.0001972080037226617,
      "loss": 0.3094,
      "step": 365
    },
    {
      "epoch": 0.014189483886600436,
      "grad_norm": 0.1463170051574707,
      "learning_rate": 0.00019720024817744688,
      "loss": 0.3428,
      "step": 366
    },
    {
      "epoch": 0.014228252968257814,
      "grad_norm": 0.1298328936100006,
      "learning_rate": 0.00019719249263223205,
      "loss": 0.297,
      "step": 367
    },
    {
      "epoch": 0.014267022049915193,
      "grad_norm": 0.18295900523662567,
      "learning_rate": 0.00019718473708701722,
      "loss": 0.3135,
      "step": 368
    },
    {
      "epoch": 0.01430579113157257,
      "grad_norm": 0.15160812437534332,
      "learning_rate": 0.0001971769815418024,
      "loss": 0.3059,
      "step": 369
    },
    {
      "epoch": 0.014344560213229949,
      "grad_norm": 0.4178626835346222,
      "learning_rate": 0.00019716922599658757,
      "loss": 0.3444,
      "step": 370
    },
    {
      "epoch": 0.014383329294887327,
      "grad_norm": 0.22701288759708405,
      "learning_rate": 0.00019716147045137274,
      "loss": 0.314,
      "step": 371
    },
    {
      "epoch": 0.014422098376544706,
      "grad_norm": 4.2072601318359375,
      "learning_rate": 0.00019715371490615791,
      "loss": 0.3212,
      "step": 372
    },
    {
      "epoch": 0.014460867458202084,
      "grad_norm": 0.1444479525089264,
      "learning_rate": 0.0001971459593609431,
      "loss": 0.3968,
      "step": 373
    },
    {
      "epoch": 0.014499636539859462,
      "grad_norm": 0.46109452843666077,
      "learning_rate": 0.00019713820381572826,
      "loss": 0.286,
      "step": 374
    },
    {
      "epoch": 0.01453840562151684,
      "grad_norm": 0.27657434344291687,
      "learning_rate": 0.00019713044827051343,
      "loss": 0.2882,
      "step": 375
    },
    {
      "epoch": 0.014577174703174219,
      "grad_norm": 0.1620926856994629,
      "learning_rate": 0.0001971226927252986,
      "loss": 0.2686,
      "step": 376
    },
    {
      "epoch": 0.014615943784831597,
      "grad_norm": 0.1863682121038437,
      "learning_rate": 0.00019711493718008375,
      "loss": 0.3335,
      "step": 377
    },
    {
      "epoch": 0.014654712866488975,
      "grad_norm": 0.3389407694339752,
      "learning_rate": 0.00019710718163486895,
      "loss": 0.3859,
      "step": 378
    },
    {
      "epoch": 0.014693481948146353,
      "grad_norm": 0.2953808903694153,
      "learning_rate": 0.0001970994260896541,
      "loss": 0.2865,
      "step": 379
    },
    {
      "epoch": 0.01473225102980373,
      "grad_norm": 0.1781793236732483,
      "learning_rate": 0.0001970916705444393,
      "loss": 0.2588,
      "step": 380
    },
    {
      "epoch": 0.01477102011146111,
      "grad_norm": 0.21816477179527283,
      "learning_rate": 0.00019708391499922444,
      "loss": 0.3586,
      "step": 381
    },
    {
      "epoch": 0.014809789193118489,
      "grad_norm": 0.1896308809518814,
      "learning_rate": 0.00019707615945400964,
      "loss": 0.2902,
      "step": 382
    },
    {
      "epoch": 0.014848558274775867,
      "grad_norm": 0.19732654094696045,
      "learning_rate": 0.0001970684039087948,
      "loss": 0.3138,
      "step": 383
    },
    {
      "epoch": 0.014887327356433245,
      "grad_norm": 0.1830669343471527,
      "learning_rate": 0.00019706064836357996,
      "loss": 0.305,
      "step": 384
    },
    {
      "epoch": 0.014926096438090623,
      "grad_norm": 0.15074564516544342,
      "learning_rate": 0.00019705289281836513,
      "loss": 0.3441,
      "step": 385
    },
    {
      "epoch": 0.014964865519748,
      "grad_norm": 0.1711883693933487,
      "learning_rate": 0.0001970451372731503,
      "loss": 0.3178,
      "step": 386
    },
    {
      "epoch": 0.015003634601405379,
      "grad_norm": 0.15485863387584686,
      "learning_rate": 0.00019703738172793548,
      "loss": 0.3019,
      "step": 387
    },
    {
      "epoch": 0.015042403683062757,
      "grad_norm": 0.1592789590358734,
      "learning_rate": 0.00019702962618272065,
      "loss": 0.2968,
      "step": 388
    },
    {
      "epoch": 0.015081172764720137,
      "grad_norm": 0.17003025114536285,
      "learning_rate": 0.00019702187063750583,
      "loss": 0.2776,
      "step": 389
    },
    {
      "epoch": 0.015119941846377515,
      "grad_norm": 0.18380650877952576,
      "learning_rate": 0.000197014115092291,
      "loss": 0.3752,
      "step": 390
    },
    {
      "epoch": 0.015158710928034893,
      "grad_norm": 0.12687113881111145,
      "learning_rate": 0.00019700635954707617,
      "loss": 0.267,
      "step": 391
    },
    {
      "epoch": 0.01519748000969227,
      "grad_norm": 0.18678773939609528,
      "learning_rate": 0.00019699860400186134,
      "loss": 0.3703,
      "step": 392
    },
    {
      "epoch": 0.015236249091349649,
      "grad_norm": 0.13310611248016357,
      "learning_rate": 0.00019699084845664652,
      "loss": 0.2528,
      "step": 393
    },
    {
      "epoch": 0.015275018173007027,
      "grad_norm": 0.14297664165496826,
      "learning_rate": 0.0001969830929114317,
      "loss": 0.2971,
      "step": 394
    },
    {
      "epoch": 0.015313787254664405,
      "grad_norm": 0.1365012526512146,
      "learning_rate": 0.00019697533736621686,
      "loss": 0.3633,
      "step": 395
    },
    {
      "epoch": 0.015352556336321783,
      "grad_norm": 0.11892976611852646,
      "learning_rate": 0.00019696758182100203,
      "loss": 0.2151,
      "step": 396
    },
    {
      "epoch": 0.01539132541797916,
      "grad_norm": 0.17364825308322906,
      "learning_rate": 0.0001969598262757872,
      "loss": 0.4132,
      "step": 397
    },
    {
      "epoch": 0.01543009449963654,
      "grad_norm": 0.15145424008369446,
      "learning_rate": 0.00019695207073057235,
      "loss": 0.3646,
      "step": 398
    },
    {
      "epoch": 0.015468863581293919,
      "grad_norm": 0.1389559805393219,
      "learning_rate": 0.00019694431518535755,
      "loss": 0.2742,
      "step": 399
    },
    {
      "epoch": 0.015507632662951297,
      "grad_norm": 0.20473559200763702,
      "learning_rate": 0.0001969365596401427,
      "loss": 0.2589,
      "step": 400
    },
    {
      "epoch": 0.015546401744608675,
      "grad_norm": 0.13872358202934265,
      "learning_rate": 0.0001969288040949279,
      "loss": 0.3118,
      "step": 401
    },
    {
      "epoch": 0.015585170826266053,
      "grad_norm": 0.14497703313827515,
      "learning_rate": 0.00019692104854971304,
      "loss": 0.356,
      "step": 402
    },
    {
      "epoch": 0.01562393990792343,
      "grad_norm": 0.15379808843135834,
      "learning_rate": 0.00019691329300449824,
      "loss": 0.3221,
      "step": 403
    },
    {
      "epoch": 0.01566270898958081,
      "grad_norm": 0.1287560611963272,
      "learning_rate": 0.0001969055374592834,
      "loss": 0.2963,
      "step": 404
    },
    {
      "epoch": 0.015701478071238187,
      "grad_norm": 0.13889673352241516,
      "learning_rate": 0.00019689778191406856,
      "loss": 0.2836,
      "step": 405
    },
    {
      "epoch": 0.015740247152895567,
      "grad_norm": 0.23568636178970337,
      "learning_rate": 0.00019689002636885374,
      "loss": 0.3633,
      "step": 406
    },
    {
      "epoch": 0.015779016234552943,
      "grad_norm": 0.20410463213920593,
      "learning_rate": 0.0001968822708236389,
      "loss": 0.4038,
      "step": 407
    },
    {
      "epoch": 0.015817785316210323,
      "grad_norm": 0.23576930165290833,
      "learning_rate": 0.00019687451527842408,
      "loss": 0.3234,
      "step": 408
    },
    {
      "epoch": 0.0158565543978677,
      "grad_norm": 0.14117656648159027,
      "learning_rate": 0.00019686675973320925,
      "loss": 0.2376,
      "step": 409
    },
    {
      "epoch": 0.01589532347952508,
      "grad_norm": 0.23222610354423523,
      "learning_rate": 0.00019685900418799443,
      "loss": 0.3052,
      "step": 410
    },
    {
      "epoch": 0.01593409256118246,
      "grad_norm": 0.13417376577854156,
      "learning_rate": 0.0001968512486427796,
      "loss": 0.3504,
      "step": 411
    },
    {
      "epoch": 0.015972861642839835,
      "grad_norm": 0.20743921399116516,
      "learning_rate": 0.00019684349309756475,
      "loss": 0.3812,
      "step": 412
    },
    {
      "epoch": 0.016011630724497215,
      "grad_norm": 0.152695432305336,
      "learning_rate": 0.00019683573755234995,
      "loss": 0.3319,
      "step": 413
    },
    {
      "epoch": 0.01605039980615459,
      "grad_norm": 0.1172674149274826,
      "learning_rate": 0.0001968279820071351,
      "loss": 0.3222,
      "step": 414
    },
    {
      "epoch": 0.01608916888781197,
      "grad_norm": 0.15249352157115936,
      "learning_rate": 0.0001968202264619203,
      "loss": 0.3393,
      "step": 415
    },
    {
      "epoch": 0.016127937969469347,
      "grad_norm": 0.17346139252185822,
      "learning_rate": 0.00019681247091670544,
      "loss": 0.3137,
      "step": 416
    },
    {
      "epoch": 0.016166707051126727,
      "grad_norm": 0.1215927004814148,
      "learning_rate": 0.00019680471537149064,
      "loss": 0.2773,
      "step": 417
    },
    {
      "epoch": 0.016205476132784103,
      "grad_norm": 0.1866036355495453,
      "learning_rate": 0.00019679695982627578,
      "loss": 0.3991,
      "step": 418
    },
    {
      "epoch": 0.016244245214441483,
      "grad_norm": 0.13450121879577637,
      "learning_rate": 0.00019678920428106096,
      "loss": 0.3733,
      "step": 419
    },
    {
      "epoch": 0.016283014296098863,
      "grad_norm": 0.14097674190998077,
      "learning_rate": 0.00019678144873584613,
      "loss": 0.3737,
      "step": 420
    },
    {
      "epoch": 0.01632178337775624,
      "grad_norm": 0.1487146019935608,
      "learning_rate": 0.0001967736931906313,
      "loss": 0.2737,
      "step": 421
    },
    {
      "epoch": 0.01636055245941362,
      "grad_norm": 0.15188582241535187,
      "learning_rate": 0.00019676593764541647,
      "loss": 0.3997,
      "step": 422
    },
    {
      "epoch": 0.016399321541070995,
      "grad_norm": 0.148170605301857,
      "learning_rate": 0.00019675818210020165,
      "loss": 0.411,
      "step": 423
    },
    {
      "epoch": 0.016438090622728375,
      "grad_norm": 0.16563136875629425,
      "learning_rate": 0.00019675042655498685,
      "loss": 0.3869,
      "step": 424
    },
    {
      "epoch": 0.01647685970438575,
      "grad_norm": 0.12495933473110199,
      "learning_rate": 0.000196742671009772,
      "loss": 0.3035,
      "step": 425
    },
    {
      "epoch": 0.01651562878604313,
      "grad_norm": 0.11250076442956924,
      "learning_rate": 0.00019673491546455716,
      "loss": 0.267,
      "step": 426
    },
    {
      "epoch": 0.016554397867700507,
      "grad_norm": 0.1906145066022873,
      "learning_rate": 0.00019672715991934234,
      "loss": 0.24,
      "step": 427
    },
    {
      "epoch": 0.016593166949357887,
      "grad_norm": 0.1340598315000534,
      "learning_rate": 0.0001967194043741275,
      "loss": 0.2978,
      "step": 428
    },
    {
      "epoch": 0.016631936031015267,
      "grad_norm": 0.17734038829803467,
      "learning_rate": 0.00019671164882891268,
      "loss": 0.3504,
      "step": 429
    },
    {
      "epoch": 0.016670705112672643,
      "grad_norm": 0.17504557967185974,
      "learning_rate": 0.00019670389328369786,
      "loss": 0.3364,
      "step": 430
    },
    {
      "epoch": 0.016709474194330023,
      "grad_norm": 0.15641652047634125,
      "learning_rate": 0.00019669613773848303,
      "loss": 0.3275,
      "step": 431
    },
    {
      "epoch": 0.0167482432759874,
      "grad_norm": 0.1359223872423172,
      "learning_rate": 0.0001966883821932682,
      "loss": 0.2901,
      "step": 432
    },
    {
      "epoch": 0.01678701235764478,
      "grad_norm": 0.1247914656996727,
      "learning_rate": 0.00019668062664805335,
      "loss": 0.3211,
      "step": 433
    },
    {
      "epoch": 0.016825781439302155,
      "grad_norm": 0.182841956615448,
      "learning_rate": 0.00019667287110283855,
      "loss": 0.2943,
      "step": 434
    },
    {
      "epoch": 0.016864550520959535,
      "grad_norm": 0.12591026723384857,
      "learning_rate": 0.0001966651155576237,
      "loss": 0.3056,
      "step": 435
    },
    {
      "epoch": 0.016903319602616915,
      "grad_norm": 0.17070595920085907,
      "learning_rate": 0.0001966573600124089,
      "loss": 0.3261,
      "step": 436
    },
    {
      "epoch": 0.01694208868427429,
      "grad_norm": 0.11679597198963165,
      "learning_rate": 0.00019664960446719404,
      "loss": 0.2997,
      "step": 437
    },
    {
      "epoch": 0.01698085776593167,
      "grad_norm": 0.1818968951702118,
      "learning_rate": 0.00019664184892197924,
      "loss": 0.3805,
      "step": 438
    },
    {
      "epoch": 0.017019626847589047,
      "grad_norm": 0.10652333498001099,
      "learning_rate": 0.00019663409337676438,
      "loss": 0.2274,
      "step": 439
    },
    {
      "epoch": 0.017058395929246427,
      "grad_norm": 0.13854193687438965,
      "learning_rate": 0.00019662633783154956,
      "loss": 0.3703,
      "step": 440
    },
    {
      "epoch": 0.017097165010903803,
      "grad_norm": 0.14380992949008942,
      "learning_rate": 0.00019661858228633473,
      "loss": 0.2861,
      "step": 441
    },
    {
      "epoch": 0.017135934092561183,
      "grad_norm": 0.1269422024488449,
      "learning_rate": 0.0001966108267411199,
      "loss": 0.3638,
      "step": 442
    },
    {
      "epoch": 0.01717470317421856,
      "grad_norm": 0.12065412104129791,
      "learning_rate": 0.00019660307119590508,
      "loss": 0.2765,
      "step": 443
    },
    {
      "epoch": 0.01721347225587594,
      "grad_norm": 0.11309611797332764,
      "learning_rate": 0.00019659531565069025,
      "loss": 0.2974,
      "step": 444
    },
    {
      "epoch": 0.01725224133753332,
      "grad_norm": 0.12600910663604736,
      "learning_rate": 0.00019658756010547542,
      "loss": 0.3662,
      "step": 445
    },
    {
      "epoch": 0.017291010419190695,
      "grad_norm": 0.11566796898841858,
      "learning_rate": 0.0001965798045602606,
      "loss": 0.3736,
      "step": 446
    },
    {
      "epoch": 0.017329779500848075,
      "grad_norm": 0.1661793291568756,
      "learning_rate": 0.00019657204901504577,
      "loss": 0.335,
      "step": 447
    },
    {
      "epoch": 0.01736854858250545,
      "grad_norm": 0.11099682748317719,
      "learning_rate": 0.00019656429346983094,
      "loss": 0.2729,
      "step": 448
    },
    {
      "epoch": 0.01740731766416283,
      "grad_norm": 0.16623780131340027,
      "learning_rate": 0.0001965565379246161,
      "loss": 0.3343,
      "step": 449
    },
    {
      "epoch": 0.017446086745820207,
      "grad_norm": 0.17180109024047852,
      "learning_rate": 0.00019654878237940129,
      "loss": 0.2921,
      "step": 450
    },
    {
      "epoch": 0.017484855827477587,
      "grad_norm": 0.1504889875650406,
      "learning_rate": 0.00019654102683418646,
      "loss": 0.2741,
      "step": 451
    },
    {
      "epoch": 0.017523624909134963,
      "grad_norm": 0.10422014445066452,
      "learning_rate": 0.00019653327128897163,
      "loss": 0.2226,
      "step": 452
    },
    {
      "epoch": 0.017562393990792343,
      "grad_norm": 0.14729394018650055,
      "learning_rate": 0.0001965255157437568,
      "loss": 0.3405,
      "step": 453
    },
    {
      "epoch": 0.017601163072449723,
      "grad_norm": 0.1776709109544754,
      "learning_rate": 0.00019651776019854195,
      "loss": 0.3777,
      "step": 454
    },
    {
      "epoch": 0.0176399321541071,
      "grad_norm": 0.13256187736988068,
      "learning_rate": 0.00019651000465332715,
      "loss": 0.3678,
      "step": 455
    },
    {
      "epoch": 0.01767870123576448,
      "grad_norm": 0.1357676386833191,
      "learning_rate": 0.0001965022491081123,
      "loss": 0.3796,
      "step": 456
    },
    {
      "epoch": 0.017717470317421855,
      "grad_norm": 0.15299804508686066,
      "learning_rate": 0.0001964944935628975,
      "loss": 0.3233,
      "step": 457
    },
    {
      "epoch": 0.017756239399079235,
      "grad_norm": 0.14166061580181122,
      "learning_rate": 0.00019648673801768264,
      "loss": 0.3091,
      "step": 458
    },
    {
      "epoch": 0.01779500848073661,
      "grad_norm": 0.15285156667232513,
      "learning_rate": 0.00019647898247246784,
      "loss": 0.3561,
      "step": 459
    },
    {
      "epoch": 0.01783377756239399,
      "grad_norm": 0.16991491615772247,
      "learning_rate": 0.00019647122692725299,
      "loss": 0.3442,
      "step": 460
    },
    {
      "epoch": 0.017872546644051367,
      "grad_norm": 0.17830587923526764,
      "learning_rate": 0.00019646347138203816,
      "loss": 0.3461,
      "step": 461
    },
    {
      "epoch": 0.017911315725708747,
      "grad_norm": 0.1583179235458374,
      "learning_rate": 0.00019645571583682333,
      "loss": 0.3149,
      "step": 462
    },
    {
      "epoch": 0.017950084807366127,
      "grad_norm": 0.2645764946937561,
      "learning_rate": 0.0001964479602916085,
      "loss": 0.365,
      "step": 463
    },
    {
      "epoch": 0.017988853889023503,
      "grad_norm": 0.18679235875606537,
      "learning_rate": 0.00019644020474639368,
      "loss": 0.3996,
      "step": 464
    },
    {
      "epoch": 0.018027622970680883,
      "grad_norm": 0.23870238661766052,
      "learning_rate": 0.00019643244920117885,
      "loss": 0.3848,
      "step": 465
    },
    {
      "epoch": 0.01806639205233826,
      "grad_norm": 0.20034120976924896,
      "learning_rate": 0.00019642469365596402,
      "loss": 0.28,
      "step": 466
    },
    {
      "epoch": 0.01810516113399564,
      "grad_norm": 0.12934046983718872,
      "learning_rate": 0.0001964169381107492,
      "loss": 0.3173,
      "step": 467
    },
    {
      "epoch": 0.018143930215653015,
      "grad_norm": 0.2263900190591812,
      "learning_rate": 0.00019640918256553437,
      "loss": 0.2826,
      "step": 468
    },
    {
      "epoch": 0.018182699297310395,
      "grad_norm": 0.17984196543693542,
      "learning_rate": 0.00019640142702031954,
      "loss": 0.3531,
      "step": 469
    },
    {
      "epoch": 0.018221468378967775,
      "grad_norm": 0.13668012619018555,
      "learning_rate": 0.00019639367147510471,
      "loss": 0.3074,
      "step": 470
    },
    {
      "epoch": 0.01826023746062515,
      "grad_norm": 0.15420718491077423,
      "learning_rate": 0.0001963859159298899,
      "loss": 0.3384,
      "step": 471
    },
    {
      "epoch": 0.01829900654228253,
      "grad_norm": 0.14365902543067932,
      "learning_rate": 0.00019637816038467506,
      "loss": 0.2748,
      "step": 472
    },
    {
      "epoch": 0.018337775623939907,
      "grad_norm": 0.1230478435754776,
      "learning_rate": 0.00019637040483946023,
      "loss": 0.2912,
      "step": 473
    },
    {
      "epoch": 0.018376544705597287,
      "grad_norm": 0.14745737612247467,
      "learning_rate": 0.0001963626492942454,
      "loss": 0.341,
      "step": 474
    },
    {
      "epoch": 0.018415313787254663,
      "grad_norm": 0.15114833414554596,
      "learning_rate": 0.00019635489374903058,
      "loss": 0.3764,
      "step": 475
    },
    {
      "epoch": 0.018454082868912043,
      "grad_norm": 0.14132431149482727,
      "learning_rate": 0.00019634713820381575,
      "loss": 0.3807,
      "step": 476
    },
    {
      "epoch": 0.01849285195056942,
      "grad_norm": 0.14662304520606995,
      "learning_rate": 0.0001963393826586009,
      "loss": 0.3174,
      "step": 477
    },
    {
      "epoch": 0.0185316210322268,
      "grad_norm": 0.1288323551416397,
      "learning_rate": 0.0001963316271133861,
      "loss": 0.3244,
      "step": 478
    },
    {
      "epoch": 0.01857039011388418,
      "grad_norm": 0.11655396968126297,
      "learning_rate": 0.00019632387156817124,
      "loss": 0.2948,
      "step": 479
    },
    {
      "epoch": 0.018609159195541555,
      "grad_norm": 0.13553081452846527,
      "learning_rate": 0.00019631611602295644,
      "loss": 0.3089,
      "step": 480
    },
    {
      "epoch": 0.018647928277198935,
      "grad_norm": 0.12978409230709076,
      "learning_rate": 0.0001963083604777416,
      "loss": 0.3753,
      "step": 481
    },
    {
      "epoch": 0.01868669735885631,
      "grad_norm": 0.12580548226833344,
      "learning_rate": 0.0001963006049325268,
      "loss": 0.35,
      "step": 482
    },
    {
      "epoch": 0.01872546644051369,
      "grad_norm": 0.1397409737110138,
      "learning_rate": 0.00019629284938731193,
      "loss": 0.4071,
      "step": 483
    },
    {
      "epoch": 0.018764235522171067,
      "grad_norm": 0.24354536831378937,
      "learning_rate": 0.0001962850938420971,
      "loss": 0.3744,
      "step": 484
    },
    {
      "epoch": 0.018803004603828447,
      "grad_norm": 0.12411569803953171,
      "learning_rate": 0.00019627733829688228,
      "loss": 0.4022,
      "step": 485
    },
    {
      "epoch": 0.018841773685485824,
      "grad_norm": 0.1271260678768158,
      "learning_rate": 0.00019626958275166745,
      "loss": 0.3319,
      "step": 486
    },
    {
      "epoch": 0.018880542767143203,
      "grad_norm": 0.14929701387882233,
      "learning_rate": 0.00019626182720645262,
      "loss": 0.3811,
      "step": 487
    },
    {
      "epoch": 0.018919311848800583,
      "grad_norm": 0.15954889357089996,
      "learning_rate": 0.0001962540716612378,
      "loss": 0.384,
      "step": 488
    },
    {
      "epoch": 0.01895808093045796,
      "grad_norm": 0.12126569449901581,
      "learning_rate": 0.00019624631611602297,
      "loss": 0.3006,
      "step": 489
    },
    {
      "epoch": 0.01899685001211534,
      "grad_norm": 0.16860656440258026,
      "learning_rate": 0.00019623856057080814,
      "loss": 0.3763,
      "step": 490
    },
    {
      "epoch": 0.019035619093772715,
      "grad_norm": 0.132612943649292,
      "learning_rate": 0.0001962308050255933,
      "loss": 0.2896,
      "step": 491
    },
    {
      "epoch": 0.019074388175430095,
      "grad_norm": 0.09728425741195679,
      "learning_rate": 0.0001962230494803785,
      "loss": 0.2022,
      "step": 492
    },
    {
      "epoch": 0.01911315725708747,
      "grad_norm": 0.12854249775409698,
      "learning_rate": 0.00019621529393516363,
      "loss": 0.2721,
      "step": 493
    },
    {
      "epoch": 0.01915192633874485,
      "grad_norm": 0.14845336973667145,
      "learning_rate": 0.00019620753838994883,
      "loss": 0.3557,
      "step": 494
    },
    {
      "epoch": 0.019190695420402228,
      "grad_norm": 0.1459975242614746,
      "learning_rate": 0.00019619978284473398,
      "loss": 0.359,
      "step": 495
    },
    {
      "epoch": 0.019229464502059607,
      "grad_norm": 0.1503349244594574,
      "learning_rate": 0.00019619202729951918,
      "loss": 0.3922,
      "step": 496
    },
    {
      "epoch": 0.019268233583716987,
      "grad_norm": 0.15641088783740997,
      "learning_rate": 0.00019618427175430433,
      "loss": 0.4299,
      "step": 497
    },
    {
      "epoch": 0.019307002665374363,
      "grad_norm": 0.12854236364364624,
      "learning_rate": 0.0001961765162090895,
      "loss": 0.4304,
      "step": 498
    },
    {
      "epoch": 0.019345771747031743,
      "grad_norm": 0.1404946744441986,
      "learning_rate": 0.00019616876066387467,
      "loss": 0.2577,
      "step": 499
    },
    {
      "epoch": 0.01938454082868912,
      "grad_norm": 0.11552102118730545,
      "learning_rate": 0.00019616100511865984,
      "loss": 0.3135,
      "step": 500
    },
    {
      "epoch": 0.0194233099103465,
      "grad_norm": 0.12962807714939117,
      "learning_rate": 0.00019615324957344502,
      "loss": 0.394,
      "step": 501
    },
    {
      "epoch": 0.019462078992003876,
      "grad_norm": 0.12883909046649933,
      "learning_rate": 0.0001961454940282302,
      "loss": 0.2899,
      "step": 502
    },
    {
      "epoch": 0.019500848073661255,
      "grad_norm": 0.143062561750412,
      "learning_rate": 0.0001961377384830154,
      "loss": 0.3447,
      "step": 503
    },
    {
      "epoch": 0.01953961715531863,
      "grad_norm": 0.15881700813770294,
      "learning_rate": 0.00019612998293780054,
      "loss": 0.3562,
      "step": 504
    },
    {
      "epoch": 0.01957838623697601,
      "grad_norm": 0.1384359449148178,
      "learning_rate": 0.0001961222273925857,
      "loss": 0.27,
      "step": 505
    },
    {
      "epoch": 0.01961715531863339,
      "grad_norm": 0.11900326609611511,
      "learning_rate": 0.00019611447184737088,
      "loss": 0.3023,
      "step": 506
    },
    {
      "epoch": 0.019655924400290768,
      "grad_norm": 0.13315245509147644,
      "learning_rate": 0.00019610671630215605,
      "loss": 0.3775,
      "step": 507
    },
    {
      "epoch": 0.019694693481948147,
      "grad_norm": 0.1406663954257965,
      "learning_rate": 0.00019609896075694123,
      "loss": 0.3684,
      "step": 508
    },
    {
      "epoch": 0.019733462563605524,
      "grad_norm": 0.14245502650737762,
      "learning_rate": 0.0001960912052117264,
      "loss": 0.2863,
      "step": 509
    },
    {
      "epoch": 0.019772231645262903,
      "grad_norm": 0.12749171257019043,
      "learning_rate": 0.00019608344966651157,
      "loss": 0.3379,
      "step": 510
    },
    {
      "epoch": 0.01981100072692028,
      "grad_norm": 0.1899285763502121,
      "learning_rate": 0.00019607569412129675,
      "loss": 0.4041,
      "step": 511
    },
    {
      "epoch": 0.01984976980857766,
      "grad_norm": 0.13571697473526,
      "learning_rate": 0.0001960679385760819,
      "loss": 0.2902,
      "step": 512
    },
    {
      "epoch": 0.01988853889023504,
      "grad_norm": 0.1318952441215515,
      "learning_rate": 0.0001960601830308671,
      "loss": 0.3462,
      "step": 513
    },
    {
      "epoch": 0.019927307971892416,
      "grad_norm": 0.1319742649793625,
      "learning_rate": 0.00019605242748565224,
      "loss": 0.37,
      "step": 514
    },
    {
      "epoch": 0.019966077053549795,
      "grad_norm": 0.1283925175666809,
      "learning_rate": 0.00019604467194043744,
      "loss": 0.2618,
      "step": 515
    },
    {
      "epoch": 0.02000484613520717,
      "grad_norm": 0.12849412858486176,
      "learning_rate": 0.00019603691639522258,
      "loss": 0.3217,
      "step": 516
    },
    {
      "epoch": 0.02004361521686455,
      "grad_norm": 0.13619528710842133,
      "learning_rate": 0.00019602916085000778,
      "loss": 0.3684,
      "step": 517
    },
    {
      "epoch": 0.020082384298521928,
      "grad_norm": 0.13677331805229187,
      "learning_rate": 0.00019602140530479293,
      "loss": 0.3229,
      "step": 518
    },
    {
      "epoch": 0.020121153380179307,
      "grad_norm": 0.15164855122566223,
      "learning_rate": 0.0001960136497595781,
      "loss": 0.3773,
      "step": 519
    },
    {
      "epoch": 0.020159922461836684,
      "grad_norm": 0.1185174211859703,
      "learning_rate": 0.00019600589421436327,
      "loss": 0.3227,
      "step": 520
    },
    {
      "epoch": 0.020198691543494064,
      "grad_norm": 0.1222425103187561,
      "learning_rate": 0.00019599813866914845,
      "loss": 0.2799,
      "step": 521
    },
    {
      "epoch": 0.020237460625151443,
      "grad_norm": 0.1297154426574707,
      "learning_rate": 0.00019599038312393362,
      "loss": 0.3169,
      "step": 522
    },
    {
      "epoch": 0.02027622970680882,
      "grad_norm": 0.14259441196918488,
      "learning_rate": 0.0001959826275787188,
      "loss": 0.4284,
      "step": 523
    },
    {
      "epoch": 0.0203149987884662,
      "grad_norm": 0.1319076269865036,
      "learning_rate": 0.00019597487203350396,
      "loss": 0.3819,
      "step": 524
    },
    {
      "epoch": 0.020353767870123576,
      "grad_norm": 0.11624086648225784,
      "learning_rate": 0.00019596711648828914,
      "loss": 0.2578,
      "step": 525
    },
    {
      "epoch": 0.020392536951780955,
      "grad_norm": 0.15573687851428986,
      "learning_rate": 0.0001959593609430743,
      "loss": 0.4028,
      "step": 526
    },
    {
      "epoch": 0.020431306033438332,
      "grad_norm": 0.1307070255279541,
      "learning_rate": 0.00019595160539785948,
      "loss": 0.3524,
      "step": 527
    },
    {
      "epoch": 0.02047007511509571,
      "grad_norm": 0.12567265331745148,
      "learning_rate": 0.00019594384985264466,
      "loss": 0.3184,
      "step": 528
    },
    {
      "epoch": 0.020508844196753088,
      "grad_norm": 0.12364505231380463,
      "learning_rate": 0.00019593609430742983,
      "loss": 0.3402,
      "step": 529
    },
    {
      "epoch": 0.020547613278410468,
      "grad_norm": 0.1254902333021164,
      "learning_rate": 0.000195928338762215,
      "loss": 0.3422,
      "step": 530
    },
    {
      "epoch": 0.020586382360067847,
      "grad_norm": 0.10445648431777954,
      "learning_rate": 0.00019592058321700017,
      "loss": 0.2714,
      "step": 531
    },
    {
      "epoch": 0.020625151441725224,
      "grad_norm": 0.14284870028495789,
      "learning_rate": 0.00019591282767178535,
      "loss": 0.3782,
      "step": 532
    },
    {
      "epoch": 0.020663920523382603,
      "grad_norm": 0.12446285039186478,
      "learning_rate": 0.0001959050721265705,
      "loss": 0.3693,
      "step": 533
    },
    {
      "epoch": 0.02070268960503998,
      "grad_norm": 0.10959059745073318,
      "learning_rate": 0.0001958973165813557,
      "loss": 0.2586,
      "step": 534
    },
    {
      "epoch": 0.02074145868669736,
      "grad_norm": 0.12660424411296844,
      "learning_rate": 0.00019588956103614084,
      "loss": 0.332,
      "step": 535
    },
    {
      "epoch": 0.020780227768354736,
      "grad_norm": 0.15643177926540375,
      "learning_rate": 0.00019588180549092604,
      "loss": 0.4298,
      "step": 536
    },
    {
      "epoch": 0.020818996850012116,
      "grad_norm": 0.11920110881328583,
      "learning_rate": 0.00019587404994571118,
      "loss": 0.2693,
      "step": 537
    },
    {
      "epoch": 0.020857765931669492,
      "grad_norm": 0.12852129340171814,
      "learning_rate": 0.00019586629440049638,
      "loss": 0.3034,
      "step": 538
    },
    {
      "epoch": 0.02089653501332687,
      "grad_norm": 0.15311305224895477,
      "learning_rate": 0.00019585853885528153,
      "loss": 0.3067,
      "step": 539
    },
    {
      "epoch": 0.02093530409498425,
      "grad_norm": 0.23173239827156067,
      "learning_rate": 0.0001958507833100667,
      "loss": 0.3723,
      "step": 540
    },
    {
      "epoch": 0.020974073176641628,
      "grad_norm": 0.13428598642349243,
      "learning_rate": 0.00019584302776485188,
      "loss": 0.3282,
      "step": 541
    },
    {
      "epoch": 0.021012842258299008,
      "grad_norm": 0.1327797770500183,
      "learning_rate": 0.00019583527221963705,
      "loss": 0.3738,
      "step": 542
    },
    {
      "epoch": 0.021051611339956384,
      "grad_norm": 0.1711985021829605,
      "learning_rate": 0.00019582751667442222,
      "loss": 0.3279,
      "step": 543
    },
    {
      "epoch": 0.021090380421613764,
      "grad_norm": 0.17140528559684753,
      "learning_rate": 0.0001958197611292074,
      "loss": 0.306,
      "step": 544
    },
    {
      "epoch": 0.02112914950327114,
      "grad_norm": 0.12347251176834106,
      "learning_rate": 0.00019581200558399257,
      "loss": 0.308,
      "step": 545
    },
    {
      "epoch": 0.02116791858492852,
      "grad_norm": 0.13194917142391205,
      "learning_rate": 0.00019580425003877774,
      "loss": 0.2325,
      "step": 546
    },
    {
      "epoch": 0.0212066876665859,
      "grad_norm": 0.24842607975006104,
      "learning_rate": 0.0001957964944935629,
      "loss": 0.459,
      "step": 547
    },
    {
      "epoch": 0.021245456748243276,
      "grad_norm": 0.18623337149620056,
      "learning_rate": 0.00019578873894834808,
      "loss": 0.3881,
      "step": 548
    },
    {
      "epoch": 0.021284225829900656,
      "grad_norm": 0.1485421061515808,
      "learning_rate": 0.00019578098340313326,
      "loss": 0.3111,
      "step": 549
    },
    {
      "epoch": 0.021322994911558032,
      "grad_norm": 0.16233593225479126,
      "learning_rate": 0.00019577322785791843,
      "loss": 0.3066,
      "step": 550
    },
    {
      "epoch": 0.02136176399321541,
      "grad_norm": 0.12624491751194,
      "learning_rate": 0.0001957654723127036,
      "loss": 0.341,
      "step": 551
    },
    {
      "epoch": 0.021400533074872788,
      "grad_norm": 0.1632484346628189,
      "learning_rate": 0.00019575771676748878,
      "loss": 0.2537,
      "step": 552
    },
    {
      "epoch": 0.021439302156530168,
      "grad_norm": 0.1417379379272461,
      "learning_rate": 0.00019574996122227395,
      "loss": 0.307,
      "step": 553
    },
    {
      "epoch": 0.021478071238187544,
      "grad_norm": 0.1387166678905487,
      "learning_rate": 0.0001957422056770591,
      "loss": 0.2962,
      "step": 554
    },
    {
      "epoch": 0.021516840319844924,
      "grad_norm": 0.17933198809623718,
      "learning_rate": 0.0001957344501318443,
      "loss": 0.3122,
      "step": 555
    },
    {
      "epoch": 0.021555609401502304,
      "grad_norm": 0.1597783863544464,
      "learning_rate": 0.00019572669458662944,
      "loss": 0.3157,
      "step": 556
    },
    {
      "epoch": 0.02159437848315968,
      "grad_norm": 0.13877026736736298,
      "learning_rate": 0.00019571893904141464,
      "loss": 0.2972,
      "step": 557
    },
    {
      "epoch": 0.02163314756481706,
      "grad_norm": 0.150285542011261,
      "learning_rate": 0.00019571118349619979,
      "loss": 0.3865,
      "step": 558
    },
    {
      "epoch": 0.021671916646474436,
      "grad_norm": 0.13468581438064575,
      "learning_rate": 0.00019570342795098499,
      "loss": 0.2857,
      "step": 559
    },
    {
      "epoch": 0.021710685728131816,
      "grad_norm": 0.14711631834506989,
      "learning_rate": 0.00019569567240577013,
      "loss": 0.387,
      "step": 560
    },
    {
      "epoch": 0.021749454809789192,
      "grad_norm": 0.149411141872406,
      "learning_rate": 0.0001956879168605553,
      "loss": 0.3081,
      "step": 561
    },
    {
      "epoch": 0.02178822389144657,
      "grad_norm": 0.1490873247385025,
      "learning_rate": 0.00019568016131534048,
      "loss": 0.3752,
      "step": 562
    },
    {
      "epoch": 0.021826992973103948,
      "grad_norm": 0.1344262808561325,
      "learning_rate": 0.00019567240577012565,
      "loss": 0.2954,
      "step": 563
    },
    {
      "epoch": 0.021865762054761328,
      "grad_norm": 0.12403842061758041,
      "learning_rate": 0.00019566465022491082,
      "loss": 0.2841,
      "step": 564
    },
    {
      "epoch": 0.021904531136418708,
      "grad_norm": 0.1263134926557541,
      "learning_rate": 0.000195656894679696,
      "loss": 0.2931,
      "step": 565
    },
    {
      "epoch": 0.021943300218076084,
      "grad_norm": 0.10279902815818787,
      "learning_rate": 0.00019564913913448117,
      "loss": 0.2329,
      "step": 566
    },
    {
      "epoch": 0.021982069299733464,
      "grad_norm": 0.1511155515909195,
      "learning_rate": 0.00019564138358926634,
      "loss": 0.3153,
      "step": 567
    },
    {
      "epoch": 0.02202083838139084,
      "grad_norm": 0.16868379712104797,
      "learning_rate": 0.0001956336280440515,
      "loss": 0.347,
      "step": 568
    },
    {
      "epoch": 0.02205960746304822,
      "grad_norm": 0.11417216807603836,
      "learning_rate": 0.0001956258724988367,
      "loss": 0.3427,
      "step": 569
    },
    {
      "epoch": 0.022098376544705596,
      "grad_norm": 0.12108314037322998,
      "learning_rate": 0.00019561811695362183,
      "loss": 0.3067,
      "step": 570
    },
    {
      "epoch": 0.022137145626362976,
      "grad_norm": 0.1670752763748169,
      "learning_rate": 0.00019561036140840703,
      "loss": 0.3922,
      "step": 571
    },
    {
      "epoch": 0.022175914708020352,
      "grad_norm": 0.15320830047130585,
      "learning_rate": 0.00019560260586319218,
      "loss": 0.3619,
      "step": 572
    },
    {
      "epoch": 0.022214683789677732,
      "grad_norm": 0.14522773027420044,
      "learning_rate": 0.00019559485031797738,
      "loss": 0.2927,
      "step": 573
    },
    {
      "epoch": 0.02225345287133511,
      "grad_norm": 0.12948980927467346,
      "learning_rate": 0.00019558709477276252,
      "loss": 0.2989,
      "step": 574
    },
    {
      "epoch": 0.022292221952992488,
      "grad_norm": 0.177107572555542,
      "learning_rate": 0.0001955793392275477,
      "loss": 0.3554,
      "step": 575
    },
    {
      "epoch": 0.022330991034649868,
      "grad_norm": 0.14537151157855988,
      "learning_rate": 0.00019557158368233287,
      "loss": 0.3583,
      "step": 576
    },
    {
      "epoch": 0.022369760116307244,
      "grad_norm": 0.12663856148719788,
      "learning_rate": 0.00019556382813711804,
      "loss": 0.3069,
      "step": 577
    },
    {
      "epoch": 0.022408529197964624,
      "grad_norm": 0.13705222308635712,
      "learning_rate": 0.00019555607259190321,
      "loss": 0.3509,
      "step": 578
    },
    {
      "epoch": 0.022447298279622,
      "grad_norm": 0.19195543229579926,
      "learning_rate": 0.0001955483170466884,
      "loss": 0.3786,
      "step": 579
    },
    {
      "epoch": 0.02248606736127938,
      "grad_norm": 0.15169772505760193,
      "learning_rate": 0.00019554056150147356,
      "loss": 0.4077,
      "step": 580
    },
    {
      "epoch": 0.02252483644293676,
      "grad_norm": 0.16389256715774536,
      "learning_rate": 0.00019553280595625873,
      "loss": 0.3946,
      "step": 581
    },
    {
      "epoch": 0.022563605524594136,
      "grad_norm": 0.18961650133132935,
      "learning_rate": 0.0001955250504110439,
      "loss": 0.3417,
      "step": 582
    },
    {
      "epoch": 0.022602374606251516,
      "grad_norm": 0.13885752856731415,
      "learning_rate": 0.00019551729486582908,
      "loss": 0.331,
      "step": 583
    },
    {
      "epoch": 0.022641143687908892,
      "grad_norm": 0.13469475507736206,
      "learning_rate": 0.00019550953932061425,
      "loss": 0.351,
      "step": 584
    },
    {
      "epoch": 0.022679912769566272,
      "grad_norm": 0.12188803404569626,
      "learning_rate": 0.00019550178377539942,
      "loss": 0.3237,
      "step": 585
    },
    {
      "epoch": 0.022718681851223648,
      "grad_norm": 0.15183627605438232,
      "learning_rate": 0.0001954940282301846,
      "loss": 0.3017,
      "step": 586
    },
    {
      "epoch": 0.022757450932881028,
      "grad_norm": 0.12989862263202667,
      "learning_rate": 0.00019548627268496977,
      "loss": 0.3267,
      "step": 587
    },
    {
      "epoch": 0.022796220014538404,
      "grad_norm": 0.12109469622373581,
      "learning_rate": 0.00019547851713975494,
      "loss": 0.3352,
      "step": 588
    },
    {
      "epoch": 0.022834989096195784,
      "grad_norm": 0.11856376379728317,
      "learning_rate": 0.0001954707615945401,
      "loss": 0.3174,
      "step": 589
    },
    {
      "epoch": 0.022873758177853164,
      "grad_norm": 0.12232605367898941,
      "learning_rate": 0.0001954630060493253,
      "loss": 0.2982,
      "step": 590
    },
    {
      "epoch": 0.02291252725951054,
      "grad_norm": 0.12295857071876526,
      "learning_rate": 0.00019545525050411043,
      "loss": 0.3465,
      "step": 591
    },
    {
      "epoch": 0.02295129634116792,
      "grad_norm": 0.11086241155862808,
      "learning_rate": 0.00019544749495889563,
      "loss": 0.3219,
      "step": 592
    },
    {
      "epoch": 0.022990065422825296,
      "grad_norm": 0.14378538727760315,
      "learning_rate": 0.00019543973941368078,
      "loss": 0.2887,
      "step": 593
    },
    {
      "epoch": 0.023028834504482676,
      "grad_norm": 0.1273319274187088,
      "learning_rate": 0.00019543198386846598,
      "loss": 0.3514,
      "step": 594
    },
    {
      "epoch": 0.023067603586140052,
      "grad_norm": 0.13463422656059265,
      "learning_rate": 0.00019542422832325113,
      "loss": 0.299,
      "step": 595
    },
    {
      "epoch": 0.023106372667797432,
      "grad_norm": 0.09827540814876556,
      "learning_rate": 0.0001954164727780363,
      "loss": 0.2356,
      "step": 596
    },
    {
      "epoch": 0.023145141749454808,
      "grad_norm": 0.09991138428449631,
      "learning_rate": 0.00019540871723282147,
      "loss": 0.2218,
      "step": 597
    },
    {
      "epoch": 0.023183910831112188,
      "grad_norm": 0.1412734091281891,
      "learning_rate": 0.00019540096168760664,
      "loss": 0.3898,
      "step": 598
    },
    {
      "epoch": 0.023222679912769568,
      "grad_norm": 0.18407855927944183,
      "learning_rate": 0.00019539320614239182,
      "loss": 0.4495,
      "step": 599
    },
    {
      "epoch": 0.023261448994426944,
      "grad_norm": 0.12616126239299774,
      "learning_rate": 0.000195385450597177,
      "loss": 0.3155,
      "step": 600
    },
    {
      "epoch": 0.023300218076084324,
      "grad_norm": 0.13293202221393585,
      "learning_rate": 0.00019537769505196216,
      "loss": 0.396,
      "step": 601
    },
    {
      "epoch": 0.0233389871577417,
      "grad_norm": 0.1187300980091095,
      "learning_rate": 0.00019536993950674733,
      "loss": 0.2838,
      "step": 602
    },
    {
      "epoch": 0.02337775623939908,
      "grad_norm": 0.14480841159820557,
      "learning_rate": 0.0001953621839615325,
      "loss": 0.3074,
      "step": 603
    },
    {
      "epoch": 0.023416525321056456,
      "grad_norm": 0.14340215921401978,
      "learning_rate": 0.00019535442841631768,
      "loss": 0.3066,
      "step": 604
    },
    {
      "epoch": 0.023455294402713836,
      "grad_norm": 0.12257065623998642,
      "learning_rate": 0.00019534667287110285,
      "loss": 0.2642,
      "step": 605
    },
    {
      "epoch": 0.023494063484371212,
      "grad_norm": 0.13920994102954865,
      "learning_rate": 0.00019533891732588803,
      "loss": 0.3327,
      "step": 606
    },
    {
      "epoch": 0.023532832566028592,
      "grad_norm": 0.1343422681093216,
      "learning_rate": 0.0001953311617806732,
      "loss": 0.3105,
      "step": 607
    },
    {
      "epoch": 0.023571601647685972,
      "grad_norm": 0.12058977782726288,
      "learning_rate": 0.00019532340623545837,
      "loss": 0.2646,
      "step": 608
    },
    {
      "epoch": 0.023610370729343348,
      "grad_norm": 0.14060011506080627,
      "learning_rate": 0.00019531565069024354,
      "loss": 0.3484,
      "step": 609
    },
    {
      "epoch": 0.023649139811000728,
      "grad_norm": 0.10002709925174713,
      "learning_rate": 0.0001953078951450287,
      "loss": 0.2553,
      "step": 610
    },
    {
      "epoch": 0.023687908892658104,
      "grad_norm": 0.11270496994256973,
      "learning_rate": 0.0001953001395998139,
      "loss": 0.2836,
      "step": 611
    },
    {
      "epoch": 0.023726677974315484,
      "grad_norm": 0.1107255220413208,
      "learning_rate": 0.00019529238405459904,
      "loss": 0.2674,
      "step": 612
    },
    {
      "epoch": 0.02376544705597286,
      "grad_norm": 0.12314381450414658,
      "learning_rate": 0.00019528462850938424,
      "loss": 0.2573,
      "step": 613
    },
    {
      "epoch": 0.02380421613763024,
      "grad_norm": 0.15155215561389923,
      "learning_rate": 0.00019527687296416938,
      "loss": 0.4061,
      "step": 614
    },
    {
      "epoch": 0.023842985219287616,
      "grad_norm": 0.11522822082042694,
      "learning_rate": 0.00019526911741895458,
      "loss": 0.3201,
      "step": 615
    },
    {
      "epoch": 0.023881754300944996,
      "grad_norm": 0.1298917829990387,
      "learning_rate": 0.00019526136187373973,
      "loss": 0.3111,
      "step": 616
    },
    {
      "epoch": 0.023920523382602376,
      "grad_norm": 0.13414376974105835,
      "learning_rate": 0.0001952536063285249,
      "loss": 0.3496,
      "step": 617
    },
    {
      "epoch": 0.023959292464259752,
      "grad_norm": 0.14394275844097137,
      "learning_rate": 0.00019524585078331007,
      "loss": 0.3478,
      "step": 618
    },
    {
      "epoch": 0.023998061545917132,
      "grad_norm": 0.11528077721595764,
      "learning_rate": 0.00019523809523809525,
      "loss": 0.3181,
      "step": 619
    },
    {
      "epoch": 0.02403683062757451,
      "grad_norm": 0.09663040190935135,
      "learning_rate": 0.00019523033969288042,
      "loss": 0.2396,
      "step": 620
    },
    {
      "epoch": 0.024075599709231888,
      "grad_norm": 0.1234399750828743,
      "learning_rate": 0.0001952225841476656,
      "loss": 0.2823,
      "step": 621
    },
    {
      "epoch": 0.024114368790889264,
      "grad_norm": 0.133757084608078,
      "learning_rate": 0.00019521482860245076,
      "loss": 0.2902,
      "step": 622
    },
    {
      "epoch": 0.024153137872546644,
      "grad_norm": 0.12695614993572235,
      "learning_rate": 0.00019520707305723594,
      "loss": 0.3262,
      "step": 623
    },
    {
      "epoch": 0.024191906954204024,
      "grad_norm": 0.16256925463676453,
      "learning_rate": 0.00019519931751202108,
      "loss": 0.3897,
      "step": 624
    },
    {
      "epoch": 0.0242306760358614,
      "grad_norm": 0.12378588318824768,
      "learning_rate": 0.00019519156196680628,
      "loss": 0.247,
      "step": 625
    },
    {
      "epoch": 0.02426944511751878,
      "grad_norm": 0.11506015062332153,
      "learning_rate": 0.00019518380642159146,
      "loss": 0.3141,
      "step": 626
    },
    {
      "epoch": 0.024308214199176156,
      "grad_norm": 0.12180662155151367,
      "learning_rate": 0.00019517605087637663,
      "loss": 0.3334,
      "step": 627
    },
    {
      "epoch": 0.024346983280833536,
      "grad_norm": 0.149561807513237,
      "learning_rate": 0.0001951682953311618,
      "loss": 0.3785,
      "step": 628
    },
    {
      "epoch": 0.024385752362490912,
      "grad_norm": 0.14058031141757965,
      "learning_rate": 0.00019516053978594697,
      "loss": 0.362,
      "step": 629
    },
    {
      "epoch": 0.024424521444148292,
      "grad_norm": 0.09376820921897888,
      "learning_rate": 0.00019515278424073215,
      "loss": 0.2687,
      "step": 630
    },
    {
      "epoch": 0.02446329052580567,
      "grad_norm": 0.12653778493404388,
      "learning_rate": 0.0001951450286955173,
      "loss": 0.2993,
      "step": 631
    },
    {
      "epoch": 0.024502059607463048,
      "grad_norm": 0.10704130679368973,
      "learning_rate": 0.0001951372731503025,
      "loss": 0.3157,
      "step": 632
    },
    {
      "epoch": 0.024540828689120428,
      "grad_norm": 0.10715336352586746,
      "learning_rate": 0.00019512951760508764,
      "loss": 0.2859,
      "step": 633
    },
    {
      "epoch": 0.024579597770777804,
      "grad_norm": 0.12625454366207123,
      "learning_rate": 0.00019512176205987284,
      "loss": 0.3039,
      "step": 634
    },
    {
      "epoch": 0.024618366852435184,
      "grad_norm": 0.13774466514587402,
      "learning_rate": 0.00019511400651465798,
      "loss": 0.3188,
      "step": 635
    },
    {
      "epoch": 0.02465713593409256,
      "grad_norm": 0.12789593636989594,
      "learning_rate": 0.00019510625096944318,
      "loss": 0.2789,
      "step": 636
    },
    {
      "epoch": 0.02469590501574994,
      "grad_norm": 0.1307429075241089,
      "learning_rate": 0.00019509849542422833,
      "loss": 0.3155,
      "step": 637
    },
    {
      "epoch": 0.024734674097407316,
      "grad_norm": 0.15394969284534454,
      "learning_rate": 0.0001950907398790135,
      "loss": 0.295,
      "step": 638
    },
    {
      "epoch": 0.024773443179064696,
      "grad_norm": 0.15301308035850525,
      "learning_rate": 0.00019508298433379867,
      "loss": 0.2991,
      "step": 639
    },
    {
      "epoch": 0.024812212260722073,
      "grad_norm": 0.23886895179748535,
      "learning_rate": 0.00019507522878858385,
      "loss": 0.3829,
      "step": 640
    },
    {
      "epoch": 0.024850981342379452,
      "grad_norm": 0.134431853890419,
      "learning_rate": 0.00019506747324336902,
      "loss": 0.3203,
      "step": 641
    },
    {
      "epoch": 0.024889750424036832,
      "grad_norm": 0.2411772906780243,
      "learning_rate": 0.0001950597176981542,
      "loss": 0.3468,
      "step": 642
    },
    {
      "epoch": 0.02492851950569421,
      "grad_norm": 0.1183103695511818,
      "learning_rate": 0.00019505196215293937,
      "loss": 0.3467,
      "step": 643
    },
    {
      "epoch": 0.024967288587351588,
      "grad_norm": 0.15900203585624695,
      "learning_rate": 0.00019504420660772454,
      "loss": 0.2069,
      "step": 644
    },
    {
      "epoch": 0.025006057669008964,
      "grad_norm": 0.13621824979782104,
      "learning_rate": 0.00019503645106250968,
      "loss": 0.3155,
      "step": 645
    },
    {
      "epoch": 0.025044826750666344,
      "grad_norm": 0.19061224162578583,
      "learning_rate": 0.00019502869551729488,
      "loss": 0.3978,
      "step": 646
    },
    {
      "epoch": 0.02508359583232372,
      "grad_norm": 0.14527463912963867,
      "learning_rate": 0.00019502093997208003,
      "loss": 0.3357,
      "step": 647
    },
    {
      "epoch": 0.0251223649139811,
      "grad_norm": 0.12885603308677673,
      "learning_rate": 0.00019501318442686523,
      "loss": 0.2885,
      "step": 648
    },
    {
      "epoch": 0.025161133995638477,
      "grad_norm": 0.1465957760810852,
      "learning_rate": 0.00019500542888165038,
      "loss": 0.3385,
      "step": 649
    },
    {
      "epoch": 0.025199903077295856,
      "grad_norm": 0.14532943069934845,
      "learning_rate": 0.00019499767333643558,
      "loss": 0.3707,
      "step": 650
    },
    {
      "epoch": 0.025238672158953236,
      "grad_norm": 0.11639929562807083,
      "learning_rate": 0.00019498991779122072,
      "loss": 0.2839,
      "step": 651
    },
    {
      "epoch": 0.025277441240610612,
      "grad_norm": 0.13418340682983398,
      "learning_rate": 0.0001949821622460059,
      "loss": 0.3513,
      "step": 652
    },
    {
      "epoch": 0.025316210322267992,
      "grad_norm": 0.0914546400308609,
      "learning_rate": 0.00019497440670079107,
      "loss": 0.2284,
      "step": 653
    },
    {
      "epoch": 0.02535497940392537,
      "grad_norm": 0.119635209441185,
      "learning_rate": 0.00019496665115557624,
      "loss": 0.3385,
      "step": 654
    },
    {
      "epoch": 0.02539374848558275,
      "grad_norm": 0.12518243491649628,
      "learning_rate": 0.0001949588956103614,
      "loss": 0.3313,
      "step": 655
    },
    {
      "epoch": 0.025432517567240125,
      "grad_norm": 0.1350732445716858,
      "learning_rate": 0.00019495114006514659,
      "loss": 0.3358,
      "step": 656
    },
    {
      "epoch": 0.025471286648897504,
      "grad_norm": 0.11867859959602356,
      "learning_rate": 0.00019494338451993176,
      "loss": 0.2518,
      "step": 657
    },
    {
      "epoch": 0.025510055730554884,
      "grad_norm": 0.13726024329662323,
      "learning_rate": 0.00019493562897471693,
      "loss": 0.3213,
      "step": 658
    },
    {
      "epoch": 0.02554882481221226,
      "grad_norm": 0.11719154566526413,
      "learning_rate": 0.0001949278734295021,
      "loss": 0.3081,
      "step": 659
    },
    {
      "epoch": 0.02558759389386964,
      "grad_norm": 0.1272508054971695,
      "learning_rate": 0.00019492011788428728,
      "loss": 0.2784,
      "step": 660
    },
    {
      "epoch": 0.025626362975527017,
      "grad_norm": 0.14191778004169464,
      "learning_rate": 0.00019491236233907245,
      "loss": 0.3479,
      "step": 661
    },
    {
      "epoch": 0.025665132057184396,
      "grad_norm": 0.12302829325199127,
      "learning_rate": 0.00019490460679385762,
      "loss": 0.3002,
      "step": 662
    },
    {
      "epoch": 0.025703901138841773,
      "grad_norm": 0.13509613275527954,
      "learning_rate": 0.0001948968512486428,
      "loss": 0.3271,
      "step": 663
    },
    {
      "epoch": 0.025742670220499152,
      "grad_norm": 0.14365938305854797,
      "learning_rate": 0.00019488909570342797,
      "loss": 0.3904,
      "step": 664
    },
    {
      "epoch": 0.02578143930215653,
      "grad_norm": 0.11827735602855682,
      "learning_rate": 0.00019488134015821314,
      "loss": 0.3503,
      "step": 665
    },
    {
      "epoch": 0.02582020838381391,
      "grad_norm": 0.13390251994132996,
      "learning_rate": 0.00019487358461299829,
      "loss": 0.2458,
      "step": 666
    },
    {
      "epoch": 0.025858977465471288,
      "grad_norm": 0.16371305286884308,
      "learning_rate": 0.00019486582906778349,
      "loss": 0.3456,
      "step": 667
    },
    {
      "epoch": 0.025897746547128665,
      "grad_norm": 0.15518660843372345,
      "learning_rate": 0.00019485807352256863,
      "loss": 0.3573,
      "step": 668
    },
    {
      "epoch": 0.025936515628786044,
      "grad_norm": 0.1372368186712265,
      "learning_rate": 0.00019485031797735383,
      "loss": 0.3055,
      "step": 669
    },
    {
      "epoch": 0.02597528471044342,
      "grad_norm": 0.14872153103351593,
      "learning_rate": 0.00019484256243213898,
      "loss": 0.3684,
      "step": 670
    },
    {
      "epoch": 0.0260140537921008,
      "grad_norm": 0.16002589464187622,
      "learning_rate": 0.00019483480688692418,
      "loss": 0.3622,
      "step": 671
    },
    {
      "epoch": 0.026052822873758177,
      "grad_norm": 0.1404937356710434,
      "learning_rate": 0.00019482705134170932,
      "loss": 0.2951,
      "step": 672
    },
    {
      "epoch": 0.026091591955415556,
      "grad_norm": 0.1238628700375557,
      "learning_rate": 0.0001948192957964945,
      "loss": 0.2541,
      "step": 673
    },
    {
      "epoch": 0.026130361037072933,
      "grad_norm": 0.148197203874588,
      "learning_rate": 0.00019481154025127967,
      "loss": 0.4259,
      "step": 674
    },
    {
      "epoch": 0.026169130118730313,
      "grad_norm": 0.14262980222702026,
      "learning_rate": 0.00019480378470606484,
      "loss": 0.3465,
      "step": 675
    },
    {
      "epoch": 0.026207899200387692,
      "grad_norm": 0.13434624671936035,
      "learning_rate": 0.00019479602916085001,
      "loss": 0.3408,
      "step": 676
    },
    {
      "epoch": 0.02624666828204507,
      "grad_norm": 0.1397152543067932,
      "learning_rate": 0.0001947882736156352,
      "loss": 0.3157,
      "step": 677
    },
    {
      "epoch": 0.02628543736370245,
      "grad_norm": 0.13294711709022522,
      "learning_rate": 0.00019478051807042036,
      "loss": 0.4302,
      "step": 678
    },
    {
      "epoch": 0.026324206445359825,
      "grad_norm": 0.12388382852077484,
      "learning_rate": 0.00019477276252520553,
      "loss": 0.3145,
      "step": 679
    },
    {
      "epoch": 0.026362975527017204,
      "grad_norm": 0.14480042457580566,
      "learning_rate": 0.0001947650069799907,
      "loss": 0.3307,
      "step": 680
    },
    {
      "epoch": 0.02640174460867458,
      "grad_norm": 0.1404106169939041,
      "learning_rate": 0.00019475725143477588,
      "loss": 0.3222,
      "step": 681
    },
    {
      "epoch": 0.02644051369033196,
      "grad_norm": 0.11089223623275757,
      "learning_rate": 0.00019474949588956105,
      "loss": 0.3038,
      "step": 682
    },
    {
      "epoch": 0.026479282771989337,
      "grad_norm": 0.1852339655160904,
      "learning_rate": 0.00019474174034434622,
      "loss": 0.4234,
      "step": 683
    },
    {
      "epoch": 0.026518051853646717,
      "grad_norm": 0.1319822371006012,
      "learning_rate": 0.0001947339847991314,
      "loss": 0.3416,
      "step": 684
    },
    {
      "epoch": 0.026556820935304096,
      "grad_norm": 0.1651543825864792,
      "learning_rate": 0.00019472622925391657,
      "loss": 0.28,
      "step": 685
    },
    {
      "epoch": 0.026595590016961473,
      "grad_norm": 0.10592887550592422,
      "learning_rate": 0.00019471847370870174,
      "loss": 0.2794,
      "step": 686
    },
    {
      "epoch": 0.026634359098618852,
      "grad_norm": 0.12917570769786835,
      "learning_rate": 0.0001947107181634869,
      "loss": 0.4054,
      "step": 687
    },
    {
      "epoch": 0.02667312818027623,
      "grad_norm": 0.1140102744102478,
      "learning_rate": 0.0001947029626182721,
      "loss": 0.2525,
      "step": 688
    },
    {
      "epoch": 0.02671189726193361,
      "grad_norm": 0.13529515266418457,
      "learning_rate": 0.00019469520707305723,
      "loss": 0.3597,
      "step": 689
    },
    {
      "epoch": 0.026750666343590985,
      "grad_norm": 0.12259211391210556,
      "learning_rate": 0.00019468745152784243,
      "loss": 0.3171,
      "step": 690
    },
    {
      "epoch": 0.026789435425248365,
      "grad_norm": 0.1498337984085083,
      "learning_rate": 0.00019467969598262758,
      "loss": 0.3112,
      "step": 691
    },
    {
      "epoch": 0.026828204506905744,
      "grad_norm": 0.10360490530729294,
      "learning_rate": 0.00019467194043741278,
      "loss": 0.2515,
      "step": 692
    },
    {
      "epoch": 0.02686697358856312,
      "grad_norm": 0.11671663820743561,
      "learning_rate": 0.00019466418489219792,
      "loss": 0.293,
      "step": 693
    },
    {
      "epoch": 0.0269057426702205,
      "grad_norm": 0.17023327946662903,
      "learning_rate": 0.0001946564293469831,
      "loss": 0.3288,
      "step": 694
    },
    {
      "epoch": 0.026944511751877877,
      "grad_norm": 0.14679062366485596,
      "learning_rate": 0.00019464867380176827,
      "loss": 0.3561,
      "step": 695
    },
    {
      "epoch": 0.026983280833535257,
      "grad_norm": 0.1319226324558258,
      "learning_rate": 0.00019464091825655344,
      "loss": 0.3017,
      "step": 696
    },
    {
      "epoch": 0.027022049915192633,
      "grad_norm": 0.1411600559949875,
      "learning_rate": 0.00019463316271133862,
      "loss": 0.3767,
      "step": 697
    },
    {
      "epoch": 0.027060818996850013,
      "grad_norm": 0.18186801671981812,
      "learning_rate": 0.0001946254071661238,
      "loss": 0.3622,
      "step": 698
    },
    {
      "epoch": 0.02709958807850739,
      "grad_norm": 0.14490680396556854,
      "learning_rate": 0.00019461765162090896,
      "loss": 0.3197,
      "step": 699
    },
    {
      "epoch": 0.02713835716016477,
      "grad_norm": 0.1445772647857666,
      "learning_rate": 0.00019460989607569413,
      "loss": 0.3397,
      "step": 700
    },
    {
      "epoch": 0.02717712624182215,
      "grad_norm": 0.18242844939231873,
      "learning_rate": 0.00019460214053047928,
      "loss": 0.2718,
      "step": 701
    },
    {
      "epoch": 0.027215895323479525,
      "grad_norm": 0.14769017696380615,
      "learning_rate": 0.00019459438498526448,
      "loss": 0.302,
      "step": 702
    },
    {
      "epoch": 0.027254664405136905,
      "grad_norm": 0.25972458720207214,
      "learning_rate": 0.00019458662944004963,
      "loss": 0.4131,
      "step": 703
    },
    {
      "epoch": 0.02729343348679428,
      "grad_norm": 0.15456551313400269,
      "learning_rate": 0.00019457887389483483,
      "loss": 0.4047,
      "step": 704
    },
    {
      "epoch": 0.02733220256845166,
      "grad_norm": 0.18481768667697906,
      "learning_rate": 0.00019457111834962,
      "loss": 0.3101,
      "step": 705
    },
    {
      "epoch": 0.027370971650109037,
      "grad_norm": 0.2022651880979538,
      "learning_rate": 0.00019456336280440517,
      "loss": 0.2851,
      "step": 706
    },
    {
      "epoch": 0.027409740731766417,
      "grad_norm": 0.17370358109474182,
      "learning_rate": 0.00019455560725919034,
      "loss": 0.3134,
      "step": 707
    },
    {
      "epoch": 0.027448509813423793,
      "grad_norm": 0.17657160758972168,
      "learning_rate": 0.0001945478517139755,
      "loss": 0.3551,
      "step": 708
    },
    {
      "epoch": 0.027487278895081173,
      "grad_norm": 0.15798459947109222,
      "learning_rate": 0.0001945400961687607,
      "loss": 0.3694,
      "step": 709
    },
    {
      "epoch": 0.027526047976738553,
      "grad_norm": 0.14333398640155792,
      "learning_rate": 0.00019453234062354584,
      "loss": 0.3064,
      "step": 710
    },
    {
      "epoch": 0.02756481705839593,
      "grad_norm": 0.14605876803398132,
      "learning_rate": 0.00019452458507833104,
      "loss": 0.3389,
      "step": 711
    },
    {
      "epoch": 0.02760358614005331,
      "grad_norm": 0.1302778273820877,
      "learning_rate": 0.00019451682953311618,
      "loss": 0.2969,
      "step": 712
    },
    {
      "epoch": 0.027642355221710685,
      "grad_norm": 0.17657847702503204,
      "learning_rate": 0.00019450907398790138,
      "loss": 0.4163,
      "step": 713
    },
    {
      "epoch": 0.027681124303368065,
      "grad_norm": 0.1313713788986206,
      "learning_rate": 0.00019450131844268653,
      "loss": 0.3067,
      "step": 714
    },
    {
      "epoch": 0.02771989338502544,
      "grad_norm": 0.13632048666477203,
      "learning_rate": 0.0001944935628974717,
      "loss": 0.334,
      "step": 715
    },
    {
      "epoch": 0.02775866246668282,
      "grad_norm": 0.13048508763313293,
      "learning_rate": 0.00019448580735225687,
      "loss": 0.3717,
      "step": 716
    },
    {
      "epoch": 0.027797431548340197,
      "grad_norm": 0.18374419212341309,
      "learning_rate": 0.00019447805180704205,
      "loss": 0.2773,
      "step": 717
    },
    {
      "epoch": 0.027836200629997577,
      "grad_norm": 0.1534816473722458,
      "learning_rate": 0.00019447029626182722,
      "loss": 0.3123,
      "step": 718
    },
    {
      "epoch": 0.027874969711654957,
      "grad_norm": 0.15622659027576447,
      "learning_rate": 0.0001944625407166124,
      "loss": 0.3614,
      "step": 719
    },
    {
      "epoch": 0.027913738793312333,
      "grad_norm": 0.1261596381664276,
      "learning_rate": 0.00019445478517139756,
      "loss": 0.2446,
      "step": 720
    },
    {
      "epoch": 0.027952507874969713,
      "grad_norm": 0.1306779533624649,
      "learning_rate": 0.00019444702962618274,
      "loss": 0.3019,
      "step": 721
    },
    {
      "epoch": 0.02799127695662709,
      "grad_norm": 0.1595265120267868,
      "learning_rate": 0.00019443927408096788,
      "loss": 0.3433,
      "step": 722
    },
    {
      "epoch": 0.02803004603828447,
      "grad_norm": 0.172409325838089,
      "learning_rate": 0.00019443151853575308,
      "loss": 0.3266,
      "step": 723
    },
    {
      "epoch": 0.028068815119941845,
      "grad_norm": 0.10989250987768173,
      "learning_rate": 0.00019442376299053823,
      "loss": 0.2693,
      "step": 724
    },
    {
      "epoch": 0.028107584201599225,
      "grad_norm": 0.16622808575630188,
      "learning_rate": 0.00019441600744532343,
      "loss": 0.3929,
      "step": 725
    },
    {
      "epoch": 0.0281463532832566,
      "grad_norm": 0.16404926776885986,
      "learning_rate": 0.00019440825190010857,
      "loss": 0.3698,
      "step": 726
    },
    {
      "epoch": 0.02818512236491398,
      "grad_norm": 0.15075446665287018,
      "learning_rate": 0.00019440049635489377,
      "loss": 0.3951,
      "step": 727
    },
    {
      "epoch": 0.02822389144657136,
      "grad_norm": 0.16445392370224,
      "learning_rate": 0.00019439274080967892,
      "loss": 0.3701,
      "step": 728
    },
    {
      "epoch": 0.028262660528228737,
      "grad_norm": 0.17430756986141205,
      "learning_rate": 0.0001943849852644641,
      "loss": 0.2421,
      "step": 729
    },
    {
      "epoch": 0.028301429609886117,
      "grad_norm": 0.1404775232076645,
      "learning_rate": 0.00019437722971924926,
      "loss": 0.2956,
      "step": 730
    },
    {
      "epoch": 0.028340198691543493,
      "grad_norm": 0.14161787927150726,
      "learning_rate": 0.00019436947417403444,
      "loss": 0.3527,
      "step": 731
    },
    {
      "epoch": 0.028378967773200873,
      "grad_norm": 0.11306299269199371,
      "learning_rate": 0.0001943617186288196,
      "loss": 0.2519,
      "step": 732
    },
    {
      "epoch": 0.02841773685485825,
      "grad_norm": 0.18045757710933685,
      "learning_rate": 0.00019435396308360478,
      "loss": 0.3603,
      "step": 733
    },
    {
      "epoch": 0.02845650593651563,
      "grad_norm": 0.11096648126840591,
      "learning_rate": 0.00019434620753838996,
      "loss": 0.2622,
      "step": 734
    },
    {
      "epoch": 0.02849527501817301,
      "grad_norm": 0.16388414800167084,
      "learning_rate": 0.00019433845199317513,
      "loss": 0.3716,
      "step": 735
    },
    {
      "epoch": 0.028534044099830385,
      "grad_norm": 0.1342810094356537,
      "learning_rate": 0.0001943306964479603,
      "loss": 0.2954,
      "step": 736
    },
    {
      "epoch": 0.028572813181487765,
      "grad_norm": 0.1282866895198822,
      "learning_rate": 0.00019432294090274547,
      "loss": 0.3561,
      "step": 737
    },
    {
      "epoch": 0.02861158226314514,
      "grad_norm": 0.16172905266284943,
      "learning_rate": 0.00019431518535753065,
      "loss": 0.3447,
      "step": 738
    },
    {
      "epoch": 0.02865035134480252,
      "grad_norm": 0.14611168205738068,
      "learning_rate": 0.00019430742981231582,
      "loss": 0.3186,
      "step": 739
    },
    {
      "epoch": 0.028689120426459897,
      "grad_norm": 0.13278616964817047,
      "learning_rate": 0.000194299674267101,
      "loss": 0.2807,
      "step": 740
    },
    {
      "epoch": 0.028727889508117277,
      "grad_norm": 0.12437865138053894,
      "learning_rate": 0.00019429191872188617,
      "loss": 0.3152,
      "step": 741
    },
    {
      "epoch": 0.028766658589774653,
      "grad_norm": 0.13067172467708588,
      "learning_rate": 0.00019428416317667134,
      "loss": 0.2745,
      "step": 742
    },
    {
      "epoch": 0.028805427671432033,
      "grad_norm": 0.14089664816856384,
      "learning_rate": 0.00019427640763145648,
      "loss": 0.3293,
      "step": 743
    },
    {
      "epoch": 0.028844196753089413,
      "grad_norm": 0.13951994478702545,
      "learning_rate": 0.00019426865208624168,
      "loss": 0.2769,
      "step": 744
    },
    {
      "epoch": 0.02888296583474679,
      "grad_norm": 0.1485651135444641,
      "learning_rate": 0.00019426089654102683,
      "loss": 0.3015,
      "step": 745
    },
    {
      "epoch": 0.02892173491640417,
      "grad_norm": 0.17117519676685333,
      "learning_rate": 0.00019425314099581203,
      "loss": 0.3551,
      "step": 746
    },
    {
      "epoch": 0.028960503998061545,
      "grad_norm": 0.11147894710302353,
      "learning_rate": 0.00019424538545059718,
      "loss": 0.2338,
      "step": 747
    },
    {
      "epoch": 0.028999273079718925,
      "grad_norm": 0.09206092357635498,
      "learning_rate": 0.00019423762990538237,
      "loss": 0.2328,
      "step": 748
    },
    {
      "epoch": 0.0290380421613763,
      "grad_norm": 0.16016414761543274,
      "learning_rate": 0.00019422987436016752,
      "loss": 0.2821,
      "step": 749
    },
    {
      "epoch": 0.02907681124303368,
      "grad_norm": 0.17423999309539795,
      "learning_rate": 0.0001942221188149527,
      "loss": 0.2821,
      "step": 750
    },
    {
      "epoch": 0.029115580324691057,
      "grad_norm": 0.1300264298915863,
      "learning_rate": 0.00019421436326973787,
      "loss": 0.3228,
      "step": 751
    },
    {
      "epoch": 0.029154349406348437,
      "grad_norm": 0.1329835206270218,
      "learning_rate": 0.00019420660772452304,
      "loss": 0.2264,
      "step": 752
    },
    {
      "epoch": 0.029193118488005817,
      "grad_norm": 0.19674628973007202,
      "learning_rate": 0.0001941988521793082,
      "loss": 0.2992,
      "step": 753
    },
    {
      "epoch": 0.029231887569663193,
      "grad_norm": 0.15477760136127472,
      "learning_rate": 0.00019419109663409338,
      "loss": 0.3778,
      "step": 754
    },
    {
      "epoch": 0.029270656651320573,
      "grad_norm": 0.14475341141223907,
      "learning_rate": 0.00019418334108887856,
      "loss": 0.315,
      "step": 755
    },
    {
      "epoch": 0.02930942573297795,
      "grad_norm": 0.12399698048830032,
      "learning_rate": 0.00019417558554366373,
      "loss": 0.2596,
      "step": 756
    },
    {
      "epoch": 0.02934819481463533,
      "grad_norm": 0.14044028520584106,
      "learning_rate": 0.0001941678299984489,
      "loss": 0.2607,
      "step": 757
    },
    {
      "epoch": 0.029386963896292705,
      "grad_norm": 0.13607138395309448,
      "learning_rate": 0.00019416007445323408,
      "loss": 0.3329,
      "step": 758
    },
    {
      "epoch": 0.029425732977950085,
      "grad_norm": 0.13331790268421173,
      "learning_rate": 0.00019415231890801925,
      "loss": 0.3252,
      "step": 759
    },
    {
      "epoch": 0.02946450205960746,
      "grad_norm": 0.12203644961118698,
      "learning_rate": 0.00019414456336280442,
      "loss": 0.3277,
      "step": 760
    },
    {
      "epoch": 0.02950327114126484,
      "grad_norm": 0.13406938314437866,
      "learning_rate": 0.0001941368078175896,
      "loss": 0.3286,
      "step": 761
    },
    {
      "epoch": 0.02954204022292222,
      "grad_norm": 0.12544918060302734,
      "learning_rate": 0.00019412905227237477,
      "loss": 0.2961,
      "step": 762
    },
    {
      "epoch": 0.029580809304579597,
      "grad_norm": 0.10152918100357056,
      "learning_rate": 0.00019412129672715994,
      "loss": 0.2956,
      "step": 763
    },
    {
      "epoch": 0.029619578386236977,
      "grad_norm": 0.10760189592838287,
      "learning_rate": 0.00019411354118194509,
      "loss": 0.2938,
      "step": 764
    },
    {
      "epoch": 0.029658347467894353,
      "grad_norm": 0.12857438623905182,
      "learning_rate": 0.00019410578563673029,
      "loss": 0.281,
      "step": 765
    },
    {
      "epoch": 0.029697116549551733,
      "grad_norm": 0.11939510703086853,
      "learning_rate": 0.00019409803009151543,
      "loss": 0.3229,
      "step": 766
    },
    {
      "epoch": 0.02973588563120911,
      "grad_norm": 0.10003798454999924,
      "learning_rate": 0.00019409027454630063,
      "loss": 0.2575,
      "step": 767
    },
    {
      "epoch": 0.02977465471286649,
      "grad_norm": 0.14389479160308838,
      "learning_rate": 0.00019408251900108578,
      "loss": 0.3423,
      "step": 768
    },
    {
      "epoch": 0.02981342379452387,
      "grad_norm": 0.11190913617610931,
      "learning_rate": 0.00019407476345587098,
      "loss": 0.3013,
      "step": 769
    },
    {
      "epoch": 0.029852192876181245,
      "grad_norm": 0.1256823092699051,
      "learning_rate": 0.00019406700791065612,
      "loss": 0.2944,
      "step": 770
    },
    {
      "epoch": 0.029890961957838625,
      "grad_norm": 0.13585689663887024,
      "learning_rate": 0.0001940592523654413,
      "loss": 0.3719,
      "step": 771
    },
    {
      "epoch": 0.029929731039496,
      "grad_norm": 0.14416959881782532,
      "learning_rate": 0.00019405149682022647,
      "loss": 0.3678,
      "step": 772
    },
    {
      "epoch": 0.02996850012115338,
      "grad_norm": 0.15254203975200653,
      "learning_rate": 0.00019404374127501164,
      "loss": 0.3206,
      "step": 773
    },
    {
      "epoch": 0.030007269202810757,
      "grad_norm": 0.12151040136814117,
      "learning_rate": 0.00019403598572979681,
      "loss": 0.2652,
      "step": 774
    },
    {
      "epoch": 0.030046038284468137,
      "grad_norm": 0.14080436527729034,
      "learning_rate": 0.000194028230184582,
      "loss": 0.3002,
      "step": 775
    },
    {
      "epoch": 0.030084807366125513,
      "grad_norm": 0.15175214409828186,
      "learning_rate": 0.00019402047463936716,
      "loss": 0.3845,
      "step": 776
    },
    {
      "epoch": 0.030123576447782893,
      "grad_norm": 0.15180891752243042,
      "learning_rate": 0.00019401271909415233,
      "loss": 0.3847,
      "step": 777
    },
    {
      "epoch": 0.030162345529440273,
      "grad_norm": 0.13608646392822266,
      "learning_rate": 0.00019400496354893748,
      "loss": 0.3181,
      "step": 778
    },
    {
      "epoch": 0.03020111461109765,
      "grad_norm": 0.140138640999794,
      "learning_rate": 0.00019399720800372268,
      "loss": 0.3499,
      "step": 779
    },
    {
      "epoch": 0.03023988369275503,
      "grad_norm": 0.12361879646778107,
      "learning_rate": 0.00019398945245850782,
      "loss": 0.3572,
      "step": 780
    },
    {
      "epoch": 0.030278652774412405,
      "grad_norm": 0.1549651175737381,
      "learning_rate": 0.00019398169691329302,
      "loss": 0.3877,
      "step": 781
    },
    {
      "epoch": 0.030317421856069785,
      "grad_norm": 0.15109595656394958,
      "learning_rate": 0.00019397394136807817,
      "loss": 0.3705,
      "step": 782
    },
    {
      "epoch": 0.03035619093772716,
      "grad_norm": 0.14399391412734985,
      "learning_rate": 0.00019396618582286337,
      "loss": 0.3274,
      "step": 783
    },
    {
      "epoch": 0.03039496001938454,
      "grad_norm": 0.10926223546266556,
      "learning_rate": 0.00019395843027764854,
      "loss": 0.2712,
      "step": 784
    },
    {
      "epoch": 0.030433729101041918,
      "grad_norm": 0.11028644442558289,
      "learning_rate": 0.0001939506747324337,
      "loss": 0.2856,
      "step": 785
    },
    {
      "epoch": 0.030472498182699297,
      "grad_norm": 0.12433174252510071,
      "learning_rate": 0.0001939429191872189,
      "loss": 0.346,
      "step": 786
    },
    {
      "epoch": 0.030511267264356677,
      "grad_norm": 0.12495725601911545,
      "learning_rate": 0.00019393516364200403,
      "loss": 0.2653,
      "step": 787
    },
    {
      "epoch": 0.030550036346014053,
      "grad_norm": 0.1321128010749817,
      "learning_rate": 0.00019392740809678923,
      "loss": 0.3759,
      "step": 788
    },
    {
      "epoch": 0.030588805427671433,
      "grad_norm": 0.13081303238868713,
      "learning_rate": 0.00019391965255157438,
      "loss": 0.3364,
      "step": 789
    },
    {
      "epoch": 0.03062757450932881,
      "grad_norm": 0.12332726269960403,
      "learning_rate": 0.00019391189700635958,
      "loss": 0.311,
      "step": 790
    },
    {
      "epoch": 0.03066634359098619,
      "grad_norm": 0.10232312977313995,
      "learning_rate": 0.00019390414146114472,
      "loss": 0.2449,
      "step": 791
    },
    {
      "epoch": 0.030705112672643566,
      "grad_norm": 0.13930626213550568,
      "learning_rate": 0.0001938963859159299,
      "loss": 0.3776,
      "step": 792
    },
    {
      "epoch": 0.030743881754300945,
      "grad_norm": 0.1118914932012558,
      "learning_rate": 0.00019388863037071507,
      "loss": 0.2767,
      "step": 793
    },
    {
      "epoch": 0.03078265083595832,
      "grad_norm": 0.13433198630809784,
      "learning_rate": 0.00019388087482550024,
      "loss": 0.2965,
      "step": 794
    },
    {
      "epoch": 0.0308214199176157,
      "grad_norm": 0.14880095422267914,
      "learning_rate": 0.00019387311928028542,
      "loss": 0.3768,
      "step": 795
    },
    {
      "epoch": 0.03086018899927308,
      "grad_norm": 0.1496611088514328,
      "learning_rate": 0.0001938653637350706,
      "loss": 0.3296,
      "step": 796
    },
    {
      "epoch": 0.030898958080930457,
      "grad_norm": 0.10669679194688797,
      "learning_rate": 0.00019385760818985576,
      "loss": 0.2565,
      "step": 797
    },
    {
      "epoch": 0.030937727162587837,
      "grad_norm": 0.14221236109733582,
      "learning_rate": 0.00019384985264464093,
      "loss": 0.4157,
      "step": 798
    },
    {
      "epoch": 0.030976496244245214,
      "grad_norm": 0.1260707527399063,
      "learning_rate": 0.00019384209709942608,
      "loss": 0.3254,
      "step": 799
    },
    {
      "epoch": 0.031015265325902593,
      "grad_norm": 0.1774669736623764,
      "learning_rate": 0.00019383434155421128,
      "loss": 0.3883,
      "step": 800
    },
    {
      "epoch": 0.03105403440755997,
      "grad_norm": 0.13095124065876007,
      "learning_rate": 0.00019382658600899643,
      "loss": 0.3114,
      "step": 801
    },
    {
      "epoch": 0.03109280348921735,
      "grad_norm": 0.12306799739599228,
      "learning_rate": 0.00019381883046378163,
      "loss": 0.3527,
      "step": 802
    },
    {
      "epoch": 0.031131572570874726,
      "grad_norm": 0.11955998092889786,
      "learning_rate": 0.00019381107491856677,
      "loss": 0.252,
      "step": 803
    },
    {
      "epoch": 0.031170341652532105,
      "grad_norm": 0.1580415964126587,
      "learning_rate": 0.00019380331937335197,
      "loss": 0.4179,
      "step": 804
    },
    {
      "epoch": 0.031209110734189485,
      "grad_norm": 0.14099563658237457,
      "learning_rate": 0.00019379556382813712,
      "loss": 0.3417,
      "step": 805
    },
    {
      "epoch": 0.03124787981584686,
      "grad_norm": 0.13748401403427124,
      "learning_rate": 0.0001937878082829223,
      "loss": 0.2761,
      "step": 806
    },
    {
      "epoch": 0.03128664889750424,
      "grad_norm": 0.12424538284540176,
      "learning_rate": 0.00019378005273770746,
      "loss": 0.2977,
      "step": 807
    },
    {
      "epoch": 0.03132541797916162,
      "grad_norm": 0.18095247447490692,
      "learning_rate": 0.00019377229719249263,
      "loss": 0.3268,
      "step": 808
    },
    {
      "epoch": 0.031364187060819,
      "grad_norm": 0.13995100557804108,
      "learning_rate": 0.0001937645416472778,
      "loss": 0.3692,
      "step": 809
    },
    {
      "epoch": 0.031402956142476374,
      "grad_norm": 0.12743808329105377,
      "learning_rate": 0.00019375678610206298,
      "loss": 0.2646,
      "step": 810
    },
    {
      "epoch": 0.03144172522413375,
      "grad_norm": 0.12953948974609375,
      "learning_rate": 0.00019374903055684815,
      "loss": 0.3118,
      "step": 811
    },
    {
      "epoch": 0.03148049430579113,
      "grad_norm": 0.11457757651805878,
      "learning_rate": 0.00019374127501163333,
      "loss": 0.2517,
      "step": 812
    },
    {
      "epoch": 0.03151926338744851,
      "grad_norm": 0.13526013493537903,
      "learning_rate": 0.0001937335194664185,
      "loss": 0.32,
      "step": 813
    },
    {
      "epoch": 0.031558032469105886,
      "grad_norm": 0.16262298822402954,
      "learning_rate": 0.00019372576392120367,
      "loss": 0.3082,
      "step": 814
    },
    {
      "epoch": 0.03159680155076327,
      "grad_norm": 0.11659786850214005,
      "learning_rate": 0.00019371800837598884,
      "loss": 0.2854,
      "step": 815
    },
    {
      "epoch": 0.031635570632420645,
      "grad_norm": 0.09954003989696503,
      "learning_rate": 0.00019371025283077402,
      "loss": 0.1976,
      "step": 816
    },
    {
      "epoch": 0.03167433971407802,
      "grad_norm": 0.12900231778621674,
      "learning_rate": 0.0001937024972855592,
      "loss": 0.3197,
      "step": 817
    },
    {
      "epoch": 0.0317131087957354,
      "grad_norm": 0.1315697580575943,
      "learning_rate": 0.00019369474174034436,
      "loss": 0.3144,
      "step": 818
    },
    {
      "epoch": 0.03175187787739278,
      "grad_norm": 0.1196579709649086,
      "learning_rate": 0.00019368698619512954,
      "loss": 0.2948,
      "step": 819
    },
    {
      "epoch": 0.03179064695905016,
      "grad_norm": 0.1263587772846222,
      "learning_rate": 0.00019367923064991468,
      "loss": 0.3407,
      "step": 820
    },
    {
      "epoch": 0.031829416040707534,
      "grad_norm": 0.13021977245807648,
      "learning_rate": 0.00019367147510469988,
      "loss": 0.3311,
      "step": 821
    },
    {
      "epoch": 0.03186818512236492,
      "grad_norm": 0.141276016831398,
      "learning_rate": 0.00019366371955948503,
      "loss": 0.3284,
      "step": 822
    },
    {
      "epoch": 0.03190695420402229,
      "grad_norm": 0.14128659665584564,
      "learning_rate": 0.00019365596401427023,
      "loss": 0.3306,
      "step": 823
    },
    {
      "epoch": 0.03194572328567967,
      "grad_norm": 0.15174193680286407,
      "learning_rate": 0.00019364820846905537,
      "loss": 0.2994,
      "step": 824
    },
    {
      "epoch": 0.031984492367337046,
      "grad_norm": 0.1263010948896408,
      "learning_rate": 0.00019364045292384057,
      "loss": 0.2995,
      "step": 825
    },
    {
      "epoch": 0.03202326144899443,
      "grad_norm": 0.13424323499202728,
      "learning_rate": 0.00019363269737862572,
      "loss": 0.2979,
      "step": 826
    },
    {
      "epoch": 0.032062030530651806,
      "grad_norm": 0.12265142053365707,
      "learning_rate": 0.0001936249418334109,
      "loss": 0.3122,
      "step": 827
    },
    {
      "epoch": 0.03210079961230918,
      "grad_norm": 0.1278090924024582,
      "learning_rate": 0.00019361718628819606,
      "loss": 0.2711,
      "step": 828
    },
    {
      "epoch": 0.032139568693966565,
      "grad_norm": 0.13018886744976044,
      "learning_rate": 0.00019360943074298124,
      "loss": 0.3368,
      "step": 829
    },
    {
      "epoch": 0.03217833777562394,
      "grad_norm": 0.12724225223064423,
      "learning_rate": 0.0001936016751977664,
      "loss": 0.3063,
      "step": 830
    },
    {
      "epoch": 0.03221710685728132,
      "grad_norm": 0.15339180827140808,
      "learning_rate": 0.00019359391965255158,
      "loss": 0.3721,
      "step": 831
    },
    {
      "epoch": 0.032255875938938694,
      "grad_norm": 0.1422792226076126,
      "learning_rate": 0.00019358616410733676,
      "loss": 0.3881,
      "step": 832
    },
    {
      "epoch": 0.03229464502059608,
      "grad_norm": 0.13750825822353363,
      "learning_rate": 0.00019357840856212193,
      "loss": 0.251,
      "step": 833
    },
    {
      "epoch": 0.032333414102253454,
      "grad_norm": 0.1290092170238495,
      "learning_rate": 0.0001935706530169071,
      "loss": 0.2739,
      "step": 834
    },
    {
      "epoch": 0.03237218318391083,
      "grad_norm": 0.12932749092578888,
      "learning_rate": 0.00019356289747169227,
      "loss": 0.3215,
      "step": 835
    },
    {
      "epoch": 0.032410952265568206,
      "grad_norm": 0.11094553768634796,
      "learning_rate": 0.00019355514192647745,
      "loss": 0.2275,
      "step": 836
    },
    {
      "epoch": 0.03244972134722559,
      "grad_norm": 0.14805321395397186,
      "learning_rate": 0.00019354738638126262,
      "loss": 0.3593,
      "step": 837
    },
    {
      "epoch": 0.032488490428882966,
      "grad_norm": 0.14882820844650269,
      "learning_rate": 0.0001935396308360478,
      "loss": 0.3375,
      "step": 838
    },
    {
      "epoch": 0.03252725951054034,
      "grad_norm": 0.16672062873840332,
      "learning_rate": 0.00019353187529083296,
      "loss": 0.275,
      "step": 839
    },
    {
      "epoch": 0.032566028592197725,
      "grad_norm": 0.12603729963302612,
      "learning_rate": 0.00019352411974561814,
      "loss": 0.2393,
      "step": 840
    },
    {
      "epoch": 0.0326047976738551,
      "grad_norm": 0.13296982645988464,
      "learning_rate": 0.00019351636420040328,
      "loss": 0.3255,
      "step": 841
    },
    {
      "epoch": 0.03264356675551248,
      "grad_norm": 0.1464458554983139,
      "learning_rate": 0.00019350860865518848,
      "loss": 0.4239,
      "step": 842
    },
    {
      "epoch": 0.032682335837169854,
      "grad_norm": 0.13616709411144257,
      "learning_rate": 0.00019350085310997363,
      "loss": 0.3559,
      "step": 843
    },
    {
      "epoch": 0.03272110491882724,
      "grad_norm": 0.1758919507265091,
      "learning_rate": 0.00019349309756475883,
      "loss": 0.3302,
      "step": 844
    },
    {
      "epoch": 0.032759874000484614,
      "grad_norm": 0.14538666605949402,
      "learning_rate": 0.00019348534201954397,
      "loss": 0.3616,
      "step": 845
    },
    {
      "epoch": 0.03279864308214199,
      "grad_norm": 0.1322665959596634,
      "learning_rate": 0.00019347758647432917,
      "loss": 0.2803,
      "step": 846
    },
    {
      "epoch": 0.03283741216379937,
      "grad_norm": 0.1375095695257187,
      "learning_rate": 0.00019346983092911432,
      "loss": 0.369,
      "step": 847
    },
    {
      "epoch": 0.03287618124545675,
      "grad_norm": 0.12812593579292297,
      "learning_rate": 0.0001934620753838995,
      "loss": 0.2961,
      "step": 848
    },
    {
      "epoch": 0.032914950327114126,
      "grad_norm": 0.16398930549621582,
      "learning_rate": 0.00019345431983868467,
      "loss": 0.3508,
      "step": 849
    },
    {
      "epoch": 0.0329537194087715,
      "grad_norm": 0.13245096802711487,
      "learning_rate": 0.00019344656429346984,
      "loss": 0.3176,
      "step": 850
    },
    {
      "epoch": 0.032992488490428885,
      "grad_norm": 0.14063836634159088,
      "learning_rate": 0.000193438808748255,
      "loss": 0.3785,
      "step": 851
    },
    {
      "epoch": 0.03303125757208626,
      "grad_norm": 0.14482952654361725,
      "learning_rate": 0.00019343105320304018,
      "loss": 0.285,
      "step": 852
    },
    {
      "epoch": 0.03307002665374364,
      "grad_norm": 0.12712760269641876,
      "learning_rate": 0.00019342329765782536,
      "loss": 0.3573,
      "step": 853
    },
    {
      "epoch": 0.033108795735401014,
      "grad_norm": 0.12327287346124649,
      "learning_rate": 0.00019341554211261053,
      "loss": 0.3054,
      "step": 854
    },
    {
      "epoch": 0.0331475648170584,
      "grad_norm": 0.16542963683605194,
      "learning_rate": 0.00019340778656739568,
      "loss": 0.3448,
      "step": 855
    },
    {
      "epoch": 0.033186333898715774,
      "grad_norm": 0.15825551748275757,
      "learning_rate": 0.00019340003102218088,
      "loss": 0.3703,
      "step": 856
    },
    {
      "epoch": 0.03322510298037315,
      "grad_norm": 0.14353562891483307,
      "learning_rate": 0.00019339227547696602,
      "loss": 0.3632,
      "step": 857
    },
    {
      "epoch": 0.03326387206203053,
      "grad_norm": 0.10636740922927856,
      "learning_rate": 0.00019338451993175122,
      "loss": 0.2106,
      "step": 858
    },
    {
      "epoch": 0.03330264114368791,
      "grad_norm": 0.17433485388755798,
      "learning_rate": 0.00019337676438653637,
      "loss": 0.3621,
      "step": 859
    },
    {
      "epoch": 0.033341410225345286,
      "grad_norm": 0.1711527407169342,
      "learning_rate": 0.00019336900884132157,
      "loss": 0.3832,
      "step": 860
    },
    {
      "epoch": 0.03338017930700266,
      "grad_norm": 0.14198794960975647,
      "learning_rate": 0.0001933612532961067,
      "loss": 0.367,
      "step": 861
    },
    {
      "epoch": 0.033418948388660046,
      "grad_norm": 0.12300233542919159,
      "learning_rate": 0.00019335349775089189,
      "loss": 0.2441,
      "step": 862
    },
    {
      "epoch": 0.03345771747031742,
      "grad_norm": 0.1794155389070511,
      "learning_rate": 0.00019334574220567709,
      "loss": 0.4896,
      "step": 863
    },
    {
      "epoch": 0.0334964865519748,
      "grad_norm": 0.13505582511425018,
      "learning_rate": 0.00019333798666046223,
      "loss": 0.2897,
      "step": 864
    },
    {
      "epoch": 0.03353525563363218,
      "grad_norm": 0.14538222551345825,
      "learning_rate": 0.00019333023111524743,
      "loss": 0.4278,
      "step": 865
    },
    {
      "epoch": 0.03357402471528956,
      "grad_norm": 0.11059290915727615,
      "learning_rate": 0.00019332247557003258,
      "loss": 0.3035,
      "step": 866
    },
    {
      "epoch": 0.033612793796946934,
      "grad_norm": 0.1608155518770218,
      "learning_rate": 0.00019331472002481778,
      "loss": 0.4189,
      "step": 867
    },
    {
      "epoch": 0.03365156287860431,
      "grad_norm": 0.17251229286193848,
      "learning_rate": 0.00019330696447960292,
      "loss": 0.4307,
      "step": 868
    },
    {
      "epoch": 0.033690331960261694,
      "grad_norm": 0.1353481113910675,
      "learning_rate": 0.0001932992089343881,
      "loss": 0.2828,
      "step": 869
    },
    {
      "epoch": 0.03372910104191907,
      "grad_norm": 0.12291131168603897,
      "learning_rate": 0.00019329145338917327,
      "loss": 0.3265,
      "step": 870
    },
    {
      "epoch": 0.033767870123576446,
      "grad_norm": 0.18789708614349365,
      "learning_rate": 0.00019328369784395844,
      "loss": 0.2899,
      "step": 871
    },
    {
      "epoch": 0.03380663920523383,
      "grad_norm": 0.14935733377933502,
      "learning_rate": 0.0001932759422987436,
      "loss": 0.3224,
      "step": 872
    },
    {
      "epoch": 0.033845408286891206,
      "grad_norm": 0.11495187133550644,
      "learning_rate": 0.00019326818675352879,
      "loss": 0.25,
      "step": 873
    },
    {
      "epoch": 0.03388417736854858,
      "grad_norm": 0.13951095938682556,
      "learning_rate": 0.00019326043120831396,
      "loss": 0.328,
      "step": 874
    },
    {
      "epoch": 0.03392294645020596,
      "grad_norm": 0.1837310940027237,
      "learning_rate": 0.00019325267566309913,
      "loss": 0.3429,
      "step": 875
    },
    {
      "epoch": 0.03396171553186334,
      "grad_norm": 0.13523560762405396,
      "learning_rate": 0.00019324492011788428,
      "loss": 0.3,
      "step": 876
    },
    {
      "epoch": 0.03400048461352072,
      "grad_norm": 0.1666059046983719,
      "learning_rate": 0.00019323716457266948,
      "loss": 0.3698,
      "step": 877
    },
    {
      "epoch": 0.034039253695178094,
      "grad_norm": 0.16425132751464844,
      "learning_rate": 0.00019322940902745462,
      "loss": 0.376,
      "step": 878
    },
    {
      "epoch": 0.03407802277683547,
      "grad_norm": 0.1665574461221695,
      "learning_rate": 0.00019322165348223982,
      "loss": 0.3563,
      "step": 879
    },
    {
      "epoch": 0.034116791858492854,
      "grad_norm": 0.13653564453125,
      "learning_rate": 0.00019321389793702497,
      "loss": 0.3255,
      "step": 880
    },
    {
      "epoch": 0.03415556094015023,
      "grad_norm": 0.1572851687669754,
      "learning_rate": 0.00019320614239181017,
      "loss": 0.3176,
      "step": 881
    },
    {
      "epoch": 0.034194330021807606,
      "grad_norm": 0.18107907474040985,
      "learning_rate": 0.00019319838684659531,
      "loss": 0.3676,
      "step": 882
    },
    {
      "epoch": 0.03423309910346499,
      "grad_norm": 0.12038631737232208,
      "learning_rate": 0.0001931906313013805,
      "loss": 0.2951,
      "step": 883
    },
    {
      "epoch": 0.034271868185122366,
      "grad_norm": 0.14985930919647217,
      "learning_rate": 0.00019318287575616566,
      "loss": 0.2762,
      "step": 884
    },
    {
      "epoch": 0.03431063726677974,
      "grad_norm": 0.13211242854595184,
      "learning_rate": 0.00019317512021095083,
      "loss": 0.3015,
      "step": 885
    },
    {
      "epoch": 0.03434940634843712,
      "grad_norm": 0.09504321962594986,
      "learning_rate": 0.000193167364665736,
      "loss": 0.176,
      "step": 886
    },
    {
      "epoch": 0.0343881754300945,
      "grad_norm": 0.17940092086791992,
      "learning_rate": 0.00019315960912052118,
      "loss": 0.328,
      "step": 887
    },
    {
      "epoch": 0.03442694451175188,
      "grad_norm": 0.1794150322675705,
      "learning_rate": 0.00019315185357530635,
      "loss": 0.352,
      "step": 888
    },
    {
      "epoch": 0.034465713593409254,
      "grad_norm": 0.14826002717018127,
      "learning_rate": 0.00019314409803009152,
      "loss": 0.3372,
      "step": 889
    },
    {
      "epoch": 0.03450448267506664,
      "grad_norm": 0.1436523199081421,
      "learning_rate": 0.0001931363424848767,
      "loss": 0.3389,
      "step": 890
    },
    {
      "epoch": 0.034543251756724014,
      "grad_norm": 0.17238236963748932,
      "learning_rate": 0.00019312858693966187,
      "loss": 0.3438,
      "step": 891
    },
    {
      "epoch": 0.03458202083838139,
      "grad_norm": 0.1298636496067047,
      "learning_rate": 0.00019312083139444704,
      "loss": 0.2474,
      "step": 892
    },
    {
      "epoch": 0.034620789920038766,
      "grad_norm": 0.14230576157569885,
      "learning_rate": 0.00019311307584923222,
      "loss": 0.3312,
      "step": 893
    },
    {
      "epoch": 0.03465955900169615,
      "grad_norm": 0.11337213963270187,
      "learning_rate": 0.0001931053203040174,
      "loss": 0.2412,
      "step": 894
    },
    {
      "epoch": 0.034698328083353526,
      "grad_norm": 0.15697747468948364,
      "learning_rate": 0.00019309756475880256,
      "loss": 0.3001,
      "step": 895
    },
    {
      "epoch": 0.0347370971650109,
      "grad_norm": 0.13204480707645416,
      "learning_rate": 0.00019308980921358773,
      "loss": 0.2832,
      "step": 896
    },
    {
      "epoch": 0.03477586624666828,
      "grad_norm": 0.15225596725940704,
      "learning_rate": 0.00019308205366837288,
      "loss": 0.3051,
      "step": 897
    },
    {
      "epoch": 0.03481463532832566,
      "grad_norm": 0.17493227124214172,
      "learning_rate": 0.00019307429812315808,
      "loss": 0.2377,
      "step": 898
    },
    {
      "epoch": 0.03485340440998304,
      "grad_norm": 0.12631157040596008,
      "learning_rate": 0.00019306654257794322,
      "loss": 0.2516,
      "step": 899
    },
    {
      "epoch": 0.034892173491640414,
      "grad_norm": 0.1723453253507614,
      "learning_rate": 0.00019305878703272842,
      "loss": 0.3129,
      "step": 900
    },
    {
      "epoch": 0.0349309425732978,
      "grad_norm": 0.13765284419059753,
      "learning_rate": 0.00019305103148751357,
      "loss": 0.2722,
      "step": 901
    },
    {
      "epoch": 0.034969711654955174,
      "grad_norm": 0.13655132055282593,
      "learning_rate": 0.00019304327594229877,
      "loss": 0.2474,
      "step": 902
    },
    {
      "epoch": 0.03500848073661255,
      "grad_norm": 0.128972589969635,
      "learning_rate": 0.00019303552039708392,
      "loss": 0.321,
      "step": 903
    },
    {
      "epoch": 0.03504724981826993,
      "grad_norm": 0.1534874141216278,
      "learning_rate": 0.0001930277648518691,
      "loss": 0.3629,
      "step": 904
    },
    {
      "epoch": 0.03508601889992731,
      "grad_norm": 0.18492062389850616,
      "learning_rate": 0.00019302000930665426,
      "loss": 0.3327,
      "step": 905
    },
    {
      "epoch": 0.035124787981584686,
      "grad_norm": 0.1161053329706192,
      "learning_rate": 0.00019301225376143943,
      "loss": 0.2649,
      "step": 906
    },
    {
      "epoch": 0.03516355706324206,
      "grad_norm": 0.25473079085350037,
      "learning_rate": 0.0001930044982162246,
      "loss": 0.3095,
      "step": 907
    },
    {
      "epoch": 0.035202326144899446,
      "grad_norm": 0.13101612031459808,
      "learning_rate": 0.00019299674267100978,
      "loss": 0.2989,
      "step": 908
    },
    {
      "epoch": 0.03524109522655682,
      "grad_norm": 0.229856938123703,
      "learning_rate": 0.00019298898712579495,
      "loss": 0.2952,
      "step": 909
    },
    {
      "epoch": 0.0352798643082142,
      "grad_norm": 0.16036251187324524,
      "learning_rate": 0.00019298123158058013,
      "loss": 0.3277,
      "step": 910
    },
    {
      "epoch": 0.035318633389871575,
      "grad_norm": 0.1423872709274292,
      "learning_rate": 0.0001929734760353653,
      "loss": 0.3075,
      "step": 911
    },
    {
      "epoch": 0.03535740247152896,
      "grad_norm": 0.18285229802131653,
      "learning_rate": 0.00019296572049015047,
      "loss": 0.3212,
      "step": 912
    },
    {
      "epoch": 0.035396171553186334,
      "grad_norm": 0.10555001348257065,
      "learning_rate": 0.00019295796494493564,
      "loss": 0.2837,
      "step": 913
    },
    {
      "epoch": 0.03543494063484371,
      "grad_norm": 0.13066385686397552,
      "learning_rate": 0.00019295020939972082,
      "loss": 0.3037,
      "step": 914
    },
    {
      "epoch": 0.035473709716501094,
      "grad_norm": 0.13312584161758423,
      "learning_rate": 0.000192942453854506,
      "loss": 0.288,
      "step": 915
    },
    {
      "epoch": 0.03551247879815847,
      "grad_norm": 0.1309642493724823,
      "learning_rate": 0.00019293469830929116,
      "loss": 0.2697,
      "step": 916
    },
    {
      "epoch": 0.035551247879815846,
      "grad_norm": 0.13193777203559875,
      "learning_rate": 0.00019292694276407634,
      "loss": 0.3214,
      "step": 917
    },
    {
      "epoch": 0.03559001696147322,
      "grad_norm": 0.15440607070922852,
      "learning_rate": 0.00019291918721886148,
      "loss": 0.3387,
      "step": 918
    },
    {
      "epoch": 0.035628786043130606,
      "grad_norm": 0.17489242553710938,
      "learning_rate": 0.00019291143167364668,
      "loss": 0.3489,
      "step": 919
    },
    {
      "epoch": 0.03566755512478798,
      "grad_norm": 0.19783291220664978,
      "learning_rate": 0.00019290367612843183,
      "loss": 0.3735,
      "step": 920
    },
    {
      "epoch": 0.03570632420644536,
      "grad_norm": 0.12295008450746536,
      "learning_rate": 0.00019289592058321703,
      "loss": 0.2581,
      "step": 921
    },
    {
      "epoch": 0.035745093288102735,
      "grad_norm": 0.1500101387500763,
      "learning_rate": 0.00019288816503800217,
      "loss": 0.2955,
      "step": 922
    },
    {
      "epoch": 0.03578386236976012,
      "grad_norm": 0.1304815709590912,
      "learning_rate": 0.00019288040949278737,
      "loss": 0.2512,
      "step": 923
    },
    {
      "epoch": 0.035822631451417494,
      "grad_norm": 0.15976041555404663,
      "learning_rate": 0.00019287265394757252,
      "loss": 0.3688,
      "step": 924
    },
    {
      "epoch": 0.03586140053307487,
      "grad_norm": 0.12335044145584106,
      "learning_rate": 0.0001928648984023577,
      "loss": 0.3198,
      "step": 925
    },
    {
      "epoch": 0.035900169614732254,
      "grad_norm": 0.13089574873447418,
      "learning_rate": 0.00019285714285714286,
      "loss": 0.3048,
      "step": 926
    },
    {
      "epoch": 0.03593893869638963,
      "grad_norm": 0.14263953268527985,
      "learning_rate": 0.00019284938731192804,
      "loss": 0.3153,
      "step": 927
    },
    {
      "epoch": 0.035977707778047006,
      "grad_norm": 0.14626367390155792,
      "learning_rate": 0.0001928416317667132,
      "loss": 0.3765,
      "step": 928
    },
    {
      "epoch": 0.03601647685970438,
      "grad_norm": 0.16173717379570007,
      "learning_rate": 0.00019283387622149838,
      "loss": 0.333,
      "step": 929
    },
    {
      "epoch": 0.036055245941361766,
      "grad_norm": 0.12735174596309662,
      "learning_rate": 0.00019282612067628355,
      "loss": 0.2897,
      "step": 930
    },
    {
      "epoch": 0.03609401502301914,
      "grad_norm": 0.15697994828224182,
      "learning_rate": 0.00019281836513106873,
      "loss": 0.3469,
      "step": 931
    },
    {
      "epoch": 0.03613278410467652,
      "grad_norm": 0.17037953436374664,
      "learning_rate": 0.00019281060958585387,
      "loss": 0.3497,
      "step": 932
    },
    {
      "epoch": 0.0361715531863339,
      "grad_norm": 0.128032386302948,
      "learning_rate": 0.00019280285404063907,
      "loss": 0.3072,
      "step": 933
    },
    {
      "epoch": 0.03621032226799128,
      "grad_norm": 0.1651209592819214,
      "learning_rate": 0.00019279509849542422,
      "loss": 0.3637,
      "step": 934
    },
    {
      "epoch": 0.036249091349648654,
      "grad_norm": 0.1338297724723816,
      "learning_rate": 0.00019278734295020942,
      "loss": 0.3272,
      "step": 935
    },
    {
      "epoch": 0.03628786043130603,
      "grad_norm": 0.10298994183540344,
      "learning_rate": 0.00019277958740499456,
      "loss": 0.2216,
      "step": 936
    },
    {
      "epoch": 0.036326629512963414,
      "grad_norm": 0.17320649325847626,
      "learning_rate": 0.00019277183185977976,
      "loss": 0.3997,
      "step": 937
    },
    {
      "epoch": 0.03636539859462079,
      "grad_norm": 0.14445610344409943,
      "learning_rate": 0.0001927640763145649,
      "loss": 0.3475,
      "step": 938
    },
    {
      "epoch": 0.03640416767627817,
      "grad_norm": 0.12820567190647125,
      "learning_rate": 0.00019275632076935008,
      "loss": 0.2973,
      "step": 939
    },
    {
      "epoch": 0.03644293675793555,
      "grad_norm": 0.13473553955554962,
      "learning_rate": 0.00019274856522413526,
      "loss": 0.3068,
      "step": 940
    },
    {
      "epoch": 0.036481705839592926,
      "grad_norm": 0.11205225437879562,
      "learning_rate": 0.00019274080967892043,
      "loss": 0.2286,
      "step": 941
    },
    {
      "epoch": 0.0365204749212503,
      "grad_norm": 0.13311296701431274,
      "learning_rate": 0.00019273305413370563,
      "loss": 0.3042,
      "step": 942
    },
    {
      "epoch": 0.03655924400290768,
      "grad_norm": 0.13391515612602234,
      "learning_rate": 0.00019272529858849077,
      "loss": 0.3145,
      "step": 943
    },
    {
      "epoch": 0.03659801308456506,
      "grad_norm": 0.12659214437007904,
      "learning_rate": 0.00019271754304327597,
      "loss": 0.2661,
      "step": 944
    },
    {
      "epoch": 0.03663678216622244,
      "grad_norm": 0.12359654903411865,
      "learning_rate": 0.00019270978749806112,
      "loss": 0.3054,
      "step": 945
    },
    {
      "epoch": 0.036675551247879815,
      "grad_norm": 0.12099438905715942,
      "learning_rate": 0.0001927020319528463,
      "loss": 0.2712,
      "step": 946
    },
    {
      "epoch": 0.03671432032953719,
      "grad_norm": 0.148103266954422,
      "learning_rate": 0.00019269427640763147,
      "loss": 0.279,
      "step": 947
    },
    {
      "epoch": 0.036753089411194574,
      "grad_norm": 0.1264447122812271,
      "learning_rate": 0.00019268652086241664,
      "loss": 0.2956,
      "step": 948
    },
    {
      "epoch": 0.03679185849285195,
      "grad_norm": 0.17909115552902222,
      "learning_rate": 0.0001926787653172018,
      "loss": 0.303,
      "step": 949
    },
    {
      "epoch": 0.03683062757450933,
      "grad_norm": 0.17396380007266998,
      "learning_rate": 0.00019267100977198698,
      "loss": 0.2996,
      "step": 950
    },
    {
      "epoch": 0.03686939665616671,
      "grad_norm": 0.7908211350440979,
      "learning_rate": 0.00019266325422677216,
      "loss": 0.3657,
      "step": 951
    },
    {
      "epoch": 0.036908165737824086,
      "grad_norm": 0.2606472074985504,
      "learning_rate": 0.00019265549868155733,
      "loss": 0.3211,
      "step": 952
    },
    {
      "epoch": 0.03694693481948146,
      "grad_norm": 0.3000714182853699,
      "learning_rate": 0.00019264774313634248,
      "loss": 0.3642,
      "step": 953
    },
    {
      "epoch": 0.03698570390113884,
      "grad_norm": 0.3904194235801697,
      "learning_rate": 0.00019263998759112767,
      "loss": 0.3027,
      "step": 954
    },
    {
      "epoch": 0.03702447298279622,
      "grad_norm": 0.17169423401355743,
      "learning_rate": 0.00019263223204591282,
      "loss": 0.3122,
      "step": 955
    },
    {
      "epoch": 0.0370632420644536,
      "grad_norm": 0.13592886924743652,
      "learning_rate": 0.00019262447650069802,
      "loss": 0.304,
      "step": 956
    },
    {
      "epoch": 0.037102011146110975,
      "grad_norm": 0.21045558154582977,
      "learning_rate": 0.00019261672095548317,
      "loss": 0.3641,
      "step": 957
    },
    {
      "epoch": 0.03714078022776836,
      "grad_norm": 0.49680420756340027,
      "learning_rate": 0.00019260896541026837,
      "loss": 0.2969,
      "step": 958
    },
    {
      "epoch": 0.037179549309425734,
      "grad_norm": 0.3279576301574707,
      "learning_rate": 0.0001926012098650535,
      "loss": 0.3053,
      "step": 959
    },
    {
      "epoch": 0.03721831839108311,
      "grad_norm": 0.11861515045166016,
      "learning_rate": 0.00019259345431983868,
      "loss": 0.2546,
      "step": 960
    },
    {
      "epoch": 0.03725708747274049,
      "grad_norm": 0.1322314292192459,
      "learning_rate": 0.00019258569877462386,
      "loss": 0.2694,
      "step": 961
    },
    {
      "epoch": 0.03729585655439787,
      "grad_norm": 0.17824684083461761,
      "learning_rate": 0.00019257794322940903,
      "loss": 0.391,
      "step": 962
    },
    {
      "epoch": 0.037334625636055246,
      "grad_norm": 0.16201791167259216,
      "learning_rate": 0.0001925701876841942,
      "loss": 0.3447,
      "step": 963
    },
    {
      "epoch": 0.03737339471771262,
      "grad_norm": 0.16781245172023773,
      "learning_rate": 0.00019256243213897938,
      "loss": 0.347,
      "step": 964
    },
    {
      "epoch": 0.03741216379937,
      "grad_norm": 0.15097524225711823,
      "learning_rate": 0.00019255467659376455,
      "loss": 0.3245,
      "step": 965
    },
    {
      "epoch": 0.03745093288102738,
      "grad_norm": 0.12509925663471222,
      "learning_rate": 0.00019254692104854972,
      "loss": 0.2202,
      "step": 966
    },
    {
      "epoch": 0.03748970196268476,
      "grad_norm": 0.115751251578331,
      "learning_rate": 0.0001925391655033349,
      "loss": 0.2504,
      "step": 967
    },
    {
      "epoch": 0.037528471044342135,
      "grad_norm": 0.14918749034404755,
      "learning_rate": 0.00019253140995812007,
      "loss": 0.3704,
      "step": 968
    },
    {
      "epoch": 0.03756724012599952,
      "grad_norm": 0.1071697548031807,
      "learning_rate": 0.00019252365441290524,
      "loss": 0.245,
      "step": 969
    },
    {
      "epoch": 0.037606009207656894,
      "grad_norm": 0.12987160682678223,
      "learning_rate": 0.0001925158988676904,
      "loss": 0.2767,
      "step": 970
    },
    {
      "epoch": 0.03764477828931427,
      "grad_norm": 0.1221848577260971,
      "learning_rate": 0.00019250814332247559,
      "loss": 0.2498,
      "step": 971
    },
    {
      "epoch": 0.03768354737097165,
      "grad_norm": 0.14096197485923767,
      "learning_rate": 0.00019250038777726076,
      "loss": 0.2767,
      "step": 972
    },
    {
      "epoch": 0.03772231645262903,
      "grad_norm": 0.15819507837295532,
      "learning_rate": 0.00019249263223204593,
      "loss": 0.3563,
      "step": 973
    },
    {
      "epoch": 0.03776108553428641,
      "grad_norm": 0.19797322154045105,
      "learning_rate": 0.00019248487668683108,
      "loss": 0.297,
      "step": 974
    },
    {
      "epoch": 0.03779985461594378,
      "grad_norm": 0.15974657237529755,
      "learning_rate": 0.00019247712114161628,
      "loss": 0.2816,
      "step": 975
    },
    {
      "epoch": 0.037838623697601166,
      "grad_norm": 0.2392984926700592,
      "learning_rate": 0.00019246936559640142,
      "loss": 0.3176,
      "step": 976
    },
    {
      "epoch": 0.03787739277925854,
      "grad_norm": 0.141859233379364,
      "learning_rate": 0.00019246161005118662,
      "loss": 0.3335,
      "step": 977
    },
    {
      "epoch": 0.03791616186091592,
      "grad_norm": 0.19285033643245697,
      "learning_rate": 0.00019245385450597177,
      "loss": 0.3771,
      "step": 978
    },
    {
      "epoch": 0.037954930942573295,
      "grad_norm": 0.15040834248065948,
      "learning_rate": 0.00019244609896075697,
      "loss": 0.3318,
      "step": 979
    },
    {
      "epoch": 0.03799370002423068,
      "grad_norm": 0.17104516923427582,
      "learning_rate": 0.00019243834341554211,
      "loss": 0.3428,
      "step": 980
    },
    {
      "epoch": 0.038032469105888055,
      "grad_norm": 0.1510593444108963,
      "learning_rate": 0.0001924305878703273,
      "loss": 0.2568,
      "step": 981
    },
    {
      "epoch": 0.03807123818754543,
      "grad_norm": 0.22821930050849915,
      "learning_rate": 0.00019242283232511246,
      "loss": 0.3191,
      "step": 982
    },
    {
      "epoch": 0.038110007269202814,
      "grad_norm": 0.22023746371269226,
      "learning_rate": 0.00019241507677989763,
      "loss": 0.3941,
      "step": 983
    },
    {
      "epoch": 0.03814877635086019,
      "grad_norm": 0.13646934926509857,
      "learning_rate": 0.0001924073212346828,
      "loss": 0.2487,
      "step": 984
    },
    {
      "epoch": 0.03818754543251757,
      "grad_norm": 0.12534765899181366,
      "learning_rate": 0.00019239956568946798,
      "loss": 0.3099,
      "step": 985
    },
    {
      "epoch": 0.03822631451417494,
      "grad_norm": 0.16129998862743378,
      "learning_rate": 0.00019239181014425315,
      "loss": 0.3304,
      "step": 986
    },
    {
      "epoch": 0.038265083595832326,
      "grad_norm": 0.13509014248847961,
      "learning_rate": 0.00019238405459903832,
      "loss": 0.2977,
      "step": 987
    },
    {
      "epoch": 0.0383038526774897,
      "grad_norm": 0.12439534813165665,
      "learning_rate": 0.0001923762990538235,
      "loss": 0.2665,
      "step": 988
    },
    {
      "epoch": 0.03834262175914708,
      "grad_norm": 0.16448821127414703,
      "learning_rate": 0.00019236854350860867,
      "loss": 0.3894,
      "step": 989
    },
    {
      "epoch": 0.038381390840804455,
      "grad_norm": 0.17906717956066132,
      "learning_rate": 0.00019236078796339384,
      "loss": 0.4008,
      "step": 990
    },
    {
      "epoch": 0.03842015992246184,
      "grad_norm": 0.20100383460521698,
      "learning_rate": 0.00019235303241817901,
      "loss": 0.3907,
      "step": 991
    },
    {
      "epoch": 0.038458929004119215,
      "grad_norm": 0.15293939411640167,
      "learning_rate": 0.0001923452768729642,
      "loss": 0.3643,
      "step": 992
    },
    {
      "epoch": 0.03849769808577659,
      "grad_norm": 0.19783005118370056,
      "learning_rate": 0.00019233752132774936,
      "loss": 0.3053,
      "step": 993
    },
    {
      "epoch": 0.038536467167433974,
      "grad_norm": 0.1758667379617691,
      "learning_rate": 0.00019232976578253453,
      "loss": 0.4014,
      "step": 994
    },
    {
      "epoch": 0.03857523624909135,
      "grad_norm": 0.1735306829214096,
      "learning_rate": 0.00019232201023731968,
      "loss": 0.3833,
      "step": 995
    },
    {
      "epoch": 0.03861400533074873,
      "grad_norm": 0.14058439433574677,
      "learning_rate": 0.00019231425469210488,
      "loss": 0.2974,
      "step": 996
    },
    {
      "epoch": 0.0386527744124061,
      "grad_norm": 0.15278314054012299,
      "learning_rate": 0.00019230649914689002,
      "loss": 0.2696,
      "step": 997
    },
    {
      "epoch": 0.038691543494063486,
      "grad_norm": 0.12250705063343048,
      "learning_rate": 0.00019229874360167522,
      "loss": 0.2667,
      "step": 998
    },
    {
      "epoch": 0.03873031257572086,
      "grad_norm": 0.1269567608833313,
      "learning_rate": 0.00019229098805646037,
      "loss": 0.2737,
      "step": 999
    },
    {
      "epoch": 0.03876908165737824,
      "grad_norm": 0.18468885123729706,
      "learning_rate": 0.00019228323251124557,
      "loss": 0.3561,
      "step": 1000
    },
    {
      "epoch": 0.03880785073903562,
      "grad_norm": 0.10938497632741928,
      "learning_rate": 0.00019227547696603072,
      "loss": 0.2646,
      "step": 1001
    },
    {
      "epoch": 0.038846619820693,
      "grad_norm": 0.11841943114995956,
      "learning_rate": 0.0001922677214208159,
      "loss": 0.2754,
      "step": 1002
    },
    {
      "epoch": 0.038885388902350375,
      "grad_norm": 0.11554040014743805,
      "learning_rate": 0.00019225996587560106,
      "loss": 0.2689,
      "step": 1003
    },
    {
      "epoch": 0.03892415798400775,
      "grad_norm": 0.17187446355819702,
      "learning_rate": 0.00019225221033038623,
      "loss": 0.3638,
      "step": 1004
    },
    {
      "epoch": 0.038962927065665134,
      "grad_norm": 0.11603958159685135,
      "learning_rate": 0.0001922444547851714,
      "loss": 0.2328,
      "step": 1005
    },
    {
      "epoch": 0.03900169614732251,
      "grad_norm": 0.13188624382019043,
      "learning_rate": 0.00019223669923995658,
      "loss": 0.278,
      "step": 1006
    },
    {
      "epoch": 0.03904046522897989,
      "grad_norm": 0.13582004606723785,
      "learning_rate": 0.00019222894369474175,
      "loss": 0.288,
      "step": 1007
    },
    {
      "epoch": 0.03907923431063726,
      "grad_norm": 0.11915206909179688,
      "learning_rate": 0.00019222118814952693,
      "loss": 0.2198,
      "step": 1008
    },
    {
      "epoch": 0.03911800339229465,
      "grad_norm": 0.1836741417646408,
      "learning_rate": 0.00019221343260431207,
      "loss": 0.3112,
      "step": 1009
    },
    {
      "epoch": 0.03915677247395202,
      "grad_norm": 0.19133983552455902,
      "learning_rate": 0.00019220567705909727,
      "loss": 0.3196,
      "step": 1010
    },
    {
      "epoch": 0.0391955415556094,
      "grad_norm": 0.18302839994430542,
      "learning_rate": 0.00019219792151388242,
      "loss": 0.3649,
      "step": 1011
    },
    {
      "epoch": 0.03923431063726678,
      "grad_norm": 0.18421344459056854,
      "learning_rate": 0.00019219016596866762,
      "loss": 0.3439,
      "step": 1012
    },
    {
      "epoch": 0.03927307971892416,
      "grad_norm": 0.16872531175613403,
      "learning_rate": 0.00019218241042345276,
      "loss": 0.2657,
      "step": 1013
    },
    {
      "epoch": 0.039311848800581535,
      "grad_norm": 0.15112949907779694,
      "learning_rate": 0.00019217465487823796,
      "loss": 0.3142,
      "step": 1014
    },
    {
      "epoch": 0.03935061788223891,
      "grad_norm": 0.1508398950099945,
      "learning_rate": 0.0001921668993330231,
      "loss": 0.2994,
      "step": 1015
    },
    {
      "epoch": 0.039389386963896295,
      "grad_norm": 0.14406360685825348,
      "learning_rate": 0.00019215914378780828,
      "loss": 0.3277,
      "step": 1016
    },
    {
      "epoch": 0.03942815604555367,
      "grad_norm": 0.10519345849752426,
      "learning_rate": 0.00019215138824259345,
      "loss": 0.228,
      "step": 1017
    },
    {
      "epoch": 0.03946692512721105,
      "grad_norm": 0.16050678491592407,
      "learning_rate": 0.00019214363269737863,
      "loss": 0.3237,
      "step": 1018
    },
    {
      "epoch": 0.03950569420886843,
      "grad_norm": 0.1443682163953781,
      "learning_rate": 0.0001921358771521638,
      "loss": 0.2682,
      "step": 1019
    },
    {
      "epoch": 0.03954446329052581,
      "grad_norm": 0.146948903799057,
      "learning_rate": 0.00019212812160694897,
      "loss": 0.3273,
      "step": 1020
    },
    {
      "epoch": 0.03958323237218318,
      "grad_norm": 0.1646176278591156,
      "learning_rate": 0.00019212036606173417,
      "loss": 0.2421,
      "step": 1021
    },
    {
      "epoch": 0.03962200145384056,
      "grad_norm": 0.15427687764167786,
      "learning_rate": 0.00019211261051651932,
      "loss": 0.284,
      "step": 1022
    },
    {
      "epoch": 0.03966077053549794,
      "grad_norm": 0.1562151461839676,
      "learning_rate": 0.0001921048549713045,
      "loss": 0.3583,
      "step": 1023
    },
    {
      "epoch": 0.03969953961715532,
      "grad_norm": 0.15102559328079224,
      "learning_rate": 0.00019209709942608966,
      "loss": 0.3424,
      "step": 1024
    },
    {
      "epoch": 0.039738308698812695,
      "grad_norm": 0.12357523292303085,
      "learning_rate": 0.00019208934388087484,
      "loss": 0.241,
      "step": 1025
    },
    {
      "epoch": 0.03977707778047008,
      "grad_norm": 0.18111582100391388,
      "learning_rate": 0.00019208158833566,
      "loss": 0.3081,
      "step": 1026
    },
    {
      "epoch": 0.039815846862127455,
      "grad_norm": 0.1302623748779297,
      "learning_rate": 0.00019207383279044518,
      "loss": 0.2659,
      "step": 1027
    },
    {
      "epoch": 0.03985461594378483,
      "grad_norm": 0.1723185032606125,
      "learning_rate": 0.00019206607724523035,
      "loss": 0.3028,
      "step": 1028
    },
    {
      "epoch": 0.03989338502544221,
      "grad_norm": 0.1626032292842865,
      "learning_rate": 0.00019205832170001553,
      "loss": 0.2726,
      "step": 1029
    },
    {
      "epoch": 0.03993215410709959,
      "grad_norm": 0.17040766775608063,
      "learning_rate": 0.00019205056615480067,
      "loss": 0.3565,
      "step": 1030
    },
    {
      "epoch": 0.03997092318875697,
      "grad_norm": 0.16438919305801392,
      "learning_rate": 0.00019204281060958587,
      "loss": 0.3389,
      "step": 1031
    },
    {
      "epoch": 0.04000969227041434,
      "grad_norm": 0.16920390725135803,
      "learning_rate": 0.00019203505506437102,
      "loss": 0.3921,
      "step": 1032
    },
    {
      "epoch": 0.04004846135207172,
      "grad_norm": 0.16164720058441162,
      "learning_rate": 0.00019202729951915622,
      "loss": 0.3403,
      "step": 1033
    },
    {
      "epoch": 0.0400872304337291,
      "grad_norm": 0.1603083312511444,
      "learning_rate": 0.00019201954397394136,
      "loss": 0.3289,
      "step": 1034
    },
    {
      "epoch": 0.04012599951538648,
      "grad_norm": 0.17963385581970215,
      "learning_rate": 0.00019201178842872656,
      "loss": 0.2892,
      "step": 1035
    },
    {
      "epoch": 0.040164768597043855,
      "grad_norm": 0.14456933736801147,
      "learning_rate": 0.0001920040328835117,
      "loss": 0.3156,
      "step": 1036
    },
    {
      "epoch": 0.04020353767870124,
      "grad_norm": 0.1802816540002823,
      "learning_rate": 0.00019199627733829688,
      "loss": 0.356,
      "step": 1037
    },
    {
      "epoch": 0.040242306760358615,
      "grad_norm": 0.2165185958147049,
      "learning_rate": 0.00019198852179308206,
      "loss": 0.3106,
      "step": 1038
    },
    {
      "epoch": 0.04028107584201599,
      "grad_norm": 0.24969595670700073,
      "learning_rate": 0.00019198076624786723,
      "loss": 0.348,
      "step": 1039
    },
    {
      "epoch": 0.04031984492367337,
      "grad_norm": 0.16238240897655487,
      "learning_rate": 0.0001919730107026524,
      "loss": 0.2928,
      "step": 1040
    },
    {
      "epoch": 0.04035861400533075,
      "grad_norm": 0.14850559830665588,
      "learning_rate": 0.00019196525515743757,
      "loss": 0.3168,
      "step": 1041
    },
    {
      "epoch": 0.04039738308698813,
      "grad_norm": 0.13596273958683014,
      "learning_rate": 0.00019195749961222275,
      "loss": 0.305,
      "step": 1042
    },
    {
      "epoch": 0.0404361521686455,
      "grad_norm": 0.20457106828689575,
      "learning_rate": 0.00019194974406700792,
      "loss": 0.326,
      "step": 1043
    },
    {
      "epoch": 0.04047492125030289,
      "grad_norm": 0.15543028712272644,
      "learning_rate": 0.0001919419885217931,
      "loss": 0.2935,
      "step": 1044
    },
    {
      "epoch": 0.04051369033196026,
      "grad_norm": 0.1882372945547104,
      "learning_rate": 0.00019193423297657826,
      "loss": 0.3013,
      "step": 1045
    },
    {
      "epoch": 0.04055245941361764,
      "grad_norm": 0.1630702167749405,
      "learning_rate": 0.00019192647743136344,
      "loss": 0.3295,
      "step": 1046
    },
    {
      "epoch": 0.040591228495275015,
      "grad_norm": 0.1380644291639328,
      "learning_rate": 0.0001919187218861486,
      "loss": 0.2842,
      "step": 1047
    },
    {
      "epoch": 0.0406299975769324,
      "grad_norm": 0.12287256866693497,
      "learning_rate": 0.00019191096634093378,
      "loss": 0.2587,
      "step": 1048
    },
    {
      "epoch": 0.040668766658589775,
      "grad_norm": 0.14010193943977356,
      "learning_rate": 0.00019190321079571896,
      "loss": 0.2986,
      "step": 1049
    },
    {
      "epoch": 0.04070753574024715,
      "grad_norm": 0.13120055198669434,
      "learning_rate": 0.00019189545525050413,
      "loss": 0.283,
      "step": 1050
    },
    {
      "epoch": 0.040746304821904535,
      "grad_norm": 0.12597781419754028,
      "learning_rate": 0.00019188769970528927,
      "loss": 0.2692,
      "step": 1051
    },
    {
      "epoch": 0.04078507390356191,
      "grad_norm": 0.14740675687789917,
      "learning_rate": 0.00019187994416007447,
      "loss": 0.3395,
      "step": 1052
    },
    {
      "epoch": 0.04082384298521929,
      "grad_norm": 0.17585153877735138,
      "learning_rate": 0.00019187218861485962,
      "loss": 0.3927,
      "step": 1053
    },
    {
      "epoch": 0.040862612066876663,
      "grad_norm": 0.152389794588089,
      "learning_rate": 0.00019186443306964482,
      "loss": 0.3659,
      "step": 1054
    },
    {
      "epoch": 0.04090138114853405,
      "grad_norm": 0.13030126690864563,
      "learning_rate": 0.00019185667752442997,
      "loss": 0.2558,
      "step": 1055
    },
    {
      "epoch": 0.04094015023019142,
      "grad_norm": 0.17685742676258087,
      "learning_rate": 0.00019184892197921517,
      "loss": 0.3458,
      "step": 1056
    },
    {
      "epoch": 0.0409789193118488,
      "grad_norm": 0.1519746631383896,
      "learning_rate": 0.0001918411664340003,
      "loss": 0.2558,
      "step": 1057
    },
    {
      "epoch": 0.041017688393506176,
      "grad_norm": 0.18119695782661438,
      "learning_rate": 0.00019183341088878548,
      "loss": 0.3566,
      "step": 1058
    },
    {
      "epoch": 0.04105645747516356,
      "grad_norm": 0.19026875495910645,
      "learning_rate": 0.00019182565534357066,
      "loss": 0.3647,
      "step": 1059
    },
    {
      "epoch": 0.041095226556820935,
      "grad_norm": 0.1555047631263733,
      "learning_rate": 0.00019181789979835583,
      "loss": 0.3474,
      "step": 1060
    },
    {
      "epoch": 0.04113399563847831,
      "grad_norm": 0.14596149325370789,
      "learning_rate": 0.000191810144253141,
      "loss": 0.3144,
      "step": 1061
    },
    {
      "epoch": 0.041172764720135695,
      "grad_norm": 0.15265019237995148,
      "learning_rate": 0.00019180238870792618,
      "loss": 0.3896,
      "step": 1062
    },
    {
      "epoch": 0.04121153380179307,
      "grad_norm": 0.11973228305578232,
      "learning_rate": 0.00019179463316271135,
      "loss": 0.2764,
      "step": 1063
    },
    {
      "epoch": 0.04125030288345045,
      "grad_norm": 0.14164938032627106,
      "learning_rate": 0.00019178687761749652,
      "loss": 0.3375,
      "step": 1064
    },
    {
      "epoch": 0.041289071965107824,
      "grad_norm": 0.15436412394046783,
      "learning_rate": 0.0001917791220722817,
      "loss": 0.3511,
      "step": 1065
    },
    {
      "epoch": 0.04132784104676521,
      "grad_norm": 0.12319612503051758,
      "learning_rate": 0.00019177136652706687,
      "loss": 0.2828,
      "step": 1066
    },
    {
      "epoch": 0.04136661012842258,
      "grad_norm": 0.13342507183551788,
      "learning_rate": 0.00019176361098185204,
      "loss": 0.3083,
      "step": 1067
    },
    {
      "epoch": 0.04140537921007996,
      "grad_norm": 0.12118097394704819,
      "learning_rate": 0.0001917558554366372,
      "loss": 0.312,
      "step": 1068
    },
    {
      "epoch": 0.04144414829173734,
      "grad_norm": 0.13895998895168304,
      "learning_rate": 0.00019174809989142239,
      "loss": 0.254,
      "step": 1069
    },
    {
      "epoch": 0.04148291737339472,
      "grad_norm": 0.13013778626918793,
      "learning_rate": 0.00019174034434620756,
      "loss": 0.2695,
      "step": 1070
    },
    {
      "epoch": 0.041521686455052095,
      "grad_norm": 0.14541898667812347,
      "learning_rate": 0.00019173258880099273,
      "loss": 0.3184,
      "step": 1071
    },
    {
      "epoch": 0.04156045553670947,
      "grad_norm": 0.11384215205907822,
      "learning_rate": 0.00019172483325577788,
      "loss": 0.2473,
      "step": 1072
    },
    {
      "epoch": 0.041599224618366855,
      "grad_norm": 0.14325159788131714,
      "learning_rate": 0.00019171707771056308,
      "loss": 0.2607,
      "step": 1073
    },
    {
      "epoch": 0.04163799370002423,
      "grad_norm": 0.16431660950183868,
      "learning_rate": 0.00019170932216534822,
      "loss": 0.3571,
      "step": 1074
    },
    {
      "epoch": 0.04167676278168161,
      "grad_norm": 0.1556822657585144,
      "learning_rate": 0.00019170156662013342,
      "loss": 0.3124,
      "step": 1075
    },
    {
      "epoch": 0.041715531863338984,
      "grad_norm": 0.15901771187782288,
      "learning_rate": 0.00019169381107491857,
      "loss": 0.3166,
      "step": 1076
    },
    {
      "epoch": 0.04175430094499637,
      "grad_norm": 0.131149560213089,
      "learning_rate": 0.00019168605552970377,
      "loss": 0.2976,
      "step": 1077
    },
    {
      "epoch": 0.04179307002665374,
      "grad_norm": 0.1564570814371109,
      "learning_rate": 0.0001916782999844889,
      "loss": 0.377,
      "step": 1078
    },
    {
      "epoch": 0.04183183910831112,
      "grad_norm": 0.13498398661613464,
      "learning_rate": 0.00019167054443927409,
      "loss": 0.3013,
      "step": 1079
    },
    {
      "epoch": 0.0418706081899685,
      "grad_norm": 0.1367102712392807,
      "learning_rate": 0.00019166278889405926,
      "loss": 0.3129,
      "step": 1080
    },
    {
      "epoch": 0.04190937727162588,
      "grad_norm": 0.2120504230260849,
      "learning_rate": 0.00019165503334884443,
      "loss": 0.4262,
      "step": 1081
    },
    {
      "epoch": 0.041948146353283255,
      "grad_norm": 0.12638281285762787,
      "learning_rate": 0.0001916472778036296,
      "loss": 0.2978,
      "step": 1082
    },
    {
      "epoch": 0.04198691543494063,
      "grad_norm": 0.127359077334404,
      "learning_rate": 0.00019163952225841478,
      "loss": 0.2559,
      "step": 1083
    },
    {
      "epoch": 0.042025684516598015,
      "grad_norm": 0.13910773396492004,
      "learning_rate": 0.00019163176671319995,
      "loss": 0.2715,
      "step": 1084
    },
    {
      "epoch": 0.04206445359825539,
      "grad_norm": 0.17901891469955444,
      "learning_rate": 0.00019162401116798512,
      "loss": 0.2924,
      "step": 1085
    },
    {
      "epoch": 0.04210322267991277,
      "grad_norm": 0.21571223437786102,
      "learning_rate": 0.00019161625562277027,
      "loss": 0.3772,
      "step": 1086
    },
    {
      "epoch": 0.04214199176157015,
      "grad_norm": 0.14024828374385834,
      "learning_rate": 0.00019160850007755547,
      "loss": 0.2776,
      "step": 1087
    },
    {
      "epoch": 0.04218076084322753,
      "grad_norm": 0.14955635368824005,
      "learning_rate": 0.00019160074453234061,
      "loss": 0.3034,
      "step": 1088
    },
    {
      "epoch": 0.042219529924884903,
      "grad_norm": 0.1433812826871872,
      "learning_rate": 0.00019159298898712581,
      "loss": 0.3199,
      "step": 1089
    },
    {
      "epoch": 0.04225829900654228,
      "grad_norm": 0.19958509504795074,
      "learning_rate": 0.00019158523344191096,
      "loss": 0.4366,
      "step": 1090
    },
    {
      "epoch": 0.04229706808819966,
      "grad_norm": 0.1720164269208908,
      "learning_rate": 0.00019157747789669616,
      "loss": 0.3571,
      "step": 1091
    },
    {
      "epoch": 0.04233583716985704,
      "grad_norm": 0.14786037802696228,
      "learning_rate": 0.0001915697223514813,
      "loss": 0.3304,
      "step": 1092
    },
    {
      "epoch": 0.042374606251514416,
      "grad_norm": 0.1629163920879364,
      "learning_rate": 0.00019156196680626648,
      "loss": 0.299,
      "step": 1093
    },
    {
      "epoch": 0.0424133753331718,
      "grad_norm": 0.15431521832942963,
      "learning_rate": 0.00019155421126105165,
      "loss": 0.3189,
      "step": 1094
    },
    {
      "epoch": 0.042452144414829175,
      "grad_norm": 0.16139106452465057,
      "learning_rate": 0.00019154645571583682,
      "loss": 0.2645,
      "step": 1095
    },
    {
      "epoch": 0.04249091349648655,
      "grad_norm": 0.15707392990589142,
      "learning_rate": 0.000191538700170622,
      "loss": 0.2529,
      "step": 1096
    },
    {
      "epoch": 0.04252968257814393,
      "grad_norm": 0.13992275297641754,
      "learning_rate": 0.00019153094462540717,
      "loss": 0.2744,
      "step": 1097
    },
    {
      "epoch": 0.04256845165980131,
      "grad_norm": 0.1703910082578659,
      "learning_rate": 0.00019152318908019234,
      "loss": 0.3812,
      "step": 1098
    },
    {
      "epoch": 0.04260722074145869,
      "grad_norm": 0.17049433290958405,
      "learning_rate": 0.00019151543353497752,
      "loss": 0.3583,
      "step": 1099
    },
    {
      "epoch": 0.042645989823116064,
      "grad_norm": 0.12582501769065857,
      "learning_rate": 0.0001915076779897627,
      "loss": 0.2574,
      "step": 1100
    },
    {
      "epoch": 0.04268475890477344,
      "grad_norm": 0.1391085535287857,
      "learning_rate": 0.00019149992244454786,
      "loss": 0.3338,
      "step": 1101
    },
    {
      "epoch": 0.04272352798643082,
      "grad_norm": 0.1461075246334076,
      "learning_rate": 0.00019149216689933303,
      "loss": 0.2383,
      "step": 1102
    },
    {
      "epoch": 0.0427622970680882,
      "grad_norm": 0.1238454058766365,
      "learning_rate": 0.0001914844113541182,
      "loss": 0.2644,
      "step": 1103
    },
    {
      "epoch": 0.042801066149745576,
      "grad_norm": 0.13240781426429749,
      "learning_rate": 0.00019147665580890338,
      "loss": 0.3427,
      "step": 1104
    },
    {
      "epoch": 0.04283983523140296,
      "grad_norm": 0.18214289844036102,
      "learning_rate": 0.00019146890026368855,
      "loss": 0.4134,
      "step": 1105
    },
    {
      "epoch": 0.042878604313060335,
      "grad_norm": 0.1458330601453781,
      "learning_rate": 0.00019146114471847372,
      "loss": 0.3157,
      "step": 1106
    },
    {
      "epoch": 0.04291737339471771,
      "grad_norm": 0.11839393526315689,
      "learning_rate": 0.00019145338917325887,
      "loss": 0.2413,
      "step": 1107
    },
    {
      "epoch": 0.04295614247637509,
      "grad_norm": 0.16876420378684998,
      "learning_rate": 0.00019144563362804407,
      "loss": 0.3733,
      "step": 1108
    },
    {
      "epoch": 0.04299491155803247,
      "grad_norm": 0.15868891775608063,
      "learning_rate": 0.00019143787808282922,
      "loss": 0.2889,
      "step": 1109
    },
    {
      "epoch": 0.04303368063968985,
      "grad_norm": 0.15119870007038116,
      "learning_rate": 0.00019143012253761442,
      "loss": 0.2681,
      "step": 1110
    },
    {
      "epoch": 0.043072449721347224,
      "grad_norm": 0.15356872975826263,
      "learning_rate": 0.00019142236699239956,
      "loss": 0.2564,
      "step": 1111
    },
    {
      "epoch": 0.04311121880300461,
      "grad_norm": 0.1684194952249527,
      "learning_rate": 0.00019141461144718476,
      "loss": 0.2849,
      "step": 1112
    },
    {
      "epoch": 0.04314998788466198,
      "grad_norm": 0.242111474275589,
      "learning_rate": 0.0001914068559019699,
      "loss": 0.3148,
      "step": 1113
    },
    {
      "epoch": 0.04318875696631936,
      "grad_norm": 0.16637060046195984,
      "learning_rate": 0.00019139910035675508,
      "loss": 0.3098,
      "step": 1114
    },
    {
      "epoch": 0.043227526047976736,
      "grad_norm": 0.18849480152130127,
      "learning_rate": 0.00019139134481154025,
      "loss": 0.3447,
      "step": 1115
    },
    {
      "epoch": 0.04326629512963412,
      "grad_norm": 0.15850548446178436,
      "learning_rate": 0.00019138358926632543,
      "loss": 0.2233,
      "step": 1116
    },
    {
      "epoch": 0.043305064211291495,
      "grad_norm": 0.1555405706167221,
      "learning_rate": 0.0001913758337211106,
      "loss": 0.2977,
      "step": 1117
    },
    {
      "epoch": 0.04334383329294887,
      "grad_norm": 0.16404424607753754,
      "learning_rate": 0.00019136807817589577,
      "loss": 0.2954,
      "step": 1118
    },
    {
      "epoch": 0.04338260237460625,
      "grad_norm": 0.15296603739261627,
      "learning_rate": 0.00019136032263068094,
      "loss": 0.2593,
      "step": 1119
    },
    {
      "epoch": 0.04342137145626363,
      "grad_norm": 0.16367478668689728,
      "learning_rate": 0.00019135256708546612,
      "loss": 0.2894,
      "step": 1120
    },
    {
      "epoch": 0.04346014053792101,
      "grad_norm": 0.17015162110328674,
      "learning_rate": 0.0001913448115402513,
      "loss": 0.3745,
      "step": 1121
    },
    {
      "epoch": 0.043498909619578384,
      "grad_norm": 0.21337780356407166,
      "learning_rate": 0.00019133705599503646,
      "loss": 0.3652,
      "step": 1122
    },
    {
      "epoch": 0.04353767870123577,
      "grad_norm": 0.13048651814460754,
      "learning_rate": 0.00019132930044982164,
      "loss": 0.2656,
      "step": 1123
    },
    {
      "epoch": 0.04357644778289314,
      "grad_norm": 0.13125291466712952,
      "learning_rate": 0.0001913215449046068,
      "loss": 0.2421,
      "step": 1124
    },
    {
      "epoch": 0.04361521686455052,
      "grad_norm": 0.1600731760263443,
      "learning_rate": 0.00019131378935939198,
      "loss": 0.3151,
      "step": 1125
    },
    {
      "epoch": 0.043653985946207896,
      "grad_norm": 0.1512347161769867,
      "learning_rate": 0.00019130603381417715,
      "loss": 0.2809,
      "step": 1126
    },
    {
      "epoch": 0.04369275502786528,
      "grad_norm": 0.11790039390325546,
      "learning_rate": 0.00019129827826896233,
      "loss": 0.2602,
      "step": 1127
    },
    {
      "epoch": 0.043731524109522656,
      "grad_norm": 0.13889361917972565,
      "learning_rate": 0.00019129052272374747,
      "loss": 0.2642,
      "step": 1128
    },
    {
      "epoch": 0.04377029319118003,
      "grad_norm": 0.13904763758182526,
      "learning_rate": 0.00019128276717853267,
      "loss": 0.3041,
      "step": 1129
    },
    {
      "epoch": 0.043809062272837415,
      "grad_norm": 0.146865576505661,
      "learning_rate": 0.00019127501163331782,
      "loss": 0.2933,
      "step": 1130
    },
    {
      "epoch": 0.04384783135449479,
      "grad_norm": 0.13948868215084076,
      "learning_rate": 0.00019126725608810302,
      "loss": 0.2856,
      "step": 1131
    },
    {
      "epoch": 0.04388660043615217,
      "grad_norm": 0.14435498416423798,
      "learning_rate": 0.00019125950054288816,
      "loss": 0.3214,
      "step": 1132
    },
    {
      "epoch": 0.043925369517809544,
      "grad_norm": 0.13480806350708008,
      "learning_rate": 0.00019125174499767336,
      "loss": 0.3013,
      "step": 1133
    },
    {
      "epoch": 0.04396413859946693,
      "grad_norm": 0.16888202726840973,
      "learning_rate": 0.0001912439894524585,
      "loss": 0.3389,
      "step": 1134
    },
    {
      "epoch": 0.044002907681124304,
      "grad_norm": 0.17813409864902496,
      "learning_rate": 0.00019123623390724368,
      "loss": 0.3111,
      "step": 1135
    },
    {
      "epoch": 0.04404167676278168,
      "grad_norm": 0.13458244502544403,
      "learning_rate": 0.00019122847836202885,
      "loss": 0.3269,
      "step": 1136
    },
    {
      "epoch": 0.04408044584443906,
      "grad_norm": 0.19575384259223938,
      "learning_rate": 0.00019122072281681403,
      "loss": 0.3969,
      "step": 1137
    },
    {
      "epoch": 0.04411921492609644,
      "grad_norm": 0.15668639540672302,
      "learning_rate": 0.0001912129672715992,
      "loss": 0.2875,
      "step": 1138
    },
    {
      "epoch": 0.044157984007753816,
      "grad_norm": 0.1765872985124588,
      "learning_rate": 0.00019120521172638437,
      "loss": 0.3842,
      "step": 1139
    },
    {
      "epoch": 0.04419675308941119,
      "grad_norm": 0.3117939829826355,
      "learning_rate": 0.00019119745618116955,
      "loss": 0.3469,
      "step": 1140
    },
    {
      "epoch": 0.044235522171068575,
      "grad_norm": 0.18938064575195312,
      "learning_rate": 0.00019118970063595472,
      "loss": 0.3186,
      "step": 1141
    },
    {
      "epoch": 0.04427429125272595,
      "grad_norm": 0.24405312538146973,
      "learning_rate": 0.00019118194509073986,
      "loss": 0.3605,
      "step": 1142
    },
    {
      "epoch": 0.04431306033438333,
      "grad_norm": 0.15896755456924438,
      "learning_rate": 0.00019117418954552506,
      "loss": 0.3284,
      "step": 1143
    },
    {
      "epoch": 0.044351829416040704,
      "grad_norm": 0.14227941632270813,
      "learning_rate": 0.00019116643400031024,
      "loss": 0.241,
      "step": 1144
    },
    {
      "epoch": 0.04439059849769809,
      "grad_norm": 0.20968283712863922,
      "learning_rate": 0.0001911586784550954,
      "loss": 0.3444,
      "step": 1145
    },
    {
      "epoch": 0.044429367579355464,
      "grad_norm": 0.16721270978450775,
      "learning_rate": 0.00019115092290988058,
      "loss": 0.3004,
      "step": 1146
    },
    {
      "epoch": 0.04446813666101284,
      "grad_norm": 0.1657046526670456,
      "learning_rate": 0.00019114316736466576,
      "loss": 0.2493,
      "step": 1147
    },
    {
      "epoch": 0.04450690574267022,
      "grad_norm": 0.19163669645786285,
      "learning_rate": 0.00019113541181945093,
      "loss": 0.287,
      "step": 1148
    },
    {
      "epoch": 0.0445456748243276,
      "grad_norm": 0.1655362844467163,
      "learning_rate": 0.00019112765627423607,
      "loss": 0.2755,
      "step": 1149
    },
    {
      "epoch": 0.044584443905984976,
      "grad_norm": 0.22268171608448029,
      "learning_rate": 0.00019111990072902127,
      "loss": 0.3551,
      "step": 1150
    },
    {
      "epoch": 0.04462321298764235,
      "grad_norm": 0.16557715833187103,
      "learning_rate": 0.00019111214518380642,
      "loss": 0.3156,
      "step": 1151
    },
    {
      "epoch": 0.044661982069299735,
      "grad_norm": 0.18223527073860168,
      "learning_rate": 0.00019110438963859162,
      "loss": 0.3535,
      "step": 1152
    },
    {
      "epoch": 0.04470075115095711,
      "grad_norm": 0.1568605899810791,
      "learning_rate": 0.00019109663409337677,
      "loss": 0.3262,
      "step": 1153
    },
    {
      "epoch": 0.04473952023261449,
      "grad_norm": 0.17063497006893158,
      "learning_rate": 0.00019108887854816197,
      "loss": 0.3494,
      "step": 1154
    },
    {
      "epoch": 0.04477828931427187,
      "grad_norm": 0.594967246055603,
      "learning_rate": 0.0001910811230029471,
      "loss": 0.3059,
      "step": 1155
    },
    {
      "epoch": 0.04481705839592925,
      "grad_norm": 2.060595989227295,
      "learning_rate": 0.00019107336745773228,
      "loss": 0.3994,
      "step": 1156
    },
    {
      "epoch": 0.044855827477586624,
      "grad_norm": 1.728804349899292,
      "learning_rate": 0.00019106561191251746,
      "loss": 0.4489,
      "step": 1157
    },
    {
      "epoch": 0.044894596559244,
      "grad_norm": 1.0568413734436035,
      "learning_rate": 0.00019105785636730263,
      "loss": 0.3832,
      "step": 1158
    },
    {
      "epoch": 0.04493336564090138,
      "grad_norm": 0.8656743168830872,
      "learning_rate": 0.0001910501008220878,
      "loss": 0.282,
      "step": 1159
    },
    {
      "epoch": 0.04497213472255876,
      "grad_norm": 0.7202917337417603,
      "learning_rate": 0.00019104234527687297,
      "loss": 0.3457,
      "step": 1160
    },
    {
      "epoch": 0.045010903804216136,
      "grad_norm": 0.3764117360115051,
      "learning_rate": 0.00019103458973165815,
      "loss": 0.3041,
      "step": 1161
    },
    {
      "epoch": 0.04504967288587352,
      "grad_norm": 0.38235554099082947,
      "learning_rate": 0.00019102683418644332,
      "loss": 0.3634,
      "step": 1162
    },
    {
      "epoch": 0.045088441967530896,
      "grad_norm": 0.2462264448404312,
      "learning_rate": 0.00019101907864122847,
      "loss": 0.2985,
      "step": 1163
    },
    {
      "epoch": 0.04512721104918827,
      "grad_norm": 0.2747212052345276,
      "learning_rate": 0.00019101132309601367,
      "loss": 0.3886,
      "step": 1164
    },
    {
      "epoch": 0.04516598013084565,
      "grad_norm": 0.24255038797855377,
      "learning_rate": 0.0001910035675507988,
      "loss": 0.281,
      "step": 1165
    },
    {
      "epoch": 0.04520474921250303,
      "grad_norm": 0.20774312317371368,
      "learning_rate": 0.000190995812005584,
      "loss": 0.3973,
      "step": 1166
    },
    {
      "epoch": 0.04524351829416041,
      "grad_norm": 0.21822619438171387,
      "learning_rate": 0.00019098805646036916,
      "loss": 0.3519,
      "step": 1167
    },
    {
      "epoch": 0.045282287375817784,
      "grad_norm": 0.2687772810459137,
      "learning_rate": 0.00019098030091515436,
      "loss": 0.2446,
      "step": 1168
    },
    {
      "epoch": 0.04532105645747516,
      "grad_norm": 0.20801101624965668,
      "learning_rate": 0.0001909725453699395,
      "loss": 0.2433,
      "step": 1169
    },
    {
      "epoch": 0.045359825539132544,
      "grad_norm": 0.23361794650554657,
      "learning_rate": 0.00019096478982472468,
      "loss": 0.3976,
      "step": 1170
    },
    {
      "epoch": 0.04539859462078992,
      "grad_norm": 0.18996748328208923,
      "learning_rate": 0.00019095703427950985,
      "loss": 0.2823,
      "step": 1171
    },
    {
      "epoch": 0.045437363702447296,
      "grad_norm": 0.18457439541816711,
      "learning_rate": 0.00019094927873429502,
      "loss": 0.3637,
      "step": 1172
    },
    {
      "epoch": 0.04547613278410468,
      "grad_norm": 0.17107027769088745,
      "learning_rate": 0.0001909415231890802,
      "loss": 0.2799,
      "step": 1173
    },
    {
      "epoch": 0.045514901865762056,
      "grad_norm": 0.16783396899700165,
      "learning_rate": 0.00019093376764386537,
      "loss": 0.2806,
      "step": 1174
    },
    {
      "epoch": 0.04555367094741943,
      "grad_norm": 0.18045346438884735,
      "learning_rate": 0.00019092601209865054,
      "loss": 0.3149,
      "step": 1175
    },
    {
      "epoch": 0.04559244002907681,
      "grad_norm": 0.1432599276304245,
      "learning_rate": 0.0001909182565534357,
      "loss": 0.2445,
      "step": 1176
    },
    {
      "epoch": 0.04563120911073419,
      "grad_norm": 0.2008724808692932,
      "learning_rate": 0.00019091050100822089,
      "loss": 0.32,
      "step": 1177
    },
    {
      "epoch": 0.04566997819239157,
      "grad_norm": 0.15708258748054504,
      "learning_rate": 0.00019090274546300606,
      "loss": 0.2723,
      "step": 1178
    },
    {
      "epoch": 0.045708747274048944,
      "grad_norm": 0.1428544968366623,
      "learning_rate": 0.00019089498991779123,
      "loss": 0.2863,
      "step": 1179
    },
    {
      "epoch": 0.04574751635570633,
      "grad_norm": 0.15903356671333313,
      "learning_rate": 0.0001908872343725764,
      "loss": 0.3231,
      "step": 1180
    },
    {
      "epoch": 0.045786285437363704,
      "grad_norm": 0.17987680435180664,
      "learning_rate": 0.00019087947882736158,
      "loss": 0.265,
      "step": 1181
    },
    {
      "epoch": 0.04582505451902108,
      "grad_norm": 0.17886243760585785,
      "learning_rate": 0.00019087172328214675,
      "loss": 0.3469,
      "step": 1182
    },
    {
      "epoch": 0.045863823600678456,
      "grad_norm": 0.1768224984407425,
      "learning_rate": 0.00019086396773693192,
      "loss": 0.3406,
      "step": 1183
    },
    {
      "epoch": 0.04590259268233584,
      "grad_norm": 0.18874506652355194,
      "learning_rate": 0.00019085621219171707,
      "loss": 0.311,
      "step": 1184
    },
    {
      "epoch": 0.045941361763993216,
      "grad_norm": 0.18169599771499634,
      "learning_rate": 0.00019084845664650227,
      "loss": 0.3936,
      "step": 1185
    },
    {
      "epoch": 0.04598013084565059,
      "grad_norm": 0.18737947940826416,
      "learning_rate": 0.00019084070110128741,
      "loss": 0.3183,
      "step": 1186
    },
    {
      "epoch": 0.04601889992730797,
      "grad_norm": 0.19024930894374847,
      "learning_rate": 0.00019083294555607261,
      "loss": 0.269,
      "step": 1187
    },
    {
      "epoch": 0.04605766900896535,
      "grad_norm": 0.16128702461719513,
      "learning_rate": 0.00019082519001085776,
      "loss": 0.2907,
      "step": 1188
    },
    {
      "epoch": 0.04609643809062273,
      "grad_norm": 0.17328044772148132,
      "learning_rate": 0.00019081743446564296,
      "loss": 0.3193,
      "step": 1189
    },
    {
      "epoch": 0.046135207172280104,
      "grad_norm": 0.19893445074558258,
      "learning_rate": 0.0001908096789204281,
      "loss": 0.2536,
      "step": 1190
    },
    {
      "epoch": 0.04617397625393749,
      "grad_norm": 0.1664832979440689,
      "learning_rate": 0.00019080192337521328,
      "loss": 0.3139,
      "step": 1191
    },
    {
      "epoch": 0.046212745335594864,
      "grad_norm": 0.14880971610546112,
      "learning_rate": 0.00019079416782999845,
      "loss": 0.2492,
      "step": 1192
    },
    {
      "epoch": 0.04625151441725224,
      "grad_norm": 0.1609460860490799,
      "learning_rate": 0.00019078641228478362,
      "loss": 0.2744,
      "step": 1193
    },
    {
      "epoch": 0.046290283498909617,
      "grad_norm": 0.18092599511146545,
      "learning_rate": 0.0001907786567395688,
      "loss": 0.3209,
      "step": 1194
    },
    {
      "epoch": 0.046329052580567,
      "grad_norm": 0.15232835710048676,
      "learning_rate": 0.00019077090119435397,
      "loss": 0.2751,
      "step": 1195
    },
    {
      "epoch": 0.046367821662224376,
      "grad_norm": 0.15800721943378448,
      "learning_rate": 0.00019076314564913914,
      "loss": 0.3097,
      "step": 1196
    },
    {
      "epoch": 0.04640659074388175,
      "grad_norm": 0.16375567018985748,
      "learning_rate": 0.00019075539010392431,
      "loss": 0.3271,
      "step": 1197
    },
    {
      "epoch": 0.046445359825539136,
      "grad_norm": 0.14950555562973022,
      "learning_rate": 0.0001907476345587095,
      "loss": 0.2974,
      "step": 1198
    },
    {
      "epoch": 0.04648412890719651,
      "grad_norm": 0.12336399406194687,
      "learning_rate": 0.00019073987901349466,
      "loss": 0.2452,
      "step": 1199
    },
    {
      "epoch": 0.04652289798885389,
      "grad_norm": 0.17376770079135895,
      "learning_rate": 0.00019073212346827983,
      "loss": 0.346,
      "step": 1200
    },
    {
      "epoch": 0.046561667070511265,
      "grad_norm": 0.1505773812532425,
      "learning_rate": 0.000190724367923065,
      "loss": 0.2984,
      "step": 1201
    },
    {
      "epoch": 0.04660043615216865,
      "grad_norm": 0.17459559440612793,
      "learning_rate": 0.00019071661237785018,
      "loss": 0.3876,
      "step": 1202
    },
    {
      "epoch": 0.046639205233826024,
      "grad_norm": 0.2022859752178192,
      "learning_rate": 0.00019070885683263535,
      "loss": 0.4255,
      "step": 1203
    },
    {
      "epoch": 0.0466779743154834,
      "grad_norm": 0.15829923748970032,
      "learning_rate": 0.00019070110128742052,
      "loss": 0.3049,
      "step": 1204
    },
    {
      "epoch": 0.046716743397140784,
      "grad_norm": 0.15706948935985565,
      "learning_rate": 0.00019069334574220567,
      "loss": 0.3077,
      "step": 1205
    },
    {
      "epoch": 0.04675551247879816,
      "grad_norm": 0.1492292732000351,
      "learning_rate": 0.00019068559019699087,
      "loss": 0.2757,
      "step": 1206
    },
    {
      "epoch": 0.046794281560455536,
      "grad_norm": 0.15624448657035828,
      "learning_rate": 0.00019067783465177602,
      "loss": 0.3107,
      "step": 1207
    },
    {
      "epoch": 0.04683305064211291,
      "grad_norm": 0.1336718648672104,
      "learning_rate": 0.00019067007910656122,
      "loss": 0.2933,
      "step": 1208
    },
    {
      "epoch": 0.046871819723770296,
      "grad_norm": 0.1970854550600052,
      "learning_rate": 0.00019066232356134636,
      "loss": 0.3706,
      "step": 1209
    },
    {
      "epoch": 0.04691058880542767,
      "grad_norm": 0.17931139469146729,
      "learning_rate": 0.00019065456801613156,
      "loss": 0.2959,
      "step": 1210
    },
    {
      "epoch": 0.04694935788708505,
      "grad_norm": 0.13239867985248566,
      "learning_rate": 0.0001906468124709167,
      "loss": 0.2943,
      "step": 1211
    },
    {
      "epoch": 0.046988126968742425,
      "grad_norm": 0.18027722835540771,
      "learning_rate": 0.00019063905692570188,
      "loss": 0.3435,
      "step": 1212
    },
    {
      "epoch": 0.04702689605039981,
      "grad_norm": 0.13321274518966675,
      "learning_rate": 0.00019063130138048705,
      "loss": 0.2702,
      "step": 1213
    },
    {
      "epoch": 0.047065665132057184,
      "grad_norm": 0.1656215637922287,
      "learning_rate": 0.00019062354583527223,
      "loss": 0.3237,
      "step": 1214
    },
    {
      "epoch": 0.04710443421371456,
      "grad_norm": 0.15544946491718292,
      "learning_rate": 0.0001906157902900574,
      "loss": 0.3222,
      "step": 1215
    },
    {
      "epoch": 0.047143203295371944,
      "grad_norm": 0.13284923136234283,
      "learning_rate": 0.00019060803474484257,
      "loss": 0.2689,
      "step": 1216
    },
    {
      "epoch": 0.04718197237702932,
      "grad_norm": 0.1623963564634323,
      "learning_rate": 0.00019060027919962774,
      "loss": 0.3332,
      "step": 1217
    },
    {
      "epoch": 0.047220741458686696,
      "grad_norm": 0.15613529086112976,
      "learning_rate": 0.00019059252365441292,
      "loss": 0.2809,
      "step": 1218
    },
    {
      "epoch": 0.04725951054034407,
      "grad_norm": 0.17250871658325195,
      "learning_rate": 0.00019058476810919806,
      "loss": 0.3154,
      "step": 1219
    },
    {
      "epoch": 0.047298279622001456,
      "grad_norm": 0.1848672479391098,
      "learning_rate": 0.00019057701256398326,
      "loss": 0.3267,
      "step": 1220
    },
    {
      "epoch": 0.04733704870365883,
      "grad_norm": 0.19070403277873993,
      "learning_rate": 0.0001905692570187684,
      "loss": 0.3114,
      "step": 1221
    },
    {
      "epoch": 0.04737581778531621,
      "grad_norm": 0.1568240374326706,
      "learning_rate": 0.0001905615014735536,
      "loss": 0.3098,
      "step": 1222
    },
    {
      "epoch": 0.04741458686697359,
      "grad_norm": 0.1306370049715042,
      "learning_rate": 0.00019055374592833878,
      "loss": 0.251,
      "step": 1223
    },
    {
      "epoch": 0.04745335594863097,
      "grad_norm": 0.21879929304122925,
      "learning_rate": 0.00019054599038312395,
      "loss": 0.3491,
      "step": 1224
    },
    {
      "epoch": 0.047492125030288344,
      "grad_norm": 0.1279248595237732,
      "learning_rate": 0.00019053823483790913,
      "loss": 0.2476,
      "step": 1225
    },
    {
      "epoch": 0.04753089411194572,
      "grad_norm": 0.19536158442497253,
      "learning_rate": 0.00019053047929269427,
      "loss": 0.354,
      "step": 1226
    },
    {
      "epoch": 0.047569663193603104,
      "grad_norm": 0.1490105241537094,
      "learning_rate": 0.00019052272374747947,
      "loss": 0.3617,
      "step": 1227
    },
    {
      "epoch": 0.04760843227526048,
      "grad_norm": 0.16068986058235168,
      "learning_rate": 0.00019051496820226462,
      "loss": 0.2771,
      "step": 1228
    },
    {
      "epoch": 0.047647201356917857,
      "grad_norm": 0.17489393055438995,
      "learning_rate": 0.00019050721265704982,
      "loss": 0.3313,
      "step": 1229
    },
    {
      "epoch": 0.04768597043857523,
      "grad_norm": 0.1751660257577896,
      "learning_rate": 0.00019049945711183496,
      "loss": 0.3946,
      "step": 1230
    },
    {
      "epoch": 0.047724739520232616,
      "grad_norm": 0.1478973925113678,
      "learning_rate": 0.00019049170156662016,
      "loss": 0.2975,
      "step": 1231
    },
    {
      "epoch": 0.04776350860188999,
      "grad_norm": 0.1558416336774826,
      "learning_rate": 0.0001904839460214053,
      "loss": 0.3292,
      "step": 1232
    },
    {
      "epoch": 0.04780227768354737,
      "grad_norm": 0.12206907570362091,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.2472,
      "step": 1233
    },
    {
      "epoch": 0.04784104676520475,
      "grad_norm": 0.16052387654781342,
      "learning_rate": 0.00019046843493097565,
      "loss": 0.2906,
      "step": 1234
    },
    {
      "epoch": 0.04787981584686213,
      "grad_norm": 0.18401873111724854,
      "learning_rate": 0.00019046067938576083,
      "loss": 0.317,
      "step": 1235
    },
    {
      "epoch": 0.047918584928519505,
      "grad_norm": 0.16503062844276428,
      "learning_rate": 0.000190452923840546,
      "loss": 0.3772,
      "step": 1236
    },
    {
      "epoch": 0.04795735401017688,
      "grad_norm": 0.15124279260635376,
      "learning_rate": 0.00019044516829533117,
      "loss": 0.2861,
      "step": 1237
    },
    {
      "epoch": 0.047996123091834264,
      "grad_norm": 0.1728893667459488,
      "learning_rate": 0.00019043741275011635,
      "loss": 0.2996,
      "step": 1238
    },
    {
      "epoch": 0.04803489217349164,
      "grad_norm": 0.18140040338039398,
      "learning_rate": 0.00019042965720490152,
      "loss": 0.3056,
      "step": 1239
    },
    {
      "epoch": 0.04807366125514902,
      "grad_norm": 0.16766802966594696,
      "learning_rate": 0.00019042190165968666,
      "loss": 0.2789,
      "step": 1240
    },
    {
      "epoch": 0.0481124303368064,
      "grad_norm": 0.19070635735988617,
      "learning_rate": 0.00019041414611447186,
      "loss": 0.3127,
      "step": 1241
    },
    {
      "epoch": 0.048151199418463776,
      "grad_norm": 0.20055851340293884,
      "learning_rate": 0.000190406390569257,
      "loss": 0.34,
      "step": 1242
    },
    {
      "epoch": 0.04818996850012115,
      "grad_norm": 0.1974986046552658,
      "learning_rate": 0.0001903986350240422,
      "loss": 0.3042,
      "step": 1243
    },
    {
      "epoch": 0.04822873758177853,
      "grad_norm": 0.2900586724281311,
      "learning_rate": 0.00019039087947882736,
      "loss": 0.3861,
      "step": 1244
    },
    {
      "epoch": 0.04826750666343591,
      "grad_norm": 0.1652153879404068,
      "learning_rate": 0.00019038312393361256,
      "loss": 0.2885,
      "step": 1245
    },
    {
      "epoch": 0.04830627574509329,
      "grad_norm": 0.15623077750205994,
      "learning_rate": 0.0001903753683883977,
      "loss": 0.2982,
      "step": 1246
    },
    {
      "epoch": 0.048345044826750665,
      "grad_norm": 0.15804502367973328,
      "learning_rate": 0.00019036761284318287,
      "loss": 0.3374,
      "step": 1247
    },
    {
      "epoch": 0.04838381390840805,
      "grad_norm": 0.1398867517709732,
      "learning_rate": 0.00019035985729796805,
      "loss": 0.29,
      "step": 1248
    },
    {
      "epoch": 0.048422582990065424,
      "grad_norm": 0.14264965057373047,
      "learning_rate": 0.00019035210175275322,
      "loss": 0.3094,
      "step": 1249
    },
    {
      "epoch": 0.0484613520717228,
      "grad_norm": 0.12672527134418488,
      "learning_rate": 0.0001903443462075384,
      "loss": 0.2668,
      "step": 1250
    },
    {
      "epoch": 0.04850012115338018,
      "grad_norm": 0.15897561609745026,
      "learning_rate": 0.00019033659066232356,
      "loss": 0.2974,
      "step": 1251
    },
    {
      "epoch": 0.04853889023503756,
      "grad_norm": 0.13664951920509338,
      "learning_rate": 0.00019032883511710874,
      "loss": 0.2129,
      "step": 1252
    },
    {
      "epoch": 0.048577659316694936,
      "grad_norm": 0.13686464726924896,
      "learning_rate": 0.0001903210795718939,
      "loss": 0.2433,
      "step": 1253
    },
    {
      "epoch": 0.04861642839835231,
      "grad_norm": 0.14961035549640656,
      "learning_rate": 0.00019031332402667908,
      "loss": 0.2996,
      "step": 1254
    },
    {
      "epoch": 0.04865519748000969,
      "grad_norm": 0.10893859714269638,
      "learning_rate": 0.00019030556848146426,
      "loss": 0.2099,
      "step": 1255
    },
    {
      "epoch": 0.04869396656166707,
      "grad_norm": 0.13030117750167847,
      "learning_rate": 0.00019029781293624943,
      "loss": 0.2081,
      "step": 1256
    },
    {
      "epoch": 0.04873273564332445,
      "grad_norm": 0.20121440291404724,
      "learning_rate": 0.0001902900573910346,
      "loss": 0.3389,
      "step": 1257
    },
    {
      "epoch": 0.048771504724981825,
      "grad_norm": 0.17991968989372253,
      "learning_rate": 0.00019028230184581977,
      "loss": 0.3333,
      "step": 1258
    },
    {
      "epoch": 0.04881027380663921,
      "grad_norm": 0.13191340863704681,
      "learning_rate": 0.00019027454630060495,
      "loss": 0.2511,
      "step": 1259
    },
    {
      "epoch": 0.048849042888296584,
      "grad_norm": 0.14098984003067017,
      "learning_rate": 0.00019026679075539012,
      "loss": 0.2396,
      "step": 1260
    },
    {
      "epoch": 0.04888781196995396,
      "grad_norm": 0.15114878118038177,
      "learning_rate": 0.00019025903521017527,
      "loss": 0.3098,
      "step": 1261
    },
    {
      "epoch": 0.04892658105161134,
      "grad_norm": 0.1623126119375229,
      "learning_rate": 0.00019025127966496047,
      "loss": 0.3347,
      "step": 1262
    },
    {
      "epoch": 0.04896535013326872,
      "grad_norm": 0.14797888696193695,
      "learning_rate": 0.0001902435241197456,
      "loss": 0.2531,
      "step": 1263
    },
    {
      "epoch": 0.049004119214926097,
      "grad_norm": 0.1643858700990677,
      "learning_rate": 0.0001902357685745308,
      "loss": 0.2865,
      "step": 1264
    },
    {
      "epoch": 0.04904288829658347,
      "grad_norm": 0.1345081925392151,
      "learning_rate": 0.00019022801302931596,
      "loss": 0.2793,
      "step": 1265
    },
    {
      "epoch": 0.049081657378240856,
      "grad_norm": 0.1494012475013733,
      "learning_rate": 0.00019022025748410116,
      "loss": 0.3229,
      "step": 1266
    },
    {
      "epoch": 0.04912042645989823,
      "grad_norm": 0.16267673671245575,
      "learning_rate": 0.0001902125019388863,
      "loss": 0.3345,
      "step": 1267
    },
    {
      "epoch": 0.04915919554155561,
      "grad_norm": 0.15464673936367035,
      "learning_rate": 0.00019020474639367148,
      "loss": 0.2956,
      "step": 1268
    },
    {
      "epoch": 0.049197964623212985,
      "grad_norm": 0.1456122249364853,
      "learning_rate": 0.00019019699084845665,
      "loss": 0.2604,
      "step": 1269
    },
    {
      "epoch": 0.04923673370487037,
      "grad_norm": 0.14255031943321228,
      "learning_rate": 0.00019018923530324182,
      "loss": 0.2382,
      "step": 1270
    },
    {
      "epoch": 0.049275502786527745,
      "grad_norm": 0.25736260414123535,
      "learning_rate": 0.000190181479758027,
      "loss": 0.3668,
      "step": 1271
    },
    {
      "epoch": 0.04931427186818512,
      "grad_norm": 0.21977201104164124,
      "learning_rate": 0.00019017372421281217,
      "loss": 0.3583,
      "step": 1272
    },
    {
      "epoch": 0.049353040949842504,
      "grad_norm": 0.15841564536094666,
      "learning_rate": 0.00019016596866759734,
      "loss": 0.284,
      "step": 1273
    },
    {
      "epoch": 0.04939181003149988,
      "grad_norm": 0.1447611153125763,
      "learning_rate": 0.0001901582131223825,
      "loss": 0.2124,
      "step": 1274
    },
    {
      "epoch": 0.04943057911315726,
      "grad_norm": 0.1743115484714508,
      "learning_rate": 0.00019015045757716769,
      "loss": 0.3202,
      "step": 1275
    },
    {
      "epoch": 0.04946934819481463,
      "grad_norm": 0.1802697628736496,
      "learning_rate": 0.00019014270203195286,
      "loss": 0.364,
      "step": 1276
    },
    {
      "epoch": 0.049508117276472016,
      "grad_norm": 0.1318066120147705,
      "learning_rate": 0.00019013494648673803,
      "loss": 0.2752,
      "step": 1277
    },
    {
      "epoch": 0.04954688635812939,
      "grad_norm": 0.17642803490161896,
      "learning_rate": 0.0001901271909415232,
      "loss": 0.3248,
      "step": 1278
    },
    {
      "epoch": 0.04958565543978677,
      "grad_norm": 0.17737984657287598,
      "learning_rate": 0.00019011943539630838,
      "loss": 0.3185,
      "step": 1279
    },
    {
      "epoch": 0.049624424521444145,
      "grad_norm": 0.13974209129810333,
      "learning_rate": 0.00019011167985109355,
      "loss": 0.2784,
      "step": 1280
    },
    {
      "epoch": 0.04966319360310153,
      "grad_norm": 0.20122544467449188,
      "learning_rate": 0.00019010392430587872,
      "loss": 0.3238,
      "step": 1281
    },
    {
      "epoch": 0.049701962684758905,
      "grad_norm": 0.17262905836105347,
      "learning_rate": 0.00019009616876066387,
      "loss": 0.2855,
      "step": 1282
    },
    {
      "epoch": 0.04974073176641628,
      "grad_norm": 0.20964425802230835,
      "learning_rate": 0.00019008841321544907,
      "loss": 0.3911,
      "step": 1283
    },
    {
      "epoch": 0.049779500848073664,
      "grad_norm": 0.18950605392456055,
      "learning_rate": 0.0001900806576702342,
      "loss": 0.2732,
      "step": 1284
    },
    {
      "epoch": 0.04981826992973104,
      "grad_norm": 0.14198113977909088,
      "learning_rate": 0.0001900729021250194,
      "loss": 0.2345,
      "step": 1285
    },
    {
      "epoch": 0.04985703901138842,
      "grad_norm": 0.1794486790895462,
      "learning_rate": 0.00019006514657980456,
      "loss": 0.3267,
      "step": 1286
    },
    {
      "epoch": 0.04989580809304579,
      "grad_norm": 0.14544712007045746,
      "learning_rate": 0.00019005739103458976,
      "loss": 0.2875,
      "step": 1287
    },
    {
      "epoch": 0.049934577174703176,
      "grad_norm": 0.18391314148902893,
      "learning_rate": 0.0001900496354893749,
      "loss": 0.3403,
      "step": 1288
    },
    {
      "epoch": 0.04997334625636055,
      "grad_norm": 0.1305246204137802,
      "learning_rate": 0.00019004187994416008,
      "loss": 0.2183,
      "step": 1289
    },
    {
      "epoch": 0.05001211533801793,
      "grad_norm": 0.15464550256729126,
      "learning_rate": 0.00019003412439894525,
      "loss": 0.2989,
      "step": 1290
    },
    {
      "epoch": 0.05005088441967531,
      "grad_norm": 0.15933439135551453,
      "learning_rate": 0.00019002636885373042,
      "loss": 0.3216,
      "step": 1291
    },
    {
      "epoch": 0.05008965350133269,
      "grad_norm": 0.14435285329818726,
      "learning_rate": 0.0001900186133085156,
      "loss": 0.2688,
      "step": 1292
    },
    {
      "epoch": 0.050128422582990065,
      "grad_norm": 0.11549913138151169,
      "learning_rate": 0.00019001085776330077,
      "loss": 0.2095,
      "step": 1293
    },
    {
      "epoch": 0.05016719166464744,
      "grad_norm": 0.15824031829833984,
      "learning_rate": 0.00019000310221808594,
      "loss": 0.3267,
      "step": 1294
    },
    {
      "epoch": 0.050205960746304824,
      "grad_norm": 0.15752612054347992,
      "learning_rate": 0.00018999534667287111,
      "loss": 0.2922,
      "step": 1295
    },
    {
      "epoch": 0.0502447298279622,
      "grad_norm": 0.171314537525177,
      "learning_rate": 0.00018998759112765626,
      "loss": 0.3143,
      "step": 1296
    },
    {
      "epoch": 0.05028349890961958,
      "grad_norm": 0.18380162119865417,
      "learning_rate": 0.00018997983558244146,
      "loss": 0.3578,
      "step": 1297
    },
    {
      "epoch": 0.05032226799127695,
      "grad_norm": 0.16371405124664307,
      "learning_rate": 0.0001899720800372266,
      "loss": 0.3113,
      "step": 1298
    },
    {
      "epoch": 0.050361037072934337,
      "grad_norm": 0.17806553840637207,
      "learning_rate": 0.0001899643244920118,
      "loss": 0.2502,
      "step": 1299
    },
    {
      "epoch": 0.05039980615459171,
      "grad_norm": 0.21683789789676666,
      "learning_rate": 0.00018995656894679695,
      "loss": 0.3145,
      "step": 1300
    },
    {
      "epoch": 0.05043857523624909,
      "grad_norm": 0.19938446581363678,
      "learning_rate": 0.00018994881340158215,
      "loss": 0.3116,
      "step": 1301
    },
    {
      "epoch": 0.05047734431790647,
      "grad_norm": 0.21240560710430145,
      "learning_rate": 0.00018994105785636732,
      "loss": 0.3725,
      "step": 1302
    },
    {
      "epoch": 0.05051611339956385,
      "grad_norm": 0.1313590705394745,
      "learning_rate": 0.00018993330231115247,
      "loss": 0.2366,
      "step": 1303
    },
    {
      "epoch": 0.050554882481221225,
      "grad_norm": 0.19213294982910156,
      "learning_rate": 0.00018992554676593767,
      "loss": 0.3104,
      "step": 1304
    },
    {
      "epoch": 0.0505936515628786,
      "grad_norm": 0.1868840754032135,
      "learning_rate": 0.00018991779122072282,
      "loss": 0.3673,
      "step": 1305
    },
    {
      "epoch": 0.050632420644535985,
      "grad_norm": 0.17773647606372833,
      "learning_rate": 0.00018991003567550802,
      "loss": 0.3329,
      "step": 1306
    },
    {
      "epoch": 0.05067118972619336,
      "grad_norm": 0.14409928023815155,
      "learning_rate": 0.00018990228013029316,
      "loss": 0.263,
      "step": 1307
    },
    {
      "epoch": 0.05070995880785074,
      "grad_norm": 0.14993499219417572,
      "learning_rate": 0.00018989452458507836,
      "loss": 0.282,
      "step": 1308
    },
    {
      "epoch": 0.05074872788950812,
      "grad_norm": 0.13832543790340424,
      "learning_rate": 0.0001898867690398635,
      "loss": 0.2311,
      "step": 1309
    },
    {
      "epoch": 0.0507874969711655,
      "grad_norm": 0.20288538932800293,
      "learning_rate": 0.00018987901349464868,
      "loss": 0.3935,
      "step": 1310
    },
    {
      "epoch": 0.05082626605282287,
      "grad_norm": 0.15707054734230042,
      "learning_rate": 0.00018987125794943385,
      "loss": 0.3085,
      "step": 1311
    },
    {
      "epoch": 0.05086503513448025,
      "grad_norm": 0.18386471271514893,
      "learning_rate": 0.00018986350240421902,
      "loss": 0.2978,
      "step": 1312
    },
    {
      "epoch": 0.05090380421613763,
      "grad_norm": 0.1503218114376068,
      "learning_rate": 0.0001898557468590042,
      "loss": 0.2889,
      "step": 1313
    },
    {
      "epoch": 0.05094257329779501,
      "grad_norm": 0.20429766178131104,
      "learning_rate": 0.00018984799131378937,
      "loss": 0.2979,
      "step": 1314
    },
    {
      "epoch": 0.050981342379452385,
      "grad_norm": 0.15535986423492432,
      "learning_rate": 0.00018984023576857454,
      "loss": 0.2748,
      "step": 1315
    },
    {
      "epoch": 0.05102011146110977,
      "grad_norm": 0.14341658353805542,
      "learning_rate": 0.00018983248022335972,
      "loss": 0.2875,
      "step": 1316
    },
    {
      "epoch": 0.051058880542767145,
      "grad_norm": 0.1835351139307022,
      "learning_rate": 0.00018982472467814486,
      "loss": 0.3156,
      "step": 1317
    },
    {
      "epoch": 0.05109764962442452,
      "grad_norm": 0.14508654177188873,
      "learning_rate": 0.00018981696913293006,
      "loss": 0.2349,
      "step": 1318
    },
    {
      "epoch": 0.0511364187060819,
      "grad_norm": 0.1410265862941742,
      "learning_rate": 0.0001898092135877152,
      "loss": 0.2294,
      "step": 1319
    },
    {
      "epoch": 0.05117518778773928,
      "grad_norm": 0.1604386419057846,
      "learning_rate": 0.0001898014580425004,
      "loss": 0.2862,
      "step": 1320
    },
    {
      "epoch": 0.05121395686939666,
      "grad_norm": 0.20947642624378204,
      "learning_rate": 0.00018979370249728555,
      "loss": 0.391,
      "step": 1321
    },
    {
      "epoch": 0.05125272595105403,
      "grad_norm": 0.16305719316005707,
      "learning_rate": 0.00018978594695207075,
      "loss": 0.2864,
      "step": 1322
    },
    {
      "epoch": 0.05129149503271141,
      "grad_norm": 0.1754949539899826,
      "learning_rate": 0.0001897781914068559,
      "loss": 0.2666,
      "step": 1323
    },
    {
      "epoch": 0.05133026411436879,
      "grad_norm": 0.2366311252117157,
      "learning_rate": 0.00018977043586164107,
      "loss": 0.3046,
      "step": 1324
    },
    {
      "epoch": 0.05136903319602617,
      "grad_norm": 0.17259123921394348,
      "learning_rate": 0.00018976268031642624,
      "loss": 0.2605,
      "step": 1325
    },
    {
      "epoch": 0.051407802277683545,
      "grad_norm": 0.15291200578212738,
      "learning_rate": 0.00018975492477121142,
      "loss": 0.2284,
      "step": 1326
    },
    {
      "epoch": 0.05144657135934093,
      "grad_norm": 0.1762566715478897,
      "learning_rate": 0.0001897471692259966,
      "loss": 0.2794,
      "step": 1327
    },
    {
      "epoch": 0.051485340440998305,
      "grad_norm": 0.16254785656929016,
      "learning_rate": 0.00018973941368078176,
      "loss": 0.2311,
      "step": 1328
    },
    {
      "epoch": 0.05152410952265568,
      "grad_norm": 0.13527676463127136,
      "learning_rate": 0.00018973165813556694,
      "loss": 0.2151,
      "step": 1329
    },
    {
      "epoch": 0.05156287860431306,
      "grad_norm": 0.17779748141765594,
      "learning_rate": 0.0001897239025903521,
      "loss": 0.2833,
      "step": 1330
    },
    {
      "epoch": 0.05160164768597044,
      "grad_norm": 0.12592124938964844,
      "learning_rate": 0.00018971614704513728,
      "loss": 0.2405,
      "step": 1331
    },
    {
      "epoch": 0.05164041676762782,
      "grad_norm": 0.13967913389205933,
      "learning_rate": 0.00018970839149992245,
      "loss": 0.2315,
      "step": 1332
    },
    {
      "epoch": 0.05167918584928519,
      "grad_norm": 0.1606459617614746,
      "learning_rate": 0.00018970063595470763,
      "loss": 0.2754,
      "step": 1333
    },
    {
      "epoch": 0.051717954930942577,
      "grad_norm": 0.1563214361667633,
      "learning_rate": 0.0001896928804094928,
      "loss": 0.2587,
      "step": 1334
    },
    {
      "epoch": 0.05175672401259995,
      "grad_norm": 0.17626021802425385,
      "learning_rate": 0.00018968512486427797,
      "loss": 0.2469,
      "step": 1335
    },
    {
      "epoch": 0.05179549309425733,
      "grad_norm": 0.14204755425453186,
      "learning_rate": 0.00018967736931906315,
      "loss": 0.265,
      "step": 1336
    },
    {
      "epoch": 0.051834262175914705,
      "grad_norm": 0.19733457267284393,
      "learning_rate": 0.00018966961377384832,
      "loss": 0.3692,
      "step": 1337
    },
    {
      "epoch": 0.05187303125757209,
      "grad_norm": 0.15215571224689484,
      "learning_rate": 0.00018966185822863346,
      "loss": 0.2825,
      "step": 1338
    },
    {
      "epoch": 0.051911800339229465,
      "grad_norm": 0.18153204023838043,
      "learning_rate": 0.00018965410268341866,
      "loss": 0.3373,
      "step": 1339
    },
    {
      "epoch": 0.05195056942088684,
      "grad_norm": 0.17211318016052246,
      "learning_rate": 0.0001896463471382038,
      "loss": 0.3243,
      "step": 1340
    },
    {
      "epoch": 0.05198933850254422,
      "grad_norm": 0.15258894860744476,
      "learning_rate": 0.000189638591592989,
      "loss": 0.3125,
      "step": 1341
    },
    {
      "epoch": 0.0520281075842016,
      "grad_norm": 0.16145873069763184,
      "learning_rate": 0.00018963083604777415,
      "loss": 0.2841,
      "step": 1342
    },
    {
      "epoch": 0.05206687666585898,
      "grad_norm": 0.17518019676208496,
      "learning_rate": 0.00018962308050255935,
      "loss": 0.3404,
      "step": 1343
    },
    {
      "epoch": 0.05210564574751635,
      "grad_norm": 0.13912883400917053,
      "learning_rate": 0.0001896153249573445,
      "loss": 0.2552,
      "step": 1344
    },
    {
      "epoch": 0.05214441482917374,
      "grad_norm": 0.1434067338705063,
      "learning_rate": 0.00018960756941212967,
      "loss": 0.2292,
      "step": 1345
    },
    {
      "epoch": 0.05218318391083111,
      "grad_norm": 0.1279401332139969,
      "learning_rate": 0.00018959981386691485,
      "loss": 0.231,
      "step": 1346
    },
    {
      "epoch": 0.05222195299248849,
      "grad_norm": 0.16890940070152283,
      "learning_rate": 0.00018959205832170002,
      "loss": 0.2447,
      "step": 1347
    },
    {
      "epoch": 0.052260722074145866,
      "grad_norm": 0.19846923649311066,
      "learning_rate": 0.0001895843027764852,
      "loss": 0.3302,
      "step": 1348
    },
    {
      "epoch": 0.05229949115580325,
      "grad_norm": 0.15181496739387512,
      "learning_rate": 0.00018957654723127036,
      "loss": 0.2072,
      "step": 1349
    },
    {
      "epoch": 0.052338260237460625,
      "grad_norm": 0.1852792352437973,
      "learning_rate": 0.00018956879168605554,
      "loss": 0.284,
      "step": 1350
    },
    {
      "epoch": 0.052377029319118,
      "grad_norm": 0.2757658362388611,
      "learning_rate": 0.0001895610361408407,
      "loss": 0.3584,
      "step": 1351
    },
    {
      "epoch": 0.052415798400775385,
      "grad_norm": 0.26778268814086914,
      "learning_rate": 0.00018955328059562588,
      "loss": 0.3057,
      "step": 1352
    },
    {
      "epoch": 0.05245456748243276,
      "grad_norm": 0.22625887393951416,
      "learning_rate": 0.00018954552505041106,
      "loss": 0.32,
      "step": 1353
    },
    {
      "epoch": 0.05249333656409014,
      "grad_norm": 0.16503183543682098,
      "learning_rate": 0.00018953776950519623,
      "loss": 0.2881,
      "step": 1354
    },
    {
      "epoch": 0.052532105645747514,
      "grad_norm": 0.21086500585079193,
      "learning_rate": 0.0001895300139599814,
      "loss": 0.3276,
      "step": 1355
    },
    {
      "epoch": 0.0525708747274049,
      "grad_norm": 0.17478974163532257,
      "learning_rate": 0.00018952225841476657,
      "loss": 0.2789,
      "step": 1356
    },
    {
      "epoch": 0.05260964380906227,
      "grad_norm": 0.17018024623394012,
      "learning_rate": 0.00018951450286955175,
      "loss": 0.3378,
      "step": 1357
    },
    {
      "epoch": 0.05264841289071965,
      "grad_norm": 0.16666750609874725,
      "learning_rate": 0.00018950674732433692,
      "loss": 0.3245,
      "step": 1358
    },
    {
      "epoch": 0.05268718197237703,
      "grad_norm": 0.15480469167232513,
      "learning_rate": 0.00018949899177912207,
      "loss": 0.2208,
      "step": 1359
    },
    {
      "epoch": 0.05272595105403441,
      "grad_norm": 0.13670404255390167,
      "learning_rate": 0.00018949123623390727,
      "loss": 0.2953,
      "step": 1360
    },
    {
      "epoch": 0.052764720135691785,
      "grad_norm": 0.14339607954025269,
      "learning_rate": 0.0001894834806886924,
      "loss": 0.3011,
      "step": 1361
    },
    {
      "epoch": 0.05280348921734916,
      "grad_norm": 0.11753711104393005,
      "learning_rate": 0.0001894757251434776,
      "loss": 0.2433,
      "step": 1362
    },
    {
      "epoch": 0.052842258299006545,
      "grad_norm": 0.13163185119628906,
      "learning_rate": 0.00018946796959826276,
      "loss": 0.243,
      "step": 1363
    },
    {
      "epoch": 0.05288102738066392,
      "grad_norm": 0.2014487087726593,
      "learning_rate": 0.00018946021405304796,
      "loss": 0.2815,
      "step": 1364
    },
    {
      "epoch": 0.0529197964623213,
      "grad_norm": 0.1968463957309723,
      "learning_rate": 0.0001894524585078331,
      "loss": 0.2889,
      "step": 1365
    },
    {
      "epoch": 0.052958565543978674,
      "grad_norm": 0.13837775588035583,
      "learning_rate": 0.00018944470296261827,
      "loss": 0.191,
      "step": 1366
    },
    {
      "epoch": 0.05299733462563606,
      "grad_norm": 0.21240650117397308,
      "learning_rate": 0.00018943694741740345,
      "loss": 0.3546,
      "step": 1367
    },
    {
      "epoch": 0.05303610370729343,
      "grad_norm": 0.15740200877189636,
      "learning_rate": 0.00018942919187218862,
      "loss": 0.2738,
      "step": 1368
    },
    {
      "epoch": 0.05307487278895081,
      "grad_norm": 0.21544453501701355,
      "learning_rate": 0.0001894214363269738,
      "loss": 0.3565,
      "step": 1369
    },
    {
      "epoch": 0.05311364187060819,
      "grad_norm": 0.18889273703098297,
      "learning_rate": 0.00018941368078175897,
      "loss": 0.2479,
      "step": 1370
    },
    {
      "epoch": 0.05315241095226557,
      "grad_norm": 0.1865210235118866,
      "learning_rate": 0.00018940592523654414,
      "loss": 0.2898,
      "step": 1371
    },
    {
      "epoch": 0.053191180033922945,
      "grad_norm": 0.23924489319324493,
      "learning_rate": 0.0001893981696913293,
      "loss": 0.3747,
      "step": 1372
    },
    {
      "epoch": 0.05322994911558032,
      "grad_norm": 0.16184408962726593,
      "learning_rate": 0.00018939041414611446,
      "loss": 0.2784,
      "step": 1373
    },
    {
      "epoch": 0.053268718197237705,
      "grad_norm": 0.18587490916252136,
      "learning_rate": 0.00018938265860089966,
      "loss": 0.2801,
      "step": 1374
    },
    {
      "epoch": 0.05330748727889508,
      "grad_norm": 0.2116042673587799,
      "learning_rate": 0.0001893749030556848,
      "loss": 0.349,
      "step": 1375
    },
    {
      "epoch": 0.05334625636055246,
      "grad_norm": 0.2012963891029358,
      "learning_rate": 0.00018936714751047,
      "loss": 0.3189,
      "step": 1376
    },
    {
      "epoch": 0.05338502544220984,
      "grad_norm": 0.18574388325214386,
      "learning_rate": 0.00018935939196525515,
      "loss": 0.3259,
      "step": 1377
    },
    {
      "epoch": 0.05342379452386722,
      "grad_norm": 0.1738116592168808,
      "learning_rate": 0.00018935163642004035,
      "loss": 0.253,
      "step": 1378
    },
    {
      "epoch": 0.05346256360552459,
      "grad_norm": 0.16929064691066742,
      "learning_rate": 0.0001893438808748255,
      "loss": 0.2933,
      "step": 1379
    },
    {
      "epoch": 0.05350133268718197,
      "grad_norm": 0.19930222630500793,
      "learning_rate": 0.00018933612532961067,
      "loss": 0.34,
      "step": 1380
    },
    {
      "epoch": 0.05354010176883935,
      "grad_norm": 0.20674629509449005,
      "learning_rate": 0.00018932836978439587,
      "loss": 0.3499,
      "step": 1381
    },
    {
      "epoch": 0.05357887085049673,
      "grad_norm": 0.19739441573619843,
      "learning_rate": 0.000189320614239181,
      "loss": 0.3657,
      "step": 1382
    },
    {
      "epoch": 0.053617639932154106,
      "grad_norm": 0.12049634754657745,
      "learning_rate": 0.0001893128586939662,
      "loss": 0.214,
      "step": 1383
    },
    {
      "epoch": 0.05365640901381149,
      "grad_norm": 0.1583579033613205,
      "learning_rate": 0.00018930510314875136,
      "loss": 0.263,
      "step": 1384
    },
    {
      "epoch": 0.053695178095468865,
      "grad_norm": 0.18846438825130463,
      "learning_rate": 0.00018929734760353656,
      "loss": 0.2941,
      "step": 1385
    },
    {
      "epoch": 0.05373394717712624,
      "grad_norm": 0.1750931292772293,
      "learning_rate": 0.0001892895920583217,
      "loss": 0.2853,
      "step": 1386
    },
    {
      "epoch": 0.05377271625878362,
      "grad_norm": 0.1932060569524765,
      "learning_rate": 0.0001892818365131069,
      "loss": 0.3254,
      "step": 1387
    },
    {
      "epoch": 0.053811485340441,
      "grad_norm": 0.1891627162694931,
      "learning_rate": 0.00018927408096789205,
      "loss": 0.2633,
      "step": 1388
    },
    {
      "epoch": 0.05385025442209838,
      "grad_norm": 0.21227481961250305,
      "learning_rate": 0.00018926632542267722,
      "loss": 0.3346,
      "step": 1389
    },
    {
      "epoch": 0.053889023503755754,
      "grad_norm": 0.19533886015415192,
      "learning_rate": 0.0001892585698774624,
      "loss": 0.264,
      "step": 1390
    },
    {
      "epoch": 0.05392779258541313,
      "grad_norm": 0.19810503721237183,
      "learning_rate": 0.00018925081433224757,
      "loss": 0.2334,
      "step": 1391
    },
    {
      "epoch": 0.05396656166707051,
      "grad_norm": 0.20988938212394714,
      "learning_rate": 0.00018924305878703274,
      "loss": 0.3144,
      "step": 1392
    },
    {
      "epoch": 0.05400533074872789,
      "grad_norm": 0.15675172209739685,
      "learning_rate": 0.00018923530324181791,
      "loss": 0.2184,
      "step": 1393
    },
    {
      "epoch": 0.054044099830385266,
      "grad_norm": 0.18847598135471344,
      "learning_rate": 0.0001892275476966031,
      "loss": 0.3164,
      "step": 1394
    },
    {
      "epoch": 0.05408286891204265,
      "grad_norm": 0.1235673651099205,
      "learning_rate": 0.00018921979215138826,
      "loss": 0.263,
      "step": 1395
    },
    {
      "epoch": 0.054121637993700025,
      "grad_norm": 0.17130737006664276,
      "learning_rate": 0.0001892120366061734,
      "loss": 0.3383,
      "step": 1396
    },
    {
      "epoch": 0.0541604070753574,
      "grad_norm": 0.15761765837669373,
      "learning_rate": 0.0001892042810609586,
      "loss": 0.277,
      "step": 1397
    },
    {
      "epoch": 0.05419917615701478,
      "grad_norm": 0.13998673856258392,
      "learning_rate": 0.00018919652551574375,
      "loss": 0.2665,
      "step": 1398
    },
    {
      "epoch": 0.05423794523867216,
      "grad_norm": 0.17006781697273254,
      "learning_rate": 0.00018918876997052895,
      "loss": 0.3152,
      "step": 1399
    },
    {
      "epoch": 0.05427671432032954,
      "grad_norm": 0.17954888939857483,
      "learning_rate": 0.0001891810144253141,
      "loss": 0.3259,
      "step": 1400
    },
    {
      "epoch": 0.054315483401986914,
      "grad_norm": 0.1593959629535675,
      "learning_rate": 0.0001891732588800993,
      "loss": 0.2854,
      "step": 1401
    },
    {
      "epoch": 0.0543542524836443,
      "grad_norm": 0.14933505654335022,
      "learning_rate": 0.00018916550333488444,
      "loss": 0.2226,
      "step": 1402
    },
    {
      "epoch": 0.05439302156530167,
      "grad_norm": 0.16035352647304535,
      "learning_rate": 0.00018915774778966961,
      "loss": 0.2578,
      "step": 1403
    },
    {
      "epoch": 0.05443179064695905,
      "grad_norm": 0.16602979600429535,
      "learning_rate": 0.0001891499922444548,
      "loss": 0.2869,
      "step": 1404
    },
    {
      "epoch": 0.054470559728616426,
      "grad_norm": 0.21033421158790588,
      "learning_rate": 0.00018914223669923996,
      "loss": 0.3911,
      "step": 1405
    },
    {
      "epoch": 0.05450932881027381,
      "grad_norm": 0.16647061705589294,
      "learning_rate": 0.00018913448115402513,
      "loss": 0.2983,
      "step": 1406
    },
    {
      "epoch": 0.054548097891931185,
      "grad_norm": 0.21003040671348572,
      "learning_rate": 0.0001891267256088103,
      "loss": 0.3958,
      "step": 1407
    },
    {
      "epoch": 0.05458686697358856,
      "grad_norm": 0.1200169175863266,
      "learning_rate": 0.00018911897006359548,
      "loss": 0.2105,
      "step": 1408
    },
    {
      "epoch": 0.05462563605524594,
      "grad_norm": 0.13867157697677612,
      "learning_rate": 0.00018911121451838065,
      "loss": 0.3362,
      "step": 1409
    },
    {
      "epoch": 0.05466440513690332,
      "grad_norm": 0.17727093398571014,
      "learning_rate": 0.00018910345897316582,
      "loss": 0.299,
      "step": 1410
    },
    {
      "epoch": 0.0547031742185607,
      "grad_norm": 0.13945186138153076,
      "learning_rate": 0.000189095703427951,
      "loss": 0.2291,
      "step": 1411
    },
    {
      "epoch": 0.054741943300218074,
      "grad_norm": 0.1496758610010147,
      "learning_rate": 0.00018908794788273617,
      "loss": 0.3201,
      "step": 1412
    },
    {
      "epoch": 0.05478071238187546,
      "grad_norm": 0.1648135632276535,
      "learning_rate": 0.00018908019233752134,
      "loss": 0.3086,
      "step": 1413
    },
    {
      "epoch": 0.05481948146353283,
      "grad_norm": 0.16413284838199615,
      "learning_rate": 0.00018907243679230652,
      "loss": 0.2625,
      "step": 1414
    },
    {
      "epoch": 0.05485825054519021,
      "grad_norm": 0.19841086864471436,
      "learning_rate": 0.0001890646812470917,
      "loss": 0.3136,
      "step": 1415
    },
    {
      "epoch": 0.054897019626847586,
      "grad_norm": 0.18619300425052643,
      "learning_rate": 0.00018905692570187686,
      "loss": 0.298,
      "step": 1416
    },
    {
      "epoch": 0.05493578870850497,
      "grad_norm": 0.14828790724277496,
      "learning_rate": 0.000189049170156662,
      "loss": 0.2181,
      "step": 1417
    },
    {
      "epoch": 0.054974557790162346,
      "grad_norm": 0.1835087686777115,
      "learning_rate": 0.0001890414146114472,
      "loss": 0.281,
      "step": 1418
    },
    {
      "epoch": 0.05501332687181972,
      "grad_norm": 0.181406170129776,
      "learning_rate": 0.00018903365906623235,
      "loss": 0.2814,
      "step": 1419
    },
    {
      "epoch": 0.055052095953477105,
      "grad_norm": 0.18980099260807037,
      "learning_rate": 0.00018902590352101755,
      "loss": 0.2739,
      "step": 1420
    },
    {
      "epoch": 0.05509086503513448,
      "grad_norm": 0.1493472009897232,
      "learning_rate": 0.0001890181479758027,
      "loss": 0.2259,
      "step": 1421
    },
    {
      "epoch": 0.05512963411679186,
      "grad_norm": 0.20507971942424774,
      "learning_rate": 0.0001890103924305879,
      "loss": 0.2714,
      "step": 1422
    },
    {
      "epoch": 0.055168403198449234,
      "grad_norm": 0.17844855785369873,
      "learning_rate": 0.00018900263688537304,
      "loss": 0.2435,
      "step": 1423
    },
    {
      "epoch": 0.05520717228010662,
      "grad_norm": 0.22507676482200623,
      "learning_rate": 0.00018899488134015822,
      "loss": 0.4036,
      "step": 1424
    },
    {
      "epoch": 0.055245941361763994,
      "grad_norm": 0.16381637752056122,
      "learning_rate": 0.0001889871257949434,
      "loss": 0.275,
      "step": 1425
    },
    {
      "epoch": 0.05528471044342137,
      "grad_norm": 0.16631734371185303,
      "learning_rate": 0.00018897937024972856,
      "loss": 0.2618,
      "step": 1426
    },
    {
      "epoch": 0.05532347952507875,
      "grad_norm": 0.1781192570924759,
      "learning_rate": 0.00018897161470451373,
      "loss": 0.2846,
      "step": 1427
    },
    {
      "epoch": 0.05536224860673613,
      "grad_norm": 0.18233266472816467,
      "learning_rate": 0.0001889638591592989,
      "loss": 0.2563,
      "step": 1428
    },
    {
      "epoch": 0.055401017688393506,
      "grad_norm": 0.21910659968852997,
      "learning_rate": 0.00018895610361408408,
      "loss": 0.3136,
      "step": 1429
    },
    {
      "epoch": 0.05543978677005088,
      "grad_norm": 0.19021914899349213,
      "learning_rate": 0.00018894834806886925,
      "loss": 0.3223,
      "step": 1430
    },
    {
      "epoch": 0.055478555851708265,
      "grad_norm": 0.1793530285358429,
      "learning_rate": 0.00018894059252365443,
      "loss": 0.2505,
      "step": 1431
    },
    {
      "epoch": 0.05551732493336564,
      "grad_norm": 0.16662636399269104,
      "learning_rate": 0.0001889328369784396,
      "loss": 0.2869,
      "step": 1432
    },
    {
      "epoch": 0.05555609401502302,
      "grad_norm": 0.15557487308979034,
      "learning_rate": 0.00018892508143322477,
      "loss": 0.2454,
      "step": 1433
    },
    {
      "epoch": 0.055594863096680394,
      "grad_norm": 0.20977266132831573,
      "learning_rate": 0.00018891732588800994,
      "loss": 0.2919,
      "step": 1434
    },
    {
      "epoch": 0.05563363217833778,
      "grad_norm": 0.2137739360332489,
      "learning_rate": 0.00018890957034279512,
      "loss": 0.2908,
      "step": 1435
    },
    {
      "epoch": 0.055672401259995154,
      "grad_norm": 0.190482497215271,
      "learning_rate": 0.0001889018147975803,
      "loss": 0.2226,
      "step": 1436
    },
    {
      "epoch": 0.05571117034165253,
      "grad_norm": 0.2179059386253357,
      "learning_rate": 0.00018889405925236546,
      "loss": 0.2849,
      "step": 1437
    },
    {
      "epoch": 0.05574993942330991,
      "grad_norm": 0.24024561047554016,
      "learning_rate": 0.0001888863037071506,
      "loss": 0.296,
      "step": 1438
    },
    {
      "epoch": 0.05578870850496729,
      "grad_norm": 0.17551369965076447,
      "learning_rate": 0.0001888785481619358,
      "loss": 0.2813,
      "step": 1439
    },
    {
      "epoch": 0.055827477586624666,
      "grad_norm": 0.15175218880176544,
      "learning_rate": 0.00018887079261672095,
      "loss": 0.2285,
      "step": 1440
    },
    {
      "epoch": 0.05586624666828204,
      "grad_norm": 0.23114000260829926,
      "learning_rate": 0.00018886303707150615,
      "loss": 0.27,
      "step": 1441
    },
    {
      "epoch": 0.055905015749939425,
      "grad_norm": 0.20595529675483704,
      "learning_rate": 0.0001888552815262913,
      "loss": 0.3086,
      "step": 1442
    },
    {
      "epoch": 0.0559437848315968,
      "grad_norm": 0.19397370517253876,
      "learning_rate": 0.0001888475259810765,
      "loss": 0.2316,
      "step": 1443
    },
    {
      "epoch": 0.05598255391325418,
      "grad_norm": 0.2796619236469269,
      "learning_rate": 0.00018883977043586165,
      "loss": 0.3682,
      "step": 1444
    },
    {
      "epoch": 0.05602132299491156,
      "grad_norm": 0.17325997352600098,
      "learning_rate": 0.00018883201489064682,
      "loss": 0.252,
      "step": 1445
    },
    {
      "epoch": 0.05606009207656894,
      "grad_norm": 0.19931234419345856,
      "learning_rate": 0.000188824259345432,
      "loss": 0.2501,
      "step": 1446
    },
    {
      "epoch": 0.056098861158226314,
      "grad_norm": 0.24796171486377716,
      "learning_rate": 0.00018881650380021716,
      "loss": 0.2769,
      "step": 1447
    },
    {
      "epoch": 0.05613763023988369,
      "grad_norm": 0.22545084357261658,
      "learning_rate": 0.00018880874825500234,
      "loss": 0.3361,
      "step": 1448
    },
    {
      "epoch": 0.05617639932154107,
      "grad_norm": 0.20321384072303772,
      "learning_rate": 0.0001888009927097875,
      "loss": 0.2935,
      "step": 1449
    },
    {
      "epoch": 0.05621516840319845,
      "grad_norm": 0.20022350549697876,
      "learning_rate": 0.00018879323716457268,
      "loss": 0.298,
      "step": 1450
    },
    {
      "epoch": 0.056253937484855826,
      "grad_norm": 0.18166030943393707,
      "learning_rate": 0.00018878548161935786,
      "loss": 0.3002,
      "step": 1451
    },
    {
      "epoch": 0.0562927065665132,
      "grad_norm": 0.20590457320213318,
      "learning_rate": 0.000188777726074143,
      "loss": 0.3375,
      "step": 1452
    },
    {
      "epoch": 0.056331475648170586,
      "grad_norm": 0.1818399727344513,
      "learning_rate": 0.0001887699705289282,
      "loss": 0.3079,
      "step": 1453
    },
    {
      "epoch": 0.05637024472982796,
      "grad_norm": 0.15604618191719055,
      "learning_rate": 0.00018876221498371335,
      "loss": 0.2302,
      "step": 1454
    },
    {
      "epoch": 0.05640901381148534,
      "grad_norm": 0.17119601368904114,
      "learning_rate": 0.00018875445943849855,
      "loss": 0.2763,
      "step": 1455
    },
    {
      "epoch": 0.05644778289314272,
      "grad_norm": 0.13665904104709625,
      "learning_rate": 0.0001887467038932837,
      "loss": 0.2096,
      "step": 1456
    },
    {
      "epoch": 0.0564865519748001,
      "grad_norm": 0.2158258706331253,
      "learning_rate": 0.0001887389483480689,
      "loss": 0.3722,
      "step": 1457
    },
    {
      "epoch": 0.056525321056457474,
      "grad_norm": 0.19071806967258453,
      "learning_rate": 0.00018873119280285404,
      "loss": 0.2644,
      "step": 1458
    },
    {
      "epoch": 0.05656409013811485,
      "grad_norm": 0.20921936631202698,
      "learning_rate": 0.0001887234372576392,
      "loss": 0.3036,
      "step": 1459
    },
    {
      "epoch": 0.056602859219772234,
      "grad_norm": 0.1764773428440094,
      "learning_rate": 0.0001887156817124244,
      "loss": 0.2938,
      "step": 1460
    },
    {
      "epoch": 0.05664162830142961,
      "grad_norm": 0.15639685094356537,
      "learning_rate": 0.00018870792616720956,
      "loss": 0.2287,
      "step": 1461
    },
    {
      "epoch": 0.056680397383086986,
      "grad_norm": 0.21784788370132446,
      "learning_rate": 0.00018870017062199476,
      "loss": 0.346,
      "step": 1462
    },
    {
      "epoch": 0.05671916646474437,
      "grad_norm": 0.19024091958999634,
      "learning_rate": 0.0001886924150767799,
      "loss": 0.3147,
      "step": 1463
    },
    {
      "epoch": 0.056757935546401746,
      "grad_norm": 0.17092864215373993,
      "learning_rate": 0.0001886846595315651,
      "loss": 0.2751,
      "step": 1464
    },
    {
      "epoch": 0.05679670462805912,
      "grad_norm": 0.20992577075958252,
      "learning_rate": 0.00018867690398635025,
      "loss": 0.3778,
      "step": 1465
    },
    {
      "epoch": 0.0568354737097165,
      "grad_norm": 0.17547455430030823,
      "learning_rate": 0.00018866914844113542,
      "loss": 0.2566,
      "step": 1466
    },
    {
      "epoch": 0.05687424279137388,
      "grad_norm": 0.1651827096939087,
      "learning_rate": 0.0001886613928959206,
      "loss": 0.3121,
      "step": 1467
    },
    {
      "epoch": 0.05691301187303126,
      "grad_norm": 0.20138555765151978,
      "learning_rate": 0.00018865363735070577,
      "loss": 0.3161,
      "step": 1468
    },
    {
      "epoch": 0.056951780954688634,
      "grad_norm": 0.14640842378139496,
      "learning_rate": 0.00018864588180549094,
      "loss": 0.2337,
      "step": 1469
    },
    {
      "epoch": 0.05699055003634602,
      "grad_norm": 0.1826796978712082,
      "learning_rate": 0.0001886381262602761,
      "loss": 0.2956,
      "step": 1470
    },
    {
      "epoch": 0.057029319118003394,
      "grad_norm": 0.1469845175743103,
      "learning_rate": 0.00018863037071506128,
      "loss": 0.2514,
      "step": 1471
    },
    {
      "epoch": 0.05706808819966077,
      "grad_norm": 0.1973622590303421,
      "learning_rate": 0.00018862261516984646,
      "loss": 0.2707,
      "step": 1472
    },
    {
      "epoch": 0.057106857281318146,
      "grad_norm": 0.2816251516342163,
      "learning_rate": 0.0001886148596246316,
      "loss": 0.3362,
      "step": 1473
    },
    {
      "epoch": 0.05714562636297553,
      "grad_norm": 0.22024424374103546,
      "learning_rate": 0.0001886071040794168,
      "loss": 0.3452,
      "step": 1474
    },
    {
      "epoch": 0.057184395444632906,
      "grad_norm": 0.1387525200843811,
      "learning_rate": 0.00018859934853420195,
      "loss": 0.2164,
      "step": 1475
    },
    {
      "epoch": 0.05722316452629028,
      "grad_norm": 0.15946665406227112,
      "learning_rate": 0.00018859159298898715,
      "loss": 0.2778,
      "step": 1476
    },
    {
      "epoch": 0.05726193360794766,
      "grad_norm": 0.2004278153181076,
      "learning_rate": 0.0001885838374437723,
      "loss": 0.2902,
      "step": 1477
    },
    {
      "epoch": 0.05730070268960504,
      "grad_norm": 0.1756836473941803,
      "learning_rate": 0.0001885760818985575,
      "loss": 0.2655,
      "step": 1478
    },
    {
      "epoch": 0.05733947177126242,
      "grad_norm": 0.2524718940258026,
      "learning_rate": 0.00018856832635334264,
      "loss": 0.3514,
      "step": 1479
    },
    {
      "epoch": 0.057378240852919794,
      "grad_norm": 0.130469411611557,
      "learning_rate": 0.0001885605708081278,
      "loss": 0.201,
      "step": 1480
    },
    {
      "epoch": 0.05741700993457718,
      "grad_norm": 0.18980170786380768,
      "learning_rate": 0.00018855281526291299,
      "loss": 0.2561,
      "step": 1481
    },
    {
      "epoch": 0.057455779016234554,
      "grad_norm": 0.20276950299739838,
      "learning_rate": 0.00018854505971769816,
      "loss": 0.2936,
      "step": 1482
    },
    {
      "epoch": 0.05749454809789193,
      "grad_norm": 0.17888379096984863,
      "learning_rate": 0.00018853730417248333,
      "loss": 0.2643,
      "step": 1483
    },
    {
      "epoch": 0.057533317179549306,
      "grad_norm": 0.1785721629858017,
      "learning_rate": 0.0001885295486272685,
      "loss": 0.2836,
      "step": 1484
    },
    {
      "epoch": 0.05757208626120669,
      "grad_norm": 0.21786034107208252,
      "learning_rate": 0.00018852179308205368,
      "loss": 0.329,
      "step": 1485
    },
    {
      "epoch": 0.057610855342864066,
      "grad_norm": 0.22554899752140045,
      "learning_rate": 0.00018851403753683885,
      "loss": 0.3002,
      "step": 1486
    },
    {
      "epoch": 0.05764962442452144,
      "grad_norm": 0.1540171056985855,
      "learning_rate": 0.00018850628199162402,
      "loss": 0.2003,
      "step": 1487
    },
    {
      "epoch": 0.057688393506178826,
      "grad_norm": 0.23229603469371796,
      "learning_rate": 0.0001884985264464092,
      "loss": 0.3135,
      "step": 1488
    },
    {
      "epoch": 0.0577271625878362,
      "grad_norm": 0.222370907664299,
      "learning_rate": 0.00018849077090119437,
      "loss": 0.343,
      "step": 1489
    },
    {
      "epoch": 0.05776593166949358,
      "grad_norm": 0.1935359239578247,
      "learning_rate": 0.00018848301535597954,
      "loss": 0.2586,
      "step": 1490
    },
    {
      "epoch": 0.057804700751150954,
      "grad_norm": 0.186928853392601,
      "learning_rate": 0.0001884752598107647,
      "loss": 0.2346,
      "step": 1491
    },
    {
      "epoch": 0.05784346983280834,
      "grad_norm": 0.2035459280014038,
      "learning_rate": 0.00018846750426554989,
      "loss": 0.311,
      "step": 1492
    },
    {
      "epoch": 0.057882238914465714,
      "grad_norm": 0.17360734939575195,
      "learning_rate": 0.00018845974872033506,
      "loss": 0.306,
      "step": 1493
    },
    {
      "epoch": 0.05792100799612309,
      "grad_norm": 0.15943557024002075,
      "learning_rate": 0.0001884519931751202,
      "loss": 0.2724,
      "step": 1494
    },
    {
      "epoch": 0.05795977707778047,
      "grad_norm": 0.2060958743095398,
      "learning_rate": 0.0001884442376299054,
      "loss": 0.313,
      "step": 1495
    },
    {
      "epoch": 0.05799854615943785,
      "grad_norm": 0.2767384946346283,
      "learning_rate": 0.00018843648208469055,
      "loss": 0.4367,
      "step": 1496
    },
    {
      "epoch": 0.058037315241095226,
      "grad_norm": 0.13817454874515533,
      "learning_rate": 0.00018842872653947575,
      "loss": 0.2381,
      "step": 1497
    },
    {
      "epoch": 0.0580760843227526,
      "grad_norm": 0.16146117448806763,
      "learning_rate": 0.0001884209709942609,
      "loss": 0.2527,
      "step": 1498
    },
    {
      "epoch": 0.058114853404409986,
      "grad_norm": 0.19155174493789673,
      "learning_rate": 0.0001884132154490461,
      "loss": 0.2872,
      "step": 1499
    },
    {
      "epoch": 0.05815362248606736,
      "grad_norm": 0.1773339956998825,
      "learning_rate": 0.00018840545990383124,
      "loss": 0.2921,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 25793,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.69203859390464e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
