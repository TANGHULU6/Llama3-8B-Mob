{
  "best_metric": 1.1555155515670776,
  "best_model_checkpoint": "outputs/checkpoint-1500",
  "epoch": 3.0,
  "eval_steps": 100,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 0.07153336703777313,
      "learning_rate": 0.0004,
      "loss": 0.3265,
      "step": 1
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.07364838570356369,
      "learning_rate": 0.0008,
      "loss": 0.3142,
      "step": 2
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.12498562783002853,
      "learning_rate": 0.0012,
      "loss": 0.3848,
      "step": 3
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.18664319813251495,
      "learning_rate": 0.0016,
      "loss": 0.3473,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.0481288433074951,
      "learning_rate": 0.002,
      "loss": 0.5477,
      "step": 5
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.36579111218452454,
      "learning_rate": 0.0019999977920603196,
      "loss": 0.4836,
      "step": 6
    },
    {
      "epoch": 0.014,
      "grad_norm": 2.3086600303649902,
      "learning_rate": 0.0019999911682510277,
      "loss": 0.5702,
      "step": 7
    },
    {
      "epoch": 0.016,
      "grad_norm": 22.525005340576172,
      "learning_rate": 0.001999980128601375,
      "loss": 1.0408,
      "step": 8
    },
    {
      "epoch": 0.018,
      "grad_norm": 21.848602294921875,
      "learning_rate": 0.00199996467316011,
      "loss": 3.2526,
      "step": 9
    },
    {
      "epoch": 0.02,
      "grad_norm": 14.757415771484375,
      "learning_rate": 0.001999944801995484,
      "loss": 1.8052,
      "step": 10
    },
    {
      "epoch": 0.022,
      "grad_norm": 17.1826171875,
      "learning_rate": 0.0019999205151952438,
      "loss": 2.3941,
      "step": 11
    },
    {
      "epoch": 0.024,
      "grad_norm": 6.885984420776367,
      "learning_rate": 0.001999891812866638,
      "loss": 0.9802,
      "step": 12
    },
    {
      "epoch": 0.026,
      "grad_norm": 10.152630805969238,
      "learning_rate": 0.0019998586951364126,
      "loss": 1.7751,
      "step": 13
    },
    {
      "epoch": 0.028,
      "grad_norm": 7.794259548187256,
      "learning_rate": 0.0019998211621508107,
      "loss": 0.9923,
      "step": 14
    },
    {
      "epoch": 0.03,
      "grad_norm": 9.997300148010254,
      "learning_rate": 0.0019997792140755742,
      "loss": 1.235,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 6.218258380889893,
      "learning_rate": 0.0019997328510959413,
      "loss": 0.8716,
      "step": 16
    },
    {
      "epoch": 0.034,
      "grad_norm": 7.197497844696045,
      "learning_rate": 0.001999682073416644,
      "loss": 0.9204,
      "step": 17
    },
    {
      "epoch": 0.036,
      "grad_norm": 40.00090789794922,
      "learning_rate": 0.0019996268812619107,
      "loss": 2.4472,
      "step": 18
    },
    {
      "epoch": 0.038,
      "grad_norm": 5.918007850646973,
      "learning_rate": 0.001999567274875464,
      "loss": 0.8146,
      "step": 19
    },
    {
      "epoch": 0.04,
      "grad_norm": 14.861492156982422,
      "learning_rate": 0.0019995032545205176,
      "loss": 1.3966,
      "step": 20
    },
    {
      "epoch": 0.042,
      "grad_norm": 15.856351852416992,
      "learning_rate": 0.0019994348204797788,
      "loss": 0.982,
      "step": 21
    },
    {
      "epoch": 0.044,
      "grad_norm": 11.071501731872559,
      "learning_rate": 0.001999361973055443,
      "loss": 1.5479,
      "step": 22
    },
    {
      "epoch": 0.046,
      "grad_norm": 108.72428894042969,
      "learning_rate": 0.001999284712569196,
      "loss": 3.0887,
      "step": 23
    },
    {
      "epoch": 0.048,
      "grad_norm": 59.42506408691406,
      "learning_rate": 0.0019992030393622107,
      "loss": 3.3124,
      "step": 24
    },
    {
      "epoch": 0.05,
      "grad_norm": 5.313709735870361,
      "learning_rate": 0.0019991169537951466,
      "loss": 0.9667,
      "step": 25
    },
    {
      "epoch": 0.052,
      "grad_norm": 13.352042198181152,
      "learning_rate": 0.001999026456248147,
      "loss": 1.5735,
      "step": 26
    },
    {
      "epoch": 0.054,
      "grad_norm": 4.222155570983887,
      "learning_rate": 0.001998931547120838,
      "loss": 1.113,
      "step": 27
    },
    {
      "epoch": 0.056,
      "grad_norm": 8.537115097045898,
      "learning_rate": 0.0019988322268323267,
      "loss": 1.0243,
      "step": 28
    },
    {
      "epoch": 0.058,
      "grad_norm": 1.7223957777023315,
      "learning_rate": 0.0019987284958211996,
      "loss": 0.6193,
      "step": 29
    },
    {
      "epoch": 0.06,
      "grad_norm": 18.858154296875,
      "learning_rate": 0.0019986203545455205,
      "loss": 1.6633,
      "step": 30
    },
    {
      "epoch": 0.062,
      "grad_norm": 14.651820182800293,
      "learning_rate": 0.001998507803482828,
      "loss": 2.0824,
      "step": 31
    },
    {
      "epoch": 0.064,
      "grad_norm": 33.13444900512695,
      "learning_rate": 0.0019983908431301343,
      "loss": 2.237,
      "step": 32
    },
    {
      "epoch": 0.066,
      "grad_norm": 49.49964141845703,
      "learning_rate": 0.001998269474003922,
      "loss": 3.2682,
      "step": 33
    },
    {
      "epoch": 0.068,
      "grad_norm": 11.279411315917969,
      "learning_rate": 0.0019981436966401427,
      "loss": 1.8529,
      "step": 34
    },
    {
      "epoch": 0.07,
      "grad_norm": 11.400317192077637,
      "learning_rate": 0.0019980135115942135,
      "loss": 1.2975,
      "step": 35
    },
    {
      "epoch": 0.072,
      "grad_norm": 2.172196865081787,
      "learning_rate": 0.0019978789194410166,
      "loss": 0.667,
      "step": 36
    },
    {
      "epoch": 0.074,
      "grad_norm": 4.283565998077393,
      "learning_rate": 0.0019977399207748944,
      "loss": 0.7487,
      "step": 37
    },
    {
      "epoch": 0.076,
      "grad_norm": 16.038217544555664,
      "learning_rate": 0.0019975965162096483,
      "loss": 1.1877,
      "step": 38
    },
    {
      "epoch": 0.078,
      "grad_norm": 42.251346588134766,
      "learning_rate": 0.0019974487063785353,
      "loss": 1.4782,
      "step": 39
    },
    {
      "epoch": 0.08,
      "grad_norm": 38.56047058105469,
      "learning_rate": 0.0019972964919342663,
      "loss": 5.3519,
      "step": 40
    },
    {
      "epoch": 0.082,
      "grad_norm": 99.82169342041016,
      "learning_rate": 0.0019971398735490016,
      "loss": 13.7152,
      "step": 41
    },
    {
      "epoch": 0.084,
      "grad_norm": 53.84722900390625,
      "learning_rate": 0.001996978851914349,
      "loss": 14.451,
      "step": 42
    },
    {
      "epoch": 0.086,
      "grad_norm": 23.021757125854492,
      "learning_rate": 0.0019968134277413606,
      "loss": 10.9631,
      "step": 43
    },
    {
      "epoch": 0.088,
      "grad_norm": 30.860822677612305,
      "learning_rate": 0.0019966436017605296,
      "loss": 12.2249,
      "step": 44
    },
    {
      "epoch": 0.09,
      "grad_norm": 26.272480010986328,
      "learning_rate": 0.0019964693747217873,
      "loss": 10.3854,
      "step": 45
    },
    {
      "epoch": 0.092,
      "grad_norm": 25.899433135986328,
      "learning_rate": 0.0019962907473944995,
      "loss": 11.9361,
      "step": 46
    },
    {
      "epoch": 0.094,
      "grad_norm": 16.461830139160156,
      "learning_rate": 0.001996107720567462,
      "loss": 10.2164,
      "step": 47
    },
    {
      "epoch": 0.096,
      "grad_norm": 7.193535804748535,
      "learning_rate": 0.0019959202950489,
      "loss": 5.7087,
      "step": 48
    },
    {
      "epoch": 0.098,
      "grad_norm": 33.34125900268555,
      "learning_rate": 0.0019957284716664615,
      "loss": 11.7368,
      "step": 49
    },
    {
      "epoch": 0.1,
      "grad_norm": 24.017000198364258,
      "learning_rate": 0.001995532251267216,
      "loss": 5.1569,
      "step": 50
    },
    {
      "epoch": 0.102,
      "grad_norm": 54.824546813964844,
      "learning_rate": 0.0019953316347176486,
      "loss": 15.9276,
      "step": 51
    },
    {
      "epoch": 0.104,
      "grad_norm": 35.168212890625,
      "learning_rate": 0.001995126622903658,
      "loss": 12.9034,
      "step": 52
    },
    {
      "epoch": 0.106,
      "grad_norm": 22.955968856811523,
      "learning_rate": 0.0019949172167305516,
      "loss": 8.2095,
      "step": 53
    },
    {
      "epoch": 0.108,
      "grad_norm": 11.206243515014648,
      "learning_rate": 0.001994703417123042,
      "loss": 5.0177,
      "step": 54
    },
    {
      "epoch": 0.11,
      "grad_norm": 25.80901527404785,
      "learning_rate": 0.0019944852250252418,
      "loss": 10.7141,
      "step": 55
    },
    {
      "epoch": 0.112,
      "grad_norm": 26.53685760498047,
      "learning_rate": 0.0019942626414006614,
      "loss": 8.0554,
      "step": 56
    },
    {
      "epoch": 0.114,
      "grad_norm": 26.4266300201416,
      "learning_rate": 0.0019940356672322034,
      "loss": 8.1232,
      "step": 57
    },
    {
      "epoch": 0.116,
      "grad_norm": 27.160112380981445,
      "learning_rate": 0.0019938043035221584,
      "loss": 8.1943,
      "step": 58
    },
    {
      "epoch": 0.118,
      "grad_norm": 14.713798522949219,
      "learning_rate": 0.0019935685512922005,
      "loss": 6.0858,
      "step": 59
    },
    {
      "epoch": 0.12,
      "grad_norm": 37.84613800048828,
      "learning_rate": 0.0019933284115833828,
      "loss": 6.1043,
      "step": 60
    },
    {
      "epoch": 0.122,
      "grad_norm": 36.95431900024414,
      "learning_rate": 0.001993083885456134,
      "loss": 5.8995,
      "step": 61
    },
    {
      "epoch": 0.124,
      "grad_norm": 37.088035583496094,
      "learning_rate": 0.001992834973990251,
      "loss": 7.143,
      "step": 62
    },
    {
      "epoch": 0.126,
      "grad_norm": 39.99118423461914,
      "learning_rate": 0.0019925816782848976,
      "loss": 6.7677,
      "step": 63
    },
    {
      "epoch": 0.128,
      "grad_norm": 8.601700782775879,
      "learning_rate": 0.0019923239994585965,
      "loss": 4.354,
      "step": 64
    },
    {
      "epoch": 0.13,
      "grad_norm": 76.58305358886719,
      "learning_rate": 0.0019920619386492268,
      "loss": 8.6727,
      "step": 65
    },
    {
      "epoch": 0.132,
      "grad_norm": 215.24545288085938,
      "learning_rate": 0.0019917954970140174,
      "loss": 19.3494,
      "step": 66
    },
    {
      "epoch": 0.134,
      "grad_norm": 317.1894836425781,
      "learning_rate": 0.001991524675729542,
      "loss": 13.2993,
      "step": 67
    },
    {
      "epoch": 0.136,
      "grad_norm": 63.986480712890625,
      "learning_rate": 0.001991249475991715,
      "loss": 8.1727,
      "step": 68
    },
    {
      "epoch": 0.138,
      "grad_norm": 61.14423370361328,
      "learning_rate": 0.001990969899015785,
      "loss": 18.8724,
      "step": 69
    },
    {
      "epoch": 0.14,
      "grad_norm": 48.705894470214844,
      "learning_rate": 0.0019906859460363307,
      "loss": 18.8833,
      "step": 70
    },
    {
      "epoch": 0.142,
      "grad_norm": 44.769229888916016,
      "learning_rate": 0.001990397618307254,
      "loss": 9.431,
      "step": 71
    },
    {
      "epoch": 0.144,
      "grad_norm": 36.0777702331543,
      "learning_rate": 0.001990104917101775,
      "loss": 9.1747,
      "step": 72
    },
    {
      "epoch": 0.146,
      "grad_norm": 20.258756637573242,
      "learning_rate": 0.0019898078437124273,
      "loss": 9.9993,
      "step": 73
    },
    {
      "epoch": 0.148,
      "grad_norm": 19.348947525024414,
      "learning_rate": 0.001989506399451051,
      "loss": 8.0574,
      "step": 74
    },
    {
      "epoch": 0.15,
      "grad_norm": 12.677506446838379,
      "learning_rate": 0.0019892005856487877,
      "loss": 4.6704,
      "step": 75
    },
    {
      "epoch": 0.152,
      "grad_norm": 7.288967132568359,
      "learning_rate": 0.0019888904036560744,
      "loss": 4.3465,
      "step": 76
    },
    {
      "epoch": 0.154,
      "grad_norm": 25.731111526489258,
      "learning_rate": 0.0019885758548426366,
      "loss": 4.551,
      "step": 77
    },
    {
      "epoch": 0.156,
      "grad_norm": 10.50670051574707,
      "learning_rate": 0.001988256940597485,
      "loss": 4.0567,
      "step": 78
    },
    {
      "epoch": 0.158,
      "grad_norm": 38.44298553466797,
      "learning_rate": 0.0019879336623289056,
      "loss": 5.0879,
      "step": 79
    },
    {
      "epoch": 0.16,
      "grad_norm": 18.668872833251953,
      "learning_rate": 0.0019876060214644568,
      "loss": 5.8952,
      "step": 80
    },
    {
      "epoch": 0.162,
      "grad_norm": 10.860904693603516,
      "learning_rate": 0.0019872740194509606,
      "loss": 4.8303,
      "step": 81
    },
    {
      "epoch": 0.164,
      "grad_norm": 16.271303176879883,
      "learning_rate": 0.0019869376577544983,
      "loss": 4.5477,
      "step": 82
    },
    {
      "epoch": 0.166,
      "grad_norm": 16.538307189941406,
      "learning_rate": 0.001986596937860402,
      "loss": 4.1279,
      "step": 83
    },
    {
      "epoch": 0.168,
      "grad_norm": 10.17186450958252,
      "learning_rate": 0.0019862518612732503,
      "loss": 3.4794,
      "step": 84
    },
    {
      "epoch": 0.17,
      "grad_norm": 126.83295440673828,
      "learning_rate": 0.0019859024295168595,
      "loss": 3.9457,
      "step": 85
    },
    {
      "epoch": 0.172,
      "grad_norm": 12.776949882507324,
      "learning_rate": 0.001985548644134278,
      "loss": 4.4403,
      "step": 86
    },
    {
      "epoch": 0.174,
      "grad_norm": 17.375917434692383,
      "learning_rate": 0.0019851905066877794,
      "loss": 4.2164,
      "step": 87
    },
    {
      "epoch": 0.176,
      "grad_norm": 8.938773155212402,
      "learning_rate": 0.0019848280187588557,
      "loss": 3.6192,
      "step": 88
    },
    {
      "epoch": 0.178,
      "grad_norm": 5.6368088722229,
      "learning_rate": 0.0019844611819482094,
      "loss": 3.4678,
      "step": 89
    },
    {
      "epoch": 0.18,
      "grad_norm": 6.4656782150268555,
      "learning_rate": 0.0019840899978757483,
      "loss": 3.3958,
      "step": 90
    },
    {
      "epoch": 0.182,
      "grad_norm": 3.5705130100250244,
      "learning_rate": 0.0019837144681805756,
      "loss": 3.1315,
      "step": 91
    },
    {
      "epoch": 0.184,
      "grad_norm": 4.337969779968262,
      "learning_rate": 0.0019833345945209856,
      "loss": 3.2196,
      "step": 92
    },
    {
      "epoch": 0.186,
      "grad_norm": 4.525212287902832,
      "learning_rate": 0.001982950378574455,
      "loss": 3.3266,
      "step": 93
    },
    {
      "epoch": 0.188,
      "grad_norm": 10.478811264038086,
      "learning_rate": 0.0019825618220376344,
      "loss": 3.6558,
      "step": 94
    },
    {
      "epoch": 0.19,
      "grad_norm": 2.0926499366760254,
      "learning_rate": 0.0019821689266263424,
      "loss": 2.8955,
      "step": 95
    },
    {
      "epoch": 0.192,
      "grad_norm": 7.8299784660339355,
      "learning_rate": 0.0019817716940755585,
      "loss": 3.3969,
      "step": 96
    },
    {
      "epoch": 0.194,
      "grad_norm": 21.893238067626953,
      "learning_rate": 0.0019813701261394137,
      "loss": 6.187,
      "step": 97
    },
    {
      "epoch": 0.196,
      "grad_norm": 12.930903434753418,
      "learning_rate": 0.001980964224591183,
      "loss": 4.349,
      "step": 98
    },
    {
      "epoch": 0.198,
      "grad_norm": 6.7530388832092285,
      "learning_rate": 0.0019805539912232783,
      "loss": 3.2752,
      "step": 99
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.532876491546631,
      "learning_rate": 0.0019801394278472417,
      "loss": 3.7424,
      "step": 100
    },
    {
      "epoch": 0.2,
      "eval_loss": 3.5269076824188232,
      "eval_runtime": 228.953,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 100
    },
    {
      "epoch": 0.202,
      "grad_norm": 5.784553050994873,
      "learning_rate": 0.0019797205362937346,
      "loss": 3.3643,
      "step": 101
    },
    {
      "epoch": 0.204,
      "grad_norm": 9.127640724182129,
      "learning_rate": 0.0019792973184125317,
      "loss": 3.704,
      "step": 102
    },
    {
      "epoch": 0.206,
      "grad_norm": 11.89439868927002,
      "learning_rate": 0.001978869776072512,
      "loss": 3.7486,
      "step": 103
    },
    {
      "epoch": 0.208,
      "grad_norm": 3.653444528579712,
      "learning_rate": 0.0019784379111616505,
      "loss": 3.1518,
      "step": 104
    },
    {
      "epoch": 0.21,
      "grad_norm": 6.55711555480957,
      "learning_rate": 0.001978001725587011,
      "loss": 3.7818,
      "step": 105
    },
    {
      "epoch": 0.212,
      "grad_norm": 5.933317184448242,
      "learning_rate": 0.001977561221274737,
      "loss": 3.4243,
      "step": 106
    },
    {
      "epoch": 0.214,
      "grad_norm": 6.509677886962891,
      "learning_rate": 0.001977116400170041,
      "loss": 3.2879,
      "step": 107
    },
    {
      "epoch": 0.216,
      "grad_norm": 4.7551188468933105,
      "learning_rate": 0.0019766672642372,
      "loss": 3.221,
      "step": 108
    },
    {
      "epoch": 0.218,
      "grad_norm": 5.226152420043945,
      "learning_rate": 0.0019762138154595446,
      "loss": 3.4831,
      "step": 109
    },
    {
      "epoch": 0.22,
      "grad_norm": 5.412924289703369,
      "learning_rate": 0.0019757560558394493,
      "loss": 3.2454,
      "step": 110
    },
    {
      "epoch": 0.222,
      "grad_norm": 2.984659433364868,
      "learning_rate": 0.0019752939873983254,
      "loss": 3.0154,
      "step": 111
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.7997300624847412,
      "learning_rate": 0.0019748276121766117,
      "loss": 2.984,
      "step": 112
    },
    {
      "epoch": 0.226,
      "grad_norm": 4.594057083129883,
      "learning_rate": 0.001974356932233764,
      "loss": 2.9406,
      "step": 113
    },
    {
      "epoch": 0.228,
      "grad_norm": 3.41648268699646,
      "learning_rate": 0.0019738819496482496,
      "loss": 3.0348,
      "step": 114
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.4951449632644653,
      "learning_rate": 0.0019734026665175334,
      "loss": 2.7256,
      "step": 115
    },
    {
      "epoch": 0.232,
      "grad_norm": 1.9623335599899292,
      "learning_rate": 0.001972919084958072,
      "loss": 2.8238,
      "step": 116
    },
    {
      "epoch": 0.234,
      "grad_norm": 1.9828375577926636,
      "learning_rate": 0.001972431207105303,
      "loss": 2.6557,
      "step": 117
    },
    {
      "epoch": 0.236,
      "grad_norm": 2.3744359016418457,
      "learning_rate": 0.001971939035113636,
      "loss": 2.6103,
      "step": 118
    },
    {
      "epoch": 0.238,
      "grad_norm": 3.2531588077545166,
      "learning_rate": 0.0019714425711564445,
      "loss": 2.7168,
      "step": 119
    },
    {
      "epoch": 0.24,
      "grad_norm": 7.147645950317383,
      "learning_rate": 0.001970941817426052,
      "loss": 2.9605,
      "step": 120
    },
    {
      "epoch": 0.242,
      "grad_norm": 2.484334945678711,
      "learning_rate": 0.001970436776133727,
      "loss": 2.7385,
      "step": 121
    },
    {
      "epoch": 0.244,
      "grad_norm": 1.369204044342041,
      "learning_rate": 0.0019699274495096715,
      "loss": 2.569,
      "step": 122
    },
    {
      "epoch": 0.246,
      "grad_norm": 10.312801361083984,
      "learning_rate": 0.0019694138398030094,
      "loss": 3.4052,
      "step": 123
    },
    {
      "epoch": 0.248,
      "grad_norm": 9.634992599487305,
      "learning_rate": 0.00196889594928178,
      "loss": 3.2181,
      "step": 124
    },
    {
      "epoch": 0.25,
      "grad_norm": 6.4653096199035645,
      "learning_rate": 0.001968373780232924,
      "loss": 2.8706,
      "step": 125
    },
    {
      "epoch": 0.252,
      "grad_norm": 4.661166667938232,
      "learning_rate": 0.0019678473349622793,
      "loss": 2.9823,
      "step": 126
    },
    {
      "epoch": 0.254,
      "grad_norm": 11.554218292236328,
      "learning_rate": 0.0019673166157945627,
      "loss": 3.067,
      "step": 127
    },
    {
      "epoch": 0.256,
      "grad_norm": 12.641718864440918,
      "learning_rate": 0.001966781625073367,
      "loss": 2.8902,
      "step": 128
    },
    {
      "epoch": 0.258,
      "grad_norm": 2.1051597595214844,
      "learning_rate": 0.001966242365161146,
      "loss": 2.687,
      "step": 129
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.1415337324142456,
      "learning_rate": 0.0019656988384392075,
      "loss": 2.6062,
      "step": 130
    },
    {
      "epoch": 0.262,
      "grad_norm": 7.867302417755127,
      "learning_rate": 0.0019651510473076986,
      "loss": 3.0761,
      "step": 131
    },
    {
      "epoch": 0.264,
      "grad_norm": 8.365605354309082,
      "learning_rate": 0.0019645989941856,
      "loss": 3.1222,
      "step": 132
    },
    {
      "epoch": 0.266,
      "grad_norm": 2.648066282272339,
      "learning_rate": 0.0019640426815107108,
      "loss": 2.7301,
      "step": 133
    },
    {
      "epoch": 0.268,
      "grad_norm": 4.7227349281311035,
      "learning_rate": 0.001963482111739641,
      "loss": 2.7605,
      "step": 134
    },
    {
      "epoch": 0.27,
      "grad_norm": 3.2076096534729004,
      "learning_rate": 0.0019629172873477994,
      "loss": 2.7632,
      "step": 135
    },
    {
      "epoch": 0.272,
      "grad_norm": 2.9289326667785645,
      "learning_rate": 0.0019623482108293818,
      "loss": 2.6788,
      "step": 136
    },
    {
      "epoch": 0.274,
      "grad_norm": 3.5195915699005127,
      "learning_rate": 0.001961774884697362,
      "loss": 2.5345,
      "step": 137
    },
    {
      "epoch": 0.276,
      "grad_norm": 2.2036643028259277,
      "learning_rate": 0.001961197311483479,
      "loss": 2.614,
      "step": 138
    },
    {
      "epoch": 0.278,
      "grad_norm": 2.5068509578704834,
      "learning_rate": 0.001960615493738226,
      "loss": 2.5876,
      "step": 139
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.586123943328857,
      "learning_rate": 0.0019600294340308398,
      "loss": 2.5494,
      "step": 140
    },
    {
      "epoch": 0.282,
      "grad_norm": 2.2469773292541504,
      "learning_rate": 0.0019594391349492903,
      "loss": 2.4626,
      "step": 141
    },
    {
      "epoch": 0.284,
      "grad_norm": 3.114971399307251,
      "learning_rate": 0.001958844599100266,
      "loss": 2.5529,
      "step": 142
    },
    {
      "epoch": 0.286,
      "grad_norm": 3.3518009185791016,
      "learning_rate": 0.0019582458291091663,
      "loss": 2.4463,
      "step": 143
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.3204469680786133,
      "learning_rate": 0.001957642827620087,
      "loss": 2.3298,
      "step": 144
    },
    {
      "epoch": 0.29,
      "grad_norm": 2.791255235671997,
      "learning_rate": 0.00195703559729581,
      "loss": 2.4031,
      "step": 145
    },
    {
      "epoch": 0.292,
      "grad_norm": 2.419426918029785,
      "learning_rate": 0.00195642414081779,
      "loss": 2.2956,
      "step": 146
    },
    {
      "epoch": 0.294,
      "grad_norm": 3.620445728302002,
      "learning_rate": 0.0019558084608861472,
      "loss": 2.3149,
      "step": 147
    },
    {
      "epoch": 0.296,
      "grad_norm": 3.8125832080841064,
      "learning_rate": 0.001955188560219648,
      "loss": 2.3881,
      "step": 148
    },
    {
      "epoch": 0.298,
      "grad_norm": 5.389157772064209,
      "learning_rate": 0.0019545644415557,
      "loss": 2.4915,
      "step": 149
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.9504411220550537,
      "learning_rate": 0.001953936107650336,
      "loss": 2.3557,
      "step": 150
    },
    {
      "epoch": 0.302,
      "grad_norm": 4.520712375640869,
      "learning_rate": 0.001953303561278202,
      "loss": 2.4204,
      "step": 151
    },
    {
      "epoch": 0.304,
      "grad_norm": 2.5971944332122803,
      "learning_rate": 0.0019526668052325467,
      "loss": 2.277,
      "step": 152
    },
    {
      "epoch": 0.306,
      "grad_norm": 4.897330284118652,
      "learning_rate": 0.001952025842325208,
      "loss": 2.3802,
      "step": 153
    },
    {
      "epoch": 0.308,
      "grad_norm": 7.155513763427734,
      "learning_rate": 0.0019513806753866014,
      "loss": 2.5903,
      "step": 154
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.4044182300567627,
      "learning_rate": 0.0019507313072657055,
      "loss": 2.3653,
      "step": 155
    },
    {
      "epoch": 0.312,
      "grad_norm": 2.7513935565948486,
      "learning_rate": 0.0019500777408300517,
      "loss": 2.5413,
      "step": 156
    },
    {
      "epoch": 0.314,
      "grad_norm": 3.6750152111053467,
      "learning_rate": 0.001949419978965711,
      "loss": 2.4324,
      "step": 157
    },
    {
      "epoch": 0.316,
      "grad_norm": 2.800503730773926,
      "learning_rate": 0.00194875802457728,
      "loss": 2.4592,
      "step": 158
    },
    {
      "epoch": 0.318,
      "grad_norm": 1.5488383769989014,
      "learning_rate": 0.0019480918805878697,
      "loss": 2.5536,
      "step": 159
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.892378807067871,
      "learning_rate": 0.001947421549939091,
      "loss": 2.3528,
      "step": 160
    },
    {
      "epoch": 0.322,
      "grad_norm": 2.5410561561584473,
      "learning_rate": 0.0019467470355910438,
      "loss": 2.5435,
      "step": 161
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.5102311372756958,
      "learning_rate": 0.0019460683405223018,
      "loss": 2.3692,
      "step": 162
    },
    {
      "epoch": 0.326,
      "grad_norm": 1.8860889673233032,
      "learning_rate": 0.001945385467729901,
      "loss": 2.4393,
      "step": 163
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.9577900171279907,
      "learning_rate": 0.0019446984202293246,
      "loss": 2.6922,
      "step": 164
    },
    {
      "epoch": 0.33,
      "grad_norm": 2.702770948410034,
      "learning_rate": 0.0019440072010544918,
      "loss": 2.4289,
      "step": 165
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.7343031167984009,
      "learning_rate": 0.001943311813257743,
      "loss": 2.4545,
      "step": 166
    },
    {
      "epoch": 0.334,
      "grad_norm": 2.591649055480957,
      "learning_rate": 0.001942612259909827,
      "loss": 2.3311,
      "step": 167
    },
    {
      "epoch": 0.336,
      "grad_norm": 4.664798736572266,
      "learning_rate": 0.0019419085440998871,
      "loss": 2.3579,
      "step": 168
    },
    {
      "epoch": 0.338,
      "grad_norm": 2.376110792160034,
      "learning_rate": 0.0019412006689354469,
      "loss": 2.3356,
      "step": 169
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.4358927011489868,
      "learning_rate": 0.0019404886375423982,
      "loss": 2.5352,
      "step": 170
    },
    {
      "epoch": 0.342,
      "grad_norm": 5.066070556640625,
      "learning_rate": 0.0019397724530649857,
      "loss": 2.286,
      "step": 171
    },
    {
      "epoch": 0.344,
      "grad_norm": 3.3609206676483154,
      "learning_rate": 0.0019390521186657935,
      "loss": 2.4734,
      "step": 172
    },
    {
      "epoch": 0.346,
      "grad_norm": 2.534966230392456,
      "learning_rate": 0.001938327637525731,
      "loss": 2.3758,
      "step": 173
    },
    {
      "epoch": 0.348,
      "grad_norm": 3.235281467437744,
      "learning_rate": 0.0019375990128440205,
      "loss": 2.2871,
      "step": 174
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.8142403960227966,
      "learning_rate": 0.0019368662478381799,
      "loss": 2.3591,
      "step": 175
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.0178661346435547,
      "learning_rate": 0.001936129345744011,
      "loss": 2.2127,
      "step": 176
    },
    {
      "epoch": 0.354,
      "grad_norm": 1.2104151248931885,
      "learning_rate": 0.0019353883098155854,
      "loss": 2.3315,
      "step": 177
    },
    {
      "epoch": 0.356,
      "grad_norm": 1.7122215032577515,
      "learning_rate": 0.0019346431433252273,
      "loss": 2.2473,
      "step": 178
    },
    {
      "epoch": 0.358,
      "grad_norm": 0.9045205116271973,
      "learning_rate": 0.001933893849563503,
      "loss": 2.2175,
      "step": 179
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.8639039993286133,
      "learning_rate": 0.0019331404318392025,
      "loss": 2.0355,
      "step": 180
    },
    {
      "epoch": 0.362,
      "grad_norm": 0.5442056655883789,
      "learning_rate": 0.0019323828934793284,
      "loss": 2.1411,
      "step": 181
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.9039509296417236,
      "learning_rate": 0.0019316212378290782,
      "loss": 2.1988,
      "step": 182
    },
    {
      "epoch": 0.366,
      "grad_norm": 1.0469768047332764,
      "learning_rate": 0.0019308554682518312,
      "loss": 2.2151,
      "step": 183
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.0894259214401245,
      "learning_rate": 0.001930085588129134,
      "loss": 2.2926,
      "step": 184
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.074076771736145,
      "learning_rate": 0.0019293116008606837,
      "loss": 2.2157,
      "step": 185
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.7083254456520081,
      "learning_rate": 0.0019285335098643153,
      "loss": 2.0883,
      "step": 186
    },
    {
      "epoch": 0.374,
      "grad_norm": 0.7717963457107544,
      "learning_rate": 0.0019277513185759845,
      "loss": 2.0789,
      "step": 187
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.0702821016311646,
      "learning_rate": 0.001926965030449754,
      "loss": 2.0811,
      "step": 188
    },
    {
      "epoch": 0.378,
      "grad_norm": 1.4277511835098267,
      "learning_rate": 0.0019261746489577765,
      "loss": 2.0163,
      "step": 189
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.046269178390503,
      "learning_rate": 0.001925380177590282,
      "loss": 2.0642,
      "step": 190
    },
    {
      "epoch": 0.382,
      "grad_norm": 2.1891238689422607,
      "learning_rate": 0.0019245816198555604,
      "loss": 2.1662,
      "step": 191
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.068668842315674,
      "learning_rate": 0.0019237789792799457,
      "loss": 2.0996,
      "step": 192
    },
    {
      "epoch": 0.386,
      "grad_norm": 2.1617050170898438,
      "learning_rate": 0.001922972259407802,
      "loss": 2.11,
      "step": 193
    },
    {
      "epoch": 0.388,
      "grad_norm": 3.4916043281555176,
      "learning_rate": 0.0019221614638015075,
      "loss": 2.0878,
      "step": 194
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.109224319458008,
      "learning_rate": 0.001921346596041437,
      "loss": 2.1832,
      "step": 195
    },
    {
      "epoch": 0.392,
      "grad_norm": 4.173354625701904,
      "learning_rate": 0.0019205276597259483,
      "loss": 2.2866,
      "step": 196
    },
    {
      "epoch": 0.394,
      "grad_norm": 1.5933623313903809,
      "learning_rate": 0.0019197046584713661,
      "loss": 2.153,
      "step": 197
    },
    {
      "epoch": 0.396,
      "grad_norm": 1.0274534225463867,
      "learning_rate": 0.0019188775959119641,
      "loss": 1.9966,
      "step": 198
    },
    {
      "epoch": 0.398,
      "grad_norm": 1.349075436592102,
      "learning_rate": 0.0019180464756999509,
      "loss": 1.9761,
      "step": 199
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.169824481010437,
      "learning_rate": 0.001917211301505453,
      "loss": 2.3021,
      "step": 200
    },
    {
      "epoch": 0.4,
      "eval_loss": 2.087155342102051,
      "eval_runtime": 228.7401,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 200
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.9232484102249146,
      "learning_rate": 0.001916372077016499,
      "loss": 2.004,
      "step": 201
    },
    {
      "epoch": 0.404,
      "grad_norm": 4.446470260620117,
      "learning_rate": 0.0019155288059390027,
      "loss": 2.0448,
      "step": 202
    },
    {
      "epoch": 0.406,
      "grad_norm": 2.6597437858581543,
      "learning_rate": 0.0019146814919967481,
      "loss": 2.0506,
      "step": 203
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.728100538253784,
      "learning_rate": 0.0019138301389313708,
      "loss": 2.1125,
      "step": 204
    },
    {
      "epoch": 0.41,
      "grad_norm": 1.0705533027648926,
      "learning_rate": 0.0019129747505023437,
      "loss": 1.9811,
      "step": 205
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.311711311340332,
      "learning_rate": 0.0019121153304869584,
      "loss": 1.8992,
      "step": 206
    },
    {
      "epoch": 0.414,
      "grad_norm": 1.3047494888305664,
      "learning_rate": 0.0019112518826803098,
      "loss": 1.8774,
      "step": 207
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.7055065631866455,
      "learning_rate": 0.00191038441089528,
      "loss": 2.0167,
      "step": 208
    },
    {
      "epoch": 0.418,
      "grad_norm": 5.122485160827637,
      "learning_rate": 0.0019095129189625193,
      "loss": 2.3276,
      "step": 209
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.042346954345703,
      "learning_rate": 0.0019086374107304311,
      "loss": 1.9327,
      "step": 210
    },
    {
      "epoch": 0.422,
      "grad_norm": 2.1639699935913086,
      "learning_rate": 0.0019077578900651541,
      "loss": 2.1737,
      "step": 211
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.4633746147155762,
      "learning_rate": 0.0019068743608505454,
      "loss": 2.1236,
      "step": 212
    },
    {
      "epoch": 0.426,
      "grad_norm": 1.1507340669631958,
      "learning_rate": 0.0019059868269881636,
      "loss": 1.9466,
      "step": 213
    },
    {
      "epoch": 0.428,
      "grad_norm": 22.211172103881836,
      "learning_rate": 0.0019050952923972508,
      "loss": 2.2025,
      "step": 214
    },
    {
      "epoch": 0.43,
      "grad_norm": 12.042900085449219,
      "learning_rate": 0.0019041997610147166,
      "loss": 3.1032,
      "step": 215
    },
    {
      "epoch": 0.432,
      "grad_norm": 6.174982070922852,
      "learning_rate": 0.0019033002367951192,
      "loss": 2.6387,
      "step": 216
    },
    {
      "epoch": 0.434,
      "grad_norm": 1.4321067333221436,
      "learning_rate": 0.0019023967237106491,
      "loss": 2.1359,
      "step": 217
    },
    {
      "epoch": 0.436,
      "grad_norm": 8.086593627929688,
      "learning_rate": 0.0019014892257511117,
      "loss": 2.6089,
      "step": 218
    },
    {
      "epoch": 0.438,
      "grad_norm": 6.714529514312744,
      "learning_rate": 0.0019005777469239076,
      "loss": 2.895,
      "step": 219
    },
    {
      "epoch": 0.44,
      "grad_norm": 4.731200695037842,
      "learning_rate": 0.001899662291254018,
      "loss": 2.8271,
      "step": 220
    },
    {
      "epoch": 0.442,
      "grad_norm": 13.793353080749512,
      "learning_rate": 0.0018987428627839842,
      "loss": 2.4418,
      "step": 221
    },
    {
      "epoch": 0.444,
      "grad_norm": 2.278334617614746,
      "learning_rate": 0.0018978194655738915,
      "loss": 2.0087,
      "step": 222
    },
    {
      "epoch": 0.446,
      "grad_norm": 5.2318806648254395,
      "learning_rate": 0.0018968921037013512,
      "loss": 2.0781,
      "step": 223
    },
    {
      "epoch": 0.448,
      "grad_norm": 9.879586219787598,
      "learning_rate": 0.0018959607812614806,
      "loss": 2.2921,
      "step": 224
    },
    {
      "epoch": 0.45,
      "grad_norm": 3.0180296897888184,
      "learning_rate": 0.0018950255023668877,
      "loss": 2.1912,
      "step": 225
    },
    {
      "epoch": 0.452,
      "grad_norm": 2.880418300628662,
      "learning_rate": 0.0018940862711476511,
      "loss": 2.2629,
      "step": 226
    },
    {
      "epoch": 0.454,
      "grad_norm": 1.419576644897461,
      "learning_rate": 0.0018931430917513029,
      "loss": 2.0631,
      "step": 227
    },
    {
      "epoch": 0.456,
      "grad_norm": 1.007103681564331,
      "learning_rate": 0.001892195968342809,
      "loss": 1.9057,
      "step": 228
    },
    {
      "epoch": 0.458,
      "grad_norm": 1.1325498819351196,
      "learning_rate": 0.0018912449051045525,
      "loss": 1.9321,
      "step": 229
    },
    {
      "epoch": 0.46,
      "grad_norm": 2.100541114807129,
      "learning_rate": 0.0018902899062363141,
      "loss": 2.0121,
      "step": 230
    },
    {
      "epoch": 0.462,
      "grad_norm": 1.1221510171890259,
      "learning_rate": 0.0018893309759552529,
      "loss": 1.938,
      "step": 231
    },
    {
      "epoch": 0.464,
      "grad_norm": 1.1299188137054443,
      "learning_rate": 0.0018883681184958898,
      "loss": 2.1734,
      "step": 232
    },
    {
      "epoch": 0.466,
      "grad_norm": 1.4051908254623413,
      "learning_rate": 0.0018874013381100874,
      "loss": 1.846,
      "step": 233
    },
    {
      "epoch": 0.468,
      "grad_norm": 1.5859426259994507,
      "learning_rate": 0.0018864306390670308,
      "loss": 1.8241,
      "step": 234
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.8983497023582458,
      "learning_rate": 0.0018854560256532098,
      "loss": 1.8387,
      "step": 235
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.0259063243865967,
      "learning_rate": 0.0018844775021724003,
      "loss": 1.9667,
      "step": 236
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.7260911464691162,
      "learning_rate": 0.0018834950729456432,
      "loss": 1.9189,
      "step": 237
    },
    {
      "epoch": 0.476,
      "grad_norm": 1.650926947593689,
      "learning_rate": 0.001882508742311228,
      "loss": 1.7305,
      "step": 238
    },
    {
      "epoch": 0.478,
      "grad_norm": 1.172369360923767,
      "learning_rate": 0.0018815185146246716,
      "loss": 1.8847,
      "step": 239
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.6931760907173157,
      "learning_rate": 0.0018805243942587,
      "loss": 1.8689,
      "step": 240
    },
    {
      "epoch": 0.482,
      "grad_norm": 1.2832351922988892,
      "learning_rate": 0.0018795263856032287,
      "loss": 1.9057,
      "step": 241
    },
    {
      "epoch": 0.484,
      "grad_norm": 26.47721290588379,
      "learning_rate": 0.0018785244930653439,
      "loss": 1.9442,
      "step": 242
    },
    {
      "epoch": 0.486,
      "grad_norm": 0.9876003861427307,
      "learning_rate": 0.0018775187210692814,
      "loss": 1.8215,
      "step": 243
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.9097820520401001,
      "learning_rate": 0.0018765090740564098,
      "loss": 1.855,
      "step": 244
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.9427711963653564,
      "learning_rate": 0.001875495556485208,
      "loss": 1.9431,
      "step": 245
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.7460276484489441,
      "learning_rate": 0.001874478172831248,
      "loss": 1.8116,
      "step": 246
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.9198756814002991,
      "learning_rate": 0.0018734569275871726,
      "loss": 1.8921,
      "step": 247
    },
    {
      "epoch": 0.496,
      "grad_norm": 1.4212478399276733,
      "learning_rate": 0.0018724318252626776,
      "loss": 2.0372,
      "step": 248
    },
    {
      "epoch": 0.498,
      "grad_norm": 0.5720387697219849,
      "learning_rate": 0.0018714028703844914,
      "loss": 1.6899,
      "step": 249
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.8207013010978699,
      "learning_rate": 0.0018703700674963545,
      "loss": 1.8318,
      "step": 250
    },
    {
      "epoch": 0.502,
      "grad_norm": 1.0384647846221924,
      "learning_rate": 0.0018693334211590006,
      "loss": 1.8637,
      "step": 251
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.8766223192214966,
      "learning_rate": 0.0018682929359501337,
      "loss": 1.7961,
      "step": 252
    },
    {
      "epoch": 0.506,
      "grad_norm": 1.039932131767273,
      "learning_rate": 0.0018672486164644116,
      "loss": 1.7555,
      "step": 253
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.5402951836585999,
      "learning_rate": 0.0018662004673134231,
      "loss": 1.757,
      "step": 254
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.9596356749534607,
      "learning_rate": 0.0018651484931256684,
      "loss": 1.6867,
      "step": 255
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.8282240629196167,
      "learning_rate": 0.0018640926985465387,
      "loss": 1.6481,
      "step": 256
    },
    {
      "epoch": 0.514,
      "grad_norm": 1.3981173038482666,
      "learning_rate": 0.0018630330882382952,
      "loss": 1.8429,
      "step": 257
    },
    {
      "epoch": 0.516,
      "grad_norm": 1.1158251762390137,
      "learning_rate": 0.0018619696668800492,
      "loss": 1.8297,
      "step": 258
    },
    {
      "epoch": 0.518,
      "grad_norm": 1.6066783666610718,
      "learning_rate": 0.0018609024391677417,
      "loss": 1.7256,
      "step": 259
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.7062008380889893,
      "learning_rate": 0.0018598314098141207,
      "loss": 1.7927,
      "step": 260
    },
    {
      "epoch": 0.522,
      "grad_norm": 2.072417736053467,
      "learning_rate": 0.0018587565835487233,
      "loss": 1.8325,
      "step": 261
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.968880295753479,
      "learning_rate": 0.0018576779651178522,
      "loss": 1.771,
      "step": 262
    },
    {
      "epoch": 0.526,
      "grad_norm": 1.589495062828064,
      "learning_rate": 0.0018565955592845563,
      "loss": 1.8607,
      "step": 263
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.3808385133743286,
      "learning_rate": 0.0018555093708286093,
      "loss": 1.814,
      "step": 264
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.6895244717597961,
      "learning_rate": 0.0018544194045464887,
      "loss": 1.7371,
      "step": 265
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.941152572631836,
      "learning_rate": 0.0018533256652513534,
      "loss": 2.0481,
      "step": 266
    },
    {
      "epoch": 0.534,
      "grad_norm": 1.556852102279663,
      "learning_rate": 0.001852228157773025,
      "loss": 1.7803,
      "step": 267
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.5502138137817383,
      "learning_rate": 0.0018511268869579635,
      "loss": 1.7977,
      "step": 268
    },
    {
      "epoch": 0.538,
      "grad_norm": 11.519356727600098,
      "learning_rate": 0.001850021857669248,
      "loss": 1.8868,
      "step": 269
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.5086493492126465,
      "learning_rate": 0.0018489130747865548,
      "loss": 2.0024,
      "step": 270
    },
    {
      "epoch": 0.542,
      "grad_norm": 2.329739809036255,
      "learning_rate": 0.0018478005432061352,
      "loss": 1.6887,
      "step": 271
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.3066250085830688,
      "learning_rate": 0.0018466842678407944,
      "loss": 1.8537,
      "step": 272
    },
    {
      "epoch": 0.546,
      "grad_norm": 2.0700197219848633,
      "learning_rate": 0.00184556425361987,
      "loss": 2.0206,
      "step": 273
    },
    {
      "epoch": 0.548,
      "grad_norm": 2.1918694972991943,
      "learning_rate": 0.0018444405054892092,
      "loss": 1.7804,
      "step": 274
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9389934539794922,
      "learning_rate": 0.001843313028411149,
      "loss": 1.7287,
      "step": 275
    },
    {
      "epoch": 0.552,
      "grad_norm": 2.0404369831085205,
      "learning_rate": 0.0018421818273644912,
      "loss": 1.7987,
      "step": 276
    },
    {
      "epoch": 0.554,
      "grad_norm": 1.5645990371704102,
      "learning_rate": 0.001841046907344484,
      "loss": 1.7326,
      "step": 277
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.0867469310760498,
      "learning_rate": 0.0018399082733627965,
      "loss": 1.8908,
      "step": 278
    },
    {
      "epoch": 0.558,
      "grad_norm": 1.9863090515136719,
      "learning_rate": 0.0018387659304474994,
      "loss": 1.8284,
      "step": 279
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.9959880113601685,
      "learning_rate": 0.0018376198836430415,
      "loss": 1.7495,
      "step": 280
    },
    {
      "epoch": 0.562,
      "grad_norm": 1.2685647010803223,
      "learning_rate": 0.0018364701380102267,
      "loss": 1.5932,
      "step": 281
    },
    {
      "epoch": 0.564,
      "grad_norm": 3.379605770111084,
      "learning_rate": 0.0018353166986261936,
      "loss": 1.8427,
      "step": 282
    },
    {
      "epoch": 0.566,
      "grad_norm": 1.1339600086212158,
      "learning_rate": 0.0018341595705843906,
      "loss": 1.7817,
      "step": 283
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.6808093786239624,
      "learning_rate": 0.001832998758994556,
      "loss": 1.7777,
      "step": 284
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.0846030712127686,
      "learning_rate": 0.0018318342689826936,
      "loss": 1.5967,
      "step": 285
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.857460081577301,
      "learning_rate": 0.001830666105691051,
      "loss": 1.7492,
      "step": 286
    },
    {
      "epoch": 0.574,
      "grad_norm": 18.6268253326416,
      "learning_rate": 0.0018294942742780964,
      "loss": 2.3627,
      "step": 287
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.662473201751709,
      "learning_rate": 0.0018283187799184957,
      "loss": 1.8249,
      "step": 288
    },
    {
      "epoch": 0.578,
      "grad_norm": 1.8290801048278809,
      "learning_rate": 0.0018271396278030903,
      "loss": 1.81,
      "step": 289
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.5882046222686768,
      "learning_rate": 0.0018259568231388736,
      "loss": 2.0656,
      "step": 290
    },
    {
      "epoch": 0.582,
      "grad_norm": 2.7158992290496826,
      "learning_rate": 0.0018247703711489684,
      "loss": 1.7376,
      "step": 291
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.7360192537307739,
      "learning_rate": 0.0018235802770726038,
      "loss": 1.6592,
      "step": 292
    },
    {
      "epoch": 0.586,
      "grad_norm": 0.9611104726791382,
      "learning_rate": 0.0018223865461650913,
      "loss": 1.6154,
      "step": 293
    },
    {
      "epoch": 0.588,
      "grad_norm": 1.0467870235443115,
      "learning_rate": 0.0018211891836978028,
      "loss": 1.7841,
      "step": 294
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.962512731552124,
      "learning_rate": 0.001819988194958146,
      "loss": 1.794,
      "step": 295
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.1482343673706055,
      "learning_rate": 0.0018187835852495429,
      "loss": 1.7645,
      "step": 296
    },
    {
      "epoch": 0.594,
      "grad_norm": 0.7849602699279785,
      "learning_rate": 0.0018175753598914047,
      "loss": 1.82,
      "step": 297
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.269761562347412,
      "learning_rate": 0.0018163635242191083,
      "loss": 1.7728,
      "step": 298
    },
    {
      "epoch": 0.598,
      "grad_norm": 0.8502991199493408,
      "learning_rate": 0.0018151480835839743,
      "loss": 1.6464,
      "step": 299
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5836257934570312,
      "learning_rate": 0.0018139290433532413,
      "loss": 1.652,
      "step": 300
    },
    {
      "epoch": 0.6,
      "eval_loss": 1.7823143005371094,
      "eval_runtime": 228.8832,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 300
    },
    {
      "epoch": 0.602,
      "grad_norm": 1.5231127738952637,
      "learning_rate": 0.0018127064089100446,
      "loss": 1.8211,
      "step": 301
    },
    {
      "epoch": 0.604,
      "grad_norm": 1.0323700904846191,
      "learning_rate": 0.00181148018565339,
      "loss": 1.7974,
      "step": 302
    },
    {
      "epoch": 0.606,
      "grad_norm": 1.9231871366500854,
      "learning_rate": 0.001810250378998132,
      "loss": 1.8587,
      "step": 303
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.168794870376587,
      "learning_rate": 0.0018090169943749475,
      "loss": 1.7838,
      "step": 304
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.2404541969299316,
      "learning_rate": 0.001807780037230315,
      "loss": 1.8,
      "step": 305
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.8690732717514038,
      "learning_rate": 0.0018065395130264874,
      "loss": 1.6905,
      "step": 306
    },
    {
      "epoch": 0.614,
      "grad_norm": 0.57569420337677,
      "learning_rate": 0.0018052954272414707,
      "loss": 1.7044,
      "step": 307
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.8931528925895691,
      "learning_rate": 0.0018040477853689969,
      "loss": 1.6956,
      "step": 308
    },
    {
      "epoch": 0.618,
      "grad_norm": 0.9971257448196411,
      "learning_rate": 0.0018027965929185024,
      "loss": 1.8003,
      "step": 309
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.4838508367538452,
      "learning_rate": 0.0018015418554151023,
      "loss": 1.5543,
      "step": 310
    },
    {
      "epoch": 0.622,
      "grad_norm": 1.077484130859375,
      "learning_rate": 0.0018002835783995652,
      "loss": 1.8775,
      "step": 311
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.4516801834106445,
      "learning_rate": 0.0017990217674282915,
      "loss": 1.8355,
      "step": 312
    },
    {
      "epoch": 0.626,
      "grad_norm": 1.5076583623886108,
      "learning_rate": 0.001797756428073286,
      "loss": 1.6261,
      "step": 313
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.9125683903694153,
      "learning_rate": 0.0017964875659221343,
      "loss": 1.7385,
      "step": 314
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.3437045812606812,
      "learning_rate": 0.0017952151865779792,
      "loss": 1.5429,
      "step": 315
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.8631296157836914,
      "learning_rate": 0.0017939392956594932,
      "loss": 1.6427,
      "step": 316
    },
    {
      "epoch": 0.634,
      "grad_norm": 1.231150507926941,
      "learning_rate": 0.0017926598988008582,
      "loss": 1.6562,
      "step": 317
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.7557599544525146,
      "learning_rate": 0.0017913770016517354,
      "loss": 1.6754,
      "step": 318
    },
    {
      "epoch": 0.638,
      "grad_norm": 1.099472165107727,
      "learning_rate": 0.0017900906098772444,
      "loss": 1.6691,
      "step": 319
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.3493553400039673,
      "learning_rate": 0.0017888007291579355,
      "loss": 1.6849,
      "step": 320
    },
    {
      "epoch": 0.642,
      "grad_norm": 0.7711889743804932,
      "learning_rate": 0.001787507365189767,
      "loss": 1.702,
      "step": 321
    },
    {
      "epoch": 0.644,
      "grad_norm": 1.1909146308898926,
      "learning_rate": 0.0017862105236840775,
      "loss": 1.6445,
      "step": 322
    },
    {
      "epoch": 0.646,
      "grad_norm": 1.202271819114685,
      "learning_rate": 0.001784910210367563,
      "loss": 1.7829,
      "step": 323
    },
    {
      "epoch": 0.648,
      "grad_norm": 1.0791819095611572,
      "learning_rate": 0.0017836064309822502,
      "loss": 1.5777,
      "step": 324
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.8754224181175232,
      "learning_rate": 0.0017822991912854714,
      "loss": 1.6147,
      "step": 325
    },
    {
      "epoch": 0.652,
      "grad_norm": 1.19380521774292,
      "learning_rate": 0.0017809884970498395,
      "loss": 1.7346,
      "step": 326
    },
    {
      "epoch": 0.654,
      "grad_norm": 1.2211077213287354,
      "learning_rate": 0.0017796743540632223,
      "loss": 1.6115,
      "step": 327
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.0480109453201294,
      "learning_rate": 0.0017783567681287167,
      "loss": 1.6735,
      "step": 328
    },
    {
      "epoch": 0.658,
      "grad_norm": 0.9083231091499329,
      "learning_rate": 0.001777035745064623,
      "loss": 1.6116,
      "step": 329
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.7638611793518066,
      "learning_rate": 0.0017757112907044199,
      "loss": 1.6513,
      "step": 330
    },
    {
      "epoch": 0.662,
      "grad_norm": 1.1647852659225464,
      "learning_rate": 0.001774383410896738,
      "loss": 1.6887,
      "step": 331
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.099047064781189,
      "learning_rate": 0.001773052111505334,
      "loss": 1.7065,
      "step": 332
    },
    {
      "epoch": 0.666,
      "grad_norm": 1.370642900466919,
      "learning_rate": 0.0017717173984090658,
      "loss": 1.7755,
      "step": 333
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.6246787905693054,
      "learning_rate": 0.0017703792775018655,
      "loss": 1.577,
      "step": 334
    },
    {
      "epoch": 0.67,
      "grad_norm": 1.0504446029663086,
      "learning_rate": 0.0017690377546927133,
      "loss": 1.6766,
      "step": 335
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.7460998892784119,
      "learning_rate": 0.001767692835905612,
      "loss": 1.4748,
      "step": 336
    },
    {
      "epoch": 0.674,
      "grad_norm": 1.4882992506027222,
      "learning_rate": 0.001766344527079561,
      "loss": 1.695,
      "step": 337
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.9611985683441162,
      "learning_rate": 0.0017649928341685298,
      "loss": 1.7733,
      "step": 338
    },
    {
      "epoch": 0.678,
      "grad_norm": 1.264172077178955,
      "learning_rate": 0.0017636377631414302,
      "loss": 1.6448,
      "step": 339
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.1381560564041138,
      "learning_rate": 0.0017622793199820932,
      "loss": 1.6442,
      "step": 340
    },
    {
      "epoch": 0.682,
      "grad_norm": 0.9429733753204346,
      "learning_rate": 0.0017609175106892395,
      "loss": 1.597,
      "step": 341
    },
    {
      "epoch": 0.684,
      "grad_norm": 1.1724679470062256,
      "learning_rate": 0.001759552341276455,
      "loss": 1.6193,
      "step": 342
    },
    {
      "epoch": 0.686,
      "grad_norm": 0.9145501255989075,
      "learning_rate": 0.0017581838177721627,
      "loss": 1.6621,
      "step": 343
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.8912887573242188,
      "learning_rate": 0.0017568119462195977,
      "loss": 1.6034,
      "step": 344
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.8403961658477783,
      "learning_rate": 0.0017554367326767792,
      "loss": 1.5582,
      "step": 345
    },
    {
      "epoch": 0.692,
      "grad_norm": 5.842499256134033,
      "learning_rate": 0.0017540581832164838,
      "loss": 1.7362,
      "step": 346
    },
    {
      "epoch": 0.694,
      "grad_norm": 2.252243757247925,
      "learning_rate": 0.0017526763039262207,
      "loss": 1.6522,
      "step": 347
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.9733878970146179,
      "learning_rate": 0.0017512911009082012,
      "loss": 1.6934,
      "step": 348
    },
    {
      "epoch": 0.698,
      "grad_norm": 1.1161422729492188,
      "learning_rate": 0.0017499025802793146,
      "loss": 1.739,
      "step": 349
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.8673473596572876,
      "learning_rate": 0.001748510748171101,
      "loss": 1.5813,
      "step": 350
    },
    {
      "epoch": 0.702,
      "grad_norm": 0.8545043468475342,
      "learning_rate": 0.0017471156107297233,
      "loss": 1.6128,
      "step": 351
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.7536791563034058,
      "learning_rate": 0.0017457171741159395,
      "loss": 1.7518,
      "step": 352
    },
    {
      "epoch": 0.706,
      "grad_norm": 0.9054182171821594,
      "learning_rate": 0.0017443154445050772,
      "loss": 1.6668,
      "step": 353
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.9451432228088379,
      "learning_rate": 0.0017429104280870056,
      "loss": 1.5543,
      "step": 354
    },
    {
      "epoch": 0.71,
      "grad_norm": 4.472679138183594,
      "learning_rate": 0.001741502131066107,
      "loss": 1.7556,
      "step": 355
    },
    {
      "epoch": 0.712,
      "grad_norm": 1.3667792081832886,
      "learning_rate": 0.001740090559661252,
      "loss": 1.7082,
      "step": 356
    },
    {
      "epoch": 0.714,
      "grad_norm": 0.6175951957702637,
      "learning_rate": 0.001738675720105769,
      "loss": 1.593,
      "step": 357
    },
    {
      "epoch": 0.716,
      "grad_norm": 29.68411636352539,
      "learning_rate": 0.001737257618647419,
      "loss": 1.6585,
      "step": 358
    },
    {
      "epoch": 0.718,
      "grad_norm": 4.170158863067627,
      "learning_rate": 0.0017358362615483669,
      "loss": 1.838,
      "step": 359
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.307258129119873,
      "learning_rate": 0.0017344116550851542,
      "loss": 1.6608,
      "step": 360
    },
    {
      "epoch": 0.722,
      "grad_norm": 3.2212538719177246,
      "learning_rate": 0.0017329838055486716,
      "loss": 1.7082,
      "step": 361
    },
    {
      "epoch": 0.724,
      "grad_norm": 1.291123867034912,
      "learning_rate": 0.0017315527192441297,
      "loss": 1.5484,
      "step": 362
    },
    {
      "epoch": 0.726,
      "grad_norm": 0.9879775643348694,
      "learning_rate": 0.0017301184024910332,
      "loss": 1.6113,
      "step": 363
    },
    {
      "epoch": 0.728,
      "grad_norm": 2.1589083671569824,
      "learning_rate": 0.0017286808616231522,
      "loss": 1.5939,
      "step": 364
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.538110613822937,
      "learning_rate": 0.0017272401029884933,
      "loss": 1.6761,
      "step": 365
    },
    {
      "epoch": 0.732,
      "grad_norm": 1.4945731163024902,
      "learning_rate": 0.0017257961329492728,
      "loss": 1.5987,
      "step": 366
    },
    {
      "epoch": 0.734,
      "grad_norm": 1.0853922367095947,
      "learning_rate": 0.001724348957881889,
      "loss": 1.6008,
      "step": 367
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.185198426246643,
      "learning_rate": 0.0017228985841768914,
      "loss": 1.6473,
      "step": 368
    },
    {
      "epoch": 0.738,
      "grad_norm": 2.011244535446167,
      "learning_rate": 0.0017214450182389558,
      "loss": 1.702,
      "step": 369
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.352113127708435,
      "learning_rate": 0.0017199882664868538,
      "loss": 1.5394,
      "step": 370
    },
    {
      "epoch": 0.742,
      "grad_norm": 1.401039719581604,
      "learning_rate": 0.0017185283353534258,
      "loss": 1.6784,
      "step": 371
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.957292377948761,
      "learning_rate": 0.0017170652312855513,
      "loss": 1.5743,
      "step": 372
    },
    {
      "epoch": 0.746,
      "grad_norm": 1.1197599172592163,
      "learning_rate": 0.0017155989607441212,
      "loss": 1.6577,
      "step": 373
    },
    {
      "epoch": 0.748,
      "grad_norm": 1.1531860828399658,
      "learning_rate": 0.0017141295302040094,
      "loss": 1.5859,
      "step": 374
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.8067148327827454,
      "learning_rate": 0.0017126569461540441,
      "loss": 1.7011,
      "step": 375
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.7454786896705627,
      "learning_rate": 0.0017111812150969788,
      "loss": 1.6242,
      "step": 376
    },
    {
      "epoch": 0.754,
      "grad_norm": 0.8122573494911194,
      "learning_rate": 0.0017097023435494636,
      "loss": 1.65,
      "step": 377
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.8289828300476074,
      "learning_rate": 0.001708220338042017,
      "loss": 1.4458,
      "step": 378
    },
    {
      "epoch": 0.758,
      "grad_norm": 1.8177695274353027,
      "learning_rate": 0.0017067352051189967,
      "loss": 1.5271,
      "step": 379
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9217049479484558,
      "learning_rate": 0.0017052469513385699,
      "loss": 1.5631,
      "step": 380
    },
    {
      "epoch": 0.762,
      "grad_norm": 0.8236808180809021,
      "learning_rate": 0.0017037555832726864,
      "loss": 1.5464,
      "step": 381
    },
    {
      "epoch": 0.764,
      "grad_norm": 2.1468148231506348,
      "learning_rate": 0.0017022611075070474,
      "loss": 1.6201,
      "step": 382
    },
    {
      "epoch": 0.766,
      "grad_norm": 1.1715935468673706,
      "learning_rate": 0.0017007635306410774,
      "loss": 1.6249,
      "step": 383
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.9796546101570129,
      "learning_rate": 0.0016992628592878956,
      "loss": 1.4883,
      "step": 384
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.1490569114685059,
      "learning_rate": 0.0016977591000742853,
      "loss": 1.6485,
      "step": 385
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.8662044405937195,
      "learning_rate": 0.0016962522596406663,
      "loss": 1.4815,
      "step": 386
    },
    {
      "epoch": 0.774,
      "grad_norm": 1.4506090879440308,
      "learning_rate": 0.0016947423446410635,
      "loss": 1.6829,
      "step": 387
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.8883365392684937,
      "learning_rate": 0.0016932293617430796,
      "loss": 1.5468,
      "step": 388
    },
    {
      "epoch": 0.778,
      "grad_norm": 1.3998026847839355,
      "learning_rate": 0.0016917133176278648,
      "loss": 1.5558,
      "step": 389
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.281177282333374,
      "learning_rate": 0.0016901942189900866,
      "loss": 1.5969,
      "step": 390
    },
    {
      "epoch": 0.782,
      "grad_norm": 1.1527841091156006,
      "learning_rate": 0.001688672072537902,
      "loss": 1.5642,
      "step": 391
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.9075772166252136,
      "learning_rate": 0.0016871468849929253,
      "loss": 1.5742,
      "step": 392
    },
    {
      "epoch": 0.786,
      "grad_norm": 1.0418435335159302,
      "learning_rate": 0.0016856186630902013,
      "loss": 1.4203,
      "step": 393
    },
    {
      "epoch": 0.788,
      "grad_norm": 1.427114725112915,
      "learning_rate": 0.001684087413578173,
      "loss": 1.5263,
      "step": 394
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.8930515050888062,
      "learning_rate": 0.0016825531432186542,
      "loss": 1.5345,
      "step": 395
    },
    {
      "epoch": 0.792,
      "grad_norm": 2.4928700923919678,
      "learning_rate": 0.0016810158587867972,
      "loss": 1.438,
      "step": 396
    },
    {
      "epoch": 0.794,
      "grad_norm": 1.422499656677246,
      "learning_rate": 0.001679475567071065,
      "loss": 1.7737,
      "step": 397
    },
    {
      "epoch": 0.796,
      "grad_norm": 1.565308690071106,
      "learning_rate": 0.0016779322748731995,
      "loss": 1.608,
      "step": 398
    },
    {
      "epoch": 0.798,
      "grad_norm": 3.0083491802215576,
      "learning_rate": 0.001676385989008193,
      "loss": 1.5348,
      "step": 399
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.825927257537842,
      "learning_rate": 0.0016748367163042577,
      "loss": 1.894,
      "step": 400
    },
    {
      "epoch": 0.8,
      "eval_loss": 2.663140058517456,
      "eval_runtime": 228.5861,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 400
    },
    {
      "epoch": 0.802,
      "grad_norm": 49.084259033203125,
      "learning_rate": 0.0016732844636027945,
      "loss": 2.8966,
      "step": 401
    },
    {
      "epoch": 0.804,
      "grad_norm": 7.208386421203613,
      "learning_rate": 0.0016717292377583647,
      "loss": 2.3815,
      "step": 402
    },
    {
      "epoch": 0.806,
      "grad_norm": 11.825223922729492,
      "learning_rate": 0.0016701710456386572,
      "loss": 2.3087,
      "step": 403
    },
    {
      "epoch": 0.808,
      "grad_norm": 18.69350814819336,
      "learning_rate": 0.0016686098941244613,
      "loss": 2.1081,
      "step": 404
    },
    {
      "epoch": 0.81,
      "grad_norm": 4.945819854736328,
      "learning_rate": 0.0016670457901096327,
      "loss": 2.0841,
      "step": 405
    },
    {
      "epoch": 0.812,
      "grad_norm": 2.2382442951202393,
      "learning_rate": 0.001665478740501067,
      "loss": 1.9495,
      "step": 406
    },
    {
      "epoch": 0.814,
      "grad_norm": 3.30540132522583,
      "learning_rate": 0.0016639087522186658,
      "loss": 2.0836,
      "step": 407
    },
    {
      "epoch": 0.816,
      "grad_norm": 2.6894960403442383,
      "learning_rate": 0.0016623358321953077,
      "loss": 1.7824,
      "step": 408
    },
    {
      "epoch": 0.818,
      "grad_norm": 1.501424789428711,
      "learning_rate": 0.0016607599873768183,
      "loss": 1.7852,
      "step": 409
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.4573293924331665,
      "learning_rate": 0.0016591812247219377,
      "loss": 1.7624,
      "step": 410
    },
    {
      "epoch": 0.822,
      "grad_norm": 1.5187232494354248,
      "learning_rate": 0.0016575995512022922,
      "loss": 1.7677,
      "step": 411
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.3021838665008545,
      "learning_rate": 0.00165601497380236,
      "loss": 1.5918,
      "step": 412
    },
    {
      "epoch": 0.826,
      "grad_norm": 1.3647332191467285,
      "learning_rate": 0.0016544274995194447,
      "loss": 1.728,
      "step": 413
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.6052151918411255,
      "learning_rate": 0.0016528371353636408,
      "loss": 1.6632,
      "step": 414
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.3614898920059204,
      "learning_rate": 0.0016512438883578046,
      "loss": 1.7832,
      "step": 415
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.2418807744979858,
      "learning_rate": 0.0016496477655375227,
      "loss": 1.7268,
      "step": 416
    },
    {
      "epoch": 0.834,
      "grad_norm": 1.1295928955078125,
      "learning_rate": 0.0016480487739510808,
      "loss": 1.7145,
      "step": 417
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.997776448726654,
      "learning_rate": 0.0016464469206594332,
      "loss": 1.5929,
      "step": 418
    },
    {
      "epoch": 0.838,
      "grad_norm": 2.637593984603882,
      "learning_rate": 0.0016448422127361706,
      "loss": 1.5882,
      "step": 419
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.6778512001037598,
      "learning_rate": 0.0016432346572674897,
      "loss": 1.5691,
      "step": 420
    },
    {
      "epoch": 0.842,
      "grad_norm": 1.4109376668930054,
      "learning_rate": 0.001641624261352161,
      "loss": 1.7181,
      "step": 421
    },
    {
      "epoch": 0.844,
      "grad_norm": 2.1076979637145996,
      "learning_rate": 0.0016400110321014992,
      "loss": 1.5405,
      "step": 422
    },
    {
      "epoch": 0.846,
      "grad_norm": 1.2131354808807373,
      "learning_rate": 0.00163839497663933,
      "loss": 1.6033,
      "step": 423
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.8180492520332336,
      "learning_rate": 0.001636776102101959,
      "loss": 1.6174,
      "step": 424
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.8554771542549133,
      "learning_rate": 0.0016351544156381413,
      "loss": 1.6058,
      "step": 425
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.9781724214553833,
      "learning_rate": 0.0016335299244090479,
      "loss": 1.6724,
      "step": 426
    },
    {
      "epoch": 0.854,
      "grad_norm": 0.9397765398025513,
      "learning_rate": 0.001631902635588237,
      "loss": 1.667,
      "step": 427
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.7167592644691467,
      "learning_rate": 0.0016302725563616192,
      "loss": 1.7178,
      "step": 428
    },
    {
      "epoch": 0.858,
      "grad_norm": 1.1971646547317505,
      "learning_rate": 0.001628639693927428,
      "loss": 1.5612,
      "step": 429
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.537182331085205,
      "learning_rate": 0.0016270040554961867,
      "loss": 1.6573,
      "step": 430
    },
    {
      "epoch": 0.862,
      "grad_norm": 1.295730710029602,
      "learning_rate": 0.0016253656482906776,
      "loss": 1.8103,
      "step": 431
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.6355118155479431,
      "learning_rate": 0.0016237244795459084,
      "loss": 1.4355,
      "step": 432
    },
    {
      "epoch": 0.866,
      "grad_norm": 0.9493564367294312,
      "learning_rate": 0.0016220805565090837,
      "loss": 1.7143,
      "step": 433
    },
    {
      "epoch": 0.868,
      "grad_norm": 1.1721752882003784,
      "learning_rate": 0.0016204338864395681,
      "loss": 1.6398,
      "step": 434
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.8405128717422485,
      "learning_rate": 0.0016187844766088583,
      "loss": 1.4582,
      "step": 435
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.164881467819214,
      "learning_rate": 0.0016171323343005498,
      "loss": 1.7084,
      "step": 436
    },
    {
      "epoch": 0.874,
      "grad_norm": 2.0078232288360596,
      "learning_rate": 0.0016154774668103028,
      "loss": 1.668,
      "step": 437
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.6471284627914429,
      "learning_rate": 0.0016138198814458127,
      "loss": 1.5394,
      "step": 438
    },
    {
      "epoch": 0.878,
      "grad_norm": 257.2276306152344,
      "learning_rate": 0.0016121595855267765,
      "loss": 1.6876,
      "step": 439
    },
    {
      "epoch": 0.88,
      "grad_norm": 4.955711364746094,
      "learning_rate": 0.0016104965863848616,
      "loss": 1.8844,
      "step": 440
    },
    {
      "epoch": 0.882,
      "grad_norm": 2.479661226272583,
      "learning_rate": 0.0016088308913636703,
      "loss": 1.8095,
      "step": 441
    },
    {
      "epoch": 0.884,
      "grad_norm": 3.6681101322174072,
      "learning_rate": 0.0016071625078187112,
      "loss": 1.8407,
      "step": 442
    },
    {
      "epoch": 0.886,
      "grad_norm": 4.782470226287842,
      "learning_rate": 0.0016054914431173652,
      "loss": 1.8935,
      "step": 443
    },
    {
      "epoch": 0.888,
      "grad_norm": 30.53533172607422,
      "learning_rate": 0.0016038177046388523,
      "loss": 1.7284,
      "step": 444
    },
    {
      "epoch": 0.89,
      "grad_norm": 17.501195907592773,
      "learning_rate": 0.0016021412997741992,
      "loss": 1.7687,
      "step": 445
    },
    {
      "epoch": 0.892,
      "grad_norm": 3.643260955810547,
      "learning_rate": 0.0016004622359262085,
      "loss": 1.756,
      "step": 446
    },
    {
      "epoch": 0.894,
      "grad_norm": 8.269022941589355,
      "learning_rate": 0.0015987805205094226,
      "loss": 1.8604,
      "step": 447
    },
    {
      "epoch": 0.896,
      "grad_norm": 3.765413761138916,
      "learning_rate": 0.0015970961609500945,
      "loss": 1.7478,
      "step": 448
    },
    {
      "epoch": 0.898,
      "grad_norm": 1.3105733394622803,
      "learning_rate": 0.0015954091646861524,
      "loss": 1.6538,
      "step": 449
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.932565450668335,
      "learning_rate": 0.0015937195391671688,
      "loss": 1.6795,
      "step": 450
    },
    {
      "epoch": 0.902,
      "grad_norm": 2.3548834323883057,
      "learning_rate": 0.0015920272918543256,
      "loss": 1.7642,
      "step": 451
    },
    {
      "epoch": 0.904,
      "grad_norm": 1.330745816230774,
      "learning_rate": 0.0015903324302203835,
      "loss": 1.4647,
      "step": 452
    },
    {
      "epoch": 0.906,
      "grad_norm": 1.658000111579895,
      "learning_rate": 0.001588634961749646,
      "loss": 1.7556,
      "step": 453
    },
    {
      "epoch": 0.908,
      "grad_norm": 3.387324810028076,
      "learning_rate": 0.0015869348939379303,
      "loss": 1.6538,
      "step": 454
    },
    {
      "epoch": 0.91,
      "grad_norm": 152.97799682617188,
      "learning_rate": 0.0015852322342925294,
      "loss": 1.7838,
      "step": 455
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.4772089719772339,
      "learning_rate": 0.0015835269903321841,
      "loss": 1.6393,
      "step": 456
    },
    {
      "epoch": 0.914,
      "grad_norm": 1.7114837169647217,
      "learning_rate": 0.0015818191695870453,
      "loss": 1.7409,
      "step": 457
    },
    {
      "epoch": 0.916,
      "grad_norm": 1.6915321350097656,
      "learning_rate": 0.0015801087795986437,
      "loss": 1.7024,
      "step": 458
    },
    {
      "epoch": 0.918,
      "grad_norm": 2.0051369667053223,
      "learning_rate": 0.0015783958279198549,
      "loss": 1.7917,
      "step": 459
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.8461072444915771,
      "learning_rate": 0.0015766803221148673,
      "loss": 1.6529,
      "step": 460
    },
    {
      "epoch": 0.922,
      "grad_norm": 1.5127614736557007,
      "learning_rate": 0.001574962269759147,
      "loss": 1.6524,
      "step": 461
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.6698948740959167,
      "learning_rate": 0.0015732416784394064,
      "loss": 1.7163,
      "step": 462
    },
    {
      "epoch": 0.926,
      "grad_norm": 1.6204802989959717,
      "learning_rate": 0.0015715185557535689,
      "loss": 1.5534,
      "step": 463
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.6415321826934814,
      "learning_rate": 0.0015697929093107365,
      "loss": 1.6848,
      "step": 464
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.0659085512161255,
      "learning_rate": 0.0015680647467311557,
      "loss": 1.5771,
      "step": 465
    },
    {
      "epoch": 0.932,
      "grad_norm": 1.336102843284607,
      "learning_rate": 0.0015663340756461844,
      "loss": 1.566,
      "step": 466
    },
    {
      "epoch": 0.934,
      "grad_norm": 1.5215941667556763,
      "learning_rate": 0.0015646009036982566,
      "loss": 1.8299,
      "step": 467
    },
    {
      "epoch": 0.936,
      "grad_norm": 2.7674427032470703,
      "learning_rate": 0.0015628652385408508,
      "loss": 1.6396,
      "step": 468
    },
    {
      "epoch": 0.938,
      "grad_norm": 1.0575882196426392,
      "learning_rate": 0.001561127087838455,
      "loss": 1.4491,
      "step": 469
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.733551263809204,
      "learning_rate": 0.0015593864592665333,
      "loss": 1.8176,
      "step": 470
    },
    {
      "epoch": 0.942,
      "grad_norm": 1.4474126100540161,
      "learning_rate": 0.001557643360511491,
      "loss": 1.615,
      "step": 471
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.8422797322273254,
      "learning_rate": 0.0015558977992706424,
      "loss": 1.5498,
      "step": 472
    },
    {
      "epoch": 0.946,
      "grad_norm": 1.0337456464767456,
      "learning_rate": 0.001554149783252175,
      "loss": 1.6324,
      "step": 473
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.8075729608535767,
      "learning_rate": 0.0015523993201751166,
      "loss": 1.5321,
      "step": 474
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.2668739557266235,
      "learning_rate": 0.0015506464177693008,
      "loss": 1.7793,
      "step": 475
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.7997688055038452,
      "learning_rate": 0.001548891083775334,
      "loss": 1.7718,
      "step": 476
    },
    {
      "epoch": 0.954,
      "grad_norm": 1.2308053970336914,
      "learning_rate": 0.001547133325944559,
      "loss": 1.6295,
      "step": 477
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.8287737965583801,
      "learning_rate": 0.0015453731520390214,
      "loss": 1.582,
      "step": 478
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.7936168313026428,
      "learning_rate": 0.0015436105698314383,
      "loss": 1.6437,
      "step": 479
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.871144413948059,
      "learning_rate": 0.001541845587105159,
      "loss": 1.7393,
      "step": 480
    },
    {
      "epoch": 0.962,
      "grad_norm": 1.0065170526504517,
      "learning_rate": 0.001540078211654135,
      "loss": 1.5133,
      "step": 481
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.7463515996932983,
      "learning_rate": 0.0015383084512828825,
      "loss": 1.5742,
      "step": 482
    },
    {
      "epoch": 0.966,
      "grad_norm": 1.4624966382980347,
      "learning_rate": 0.0015365363138064498,
      "loss": 1.5693,
      "step": 483
    },
    {
      "epoch": 0.968,
      "grad_norm": 2.0347917079925537,
      "learning_rate": 0.0015347618070503826,
      "loss": 1.638,
      "step": 484
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.8479105234146118,
      "learning_rate": 0.0015329849388506886,
      "loss": 1.5787,
      "step": 485
    },
    {
      "epoch": 0.972,
      "grad_norm": 1.2716195583343506,
      "learning_rate": 0.0015312057170538034,
      "loss": 1.4339,
      "step": 486
    },
    {
      "epoch": 0.974,
      "grad_norm": 2.105614185333252,
      "learning_rate": 0.0015294241495165558,
      "loss": 1.6836,
      "step": 487
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.3805021047592163,
      "learning_rate": 0.0015276402441061327,
      "loss": 1.6813,
      "step": 488
    },
    {
      "epoch": 0.978,
      "grad_norm": 1.1508023738861084,
      "learning_rate": 0.0015258540087000458,
      "loss": 1.5458,
      "step": 489
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.9013211727142334,
      "learning_rate": 0.001524065451186095,
      "loss": 1.3457,
      "step": 490
    },
    {
      "epoch": 0.982,
      "grad_norm": 0.956143319606781,
      "learning_rate": 0.0015222745794623341,
      "loss": 1.601,
      "step": 491
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.786769688129425,
      "learning_rate": 0.0015204814014370372,
      "loss": 1.5378,
      "step": 492
    },
    {
      "epoch": 0.986,
      "grad_norm": 0.7278709411621094,
      "learning_rate": 0.0015186859250286616,
      "loss": 1.4091,
      "step": 493
    },
    {
      "epoch": 0.988,
      "grad_norm": 1.3844939470291138,
      "learning_rate": 0.0015168881581658147,
      "loss": 1.6194,
      "step": 494
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.487123727798462,
      "learning_rate": 0.0015150881087872183,
      "loss": 1.6873,
      "step": 495
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.6781864762306213,
      "learning_rate": 0.0015132857848416733,
      "loss": 1.5885,
      "step": 496
    },
    {
      "epoch": 0.994,
      "grad_norm": 1.135419487953186,
      "learning_rate": 0.0015114811942880243,
      "loss": 1.594,
      "step": 497
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.0823935270309448,
      "learning_rate": 0.0015096743450951258,
      "loss": 1.614,
      "step": 498
    },
    {
      "epoch": 0.998,
      "grad_norm": 0.9972948431968689,
      "learning_rate": 0.0015078652452418062,
      "loss": 1.4762,
      "step": 499
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7504764795303345,
      "learning_rate": 0.0015060539027168317,
      "loss": 1.4587,
      "step": 500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.5547724962234497,
      "eval_runtime": 228.9372,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 500
    },
    {
      "epoch": 1.002,
      "grad_norm": 1.1780281066894531,
      "learning_rate": 0.0015042403255188723,
      "loss": 1.4629,
      "step": 501
    },
    {
      "epoch": 1.004,
      "grad_norm": 1.1023913621902466,
      "learning_rate": 0.0015024245216564668,
      "loss": 1.5572,
      "step": 502
    },
    {
      "epoch": 1.006,
      "grad_norm": 1.5896475315093994,
      "learning_rate": 0.0015006064991479853,
      "loss": 1.6457,
      "step": 503
    },
    {
      "epoch": 1.008,
      "grad_norm": 1.242985486984253,
      "learning_rate": 0.0014987862660215965,
      "loss": 1.6242,
      "step": 504
    },
    {
      "epoch": 1.01,
      "grad_norm": 1.5107595920562744,
      "learning_rate": 0.0014969638303152296,
      "loss": 1.4774,
      "step": 505
    },
    {
      "epoch": 1.012,
      "grad_norm": 1.1105130910873413,
      "learning_rate": 0.0014951392000765412,
      "loss": 1.4446,
      "step": 506
    },
    {
      "epoch": 1.014,
      "grad_norm": 1.0065275430679321,
      "learning_rate": 0.0014933123833628787,
      "loss": 1.5513,
      "step": 507
    },
    {
      "epoch": 1.016,
      "grad_norm": 1.1883883476257324,
      "learning_rate": 0.0014914833882412432,
      "loss": 1.4275,
      "step": 508
    },
    {
      "epoch": 1.018,
      "grad_norm": 1.639533281326294,
      "learning_rate": 0.0014896522227882578,
      "loss": 1.639,
      "step": 509
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.9548944234848022,
      "learning_rate": 0.0014878188950901274,
      "loss": 1.5666,
      "step": 510
    },
    {
      "epoch": 1.022,
      "grad_norm": 1.0928623676300049,
      "learning_rate": 0.001485983413242606,
      "loss": 1.5923,
      "step": 511
    },
    {
      "epoch": 1.024,
      "grad_norm": 1.169904112815857,
      "learning_rate": 0.0014841457853509606,
      "loss": 1.4162,
      "step": 512
    },
    {
      "epoch": 1.026,
      "grad_norm": 0.9876121878623962,
      "learning_rate": 0.0014823060195299335,
      "loss": 1.6376,
      "step": 513
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.9296040534973145,
      "learning_rate": 0.0014804641239037095,
      "loss": 1.4292,
      "step": 514
    },
    {
      "epoch": 1.03,
      "grad_norm": 1.4444328546524048,
      "learning_rate": 0.0014786201066058766,
      "loss": 1.6087,
      "step": 515
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.707074761390686,
      "learning_rate": 0.001476773975779393,
      "loss": 1.696,
      "step": 516
    },
    {
      "epoch": 1.034,
      "grad_norm": 1.3413773775100708,
      "learning_rate": 0.0014749257395765502,
      "loss": 1.5488,
      "step": 517
    },
    {
      "epoch": 1.036,
      "grad_norm": 1.2088373899459839,
      "learning_rate": 0.0014730754061589356,
      "loss": 1.4281,
      "step": 518
    },
    {
      "epoch": 1.038,
      "grad_norm": 1.576151728630066,
      "learning_rate": 0.0014712229836973986,
      "loss": 1.5066,
      "step": 519
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.3051886558532715,
      "learning_rate": 0.0014693684803720138,
      "loss": 1.5675,
      "step": 520
    },
    {
      "epoch": 1.042,
      "grad_norm": 10.94564151763916,
      "learning_rate": 0.0014675119043720435,
      "loss": 1.6284,
      "step": 521
    },
    {
      "epoch": 1.044,
      "grad_norm": 10.788118362426758,
      "learning_rate": 0.0014656532638959035,
      "loss": 1.6135,
      "step": 522
    },
    {
      "epoch": 1.046,
      "grad_norm": 2.6469173431396484,
      "learning_rate": 0.001463792567151126,
      "loss": 1.6918,
      "step": 523
    },
    {
      "epoch": 1.048,
      "grad_norm": 35.90705490112305,
      "learning_rate": 0.0014619298223543236,
      "loss": 1.6402,
      "step": 524
    },
    {
      "epoch": 1.05,
      "grad_norm": 4.172820568084717,
      "learning_rate": 0.0014600650377311522,
      "loss": 1.4812,
      "step": 525
    },
    {
      "epoch": 1.052,
      "grad_norm": 1.5786224603652954,
      "learning_rate": 0.001458198221516276,
      "loss": 1.4829,
      "step": 526
    },
    {
      "epoch": 1.054,
      "grad_norm": 4.870033264160156,
      "learning_rate": 0.0014563293819533298,
      "loss": 1.6162,
      "step": 527
    },
    {
      "epoch": 1.056,
      "grad_norm": 2.5753307342529297,
      "learning_rate": 0.0014544585272948842,
      "loss": 1.6945,
      "step": 528
    },
    {
      "epoch": 1.058,
      "grad_norm": 2.546924114227295,
      "learning_rate": 0.0014525856658024075,
      "loss": 1.7231,
      "step": 529
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.9857914447784424,
      "learning_rate": 0.0014507108057462297,
      "loss": 1.6715,
      "step": 530
    },
    {
      "epoch": 1.062,
      "grad_norm": 1.2026174068450928,
      "learning_rate": 0.0014488339554055072,
      "loss": 1.4322,
      "step": 531
    },
    {
      "epoch": 1.064,
      "grad_norm": 6.831431865692139,
      "learning_rate": 0.0014469551230681843,
      "loss": 1.3952,
      "step": 532
    },
    {
      "epoch": 1.066,
      "grad_norm": 5.6800127029418945,
      "learning_rate": 0.0014450743170309583,
      "loss": 1.8683,
      "step": 533
    },
    {
      "epoch": 1.068,
      "grad_norm": 4.73813009262085,
      "learning_rate": 0.0014431915455992415,
      "loss": 1.82,
      "step": 534
    },
    {
      "epoch": 1.07,
      "grad_norm": 4.033404350280762,
      "learning_rate": 0.001441306817087125,
      "loss": 1.6389,
      "step": 535
    },
    {
      "epoch": 1.072,
      "grad_norm": 5.071736812591553,
      "learning_rate": 0.0014394201398173437,
      "loss": 1.6672,
      "step": 536
    },
    {
      "epoch": 1.074,
      "grad_norm": 1.3827528953552246,
      "learning_rate": 0.0014375315221212357,
      "loss": 1.5036,
      "step": 537
    },
    {
      "epoch": 1.076,
      "grad_norm": 2.5762128829956055,
      "learning_rate": 0.001435640972338709,
      "loss": 1.5455,
      "step": 538
    },
    {
      "epoch": 1.078,
      "grad_norm": 1.3640952110290527,
      "learning_rate": 0.001433748498818204,
      "loss": 1.5559,
      "step": 539
    },
    {
      "epoch": 1.08,
      "grad_norm": 6.8236083984375,
      "learning_rate": 0.0014318541099166556,
      "loss": 1.4586,
      "step": 540
    },
    {
      "epoch": 1.082,
      "grad_norm": 0.9722149968147278,
      "learning_rate": 0.0014299578139994557,
      "loss": 1.5188,
      "step": 541
    },
    {
      "epoch": 1.084,
      "grad_norm": 5.86314582824707,
      "learning_rate": 0.0014280596194404186,
      "loss": 1.4754,
      "step": 542
    },
    {
      "epoch": 1.086,
      "grad_norm": 0.9638668298721313,
      "learning_rate": 0.001426159534621743,
      "loss": 1.5417,
      "step": 543
    },
    {
      "epoch": 1.088,
      "grad_norm": 1.4243652820587158,
      "learning_rate": 0.0014242575679339737,
      "loss": 1.4833,
      "step": 544
    },
    {
      "epoch": 1.09,
      "grad_norm": 18.954627990722656,
      "learning_rate": 0.0014223537277759666,
      "loss": 1.468,
      "step": 545
    },
    {
      "epoch": 1.092,
      "grad_norm": 1.109449028968811,
      "learning_rate": 0.0014204480225548494,
      "loss": 1.5664,
      "step": 546
    },
    {
      "epoch": 1.094,
      "grad_norm": 0.8905270099639893,
      "learning_rate": 0.0014185404606859874,
      "loss": 1.5251,
      "step": 547
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.262162208557129,
      "learning_rate": 0.0014166310505929435,
      "loss": 1.5743,
      "step": 548
    },
    {
      "epoch": 1.098,
      "grad_norm": 0.8477277755737305,
      "learning_rate": 0.0014147198007074416,
      "loss": 1.5832,
      "step": 549
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.8544580936431885,
      "learning_rate": 0.0014128067194693315,
      "loss": 1.6135,
      "step": 550
    },
    {
      "epoch": 1.102,
      "grad_norm": 1.6738367080688477,
      "learning_rate": 0.0014108918153265485,
      "loss": 1.41,
      "step": 551
    },
    {
      "epoch": 1.104,
      "grad_norm": 1.5051926374435425,
      "learning_rate": 0.001408975096735078,
      "loss": 1.6038,
      "step": 552
    },
    {
      "epoch": 1.106,
      "grad_norm": 1.3606088161468506,
      "learning_rate": 0.0014070565721589195,
      "loss": 1.6568,
      "step": 553
    },
    {
      "epoch": 1.108,
      "grad_norm": 1.434888243675232,
      "learning_rate": 0.0014051362500700447,
      "loss": 1.5436,
      "step": 554
    },
    {
      "epoch": 1.11,
      "grad_norm": 2.0266904830932617,
      "learning_rate": 0.0014032141389483648,
      "loss": 1.5003,
      "step": 555
    },
    {
      "epoch": 1.112,
      "grad_norm": 18.30324935913086,
      "learning_rate": 0.0014012902472816907,
      "loss": 1.5255,
      "step": 556
    },
    {
      "epoch": 1.114,
      "grad_norm": 6.174690246582031,
      "learning_rate": 0.0013993645835656955,
      "loss": 1.9058,
      "step": 557
    },
    {
      "epoch": 1.116,
      "grad_norm": 7.176418304443359,
      "learning_rate": 0.0013974371563038785,
      "loss": 2.2636,
      "step": 558
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 4.957699298858643,
      "learning_rate": 0.0013955079740075255,
      "loss": 2.0708,
      "step": 559
    },
    {
      "epoch": 1.12,
      "grad_norm": 20.495866775512695,
      "learning_rate": 0.0013935770451956732,
      "loss": 1.6678,
      "step": 560
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 9.747633934020996,
      "learning_rate": 0.0013916443783950694,
      "loss": 1.8024,
      "step": 561
    },
    {
      "epoch": 1.124,
      "grad_norm": 13.572236061096191,
      "learning_rate": 0.0013897099821401384,
      "loss": 2.239,
      "step": 562
    },
    {
      "epoch": 1.126,
      "grad_norm": 5.918147087097168,
      "learning_rate": 0.0013877738649729406,
      "loss": 2.621,
      "step": 563
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.567435622215271,
      "learning_rate": 0.0013858360354431355,
      "loss": 2.1715,
      "step": 564
    },
    {
      "epoch": 1.13,
      "grad_norm": 5.028715133666992,
      "learning_rate": 0.0013838965021079445,
      "loss": 1.8252,
      "step": 565
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 4.168030738830566,
      "learning_rate": 0.0013819552735321134,
      "loss": 2.018,
      "step": 566
    },
    {
      "epoch": 1.134,
      "grad_norm": 2.295701503753662,
      "learning_rate": 0.001380012358287873,
      "loss": 1.941,
      "step": 567
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.4259865283966064,
      "learning_rate": 0.0013780677649549026,
      "loss": 1.7363,
      "step": 568
    },
    {
      "epoch": 1.138,
      "grad_norm": 1.300613522529602,
      "learning_rate": 0.0013761215021202914,
      "loss": 1.8905,
      "step": 569
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.816460132598877,
      "learning_rate": 0.001374173578378502,
      "loss": 1.583,
      "step": 570
    },
    {
      "epoch": 1.142,
      "grad_norm": 1.21202552318573,
      "learning_rate": 0.0013722240023313307,
      "loss": 1.5779,
      "step": 571
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.182912826538086,
      "learning_rate": 0.0013702727825878694,
      "loss": 1.7754,
      "step": 572
    },
    {
      "epoch": 1.146,
      "grad_norm": 0.9406577348709106,
      "learning_rate": 0.0013683199277644694,
      "loss": 1.6909,
      "step": 573
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.9332537651062012,
      "learning_rate": 0.001366365446484702,
      "loss": 1.6569,
      "step": 574
    },
    {
      "epoch": 1.15,
      "grad_norm": 1.0008440017700195,
      "learning_rate": 0.0013644093473793213,
      "loss": 1.6609,
      "step": 575
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.8391348719596863,
      "learning_rate": 0.0013624516390862242,
      "loss": 1.6126,
      "step": 576
    },
    {
      "epoch": 1.154,
      "grad_norm": 1.2672902345657349,
      "learning_rate": 0.0013604923302504147,
      "loss": 1.7475,
      "step": 577
    },
    {
      "epoch": 1.156,
      "grad_norm": 1.0147987604141235,
      "learning_rate": 0.0013585314295239644,
      "loss": 1.5618,
      "step": 578
    },
    {
      "epoch": 1.158,
      "grad_norm": 1.1782443523406982,
      "learning_rate": 0.0013565689455659737,
      "loss": 1.6344,
      "step": 579
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.077957034111023,
      "learning_rate": 0.0013546048870425357,
      "loss": 1.5838,
      "step": 580
    },
    {
      "epoch": 1.162,
      "grad_norm": 2.2564644813537598,
      "learning_rate": 0.0013526392626266957,
      "loss": 1.6375,
      "step": 581
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.196020483970642,
      "learning_rate": 0.0013506720809984137,
      "loss": 1.6993,
      "step": 582
    },
    {
      "epoch": 1.166,
      "grad_norm": 1.3285222053527832,
      "learning_rate": 0.001348703350844527,
      "loss": 1.5924,
      "step": 583
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.7322521209716797,
      "learning_rate": 0.0013467330808587098,
      "loss": 1.7281,
      "step": 584
    },
    {
      "epoch": 1.17,
      "grad_norm": 1.1363630294799805,
      "learning_rate": 0.001344761279741437,
      "loss": 1.5261,
      "step": 585
    },
    {
      "epoch": 1.172,
      "grad_norm": 1.2277910709381104,
      "learning_rate": 0.001342787956199945,
      "loss": 1.7118,
      "step": 586
    },
    {
      "epoch": 1.174,
      "grad_norm": 0.9945822358131409,
      "learning_rate": 0.001340813118948191,
      "loss": 1.6094,
      "step": 587
    },
    {
      "epoch": 1.176,
      "grad_norm": 2.0086967945098877,
      "learning_rate": 0.0013388367767068199,
      "loss": 1.645,
      "step": 588
    },
    {
      "epoch": 1.178,
      "grad_norm": 1.126505732536316,
      "learning_rate": 0.0013368589382031196,
      "loss": 1.6289,
      "step": 589
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.1656737327575684,
      "learning_rate": 0.0013348796121709862,
      "loss": 1.5489,
      "step": 590
    },
    {
      "epoch": 1.182,
      "grad_norm": 1.0129072666168213,
      "learning_rate": 0.0013328988073508853,
      "loss": 1.6256,
      "step": 591
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.9766730070114136,
      "learning_rate": 0.001330916532489811,
      "loss": 1.5712,
      "step": 592
    },
    {
      "epoch": 1.186,
      "grad_norm": 1.049911618232727,
      "learning_rate": 0.001328932796341251,
      "loss": 1.6635,
      "step": 593
    },
    {
      "epoch": 1.188,
      "grad_norm": 1.2256388664245605,
      "learning_rate": 0.0013269476076651447,
      "loss": 1.5562,
      "step": 594
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.9173448085784912,
      "learning_rate": 0.0013249609752278453,
      "loss": 1.5869,
      "step": 595
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.167176604270935,
      "learning_rate": 0.0013229729078020822,
      "loss": 1.4529,
      "step": 596
    },
    {
      "epoch": 1.194,
      "grad_norm": 0.7670183777809143,
      "learning_rate": 0.0013209834141669212,
      "loss": 1.5984,
      "step": 597
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.9808332920074463,
      "learning_rate": 0.0013189925031077267,
      "loss": 1.5442,
      "step": 598
    },
    {
      "epoch": 1.198,
      "grad_norm": 1.0966367721557617,
      "learning_rate": 0.0013170001834161209,
      "loss": 1.4578,
      "step": 599
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.2338659763336182,
      "learning_rate": 0.001315006463889948,
      "loss": 1.608,
      "step": 600
    },
    {
      "epoch": 1.2,
      "eval_loss": 1.5334744453430176,
      "eval_runtime": 229.0031,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 600
    },
    {
      "epoch": 1.202,
      "grad_norm": 1.4098153114318848,
      "learning_rate": 0.0013130113533332325,
      "loss": 1.652,
      "step": 601
    },
    {
      "epoch": 1.204,
      "grad_norm": 4.585978984832764,
      "learning_rate": 0.0013110148605561419,
      "loss": 1.5079,
      "step": 602
    },
    {
      "epoch": 1.206,
      "grad_norm": 1.1779398918151855,
      "learning_rate": 0.0013090169943749475,
      "loss": 1.4973,
      "step": 603
    },
    {
      "epoch": 1.208,
      "grad_norm": 1.004794716835022,
      "learning_rate": 0.0013070177636119854,
      "loss": 1.5554,
      "step": 604
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.9892309904098511,
      "learning_rate": 0.0013050171770956176,
      "loss": 1.5823,
      "step": 605
    },
    {
      "epoch": 1.212,
      "grad_norm": 2.153183937072754,
      "learning_rate": 0.0013030152436601927,
      "loss": 1.453,
      "step": 606
    },
    {
      "epoch": 1.214,
      "grad_norm": 1.1621555089950562,
      "learning_rate": 0.0013010119721460073,
      "loss": 1.5634,
      "step": 607
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.8639954328536987,
      "learning_rate": 0.0012990073713992662,
      "loss": 1.3446,
      "step": 608
    },
    {
      "epoch": 1.218,
      "grad_norm": 0.9370217323303223,
      "learning_rate": 0.0012970014502720452,
      "loss": 1.5977,
      "step": 609
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.5776727199554443,
      "learning_rate": 0.0012949942176222495,
      "loss": 1.7048,
      "step": 610
    },
    {
      "epoch": 1.222,
      "grad_norm": 1.0274442434310913,
      "learning_rate": 0.0012929856823135771,
      "loss": 1.5001,
      "step": 611
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.8040680289268494,
      "learning_rate": 0.0012909758532154765,
      "loss": 1.5553,
      "step": 612
    },
    {
      "epoch": 1.226,
      "grad_norm": 1.5417072772979736,
      "learning_rate": 0.0012889647392031111,
      "loss": 1.5585,
      "step": 613
    },
    {
      "epoch": 1.228,
      "grad_norm": 1.0702061653137207,
      "learning_rate": 0.0012869523491573181,
      "loss": 1.4095,
      "step": 614
    },
    {
      "epoch": 1.23,
      "grad_norm": 1.9772862195968628,
      "learning_rate": 0.0012849386919645686,
      "loss": 1.7985,
      "step": 615
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.4403539896011353,
      "learning_rate": 0.00128292377651693,
      "loss": 1.5532,
      "step": 616
    },
    {
      "epoch": 1.234,
      "grad_norm": 1.5148303508758545,
      "learning_rate": 0.001280907611712026,
      "loss": 1.492,
      "step": 617
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.1462969779968262,
      "learning_rate": 0.001278890206452997,
      "loss": 1.5264,
      "step": 618
    },
    {
      "epoch": 1.238,
      "grad_norm": 1.2081433534622192,
      "learning_rate": 0.0012768715696484616,
      "loss": 1.6925,
      "step": 619
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.8226962089538574,
      "learning_rate": 0.0012748517102124754,
      "loss": 1.4884,
      "step": 620
    },
    {
      "epoch": 1.242,
      "grad_norm": 1.627720832824707,
      "learning_rate": 0.0012728306370644953,
      "loss": 1.5379,
      "step": 621
    },
    {
      "epoch": 1.244,
      "grad_norm": 1.1967682838439941,
      "learning_rate": 0.0012708083591293359,
      "loss": 1.5264,
      "step": 622
    },
    {
      "epoch": 1.246,
      "grad_norm": 0.9124382734298706,
      "learning_rate": 0.0012687848853371322,
      "loss": 1.5769,
      "step": 623
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.6862711906433105,
      "learning_rate": 0.001266760224623301,
      "loss": 1.441,
      "step": 624
    },
    {
      "epoch": 1.25,
      "grad_norm": 2.2149643898010254,
      "learning_rate": 0.0012647343859284997,
      "loss": 1.6024,
      "step": 625
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.9566590189933777,
      "learning_rate": 0.0012627073781985869,
      "loss": 1.5418,
      "step": 626
    },
    {
      "epoch": 1.254,
      "grad_norm": 1.0358984470367432,
      "learning_rate": 0.001260679210384585,
      "loss": 1.4036,
      "step": 627
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.0196791887283325,
      "learning_rate": 0.0012586498914426382,
      "loss": 1.5592,
      "step": 628
    },
    {
      "epoch": 1.258,
      "grad_norm": 0.9406511783599854,
      "learning_rate": 0.0012566194303339738,
      "loss": 1.3775,
      "step": 629
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.7432499527931213,
      "learning_rate": 0.0012545878360248632,
      "loss": 1.5476,
      "step": 630
    },
    {
      "epoch": 1.262,
      "grad_norm": 0.7222140431404114,
      "learning_rate": 0.001252555117486582,
      "loss": 1.3878,
      "step": 631
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.2760851383209229,
      "learning_rate": 0.00125052128369537,
      "loss": 1.4156,
      "step": 632
    },
    {
      "epoch": 1.266,
      "grad_norm": 1.6329388618469238,
      "learning_rate": 0.001248486343632392,
      "loss": 1.5439,
      "step": 633
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.8072954416275024,
      "learning_rate": 0.0012464503062836975,
      "loss": 1.5009,
      "step": 634
    },
    {
      "epoch": 1.27,
      "grad_norm": 1.111607551574707,
      "learning_rate": 0.0012444131806401816,
      "loss": 1.4491,
      "step": 635
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.7511457204818726,
      "learning_rate": 0.001242374975697546,
      "loss": 1.372,
      "step": 636
    },
    {
      "epoch": 1.274,
      "grad_norm": 1.320016860961914,
      "learning_rate": 0.0012403357004562574,
      "loss": 1.4534,
      "step": 637
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.958395779132843,
      "learning_rate": 0.0012382953639215096,
      "loss": 1.3985,
      "step": 638
    },
    {
      "epoch": 1.278,
      "grad_norm": 0.6014586687088013,
      "learning_rate": 0.0012362539751031823,
      "loss": 1.449,
      "step": 639
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.2806674242019653,
      "learning_rate": 0.0012342115430158023,
      "loss": 1.4922,
      "step": 640
    },
    {
      "epoch": 1.282,
      "grad_norm": 0.6254866719245911,
      "learning_rate": 0.0012321680766785035,
      "loss": 1.5013,
      "step": 641
    },
    {
      "epoch": 1.284,
      "grad_norm": 1.0840685367584229,
      "learning_rate": 0.0012301235851149865,
      "loss": 1.7222,
      "step": 642
    },
    {
      "epoch": 1.286,
      "grad_norm": 0.7782782912254333,
      "learning_rate": 0.0012280780773534794,
      "loss": 1.4106,
      "step": 643
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.9271112680435181,
      "learning_rate": 0.001226031562426698,
      "loss": 1.5154,
      "step": 644
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.7043904066085815,
      "learning_rate": 0.0012239840493718048,
      "loss": 1.3854,
      "step": 645
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.8749702572822571,
      "learning_rate": 0.001221935547230371,
      "loss": 1.4865,
      "step": 646
    },
    {
      "epoch": 1.294,
      "grad_norm": 0.9267475605010986,
      "learning_rate": 0.0012198860650483344,
      "loss": 1.4511,
      "step": 647
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.5016639232635498,
      "learning_rate": 0.0012178356118759618,
      "loss": 1.3611,
      "step": 648
    },
    {
      "epoch": 1.298,
      "grad_norm": 0.618175745010376,
      "learning_rate": 0.0012157841967678062,
      "loss": 1.4025,
      "step": 649
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.0168097019195557,
      "learning_rate": 0.0012137318287826697,
      "loss": 1.5556,
      "step": 650
    },
    {
      "epoch": 1.302,
      "grad_norm": 2.2081856727600098,
      "learning_rate": 0.0012116785169835617,
      "loss": 1.4574,
      "step": 651
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.8473395109176636,
      "learning_rate": 0.0012096242704376598,
      "loss": 1.465,
      "step": 652
    },
    {
      "epoch": 1.306,
      "grad_norm": 0.8876979351043701,
      "learning_rate": 0.0012075690982162677,
      "loss": 1.5276,
      "step": 653
    },
    {
      "epoch": 1.308,
      "grad_norm": 1.0146435499191284,
      "learning_rate": 0.001205513009394779,
      "loss": 1.5959,
      "step": 654
    },
    {
      "epoch": 1.31,
      "grad_norm": 5.470303058624268,
      "learning_rate": 0.001203456013052634,
      "loss": 1.5064,
      "step": 655
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.755900502204895,
      "learning_rate": 0.0012013981182732796,
      "loss": 1.5524,
      "step": 656
    },
    {
      "epoch": 1.314,
      "grad_norm": 3.6215336322784424,
      "learning_rate": 0.0011993393341441319,
      "loss": 1.5399,
      "step": 657
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.1844679117202759,
      "learning_rate": 0.0011972796697565322,
      "loss": 1.5394,
      "step": 658
    },
    {
      "epoch": 1.318,
      "grad_norm": 0.919153094291687,
      "learning_rate": 0.0011952191342057103,
      "loss": 1.4475,
      "step": 659
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.9697510600090027,
      "learning_rate": 0.0011931577365907433,
      "loss": 1.5113,
      "step": 660
    },
    {
      "epoch": 1.322,
      "grad_norm": 1.5506731271743774,
      "learning_rate": 0.0011910954860145137,
      "loss": 1.5614,
      "step": 661
    },
    {
      "epoch": 1.324,
      "grad_norm": 2.0964367389678955,
      "learning_rate": 0.0011890323915836713,
      "loss": 1.5209,
      "step": 662
    },
    {
      "epoch": 1.326,
      "grad_norm": 1.372867465019226,
      "learning_rate": 0.0011869684624085924,
      "loss": 1.4731,
      "step": 663
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.275971531867981,
      "learning_rate": 0.001184903707603339,
      "loss": 1.512,
      "step": 664
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.9447258114814758,
      "learning_rate": 0.0011828381362856196,
      "loss": 1.4427,
      "step": 665
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.9499161243438721,
      "learning_rate": 0.0011807717575767474,
      "loss": 1.5645,
      "step": 666
    },
    {
      "epoch": 1.334,
      "grad_norm": 0.5635765790939331,
      "learning_rate": 0.001178704580601602,
      "loss": 1.564,
      "step": 667
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.2151210308074951,
      "learning_rate": 0.0011766366144885876,
      "loss": 1.6054,
      "step": 668
    },
    {
      "epoch": 1.338,
      "grad_norm": 0.792966902256012,
      "learning_rate": 0.0011745678683695927,
      "loss": 1.568,
      "step": 669
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.1206854581832886,
      "learning_rate": 0.0011724983513799504,
      "loss": 1.4106,
      "step": 670
    },
    {
      "epoch": 1.342,
      "grad_norm": 0.6191392540931702,
      "learning_rate": 0.0011704280726583989,
      "loss": 1.488,
      "step": 671
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.9175344705581665,
      "learning_rate": 0.0011683570413470383,
      "loss": 1.5011,
      "step": 672
    },
    {
      "epoch": 1.346,
      "grad_norm": 0.8753415942192078,
      "learning_rate": 0.0011662852665912942,
      "loss": 1.4305,
      "step": 673
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 1.0487139225006104,
      "learning_rate": 0.0011642127575398728,
      "loss": 1.359,
      "step": 674
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.091314435005188,
      "learning_rate": 0.0011621395233447247,
      "loss": 1.4001,
      "step": 675
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.6114835143089294,
      "learning_rate": 0.001160065573161002,
      "loss": 1.5224,
      "step": 676
    },
    {
      "epoch": 1.354,
      "grad_norm": 1.3227825164794922,
      "learning_rate": 0.001157990916147018,
      "loss": 1.4068,
      "step": 677
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 1.2252479791641235,
      "learning_rate": 0.0011559155614642082,
      "loss": 1.5362,
      "step": 678
    },
    {
      "epoch": 1.358,
      "grad_norm": 0.8008044958114624,
      "learning_rate": 0.0011538395182770886,
      "loss": 1.4365,
      "step": 679
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.922588586807251,
      "learning_rate": 0.0011517627957532152,
      "loss": 1.5931,
      "step": 680
    },
    {
      "epoch": 1.362,
      "grad_norm": 0.90240079164505,
      "learning_rate": 0.0011496854030631444,
      "loss": 1.3875,
      "step": 681
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 1.2543224096298218,
      "learning_rate": 0.0011476073493803913,
      "loss": 1.625,
      "step": 682
    },
    {
      "epoch": 1.366,
      "grad_norm": 0.8563950061798096,
      "learning_rate": 0.0011455286438813907,
      "loss": 1.4075,
      "step": 683
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.7726670503616333,
      "learning_rate": 0.001143449295745455,
      "loss": 1.4097,
      "step": 684
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.7912070155143738,
      "learning_rate": 0.0011413693141547351,
      "loss": 1.4951,
      "step": 685
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.6634076237678528,
      "learning_rate": 0.001139288708294178,
      "loss": 1.4517,
      "step": 686
    },
    {
      "epoch": 1.374,
      "grad_norm": 1.0928665399551392,
      "learning_rate": 0.0011372074873514893,
      "loss": 1.3551,
      "step": 687
    },
    {
      "epoch": 1.376,
      "grad_norm": 1.1028386354446411,
      "learning_rate": 0.0011351256605170886,
      "loss": 1.5235,
      "step": 688
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 0.6663061380386353,
      "learning_rate": 0.0011330432369840726,
      "loss": 1.364,
      "step": 689
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.0999181270599365,
      "learning_rate": 0.0011309602259481726,
      "loss": 1.4756,
      "step": 690
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 0.931542694568634,
      "learning_rate": 0.001128876636607713,
      "loss": 1.5298,
      "step": 691
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.8382184505462646,
      "learning_rate": 0.0011267924781635742,
      "loss": 1.4457,
      "step": 692
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 0.7070698738098145,
      "learning_rate": 0.0011247077598191479,
      "loss": 1.4244,
      "step": 693
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.1126073598861694,
      "learning_rate": 0.0011226224907802983,
      "loss": 1.4628,
      "step": 694
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.8875011801719666,
      "learning_rate": 0.0011205366802553229,
      "loss": 1.3634,
      "step": 695
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.8479020595550537,
      "learning_rate": 0.001118450337454909,
      "loss": 1.3608,
      "step": 696
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 1.254683017730713,
      "learning_rate": 0.0011163634715920946,
      "loss": 1.4841,
      "step": 697
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.8564544320106506,
      "learning_rate": 0.0011142760918822275,
      "loss": 1.3678,
      "step": 698
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 0.5995428562164307,
      "learning_rate": 0.0011121882075429248,
      "loss": 1.3635,
      "step": 699
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.453490138053894,
      "learning_rate": 0.0011100998277940315,
      "loss": 1.4779,
      "step": 700
    },
    {
      "epoch": 1.4,
      "eval_loss": 1.4298977851867676,
      "eval_runtime": 228.9961,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 700
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 0.9146859049797058,
      "learning_rate": 0.0011080109618575816,
      "loss": 1.4162,
      "step": 701
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.8046936392784119,
      "learning_rate": 0.001105921618957754,
      "loss": 1.3655,
      "step": 702
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 1.1970430612564087,
      "learning_rate": 0.0011038318083208354,
      "loss": 1.4462,
      "step": 703
    },
    {
      "epoch": 1.408,
      "grad_norm": 3.2156169414520264,
      "learning_rate": 0.0011017415391751774,
      "loss": 1.5788,
      "step": 704
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.9793831706047058,
      "learning_rate": 0.0010996508207511565,
      "loss": 1.365,
      "step": 705
    },
    {
      "epoch": 1.412,
      "grad_norm": 17.40815544128418,
      "learning_rate": 0.001097559662281133,
      "loss": 1.4031,
      "step": 706
    },
    {
      "epoch": 1.414,
      "grad_norm": 1.0826998949050903,
      "learning_rate": 0.0010954680729994102,
      "loss": 1.5355,
      "step": 707
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.2123832702636719,
      "learning_rate": 0.0010933760621421942,
      "loss": 1.45,
      "step": 708
    },
    {
      "epoch": 1.418,
      "grad_norm": 1.2673410177230835,
      "learning_rate": 0.0010912836389475526,
      "loss": 1.4811,
      "step": 709
    },
    {
      "epoch": 1.42,
      "grad_norm": 2.361619710922241,
      "learning_rate": 0.0010891908126553738,
      "loss": 1.4186,
      "step": 710
    },
    {
      "epoch": 1.422,
      "grad_norm": 1.041494607925415,
      "learning_rate": 0.0010870975925073262,
      "loss": 1.3588,
      "step": 711
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.0763741731643677,
      "learning_rate": 0.0010850039877468172,
      "loss": 1.4442,
      "step": 712
    },
    {
      "epoch": 1.426,
      "grad_norm": 0.9534597396850586,
      "learning_rate": 0.0010829100076189533,
      "loss": 1.4104,
      "step": 713
    },
    {
      "epoch": 1.428,
      "grad_norm": 1.2166271209716797,
      "learning_rate": 0.0010808156613704978,
      "loss": 1.5201,
      "step": 714
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.6957953572273254,
      "learning_rate": 0.0010787209582498315,
      "loss": 1.4098,
      "step": 715
    },
    {
      "epoch": 1.432,
      "grad_norm": 2.021681547164917,
      "learning_rate": 0.00107662590750691,
      "loss": 1.5112,
      "step": 716
    },
    {
      "epoch": 1.434,
      "grad_norm": 0.8147386312484741,
      "learning_rate": 0.0010745305183932252,
      "loss": 1.5166,
      "step": 717
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.8298325538635254,
      "learning_rate": 0.0010724348001617625,
      "loss": 1.4171,
      "step": 718
    },
    {
      "epoch": 1.438,
      "grad_norm": 1.4096384048461914,
      "learning_rate": 0.0010703387620669606,
      "loss": 1.4327,
      "step": 719
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.309157371520996,
      "learning_rate": 0.0010682424133646711,
      "loss": 1.3511,
      "step": 720
    },
    {
      "epoch": 1.442,
      "grad_norm": 2.130545139312744,
      "learning_rate": 0.0010661457633121168,
      "loss": 1.3861,
      "step": 721
    },
    {
      "epoch": 1.444,
      "grad_norm": 1.616632342338562,
      "learning_rate": 0.0010640488211678513,
      "loss": 1.5558,
      "step": 722
    },
    {
      "epoch": 1.446,
      "grad_norm": 1.047716498374939,
      "learning_rate": 0.0010619515961917186,
      "loss": 1.4592,
      "step": 723
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.9202664494514465,
      "learning_rate": 0.0010598540976448107,
      "loss": 1.4707,
      "step": 724
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.661590814590454,
      "learning_rate": 0.0010577563347894286,
      "loss": 1.4189,
      "step": 725
    },
    {
      "epoch": 1.452,
      "grad_norm": 1.6137840747833252,
      "learning_rate": 0.0010556583168890396,
      "loss": 1.4834,
      "step": 726
    },
    {
      "epoch": 1.454,
      "grad_norm": 1.4684791564941406,
      "learning_rate": 0.0010535600532082373,
      "loss": 1.5801,
      "step": 727
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.6331346035003662,
      "learning_rate": 0.001051461553012702,
      "loss": 1.3842,
      "step": 728
    },
    {
      "epoch": 1.458,
      "grad_norm": 0.7180582284927368,
      "learning_rate": 0.001049362825569157,
      "loss": 1.4263,
      "step": 729
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.9320597052574158,
      "learning_rate": 0.0010472638801453287,
      "loss": 1.4636,
      "step": 730
    },
    {
      "epoch": 1.462,
      "grad_norm": 4.579135894775391,
      "learning_rate": 0.0010451647260099081,
      "loss": 1.3875,
      "step": 731
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.3215413093566895,
      "learning_rate": 0.0010430653724325058,
      "loss": 1.416,
      "step": 732
    },
    {
      "epoch": 1.466,
      "grad_norm": 1.0294979810714722,
      "learning_rate": 0.0010409658286836144,
      "loss": 1.3392,
      "step": 733
    },
    {
      "epoch": 1.468,
      "grad_norm": 1.1025134325027466,
      "learning_rate": 0.0010388661040345655,
      "loss": 1.4547,
      "step": 734
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.8343940377235413,
      "learning_rate": 0.0010367662077574898,
      "loss": 1.3415,
      "step": 735
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.2966703176498413,
      "learning_rate": 0.0010346661491252762,
      "loss": 1.4648,
      "step": 736
    },
    {
      "epoch": 1.474,
      "grad_norm": 0.7439001202583313,
      "learning_rate": 0.0010325659374115302,
      "loss": 1.3244,
      "step": 737
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.9620084762573242,
      "learning_rate": 0.001030465581890533,
      "loss": 1.3488,
      "step": 738
    },
    {
      "epoch": 1.478,
      "grad_norm": 0.9699628949165344,
      "learning_rate": 0.001028365091837202,
      "loss": 1.4863,
      "step": 739
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.9144236445426941,
      "learning_rate": 0.0010262644765270472,
      "loss": 1.4333,
      "step": 740
    },
    {
      "epoch": 1.482,
      "grad_norm": 0.8218076229095459,
      "learning_rate": 0.0010241637452361324,
      "loss": 1.3048,
      "step": 741
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.9960999488830566,
      "learning_rate": 0.0010220629072410338,
      "loss": 1.4169,
      "step": 742
    },
    {
      "epoch": 1.486,
      "grad_norm": 0.8271291851997375,
      "learning_rate": 0.0010199619718187984,
      "loss": 1.3285,
      "step": 743
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.013687252998352,
      "learning_rate": 0.001017860948246904,
      "loss": 1.5443,
      "step": 744
    },
    {
      "epoch": 1.49,
      "grad_norm": 1.2359248399734497,
      "learning_rate": 0.0010157598458032165,
      "loss": 1.4351,
      "step": 745
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.8602597713470459,
      "learning_rate": 0.001013658673765951,
      "loss": 1.4661,
      "step": 746
    },
    {
      "epoch": 1.494,
      "grad_norm": 1.023962378501892,
      "learning_rate": 0.0010115574414136304,
      "loss": 1.3939,
      "step": 747
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.651198148727417,
      "learning_rate": 0.0010094561580250426,
      "loss": 1.4309,
      "step": 748
    },
    {
      "epoch": 1.498,
      "grad_norm": 0.9243757128715515,
      "learning_rate": 0.0010073548328792016,
      "loss": 1.2947,
      "step": 749
    },
    {
      "epoch": 1.5,
      "grad_norm": 2.1267635822296143,
      "learning_rate": 0.0010052534752553063,
      "loss": 1.4072,
      "step": 750
    },
    {
      "epoch": 1.502,
      "grad_norm": 2.218168258666992,
      "learning_rate": 0.0010031520944326975,
      "loss": 1.5654,
      "step": 751
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.5017553567886353,
      "learning_rate": 0.0010010506996908201,
      "loss": 1.6466,
      "step": 752
    },
    {
      "epoch": 1.506,
      "grad_norm": 1.3307305574417114,
      "learning_rate": 0.0009989493003091801,
      "loss": 1.489,
      "step": 753
    },
    {
      "epoch": 1.508,
      "grad_norm": 3.102775812149048,
      "learning_rate": 0.0009968479055673027,
      "loss": 1.485,
      "step": 754
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.1093436479568481,
      "learning_rate": 0.000994746524744694,
      "loss": 1.3985,
      "step": 755
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.0472590923309326,
      "learning_rate": 0.0009926451671207984,
      "loss": 1.4356,
      "step": 756
    },
    {
      "epoch": 1.514,
      "grad_norm": 0.7667849063873291,
      "learning_rate": 0.0009905438419749576,
      "loss": 1.4919,
      "step": 757
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.5584935545921326,
      "learning_rate": 0.00098844255858637,
      "loss": 1.4791,
      "step": 758
    },
    {
      "epoch": 1.518,
      "grad_norm": 0.9921008944511414,
      "learning_rate": 0.000986341326234049,
      "loss": 1.5479,
      "step": 759
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.8702777028083801,
      "learning_rate": 0.0009842401541967838,
      "loss": 1.5382,
      "step": 760
    },
    {
      "epoch": 1.522,
      "grad_norm": 0.643673300743103,
      "learning_rate": 0.0009821390517530963,
      "loss": 1.3854,
      "step": 761
    },
    {
      "epoch": 1.524,
      "grad_norm": 1.310935139656067,
      "learning_rate": 0.0009800380281812016,
      "loss": 1.419,
      "step": 762
    },
    {
      "epoch": 1.526,
      "grad_norm": 0.7780073285102844,
      "learning_rate": 0.0009779370927589666,
      "loss": 1.4644,
      "step": 763
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.2970837354660034,
      "learning_rate": 0.000975836254763868,
      "loss": 1.4596,
      "step": 764
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.7681534290313721,
      "learning_rate": 0.000973735523472953,
      "loss": 1.4772,
      "step": 765
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.5821118354797363,
      "learning_rate": 0.0009716349081627981,
      "loss": 1.3598,
      "step": 766
    },
    {
      "epoch": 1.534,
      "grad_norm": 1.0757631063461304,
      "learning_rate": 0.0009695344181094668,
      "loss": 1.405,
      "step": 767
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.5479196310043335,
      "learning_rate": 0.0009674340625884701,
      "loss": 1.4294,
      "step": 768
    },
    {
      "epoch": 1.538,
      "grad_norm": 0.6241746544837952,
      "learning_rate": 0.000965333850874724,
      "loss": 1.359,
      "step": 769
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.6383098363876343,
      "learning_rate": 0.0009632337922425105,
      "loss": 1.3162,
      "step": 770
    },
    {
      "epoch": 1.542,
      "grad_norm": 0.9284193515777588,
      "learning_rate": 0.0009611338959654346,
      "loss": 1.3229,
      "step": 771
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.8746408224105835,
      "learning_rate": 0.0009590341713163857,
      "loss": 1.4197,
      "step": 772
    },
    {
      "epoch": 1.546,
      "grad_norm": 0.8771669864654541,
      "learning_rate": 0.0009569346275674944,
      "loss": 1.4023,
      "step": 773
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.6774560213088989,
      "learning_rate": 0.0009548352739900921,
      "loss": 1.3769,
      "step": 774
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.0013619661331177,
      "learning_rate": 0.0009527361198546714,
      "loss": 1.4379,
      "step": 775
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.9200623035430908,
      "learning_rate": 0.0009506371744308432,
      "loss": 1.4775,
      "step": 776
    },
    {
      "epoch": 1.554,
      "grad_norm": 0.9188187122344971,
      "learning_rate": 0.0009485384469872979,
      "loss": 1.4906,
      "step": 777
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.7449846863746643,
      "learning_rate": 0.0009464399467917625,
      "loss": 1.4393,
      "step": 778
    },
    {
      "epoch": 1.558,
      "grad_norm": 0.9869486093521118,
      "learning_rate": 0.0009443416831109608,
      "loss": 1.4814,
      "step": 779
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.9344437718391418,
      "learning_rate": 0.0009422436652105717,
      "loss": 1.3643,
      "step": 780
    },
    {
      "epoch": 1.562,
      "grad_norm": 0.862073540687561,
      "learning_rate": 0.0009401459023551894,
      "loss": 1.3963,
      "step": 781
    },
    {
      "epoch": 1.564,
      "grad_norm": 1.139992117881775,
      "learning_rate": 0.0009380484038082813,
      "loss": 1.3761,
      "step": 782
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 0.9760828614234924,
      "learning_rate": 0.0009359511788321485,
      "loss": 1.4262,
      "step": 783
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.6173897981643677,
      "learning_rate": 0.0009338542366878834,
      "loss": 1.3022,
      "step": 784
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 1.2682507038116455,
      "learning_rate": 0.0009317575866353291,
      "loss": 1.4142,
      "step": 785
    },
    {
      "epoch": 1.572,
      "grad_norm": 1.0130372047424316,
      "learning_rate": 0.0009296612379330396,
      "loss": 1.2608,
      "step": 786
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 0.7193174362182617,
      "learning_rate": 0.0009275651998382377,
      "loss": 1.3368,
      "step": 787
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.0688443183898926,
      "learning_rate": 0.0009254694816067747,
      "loss": 1.3775,
      "step": 788
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 0.5633663535118103,
      "learning_rate": 0.0009233740924930904,
      "loss": 1.262,
      "step": 789
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.3359781503677368,
      "learning_rate": 0.0009212790417501688,
      "loss": 1.4616,
      "step": 790
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 0.8400145769119263,
      "learning_rate": 0.0009191843386295022,
      "loss": 1.3133,
      "step": 791
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.7709165215492249,
      "learning_rate": 0.0009170899923810469,
      "loss": 1.4751,
      "step": 792
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 1.0307276248931885,
      "learning_rate": 0.0009149960122531826,
      "loss": 1.4164,
      "step": 793
    },
    {
      "epoch": 1.588,
      "grad_norm": 1.6415058374404907,
      "learning_rate": 0.0009129024074926743,
      "loss": 1.5702,
      "step": 794
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 1.1347970962524414,
      "learning_rate": 0.0009108091873446264,
      "loss": 1.3426,
      "step": 795
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.7205044627189636,
      "learning_rate": 0.0009087163610524475,
      "loss": 1.3236,
      "step": 796
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 1.1659268140792847,
      "learning_rate": 0.0009066239378578059,
      "loss": 1.4707,
      "step": 797
    },
    {
      "epoch": 1.596,
      "grad_norm": 1.2072844505310059,
      "learning_rate": 0.00090453192700059,
      "loss": 1.2708,
      "step": 798
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 0.879864513874054,
      "learning_rate": 0.0009024403377188673,
      "loss": 1.3622,
      "step": 799
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.7675594091415405,
      "learning_rate": 0.0009003491792488438,
      "loss": 1.3649,
      "step": 800
    },
    {
      "epoch": 1.6,
      "eval_loss": 1.4199143648147583,
      "eval_runtime": 228.8149,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 800
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 1.2268770933151245,
      "learning_rate": 0.0008982584608248225,
      "loss": 1.5325,
      "step": 801
    },
    {
      "epoch": 1.604,
      "grad_norm": 1.4975658655166626,
      "learning_rate": 0.0008961681916791646,
      "loss": 1.441,
      "step": 802
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 1.3009151220321655,
      "learning_rate": 0.0008940783810422461,
      "loss": 1.4345,
      "step": 803
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.3389434814453125,
      "learning_rate": 0.0008919890381424188,
      "loss": 1.474,
      "step": 804
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.6857113838195801,
      "learning_rate": 0.0008899001722059687,
      "loss": 1.3578,
      "step": 805
    },
    {
      "epoch": 1.612,
      "grad_norm": 1.199568748474121,
      "learning_rate": 0.0008878117924570754,
      "loss": 1.4888,
      "step": 806
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 0.8412646055221558,
      "learning_rate": 0.0008857239081177725,
      "loss": 1.2545,
      "step": 807
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.155876874923706,
      "learning_rate": 0.0008836365284079056,
      "loss": 1.434,
      "step": 808
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 1.0153887271881104,
      "learning_rate": 0.0008815496625450912,
      "loss": 1.3138,
      "step": 809
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.630708634853363,
      "learning_rate": 0.0008794633197446771,
      "loss": 1.458,
      "step": 810
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 1.09669828414917,
      "learning_rate": 0.0008773775092197017,
      "loss": 1.2675,
      "step": 811
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.0127789974212646,
      "learning_rate": 0.0008752922401808523,
      "loss": 1.3657,
      "step": 812
    },
    {
      "epoch": 1.626,
      "grad_norm": 0.9662420153617859,
      "learning_rate": 0.0008732075218364258,
      "loss": 1.2541,
      "step": 813
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.8105437755584717,
      "learning_rate": 0.0008711233633922871,
      "loss": 1.4149,
      "step": 814
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.8095434904098511,
      "learning_rate": 0.0008690397740518279,
      "loss": 1.4131,
      "step": 815
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.0878397226333618,
      "learning_rate": 0.0008669567630159276,
      "loss": 1.3907,
      "step": 816
    },
    {
      "epoch": 1.634,
      "grad_norm": 0.706540584564209,
      "learning_rate": 0.0008648743394829115,
      "loss": 1.305,
      "step": 817
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.874624490737915,
      "learning_rate": 0.0008627925126485108,
      "loss": 1.3637,
      "step": 818
    },
    {
      "epoch": 1.638,
      "grad_norm": 1.2189394235610962,
      "learning_rate": 0.0008607112917058222,
      "loss": 1.3615,
      "step": 819
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.5655207633972168,
      "learning_rate": 0.0008586306858452652,
      "loss": 1.2832,
      "step": 820
    },
    {
      "epoch": 1.642,
      "grad_norm": 0.6573654413223267,
      "learning_rate": 0.0008565507042545451,
      "loss": 1.2938,
      "step": 821
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.6890116930007935,
      "learning_rate": 0.0008544713561186095,
      "loss": 1.3863,
      "step": 822
    },
    {
      "epoch": 1.646,
      "grad_norm": 0.7140735387802124,
      "learning_rate": 0.0008523926506196085,
      "loss": 1.4188,
      "step": 823
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.6521961688995361,
      "learning_rate": 0.0008503145969368561,
      "loss": 1.3513,
      "step": 824
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.7746971249580383,
      "learning_rate": 0.000848237204246785,
      "loss": 1.3303,
      "step": 825
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.7463425397872925,
      "learning_rate": 0.0008461604817229115,
      "loss": 1.3033,
      "step": 826
    },
    {
      "epoch": 1.654,
      "grad_norm": 0.780606746673584,
      "learning_rate": 0.0008440844385357918,
      "loss": 1.3075,
      "step": 827
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.7568067312240601,
      "learning_rate": 0.0008420090838529822,
      "loss": 1.428,
      "step": 828
    },
    {
      "epoch": 1.658,
      "grad_norm": 0.5553720593452454,
      "learning_rate": 0.0008399344268389981,
      "loss": 1.2664,
      "step": 829
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.7965715527534485,
      "learning_rate": 0.0008378604766552756,
      "loss": 1.4849,
      "step": 830
    },
    {
      "epoch": 1.662,
      "grad_norm": 0.8815155029296875,
      "learning_rate": 0.0008357872424601272,
      "loss": 1.3942,
      "step": 831
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.8622299432754517,
      "learning_rate": 0.000833714733408706,
      "loss": 1.4476,
      "step": 832
    },
    {
      "epoch": 1.666,
      "grad_norm": 1.224653959274292,
      "learning_rate": 0.0008316429586529614,
      "loss": 1.347,
      "step": 833
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.6635845899581909,
      "learning_rate": 0.0008295719273416011,
      "loss": 1.2042,
      "step": 834
    },
    {
      "epoch": 1.67,
      "grad_norm": 1.0669357776641846,
      "learning_rate": 0.0008275016486200497,
      "loss": 1.4224,
      "step": 835
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.9831957221031189,
      "learning_rate": 0.0008254321316304075,
      "loss": 1.3628,
      "step": 836
    },
    {
      "epoch": 1.674,
      "grad_norm": 0.8810164928436279,
      "learning_rate": 0.0008233633855114126,
      "loss": 1.3366,
      "step": 837
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.9591448903083801,
      "learning_rate": 0.000821295419398398,
      "loss": 1.3587,
      "step": 838
    },
    {
      "epoch": 1.678,
      "grad_norm": 1.301722764968872,
      "learning_rate": 0.0008192282424232527,
      "loss": 1.384,
      "step": 839
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.0059534311294556,
      "learning_rate": 0.000817161863714381,
      "loss": 1.2518,
      "step": 840
    },
    {
      "epoch": 1.682,
      "grad_norm": 0.8883270025253296,
      "learning_rate": 0.0008150962923966614,
      "loss": 1.3703,
      "step": 841
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.779270589351654,
      "learning_rate": 0.0008130315375914079,
      "loss": 1.1661,
      "step": 842
    },
    {
      "epoch": 1.686,
      "grad_norm": 1.5723451375961304,
      "learning_rate": 0.0008109676084163289,
      "loss": 1.274,
      "step": 843
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.1115530729293823,
      "learning_rate": 0.0008089045139854865,
      "loss": 1.4051,
      "step": 844
    },
    {
      "epoch": 1.69,
      "grad_norm": 1.487997055053711,
      "learning_rate": 0.000806842263409257,
      "loss": 1.6282,
      "step": 845
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.7704240679740906,
      "learning_rate": 0.0008047808657942897,
      "loss": 1.3359,
      "step": 846
    },
    {
      "epoch": 1.694,
      "grad_norm": 0.8952732682228088,
      "learning_rate": 0.0008027203302434679,
      "loss": 1.3409,
      "step": 847
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.237674355506897,
      "learning_rate": 0.0008006606658558684,
      "loss": 1.3672,
      "step": 848
    },
    {
      "epoch": 1.698,
      "grad_norm": 0.8390824198722839,
      "learning_rate": 0.0007986018817267203,
      "loss": 1.3574,
      "step": 849
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.6503231525421143,
      "learning_rate": 0.0007965439869473664,
      "loss": 1.4412,
      "step": 850
    },
    {
      "epoch": 1.702,
      "grad_norm": 1.0847594738006592,
      "learning_rate": 0.0007944869906052211,
      "loss": 1.3912,
      "step": 851
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.8952080011367798,
      "learning_rate": 0.0007924309017837325,
      "loss": 1.3962,
      "step": 852
    },
    {
      "epoch": 1.706,
      "grad_norm": 0.9757710695266724,
      "learning_rate": 0.0007903757295623406,
      "loss": 1.3785,
      "step": 853
    },
    {
      "epoch": 1.708,
      "grad_norm": 1.7027065753936768,
      "learning_rate": 0.0007883214830164383,
      "loss": 1.4927,
      "step": 854
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.850113570690155,
      "learning_rate": 0.0007862681712173304,
      "loss": 1.3763,
      "step": 855
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.7790515422821045,
      "learning_rate": 0.000784215803232194,
      "loss": 1.2991,
      "step": 856
    },
    {
      "epoch": 1.714,
      "grad_norm": 0.8169605731964111,
      "learning_rate": 0.0007821643881240386,
      "loss": 1.2691,
      "step": 857
    },
    {
      "epoch": 1.716,
      "grad_norm": 1.2181730270385742,
      "learning_rate": 0.0007801139349516656,
      "loss": 1.3741,
      "step": 858
    },
    {
      "epoch": 1.718,
      "grad_norm": 0.996147871017456,
      "learning_rate": 0.000778064452769629,
      "loss": 1.3971,
      "step": 859
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.7795325517654419,
      "learning_rate": 0.0007760159506281955,
      "loss": 1.3294,
      "step": 860
    },
    {
      "epoch": 1.722,
      "grad_norm": 0.7389630079269409,
      "learning_rate": 0.0007739684375733022,
      "loss": 1.3196,
      "step": 861
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.884056031703949,
      "learning_rate": 0.0007719219226465208,
      "loss": 1.295,
      "step": 862
    },
    {
      "epoch": 1.726,
      "grad_norm": 0.8399612903594971,
      "learning_rate": 0.0007698764148850137,
      "loss": 1.3094,
      "step": 863
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.8431825041770935,
      "learning_rate": 0.0007678319233214965,
      "loss": 1.3578,
      "step": 864
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.9728361964225769,
      "learning_rate": 0.000765788456984198,
      "loss": 1.2389,
      "step": 865
    },
    {
      "epoch": 1.732,
      "grad_norm": 1.6672265529632568,
      "learning_rate": 0.0007637460248968177,
      "loss": 1.2292,
      "step": 866
    },
    {
      "epoch": 1.734,
      "grad_norm": 1.0032707452774048,
      "learning_rate": 0.0007617046360784905,
      "loss": 1.2686,
      "step": 867
    },
    {
      "epoch": 1.736,
      "grad_norm": 2.4142684936523438,
      "learning_rate": 0.0007596642995437426,
      "loss": 1.2912,
      "step": 868
    },
    {
      "epoch": 1.738,
      "grad_norm": 1.2539838552474976,
      "learning_rate": 0.0007576250243024542,
      "loss": 1.4946,
      "step": 869
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.4700801372528076,
      "learning_rate": 0.0007555868193598187,
      "loss": 1.31,
      "step": 870
    },
    {
      "epoch": 1.742,
      "grad_norm": 2.9111111164093018,
      "learning_rate": 0.0007535496937163031,
      "loss": 1.3829,
      "step": 871
    },
    {
      "epoch": 1.744,
      "grad_norm": 2.5368053913116455,
      "learning_rate": 0.0007515136563676083,
      "loss": 1.4724,
      "step": 872
    },
    {
      "epoch": 1.746,
      "grad_norm": 1.983561396598816,
      "learning_rate": 0.0007494787163046299,
      "loss": 1.359,
      "step": 873
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.4596829414367676,
      "learning_rate": 0.000747444882513418,
      "loss": 1.42,
      "step": 874
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.2161825895309448,
      "learning_rate": 0.0007454121639751371,
      "loss": 1.3272,
      "step": 875
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.9522807598114014,
      "learning_rate": 0.0007433805696660267,
      "loss": 1.3882,
      "step": 876
    },
    {
      "epoch": 1.754,
      "grad_norm": 1.2763506174087524,
      "learning_rate": 0.000741350108557362,
      "loss": 1.3371,
      "step": 877
    },
    {
      "epoch": 1.756,
      "grad_norm": 1.5125024318695068,
      "learning_rate": 0.0007393207896154151,
      "loss": 1.3435,
      "step": 878
    },
    {
      "epoch": 1.758,
      "grad_norm": 0.9390172958374023,
      "learning_rate": 0.000737292621801413,
      "loss": 1.3229,
      "step": 879
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.598934531211853,
      "learning_rate": 0.0007352656140715006,
      "loss": 1.3302,
      "step": 880
    },
    {
      "epoch": 1.762,
      "grad_norm": 1.881211280822754,
      "learning_rate": 0.0007332397753766993,
      "loss": 1.2742,
      "step": 881
    },
    {
      "epoch": 1.764,
      "grad_norm": 1.1944831609725952,
      "learning_rate": 0.000731215114662868,
      "loss": 1.2787,
      "step": 882
    },
    {
      "epoch": 1.766,
      "grad_norm": 0.725105881690979,
      "learning_rate": 0.0007291916408706643,
      "loss": 1.4093,
      "step": 883
    },
    {
      "epoch": 1.768,
      "grad_norm": 2.9108726978302,
      "learning_rate": 0.0007271693629355047,
      "loss": 1.3575,
      "step": 884
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.931761622428894,
      "learning_rate": 0.0007251482897875244,
      "loss": 1.242,
      "step": 885
    },
    {
      "epoch": 1.772,
      "grad_norm": 1.1507843732833862,
      "learning_rate": 0.0007231284303515389,
      "loss": 1.431,
      "step": 886
    },
    {
      "epoch": 1.774,
      "grad_norm": 1.2774746417999268,
      "learning_rate": 0.0007211097935470031,
      "loss": 1.2422,
      "step": 887
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.8730714917182922,
      "learning_rate": 0.0007190923882879742,
      "loss": 1.3481,
      "step": 888
    },
    {
      "epoch": 1.778,
      "grad_norm": 0.7727015018463135,
      "learning_rate": 0.0007170762234830699,
      "loss": 1.3902,
      "step": 889
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.6107262969017029,
      "learning_rate": 0.0007150613080354315,
      "loss": 1.2238,
      "step": 890
    },
    {
      "epoch": 1.782,
      "grad_norm": 0.8112109303474426,
      "learning_rate": 0.0007130476508426822,
      "loss": 1.2232,
      "step": 891
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.118578553199768,
      "learning_rate": 0.0007110352607968889,
      "loss": 1.3424,
      "step": 892
    },
    {
      "epoch": 1.786,
      "grad_norm": 1.05631422996521,
      "learning_rate": 0.0007090241467845237,
      "loss": 1.5517,
      "step": 893
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.9813739657402039,
      "learning_rate": 0.0007070143176864231,
      "loss": 1.4033,
      "step": 894
    },
    {
      "epoch": 1.79,
      "grad_norm": 1.1957199573516846,
      "learning_rate": 0.0007050057823777502,
      "loss": 1.4746,
      "step": 895
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.3153221607208252,
      "learning_rate": 0.0007029985497279549,
      "loss": 1.3135,
      "step": 896
    },
    {
      "epoch": 1.794,
      "grad_norm": 1.3569353818893433,
      "learning_rate": 0.000700992628600734,
      "loss": 1.4059,
      "step": 897
    },
    {
      "epoch": 1.796,
      "grad_norm": 1.11194908618927,
      "learning_rate": 0.0006989880278539931,
      "loss": 1.2228,
      "step": 898
    },
    {
      "epoch": 1.798,
      "grad_norm": 1.030229926109314,
      "learning_rate": 0.0006969847563398075,
      "loss": 1.3092,
      "step": 899
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.8848022222518921,
      "learning_rate": 0.0006949828229043824,
      "loss": 1.4191,
      "step": 900
    },
    {
      "epoch": 1.8,
      "eval_loss": 1.3276846408843994,
      "eval_runtime": 228.9366,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 900
    },
    {
      "epoch": 1.802,
      "grad_norm": 0.9436598420143127,
      "learning_rate": 0.0006929822363880149,
      "loss": 1.2792,
      "step": 901
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.2280137538909912,
      "learning_rate": 0.0006909830056250527,
      "loss": 1.3061,
      "step": 902
    },
    {
      "epoch": 1.806,
      "grad_norm": 1.796993374824524,
      "learning_rate": 0.0006889851394438584,
      "loss": 1.3197,
      "step": 903
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.8879406452178955,
      "learning_rate": 0.0006869886466667679,
      "loss": 1.392,
      "step": 904
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.4102129936218262,
      "learning_rate": 0.0006849935361100521,
      "loss": 1.4399,
      "step": 905
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.4425201416015625,
      "learning_rate": 0.0006829998165838793,
      "loss": 1.4412,
      "step": 906
    },
    {
      "epoch": 1.814,
      "grad_norm": 1.2780940532684326,
      "learning_rate": 0.0006810074968922736,
      "loss": 1.3747,
      "step": 907
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 6.555848121643066,
      "learning_rate": 0.0006790165858330788,
      "loss": 1.3669,
      "step": 908
    },
    {
      "epoch": 1.818,
      "grad_norm": 1.1609936952590942,
      "learning_rate": 0.0006770270921979179,
      "loss": 1.3253,
      "step": 909
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 3.1202189922332764,
      "learning_rate": 0.0006750390247721548,
      "loss": 1.3365,
      "step": 910
    },
    {
      "epoch": 1.822,
      "grad_norm": 1.0136892795562744,
      "learning_rate": 0.0006730523923348556,
      "loss": 1.5277,
      "step": 911
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.246198058128357,
      "learning_rate": 0.0006710672036587491,
      "loss": 1.3412,
      "step": 912
    },
    {
      "epoch": 1.826,
      "grad_norm": 1.0435909032821655,
      "learning_rate": 0.0006690834675101889,
      "loss": 1.3737,
      "step": 913
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.7781681418418884,
      "learning_rate": 0.0006671011926491151,
      "loss": 1.3023,
      "step": 914
    },
    {
      "epoch": 1.83,
      "grad_norm": 1.7740411758422852,
      "learning_rate": 0.0006651203878290139,
      "loss": 1.4406,
      "step": 915
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.7292230129241943,
      "learning_rate": 0.0006631410617968807,
      "loss": 1.3168,
      "step": 916
    },
    {
      "epoch": 1.834,
      "grad_norm": 1.0872355699539185,
      "learning_rate": 0.0006611632232931804,
      "loss": 1.2335,
      "step": 917
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.7062076926231384,
      "learning_rate": 0.000659186881051809,
      "loss": 1.2892,
      "step": 918
    },
    {
      "epoch": 1.838,
      "grad_norm": 0.7790378928184509,
      "learning_rate": 0.0006572120438000553,
      "loss": 1.2587,
      "step": 919
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.9674646258354187,
      "learning_rate": 0.0006552387202585629,
      "loss": 1.353,
      "step": 920
    },
    {
      "epoch": 1.842,
      "grad_norm": 1.7633609771728516,
      "learning_rate": 0.0006532669191412905,
      "loss": 1.2777,
      "step": 921
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.206470012664795,
      "learning_rate": 0.0006512966491554735,
      "loss": 1.4467,
      "step": 922
    },
    {
      "epoch": 1.846,
      "grad_norm": 0.6656424403190613,
      "learning_rate": 0.0006493279190015865,
      "loss": 1.4155,
      "step": 923
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 1.0454294681549072,
      "learning_rate": 0.0006473607373733044,
      "loss": 1.3616,
      "step": 924
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.8079620599746704,
      "learning_rate": 0.0006453951129574643,
      "loss": 1.3604,
      "step": 925
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.9291304349899292,
      "learning_rate": 0.0006434310544340265,
      "loss": 1.3812,
      "step": 926
    },
    {
      "epoch": 1.854,
      "grad_norm": 1.4500478506088257,
      "learning_rate": 0.000641468570476036,
      "loss": 1.3826,
      "step": 927
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.0876476764678955,
      "learning_rate": 0.0006395076697495854,
      "loss": 1.2446,
      "step": 928
    },
    {
      "epoch": 1.858,
      "grad_norm": 1.0329389572143555,
      "learning_rate": 0.000637548360913776,
      "loss": 1.3059,
      "step": 929
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.6684793829917908,
      "learning_rate": 0.0006355906526206787,
      "loss": 1.2859,
      "step": 930
    },
    {
      "epoch": 1.862,
      "grad_norm": 0.8531168699264526,
      "learning_rate": 0.0006336345535152976,
      "loss": 1.3027,
      "step": 931
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.7595357298851013,
      "learning_rate": 0.0006316800722355307,
      "loss": 1.3367,
      "step": 932
    },
    {
      "epoch": 1.866,
      "grad_norm": 0.739008367061615,
      "learning_rate": 0.0006297272174121309,
      "loss": 1.2969,
      "step": 933
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.7292867302894592,
      "learning_rate": 0.0006277759976686697,
      "loss": 1.2914,
      "step": 934
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.7147024869918823,
      "learning_rate": 0.0006258264216214977,
      "loss": 1.3277,
      "step": 935
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1.1261142492294312,
      "learning_rate": 0.0006238784978797084,
      "loss": 1.2831,
      "step": 936
    },
    {
      "epoch": 1.874,
      "grad_norm": 0.678737998008728,
      "learning_rate": 0.0006219322350450978,
      "loss": 1.237,
      "step": 937
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.8743345141410828,
      "learning_rate": 0.0006199876417121272,
      "loss": 1.2751,
      "step": 938
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 1.2182174921035767,
      "learning_rate": 0.0006180447264678868,
      "loss": 1.3753,
      "step": 939
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.374388337135315,
      "learning_rate": 0.0006161034978920554,
      "loss": 1.3174,
      "step": 940
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 1.2746751308441162,
      "learning_rate": 0.0006141639645568645,
      "loss": 1.3387,
      "step": 941
    },
    {
      "epoch": 1.884,
      "grad_norm": 1.9022868871688843,
      "learning_rate": 0.0006122261350270598,
      "loss": 1.4045,
      "step": 942
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 1.1478205919265747,
      "learning_rate": 0.0006102900178598616,
      "loss": 1.4117,
      "step": 943
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.0603108406066895,
      "learning_rate": 0.0006083556216049306,
      "loss": 1.3974,
      "step": 944
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 1.196036696434021,
      "learning_rate": 0.0006064229548043272,
      "loss": 1.3568,
      "step": 945
    },
    {
      "epoch": 1.892,
      "grad_norm": 3.8378682136535645,
      "learning_rate": 0.0006044920259924747,
      "loss": 1.3475,
      "step": 946
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 0.8932080268859863,
      "learning_rate": 0.0006025628436961218,
      "loss": 1.3069,
      "step": 947
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.7816734910011292,
      "learning_rate": 0.0006006354164343047,
      "loss": 1.2726,
      "step": 948
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 1.0551565885543823,
      "learning_rate": 0.0005987097527183096,
      "loss": 1.234,
      "step": 949
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.8642141819000244,
      "learning_rate": 0.0005967858610516353,
      "loss": 1.2678,
      "step": 950
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 0.7811188101768494,
      "learning_rate": 0.0005948637499299554,
      "loss": 1.3113,
      "step": 951
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.2245465517044067,
      "learning_rate": 0.000592943427841081,
      "loss": 1.5042,
      "step": 952
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 1.601223349571228,
      "learning_rate": 0.000591024903264922,
      "loss": 1.2924,
      "step": 953
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.655718207359314,
      "learning_rate": 0.0005891081846734518,
      "loss": 1.2352,
      "step": 954
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.6960938572883606,
      "learning_rate": 0.0005871932805306688,
      "loss": 1.3543,
      "step": 955
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.2658891677856445,
      "learning_rate": 0.0005852801992925585,
      "loss": 1.3392,
      "step": 956
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 0.9146814346313477,
      "learning_rate": 0.0005833689494070569,
      "loss": 1.2642,
      "step": 957
    },
    {
      "epoch": 1.916,
      "grad_norm": 1.0535380840301514,
      "learning_rate": 0.0005814595393140126,
      "loss": 1.2815,
      "step": 958
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 0.5757153630256653,
      "learning_rate": 0.0005795519774451505,
      "loss": 1.2035,
      "step": 959
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.7727646827697754,
      "learning_rate": 0.0005776462722240337,
      "loss": 1.2489,
      "step": 960
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 0.6635884046554565,
      "learning_rate": 0.0005757424320660264,
      "loss": 1.1938,
      "step": 961
    },
    {
      "epoch": 1.924,
      "grad_norm": 1.0937458276748657,
      "learning_rate": 0.0005738404653782571,
      "loss": 1.329,
      "step": 962
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 1.3761671781539917,
      "learning_rate": 0.0005719403805595815,
      "loss": 1.2351,
      "step": 963
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.1333712339401245,
      "learning_rate": 0.0005700421860005447,
      "loss": 1.358,
      "step": 964
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 1.2402344942092896,
      "learning_rate": 0.0005681458900833447,
      "loss": 1.2129,
      "step": 965
    },
    {
      "epoch": 1.932,
      "grad_norm": 1.0103952884674072,
      "learning_rate": 0.0005662515011817959,
      "loss": 1.2593,
      "step": 966
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 1.0767766237258911,
      "learning_rate": 0.0005643590276612909,
      "loss": 1.2415,
      "step": 967
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.0340982675552368,
      "learning_rate": 0.0005624684778787646,
      "loss": 1.2964,
      "step": 968
    },
    {
      "epoch": 1.938,
      "grad_norm": 0.7146506309509277,
      "learning_rate": 0.0005605798601826566,
      "loss": 1.2694,
      "step": 969
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.8860993385314941,
      "learning_rate": 0.0005586931829128749,
      "loss": 1.2318,
      "step": 970
    },
    {
      "epoch": 1.942,
      "grad_norm": 1.193990707397461,
      "learning_rate": 0.0005568084544007588,
      "loss": 1.3248,
      "step": 971
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.2207388877868652,
      "learning_rate": 0.0005549256829690418,
      "loss": 1.2085,
      "step": 972
    },
    {
      "epoch": 1.946,
      "grad_norm": 0.8959752917289734,
      "learning_rate": 0.0005530448769318157,
      "loss": 1.2504,
      "step": 973
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.9239142537117004,
      "learning_rate": 0.0005511660445944929,
      "loss": 1.3325,
      "step": 974
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.2312010526657104,
      "learning_rate": 0.0005492891942537703,
      "loss": 1.2116,
      "step": 975
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.001709222793579,
      "learning_rate": 0.0005474143341975928,
      "loss": 1.1787,
      "step": 976
    },
    {
      "epoch": 1.954,
      "grad_norm": 0.6679803133010864,
      "learning_rate": 0.0005455414727051159,
      "loss": 1.2397,
      "step": 977
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.9275016784667969,
      "learning_rate": 0.0005436706180466702,
      "loss": 1.3225,
      "step": 978
    },
    {
      "epoch": 1.958,
      "grad_norm": 0.7664514183998108,
      "learning_rate": 0.0005418017784837243,
      "loss": 1.3626,
      "step": 979
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.8553375005722046,
      "learning_rate": 0.0005399349622688479,
      "loss": 1.2627,
      "step": 980
    },
    {
      "epoch": 1.962,
      "grad_norm": 1.1441752910614014,
      "learning_rate": 0.0005380701776456766,
      "loss": 1.2463,
      "step": 981
    },
    {
      "epoch": 1.964,
      "grad_norm": 1.3491289615631104,
      "learning_rate": 0.000536207432848874,
      "loss": 1.3563,
      "step": 982
    },
    {
      "epoch": 1.966,
      "grad_norm": 1.1952950954437256,
      "learning_rate": 0.0005343467361040966,
      "loss": 1.3492,
      "step": 983
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.8893725275993347,
      "learning_rate": 0.0005324880956279567,
      "loss": 1.2664,
      "step": 984
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.9221228361129761,
      "learning_rate": 0.0005306315196279864,
      "loss": 1.2169,
      "step": 985
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.9341763854026794,
      "learning_rate": 0.0005287770163026013,
      "loss": 1.3376,
      "step": 986
    },
    {
      "epoch": 1.974,
      "grad_norm": 0.7157062888145447,
      "learning_rate": 0.0005269245938410646,
      "loss": 1.2016,
      "step": 987
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.1518175601959229,
      "learning_rate": 0.00052507426042345,
      "loss": 1.2765,
      "step": 988
    },
    {
      "epoch": 1.978,
      "grad_norm": 1.3914059400558472,
      "learning_rate": 0.0005232260242206071,
      "loss": 1.2225,
      "step": 989
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.4036592245101929,
      "learning_rate": 0.0005213798933941236,
      "loss": 1.255,
      "step": 990
    },
    {
      "epoch": 1.982,
      "grad_norm": 0.9078657031059265,
      "learning_rate": 0.0005195358760962907,
      "loss": 1.258,
      "step": 991
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.0889135599136353,
      "learning_rate": 0.0005176939804700664,
      "loss": 1.2556,
      "step": 992
    },
    {
      "epoch": 1.986,
      "grad_norm": 0.593640148639679,
      "learning_rate": 0.0005158542146490399,
      "loss": 1.2488,
      "step": 993
    },
    {
      "epoch": 1.988,
      "grad_norm": 1.2038726806640625,
      "learning_rate": 0.0005140165867573939,
      "loss": 1.4031,
      "step": 994
    },
    {
      "epoch": 1.99,
      "grad_norm": 1.2918527126312256,
      "learning_rate": 0.0005121811049098728,
      "loss": 1.3647,
      "step": 995
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.6026954054832458,
      "learning_rate": 0.0005103477772117424,
      "loss": 1.2078,
      "step": 996
    },
    {
      "epoch": 1.994,
      "grad_norm": 0.7251142859458923,
      "learning_rate": 0.0005085166117587567,
      "loss": 1.2726,
      "step": 997
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.4736086130142212,
      "learning_rate": 0.0005066876166371219,
      "loss": 1.2976,
      "step": 998
    },
    {
      "epoch": 1.998,
      "grad_norm": 0.6697493195533752,
      "learning_rate": 0.0005048607999234587,
      "loss": 1.2542,
      "step": 999
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1312150955200195,
      "learning_rate": 0.0005030361696847705,
      "loss": 1.281,
      "step": 1000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2809292078018188,
      "eval_runtime": 228.91,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1000
    },
    {
      "epoch": 2.002,
      "grad_norm": 1.260601282119751,
      "learning_rate": 0.0005012137339784037,
      "loss": 1.3179,
      "step": 1001
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.6387666463851929,
      "learning_rate": 0.0004993935008520146,
      "loss": 1.3127,
      "step": 1002
    },
    {
      "epoch": 2.006,
      "grad_norm": 0.7132006883621216,
      "learning_rate": 0.0004975754783435336,
      "loss": 1.2519,
      "step": 1003
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.8542735576629639,
      "learning_rate": 0.0004957596744811279,
      "loss": 1.2733,
      "step": 1004
    },
    {
      "epoch": 2.01,
      "grad_norm": 2.048156261444092,
      "learning_rate": 0.0004939460972831684,
      "loss": 1.2338,
      "step": 1005
    },
    {
      "epoch": 2.012,
      "grad_norm": 1.0398590564727783,
      "learning_rate": 0.000492134754758194,
      "loss": 1.2664,
      "step": 1006
    },
    {
      "epoch": 2.014,
      "grad_norm": 1.0643529891967773,
      "learning_rate": 0.0004903256549048742,
      "loss": 1.3588,
      "step": 1007
    },
    {
      "epoch": 2.016,
      "grad_norm": 1.962151288986206,
      "learning_rate": 0.0004885188057119762,
      "loss": 1.3598,
      "step": 1008
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.764815092086792,
      "learning_rate": 0.00048671421515832726,
      "loss": 1.3851,
      "step": 1009
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.5039572715759277,
      "learning_rate": 0.00048491189121278167,
      "loss": 1.2565,
      "step": 1010
    },
    {
      "epoch": 2.022,
      "grad_norm": 0.8041626214981079,
      "learning_rate": 0.0004831118418341852,
      "loss": 1.3229,
      "step": 1011
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.8319966197013855,
      "learning_rate": 0.0004813140749713384,
      "loss": 1.0954,
      "step": 1012
    },
    {
      "epoch": 2.026,
      "grad_norm": 1.1450577974319458,
      "learning_rate": 0.0004795185985629632,
      "loss": 1.2874,
      "step": 1013
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.6299481987953186,
      "learning_rate": 0.0004777254205376662,
      "loss": 1.2191,
      "step": 1014
    },
    {
      "epoch": 2.03,
      "grad_norm": 1.7483670711517334,
      "learning_rate": 0.0004759345488139054,
      "loss": 1.3568,
      "step": 1015
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.8059095144271851,
      "learning_rate": 0.00047414599129995405,
      "loss": 1.2062,
      "step": 1016
    },
    {
      "epoch": 2.034,
      "grad_norm": 2.583371162414551,
      "learning_rate": 0.00047235975589386713,
      "loss": 1.246,
      "step": 1017
    },
    {
      "epoch": 2.036,
      "grad_norm": 1.5294904708862305,
      "learning_rate": 0.00047057585048344467,
      "loss": 1.1983,
      "step": 1018
    },
    {
      "epoch": 2.038,
      "grad_norm": 1.1150835752487183,
      "learning_rate": 0.0004687942829461969,
      "loss": 1.2,
      "step": 1019
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.95988529920578,
      "learning_rate": 0.00046701506114931157,
      "loss": 1.2262,
      "step": 1020
    },
    {
      "epoch": 2.042,
      "grad_norm": 0.7685225605964661,
      "learning_rate": 0.0004652381929496172,
      "loss": 1.1958,
      "step": 1021
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.9924214482307434,
      "learning_rate": 0.00046346368619355007,
      "loss": 1.2098,
      "step": 1022
    },
    {
      "epoch": 2.046,
      "grad_norm": 0.4949361979961395,
      "learning_rate": 0.00046169154871711804,
      "loss": 1.2011,
      "step": 1023
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.6212730407714844,
      "learning_rate": 0.0004599217883458655,
      "loss": 1.2585,
      "step": 1024
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.6872054934501648,
      "learning_rate": 0.00045815441289484126,
      "loss": 1.2574,
      "step": 1025
    },
    {
      "epoch": 2.052,
      "grad_norm": 1.3634370565414429,
      "learning_rate": 0.00045638943016856204,
      "loss": 1.239,
      "step": 1026
    },
    {
      "epoch": 2.054,
      "grad_norm": 1.48817777633667,
      "learning_rate": 0.0004546268479609783,
      "loss": 1.279,
      "step": 1027
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.0784010887145996,
      "learning_rate": 0.00045286667405544114,
      "loss": 1.2757,
      "step": 1028
    },
    {
      "epoch": 2.058,
      "grad_norm": 1.4315115213394165,
      "learning_rate": 0.0004511089162246661,
      "loss": 1.2706,
      "step": 1029
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.48414483666419983,
      "learning_rate": 0.00044935358223069925,
      "loss": 1.1502,
      "step": 1030
    },
    {
      "epoch": 2.062,
      "grad_norm": 0.6347913146018982,
      "learning_rate": 0.0004476006798248837,
      "loss": 1.2065,
      "step": 1031
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.2326308488845825,
      "learning_rate": 0.00044585021674782533,
      "loss": 1.1202,
      "step": 1032
    },
    {
      "epoch": 2.066,
      "grad_norm": 2.6398048400878906,
      "learning_rate": 0.0004441022007293575,
      "loss": 1.1204,
      "step": 1033
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.9222638010978699,
      "learning_rate": 0.0004423566394885091,
      "loss": 1.2411,
      "step": 1034
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.82872074842453,
      "learning_rate": 0.0004406135407334668,
      "loss": 1.2404,
      "step": 1035
    },
    {
      "epoch": 2.072,
      "grad_norm": 1.5441759824752808,
      "learning_rate": 0.000438872912161545,
      "loss": 1.3331,
      "step": 1036
    },
    {
      "epoch": 2.074,
      "grad_norm": 0.9413723945617676,
      "learning_rate": 0.0004371347614591493,
      "loss": 1.2449,
      "step": 1037
    },
    {
      "epoch": 2.076,
      "grad_norm": 1.0385771989822388,
      "learning_rate": 0.0004353990963017433,
      "loss": 1.243,
      "step": 1038
    },
    {
      "epoch": 2.078,
      "grad_norm": 0.767503023147583,
      "learning_rate": 0.0004336659243538159,
      "loss": 1.1864,
      "step": 1039
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.1973607540130615,
      "learning_rate": 0.0004319352532688443,
      "loss": 1.3591,
      "step": 1040
    },
    {
      "epoch": 2.082,
      "grad_norm": 1.0760889053344727,
      "learning_rate": 0.00043020709068926366,
      "loss": 1.1311,
      "step": 1041
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.7123854756355286,
      "learning_rate": 0.00042848144424643134,
      "loss": 1.3394,
      "step": 1042
    },
    {
      "epoch": 2.086,
      "grad_norm": 0.8518358469009399,
      "learning_rate": 0.0004267583215605939,
      "loss": 1.2188,
      "step": 1043
    },
    {
      "epoch": 2.088,
      "grad_norm": 1.6319721937179565,
      "learning_rate": 0.0004250377302408531,
      "loss": 1.3699,
      "step": 1044
    },
    {
      "epoch": 2.09,
      "grad_norm": 32.976619720458984,
      "learning_rate": 0.0004233196778851329,
      "loss": 1.2541,
      "step": 1045
    },
    {
      "epoch": 2.092,
      "grad_norm": 1.1725808382034302,
      "learning_rate": 0.0004216041720801451,
      "loss": 1.189,
      "step": 1046
    },
    {
      "epoch": 2.094,
      "grad_norm": 0.895893931388855,
      "learning_rate": 0.00041989122040135654,
      "loss": 1.2404,
      "step": 1047
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.6142410039901733,
      "learning_rate": 0.00041818083041295486,
      "loss": 1.2162,
      "step": 1048
    },
    {
      "epoch": 2.098,
      "grad_norm": 0.7102110981941223,
      "learning_rate": 0.0004164730096678161,
      "loss": 1.2413,
      "step": 1049
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.7504299283027649,
      "learning_rate": 0.00041476776570747065,
      "loss": 1.2609,
      "step": 1050
    },
    {
      "epoch": 2.102,
      "grad_norm": 0.7273263931274414,
      "learning_rate": 0.00041306510606207005,
      "loss": 1.2355,
      "step": 1051
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.358378529548645,
      "learning_rate": 0.00041136503825035396,
      "loss": 1.3182,
      "step": 1052
    },
    {
      "epoch": 2.106,
      "grad_norm": 0.9387801289558411,
      "learning_rate": 0.00040966756977961683,
      "loss": 1.2397,
      "step": 1053
    },
    {
      "epoch": 2.108,
      "grad_norm": 1.3348349332809448,
      "learning_rate": 0.00040797270814567447,
      "loss": 1.3666,
      "step": 1054
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.8824655413627625,
      "learning_rate": 0.00040628046083283133,
      "loss": 1.2033,
      "step": 1055
    },
    {
      "epoch": 2.112,
      "grad_norm": 357.9854431152344,
      "learning_rate": 0.0004045908353138477,
      "loss": 1.2353,
      "step": 1056
    },
    {
      "epoch": 2.114,
      "grad_norm": 0.8159214854240417,
      "learning_rate": 0.0004029038390499057,
      "loss": 1.3021,
      "step": 1057
    },
    {
      "epoch": 2.116,
      "grad_norm": 1.2118078470230103,
      "learning_rate": 0.0004012194794905775,
      "loss": 1.316,
      "step": 1058
    },
    {
      "epoch": 2.118,
      "grad_norm": 1.910038709640503,
      "learning_rate": 0.0003995377640737917,
      "loss": 1.2416,
      "step": 1059
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.7945699095726013,
      "learning_rate": 0.00039785870022580075,
      "loss": 1.1367,
      "step": 1060
    },
    {
      "epoch": 2.122,
      "grad_norm": 1.9691715240478516,
      "learning_rate": 0.0003961822953611478,
      "loss": 1.3261,
      "step": 1061
    },
    {
      "epoch": 2.124,
      "grad_norm": 2.208477258682251,
      "learning_rate": 0.00039450855688263485,
      "loss": 1.1206,
      "step": 1062
    },
    {
      "epoch": 2.126,
      "grad_norm": 1.433426856994629,
      "learning_rate": 0.00039283749218128883,
      "loss": 1.2391,
      "step": 1063
    },
    {
      "epoch": 2.128,
      "grad_norm": 1.8800711631774902,
      "learning_rate": 0.00039116910863633037,
      "loss": 1.3815,
      "step": 1064
    },
    {
      "epoch": 2.13,
      "grad_norm": 1.4295191764831543,
      "learning_rate": 0.00038950341361513875,
      "loss": 1.4407,
      "step": 1065
    },
    {
      "epoch": 2.132,
      "grad_norm": 3.106860876083374,
      "learning_rate": 0.0003878404144732234,
      "loss": 1.2233,
      "step": 1066
    },
    {
      "epoch": 2.134,
      "grad_norm": 0.6160525679588318,
      "learning_rate": 0.00038618011855418743,
      "loss": 1.3117,
      "step": 1067
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.9381053447723389,
      "learning_rate": 0.0003845225331896974,
      "loss": 1.2647,
      "step": 1068
    },
    {
      "epoch": 2.138,
      "grad_norm": 0.8617138862609863,
      "learning_rate": 0.00038286766569945076,
      "loss": 1.1629,
      "step": 1069
    },
    {
      "epoch": 2.14,
      "grad_norm": 2.7235515117645264,
      "learning_rate": 0.00038121552339114164,
      "loss": 1.2817,
      "step": 1070
    },
    {
      "epoch": 2.142,
      "grad_norm": 5.825747966766357,
      "learning_rate": 0.0003795661135604319,
      "loss": 1.2191,
      "step": 1071
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.6456683874130249,
      "learning_rate": 0.00037791944349091646,
      "loss": 1.2362,
      "step": 1072
    },
    {
      "epoch": 2.146,
      "grad_norm": 0.8307990431785583,
      "learning_rate": 0.0003762755204540914,
      "loss": 1.1669,
      "step": 1073
    },
    {
      "epoch": 2.148,
      "grad_norm": 1.1561943292617798,
      "learning_rate": 0.0003746343517093229,
      "loss": 1.248,
      "step": 1074
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.7672144174575806,
      "learning_rate": 0.0003729959445038136,
      "loss": 1.1625,
      "step": 1075
    },
    {
      "epoch": 2.152,
      "grad_norm": 1.4660394191741943,
      "learning_rate": 0.00037136030607257196,
      "loss": 1.1508,
      "step": 1076
    },
    {
      "epoch": 2.154,
      "grad_norm": 0.900613009929657,
      "learning_rate": 0.00036972744363838073,
      "loss": 1.3045,
      "step": 1077
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.9442479610443115,
      "learning_rate": 0.000368097364411763,
      "loss": 1.1733,
      "step": 1078
    },
    {
      "epoch": 2.158,
      "grad_norm": 0.7328196167945862,
      "learning_rate": 0.00036647007559095204,
      "loss": 1.1997,
      "step": 1079
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.6013145446777344,
      "learning_rate": 0.00036484558436185934,
      "loss": 1.1934,
      "step": 1080
    },
    {
      "epoch": 2.162,
      "grad_norm": 0.6774169206619263,
      "learning_rate": 0.000363223897898041,
      "loss": 1.1324,
      "step": 1081
    },
    {
      "epoch": 2.164,
      "grad_norm": 1.394971489906311,
      "learning_rate": 0.00036160502336067004,
      "loss": 1.5174,
      "step": 1082
    },
    {
      "epoch": 2.166,
      "grad_norm": 1.6546937227249146,
      "learning_rate": 0.00035998896789850066,
      "loss": 1.1927,
      "step": 1083
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.968611478805542,
      "learning_rate": 0.0003583757386478389,
      "loss": 1.2004,
      "step": 1084
    },
    {
      "epoch": 2.17,
      "grad_norm": 3.8247435092926025,
      "learning_rate": 0.0003567653427325107,
      "loss": 1.2455,
      "step": 1085
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.6899540424346924,
      "learning_rate": 0.00035515778726382964,
      "loss": 1.2122,
      "step": 1086
    },
    {
      "epoch": 2.174,
      "grad_norm": 0.870061993598938,
      "learning_rate": 0.00035355307934056666,
      "loss": 1.3438,
      "step": 1087
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.6223017573356628,
      "learning_rate": 0.00035195122604891904,
      "loss": 1.268,
      "step": 1088
    },
    {
      "epoch": 2.178,
      "grad_norm": 0.6044483184814453,
      "learning_rate": 0.00035035223446247733,
      "loss": 1.2567,
      "step": 1089
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.0229498147964478,
      "learning_rate": 0.0003487561116421958,
      "loss": 1.1847,
      "step": 1090
    },
    {
      "epoch": 2.182,
      "grad_norm": 0.7255319952964783,
      "learning_rate": 0.0003471628646363597,
      "loss": 1.2119,
      "step": 1091
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.9413844347000122,
      "learning_rate": 0.00034557250048055575,
      "loss": 1.2227,
      "step": 1092
    },
    {
      "epoch": 2.186,
      "grad_norm": 1.0329883098602295,
      "learning_rate": 0.00034398502619764,
      "loss": 1.1254,
      "step": 1093
    },
    {
      "epoch": 2.188,
      "grad_norm": 1.2365890741348267,
      "learning_rate": 0.00034240044879770806,
      "loss": 1.1678,
      "step": 1094
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9060811400413513,
      "learning_rate": 0.0003408187752780624,
      "loss": 1.3044,
      "step": 1095
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.8601967692375183,
      "learning_rate": 0.00033924001262318205,
      "loss": 1.2258,
      "step": 1096
    },
    {
      "epoch": 2.194,
      "grad_norm": 0.9049960970878601,
      "learning_rate": 0.00033766416780469256,
      "loss": 1.2378,
      "step": 1097
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.8570452332496643,
      "learning_rate": 0.00033609124778133425,
      "loss": 1.246,
      "step": 1098
    },
    {
      "epoch": 2.198,
      "grad_norm": 0.8404725193977356,
      "learning_rate": 0.000334521259498933,
      "loss": 1.1667,
      "step": 1099
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.7540146112442017,
      "learning_rate": 0.0003329542098903674,
      "loss": 1.2456,
      "step": 1100
    },
    {
      "epoch": 2.2,
      "eval_loss": 1.223738431930542,
      "eval_runtime": 228.9309,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1100
    },
    {
      "epoch": 2.202,
      "grad_norm": 0.8972026705741882,
      "learning_rate": 0.00033139010587553906,
      "loss": 1.1779,
      "step": 1101
    },
    {
      "epoch": 2.204,
      "grad_norm": 1.1864666938781738,
      "learning_rate": 0.0003298289543613429,
      "loss": 1.2101,
      "step": 1102
    },
    {
      "epoch": 2.206,
      "grad_norm": 1.7911183834075928,
      "learning_rate": 0.00032827076224163556,
      "loss": 1.2475,
      "step": 1103
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.6897879242897034,
      "learning_rate": 0.0003267155363972052,
      "loss": 1.2554,
      "step": 1104
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.0919909477233887,
      "learning_rate": 0.00032516328369574247,
      "loss": 1.1958,
      "step": 1105
    },
    {
      "epoch": 2.212,
      "grad_norm": 1.0828170776367188,
      "learning_rate": 0.0003236140109918071,
      "loss": 1.2156,
      "step": 1106
    },
    {
      "epoch": 2.214,
      "grad_norm": 0.8996362686157227,
      "learning_rate": 0.0003220677251268008,
      "loss": 1.2282,
      "step": 1107
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.7800846099853516,
      "learning_rate": 0.0003205244329289354,
      "loss": 1.2108,
      "step": 1108
    },
    {
      "epoch": 2.218,
      "grad_norm": 1.8131059408187866,
      "learning_rate": 0.0003189841412132027,
      "loss": 1.3953,
      "step": 1109
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.8126987218856812,
      "learning_rate": 0.0003174468567813461,
      "loss": 1.181,
      "step": 1110
    },
    {
      "epoch": 2.222,
      "grad_norm": 0.8871451020240784,
      "learning_rate": 0.0003159125864218272,
      "loss": 1.1773,
      "step": 1111
    },
    {
      "epoch": 2.224,
      "grad_norm": 2.5925538539886475,
      "learning_rate": 0.0003143813369097991,
      "loss": 1.1603,
      "step": 1112
    },
    {
      "epoch": 2.226,
      "grad_norm": 1.192246437072754,
      "learning_rate": 0.0003128531150070749,
      "loss": 1.1741,
      "step": 1113
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.990601122379303,
      "learning_rate": 0.00031132792746209836,
      "loss": 1.2121,
      "step": 1114
    },
    {
      "epoch": 2.23,
      "grad_norm": 1.4492919445037842,
      "learning_rate": 0.0003098057810099135,
      "loss": 1.3008,
      "step": 1115
    },
    {
      "epoch": 2.232,
      "grad_norm": 1.7242426872253418,
      "learning_rate": 0.00030828668237213553,
      "loss": 1.2176,
      "step": 1116
    },
    {
      "epoch": 2.234,
      "grad_norm": 1.2671180963516235,
      "learning_rate": 0.00030677063825692067,
      "loss": 1.1601,
      "step": 1117
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.9625851511955261,
      "learning_rate": 0.0003052576553589368,
      "loss": 1.2086,
      "step": 1118
    },
    {
      "epoch": 2.238,
      "grad_norm": 0.7944760918617249,
      "learning_rate": 0.00030374774035933405,
      "loss": 1.1492,
      "step": 1119
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.1856731176376343,
      "learning_rate": 0.0003022408999257148,
      "loss": 1.1949,
      "step": 1120
    },
    {
      "epoch": 2.242,
      "grad_norm": 1.2841112613677979,
      "learning_rate": 0.00030073714071210454,
      "loss": 1.1844,
      "step": 1121
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2.0286865234375,
      "learning_rate": 0.0002992364693589228,
      "loss": 1.2361,
      "step": 1122
    },
    {
      "epoch": 2.246,
      "grad_norm": 0.8142291903495789,
      "learning_rate": 0.00029773889249295294,
      "loss": 1.2189,
      "step": 1123
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.08571457862854,
      "learning_rate": 0.0002962444167273138,
      "loss": 1.1834,
      "step": 1124
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.0786292552947998,
      "learning_rate": 0.0002947530486614303,
      "loss": 1.3278,
      "step": 1125
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.8638723492622375,
      "learning_rate": 0.0002932647948810037,
      "loss": 1.31,
      "step": 1126
    },
    {
      "epoch": 2.254,
      "grad_norm": 1.383068323135376,
      "learning_rate": 0.0002917796619579831,
      "loss": 1.1838,
      "step": 1127
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.8946739435195923,
      "learning_rate": 0.0002902976564505365,
      "loss": 1.0781,
      "step": 1128
    },
    {
      "epoch": 2.258,
      "grad_norm": 1.1314265727996826,
      "learning_rate": 0.00028881878490302125,
      "loss": 1.1956,
      "step": 1129
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.7673801183700562,
      "learning_rate": 0.0002873430538459559,
      "loss": 1.2525,
      "step": 1130
    },
    {
      "epoch": 2.262,
      "grad_norm": 2.440962314605713,
      "learning_rate": 0.00028587046979599066,
      "loss": 1.0903,
      "step": 1131
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.3382593393325806,
      "learning_rate": 0.00028440103925587903,
      "loss": 1.2138,
      "step": 1132
    },
    {
      "epoch": 2.266,
      "grad_norm": 2.586690664291382,
      "learning_rate": 0.0002829347687144489,
      "loss": 1.2251,
      "step": 1133
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.9078938364982605,
      "learning_rate": 0.00028147166464657426,
      "loss": 1.1711,
      "step": 1134
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.8347866535186768,
      "learning_rate": 0.00028001173351314626,
      "loss": 1.3059,
      "step": 1135
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.9196872711181641,
      "learning_rate": 0.00027855498176104434,
      "loss": 1.2093,
      "step": 1136
    },
    {
      "epoch": 2.274,
      "grad_norm": 1.1543989181518555,
      "learning_rate": 0.0002771014158231088,
      "loss": 1.1454,
      "step": 1137
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.8811867237091064,
      "learning_rate": 0.0002756510421181112,
      "loss": 1.2892,
      "step": 1138
    },
    {
      "epoch": 2.278,
      "grad_norm": 1.1446958780288696,
      "learning_rate": 0.0002742038670507271,
      "loss": 1.3311,
      "step": 1139
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.0929967164993286,
      "learning_rate": 0.0002727598970115068,
      "loss": 1.1647,
      "step": 1140
    },
    {
      "epoch": 2.282,
      "grad_norm": 0.6311975717544556,
      "learning_rate": 0.00027131913837684797,
      "loss": 1.2614,
      "step": 1141
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.6510522961616516,
      "learning_rate": 0.0002698815975089668,
      "loss": 1.2589,
      "step": 1142
    },
    {
      "epoch": 2.286,
      "grad_norm": 0.6521551609039307,
      "learning_rate": 0.00026844728075587043,
      "loss": 1.1729,
      "step": 1143
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.7907068729400635,
      "learning_rate": 0.00026701619445132854,
      "loss": 1.1549,
      "step": 1144
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.73386549949646,
      "learning_rate": 0.00026558834491484574,
      "loss": 1.1601,
      "step": 1145
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.9883718490600586,
      "learning_rate": 0.00026416373845163343,
      "loss": 1.1738,
      "step": 1146
    },
    {
      "epoch": 2.294,
      "grad_norm": 1.001053810119629,
      "learning_rate": 0.00026274238135258113,
      "loss": 1.2483,
      "step": 1147
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.8780947327613831,
      "learning_rate": 0.000261324279894231,
      "loss": 1.2073,
      "step": 1148
    },
    {
      "epoch": 2.298,
      "grad_norm": 0.6157861351966858,
      "learning_rate": 0.00025990944033874806,
      "loss": 1.106,
      "step": 1149
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.897179901599884,
      "learning_rate": 0.00025849786893389294,
      "loss": 1.2062,
      "step": 1150
    },
    {
      "epoch": 2.302,
      "grad_norm": 1.0566297769546509,
      "learning_rate": 0.0002570895719129949,
      "loss": 1.1022,
      "step": 1151
    },
    {
      "epoch": 2.304,
      "grad_norm": 1.2217146158218384,
      "learning_rate": 0.00025568455549492306,
      "loss": 1.2528,
      "step": 1152
    },
    {
      "epoch": 2.306,
      "grad_norm": 1.6479134559631348,
      "learning_rate": 0.0002542828258840606,
      "loss": 1.3081,
      "step": 1153
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.9183220267295837,
      "learning_rate": 0.0002528843892702768,
      "loss": 1.346,
      "step": 1154
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.8748323321342468,
      "learning_rate": 0.0002514892518288988,
      "loss": 1.1674,
      "step": 1155
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.7848238945007324,
      "learning_rate": 0.0002500974197206857,
      "loss": 1.1249,
      "step": 1156
    },
    {
      "epoch": 2.314,
      "grad_norm": 0.9999386668205261,
      "learning_rate": 0.00024870889909179927,
      "loss": 1.2195,
      "step": 1157
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.6051103472709656,
      "learning_rate": 0.0002473236960737794,
      "loss": 1.2,
      "step": 1158
    },
    {
      "epoch": 2.318,
      "grad_norm": 1.2164219617843628,
      "learning_rate": 0.0002459418167835159,
      "loss": 1.3409,
      "step": 1159
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.3100993633270264,
      "learning_rate": 0.00024456326732322074,
      "loss": 1.1904,
      "step": 1160
    },
    {
      "epoch": 2.322,
      "grad_norm": 1.7274528741836548,
      "learning_rate": 0.00024318805378040242,
      "loss": 1.2029,
      "step": 1161
    },
    {
      "epoch": 2.324,
      "grad_norm": 16.41606330871582,
      "learning_rate": 0.00024181618222783742,
      "loss": 1.3167,
      "step": 1162
    },
    {
      "epoch": 2.326,
      "grad_norm": 1.058780312538147,
      "learning_rate": 0.00024044765872354524,
      "loss": 1.4933,
      "step": 1163
    },
    {
      "epoch": 2.328,
      "grad_norm": 2.2237493991851807,
      "learning_rate": 0.00023908248931076037,
      "loss": 1.2102,
      "step": 1164
    },
    {
      "epoch": 2.33,
      "grad_norm": 1.9099235534667969,
      "learning_rate": 0.0002377206800179068,
      "loss": 1.1465,
      "step": 1165
    },
    {
      "epoch": 2.332,
      "grad_norm": 1.6235295534133911,
      "learning_rate": 0.00023636223685857005,
      "loss": 1.1739,
      "step": 1166
    },
    {
      "epoch": 2.334,
      "grad_norm": 1.0623067617416382,
      "learning_rate": 0.00023500716583147065,
      "loss": 1.1586,
      "step": 1167
    },
    {
      "epoch": 2.336,
      "grad_norm": 1.1860811710357666,
      "learning_rate": 0.0002336554729204391,
      "loss": 1.0989,
      "step": 1168
    },
    {
      "epoch": 2.338,
      "grad_norm": 0.9525923728942871,
      "learning_rate": 0.0002323071640943879,
      "loss": 1.3638,
      "step": 1169
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.1978726387023926,
      "learning_rate": 0.00023096224530728672,
      "loss": 1.1981,
      "step": 1170
    },
    {
      "epoch": 2.342,
      "grad_norm": 0.895959734916687,
      "learning_rate": 0.00022962072249813482,
      "loss": 1.2132,
      "step": 1171
    },
    {
      "epoch": 2.344,
      "grad_norm": 1.0326093435287476,
      "learning_rate": 0.00022828260159093438,
      "loss": 1.2563,
      "step": 1172
    },
    {
      "epoch": 2.346,
      "grad_norm": 1.0601415634155273,
      "learning_rate": 0.0002269478884946663,
      "loss": 1.19,
      "step": 1173
    },
    {
      "epoch": 2.348,
      "grad_norm": 1.3614258766174316,
      "learning_rate": 0.00022561658910326244,
      "loss": 1.2478,
      "step": 1174
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.7380846738815308,
      "learning_rate": 0.0002242887092955801,
      "loss": 1.1957,
      "step": 1175
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.3279728889465332,
      "learning_rate": 0.0002229642549353772,
      "loss": 1.1717,
      "step": 1176
    },
    {
      "epoch": 2.354,
      "grad_norm": 2.3561320304870605,
      "learning_rate": 0.00022164323187128342,
      "loss": 1.1995,
      "step": 1177
    },
    {
      "epoch": 2.356,
      "grad_norm": 1.1536132097244263,
      "learning_rate": 0.00022032564593677773,
      "loss": 1.2791,
      "step": 1178
    },
    {
      "epoch": 2.358,
      "grad_norm": 0.8926586508750916,
      "learning_rate": 0.0002190115029501606,
      "loss": 1.3008,
      "step": 1179
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.8653767108917236,
      "learning_rate": 0.00021770080871452856,
      "loss": 1.2259,
      "step": 1180
    },
    {
      "epoch": 2.362,
      "grad_norm": 0.7813012003898621,
      "learning_rate": 0.00021639356901774986,
      "loss": 1.1438,
      "step": 1181
    },
    {
      "epoch": 2.364,
      "grad_norm": 1.0221229791641235,
      "learning_rate": 0.0002150897896324373,
      "loss": 1.1518,
      "step": 1182
    },
    {
      "epoch": 2.366,
      "grad_norm": 1.5640069246292114,
      "learning_rate": 0.00021378947631592283,
      "loss": 1.2404,
      "step": 1183
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.1806979179382324,
      "learning_rate": 0.0002124926348102333,
      "loss": 1.2573,
      "step": 1184
    },
    {
      "epoch": 2.37,
      "grad_norm": 1.0675095319747925,
      "learning_rate": 0.0002111992708420646,
      "loss": 1.1921,
      "step": 1185
    },
    {
      "epoch": 2.372,
      "grad_norm": 1.0778300762176514,
      "learning_rate": 0.00020990939012275556,
      "loss": 1.2647,
      "step": 1186
    },
    {
      "epoch": 2.374,
      "grad_norm": 1.5715465545654297,
      "learning_rate": 0.0002086229983482646,
      "loss": 1.2083,
      "step": 1187
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.8666690587997437,
      "learning_rate": 0.00020734010119914192,
      "loss": 1.1344,
      "step": 1188
    },
    {
      "epoch": 2.378,
      "grad_norm": 0.7715266942977905,
      "learning_rate": 0.00020606070434050672,
      "loss": 1.2488,
      "step": 1189
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.51156485080719,
      "learning_rate": 0.00020478481342202126,
      "loss": 1.1775,
      "step": 1190
    },
    {
      "epoch": 2.382,
      "grad_norm": 0.9266802668571472,
      "learning_rate": 0.0002035124340778659,
      "loss": 1.1112,
      "step": 1191
    },
    {
      "epoch": 2.384,
      "grad_norm": 2.1647675037384033,
      "learning_rate": 0.00020224357192671428,
      "loss": 1.1344,
      "step": 1192
    },
    {
      "epoch": 2.386,
      "grad_norm": 1.1963342428207397,
      "learning_rate": 0.00020097823257170868,
      "loss": 1.2974,
      "step": 1193
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.888167679309845,
      "learning_rate": 0.0001997164216004349,
      "loss": 1.1153,
      "step": 1194
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.2145764827728271,
      "learning_rate": 0.00019845814458489808,
      "loss": 1.193,
      "step": 1195
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.7170302271842957,
      "learning_rate": 0.00019720340708149776,
      "loss": 1.1619,
      "step": 1196
    },
    {
      "epoch": 2.394,
      "grad_norm": 0.770683228969574,
      "learning_rate": 0.0001959522146310032,
      "loss": 1.1974,
      "step": 1197
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.5985816717147827,
      "learning_rate": 0.00019470457275852949,
      "loss": 1.2185,
      "step": 1198
    },
    {
      "epoch": 2.398,
      "grad_norm": 0.46223515272140503,
      "learning_rate": 0.0001934604869735126,
      "loss": 1.2039,
      "step": 1199
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.7575311660766602,
      "learning_rate": 0.00019221996276968522,
      "loss": 1.1736,
      "step": 1200
    },
    {
      "epoch": 2.4,
      "eval_loss": 1.194549560546875,
      "eval_runtime": 228.9122,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1200
    },
    {
      "epoch": 2.402,
      "grad_norm": 2.340369701385498,
      "learning_rate": 0.00019098300562505265,
      "loss": 1.2523,
      "step": 1201
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.8958458304405212,
      "learning_rate": 0.00018974962100186833,
      "loss": 1.2081,
      "step": 1202
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.7341375350952148,
      "learning_rate": 0.00018851981434660992,
      "loss": 1.1425,
      "step": 1203
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.8987573385238647,
      "learning_rate": 0.00018729359108995546,
      "loss": 1.3301,
      "step": 1204
    },
    {
      "epoch": 2.41,
      "grad_norm": 1.9877313375473022,
      "learning_rate": 0.00018607095664675865,
      "loss": 1.2504,
      "step": 1205
    },
    {
      "epoch": 2.412,
      "grad_norm": 1.1533316373825073,
      "learning_rate": 0.00018485191641602595,
      "loss": 1.1762,
      "step": 1206
    },
    {
      "epoch": 2.414,
      "grad_norm": 0.9168343544006348,
      "learning_rate": 0.00018363647578089183,
      "loss": 1.2108,
      "step": 1207
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.7675278186798096,
      "learning_rate": 0.0001824246401085955,
      "loss": 1.2572,
      "step": 1208
    },
    {
      "epoch": 2.418,
      "grad_norm": 1.125416874885559,
      "learning_rate": 0.00018121641475045704,
      "loss": 1.3603,
      "step": 1209
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.622018814086914,
      "learning_rate": 0.000180011805041854,
      "loss": 1.1742,
      "step": 1210
    },
    {
      "epoch": 2.422,
      "grad_norm": 0.9470522403717041,
      "learning_rate": 0.00017881081630219742,
      "loss": 1.1954,
      "step": 1211
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.5685069561004639,
      "learning_rate": 0.00017761345383490878,
      "loss": 1.1596,
      "step": 1212
    },
    {
      "epoch": 2.426,
      "grad_norm": 1.3513908386230469,
      "learning_rate": 0.00017641972292739628,
      "loss": 1.1191,
      "step": 1213
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.859259843826294,
      "learning_rate": 0.00017522962885103144,
      "loss": 1.2207,
      "step": 1214
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.6933746337890625,
      "learning_rate": 0.00017404317686112636,
      "loss": 1.1441,
      "step": 1215
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.8926721811294556,
      "learning_rate": 0.00017286037219690976,
      "loss": 1.2511,
      "step": 1216
    },
    {
      "epoch": 2.434,
      "grad_norm": 29.02939796447754,
      "learning_rate": 0.00017168122008150456,
      "loss": 1.2496,
      "step": 1217
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.8222680687904358,
      "learning_rate": 0.0001705057257219036,
      "loss": 1.1759,
      "step": 1218
    },
    {
      "epoch": 2.438,
      "grad_norm": 0.925614058971405,
      "learning_rate": 0.00016933389430894897,
      "loss": 1.229,
      "step": 1219
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.7584233283996582,
      "learning_rate": 0.00016816573101730638,
      "loss": 1.1383,
      "step": 1220
    },
    {
      "epoch": 2.442,
      "grad_norm": 0.648844838142395,
      "learning_rate": 0.00016700124100544413,
      "loss": 1.134,
      "step": 1221
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.8547199964523315,
      "learning_rate": 0.00016584042941560973,
      "loss": 1.1964,
      "step": 1222
    },
    {
      "epoch": 2.446,
      "grad_norm": 1.2093325853347778,
      "learning_rate": 0.00016468330137380693,
      "loss": 1.2303,
      "step": 1223
    },
    {
      "epoch": 2.448,
      "grad_norm": 1.7031357288360596,
      "learning_rate": 0.00016352986198977327,
      "loss": 1.0827,
      "step": 1224
    },
    {
      "epoch": 2.45,
      "grad_norm": 1.1039471626281738,
      "learning_rate": 0.0001623801163569585,
      "loss": 1.1898,
      "step": 1225
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.6647872924804688,
      "learning_rate": 0.0001612340695525004,
      "loss": 1.1611,
      "step": 1226
    },
    {
      "epoch": 2.454,
      "grad_norm": 4.50335168838501,
      "learning_rate": 0.00016009172663720351,
      "loss": 1.2987,
      "step": 1227
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.6854565143585205,
      "learning_rate": 0.00015895309265551638,
      "loss": 1.2032,
      "step": 1228
    },
    {
      "epoch": 2.458,
      "grad_norm": 0.9440429210662842,
      "learning_rate": 0.00015781817263550867,
      "loss": 1.1548,
      "step": 1229
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.8159842491149902,
      "learning_rate": 0.00015668697158885102,
      "loss": 1.1853,
      "step": 1230
    },
    {
      "epoch": 2.462,
      "grad_norm": 0.4479374587535858,
      "learning_rate": 0.00015555949451079054,
      "loss": 1.0976,
      "step": 1231
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.8630422949790955,
      "learning_rate": 0.00015443574638013003,
      "loss": 1.1626,
      "step": 1232
    },
    {
      "epoch": 2.466,
      "grad_norm": 0.6196279525756836,
      "learning_rate": 0.0001533157321592058,
      "loss": 1.1344,
      "step": 1233
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.6253308653831482,
      "learning_rate": 0.00015219945679386505,
      "loss": 1.0728,
      "step": 1234
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.5779390931129456,
      "learning_rate": 0.00015108692521344526,
      "loss": 1.1569,
      "step": 1235
    },
    {
      "epoch": 2.472,
      "grad_norm": 1.565556287765503,
      "learning_rate": 0.00014997814233075202,
      "loss": 1.2026,
      "step": 1236
    },
    {
      "epoch": 2.474,
      "grad_norm": 2.460574150085449,
      "learning_rate": 0.00014887311304203666,
      "loss": 1.1981,
      "step": 1237
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.7537539601325989,
      "learning_rate": 0.00014777184222697538,
      "loss": 1.1608,
      "step": 1238
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 1.20552396774292,
      "learning_rate": 0.00014667433474864678,
      "loss": 1.1489,
      "step": 1239
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.125731110572815,
      "learning_rate": 0.00014558059545351142,
      "loss": 1.2251,
      "step": 1240
    },
    {
      "epoch": 2.482,
      "grad_norm": 1.267386555671692,
      "learning_rate": 0.00014449062917139055,
      "loss": 1.2781,
      "step": 1241
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.6703099012374878,
      "learning_rate": 0.00014340444071544368,
      "loss": 1.1919,
      "step": 1242
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 0.9751828908920288,
      "learning_rate": 0.00014232203488214812,
      "loss": 1.2864,
      "step": 1243
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.8636718988418579,
      "learning_rate": 0.00014124341645127702,
      "loss": 1.1735,
      "step": 1244
    },
    {
      "epoch": 2.49,
      "grad_norm": 6.062546730041504,
      "learning_rate": 0.00014016859018587958,
      "loss": 1.1584,
      "step": 1245
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.698689341545105,
      "learning_rate": 0.00013909756083225843,
      "loss": 1.1289,
      "step": 1246
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 1.3429627418518066,
      "learning_rate": 0.00013803033311995074,
      "loss": 1.1173,
      "step": 1247
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.8070347309112549,
      "learning_rate": 0.00013696691176170507,
      "loss": 1.1749,
      "step": 1248
    },
    {
      "epoch": 2.498,
      "grad_norm": 0.7851744890213013,
      "learning_rate": 0.00013590730145346153,
      "loss": 1.1598,
      "step": 1249
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.1090298891067505,
      "learning_rate": 0.00013485150687433166,
      "loss": 1.2535,
      "step": 1250
    },
    {
      "epoch": 2.502,
      "grad_norm": 0.5813297033309937,
      "learning_rate": 0.00013379953268657696,
      "loss": 1.2851,
      "step": 1251
    },
    {
      "epoch": 2.504,
      "grad_norm": 2.107870578765869,
      "learning_rate": 0.00013275138353558823,
      "loss": 1.2918,
      "step": 1252
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 0.9784218072891235,
      "learning_rate": 0.00013170706404986644,
      "loss": 1.2997,
      "step": 1253
    },
    {
      "epoch": 2.508,
      "grad_norm": 1.3459959030151367,
      "learning_rate": 0.00013066657884099964,
      "loss": 1.1573,
      "step": 1254
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.2722069025039673,
      "learning_rate": 0.0001296299325036454,
      "loss": 1.3228,
      "step": 1255
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.9677184820175171,
      "learning_rate": 0.00012859712961550874,
      "loss": 1.158,
      "step": 1256
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 1.1988757848739624,
      "learning_rate": 0.0001275681747373224,
      "loss": 1.2889,
      "step": 1257
    },
    {
      "epoch": 2.516,
      "grad_norm": 1.0743237733840942,
      "learning_rate": 0.0001265430724128277,
      "loss": 1.1659,
      "step": 1258
    },
    {
      "epoch": 2.518,
      "grad_norm": 1.6720274686813354,
      "learning_rate": 0.00012552182716875227,
      "loss": 1.2034,
      "step": 1259
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.0819056034088135,
      "learning_rate": 0.00012450444351479195,
      "loss": 1.1648,
      "step": 1260
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 1.2290116548538208,
      "learning_rate": 0.00012349092594359036,
      "loss": 1.2257,
      "step": 1261
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.71037757396698,
      "learning_rate": 0.0001224812789307187,
      "loss": 1.1063,
      "step": 1262
    },
    {
      "epoch": 2.526,
      "grad_norm": 0.7787315845489502,
      "learning_rate": 0.00012147550693465636,
      "loss": 1.2588,
      "step": 1263
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.5615307688713074,
      "learning_rate": 0.0001204736143967714,
      "loss": 1.1599,
      "step": 1264
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 1.0727591514587402,
      "learning_rate": 0.00011947560574130011,
      "loss": 1.2504,
      "step": 1265
    },
    {
      "epoch": 2.532,
      "grad_norm": 1.118167757987976,
      "learning_rate": 0.00011848148537532844,
      "loss": 1.1261,
      "step": 1266
    },
    {
      "epoch": 2.534,
      "grad_norm": 0.7857189774513245,
      "learning_rate": 0.000117491257688772,
      "loss": 1.3343,
      "step": 1267
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.8771981000900269,
      "learning_rate": 0.00011650492705435678,
      "loss": 1.2004,
      "step": 1268
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 1.996174931526184,
      "learning_rate": 0.00011552249782759983,
      "loss": 1.171,
      "step": 1269
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.4410886764526367,
      "learning_rate": 0.0001145439743467902,
      "loss": 1.2244,
      "step": 1270
    },
    {
      "epoch": 2.542,
      "grad_norm": 0.5451167821884155,
      "learning_rate": 0.00011356936093296944,
      "loss": 1.3519,
      "step": 1271
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.1310733556747437,
      "learning_rate": 0.00011259866188991275,
      "loss": 1.3504,
      "step": 1272
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 0.6089124083518982,
      "learning_rate": 0.00011163188150411019,
      "loss": 1.0943,
      "step": 1273
    },
    {
      "epoch": 2.548,
      "grad_norm": 1.431448221206665,
      "learning_rate": 0.0001106690240447471,
      "loss": 1.2376,
      "step": 1274
    },
    {
      "epoch": 2.55,
      "grad_norm": 2.38862681388855,
      "learning_rate": 0.00010971009376368612,
      "loss": 1.1949,
      "step": 1275
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.7030392289161682,
      "learning_rate": 0.00010875509489544744,
      "loss": 1.2059,
      "step": 1276
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 1.024477243423462,
      "learning_rate": 0.00010780403165719088,
      "loss": 1.2012,
      "step": 1277
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.9852086305618286,
      "learning_rate": 0.0001068569082486972,
      "loss": 1.1099,
      "step": 1278
    },
    {
      "epoch": 2.558,
      "grad_norm": 1.1219964027404785,
      "learning_rate": 0.00010591372885234885,
      "loss": 1.2262,
      "step": 1279
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.1026920080184937,
      "learning_rate": 0.0001049744976331124,
      "loss": 1.1235,
      "step": 1280
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 0.9077086448669434,
      "learning_rate": 0.00010403921873851951,
      "loss": 1.0908,
      "step": 1281
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.6095170378684998,
      "learning_rate": 0.00010310789629864903,
      "loss": 1.1181,
      "step": 1282
    },
    {
      "epoch": 2.566,
      "grad_norm": 1.296194076538086,
      "learning_rate": 0.00010218053442610842,
      "loss": 1.2078,
      "step": 1283
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.7417304515838623,
      "learning_rate": 0.00010125713721601593,
      "loss": 1.2147,
      "step": 1284
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.5865510106086731,
      "learning_rate": 0.00010033770874598224,
      "loss": 1.204,
      "step": 1285
    },
    {
      "epoch": 2.572,
      "grad_norm": 1.1880731582641602,
      "learning_rate": 9.942225307609243e-05,
      "loss": 1.1858,
      "step": 1286
    },
    {
      "epoch": 2.574,
      "grad_norm": 0.7451639175415039,
      "learning_rate": 9.851077424888844e-05,
      "loss": 1.1187,
      "step": 1287
    },
    {
      "epoch": 2.576,
      "grad_norm": 1.0617671012878418,
      "learning_rate": 9.760327628935073e-05,
      "loss": 1.2949,
      "step": 1288
    },
    {
      "epoch": 2.578,
      "grad_norm": 1.3907320499420166,
      "learning_rate": 9.669976320488083e-05,
      "loss": 1.2577,
      "step": 1289
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.2007839679718018,
      "learning_rate": 9.580023898528345e-05,
      "loss": 1.1211,
      "step": 1290
    },
    {
      "epoch": 2.582,
      "grad_norm": 0.5458554625511169,
      "learning_rate": 9.490470760274917e-05,
      "loss": 1.2962,
      "step": 1291
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.9216965436935425,
      "learning_rate": 9.401317301183654e-05,
      "loss": 1.3402,
      "step": 1292
    },
    {
      "epoch": 2.586,
      "grad_norm": 1.0911626815795898,
      "learning_rate": 9.31256391494546e-05,
      "loss": 1.1126,
      "step": 1293
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.5937939286231995,
      "learning_rate": 9.224210993484605e-05,
      "loss": 1.1431,
      "step": 1294
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.7490249276161194,
      "learning_rate": 9.136258926956886e-05,
      "loss": 1.1252,
      "step": 1295
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.013742446899414,
      "learning_rate": 9.048708103748071e-05,
      "loss": 1.0415,
      "step": 1296
    },
    {
      "epoch": 2.594,
      "grad_norm": 1.856061577796936,
      "learning_rate": 8.961558910472e-05,
      "loss": 1.0978,
      "step": 1297
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.8585401177406311,
      "learning_rate": 8.874811731969023e-05,
      "loss": 1.1062,
      "step": 1298
    },
    {
      "epoch": 2.598,
      "grad_norm": 2.844616651535034,
      "learning_rate": 8.788466951304208e-05,
      "loss": 1.1392,
      "step": 1299
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.9072121977806091,
      "learning_rate": 8.702524949765645e-05,
      "loss": 1.1951,
      "step": 1300
    },
    {
      "epoch": 2.6,
      "eval_loss": 1.1694953441619873,
      "eval_runtime": 228.8019,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 1300
    },
    {
      "epoch": 2.602,
      "grad_norm": 1.4769704341888428,
      "learning_rate": 8.616986106862911e-05,
      "loss": 1.076,
      "step": 1301
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.6576958894729614,
      "learning_rate": 8.531850800325181e-05,
      "loss": 1.1688,
      "step": 1302
    },
    {
      "epoch": 2.606,
      "grad_norm": 1.004211187362671,
      "learning_rate": 8.447119406099702e-05,
      "loss": 1.0932,
      "step": 1303
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.6466251611709595,
      "learning_rate": 8.36279229835012e-05,
      "loss": 1.1715,
      "step": 1304
    },
    {
      "epoch": 2.61,
      "grad_norm": 2.2290782928466797,
      "learning_rate": 8.278869849454718e-05,
      "loss": 1.1155,
      "step": 1305
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.9843249320983887,
      "learning_rate": 8.19535243000491e-05,
      "loss": 1.2098,
      "step": 1306
    },
    {
      "epoch": 2.614,
      "grad_norm": 1.1420575380325317,
      "learning_rate": 8.112240408803583e-05,
      "loss": 1.1696,
      "step": 1307
    },
    {
      "epoch": 2.616,
      "grad_norm": 1.3407143354415894,
      "learning_rate": 8.029534152863383e-05,
      "loss": 1.1886,
      "step": 1308
    },
    {
      "epoch": 2.618,
      "grad_norm": 1.0619971752166748,
      "learning_rate": 7.947234027405159e-05,
      "loss": 1.0815,
      "step": 1309
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.7099370956420898,
      "learning_rate": 7.865340395856324e-05,
      "loss": 1.1569,
      "step": 1310
    },
    {
      "epoch": 2.622,
      "grad_norm": 1.1468371152877808,
      "learning_rate": 7.783853619849279e-05,
      "loss": 1.2434,
      "step": 1311
    },
    {
      "epoch": 2.624,
      "grad_norm": 1.1868855953216553,
      "learning_rate": 7.702774059219786e-05,
      "loss": 1.1602,
      "step": 1312
    },
    {
      "epoch": 2.626,
      "grad_norm": 1.156132459640503,
      "learning_rate": 7.622102072005432e-05,
      "loss": 1.1276,
      "step": 1313
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.8502592444419861,
      "learning_rate": 7.54183801444398e-05,
      "loss": 1.1712,
      "step": 1314
    },
    {
      "epoch": 2.63,
      "grad_norm": 3.326014518737793,
      "learning_rate": 7.461982240971799e-05,
      "loss": 1.3045,
      "step": 1315
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.8292366862297058,
      "learning_rate": 7.382535104222366e-05,
      "loss": 1.1541,
      "step": 1316
    },
    {
      "epoch": 2.634,
      "grad_norm": 0.9914650917053223,
      "learning_rate": 7.303496955024625e-05,
      "loss": 1.164,
      "step": 1317
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.7969197034835815,
      "learning_rate": 7.224868142401542e-05,
      "loss": 1.0908,
      "step": 1318
    },
    {
      "epoch": 2.638,
      "grad_norm": 6.267918586730957,
      "learning_rate": 7.146649013568484e-05,
      "loss": 1.2079,
      "step": 1319
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.4954153299331665,
      "learning_rate": 7.068839913931646e-05,
      "loss": 1.3111,
      "step": 1320
    },
    {
      "epoch": 2.642,
      "grad_norm": 1.8461318016052246,
      "learning_rate": 6.991441187086633e-05,
      "loss": 1.1322,
      "step": 1321
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.9345418214797974,
      "learning_rate": 6.914453174816904e-05,
      "loss": 1.2473,
      "step": 1322
    },
    {
      "epoch": 2.646,
      "grad_norm": 1.4575597047805786,
      "learning_rate": 6.837876217092198e-05,
      "loss": 1.1524,
      "step": 1323
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.8355869650840759,
      "learning_rate": 6.761710652067177e-05,
      "loss": 1.1094,
      "step": 1324
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.9907805323600769,
      "learning_rate": 6.685956816079752e-05,
      "loss": 1.2773,
      "step": 1325
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.6780723333358765,
      "learning_rate": 6.610615043649714e-05,
      "loss": 1.202,
      "step": 1326
    },
    {
      "epoch": 2.654,
      "grad_norm": 1.2528053522109985,
      "learning_rate": 6.535685667477264e-05,
      "loss": 1.1038,
      "step": 1327
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.3824992179870605,
      "learning_rate": 6.46116901844147e-05,
      "loss": 1.0558,
      "step": 1328
    },
    {
      "epoch": 2.658,
      "grad_norm": 1.2255935668945312,
      "learning_rate": 6.387065425598882e-05,
      "loss": 1.2347,
      "step": 1329
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.6575255990028381,
      "learning_rate": 6.313375216182038e-05,
      "loss": 1.152,
      "step": 1330
    },
    {
      "epoch": 2.662,
      "grad_norm": 1.5571699142456055,
      "learning_rate": 6.240098715597975e-05,
      "loss": 1.1597,
      "step": 1331
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.2184648513793945,
      "learning_rate": 6.1672362474269e-05,
      "loss": 1.0664,
      "step": 1332
    },
    {
      "epoch": 2.666,
      "grad_norm": 0.9221116304397583,
      "learning_rate": 6.094788133420681e-05,
      "loss": 1.1845,
      "step": 1333
    },
    {
      "epoch": 2.668,
      "grad_norm": 2.531715154647827,
      "learning_rate": 6.022754693501431e-05,
      "loss": 1.1337,
      "step": 1334
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.8316018581390381,
      "learning_rate": 5.95113624576018e-05,
      "loss": 1.2518,
      "step": 1335
    },
    {
      "epoch": 2.672,
      "grad_norm": 1.0826908349990845,
      "learning_rate": 5.879933106455304e-05,
      "loss": 1.1686,
      "step": 1336
    },
    {
      "epoch": 2.674,
      "grad_norm": 0.7599675059318542,
      "learning_rate": 5.8091455900113e-05,
      "loss": 1.1548,
      "step": 1337
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.6325603723526001,
      "learning_rate": 5.7387740090172894e-05,
      "loss": 1.0962,
      "step": 1338
    },
    {
      "epoch": 2.678,
      "grad_norm": 1.436640739440918,
      "learning_rate": 5.668818674225684e-05,
      "loss": 1.2327,
      "step": 1339
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.2399711608886719,
      "learning_rate": 5.599279894550824e-05,
      "loss": 1.187,
      "step": 1340
    },
    {
      "epoch": 2.682,
      "grad_norm": 0.6330413222312927,
      "learning_rate": 5.530157977067552e-05,
      "loss": 1.162,
      "step": 1341
    },
    {
      "epoch": 2.684,
      "grad_norm": 1.1452242136001587,
      "learning_rate": 5.461453227009916e-05,
      "loss": 1.2129,
      "step": 1342
    },
    {
      "epoch": 2.686,
      "grad_norm": 0.6809900999069214,
      "learning_rate": 5.393165947769818e-05,
      "loss": 1.136,
      "step": 1343
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.8221299648284912,
      "learning_rate": 5.3252964408956216e-05,
      "loss": 1.1498,
      "step": 1344
    },
    {
      "epoch": 2.69,
      "grad_norm": 1.3665423393249512,
      "learning_rate": 5.257845006090911e-05,
      "loss": 1.203,
      "step": 1345
    },
    {
      "epoch": 2.692,
      "grad_norm": 1.6640723943710327,
      "learning_rate": 5.1908119412130586e-05,
      "loss": 1.2439,
      "step": 1346
    },
    {
      "epoch": 2.694,
      "grad_norm": 3.4374876022338867,
      "learning_rate": 5.124197542272002e-05,
      "loss": 1.1002,
      "step": 1347
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.6711872816085815,
      "learning_rate": 5.058002103428905e-05,
      "loss": 1.208,
      "step": 1348
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.9059156775474548,
      "learning_rate": 4.992225916994819e-05,
      "loss": 1.1195,
      "step": 1349
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.8150299787521362,
      "learning_rate": 4.9268692734294464e-05,
      "loss": 1.2466,
      "step": 1350
    },
    {
      "epoch": 2.702,
      "grad_norm": 0.8379353880882263,
      "learning_rate": 4.8619324613398576e-05,
      "loss": 1.1719,
      "step": 1351
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.0148833990097046,
      "learning_rate": 4.797415767479174e-05,
      "loss": 1.098,
      "step": 1352
    },
    {
      "epoch": 2.706,
      "grad_norm": 1.976536512374878,
      "learning_rate": 4.733319476745335e-05,
      "loss": 1.107,
      "step": 1353
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.6910558342933655,
      "learning_rate": 4.6696438721798184e-05,
      "loss": 1.0683,
      "step": 1354
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.5749010443687439,
      "learning_rate": 4.6063892349664236e-05,
      "loss": 1.037,
      "step": 1355
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.7348378896713257,
      "learning_rate": 4.543555844429992e-05,
      "loss": 1.1438,
      "step": 1356
    },
    {
      "epoch": 2.714,
      "grad_norm": 0.5497878789901733,
      "learning_rate": 4.481143978035196e-05,
      "loss": 1.0427,
      "step": 1357
    },
    {
      "epoch": 2.716,
      "grad_norm": 1.050514817237854,
      "learning_rate": 4.41915391138531e-05,
      "loss": 1.2128,
      "step": 1358
    },
    {
      "epoch": 2.718,
      "grad_norm": 0.7912823557853699,
      "learning_rate": 4.357585918220986e-05,
      "loss": 1.1452,
      "step": 1359
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.7404230237007141,
      "learning_rate": 4.2964402704190555e-05,
      "loss": 1.0887,
      "step": 1360
    },
    {
      "epoch": 2.722,
      "grad_norm": 1.248901605606079,
      "learning_rate": 4.235717237991321e-05,
      "loss": 1.1772,
      "step": 1361
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.5380420088768005,
      "learning_rate": 4.175417089083378e-05,
      "loss": 1.1233,
      "step": 1362
    },
    {
      "epoch": 2.726,
      "grad_norm": 1.2176626920700073,
      "learning_rate": 4.115540089973402e-05,
      "loss": 1.2243,
      "step": 1363
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.5942589640617371,
      "learning_rate": 4.0560865050709997e-05,
      "loss": 1.2372,
      "step": 1364
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.3833152055740356,
      "learning_rate": 3.997056596916038e-05,
      "loss": 1.1564,
      "step": 1365
    },
    {
      "epoch": 2.732,
      "grad_norm": 1.006684422492981,
      "learning_rate": 3.9384506261774365e-05,
      "loss": 1.1664,
      "step": 1366
    },
    {
      "epoch": 2.734,
      "grad_norm": 0.6274227499961853,
      "learning_rate": 3.8802688516521356e-05,
      "loss": 1.1101,
      "step": 1367
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.8958070874214172,
      "learning_rate": 3.822511530263806e-05,
      "loss": 1.1547,
      "step": 1368
    },
    {
      "epoch": 2.738,
      "grad_norm": 1.6800442934036255,
      "learning_rate": 3.765178917061818e-05,
      "loss": 1.1318,
      "step": 1369
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.835320770740509,
      "learning_rate": 3.7082712652200864e-05,
      "loss": 1.07,
      "step": 1370
    },
    {
      "epoch": 2.742,
      "grad_norm": 0.7992783188819885,
      "learning_rate": 3.651788826035895e-05,
      "loss": 1.171,
      "step": 1371
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.8570391535758972,
      "learning_rate": 3.595731848928929e-05,
      "loss": 1.2374,
      "step": 1372
    },
    {
      "epoch": 2.746,
      "grad_norm": 0.7907513380050659,
      "learning_rate": 3.54010058144002e-05,
      "loss": 1.1599,
      "step": 1373
    },
    {
      "epoch": 2.748,
      "grad_norm": 1.5618098974227905,
      "learning_rate": 3.484895269230137e-05,
      "loss": 1.1437,
      "step": 1374
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.86674964427948,
      "learning_rate": 3.430116156079277e-05,
      "loss": 1.2415,
      "step": 1375
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.6564878821372986,
      "learning_rate": 3.375763483885386e-05,
      "loss": 1.1287,
      "step": 1376
    },
    {
      "epoch": 2.754,
      "grad_norm": 0.744292140007019,
      "learning_rate": 3.321837492663304e-05,
      "loss": 1.2203,
      "step": 1377
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 1.8992972373962402,
      "learning_rate": 3.268338420543726e-05,
      "loss": 1.1842,
      "step": 1378
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.9391039609909058,
      "learning_rate": 3.215266503772085e-05,
      "loss": 1.2196,
      "step": 1379
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.9436885118484497,
      "learning_rate": 3.162621976707558e-05,
      "loss": 1.1597,
      "step": 1380
    },
    {
      "epoch": 2.762,
      "grad_norm": 0.9579997062683105,
      "learning_rate": 3.1104050718220536e-05,
      "loss": 1.1682,
      "step": 1381
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.819248616695404,
      "learning_rate": 3.0586160196990894e-05,
      "loss": 1.1388,
      "step": 1382
    },
    {
      "epoch": 2.766,
      "grad_norm": 1.0031737089157104,
      "learning_rate": 3.0072550490328753e-05,
      "loss": 1.2027,
      "step": 1383
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.8007104396820068,
      "learning_rate": 2.9563223866272858e-05,
      "loss": 1.0349,
      "step": 1384
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.8350557088851929,
      "learning_rate": 2.905818257394799e-05,
      "loss": 1.1658,
      "step": 1385
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.7059655785560608,
      "learning_rate": 2.855742884355561e-05,
      "loss": 1.1704,
      "step": 1386
    },
    {
      "epoch": 2.774,
      "grad_norm": 0.745569109916687,
      "learning_rate": 2.806096488636367e-05,
      "loss": 1.1242,
      "step": 1387
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.737017035484314,
      "learning_rate": 2.756879289469716e-05,
      "loss": 1.1425,
      "step": 1388
    },
    {
      "epoch": 2.778,
      "grad_norm": 1.0294549465179443,
      "learning_rate": 2.7080915041928332e-05,
      "loss": 1.1602,
      "step": 1389
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.783542275428772,
      "learning_rate": 2.6597333482466847e-05,
      "loss": 1.1624,
      "step": 1390
    },
    {
      "epoch": 2.782,
      "grad_norm": 1.0813747644424438,
      "learning_rate": 2.6118050351750635e-05,
      "loss": 1.193,
      "step": 1391
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.9776589870452881,
      "learning_rate": 2.564306776623593e-05,
      "loss": 1.1478,
      "step": 1392
    },
    {
      "epoch": 2.786,
      "grad_norm": 0.7292658090591431,
      "learning_rate": 2.5172387823388708e-05,
      "loss": 1.2287,
      "step": 1393
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 1.4828966856002808,
      "learning_rate": 2.470601260167471e-05,
      "loss": 1.1607,
      "step": 1394
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.5667781829833984,
      "learning_rate": 2.424394416055076e-05,
      "loss": 1.2155,
      "step": 1395
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.6823787093162537,
      "learning_rate": 2.3786184540455446e-05,
      "loss": 1.2241,
      "step": 1396
    },
    {
      "epoch": 2.794,
      "grad_norm": 0.7523752450942993,
      "learning_rate": 2.3332735762799817e-05,
      "loss": 1.1011,
      "step": 1397
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 1.2191928625106812,
      "learning_rate": 2.2883599829959134e-05,
      "loss": 1.1675,
      "step": 1398
    },
    {
      "epoch": 2.798,
      "grad_norm": 1.0180639028549194,
      "learning_rate": 2.2438778725263232e-05,
      "loss": 1.0716,
      "step": 1399
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.7864862680435181,
      "learning_rate": 2.1998274412988628e-05,
      "loss": 1.1271,
      "step": 1400
    },
    {
      "epoch": 2.8,
      "eval_loss": 1.1610379219055176,
      "eval_runtime": 223.9142,
      "eval_samples_per_second": 0.447,
      "eval_steps_per_second": 0.447,
      "step": 1400
    },
    {
      "epoch": 2.802,
      "grad_norm": 2.5688652992248535,
      "learning_rate": 2.1562088838349425e-05,
      "loss": 1.1633,
      "step": 1401
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 1.2911415100097656,
      "learning_rate": 2.11302239274882e-05,
      "loss": 1.1148,
      "step": 1402
    },
    {
      "epoch": 2.806,
      "grad_norm": 0.8174794912338257,
      "learning_rate": 2.0702681587468353e-05,
      "loss": 1.1953,
      "step": 1403
    },
    {
      "epoch": 2.808,
      "grad_norm": 1.8233996629714966,
      "learning_rate": 2.027946370626532e-05,
      "loss": 1.2738,
      "step": 1404
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.6656189560890198,
      "learning_rate": 1.986057215275816e-05,
      "loss": 1.1692,
      "step": 1405
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.8911401629447937,
      "learning_rate": 1.9446008776721645e-05,
      "loss": 1.0925,
      "step": 1406
    },
    {
      "epoch": 2.814,
      "grad_norm": 0.8339460492134094,
      "learning_rate": 1.9035775408817403e-05,
      "loss": 1.1008,
      "step": 1407
    },
    {
      "epoch": 2.816,
      "grad_norm": 1.2991700172424316,
      "learning_rate": 1.8629873860586568e-05,
      "loss": 1.2658,
      "step": 1408
    },
    {
      "epoch": 2.818,
      "grad_norm": 2.8445186614990234,
      "learning_rate": 1.822830592444147e-05,
      "loss": 1.1376,
      "step": 1409
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.6518149375915527,
      "learning_rate": 1.7831073373657525e-05,
      "loss": 1.1286,
      "step": 1410
    },
    {
      "epoch": 2.822,
      "grad_norm": 1.2559632062911987,
      "learning_rate": 1.74381779623658e-05,
      "loss": 1.0833,
      "step": 1411
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.6040147542953491,
      "learning_rate": 1.7049621425545115e-05,
      "loss": 1.1359,
      "step": 1412
    },
    {
      "epoch": 2.826,
      "grad_norm": 0.8154429197311401,
      "learning_rate": 1.6665405479014294e-05,
      "loss": 1.1746,
      "step": 1413
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.7796658873558044,
      "learning_rate": 1.6285531819424494e-05,
      "loss": 1.0685,
      "step": 1414
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.6060484051704407,
      "learning_rate": 1.5910002124251975e-05,
      "loss": 1.2162,
      "step": 1415
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.607424259185791,
      "learning_rate": 1.5538818051790583e-05,
      "loss": 1.1406,
      "step": 1416
    },
    {
      "epoch": 2.834,
      "grad_norm": 0.9277681708335876,
      "learning_rate": 1.5171981241144494e-05,
      "loss": 1.2053,
      "step": 1417
    },
    {
      "epoch": 2.836,
      "grad_norm": 1.2125309705734253,
      "learning_rate": 1.480949331222059e-05,
      "loss": 1.1157,
      "step": 1418
    },
    {
      "epoch": 2.838,
      "grad_norm": 0.7117336988449097,
      "learning_rate": 1.4451355865722105e-05,
      "loss": 1.0427,
      "step": 1419
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.6711523532867432,
      "learning_rate": 1.4097570483140642e-05,
      "loss": 1.22,
      "step": 1420
    },
    {
      "epoch": 2.842,
      "grad_norm": 0.7561403512954712,
      "learning_rate": 1.3748138726749737e-05,
      "loss": 1.1805,
      "step": 1421
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.7788792252540588,
      "learning_rate": 1.3403062139598077e-05,
      "loss": 1.1433,
      "step": 1422
    },
    {
      "epoch": 2.846,
      "grad_norm": 0.5204653739929199,
      "learning_rate": 1.3062342245501958e-05,
      "loss": 1.0782,
      "step": 1423
    },
    {
      "epoch": 2.848,
      "grad_norm": 2.019362688064575,
      "learning_rate": 1.2725980549039506e-05,
      "loss": 1.1485,
      "step": 1424
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.9201071858406067,
      "learning_rate": 1.239397853554336e-05,
      "loss": 1.1673,
      "step": 1425
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.9530510306358337,
      "learning_rate": 1.206633767109444e-05,
      "loss": 1.118,
      "step": 1426
    },
    {
      "epoch": 2.854,
      "grad_norm": 0.6416634321212769,
      "learning_rate": 1.1743059402515077e-05,
      "loss": 1.0943,
      "step": 1427
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.49372297525405884,
      "learning_rate": 1.142414515736323e-05,
      "loss": 1.0853,
      "step": 1428
    },
    {
      "epoch": 2.858,
      "grad_norm": 0.5072737336158752,
      "learning_rate": 1.1109596343925721e-05,
      "loss": 1.0929,
      "step": 1429
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.2669074535369873,
      "learning_rate": 1.0799414351212234e-05,
      "loss": 1.2154,
      "step": 1430
    },
    {
      "epoch": 2.862,
      "grad_norm": 0.8839138150215149,
      "learning_rate": 1.0493600548948879e-05,
      "loss": 1.1126,
      "step": 1431
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.4417452812194824,
      "learning_rate": 1.019215628757264e-05,
      "loss": 1.2775,
      "step": 1432
    },
    {
      "epoch": 2.866,
      "grad_norm": 1.3309309482574463,
      "learning_rate": 9.895082898224939e-06,
      "loss": 1.1456,
      "step": 1433
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.7321348190307617,
      "learning_rate": 9.602381692746077e-06,
      "loss": 1.1673,
      "step": 1434
    },
    {
      "epoch": 2.87,
      "grad_norm": 5.895889759063721,
      "learning_rate": 9.314053963669244e-06,
      "loss": 1.125,
      "step": 1435
    },
    {
      "epoch": 2.872,
      "grad_norm": 2.055471420288086,
      "learning_rate": 9.030100984214861e-06,
      "loss": 1.1266,
      "step": 1436
    },
    {
      "epoch": 2.874,
      "grad_norm": 1.0695024728775024,
      "learning_rate": 8.750524008285132e-06,
      "loss": 1.2282,
      "step": 1437
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.8988561630249023,
      "learning_rate": 8.475324270458163e-06,
      "loss": 1.2413,
      "step": 1438
    },
    {
      "epoch": 2.878,
      "grad_norm": 1.0401649475097656,
      "learning_rate": 8.204502985982854e-06,
      "loss": 1.256,
      "step": 1439
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.081560730934143,
      "learning_rate": 7.93806135077324e-06,
      "loss": 1.231,
      "step": 1440
    },
    {
      "epoch": 2.882,
      "grad_norm": 0.9232778549194336,
      "learning_rate": 7.676000541403494e-06,
      "loss": 1.2558,
      "step": 1441
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.6926286816596985,
      "learning_rate": 7.418321715102705e-06,
      "loss": 1.0527,
      "step": 1442
    },
    {
      "epoch": 2.886,
      "grad_norm": 0.7142324447631836,
      "learning_rate": 7.165026009749109e-06,
      "loss": 1.1729,
      "step": 1443
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.6389784216880798,
      "learning_rate": 6.9161145438662035e-06,
      "loss": 1.1365,
      "step": 1444
    },
    {
      "epoch": 2.89,
      "grad_norm": 1.0790841579437256,
      "learning_rate": 6.6715884166170806e-06,
      "loss": 1.1468,
      "step": 1445
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.8875596523284912,
      "learning_rate": 6.431448707799436e-06,
      "loss": 1.1949,
      "step": 1446
    },
    {
      "epoch": 2.894,
      "grad_norm": 0.8199006915092468,
      "learning_rate": 6.195696477841462e-06,
      "loss": 1.1639,
      "step": 1447
    },
    {
      "epoch": 2.896,
      "grad_norm": 1.0412195920944214,
      "learning_rate": 5.964332767796399e-06,
      "loss": 1.1549,
      "step": 1448
    },
    {
      "epoch": 2.898,
      "grad_norm": 1.182554841041565,
      "learning_rate": 5.737358599338438e-06,
      "loss": 1.1761,
      "step": 1449
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.5320535898208618,
      "learning_rate": 5.514774974758274e-06,
      "loss": 1.2214,
      "step": 1450
    },
    {
      "epoch": 2.902,
      "grad_norm": 0.732118546962738,
      "learning_rate": 5.2965828769582225e-06,
      "loss": 1.1452,
      "step": 1451
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.6332452297210693,
      "learning_rate": 5.082783269448443e-06,
      "loss": 1.1254,
      "step": 1452
    },
    {
      "epoch": 2.906,
      "grad_norm": 0.7130746841430664,
      "learning_rate": 4.873377096342058e-06,
      "loss": 1.1247,
      "step": 1453
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.6948849558830261,
      "learning_rate": 4.668365282351372e-06,
      "loss": 1.1196,
      "step": 1454
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.5575879216194153,
      "learning_rate": 4.467748732783994e-06,
      "loss": 1.1283,
      "step": 1455
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.8460903763771057,
      "learning_rate": 4.271528333538388e-06,
      "loss": 1.1877,
      "step": 1456
    },
    {
      "epoch": 2.914,
      "grad_norm": 0.9577768445014954,
      "learning_rate": 4.079704951100105e-06,
      "loss": 1.0543,
      "step": 1457
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.7064356803894043,
      "learning_rate": 3.892279432538115e-06,
      "loss": 1.1348,
      "step": 1458
    },
    {
      "epoch": 2.918,
      "grad_norm": 0.7258855104446411,
      "learning_rate": 3.7092526055008126e-06,
      "loss": 1.1667,
      "step": 1459
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.7106546759605408,
      "learning_rate": 3.5306252782126846e-06,
      "loss": 1.1296,
      "step": 1460
    },
    {
      "epoch": 2.922,
      "grad_norm": 1.5663115978240967,
      "learning_rate": 3.3563982394704262e-06,
      "loss": 1.1795,
      "step": 1461
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.9657493233680725,
      "learning_rate": 3.1865722586397196e-06,
      "loss": 1.2408,
      "step": 1462
    },
    {
      "epoch": 2.926,
      "grad_norm": 0.6003562211990356,
      "learning_rate": 3.0211480856513484e-06,
      "loss": 1.1528,
      "step": 1463
    },
    {
      "epoch": 2.928,
      "grad_norm": 1.1179860830307007,
      "learning_rate": 2.860126450998646e-06,
      "loss": 1.2068,
      "step": 1464
    },
    {
      "epoch": 2.93,
      "grad_norm": 1.0323444604873657,
      "learning_rate": 2.7035080657338287e-06,
      "loss": 1.1822,
      "step": 1465
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.5777507424354553,
      "learning_rate": 2.5512936214646675e-06,
      "loss": 1.1221,
      "step": 1466
    },
    {
      "epoch": 2.934,
      "grad_norm": 0.854200005531311,
      "learning_rate": 2.403483790351824e-06,
      "loss": 1.1476,
      "step": 1467
    },
    {
      "epoch": 2.936,
      "grad_norm": 1.024603009223938,
      "learning_rate": 2.260079225105627e-06,
      "loss": 1.1415,
      "step": 1468
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 0.7916037440299988,
      "learning_rate": 2.1210805589834128e-06,
      "loss": 1.1817,
      "step": 1469
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.7348964214324951,
      "learning_rate": 1.986488405786524e-06,
      "loss": 1.2178,
      "step": 1470
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.6546357870101929,
      "learning_rate": 1.8563033598575363e-06,
      "loss": 1.223,
      "step": 1471
    },
    {
      "epoch": 2.944,
      "grad_norm": 3.553351640701294,
      "learning_rate": 1.7305259960781471e-06,
      "loss": 1.1742,
      "step": 1472
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 1.0078611373901367,
      "learning_rate": 1.6091568698658465e-06,
      "loss": 1.2014,
      "step": 1473
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.7355649471282959,
      "learning_rate": 1.4921965171720286e-06,
      "loss": 1.1466,
      "step": 1474
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.846461832523346,
      "learning_rate": 1.379645454479661e-06,
      "loss": 1.1856,
      "step": 1475
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.7628452777862549,
      "learning_rate": 1.2715041788003978e-06,
      "loss": 1.1743,
      "step": 1476
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 0.649842381477356,
      "learning_rate": 1.1677731676733582e-06,
      "loss": 1.1431,
      "step": 1477
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.8774417042732239,
      "learning_rate": 1.0684528791621296e-06,
      "loss": 1.0199,
      "step": 1478
    },
    {
      "epoch": 2.958,
      "grad_norm": 2.2225441932678223,
      "learning_rate": 9.735437518528788e-07,
      "loss": 1.1591,
      "step": 1479
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.0392374992370605,
      "learning_rate": 8.830462048531329e-07,
      "loss": 1.248,
      "step": 1480
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 0.919577956199646,
      "learning_rate": 7.969606377890015e-07,
      "loss": 1.1361,
      "step": 1481
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.735273003578186,
      "learning_rate": 7.15287430803957e-07,
      "loss": 1.0976,
      "step": 1482
    },
    {
      "epoch": 2.966,
      "grad_norm": 0.6329562067985535,
      "learning_rate": 6.380269445571685e-07,
      "loss": 1.1432,
      "step": 1483
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.9103863835334778,
      "learning_rate": 5.651795202213927e-07,
      "loss": 1.1865,
      "step": 1484
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.6425986886024475,
      "learning_rate": 4.967454794823078e-07,
      "loss": 1.1343,
      "step": 1485
    },
    {
      "epoch": 2.972,
      "grad_norm": 1.6602259874343872,
      "learning_rate": 4.3272512453618184e-07,
      "loss": 1.1156,
      "step": 1486
    },
    {
      "epoch": 2.974,
      "grad_norm": 0.9613580703735352,
      "learning_rate": 3.7311873808931753e-07,
      "loss": 1.1746,
      "step": 1487
    },
    {
      "epoch": 2.976,
      "grad_norm": 1.0959935188293457,
      "learning_rate": 3.179265833562761e-07,
      "loss": 1.141,
      "step": 1488
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 0.7517024874687195,
      "learning_rate": 2.671489040589892e-07,
      "loss": 1.1198,
      "step": 1489
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.7404323816299438,
      "learning_rate": 2.2078592442553725e-07,
      "loss": 1.0939,
      "step": 1490
    },
    {
      "epoch": 2.982,
      "grad_norm": 1.3180049657821655,
      "learning_rate": 1.788378491891507e-07,
      "loss": 1.3143,
      "step": 1491
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.7441008687019348,
      "learning_rate": 1.413048635876546e-07,
      "loss": 1.2151,
      "step": 1492
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 0.5124862790107727,
      "learning_rate": 1.0818713336202546e-07,
      "loss": 1.1712,
      "step": 1493
    },
    {
      "epoch": 2.988,
      "grad_norm": 1.3090676069259644,
      "learning_rate": 7.948480475616915e-08,
      "loss": 1.2475,
      "step": 1494
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.714987576007843,
      "learning_rate": 5.5198004516254786e-08,
      "loss": 1.1672,
      "step": 1495
    },
    {
      "epoch": 2.992,
      "grad_norm": 2.2725274562835693,
      "learning_rate": 3.532683988971552e-08,
      "loss": 1.1098,
      "step": 1496
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 0.7502026557922363,
      "learning_rate": 1.987139862524856e-08,
      "loss": 1.1846,
      "step": 1497
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.8905120491981506,
      "learning_rate": 8.831748972371045e-09,
      "loss": 1.1747,
      "step": 1498
    },
    {
      "epoch": 2.998,
      "grad_norm": 0.8033991456031799,
      "learning_rate": 2.2079396805319007e-09,
      "loss": 1.0997,
      "step": 1499
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.6741493940353394,
      "learning_rate": 0.0,
      "loss": 1.1053,
      "step": 1500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.1555155515670776,
      "eval_runtime": 224.04,
      "eval_samples_per_second": 0.446,
      "eval_steps_per_second": 0.446,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.5121226558478746e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
