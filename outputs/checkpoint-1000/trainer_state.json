{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.29411764705882354,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002941176470588235,
      "grad_norm": 1.9017901420593262,
      "learning_rate": 4e-05,
      "loss": 1.8347,
      "step": 1
    },
    {
      "epoch": 0.000588235294117647,
      "grad_norm": 1.8029898405075073,
      "learning_rate": 8e-05,
      "loss": 1.9589,
      "step": 2
    },
    {
      "epoch": 0.0008823529411764706,
      "grad_norm": 1.828238844871521,
      "learning_rate": 0.00012,
      "loss": 1.8937,
      "step": 3
    },
    {
      "epoch": 0.001176470588235294,
      "grad_norm": 1.7710850238800049,
      "learning_rate": 0.00016,
      "loss": 1.9004,
      "step": 4
    },
    {
      "epoch": 0.0014705882352941176,
      "grad_norm": 1.2679718732833862,
      "learning_rate": 0.0002,
      "loss": 1.4987,
      "step": 5
    },
    {
      "epoch": 0.0017647058823529412,
      "grad_norm": 2.1268582344055176,
      "learning_rate": 0.00019994108983799707,
      "loss": 1.1144,
      "step": 6
    },
    {
      "epoch": 0.002058823529411765,
      "grad_norm": 1.4259804487228394,
      "learning_rate": 0.00019988217967599413,
      "loss": 1.0524,
      "step": 7
    },
    {
      "epoch": 0.002352941176470588,
      "grad_norm": 1.2433640956878662,
      "learning_rate": 0.00019982326951399116,
      "loss": 1.0174,
      "step": 8
    },
    {
      "epoch": 0.0026470588235294116,
      "grad_norm": 1.1211179494857788,
      "learning_rate": 0.00019976435935198822,
      "loss": 0.9121,
      "step": 9
    },
    {
      "epoch": 0.0029411764705882353,
      "grad_norm": 1.2409859895706177,
      "learning_rate": 0.00019970544918998528,
      "loss": 0.8452,
      "step": 10
    },
    {
      "epoch": 0.003235294117647059,
      "grad_norm": 1.1146550178527832,
      "learning_rate": 0.00019964653902798234,
      "loss": 0.651,
      "step": 11
    },
    {
      "epoch": 0.0035294117647058825,
      "grad_norm": 0.5345523357391357,
      "learning_rate": 0.0001995876288659794,
      "loss": 0.6296,
      "step": 12
    },
    {
      "epoch": 0.003823529411764706,
      "grad_norm": 0.4835131764411926,
      "learning_rate": 0.00019952871870397644,
      "loss": 0.7068,
      "step": 13
    },
    {
      "epoch": 0.00411764705882353,
      "grad_norm": 0.35805419087409973,
      "learning_rate": 0.0001994698085419735,
      "loss": 0.5234,
      "step": 14
    },
    {
      "epoch": 0.004411764705882353,
      "grad_norm": 0.46082064509391785,
      "learning_rate": 0.00019941089837997056,
      "loss": 0.5471,
      "step": 15
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 0.35709115862846375,
      "learning_rate": 0.00019935198821796762,
      "loss": 0.5731,
      "step": 16
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.3679710328578949,
      "learning_rate": 0.00019929307805596468,
      "loss": 0.6501,
      "step": 17
    },
    {
      "epoch": 0.005294117647058823,
      "grad_norm": 0.41414979100227356,
      "learning_rate": 0.0001992341678939617,
      "loss": 0.6627,
      "step": 18
    },
    {
      "epoch": 0.005588235294117647,
      "grad_norm": 0.21425144374370575,
      "learning_rate": 0.00019917525773195877,
      "loss": 0.4337,
      "step": 19
    },
    {
      "epoch": 0.0058823529411764705,
      "grad_norm": 0.2038000077009201,
      "learning_rate": 0.00019911634756995583,
      "loss": 0.5012,
      "step": 20
    },
    {
      "epoch": 0.006176470588235294,
      "grad_norm": 0.2082575261592865,
      "learning_rate": 0.0001990574374079529,
      "loss": 0.5084,
      "step": 21
    },
    {
      "epoch": 0.006470588235294118,
      "grad_norm": 0.17989258468151093,
      "learning_rate": 0.00019899852724594995,
      "loss": 0.4244,
      "step": 22
    },
    {
      "epoch": 0.006764705882352941,
      "grad_norm": 0.20391178131103516,
      "learning_rate": 0.00019893961708394698,
      "loss": 0.5454,
      "step": 23
    },
    {
      "epoch": 0.007058823529411765,
      "grad_norm": 0.1495966911315918,
      "learning_rate": 0.00019888070692194404,
      "loss": 0.4167,
      "step": 24
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 0.2706458270549774,
      "learning_rate": 0.0001988217967599411,
      "loss": 0.4934,
      "step": 25
    },
    {
      "epoch": 0.007647058823529412,
      "grad_norm": 0.20677486062049866,
      "learning_rate": 0.00019876288659793816,
      "loss": 0.5307,
      "step": 26
    },
    {
      "epoch": 0.007941176470588234,
      "grad_norm": 0.1531270444393158,
      "learning_rate": 0.00019870397643593522,
      "loss": 0.4436,
      "step": 27
    },
    {
      "epoch": 0.00823529411764706,
      "grad_norm": 0.12485053390264511,
      "learning_rate": 0.00019864506627393226,
      "loss": 0.432,
      "step": 28
    },
    {
      "epoch": 0.008529411764705883,
      "grad_norm": 0.17757835984230042,
      "learning_rate": 0.00019858615611192932,
      "loss": 0.4469,
      "step": 29
    },
    {
      "epoch": 0.008823529411764706,
      "grad_norm": 0.1437951624393463,
      "learning_rate": 0.00019852724594992638,
      "loss": 0.348,
      "step": 30
    },
    {
      "epoch": 0.009117647058823529,
      "grad_norm": 0.15121173858642578,
      "learning_rate": 0.00019846833578792344,
      "loss": 0.3762,
      "step": 31
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 0.12074103206396103,
      "learning_rate": 0.0001984094256259205,
      "loss": 0.3952,
      "step": 32
    },
    {
      "epoch": 0.009705882352941177,
      "grad_norm": 0.14854411780834198,
      "learning_rate": 0.00019835051546391753,
      "loss": 0.4861,
      "step": 33
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1415865570306778,
      "learning_rate": 0.0001982916053019146,
      "loss": 0.4645,
      "step": 34
    },
    {
      "epoch": 0.010294117647058823,
      "grad_norm": 0.09063605219125748,
      "learning_rate": 0.00019823269513991165,
      "loss": 0.3487,
      "step": 35
    },
    {
      "epoch": 0.010588235294117647,
      "grad_norm": 0.08852742612361908,
      "learning_rate": 0.0001981737849779087,
      "loss": 0.4835,
      "step": 36
    },
    {
      "epoch": 0.01088235294117647,
      "grad_norm": 0.11155688762664795,
      "learning_rate": 0.00019811487481590577,
      "loss": 0.3558,
      "step": 37
    },
    {
      "epoch": 0.011176470588235295,
      "grad_norm": 0.1143687292933464,
      "learning_rate": 0.0001980559646539028,
      "loss": 0.4766,
      "step": 38
    },
    {
      "epoch": 0.011470588235294118,
      "grad_norm": 0.07191305607557297,
      "learning_rate": 0.00019799705449189987,
      "loss": 0.2243,
      "step": 39
    },
    {
      "epoch": 0.011764705882352941,
      "grad_norm": 0.0937204584479332,
      "learning_rate": 0.00019793814432989693,
      "loss": 0.3691,
      "step": 40
    },
    {
      "epoch": 0.012058823529411764,
      "grad_norm": 0.11187702417373657,
      "learning_rate": 0.00019787923416789399,
      "loss": 0.4092,
      "step": 41
    },
    {
      "epoch": 0.012352941176470587,
      "grad_norm": 0.07088126987218857,
      "learning_rate": 0.00019782032400589105,
      "loss": 0.3516,
      "step": 42
    },
    {
      "epoch": 0.012647058823529412,
      "grad_norm": 0.09653101861476898,
      "learning_rate": 0.00019776141384388808,
      "loss": 0.4344,
      "step": 43
    },
    {
      "epoch": 0.012941176470588235,
      "grad_norm": 0.0715731680393219,
      "learning_rate": 0.00019770250368188514,
      "loss": 0.3558,
      "step": 44
    },
    {
      "epoch": 0.013235294117647059,
      "grad_norm": 0.08587374538183212,
      "learning_rate": 0.0001976435935198822,
      "loss": 0.4738,
      "step": 45
    },
    {
      "epoch": 0.013529411764705882,
      "grad_norm": 0.07828318327665329,
      "learning_rate": 0.00019758468335787926,
      "loss": 0.3747,
      "step": 46
    },
    {
      "epoch": 0.013823529411764707,
      "grad_norm": 0.08546391129493713,
      "learning_rate": 0.00019752577319587632,
      "loss": 0.4248,
      "step": 47
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 0.08674682676792145,
      "learning_rate": 0.00019746686303387335,
      "loss": 0.4896,
      "step": 48
    },
    {
      "epoch": 0.014411764705882353,
      "grad_norm": 0.07425059378147125,
      "learning_rate": 0.0001974079528718704,
      "loss": 0.4127,
      "step": 49
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 0.08759815245866776,
      "learning_rate": 0.00019734904270986747,
      "loss": 0.4475,
      "step": 50
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.06706732511520386,
      "learning_rate": 0.00019729013254786453,
      "loss": 0.3203,
      "step": 51
    },
    {
      "epoch": 0.015294117647058824,
      "grad_norm": 0.08208151161670685,
      "learning_rate": 0.0001972312223858616,
      "loss": 0.356,
      "step": 52
    },
    {
      "epoch": 0.015588235294117648,
      "grad_norm": 0.09781312942504883,
      "learning_rate": 0.00019717231222385863,
      "loss": 0.4132,
      "step": 53
    },
    {
      "epoch": 0.01588235294117647,
      "grad_norm": 0.07092918455600739,
      "learning_rate": 0.0001971134020618557,
      "loss": 0.335,
      "step": 54
    },
    {
      "epoch": 0.016176470588235296,
      "grad_norm": 0.09368723630905151,
      "learning_rate": 0.00019705449189985275,
      "loss": 0.4217,
      "step": 55
    },
    {
      "epoch": 0.01647058823529412,
      "grad_norm": 0.07235271483659744,
      "learning_rate": 0.0001969955817378498,
      "loss": 0.3666,
      "step": 56
    },
    {
      "epoch": 0.016764705882352942,
      "grad_norm": 0.07903876900672913,
      "learning_rate": 0.00019693667157584687,
      "loss": 0.4831,
      "step": 57
    },
    {
      "epoch": 0.017058823529411765,
      "grad_norm": 0.08748850226402283,
      "learning_rate": 0.0001968777614138439,
      "loss": 0.3627,
      "step": 58
    },
    {
      "epoch": 0.01735294117647059,
      "grad_norm": 0.05902159586548805,
      "learning_rate": 0.00019681885125184093,
      "loss": 0.3672,
      "step": 59
    },
    {
      "epoch": 0.01764705882352941,
      "grad_norm": 0.09208288788795471,
      "learning_rate": 0.000196759941089838,
      "loss": 0.4468,
      "step": 60
    },
    {
      "epoch": 0.017941176470588235,
      "grad_norm": 0.07281191647052765,
      "learning_rate": 0.00019670103092783505,
      "loss": 0.3752,
      "step": 61
    },
    {
      "epoch": 0.018235294117647058,
      "grad_norm": 0.07336322218179703,
      "learning_rate": 0.00019664212076583211,
      "loss": 0.3351,
      "step": 62
    },
    {
      "epoch": 0.01852941176470588,
      "grad_norm": 0.0857594832777977,
      "learning_rate": 0.00019658321060382915,
      "loss": 0.3486,
      "step": 63
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 0.07910019904375076,
      "learning_rate": 0.0001965243004418262,
      "loss": 0.3821,
      "step": 64
    },
    {
      "epoch": 0.01911764705882353,
      "grad_norm": 0.06599901616573334,
      "learning_rate": 0.00019646539027982327,
      "loss": 0.3421,
      "step": 65
    },
    {
      "epoch": 0.019411764705882354,
      "grad_norm": 0.13408559560775757,
      "learning_rate": 0.00019640648011782033,
      "loss": 0.4391,
      "step": 66
    },
    {
      "epoch": 0.019705882352941177,
      "grad_norm": 0.09280645102262497,
      "learning_rate": 0.0001963475699558174,
      "loss": 0.2835,
      "step": 67
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.12123672664165497,
      "learning_rate": 0.00019628865979381442,
      "loss": 0.3594,
      "step": 68
    },
    {
      "epoch": 0.020294117647058824,
      "grad_norm": 0.05456243455410004,
      "learning_rate": 0.00019622974963181148,
      "loss": 0.3202,
      "step": 69
    },
    {
      "epoch": 0.020588235294117647,
      "grad_norm": 0.08300095051527023,
      "learning_rate": 0.00019617083946980854,
      "loss": 0.2711,
      "step": 70
    },
    {
      "epoch": 0.02088235294117647,
      "grad_norm": 0.11253716796636581,
      "learning_rate": 0.0001961119293078056,
      "loss": 0.434,
      "step": 71
    },
    {
      "epoch": 0.021176470588235293,
      "grad_norm": 0.08026277273893356,
      "learning_rate": 0.00019605301914580266,
      "loss": 0.3116,
      "step": 72
    },
    {
      "epoch": 0.021470588235294116,
      "grad_norm": 0.10930692404508591,
      "learning_rate": 0.0001959941089837997,
      "loss": 0.4016,
      "step": 73
    },
    {
      "epoch": 0.02176470588235294,
      "grad_norm": 0.10134784132242203,
      "learning_rate": 0.00019593519882179675,
      "loss": 0.4533,
      "step": 74
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 0.06525098532438278,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.42,
      "step": 75
    },
    {
      "epoch": 0.02235294117647059,
      "grad_norm": 0.08386800438165665,
      "learning_rate": 0.00019581737849779087,
      "loss": 0.3532,
      "step": 76
    },
    {
      "epoch": 0.022647058823529412,
      "grad_norm": 0.09660832583904266,
      "learning_rate": 0.00019575846833578793,
      "loss": 0.342,
      "step": 77
    },
    {
      "epoch": 0.022941176470588236,
      "grad_norm": 0.067966029047966,
      "learning_rate": 0.00019569955817378497,
      "loss": 0.3326,
      "step": 78
    },
    {
      "epoch": 0.02323529411764706,
      "grad_norm": 0.05596631020307541,
      "learning_rate": 0.00019564064801178203,
      "loss": 0.3234,
      "step": 79
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 0.09456225484609604,
      "learning_rate": 0.0001955817378497791,
      "loss": 0.4232,
      "step": 80
    },
    {
      "epoch": 0.023823529411764705,
      "grad_norm": 0.08949293196201324,
      "learning_rate": 0.00019552282768777615,
      "loss": 0.3799,
      "step": 81
    },
    {
      "epoch": 0.02411764705882353,
      "grad_norm": 0.10172037780284882,
      "learning_rate": 0.0001954639175257732,
      "loss": 0.4274,
      "step": 82
    },
    {
      "epoch": 0.02441176470588235,
      "grad_norm": 0.12107711285352707,
      "learning_rate": 0.00019540500736377024,
      "loss": 0.4931,
      "step": 83
    },
    {
      "epoch": 0.024705882352941175,
      "grad_norm": 0.08407244831323624,
      "learning_rate": 0.0001953460972017673,
      "loss": 0.4311,
      "step": 84
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.09880246967077255,
      "learning_rate": 0.00019528718703976436,
      "loss": 0.2732,
      "step": 85
    },
    {
      "epoch": 0.025294117647058825,
      "grad_norm": 0.10875432193279266,
      "learning_rate": 0.00019522827687776142,
      "loss": 0.3692,
      "step": 86
    },
    {
      "epoch": 0.025588235294117648,
      "grad_norm": 0.09373142570257187,
      "learning_rate": 0.00019516936671575848,
      "loss": 0.4146,
      "step": 87
    },
    {
      "epoch": 0.02588235294117647,
      "grad_norm": 0.06063848361372948,
      "learning_rate": 0.00019511045655375552,
      "loss": 0.2976,
      "step": 88
    },
    {
      "epoch": 0.026176470588235294,
      "grad_norm": 0.05426608398556709,
      "learning_rate": 0.00019505154639175258,
      "loss": 0.3455,
      "step": 89
    },
    {
      "epoch": 0.026470588235294117,
      "grad_norm": 0.09773965179920197,
      "learning_rate": 0.00019499263622974964,
      "loss": 0.4176,
      "step": 90
    },
    {
      "epoch": 0.02676470588235294,
      "grad_norm": 0.07388195395469666,
      "learning_rate": 0.0001949337260677467,
      "loss": 0.4503,
      "step": 91
    },
    {
      "epoch": 0.027058823529411764,
      "grad_norm": 0.09933801740407944,
      "learning_rate": 0.00019487481590574376,
      "loss": 0.3875,
      "step": 92
    },
    {
      "epoch": 0.027352941176470587,
      "grad_norm": 0.08318546414375305,
      "learning_rate": 0.0001948159057437408,
      "loss": 0.3592,
      "step": 93
    },
    {
      "epoch": 0.027647058823529413,
      "grad_norm": 0.07178034633398056,
      "learning_rate": 0.00019475699558173785,
      "loss": 0.3976,
      "step": 94
    },
    {
      "epoch": 0.027941176470588237,
      "grad_norm": 0.06823879480361938,
      "learning_rate": 0.0001946980854197349,
      "loss": 0.3504,
      "step": 95
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 0.0746409222483635,
      "learning_rate": 0.00019463917525773197,
      "loss": 0.3232,
      "step": 96
    },
    {
      "epoch": 0.028529411764705883,
      "grad_norm": 0.09007870405912399,
      "learning_rate": 0.00019458026509572903,
      "loss": 0.4303,
      "step": 97
    },
    {
      "epoch": 0.028823529411764706,
      "grad_norm": 0.08346293866634369,
      "learning_rate": 0.00019452135493372606,
      "loss": 0.454,
      "step": 98
    },
    {
      "epoch": 0.02911764705882353,
      "grad_norm": 0.05117262527346611,
      "learning_rate": 0.00019446244477172312,
      "loss": 0.3047,
      "step": 99
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.08778347074985504,
      "learning_rate": 0.00019440353460972018,
      "loss": 0.4976,
      "step": 100
    },
    {
      "epoch": 0.029705882352941176,
      "grad_norm": 0.07785649597644806,
      "learning_rate": 0.00019434462444771724,
      "loss": 0.3745,
      "step": 101
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.05458449572324753,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.3561,
      "step": 102
    },
    {
      "epoch": 0.030294117647058822,
      "grad_norm": 0.06850040704011917,
      "learning_rate": 0.00019422680412371134,
      "loss": 0.3486,
      "step": 103
    },
    {
      "epoch": 0.03058823529411765,
      "grad_norm": 0.07516885548830032,
      "learning_rate": 0.0001941678939617084,
      "loss": 0.4176,
      "step": 104
    },
    {
      "epoch": 0.030882352941176472,
      "grad_norm": 0.07857518643140793,
      "learning_rate": 0.00019410898379970546,
      "loss": 0.3769,
      "step": 105
    },
    {
      "epoch": 0.031176470588235295,
      "grad_norm": 0.07482893019914627,
      "learning_rate": 0.00019405007363770252,
      "loss": 0.4067,
      "step": 106
    },
    {
      "epoch": 0.03147058823529412,
      "grad_norm": 0.09830916672945023,
      "learning_rate": 0.00019399116347569958,
      "loss": 0.4196,
      "step": 107
    },
    {
      "epoch": 0.03176470588235294,
      "grad_norm": 0.07468336075544357,
      "learning_rate": 0.0001939322533136966,
      "loss": 0.3891,
      "step": 108
    },
    {
      "epoch": 0.032058823529411765,
      "grad_norm": 0.0617087185382843,
      "learning_rate": 0.00019387334315169367,
      "loss": 0.2831,
      "step": 109
    },
    {
      "epoch": 0.03235294117647059,
      "grad_norm": 0.08578453958034515,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.3942,
      "step": 110
    },
    {
      "epoch": 0.03264705882352941,
      "grad_norm": 0.0785159096121788,
      "learning_rate": 0.0001937555228276878,
      "loss": 0.3718,
      "step": 111
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 0.07825743407011032,
      "learning_rate": 0.00019369661266568485,
      "loss": 0.4118,
      "step": 112
    },
    {
      "epoch": 0.03323529411764706,
      "grad_norm": 0.0976356640458107,
      "learning_rate": 0.00019363770250368188,
      "loss": 0.3632,
      "step": 113
    },
    {
      "epoch": 0.033529411764705884,
      "grad_norm": 0.08399389684200287,
      "learning_rate": 0.00019357879234167894,
      "loss": 0.3752,
      "step": 114
    },
    {
      "epoch": 0.033823529411764704,
      "grad_norm": 0.08931345492601395,
      "learning_rate": 0.000193519882179676,
      "loss": 0.4144,
      "step": 115
    },
    {
      "epoch": 0.03411764705882353,
      "grad_norm": 0.09492973238229752,
      "learning_rate": 0.00019346097201767306,
      "loss": 0.3792,
      "step": 116
    },
    {
      "epoch": 0.03441176470588235,
      "grad_norm": 0.09350302815437317,
      "learning_rate": 0.00019340206185567012,
      "loss": 0.4238,
      "step": 117
    },
    {
      "epoch": 0.03470588235294118,
      "grad_norm": 0.0785617008805275,
      "learning_rate": 0.00019334315169366716,
      "loss": 0.4334,
      "step": 118
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.08060774207115173,
      "learning_rate": 0.00019328424153166422,
      "loss": 0.4303,
      "step": 119
    },
    {
      "epoch": 0.03529411764705882,
      "grad_norm": 0.07216943055391312,
      "learning_rate": 0.00019322533136966128,
      "loss": 0.4022,
      "step": 120
    },
    {
      "epoch": 0.03558823529411765,
      "grad_norm": 0.08611080795526505,
      "learning_rate": 0.00019316642120765834,
      "loss": 0.3385,
      "step": 121
    },
    {
      "epoch": 0.03588235294117647,
      "grad_norm": 0.09069265425205231,
      "learning_rate": 0.0001931075110456554,
      "loss": 0.4865,
      "step": 122
    },
    {
      "epoch": 0.036176470588235296,
      "grad_norm": 0.06891902536153793,
      "learning_rate": 0.00019304860088365243,
      "loss": 0.3918,
      "step": 123
    },
    {
      "epoch": 0.036470588235294116,
      "grad_norm": 0.07725144922733307,
      "learning_rate": 0.0001929896907216495,
      "loss": 0.4358,
      "step": 124
    },
    {
      "epoch": 0.03676470588235294,
      "grad_norm": 0.06405150890350342,
      "learning_rate": 0.00019293078055964655,
      "loss": 0.307,
      "step": 125
    },
    {
      "epoch": 0.03705882352941176,
      "grad_norm": 0.09536789357662201,
      "learning_rate": 0.0001928718703976436,
      "loss": 0.4015,
      "step": 126
    },
    {
      "epoch": 0.03735294117647059,
      "grad_norm": 0.07194528728723526,
      "learning_rate": 0.00019281296023564067,
      "loss": 0.3593,
      "step": 127
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 0.053252458572387695,
      "learning_rate": 0.0001927540500736377,
      "loss": 0.3293,
      "step": 128
    },
    {
      "epoch": 0.037941176470588235,
      "grad_norm": 0.0684155523777008,
      "learning_rate": 0.00019269513991163477,
      "loss": 0.4011,
      "step": 129
    },
    {
      "epoch": 0.03823529411764706,
      "grad_norm": 0.07344681769609451,
      "learning_rate": 0.00019263622974963183,
      "loss": 0.3686,
      "step": 130
    },
    {
      "epoch": 0.03852941176470588,
      "grad_norm": 0.051469989120960236,
      "learning_rate": 0.00019257731958762889,
      "loss": 0.323,
      "step": 131
    },
    {
      "epoch": 0.03882352941176471,
      "grad_norm": 0.08393733203411102,
      "learning_rate": 0.00019251840942562595,
      "loss": 0.4296,
      "step": 132
    },
    {
      "epoch": 0.03911764705882353,
      "grad_norm": 0.05931030958890915,
      "learning_rate": 0.00019245949926362298,
      "loss": 0.2892,
      "step": 133
    },
    {
      "epoch": 0.039411764705882354,
      "grad_norm": 0.07138442993164062,
      "learning_rate": 0.00019240058910162004,
      "loss": 0.4097,
      "step": 134
    },
    {
      "epoch": 0.039705882352941174,
      "grad_norm": 0.0805647075176239,
      "learning_rate": 0.0001923416789396171,
      "loss": 0.4004,
      "step": 135
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.07254071533679962,
      "learning_rate": 0.00019228276877761416,
      "loss": 0.4183,
      "step": 136
    },
    {
      "epoch": 0.04029411764705882,
      "grad_norm": 0.07440889626741409,
      "learning_rate": 0.00019222385861561122,
      "loss": 0.3521,
      "step": 137
    },
    {
      "epoch": 0.04058823529411765,
      "grad_norm": 0.06398832052946091,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.2904,
      "step": 138
    },
    {
      "epoch": 0.040882352941176474,
      "grad_norm": 0.10812432318925858,
      "learning_rate": 0.0001921060382916053,
      "loss": 0.421,
      "step": 139
    },
    {
      "epoch": 0.041176470588235294,
      "grad_norm": 0.05943019315600395,
      "learning_rate": 0.00019204712812960237,
      "loss": 0.3607,
      "step": 140
    },
    {
      "epoch": 0.04147058823529412,
      "grad_norm": 0.07914503663778305,
      "learning_rate": 0.00019198821796759943,
      "loss": 0.4313,
      "step": 141
    },
    {
      "epoch": 0.04176470588235294,
      "grad_norm": 0.07476179301738739,
      "learning_rate": 0.0001919293078055965,
      "loss": 0.423,
      "step": 142
    },
    {
      "epoch": 0.04205882352941177,
      "grad_norm": 0.06569647043943405,
      "learning_rate": 0.00019187039764359353,
      "loss": 0.3882,
      "step": 143
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 0.0693218782544136,
      "learning_rate": 0.0001918114874815906,
      "loss": 0.4107,
      "step": 144
    },
    {
      "epoch": 0.04264705882352941,
      "grad_norm": 0.08186395466327667,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.3618,
      "step": 145
    },
    {
      "epoch": 0.04294117647058823,
      "grad_norm": 0.08952964842319489,
      "learning_rate": 0.0001916936671575847,
      "loss": 0.4112,
      "step": 146
    },
    {
      "epoch": 0.04323529411764706,
      "grad_norm": 0.07004809379577637,
      "learning_rate": 0.00019163475699558177,
      "loss": 0.3516,
      "step": 147
    },
    {
      "epoch": 0.04352941176470588,
      "grad_norm": 0.06543531268835068,
      "learning_rate": 0.0001915758468335788,
      "loss": 0.3691,
      "step": 148
    },
    {
      "epoch": 0.043823529411764706,
      "grad_norm": 0.0856979712843895,
      "learning_rate": 0.00019151693667157586,
      "loss": 0.4403,
      "step": 149
    },
    {
      "epoch": 0.04411764705882353,
      "grad_norm": 0.08877045661211014,
      "learning_rate": 0.00019145802650957292,
      "loss": 0.3939,
      "step": 150
    },
    {
      "epoch": 0.04441176470588235,
      "grad_norm": 0.05518195405602455,
      "learning_rate": 0.00019139911634756998,
      "loss": 0.3232,
      "step": 151
    },
    {
      "epoch": 0.04470588235294118,
      "grad_norm": 0.06716704368591309,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.3184,
      "step": 152
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.08146704733371735,
      "learning_rate": 0.00019128129602356407,
      "loss": 0.4199,
      "step": 153
    },
    {
      "epoch": 0.045294117647058825,
      "grad_norm": 0.07697336375713348,
      "learning_rate": 0.00019122238586156113,
      "loss": 0.4123,
      "step": 154
    },
    {
      "epoch": 0.045588235294117645,
      "grad_norm": 0.0758376270532608,
      "learning_rate": 0.0001911634756995582,
      "loss": 0.3689,
      "step": 155
    },
    {
      "epoch": 0.04588235294117647,
      "grad_norm": 0.052515652030706406,
      "learning_rate": 0.00019110456553755525,
      "loss": 0.3013,
      "step": 156
    },
    {
      "epoch": 0.04617647058823529,
      "grad_norm": 0.09197742491960526,
      "learning_rate": 0.00019104565537555231,
      "loss": 0.4684,
      "step": 157
    },
    {
      "epoch": 0.04647058823529412,
      "grad_norm": 0.06481204181909561,
      "learning_rate": 0.00019098674521354935,
      "loss": 0.3764,
      "step": 158
    },
    {
      "epoch": 0.046764705882352944,
      "grad_norm": 0.11131249368190765,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.4198,
      "step": 159
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 0.06052528694272041,
      "learning_rate": 0.00019086892488954347,
      "loss": 0.2726,
      "step": 160
    },
    {
      "epoch": 0.04735294117647059,
      "grad_norm": 0.0812031552195549,
      "learning_rate": 0.00019081001472754053,
      "loss": 0.3611,
      "step": 161
    },
    {
      "epoch": 0.04764705882352941,
      "grad_norm": 0.08201907575130463,
      "learning_rate": 0.0001907511045655376,
      "loss": 0.3686,
      "step": 162
    },
    {
      "epoch": 0.04794117647058824,
      "grad_norm": 0.08419091999530792,
      "learning_rate": 0.00019069219440353462,
      "loss": 0.3642,
      "step": 163
    },
    {
      "epoch": 0.04823529411764706,
      "grad_norm": 0.07594932615756989,
      "learning_rate": 0.00019063328424153168,
      "loss": 0.3521,
      "step": 164
    },
    {
      "epoch": 0.04852941176470588,
      "grad_norm": 0.08616840094327927,
      "learning_rate": 0.00019057437407952871,
      "loss": 0.348,
      "step": 165
    },
    {
      "epoch": 0.0488235294117647,
      "grad_norm": 0.09599340707063675,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.4184,
      "step": 166
    },
    {
      "epoch": 0.04911764705882353,
      "grad_norm": 0.06443074345588684,
      "learning_rate": 0.00019045655375552283,
      "loss": 0.3023,
      "step": 167
    },
    {
      "epoch": 0.04941176470588235,
      "grad_norm": 0.07103496789932251,
      "learning_rate": 0.00019039764359351987,
      "loss": 0.3338,
      "step": 168
    },
    {
      "epoch": 0.049705882352941176,
      "grad_norm": 0.07142699509859085,
      "learning_rate": 0.00019033873343151693,
      "loss": 0.3735,
      "step": 169
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.06523020565509796,
      "learning_rate": 0.000190279823269514,
      "loss": 0.3259,
      "step": 170
    },
    {
      "epoch": 0.05029411764705882,
      "grad_norm": 0.06932653486728668,
      "learning_rate": 0.00019022091310751105,
      "loss": 0.4123,
      "step": 171
    },
    {
      "epoch": 0.05058823529411765,
      "grad_norm": 0.07477002590894699,
      "learning_rate": 0.0001901620029455081,
      "loss": 0.3851,
      "step": 172
    },
    {
      "epoch": 0.05088235294117647,
      "grad_norm": 0.06321363896131516,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.3298,
      "step": 173
    },
    {
      "epoch": 0.051176470588235295,
      "grad_norm": 0.06770181655883789,
      "learning_rate": 0.0001900441826215022,
      "loss": 0.3327,
      "step": 174
    },
    {
      "epoch": 0.051470588235294115,
      "grad_norm": 0.08581835776567459,
      "learning_rate": 0.00018998527245949926,
      "loss": 0.4237,
      "step": 175
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 0.08285832405090332,
      "learning_rate": 0.00018992636229749632,
      "loss": 0.3257,
      "step": 176
    },
    {
      "epoch": 0.05205882352941176,
      "grad_norm": 0.0945327952504158,
      "learning_rate": 0.00018986745213549338,
      "loss": 0.3499,
      "step": 177
    },
    {
      "epoch": 0.05235294117647059,
      "grad_norm": 0.07117632776498795,
      "learning_rate": 0.00018980854197349042,
      "loss": 0.3369,
      "step": 178
    },
    {
      "epoch": 0.052647058823529415,
      "grad_norm": 0.0673501193523407,
      "learning_rate": 0.00018974963181148748,
      "loss": 0.3608,
      "step": 179
    },
    {
      "epoch": 0.052941176470588235,
      "grad_norm": 0.08254967629909515,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.3696,
      "step": 180
    },
    {
      "epoch": 0.05323529411764706,
      "grad_norm": 0.06940854340791702,
      "learning_rate": 0.0001896318114874816,
      "loss": 0.4267,
      "step": 181
    },
    {
      "epoch": 0.05352941176470588,
      "grad_norm": 0.07577857375144958,
      "learning_rate": 0.00018957290132547866,
      "loss": 0.3282,
      "step": 182
    },
    {
      "epoch": 0.05382352941176471,
      "grad_norm": 0.05162256956100464,
      "learning_rate": 0.0001895139911634757,
      "loss": 0.2629,
      "step": 183
    },
    {
      "epoch": 0.05411764705882353,
      "grad_norm": 0.06116514280438423,
      "learning_rate": 0.00018945508100147275,
      "loss": 0.3658,
      "step": 184
    },
    {
      "epoch": 0.054411764705882354,
      "grad_norm": 0.07126085460186005,
      "learning_rate": 0.0001893961708394698,
      "loss": 0.3358,
      "step": 185
    },
    {
      "epoch": 0.054705882352941174,
      "grad_norm": 0.07707571983337402,
      "learning_rate": 0.00018933726067746687,
      "loss": 0.3492,
      "step": 186
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.10023003071546555,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.3914,
      "step": 187
    },
    {
      "epoch": 0.05529411764705883,
      "grad_norm": 0.058111727237701416,
      "learning_rate": 0.00018921944035346096,
      "loss": 0.344,
      "step": 188
    },
    {
      "epoch": 0.05558823529411765,
      "grad_norm": 0.05905693396925926,
      "learning_rate": 0.00018916053019145802,
      "loss": 0.2973,
      "step": 189
    },
    {
      "epoch": 0.05588235294117647,
      "grad_norm": 0.07258603721857071,
      "learning_rate": 0.00018910162002945508,
      "loss": 0.4136,
      "step": 190
    },
    {
      "epoch": 0.05617647058823529,
      "grad_norm": 0.056615542620420456,
      "learning_rate": 0.00018904270986745214,
      "loss": 0.3189,
      "step": 191
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 0.06405438482761383,
      "learning_rate": 0.0001889837997054492,
      "loss": 0.4238,
      "step": 192
    },
    {
      "epoch": 0.05676470588235294,
      "grad_norm": 0.059019383043050766,
      "learning_rate": 0.00018892488954344624,
      "loss": 0.3804,
      "step": 193
    },
    {
      "epoch": 0.057058823529411766,
      "grad_norm": 0.0648026242852211,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.3693,
      "step": 194
    },
    {
      "epoch": 0.057352941176470586,
      "grad_norm": 0.08629299700260162,
      "learning_rate": 0.00018880706921944036,
      "loss": 0.4859,
      "step": 195
    },
    {
      "epoch": 0.05764705882352941,
      "grad_norm": 0.08178982138633728,
      "learning_rate": 0.00018874815905743742,
      "loss": 0.4264,
      "step": 196
    },
    {
      "epoch": 0.05794117647058823,
      "grad_norm": 0.06669653207063675,
      "learning_rate": 0.00018868924889543448,
      "loss": 0.29,
      "step": 197
    },
    {
      "epoch": 0.05823529411764706,
      "grad_norm": 0.07139337062835693,
      "learning_rate": 0.0001886303387334315,
      "loss": 0.3713,
      "step": 198
    },
    {
      "epoch": 0.058529411764705885,
      "grad_norm": 0.06619426608085632,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3224,
      "step": 199
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.07070176303386688,
      "learning_rate": 0.00018851251840942563,
      "loss": 0.3441,
      "step": 200
    },
    {
      "epoch": 0.05911764705882353,
      "grad_norm": 0.08892921358346939,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.3867,
      "step": 201
    },
    {
      "epoch": 0.05941176470588235,
      "grad_norm": 0.06082486733794212,
      "learning_rate": 0.00018839469808541975,
      "loss": 0.3161,
      "step": 202
    },
    {
      "epoch": 0.05970588235294118,
      "grad_norm": 0.05711718276143074,
      "learning_rate": 0.00018833578792341678,
      "loss": 0.3822,
      "step": 203
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.05581040307879448,
      "learning_rate": 0.00018827687776141384,
      "loss": 0.2972,
      "step": 204
    },
    {
      "epoch": 0.060294117647058824,
      "grad_norm": 0.09559269994497299,
      "learning_rate": 0.0001882179675994109,
      "loss": 0.396,
      "step": 205
    },
    {
      "epoch": 0.060588235294117644,
      "grad_norm": 0.05928659066557884,
      "learning_rate": 0.00018815905743740796,
      "loss": 0.3453,
      "step": 206
    },
    {
      "epoch": 0.06088235294117647,
      "grad_norm": 0.06149083375930786,
      "learning_rate": 0.00018810014727540502,
      "loss": 0.3259,
      "step": 207
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 0.06094400957226753,
      "learning_rate": 0.00018804123711340206,
      "loss": 0.3424,
      "step": 208
    },
    {
      "epoch": 0.06147058823529412,
      "grad_norm": 0.0803094357252121,
      "learning_rate": 0.00018798232695139912,
      "loss": 0.378,
      "step": 209
    },
    {
      "epoch": 0.061764705882352944,
      "grad_norm": 0.05801349878311157,
      "learning_rate": 0.00018792341678939618,
      "loss": 0.3371,
      "step": 210
    },
    {
      "epoch": 0.062058823529411763,
      "grad_norm": 0.0828152596950531,
      "learning_rate": 0.00018786450662739324,
      "loss": 0.4039,
      "step": 211
    },
    {
      "epoch": 0.06235294117647059,
      "grad_norm": 0.05826396495103836,
      "learning_rate": 0.0001878055964653903,
      "loss": 0.353,
      "step": 212
    },
    {
      "epoch": 0.06264705882352942,
      "grad_norm": 0.06452875584363937,
      "learning_rate": 0.00018774668630338733,
      "loss": 0.3836,
      "step": 213
    },
    {
      "epoch": 0.06294117647058824,
      "grad_norm": 0.05847480520606041,
      "learning_rate": 0.0001876877761413844,
      "loss": 0.3062,
      "step": 214
    },
    {
      "epoch": 0.06323529411764706,
      "grad_norm": 0.06894835084676743,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.3715,
      "step": 215
    },
    {
      "epoch": 0.06352941176470588,
      "grad_norm": 0.06043218821287155,
      "learning_rate": 0.0001875699558173785,
      "loss": 0.3197,
      "step": 216
    },
    {
      "epoch": 0.06382352941176471,
      "grad_norm": 0.060165759176015854,
      "learning_rate": 0.00018751104565537557,
      "loss": 0.2788,
      "step": 217
    },
    {
      "epoch": 0.06411764705882353,
      "grad_norm": 0.057238586246967316,
      "learning_rate": 0.0001874521354933726,
      "loss": 0.3481,
      "step": 218
    },
    {
      "epoch": 0.06441176470588235,
      "grad_norm": 0.08119262009859085,
      "learning_rate": 0.00018739322533136967,
      "loss": 0.3352,
      "step": 219
    },
    {
      "epoch": 0.06470588235294118,
      "grad_norm": 0.08880080282688141,
      "learning_rate": 0.00018733431516936673,
      "loss": 0.4009,
      "step": 220
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.06980836391448975,
      "learning_rate": 0.00018727540500736379,
      "loss": 0.3645,
      "step": 221
    },
    {
      "epoch": 0.06529411764705882,
      "grad_norm": 0.05632235109806061,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.328,
      "step": 222
    },
    {
      "epoch": 0.06558823529411764,
      "grad_norm": 0.04948994517326355,
      "learning_rate": 0.00018715758468335788,
      "loss": 0.3328,
      "step": 223
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 0.07953855395317078,
      "learning_rate": 0.00018709867452135494,
      "loss": 0.3155,
      "step": 224
    },
    {
      "epoch": 0.0661764705882353,
      "grad_norm": 0.07696998119354248,
      "learning_rate": 0.000187039764359352,
      "loss": 0.389,
      "step": 225
    },
    {
      "epoch": 0.06647058823529411,
      "grad_norm": 0.07052081823348999,
      "learning_rate": 0.00018698085419734906,
      "loss": 0.3984,
      "step": 226
    },
    {
      "epoch": 0.06676470588235293,
      "grad_norm": 0.05734168365597725,
      "learning_rate": 0.00018692194403534612,
      "loss": 0.3694,
      "step": 227
    },
    {
      "epoch": 0.06705882352941177,
      "grad_norm": 0.05877940356731415,
      "learning_rate": 0.00018686303387334315,
      "loss": 0.3408,
      "step": 228
    },
    {
      "epoch": 0.06735294117647059,
      "grad_norm": 0.060689233243465424,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.29,
      "step": 229
    },
    {
      "epoch": 0.06764705882352941,
      "grad_norm": 0.05925682932138443,
      "learning_rate": 0.00018674521354933727,
      "loss": 0.3687,
      "step": 230
    },
    {
      "epoch": 0.06794117647058824,
      "grad_norm": 0.07324173301458359,
      "learning_rate": 0.00018668630338733433,
      "loss": 0.3666,
      "step": 231
    },
    {
      "epoch": 0.06823529411764706,
      "grad_norm": 0.07756275683641434,
      "learning_rate": 0.0001866273932253314,
      "loss": 0.4314,
      "step": 232
    },
    {
      "epoch": 0.06852941176470588,
      "grad_norm": 0.07440493255853653,
      "learning_rate": 0.00018656848306332843,
      "loss": 0.3846,
      "step": 233
    },
    {
      "epoch": 0.0688235294117647,
      "grad_norm": 0.06739978492259979,
      "learning_rate": 0.0001865095729013255,
      "loss": 0.382,
      "step": 234
    },
    {
      "epoch": 0.06911764705882353,
      "grad_norm": 0.05041012167930603,
      "learning_rate": 0.00018645066273932255,
      "loss": 0.3368,
      "step": 235
    },
    {
      "epoch": 0.06941176470588235,
      "grad_norm": 0.066290944814682,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.3793,
      "step": 236
    },
    {
      "epoch": 0.06970588235294117,
      "grad_norm": 0.06832732260227203,
      "learning_rate": 0.00018633284241531667,
      "loss": 0.3489,
      "step": 237
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0740690529346466,
      "learning_rate": 0.0001862739322533137,
      "loss": 0.4941,
      "step": 238
    },
    {
      "epoch": 0.07029411764705883,
      "grad_norm": 0.07410252094268799,
      "learning_rate": 0.00018621502209131076,
      "loss": 0.3707,
      "step": 239
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 0.07068129628896713,
      "learning_rate": 0.00018615611192930782,
      "loss": 0.4141,
      "step": 240
    },
    {
      "epoch": 0.07088235294117647,
      "grad_norm": 0.06870078295469284,
      "learning_rate": 0.00018609720176730488,
      "loss": 0.3152,
      "step": 241
    },
    {
      "epoch": 0.0711764705882353,
      "grad_norm": 0.06046896427869797,
      "learning_rate": 0.00018603829160530194,
      "loss": 0.3597,
      "step": 242
    },
    {
      "epoch": 0.07147058823529412,
      "grad_norm": 0.08149320632219315,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.3901,
      "step": 243
    },
    {
      "epoch": 0.07176470588235294,
      "grad_norm": 0.06869029998779297,
      "learning_rate": 0.00018592047128129603,
      "loss": 0.3938,
      "step": 244
    },
    {
      "epoch": 0.07205882352941176,
      "grad_norm": 0.08220042288303375,
      "learning_rate": 0.0001858615611192931,
      "loss": 0.362,
      "step": 245
    },
    {
      "epoch": 0.07235294117647059,
      "grad_norm": 0.09427651017904282,
      "learning_rate": 0.00018580265095729015,
      "loss": 0.4375,
      "step": 246
    },
    {
      "epoch": 0.07264705882352941,
      "grad_norm": 0.09413967281579971,
      "learning_rate": 0.00018574374079528721,
      "loss": 0.4539,
      "step": 247
    },
    {
      "epoch": 0.07294117647058823,
      "grad_norm": 0.06712189316749573,
      "learning_rate": 0.00018568483063328425,
      "loss": 0.3547,
      "step": 248
    },
    {
      "epoch": 0.07323529411764707,
      "grad_norm": 0.08505844324827194,
      "learning_rate": 0.0001856259204712813,
      "loss": 0.3818,
      "step": 249
    },
    {
      "epoch": 0.07352941176470588,
      "grad_norm": 0.09507601708173752,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.4742,
      "step": 250
    },
    {
      "epoch": 0.0738235294117647,
      "grad_norm": 0.05248608440160751,
      "learning_rate": 0.00018550810014727543,
      "loss": 0.2966,
      "step": 251
    },
    {
      "epoch": 0.07411764705882352,
      "grad_norm": 0.06461775302886963,
      "learning_rate": 0.0001854491899852725,
      "loss": 0.3861,
      "step": 252
    },
    {
      "epoch": 0.07441176470588236,
      "grad_norm": 0.06287456303834915,
      "learning_rate": 0.00018539027982326952,
      "loss": 0.3106,
      "step": 253
    },
    {
      "epoch": 0.07470588235294118,
      "grad_norm": 0.055949170142412186,
      "learning_rate": 0.00018533136966126658,
      "loss": 0.3145,
      "step": 254
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.07702046632766724,
      "learning_rate": 0.00018527245949926364,
      "loss": 0.4529,
      "step": 255
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 0.05366233363747597,
      "learning_rate": 0.0001852135493372607,
      "loss": 0.3212,
      "step": 256
    },
    {
      "epoch": 0.07558823529411765,
      "grad_norm": 0.04590430483222008,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.2962,
      "step": 257
    },
    {
      "epoch": 0.07588235294117647,
      "grad_norm": 0.06390290707349777,
      "learning_rate": 0.0001850957290132548,
      "loss": 0.3636,
      "step": 258
    },
    {
      "epoch": 0.07617647058823529,
      "grad_norm": 0.079315684735775,
      "learning_rate": 0.00018503681885125186,
      "loss": 0.3795,
      "step": 259
    },
    {
      "epoch": 0.07647058823529412,
      "grad_norm": 0.068527452647686,
      "learning_rate": 0.00018497790868924892,
      "loss": 0.4243,
      "step": 260
    },
    {
      "epoch": 0.07676470588235294,
      "grad_norm": 0.07096641510725021,
      "learning_rate": 0.00018491899852724598,
      "loss": 0.4449,
      "step": 261
    },
    {
      "epoch": 0.07705882352941176,
      "grad_norm": 0.07984885573387146,
      "learning_rate": 0.00018486008836524304,
      "loss": 0.4637,
      "step": 262
    },
    {
      "epoch": 0.07735294117647058,
      "grad_norm": 0.05733319744467735,
      "learning_rate": 0.00018480117820324007,
      "loss": 0.3345,
      "step": 263
    },
    {
      "epoch": 0.07764705882352942,
      "grad_norm": 0.07460702210664749,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.3905,
      "step": 264
    },
    {
      "epoch": 0.07794117647058824,
      "grad_norm": 0.08981731534004211,
      "learning_rate": 0.0001846833578792342,
      "loss": 0.3799,
      "step": 265
    },
    {
      "epoch": 0.07823529411764706,
      "grad_norm": 0.07102034240961075,
      "learning_rate": 0.00018462444771723125,
      "loss": 0.4121,
      "step": 266
    },
    {
      "epoch": 0.07852941176470589,
      "grad_norm": 0.09098925441503525,
      "learning_rate": 0.0001845655375552283,
      "loss": 0.4233,
      "step": 267
    },
    {
      "epoch": 0.07882352941176471,
      "grad_norm": 0.0705140233039856,
      "learning_rate": 0.00018450662739322534,
      "loss": 0.3948,
      "step": 268
    },
    {
      "epoch": 0.07911764705882353,
      "grad_norm": 0.05768349766731262,
      "learning_rate": 0.0001844477172312224,
      "loss": 0.3722,
      "step": 269
    },
    {
      "epoch": 0.07941176470588235,
      "grad_norm": 0.0816139355301857,
      "learning_rate": 0.00018438880706921946,
      "loss": 0.3795,
      "step": 270
    },
    {
      "epoch": 0.07970588235294118,
      "grad_norm": 0.06616862863302231,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.415,
      "step": 271
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.05865853652358055,
      "learning_rate": 0.00018427098674521356,
      "loss": 0.2895,
      "step": 272
    },
    {
      "epoch": 0.08029411764705882,
      "grad_norm": 0.0715838149189949,
      "learning_rate": 0.0001842120765832106,
      "loss": 0.3233,
      "step": 273
    },
    {
      "epoch": 0.08058823529411764,
      "grad_norm": 0.07817074656486511,
      "learning_rate": 0.00018415316642120765,
      "loss": 0.3683,
      "step": 274
    },
    {
      "epoch": 0.08088235294117647,
      "grad_norm": 0.06610382348299026,
      "learning_rate": 0.0001840942562592047,
      "loss": 0.3951,
      "step": 275
    },
    {
      "epoch": 0.0811764705882353,
      "grad_norm": 0.07598253339529037,
      "learning_rate": 0.00018403534609720177,
      "loss": 0.3353,
      "step": 276
    },
    {
      "epoch": 0.08147058823529411,
      "grad_norm": 0.07075203210115433,
      "learning_rate": 0.00018397643593519883,
      "loss": 0.3959,
      "step": 277
    },
    {
      "epoch": 0.08176470588235295,
      "grad_norm": 0.06762167066335678,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.3882,
      "step": 278
    },
    {
      "epoch": 0.08205882352941177,
      "grad_norm": 0.07061689347028732,
      "learning_rate": 0.00018385861561119292,
      "loss": 0.4008,
      "step": 279
    },
    {
      "epoch": 0.08235294117647059,
      "grad_norm": 0.08138107508420944,
      "learning_rate": 0.00018379970544918998,
      "loss": 0.4333,
      "step": 280
    },
    {
      "epoch": 0.0826470588235294,
      "grad_norm": 0.07874414324760437,
      "learning_rate": 0.00018374079528718704,
      "loss": 0.4537,
      "step": 281
    },
    {
      "epoch": 0.08294117647058824,
      "grad_norm": 0.0925433561205864,
      "learning_rate": 0.0001836818851251841,
      "loss": 0.4508,
      "step": 282
    },
    {
      "epoch": 0.08323529411764706,
      "grad_norm": 0.05465120077133179,
      "learning_rate": 0.00018362297496318114,
      "loss": 0.3921,
      "step": 283
    },
    {
      "epoch": 0.08352941176470588,
      "grad_norm": 0.059858329594135284,
      "learning_rate": 0.0001835640648011782,
      "loss": 0.3174,
      "step": 284
    },
    {
      "epoch": 0.0838235294117647,
      "grad_norm": 0.05330105498433113,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.336,
      "step": 285
    },
    {
      "epoch": 0.08411764705882353,
      "grad_norm": 0.06698829680681229,
      "learning_rate": 0.00018344624447717232,
      "loss": 0.3269,
      "step": 286
    },
    {
      "epoch": 0.08441176470588235,
      "grad_norm": 0.08343657106161118,
      "learning_rate": 0.00018338733431516938,
      "loss": 0.4219,
      "step": 287
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 0.060324955731630325,
      "learning_rate": 0.0001833284241531664,
      "loss": 0.3466,
      "step": 288
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.057091135531663895,
      "learning_rate": 0.00018326951399116347,
      "loss": 0.4132,
      "step": 289
    },
    {
      "epoch": 0.08529411764705883,
      "grad_norm": 0.08447415381669998,
      "learning_rate": 0.00018321060382916053,
      "loss": 0.4419,
      "step": 290
    },
    {
      "epoch": 0.08558823529411765,
      "grad_norm": 0.049560029059648514,
      "learning_rate": 0.0001831516936671576,
      "loss": 0.2815,
      "step": 291
    },
    {
      "epoch": 0.08588235294117647,
      "grad_norm": 0.12339417636394501,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.4795,
      "step": 292
    },
    {
      "epoch": 0.0861764705882353,
      "grad_norm": 0.0648915022611618,
      "learning_rate": 0.00018303387334315168,
      "loss": 0.3854,
      "step": 293
    },
    {
      "epoch": 0.08647058823529412,
      "grad_norm": 0.06920251995325089,
      "learning_rate": 0.00018297496318114874,
      "loss": 0.3784,
      "step": 294
    },
    {
      "epoch": 0.08676470588235294,
      "grad_norm": 0.06751497834920883,
      "learning_rate": 0.0001829160530191458,
      "loss": 0.3685,
      "step": 295
    },
    {
      "epoch": 0.08705882352941176,
      "grad_norm": 0.06494586169719696,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.4147,
      "step": 296
    },
    {
      "epoch": 0.08735294117647059,
      "grad_norm": 0.0658063217997551,
      "learning_rate": 0.00018279823269513992,
      "loss": 0.3549,
      "step": 297
    },
    {
      "epoch": 0.08764705882352941,
      "grad_norm": 0.07966029644012451,
      "learning_rate": 0.00018273932253313696,
      "loss": 0.3485,
      "step": 298
    },
    {
      "epoch": 0.08794117647058823,
      "grad_norm": 0.06242798641324043,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.3608,
      "step": 299
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.06966325640678406,
      "learning_rate": 0.00018262150220913108,
      "loss": 0.4039,
      "step": 300
    },
    {
      "epoch": 0.08852941176470588,
      "grad_norm": 0.046506062150001526,
      "learning_rate": 0.00018256259204712814,
      "loss": 0.2942,
      "step": 301
    },
    {
      "epoch": 0.0888235294117647,
      "grad_norm": 0.06489913165569305,
      "learning_rate": 0.0001825036818851252,
      "loss": 0.3899,
      "step": 302
    },
    {
      "epoch": 0.08911764705882352,
      "grad_norm": 0.07311385869979858,
      "learning_rate": 0.00018244477172312223,
      "loss": 0.3858,
      "step": 303
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 0.0623113214969635,
      "learning_rate": 0.0001823858615611193,
      "loss": 0.363,
      "step": 304
    },
    {
      "epoch": 0.08970588235294118,
      "grad_norm": 0.058694787323474884,
      "learning_rate": 0.00018232695139911635,
      "loss": 0.3917,
      "step": 305
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.049128107726573944,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.3205,
      "step": 306
    },
    {
      "epoch": 0.09029411764705883,
      "grad_norm": 0.07686536014080048,
      "learning_rate": 0.00018220913107511047,
      "loss": 0.3624,
      "step": 307
    },
    {
      "epoch": 0.09058823529411765,
      "grad_norm": 0.06679993122816086,
      "learning_rate": 0.0001821502209131075,
      "loss": 0.3594,
      "step": 308
    },
    {
      "epoch": 0.09088235294117647,
      "grad_norm": 0.06321009248495102,
      "learning_rate": 0.00018209131075110457,
      "loss": 0.3755,
      "step": 309
    },
    {
      "epoch": 0.09117647058823529,
      "grad_norm": 0.05005679279565811,
      "learning_rate": 0.00018203240058910163,
      "loss": 0.3325,
      "step": 310
    },
    {
      "epoch": 0.09147058823529412,
      "grad_norm": 0.07738867402076721,
      "learning_rate": 0.00018197349042709869,
      "loss": 0.4073,
      "step": 311
    },
    {
      "epoch": 0.09176470588235294,
      "grad_norm": 0.05306961387395859,
      "learning_rate": 0.00018191458026509575,
      "loss": 0.3559,
      "step": 312
    },
    {
      "epoch": 0.09205882352941176,
      "grad_norm": 0.07010605186223984,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.4019,
      "step": 313
    },
    {
      "epoch": 0.09235294117647058,
      "grad_norm": 0.0561184287071228,
      "learning_rate": 0.00018179675994108984,
      "loss": 0.3662,
      "step": 314
    },
    {
      "epoch": 0.09264705882352942,
      "grad_norm": 0.06064090132713318,
      "learning_rate": 0.0001817378497790869,
      "loss": 0.3442,
      "step": 315
    },
    {
      "epoch": 0.09294117647058824,
      "grad_norm": 0.055273476988077164,
      "learning_rate": 0.00018167893961708396,
      "loss": 0.3867,
      "step": 316
    },
    {
      "epoch": 0.09323529411764706,
      "grad_norm": 0.06528283655643463,
      "learning_rate": 0.00018162002945508102,
      "loss": 0.2962,
      "step": 317
    },
    {
      "epoch": 0.09352941176470589,
      "grad_norm": 0.054356832057237625,
      "learning_rate": 0.00018156111929307805,
      "loss": 0.3098,
      "step": 318
    },
    {
      "epoch": 0.09382352941176471,
      "grad_norm": 0.0716046467423439,
      "learning_rate": 0.0001815022091310751,
      "loss": 0.416,
      "step": 319
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.048138394951820374,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.3053,
      "step": 320
    },
    {
      "epoch": 0.09441176470588235,
      "grad_norm": 0.059518963098526,
      "learning_rate": 0.00018138438880706923,
      "loss": 0.3306,
      "step": 321
    },
    {
      "epoch": 0.09470588235294118,
      "grad_norm": 0.08200190216302872,
      "learning_rate": 0.0001813254786450663,
      "loss": 0.4431,
      "step": 322
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06480855494737625,
      "learning_rate": 0.00018126656848306333,
      "loss": 0.3429,
      "step": 323
    },
    {
      "epoch": 0.09529411764705882,
      "grad_norm": 0.05147107318043709,
      "learning_rate": 0.00018120765832106039,
      "loss": 0.347,
      "step": 324
    },
    {
      "epoch": 0.09558823529411764,
      "grad_norm": 0.07055281102657318,
      "learning_rate": 0.00018114874815905745,
      "loss": 0.3451,
      "step": 325
    },
    {
      "epoch": 0.09588235294117647,
      "grad_norm": 0.07833453267812729,
      "learning_rate": 0.0001810898379970545,
      "loss": 0.4076,
      "step": 326
    },
    {
      "epoch": 0.0961764705882353,
      "grad_norm": 0.05751001834869385,
      "learning_rate": 0.00018103092783505157,
      "loss": 0.3703,
      "step": 327
    },
    {
      "epoch": 0.09647058823529411,
      "grad_norm": 0.05488632619380951,
      "learning_rate": 0.0001809720176730486,
      "loss": 0.3214,
      "step": 328
    },
    {
      "epoch": 0.09676470588235295,
      "grad_norm": 0.059503521770238876,
      "learning_rate": 0.00018091310751104566,
      "loss": 0.3217,
      "step": 329
    },
    {
      "epoch": 0.09705882352941177,
      "grad_norm": 0.0577617809176445,
      "learning_rate": 0.00018085419734904272,
      "loss": 0.3086,
      "step": 330
    },
    {
      "epoch": 0.09735294117647059,
      "grad_norm": 0.0629652738571167,
      "learning_rate": 0.00018079528718703978,
      "loss": 0.3844,
      "step": 331
    },
    {
      "epoch": 0.0976470588235294,
      "grad_norm": 0.06608890742063522,
      "learning_rate": 0.00018073637702503684,
      "loss": 0.3571,
      "step": 332
    },
    {
      "epoch": 0.09794117647058824,
      "grad_norm": 0.05386192724108696,
      "learning_rate": 0.00018067746686303387,
      "loss": 0.3223,
      "step": 333
    },
    {
      "epoch": 0.09823529411764706,
      "grad_norm": 0.07279947400093079,
      "learning_rate": 0.00018061855670103093,
      "loss": 0.3918,
      "step": 334
    },
    {
      "epoch": 0.09852941176470588,
      "grad_norm": 0.06606367975473404,
      "learning_rate": 0.000180559646539028,
      "loss": 0.3674,
      "step": 335
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 0.0569785013794899,
      "learning_rate": 0.00018050073637702505,
      "loss": 0.3349,
      "step": 336
    },
    {
      "epoch": 0.09911764705882353,
      "grad_norm": 0.056245751678943634,
      "learning_rate": 0.00018044182621502211,
      "loss": 0.4027,
      "step": 337
    },
    {
      "epoch": 0.09941176470588235,
      "grad_norm": 0.06317412853240967,
      "learning_rate": 0.00018038291605301915,
      "loss": 0.3482,
      "step": 338
    },
    {
      "epoch": 0.09970588235294117,
      "grad_norm": 0.048747770488262177,
      "learning_rate": 0.0001803240058910162,
      "loss": 0.3146,
      "step": 339
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.06825648248195648,
      "learning_rate": 0.00018026509572901327,
      "loss": 0.4104,
      "step": 340
    },
    {
      "epoch": 0.10029411764705883,
      "grad_norm": 0.05543675646185875,
      "learning_rate": 0.00018020618556701033,
      "loss": 0.3656,
      "step": 341
    },
    {
      "epoch": 0.10058823529411764,
      "grad_norm": 0.0682123601436615,
      "learning_rate": 0.0001801472754050074,
      "loss": 0.3719,
      "step": 342
    },
    {
      "epoch": 0.10088235294117646,
      "grad_norm": 0.06396583467721939,
      "learning_rate": 0.00018008836524300442,
      "loss": 0.3392,
      "step": 343
    },
    {
      "epoch": 0.1011764705882353,
      "grad_norm": 0.06525035947561264,
      "learning_rate": 0.00018002945508100148,
      "loss": 0.4001,
      "step": 344
    },
    {
      "epoch": 0.10147058823529412,
      "grad_norm": 0.06932403892278671,
      "learning_rate": 0.00017997054491899854,
      "loss": 0.3657,
      "step": 345
    },
    {
      "epoch": 0.10176470588235294,
      "grad_norm": 0.05467165634036064,
      "learning_rate": 0.0001799116347569956,
      "loss": 0.4113,
      "step": 346
    },
    {
      "epoch": 0.10205882352941177,
      "grad_norm": 0.06200922280550003,
      "learning_rate": 0.00017985272459499266,
      "loss": 0.3121,
      "step": 347
    },
    {
      "epoch": 0.10235294117647059,
      "grad_norm": 0.061485063284635544,
      "learning_rate": 0.0001797938144329897,
      "loss": 0.3098,
      "step": 348
    },
    {
      "epoch": 0.10264705882352941,
      "grad_norm": 0.0642416849732399,
      "learning_rate": 0.00017973490427098675,
      "loss": 0.3411,
      "step": 349
    },
    {
      "epoch": 0.10294117647058823,
      "grad_norm": 0.04680927097797394,
      "learning_rate": 0.00017967599410898382,
      "loss": 0.324,
      "step": 350
    },
    {
      "epoch": 0.10323529411764706,
      "grad_norm": 0.05197561904788017,
      "learning_rate": 0.00017961708394698088,
      "loss": 0.3162,
      "step": 351
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 0.054913997650146484,
      "learning_rate": 0.00017955817378497794,
      "loss": 0.3408,
      "step": 352
    },
    {
      "epoch": 0.1038235294117647,
      "grad_norm": 0.07395322620868683,
      "learning_rate": 0.00017949926362297497,
      "loss": 0.3652,
      "step": 353
    },
    {
      "epoch": 0.10411764705882352,
      "grad_norm": 0.06962316483259201,
      "learning_rate": 0.00017944035346097203,
      "loss": 0.4478,
      "step": 354
    },
    {
      "epoch": 0.10441176470588236,
      "grad_norm": 0.06380102783441544,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4222,
      "step": 355
    },
    {
      "epoch": 0.10470588235294118,
      "grad_norm": 0.08563566952943802,
      "learning_rate": 0.00017932253313696615,
      "loss": 0.4801,
      "step": 356
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.05784589424729347,
      "learning_rate": 0.0001792636229749632,
      "loss": 0.3583,
      "step": 357
    },
    {
      "epoch": 0.10529411764705883,
      "grad_norm": 0.05640096217393875,
      "learning_rate": 0.00017920471281296024,
      "loss": 0.3538,
      "step": 358
    },
    {
      "epoch": 0.10558823529411765,
      "grad_norm": 0.06621186435222626,
      "learning_rate": 0.0001791458026509573,
      "loss": 0.4273,
      "step": 359
    },
    {
      "epoch": 0.10588235294117647,
      "grad_norm": 0.06506894528865814,
      "learning_rate": 0.00017908689248895436,
      "loss": 0.431,
      "step": 360
    },
    {
      "epoch": 0.10617647058823529,
      "grad_norm": 0.07250071316957474,
      "learning_rate": 0.00017902798232695142,
      "loss": 0.42,
      "step": 361
    },
    {
      "epoch": 0.10647058823529412,
      "grad_norm": 0.06515748053789139,
      "learning_rate": 0.00017896907216494848,
      "loss": 0.4368,
      "step": 362
    },
    {
      "epoch": 0.10676470588235294,
      "grad_norm": 0.06887184083461761,
      "learning_rate": 0.00017891016200294552,
      "loss": 0.3783,
      "step": 363
    },
    {
      "epoch": 0.10705882352941176,
      "grad_norm": 0.06759919226169586,
      "learning_rate": 0.00017885125184094258,
      "loss": 0.3283,
      "step": 364
    },
    {
      "epoch": 0.10735294117647058,
      "grad_norm": 0.058880142867565155,
      "learning_rate": 0.00017879234167893964,
      "loss": 0.3454,
      "step": 365
    },
    {
      "epoch": 0.10764705882352942,
      "grad_norm": 0.07714717835187912,
      "learning_rate": 0.0001787334315169367,
      "loss": 0.3812,
      "step": 366
    },
    {
      "epoch": 0.10794117647058823,
      "grad_norm": 0.06342080980539322,
      "learning_rate": 0.00017867452135493376,
      "loss": 0.3334,
      "step": 367
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 0.08185877650976181,
      "learning_rate": 0.0001786156111929308,
      "loss": 0.4489,
      "step": 368
    },
    {
      "epoch": 0.10852941176470589,
      "grad_norm": 0.06441660970449448,
      "learning_rate": 0.00017855670103092785,
      "loss": 0.4196,
      "step": 369
    },
    {
      "epoch": 0.10882352941176471,
      "grad_norm": 0.0684155523777008,
      "learning_rate": 0.0001784977908689249,
      "loss": 0.3998,
      "step": 370
    },
    {
      "epoch": 0.10911764705882353,
      "grad_norm": 0.06541825085878372,
      "learning_rate": 0.00017843888070692197,
      "loss": 0.3635,
      "step": 371
    },
    {
      "epoch": 0.10941176470588235,
      "grad_norm": 0.06810446083545685,
      "learning_rate": 0.00017837997054491903,
      "loss": 0.3301,
      "step": 372
    },
    {
      "epoch": 0.10970588235294118,
      "grad_norm": 0.06217741221189499,
      "learning_rate": 0.00017832106038291606,
      "loss": 0.3809,
      "step": 373
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.05610036849975586,
      "learning_rate": 0.00017826215022091312,
      "loss": 0.3398,
      "step": 374
    },
    {
      "epoch": 0.11029411764705882,
      "grad_norm": 0.06394899636507034,
      "learning_rate": 0.00017820324005891018,
      "loss": 0.3713,
      "step": 375
    },
    {
      "epoch": 0.11058823529411765,
      "grad_norm": 0.04780898615717888,
      "learning_rate": 0.00017814432989690724,
      "loss": 0.2578,
      "step": 376
    },
    {
      "epoch": 0.11088235294117647,
      "grad_norm": 0.055820561945438385,
      "learning_rate": 0.00017808541973490428,
      "loss": 0.3816,
      "step": 377
    },
    {
      "epoch": 0.1111764705882353,
      "grad_norm": 0.08161716163158417,
      "learning_rate": 0.0001780265095729013,
      "loss": 0.3929,
      "step": 378
    },
    {
      "epoch": 0.11147058823529411,
      "grad_norm": 0.06336405873298645,
      "learning_rate": 0.00017796759941089837,
      "loss": 0.3856,
      "step": 379
    },
    {
      "epoch": 0.11176470588235295,
      "grad_norm": 0.0461711585521698,
      "learning_rate": 0.00017790868924889543,
      "loss": 0.3168,
      "step": 380
    },
    {
      "epoch": 0.11205882352941177,
      "grad_norm": 0.05394534766674042,
      "learning_rate": 0.0001778497790868925,
      "loss": 0.3194,
      "step": 381
    },
    {
      "epoch": 0.11235294117647059,
      "grad_norm": 0.060035914182662964,
      "learning_rate": 0.00017779086892488955,
      "loss": 0.3661,
      "step": 382
    },
    {
      "epoch": 0.1126470588235294,
      "grad_norm": 0.06650905311107635,
      "learning_rate": 0.00017773195876288658,
      "loss": 0.3788,
      "step": 383
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.0511661060154438,
      "learning_rate": 0.00017767304860088364,
      "loss": 0.3154,
      "step": 384
    },
    {
      "epoch": 0.11323529411764706,
      "grad_norm": 0.06222601607441902,
      "learning_rate": 0.0001776141384388807,
      "loss": 0.4536,
      "step": 385
    },
    {
      "epoch": 0.11352941176470588,
      "grad_norm": 0.05904239043593407,
      "learning_rate": 0.00017755522827687776,
      "loss": 0.3199,
      "step": 386
    },
    {
      "epoch": 0.11382352941176471,
      "grad_norm": 0.05111019313335419,
      "learning_rate": 0.00017749631811487482,
      "loss": 0.318,
      "step": 387
    },
    {
      "epoch": 0.11411764705882353,
      "grad_norm": 0.06134296953678131,
      "learning_rate": 0.00017743740795287186,
      "loss": 0.3932,
      "step": 388
    },
    {
      "epoch": 0.11441176470588235,
      "grad_norm": 0.04546453803777695,
      "learning_rate": 0.00017737849779086892,
      "loss": 0.3217,
      "step": 389
    },
    {
      "epoch": 0.11470588235294117,
      "grad_norm": 0.044524893164634705,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.3199,
      "step": 390
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.04633469134569168,
      "learning_rate": 0.00017726067746686304,
      "loss": 0.3078,
      "step": 391
    },
    {
      "epoch": 0.11529411764705882,
      "grad_norm": 0.0478605292737484,
      "learning_rate": 0.0001772017673048601,
      "loss": 0.317,
      "step": 392
    },
    {
      "epoch": 0.11558823529411764,
      "grad_norm": 0.06444822251796722,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3799,
      "step": 393
    },
    {
      "epoch": 0.11588235294117646,
      "grad_norm": 0.05480935424566269,
      "learning_rate": 0.0001770839469808542,
      "loss": 0.3854,
      "step": 394
    },
    {
      "epoch": 0.1161764705882353,
      "grad_norm": 0.07725254446268082,
      "learning_rate": 0.00017702503681885125,
      "loss": 0.4353,
      "step": 395
    },
    {
      "epoch": 0.11647058823529412,
      "grad_norm": 0.07547573745250702,
      "learning_rate": 0.0001769661266568483,
      "loss": 0.4639,
      "step": 396
    },
    {
      "epoch": 0.11676470588235294,
      "grad_norm": 0.05252469331026077,
      "learning_rate": 0.00017690721649484537,
      "loss": 0.3224,
      "step": 397
    },
    {
      "epoch": 0.11705882352941177,
      "grad_norm": 0.06379898637533188,
      "learning_rate": 0.0001768483063328424,
      "loss": 0.3626,
      "step": 398
    },
    {
      "epoch": 0.11735294117647059,
      "grad_norm": 0.07324875146150589,
      "learning_rate": 0.00017678939617083947,
      "loss": 0.3845,
      "step": 399
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.07202717661857605,
      "learning_rate": 0.00017673048600883653,
      "loss": 0.3564,
      "step": 400
    },
    {
      "epoch": 0.11794117647058823,
      "grad_norm": 0.07221133261919022,
      "learning_rate": 0.00017667157584683359,
      "loss": 0.459,
      "step": 401
    },
    {
      "epoch": 0.11823529411764706,
      "grad_norm": 0.06054593622684479,
      "learning_rate": 0.00017661266568483065,
      "loss": 0.4718,
      "step": 402
    },
    {
      "epoch": 0.11852941176470588,
      "grad_norm": 0.0724506825208664,
      "learning_rate": 0.00017655375552282768,
      "loss": 0.3528,
      "step": 403
    },
    {
      "epoch": 0.1188235294117647,
      "grad_norm": 0.06451458483934402,
      "learning_rate": 0.00017649484536082474,
      "loss": 0.3355,
      "step": 404
    },
    {
      "epoch": 0.11911764705882352,
      "grad_norm": 0.057649191468954086,
      "learning_rate": 0.0001764359351988218,
      "loss": 0.3808,
      "step": 405
    },
    {
      "epoch": 0.11941176470588236,
      "grad_norm": 0.06574881821870804,
      "learning_rate": 0.00017637702503681886,
      "loss": 0.3834,
      "step": 406
    },
    {
      "epoch": 0.11970588235294118,
      "grad_norm": 0.05234568566083908,
      "learning_rate": 0.00017631811487481592,
      "loss": 0.3308,
      "step": 407
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07136711478233337,
      "learning_rate": 0.00017625920471281295,
      "loss": 0.4331,
      "step": 408
    },
    {
      "epoch": 0.12029411764705883,
      "grad_norm": 0.0593692921102047,
      "learning_rate": 0.00017620029455081,
      "loss": 0.3605,
      "step": 409
    },
    {
      "epoch": 0.12058823529411765,
      "grad_norm": 0.061242297291755676,
      "learning_rate": 0.00017614138438880707,
      "loss": 0.4166,
      "step": 410
    },
    {
      "epoch": 0.12088235294117647,
      "grad_norm": 0.06117153540253639,
      "learning_rate": 0.00017608247422680413,
      "loss": 0.388,
      "step": 411
    },
    {
      "epoch": 0.12117647058823529,
      "grad_norm": 0.05603696033358574,
      "learning_rate": 0.0001760235640648012,
      "loss": 0.3718,
      "step": 412
    },
    {
      "epoch": 0.12147058823529412,
      "grad_norm": 0.06118542328476906,
      "learning_rate": 0.00017596465390279823,
      "loss": 0.4067,
      "step": 413
    },
    {
      "epoch": 0.12176470588235294,
      "grad_norm": 0.039172861725091934,
      "learning_rate": 0.00017590574374079529,
      "loss": 0.3015,
      "step": 414
    },
    {
      "epoch": 0.12205882352941176,
      "grad_norm": 0.06444255262613297,
      "learning_rate": 0.00017584683357879235,
      "loss": 0.3892,
      "step": 415
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 0.05588488280773163,
      "learning_rate": 0.0001757879234167894,
      "loss": 0.3118,
      "step": 416
    },
    {
      "epoch": 0.12264705882352941,
      "grad_norm": 0.07427003979682922,
      "learning_rate": 0.00017572901325478647,
      "loss": 0.4167,
      "step": 417
    },
    {
      "epoch": 0.12294117647058823,
      "grad_norm": 0.0465058796107769,
      "learning_rate": 0.0001756701030927835,
      "loss": 0.3195,
      "step": 418
    },
    {
      "epoch": 0.12323529411764705,
      "grad_norm": 0.050864074379205704,
      "learning_rate": 0.00017561119293078056,
      "loss": 0.3268,
      "step": 419
    },
    {
      "epoch": 0.12352941176470589,
      "grad_norm": 0.09469547122716904,
      "learning_rate": 0.00017555228276877762,
      "loss": 0.3025,
      "step": 420
    },
    {
      "epoch": 0.12382352941176471,
      "grad_norm": 0.07172059267759323,
      "learning_rate": 0.00017549337260677468,
      "loss": 0.4325,
      "step": 421
    },
    {
      "epoch": 0.12411764705882353,
      "grad_norm": 0.051121532917022705,
      "learning_rate": 0.00017543446244477174,
      "loss": 0.3146,
      "step": 422
    },
    {
      "epoch": 0.12441176470588235,
      "grad_norm": 0.07183513045310974,
      "learning_rate": 0.00017537555228276877,
      "loss": 0.3588,
      "step": 423
    },
    {
      "epoch": 0.12470588235294118,
      "grad_norm": 0.0905645564198494,
      "learning_rate": 0.00017531664212076583,
      "loss": 0.4159,
      "step": 424
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06450112164020538,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4084,
      "step": 425
    },
    {
      "epoch": 0.12529411764705883,
      "grad_norm": 0.06768307834863663,
      "learning_rate": 0.00017519882179675995,
      "loss": 0.4074,
      "step": 426
    },
    {
      "epoch": 0.12558823529411764,
      "grad_norm": 0.07054144144058228,
      "learning_rate": 0.00017513991163475701,
      "loss": 0.3664,
      "step": 427
    },
    {
      "epoch": 0.12588235294117647,
      "grad_norm": 0.05521046370267868,
      "learning_rate": 0.00017508100147275405,
      "loss": 0.3123,
      "step": 428
    },
    {
      "epoch": 0.1261764705882353,
      "grad_norm": 0.049050744622945786,
      "learning_rate": 0.0001750220913107511,
      "loss": 0.2982,
      "step": 429
    },
    {
      "epoch": 0.1264705882352941,
      "grad_norm": 0.07248954474925995,
      "learning_rate": 0.00017496318114874817,
      "loss": 0.3856,
      "step": 430
    },
    {
      "epoch": 0.12676470588235295,
      "grad_norm": 0.05912009999155998,
      "learning_rate": 0.00017490427098674523,
      "loss": 0.3263,
      "step": 431
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 0.08046949654817581,
      "learning_rate": 0.0001748453608247423,
      "loss": 0.4143,
      "step": 432
    },
    {
      "epoch": 0.12735294117647059,
      "grad_norm": 0.05608232691884041,
      "learning_rate": 0.00017478645066273932,
      "loss": 0.3189,
      "step": 433
    },
    {
      "epoch": 0.12764705882352942,
      "grad_norm": 0.05293964222073555,
      "learning_rate": 0.00017472754050073638,
      "loss": 0.3381,
      "step": 434
    },
    {
      "epoch": 0.12794117647058822,
      "grad_norm": 0.05264270305633545,
      "learning_rate": 0.00017466863033873344,
      "loss": 0.3527,
      "step": 435
    },
    {
      "epoch": 0.12823529411764706,
      "grad_norm": 0.06921862810850143,
      "learning_rate": 0.0001746097201767305,
      "loss": 0.4024,
      "step": 436
    },
    {
      "epoch": 0.1285294117647059,
      "grad_norm": 0.05247751623392105,
      "learning_rate": 0.00017455081001472756,
      "loss": 0.3725,
      "step": 437
    },
    {
      "epoch": 0.1288235294117647,
      "grad_norm": 0.06483626365661621,
      "learning_rate": 0.0001744918998527246,
      "loss": 0.4246,
      "step": 438
    },
    {
      "epoch": 0.12911764705882353,
      "grad_norm": 0.05579894781112671,
      "learning_rate": 0.00017443298969072165,
      "loss": 0.3929,
      "step": 439
    },
    {
      "epoch": 0.12941176470588237,
      "grad_norm": 0.05639004707336426,
      "learning_rate": 0.00017437407952871872,
      "loss": 0.3702,
      "step": 440
    },
    {
      "epoch": 0.12970588235294117,
      "grad_norm": 0.05637900531291962,
      "learning_rate": 0.00017431516936671578,
      "loss": 0.3456,
      "step": 441
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.058856867253780365,
      "learning_rate": 0.00017425625920471284,
      "loss": 0.3508,
      "step": 442
    },
    {
      "epoch": 0.1302941176470588,
      "grad_norm": 0.06624807417392731,
      "learning_rate": 0.00017419734904270987,
      "loss": 0.367,
      "step": 443
    },
    {
      "epoch": 0.13058823529411764,
      "grad_norm": 0.05654902011156082,
      "learning_rate": 0.00017413843888070693,
      "loss": 0.3695,
      "step": 444
    },
    {
      "epoch": 0.13088235294117648,
      "grad_norm": 0.05620846897363663,
      "learning_rate": 0.000174079528718704,
      "loss": 0.3235,
      "step": 445
    },
    {
      "epoch": 0.13117647058823528,
      "grad_norm": 0.07250788062810898,
      "learning_rate": 0.00017402061855670105,
      "loss": 0.4003,
      "step": 446
    },
    {
      "epoch": 0.13147058823529412,
      "grad_norm": 0.0533524826169014,
      "learning_rate": 0.0001739617083946981,
      "loss": 0.321,
      "step": 447
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 0.04872708022594452,
      "learning_rate": 0.00017390279823269514,
      "loss": 0.3145,
      "step": 448
    },
    {
      "epoch": 0.13205882352941176,
      "grad_norm": 0.0677766501903534,
      "learning_rate": 0.0001738438880706922,
      "loss": 0.3661,
      "step": 449
    },
    {
      "epoch": 0.1323529411764706,
      "grad_norm": 0.06011781096458435,
      "learning_rate": 0.00017378497790868926,
      "loss": 0.3321,
      "step": 450
    },
    {
      "epoch": 0.13264705882352942,
      "grad_norm": 0.059614550322294235,
      "learning_rate": 0.00017372606774668632,
      "loss": 0.4049,
      "step": 451
    },
    {
      "epoch": 0.13294117647058823,
      "grad_norm": 0.04442768916487694,
      "learning_rate": 0.00017366715758468338,
      "loss": 0.28,
      "step": 452
    },
    {
      "epoch": 0.13323529411764706,
      "grad_norm": 0.05528303235769272,
      "learning_rate": 0.00017360824742268042,
      "loss": 0.3638,
      "step": 453
    },
    {
      "epoch": 0.13352941176470587,
      "grad_norm": 0.05034511163830757,
      "learning_rate": 0.00017354933726067748,
      "loss": 0.3053,
      "step": 454
    },
    {
      "epoch": 0.1338235294117647,
      "grad_norm": 0.05702619254589081,
      "learning_rate": 0.00017349042709867454,
      "loss": 0.3265,
      "step": 455
    },
    {
      "epoch": 0.13411764705882354,
      "grad_norm": 0.06102799251675606,
      "learning_rate": 0.0001734315169366716,
      "loss": 0.3186,
      "step": 456
    },
    {
      "epoch": 0.13441176470588234,
      "grad_norm": 0.053947534412145615,
      "learning_rate": 0.00017337260677466866,
      "loss": 0.3026,
      "step": 457
    },
    {
      "epoch": 0.13470588235294118,
      "grad_norm": 0.05989567190408707,
      "learning_rate": 0.0001733136966126657,
      "loss": 0.3526,
      "step": 458
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.048722539097070694,
      "learning_rate": 0.00017325478645066275,
      "loss": 0.353,
      "step": 459
    },
    {
      "epoch": 0.13529411764705881,
      "grad_norm": 0.04778992384672165,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.2815,
      "step": 460
    },
    {
      "epoch": 0.13558823529411765,
      "grad_norm": 0.06761613488197327,
      "learning_rate": 0.00017313696612665687,
      "loss": 0.3809,
      "step": 461
    },
    {
      "epoch": 0.13588235294117648,
      "grad_norm": 0.050742797553539276,
      "learning_rate": 0.00017307805596465393,
      "loss": 0.32,
      "step": 462
    },
    {
      "epoch": 0.1361764705882353,
      "grad_norm": 0.049926646053791046,
      "learning_rate": 0.00017301914580265096,
      "loss": 0.3056,
      "step": 463
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 0.11792955547571182,
      "learning_rate": 0.00017296023564064802,
      "loss": 0.3385,
      "step": 464
    },
    {
      "epoch": 0.13676470588235295,
      "grad_norm": 0.04991514980792999,
      "learning_rate": 0.00017290132547864508,
      "loss": 0.3578,
      "step": 465
    },
    {
      "epoch": 0.13705882352941176,
      "grad_norm": 0.07346171885728836,
      "learning_rate": 0.00017284241531664214,
      "loss": 0.4517,
      "step": 466
    },
    {
      "epoch": 0.1373529411764706,
      "grad_norm": 0.05691985785961151,
      "learning_rate": 0.0001727835051546392,
      "loss": 0.3876,
      "step": 467
    },
    {
      "epoch": 0.1376470588235294,
      "grad_norm": 0.053480759263038635,
      "learning_rate": 0.00017272459499263624,
      "loss": 0.3981,
      "step": 468
    },
    {
      "epoch": 0.13794117647058823,
      "grad_norm": 0.0681968703866005,
      "learning_rate": 0.0001726656848306333,
      "loss": 0.3489,
      "step": 469
    },
    {
      "epoch": 0.13823529411764707,
      "grad_norm": 0.05734647065401077,
      "learning_rate": 0.00017260677466863036,
      "loss": 0.3242,
      "step": 470
    },
    {
      "epoch": 0.13852941176470587,
      "grad_norm": 0.05011032149195671,
      "learning_rate": 0.00017254786450662742,
      "loss": 0.3068,
      "step": 471
    },
    {
      "epoch": 0.1388235294117647,
      "grad_norm": 0.05899882689118385,
      "learning_rate": 0.00017248895434462448,
      "loss": 0.3522,
      "step": 472
    },
    {
      "epoch": 0.13911764705882354,
      "grad_norm": 0.06030746549367905,
      "learning_rate": 0.0001724300441826215,
      "loss": 0.2981,
      "step": 473
    },
    {
      "epoch": 0.13941176470588235,
      "grad_norm": 0.06484637409448624,
      "learning_rate": 0.00017237113402061857,
      "loss": 0.3812,
      "step": 474
    },
    {
      "epoch": 0.13970588235294118,
      "grad_norm": 0.04798899218440056,
      "learning_rate": 0.00017231222385861563,
      "loss": 0.3134,
      "step": 475
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0365891270339489,
      "learning_rate": 0.0001722533136966127,
      "loss": 0.2699,
      "step": 476
    },
    {
      "epoch": 0.14029411764705882,
      "grad_norm": 0.04788472130894661,
      "learning_rate": 0.00017219440353460975,
      "loss": 0.3992,
      "step": 477
    },
    {
      "epoch": 0.14058823529411765,
      "grad_norm": 0.05779784545302391,
      "learning_rate": 0.00017213549337260678,
      "loss": 0.3754,
      "step": 478
    },
    {
      "epoch": 0.14088235294117646,
      "grad_norm": 0.05887613445520401,
      "learning_rate": 0.00017207658321060384,
      "loss": 0.3312,
      "step": 479
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 0.050651371479034424,
      "learning_rate": 0.0001720176730486009,
      "loss": 0.3383,
      "step": 480
    },
    {
      "epoch": 0.14147058823529413,
      "grad_norm": 0.06277495622634888,
      "learning_rate": 0.00017195876288659796,
      "loss": 0.355,
      "step": 481
    },
    {
      "epoch": 0.14176470588235293,
      "grad_norm": 0.05530104786157608,
      "learning_rate": 0.00017189985272459503,
      "loss": 0.3773,
      "step": 482
    },
    {
      "epoch": 0.14205882352941177,
      "grad_norm": 0.06142742931842804,
      "learning_rate": 0.00017184094256259203,
      "loss": 0.3146,
      "step": 483
    },
    {
      "epoch": 0.1423529411764706,
      "grad_norm": 0.05781945958733559,
      "learning_rate": 0.0001717820324005891,
      "loss": 0.3646,
      "step": 484
    },
    {
      "epoch": 0.1426470588235294,
      "grad_norm": 0.04569896683096886,
      "learning_rate": 0.00017172312223858615,
      "loss": 0.312,
      "step": 485
    },
    {
      "epoch": 0.14294117647058824,
      "grad_norm": 0.06400693207979202,
      "learning_rate": 0.0001716642120765832,
      "loss": 0.4075,
      "step": 486
    },
    {
      "epoch": 0.14323529411764707,
      "grad_norm": 0.05434136092662811,
      "learning_rate": 0.00017160530191458027,
      "loss": 0.3659,
      "step": 487
    },
    {
      "epoch": 0.14352941176470588,
      "grad_norm": 0.049584534019231796,
      "learning_rate": 0.0001715463917525773,
      "loss": 0.2925,
      "step": 488
    },
    {
      "epoch": 0.1438235294117647,
      "grad_norm": 0.06350846588611603,
      "learning_rate": 0.00017148748159057437,
      "loss": 0.3843,
      "step": 489
    },
    {
      "epoch": 0.14411764705882352,
      "grad_norm": 0.07016438245773315,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4143,
      "step": 490
    },
    {
      "epoch": 0.14441176470588235,
      "grad_norm": 0.045964185148477554,
      "learning_rate": 0.00017136966126656849,
      "loss": 0.3264,
      "step": 491
    },
    {
      "epoch": 0.14470588235294118,
      "grad_norm": 0.06972756236791611,
      "learning_rate": 0.00017131075110456555,
      "loss": 0.3941,
      "step": 492
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05731245130300522,
      "learning_rate": 0.00017125184094256258,
      "loss": 0.306,
      "step": 493
    },
    {
      "epoch": 0.14529411764705882,
      "grad_norm": 0.055932413786649704,
      "learning_rate": 0.00017119293078055964,
      "loss": 0.4001,
      "step": 494
    },
    {
      "epoch": 0.14558823529411766,
      "grad_norm": 0.05855575576424599,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.3782,
      "step": 495
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 0.043309926986694336,
      "learning_rate": 0.00017107511045655376,
      "loss": 0.2886,
      "step": 496
    },
    {
      "epoch": 0.1461764705882353,
      "grad_norm": 0.048510197550058365,
      "learning_rate": 0.00017101620029455082,
      "loss": 0.3289,
      "step": 497
    },
    {
      "epoch": 0.14647058823529413,
      "grad_norm": 0.08199948817491531,
      "learning_rate": 0.00017095729013254785,
      "loss": 0.3627,
      "step": 498
    },
    {
      "epoch": 0.14676470588235294,
      "grad_norm": 0.04825655370950699,
      "learning_rate": 0.0001708983799705449,
      "loss": 0.3373,
      "step": 499
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.04910390079021454,
      "learning_rate": 0.00017083946980854197,
      "loss": 0.3566,
      "step": 500
    },
    {
      "epoch": 0.14735294117647058,
      "grad_norm": 0.0573808029294014,
      "learning_rate": 0.00017078055964653903,
      "loss": 0.3604,
      "step": 501
    },
    {
      "epoch": 0.1476470588235294,
      "grad_norm": 0.06340555101633072,
      "learning_rate": 0.0001707216494845361,
      "loss": 0.435,
      "step": 502
    },
    {
      "epoch": 0.14794117647058824,
      "grad_norm": 0.05482769384980202,
      "learning_rate": 0.00017066273932253313,
      "loss": 0.281,
      "step": 503
    },
    {
      "epoch": 0.14823529411764705,
      "grad_norm": 0.0692114606499672,
      "learning_rate": 0.00017060382916053019,
      "loss": 0.365,
      "step": 504
    },
    {
      "epoch": 0.14852941176470588,
      "grad_norm": 0.05676070973277092,
      "learning_rate": 0.00017054491899852725,
      "loss": 0.3232,
      "step": 505
    },
    {
      "epoch": 0.14882352941176472,
      "grad_norm": 0.0686754509806633,
      "learning_rate": 0.0001704860088365243,
      "loss": 0.4192,
      "step": 506
    },
    {
      "epoch": 0.14911764705882352,
      "grad_norm": 0.05331839621067047,
      "learning_rate": 0.00017042709867452137,
      "loss": 0.3905,
      "step": 507
    },
    {
      "epoch": 0.14941176470588236,
      "grad_norm": 0.06481009721755981,
      "learning_rate": 0.0001703681885125184,
      "loss": 0.4078,
      "step": 508
    },
    {
      "epoch": 0.1497058823529412,
      "grad_norm": 0.05852285772562027,
      "learning_rate": 0.00017030927835051546,
      "loss": 0.3454,
      "step": 509
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.07182742655277252,
      "learning_rate": 0.00017025036818851252,
      "loss": 0.3649,
      "step": 510
    },
    {
      "epoch": 0.15029411764705883,
      "grad_norm": 0.05049244686961174,
      "learning_rate": 0.00017019145802650958,
      "loss": 0.3494,
      "step": 511
    },
    {
      "epoch": 0.15058823529411763,
      "grad_norm": 0.03792951628565788,
      "learning_rate": 0.00017013254786450664,
      "loss": 0.2962,
      "step": 512
    },
    {
      "epoch": 0.15088235294117647,
      "grad_norm": 0.048329319804906845,
      "learning_rate": 0.00017007363770250367,
      "loss": 0.3306,
      "step": 513
    },
    {
      "epoch": 0.1511764705882353,
      "grad_norm": 0.04937306419014931,
      "learning_rate": 0.00017001472754050073,
      "loss": 0.3415,
      "step": 514
    },
    {
      "epoch": 0.1514705882352941,
      "grad_norm": 0.05210938677191734,
      "learning_rate": 0.0001699558173784978,
      "loss": 0.3288,
      "step": 515
    },
    {
      "epoch": 0.15176470588235294,
      "grad_norm": 0.05756673961877823,
      "learning_rate": 0.00016989690721649485,
      "loss": 0.3816,
      "step": 516
    },
    {
      "epoch": 0.15205882352941177,
      "grad_norm": 0.05787445977330208,
      "learning_rate": 0.00016983799705449191,
      "loss": 0.3231,
      "step": 517
    },
    {
      "epoch": 0.15235294117647058,
      "grad_norm": 0.07375726848840714,
      "learning_rate": 0.00016977908689248895,
      "loss": 0.3619,
      "step": 518
    },
    {
      "epoch": 0.1526470588235294,
      "grad_norm": 0.05101548880338669,
      "learning_rate": 0.000169720176730486,
      "loss": 0.2977,
      "step": 519
    },
    {
      "epoch": 0.15294117647058825,
      "grad_norm": 0.0446159653365612,
      "learning_rate": 0.00016966126656848307,
      "loss": 0.2581,
      "step": 520
    },
    {
      "epoch": 0.15323529411764705,
      "grad_norm": 0.07295361906290054,
      "learning_rate": 0.00016960235640648013,
      "loss": 0.3602,
      "step": 521
    },
    {
      "epoch": 0.1535294117647059,
      "grad_norm": 0.0641077309846878,
      "learning_rate": 0.0001695434462444772,
      "loss": 0.3963,
      "step": 522
    },
    {
      "epoch": 0.1538235294117647,
      "grad_norm": 0.04510731250047684,
      "learning_rate": 0.00016948453608247422,
      "loss": 0.2946,
      "step": 523
    },
    {
      "epoch": 0.15411764705882353,
      "grad_norm": 0.03957885876297951,
      "learning_rate": 0.00016942562592047128,
      "loss": 0.3174,
      "step": 524
    },
    {
      "epoch": 0.15441176470588236,
      "grad_norm": 0.04783393815159798,
      "learning_rate": 0.00016936671575846834,
      "loss": 0.3013,
      "step": 525
    },
    {
      "epoch": 0.15470588235294117,
      "grad_norm": 0.06158773601055145,
      "learning_rate": 0.0001693078055964654,
      "loss": 0.2906,
      "step": 526
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.05866675078868866,
      "learning_rate": 0.00016924889543446246,
      "loss": 0.3695,
      "step": 527
    },
    {
      "epoch": 0.15529411764705883,
      "grad_norm": 0.04990405589342117,
      "learning_rate": 0.0001691899852724595,
      "loss": 0.326,
      "step": 528
    },
    {
      "epoch": 0.15558823529411764,
      "grad_norm": 0.06734286993741989,
      "learning_rate": 0.00016913107511045655,
      "loss": 0.4319,
      "step": 529
    },
    {
      "epoch": 0.15588235294117647,
      "grad_norm": 0.0512085035443306,
      "learning_rate": 0.00016907216494845361,
      "loss": 0.2857,
      "step": 530
    },
    {
      "epoch": 0.1561764705882353,
      "grad_norm": 0.04871075227856636,
      "learning_rate": 0.00016901325478645068,
      "loss": 0.3725,
      "step": 531
    },
    {
      "epoch": 0.1564705882352941,
      "grad_norm": 0.05425991117954254,
      "learning_rate": 0.00016895434462444774,
      "loss": 0.396,
      "step": 532
    },
    {
      "epoch": 0.15676470588235294,
      "grad_norm": 0.04608534649014473,
      "learning_rate": 0.00016889543446244477,
      "loss": 0.3113,
      "step": 533
    },
    {
      "epoch": 0.15705882352941178,
      "grad_norm": 0.06256171315908432,
      "learning_rate": 0.00016883652430044183,
      "loss": 0.3607,
      "step": 534
    },
    {
      "epoch": 0.15735294117647058,
      "grad_norm": 0.057181719690561295,
      "learning_rate": 0.0001687776141384389,
      "loss": 0.3743,
      "step": 535
    },
    {
      "epoch": 0.15764705882352942,
      "grad_norm": 0.048688970506191254,
      "learning_rate": 0.00016871870397643595,
      "loss": 0.3205,
      "step": 536
    },
    {
      "epoch": 0.15794117647058822,
      "grad_norm": 0.05092523992061615,
      "learning_rate": 0.000168659793814433,
      "loss": 0.3557,
      "step": 537
    },
    {
      "epoch": 0.15823529411764706,
      "grad_norm": 0.047759998589754105,
      "learning_rate": 0.00016860088365243004,
      "loss": 0.3423,
      "step": 538
    },
    {
      "epoch": 0.1585294117647059,
      "grad_norm": 0.048950258642435074,
      "learning_rate": 0.0001685419734904271,
      "loss": 0.347,
      "step": 539
    },
    {
      "epoch": 0.1588235294117647,
      "grad_norm": 0.04936043918132782,
      "learning_rate": 0.00016848306332842416,
      "loss": 0.3267,
      "step": 540
    },
    {
      "epoch": 0.15911764705882353,
      "grad_norm": 0.07207466661930084,
      "learning_rate": 0.00016842415316642122,
      "loss": 0.3581,
      "step": 541
    },
    {
      "epoch": 0.15941176470588236,
      "grad_norm": 0.057818200439214706,
      "learning_rate": 0.00016836524300441828,
      "loss": 0.2711,
      "step": 542
    },
    {
      "epoch": 0.15970588235294117,
      "grad_norm": 0.06064300984144211,
      "learning_rate": 0.00016830633284241532,
      "loss": 0.4004,
      "step": 543
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.06050284579396248,
      "learning_rate": 0.00016824742268041238,
      "loss": 0.3694,
      "step": 544
    },
    {
      "epoch": 0.16029411764705884,
      "grad_norm": 0.03625744581222534,
      "learning_rate": 0.00016818851251840944,
      "loss": 0.2673,
      "step": 545
    },
    {
      "epoch": 0.16058823529411764,
      "grad_norm": 0.05593220889568329,
      "learning_rate": 0.0001681296023564065,
      "loss": 0.3553,
      "step": 546
    },
    {
      "epoch": 0.16088235294117648,
      "grad_norm": 0.045184176415205,
      "learning_rate": 0.00016807069219440356,
      "loss": 0.2528,
      "step": 547
    },
    {
      "epoch": 0.16117647058823528,
      "grad_norm": 0.053242817521095276,
      "learning_rate": 0.0001680117820324006,
      "loss": 0.2954,
      "step": 548
    },
    {
      "epoch": 0.16147058823529412,
      "grad_norm": 0.05241779237985611,
      "learning_rate": 0.00016795287187039765,
      "loss": 0.3569,
      "step": 549
    },
    {
      "epoch": 0.16176470588235295,
      "grad_norm": 0.04948525130748749,
      "learning_rate": 0.0001678939617083947,
      "loss": 0.3856,
      "step": 550
    },
    {
      "epoch": 0.16205882352941176,
      "grad_norm": 0.04558991640806198,
      "learning_rate": 0.00016783505154639177,
      "loss": 0.3405,
      "step": 551
    },
    {
      "epoch": 0.1623529411764706,
      "grad_norm": 0.04469487816095352,
      "learning_rate": 0.00016777614138438883,
      "loss": 0.31,
      "step": 552
    },
    {
      "epoch": 0.16264705882352942,
      "grad_norm": 0.05471348389983177,
      "learning_rate": 0.00016771723122238586,
      "loss": 0.2669,
      "step": 553
    },
    {
      "epoch": 0.16294117647058823,
      "grad_norm": 0.0637243464589119,
      "learning_rate": 0.00016765832106038292,
      "loss": 0.4418,
      "step": 554
    },
    {
      "epoch": 0.16323529411764706,
      "grad_norm": 0.0639851912856102,
      "learning_rate": 0.00016759941089837998,
      "loss": 0.4218,
      "step": 555
    },
    {
      "epoch": 0.1635294117647059,
      "grad_norm": 0.0473896823823452,
      "learning_rate": 0.00016754050073637704,
      "loss": 0.316,
      "step": 556
    },
    {
      "epoch": 0.1638235294117647,
      "grad_norm": 0.058715976774692535,
      "learning_rate": 0.0001674815905743741,
      "loss": 0.322,
      "step": 557
    },
    {
      "epoch": 0.16411764705882353,
      "grad_norm": 0.05654335021972656,
      "learning_rate": 0.00016742268041237114,
      "loss": 0.3763,
      "step": 558
    },
    {
      "epoch": 0.16441176470588234,
      "grad_norm": 0.058329492807388306,
      "learning_rate": 0.0001673637702503682,
      "loss": 0.3692,
      "step": 559
    },
    {
      "epoch": 0.16470588235294117,
      "grad_norm": 0.05786561220884323,
      "learning_rate": 0.00016730486008836526,
      "loss": 0.4071,
      "step": 560
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.0611722432076931,
      "learning_rate": 0.00016724594992636232,
      "loss": 0.3551,
      "step": 561
    },
    {
      "epoch": 0.1652941176470588,
      "grad_norm": 0.06549215316772461,
      "learning_rate": 0.00016718703976435938,
      "loss": 0.3571,
      "step": 562
    },
    {
      "epoch": 0.16558823529411765,
      "grad_norm": 0.06165051460266113,
      "learning_rate": 0.0001671281296023564,
      "loss": 0.4217,
      "step": 563
    },
    {
      "epoch": 0.16588235294117648,
      "grad_norm": 0.047660134732723236,
      "learning_rate": 0.00016706921944035347,
      "loss": 0.3362,
      "step": 564
    },
    {
      "epoch": 0.1661764705882353,
      "grad_norm": 0.05546798184514046,
      "learning_rate": 0.00016701030927835053,
      "loss": 0.338,
      "step": 565
    },
    {
      "epoch": 0.16647058823529412,
      "grad_norm": 0.0607416033744812,
      "learning_rate": 0.0001669513991163476,
      "loss": 0.399,
      "step": 566
    },
    {
      "epoch": 0.16676470588235295,
      "grad_norm": 0.05128588527441025,
      "learning_rate": 0.00016689248895434465,
      "loss": 0.3608,
      "step": 567
    },
    {
      "epoch": 0.16705882352941176,
      "grad_norm": 0.049430716782808304,
      "learning_rate": 0.00016683357879234168,
      "loss": 0.3231,
      "step": 568
    },
    {
      "epoch": 0.1673529411764706,
      "grad_norm": 0.06465287506580353,
      "learning_rate": 0.00016677466863033874,
      "loss": 0.2996,
      "step": 569
    },
    {
      "epoch": 0.1676470588235294,
      "grad_norm": 0.06682240962982178,
      "learning_rate": 0.0001667157584683358,
      "loss": 0.3194,
      "step": 570
    },
    {
      "epoch": 0.16794117647058823,
      "grad_norm": 0.061273884028196335,
      "learning_rate": 0.00016665684830633286,
      "loss": 0.4112,
      "step": 571
    },
    {
      "epoch": 0.16823529411764707,
      "grad_norm": 0.05382528901100159,
      "learning_rate": 0.00016659793814432993,
      "loss": 0.3636,
      "step": 572
    },
    {
      "epoch": 0.16852941176470587,
      "grad_norm": 0.06397291272878647,
      "learning_rate": 0.00016653902798232696,
      "loss": 0.3793,
      "step": 573
    },
    {
      "epoch": 0.1688235294117647,
      "grad_norm": 0.051190584897994995,
      "learning_rate": 0.00016648011782032402,
      "loss": 0.3307,
      "step": 574
    },
    {
      "epoch": 0.16911764705882354,
      "grad_norm": 0.05878235399723053,
      "learning_rate": 0.00016642120765832108,
      "loss": 0.4074,
      "step": 575
    },
    {
      "epoch": 0.16941176470588235,
      "grad_norm": 0.05806456133723259,
      "learning_rate": 0.00016636229749631814,
      "loss": 0.395,
      "step": 576
    },
    {
      "epoch": 0.16970588235294118,
      "grad_norm": 0.057401202619075775,
      "learning_rate": 0.0001663033873343152,
      "loss": 0.4385,
      "step": 577
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.04135361313819885,
      "learning_rate": 0.00016624447717231223,
      "loss": 0.2694,
      "step": 578
    },
    {
      "epoch": 0.17029411764705882,
      "grad_norm": 0.04863870516419411,
      "learning_rate": 0.0001661855670103093,
      "loss": 0.4052,
      "step": 579
    },
    {
      "epoch": 0.17058823529411765,
      "grad_norm": 0.04994136095046997,
      "learning_rate": 0.00016612665684830635,
      "loss": 0.3556,
      "step": 580
    },
    {
      "epoch": 0.17088235294117646,
      "grad_norm": 0.04862120747566223,
      "learning_rate": 0.0001660677466863034,
      "loss": 0.2896,
      "step": 581
    },
    {
      "epoch": 0.1711764705882353,
      "grad_norm": 0.04310249909758568,
      "learning_rate": 0.00016600883652430047,
      "loss": 0.3386,
      "step": 582
    },
    {
      "epoch": 0.17147058823529412,
      "grad_norm": 0.05747274309396744,
      "learning_rate": 0.0001659499263622975,
      "loss": 0.3518,
      "step": 583
    },
    {
      "epoch": 0.17176470588235293,
      "grad_norm": 0.06045641377568245,
      "learning_rate": 0.00016589101620029457,
      "loss": 0.3816,
      "step": 584
    },
    {
      "epoch": 0.17205882352941176,
      "grad_norm": 0.04873727634549141,
      "learning_rate": 0.00016583210603829163,
      "loss": 0.3588,
      "step": 585
    },
    {
      "epoch": 0.1723529411764706,
      "grad_norm": 0.05270306020975113,
      "learning_rate": 0.00016577319587628869,
      "loss": 0.4331,
      "step": 586
    },
    {
      "epoch": 0.1726470588235294,
      "grad_norm": 0.05631241202354431,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.3889,
      "step": 587
    },
    {
      "epoch": 0.17294117647058824,
      "grad_norm": 0.03727398067712784,
      "learning_rate": 0.00016565537555228278,
      "loss": 0.2899,
      "step": 588
    },
    {
      "epoch": 0.17323529411764707,
      "grad_norm": 0.05661847069859505,
      "learning_rate": 0.0001655964653902798,
      "loss": 0.4203,
      "step": 589
    },
    {
      "epoch": 0.17352941176470588,
      "grad_norm": 0.04643791913986206,
      "learning_rate": 0.00016553755522827687,
      "loss": 0.293,
      "step": 590
    },
    {
      "epoch": 0.1738235294117647,
      "grad_norm": 0.03908791020512581,
      "learning_rate": 0.00016547864506627393,
      "loss": 0.2877,
      "step": 591
    },
    {
      "epoch": 0.17411764705882352,
      "grad_norm": 0.047371864318847656,
      "learning_rate": 0.000165419734904271,
      "loss": 0.36,
      "step": 592
    },
    {
      "epoch": 0.17441176470588235,
      "grad_norm": 0.05554131790995598,
      "learning_rate": 0.00016536082474226803,
      "loss": 0.3657,
      "step": 593
    },
    {
      "epoch": 0.17470588235294118,
      "grad_norm": 0.057916540652513504,
      "learning_rate": 0.00016530191458026509,
      "loss": 0.3397,
      "step": 594
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.047578662633895874,
      "learning_rate": 0.00016524300441826215,
      "loss": 0.407,
      "step": 595
    },
    {
      "epoch": 0.17529411764705882,
      "grad_norm": 0.0585145503282547,
      "learning_rate": 0.0001651840942562592,
      "loss": 0.3813,
      "step": 596
    },
    {
      "epoch": 0.17558823529411766,
      "grad_norm": 0.0558103583753109,
      "learning_rate": 0.00016512518409425627,
      "loss": 0.3802,
      "step": 597
    },
    {
      "epoch": 0.17588235294117646,
      "grad_norm": 0.06309595704078674,
      "learning_rate": 0.0001650662739322533,
      "loss": 0.3633,
      "step": 598
    },
    {
      "epoch": 0.1761764705882353,
      "grad_norm": 0.0574399009346962,
      "learning_rate": 0.00016500736377025036,
      "loss": 0.3596,
      "step": 599
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.06513906270265579,
      "learning_rate": 0.00016494845360824742,
      "loss": 0.3743,
      "step": 600
    },
    {
      "epoch": 0.17676470588235293,
      "grad_norm": 0.05603781342506409,
      "learning_rate": 0.00016488954344624448,
      "loss": 0.4191,
      "step": 601
    },
    {
      "epoch": 0.17705882352941177,
      "grad_norm": 0.06779880821704865,
      "learning_rate": 0.00016483063328424154,
      "loss": 0.4241,
      "step": 602
    },
    {
      "epoch": 0.1773529411764706,
      "grad_norm": 0.050699032843112946,
      "learning_rate": 0.00016477172312223857,
      "loss": 0.3412,
      "step": 603
    },
    {
      "epoch": 0.1776470588235294,
      "grad_norm": 0.046759288758039474,
      "learning_rate": 0.00016471281296023563,
      "loss": 0.3516,
      "step": 604
    },
    {
      "epoch": 0.17794117647058824,
      "grad_norm": 0.061953917145729065,
      "learning_rate": 0.0001646539027982327,
      "loss": 0.3492,
      "step": 605
    },
    {
      "epoch": 0.17823529411764705,
      "grad_norm": 0.06239992380142212,
      "learning_rate": 0.00016459499263622975,
      "loss": 0.4161,
      "step": 606
    },
    {
      "epoch": 0.17852941176470588,
      "grad_norm": 0.052264634519815445,
      "learning_rate": 0.00016453608247422681,
      "loss": 0.4153,
      "step": 607
    },
    {
      "epoch": 0.17882352941176471,
      "grad_norm": 0.05317016318440437,
      "learning_rate": 0.00016447717231222385,
      "loss": 0.3983,
      "step": 608
    },
    {
      "epoch": 0.17911764705882352,
      "grad_norm": 0.03877752646803856,
      "learning_rate": 0.0001644182621502209,
      "loss": 0.2536,
      "step": 609
    },
    {
      "epoch": 0.17941176470588235,
      "grad_norm": 0.06518052518367767,
      "learning_rate": 0.00016435935198821797,
      "loss": 0.3774,
      "step": 610
    },
    {
      "epoch": 0.1797058823529412,
      "grad_norm": 0.051028233021497726,
      "learning_rate": 0.00016430044182621503,
      "loss": 0.2934,
      "step": 611
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.058856405317783356,
      "learning_rate": 0.0001642415316642121,
      "loss": 0.333,
      "step": 612
    },
    {
      "epoch": 0.18029411764705883,
      "grad_norm": 0.069431371986866,
      "learning_rate": 0.00016418262150220912,
      "loss": 0.404,
      "step": 613
    },
    {
      "epoch": 0.18058823529411766,
      "grad_norm": 0.04496341571211815,
      "learning_rate": 0.00016412371134020618,
      "loss": 0.3801,
      "step": 614
    },
    {
      "epoch": 0.18088235294117647,
      "grad_norm": 0.05607542395591736,
      "learning_rate": 0.00016406480117820324,
      "loss": 0.4137,
      "step": 615
    },
    {
      "epoch": 0.1811764705882353,
      "grad_norm": 0.06706245243549347,
      "learning_rate": 0.0001640058910162003,
      "loss": 0.3449,
      "step": 616
    },
    {
      "epoch": 0.1814705882352941,
      "grad_norm": 0.05818209797143936,
      "learning_rate": 0.00016394698085419736,
      "loss": 0.3864,
      "step": 617
    },
    {
      "epoch": 0.18176470588235294,
      "grad_norm": 0.06470128148794174,
      "learning_rate": 0.0001638880706921944,
      "loss": 0.3954,
      "step": 618
    },
    {
      "epoch": 0.18205882352941177,
      "grad_norm": 0.042941074818372726,
      "learning_rate": 0.00016382916053019145,
      "loss": 0.3039,
      "step": 619
    },
    {
      "epoch": 0.18235294117647058,
      "grad_norm": 0.04951810464262962,
      "learning_rate": 0.00016377025036818851,
      "loss": 0.377,
      "step": 620
    },
    {
      "epoch": 0.1826470588235294,
      "grad_norm": 0.041761960834264755,
      "learning_rate": 0.00016371134020618558,
      "loss": 0.3099,
      "step": 621
    },
    {
      "epoch": 0.18294117647058825,
      "grad_norm": 0.056583743542432785,
      "learning_rate": 0.00016365243004418264,
      "loss": 0.3964,
      "step": 622
    },
    {
      "epoch": 0.18323529411764705,
      "grad_norm": 0.05110488459467888,
      "learning_rate": 0.00016359351988217967,
      "loss": 0.3493,
      "step": 623
    },
    {
      "epoch": 0.18352941176470589,
      "grad_norm": 0.05364065617322922,
      "learning_rate": 0.00016353460972017673,
      "loss": 0.3701,
      "step": 624
    },
    {
      "epoch": 0.18382352941176472,
      "grad_norm": 0.04595716670155525,
      "learning_rate": 0.0001634756995581738,
      "loss": 0.3718,
      "step": 625
    },
    {
      "epoch": 0.18411764705882352,
      "grad_norm": 0.057279326021671295,
      "learning_rate": 0.00016341678939617085,
      "loss": 0.3789,
      "step": 626
    },
    {
      "epoch": 0.18441176470588236,
      "grad_norm": 0.0705619603395462,
      "learning_rate": 0.0001633578792341679,
      "loss": 0.347,
      "step": 627
    },
    {
      "epoch": 0.18470588235294116,
      "grad_norm": 0.058939144015312195,
      "learning_rate": 0.00016329896907216494,
      "loss": 0.3731,
      "step": 628
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06253059208393097,
      "learning_rate": 0.000163240058910162,
      "loss": 0.3742,
      "step": 629
    },
    {
      "epoch": 0.18529411764705883,
      "grad_norm": 0.05989518016576767,
      "learning_rate": 0.00016318114874815906,
      "loss": 0.4148,
      "step": 630
    },
    {
      "epoch": 0.18558823529411764,
      "grad_norm": 0.04479848966002464,
      "learning_rate": 0.00016312223858615612,
      "loss": 0.2803,
      "step": 631
    },
    {
      "epoch": 0.18588235294117647,
      "grad_norm": 0.04671787470579147,
      "learning_rate": 0.00016306332842415318,
      "loss": 0.3291,
      "step": 632
    },
    {
      "epoch": 0.1861764705882353,
      "grad_norm": 0.058054644614458084,
      "learning_rate": 0.00016300441826215022,
      "loss": 0.334,
      "step": 633
    },
    {
      "epoch": 0.1864705882352941,
      "grad_norm": 0.04337230697274208,
      "learning_rate": 0.00016294550810014728,
      "loss": 0.3077,
      "step": 634
    },
    {
      "epoch": 0.18676470588235294,
      "grad_norm": 0.042194150388240814,
      "learning_rate": 0.00016288659793814434,
      "loss": 0.258,
      "step": 635
    },
    {
      "epoch": 0.18705882352941178,
      "grad_norm": 0.058749184012413025,
      "learning_rate": 0.0001628276877761414,
      "loss": 0.3678,
      "step": 636
    },
    {
      "epoch": 0.18735294117647058,
      "grad_norm": 0.0942620038986206,
      "learning_rate": 0.00016276877761413846,
      "loss": 0.4247,
      "step": 637
    },
    {
      "epoch": 0.18764705882352942,
      "grad_norm": 0.05189971625804901,
      "learning_rate": 0.0001627098674521355,
      "loss": 0.334,
      "step": 638
    },
    {
      "epoch": 0.18794117647058822,
      "grad_norm": 0.04549415037035942,
      "learning_rate": 0.00016265095729013255,
      "loss": 0.2735,
      "step": 639
    },
    {
      "epoch": 0.18823529411764706,
      "grad_norm": 0.0767633393406868,
      "learning_rate": 0.0001625920471281296,
      "loss": 0.3966,
      "step": 640
    },
    {
      "epoch": 0.1885294117647059,
      "grad_norm": 0.06203904747962952,
      "learning_rate": 0.00016253313696612667,
      "loss": 0.3704,
      "step": 641
    },
    {
      "epoch": 0.1888235294117647,
      "grad_norm": 0.05918017402291298,
      "learning_rate": 0.00016247422680412373,
      "loss": 0.4162,
      "step": 642
    },
    {
      "epoch": 0.18911764705882353,
      "grad_norm": 0.07025813311338425,
      "learning_rate": 0.00016241531664212076,
      "loss": 0.3962,
      "step": 643
    },
    {
      "epoch": 0.18941176470588236,
      "grad_norm": 0.050980519503355026,
      "learning_rate": 0.00016235640648011782,
      "loss": 0.3154,
      "step": 644
    },
    {
      "epoch": 0.18970588235294117,
      "grad_norm": 0.08420702069997787,
      "learning_rate": 0.00016229749631811488,
      "loss": 0.3729,
      "step": 645
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.0650554746389389,
      "learning_rate": 0.00016223858615611194,
      "loss": 0.3608,
      "step": 646
    },
    {
      "epoch": 0.19029411764705884,
      "grad_norm": 0.06597752869129181,
      "learning_rate": 0.000162179675994109,
      "loss": 0.423,
      "step": 647
    },
    {
      "epoch": 0.19058823529411764,
      "grad_norm": 0.07035893946886063,
      "learning_rate": 0.00016212076583210604,
      "loss": 0.3983,
      "step": 648
    },
    {
      "epoch": 0.19088235294117648,
      "grad_norm": 0.06521248817443848,
      "learning_rate": 0.0001620618556701031,
      "loss": 0.3578,
      "step": 649
    },
    {
      "epoch": 0.19117647058823528,
      "grad_norm": 0.05368654429912567,
      "learning_rate": 0.00016200294550810016,
      "loss": 0.3659,
      "step": 650
    },
    {
      "epoch": 0.19147058823529411,
      "grad_norm": 0.06413117051124573,
      "learning_rate": 0.00016194403534609722,
      "loss": 0.3866,
      "step": 651
    },
    {
      "epoch": 0.19176470588235295,
      "grad_norm": 0.07158608734607697,
      "learning_rate": 0.00016188512518409428,
      "loss": 0.3757,
      "step": 652
    },
    {
      "epoch": 0.19205882352941175,
      "grad_norm": 0.06080062687397003,
      "learning_rate": 0.0001618262150220913,
      "loss": 0.3665,
      "step": 653
    },
    {
      "epoch": 0.1923529411764706,
      "grad_norm": 0.06375561654567719,
      "learning_rate": 0.00016176730486008837,
      "loss": 0.3655,
      "step": 654
    },
    {
      "epoch": 0.19264705882352942,
      "grad_norm": 0.04518198221921921,
      "learning_rate": 0.00016170839469808543,
      "loss": 0.3282,
      "step": 655
    },
    {
      "epoch": 0.19294117647058823,
      "grad_norm": 0.03015110455453396,
      "learning_rate": 0.0001616494845360825,
      "loss": 0.193,
      "step": 656
    },
    {
      "epoch": 0.19323529411764706,
      "grad_norm": 0.05787768214941025,
      "learning_rate": 0.00016159057437407955,
      "loss": 0.329,
      "step": 657
    },
    {
      "epoch": 0.1935294117647059,
      "grad_norm": 0.07779692113399506,
      "learning_rate": 0.00016153166421207658,
      "loss": 0.3769,
      "step": 658
    },
    {
      "epoch": 0.1938235294117647,
      "grad_norm": 0.05952644348144531,
      "learning_rate": 0.00016147275405007364,
      "loss": 0.3808,
      "step": 659
    },
    {
      "epoch": 0.19411764705882353,
      "grad_norm": 0.05178989842534065,
      "learning_rate": 0.0001614138438880707,
      "loss": 0.3536,
      "step": 660
    },
    {
      "epoch": 0.19441176470588234,
      "grad_norm": 0.08462893217802048,
      "learning_rate": 0.00016135493372606776,
      "loss": 0.3968,
      "step": 661
    },
    {
      "epoch": 0.19470588235294117,
      "grad_norm": 0.05676606670022011,
      "learning_rate": 0.00016129602356406482,
      "loss": 0.2855,
      "step": 662
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.07089997828006744,
      "learning_rate": 0.00016123711340206186,
      "loss": 0.3985,
      "step": 663
    },
    {
      "epoch": 0.1952941176470588,
      "grad_norm": 0.062299974262714386,
      "learning_rate": 0.00016117820324005892,
      "loss": 0.3651,
      "step": 664
    },
    {
      "epoch": 0.19558823529411765,
      "grad_norm": 0.05192005634307861,
      "learning_rate": 0.00016111929307805598,
      "loss": 0.3561,
      "step": 665
    },
    {
      "epoch": 0.19588235294117648,
      "grad_norm": 0.054980747401714325,
      "learning_rate": 0.00016106038291605304,
      "loss": 0.3278,
      "step": 666
    },
    {
      "epoch": 0.19617647058823529,
      "grad_norm": 0.06903820484876633,
      "learning_rate": 0.0001610014727540501,
      "loss": 0.3621,
      "step": 667
    },
    {
      "epoch": 0.19647058823529412,
      "grad_norm": 0.0746089294552803,
      "learning_rate": 0.00016094256259204713,
      "loss": 0.3975,
      "step": 668
    },
    {
      "epoch": 0.19676470588235295,
      "grad_norm": 0.05769539251923561,
      "learning_rate": 0.0001608836524300442,
      "loss": 0.3656,
      "step": 669
    },
    {
      "epoch": 0.19705882352941176,
      "grad_norm": 0.04117922857403755,
      "learning_rate": 0.00016082474226804125,
      "loss": 0.3075,
      "step": 670
    },
    {
      "epoch": 0.1973529411764706,
      "grad_norm": 0.06422600895166397,
      "learning_rate": 0.0001607658321060383,
      "loss": 0.3586,
      "step": 671
    },
    {
      "epoch": 0.1976470588235294,
      "grad_norm": 0.07708392292261124,
      "learning_rate": 0.00016070692194403537,
      "loss": 0.3325,
      "step": 672
    },
    {
      "epoch": 0.19794117647058823,
      "grad_norm": 0.0519091822206974,
      "learning_rate": 0.0001606480117820324,
      "loss": 0.3443,
      "step": 673
    },
    {
      "epoch": 0.19823529411764707,
      "grad_norm": 0.045184679329395294,
      "learning_rate": 0.00016058910162002947,
      "loss": 0.3754,
      "step": 674
    },
    {
      "epoch": 0.19852941176470587,
      "grad_norm": 0.06444626301527023,
      "learning_rate": 0.00016053019145802653,
      "loss": 0.411,
      "step": 675
    },
    {
      "epoch": 0.1988235294117647,
      "grad_norm": 0.06421050429344177,
      "learning_rate": 0.00016047128129602359,
      "loss": 0.4091,
      "step": 676
    },
    {
      "epoch": 0.19911764705882354,
      "grad_norm": 0.044406335800886154,
      "learning_rate": 0.00016041237113402065,
      "loss": 0.3392,
      "step": 677
    },
    {
      "epoch": 0.19941176470588234,
      "grad_norm": 0.04102481156587601,
      "learning_rate": 0.00016035346097201768,
      "loss": 0.3186,
      "step": 678
    },
    {
      "epoch": 0.19970588235294118,
      "grad_norm": 0.05563235282897949,
      "learning_rate": 0.00016029455081001474,
      "loss": 0.4116,
      "step": 679
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05457448959350586,
      "learning_rate": 0.0001602356406480118,
      "loss": 0.3439,
      "step": 680
    },
    {
      "epoch": 0.20029411764705882,
      "grad_norm": 0.07011327892541885,
      "learning_rate": 0.00016017673048600886,
      "loss": 0.4086,
      "step": 681
    },
    {
      "epoch": 0.20058823529411765,
      "grad_norm": 0.04658959433436394,
      "learning_rate": 0.00016011782032400592,
      "loss": 0.3662,
      "step": 682
    },
    {
      "epoch": 0.20088235294117648,
      "grad_norm": 0.04095442593097687,
      "learning_rate": 0.00016005891016200295,
      "loss": 0.3566,
      "step": 683
    },
    {
      "epoch": 0.2011764705882353,
      "grad_norm": 0.04490886256098747,
      "learning_rate": 0.00016,
      "loss": 0.2941,
      "step": 684
    },
    {
      "epoch": 0.20147058823529412,
      "grad_norm": 0.05372260510921478,
      "learning_rate": 0.00015994108983799707,
      "loss": 0.2977,
      "step": 685
    },
    {
      "epoch": 0.20176470588235293,
      "grad_norm": 0.061746738851070404,
      "learning_rate": 0.00015988217967599413,
      "loss": 0.3786,
      "step": 686
    },
    {
      "epoch": 0.20205882352941176,
      "grad_norm": 0.04059889167547226,
      "learning_rate": 0.0001598232695139912,
      "loss": 0.3245,
      "step": 687
    },
    {
      "epoch": 0.2023529411764706,
      "grad_norm": 0.04672684147953987,
      "learning_rate": 0.00015976435935198823,
      "loss": 0.2976,
      "step": 688
    },
    {
      "epoch": 0.2026470588235294,
      "grad_norm": 0.04401245340704918,
      "learning_rate": 0.0001597054491899853,
      "loss": 0.3552,
      "step": 689
    },
    {
      "epoch": 0.20294117647058824,
      "grad_norm": 0.04471776634454727,
      "learning_rate": 0.00015964653902798235,
      "loss": 0.2795,
      "step": 690
    },
    {
      "epoch": 0.20323529411764707,
      "grad_norm": 0.04701891914010048,
      "learning_rate": 0.0001595876288659794,
      "loss": 0.2986,
      "step": 691
    },
    {
      "epoch": 0.20352941176470588,
      "grad_norm": 0.05937869846820831,
      "learning_rate": 0.00015952871870397647,
      "loss": 0.3862,
      "step": 692
    },
    {
      "epoch": 0.2038235294117647,
      "grad_norm": 0.05754106491804123,
      "learning_rate": 0.0001594698085419735,
      "loss": 0.4036,
      "step": 693
    },
    {
      "epoch": 0.20411764705882354,
      "grad_norm": 0.058496080338954926,
      "learning_rate": 0.00015941089837997056,
      "loss": 0.3581,
      "step": 694
    },
    {
      "epoch": 0.20441176470588235,
      "grad_norm": 0.04878697171807289,
      "learning_rate": 0.0001593519882179676,
      "loss": 0.3516,
      "step": 695
    },
    {
      "epoch": 0.20470588235294118,
      "grad_norm": 0.0696541965007782,
      "learning_rate": 0.00015929307805596465,
      "loss": 0.3769,
      "step": 696
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.056499652564525604,
      "learning_rate": 0.00015923416789396171,
      "loss": 0.4029,
      "step": 697
    },
    {
      "epoch": 0.20529411764705882,
      "grad_norm": 0.04655205085873604,
      "learning_rate": 0.00015917525773195875,
      "loss": 0.338,
      "step": 698
    },
    {
      "epoch": 0.20558823529411765,
      "grad_norm": 0.048547763377428055,
      "learning_rate": 0.0001591163475699558,
      "loss": 0.3299,
      "step": 699
    },
    {
      "epoch": 0.20588235294117646,
      "grad_norm": 0.052729442715644836,
      "learning_rate": 0.00015905743740795287,
      "loss": 0.3723,
      "step": 700
    },
    {
      "epoch": 0.2061764705882353,
      "grad_norm": 0.054548896849155426,
      "learning_rate": 0.00015899852724594993,
      "loss": 0.3455,
      "step": 701
    },
    {
      "epoch": 0.20647058823529413,
      "grad_norm": 0.05383972078561783,
      "learning_rate": 0.000158939617083947,
      "loss": 0.3479,
      "step": 702
    },
    {
      "epoch": 0.20676470588235293,
      "grad_norm": 0.058088261634111404,
      "learning_rate": 0.00015888070692194402,
      "loss": 0.3416,
      "step": 703
    },
    {
      "epoch": 0.20705882352941177,
      "grad_norm": 0.05009713023900986,
      "learning_rate": 0.00015882179675994108,
      "loss": 0.3278,
      "step": 704
    },
    {
      "epoch": 0.2073529411764706,
      "grad_norm": 0.07080308347940445,
      "learning_rate": 0.00015876288659793814,
      "loss": 0.4768,
      "step": 705
    },
    {
      "epoch": 0.2076470588235294,
      "grad_norm": 0.05786992609500885,
      "learning_rate": 0.0001587039764359352,
      "loss": 0.3883,
      "step": 706
    },
    {
      "epoch": 0.20794117647058824,
      "grad_norm": 0.05259132757782936,
      "learning_rate": 0.00015864506627393226,
      "loss": 0.3626,
      "step": 707
    },
    {
      "epoch": 0.20823529411764705,
      "grad_norm": 0.04518746957182884,
      "learning_rate": 0.0001585861561119293,
      "loss": 0.2702,
      "step": 708
    },
    {
      "epoch": 0.20852941176470588,
      "grad_norm": 0.060879115015268326,
      "learning_rate": 0.00015852724594992635,
      "loss": 0.4126,
      "step": 709
    },
    {
      "epoch": 0.2088235294117647,
      "grad_norm": 0.040087949484586716,
      "learning_rate": 0.00015846833578792341,
      "loss": 0.2942,
      "step": 710
    },
    {
      "epoch": 0.20911764705882352,
      "grad_norm": 0.052076999098062515,
      "learning_rate": 0.00015840942562592047,
      "loss": 0.3178,
      "step": 711
    },
    {
      "epoch": 0.20941176470588235,
      "grad_norm": 0.07085290551185608,
      "learning_rate": 0.00015835051546391754,
      "loss": 0.384,
      "step": 712
    },
    {
      "epoch": 0.2097058823529412,
      "grad_norm": 0.047834236174821854,
      "learning_rate": 0.00015829160530191457,
      "loss": 0.3514,
      "step": 713
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.042144037783145905,
      "learning_rate": 0.00015823269513991163,
      "loss": 0.3307,
      "step": 714
    },
    {
      "epoch": 0.21029411764705883,
      "grad_norm": 0.05275077745318413,
      "learning_rate": 0.0001581737849779087,
      "loss": 0.3057,
      "step": 715
    },
    {
      "epoch": 0.21058823529411766,
      "grad_norm": 0.05132244527339935,
      "learning_rate": 0.00015811487481590575,
      "loss": 0.3511,
      "step": 716
    },
    {
      "epoch": 0.21088235294117647,
      "grad_norm": 0.06462177634239197,
      "learning_rate": 0.0001580559646539028,
      "loss": 0.3989,
      "step": 717
    },
    {
      "epoch": 0.2111764705882353,
      "grad_norm": 0.053084198385477066,
      "learning_rate": 0.00015799705449189984,
      "loss": 0.4056,
      "step": 718
    },
    {
      "epoch": 0.2114705882352941,
      "grad_norm": 0.04468193277716637,
      "learning_rate": 0.0001579381443298969,
      "loss": 0.3388,
      "step": 719
    },
    {
      "epoch": 0.21176470588235294,
      "grad_norm": 0.049599166959524155,
      "learning_rate": 0.00015787923416789396,
      "loss": 0.3569,
      "step": 720
    },
    {
      "epoch": 0.21205882352941177,
      "grad_norm": 0.04233685880899429,
      "learning_rate": 0.00015782032400589102,
      "loss": 0.2935,
      "step": 721
    },
    {
      "epoch": 0.21235294117647058,
      "grad_norm": 0.057112377136945724,
      "learning_rate": 0.00015776141384388808,
      "loss": 0.3491,
      "step": 722
    },
    {
      "epoch": 0.2126470588235294,
      "grad_norm": 0.060387685894966125,
      "learning_rate": 0.00015770250368188512,
      "loss": 0.391,
      "step": 723
    },
    {
      "epoch": 0.21294117647058824,
      "grad_norm": 0.05470983684062958,
      "learning_rate": 0.00015764359351988218,
      "loss": 0.3195,
      "step": 724
    },
    {
      "epoch": 0.21323529411764705,
      "grad_norm": 0.04368290677666664,
      "learning_rate": 0.00015758468335787924,
      "loss": 0.2999,
      "step": 725
    },
    {
      "epoch": 0.21352941176470588,
      "grad_norm": 0.05471691116690636,
      "learning_rate": 0.0001575257731958763,
      "loss": 0.2943,
      "step": 726
    },
    {
      "epoch": 0.21382352941176472,
      "grad_norm": 0.05918719246983528,
      "learning_rate": 0.00015746686303387336,
      "loss": 0.3969,
      "step": 727
    },
    {
      "epoch": 0.21411764705882352,
      "grad_norm": 0.05261335149407387,
      "learning_rate": 0.0001574079528718704,
      "loss": 0.3862,
      "step": 728
    },
    {
      "epoch": 0.21441176470588236,
      "grad_norm": 0.06089548021554947,
      "learning_rate": 0.00015734904270986745,
      "loss": 0.412,
      "step": 729
    },
    {
      "epoch": 0.21470588235294116,
      "grad_norm": 0.05081590637564659,
      "learning_rate": 0.0001572901325478645,
      "loss": 0.4045,
      "step": 730
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.044007789343595505,
      "learning_rate": 0.00015723122238586157,
      "loss": 0.3607,
      "step": 731
    },
    {
      "epoch": 0.21529411764705883,
      "grad_norm": 0.039382923394441605,
      "learning_rate": 0.00015717231222385863,
      "loss": 0.2956,
      "step": 732
    },
    {
      "epoch": 0.21558823529411764,
      "grad_norm": 0.07189638912677765,
      "learning_rate": 0.00015711340206185566,
      "loss": 0.4186,
      "step": 733
    },
    {
      "epoch": 0.21588235294117647,
      "grad_norm": 0.05199004337191582,
      "learning_rate": 0.00015705449189985272,
      "loss": 0.3907,
      "step": 734
    },
    {
      "epoch": 0.2161764705882353,
      "grad_norm": 0.04494010657072067,
      "learning_rate": 0.00015699558173784978,
      "loss": 0.3289,
      "step": 735
    },
    {
      "epoch": 0.2164705882352941,
      "grad_norm": 0.05048372969031334,
      "learning_rate": 0.00015693667157584684,
      "loss": 0.3528,
      "step": 736
    },
    {
      "epoch": 0.21676470588235294,
      "grad_norm": 0.04704713821411133,
      "learning_rate": 0.0001568777614138439,
      "loss": 0.3811,
      "step": 737
    },
    {
      "epoch": 0.21705882352941178,
      "grad_norm": 0.04808109998703003,
      "learning_rate": 0.00015681885125184094,
      "loss": 0.3923,
      "step": 738
    },
    {
      "epoch": 0.21735294117647058,
      "grad_norm": 0.04714417830109596,
      "learning_rate": 0.000156759941089838,
      "loss": 0.3302,
      "step": 739
    },
    {
      "epoch": 0.21764705882352942,
      "grad_norm": 0.039318475872278214,
      "learning_rate": 0.00015670103092783506,
      "loss": 0.2794,
      "step": 740
    },
    {
      "epoch": 0.21794117647058822,
      "grad_norm": 0.03107844479382038,
      "learning_rate": 0.00015664212076583212,
      "loss": 0.2473,
      "step": 741
    },
    {
      "epoch": 0.21823529411764706,
      "grad_norm": 0.060445819050073624,
      "learning_rate": 0.00015658321060382918,
      "loss": 0.4345,
      "step": 742
    },
    {
      "epoch": 0.2185294117647059,
      "grad_norm": 0.034567248076200485,
      "learning_rate": 0.0001565243004418262,
      "loss": 0.2868,
      "step": 743
    },
    {
      "epoch": 0.2188235294117647,
      "grad_norm": 0.051404520869255066,
      "learning_rate": 0.00015646539027982327,
      "loss": 0.3215,
      "step": 744
    },
    {
      "epoch": 0.21911764705882353,
      "grad_norm": 0.05068640038371086,
      "learning_rate": 0.00015640648011782033,
      "loss": 0.3587,
      "step": 745
    },
    {
      "epoch": 0.21941176470588236,
      "grad_norm": 0.05996726453304291,
      "learning_rate": 0.0001563475699558174,
      "loss": 0.4104,
      "step": 746
    },
    {
      "epoch": 0.21970588235294117,
      "grad_norm": 0.04572027921676636,
      "learning_rate": 0.00015628865979381445,
      "loss": 0.316,
      "step": 747
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.04572942852973938,
      "learning_rate": 0.00015622974963181148,
      "loss": 0.3455,
      "step": 748
    },
    {
      "epoch": 0.22029411764705883,
      "grad_norm": 0.03836841508746147,
      "learning_rate": 0.00015617083946980854,
      "loss": 0.2826,
      "step": 749
    },
    {
      "epoch": 0.22058823529411764,
      "grad_norm": 0.06018761172890663,
      "learning_rate": 0.0001561119293078056,
      "loss": 0.3791,
      "step": 750
    },
    {
      "epoch": 0.22088235294117647,
      "grad_norm": 0.04149824008345604,
      "learning_rate": 0.00015605301914580266,
      "loss": 0.3508,
      "step": 751
    },
    {
      "epoch": 0.2211764705882353,
      "grad_norm": 0.06412478536367416,
      "learning_rate": 0.00015599410898379972,
      "loss": 0.4411,
      "step": 752
    },
    {
      "epoch": 0.2214705882352941,
      "grad_norm": 0.04851798713207245,
      "learning_rate": 0.00015593519882179676,
      "loss": 0.3949,
      "step": 753
    },
    {
      "epoch": 0.22176470588235295,
      "grad_norm": 0.04657121002674103,
      "learning_rate": 0.00015587628865979382,
      "loss": 0.3591,
      "step": 754
    },
    {
      "epoch": 0.22205882352941175,
      "grad_norm": 0.05091472715139389,
      "learning_rate": 0.00015581737849779088,
      "loss": 0.3302,
      "step": 755
    },
    {
      "epoch": 0.2223529411764706,
      "grad_norm": 0.03741994872689247,
      "learning_rate": 0.00015575846833578794,
      "loss": 0.3334,
      "step": 756
    },
    {
      "epoch": 0.22264705882352942,
      "grad_norm": 0.06529872119426727,
      "learning_rate": 0.000155699558173785,
      "loss": 0.3588,
      "step": 757
    },
    {
      "epoch": 0.22294117647058823,
      "grad_norm": 0.04976775497198105,
      "learning_rate": 0.00015564064801178203,
      "loss": 0.3342,
      "step": 758
    },
    {
      "epoch": 0.22323529411764706,
      "grad_norm": 0.054937053471803665,
      "learning_rate": 0.0001555817378497791,
      "loss": 0.3421,
      "step": 759
    },
    {
      "epoch": 0.2235294117647059,
      "grad_norm": 0.04315875843167305,
      "learning_rate": 0.00015552282768777615,
      "loss": 0.3413,
      "step": 760
    },
    {
      "epoch": 0.2238235294117647,
      "grad_norm": 0.056706156581640244,
      "learning_rate": 0.0001554639175257732,
      "loss": 0.3598,
      "step": 761
    },
    {
      "epoch": 0.22411764705882353,
      "grad_norm": 0.042222760617733,
      "learning_rate": 0.00015540500736377027,
      "loss": 0.3436,
      "step": 762
    },
    {
      "epoch": 0.22441176470588237,
      "grad_norm": 0.04501550644636154,
      "learning_rate": 0.0001553460972017673,
      "loss": 0.3403,
      "step": 763
    },
    {
      "epoch": 0.22470588235294117,
      "grad_norm": 0.04796279966831207,
      "learning_rate": 0.00015528718703976437,
      "loss": 0.3942,
      "step": 764
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.041126664727926254,
      "learning_rate": 0.00015522827687776143,
      "loss": 0.3552,
      "step": 765
    },
    {
      "epoch": 0.2252941176470588,
      "grad_norm": 0.04360002651810646,
      "learning_rate": 0.00015516936671575849,
      "loss": 0.36,
      "step": 766
    },
    {
      "epoch": 0.22558823529411764,
      "grad_norm": 0.0663779079914093,
      "learning_rate": 0.00015511045655375555,
      "loss": 0.4187,
      "step": 767
    },
    {
      "epoch": 0.22588235294117648,
      "grad_norm": 0.057415034621953964,
      "learning_rate": 0.00015505154639175258,
      "loss": 0.3056,
      "step": 768
    },
    {
      "epoch": 0.22617647058823528,
      "grad_norm": 0.05373634025454521,
      "learning_rate": 0.00015499263622974964,
      "loss": 0.3624,
      "step": 769
    },
    {
      "epoch": 0.22647058823529412,
      "grad_norm": 0.056181762367486954,
      "learning_rate": 0.0001549337260677467,
      "loss": 0.3832,
      "step": 770
    },
    {
      "epoch": 0.22676470588235295,
      "grad_norm": 0.04743295535445213,
      "learning_rate": 0.00015487481590574376,
      "loss": 0.3575,
      "step": 771
    },
    {
      "epoch": 0.22705882352941176,
      "grad_norm": 0.048589739948511124,
      "learning_rate": 0.00015481590574374082,
      "loss": 0.3003,
      "step": 772
    },
    {
      "epoch": 0.2273529411764706,
      "grad_norm": 0.03990747779607773,
      "learning_rate": 0.00015475699558173785,
      "loss": 0.2991,
      "step": 773
    },
    {
      "epoch": 0.22764705882352942,
      "grad_norm": 0.0427769310772419,
      "learning_rate": 0.0001546980854197349,
      "loss": 0.3691,
      "step": 774
    },
    {
      "epoch": 0.22794117647058823,
      "grad_norm": 0.053190749138593674,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.4034,
      "step": 775
    },
    {
      "epoch": 0.22823529411764706,
      "grad_norm": 0.04280931502580643,
      "learning_rate": 0.00015458026509572903,
      "loss": 0.3076,
      "step": 776
    },
    {
      "epoch": 0.22852941176470587,
      "grad_norm": 0.047508563846349716,
      "learning_rate": 0.0001545213549337261,
      "loss": 0.3539,
      "step": 777
    },
    {
      "epoch": 0.2288235294117647,
      "grad_norm": 0.04207191243767738,
      "learning_rate": 0.00015446244477172313,
      "loss": 0.2667,
      "step": 778
    },
    {
      "epoch": 0.22911764705882354,
      "grad_norm": 0.042812615633010864,
      "learning_rate": 0.0001544035346097202,
      "loss": 0.3236,
      "step": 779
    },
    {
      "epoch": 0.22941176470588234,
      "grad_norm": 0.03872040659189224,
      "learning_rate": 0.00015434462444771725,
      "loss": 0.2641,
      "step": 780
    },
    {
      "epoch": 0.22970588235294118,
      "grad_norm": 0.0453047938644886,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3695,
      "step": 781
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.05280071869492531,
      "learning_rate": 0.00015422680412371137,
      "loss": 0.3761,
      "step": 782
    },
    {
      "epoch": 0.23029411764705882,
      "grad_norm": 0.05635973811149597,
      "learning_rate": 0.0001541678939617084,
      "loss": 0.4027,
      "step": 783
    },
    {
      "epoch": 0.23058823529411765,
      "grad_norm": 0.04710683226585388,
      "learning_rate": 0.00015410898379970546,
      "loss": 0.3332,
      "step": 784
    },
    {
      "epoch": 0.23088235294117648,
      "grad_norm": 0.058609750121831894,
      "learning_rate": 0.00015405007363770252,
      "loss": 0.3663,
      "step": 785
    },
    {
      "epoch": 0.2311764705882353,
      "grad_norm": 0.045903030782938004,
      "learning_rate": 0.00015399116347569958,
      "loss": 0.3249,
      "step": 786
    },
    {
      "epoch": 0.23147058823529412,
      "grad_norm": 0.0507548525929451,
      "learning_rate": 0.00015393225331369664,
      "loss": 0.3544,
      "step": 787
    },
    {
      "epoch": 0.23176470588235293,
      "grad_norm": 0.054227057844400406,
      "learning_rate": 0.00015387334315169367,
      "loss": 0.3833,
      "step": 788
    },
    {
      "epoch": 0.23205882352941176,
      "grad_norm": 0.05504234880208969,
      "learning_rate": 0.00015381443298969073,
      "loss": 0.399,
      "step": 789
    },
    {
      "epoch": 0.2323529411764706,
      "grad_norm": 0.05211576446890831,
      "learning_rate": 0.0001537555228276878,
      "loss": 0.4129,
      "step": 790
    },
    {
      "epoch": 0.2326470588235294,
      "grad_norm": 0.04948718100786209,
      "learning_rate": 0.00015369661266568485,
      "loss": 0.3852,
      "step": 791
    },
    {
      "epoch": 0.23294117647058823,
      "grad_norm": 0.04808954522013664,
      "learning_rate": 0.00015363770250368191,
      "loss": 0.3159,
      "step": 792
    },
    {
      "epoch": 0.23323529411764707,
      "grad_norm": 0.046661972999572754,
      "learning_rate": 0.00015357879234167895,
      "loss": 0.3863,
      "step": 793
    },
    {
      "epoch": 0.23352941176470587,
      "grad_norm": 0.05235698074102402,
      "learning_rate": 0.000153519882179676,
      "loss": 0.3513,
      "step": 794
    },
    {
      "epoch": 0.2338235294117647,
      "grad_norm": 0.047518905252218246,
      "learning_rate": 0.00015346097201767307,
      "loss": 0.348,
      "step": 795
    },
    {
      "epoch": 0.23411764705882354,
      "grad_norm": 0.058201901614665985,
      "learning_rate": 0.00015340206185567013,
      "loss": 0.3643,
      "step": 796
    },
    {
      "epoch": 0.23441176470588235,
      "grad_norm": 0.0489644929766655,
      "learning_rate": 0.0001533431516936672,
      "loss": 0.3107,
      "step": 797
    },
    {
      "epoch": 0.23470588235294118,
      "grad_norm": 0.03363596647977829,
      "learning_rate": 0.00015328424153166422,
      "loss": 0.2702,
      "step": 798
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.05674545839428902,
      "learning_rate": 0.00015322533136966128,
      "loss": 0.3405,
      "step": 799
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.042102690786123276,
      "learning_rate": 0.00015316642120765834,
      "loss": 0.3236,
      "step": 800
    },
    {
      "epoch": 0.23558823529411765,
      "grad_norm": 0.06986931711435318,
      "learning_rate": 0.00015310751104565537,
      "loss": 0.371,
      "step": 801
    },
    {
      "epoch": 0.23588235294117646,
      "grad_norm": 0.04415692016482353,
      "learning_rate": 0.00015304860088365243,
      "loss": 0.3157,
      "step": 802
    },
    {
      "epoch": 0.2361764705882353,
      "grad_norm": 0.039524734020233154,
      "learning_rate": 0.00015298969072164947,
      "loss": 0.3335,
      "step": 803
    },
    {
      "epoch": 0.23647058823529413,
      "grad_norm": 0.08055469393730164,
      "learning_rate": 0.00015293078055964653,
      "loss": 0.4003,
      "step": 804
    },
    {
      "epoch": 0.23676470588235293,
      "grad_norm": 0.054536279290914536,
      "learning_rate": 0.0001528718703976436,
      "loss": 0.3788,
      "step": 805
    },
    {
      "epoch": 0.23705882352941177,
      "grad_norm": 0.05659380182623863,
      "learning_rate": 0.00015281296023564065,
      "loss": 0.3845,
      "step": 806
    },
    {
      "epoch": 0.2373529411764706,
      "grad_norm": 0.093269944190979,
      "learning_rate": 0.0001527540500736377,
      "loss": 0.4277,
      "step": 807
    },
    {
      "epoch": 0.2376470588235294,
      "grad_norm": 0.05221351981163025,
      "learning_rate": 0.00015269513991163474,
      "loss": 0.3325,
      "step": 808
    },
    {
      "epoch": 0.23794117647058824,
      "grad_norm": 0.05038915574550629,
      "learning_rate": 0.0001526362297496318,
      "loss": 0.373,
      "step": 809
    },
    {
      "epoch": 0.23823529411764705,
      "grad_norm": 0.04991335794329643,
      "learning_rate": 0.00015257731958762886,
      "loss": 0.3632,
      "step": 810
    },
    {
      "epoch": 0.23852941176470588,
      "grad_norm": 0.05436830222606659,
      "learning_rate": 0.00015251840942562592,
      "loss": 0.405,
      "step": 811
    },
    {
      "epoch": 0.2388235294117647,
      "grad_norm": 0.053016144782304764,
      "learning_rate": 0.00015245949926362298,
      "loss": 0.3225,
      "step": 812
    },
    {
      "epoch": 0.23911764705882352,
      "grad_norm": 0.05451105907559395,
      "learning_rate": 0.00015240058910162002,
      "loss": 0.3669,
      "step": 813
    },
    {
      "epoch": 0.23941176470588235,
      "grad_norm": 0.04538634791970253,
      "learning_rate": 0.00015234167893961708,
      "loss": 0.3715,
      "step": 814
    },
    {
      "epoch": 0.23970588235294119,
      "grad_norm": 0.047667670994997025,
      "learning_rate": 0.00015228276877761414,
      "loss": 0.3275,
      "step": 815
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.04488759860396385,
      "learning_rate": 0.0001522238586156112,
      "loss": 0.3555,
      "step": 816
    },
    {
      "epoch": 0.24029411764705882,
      "grad_norm": 0.053564272820949554,
      "learning_rate": 0.00015216494845360826,
      "loss": 0.3142,
      "step": 817
    },
    {
      "epoch": 0.24058823529411766,
      "grad_norm": 0.060604408383369446,
      "learning_rate": 0.0001521060382916053,
      "loss": 0.3556,
      "step": 818
    },
    {
      "epoch": 0.24088235294117646,
      "grad_norm": 0.050620079040527344,
      "learning_rate": 0.00015204712812960235,
      "loss": 0.3326,
      "step": 819
    },
    {
      "epoch": 0.2411764705882353,
      "grad_norm": 0.05900190398097038,
      "learning_rate": 0.0001519882179675994,
      "loss": 0.3857,
      "step": 820
    },
    {
      "epoch": 0.24147058823529413,
      "grad_norm": 0.05687745660543442,
      "learning_rate": 0.00015192930780559647,
      "loss": 0.3603,
      "step": 821
    },
    {
      "epoch": 0.24176470588235294,
      "grad_norm": 0.048534080386161804,
      "learning_rate": 0.00015187039764359353,
      "loss": 0.2943,
      "step": 822
    },
    {
      "epoch": 0.24205882352941177,
      "grad_norm": 0.06559184193611145,
      "learning_rate": 0.00015181148748159056,
      "loss": 0.3651,
      "step": 823
    },
    {
      "epoch": 0.24235294117647058,
      "grad_norm": 0.060589712113142014,
      "learning_rate": 0.00015175257731958762,
      "loss": 0.3557,
      "step": 824
    },
    {
      "epoch": 0.2426470588235294,
      "grad_norm": 0.05977138131856918,
      "learning_rate": 0.00015169366715758468,
      "loss": 0.3844,
      "step": 825
    },
    {
      "epoch": 0.24294117647058824,
      "grad_norm": 0.06815727800130844,
      "learning_rate": 0.00015163475699558174,
      "loss": 0.4818,
      "step": 826
    },
    {
      "epoch": 0.24323529411764705,
      "grad_norm": 0.06061342731118202,
      "learning_rate": 0.0001515758468335788,
      "loss": 0.3617,
      "step": 827
    },
    {
      "epoch": 0.24352941176470588,
      "grad_norm": 0.049744799733161926,
      "learning_rate": 0.00015151693667157584,
      "loss": 0.3843,
      "step": 828
    },
    {
      "epoch": 0.24382352941176472,
      "grad_norm": 0.045812372118234634,
      "learning_rate": 0.0001514580265095729,
      "loss": 0.3831,
      "step": 829
    },
    {
      "epoch": 0.24411764705882352,
      "grad_norm": 0.03728477284312248,
      "learning_rate": 0.00015139911634756996,
      "loss": 0.2735,
      "step": 830
    },
    {
      "epoch": 0.24441176470588236,
      "grad_norm": 0.06866984814405441,
      "learning_rate": 0.00015134020618556702,
      "loss": 0.4191,
      "step": 831
    },
    {
      "epoch": 0.2447058823529412,
      "grad_norm": 0.04845845699310303,
      "learning_rate": 0.00015128129602356408,
      "loss": 0.3649,
      "step": 832
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.0657864511013031,
      "learning_rate": 0.0001512223858615611,
      "loss": 0.3014,
      "step": 833
    },
    {
      "epoch": 0.24529411764705883,
      "grad_norm": 0.05138332024216652,
      "learning_rate": 0.00015116347569955817,
      "loss": 0.3488,
      "step": 834
    },
    {
      "epoch": 0.24558823529411763,
      "grad_norm": 0.059861041605472565,
      "learning_rate": 0.00015110456553755523,
      "loss": 0.3637,
      "step": 835
    },
    {
      "epoch": 0.24588235294117647,
      "grad_norm": 0.06248849257826805,
      "learning_rate": 0.0001510456553755523,
      "loss": 0.3434,
      "step": 836
    },
    {
      "epoch": 0.2461764705882353,
      "grad_norm": 0.04062151908874512,
      "learning_rate": 0.00015098674521354935,
      "loss": 0.3868,
      "step": 837
    },
    {
      "epoch": 0.2464705882352941,
      "grad_norm": 0.055742047727108,
      "learning_rate": 0.00015092783505154638,
      "loss": 0.3724,
      "step": 838
    },
    {
      "epoch": 0.24676470588235294,
      "grad_norm": 0.04621363803744316,
      "learning_rate": 0.00015086892488954344,
      "loss": 0.3178,
      "step": 839
    },
    {
      "epoch": 0.24705882352941178,
      "grad_norm": 0.05783941596746445,
      "learning_rate": 0.0001508100147275405,
      "loss": 0.3221,
      "step": 840
    },
    {
      "epoch": 0.24735294117647058,
      "grad_norm": 0.06186376512050629,
      "learning_rate": 0.00015075110456553756,
      "loss": 0.4125,
      "step": 841
    },
    {
      "epoch": 0.24764705882352941,
      "grad_norm": 0.0654083713889122,
      "learning_rate": 0.00015069219440353462,
      "loss": 0.3316,
      "step": 842
    },
    {
      "epoch": 0.24794117647058825,
      "grad_norm": 0.05576207861304283,
      "learning_rate": 0.00015063328424153166,
      "loss": 0.3444,
      "step": 843
    },
    {
      "epoch": 0.24823529411764705,
      "grad_norm": 0.05090976506471634,
      "learning_rate": 0.00015057437407952872,
      "loss": 0.2992,
      "step": 844
    },
    {
      "epoch": 0.2485294117647059,
      "grad_norm": 0.05650731176137924,
      "learning_rate": 0.00015051546391752578,
      "loss": 0.3415,
      "step": 845
    },
    {
      "epoch": 0.2488235294117647,
      "grad_norm": 0.05744515731930733,
      "learning_rate": 0.00015045655375552284,
      "loss": 0.3343,
      "step": 846
    },
    {
      "epoch": 0.24911764705882353,
      "grad_norm": 0.053106389939785004,
      "learning_rate": 0.0001503976435935199,
      "loss": 0.3575,
      "step": 847
    },
    {
      "epoch": 0.24941176470588236,
      "grad_norm": 0.06118246540427208,
      "learning_rate": 0.00015033873343151693,
      "loss": 0.4009,
      "step": 848
    },
    {
      "epoch": 0.24970588235294117,
      "grad_norm": 0.046972401440143585,
      "learning_rate": 0.000150279823269514,
      "loss": 0.333,
      "step": 849
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.06157941371202469,
      "learning_rate": 0.00015022091310751105,
      "loss": 0.4387,
      "step": 850
    },
    {
      "epoch": 0.25029411764705883,
      "grad_norm": 0.05762602388858795,
      "learning_rate": 0.0001501620029455081,
      "loss": 0.3813,
      "step": 851
    },
    {
      "epoch": 0.25058823529411767,
      "grad_norm": 0.06147916615009308,
      "learning_rate": 0.00015010309278350517,
      "loss": 0.3888,
      "step": 852
    },
    {
      "epoch": 0.25088235294117645,
      "grad_norm": 0.04638632759451866,
      "learning_rate": 0.0001500441826215022,
      "loss": 0.3935,
      "step": 853
    },
    {
      "epoch": 0.2511764705882353,
      "grad_norm": 0.05263432860374451,
      "learning_rate": 0.00014998527245949927,
      "loss": 0.3598,
      "step": 854
    },
    {
      "epoch": 0.2514705882352941,
      "grad_norm": 0.05627617985010147,
      "learning_rate": 0.00014992636229749633,
      "loss": 0.3591,
      "step": 855
    },
    {
      "epoch": 0.25176470588235295,
      "grad_norm": 0.04958704486489296,
      "learning_rate": 0.00014986745213549339,
      "loss": 0.3542,
      "step": 856
    },
    {
      "epoch": 0.2520588235294118,
      "grad_norm": 0.06744445115327835,
      "learning_rate": 0.00014980854197349045,
      "loss": 0.443,
      "step": 857
    },
    {
      "epoch": 0.2523529411764706,
      "grad_norm": 0.061551403254270554,
      "learning_rate": 0.00014974963181148748,
      "loss": 0.3964,
      "step": 858
    },
    {
      "epoch": 0.2526470588235294,
      "grad_norm": 0.054337237030267715,
      "learning_rate": 0.00014969072164948454,
      "loss": 0.3822,
      "step": 859
    },
    {
      "epoch": 0.2529411764705882,
      "grad_norm": 0.049503255635499954,
      "learning_rate": 0.0001496318114874816,
      "loss": 0.3352,
      "step": 860
    },
    {
      "epoch": 0.25323529411764706,
      "grad_norm": 0.039648011326789856,
      "learning_rate": 0.00014957290132547866,
      "loss": 0.2658,
      "step": 861
    },
    {
      "epoch": 0.2535294117647059,
      "grad_norm": 0.05104890465736389,
      "learning_rate": 0.00014951399116347572,
      "loss": 0.3418,
      "step": 862
    },
    {
      "epoch": 0.2538235294117647,
      "grad_norm": 0.050142768770456314,
      "learning_rate": 0.00014945508100147275,
      "loss": 0.3594,
      "step": 863
    },
    {
      "epoch": 0.2541176470588235,
      "grad_norm": 0.04247758165001869,
      "learning_rate": 0.0001493961708394698,
      "loss": 0.3568,
      "step": 864
    },
    {
      "epoch": 0.25441176470588234,
      "grad_norm": 0.04686996713280678,
      "learning_rate": 0.00014933726067746687,
      "loss": 0.4263,
      "step": 865
    },
    {
      "epoch": 0.25470588235294117,
      "grad_norm": 0.03805086389183998,
      "learning_rate": 0.00014927835051546393,
      "loss": 0.3073,
      "step": 866
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.046861857175827026,
      "learning_rate": 0.000149219440353461,
      "loss": 0.349,
      "step": 867
    },
    {
      "epoch": 0.25529411764705884,
      "grad_norm": 0.0589592345058918,
      "learning_rate": 0.00014916053019145803,
      "loss": 0.4094,
      "step": 868
    },
    {
      "epoch": 0.25558823529411767,
      "grad_norm": 0.05310528725385666,
      "learning_rate": 0.0001491016200294551,
      "loss": 0.3295,
      "step": 869
    },
    {
      "epoch": 0.25588235294117645,
      "grad_norm": 0.052244942635297775,
      "learning_rate": 0.00014904270986745215,
      "loss": 0.3514,
      "step": 870
    },
    {
      "epoch": 0.2561764705882353,
      "grad_norm": 0.05275652930140495,
      "learning_rate": 0.0001489837997054492,
      "loss": 0.3537,
      "step": 871
    },
    {
      "epoch": 0.2564705882352941,
      "grad_norm": 0.06178870052099228,
      "learning_rate": 0.00014892488954344627,
      "loss": 0.3596,
      "step": 872
    },
    {
      "epoch": 0.25676470588235295,
      "grad_norm": 0.05806146189570427,
      "learning_rate": 0.0001488659793814433,
      "loss": 0.3599,
      "step": 873
    },
    {
      "epoch": 0.2570588235294118,
      "grad_norm": 0.080573171377182,
      "learning_rate": 0.00014880706921944036,
      "loss": 0.4031,
      "step": 874
    },
    {
      "epoch": 0.25735294117647056,
      "grad_norm": 0.06610755622386932,
      "learning_rate": 0.00014874815905743742,
      "loss": 0.419,
      "step": 875
    },
    {
      "epoch": 0.2576470588235294,
      "grad_norm": 0.052600402384996414,
      "learning_rate": 0.00014868924889543448,
      "loss": 0.3595,
      "step": 876
    },
    {
      "epoch": 0.25794117647058823,
      "grad_norm": 0.04556182771921158,
      "learning_rate": 0.00014863033873343154,
      "loss": 0.3474,
      "step": 877
    },
    {
      "epoch": 0.25823529411764706,
      "grad_norm": 0.039866458624601364,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3106,
      "step": 878
    },
    {
      "epoch": 0.2585294117647059,
      "grad_norm": 0.03503716364502907,
      "learning_rate": 0.00014851251840942563,
      "loss": 0.25,
      "step": 879
    },
    {
      "epoch": 0.25882352941176473,
      "grad_norm": 0.05221695452928543,
      "learning_rate": 0.0001484536082474227,
      "loss": 0.3477,
      "step": 880
    },
    {
      "epoch": 0.2591176470588235,
      "grad_norm": 0.04650384560227394,
      "learning_rate": 0.00014839469808541975,
      "loss": 0.3326,
      "step": 881
    },
    {
      "epoch": 0.25941176470588234,
      "grad_norm": 0.05515836179256439,
      "learning_rate": 0.00014833578792341681,
      "loss": 0.3812,
      "step": 882
    },
    {
      "epoch": 0.2597058823529412,
      "grad_norm": 0.059091031551361084,
      "learning_rate": 0.00014827687776141385,
      "loss": 0.4201,
      "step": 883
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.04848366975784302,
      "learning_rate": 0.0001482179675994109,
      "loss": 0.3433,
      "step": 884
    },
    {
      "epoch": 0.26029411764705884,
      "grad_norm": 0.049492694437503815,
      "learning_rate": 0.00014815905743740797,
      "loss": 0.3889,
      "step": 885
    },
    {
      "epoch": 0.2605882352941176,
      "grad_norm": 0.051795728504657745,
      "learning_rate": 0.00014810014727540503,
      "loss": 0.3756,
      "step": 886
    },
    {
      "epoch": 0.26088235294117645,
      "grad_norm": 0.058234427124261856,
      "learning_rate": 0.0001480412371134021,
      "loss": 0.4508,
      "step": 887
    },
    {
      "epoch": 0.2611764705882353,
      "grad_norm": 0.05852511525154114,
      "learning_rate": 0.00014798232695139912,
      "loss": 0.3505,
      "step": 888
    },
    {
      "epoch": 0.2614705882352941,
      "grad_norm": 0.052797216922044754,
      "learning_rate": 0.00014792341678939618,
      "loss": 0.3469,
      "step": 889
    },
    {
      "epoch": 0.26176470588235295,
      "grad_norm": 0.05192058905959129,
      "learning_rate": 0.00014786450662739324,
      "loss": 0.2925,
      "step": 890
    },
    {
      "epoch": 0.2620588235294118,
      "grad_norm": 0.05261188745498657,
      "learning_rate": 0.0001478055964653903,
      "loss": 0.3722,
      "step": 891
    },
    {
      "epoch": 0.26235294117647057,
      "grad_norm": 0.04389750957489014,
      "learning_rate": 0.00014774668630338736,
      "loss": 0.2963,
      "step": 892
    },
    {
      "epoch": 0.2626470588235294,
      "grad_norm": 0.05705910176038742,
      "learning_rate": 0.0001476877761413844,
      "loss": 0.365,
      "step": 893
    },
    {
      "epoch": 0.26294117647058823,
      "grad_norm": 0.05660948157310486,
      "learning_rate": 0.00014762886597938146,
      "loss": 0.3379,
      "step": 894
    },
    {
      "epoch": 0.26323529411764707,
      "grad_norm": 0.049769893288612366,
      "learning_rate": 0.00014756995581737852,
      "loss": 0.3724,
      "step": 895
    },
    {
      "epoch": 0.2635294117647059,
      "grad_norm": 0.045160625129938126,
      "learning_rate": 0.00014751104565537558,
      "loss": 0.3038,
      "step": 896
    },
    {
      "epoch": 0.2638235294117647,
      "grad_norm": 0.0514555498957634,
      "learning_rate": 0.00014745213549337264,
      "loss": 0.3506,
      "step": 897
    },
    {
      "epoch": 0.2641176470588235,
      "grad_norm": 0.05286532640457153,
      "learning_rate": 0.00014739322533136967,
      "loss": 0.3662,
      "step": 898
    },
    {
      "epoch": 0.26441176470588235,
      "grad_norm": 0.056739773601293564,
      "learning_rate": 0.00014733431516936673,
      "loss": 0.3623,
      "step": 899
    },
    {
      "epoch": 0.2647058823529412,
      "grad_norm": 0.05321785435080528,
      "learning_rate": 0.0001472754050073638,
      "loss": 0.3582,
      "step": 900
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05282196030020714,
      "learning_rate": 0.00014721649484536085,
      "loss": 0.3744,
      "step": 901
    },
    {
      "epoch": 0.26529411764705885,
      "grad_norm": 0.04438371583819389,
      "learning_rate": 0.0001471575846833579,
      "loss": 0.3417,
      "step": 902
    },
    {
      "epoch": 0.2655882352941176,
      "grad_norm": 0.06881672888994217,
      "learning_rate": 0.00014709867452135494,
      "loss": 0.3967,
      "step": 903
    },
    {
      "epoch": 0.26588235294117646,
      "grad_norm": 0.041843902319669724,
      "learning_rate": 0.000147039764359352,
      "loss": 0.3405,
      "step": 904
    },
    {
      "epoch": 0.2661764705882353,
      "grad_norm": 0.05255134031176567,
      "learning_rate": 0.00014698085419734906,
      "loss": 0.3291,
      "step": 905
    },
    {
      "epoch": 0.2664705882352941,
      "grad_norm": 0.04878660663962364,
      "learning_rate": 0.00014692194403534612,
      "loss": 0.437,
      "step": 906
    },
    {
      "epoch": 0.26676470588235296,
      "grad_norm": 0.05067708343267441,
      "learning_rate": 0.00014686303387334316,
      "loss": 0.3734,
      "step": 907
    },
    {
      "epoch": 0.26705882352941174,
      "grad_norm": 0.049029041081666946,
      "learning_rate": 0.0001468041237113402,
      "loss": 0.3741,
      "step": 908
    },
    {
      "epoch": 0.26735294117647057,
      "grad_norm": 0.05733175948262215,
      "learning_rate": 0.00014674521354933725,
      "loss": 0.4194,
      "step": 909
    },
    {
      "epoch": 0.2676470588235294,
      "grad_norm": 0.05006872117519379,
      "learning_rate": 0.0001466863033873343,
      "loss": 0.325,
      "step": 910
    },
    {
      "epoch": 0.26794117647058824,
      "grad_norm": 0.04537907987833023,
      "learning_rate": 0.00014662739322533137,
      "loss": 0.3046,
      "step": 911
    },
    {
      "epoch": 0.26823529411764707,
      "grad_norm": 0.043141353875398636,
      "learning_rate": 0.00014656848306332843,
      "loss": 0.3046,
      "step": 912
    },
    {
      "epoch": 0.2685294117647059,
      "grad_norm": 0.0497368760406971,
      "learning_rate": 0.00014650957290132546,
      "loss": 0.3273,
      "step": 913
    },
    {
      "epoch": 0.2688235294117647,
      "grad_norm": 0.07015538960695267,
      "learning_rate": 0.00014645066273932252,
      "loss": 0.3687,
      "step": 914
    },
    {
      "epoch": 0.2691176470588235,
      "grad_norm": 0.04994068294763565,
      "learning_rate": 0.00014639175257731958,
      "loss": 0.3379,
      "step": 915
    },
    {
      "epoch": 0.26941176470588235,
      "grad_norm": 0.05028361827135086,
      "learning_rate": 0.00014633284241531664,
      "loss": 0.34,
      "step": 916
    },
    {
      "epoch": 0.2697058823529412,
      "grad_norm": 0.04698606953024864,
      "learning_rate": 0.0001462739322533137,
      "loss": 0.3151,
      "step": 917
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.04332105070352554,
      "learning_rate": 0.00014621502209131074,
      "loss": 0.2996,
      "step": 918
    },
    {
      "epoch": 0.2702941176470588,
      "grad_norm": 0.04999271780252457,
      "learning_rate": 0.0001461561119293078,
      "loss": 0.3453,
      "step": 919
    },
    {
      "epoch": 0.27058823529411763,
      "grad_norm": 0.050503771752119064,
      "learning_rate": 0.00014609720176730486,
      "loss": 0.3668,
      "step": 920
    },
    {
      "epoch": 0.27088235294117646,
      "grad_norm": 0.03924735635519028,
      "learning_rate": 0.00014603829160530192,
      "loss": 0.3591,
      "step": 921
    },
    {
      "epoch": 0.2711764705882353,
      "grad_norm": 0.046539057046175,
      "learning_rate": 0.00014597938144329898,
      "loss": 0.3399,
      "step": 922
    },
    {
      "epoch": 0.27147058823529413,
      "grad_norm": 0.05281738191843033,
      "learning_rate": 0.000145920471281296,
      "loss": 0.3429,
      "step": 923
    },
    {
      "epoch": 0.27176470588235296,
      "grad_norm": 0.05354934185743332,
      "learning_rate": 0.00014586156111929307,
      "loss": 0.4079,
      "step": 924
    },
    {
      "epoch": 0.27205882352941174,
      "grad_norm": 0.04236755892634392,
      "learning_rate": 0.00014580265095729013,
      "loss": 0.3351,
      "step": 925
    },
    {
      "epoch": 0.2723529411764706,
      "grad_norm": 0.06168770790100098,
      "learning_rate": 0.0001457437407952872,
      "loss": 0.4292,
      "step": 926
    },
    {
      "epoch": 0.2726470588235294,
      "grad_norm": 0.06615830957889557,
      "learning_rate": 0.00014568483063328425,
      "loss": 0.3949,
      "step": 927
    },
    {
      "epoch": 0.27294117647058824,
      "grad_norm": 0.04564342275261879,
      "learning_rate": 0.00014562592047128128,
      "loss": 0.3179,
      "step": 928
    },
    {
      "epoch": 0.2732352941176471,
      "grad_norm": 0.049959443509578705,
      "learning_rate": 0.00014556701030927834,
      "loss": 0.3674,
      "step": 929
    },
    {
      "epoch": 0.2735294117647059,
      "grad_norm": 0.05306636914610863,
      "learning_rate": 0.0001455081001472754,
      "loss": 0.3368,
      "step": 930
    },
    {
      "epoch": 0.2738235294117647,
      "grad_norm": 0.0397757850587368,
      "learning_rate": 0.00014544918998527246,
      "loss": 0.3243,
      "step": 931
    },
    {
      "epoch": 0.2741176470588235,
      "grad_norm": 0.05705932900309563,
      "learning_rate": 0.00014539027982326952,
      "loss": 0.3661,
      "step": 932
    },
    {
      "epoch": 0.27441176470588236,
      "grad_norm": 0.055002693086862564,
      "learning_rate": 0.00014533136966126656,
      "loss": 0.3449,
      "step": 933
    },
    {
      "epoch": 0.2747058823529412,
      "grad_norm": 0.054270386695861816,
      "learning_rate": 0.00014527245949926362,
      "loss": 0.3272,
      "step": 934
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.04974392428994179,
      "learning_rate": 0.00014521354933726068,
      "loss": 0.3563,
      "step": 935
    },
    {
      "epoch": 0.2752941176470588,
      "grad_norm": 0.049924347549676895,
      "learning_rate": 0.00014515463917525774,
      "loss": 0.4105,
      "step": 936
    },
    {
      "epoch": 0.27558823529411763,
      "grad_norm": 0.039254605770111084,
      "learning_rate": 0.0001450957290132548,
      "loss": 0.322,
      "step": 937
    },
    {
      "epoch": 0.27588235294117647,
      "grad_norm": 0.05751575902104378,
      "learning_rate": 0.00014503681885125183,
      "loss": 0.3343,
      "step": 938
    },
    {
      "epoch": 0.2761764705882353,
      "grad_norm": 0.04842580482363701,
      "learning_rate": 0.0001449779086892489,
      "loss": 0.3974,
      "step": 939
    },
    {
      "epoch": 0.27647058823529413,
      "grad_norm": 0.052153296768665314,
      "learning_rate": 0.00014491899852724595,
      "loss": 0.3776,
      "step": 940
    },
    {
      "epoch": 0.27676470588235297,
      "grad_norm": 0.04191187024116516,
      "learning_rate": 0.000144860088365243,
      "loss": 0.3789,
      "step": 941
    },
    {
      "epoch": 0.27705882352941175,
      "grad_norm": 0.0462866872549057,
      "learning_rate": 0.00014480117820324007,
      "loss": 0.338,
      "step": 942
    },
    {
      "epoch": 0.2773529411764706,
      "grad_norm": 0.03732164576649666,
      "learning_rate": 0.0001447422680412371,
      "loss": 0.2655,
      "step": 943
    },
    {
      "epoch": 0.2776470588235294,
      "grad_norm": 0.06493853777647018,
      "learning_rate": 0.00014468335787923417,
      "loss": 0.4188,
      "step": 944
    },
    {
      "epoch": 0.27794117647058825,
      "grad_norm": 0.036105502396821976,
      "learning_rate": 0.00014462444771723123,
      "loss": 0.2807,
      "step": 945
    },
    {
      "epoch": 0.2782352941176471,
      "grad_norm": 0.05518908053636551,
      "learning_rate": 0.00014456553755522829,
      "loss": 0.3562,
      "step": 946
    },
    {
      "epoch": 0.27852941176470586,
      "grad_norm": 0.05957522988319397,
      "learning_rate": 0.00014450662739322535,
      "loss": 0.3702,
      "step": 947
    },
    {
      "epoch": 0.2788235294117647,
      "grad_norm": 0.06619017571210861,
      "learning_rate": 0.00014444771723122238,
      "loss": 0.4375,
      "step": 948
    },
    {
      "epoch": 0.2791176470588235,
      "grad_norm": 0.04963508993387222,
      "learning_rate": 0.00014438880706921944,
      "loss": 0.343,
      "step": 949
    },
    {
      "epoch": 0.27941176470588236,
      "grad_norm": 0.06201440095901489,
      "learning_rate": 0.0001443298969072165,
      "loss": 0.3691,
      "step": 950
    },
    {
      "epoch": 0.2797058823529412,
      "grad_norm": 0.0652867779135704,
      "learning_rate": 0.00014427098674521356,
      "loss": 0.4061,
      "step": 951
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.05759086832404137,
      "learning_rate": 0.00014421207658321062,
      "loss": 0.4382,
      "step": 952
    },
    {
      "epoch": 0.2802941176470588,
      "grad_norm": 0.042959362268447876,
      "learning_rate": 0.00014415316642120765,
      "loss": 0.3556,
      "step": 953
    },
    {
      "epoch": 0.28058823529411764,
      "grad_norm": 0.0596841461956501,
      "learning_rate": 0.0001440942562592047,
      "loss": 0.4252,
      "step": 954
    },
    {
      "epoch": 0.28088235294117647,
      "grad_norm": 0.049326855689287186,
      "learning_rate": 0.00014403534609720177,
      "loss": 0.3003,
      "step": 955
    },
    {
      "epoch": 0.2811764705882353,
      "grad_norm": 0.04084866866469383,
      "learning_rate": 0.00014397643593519883,
      "loss": 0.3998,
      "step": 956
    },
    {
      "epoch": 0.28147058823529414,
      "grad_norm": 0.04952137917280197,
      "learning_rate": 0.0001439175257731959,
      "loss": 0.3195,
      "step": 957
    },
    {
      "epoch": 0.2817647058823529,
      "grad_norm": 0.04311099648475647,
      "learning_rate": 0.00014385861561119293,
      "loss": 0.2777,
      "step": 958
    },
    {
      "epoch": 0.28205882352941175,
      "grad_norm": 0.058379556983709335,
      "learning_rate": 0.00014379970544919,
      "loss": 0.3691,
      "step": 959
    },
    {
      "epoch": 0.2823529411764706,
      "grad_norm": 0.04629683122038841,
      "learning_rate": 0.00014374079528718705,
      "loss": 0.4168,
      "step": 960
    },
    {
      "epoch": 0.2826470588235294,
      "grad_norm": 0.04578983038663864,
      "learning_rate": 0.0001436818851251841,
      "loss": 0.3548,
      "step": 961
    },
    {
      "epoch": 0.28294117647058825,
      "grad_norm": 0.04936366155743599,
      "learning_rate": 0.00014362297496318117,
      "loss": 0.3998,
      "step": 962
    },
    {
      "epoch": 0.2832352941176471,
      "grad_norm": 0.040401116013526917,
      "learning_rate": 0.0001435640648011782,
      "loss": 0.3419,
      "step": 963
    },
    {
      "epoch": 0.28352941176470586,
      "grad_norm": 0.044860322028398514,
      "learning_rate": 0.00014350515463917526,
      "loss": 0.3741,
      "step": 964
    },
    {
      "epoch": 0.2838235294117647,
      "grad_norm": 0.04771924763917923,
      "learning_rate": 0.00014344624447717232,
      "loss": 0.3723,
      "step": 965
    },
    {
      "epoch": 0.28411764705882353,
      "grad_norm": 0.04534361883997917,
      "learning_rate": 0.00014338733431516938,
      "loss": 0.3483,
      "step": 966
    },
    {
      "epoch": 0.28441176470588236,
      "grad_norm": 0.07333830744028091,
      "learning_rate": 0.00014332842415316644,
      "loss": 0.4213,
      "step": 967
    },
    {
      "epoch": 0.2847058823529412,
      "grad_norm": 0.04066607728600502,
      "learning_rate": 0.00014326951399116347,
      "loss": 0.2486,
      "step": 968
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.04879528284072876,
      "learning_rate": 0.00014321060382916053,
      "loss": 0.3121,
      "step": 969
    },
    {
      "epoch": 0.2852941176470588,
      "grad_norm": 0.051146313548088074,
      "learning_rate": 0.0001431516936671576,
      "loss": 0.3859,
      "step": 970
    },
    {
      "epoch": 0.28558823529411764,
      "grad_norm": 0.06950093805789948,
      "learning_rate": 0.00014309278350515465,
      "loss": 0.436,
      "step": 971
    },
    {
      "epoch": 0.2858823529411765,
      "grad_norm": 0.059965830296278,
      "learning_rate": 0.00014303387334315171,
      "loss": 0.4238,
      "step": 972
    },
    {
      "epoch": 0.2861764705882353,
      "grad_norm": 0.05766318365931511,
      "learning_rate": 0.00014297496318114875,
      "loss": 0.3881,
      "step": 973
    },
    {
      "epoch": 0.28647058823529414,
      "grad_norm": 0.04504472762346268,
      "learning_rate": 0.0001429160530191458,
      "loss": 0.3146,
      "step": 974
    },
    {
      "epoch": 0.2867647058823529,
      "grad_norm": 0.056683655828237534,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3679,
      "step": 975
    },
    {
      "epoch": 0.28705882352941176,
      "grad_norm": 0.049050893634557724,
      "learning_rate": 0.00014279823269513993,
      "loss": 0.3525,
      "step": 976
    },
    {
      "epoch": 0.2873529411764706,
      "grad_norm": 0.05428948998451233,
      "learning_rate": 0.000142739322533137,
      "loss": 0.4076,
      "step": 977
    },
    {
      "epoch": 0.2876470588235294,
      "grad_norm": 0.04430536925792694,
      "learning_rate": 0.00014268041237113402,
      "loss": 0.3403,
      "step": 978
    },
    {
      "epoch": 0.28794117647058826,
      "grad_norm": 0.07218419760465622,
      "learning_rate": 0.00014262150220913108,
      "loss": 0.3817,
      "step": 979
    },
    {
      "epoch": 0.28823529411764703,
      "grad_norm": 0.04808255657553673,
      "learning_rate": 0.00014256259204712814,
      "loss": 0.3518,
      "step": 980
    },
    {
      "epoch": 0.28852941176470587,
      "grad_norm": 0.057422708719968796,
      "learning_rate": 0.0001425036818851252,
      "loss": 0.343,
      "step": 981
    },
    {
      "epoch": 0.2888235294117647,
      "grad_norm": 0.05034565553069115,
      "learning_rate": 0.00014244477172312226,
      "loss": 0.4274,
      "step": 982
    },
    {
      "epoch": 0.28911764705882353,
      "grad_norm": 0.049863360822200775,
      "learning_rate": 0.0001423858615611193,
      "loss": 0.3706,
      "step": 983
    },
    {
      "epoch": 0.28941176470588237,
      "grad_norm": 0.051067616790533066,
      "learning_rate": 0.00014232695139911636,
      "loss": 0.387,
      "step": 984
    },
    {
      "epoch": 0.2897058823529412,
      "grad_norm": 0.06971482932567596,
      "learning_rate": 0.00014226804123711342,
      "loss": 0.4349,
      "step": 985
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.040668100118637085,
      "learning_rate": 0.00014220913107511048,
      "loss": 0.3177,
      "step": 986
    },
    {
      "epoch": 0.2902941176470588,
      "grad_norm": 0.04351551830768585,
      "learning_rate": 0.00014215022091310754,
      "loss": 0.3425,
      "step": 987
    },
    {
      "epoch": 0.29058823529411765,
      "grad_norm": 0.06390437483787537,
      "learning_rate": 0.00014209131075110457,
      "loss": 0.3905,
      "step": 988
    },
    {
      "epoch": 0.2908823529411765,
      "grad_norm": 0.04128856211900711,
      "learning_rate": 0.00014203240058910163,
      "loss": 0.3415,
      "step": 989
    },
    {
      "epoch": 0.2911764705882353,
      "grad_norm": 0.04569678753614426,
      "learning_rate": 0.0001419734904270987,
      "loss": 0.3216,
      "step": 990
    },
    {
      "epoch": 0.2914705882352941,
      "grad_norm": 0.05930420383810997,
      "learning_rate": 0.00014191458026509575,
      "loss": 0.4153,
      "step": 991
    },
    {
      "epoch": 0.2917647058823529,
      "grad_norm": 0.044033098965883255,
      "learning_rate": 0.0001418556701030928,
      "loss": 0.3708,
      "step": 992
    },
    {
      "epoch": 0.29205882352941176,
      "grad_norm": 0.04454571008682251,
      "learning_rate": 0.00014179675994108984,
      "loss": 0.2674,
      "step": 993
    },
    {
      "epoch": 0.2923529411764706,
      "grad_norm": 0.04741377755999565,
      "learning_rate": 0.0001417378497790869,
      "loss": 0.3848,
      "step": 994
    },
    {
      "epoch": 0.2926470588235294,
      "grad_norm": 0.05840189382433891,
      "learning_rate": 0.00014167893961708396,
      "loss": 0.3559,
      "step": 995
    },
    {
      "epoch": 0.29294117647058826,
      "grad_norm": 0.043357573449611664,
      "learning_rate": 0.00014162002945508102,
      "loss": 0.3256,
      "step": 996
    },
    {
      "epoch": 0.29323529411764704,
      "grad_norm": 0.04567708075046539,
      "learning_rate": 0.00014156111929307808,
      "loss": 0.383,
      "step": 997
    },
    {
      "epoch": 0.29352941176470587,
      "grad_norm": 0.04342838004231453,
      "learning_rate": 0.00014150220913107512,
      "loss": 0.3304,
      "step": 998
    },
    {
      "epoch": 0.2938235294117647,
      "grad_norm": 0.04749620705842972,
      "learning_rate": 0.00014144329896907218,
      "loss": 0.3189,
      "step": 999
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.04164637252688408,
      "learning_rate": 0.00014138438880706924,
      "loss": 0.313,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 3400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.186503750246269e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
