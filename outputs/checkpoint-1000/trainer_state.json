{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 2.335993528366089,
      "learning_rate": 4e-05,
      "loss": 2.1044,
      "step": 1
    },
    {
      "epoch": 0.008,
      "grad_norm": 2.0764105319976807,
      "learning_rate": 8e-05,
      "loss": 2.0487,
      "step": 2
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.237320899963379,
      "learning_rate": 0.00012,
      "loss": 1.7674,
      "step": 3
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.9673866033554077,
      "learning_rate": 0.00016,
      "loss": 1.9002,
      "step": 4
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.7386751174926758,
      "learning_rate": 0.0002,
      "loss": 1.4927,
      "step": 5
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.9869507551193237,
      "learning_rate": 0.00019983935742971887,
      "loss": 1.5623,
      "step": 6
    },
    {
      "epoch": 0.028,
      "grad_norm": 1.6267954111099243,
      "learning_rate": 0.00019967871485943777,
      "loss": 1.3588,
      "step": 7
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.545114517211914,
      "learning_rate": 0.00019951807228915663,
      "loss": 1.011,
      "step": 8
    },
    {
      "epoch": 0.036,
      "grad_norm": 1.9192930459976196,
      "learning_rate": 0.00019935742971887552,
      "loss": 0.7881,
      "step": 9
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.575687289237976,
      "learning_rate": 0.0001991967871485944,
      "loss": 0.986,
      "step": 10
    },
    {
      "epoch": 0.044,
      "grad_norm": 1.5517014265060425,
      "learning_rate": 0.00019903614457831325,
      "loss": 0.7018,
      "step": 11
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.7378866672515869,
      "learning_rate": 0.00019887550200803214,
      "loss": 0.7582,
      "step": 12
    },
    {
      "epoch": 0.052,
      "grad_norm": 1.377540946006775,
      "learning_rate": 0.000198714859437751,
      "loss": 0.6486,
      "step": 13
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.4343770444393158,
      "learning_rate": 0.0001985542168674699,
      "loss": 0.5785,
      "step": 14
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.45771166682243347,
      "learning_rate": 0.00019839357429718877,
      "loss": 0.6001,
      "step": 15
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.4625537097454071,
      "learning_rate": 0.00019823293172690763,
      "loss": 0.6308,
      "step": 16
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.22737477719783783,
      "learning_rate": 0.00019807228915662652,
      "loss": 0.5656,
      "step": 17
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.2755506932735443,
      "learning_rate": 0.0001979116465863454,
      "loss": 0.5628,
      "step": 18
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.25809144973754883,
      "learning_rate": 0.00019775100401606425,
      "loss": 0.5148,
      "step": 19
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.26768651604652405,
      "learning_rate": 0.00019759036144578314,
      "loss": 0.6071,
      "step": 20
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.2362787276506424,
      "learning_rate": 0.000197429718875502,
      "loss": 0.5066,
      "step": 21
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.22010239958763123,
      "learning_rate": 0.0001972690763052209,
      "loss": 0.4615,
      "step": 22
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.16027754545211792,
      "learning_rate": 0.00019710843373493977,
      "loss": 0.5312,
      "step": 23
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.2091284692287445,
      "learning_rate": 0.00019694779116465866,
      "loss": 0.5346,
      "step": 24
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.16975794732570648,
      "learning_rate": 0.00019678714859437752,
      "loss": 0.414,
      "step": 25
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.1686217188835144,
      "learning_rate": 0.00019662650602409642,
      "loss": 0.3801,
      "step": 26
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.17285840213298798,
      "learning_rate": 0.00019646586345381528,
      "loss": 0.4267,
      "step": 27
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.1359291970729828,
      "learning_rate": 0.00019630522088353415,
      "loss": 0.4488,
      "step": 28
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.1422470659017563,
      "learning_rate": 0.000196144578313253,
      "loss": 0.4601,
      "step": 29
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.12334515154361725,
      "learning_rate": 0.0001959839357429719,
      "loss": 0.4322,
      "step": 30
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.1435297727584839,
      "learning_rate": 0.00019582329317269077,
      "loss": 0.3381,
      "step": 31
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.17335285246372223,
      "learning_rate": 0.00019566265060240966,
      "loss": 0.455,
      "step": 32
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.16958647966384888,
      "learning_rate": 0.00019550200803212852,
      "loss": 0.4504,
      "step": 33
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.12330922484397888,
      "learning_rate": 0.00019534136546184742,
      "loss": 0.4526,
      "step": 34
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.18745723366737366,
      "learning_rate": 0.00019518072289156628,
      "loss": 0.4559,
      "step": 35
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.1568511426448822,
      "learning_rate": 0.00019502008032128517,
      "loss": 0.3822,
      "step": 36
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.12996335327625275,
      "learning_rate": 0.000194859437751004,
      "loss": 0.3018,
      "step": 37
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.1483553797006607,
      "learning_rate": 0.0001946987951807229,
      "loss": 0.387,
      "step": 38
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.10578163713216782,
      "learning_rate": 0.00019453815261044177,
      "loss": 0.4175,
      "step": 39
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.11619345098733902,
      "learning_rate": 0.00019437751004016066,
      "loss": 0.4452,
      "step": 40
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.13259772956371307,
      "learning_rate": 0.00019421686746987952,
      "loss": 0.3646,
      "step": 41
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.11811677366495132,
      "learning_rate": 0.00019405622489959842,
      "loss": 0.3602,
      "step": 42
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.13755086064338684,
      "learning_rate": 0.00019389558232931728,
      "loss": 0.4618,
      "step": 43
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.09911821782588959,
      "learning_rate": 0.00019373493975903617,
      "loss": 0.3583,
      "step": 44
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.12044307589530945,
      "learning_rate": 0.00019357429718875504,
      "loss": 0.3332,
      "step": 45
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.1471354067325592,
      "learning_rate": 0.0001934136546184739,
      "loss": 0.384,
      "step": 46
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.08774939924478531,
      "learning_rate": 0.00019325301204819277,
      "loss": 0.3009,
      "step": 47
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.18413765728473663,
      "learning_rate": 0.00019309236947791166,
      "loss": 0.4787,
      "step": 48
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.14196979999542236,
      "learning_rate": 0.00019293172690763052,
      "loss": 0.3943,
      "step": 49
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.09552393108606339,
      "learning_rate": 0.00019277108433734942,
      "loss": 0.4077,
      "step": 50
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.06973276287317276,
      "learning_rate": 0.00019261044176706828,
      "loss": 0.4221,
      "step": 51
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.1023520901799202,
      "learning_rate": 0.00019244979919678717,
      "loss": 0.3369,
      "step": 52
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.12484478205442429,
      "learning_rate": 0.00019228915662650604,
      "loss": 0.4303,
      "step": 53
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.07435164600610733,
      "learning_rate": 0.00019212851405622493,
      "loss": 0.3116,
      "step": 54
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.09656102955341339,
      "learning_rate": 0.00019196787148594377,
      "loss": 0.3461,
      "step": 55
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.10590583831071854,
      "learning_rate": 0.00019180722891566266,
      "loss": 0.3895,
      "step": 56
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.09179916232824326,
      "learning_rate": 0.00019164658634538152,
      "loss": 0.3836,
      "step": 57
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.0764811784029007,
      "learning_rate": 0.00019148594377510042,
      "loss": 0.3577,
      "step": 58
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.07078667730093002,
      "learning_rate": 0.00019132530120481928,
      "loss": 0.3768,
      "step": 59
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.07151555269956589,
      "learning_rate": 0.00019116465863453817,
      "loss": 0.3165,
      "step": 60
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.06467336416244507,
      "learning_rate": 0.00019100401606425704,
      "loss": 0.3248,
      "step": 61
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.08873247355222702,
      "learning_rate": 0.00019084337349397593,
      "loss": 0.3983,
      "step": 62
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.07367058843374252,
      "learning_rate": 0.0001906827309236948,
      "loss": 0.3514,
      "step": 63
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.10028500854969025,
      "learning_rate": 0.00019052208835341369,
      "loss": 0.3982,
      "step": 64
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.09366507828235626,
      "learning_rate": 0.00019036144578313252,
      "loss": 0.4446,
      "step": 65
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.09211641550064087,
      "learning_rate": 0.00019020080321285142,
      "loss": 0.409,
      "step": 66
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.0891832783818245,
      "learning_rate": 0.00019004016064257028,
      "loss": 0.3825,
      "step": 67
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.09754790365695953,
      "learning_rate": 0.00018987951807228917,
      "loss": 0.3873,
      "step": 68
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.08898184448480606,
      "learning_rate": 0.00018971887550200804,
      "loss": 0.332,
      "step": 69
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.09052803367376328,
      "learning_rate": 0.00018955823293172693,
      "loss": 0.3241,
      "step": 70
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.09529199451208115,
      "learning_rate": 0.0001893975903614458,
      "loss": 0.3876,
      "step": 71
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.07163433730602264,
      "learning_rate": 0.00018923694779116469,
      "loss": 0.3512,
      "step": 72
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.08099471032619476,
      "learning_rate": 0.00018907630522088355,
      "loss": 0.3955,
      "step": 73
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.07369812577962875,
      "learning_rate": 0.00018891566265060242,
      "loss": 0.4445,
      "step": 74
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.09180382639169693,
      "learning_rate": 0.00018875502008032128,
      "loss": 0.2843,
      "step": 75
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.07807604968547821,
      "learning_rate": 0.00018859437751004017,
      "loss": 0.3374,
      "step": 76
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.07779300957918167,
      "learning_rate": 0.00018843373493975904,
      "loss": 0.3527,
      "step": 77
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.07766057550907135,
      "learning_rate": 0.00018827309236947793,
      "loss": 0.3607,
      "step": 78
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.06747744977474213,
      "learning_rate": 0.0001881124497991968,
      "loss": 0.2857,
      "step": 79
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.08666116744279861,
      "learning_rate": 0.00018795180722891569,
      "loss": 0.4099,
      "step": 80
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.06930151581764221,
      "learning_rate": 0.00018779116465863455,
      "loss": 0.3761,
      "step": 81
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.09900833666324615,
      "learning_rate": 0.00018763052208835344,
      "loss": 0.4676,
      "step": 82
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.06293882429599762,
      "learning_rate": 0.00018746987951807228,
      "loss": 0.327,
      "step": 83
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.07585560530424118,
      "learning_rate": 0.00018730923694779117,
      "loss": 0.3957,
      "step": 84
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.06081603094935417,
      "learning_rate": 0.00018714859437751004,
      "loss": 0.3235,
      "step": 85
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.07200583815574646,
      "learning_rate": 0.00018698795180722893,
      "loss": 0.3336,
      "step": 86
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.08296167850494385,
      "learning_rate": 0.0001868273092369478,
      "loss": 0.3309,
      "step": 87
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.08123473823070526,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.3498,
      "step": 88
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.07755216211080551,
      "learning_rate": 0.00018650602409638555,
      "loss": 0.3879,
      "step": 89
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.07337453961372375,
      "learning_rate": 0.00018634538152610444,
      "loss": 0.3695,
      "step": 90
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.07253798842430115,
      "learning_rate": 0.0001861847389558233,
      "loss": 0.3469,
      "step": 91
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.07289763540029526,
      "learning_rate": 0.00018602409638554217,
      "loss": 0.4025,
      "step": 92
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.07015921920537949,
      "learning_rate": 0.00018586345381526104,
      "loss": 0.3644,
      "step": 93
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.08791476488113403,
      "learning_rate": 0.00018570281124497993,
      "loss": 0.4197,
      "step": 94
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.07034123688936234,
      "learning_rate": 0.0001855421686746988,
      "loss": 0.4748,
      "step": 95
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.07798592001199722,
      "learning_rate": 0.0001853815261044177,
      "loss": 0.3519,
      "step": 96
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.1311381757259369,
      "learning_rate": 0.00018522088353413655,
      "loss": 0.4467,
      "step": 97
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.10356705635786057,
      "learning_rate": 0.00018506024096385544,
      "loss": 0.441,
      "step": 98
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.08293379843235016,
      "learning_rate": 0.0001848995983935743,
      "loss": 0.3453,
      "step": 99
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.1095842570066452,
      "learning_rate": 0.0001847389558232932,
      "loss": 0.3544,
      "step": 100
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.10393920540809631,
      "learning_rate": 0.00018457831325301204,
      "loss": 0.381,
      "step": 101
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.1124354749917984,
      "learning_rate": 0.00018441767068273093,
      "loss": 0.3562,
      "step": 102
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.11954368650913239,
      "learning_rate": 0.0001842570281124498,
      "loss": 0.3883,
      "step": 103
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.07915162295103073,
      "learning_rate": 0.0001840963855421687,
      "loss": 0.3348,
      "step": 104
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.06640903651714325,
      "learning_rate": 0.00018393574297188755,
      "loss": 0.2751,
      "step": 105
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.08325211703777313,
      "learning_rate": 0.00018377510040160644,
      "loss": 0.4079,
      "step": 106
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.10817031562328339,
      "learning_rate": 0.0001836144578313253,
      "loss": 0.4021,
      "step": 107
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.10752814263105392,
      "learning_rate": 0.0001834538152610442,
      "loss": 0.3674,
      "step": 108
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.0876161977648735,
      "learning_rate": 0.00018329317269076307,
      "loss": 0.3236,
      "step": 109
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.0885641872882843,
      "learning_rate": 0.00018313253012048193,
      "loss": 0.3748,
      "step": 110
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.0758696049451828,
      "learning_rate": 0.0001829718875502008,
      "loss": 0.3411,
      "step": 111
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.09491542726755142,
      "learning_rate": 0.0001828112449799197,
      "loss": 0.412,
      "step": 112
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.059162773191928864,
      "learning_rate": 0.00018265060240963855,
      "loss": 0.3147,
      "step": 113
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.0785461813211441,
      "learning_rate": 0.00018248995983935744,
      "loss": 0.3908,
      "step": 114
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.07412897795438766,
      "learning_rate": 0.0001823293172690763,
      "loss": 0.3639,
      "step": 115
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.09699475020170212,
      "learning_rate": 0.0001821686746987952,
      "loss": 0.374,
      "step": 116
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.0687938928604126,
      "learning_rate": 0.00018200803212851407,
      "loss": 0.3108,
      "step": 117
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.06590307503938675,
      "learning_rate": 0.00018184738955823296,
      "loss": 0.3258,
      "step": 118
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.08049190789461136,
      "learning_rate": 0.0001816867469879518,
      "loss": 0.4126,
      "step": 119
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.055834926664829254,
      "learning_rate": 0.0001815261044176707,
      "loss": 0.2752,
      "step": 120
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.06517331302165985,
      "learning_rate": 0.00018136546184738955,
      "loss": 0.3431,
      "step": 121
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.06454561650753021,
      "learning_rate": 0.00018120481927710844,
      "loss": 0.3132,
      "step": 122
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.08515077829360962,
      "learning_rate": 0.0001810441767068273,
      "loss": 0.3555,
      "step": 123
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.08610805124044418,
      "learning_rate": 0.0001808835341365462,
      "loss": 0.4093,
      "step": 124
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.07305427640676498,
      "learning_rate": 0.00018072289156626507,
      "loss": 0.398,
      "step": 125
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.0955563485622406,
      "learning_rate": 0.00018056224899598396,
      "loss": 0.3588,
      "step": 126
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.08587243407964706,
      "learning_rate": 0.00018040160642570282,
      "loss": 0.2839,
      "step": 127
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.054443467408418655,
      "learning_rate": 0.0001802409638554217,
      "loss": 0.277,
      "step": 128
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.07162444293498993,
      "learning_rate": 0.00018008032128514055,
      "loss": 0.3489,
      "step": 129
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.06221877411007881,
      "learning_rate": 0.00017991967871485944,
      "loss": 0.3312,
      "step": 130
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.07439780235290527,
      "learning_rate": 0.0001797590361445783,
      "loss": 0.3367,
      "step": 131
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.061645716428756714,
      "learning_rate": 0.0001795983935742972,
      "loss": 0.3791,
      "step": 132
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.061701029539108276,
      "learning_rate": 0.00017943775100401607,
      "loss": 0.3811,
      "step": 133
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.06690779328346252,
      "learning_rate": 0.00017927710843373496,
      "loss": 0.3455,
      "step": 134
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07746878266334534,
      "learning_rate": 0.00017911646586345382,
      "loss": 0.4215,
      "step": 135
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.07369588315486908,
      "learning_rate": 0.00017895582329317271,
      "loss": 0.4002,
      "step": 136
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.08413571119308472,
      "learning_rate": 0.00017879518072289155,
      "loss": 0.373,
      "step": 137
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.06716182082891464,
      "learning_rate": 0.00017863453815261044,
      "loss": 0.2714,
      "step": 138
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.06655866652727127,
      "learning_rate": 0.0001784738955823293,
      "loss": 0.3781,
      "step": 139
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.06919930875301361,
      "learning_rate": 0.0001783132530120482,
      "loss": 0.3294,
      "step": 140
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.06431005895137787,
      "learning_rate": 0.00017815261044176707,
      "loss": 0.338,
      "step": 141
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.08051612973213196,
      "learning_rate": 0.00017799196787148596,
      "loss": 0.3701,
      "step": 142
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.07067444175481796,
      "learning_rate": 0.00017783132530120482,
      "loss": 0.2364,
      "step": 143
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.07966411858797073,
      "learning_rate": 0.00017767068273092371,
      "loss": 0.3311,
      "step": 144
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0810563862323761,
      "learning_rate": 0.00017751004016064258,
      "loss": 0.3939,
      "step": 145
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.07686350494623184,
      "learning_rate": 0.00017734939759036144,
      "loss": 0.3207,
      "step": 146
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.07982712239027023,
      "learning_rate": 0.0001771887550200803,
      "loss": 0.3626,
      "step": 147
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.06971494108438492,
      "learning_rate": 0.0001770281124497992,
      "loss": 0.3391,
      "step": 148
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.08965300768613815,
      "learning_rate": 0.00017686746987951807,
      "loss": 0.4032,
      "step": 149
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.08622463792562485,
      "learning_rate": 0.00017670682730923696,
      "loss": 0.3394,
      "step": 150
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.08759962022304535,
      "learning_rate": 0.00017654618473895582,
      "loss": 0.3615,
      "step": 151
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.07205653190612793,
      "learning_rate": 0.00017638554216867471,
      "loss": 0.362,
      "step": 152
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.06996813416481018,
      "learning_rate": 0.00017622489959839358,
      "loss": 0.34,
      "step": 153
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.06293423473834991,
      "learning_rate": 0.00017606425702811247,
      "loss": 0.3187,
      "step": 154
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06999028474092484,
      "learning_rate": 0.00017590361445783134,
      "loss": 0.3619,
      "step": 155
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.05771356076002121,
      "learning_rate": 0.0001757429718875502,
      "loss": 0.3452,
      "step": 156
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.06877999007701874,
      "learning_rate": 0.00017558232931726907,
      "loss": 0.3305,
      "step": 157
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.08761386573314667,
      "learning_rate": 0.00017542168674698796,
      "loss": 0.4431,
      "step": 158
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.07388655096292496,
      "learning_rate": 0.00017526104417670682,
      "loss": 0.3475,
      "step": 159
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.07229222357273102,
      "learning_rate": 0.00017510040160642571,
      "loss": 0.358,
      "step": 160
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.06303266435861588,
      "learning_rate": 0.00017493975903614458,
      "loss": 0.3167,
      "step": 161
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.07816348969936371,
      "learning_rate": 0.00017477911646586347,
      "loss": 0.4034,
      "step": 162
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.0892229676246643,
      "learning_rate": 0.00017461847389558234,
      "loss": 0.3956,
      "step": 163
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.08024004846811295,
      "learning_rate": 0.00017445783132530123,
      "loss": 0.3121,
      "step": 164
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.1255861222743988,
      "learning_rate": 0.0001742971887550201,
      "loss": 0.3656,
      "step": 165
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.1040395051240921,
      "learning_rate": 0.00017413654618473896,
      "loss": 0.3903,
      "step": 166
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.0626545324921608,
      "learning_rate": 0.00017397590361445782,
      "loss": 0.3353,
      "step": 167
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.10490436851978302,
      "learning_rate": 0.00017381526104417671,
      "loss": 0.4114,
      "step": 168
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.07469852268695831,
      "learning_rate": 0.00017365461847389558,
      "loss": 0.3456,
      "step": 169
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.11949927359819412,
      "learning_rate": 0.00017349397590361447,
      "loss": 0.4294,
      "step": 170
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.10944344103336334,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4077,
      "step": 171
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.08204059302806854,
      "learning_rate": 0.00017317269076305223,
      "loss": 0.3674,
      "step": 172
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.09916073828935623,
      "learning_rate": 0.0001730120481927711,
      "loss": 0.4261,
      "step": 173
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.09841838479042053,
      "learning_rate": 0.00017285140562248996,
      "loss": 0.4026,
      "step": 174
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.06968950480222702,
      "learning_rate": 0.00017269076305220885,
      "loss": 0.344,
      "step": 175
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.06344326585531235,
      "learning_rate": 0.00017253012048192771,
      "loss": 0.4312,
      "step": 176
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.0671236515045166,
      "learning_rate": 0.00017236947791164658,
      "loss": 0.3303,
      "step": 177
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.08015843480825424,
      "learning_rate": 0.00017220883534136547,
      "loss": 0.3147,
      "step": 178
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.07488393038511276,
      "learning_rate": 0.00017204819277108434,
      "loss": 0.3574,
      "step": 179
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06061435118317604,
      "learning_rate": 0.00017188755020080323,
      "loss": 0.2953,
      "step": 180
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.070791095495224,
      "learning_rate": 0.0001717269076305221,
      "loss": 0.3448,
      "step": 181
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.06472242623567581,
      "learning_rate": 0.00017156626506024099,
      "loss": 0.3519,
      "step": 182
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.07848669588565826,
      "learning_rate": 0.00017140562248995985,
      "loss": 0.4092,
      "step": 183
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.06604913622140884,
      "learning_rate": 0.00017124497991967871,
      "loss": 0.3529,
      "step": 184
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.061453040689229965,
      "learning_rate": 0.0001710843373493976,
      "loss": 0.3624,
      "step": 185
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.05750114098191261,
      "learning_rate": 0.00017092369477911647,
      "loss": 0.3066,
      "step": 186
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.05608813464641571,
      "learning_rate": 0.00017076305220883536,
      "loss": 0.2696,
      "step": 187
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.07880958914756775,
      "learning_rate": 0.00017060240963855423,
      "loss": 0.373,
      "step": 188
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.0631590187549591,
      "learning_rate": 0.0001704417670682731,
      "loss": 0.3457,
      "step": 189
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.07894622534513474,
      "learning_rate": 0.00017028112449799199,
      "loss": 0.4256,
      "step": 190
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.061956290155649185,
      "learning_rate": 0.00017012048192771085,
      "loss": 0.3374,
      "step": 191
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.05901569128036499,
      "learning_rate": 0.00016995983935742971,
      "loss": 0.2921,
      "step": 192
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.07318167388439178,
      "learning_rate": 0.0001697991967871486,
      "loss": 0.3847,
      "step": 193
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.059468816965818405,
      "learning_rate": 0.00016963855421686747,
      "loss": 0.3339,
      "step": 194
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.07377639412879944,
      "learning_rate": 0.00016947791164658636,
      "loss": 0.3926,
      "step": 195
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.055249638855457306,
      "learning_rate": 0.00016931726907630523,
      "loss": 0.33,
      "step": 196
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.08036017417907715,
      "learning_rate": 0.00016915662650602412,
      "loss": 0.4066,
      "step": 197
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.06219193711876869,
      "learning_rate": 0.00016899598393574299,
      "loss": 0.3378,
      "step": 198
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.06654492765665054,
      "learning_rate": 0.00016883534136546185,
      "loss": 0.3968,
      "step": 199
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.07095576077699661,
      "learning_rate": 0.00016867469879518074,
      "loss": 0.4038,
      "step": 200
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.06736365705728531,
      "learning_rate": 0.0001685140562248996,
      "loss": 0.3412,
      "step": 201
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.08883492648601532,
      "learning_rate": 0.00016835341365461847,
      "loss": 0.347,
      "step": 202
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.0840194821357727,
      "learning_rate": 0.00016819277108433736,
      "loss": 0.3516,
      "step": 203
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.06955178081989288,
      "learning_rate": 0.00016803212851405623,
      "loss": 0.3325,
      "step": 204
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.07094302773475647,
      "learning_rate": 0.00016787148594377512,
      "loss": 0.3636,
      "step": 205
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.09593607485294342,
      "learning_rate": 0.00016771084337349399,
      "loss": 0.3592,
      "step": 206
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.06644279509782791,
      "learning_rate": 0.00016755020080321288,
      "loss": 0.3806,
      "step": 207
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.06576841324567795,
      "learning_rate": 0.00016738955823293174,
      "loss": 0.347,
      "step": 208
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.06923682987689972,
      "learning_rate": 0.0001672289156626506,
      "loss": 0.2953,
      "step": 209
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.06872890889644623,
      "learning_rate": 0.00016706827309236947,
      "loss": 0.3702,
      "step": 210
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.05236044526100159,
      "learning_rate": 0.00016690763052208836,
      "loss": 0.2673,
      "step": 211
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.07570517808198929,
      "learning_rate": 0.00016674698795180723,
      "loss": 0.336,
      "step": 212
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.08200521022081375,
      "learning_rate": 0.00016658634538152612,
      "loss": 0.445,
      "step": 213
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.05843085050582886,
      "learning_rate": 0.00016642570281124499,
      "loss": 0.3116,
      "step": 214
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.0784873217344284,
      "learning_rate": 0.00016626506024096388,
      "loss": 0.3977,
      "step": 215
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.08429329097270966,
      "learning_rate": 0.00016610441767068274,
      "loss": 0.341,
      "step": 216
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.06723389774560928,
      "learning_rate": 0.00016594377510040163,
      "loss": 0.3981,
      "step": 217
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.053809914737939835,
      "learning_rate": 0.0001657831325301205,
      "loss": 0.3266,
      "step": 218
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.07245859503746033,
      "learning_rate": 0.00016562248995983936,
      "loss": 0.4244,
      "step": 219
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.09080168604850769,
      "learning_rate": 0.00016546184738955823,
      "loss": 0.364,
      "step": 220
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.07945255935192108,
      "learning_rate": 0.00016530120481927712,
      "loss": 0.3989,
      "step": 221
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.06590015441179276,
      "learning_rate": 0.00016514056224899599,
      "loss": 0.4175,
      "step": 222
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.07693785429000854,
      "learning_rate": 0.00016497991967871488,
      "loss": 0.3823,
      "step": 223
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.09281361103057861,
      "learning_rate": 0.00016481927710843374,
      "loss": 0.3511,
      "step": 224
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.06478194147348404,
      "learning_rate": 0.00016465863453815263,
      "loss": 0.2972,
      "step": 225
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.058284956961870193,
      "learning_rate": 0.0001644979919678715,
      "loss": 0.356,
      "step": 226
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.06803490221500397,
      "learning_rate": 0.0001643373493975904,
      "loss": 0.3542,
      "step": 227
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.09583438187837601,
      "learning_rate": 0.00016417670682730923,
      "loss": 0.3945,
      "step": 228
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.06071958318352699,
      "learning_rate": 0.00016401606425702812,
      "loss": 0.327,
      "step": 229
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.07146310806274414,
      "learning_rate": 0.00016385542168674699,
      "loss": 0.3992,
      "step": 230
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.07429759949445724,
      "learning_rate": 0.00016369477911646588,
      "loss": 0.3296,
      "step": 231
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.06697545200586319,
      "learning_rate": 0.00016353413654618474,
      "loss": 0.3349,
      "step": 232
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.051754191517829895,
      "learning_rate": 0.00016337349397590363,
      "loss": 0.3249,
      "step": 233
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.056257762014865875,
      "learning_rate": 0.0001632128514056225,
      "loss": 0.2442,
      "step": 234
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.057602159678936005,
      "learning_rate": 0.0001630522088353414,
      "loss": 0.3051,
      "step": 235
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.06567410379648209,
      "learning_rate": 0.00016289156626506026,
      "loss": 0.293,
      "step": 236
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.060780271887779236,
      "learning_rate": 0.00016273092369477912,
      "loss": 0.2917,
      "step": 237
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.07810168713331223,
      "learning_rate": 0.00016257028112449799,
      "loss": 0.3813,
      "step": 238
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.08475077152252197,
      "learning_rate": 0.00016240963855421688,
      "loss": 0.3991,
      "step": 239
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06024686247110367,
      "learning_rate": 0.00016224899598393574,
      "loss": 0.3483,
      "step": 240
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.06779856234788895,
      "learning_rate": 0.00016208835341365463,
      "loss": 0.3596,
      "step": 241
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.06725198030471802,
      "learning_rate": 0.0001619277108433735,
      "loss": 0.3426,
      "step": 242
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.0619363971054554,
      "learning_rate": 0.0001617670682730924,
      "loss": 0.3552,
      "step": 243
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.06151701137423515,
      "learning_rate": 0.00016160642570281126,
      "loss": 0.4036,
      "step": 244
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.06504974514245987,
      "learning_rate": 0.00016144578313253015,
      "loss": 0.3599,
      "step": 245
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.058046501129865646,
      "learning_rate": 0.00016128514056224899,
      "loss": 0.3057,
      "step": 246
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.07275928556919098,
      "learning_rate": 0.00016112449799196788,
      "loss": 0.441,
      "step": 247
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.05370446294546127,
      "learning_rate": 0.00016096385542168674,
      "loss": 0.3622,
      "step": 248
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.0528927743434906,
      "learning_rate": 0.00016080321285140563,
      "loss": 0.3417,
      "step": 249
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06459882110357285,
      "learning_rate": 0.0001606425702811245,
      "loss": 0.4024,
      "step": 250
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.05694146081805229,
      "learning_rate": 0.0001604819277108434,
      "loss": 0.3239,
      "step": 251
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.052814971655607224,
      "learning_rate": 0.00016032128514056226,
      "loss": 0.2924,
      "step": 252
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.0602634958922863,
      "learning_rate": 0.00016016064257028115,
      "loss": 0.3156,
      "step": 253
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.08341137319803238,
      "learning_rate": 0.00016,
      "loss": 0.4277,
      "step": 254
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.0671900063753128,
      "learning_rate": 0.00015983935742971888,
      "loss": 0.4155,
      "step": 255
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.07289593666791916,
      "learning_rate": 0.00015967871485943774,
      "loss": 0.3468,
      "step": 256
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.08262821286916733,
      "learning_rate": 0.00015951807228915663,
      "loss": 0.3703,
      "step": 257
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.06663036346435547,
      "learning_rate": 0.0001593574297188755,
      "loss": 0.3566,
      "step": 258
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.08363865315914154,
      "learning_rate": 0.0001591967871485944,
      "loss": 0.3789,
      "step": 259
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.07572802156209946,
      "learning_rate": 0.00015903614457831326,
      "loss": 0.3779,
      "step": 260
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.05516692250967026,
      "learning_rate": 0.00015887550200803215,
      "loss": 0.3354,
      "step": 261
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.06751298904418945,
      "learning_rate": 0.000158714859437751,
      "loss": 0.3784,
      "step": 262
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.067877858877182,
      "learning_rate": 0.0001585542168674699,
      "loss": 0.3421,
      "step": 263
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.049092669039964676,
      "learning_rate": 0.00015839357429718874,
      "loss": 0.2879,
      "step": 264
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.06602587550878525,
      "learning_rate": 0.00015823293172690763,
      "loss": 0.3464,
      "step": 265
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.09919986873865128,
      "learning_rate": 0.0001580722891566265,
      "loss": 0.3853,
      "step": 266
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.07069980353116989,
      "learning_rate": 0.0001579116465863454,
      "loss": 0.3628,
      "step": 267
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.07314503937959671,
      "learning_rate": 0.00015775100401606426,
      "loss": 0.4107,
      "step": 268
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.055256351828575134,
      "learning_rate": 0.00015759036144578315,
      "loss": 0.3522,
      "step": 269
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.08744784444570541,
      "learning_rate": 0.000157429718875502,
      "loss": 0.4255,
      "step": 270
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.0754099041223526,
      "learning_rate": 0.0001572690763052209,
      "loss": 0.3107,
      "step": 271
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.09834086894989014,
      "learning_rate": 0.00015710843373493977,
      "loss": 0.4183,
      "step": 272
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.06723307073116302,
      "learning_rate": 0.00015694779116465866,
      "loss": 0.3162,
      "step": 273
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.08567360788583755,
      "learning_rate": 0.0001567871485943775,
      "loss": 0.3772,
      "step": 274
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.0799490436911583,
      "learning_rate": 0.0001566265060240964,
      "loss": 0.4218,
      "step": 275
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.08629798144102097,
      "learning_rate": 0.00015646586345381526,
      "loss": 0.3811,
      "step": 276
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.07569904625415802,
      "learning_rate": 0.00015630522088353415,
      "loss": 0.2752,
      "step": 277
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.07915880531072617,
      "learning_rate": 0.000156144578313253,
      "loss": 0.3641,
      "step": 278
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.06226642429828644,
      "learning_rate": 0.0001559839357429719,
      "loss": 0.3176,
      "step": 279
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.07813778519630432,
      "learning_rate": 0.00015582329317269077,
      "loss": 0.3488,
      "step": 280
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.09409919381141663,
      "learning_rate": 0.00015566265060240966,
      "loss": 0.3239,
      "step": 281
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.05827879160642624,
      "learning_rate": 0.00015550200803212853,
      "loss": 0.3125,
      "step": 282
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.058646511286497116,
      "learning_rate": 0.0001553413654618474,
      "loss": 0.3374,
      "step": 283
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.06671176105737686,
      "learning_rate": 0.00015518072289156626,
      "loss": 0.3279,
      "step": 284
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.061283767223358154,
      "learning_rate": 0.00015502008032128515,
      "loss": 0.3447,
      "step": 285
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.08076661825180054,
      "learning_rate": 0.000154859437751004,
      "loss": 0.361,
      "step": 286
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.0608193464577198,
      "learning_rate": 0.0001546987951807229,
      "loss": 0.2849,
      "step": 287
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.08166457712650299,
      "learning_rate": 0.00015453815261044177,
      "loss": 0.4369,
      "step": 288
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.06410278379917145,
      "learning_rate": 0.00015437751004016066,
      "loss": 0.3736,
      "step": 289
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.0636240616440773,
      "learning_rate": 0.00015421686746987953,
      "loss": 0.3278,
      "step": 290
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.05957586318254471,
      "learning_rate": 0.00015405622489959842,
      "loss": 0.2925,
      "step": 291
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.056080400943756104,
      "learning_rate": 0.00015389558232931726,
      "loss": 0.3226,
      "step": 292
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.0644913986325264,
      "learning_rate": 0.00015373493975903615,
      "loss": 0.3878,
      "step": 293
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.09171632677316666,
      "learning_rate": 0.00015357429718875501,
      "loss": 0.3462,
      "step": 294
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.05310825631022453,
      "learning_rate": 0.0001534136546184739,
      "loss": 0.3016,
      "step": 295
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.06027137488126755,
      "learning_rate": 0.00015325301204819277,
      "loss": 0.3833,
      "step": 296
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.06841080635786057,
      "learning_rate": 0.00015309236947791166,
      "loss": 0.3867,
      "step": 297
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.0633535385131836,
      "learning_rate": 0.00015293172690763053,
      "loss": 0.3505,
      "step": 298
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.0656474158167839,
      "learning_rate": 0.00015277108433734942,
      "loss": 0.3606,
      "step": 299
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.06487196683883667,
      "learning_rate": 0.00015261044176706828,
      "loss": 0.3376,
      "step": 300
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.07483614981174469,
      "learning_rate": 0.00015244979919678715,
      "loss": 0.4073,
      "step": 301
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.05843934416770935,
      "learning_rate": 0.00015228915662650601,
      "loss": 0.3911,
      "step": 302
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.07266239821910858,
      "learning_rate": 0.0001521285140562249,
      "loss": 0.4023,
      "step": 303
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.06044679135084152,
      "learning_rate": 0.00015196787148594377,
      "loss": 0.3381,
      "step": 304
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.06958542764186859,
      "learning_rate": 0.00015180722891566266,
      "loss": 0.3439,
      "step": 305
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.0557260625064373,
      "learning_rate": 0.00015164658634538153,
      "loss": 0.3503,
      "step": 306
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.08595304191112518,
      "learning_rate": 0.00015148594377510042,
      "loss": 0.3699,
      "step": 307
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.06100659444928169,
      "learning_rate": 0.00015132530120481928,
      "loss": 0.348,
      "step": 308
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.06639671325683594,
      "learning_rate": 0.00015116465863453818,
      "loss": 0.3804,
      "step": 309
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.07296732068061829,
      "learning_rate": 0.00015100401606425701,
      "loss": 0.3552,
      "step": 310
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.07658863067626953,
      "learning_rate": 0.0001508433734939759,
      "loss": 0.3849,
      "step": 311
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.08187424391508102,
      "learning_rate": 0.00015068273092369477,
      "loss": 0.3045,
      "step": 312
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.054695986211299896,
      "learning_rate": 0.00015052208835341366,
      "loss": 0.2914,
      "step": 313
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.07878047227859497,
      "learning_rate": 0.00015036144578313253,
      "loss": 0.3202,
      "step": 314
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.05848023667931557,
      "learning_rate": 0.00015020080321285142,
      "loss": 0.2703,
      "step": 315
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.07939372956752777,
      "learning_rate": 0.00015004016064257028,
      "loss": 0.3402,
      "step": 316
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.08254089951515198,
      "learning_rate": 0.00014987951807228918,
      "loss": 0.3723,
      "step": 317
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.05142083391547203,
      "learning_rate": 0.00014971887550200804,
      "loss": 0.2588,
      "step": 318
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.08336351066827774,
      "learning_rate": 0.0001495582329317269,
      "loss": 0.3993,
      "step": 319
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.08617806434631348,
      "learning_rate": 0.00014939759036144577,
      "loss": 0.3895,
      "step": 320
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.05436963215470314,
      "learning_rate": 0.00014923694779116466,
      "loss": 0.3036,
      "step": 321
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.06492237746715546,
      "learning_rate": 0.00014907630522088353,
      "loss": 0.3331,
      "step": 322
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.06815395504236221,
      "learning_rate": 0.00014891566265060242,
      "loss": 0.3081,
      "step": 323
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.07209860533475876,
      "learning_rate": 0.00014875502008032128,
      "loss": 0.3465,
      "step": 324
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.0524505153298378,
      "learning_rate": 0.00014859437751004018,
      "loss": 0.312,
      "step": 325
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.056617751717567444,
      "learning_rate": 0.00014843373493975904,
      "loss": 0.3368,
      "step": 326
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.06496227532625198,
      "learning_rate": 0.00014827309236947793,
      "loss": 0.328,
      "step": 327
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.06006057560443878,
      "learning_rate": 0.00014811244979919677,
      "loss": 0.2888,
      "step": 328
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.060342662036418915,
      "learning_rate": 0.00014795180722891566,
      "loss": 0.3124,
      "step": 329
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.07837870717048645,
      "learning_rate": 0.00014779116465863453,
      "loss": 0.3481,
      "step": 330
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.061768848448991776,
      "learning_rate": 0.00014763052208835342,
      "loss": 0.3193,
      "step": 331
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.07427514344453812,
      "learning_rate": 0.00014746987951807228,
      "loss": 0.4524,
      "step": 332
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.05896192044019699,
      "learning_rate": 0.00014730923694779118,
      "loss": 0.3636,
      "step": 333
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.06696806102991104,
      "learning_rate": 0.00014714859437751004,
      "loss": 0.3641,
      "step": 334
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.0650644451379776,
      "learning_rate": 0.00014698795180722893,
      "loss": 0.3223,
      "step": 335
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.06293586641550064,
      "learning_rate": 0.0001468273092369478,
      "loss": 0.3445,
      "step": 336
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.0640149638056755,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.3811,
      "step": 337
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.054649729281663895,
      "learning_rate": 0.00014650602409638555,
      "loss": 0.3175,
      "step": 338
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.07102999091148376,
      "learning_rate": 0.00014634538152610442,
      "loss": 0.3178,
      "step": 339
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.08287917822599411,
      "learning_rate": 0.00014618473895582328,
      "loss": 0.3797,
      "step": 340
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.07051320374011993,
      "learning_rate": 0.00014602409638554218,
      "loss": 0.3476,
      "step": 341
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.056444812566041946,
      "learning_rate": 0.00014586345381526104,
      "loss": 0.3053,
      "step": 342
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.07634054869413376,
      "learning_rate": 0.00014570281124497993,
      "loss": 0.3565,
      "step": 343
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.05936524271965027,
      "learning_rate": 0.0001455421686746988,
      "loss": 0.3073,
      "step": 344
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.07029277086257935,
      "learning_rate": 0.0001453815261044177,
      "loss": 0.41,
      "step": 345
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.07487163692712784,
      "learning_rate": 0.00014522088353413655,
      "loss": 0.2838,
      "step": 346
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.05611513555049896,
      "learning_rate": 0.00014506024096385542,
      "loss": 0.3929,
      "step": 347
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.062323667109012604,
      "learning_rate": 0.0001448995983935743,
      "loss": 0.3378,
      "step": 348
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.06772825121879578,
      "learning_rate": 0.00014473895582329318,
      "loss": 0.3344,
      "step": 349
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.06531104445457458,
      "learning_rate": 0.00014457831325301204,
      "loss": 0.3927,
      "step": 350
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.0679556280374527,
      "learning_rate": 0.00014441767068273093,
      "loss": 0.372,
      "step": 351
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.050222743302583694,
      "learning_rate": 0.0001442570281124498,
      "loss": 0.3286,
      "step": 352
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.062492139637470245,
      "learning_rate": 0.0001440963855421687,
      "loss": 0.3553,
      "step": 353
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.06843584030866623,
      "learning_rate": 0.00014393574297188756,
      "loss": 0.3963,
      "step": 354
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.07956761866807938,
      "learning_rate": 0.00014377510040160642,
      "loss": 0.4109,
      "step": 355
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.06414541602134705,
      "learning_rate": 0.0001436144578313253,
      "loss": 0.3187,
      "step": 356
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.07679648697376251,
      "learning_rate": 0.00014345381526104418,
      "loss": 0.4499,
      "step": 357
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.06024763733148575,
      "learning_rate": 0.00014329317269076307,
      "loss": 0.2708,
      "step": 358
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.06396373361349106,
      "learning_rate": 0.00014313253012048193,
      "loss": 0.3622,
      "step": 359
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.05741715058684349,
      "learning_rate": 0.0001429718875502008,
      "loss": 0.3304,
      "step": 360
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.06880754232406616,
      "learning_rate": 0.0001428112449799197,
      "loss": 0.3792,
      "step": 361
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.0671331137418747,
      "learning_rate": 0.00014265060240963856,
      "loss": 0.3078,
      "step": 362
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.05137662962079048,
      "learning_rate": 0.00014248995983935745,
      "loss": 0.3237,
      "step": 363
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.048981547355651855,
      "learning_rate": 0.0001423293172690763,
      "loss": 0.3334,
      "step": 364
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.057533781975507736,
      "learning_rate": 0.00014216867469879518,
      "loss": 0.3242,
      "step": 365
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.0870923399925232,
      "learning_rate": 0.00014200803212851407,
      "loss": 0.3538,
      "step": 366
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.07887953519821167,
      "learning_rate": 0.00014184738955823293,
      "loss": 0.3622,
      "step": 367
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.07234906405210495,
      "learning_rate": 0.00014168674698795183,
      "loss": 0.3133,
      "step": 368
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.06823708117008209,
      "learning_rate": 0.0001415261044176707,
      "loss": 0.3414,
      "step": 369
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.04216761514544487,
      "learning_rate": 0.00014136546184738956,
      "loss": 0.2962,
      "step": 370
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.0656755119562149,
      "learning_rate": 0.00014120481927710845,
      "loss": 0.3893,
      "step": 371
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.05478699132800102,
      "learning_rate": 0.0001410441767068273,
      "loss": 0.3127,
      "step": 372
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.08511320501565933,
      "learning_rate": 0.0001408835341365462,
      "loss": 0.3642,
      "step": 373
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.06668339669704437,
      "learning_rate": 0.00014072289156626507,
      "loss": 0.3729,
      "step": 374
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.05444904789328575,
      "learning_rate": 0.00014056224899598393,
      "loss": 0.3775,
      "step": 375
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.06002403795719147,
      "learning_rate": 0.00014040160642570283,
      "loss": 0.3169,
      "step": 376
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.06433095037937164,
      "learning_rate": 0.0001402409638554217,
      "loss": 0.3722,
      "step": 377
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.06275834143161774,
      "learning_rate": 0.00014008032128514058,
      "loss": 0.2992,
      "step": 378
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.06934570521116257,
      "learning_rate": 0.00013991967871485945,
      "loss": 0.3294,
      "step": 379
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.0770656168460846,
      "learning_rate": 0.00013975903614457834,
      "loss": 0.4519,
      "step": 380
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.04543275013566017,
      "learning_rate": 0.0001395983935742972,
      "loss": 0.2634,
      "step": 381
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.05905156582593918,
      "learning_rate": 0.00013943775100401607,
      "loss": 0.3331,
      "step": 382
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.06380102038383484,
      "learning_rate": 0.00013927710843373493,
      "loss": 0.3557,
      "step": 383
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.055626995861530304,
      "learning_rate": 0.00013911646586345383,
      "loss": 0.3012,
      "step": 384
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.04658617451786995,
      "learning_rate": 0.0001389558232931727,
      "loss": 0.2507,
      "step": 385
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.05973340943455696,
      "learning_rate": 0.00013879518072289158,
      "loss": 0.4102,
      "step": 386
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.06271574646234512,
      "learning_rate": 0.00013863453815261045,
      "loss": 0.262,
      "step": 387
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.07494782656431198,
      "learning_rate": 0.00013847389558232934,
      "loss": 0.4246,
      "step": 388
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.057505398988723755,
      "learning_rate": 0.0001383132530120482,
      "loss": 0.3433,
      "step": 389
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.08697894960641861,
      "learning_rate": 0.0001381526104417671,
      "loss": 0.3339,
      "step": 390
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.06824053078889847,
      "learning_rate": 0.00013799196787148596,
      "loss": 0.4237,
      "step": 391
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.05841195955872536,
      "learning_rate": 0.00013783132530120483,
      "loss": 0.3389,
      "step": 392
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.07292234897613525,
      "learning_rate": 0.0001376706827309237,
      "loss": 0.3483,
      "step": 393
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.07554447650909424,
      "learning_rate": 0.00013751004016064258,
      "loss": 0.3536,
      "step": 394
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.08072918653488159,
      "learning_rate": 0.00013734939759036145,
      "loss": 0.3924,
      "step": 395
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.05952663719654083,
      "learning_rate": 0.00013718875502008034,
      "loss": 0.2671,
      "step": 396
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.062004029750823975,
      "learning_rate": 0.0001370281124497992,
      "loss": 0.3471,
      "step": 397
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.05111677944660187,
      "learning_rate": 0.0001368674698795181,
      "loss": 0.3461,
      "step": 398
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.06390495598316193,
      "learning_rate": 0.00013670682730923696,
      "loss": 0.3126,
      "step": 399
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.10742776095867157,
      "learning_rate": 0.00013654618473895585,
      "loss": 0.4099,
      "step": 400
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.05186112970113754,
      "learning_rate": 0.0001363855421686747,
      "loss": 0.3015,
      "step": 401
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.058421120047569275,
      "learning_rate": 0.00013622489959839358,
      "loss": 0.3438,
      "step": 402
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.07301514595746994,
      "learning_rate": 0.00013606425702811245,
      "loss": 0.4,
      "step": 403
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.05842231959104538,
      "learning_rate": 0.00013590361445783134,
      "loss": 0.3682,
      "step": 404
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.05466844141483307,
      "learning_rate": 0.0001357429718875502,
      "loss": 0.3584,
      "step": 405
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.06547771394252777,
      "learning_rate": 0.0001355823293172691,
      "loss": 0.4307,
      "step": 406
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.06842739880084991,
      "learning_rate": 0.00013542168674698796,
      "loss": 0.3946,
      "step": 407
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.055751364678144455,
      "learning_rate": 0.00013526104417670685,
      "loss": 0.3435,
      "step": 408
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.06769607216119766,
      "learning_rate": 0.00013510040160642572,
      "loss": 0.3322,
      "step": 409
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.050946980714797974,
      "learning_rate": 0.00013493975903614458,
      "loss": 0.3128,
      "step": 410
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.08187974244356155,
      "learning_rate": 0.00013477911646586345,
      "loss": 0.386,
      "step": 411
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.059112466871738434,
      "learning_rate": 0.00013461847389558234,
      "loss": 0.3063,
      "step": 412
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.09134822338819504,
      "learning_rate": 0.0001344578313253012,
      "loss": 0.4101,
      "step": 413
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.0735611543059349,
      "learning_rate": 0.0001342971887550201,
      "loss": 0.3685,
      "step": 414
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.05567735806107521,
      "learning_rate": 0.00013413654618473896,
      "loss": 0.369,
      "step": 415
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.0744597464799881,
      "learning_rate": 0.00013397590361445785,
      "loss": 0.3272,
      "step": 416
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.07096382975578308,
      "learning_rate": 0.00013381526104417672,
      "loss": 0.3756,
      "step": 417
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.06880462914705276,
      "learning_rate": 0.0001336546184738956,
      "loss": 0.3668,
      "step": 418
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.05321045219898224,
      "learning_rate": 0.00013349397590361445,
      "loss": 0.3133,
      "step": 419
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.07057032734155655,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.3746,
      "step": 420
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.06684423983097076,
      "learning_rate": 0.0001331726907630522,
      "loss": 0.3178,
      "step": 421
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.06373964995145798,
      "learning_rate": 0.0001330120481927711,
      "loss": 0.3951,
      "step": 422
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.08410476893186569,
      "learning_rate": 0.00013285140562248996,
      "loss": 0.4107,
      "step": 423
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.06285464018583298,
      "learning_rate": 0.00013269076305220885,
      "loss": 0.3483,
      "step": 424
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.09642153233289719,
      "learning_rate": 0.00013253012048192772,
      "loss": 0.4496,
      "step": 425
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.05279170349240303,
      "learning_rate": 0.0001323694779116466,
      "loss": 0.276,
      "step": 426
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.061423420906066895,
      "learning_rate": 0.00013220883534136547,
      "loss": 0.2914,
      "step": 427
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.0811285600066185,
      "learning_rate": 0.00013204819277108434,
      "loss": 0.3513,
      "step": 428
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.04665320739150047,
      "learning_rate": 0.0001318875502008032,
      "loss": 0.2817,
      "step": 429
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.06472908705472946,
      "learning_rate": 0.0001317269076305221,
      "loss": 0.3985,
      "step": 430
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.07561035454273224,
      "learning_rate": 0.00013156626506024096,
      "loss": 0.3673,
      "step": 431
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.08141298592090607,
      "learning_rate": 0.00013140562248995985,
      "loss": 0.4384,
      "step": 432
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.05684703215956688,
      "learning_rate": 0.00013124497991967872,
      "loss": 0.2688,
      "step": 433
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.07057306915521622,
      "learning_rate": 0.0001310843373493976,
      "loss": 0.3571,
      "step": 434
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.06435080617666245,
      "learning_rate": 0.00013092369477911648,
      "loss": 0.3257,
      "step": 435
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.05148976296186447,
      "learning_rate": 0.00013076305220883537,
      "loss": 0.3112,
      "step": 436
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.0629786029458046,
      "learning_rate": 0.0001306024096385542,
      "loss": 0.4097,
      "step": 437
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.04584609344601631,
      "learning_rate": 0.0001304417670682731,
      "loss": 0.2812,
      "step": 438
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.04421386122703552,
      "learning_rate": 0.00013028112449799196,
      "loss": 0.2564,
      "step": 439
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.054488200694322586,
      "learning_rate": 0.00013012048192771085,
      "loss": 0.2832,
      "step": 440
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.07859399169683456,
      "learning_rate": 0.00012995983935742972,
      "loss": 0.4182,
      "step": 441
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.0666842833161354,
      "learning_rate": 0.0001297991967871486,
      "loss": 0.3526,
      "step": 442
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.0540032684803009,
      "learning_rate": 0.00012963855421686748,
      "loss": 0.3454,
      "step": 443
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.07004120200872421,
      "learning_rate": 0.00012947791164658637,
      "loss": 0.4235,
      "step": 444
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.08790916949510574,
      "learning_rate": 0.00012931726907630523,
      "loss": 0.3941,
      "step": 445
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.04883920028805733,
      "learning_rate": 0.0001291566265060241,
      "loss": 0.3172,
      "step": 446
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.08013439923524857,
      "learning_rate": 0.00012899598393574296,
      "loss": 0.3427,
      "step": 447
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.07826225459575653,
      "learning_rate": 0.00012883534136546185,
      "loss": 0.31,
      "step": 448
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.060293689370155334,
      "learning_rate": 0.00012867469879518072,
      "loss": 0.3258,
      "step": 449
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.05818617716431618,
      "learning_rate": 0.0001285140562248996,
      "loss": 0.397,
      "step": 450
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.0640728622674942,
      "learning_rate": 0.00012835341365461848,
      "loss": 0.322,
      "step": 451
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.06293360888957977,
      "learning_rate": 0.00012819277108433737,
      "loss": 0.3168,
      "step": 452
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.062387946993112564,
      "learning_rate": 0.00012803212851405623,
      "loss": 0.3307,
      "step": 453
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.052799709141254425,
      "learning_rate": 0.00012787148594377512,
      "loss": 0.3528,
      "step": 454
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.07839447259902954,
      "learning_rate": 0.00012771084337349396,
      "loss": 0.417,
      "step": 455
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.06793881952762604,
      "learning_rate": 0.00012755020080321285,
      "loss": 0.3713,
      "step": 456
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.05840355157852173,
      "learning_rate": 0.00012738955823293172,
      "loss": 0.3543,
      "step": 457
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.04605015367269516,
      "learning_rate": 0.0001272289156626506,
      "loss": 0.248,
      "step": 458
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.06799069046974182,
      "learning_rate": 0.00012706827309236948,
      "loss": 0.3863,
      "step": 459
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.06314552575349808,
      "learning_rate": 0.00012690763052208837,
      "loss": 0.3199,
      "step": 460
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.08095140755176544,
      "learning_rate": 0.00012674698795180723,
      "loss": 0.4221,
      "step": 461
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.05364196002483368,
      "learning_rate": 0.00012658634538152612,
      "loss": 0.3802,
      "step": 462
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.07214777916669846,
      "learning_rate": 0.000126425702811245,
      "loss": 0.3586,
      "step": 463
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.08265020698308945,
      "learning_rate": 0.00012626506024096385,
      "loss": 0.4133,
      "step": 464
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.06965584307909012,
      "learning_rate": 0.00012610441767068272,
      "loss": 0.3595,
      "step": 465
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.06290312856435776,
      "learning_rate": 0.0001259437751004016,
      "loss": 0.341,
      "step": 466
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.05940334498882294,
      "learning_rate": 0.00012578313253012048,
      "loss": 0.2993,
      "step": 467
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.06343154609203339,
      "learning_rate": 0.00012562248995983937,
      "loss": 0.3568,
      "step": 468
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.06140105426311493,
      "learning_rate": 0.00012546184738955823,
      "loss": 0.2582,
      "step": 469
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.07051917165517807,
      "learning_rate": 0.00012530120481927712,
      "loss": 0.3678,
      "step": 470
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.0739557221531868,
      "learning_rate": 0.000125140562248996,
      "loss": 0.3865,
      "step": 471
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.0739377811551094,
      "learning_rate": 0.00012497991967871488,
      "loss": 0.3635,
      "step": 472
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.05477158725261688,
      "learning_rate": 0.00012481927710843375,
      "loss": 0.3516,
      "step": 473
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.05563298240303993,
      "learning_rate": 0.0001246586345381526,
      "loss": 0.2941,
      "step": 474
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.076485775411129,
      "learning_rate": 0.00012449799196787148,
      "loss": 0.406,
      "step": 475
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.07891671359539032,
      "learning_rate": 0.00012433734939759037,
      "loss": 0.3989,
      "step": 476
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.07034915685653687,
      "learning_rate": 0.00012417670682730923,
      "loss": 0.3909,
      "step": 477
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.078117236495018,
      "learning_rate": 0.00012401606425702812,
      "loss": 0.4012,
      "step": 478
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.07159332185983658,
      "learning_rate": 0.000123855421686747,
      "loss": 0.2831,
      "step": 479
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.06709907203912735,
      "learning_rate": 0.00012369477911646588,
      "loss": 0.364,
      "step": 480
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.073427215218544,
      "learning_rate": 0.00012353413654618475,
      "loss": 0.3225,
      "step": 481
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.0798756331205368,
      "learning_rate": 0.00012337349397590364,
      "loss": 0.4014,
      "step": 482
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.06695383787155151,
      "learning_rate": 0.00012321285140562248,
      "loss": 0.4046,
      "step": 483
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.04132775962352753,
      "learning_rate": 0.00012305220883534137,
      "loss": 0.2848,
      "step": 484
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.06046376749873161,
      "learning_rate": 0.00012289156626506023,
      "loss": 0.2746,
      "step": 485
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.0717458575963974,
      "learning_rate": 0.00012273092369477912,
      "loss": 0.3226,
      "step": 486
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.059205252677202225,
      "learning_rate": 0.000122570281124498,
      "loss": 0.3267,
      "step": 487
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.043504565954208374,
      "learning_rate": 0.00012240963855421688,
      "loss": 0.2729,
      "step": 488
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.06624353677034378,
      "learning_rate": 0.00012224899598393575,
      "loss": 0.4015,
      "step": 489
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.08172548562288284,
      "learning_rate": 0.00012208835341365464,
      "loss": 0.3318,
      "step": 490
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.059720031917095184,
      "learning_rate": 0.00012192771084337352,
      "loss": 0.3744,
      "step": 491
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.06723903864622116,
      "learning_rate": 0.00012176706827309237,
      "loss": 0.3807,
      "step": 492
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.05478193983435631,
      "learning_rate": 0.00012160642570281125,
      "loss": 0.3462,
      "step": 493
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.06529470533132553,
      "learning_rate": 0.00012144578313253012,
      "loss": 0.3408,
      "step": 494
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.06673943996429443,
      "learning_rate": 0.000121285140562249,
      "loss": 0.3542,
      "step": 495
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.05105610191822052,
      "learning_rate": 0.00012112449799196788,
      "loss": 0.3524,
      "step": 496
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.044642169028520584,
      "learning_rate": 0.00012096385542168676,
      "loss": 0.2489,
      "step": 497
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.05824999511241913,
      "learning_rate": 0.00012080321285140564,
      "loss": 0.3408,
      "step": 498
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.06392057240009308,
      "learning_rate": 0.00012064257028112452,
      "loss": 0.3512,
      "step": 499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.07307913154363632,
      "learning_rate": 0.0001204819277108434,
      "loss": 0.4072,
      "step": 500
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.055405471473932266,
      "learning_rate": 0.00012032128514056225,
      "loss": 0.3152,
      "step": 501
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.06649139523506165,
      "learning_rate": 0.00012016064257028112,
      "loss": 0.4284,
      "step": 502
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.059203796088695526,
      "learning_rate": 0.00012,
      "loss": 0.3336,
      "step": 503
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.06327962875366211,
      "learning_rate": 0.00011983935742971888,
      "loss": 0.4096,
      "step": 504
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.0759531706571579,
      "learning_rate": 0.00011967871485943776,
      "loss": 0.4174,
      "step": 505
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.05792231857776642,
      "learning_rate": 0.00011951807228915664,
      "loss": 0.3665,
      "step": 506
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.07508721202611923,
      "learning_rate": 0.00011935742971887552,
      "loss": 0.4285,
      "step": 507
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.06456132233142853,
      "learning_rate": 0.0001191967871485944,
      "loss": 0.3748,
      "step": 508
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.05225694924592972,
      "learning_rate": 0.00011903614457831327,
      "loss": 0.2816,
      "step": 509
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.05805736035108566,
      "learning_rate": 0.00011887550200803212,
      "loss": 0.3627,
      "step": 510
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.0608530156314373,
      "learning_rate": 0.000118714859437751,
      "loss": 0.2572,
      "step": 511
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.06565160304307938,
      "learning_rate": 0.00011855421686746988,
      "loss": 0.3707,
      "step": 512
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.057280633598566055,
      "learning_rate": 0.00011839357429718876,
      "loss": 0.3667,
      "step": 513
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.06733991950750351,
      "learning_rate": 0.00011823293172690764,
      "loss": 0.4012,
      "step": 514
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.055476900190114975,
      "learning_rate": 0.00011807228915662652,
      "loss": 0.2885,
      "step": 515
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.06762271374464035,
      "learning_rate": 0.0001179116465863454,
      "loss": 0.3587,
      "step": 516
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.06388411670923233,
      "learning_rate": 0.00011775100401606427,
      "loss": 0.3084,
      "step": 517
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.05875794216990471,
      "learning_rate": 0.00011759036144578315,
      "loss": 0.2787,
      "step": 518
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.06348701566457748,
      "learning_rate": 0.000117429718875502,
      "loss": 0.3835,
      "step": 519
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.06531812995672226,
      "learning_rate": 0.00011726907630522088,
      "loss": 0.3807,
      "step": 520
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.056963302195072174,
      "learning_rate": 0.00011710843373493976,
      "loss": 0.3458,
      "step": 521
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.04476078972220421,
      "learning_rate": 0.00011694779116465864,
      "loss": 0.2507,
      "step": 522
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.060027506202459335,
      "learning_rate": 0.00011678714859437752,
      "loss": 0.3396,
      "step": 523
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.0651395246386528,
      "learning_rate": 0.0001166265060240964,
      "loss": 0.3553,
      "step": 524
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.0519057959318161,
      "learning_rate": 0.00011646586345381527,
      "loss": 0.2662,
      "step": 525
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.06216929480433464,
      "learning_rate": 0.00011630522088353415,
      "loss": 0.3389,
      "step": 526
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.062046825885772705,
      "learning_rate": 0.00011614457831325303,
      "loss": 0.3127,
      "step": 527
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.06794798374176025,
      "learning_rate": 0.00011598393574297188,
      "loss": 0.3434,
      "step": 528
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.0629834234714508,
      "learning_rate": 0.00011582329317269076,
      "loss": 0.3714,
      "step": 529
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.060365013778209686,
      "learning_rate": 0.00011566265060240964,
      "loss": 0.3266,
      "step": 530
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.05298301950097084,
      "learning_rate": 0.00011550200803212852,
      "loss": 0.3362,
      "step": 531
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.0628928542137146,
      "learning_rate": 0.0001153413654618474,
      "loss": 0.3043,
      "step": 532
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.04337809979915619,
      "learning_rate": 0.00011518072289156627,
      "loss": 0.2272,
      "step": 533
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.05928153172135353,
      "learning_rate": 0.00011502008032128515,
      "loss": 0.3368,
      "step": 534
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.08310858905315399,
      "learning_rate": 0.00011485943775100403,
      "loss": 0.4477,
      "step": 535
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.07302271574735641,
      "learning_rate": 0.00011469879518072291,
      "loss": 0.352,
      "step": 536
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.06697707623243332,
      "learning_rate": 0.00011453815261044176,
      "loss": 0.3517,
      "step": 537
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.07374100387096405,
      "learning_rate": 0.00011437751004016064,
      "loss": 0.3433,
      "step": 538
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.06807583570480347,
      "learning_rate": 0.00011421686746987952,
      "loss": 0.3142,
      "step": 539
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.049346841871738434,
      "learning_rate": 0.0001140562248995984,
      "loss": 0.2837,
      "step": 540
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.10214679688215256,
      "learning_rate": 0.00011389558232931727,
      "loss": 0.4569,
      "step": 541
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.05804583430290222,
      "learning_rate": 0.00011373493975903615,
      "loss": 0.3059,
      "step": 542
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.06731875240802765,
      "learning_rate": 0.00011357429718875503,
      "loss": 0.386,
      "step": 543
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.09399156272411346,
      "learning_rate": 0.00011341365461847391,
      "loss": 0.3992,
      "step": 544
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.07207625359296799,
      "learning_rate": 0.00011325301204819279,
      "loss": 0.3433,
      "step": 545
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.055079150944948196,
      "learning_rate": 0.00011309236947791164,
      "loss": 0.2937,
      "step": 546
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.06304877996444702,
      "learning_rate": 0.00011293172690763052,
      "loss": 0.3029,
      "step": 547
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.07973883301019669,
      "learning_rate": 0.0001127710843373494,
      "loss": 0.3467,
      "step": 548
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.08874926716089249,
      "learning_rate": 0.00011261044176706827,
      "loss": 0.4199,
      "step": 549
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.051859576255083084,
      "learning_rate": 0.00011244979919678715,
      "loss": 0.3044,
      "step": 550
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.08072362840175629,
      "learning_rate": 0.00011228915662650603,
      "loss": 0.3845,
      "step": 551
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.08989651501178741,
      "learning_rate": 0.00011212851405622491,
      "loss": 0.4638,
      "step": 552
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.07602068036794662,
      "learning_rate": 0.00011196787148594379,
      "loss": 0.4062,
      "step": 553
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.0704541727900505,
      "learning_rate": 0.00011180722891566267,
      "loss": 0.3109,
      "step": 554
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.0643598809838295,
      "learning_rate": 0.00011164658634538152,
      "loss": 0.3394,
      "step": 555
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.06563718616962433,
      "learning_rate": 0.0001114859437751004,
      "loss": 0.3969,
      "step": 556
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.06879755854606628,
      "learning_rate": 0.00011132530120481927,
      "loss": 0.3652,
      "step": 557
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.055445555597543716,
      "learning_rate": 0.00011116465863453815,
      "loss": 0.2987,
      "step": 558
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.06609378755092621,
      "learning_rate": 0.00011100401606425703,
      "loss": 0.3754,
      "step": 559
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.061825674027204514,
      "learning_rate": 0.00011084337349397591,
      "loss": 0.3138,
      "step": 560
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.05663657560944557,
      "learning_rate": 0.00011068273092369479,
      "loss": 0.3377,
      "step": 561
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.06473535299301147,
      "learning_rate": 0.00011052208835341367,
      "loss": 0.3319,
      "step": 562
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.058231256902217865,
      "learning_rate": 0.00011036144578313254,
      "loss": 0.3586,
      "step": 563
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.07010842859745026,
      "learning_rate": 0.0001102008032128514,
      "loss": 0.3739,
      "step": 564
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.06500428169965744,
      "learning_rate": 0.00011004016064257027,
      "loss": 0.3561,
      "step": 565
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.07704146206378937,
      "learning_rate": 0.00010987951807228915,
      "loss": 0.3547,
      "step": 566
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.06808086484670639,
      "learning_rate": 0.00010971887550200803,
      "loss": 0.2892,
      "step": 567
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.06329892575740814,
      "learning_rate": 0.00010955823293172691,
      "loss": 0.36,
      "step": 568
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.07368778437376022,
      "learning_rate": 0.00010939759036144579,
      "loss": 0.3185,
      "step": 569
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.06914938241243362,
      "learning_rate": 0.00010923694779116467,
      "loss": 0.3212,
      "step": 570
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.07905329763889313,
      "learning_rate": 0.00010907630522088354,
      "loss": 0.3338,
      "step": 571
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.07625938206911087,
      "learning_rate": 0.00010891566265060242,
      "loss": 0.3825,
      "step": 572
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.048118483275175095,
      "learning_rate": 0.00010875502008032127,
      "loss": 0.2699,
      "step": 573
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.061509810388088226,
      "learning_rate": 0.00010859437751004015,
      "loss": 0.3248,
      "step": 574
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.06734441965818405,
      "learning_rate": 0.00010843373493975903,
      "loss": 0.3715,
      "step": 575
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.06595247983932495,
      "learning_rate": 0.00010827309236947791,
      "loss": 0.3299,
      "step": 576
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.06982835382223129,
      "learning_rate": 0.00010811244979919679,
      "loss": 0.3339,
      "step": 577
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.0674685388803482,
      "learning_rate": 0.00010795180722891567,
      "loss": 0.3931,
      "step": 578
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.07563046365976334,
      "learning_rate": 0.00010779116465863454,
      "loss": 0.3329,
      "step": 579
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.06894244998693466,
      "learning_rate": 0.00010763052208835342,
      "loss": 0.3745,
      "step": 580
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.07127521932125092,
      "learning_rate": 0.0001074698795180723,
      "loss": 0.3789,
      "step": 581
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.054480671882629395,
      "learning_rate": 0.00010730923694779118,
      "loss": 0.2815,
      "step": 582
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.07069193571805954,
      "learning_rate": 0.00010714859437751003,
      "loss": 0.294,
      "step": 583
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.060547493398189545,
      "learning_rate": 0.00010698795180722891,
      "loss": 0.3325,
      "step": 584
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.07301385700702667,
      "learning_rate": 0.00010682730923694779,
      "loss": 0.3671,
      "step": 585
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.058200154453516006,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.3499,
      "step": 586
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.05767529830336571,
      "learning_rate": 0.00010650602409638554,
      "loss": 0.3117,
      "step": 587
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.06692565977573395,
      "learning_rate": 0.00010634538152610442,
      "loss": 0.2841,
      "step": 588
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.05908389762043953,
      "learning_rate": 0.0001061847389558233,
      "loss": 0.3403,
      "step": 589
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.06080919876694679,
      "learning_rate": 0.00010602409638554218,
      "loss": 0.284,
      "step": 590
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.05906993895769119,
      "learning_rate": 0.00010586345381526106,
      "loss": 0.3419,
      "step": 591
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.06341567635536194,
      "learning_rate": 0.00010570281124497991,
      "loss": 0.3563,
      "step": 592
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.05414258688688278,
      "learning_rate": 0.00010554216867469879,
      "loss": 0.3125,
      "step": 593
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.05058576911687851,
      "learning_rate": 0.00010538152610441767,
      "loss": 0.2632,
      "step": 594
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.0635431557893753,
      "learning_rate": 0.00010522088353413654,
      "loss": 0.3128,
      "step": 595
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.05724791809916496,
      "learning_rate": 0.00010506024096385542,
      "loss": 0.3195,
      "step": 596
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.05529351904988289,
      "learning_rate": 0.0001048995983935743,
      "loss": 0.2951,
      "step": 597
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.07328370213508606,
      "learning_rate": 0.00010473895582329318,
      "loss": 0.4148,
      "step": 598
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.05996965616941452,
      "learning_rate": 0.00010457831325301206,
      "loss": 0.3687,
      "step": 599
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.06598532944917679,
      "learning_rate": 0.00010441767068273094,
      "loss": 0.3319,
      "step": 600
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.07335791736841202,
      "learning_rate": 0.0001042570281124498,
      "loss": 0.414,
      "step": 601
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.05746404826641083,
      "learning_rate": 0.00010409638554216867,
      "loss": 0.3841,
      "step": 602
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.05675050988793373,
      "learning_rate": 0.00010393574297188754,
      "loss": 0.2982,
      "step": 603
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.07604406028985977,
      "learning_rate": 0.00010377510040160642,
      "loss": 0.3936,
      "step": 604
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.06502636522054672,
      "learning_rate": 0.0001036144578313253,
      "loss": 0.3988,
      "step": 605
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.06589922308921814,
      "learning_rate": 0.00010345381526104418,
      "loss": 0.3865,
      "step": 606
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.06779200583696365,
      "learning_rate": 0.00010329317269076306,
      "loss": 0.3784,
      "step": 607
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.06073305755853653,
      "learning_rate": 0.00010313253012048194,
      "loss": 0.2842,
      "step": 608
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.05451224744319916,
      "learning_rate": 0.00010297188755020082,
      "loss": 0.3457,
      "step": 609
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.06519901007413864,
      "learning_rate": 0.00010281124497991968,
      "loss": 0.334,
      "step": 610
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.06272966414690018,
      "learning_rate": 0.00010265060240963856,
      "loss": 0.3298,
      "step": 611
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.05813700333237648,
      "learning_rate": 0.00010248995983935742,
      "loss": 0.2917,
      "step": 612
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.05543652921915054,
      "learning_rate": 0.0001023293172690763,
      "loss": 0.3084,
      "step": 613
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.07068846374750137,
      "learning_rate": 0.00010216867469879518,
      "loss": 0.3501,
      "step": 614
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.07762624323368073,
      "learning_rate": 0.00010200803212851406,
      "loss": 0.3736,
      "step": 615
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.052042409777641296,
      "learning_rate": 0.00010184738955823294,
      "loss": 0.2914,
      "step": 616
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.050495922565460205,
      "learning_rate": 0.00010168674698795182,
      "loss": 0.2996,
      "step": 617
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.05474431440234184,
      "learning_rate": 0.0001015261044176707,
      "loss": 0.3102,
      "step": 618
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.050446365028619766,
      "learning_rate": 0.00010136546184738956,
      "loss": 0.302,
      "step": 619
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.05460984259843826,
      "learning_rate": 0.00010120481927710844,
      "loss": 0.2515,
      "step": 620
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.06691516935825348,
      "learning_rate": 0.00010104417670682732,
      "loss": 0.3644,
      "step": 621
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.04104073718190193,
      "learning_rate": 0.0001008835341365462,
      "loss": 0.255,
      "step": 622
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.06972356885671616,
      "learning_rate": 0.00010072289156626506,
      "loss": 0.2978,
      "step": 623
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.06232043728232384,
      "learning_rate": 0.00010056224899598394,
      "loss": 0.3762,
      "step": 624
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.06501064449548721,
      "learning_rate": 0.00010040160642570282,
      "loss": 0.3653,
      "step": 625
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.07496889680624008,
      "learning_rate": 0.0001002409638554217,
      "loss": 0.3997,
      "step": 626
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.05388135835528374,
      "learning_rate": 0.00010008032128514057,
      "loss": 0.334,
      "step": 627
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.07505439221858978,
      "learning_rate": 9.991967871485944e-05,
      "loss": 0.382,
      "step": 628
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.05444902926683426,
      "learning_rate": 9.975903614457832e-05,
      "loss": 0.3391,
      "step": 629
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.061701174825429916,
      "learning_rate": 9.95983935742972e-05,
      "loss": 0.293,
      "step": 630
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.0580899752676487,
      "learning_rate": 9.943775100401607e-05,
      "loss": 0.3518,
      "step": 631
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.06712877750396729,
      "learning_rate": 9.927710843373495e-05,
      "loss": 0.3401,
      "step": 632
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.0747351422905922,
      "learning_rate": 9.911646586345382e-05,
      "loss": 0.3542,
      "step": 633
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.054940078407526016,
      "learning_rate": 9.89558232931727e-05,
      "loss": 0.2927,
      "step": 634
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.06732618063688278,
      "learning_rate": 9.879518072289157e-05,
      "loss": 0.3216,
      "step": 635
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.0647784024477005,
      "learning_rate": 9.863453815261045e-05,
      "loss": 0.3372,
      "step": 636
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.06565595418214798,
      "learning_rate": 9.847389558232933e-05,
      "loss": 0.3967,
      "step": 637
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.07509265094995499,
      "learning_rate": 9.831325301204821e-05,
      "loss": 0.3699,
      "step": 638
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.0734381154179573,
      "learning_rate": 9.815261044176707e-05,
      "loss": 0.3928,
      "step": 639
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.08888192474842072,
      "learning_rate": 9.799196787148595e-05,
      "loss": 0.3329,
      "step": 640
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.06879717856645584,
      "learning_rate": 9.783132530120483e-05,
      "loss": 0.3843,
      "step": 641
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.04971776530146599,
      "learning_rate": 9.767068273092371e-05,
      "loss": 0.2313,
      "step": 642
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.088130883872509,
      "learning_rate": 9.751004016064259e-05,
      "loss": 0.3728,
      "step": 643
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.06959286332130432,
      "learning_rate": 9.734939759036145e-05,
      "loss": 0.3003,
      "step": 644
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.05551495403051376,
      "learning_rate": 9.718875502008033e-05,
      "loss": 0.2726,
      "step": 645
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.06217409297823906,
      "learning_rate": 9.702811244979921e-05,
      "loss": 0.3727,
      "step": 646
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.061373092234134674,
      "learning_rate": 9.686746987951809e-05,
      "loss": 0.3388,
      "step": 647
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.0696963220834732,
      "learning_rate": 9.670682730923695e-05,
      "loss": 0.3501,
      "step": 648
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.07098784297704697,
      "learning_rate": 9.654618473895583e-05,
      "loss": 0.3515,
      "step": 649
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.08324383199214935,
      "learning_rate": 9.638554216867471e-05,
      "loss": 0.396,
      "step": 650
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.07100134342908859,
      "learning_rate": 9.622489959839359e-05,
      "loss": 0.322,
      "step": 651
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.062463440001010895,
      "learning_rate": 9.606425702811246e-05,
      "loss": 0.3157,
      "step": 652
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.07759964466094971,
      "learning_rate": 9.590361445783133e-05,
      "loss": 0.3951,
      "step": 653
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.05050428956747055,
      "learning_rate": 9.574297188755021e-05,
      "loss": 0.2981,
      "step": 654
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.059949375689029694,
      "learning_rate": 9.558232931726909e-05,
      "loss": 0.359,
      "step": 655
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.07678517699241638,
      "learning_rate": 9.542168674698796e-05,
      "loss": 0.3625,
      "step": 656
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.0681803971529007,
      "learning_rate": 9.526104417670684e-05,
      "loss": 0.321,
      "step": 657
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.07053149491548538,
      "learning_rate": 9.510040160642571e-05,
      "loss": 0.356,
      "step": 658
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.04623858630657196,
      "learning_rate": 9.493975903614459e-05,
      "loss": 0.3022,
      "step": 659
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.06396803259849548,
      "learning_rate": 9.477911646586346e-05,
      "loss": 0.3593,
      "step": 660
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.06764748692512512,
      "learning_rate": 9.461847389558234e-05,
      "loss": 0.3776,
      "step": 661
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.06690395623445511,
      "learning_rate": 9.445783132530121e-05,
      "loss": 0.3308,
      "step": 662
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.07635217905044556,
      "learning_rate": 9.429718875502009e-05,
      "loss": 0.4256,
      "step": 663
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.06111736595630646,
      "learning_rate": 9.413654618473896e-05,
      "loss": 0.3168,
      "step": 664
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.06100204586982727,
      "learning_rate": 9.397590361445784e-05,
      "loss": 0.2841,
      "step": 665
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.059569984674453735,
      "learning_rate": 9.381526104417672e-05,
      "loss": 0.3256,
      "step": 666
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.058353327214717865,
      "learning_rate": 9.365461847389559e-05,
      "loss": 0.3652,
      "step": 667
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.06474019587039948,
      "learning_rate": 9.349397590361446e-05,
      "loss": 0.3764,
      "step": 668
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.060683105140924454,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.3659,
      "step": 669
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.08275843411684036,
      "learning_rate": 9.317269076305222e-05,
      "loss": 0.3836,
      "step": 670
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.0530012808740139,
      "learning_rate": 9.301204819277109e-05,
      "loss": 0.3137,
      "step": 671
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.07025498151779175,
      "learning_rate": 9.285140562248996e-05,
      "loss": 0.4003,
      "step": 672
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.07092162221670151,
      "learning_rate": 9.269076305220884e-05,
      "loss": 0.315,
      "step": 673
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.06941451132297516,
      "learning_rate": 9.253012048192772e-05,
      "loss": 0.2913,
      "step": 674
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.06042703613638878,
      "learning_rate": 9.23694779116466e-05,
      "loss": 0.339,
      "step": 675
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.06751547753810883,
      "learning_rate": 9.220883534136546e-05,
      "loss": 0.3317,
      "step": 676
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.05197446793317795,
      "learning_rate": 9.204819277108434e-05,
      "loss": 0.3504,
      "step": 677
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.05260242894291878,
      "learning_rate": 9.188755020080322e-05,
      "loss": 0.2957,
      "step": 678
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.04754674434661865,
      "learning_rate": 9.17269076305221e-05,
      "loss": 0.2624,
      "step": 679
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.05637321621179581,
      "learning_rate": 9.156626506024096e-05,
      "loss": 0.2964,
      "step": 680
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.05382164567708969,
      "learning_rate": 9.140562248995984e-05,
      "loss": 0.303,
      "step": 681
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.06555651873350143,
      "learning_rate": 9.124497991967872e-05,
      "loss": 0.3711,
      "step": 682
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.05630161985754967,
      "learning_rate": 9.10843373493976e-05,
      "loss": 0.3141,
      "step": 683
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.04758686572313309,
      "learning_rate": 9.092369477911648e-05,
      "loss": 0.3211,
      "step": 684
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.06941699236631393,
      "learning_rate": 9.076305220883534e-05,
      "loss": 0.4181,
      "step": 685
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.07677900046110153,
      "learning_rate": 9.060240963855422e-05,
      "loss": 0.411,
      "step": 686
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.0701543316245079,
      "learning_rate": 9.04417670682731e-05,
      "loss": 0.427,
      "step": 687
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.07042483985424042,
      "learning_rate": 9.028112449799198e-05,
      "loss": 0.365,
      "step": 688
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.0624089390039444,
      "learning_rate": 9.012048192771084e-05,
      "loss": 0.3248,
      "step": 689
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.07342611253261566,
      "learning_rate": 8.995983935742972e-05,
      "loss": 0.3961,
      "step": 690
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.067500539124012,
      "learning_rate": 8.97991967871486e-05,
      "loss": 0.3088,
      "step": 691
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.06204976513981819,
      "learning_rate": 8.963855421686748e-05,
      "loss": 0.3296,
      "step": 692
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.06406392902135849,
      "learning_rate": 8.947791164658636e-05,
      "loss": 0.3554,
      "step": 693
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.06824392825365067,
      "learning_rate": 8.931726907630522e-05,
      "loss": 0.4331,
      "step": 694
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.06836428493261337,
      "learning_rate": 8.91566265060241e-05,
      "loss": 0.3993,
      "step": 695
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.06958330422639847,
      "learning_rate": 8.899598393574298e-05,
      "loss": 0.4124,
      "step": 696
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.07536298036575317,
      "learning_rate": 8.883534136546186e-05,
      "loss": 0.3375,
      "step": 697
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.06698978692293167,
      "learning_rate": 8.867469879518072e-05,
      "loss": 0.3148,
      "step": 698
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.05479678139090538,
      "learning_rate": 8.85140562248996e-05,
      "loss": 0.2946,
      "step": 699
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.05650778114795685,
      "learning_rate": 8.835341365461848e-05,
      "loss": 0.3324,
      "step": 700
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.06264760345220566,
      "learning_rate": 8.819277108433736e-05,
      "loss": 0.3516,
      "step": 701
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.04631378501653671,
      "learning_rate": 8.803212851405624e-05,
      "loss": 0.3074,
      "step": 702
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.07527635246515274,
      "learning_rate": 8.78714859437751e-05,
      "loss": 0.3999,
      "step": 703
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.06622155755758286,
      "learning_rate": 8.771084337349398e-05,
      "loss": 0.3852,
      "step": 704
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.06164142116904259,
      "learning_rate": 8.755020080321286e-05,
      "loss": 0.3561,
      "step": 705
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.06995919346809387,
      "learning_rate": 8.738955823293174e-05,
      "loss": 0.3334,
      "step": 706
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.06369038671255112,
      "learning_rate": 8.722891566265061e-05,
      "loss": 0.3553,
      "step": 707
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.06237335130572319,
      "learning_rate": 8.706827309236948e-05,
      "loss": 0.3636,
      "step": 708
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.059221796691417694,
      "learning_rate": 8.690763052208836e-05,
      "loss": 0.3427,
      "step": 709
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.05940323323011398,
      "learning_rate": 8.674698795180724e-05,
      "loss": 0.3578,
      "step": 710
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.0697200670838356,
      "learning_rate": 8.658634538152611e-05,
      "loss": 0.415,
      "step": 711
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.04892057925462723,
      "learning_rate": 8.642570281124498e-05,
      "loss": 0.2844,
      "step": 712
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.06754235923290253,
      "learning_rate": 8.626506024096386e-05,
      "loss": 0.3717,
      "step": 713
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.07916872948408127,
      "learning_rate": 8.610441767068274e-05,
      "loss": 0.3271,
      "step": 714
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.055646251887083054,
      "learning_rate": 8.594377510040161e-05,
      "loss": 0.371,
      "step": 715
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.06692821532487869,
      "learning_rate": 8.578313253012049e-05,
      "loss": 0.3782,
      "step": 716
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.07036356627941132,
      "learning_rate": 8.562248995983936e-05,
      "loss": 0.4307,
      "step": 717
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.052394840866327286,
      "learning_rate": 8.546184738955824e-05,
      "loss": 0.3459,
      "step": 718
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.0548989363014698,
      "learning_rate": 8.530120481927711e-05,
      "loss": 0.3456,
      "step": 719
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.06297802180051804,
      "learning_rate": 8.514056224899599e-05,
      "loss": 0.3475,
      "step": 720
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.06020796671509743,
      "learning_rate": 8.497991967871486e-05,
      "loss": 0.3492,
      "step": 721
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.07616507261991501,
      "learning_rate": 8.481927710843374e-05,
      "loss": 0.3988,
      "step": 722
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.0653141662478447,
      "learning_rate": 8.465863453815261e-05,
      "loss": 0.3425,
      "step": 723
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.05484617128968239,
      "learning_rate": 8.449799196787149e-05,
      "loss": 0.3076,
      "step": 724
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.07821284979581833,
      "learning_rate": 8.433734939759037e-05,
      "loss": 0.3934,
      "step": 725
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.06438212841749191,
      "learning_rate": 8.417670682730924e-05,
      "loss": 0.3965,
      "step": 726
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.06164877489209175,
      "learning_rate": 8.401606425702811e-05,
      "loss": 0.3135,
      "step": 727
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.07923221588134766,
      "learning_rate": 8.385542168674699e-05,
      "loss": 0.4168,
      "step": 728
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.05760684236884117,
      "learning_rate": 8.369477911646587e-05,
      "loss": 0.2959,
      "step": 729
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.06142643094062805,
      "learning_rate": 8.353413654618474e-05,
      "loss": 0.2242,
      "step": 730
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.0661158412694931,
      "learning_rate": 8.337349397590361e-05,
      "loss": 0.2715,
      "step": 731
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.05614747107028961,
      "learning_rate": 8.321285140562249e-05,
      "loss": 0.299,
      "step": 732
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.0581224262714386,
      "learning_rate": 8.305220883534137e-05,
      "loss": 0.3285,
      "step": 733
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.0792611762881279,
      "learning_rate": 8.289156626506025e-05,
      "loss": 0.3715,
      "step": 734
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.06930011510848999,
      "learning_rate": 8.273092369477911e-05,
      "loss": 0.3351,
      "step": 735
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.051409684121608734,
      "learning_rate": 8.257028112449799e-05,
      "loss": 0.2843,
      "step": 736
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.05640774965286255,
      "learning_rate": 8.240963855421687e-05,
      "loss": 0.3319,
      "step": 737
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.06869612634181976,
      "learning_rate": 8.224899598393575e-05,
      "loss": 0.3738,
      "step": 738
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.06058045104146004,
      "learning_rate": 8.208835341365461e-05,
      "loss": 0.3164,
      "step": 739
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.05693518742918968,
      "learning_rate": 8.192771084337349e-05,
      "loss": 0.3008,
      "step": 740
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.07786459475755692,
      "learning_rate": 8.176706827309237e-05,
      "loss": 0.3987,
      "step": 741
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.04226836934685707,
      "learning_rate": 8.160642570281125e-05,
      "loss": 0.2837,
      "step": 742
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.0608837865293026,
      "learning_rate": 8.144578313253013e-05,
      "loss": 0.3666,
      "step": 743
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.05701594054698944,
      "learning_rate": 8.128514056224899e-05,
      "loss": 0.3085,
      "step": 744
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.056706078350543976,
      "learning_rate": 8.112449799196787e-05,
      "loss": 0.3634,
      "step": 745
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.05391532555222511,
      "learning_rate": 8.096385542168675e-05,
      "loss": 0.3253,
      "step": 746
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.06930788606405258,
      "learning_rate": 8.080321285140563e-05,
      "loss": 0.3847,
      "step": 747
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.05402973294258118,
      "learning_rate": 8.064257028112449e-05,
      "loss": 0.2224,
      "step": 748
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.04367247596383095,
      "learning_rate": 8.048192771084337e-05,
      "loss": 0.2493,
      "step": 749
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.05629059299826622,
      "learning_rate": 8.032128514056225e-05,
      "loss": 0.3259,
      "step": 750
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.06913460046052933,
      "learning_rate": 8.016064257028113e-05,
      "loss": 0.325,
      "step": 751
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.09685324132442474,
      "learning_rate": 8e-05,
      "loss": 0.412,
      "step": 752
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.058769021183252335,
      "learning_rate": 7.983935742971887e-05,
      "loss": 0.3166,
      "step": 753
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.06336981803178787,
      "learning_rate": 7.967871485943775e-05,
      "loss": 0.3478,
      "step": 754
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.07857558876276016,
      "learning_rate": 7.951807228915663e-05,
      "loss": 0.3866,
      "step": 755
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.07662692666053772,
      "learning_rate": 7.93574297188755e-05,
      "loss": 0.2993,
      "step": 756
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.08136207610368729,
      "learning_rate": 7.919678714859437e-05,
      "loss": 0.3674,
      "step": 757
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.05897491052746773,
      "learning_rate": 7.903614457831325e-05,
      "loss": 0.3512,
      "step": 758
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.06769218295812607,
      "learning_rate": 7.887550200803213e-05,
      "loss": 0.3354,
      "step": 759
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.09639851003885269,
      "learning_rate": 7.8714859437751e-05,
      "loss": 0.3951,
      "step": 760
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.06762901693582535,
      "learning_rate": 7.855421686746989e-05,
      "loss": 0.3276,
      "step": 761
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.07127787917852402,
      "learning_rate": 7.839357429718875e-05,
      "loss": 0.338,
      "step": 762
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.06763633340597153,
      "learning_rate": 7.823293172690763e-05,
      "loss": 0.3922,
      "step": 763
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.06606677919626236,
      "learning_rate": 7.80722891566265e-05,
      "loss": 0.3433,
      "step": 764
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.05927329882979393,
      "learning_rate": 7.791164658634539e-05,
      "loss": 0.3154,
      "step": 765
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.10767359286546707,
      "learning_rate": 7.775100401606426e-05,
      "loss": 0.3461,
      "step": 766
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.06738024204969406,
      "learning_rate": 7.759036144578313e-05,
      "loss": 0.3466,
      "step": 767
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.07033147662878036,
      "learning_rate": 7.7429718875502e-05,
      "loss": 0.3234,
      "step": 768
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.06247854232788086,
      "learning_rate": 7.726907630522089e-05,
      "loss": 0.2867,
      "step": 769
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.06394550949335098,
      "learning_rate": 7.710843373493976e-05,
      "loss": 0.3052,
      "step": 770
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.0793142020702362,
      "learning_rate": 7.694779116465863e-05,
      "loss": 0.3461,
      "step": 771
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.0626949667930603,
      "learning_rate": 7.678714859437751e-05,
      "loss": 0.316,
      "step": 772
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.06731603294610977,
      "learning_rate": 7.662650602409639e-05,
      "loss": 0.3225,
      "step": 773
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.0630626380443573,
      "learning_rate": 7.646586345381526e-05,
      "loss": 0.3059,
      "step": 774
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.06702890247106552,
      "learning_rate": 7.630522088353414e-05,
      "loss": 0.3234,
      "step": 775
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.06852971017360687,
      "learning_rate": 7.614457831325301e-05,
      "loss": 0.3075,
      "step": 776
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.070630744099617,
      "learning_rate": 7.598393574297189e-05,
      "loss": 0.3223,
      "step": 777
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.07377469539642334,
      "learning_rate": 7.582329317269076e-05,
      "loss": 0.3399,
      "step": 778
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.05884328484535217,
      "learning_rate": 7.566265060240964e-05,
      "loss": 0.2909,
      "step": 779
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.060448747128248215,
      "learning_rate": 7.550200803212851e-05,
      "loss": 0.3194,
      "step": 780
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.07086186110973358,
      "learning_rate": 7.534136546184739e-05,
      "loss": 0.36,
      "step": 781
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.0797935351729393,
      "learning_rate": 7.518072289156626e-05,
      "loss": 0.362,
      "step": 782
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.08758911490440369,
      "learning_rate": 7.502008032128514e-05,
      "loss": 0.3911,
      "step": 783
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.06299483776092529,
      "learning_rate": 7.485943775100402e-05,
      "loss": 0.3245,
      "step": 784
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.07413064688444138,
      "learning_rate": 7.469879518072289e-05,
      "loss": 0.3643,
      "step": 785
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.05972236767411232,
      "learning_rate": 7.453815261044176e-05,
      "loss": 0.3083,
      "step": 786
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.08040235191583633,
      "learning_rate": 7.437751004016064e-05,
      "loss": 0.3668,
      "step": 787
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.08502376079559326,
      "learning_rate": 7.421686746987952e-05,
      "loss": 0.302,
      "step": 788
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.06079428642988205,
      "learning_rate": 7.405622489959839e-05,
      "loss": 0.3174,
      "step": 789
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.06185637414455414,
      "learning_rate": 7.389558232931726e-05,
      "loss": 0.3144,
      "step": 790
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.0818169042468071,
      "learning_rate": 7.373493975903614e-05,
      "loss": 0.4196,
      "step": 791
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.06853940337896347,
      "learning_rate": 7.357429718875502e-05,
      "loss": 0.3459,
      "step": 792
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.07001756876707077,
      "learning_rate": 7.34136546184739e-05,
      "loss": 0.397,
      "step": 793
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.061555106192827225,
      "learning_rate": 7.325301204819278e-05,
      "loss": 0.3117,
      "step": 794
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.06159190461039543,
      "learning_rate": 7.309236947791164e-05,
      "loss": 0.3129,
      "step": 795
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.06964801996946335,
      "learning_rate": 7.293172690763052e-05,
      "loss": 0.3088,
      "step": 796
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.08794831484556198,
      "learning_rate": 7.27710843373494e-05,
      "loss": 0.4381,
      "step": 797
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.06179235130548477,
      "learning_rate": 7.261044176706828e-05,
      "loss": 0.3074,
      "step": 798
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.06699154525995255,
      "learning_rate": 7.244979919678716e-05,
      "loss": 0.3156,
      "step": 799
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.05596529692411423,
      "learning_rate": 7.228915662650602e-05,
      "loss": 0.3188,
      "step": 800
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.06793969869613647,
      "learning_rate": 7.21285140562249e-05,
      "loss": 0.3415,
      "step": 801
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.052193399518728256,
      "learning_rate": 7.196787148594378e-05,
      "loss": 0.2731,
      "step": 802
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.06743379682302475,
      "learning_rate": 7.180722891566266e-05,
      "loss": 0.3453,
      "step": 803
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.0719335526227951,
      "learning_rate": 7.164658634538153e-05,
      "loss": 0.3492,
      "step": 804
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.05035535991191864,
      "learning_rate": 7.14859437751004e-05,
      "loss": 0.2675,
      "step": 805
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.09651588648557663,
      "learning_rate": 7.132530120481928e-05,
      "loss": 0.4286,
      "step": 806
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.07611177861690521,
      "learning_rate": 7.116465863453816e-05,
      "loss": 0.3222,
      "step": 807
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.05719692260026932,
      "learning_rate": 7.100401606425703e-05,
      "loss": 0.2242,
      "step": 808
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.05904106795787811,
      "learning_rate": 7.084337349397591e-05,
      "loss": 0.3345,
      "step": 809
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.05010358244180679,
      "learning_rate": 7.068273092369478e-05,
      "loss": 0.2688,
      "step": 810
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.06359312683343887,
      "learning_rate": 7.052208835341366e-05,
      "loss": 0.3305,
      "step": 811
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.06419109553098679,
      "learning_rate": 7.036144578313253e-05,
      "loss": 0.3375,
      "step": 812
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.060938913375139236,
      "learning_rate": 7.020080321285141e-05,
      "loss": 0.3781,
      "step": 813
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.05595514178276062,
      "learning_rate": 7.004016064257029e-05,
      "loss": 0.332,
      "step": 814
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.0642617717385292,
      "learning_rate": 6.987951807228917e-05,
      "loss": 0.3926,
      "step": 815
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.07238379120826721,
      "learning_rate": 6.971887550200803e-05,
      "loss": 0.3487,
      "step": 816
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.06234511360526085,
      "learning_rate": 6.955823293172691e-05,
      "loss": 0.3656,
      "step": 817
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.09680210053920746,
      "learning_rate": 6.939759036144579e-05,
      "loss": 0.4089,
      "step": 818
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.0693131685256958,
      "learning_rate": 6.923694779116467e-05,
      "loss": 0.348,
      "step": 819
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.06078977510333061,
      "learning_rate": 6.907630522088355e-05,
      "loss": 0.3712,
      "step": 820
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.06521117687225342,
      "learning_rate": 6.891566265060241e-05,
      "loss": 0.3575,
      "step": 821
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.08043354004621506,
      "learning_rate": 6.875502008032129e-05,
      "loss": 0.3844,
      "step": 822
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.06076139584183693,
      "learning_rate": 6.859437751004017e-05,
      "loss": 0.3619,
      "step": 823
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.06166011467576027,
      "learning_rate": 6.843373493975905e-05,
      "loss": 0.3237,
      "step": 824
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.06582176685333252,
      "learning_rate": 6.827309236947793e-05,
      "loss": 0.3791,
      "step": 825
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.05226768180727959,
      "learning_rate": 6.811244979919679e-05,
      "loss": 0.2579,
      "step": 826
    },
    {
      "epoch": 3.308,
      "grad_norm": 0.05718173831701279,
      "learning_rate": 6.795180722891567e-05,
      "loss": 0.2933,
      "step": 827
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.06015141308307648,
      "learning_rate": 6.779116465863455e-05,
      "loss": 0.3207,
      "step": 828
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.06119931489229202,
      "learning_rate": 6.763052208835343e-05,
      "loss": 0.3267,
      "step": 829
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.062433578073978424,
      "learning_rate": 6.746987951807229e-05,
      "loss": 0.3539,
      "step": 830
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.05884864181280136,
      "learning_rate": 6.730923694779117e-05,
      "loss": 0.3224,
      "step": 831
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.03820882365107536,
      "learning_rate": 6.714859437751005e-05,
      "loss": 0.2517,
      "step": 832
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.061039358377456665,
      "learning_rate": 6.698795180722893e-05,
      "loss": 0.3079,
      "step": 833
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.07376720756292343,
      "learning_rate": 6.68273092369478e-05,
      "loss": 0.3459,
      "step": 834
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.0742548480629921,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.3665,
      "step": 835
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.06704956293106079,
      "learning_rate": 6.650602409638555e-05,
      "loss": 0.3403,
      "step": 836
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.07260894775390625,
      "learning_rate": 6.634538152610443e-05,
      "loss": 0.3836,
      "step": 837
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.06285098940134048,
      "learning_rate": 6.61847389558233e-05,
      "loss": 0.3278,
      "step": 838
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.0621136873960495,
      "learning_rate": 6.602409638554217e-05,
      "loss": 0.3044,
      "step": 839
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.0731024518609047,
      "learning_rate": 6.586345381526105e-05,
      "loss": 0.3289,
      "step": 840
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.05381637439131737,
      "learning_rate": 6.570281124497993e-05,
      "loss": 0.3167,
      "step": 841
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.06778277456760406,
      "learning_rate": 6.55421686746988e-05,
      "loss": 0.3227,
      "step": 842
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.05645051226019859,
      "learning_rate": 6.538152610441768e-05,
      "loss": 0.3458,
      "step": 843
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.07776034623384476,
      "learning_rate": 6.522088353413655e-05,
      "loss": 0.3224,
      "step": 844
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.06675401329994202,
      "learning_rate": 6.506024096385543e-05,
      "loss": 0.3405,
      "step": 845
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.06670773774385452,
      "learning_rate": 6.48995983935743e-05,
      "loss": 0.3536,
      "step": 846
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.06397136300802231,
      "learning_rate": 6.473895582329318e-05,
      "loss": 0.3487,
      "step": 847
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.06806420534849167,
      "learning_rate": 6.457831325301205e-05,
      "loss": 0.3528,
      "step": 848
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.07221612334251404,
      "learning_rate": 6.441767068273093e-05,
      "loss": 0.3237,
      "step": 849
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.058261267840862274,
      "learning_rate": 6.42570281124498e-05,
      "loss": 0.3263,
      "step": 850
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.058360081166028976,
      "learning_rate": 6.409638554216868e-05,
      "loss": 0.3279,
      "step": 851
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.05699371546506882,
      "learning_rate": 6.393574297188756e-05,
      "loss": 0.3426,
      "step": 852
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.05163797363638878,
      "learning_rate": 6.377510040160643e-05,
      "loss": 0.3005,
      "step": 853
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.07014431059360504,
      "learning_rate": 6.36144578313253e-05,
      "loss": 0.3627,
      "step": 854
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.060335516929626465,
      "learning_rate": 6.345381526104418e-05,
      "loss": 0.3856,
      "step": 855
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.08026950806379318,
      "learning_rate": 6.329317269076306e-05,
      "loss": 0.4107,
      "step": 856
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.07349849492311478,
      "learning_rate": 6.313253012048193e-05,
      "loss": 0.3465,
      "step": 857
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.06333954632282257,
      "learning_rate": 6.29718875502008e-05,
      "loss": 0.3557,
      "step": 858
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.05129498243331909,
      "learning_rate": 6.281124497991968e-05,
      "loss": 0.2538,
      "step": 859
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.0566449873149395,
      "learning_rate": 6.265060240963856e-05,
      "loss": 0.3034,
      "step": 860
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.06684695929288864,
      "learning_rate": 6.248995983935744e-05,
      "loss": 0.3498,
      "step": 861
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.05534299090504646,
      "learning_rate": 6.23293172690763e-05,
      "loss": 0.2901,
      "step": 862
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.06711357831954956,
      "learning_rate": 6.216867469879518e-05,
      "loss": 0.3661,
      "step": 863
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.060379836708307266,
      "learning_rate": 6.200803212851406e-05,
      "loss": 0.3115,
      "step": 864
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.07028962671756744,
      "learning_rate": 6.184738955823294e-05,
      "loss": 0.3282,
      "step": 865
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.06967539340257645,
      "learning_rate": 6.168674698795182e-05,
      "loss": 0.3892,
      "step": 866
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.06262396275997162,
      "learning_rate": 6.152610441767068e-05,
      "loss": 0.3699,
      "step": 867
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.06312043964862823,
      "learning_rate": 6.136546184738956e-05,
      "loss": 0.3483,
      "step": 868
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.06583817303180695,
      "learning_rate": 6.120481927710844e-05,
      "loss": 0.3508,
      "step": 869
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.05885249376296997,
      "learning_rate": 6.104417670682732e-05,
      "loss": 0.3506,
      "step": 870
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.06734023243188858,
      "learning_rate": 6.0883534136546184e-05,
      "loss": 0.3579,
      "step": 871
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.04721849039196968,
      "learning_rate": 6.072289156626506e-05,
      "loss": 0.31,
      "step": 872
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.07011149823665619,
      "learning_rate": 6.056224899598394e-05,
      "loss": 0.3604,
      "step": 873
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.0631944015622139,
      "learning_rate": 6.040160642570282e-05,
      "loss": 0.3677,
      "step": 874
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.05438712611794472,
      "learning_rate": 6.02409638554217e-05,
      "loss": 0.2788,
      "step": 875
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.044133737683296204,
      "learning_rate": 6.008032128514056e-05,
      "loss": 0.2845,
      "step": 876
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.059960611164569855,
      "learning_rate": 5.991967871485944e-05,
      "loss": 0.3133,
      "step": 877
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.05373760312795639,
      "learning_rate": 5.975903614457832e-05,
      "loss": 0.3306,
      "step": 878
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.06495556980371475,
      "learning_rate": 5.95983935742972e-05,
      "loss": 0.3843,
      "step": 879
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.08227512240409851,
      "learning_rate": 5.943775100401606e-05,
      "loss": 0.4089,
      "step": 880
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.05174765735864639,
      "learning_rate": 5.927710843373494e-05,
      "loss": 0.2823,
      "step": 881
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.057233598083257675,
      "learning_rate": 5.911646586345382e-05,
      "loss": 0.3324,
      "step": 882
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.06746894866228104,
      "learning_rate": 5.89558232931727e-05,
      "loss": 0.4,
      "step": 883
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.06282807886600494,
      "learning_rate": 5.8795180722891576e-05,
      "loss": 0.3544,
      "step": 884
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.05396772921085358,
      "learning_rate": 5.863453815261044e-05,
      "loss": 0.2981,
      "step": 885
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.057435423135757446,
      "learning_rate": 5.847389558232932e-05,
      "loss": 0.3237,
      "step": 886
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.05106205493211746,
      "learning_rate": 5.83132530120482e-05,
      "loss": 0.3197,
      "step": 887
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.06008034944534302,
      "learning_rate": 5.8152610441767076e-05,
      "loss": 0.3153,
      "step": 888
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.05237945541739464,
      "learning_rate": 5.799196787148594e-05,
      "loss": 0.3147,
      "step": 889
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.07601819932460785,
      "learning_rate": 5.783132530120482e-05,
      "loss": 0.3709,
      "step": 890
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.05103881284594536,
      "learning_rate": 5.76706827309237e-05,
      "loss": 0.3167,
      "step": 891
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.07412531226873398,
      "learning_rate": 5.7510040160642576e-05,
      "loss": 0.3841,
      "step": 892
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.07783621549606323,
      "learning_rate": 5.7349397590361454e-05,
      "loss": 0.327,
      "step": 893
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.05853015184402466,
      "learning_rate": 5.718875502008032e-05,
      "loss": 0.2904,
      "step": 894
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.06916490942239761,
      "learning_rate": 5.70281124497992e-05,
      "loss": 0.3565,
      "step": 895
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.053890060633420944,
      "learning_rate": 5.6867469879518076e-05,
      "loss": 0.3409,
      "step": 896
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.07679902017116547,
      "learning_rate": 5.6706827309236955e-05,
      "loss": 0.3485,
      "step": 897
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.07165731489658356,
      "learning_rate": 5.654618473895582e-05,
      "loss": 0.3456,
      "step": 898
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.07552235573530197,
      "learning_rate": 5.63855421686747e-05,
      "loss": 0.334,
      "step": 899
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.07578155398368835,
      "learning_rate": 5.6224899598393576e-05,
      "loss": 0.3333,
      "step": 900
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.06934592127799988,
      "learning_rate": 5.6064257028112455e-05,
      "loss": 0.3105,
      "step": 901
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.05572669953107834,
      "learning_rate": 5.590361445783133e-05,
      "loss": 0.2887,
      "step": 902
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.05328349396586418,
      "learning_rate": 5.57429718875502e-05,
      "loss": 0.2676,
      "step": 903
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.0644388422369957,
      "learning_rate": 5.5582329317269076e-05,
      "loss": 0.3148,
      "step": 904
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.06936785578727722,
      "learning_rate": 5.5421686746987955e-05,
      "loss": 0.3414,
      "step": 905
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.07503733038902283,
      "learning_rate": 5.526104417670683e-05,
      "loss": 0.3923,
      "step": 906
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.08331728726625443,
      "learning_rate": 5.51004016064257e-05,
      "loss": 0.4431,
      "step": 907
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.065740667283535,
      "learning_rate": 5.4939759036144576e-05,
      "loss": 0.3911,
      "step": 908
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.06147792935371399,
      "learning_rate": 5.4779116465863455e-05,
      "loss": 0.2752,
      "step": 909
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.06978560239076614,
      "learning_rate": 5.461847389558233e-05,
      "loss": 0.3295,
      "step": 910
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.06735303997993469,
      "learning_rate": 5.445783132530121e-05,
      "loss": 0.3009,
      "step": 911
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.06828147172927856,
      "learning_rate": 5.4297188755020076e-05,
      "loss": 0.3013,
      "step": 912
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.0634736493229866,
      "learning_rate": 5.4136546184738955e-05,
      "loss": 0.3127,
      "step": 913
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.07089237868785858,
      "learning_rate": 5.397590361445783e-05,
      "loss": 0.3122,
      "step": 914
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.06627516448497772,
      "learning_rate": 5.381526104417671e-05,
      "loss": 0.3108,
      "step": 915
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.07153508812189102,
      "learning_rate": 5.365461847389559e-05,
      "loss": 0.3648,
      "step": 916
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.05430467426776886,
      "learning_rate": 5.3493975903614455e-05,
      "loss": 0.2598,
      "step": 917
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.08301027864217758,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.4209,
      "step": 918
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.076593779027462,
      "learning_rate": 5.317269076305221e-05,
      "loss": 0.3588,
      "step": 919
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.06062394753098488,
      "learning_rate": 5.301204819277109e-05,
      "loss": 0.3424,
      "step": 920
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.0689755529165268,
      "learning_rate": 5.2851405622489955e-05,
      "loss": 0.3315,
      "step": 921
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.079436294734478,
      "learning_rate": 5.269076305220883e-05,
      "loss": 0.3275,
      "step": 922
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.07812658697366714,
      "learning_rate": 5.253012048192771e-05,
      "loss": 0.3644,
      "step": 923
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.058558546006679535,
      "learning_rate": 5.236947791164659e-05,
      "loss": 0.3116,
      "step": 924
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.0711672455072403,
      "learning_rate": 5.220883534136547e-05,
      "loss": 0.373,
      "step": 925
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.052782755345106125,
      "learning_rate": 5.204819277108433e-05,
      "loss": 0.3105,
      "step": 926
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.059886958450078964,
      "learning_rate": 5.188755020080321e-05,
      "loss": 0.3365,
      "step": 927
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.05507194995880127,
      "learning_rate": 5.172690763052209e-05,
      "loss": 0.276,
      "step": 928
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.0792672336101532,
      "learning_rate": 5.156626506024097e-05,
      "loss": 0.3649,
      "step": 929
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.06948582828044891,
      "learning_rate": 5.140562248995984e-05,
      "loss": 0.3447,
      "step": 930
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.05947618559002876,
      "learning_rate": 5.124497991967871e-05,
      "loss": 0.3414,
      "step": 931
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.07751172035932541,
      "learning_rate": 5.108433734939759e-05,
      "loss": 0.4159,
      "step": 932
    },
    {
      "epoch": 3.732,
      "grad_norm": 0.0861482098698616,
      "learning_rate": 5.092369477911647e-05,
      "loss": 0.3619,
      "step": 933
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.06493233144283295,
      "learning_rate": 5.076305220883535e-05,
      "loss": 0.3125,
      "step": 934
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.07322734594345093,
      "learning_rate": 5.060240963855422e-05,
      "loss": 0.3484,
      "step": 935
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.046704184263944626,
      "learning_rate": 5.04417670682731e-05,
      "loss": 0.255,
      "step": 936
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.0818551555275917,
      "learning_rate": 5.028112449799197e-05,
      "loss": 0.3699,
      "step": 937
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.05762128904461861,
      "learning_rate": 5.012048192771085e-05,
      "loss": 0.2649,
      "step": 938
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.06269076466560364,
      "learning_rate": 4.995983935742972e-05,
      "loss": 0.3335,
      "step": 939
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.06280229240655899,
      "learning_rate": 4.97991967871486e-05,
      "loss": 0.3341,
      "step": 940
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.0791536495089531,
      "learning_rate": 4.9638554216867475e-05,
      "loss": 0.3875,
      "step": 941
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.06359503418207169,
      "learning_rate": 4.947791164658635e-05,
      "loss": 0.3112,
      "step": 942
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.06605634838342667,
      "learning_rate": 4.9317269076305225e-05,
      "loss": 0.2986,
      "step": 943
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.07439744472503662,
      "learning_rate": 4.9156626506024104e-05,
      "loss": 0.3417,
      "step": 944
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.06638965755701065,
      "learning_rate": 4.8995983935742975e-05,
      "loss": 0.3459,
      "step": 945
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.05974597483873367,
      "learning_rate": 4.8835341365461854e-05,
      "loss": 0.3064,
      "step": 946
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.05864253267645836,
      "learning_rate": 4.8674698795180725e-05,
      "loss": 0.3442,
      "step": 947
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.07494492828845978,
      "learning_rate": 4.8514056224899604e-05,
      "loss": 0.3576,
      "step": 948
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.07719811797142029,
      "learning_rate": 4.8353413654618476e-05,
      "loss": 0.3658,
      "step": 949
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.059547703713178635,
      "learning_rate": 4.8192771084337354e-05,
      "loss": 0.33,
      "step": 950
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.0643593892455101,
      "learning_rate": 4.803212851405623e-05,
      "loss": 0.3054,
      "step": 951
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.053952284157276154,
      "learning_rate": 4.7871485943775104e-05,
      "loss": 0.3229,
      "step": 952
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 0.05609554424881935,
      "learning_rate": 4.771084337349398e-05,
      "loss": 0.2999,
      "step": 953
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.06336549669504166,
      "learning_rate": 4.7550200803212854e-05,
      "loss": 0.3297,
      "step": 954
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.060110412538051605,
      "learning_rate": 4.738955823293173e-05,
      "loss": 0.3258,
      "step": 955
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.10128485411405563,
      "learning_rate": 4.7228915662650604e-05,
      "loss": 0.4365,
      "step": 956
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.055415909737348557,
      "learning_rate": 4.706827309236948e-05,
      "loss": 0.2764,
      "step": 957
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.06552392989397049,
      "learning_rate": 4.690763052208836e-05,
      "loss": 0.3495,
      "step": 958
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.051926400512456894,
      "learning_rate": 4.674698795180723e-05,
      "loss": 0.2908,
      "step": 959
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.0885678082704544,
      "learning_rate": 4.658634538152611e-05,
      "loss": 0.4186,
      "step": 960
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.054709941148757935,
      "learning_rate": 4.642570281124498e-05,
      "loss": 0.2492,
      "step": 961
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.09979026764631271,
      "learning_rate": 4.626506024096386e-05,
      "loss": 0.3792,
      "step": 962
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.0620107427239418,
      "learning_rate": 4.610441767068273e-05,
      "loss": 0.3338,
      "step": 963
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.05976016819477081,
      "learning_rate": 4.594377510040161e-05,
      "loss": 0.3034,
      "step": 964
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.05392027646303177,
      "learning_rate": 4.578313253012048e-05,
      "loss": 0.2942,
      "step": 965
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.07095538079738617,
      "learning_rate": 4.562248995983936e-05,
      "loss": 0.3216,
      "step": 966
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.07403124868869781,
      "learning_rate": 4.546184738955824e-05,
      "loss": 0.3534,
      "step": 967
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.10078319162130356,
      "learning_rate": 4.530120481927711e-05,
      "loss": 0.4105,
      "step": 968
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.06834717094898224,
      "learning_rate": 4.514056224899599e-05,
      "loss": 0.3532,
      "step": 969
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.06663398444652557,
      "learning_rate": 4.497991967871486e-05,
      "loss": 0.3742,
      "step": 970
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.061885908246040344,
      "learning_rate": 4.481927710843374e-05,
      "loss": 0.3564,
      "step": 971
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.07040361315011978,
      "learning_rate": 4.465863453815261e-05,
      "loss": 0.3663,
      "step": 972
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.06400886178016663,
      "learning_rate": 4.449799196787149e-05,
      "loss": 0.3203,
      "step": 973
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.09045376628637314,
      "learning_rate": 4.433734939759036e-05,
      "loss": 0.3546,
      "step": 974
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.07340817898511887,
      "learning_rate": 4.417670682730924e-05,
      "loss": 0.3551,
      "step": 975
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.07468769699335098,
      "learning_rate": 4.401606425702812e-05,
      "loss": 0.3794,
      "step": 976
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.07087695598602295,
      "learning_rate": 4.385542168674699e-05,
      "loss": 0.3744,
      "step": 977
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.06122931092977524,
      "learning_rate": 4.369477911646587e-05,
      "loss": 0.3409,
      "step": 978
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.08882322162389755,
      "learning_rate": 4.353413654618474e-05,
      "loss": 0.4122,
      "step": 979
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.06084328144788742,
      "learning_rate": 4.337349397590362e-05,
      "loss": 0.3169,
      "step": 980
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.06039370968937874,
      "learning_rate": 4.321285140562249e-05,
      "loss": 0.3358,
      "step": 981
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.058218322694301605,
      "learning_rate": 4.305220883534137e-05,
      "loss": 0.296,
      "step": 982
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.04372991994023323,
      "learning_rate": 4.2891566265060246e-05,
      "loss": 0.2244,
      "step": 983
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.04712040722370148,
      "learning_rate": 4.273092369477912e-05,
      "loss": 0.2649,
      "step": 984
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.07850801944732666,
      "learning_rate": 4.2570281124497996e-05,
      "loss": 0.3852,
      "step": 985
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.07005621492862701,
      "learning_rate": 4.240963855421687e-05,
      "loss": 0.359,
      "step": 986
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.05998857691884041,
      "learning_rate": 4.2248995983935746e-05,
      "loss": 0.3494,
      "step": 987
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.08518651127815247,
      "learning_rate": 4.208835341365462e-05,
      "loss": 0.402,
      "step": 988
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.04897886514663696,
      "learning_rate": 4.1927710843373496e-05,
      "loss": 0.309,
      "step": 989
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.05251602083444595,
      "learning_rate": 4.176706827309237e-05,
      "loss": 0.2413,
      "step": 990
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.08505392074584961,
      "learning_rate": 4.1606425702811246e-05,
      "loss": 0.4225,
      "step": 991
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.06644245982170105,
      "learning_rate": 4.1445783132530125e-05,
      "loss": 0.3368,
      "step": 992
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.0680888444185257,
      "learning_rate": 4.1285140562248996e-05,
      "loss": 0.3743,
      "step": 993
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.09354901313781738,
      "learning_rate": 4.1124497991967875e-05,
      "loss": 0.3844,
      "step": 994
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.05107380077242851,
      "learning_rate": 4.0963855421686746e-05,
      "loss": 0.2765,
      "step": 995
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.07434029132127762,
      "learning_rate": 4.0803212851405625e-05,
      "loss": 0.3946,
      "step": 996
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.05985929071903229,
      "learning_rate": 4.0642570281124496e-05,
      "loss": 0.3593,
      "step": 997
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.07385115325450897,
      "learning_rate": 4.0481927710843375e-05,
      "loss": 0.3106,
      "step": 998
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.06520047038793564,
      "learning_rate": 4.0321285140562246e-05,
      "loss": 0.3321,
      "step": 999
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.07027820497751236,
      "learning_rate": 4.0160642570281125e-05,
      "loss": 0.346,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.7910915336466596e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
