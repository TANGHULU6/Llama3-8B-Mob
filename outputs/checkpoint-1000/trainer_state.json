{
  "best_metric": 0.3220631778240204,
  "best_model_checkpoint": "outputs/checkpoint-1000",
  "epoch": 1.6666666666666665,
  "eval_steps": 100,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.0955612659454346,
      "learning_rate": 4e-05,
      "loss": 1.9448,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.291522264480591,
      "learning_rate": 8e-05,
      "loss": 1.8955,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.3751931190490723,
      "learning_rate": 0.00012,
      "loss": 1.9633,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.9432048797607422,
      "learning_rate": 0.00016,
      "loss": 1.7703,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4780776500701904,
      "learning_rate": 0.0002,
      "loss": 1.5581,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.2533655166625977,
      "learning_rate": 0.00019988857938718665,
      "loss": 1.3527,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.636291265487671,
      "learning_rate": 0.00019977715877437326,
      "loss": 1.0732,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.666510820388794,
      "learning_rate": 0.0001996657381615599,
      "loss": 1.2106,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.6698962450027466,
      "learning_rate": 0.00019955431754874653,
      "loss": 0.8514,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 2.109370708465576,
      "learning_rate": 0.00019944289693593316,
      "loss": 0.9609,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.8895906209945679,
      "learning_rate": 0.0001993314763231198,
      "loss": 0.6892,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8486143946647644,
      "learning_rate": 0.0001992200557103064,
      "loss": 0.4873,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.9126704931259155,
      "learning_rate": 0.00019910863509749305,
      "loss": 0.5162,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.47595322132110596,
      "learning_rate": 0.00019899721448467968,
      "loss": 0.6428,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.41951069235801697,
      "learning_rate": 0.0001988857938718663,
      "loss": 0.4553,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.49427658319473267,
      "learning_rate": 0.00019877437325905293,
      "loss": 0.6551,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.2967330813407898,
      "learning_rate": 0.00019866295264623957,
      "loss": 0.4602,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.2054484486579895,
      "learning_rate": 0.0001985515320334262,
      "loss": 0.4395,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.28449466824531555,
      "learning_rate": 0.00019844011142061284,
      "loss": 0.4793,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.34066563844680786,
      "learning_rate": 0.00019832869080779945,
      "loss": 0.5514,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2806191146373749,
      "learning_rate": 0.00019821727019498608,
      "loss": 0.5207,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.1904837042093277,
      "learning_rate": 0.00019810584958217272,
      "loss": 0.4974,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.16037176549434662,
      "learning_rate": 0.00019799442896935933,
      "loss": 0.393,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.163355752825737,
      "learning_rate": 0.00019788300835654597,
      "loss": 0.4312,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.1961064487695694,
      "learning_rate": 0.00019777158774373258,
      "loss": 0.4412,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.19962693750858307,
      "learning_rate": 0.00019766016713091924,
      "loss": 0.5202,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.1513756513595581,
      "learning_rate": 0.00019754874651810588,
      "loss": 0.4442,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.1553286612033844,
      "learning_rate": 0.00019743732590529249,
      "loss": 0.4703,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.12882000207901,
      "learning_rate": 0.00019732590529247912,
      "loss": 0.3949,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15362459421157837,
      "learning_rate": 0.00019721448467966573,
      "loss": 0.4067,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.16636455059051514,
      "learning_rate": 0.00019710306406685237,
      "loss": 0.4751,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10700325667858124,
      "learning_rate": 0.000196991643454039,
      "loss": 0.4901,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.10109906643629074,
      "learning_rate": 0.00019688022284122561,
      "loss": 0.2806,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.15828914940357208,
      "learning_rate": 0.00019676880222841228,
      "loss": 0.4163,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.1429501175880432,
      "learning_rate": 0.00019665738161559891,
      "loss": 0.3946,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11291559785604477,
      "learning_rate": 0.00019654596100278552,
      "loss": 0.4201,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.11685312539339066,
      "learning_rate": 0.00019643454038997216,
      "loss": 0.3055,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.11478576809167862,
      "learning_rate": 0.00019632311977715877,
      "loss": 0.3478,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.11220988631248474,
      "learning_rate": 0.0001962116991643454,
      "loss": 0.3454,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.10664703696966171,
      "learning_rate": 0.00019610027855153204,
      "loss": 0.3251,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.10532573610544205,
      "learning_rate": 0.00019598885793871865,
      "loss": 0.3798,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.09253589063882828,
      "learning_rate": 0.00019587743732590532,
      "loss": 0.3477,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.10197603702545166,
      "learning_rate": 0.00019576601671309192,
      "loss": 0.3672,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09146757423877716,
      "learning_rate": 0.00019565459610027856,
      "loss": 0.4197,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12180428951978683,
      "learning_rate": 0.0001955431754874652,
      "loss": 0.4203,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.11242137849330902,
      "learning_rate": 0.0001954317548746518,
      "loss": 0.3167,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.0971837043762207,
      "learning_rate": 0.00019532033426183844,
      "loss": 0.3995,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08444882929325104,
      "learning_rate": 0.00019520891364902508,
      "loss": 0.2908,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.09193655103445053,
      "learning_rate": 0.0001950974930362117,
      "loss": 0.3851,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.10707516968250275,
      "learning_rate": 0.00019498607242339835,
      "loss": 0.3579,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.08961743116378784,
      "learning_rate": 0.00019487465181058496,
      "loss": 0.3716,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.09596701711416245,
      "learning_rate": 0.0001947632311977716,
      "loss": 0.415,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.06666703522205353,
      "learning_rate": 0.00019465181058495824,
      "loss": 0.3567,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.11535973846912384,
      "learning_rate": 0.00019454038997214484,
      "loss": 0.4295,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.08375324308872223,
      "learning_rate": 0.00019442896935933148,
      "loss": 0.3019,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.10852738469839096,
      "learning_rate": 0.00019431754874651812,
      "loss": 0.3493,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.10233605653047562,
      "learning_rate": 0.00019420612813370473,
      "loss": 0.3831,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.13586613535881042,
      "learning_rate": 0.0001940947075208914,
      "loss": 0.4087,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.07681457698345184,
      "learning_rate": 0.000193983286908078,
      "loss": 0.3024,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.09336651116609573,
      "learning_rate": 0.00019387186629526464,
      "loss": 0.3808,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.06770402193069458,
      "learning_rate": 0.00019376044568245127,
      "loss": 0.2627,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.07679001986980438,
      "learning_rate": 0.00019364902506963788,
      "loss": 0.3938,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06754700839519501,
      "learning_rate": 0.00019353760445682452,
      "loss": 0.3716,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07708554714918137,
      "learning_rate": 0.00019342618384401116,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.08859172463417053,
      "learning_rate": 0.00019331476323119776,
      "loss": 0.3443,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.10104084014892578,
      "learning_rate": 0.00019320334261838443,
      "loss": 0.3372,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.07533049583435059,
      "learning_rate": 0.00019309192200557104,
      "loss": 0.3046,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.08598867803812027,
      "learning_rate": 0.00019298050139275767,
      "loss": 0.2944,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.12070752680301666,
      "learning_rate": 0.0001928690807799443,
      "loss": 0.3952,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.10526693612337112,
      "learning_rate": 0.00019275766016713092,
      "loss": 0.379,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.0892651155591011,
      "learning_rate": 0.00019264623955431756,
      "loss": 0.3193,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07309211045503616,
      "learning_rate": 0.0001925348189415042,
      "loss": 0.3395,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.06044447422027588,
      "learning_rate": 0.0001924233983286908,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.16050706803798676,
      "learning_rate": 0.00019231197771587747,
      "loss": 0.3868,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.07688120752573013,
      "learning_rate": 0.00019220055710306408,
      "loss": 0.325,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17312116920948029,
      "learning_rate": 0.0001920891364902507,
      "loss": 0.3973,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.09058317542076111,
      "learning_rate": 0.00019197771587743735,
      "loss": 0.3466,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.08493395149707794,
      "learning_rate": 0.00019186629526462396,
      "loss": 0.3251,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.11874042451381683,
      "learning_rate": 0.0001917548746518106,
      "loss": 0.3859,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08727477490901947,
      "learning_rate": 0.0001916434540389972,
      "loss": 0.3329,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.08828488737344742,
      "learning_rate": 0.00019153203342618384,
      "loss": 0.3498,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.0929422378540039,
      "learning_rate": 0.0001914206128133705,
      "loss": 0.3477,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.06502547860145569,
      "learning_rate": 0.0001913091922005571,
      "loss": 0.2619,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0826447606086731,
      "learning_rate": 0.00019119777158774375,
      "loss": 0.375,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.06894782930612564,
      "learning_rate": 0.00019108635097493039,
      "loss": 0.384,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.08264435827732086,
      "learning_rate": 0.000190974930362117,
      "loss": 0.3475,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.08043172955513,
      "learning_rate": 0.00019086350974930363,
      "loss": 0.2602,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08756858110427856,
      "learning_rate": 0.00019075208913649024,
      "loss": 0.3371,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.07516686618328094,
      "learning_rate": 0.00019064066852367688,
      "loss": 0.3206,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06845497339963913,
      "learning_rate": 0.00019052924791086354,
      "loss": 0.2438,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.16434063017368317,
      "learning_rate": 0.00019041782729805015,
      "loss": 0.3756,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08764452487230301,
      "learning_rate": 0.0001903064066852368,
      "loss": 0.364,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.09765373170375824,
      "learning_rate": 0.0001901949860724234,
      "loss": 0.336,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.12203431129455566,
      "learning_rate": 0.00019008356545961003,
      "loss": 0.3644,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.0851789191365242,
      "learning_rate": 0.00018997214484679667,
      "loss": 0.3209,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.0814284235239029,
      "learning_rate": 0.00018986072423398328,
      "loss": 0.3538,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.10511325299739838,
      "learning_rate": 0.00018974930362116992,
      "loss": 0.3208,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.06572487205266953,
      "learning_rate": 0.00018963788300835655,
      "loss": 0.3373,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.060700900852680206,
      "learning_rate": 0.0001895264623955432,
      "loss": 0.2413,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.09838373959064484,
      "learning_rate": 0.00018941504178272982,
      "loss": 0.4219,
      "step": 100
    },
    {
      "epoch": 0.16666666666666666,
      "eval_loss": 0.3487149178981781,
      "eval_runtime": 374.2756,
      "eval_samples_per_second": 0.267,
      "eval_steps_per_second": 0.267,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.07030830532312393,
      "learning_rate": 0.00018930362116991643,
      "loss": 0.2833,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.0894852876663208,
      "learning_rate": 0.00018919220055710307,
      "loss": 0.3532,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.08684878051280975,
      "learning_rate": 0.0001890807799442897,
      "loss": 0.346,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.06322134286165237,
      "learning_rate": 0.00018896935933147632,
      "loss": 0.2794,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.11456792056560516,
      "learning_rate": 0.00018885793871866295,
      "loss": 0.4254,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.08284913748502731,
      "learning_rate": 0.0001887465181058496,
      "loss": 0.3428,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.08064185827970505,
      "learning_rate": 0.00018863509749303623,
      "loss": 0.4691,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.07856238633394241,
      "learning_rate": 0.00018852367688022286,
      "loss": 0.402,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.11941109597682953,
      "learning_rate": 0.00018841225626740947,
      "loss": 0.3084,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.059156522154808044,
      "learning_rate": 0.0001883008356545961,
      "loss": 0.3936,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06889190524816513,
      "learning_rate": 0.00018818941504178274,
      "loss": 0.3664,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06391744315624237,
      "learning_rate": 0.00018807799442896935,
      "loss": 0.3049,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.07021020352840424,
      "learning_rate": 0.000187966573816156,
      "loss": 0.379,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.13152983784675598,
      "learning_rate": 0.00018785515320334263,
      "loss": 0.4045,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08291923254728317,
      "learning_rate": 0.00018774373259052926,
      "loss": 0.3675,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.12651987373828888,
      "learning_rate": 0.0001876323119777159,
      "loss": 0.3512,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.06338682025671005,
      "learning_rate": 0.0001875208913649025,
      "loss": 0.3052,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.06231988966464996,
      "learning_rate": 0.00018740947075208915,
      "loss": 0.3496,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.092839315533638,
      "learning_rate": 0.00018729805013927578,
      "loss": 0.3263,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05712943151593208,
      "learning_rate": 0.0001871866295264624,
      "loss": 0.2907,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.07494724541902542,
      "learning_rate": 0.00018707520891364903,
      "loss": 0.3746,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06764266639947891,
      "learning_rate": 0.00018696378830083566,
      "loss": 0.34,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05663382261991501,
      "learning_rate": 0.0001868523676880223,
      "loss": 0.2878,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06405160576105118,
      "learning_rate": 0.00018674094707520894,
      "loss": 0.3435,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.07559706270694733,
      "learning_rate": 0.00018662952646239555,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.07337447255849838,
      "learning_rate": 0.00018651810584958218,
      "loss": 0.351,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.07447608560323715,
      "learning_rate": 0.00018640668523676882,
      "loss": 0.3073,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06198855862021446,
      "learning_rate": 0.00018629526462395543,
      "loss": 0.2524,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.07246571034193039,
      "learning_rate": 0.00018618384401114207,
      "loss": 0.3869,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.09226162731647491,
      "learning_rate": 0.0001860724233983287,
      "loss": 0.4094,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09503162652254105,
      "learning_rate": 0.00018596100278551534,
      "loss": 0.4443,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07748464494943619,
      "learning_rate": 0.00018584958217270198,
      "loss": 0.3511,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.05017251893877983,
      "learning_rate": 0.00018573816155988858,
      "loss": 0.295,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05881548672914505,
      "learning_rate": 0.00018562674094707522,
      "loss": 0.3032,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.08307559788227081,
      "learning_rate": 0.00018551532033426183,
      "loss": 0.3153,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05294783040881157,
      "learning_rate": 0.00018540389972144847,
      "loss": 0.299,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.08568192273378372,
      "learning_rate": 0.0001852924791086351,
      "loss": 0.3543,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.056592926383018494,
      "learning_rate": 0.00018518105849582174,
      "loss": 0.375,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.07708677649497986,
      "learning_rate": 0.00018506963788300838,
      "loss": 0.3557,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.08158115297555923,
      "learning_rate": 0.000184958217270195,
      "loss": 0.2725,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.0755927711725235,
      "learning_rate": 0.00018484679665738162,
      "loss": 0.3757,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.05458508059382439,
      "learning_rate": 0.00018473537604456826,
      "loss": 0.2738,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.06590596586465836,
      "learning_rate": 0.00018462395543175487,
      "loss": 0.3337,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.08819787949323654,
      "learning_rate": 0.0001845125348189415,
      "loss": 0.3993,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.07023905217647552,
      "learning_rate": 0.00018440111420612814,
      "loss": 0.3453,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.08446215838193893,
      "learning_rate": 0.00018428969359331478,
      "loss": 0.3801,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.08325417339801788,
      "learning_rate": 0.00018417827298050141,
      "loss": 0.3376,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06384475529193878,
      "learning_rate": 0.00018406685236768802,
      "loss": 0.3087,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.09863974153995514,
      "learning_rate": 0.00018395543175487466,
      "loss": 0.3617,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.07659132778644562,
      "learning_rate": 0.0001838440111420613,
      "loss": 0.3664,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.07176845520734787,
      "learning_rate": 0.0001837325905292479,
      "loss": 0.3111,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06705939024686813,
      "learning_rate": 0.00018362116991643454,
      "loss": 0.2932,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09360035508871078,
      "learning_rate": 0.00018350974930362118,
      "loss": 0.4418,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08361534029245377,
      "learning_rate": 0.00018339832869080782,
      "loss": 0.361,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.07738277316093445,
      "learning_rate": 0.00018328690807799445,
      "loss": 0.3252,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06970240920782089,
      "learning_rate": 0.00018317548746518106,
      "loss": 0.3677,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.07010390609502792,
      "learning_rate": 0.0001830640668523677,
      "loss": 0.296,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.05854480341076851,
      "learning_rate": 0.00018295264623955433,
      "loss": 0.3228,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05452617257833481,
      "learning_rate": 0.00018284122562674094,
      "loss": 0.256,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.08169269561767578,
      "learning_rate": 0.00018272980501392758,
      "loss": 0.3414,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.06454888731241226,
      "learning_rate": 0.00018261838440111422,
      "loss": 0.3587,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.07409106940031052,
      "learning_rate": 0.00018250696378830085,
      "loss": 0.2825,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.058084022253751755,
      "learning_rate": 0.0001823955431754875,
      "loss": 0.2171,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.08975499123334885,
      "learning_rate": 0.0001822841225626741,
      "loss": 0.365,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.07376623153686523,
      "learning_rate": 0.00018217270194986074,
      "loss": 0.3336,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.06804781407117844,
      "learning_rate": 0.00018206128133704737,
      "loss": 0.3163,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.06362677365541458,
      "learning_rate": 0.00018194986072423398,
      "loss": 0.317,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06292019784450531,
      "learning_rate": 0.00018183844011142062,
      "loss": 0.2978,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.0647486001253128,
      "learning_rate": 0.00018172701949860725,
      "loss": 0.27,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.059582650661468506,
      "learning_rate": 0.00018161559888579386,
      "loss": 0.344,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.06418173015117645,
      "learning_rate": 0.00018150417827298053,
      "loss": 0.3143,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05272924154996872,
      "learning_rate": 0.00018139275766016714,
      "loss": 0.282,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.05326687917113304,
      "learning_rate": 0.00018128133704735377,
      "loss": 0.2529,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.08164666593074799,
      "learning_rate": 0.0001811699164345404,
      "loss": 0.3672,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.08568466454744339,
      "learning_rate": 0.00018105849582172702,
      "loss": 0.3995,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.06509491801261902,
      "learning_rate": 0.00018094707520891366,
      "loss": 0.2809,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.06712925434112549,
      "learning_rate": 0.0001808356545961003,
      "loss": 0.2367,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.10027410835027695,
      "learning_rate": 0.0001807242339832869,
      "loss": 0.3808,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.07324101775884628,
      "learning_rate": 0.00018061281337047356,
      "loss": 0.3234,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10830274969339371,
      "learning_rate": 0.00018050139275766017,
      "loss": 0.4434,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.08461984992027283,
      "learning_rate": 0.0001803899721448468,
      "loss": 0.3819,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06849800795316696,
      "learning_rate": 0.00018027855153203345,
      "loss": 0.3983,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.07148388773202896,
      "learning_rate": 0.00018016713091922006,
      "loss": 0.2975,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.06068180501461029,
      "learning_rate": 0.0001800557103064067,
      "loss": 0.3016,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.051959868520498276,
      "learning_rate": 0.0001799442896935933,
      "loss": 0.2419,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.07422163337469101,
      "learning_rate": 0.00017983286908077994,
      "loss": 0.2948,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.07162340730428696,
      "learning_rate": 0.0001797214484679666,
      "loss": 0.3059,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.06812291592359543,
      "learning_rate": 0.0001796100278551532,
      "loss": 0.3171,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.07255540788173676,
      "learning_rate": 0.00017949860724233985,
      "loss": 0.3323,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.0663914754986763,
      "learning_rate": 0.00017938718662952648,
      "loss": 0.2966,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.03991664573550224,
      "learning_rate": 0.0001792757660167131,
      "loss": 0.251,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05767596513032913,
      "learning_rate": 0.00017916434540389973,
      "loss": 0.3607,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.05921991914510727,
      "learning_rate": 0.00017905292479108634,
      "loss": 0.3541,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.05089111626148224,
      "learning_rate": 0.00017894150417827298,
      "loss": 0.2928,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.05411629378795624,
      "learning_rate": 0.00017883008356545964,
      "loss": 0.3113,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.08924172818660736,
      "learning_rate": 0.00017871866295264625,
      "loss": 0.3164,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.06599845737218857,
      "learning_rate": 0.00017860724233983289,
      "loss": 0.3165,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.06638164818286896,
      "learning_rate": 0.0001784958217270195,
      "loss": 0.39,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06859147548675537,
      "learning_rate": 0.00017838440111420613,
      "loss": 0.3435,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05851481482386589,
      "learning_rate": 0.00017827298050139277,
      "loss": 0.2616,
      "step": 200
    },
    {
      "epoch": 0.3333333333333333,
      "eval_loss": 0.3388613164424896,
      "eval_runtime": 373.5218,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.06528528034687042,
      "learning_rate": 0.00017816155988857938,
      "loss": 0.3377,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.07327250391244888,
      "learning_rate": 0.00017805013927576601,
      "loss": 0.3787,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.0866728201508522,
      "learning_rate": 0.00017793871866295265,
      "loss": 0.3318,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.08070702105760574,
      "learning_rate": 0.0001778272980501393,
      "loss": 0.272,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.07358337938785553,
      "learning_rate": 0.00017771587743732592,
      "loss": 0.2816,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.11622414737939835,
      "learning_rate": 0.00017760445682451253,
      "loss": 0.3935,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.07202191650867462,
      "learning_rate": 0.00017749303621169917,
      "loss": 0.3054,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.06922760605812073,
      "learning_rate": 0.0001773816155988858,
      "loss": 0.2836,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.040258318185806274,
      "learning_rate": 0.00017727019498607242,
      "loss": 0.251,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06730931252241135,
      "learning_rate": 0.00017715877437325905,
      "loss": 0.3381,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.08930881321430206,
      "learning_rate": 0.0001770473537604457,
      "loss": 0.3098,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.08481532335281372,
      "learning_rate": 0.00017693593314763232,
      "loss": 0.3761,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.059320684522390366,
      "learning_rate": 0.00017682451253481896,
      "loss": 0.2541,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06912222504615784,
      "learning_rate": 0.00017671309192200557,
      "loss": 0.2664,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.08226578682661057,
      "learning_rate": 0.0001766016713091922,
      "loss": 0.3138,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.04609953239560127,
      "learning_rate": 0.00017649025069637884,
      "loss": 0.2542,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.06391782313585281,
      "learning_rate": 0.00017637883008356545,
      "loss": 0.3264,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.05829302966594696,
      "learning_rate": 0.0001762674094707521,
      "loss": 0.2933,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06562326848506927,
      "learning_rate": 0.00017615598885793873,
      "loss": 0.2715,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.06454933434724808,
      "learning_rate": 0.00017604456824512536,
      "loss": 0.3263,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.05812375620007515,
      "learning_rate": 0.000175933147632312,
      "loss": 0.3219,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04714621976017952,
      "learning_rate": 0.0001758217270194986,
      "loss": 0.3001,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.058413684368133545,
      "learning_rate": 0.00017571030640668524,
      "loss": 0.3342,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.05853581801056862,
      "learning_rate": 0.00017559888579387188,
      "loss": 0.3559,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.06629899889230728,
      "learning_rate": 0.0001754874651810585,
      "loss": 0.3503,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.060716353356838226,
      "learning_rate": 0.00017537604456824513,
      "loss": 0.2432,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.0828184112906456,
      "learning_rate": 0.00017526462395543176,
      "loss": 0.3287,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09290502220392227,
      "learning_rate": 0.0001751532033426184,
      "loss": 0.4283,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.0796162560582161,
      "learning_rate": 0.00017504178272980504,
      "loss": 0.3038,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.059088610112667084,
      "learning_rate": 0.00017493036211699165,
      "loss": 0.2984,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06552764028310776,
      "learning_rate": 0.00017481894150417828,
      "loss": 0.28,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.07208864390850067,
      "learning_rate": 0.00017470752089136492,
      "loss": 0.3022,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.07033393532037735,
      "learning_rate": 0.00017459610027855153,
      "loss": 0.345,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.08407337963581085,
      "learning_rate": 0.00017448467966573816,
      "loss": 0.4327,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.037531256675720215,
      "learning_rate": 0.0001743732590529248,
      "loss": 0.2626,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.06134509667754173,
      "learning_rate": 0.00017426183844011144,
      "loss": 0.3523,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.07239819318056107,
      "learning_rate": 0.00017415041782729807,
      "loss": 0.3462,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.049917254596948624,
      "learning_rate": 0.00017403899721448468,
      "loss": 0.3035,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.06036359816789627,
      "learning_rate": 0.00017392757660167132,
      "loss": 0.2773,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07293415814638138,
      "learning_rate": 0.00017381615598885793,
      "loss": 0.2826,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.11930102854967117,
      "learning_rate": 0.00017370473537604457,
      "loss": 0.4427,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06569141149520874,
      "learning_rate": 0.0001735933147632312,
      "loss": 0.3097,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.06904499232769012,
      "learning_rate": 0.00017348189415041784,
      "loss": 0.2963,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.07275153696537018,
      "learning_rate": 0.00017337047353760448,
      "loss": 0.3557,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.0644286647439003,
      "learning_rate": 0.0001732590529247911,
      "loss": 0.2972,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.09861116111278534,
      "learning_rate": 0.00017314763231197772,
      "loss": 0.426,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.06782294809818268,
      "learning_rate": 0.00017303621169916436,
      "loss": 0.4074,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.055203475058078766,
      "learning_rate": 0.00017292479108635097,
      "loss": 0.3725,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.1059175655245781,
      "learning_rate": 0.0001728133704735376,
      "loss": 0.4315,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.08250673860311508,
      "learning_rate": 0.00017270194986072424,
      "loss": 0.3823,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.06672301888465881,
      "learning_rate": 0.00017259052924791088,
      "loss": 0.272,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.07668539136648178,
      "learning_rate": 0.0001724791086350975,
      "loss": 0.3585,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.09261240810155869,
      "learning_rate": 0.00017236768802228412,
      "loss": 0.3171,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.09300730377435684,
      "learning_rate": 0.00017225626740947076,
      "loss": 0.3526,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.0796714648604393,
      "learning_rate": 0.0001721448467966574,
      "loss": 0.3038,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.06646404415369034,
      "learning_rate": 0.000172033426183844,
      "loss": 0.2964,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.06485851854085922,
      "learning_rate": 0.00017192200557103064,
      "loss": 0.3135,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.06937497109174728,
      "learning_rate": 0.00017181058495821728,
      "loss": 0.3272,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07694260776042938,
      "learning_rate": 0.00017169916434540391,
      "loss": 0.3705,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.04648123309016228,
      "learning_rate": 0.00017158774373259055,
      "loss": 0.2886,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.056954897940158844,
      "learning_rate": 0.00017147632311977716,
      "loss": 0.3066,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04178736358880997,
      "learning_rate": 0.0001713649025069638,
      "loss": 0.2673,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.08387766778469086,
      "learning_rate": 0.00017125348189415043,
      "loss": 0.3245,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.06390725821256638,
      "learning_rate": 0.00017114206128133704,
      "loss": 0.3462,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07663051038980484,
      "learning_rate": 0.00017103064066852368,
      "loss": 0.371,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.06242416799068451,
      "learning_rate": 0.00017091922005571032,
      "loss": 0.3902,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.04411499202251434,
      "learning_rate": 0.00017080779944289695,
      "loss": 0.3499,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.048253074288368225,
      "learning_rate": 0.0001706963788300836,
      "loss": 0.3073,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.05330132693052292,
      "learning_rate": 0.0001705849582172702,
      "loss": 0.2782,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.09645810723304749,
      "learning_rate": 0.00017047353760445683,
      "loss": 0.3513,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.07553944736719131,
      "learning_rate": 0.00017036211699164347,
      "loss": 0.3487,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10264316946268082,
      "learning_rate": 0.00017025069637883008,
      "loss": 0.4436,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.05646580085158348,
      "learning_rate": 0.00017013927576601672,
      "loss": 0.3528,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.07130212336778641,
      "learning_rate": 0.00017002785515320335,
      "loss": 0.3375,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.1121247336268425,
      "learning_rate": 0.00016991643454039,
      "loss": 0.3786,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05661875754594803,
      "learning_rate": 0.00016980501392757663,
      "loss": 0.2849,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.06488969922065735,
      "learning_rate": 0.00016969359331476324,
      "loss": 0.3286,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.04874249920248985,
      "learning_rate": 0.00016958217270194987,
      "loss": 0.3338,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.049081817269325256,
      "learning_rate": 0.0001694707520891365,
      "loss": 0.3112,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06580985337495804,
      "learning_rate": 0.00016935933147632312,
      "loss": 0.3001,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.055983323603868484,
      "learning_rate": 0.00016924791086350975,
      "loss": 0.312,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05061831325292587,
      "learning_rate": 0.0001691364902506964,
      "loss": 0.315,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.07002280652523041,
      "learning_rate": 0.00016902506963788303,
      "loss": 0.3657,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05577494204044342,
      "learning_rate": 0.00016891364902506966,
      "loss": 0.3993,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.0922563299536705,
      "learning_rate": 0.00016880222841225627,
      "loss": 0.3746,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.10654503852128983,
      "learning_rate": 0.0001686908077994429,
      "loss": 0.3481,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.0896528884768486,
      "learning_rate": 0.00016857938718662955,
      "loss": 0.3915,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.11687684059143066,
      "learning_rate": 0.00016846796657381616,
      "loss": 0.4647,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.05887802690267563,
      "learning_rate": 0.0001683565459610028,
      "loss": 0.3566,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.09831524640321732,
      "learning_rate": 0.0001682451253481894,
      "loss": 0.4483,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.0771913006901741,
      "learning_rate": 0.00016813370473537606,
      "loss": 0.3773,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.05990441143512726,
      "learning_rate": 0.0001680222841225627,
      "loss": 0.3402,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07814719527959824,
      "learning_rate": 0.0001679108635097493,
      "loss": 0.4058,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05076230689883232,
      "learning_rate": 0.00016779944289693595,
      "loss": 0.3336,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.0958528146147728,
      "learning_rate": 0.00016768802228412256,
      "loss": 0.4436,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.060141295194625854,
      "learning_rate": 0.0001675766016713092,
      "loss": 0.4008,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.05116834118962288,
      "learning_rate": 0.00016746518105849583,
      "loss": 0.262,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.04933657869696617,
      "learning_rate": 0.00016735376044568244,
      "loss": 0.2721,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.07373249530792236,
      "learning_rate": 0.0001672423398328691,
      "loss": 0.355,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.044377684593200684,
      "learning_rate": 0.00016713091922005574,
      "loss": 0.2971,
      "step": 300
    },
    {
      "epoch": 0.5,
      "eval_loss": 0.33406704664230347,
      "eval_runtime": 373.8116,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.09329823404550552,
      "learning_rate": 0.00016701949860724235,
      "loss": 0.3936,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.05307592451572418,
      "learning_rate": 0.00016690807799442898,
      "loss": 0.2952,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.05269429087638855,
      "learning_rate": 0.0001667966573816156,
      "loss": 0.2697,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.07882646471261978,
      "learning_rate": 0.00016668523676880223,
      "loss": 0.3733,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.07247893512248993,
      "learning_rate": 0.00016657381615598887,
      "loss": 0.4261,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05371604114770889,
      "learning_rate": 0.00016646239554317548,
      "loss": 0.3111,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.05024495720863342,
      "learning_rate": 0.00016635097493036214,
      "loss": 0.2737,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06022166833281517,
      "learning_rate": 0.00016623955431754875,
      "loss": 0.2782,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.05292487516999245,
      "learning_rate": 0.00016612813370473539,
      "loss": 0.3065,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.056927524507045746,
      "learning_rate": 0.00016601671309192202,
      "loss": 0.3671,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.0786108672618866,
      "learning_rate": 0.00016590529247910863,
      "loss": 0.3963,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09610572457313538,
      "learning_rate": 0.00016579387186629527,
      "loss": 0.3851,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.05500410869717598,
      "learning_rate": 0.0001656824512534819,
      "loss": 0.3583,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.08582132309675217,
      "learning_rate": 0.00016557103064066851,
      "loss": 0.3898,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.061810631304979324,
      "learning_rate": 0.00016545961002785518,
      "loss": 0.2483,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07196531444787979,
      "learning_rate": 0.0001653481894150418,
      "loss": 0.4125,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.06049725040793419,
      "learning_rate": 0.00016523676880222842,
      "loss": 0.3356,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06734035164117813,
      "learning_rate": 0.00016512534818941506,
      "loss": 0.3966,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.05909775570034981,
      "learning_rate": 0.00016501392757660167,
      "loss": 0.2551,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09914226830005646,
      "learning_rate": 0.0001649025069637883,
      "loss": 0.3654,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.06723151355981827,
      "learning_rate": 0.00016479108635097494,
      "loss": 0.2901,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06519628316164017,
      "learning_rate": 0.00016467966573816155,
      "loss": 0.3625,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04443543031811714,
      "learning_rate": 0.00016456824512534822,
      "loss": 0.2689,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07195848226547241,
      "learning_rate": 0.00016445682451253482,
      "loss": 0.3749,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.05139874666929245,
      "learning_rate": 0.00016434540389972146,
      "loss": 0.3168,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.06149451434612274,
      "learning_rate": 0.0001642339832869081,
      "loss": 0.3361,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.059856116771698,
      "learning_rate": 0.0001641225626740947,
      "loss": 0.2833,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.060059476643800735,
      "learning_rate": 0.00016401114206128134,
      "loss": 0.3624,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.06225313991308212,
      "learning_rate": 0.00016389972144846798,
      "loss": 0.3619,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.0349864698946476,
      "learning_rate": 0.0001637883008356546,
      "loss": 0.209,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.05916865915060043,
      "learning_rate": 0.00016367688022284125,
      "loss": 0.336,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.043841492384672165,
      "learning_rate": 0.00016356545961002786,
      "loss": 0.3148,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.05800284445285797,
      "learning_rate": 0.0001634540389972145,
      "loss": 0.3651,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08612257242202759,
      "learning_rate": 0.00016334261838440114,
      "loss": 0.3809,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.05423552915453911,
      "learning_rate": 0.00016323119777158774,
      "loss": 0.3328,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.043566737323999405,
      "learning_rate": 0.00016311977715877438,
      "loss": 0.2274,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.0462743416428566,
      "learning_rate": 0.00016300835654596102,
      "loss": 0.312,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.06379109621047974,
      "learning_rate": 0.00016289693593314763,
      "loss": 0.3236,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.054661449044942856,
      "learning_rate": 0.00016278551532033426,
      "loss": 0.2697,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.044119060039520264,
      "learning_rate": 0.0001626740947075209,
      "loss": 0.3071,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.06531501561403275,
      "learning_rate": 0.00016256267409470754,
      "loss": 0.3056,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07126521319150925,
      "learning_rate": 0.00016245125348189417,
      "loss": 0.3725,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.062475211918354034,
      "learning_rate": 0.00016233983286908078,
      "loss": 0.3481,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.051287248730659485,
      "learning_rate": 0.00016222841225626742,
      "loss": 0.3282,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.0753796398639679,
      "learning_rate": 0.00016211699164345403,
      "loss": 0.4105,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.06075955927371979,
      "learning_rate": 0.00016200557103064066,
      "loss": 0.2847,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.049342215061187744,
      "learning_rate": 0.0001618941504178273,
      "loss": 0.319,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11758521944284439,
      "learning_rate": 0.00016178272980501394,
      "loss": 0.3552,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06062491238117218,
      "learning_rate": 0.00016167130919220057,
      "loss": 0.3879,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.053289785981178284,
      "learning_rate": 0.0001615598885793872,
      "loss": 0.3227,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.046300258487463,
      "learning_rate": 0.00016144846796657382,
      "loss": 0.278,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.042093485593795776,
      "learning_rate": 0.00016133704735376046,
      "loss": 0.2906,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.0622057169675827,
      "learning_rate": 0.00016122562674094707,
      "loss": 0.3826,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.046517372131347656,
      "learning_rate": 0.0001611142061281337,
      "loss": 0.2846,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.07477085292339325,
      "learning_rate": 0.00016100278551532034,
      "loss": 0.39,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.05725261569023132,
      "learning_rate": 0.00016089136490250698,
      "loss": 0.3105,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.047877974808216095,
      "learning_rate": 0.0001607799442896936,
      "loss": 0.2642,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.06068779155611992,
      "learning_rate": 0.00016066852367688022,
      "loss": 0.3415,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.06766151636838913,
      "learning_rate": 0.00016055710306406686,
      "loss": 0.3771,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04824470356106758,
      "learning_rate": 0.0001604456824512535,
      "loss": 0.3413,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.05535799637436867,
      "learning_rate": 0.0001603342618384401,
      "loss": 0.3053,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.07063216716051102,
      "learning_rate": 0.00016022284122562674,
      "loss": 0.3687,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03917135298252106,
      "learning_rate": 0.00016011142061281338,
      "loss": 0.2866,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.0627112165093422,
      "learning_rate": 0.00016,
      "loss": 0.2673,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.09194136410951614,
      "learning_rate": 0.00015988857938718665,
      "loss": 0.4174,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.07770700752735138,
      "learning_rate": 0.00015977715877437326,
      "loss": 0.3098,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.0638580247759819,
      "learning_rate": 0.0001596657381615599,
      "loss": 0.3463,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.06891981512308121,
      "learning_rate": 0.00015955431754874653,
      "loss": 0.3873,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.0637492835521698,
      "learning_rate": 0.00015944289693593314,
      "loss": 0.3125,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06685901433229446,
      "learning_rate": 0.00015933147632311978,
      "loss": 0.291,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.06276644021272659,
      "learning_rate": 0.00015922005571030641,
      "loss": 0.3353,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06120016798377037,
      "learning_rate": 0.00015910863509749305,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.06211041286587715,
      "learning_rate": 0.0001589972144846797,
      "loss": 0.3885,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.04900497943162918,
      "learning_rate": 0.0001588857938718663,
      "loss": 0.3883,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.07299406081438065,
      "learning_rate": 0.00015877437325905293,
      "loss": 0.2706,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.09120436757802963,
      "learning_rate": 0.00015866295264623957,
      "loss": 0.3739,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.07414333522319794,
      "learning_rate": 0.00015855153203342618,
      "loss": 0.3925,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.04861472547054291,
      "learning_rate": 0.00015844011142061282,
      "loss": 0.2753,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.10256479680538177,
      "learning_rate": 0.00015832869080779945,
      "loss": 0.3699,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.05784834176301956,
      "learning_rate": 0.0001582172701949861,
      "loss": 0.3029,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.05931473523378372,
      "learning_rate": 0.00015810584958217272,
      "loss": 0.2805,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.05126677453517914,
      "learning_rate": 0.00015799442896935933,
      "loss": 0.2928,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.09290463477373123,
      "learning_rate": 0.00015788300835654597,
      "loss": 0.3602,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10042349249124527,
      "learning_rate": 0.0001577715877437326,
      "loss": 0.4415,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.08114712685346603,
      "learning_rate": 0.00015766016713091922,
      "loss": 0.391,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.07912176847457886,
      "learning_rate": 0.00015754874651810585,
      "loss": 0.3671,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.06592220067977905,
      "learning_rate": 0.0001574373259052925,
      "loss": 0.3083,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05964716896414757,
      "learning_rate": 0.00015732590529247913,
      "loss": 0.2582,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.06917083263397217,
      "learning_rate": 0.00015721448467966576,
      "loss": 0.3186,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.05812530592083931,
      "learning_rate": 0.00015710306406685237,
      "loss": 0.3,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.06314097344875336,
      "learning_rate": 0.000156991643454039,
      "loss": 0.2926,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.06183494254946709,
      "learning_rate": 0.00015688022284122564,
      "loss": 0.2942,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.05879906937479973,
      "learning_rate": 0.00015676880222841225,
      "loss": 0.2761,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.07158152759075165,
      "learning_rate": 0.0001566573816155989,
      "loss": 0.4515,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.05752398073673248,
      "learning_rate": 0.0001565459610027855,
      "loss": 0.3788,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06474988907575607,
      "learning_rate": 0.00015643454038997216,
      "loss": 0.3496,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.07224076986312866,
      "learning_rate": 0.0001563231197771588,
      "loss": 0.3171,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.04606259614229202,
      "learning_rate": 0.0001562116991643454,
      "loss": 0.2679,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.06618712842464447,
      "learning_rate": 0.00015610027855153205,
      "loss": 0.3408,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0727199912071228,
      "learning_rate": 0.00015598885793871866,
      "loss": 0.3694,
      "step": 400
    },
    {
      "epoch": 0.6666666666666666,
      "eval_loss": 0.33038070797920227,
      "eval_runtime": 373.7813,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.049671657383441925,
      "learning_rate": 0.0001558774373259053,
      "loss": 0.3638,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.0751660093665123,
      "learning_rate": 0.00015576601671309193,
      "loss": 0.3246,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.051771342754364014,
      "learning_rate": 0.00015565459610027854,
      "loss": 0.2599,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.06960935890674591,
      "learning_rate": 0.0001555431754874652,
      "loss": 0.2855,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.0638466402888298,
      "learning_rate": 0.00015543175487465184,
      "loss": 0.3257,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.06104127690196037,
      "learning_rate": 0.00015532033426183845,
      "loss": 0.2893,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.09004420042037964,
      "learning_rate": 0.00015520891364902508,
      "loss": 0.4522,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.07697346806526184,
      "learning_rate": 0.0001550974930362117,
      "loss": 0.2782,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.07419420033693314,
      "learning_rate": 0.00015498607242339833,
      "loss": 0.3739,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.055936019867658615,
      "learning_rate": 0.00015487465181058497,
      "loss": 0.3843,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.047740768641233444,
      "learning_rate": 0.00015476323119777158,
      "loss": 0.2755,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.04831160977482796,
      "learning_rate": 0.00015465181058495824,
      "loss": 0.2952,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.06364057958126068,
      "learning_rate": 0.00015454038997214485,
      "loss": 0.3322,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.05456951633095741,
      "learning_rate": 0.00015442896935933148,
      "loss": 0.3343,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.03701818734407425,
      "learning_rate": 0.00015431754874651812,
      "loss": 0.1905,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.06792543083429337,
      "learning_rate": 0.00015420612813370473,
      "loss": 0.3592,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.08207648247480392,
      "learning_rate": 0.00015409470752089137,
      "loss": 0.3399,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.06876149773597717,
      "learning_rate": 0.000153983286908078,
      "loss": 0.374,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.06378063559532166,
      "learning_rate": 0.0001538718662952646,
      "loss": 0.3097,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04361330345273018,
      "learning_rate": 0.00015376044568245128,
      "loss": 0.3016,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.05422496423125267,
      "learning_rate": 0.00015364902506963789,
      "loss": 0.4201,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.06403195112943649,
      "learning_rate": 0.00015353760445682452,
      "loss": 0.3504,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.05734192952513695,
      "learning_rate": 0.00015342618384401116,
      "loss": 0.3443,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06866508722305298,
      "learning_rate": 0.00015331476323119777,
      "loss": 0.3852,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.04271905496716499,
      "learning_rate": 0.0001532033426183844,
      "loss": 0.3524,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.056068915873765945,
      "learning_rate": 0.00015309192200557104,
      "loss": 0.2999,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.043030448257923126,
      "learning_rate": 0.00015298050139275765,
      "loss": 0.2591,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.0719999372959137,
      "learning_rate": 0.00015286908077994431,
      "loss": 0.325,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.05486292019486427,
      "learning_rate": 0.00015275766016713092,
      "loss": 0.3018,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.05790204182267189,
      "learning_rate": 0.00015264623955431756,
      "loss": 0.3348,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.0518096461892128,
      "learning_rate": 0.0001525348189415042,
      "loss": 0.3198,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06278590857982635,
      "learning_rate": 0.0001524233983286908,
      "loss": 0.3884,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.04729745164513588,
      "learning_rate": 0.00015231197771587744,
      "loss": 0.2791,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.06511681526899338,
      "learning_rate": 0.00015220055710306408,
      "loss": 0.3217,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05852040275931358,
      "learning_rate": 0.0001520891364902507,
      "loss": 0.3039,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.05285286903381348,
      "learning_rate": 0.00015197771587743735,
      "loss": 0.3112,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.0563475526869297,
      "learning_rate": 0.00015186629526462396,
      "loss": 0.3308,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.04962017387151718,
      "learning_rate": 0.0001517548746518106,
      "loss": 0.3074,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.06965400278568268,
      "learning_rate": 0.00015164345403899723,
      "loss": 0.3453,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03574811667203903,
      "learning_rate": 0.00015153203342618384,
      "loss": 0.2622,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09636316448450089,
      "learning_rate": 0.00015142061281337048,
      "loss": 0.3886,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.059329334646463394,
      "learning_rate": 0.00015130919220055712,
      "loss": 0.3156,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06609758734703064,
      "learning_rate": 0.00015119777158774373,
      "loss": 0.2453,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04046764224767685,
      "learning_rate": 0.0001510863509749304,
      "loss": 0.2396,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.10643579065799713,
      "learning_rate": 0.000150974930362117,
      "loss": 0.4159,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06950733810663223,
      "learning_rate": 0.00015086350974930364,
      "loss": 0.3398,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.0766250416636467,
      "learning_rate": 0.00015075208913649027,
      "loss": 0.3328,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05973466485738754,
      "learning_rate": 0.00015064066852367688,
      "loss": 0.3382,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.04731937497854233,
      "learning_rate": 0.00015052924791086352,
      "loss": 0.2357,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.0641368106007576,
      "learning_rate": 0.00015041782729805013,
      "loss": 0.3222,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.08205905556678772,
      "learning_rate": 0.00015030640668523676,
      "loss": 0.3659,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06015029922127724,
      "learning_rate": 0.00015019498607242343,
      "loss": 0.28,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.049014464020729065,
      "learning_rate": 0.00015008356545961004,
      "loss": 0.2436,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06603681296110153,
      "learning_rate": 0.00014997214484679667,
      "loss": 0.3804,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07793335616588593,
      "learning_rate": 0.0001498607242339833,
      "loss": 0.294,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04767234995961189,
      "learning_rate": 0.00014974930362116992,
      "loss": 0.2779,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.056623298674821854,
      "learning_rate": 0.00014963788300835656,
      "loss": 0.3908,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.05769085884094238,
      "learning_rate": 0.00014952646239554316,
      "loss": 0.3466,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.04517456516623497,
      "learning_rate": 0.0001494150417827298,
      "loss": 0.2858,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.0452684722840786,
      "learning_rate": 0.00014930362116991646,
      "loss": 0.2877,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.053076423704624176,
      "learning_rate": 0.00014919220055710307,
      "loss": 0.3587,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.05435944348573685,
      "learning_rate": 0.0001490807799442897,
      "loss": 0.2822,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.05604596063494682,
      "learning_rate": 0.00014896935933147632,
      "loss": 0.3081,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.07164648920297623,
      "learning_rate": 0.00014885793871866296,
      "loss": 0.4099,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06206504628062248,
      "learning_rate": 0.0001487465181058496,
      "loss": 0.3222,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.04434536397457123,
      "learning_rate": 0.0001486350974930362,
      "loss": 0.2555,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.042083725333213806,
      "learning_rate": 0.00014852367688022284,
      "loss": 0.2848,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04734901338815689,
      "learning_rate": 0.00014841225626740948,
      "loss": 0.3472,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.07734812796115875,
      "learning_rate": 0.0001483008356545961,
      "loss": 0.3498,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.04673198238015175,
      "learning_rate": 0.00014818941504178275,
      "loss": 0.3031,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.04137273505330086,
      "learning_rate": 0.00014807799442896936,
      "loss": 0.2605,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04433909058570862,
      "learning_rate": 0.000147966573816156,
      "loss": 0.298,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.051636550575494766,
      "learning_rate": 0.00014785515320334263,
      "loss": 0.316,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.06208198517560959,
      "learning_rate": 0.00014774373259052924,
      "loss": 0.3718,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.05418902263045311,
      "learning_rate": 0.00014763231197771588,
      "loss": 0.3801,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05110190063714981,
      "learning_rate": 0.0001475208913649025,
      "loss": 0.3136,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.09889218956232071,
      "learning_rate": 0.00014740947075208915,
      "loss": 0.3812,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.05322788283228874,
      "learning_rate": 0.00014729805013927579,
      "loss": 0.2818,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05451244115829468,
      "learning_rate": 0.0001471866295264624,
      "loss": 0.3402,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06887216866016388,
      "learning_rate": 0.00014707520891364903,
      "loss": 0.3994,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.0672229751944542,
      "learning_rate": 0.00014696378830083567,
      "loss": 0.4048,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.03675975278019905,
      "learning_rate": 0.00014685236768802228,
      "loss": 0.2377,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04699351638555527,
      "learning_rate": 0.00014674094707520891,
      "loss": 0.2695,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.06801038235425949,
      "learning_rate": 0.00014662952646239555,
      "loss": 0.3598,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.06345156580209732,
      "learning_rate": 0.0001465181058495822,
      "loss": 0.3508,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05789051949977875,
      "learning_rate": 0.00014640668523676882,
      "loss": 0.32,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.04386863857507706,
      "learning_rate": 0.00014629526462395543,
      "loss": 0.2561,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.05822966247797012,
      "learning_rate": 0.00014618384401114207,
      "loss": 0.2997,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06740472465753555,
      "learning_rate": 0.0001460724233983287,
      "loss": 0.3412,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.04136982187628746,
      "learning_rate": 0.00014596100278551532,
      "loss": 0.2713,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.04848026484251022,
      "learning_rate": 0.00014584958217270195,
      "loss": 0.2985,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.0479574128985405,
      "learning_rate": 0.0001457381615598886,
      "loss": 0.3135,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.04878498986363411,
      "learning_rate": 0.00014562674094707522,
      "loss": 0.2961,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.06789479404687881,
      "learning_rate": 0.00014551532033426186,
      "loss": 0.3353,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.05490916594862938,
      "learning_rate": 0.00014540389972144847,
      "loss": 0.3405,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.0509500578045845,
      "learning_rate": 0.0001452924791086351,
      "loss": 0.2784,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.053391579538583755,
      "learning_rate": 0.00014518105849582174,
      "loss": 0.311,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.06824196875095367,
      "learning_rate": 0.00014506963788300835,
      "loss": 0.3328,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04643777385354042,
      "learning_rate": 0.000144958217270195,
      "loss": 0.2726,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.051668308675289154,
      "learning_rate": 0.0001448467966573816,
      "loss": 0.2921,
      "step": 500
    },
    {
      "epoch": 0.8333333333333334,
      "eval_loss": 0.3281901478767395,
      "eval_runtime": 373.7636,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 500
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.060772743076086044,
      "learning_rate": 0.00014473537604456826,
      "loss": 0.2932,
      "step": 501
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.06085341051220894,
      "learning_rate": 0.0001446239554317549,
      "loss": 0.3707,
      "step": 502
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.07657663524150848,
      "learning_rate": 0.0001445125348189415,
      "loss": 0.3316,
      "step": 503
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.05329258367419243,
      "learning_rate": 0.00014440111420612814,
      "loss": 0.2978,
      "step": 504
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.07449754327535629,
      "learning_rate": 0.00014428969359331475,
      "loss": 0.3206,
      "step": 505
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.09915780276060104,
      "learning_rate": 0.0001441782729805014,
      "loss": 0.3772,
      "step": 506
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.04010171815752983,
      "learning_rate": 0.00014406685236768803,
      "loss": 0.2367,
      "step": 507
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.09146162867546082,
      "learning_rate": 0.00014395543175487464,
      "loss": 0.4259,
      "step": 508
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.03935272619128227,
      "learning_rate": 0.0001438440111420613,
      "loss": 0.2934,
      "step": 509
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.04444647580385208,
      "learning_rate": 0.00014373259052924794,
      "loss": 0.2709,
      "step": 510
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 0.04553202539682388,
      "learning_rate": 0.00014362116991643455,
      "loss": 0.2965,
      "step": 511
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.03921709582209587,
      "learning_rate": 0.00014350974930362118,
      "loss": 0.2571,
      "step": 512
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.06850571185350418,
      "learning_rate": 0.0001433983286908078,
      "loss": 0.3265,
      "step": 513
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.06620576232671738,
      "learning_rate": 0.00014328690807799443,
      "loss": 0.3389,
      "step": 514
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.054430171847343445,
      "learning_rate": 0.00014317548746518106,
      "loss": 0.2977,
      "step": 515
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.05139526352286339,
      "learning_rate": 0.00014306406685236767,
      "loss": 0.3128,
      "step": 516
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.07130229473114014,
      "learning_rate": 0.00014295264623955434,
      "loss": 0.3456,
      "step": 517
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.09925727546215057,
      "learning_rate": 0.00014284122562674095,
      "loss": 0.3533,
      "step": 518
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.05085983872413635,
      "learning_rate": 0.00014272980501392758,
      "loss": 0.3004,
      "step": 519
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04295116662979126,
      "learning_rate": 0.00014261838440111422,
      "loss": 0.2735,
      "step": 520
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 0.06154049560427666,
      "learning_rate": 0.00014250696378830083,
      "loss": 0.333,
      "step": 521
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.04609737545251846,
      "learning_rate": 0.00014239554317548747,
      "loss": 0.2828,
      "step": 522
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 0.06917523592710495,
      "learning_rate": 0.0001422841225626741,
      "loss": 0.3755,
      "step": 523
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.05453577637672424,
      "learning_rate": 0.0001421727019498607,
      "loss": 0.3307,
      "step": 524
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.0447324737906456,
      "learning_rate": 0.00014206128133704738,
      "loss": 0.2688,
      "step": 525
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.052297793328762054,
      "learning_rate": 0.00014194986072423398,
      "loss": 0.3794,
      "step": 526
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.05053732916712761,
      "learning_rate": 0.00014183844011142062,
      "loss": 0.2852,
      "step": 527
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.08157094568014145,
      "learning_rate": 0.00014172701949860726,
      "loss": 0.4197,
      "step": 528
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 0.04738731309771538,
      "learning_rate": 0.00014161559888579387,
      "loss": 0.2762,
      "step": 529
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.06784859299659729,
      "learning_rate": 0.0001415041782729805,
      "loss": 0.4156,
      "step": 530
    },
    {
      "epoch": 0.885,
      "grad_norm": 0.07192090898752213,
      "learning_rate": 0.00014139275766016714,
      "loss": 0.3342,
      "step": 531
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.08345683664083481,
      "learning_rate": 0.00014128133704735375,
      "loss": 0.3387,
      "step": 532
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 0.04191029071807861,
      "learning_rate": 0.0001411699164345404,
      "loss": 0.2741,
      "step": 533
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.05085219070315361,
      "learning_rate": 0.00014105849582172702,
      "loss": 0.2638,
      "step": 534
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.056857623159885406,
      "learning_rate": 0.00014094707520891366,
      "loss": 0.3194,
      "step": 535
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.06935551762580872,
      "learning_rate": 0.0001408356545961003,
      "loss": 0.3148,
      "step": 536
    },
    {
      "epoch": 0.895,
      "grad_norm": 0.06971725076436996,
      "learning_rate": 0.0001407242339832869,
      "loss": 0.3493,
      "step": 537
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.11990592628717422,
      "learning_rate": 0.00014061281337047354,
      "loss": 0.4268,
      "step": 538
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.054495323449373245,
      "learning_rate": 0.00014050139275766018,
      "loss": 0.3542,
      "step": 539
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.08286935836076736,
      "learning_rate": 0.0001403899721448468,
      "loss": 0.3491,
      "step": 540
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.07133619487285614,
      "learning_rate": 0.00014027855153203345,
      "loss": 0.3448,
      "step": 541
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.042325690388679504,
      "learning_rate": 0.00014016713091922006,
      "loss": 0.2781,
      "step": 542
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.032932549715042114,
      "learning_rate": 0.0001400557103064067,
      "loss": 0.2175,
      "step": 543
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.0688985288143158,
      "learning_rate": 0.00013994428969359333,
      "loss": 0.2599,
      "step": 544
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.05417398735880852,
      "learning_rate": 0.00013983286908077994,
      "loss": 0.2933,
      "step": 545
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.05415311083197594,
      "learning_rate": 0.00013972144846796658,
      "loss": 0.2908,
      "step": 546
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 0.05922441557049751,
      "learning_rate": 0.00013961002785515322,
      "loss": 0.3042,
      "step": 547
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.046018511056900024,
      "learning_rate": 0.00013949860724233982,
      "loss": 0.3232,
      "step": 548
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.0504499226808548,
      "learning_rate": 0.0001393871866295265,
      "loss": 0.2453,
      "step": 549
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.07001747936010361,
      "learning_rate": 0.0001392757660167131,
      "loss": 0.3062,
      "step": 550
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.060449179261922836,
      "learning_rate": 0.00013916434540389973,
      "loss": 0.3479,
      "step": 551
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.05341773480176926,
      "learning_rate": 0.00013905292479108637,
      "loss": 0.2807,
      "step": 552
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 0.04648570343852043,
      "learning_rate": 0.00013894150417827298,
      "loss": 0.2319,
      "step": 553
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 0.049530383199453354,
      "learning_rate": 0.00013883008356545962,
      "loss": 0.2977,
      "step": 554
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.04114844277501106,
      "learning_rate": 0.00013871866295264623,
      "loss": 0.2376,
      "step": 555
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04786466062068939,
      "learning_rate": 0.00013860724233983286,
      "loss": 0.2811,
      "step": 556
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.09112845361232758,
      "learning_rate": 0.00013849582172701953,
      "loss": 0.3976,
      "step": 557
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.04977407678961754,
      "learning_rate": 0.00013838440111420614,
      "loss": 0.2862,
      "step": 558
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 0.040610700845718384,
      "learning_rate": 0.00013827298050139277,
      "loss": 0.2682,
      "step": 559
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.07936462759971619,
      "learning_rate": 0.0001381615598885794,
      "loss": 0.3323,
      "step": 560
    },
    {
      "epoch": 0.935,
      "grad_norm": 0.05755506455898285,
      "learning_rate": 0.00013805013927576602,
      "loss": 0.3285,
      "step": 561
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.054356928914785385,
      "learning_rate": 0.00013793871866295265,
      "loss": 0.2217,
      "step": 562
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.048578329384326935,
      "learning_rate": 0.00013782729805013926,
      "loss": 0.3149,
      "step": 563
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.07088055461645126,
      "learning_rate": 0.0001377158774373259,
      "loss": 0.3515,
      "step": 564
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.05443679541349411,
      "learning_rate": 0.00013760445682451256,
      "loss": 0.3135,
      "step": 565
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.054265957325696945,
      "learning_rate": 0.00013749303621169917,
      "loss": 0.2861,
      "step": 566
    },
    {
      "epoch": 0.945,
      "grad_norm": 0.05600951611995697,
      "learning_rate": 0.0001373816155988858,
      "loss": 0.3121,
      "step": 567
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.045498721301555634,
      "learning_rate": 0.00013727019498607242,
      "loss": 0.2579,
      "step": 568
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 0.04574575647711754,
      "learning_rate": 0.00013715877437325906,
      "loss": 0.2609,
      "step": 569
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.038479067385196686,
      "learning_rate": 0.0001370473537604457,
      "loss": 0.2768,
      "step": 570
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.06350842863321304,
      "learning_rate": 0.0001369359331476323,
      "loss": 0.3884,
      "step": 571
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.05305676907300949,
      "learning_rate": 0.00013682451253481894,
      "loss": 0.305,
      "step": 572
    },
    {
      "epoch": 0.955,
      "grad_norm": 0.07867347449064255,
      "learning_rate": 0.00013671309192200557,
      "loss": 0.3895,
      "step": 573
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.04169553518295288,
      "learning_rate": 0.0001366016713091922,
      "loss": 0.2945,
      "step": 574
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.05505680665373802,
      "learning_rate": 0.00013649025069637885,
      "loss": 0.3148,
      "step": 575
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.048953715711832047,
      "learning_rate": 0.00013637883008356546,
      "loss": 0.3288,
      "step": 576
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.04455683007836342,
      "learning_rate": 0.0001362674094707521,
      "loss": 0.3008,
      "step": 577
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.042805854231119156,
      "learning_rate": 0.00013615598885793873,
      "loss": 0.3032,
      "step": 578
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.05032384768128395,
      "learning_rate": 0.00013604456824512534,
      "loss": 0.2927,
      "step": 579
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.05536658689379692,
      "learning_rate": 0.00013593314763231198,
      "loss": 0.2686,
      "step": 580
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.04495309293270111,
      "learning_rate": 0.0001358217270194986,
      "loss": 0.3429,
      "step": 581
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.04196522757411003,
      "learning_rate": 0.00013571030640668525,
      "loss": 0.2481,
      "step": 582
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.05041138082742691,
      "learning_rate": 0.00013559888579387188,
      "loss": 0.2846,
      "step": 583
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.04851841181516647,
      "learning_rate": 0.0001354874651810585,
      "loss": 0.303,
      "step": 584
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.05022190511226654,
      "learning_rate": 0.00013537604456824513,
      "loss": 0.3512,
      "step": 585
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.0567840076982975,
      "learning_rate": 0.00013526462395543177,
      "loss": 0.3161,
      "step": 586
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 0.059770599007606506,
      "learning_rate": 0.00013515320334261838,
      "loss": 0.3093,
      "step": 587
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.040113139897584915,
      "learning_rate": 0.000135041782729805,
      "loss": 0.2707,
      "step": 588
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.05541342496871948,
      "learning_rate": 0.00013493036211699165,
      "loss": 0.3682,
      "step": 589
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.048269204795360565,
      "learning_rate": 0.00013481894150417829,
      "loss": 0.3378,
      "step": 590
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.03636118397116661,
      "learning_rate": 0.00013470752089136492,
      "loss": 0.2207,
      "step": 591
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.09315942227840424,
      "learning_rate": 0.00013459610027855153,
      "loss": 0.3986,
      "step": 592
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.050616469234228134,
      "learning_rate": 0.00013448467966573817,
      "loss": 0.3024,
      "step": 593
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.08030980825424194,
      "learning_rate": 0.0001343732590529248,
      "loss": 0.2733,
      "step": 594
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.06223587691783905,
      "learning_rate": 0.00013426183844011141,
      "loss": 0.3,
      "step": 595
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.05033952370285988,
      "learning_rate": 0.00013415041782729805,
      "loss": 0.3118,
      "step": 596
    },
    {
      "epoch": 0.995,
      "grad_norm": 0.08084849268198013,
      "learning_rate": 0.0001340389972144847,
      "loss": 0.4004,
      "step": 597
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.07659777998924255,
      "learning_rate": 0.00013392757660167132,
      "loss": 0.3137,
      "step": 598
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.07017462700605392,
      "learning_rate": 0.00013381615598885796,
      "loss": 0.3972,
      "step": 599
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.05202601104974747,
      "learning_rate": 0.00013370473537604457,
      "loss": 0.3223,
      "step": 600
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3265322744846344,
      "eval_runtime": 373.6729,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 600
    },
    {
      "epoch": 1.0016666666666667,
      "grad_norm": 0.037545207887887955,
      "learning_rate": 0.0001335933147632312,
      "loss": 0.2446,
      "step": 601
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.08388081938028336,
      "learning_rate": 0.00013348189415041784,
      "loss": 0.3709,
      "step": 602
    },
    {
      "epoch": 1.005,
      "grad_norm": 0.06967391073703766,
      "learning_rate": 0.00013337047353760445,
      "loss": 0.3443,
      "step": 603
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.04108358919620514,
      "learning_rate": 0.0001332590529247911,
      "loss": 0.298,
      "step": 604
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.045270971953868866,
      "learning_rate": 0.00013314763231197772,
      "loss": 0.2757,
      "step": 605
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.0656428337097168,
      "learning_rate": 0.00013303621169916436,
      "loss": 0.3413,
      "step": 606
    },
    {
      "epoch": 1.0116666666666667,
      "grad_norm": 0.08023396134376526,
      "learning_rate": 0.000132924791086351,
      "loss": 0.4167,
      "step": 607
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.07055976986885071,
      "learning_rate": 0.0001328133704735376,
      "loss": 0.3589,
      "step": 608
    },
    {
      "epoch": 1.015,
      "grad_norm": 0.053702279925346375,
      "learning_rate": 0.00013270194986072424,
      "loss": 0.3138,
      "step": 609
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.05079668387770653,
      "learning_rate": 0.00013259052924791085,
      "loss": 0.2567,
      "step": 610
    },
    {
      "epoch": 1.0183333333333333,
      "grad_norm": 0.08691687881946564,
      "learning_rate": 0.0001324791086350975,
      "loss": 0.3413,
      "step": 611
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.05738988518714905,
      "learning_rate": 0.00013236768802228413,
      "loss": 0.3101,
      "step": 612
    },
    {
      "epoch": 1.0216666666666667,
      "grad_norm": 0.057901643216609955,
      "learning_rate": 0.00013225626740947076,
      "loss": 0.3374,
      "step": 613
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.0619928203523159,
      "learning_rate": 0.0001321448467966574,
      "loss": 0.3387,
      "step": 614
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.062190569937229156,
      "learning_rate": 0.00013203342618384404,
      "loss": 0.3594,
      "step": 615
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.043581511825323105,
      "learning_rate": 0.00013192200557103064,
      "loss": 0.2665,
      "step": 616
    },
    {
      "epoch": 1.0283333333333333,
      "grad_norm": 0.07001938670873642,
      "learning_rate": 0.00013181058495821728,
      "loss": 0.3811,
      "step": 617
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.053179021924734116,
      "learning_rate": 0.0001316991643454039,
      "loss": 0.3497,
      "step": 618
    },
    {
      "epoch": 1.0316666666666667,
      "grad_norm": 0.05566834285855293,
      "learning_rate": 0.00013158774373259053,
      "loss": 0.3463,
      "step": 619
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.057344164699316025,
      "learning_rate": 0.00013147632311977716,
      "loss": 0.2846,
      "step": 620
    },
    {
      "epoch": 1.035,
      "grad_norm": 0.1186261847615242,
      "learning_rate": 0.0001313649025069638,
      "loss": 0.3998,
      "step": 621
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.052325136959552765,
      "learning_rate": 0.00013125348189415044,
      "loss": 0.2836,
      "step": 622
    },
    {
      "epoch": 1.0383333333333333,
      "grad_norm": 0.049695033580064774,
      "learning_rate": 0.00013114206128133705,
      "loss": 0.2996,
      "step": 623
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.06760648638010025,
      "learning_rate": 0.00013103064066852368,
      "loss": 0.3445,
      "step": 624
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.04542970657348633,
      "learning_rate": 0.00013091922005571032,
      "loss": 0.3142,
      "step": 625
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.051388125866651535,
      "learning_rate": 0.00013080779944289693,
      "loss": 0.2817,
      "step": 626
    },
    {
      "epoch": 1.045,
      "grad_norm": 0.051026877015829086,
      "learning_rate": 0.00013069637883008356,
      "loss": 0.2982,
      "step": 627
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.05956486985087395,
      "learning_rate": 0.0001305849582172702,
      "loss": 0.362,
      "step": 628
    },
    {
      "epoch": 1.0483333333333333,
      "grad_norm": 0.044415973126888275,
      "learning_rate": 0.00013047353760445684,
      "loss": 0.2259,
      "step": 629
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.07529677450656891,
      "learning_rate": 0.00013036211699164347,
      "loss": 0.349,
      "step": 630
    },
    {
      "epoch": 1.0516666666666667,
      "grad_norm": 0.0490957573056221,
      "learning_rate": 0.00013025069637883008,
      "loss": 0.2862,
      "step": 631
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.04915102198719978,
      "learning_rate": 0.00013013927576601672,
      "loss": 0.3224,
      "step": 632
    },
    {
      "epoch": 1.055,
      "grad_norm": 0.04311239719390869,
      "learning_rate": 0.00013002785515320336,
      "loss": 0.2879,
      "step": 633
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.045873116701841354,
      "learning_rate": 0.00012991643454038997,
      "loss": 0.2648,
      "step": 634
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.05527368187904358,
      "learning_rate": 0.0001298050139275766,
      "loss": 0.3109,
      "step": 635
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.035854704678058624,
      "learning_rate": 0.00012969359331476324,
      "loss": 0.2428,
      "step": 636
    },
    {
      "epoch": 1.0616666666666668,
      "grad_norm": 0.06005294248461723,
      "learning_rate": 0.00012958217270194988,
      "loss": 0.343,
      "step": 637
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.05582824721932411,
      "learning_rate": 0.0001294707520891365,
      "loss": 0.3251,
      "step": 638
    },
    {
      "epoch": 1.065,
      "grad_norm": 0.050943125039339066,
      "learning_rate": 0.00012935933147632312,
      "loss": 0.2672,
      "step": 639
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.06783857941627502,
      "learning_rate": 0.00012924791086350976,
      "loss": 0.3331,
      "step": 640
    },
    {
      "epoch": 1.0683333333333334,
      "grad_norm": 0.05104564130306244,
      "learning_rate": 0.0001291364902506964,
      "loss": 0.3351,
      "step": 641
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.04604925960302353,
      "learning_rate": 0.000129025069637883,
      "loss": 0.2915,
      "step": 642
    },
    {
      "epoch": 1.0716666666666668,
      "grad_norm": 0.036190032958984375,
      "learning_rate": 0.00012891364902506964,
      "loss": 0.2309,
      "step": 643
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.05398710444569588,
      "learning_rate": 0.00012880222841225628,
      "loss": 0.265,
      "step": 644
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.042475681751966476,
      "learning_rate": 0.0001286908077994429,
      "loss": 0.2565,
      "step": 645
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.044244058430194855,
      "learning_rate": 0.00012857938718662955,
      "loss": 0.3151,
      "step": 646
    },
    {
      "epoch": 1.0783333333333334,
      "grad_norm": 0.050446126610040665,
      "learning_rate": 0.00012846796657381616,
      "loss": 0.321,
      "step": 647
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.04219895973801613,
      "learning_rate": 0.0001283565459610028,
      "loss": 0.3043,
      "step": 648
    },
    {
      "epoch": 1.0816666666666666,
      "grad_norm": 0.046174537390470505,
      "learning_rate": 0.00012824512534818943,
      "loss": 0.2782,
      "step": 649
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.05947514995932579,
      "learning_rate": 0.00012813370473537604,
      "loss": 0.3848,
      "step": 650
    },
    {
      "epoch": 1.085,
      "grad_norm": 0.04840335249900818,
      "learning_rate": 0.00012802228412256268,
      "loss": 0.3127,
      "step": 651
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.04292229190468788,
      "learning_rate": 0.00012791086350974931,
      "loss": 0.2783,
      "step": 652
    },
    {
      "epoch": 1.0883333333333334,
      "grad_norm": 0.0456630177795887,
      "learning_rate": 0.00012779944289693595,
      "loss": 0.3099,
      "step": 653
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.07853838801383972,
      "learning_rate": 0.0001276880222841226,
      "loss": 0.2929,
      "step": 654
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.05258505791425705,
      "learning_rate": 0.0001275766016713092,
      "loss": 0.2848,
      "step": 655
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.04929404705762863,
      "learning_rate": 0.00012746518105849583,
      "loss": 0.296,
      "step": 656
    },
    {
      "epoch": 1.095,
      "grad_norm": 0.06462109833955765,
      "learning_rate": 0.00012735376044568247,
      "loss": 0.3871,
      "step": 657
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.041407350450754166,
      "learning_rate": 0.00012724233983286908,
      "loss": 0.2242,
      "step": 658
    },
    {
      "epoch": 1.0983333333333334,
      "grad_norm": 0.04740098491311073,
      "learning_rate": 0.00012713091922005572,
      "loss": 0.3243,
      "step": 659
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.05279053747653961,
      "learning_rate": 0.00012701949860724232,
      "loss": 0.3107,
      "step": 660
    },
    {
      "epoch": 1.1016666666666666,
      "grad_norm": 0.05239010229706764,
      "learning_rate": 0.00012690807799442896,
      "loss": 0.2879,
      "step": 661
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.04420118406414986,
      "learning_rate": 0.00012679665738161562,
      "loss": 0.2619,
      "step": 662
    },
    {
      "epoch": 1.105,
      "grad_norm": 0.06074133142828941,
      "learning_rate": 0.00012668523676880223,
      "loss": 0.3508,
      "step": 663
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.04647393152117729,
      "learning_rate": 0.00012657381615598887,
      "loss": 0.3193,
      "step": 664
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.04810037836432457,
      "learning_rate": 0.0001264623955431755,
      "loss": 0.2773,
      "step": 665
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.05008118599653244,
      "learning_rate": 0.00012635097493036212,
      "loss": 0.3132,
      "step": 666
    },
    {
      "epoch": 1.1116666666666666,
      "grad_norm": 0.06312387436628342,
      "learning_rate": 0.00012623955431754875,
      "loss": 0.3909,
      "step": 667
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.04194491729140282,
      "learning_rate": 0.00012612813370473536,
      "loss": 0.2871,
      "step": 668
    },
    {
      "epoch": 1.115,
      "grad_norm": 0.07597023993730545,
      "learning_rate": 0.000126016713091922,
      "loss": 0.3848,
      "step": 669
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.06192773953080177,
      "learning_rate": 0.00012590529247910866,
      "loss": 0.3859,
      "step": 670
    },
    {
      "epoch": 1.1183333333333334,
      "grad_norm": 0.04794010892510414,
      "learning_rate": 0.00012579387186629527,
      "loss": 0.2845,
      "step": 671
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.04922955110669136,
      "learning_rate": 0.0001256824512534819,
      "loss": 0.3181,
      "step": 672
    },
    {
      "epoch": 1.1216666666666666,
      "grad_norm": 0.05445132032036781,
      "learning_rate": 0.00012557103064066852,
      "loss": 0.3717,
      "step": 673
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.049028895795345306,
      "learning_rate": 0.00012545961002785515,
      "loss": 0.3412,
      "step": 674
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.0510074719786644,
      "learning_rate": 0.0001253481894150418,
      "loss": 0.2722,
      "step": 675
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.05593949183821678,
      "learning_rate": 0.0001252367688022284,
      "loss": 0.3587,
      "step": 676
    },
    {
      "epoch": 1.1283333333333334,
      "grad_norm": 0.044265806674957275,
      "learning_rate": 0.00012512534818941504,
      "loss": 0.2558,
      "step": 677
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.044104140251874924,
      "learning_rate": 0.00012501392757660167,
      "loss": 0.2673,
      "step": 678
    },
    {
      "epoch": 1.1316666666666666,
      "grad_norm": 0.04890120029449463,
      "learning_rate": 0.0001249025069637883,
      "loss": 0.365,
      "step": 679
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.04026368260383606,
      "learning_rate": 0.00012479108635097495,
      "loss": 0.2255,
      "step": 680
    },
    {
      "epoch": 1.135,
      "grad_norm": 0.08276694267988205,
      "learning_rate": 0.00012467966573816156,
      "loss": 0.3267,
      "step": 681
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.049490395933389664,
      "learning_rate": 0.0001245682451253482,
      "loss": 0.2949,
      "step": 682
    },
    {
      "epoch": 1.1383333333333334,
      "grad_norm": 0.04618123918771744,
      "learning_rate": 0.00012445682451253483,
      "loss": 0.2721,
      "step": 683
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.06250430643558502,
      "learning_rate": 0.00012434540389972144,
      "loss": 0.3321,
      "step": 684
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.05551249533891678,
      "learning_rate": 0.00012423398328690807,
      "loss": 0.3071,
      "step": 685
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 0.06421687453985214,
      "learning_rate": 0.0001241225626740947,
      "loss": 0.3623,
      "step": 686
    },
    {
      "epoch": 1.145,
      "grad_norm": 0.04292492941021919,
      "learning_rate": 0.00012401114206128135,
      "loss": 0.2867,
      "step": 687
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.05317793786525726,
      "learning_rate": 0.00012389972144846798,
      "loss": 0.3102,
      "step": 688
    },
    {
      "epoch": 1.1483333333333334,
      "grad_norm": 0.04448207840323448,
      "learning_rate": 0.0001237883008356546,
      "loss": 0.2642,
      "step": 689
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.05929863080382347,
      "learning_rate": 0.00012367688022284123,
      "loss": 0.3923,
      "step": 690
    },
    {
      "epoch": 1.1516666666666666,
      "grad_norm": 0.04860999807715416,
      "learning_rate": 0.00012356545961002787,
      "loss": 0.2719,
      "step": 691
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.04562104120850563,
      "learning_rate": 0.00012345403899721448,
      "loss": 0.268,
      "step": 692
    },
    {
      "epoch": 1.155,
      "grad_norm": 0.05551109462976456,
      "learning_rate": 0.0001233426183844011,
      "loss": 0.3578,
      "step": 693
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.03982888534665108,
      "learning_rate": 0.00012323119777158775,
      "loss": 0.2445,
      "step": 694
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.04497479274868965,
      "learning_rate": 0.00012311977715877438,
      "loss": 0.2381,
      "step": 695
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.05202760547399521,
      "learning_rate": 0.00012300835654596102,
      "loss": 0.3109,
      "step": 696
    },
    {
      "epoch": 1.1616666666666666,
      "grad_norm": 0.05425313487648964,
      "learning_rate": 0.00012289693593314763,
      "loss": 0.3047,
      "step": 697
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.043152473866939545,
      "learning_rate": 0.00012278551532033427,
      "loss": 0.3205,
      "step": 698
    },
    {
      "epoch": 1.165,
      "grad_norm": 0.04097166657447815,
      "learning_rate": 0.0001226740947075209,
      "loss": 0.2996,
      "step": 699
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.05484712868928909,
      "learning_rate": 0.0001225626740947075,
      "loss": 0.2938,
      "step": 700
    },
    {
      "epoch": 1.1666666666666667,
      "eval_loss": 0.3249437212944031,
      "eval_runtime": 373.6794,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 700
    },
    {
      "epoch": 1.1683333333333334,
      "grad_norm": 0.04848772659897804,
      "learning_rate": 0.00012245125348189415,
      "loss": 0.3363,
      "step": 701
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.04571622982621193,
      "learning_rate": 0.00012233983286908079,
      "loss": 0.2947,
      "step": 702
    },
    {
      "epoch": 1.1716666666666666,
      "grad_norm": 0.05122512951493263,
      "learning_rate": 0.00012222841225626742,
      "loss": 0.2932,
      "step": 703
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.04593171924352646,
      "learning_rate": 0.00012211699164345406,
      "loss": 0.2591,
      "step": 704
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.059837035834789276,
      "learning_rate": 0.00012200557103064068,
      "loss": 0.337,
      "step": 705
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.055959466844797134,
      "learning_rate": 0.0001218941504178273,
      "loss": 0.3511,
      "step": 706
    },
    {
      "epoch": 1.1783333333333332,
      "grad_norm": 0.038410793989896774,
      "learning_rate": 0.00012178272980501393,
      "loss": 0.2529,
      "step": 707
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.06779003888368607,
      "learning_rate": 0.00012167130919220055,
      "loss": 0.3749,
      "step": 708
    },
    {
      "epoch": 1.1816666666666666,
      "grad_norm": 0.06086283177137375,
      "learning_rate": 0.00012155988857938719,
      "loss": 0.3096,
      "step": 709
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.06595747172832489,
      "learning_rate": 0.00012144846796657384,
      "loss": 0.3591,
      "step": 710
    },
    {
      "epoch": 1.185,
      "grad_norm": 0.054579734802246094,
      "learning_rate": 0.00012133704735376046,
      "loss": 0.3346,
      "step": 711
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.05444353446364403,
      "learning_rate": 0.00012122562674094708,
      "loss": 0.3249,
      "step": 712
    },
    {
      "epoch": 1.1883333333333332,
      "grad_norm": 0.05019025877118111,
      "learning_rate": 0.0001211142061281337,
      "loss": 0.3118,
      "step": 713
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.04386289417743683,
      "learning_rate": 0.00012100278551532034,
      "loss": 0.3329,
      "step": 714
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.050726648420095444,
      "learning_rate": 0.00012089136490250697,
      "loss": 0.3133,
      "step": 715
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.05836929753422737,
      "learning_rate": 0.00012077994428969359,
      "loss": 0.3348,
      "step": 716
    },
    {
      "epoch": 1.195,
      "grad_norm": 0.06014572083950043,
      "learning_rate": 0.00012066852367688022,
      "loss": 0.3553,
      "step": 717
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.0473906435072422,
      "learning_rate": 0.00012055710306406686,
      "loss": 0.3165,
      "step": 718
    },
    {
      "epoch": 1.1983333333333333,
      "grad_norm": 0.04099196940660477,
      "learning_rate": 0.0001204456824512535,
      "loss": 0.3445,
      "step": 719
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.051116038113832474,
      "learning_rate": 0.00012033426183844012,
      "loss": 0.3094,
      "step": 720
    },
    {
      "epoch": 1.2016666666666667,
      "grad_norm": 0.05735861882567406,
      "learning_rate": 0.00012022284122562674,
      "loss": 0.336,
      "step": 721
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.05691840872168541,
      "learning_rate": 0.00012011142061281338,
      "loss": 0.3509,
      "step": 722
    },
    {
      "epoch": 1.205,
      "grad_norm": 0.06288771331310272,
      "learning_rate": 0.00012,
      "loss": 0.3615,
      "step": 723
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.0397581048309803,
      "learning_rate": 0.00011988857938718663,
      "loss": 0.2513,
      "step": 724
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.059119731187820435,
      "learning_rate": 0.00011977715877437325,
      "loss": 0.3134,
      "step": 725
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.03636254742741585,
      "learning_rate": 0.0001196657381615599,
      "loss": 0.2961,
      "step": 726
    },
    {
      "epoch": 1.2116666666666667,
      "grad_norm": 0.038502391427755356,
      "learning_rate": 0.00011955431754874654,
      "loss": 0.235,
      "step": 727
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.07571970671415329,
      "learning_rate": 0.00011944289693593316,
      "loss": 0.3192,
      "step": 728
    },
    {
      "epoch": 1.215,
      "grad_norm": 0.04825633019208908,
      "learning_rate": 0.00011933147632311978,
      "loss": 0.2901,
      "step": 729
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.04018886014819145,
      "learning_rate": 0.0001192200557103064,
      "loss": 0.2673,
      "step": 730
    },
    {
      "epoch": 1.2183333333333333,
      "grad_norm": 0.04971102625131607,
      "learning_rate": 0.00011910863509749304,
      "loss": 0.3042,
      "step": 731
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.04221099615097046,
      "learning_rate": 0.00011899721448467966,
      "loss": 0.2827,
      "step": 732
    },
    {
      "epoch": 1.2216666666666667,
      "grad_norm": 0.06669100373983383,
      "learning_rate": 0.00011888579387186629,
      "loss": 0.3748,
      "step": 733
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.0487256795167923,
      "learning_rate": 0.00011877437325905294,
      "loss": 0.3002,
      "step": 734
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.05250631272792816,
      "learning_rate": 0.00011866295264623957,
      "loss": 0.3375,
      "step": 735
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.053504686802625656,
      "learning_rate": 0.0001185515320334262,
      "loss": 0.3242,
      "step": 736
    },
    {
      "epoch": 1.2283333333333333,
      "grad_norm": 0.061938539147377014,
      "learning_rate": 0.00011844011142061282,
      "loss": 0.3111,
      "step": 737
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.07530166953802109,
      "learning_rate": 0.00011832869080779944,
      "loss": 0.3552,
      "step": 738
    },
    {
      "epoch": 1.2316666666666667,
      "grad_norm": 0.05315600335597992,
      "learning_rate": 0.00011821727019498608,
      "loss": 0.2973,
      "step": 739
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.08840741962194443,
      "learning_rate": 0.0001181058495821727,
      "loss": 0.3165,
      "step": 740
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 0.06103053316473961,
      "learning_rate": 0.00011799442896935932,
      "loss": 0.3917,
      "step": 741
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 0.037969931960105896,
      "learning_rate": 0.00011788300835654597,
      "loss": 0.2706,
      "step": 742
    },
    {
      "epoch": 1.2383333333333333,
      "grad_norm": 0.05460311099886894,
      "learning_rate": 0.0001177715877437326,
      "loss": 0.3068,
      "step": 743
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.05513601377606392,
      "learning_rate": 0.00011766016713091923,
      "loss": 0.3605,
      "step": 744
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.0642230361700058,
      "learning_rate": 0.00011754874651810586,
      "loss": 0.3298,
      "step": 745
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.047271471470594406,
      "learning_rate": 0.00011743732590529248,
      "loss": 0.3171,
      "step": 746
    },
    {
      "epoch": 1.245,
      "grad_norm": 0.0600745715200901,
      "learning_rate": 0.00011732590529247912,
      "loss": 0.3494,
      "step": 747
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.045074548572301865,
      "learning_rate": 0.00011721448467966574,
      "loss": 0.2922,
      "step": 748
    },
    {
      "epoch": 1.2483333333333333,
      "grad_norm": 0.043050993233919144,
      "learning_rate": 0.00011710306406685236,
      "loss": 0.2634,
      "step": 749
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.08761952072381973,
      "learning_rate": 0.00011699164345403901,
      "loss": 0.4191,
      "step": 750
    },
    {
      "epoch": 1.2516666666666667,
      "grad_norm": 0.05572642385959625,
      "learning_rate": 0.00011688022284122563,
      "loss": 0.3562,
      "step": 751
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.047182947397232056,
      "learning_rate": 0.00011676880222841227,
      "loss": 0.3483,
      "step": 752
    },
    {
      "epoch": 1.255,
      "grad_norm": 0.0442778579890728,
      "learning_rate": 0.0001166573816155989,
      "loss": 0.2758,
      "step": 753
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.04973537102341652,
      "learning_rate": 0.00011654596100278552,
      "loss": 0.321,
      "step": 754
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.04727261886000633,
      "learning_rate": 0.00011643454038997214,
      "loss": 0.3255,
      "step": 755
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.0512387789785862,
      "learning_rate": 0.00011632311977715878,
      "loss": 0.2962,
      "step": 756
    },
    {
      "epoch": 1.2616666666666667,
      "grad_norm": 0.041468240320682526,
      "learning_rate": 0.0001162116991643454,
      "loss": 0.3404,
      "step": 757
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.054209623485803604,
      "learning_rate": 0.00011610027855153205,
      "loss": 0.3585,
      "step": 758
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 0.09473873674869537,
      "learning_rate": 0.00011598885793871867,
      "loss": 0.377,
      "step": 759
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.053749192506074905,
      "learning_rate": 0.00011587743732590531,
      "loss": 0.3183,
      "step": 760
    },
    {
      "epoch": 1.2683333333333333,
      "grad_norm": 0.04816459119319916,
      "learning_rate": 0.00011576601671309193,
      "loss": 0.3077,
      "step": 761
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.050339337438344955,
      "learning_rate": 0.00011565459610027855,
      "loss": 0.3323,
      "step": 762
    },
    {
      "epoch": 1.2716666666666667,
      "grad_norm": 0.04293170943856239,
      "learning_rate": 0.00011554317548746518,
      "loss": 0.3046,
      "step": 763
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.03964214399456978,
      "learning_rate": 0.00011543175487465181,
      "loss": 0.2726,
      "step": 764
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.053622957319021225,
      "learning_rate": 0.00011532033426183844,
      "loss": 0.3441,
      "step": 765
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.060688044875860214,
      "learning_rate": 0.00011520891364902509,
      "loss": 0.3651,
      "step": 766
    },
    {
      "epoch": 1.2783333333333333,
      "grad_norm": 0.04049067944288254,
      "learning_rate": 0.00011509749303621171,
      "loss": 0.2514,
      "step": 767
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.04279336705803871,
      "learning_rate": 0.00011498607242339833,
      "loss": 0.3028,
      "step": 768
    },
    {
      "epoch": 1.2816666666666667,
      "grad_norm": 0.03884007781744003,
      "learning_rate": 0.00011487465181058497,
      "loss": 0.2732,
      "step": 769
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.04321003705263138,
      "learning_rate": 0.00011476323119777159,
      "loss": 0.297,
      "step": 770
    },
    {
      "epoch": 1.285,
      "grad_norm": 0.04297022521495819,
      "learning_rate": 0.00011465181058495822,
      "loss": 0.3093,
      "step": 771
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.055295005440711975,
      "learning_rate": 0.00011454038997214485,
      "loss": 0.3599,
      "step": 772
    },
    {
      "epoch": 1.2883333333333333,
      "grad_norm": 0.04671638831496239,
      "learning_rate": 0.00011442896935933147,
      "loss": 0.3178,
      "step": 773
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.047454483807086945,
      "learning_rate": 0.00011431754874651812,
      "loss": 0.2839,
      "step": 774
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.04256737232208252,
      "learning_rate": 0.00011420612813370475,
      "loss": 0.2889,
      "step": 775
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.06473991274833679,
      "learning_rate": 0.00011409470752089137,
      "loss": 0.4239,
      "step": 776
    },
    {
      "epoch": 1.295,
      "grad_norm": 0.05683406814932823,
      "learning_rate": 0.00011398328690807801,
      "loss": 0.3071,
      "step": 777
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.05049443989992142,
      "learning_rate": 0.00011387186629526463,
      "loss": 0.3661,
      "step": 778
    },
    {
      "epoch": 1.2983333333333333,
      "grad_norm": 0.05286366119980812,
      "learning_rate": 0.00011376044568245125,
      "loss": 0.3027,
      "step": 779
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.0628724992275238,
      "learning_rate": 0.00011364902506963788,
      "loss": 0.405,
      "step": 780
    },
    {
      "epoch": 1.3016666666666667,
      "grad_norm": 0.05469696968793869,
      "learning_rate": 0.00011353760445682451,
      "loss": 0.3684,
      "step": 781
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.06705069541931152,
      "learning_rate": 0.00011342618384401116,
      "loss": 0.3751,
      "step": 782
    },
    {
      "epoch": 1.305,
      "grad_norm": 0.09377814084291458,
      "learning_rate": 0.00011331476323119779,
      "loss": 0.3359,
      "step": 783
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.048227708786726,
      "learning_rate": 0.00011320334261838441,
      "loss": 0.2308,
      "step": 784
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.049991101026535034,
      "learning_rate": 0.00011309192200557104,
      "loss": 0.3131,
      "step": 785
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.05291355028748512,
      "learning_rate": 0.00011298050139275767,
      "loss": 0.2895,
      "step": 786
    },
    {
      "epoch": 1.3116666666666665,
      "grad_norm": 0.04944576695561409,
      "learning_rate": 0.00011286908077994429,
      "loss": 0.2845,
      "step": 787
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.06204497069120407,
      "learning_rate": 0.00011275766016713091,
      "loss": 0.3096,
      "step": 788
    },
    {
      "epoch": 1.315,
      "grad_norm": 0.05431002005934715,
      "learning_rate": 0.00011264623955431755,
      "loss": 0.3263,
      "step": 789
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.04042857140302658,
      "learning_rate": 0.0001125348189415042,
      "loss": 0.262,
      "step": 790
    },
    {
      "epoch": 1.3183333333333334,
      "grad_norm": 0.06940200179815292,
      "learning_rate": 0.00011242339832869082,
      "loss": 0.3723,
      "step": 791
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.05204876884818077,
      "learning_rate": 0.00011231197771587745,
      "loss": 0.3301,
      "step": 792
    },
    {
      "epoch": 1.3216666666666668,
      "grad_norm": 0.040174610912799835,
      "learning_rate": 0.00011220055710306407,
      "loss": 0.2486,
      "step": 793
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.0440058633685112,
      "learning_rate": 0.0001120891364902507,
      "loss": 0.2952,
      "step": 794
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.08946824073791504,
      "learning_rate": 0.00011197771587743733,
      "loss": 0.36,
      "step": 795
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.0423639714717865,
      "learning_rate": 0.00011186629526462395,
      "loss": 0.2777,
      "step": 796
    },
    {
      "epoch": 1.3283333333333334,
      "grad_norm": 0.05461345985531807,
      "learning_rate": 0.00011175487465181059,
      "loss": 0.356,
      "step": 797
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.07325074076652527,
      "learning_rate": 0.00011164345403899722,
      "loss": 0.3202,
      "step": 798
    },
    {
      "epoch": 1.3316666666666666,
      "grad_norm": 0.05088663473725319,
      "learning_rate": 0.00011153203342618386,
      "loss": 0.3882,
      "step": 799
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.03988111764192581,
      "learning_rate": 0.00011142061281337048,
      "loss": 0.2689,
      "step": 800
    },
    {
      "epoch": 1.3333333333333333,
      "eval_loss": 0.3240317404270172,
      "eval_runtime": 373.4348,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 800
    },
    {
      "epoch": 1.335,
      "grad_norm": 0.057641930878162384,
      "learning_rate": 0.0001113091922005571,
      "loss": 0.3424,
      "step": 801
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.04876812547445297,
      "learning_rate": 0.00011119777158774374,
      "loss": 0.2695,
      "step": 802
    },
    {
      "epoch": 1.3383333333333334,
      "grad_norm": 0.046650100499391556,
      "learning_rate": 0.00011108635097493037,
      "loss": 0.3129,
      "step": 803
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.06726518273353577,
      "learning_rate": 0.00011097493036211699,
      "loss": 0.2987,
      "step": 804
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.04402477294206619,
      "learning_rate": 0.00011086350974930361,
      "loss": 0.2983,
      "step": 805
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.0868586078286171,
      "learning_rate": 0.00011075208913649026,
      "loss": 0.3249,
      "step": 806
    },
    {
      "epoch": 1.345,
      "grad_norm": 0.07503216713666916,
      "learning_rate": 0.0001106406685236769,
      "loss": 0.3765,
      "step": 807
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.06562774628400803,
      "learning_rate": 0.00011052924791086352,
      "loss": 0.315,
      "step": 808
    },
    {
      "epoch": 1.3483333333333334,
      "grad_norm": 0.04028520733118057,
      "learning_rate": 0.00011041782729805014,
      "loss": 0.2924,
      "step": 809
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.04509858042001724,
      "learning_rate": 0.00011030640668523677,
      "loss": 0.3295,
      "step": 810
    },
    {
      "epoch": 1.3516666666666666,
      "grad_norm": 0.06333824247121811,
      "learning_rate": 0.0001101949860724234,
      "loss": 0.3175,
      "step": 811
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.059569306671619415,
      "learning_rate": 0.00011008356545961003,
      "loss": 0.3195,
      "step": 812
    },
    {
      "epoch": 1.355,
      "grad_norm": 0.03712876886129379,
      "learning_rate": 0.00010997214484679665,
      "loss": 0.2937,
      "step": 813
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 0.04424058645963669,
      "learning_rate": 0.0001098607242339833,
      "loss": 0.309,
      "step": 814
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.05646035447716713,
      "learning_rate": 0.00010974930362116994,
      "loss": 0.3453,
      "step": 815
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.04758371412754059,
      "learning_rate": 0.00010963788300835656,
      "loss": 0.2891,
      "step": 816
    },
    {
      "epoch": 1.3616666666666668,
      "grad_norm": 0.05409663915634155,
      "learning_rate": 0.00010952646239554318,
      "loss": 0.3051,
      "step": 817
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.04837237671017647,
      "learning_rate": 0.0001094150417827298,
      "loss": 0.325,
      "step": 818
    },
    {
      "epoch": 1.365,
      "grad_norm": 0.03936009854078293,
      "learning_rate": 0.00010930362116991644,
      "loss": 0.2759,
      "step": 819
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.05668478459119797,
      "learning_rate": 0.00010919220055710306,
      "loss": 0.3206,
      "step": 820
    },
    {
      "epoch": 1.3683333333333334,
      "grad_norm": 0.06145121902227402,
      "learning_rate": 0.00010908077994428969,
      "loss": 0.3111,
      "step": 821
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.049643322825431824,
      "learning_rate": 0.00010896935933147632,
      "loss": 0.3238,
      "step": 822
    },
    {
      "epoch": 1.3716666666666666,
      "grad_norm": 0.04672195389866829,
      "learning_rate": 0.00010885793871866296,
      "loss": 0.2782,
      "step": 823
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.054078109562397,
      "learning_rate": 0.0001087465181058496,
      "loss": 0.317,
      "step": 824
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.08385499566793442,
      "learning_rate": 0.00010863509749303622,
      "loss": 0.3933,
      "step": 825
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.05065348371863365,
      "learning_rate": 0.00010852367688022284,
      "loss": 0.3226,
      "step": 826
    },
    {
      "epoch": 1.3783333333333334,
      "grad_norm": 0.06942520290613174,
      "learning_rate": 0.00010841225626740948,
      "loss": 0.3676,
      "step": 827
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.058958858251571655,
      "learning_rate": 0.0001083008356545961,
      "loss": 0.3398,
      "step": 828
    },
    {
      "epoch": 1.3816666666666666,
      "grad_norm": 0.0396934412419796,
      "learning_rate": 0.00010818941504178272,
      "loss": 0.2705,
      "step": 829
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.036017633974552155,
      "learning_rate": 0.00010807799442896935,
      "loss": 0.2296,
      "step": 830
    },
    {
      "epoch": 1.385,
      "grad_norm": 0.06789635866880417,
      "learning_rate": 0.000107966573816156,
      "loss": 0.4215,
      "step": 831
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.04464036226272583,
      "learning_rate": 0.00010785515320334263,
      "loss": 0.2796,
      "step": 832
    },
    {
      "epoch": 1.3883333333333332,
      "grad_norm": 0.038351599127054214,
      "learning_rate": 0.00010774373259052926,
      "loss": 0.2456,
      "step": 833
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.07164964079856873,
      "learning_rate": 0.00010763231197771588,
      "loss": 0.3818,
      "step": 834
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.054553356021642685,
      "learning_rate": 0.0001075208913649025,
      "loss": 0.2563,
      "step": 835
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.03486122563481331,
      "learning_rate": 0.00010740947075208914,
      "loss": 0.2593,
      "step": 836
    },
    {
      "epoch": 1.395,
      "grad_norm": 0.07681387662887573,
      "learning_rate": 0.00010729805013927576,
      "loss": 0.3424,
      "step": 837
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.08190979063510895,
      "learning_rate": 0.00010718662952646239,
      "loss": 0.3014,
      "step": 838
    },
    {
      "epoch": 1.3983333333333334,
      "grad_norm": 0.0377548523247242,
      "learning_rate": 0.00010707520891364904,
      "loss": 0.2923,
      "step": 839
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.04089314863085747,
      "learning_rate": 0.00010696378830083567,
      "loss": 0.2631,
      "step": 840
    },
    {
      "epoch": 1.4016666666666666,
      "grad_norm": 0.048210352659225464,
      "learning_rate": 0.0001068523676880223,
      "loss": 0.3364,
      "step": 841
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.06772482395172119,
      "learning_rate": 0.00010674094707520892,
      "loss": 0.3291,
      "step": 842
    },
    {
      "epoch": 1.405,
      "grad_norm": 0.043270956724882126,
      "learning_rate": 0.00010662952646239554,
      "loss": 0.304,
      "step": 843
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.04654589667916298,
      "learning_rate": 0.00010651810584958218,
      "loss": 0.3173,
      "step": 844
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.04368129372596741,
      "learning_rate": 0.0001064066852367688,
      "loss": 0.3324,
      "step": 845
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.048325065523386,
      "learning_rate": 0.00010629526462395542,
      "loss": 0.3019,
      "step": 846
    },
    {
      "epoch": 1.4116666666666666,
      "grad_norm": 0.04930404946208,
      "learning_rate": 0.00010618384401114207,
      "loss": 0.2915,
      "step": 847
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.05734408646821976,
      "learning_rate": 0.0001060724233983287,
      "loss": 0.2681,
      "step": 848
    },
    {
      "epoch": 1.415,
      "grad_norm": 0.0637495219707489,
      "learning_rate": 0.00010596100278551533,
      "loss": 0.3462,
      "step": 849
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.04086415842175484,
      "learning_rate": 0.00010584958217270196,
      "loss": 0.2709,
      "step": 850
    },
    {
      "epoch": 1.4183333333333334,
      "grad_norm": 0.052036724984645844,
      "learning_rate": 0.00010573816155988858,
      "loss": 0.2994,
      "step": 851
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.04468335583806038,
      "learning_rate": 0.00010562674094707521,
      "loss": 0.3165,
      "step": 852
    },
    {
      "epoch": 1.4216666666666666,
      "grad_norm": 0.03759390488266945,
      "learning_rate": 0.00010551532033426184,
      "loss": 0.2365,
      "step": 853
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.04838431254029274,
      "learning_rate": 0.00010540389972144846,
      "loss": 0.3243,
      "step": 854
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.07530464231967926,
      "learning_rate": 0.00010529247910863511,
      "loss": 0.4463,
      "step": 855
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.04751398414373398,
      "learning_rate": 0.00010518105849582173,
      "loss": 0.317,
      "step": 856
    },
    {
      "epoch": 1.4283333333333332,
      "grad_norm": 0.045988310128450394,
      "learning_rate": 0.00010506963788300837,
      "loss": 0.2684,
      "step": 857
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.06337545067071915,
      "learning_rate": 0.00010495821727019499,
      "loss": 0.3509,
      "step": 858
    },
    {
      "epoch": 1.4316666666666666,
      "grad_norm": 0.06959738582372665,
      "learning_rate": 0.00010484679665738162,
      "loss": 0.3598,
      "step": 859
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.04338691011071205,
      "learning_rate": 0.00010473537604456824,
      "loss": 0.2431,
      "step": 860
    },
    {
      "epoch": 1.435,
      "grad_norm": 0.045629557222127914,
      "learning_rate": 0.00010462395543175488,
      "loss": 0.3453,
      "step": 861
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.06907180696725845,
      "learning_rate": 0.0001045125348189415,
      "loss": 0.3901,
      "step": 862
    },
    {
      "epoch": 1.4383333333333335,
      "grad_norm": 0.05170123651623726,
      "learning_rate": 0.00010440111420612815,
      "loss": 0.3391,
      "step": 863
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.05284053087234497,
      "learning_rate": 0.00010428969359331477,
      "loss": 0.3646,
      "step": 864
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.047487154603004456,
      "learning_rate": 0.00010417827298050141,
      "loss": 0.3285,
      "step": 865
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 0.034828055649995804,
      "learning_rate": 0.00010406685236768803,
      "loss": 0.2746,
      "step": 866
    },
    {
      "epoch": 1.445,
      "grad_norm": 0.03969263657927513,
      "learning_rate": 0.00010395543175487465,
      "loss": 0.255,
      "step": 867
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.046008750796318054,
      "learning_rate": 0.00010384401114206128,
      "loss": 0.2987,
      "step": 868
    },
    {
      "epoch": 1.4483333333333333,
      "grad_norm": 0.05475010350346565,
      "learning_rate": 0.00010373259052924791,
      "loss": 0.3364,
      "step": 869
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.05276951193809509,
      "learning_rate": 0.00010362116991643454,
      "loss": 0.3285,
      "step": 870
    },
    {
      "epoch": 1.4516666666666667,
      "grad_norm": 0.0450415275990963,
      "learning_rate": 0.00010350974930362119,
      "loss": 0.3304,
      "step": 871
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.044550783932209015,
      "learning_rate": 0.00010339832869080781,
      "loss": 0.3144,
      "step": 872
    },
    {
      "epoch": 1.455,
      "grad_norm": 0.07092040777206421,
      "learning_rate": 0.00010328690807799443,
      "loss": 0.3759,
      "step": 873
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.09034174680709839,
      "learning_rate": 0.00010317548746518107,
      "loss": 0.3133,
      "step": 874
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.050970710813999176,
      "learning_rate": 0.00010306406685236769,
      "loss": 0.3221,
      "step": 875
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.06288238614797592,
      "learning_rate": 0.00010295264623955431,
      "loss": 0.3128,
      "step": 876
    },
    {
      "epoch": 1.4616666666666667,
      "grad_norm": 0.04106535017490387,
      "learning_rate": 0.00010284122562674095,
      "loss": 0.3279,
      "step": 877
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.04693116992712021,
      "learning_rate": 0.00010272980501392757,
      "loss": 0.3173,
      "step": 878
    },
    {
      "epoch": 1.465,
      "grad_norm": 0.05071249604225159,
      "learning_rate": 0.00010261838440111422,
      "loss": 0.2986,
      "step": 879
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.049158524721860886,
      "learning_rate": 0.00010250696378830085,
      "loss": 0.2869,
      "step": 880
    },
    {
      "epoch": 1.4683333333333333,
      "grad_norm": 0.047643452882766724,
      "learning_rate": 0.00010239554317548747,
      "loss": 0.3447,
      "step": 881
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.03984949365258217,
      "learning_rate": 0.0001022841225626741,
      "loss": 0.2787,
      "step": 882
    },
    {
      "epoch": 1.4716666666666667,
      "grad_norm": 0.05677858367562294,
      "learning_rate": 0.00010217270194986073,
      "loss": 0.317,
      "step": 883
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.06137717887759209,
      "learning_rate": 0.00010206128133704735,
      "loss": 0.2886,
      "step": 884
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.08303935080766678,
      "learning_rate": 0.00010194986072423397,
      "loss": 0.4525,
      "step": 885
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.07649409770965576,
      "learning_rate": 0.00010183844011142061,
      "loss": 0.313,
      "step": 886
    },
    {
      "epoch": 1.4783333333333333,
      "grad_norm": 0.05235839635133743,
      "learning_rate": 0.00010172701949860726,
      "loss": 0.3392,
      "step": 887
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.0328575000166893,
      "learning_rate": 0.00010161559888579388,
      "loss": 0.2203,
      "step": 888
    },
    {
      "epoch": 1.4816666666666667,
      "grad_norm": 0.0566297210752964,
      "learning_rate": 0.00010150417827298051,
      "loss": 0.3188,
      "step": 889
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.05074988678097725,
      "learning_rate": 0.00010139275766016714,
      "loss": 0.3088,
      "step": 890
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 0.05383668094873428,
      "learning_rate": 0.00010128133704735377,
      "loss": 0.367,
      "step": 891
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.04733072966337204,
      "learning_rate": 0.00010116991643454039,
      "loss": 0.3143,
      "step": 892
    },
    {
      "epoch": 1.4883333333333333,
      "grad_norm": 0.05412575975060463,
      "learning_rate": 0.00010105849582172701,
      "loss": 0.3076,
      "step": 893
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.06799665093421936,
      "learning_rate": 0.00010094707520891365,
      "loss": 0.2191,
      "step": 894
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.04516544193029404,
      "learning_rate": 0.0001008356545961003,
      "loss": 0.2922,
      "step": 895
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.04372014105319977,
      "learning_rate": 0.00010072423398328692,
      "loss": 0.2163,
      "step": 896
    },
    {
      "epoch": 1.495,
      "grad_norm": 0.05170045047998428,
      "learning_rate": 0.00010061281337047354,
      "loss": 0.3305,
      "step": 897
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.07278403639793396,
      "learning_rate": 0.00010050139275766017,
      "loss": 0.3428,
      "step": 898
    },
    {
      "epoch": 1.4983333333333333,
      "grad_norm": 0.06453842669725418,
      "learning_rate": 0.0001003899721448468,
      "loss": 0.3022,
      "step": 899
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.03192227706313133,
      "learning_rate": 0.00010027855153203343,
      "loss": 0.2279,
      "step": 900
    },
    {
      "epoch": 1.5,
      "eval_loss": 0.3231646716594696,
      "eval_runtime": 373.5018,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 900
    },
    {
      "epoch": 1.5016666666666667,
      "grad_norm": 0.08311466127634048,
      "learning_rate": 0.00010016713091922005,
      "loss": 0.3975,
      "step": 901
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.04909651726484299,
      "learning_rate": 0.00010005571030640669,
      "loss": 0.2727,
      "step": 902
    },
    {
      "epoch": 1.505,
      "grad_norm": 0.05146733671426773,
      "learning_rate": 9.994428969359332e-05,
      "loss": 0.3075,
      "step": 903
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.07472844421863556,
      "learning_rate": 9.983286908077995e-05,
      "loss": 0.4154,
      "step": 904
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.06042633578181267,
      "learning_rate": 9.972144846796658e-05,
      "loss": 0.2946,
      "step": 905
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.06991257518529892,
      "learning_rate": 9.96100278551532e-05,
      "loss": 0.3545,
      "step": 906
    },
    {
      "epoch": 1.5116666666666667,
      "grad_norm": 0.06294330954551697,
      "learning_rate": 9.949860724233984e-05,
      "loss": 0.3697,
      "step": 907
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.07625393569469452,
      "learning_rate": 9.938718662952646e-05,
      "loss": 0.4117,
      "step": 908
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.04371468350291252,
      "learning_rate": 9.92757660167131e-05,
      "loss": 0.2897,
      "step": 909
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.06273914128541946,
      "learning_rate": 9.916434540389972e-05,
      "loss": 0.3959,
      "step": 910
    },
    {
      "epoch": 1.5183333333333333,
      "grad_norm": 0.06369846314191818,
      "learning_rate": 9.905292479108636e-05,
      "loss": 0.2974,
      "step": 911
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.047940727323293686,
      "learning_rate": 9.894150417827298e-05,
      "loss": 0.3107,
      "step": 912
    },
    {
      "epoch": 1.5216666666666665,
      "grad_norm": 0.05870036780834198,
      "learning_rate": 9.883008356545962e-05,
      "loss": 0.3385,
      "step": 913
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.058333318680524826,
      "learning_rate": 9.871866295264624e-05,
      "loss": 0.3476,
      "step": 914
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.0731189027428627,
      "learning_rate": 9.860724233983287e-05,
      "loss": 0.3844,
      "step": 915
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.05347345024347305,
      "learning_rate": 9.84958217270195e-05,
      "loss": 0.2947,
      "step": 916
    },
    {
      "epoch": 1.5283333333333333,
      "grad_norm": 0.07489851862192154,
      "learning_rate": 9.838440111420614e-05,
      "loss": 0.3756,
      "step": 917
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.05343102291226387,
      "learning_rate": 9.827298050139276e-05,
      "loss": 0.3221,
      "step": 918
    },
    {
      "epoch": 1.5316666666666667,
      "grad_norm": 0.06133120507001877,
      "learning_rate": 9.816155988857938e-05,
      "loss": 0.3272,
      "step": 919
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.06399228423833847,
      "learning_rate": 9.805013927576602e-05,
      "loss": 0.4005,
      "step": 920
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 0.08236099779605865,
      "learning_rate": 9.793871866295266e-05,
      "loss": 0.4026,
      "step": 921
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.062330517917871475,
      "learning_rate": 9.782729805013928e-05,
      "loss": 0.3307,
      "step": 922
    },
    {
      "epoch": 1.5383333333333333,
      "grad_norm": 0.04514957219362259,
      "learning_rate": 9.77158774373259e-05,
      "loss": 0.3035,
      "step": 923
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.07358008623123169,
      "learning_rate": 9.760445682451254e-05,
      "loss": 0.3607,
      "step": 924
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.056394483894109726,
      "learning_rate": 9.749303621169918e-05,
      "loss": 0.3375,
      "step": 925
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 0.050410106778144836,
      "learning_rate": 9.73816155988858e-05,
      "loss": 0.3621,
      "step": 926
    },
    {
      "epoch": 1.545,
      "grad_norm": 0.05911073088645935,
      "learning_rate": 9.727019498607242e-05,
      "loss": 0.2999,
      "step": 927
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.09308454394340515,
      "learning_rate": 9.715877437325906e-05,
      "loss": 0.3752,
      "step": 928
    },
    {
      "epoch": 1.5483333333333333,
      "grad_norm": 0.05108194798231125,
      "learning_rate": 9.70473537604457e-05,
      "loss": 0.3351,
      "step": 929
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.038478974252939224,
      "learning_rate": 9.693593314763232e-05,
      "loss": 0.2626,
      "step": 930
    },
    {
      "epoch": 1.5516666666666667,
      "grad_norm": 0.05561017990112305,
      "learning_rate": 9.682451253481894e-05,
      "loss": 0.3155,
      "step": 931
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.058376245200634,
      "learning_rate": 9.671309192200558e-05,
      "loss": 0.3226,
      "step": 932
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 0.054469939321279526,
      "learning_rate": 9.660167130919221e-05,
      "loss": 0.2955,
      "step": 933
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.048533570021390915,
      "learning_rate": 9.649025069637884e-05,
      "loss": 0.3053,
      "step": 934
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.046764180064201355,
      "learning_rate": 9.637883008356546e-05,
      "loss": 0.3363,
      "step": 935
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05811616778373718,
      "learning_rate": 9.62674094707521e-05,
      "loss": 0.3371,
      "step": 936
    },
    {
      "epoch": 1.5616666666666665,
      "grad_norm": 0.05568284913897514,
      "learning_rate": 9.615598885793873e-05,
      "loss": 0.3529,
      "step": 937
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.058249302208423615,
      "learning_rate": 9.604456824512536e-05,
      "loss": 0.3825,
      "step": 938
    },
    {
      "epoch": 1.565,
      "grad_norm": 0.045777395367622375,
      "learning_rate": 9.593314763231198e-05,
      "loss": 0.309,
      "step": 939
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.053043436259031296,
      "learning_rate": 9.58217270194986e-05,
      "loss": 0.3832,
      "step": 940
    },
    {
      "epoch": 1.5683333333333334,
      "grad_norm": 0.08452142030000687,
      "learning_rate": 9.571030640668525e-05,
      "loss": 0.3818,
      "step": 941
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.06124549359083176,
      "learning_rate": 9.559888579387187e-05,
      "loss": 0.2691,
      "step": 942
    },
    {
      "epoch": 1.5716666666666668,
      "grad_norm": 0.05381093546748161,
      "learning_rate": 9.54874651810585e-05,
      "loss": 0.3318,
      "step": 943
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.04413091391324997,
      "learning_rate": 9.537604456824512e-05,
      "loss": 0.2974,
      "step": 944
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.07002463191747665,
      "learning_rate": 9.526462395543177e-05,
      "loss": 0.3627,
      "step": 945
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.057526927441358566,
      "learning_rate": 9.51532033426184e-05,
      "loss": 0.2648,
      "step": 946
    },
    {
      "epoch": 1.5783333333333334,
      "grad_norm": 0.03448295593261719,
      "learning_rate": 9.504178272980502e-05,
      "loss": 0.2861,
      "step": 947
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.04271212965250015,
      "learning_rate": 9.493036211699164e-05,
      "loss": 0.2355,
      "step": 948
    },
    {
      "epoch": 1.5816666666666666,
      "grad_norm": 0.048834070563316345,
      "learning_rate": 9.481894150417828e-05,
      "loss": 0.3116,
      "step": 949
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.032471898943185806,
      "learning_rate": 9.470752089136491e-05,
      "loss": 0.2525,
      "step": 950
    },
    {
      "epoch": 1.585,
      "grad_norm": 0.04324181377887726,
      "learning_rate": 9.459610027855154e-05,
      "loss": 0.3101,
      "step": 951
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.04475460946559906,
      "learning_rate": 9.448467966573816e-05,
      "loss": 0.3222,
      "step": 952
    },
    {
      "epoch": 1.5883333333333334,
      "grad_norm": 0.04748489707708359,
      "learning_rate": 9.43732590529248e-05,
      "loss": 0.3361,
      "step": 953
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.04902688413858414,
      "learning_rate": 9.426183844011143e-05,
      "loss": 0.3356,
      "step": 954
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.046897415071725845,
      "learning_rate": 9.415041782729805e-05,
      "loss": 0.3243,
      "step": 955
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.04333881661295891,
      "learning_rate": 9.403899721448468e-05,
      "loss": 0.2644,
      "step": 956
    },
    {
      "epoch": 1.595,
      "grad_norm": 0.04235534369945526,
      "learning_rate": 9.392757660167131e-05,
      "loss": 0.3333,
      "step": 957
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.05769545957446098,
      "learning_rate": 9.381615598885795e-05,
      "loss": 0.3247,
      "step": 958
    },
    {
      "epoch": 1.5983333333333334,
      "grad_norm": 0.046658098697662354,
      "learning_rate": 9.370473537604457e-05,
      "loss": 0.3516,
      "step": 959
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.06959307938814163,
      "learning_rate": 9.35933147632312e-05,
      "loss": 0.3301,
      "step": 960
    },
    {
      "epoch": 1.6016666666666666,
      "grad_norm": 0.07982617616653442,
      "learning_rate": 9.348189415041783e-05,
      "loss": 0.3099,
      "step": 961
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.053461212664842606,
      "learning_rate": 9.337047353760447e-05,
      "loss": 0.3332,
      "step": 962
    },
    {
      "epoch": 1.605,
      "grad_norm": 0.051453981548547745,
      "learning_rate": 9.325905292479109e-05,
      "loss": 0.3461,
      "step": 963
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.041737012565135956,
      "learning_rate": 9.314763231197771e-05,
      "loss": 0.2854,
      "step": 964
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.05738230049610138,
      "learning_rate": 9.303621169916435e-05,
      "loss": 0.3037,
      "step": 965
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.0427318811416626,
      "learning_rate": 9.292479108635099e-05,
      "loss": 0.3207,
      "step": 966
    },
    {
      "epoch": 1.6116666666666668,
      "grad_norm": 0.06356322020292282,
      "learning_rate": 9.281337047353761e-05,
      "loss": 0.3438,
      "step": 967
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.08278052508831024,
      "learning_rate": 9.270194986072423e-05,
      "loss": 0.3949,
      "step": 968
    },
    {
      "epoch": 1.615,
      "grad_norm": 0.04347321018576622,
      "learning_rate": 9.259052924791087e-05,
      "loss": 0.2636,
      "step": 969
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.050444383174180984,
      "learning_rate": 9.24791086350975e-05,
      "loss": 0.2684,
      "step": 970
    },
    {
      "epoch": 1.6183333333333332,
      "grad_norm": 0.048784345388412476,
      "learning_rate": 9.236768802228413e-05,
      "loss": 0.3078,
      "step": 971
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.07021452486515045,
      "learning_rate": 9.225626740947075e-05,
      "loss": 0.3033,
      "step": 972
    },
    {
      "epoch": 1.6216666666666666,
      "grad_norm": 0.04334842041134834,
      "learning_rate": 9.214484679665739e-05,
      "loss": 0.2498,
      "step": 973
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.05947504937648773,
      "learning_rate": 9.203342618384401e-05,
      "loss": 0.3875,
      "step": 974
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.05991059169173241,
      "learning_rate": 9.192200557103065e-05,
      "loss": 0.3127,
      "step": 975
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.040278319269418716,
      "learning_rate": 9.181058495821727e-05,
      "loss": 0.258,
      "step": 976
    },
    {
      "epoch": 1.6283333333333334,
      "grad_norm": 0.0503925159573555,
      "learning_rate": 9.169916434540391e-05,
      "loss": 0.2774,
      "step": 977
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.048469800502061844,
      "learning_rate": 9.158774373259053e-05,
      "loss": 0.334,
      "step": 978
    },
    {
      "epoch": 1.6316666666666668,
      "grad_norm": 0.03953427076339722,
      "learning_rate": 9.147632311977717e-05,
      "loss": 0.2564,
      "step": 979
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.0364089198410511,
      "learning_rate": 9.136490250696379e-05,
      "loss": 0.2619,
      "step": 980
    },
    {
      "epoch": 1.635,
      "grad_norm": 0.04198732599616051,
      "learning_rate": 9.125348189415043e-05,
      "loss": 0.3091,
      "step": 981
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.05511907860636711,
      "learning_rate": 9.114206128133705e-05,
      "loss": 0.3458,
      "step": 982
    },
    {
      "epoch": 1.6383333333333332,
      "grad_norm": 0.05258199945092201,
      "learning_rate": 9.103064066852369e-05,
      "loss": 0.3078,
      "step": 983
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.04512585327029228,
      "learning_rate": 9.091922005571031e-05,
      "loss": 0.3174,
      "step": 984
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.06326772272586823,
      "learning_rate": 9.080779944289693e-05,
      "loss": 0.3473,
      "step": 985
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.0529300831258297,
      "learning_rate": 9.069637883008357e-05,
      "loss": 0.3495,
      "step": 986
    },
    {
      "epoch": 1.645,
      "grad_norm": 0.04727138206362724,
      "learning_rate": 9.05849582172702e-05,
      "loss": 0.3098,
      "step": 987
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.07397496700286865,
      "learning_rate": 9.047353760445683e-05,
      "loss": 0.3076,
      "step": 988
    },
    {
      "epoch": 1.6483333333333334,
      "grad_norm": 0.04864749312400818,
      "learning_rate": 9.036211699164345e-05,
      "loss": 0.2394,
      "step": 989
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.044533081352710724,
      "learning_rate": 9.025069637883009e-05,
      "loss": 0.2835,
      "step": 990
    },
    {
      "epoch": 1.6516666666666666,
      "grad_norm": 0.054180171340703964,
      "learning_rate": 9.013927576601672e-05,
      "loss": 0.3572,
      "step": 991
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.03964740037918091,
      "learning_rate": 9.002785515320335e-05,
      "loss": 0.3126,
      "step": 992
    },
    {
      "epoch": 1.655,
      "grad_norm": 0.05723697692155838,
      "learning_rate": 8.991643454038997e-05,
      "loss": 0.338,
      "step": 993
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.060739897191524506,
      "learning_rate": 8.98050139275766e-05,
      "loss": 0.3024,
      "step": 994
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.04364289715886116,
      "learning_rate": 8.969359331476324e-05,
      "loss": 0.2868,
      "step": 995
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.03829342871904373,
      "learning_rate": 8.958217270194987e-05,
      "loss": 0.2455,
      "step": 996
    },
    {
      "epoch": 1.6616666666666666,
      "grad_norm": 0.07998751103878021,
      "learning_rate": 8.947075208913649e-05,
      "loss": 0.3685,
      "step": 997
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.03779842332005501,
      "learning_rate": 8.935933147632312e-05,
      "loss": 0.2718,
      "step": 998
    },
    {
      "epoch": 1.665,
      "grad_norm": 0.058501727879047394,
      "learning_rate": 8.924791086350975e-05,
      "loss": 0.316,
      "step": 999
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04681263864040375,
      "learning_rate": 8.913649025069638e-05,
      "loss": 0.2763,
      "step": 1000
    },
    {
      "epoch": 1.6666666666666665,
      "eval_loss": 0.3220631778240204,
      "eval_runtime": 373.5122,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.290439454302044e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
