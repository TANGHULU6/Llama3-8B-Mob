{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.22727272727272727,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00022727272727272727,
      "grad_norm": 1.45479416847229,
      "learning_rate": 4e-05,
      "loss": 1.8379,
      "step": 1
    },
    {
      "epoch": 0.00045454545454545455,
      "grad_norm": 1.5847634077072144,
      "learning_rate": 8e-05,
      "loss": 1.4501,
      "step": 2
    },
    {
      "epoch": 0.0006818181818181819,
      "grad_norm": 1.4285775423049927,
      "learning_rate": 0.00012,
      "loss": 1.4875,
      "step": 3
    },
    {
      "epoch": 0.0009090909090909091,
      "grad_norm": 1.595988154411316,
      "learning_rate": 0.00016,
      "loss": 1.3831,
      "step": 4
    },
    {
      "epoch": 0.0011363636363636363,
      "grad_norm": 1.4070570468902588,
      "learning_rate": 0.0002,
      "loss": 1.0971,
      "step": 5
    },
    {
      "epoch": 0.0013636363636363637,
      "grad_norm": 1.399474024772644,
      "learning_rate": 0.00019995449374288966,
      "loss": 1.0246,
      "step": 6
    },
    {
      "epoch": 0.001590909090909091,
      "grad_norm": 4.026010036468506,
      "learning_rate": 0.00019990898748577932,
      "loss": 0.8881,
      "step": 7
    },
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 1.6804089546203613,
      "learning_rate": 0.00019986348122866897,
      "loss": 0.6723,
      "step": 8
    },
    {
      "epoch": 0.0020454545454545456,
      "grad_norm": 1.7152425050735474,
      "learning_rate": 0.00019981797497155862,
      "loss": 0.8344,
      "step": 9
    },
    {
      "epoch": 0.0022727272727272726,
      "grad_norm": 0.5572180151939392,
      "learning_rate": 0.00019977246871444825,
      "loss": 0.5404,
      "step": 10
    },
    {
      "epoch": 0.0025,
      "grad_norm": 0.5515684485435486,
      "learning_rate": 0.00019972696245733787,
      "loss": 0.7276,
      "step": 11
    },
    {
      "epoch": 0.0027272727272727275,
      "grad_norm": 0.3038136065006256,
      "learning_rate": 0.00019968145620022753,
      "loss": 0.6523,
      "step": 12
    },
    {
      "epoch": 0.0029545454545454545,
      "grad_norm": 0.31750571727752686,
      "learning_rate": 0.00019963594994311718,
      "loss": 0.5363,
      "step": 13
    },
    {
      "epoch": 0.003181818181818182,
      "grad_norm": 0.2342630922794342,
      "learning_rate": 0.00019959044368600683,
      "loss": 0.5399,
      "step": 14
    },
    {
      "epoch": 0.003409090909090909,
      "grad_norm": 0.23592840135097504,
      "learning_rate": 0.00019954493742889648,
      "loss": 0.489,
      "step": 15
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 0.19505983591079712,
      "learning_rate": 0.00019949943117178614,
      "loss": 0.4699,
      "step": 16
    },
    {
      "epoch": 0.003863636363636364,
      "grad_norm": 0.20585204660892487,
      "learning_rate": 0.0001994539249146758,
      "loss": 0.5099,
      "step": 17
    },
    {
      "epoch": 0.004090909090909091,
      "grad_norm": 0.31587615609169006,
      "learning_rate": 0.00019940841865756544,
      "loss": 0.5524,
      "step": 18
    },
    {
      "epoch": 0.004318181818181818,
      "grad_norm": 0.1627880334854126,
      "learning_rate": 0.0001993629124004551,
      "loss": 0.5107,
      "step": 19
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 0.1614995300769806,
      "learning_rate": 0.00019931740614334472,
      "loss": 0.4987,
      "step": 20
    },
    {
      "epoch": 0.004772727272727273,
      "grad_norm": 0.20223355293273926,
      "learning_rate": 0.00019927189988623435,
      "loss": 0.5525,
      "step": 21
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.2222071886062622,
      "learning_rate": 0.000199226393629124,
      "loss": 0.4322,
      "step": 22
    },
    {
      "epoch": 0.005227272727272727,
      "grad_norm": 0.17166553437709808,
      "learning_rate": 0.00019918088737201365,
      "loss": 0.5179,
      "step": 23
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 0.10893331468105316,
      "learning_rate": 0.0001991353811149033,
      "loss": 0.3455,
      "step": 24
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.1398228257894516,
      "learning_rate": 0.00019908987485779296,
      "loss": 0.4599,
      "step": 25
    },
    {
      "epoch": 0.005909090909090909,
      "grad_norm": 0.12686246633529663,
      "learning_rate": 0.0001990443686006826,
      "loss": 0.4585,
      "step": 26
    },
    {
      "epoch": 0.006136363636363636,
      "grad_norm": 0.11207880079746246,
      "learning_rate": 0.00019899886234357226,
      "loss": 0.3897,
      "step": 27
    },
    {
      "epoch": 0.006363636363636364,
      "grad_norm": 0.12844409048557281,
      "learning_rate": 0.00019895335608646192,
      "loss": 0.5043,
      "step": 28
    },
    {
      "epoch": 0.006590909090909091,
      "grad_norm": 0.1167125403881073,
      "learning_rate": 0.00019890784982935157,
      "loss": 0.4278,
      "step": 29
    },
    {
      "epoch": 0.006818181818181818,
      "grad_norm": 0.10016275197267532,
      "learning_rate": 0.0001988623435722412,
      "loss": 0.4581,
      "step": 30
    },
    {
      "epoch": 0.007045454545454546,
      "grad_norm": 0.10191893577575684,
      "learning_rate": 0.00019881683731513082,
      "loss": 0.4259,
      "step": 31
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 0.11158163845539093,
      "learning_rate": 0.00019877133105802047,
      "loss": 0.4299,
      "step": 32
    },
    {
      "epoch": 0.0075,
      "grad_norm": 0.09299314767122269,
      "learning_rate": 0.00019872582480091013,
      "loss": 0.3289,
      "step": 33
    },
    {
      "epoch": 0.007727272727272728,
      "grad_norm": 0.11130895465612411,
      "learning_rate": 0.00019868031854379978,
      "loss": 0.3714,
      "step": 34
    },
    {
      "epoch": 0.007954545454545454,
      "grad_norm": 0.1041693389415741,
      "learning_rate": 0.00019863481228668943,
      "loss": 0.4084,
      "step": 35
    },
    {
      "epoch": 0.008181818181818182,
      "grad_norm": 0.13515861332416534,
      "learning_rate": 0.00019858930602957908,
      "loss": 0.411,
      "step": 36
    },
    {
      "epoch": 0.00840909090909091,
      "grad_norm": 0.11311323940753937,
      "learning_rate": 0.00019854379977246874,
      "loss": 0.3645,
      "step": 37
    },
    {
      "epoch": 0.008636363636363636,
      "grad_norm": 0.0863237977027893,
      "learning_rate": 0.0001984982935153584,
      "loss": 0.3374,
      "step": 38
    },
    {
      "epoch": 0.008863636363636363,
      "grad_norm": 0.10407543182373047,
      "learning_rate": 0.00019845278725824804,
      "loss": 0.3308,
      "step": 39
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.08874934911727905,
      "learning_rate": 0.00019840728100113767,
      "loss": 0.3673,
      "step": 40
    },
    {
      "epoch": 0.009318181818181817,
      "grad_norm": 0.09804616123437881,
      "learning_rate": 0.0001983617747440273,
      "loss": 0.4219,
      "step": 41
    },
    {
      "epoch": 0.009545454545454546,
      "grad_norm": 0.0771603137254715,
      "learning_rate": 0.00019831626848691695,
      "loss": 0.3773,
      "step": 42
    },
    {
      "epoch": 0.009772727272727273,
      "grad_norm": 0.10861919075250626,
      "learning_rate": 0.0001982707622298066,
      "loss": 0.3556,
      "step": 43
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.09110326319932938,
      "learning_rate": 0.00019822525597269625,
      "loss": 0.4329,
      "step": 44
    },
    {
      "epoch": 0.010227272727272727,
      "grad_norm": 0.10175798833370209,
      "learning_rate": 0.0001981797497155859,
      "loss": 0.3899,
      "step": 45
    },
    {
      "epoch": 0.010454545454545454,
      "grad_norm": 0.11669128388166428,
      "learning_rate": 0.00019813424345847556,
      "loss": 0.4649,
      "step": 46
    },
    {
      "epoch": 0.010681818181818181,
      "grad_norm": 0.10699418187141418,
      "learning_rate": 0.0001980887372013652,
      "loss": 0.364,
      "step": 47
    },
    {
      "epoch": 0.01090909090909091,
      "grad_norm": 0.10460801422595978,
      "learning_rate": 0.00019804323094425486,
      "loss": 0.3412,
      "step": 48
    },
    {
      "epoch": 0.011136363636363637,
      "grad_norm": 0.09582038223743439,
      "learning_rate": 0.0001979977246871445,
      "loss": 0.3463,
      "step": 49
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.0701802596449852,
      "learning_rate": 0.00019795221843003414,
      "loss": 0.3917,
      "step": 50
    },
    {
      "epoch": 0.011590909090909091,
      "grad_norm": 0.08793963491916656,
      "learning_rate": 0.00019790671217292377,
      "loss": 0.4233,
      "step": 51
    },
    {
      "epoch": 0.011818181818181818,
      "grad_norm": 0.06893999874591827,
      "learning_rate": 0.00019786120591581342,
      "loss": 0.3867,
      "step": 52
    },
    {
      "epoch": 0.012045454545454545,
      "grad_norm": 0.13186684250831604,
      "learning_rate": 0.00019781569965870307,
      "loss": 0.5127,
      "step": 53
    },
    {
      "epoch": 0.012272727272727272,
      "grad_norm": 0.0817624107003212,
      "learning_rate": 0.00019777019340159273,
      "loss": 0.4179,
      "step": 54
    },
    {
      "epoch": 0.0125,
      "grad_norm": 0.08261246979236603,
      "learning_rate": 0.00019772468714448238,
      "loss": 0.3597,
      "step": 55
    },
    {
      "epoch": 0.012727272727272728,
      "grad_norm": 0.09297404438257217,
      "learning_rate": 0.00019767918088737203,
      "loss": 0.4262,
      "step": 56
    },
    {
      "epoch": 0.012954545454545455,
      "grad_norm": 0.07124683260917664,
      "learning_rate": 0.00019763367463026169,
      "loss": 0.4324,
      "step": 57
    },
    {
      "epoch": 0.013181818181818182,
      "grad_norm": 0.07361629605293274,
      "learning_rate": 0.00019758816837315134,
      "loss": 0.3393,
      "step": 58
    },
    {
      "epoch": 0.013409090909090909,
      "grad_norm": 0.07667452841997147,
      "learning_rate": 0.00019754266211604096,
      "loss": 0.4144,
      "step": 59
    },
    {
      "epoch": 0.013636363636363636,
      "grad_norm": 0.08567000180482864,
      "learning_rate": 0.00019749715585893062,
      "loss": 0.3635,
      "step": 60
    },
    {
      "epoch": 0.013863636363636364,
      "grad_norm": 0.1007247045636177,
      "learning_rate": 0.00019745164960182024,
      "loss": 0.3121,
      "step": 61
    },
    {
      "epoch": 0.014090909090909091,
      "grad_norm": 0.06690343469381332,
      "learning_rate": 0.0001974061433447099,
      "loss": 0.3745,
      "step": 62
    },
    {
      "epoch": 0.014318181818181818,
      "grad_norm": 0.058853019028902054,
      "learning_rate": 0.00019736063708759955,
      "loss": 0.3538,
      "step": 63
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 0.06586425006389618,
      "learning_rate": 0.0001973151308304892,
      "loss": 0.33,
      "step": 64
    },
    {
      "epoch": 0.014772727272727272,
      "grad_norm": 0.06629971414804459,
      "learning_rate": 0.00019726962457337885,
      "loss": 0.3223,
      "step": 65
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.07425428926944733,
      "learning_rate": 0.0001972241183162685,
      "loss": 0.3839,
      "step": 66
    },
    {
      "epoch": 0.015227272727272726,
      "grad_norm": 0.07824409008026123,
      "learning_rate": 0.00019717861205915816,
      "loss": 0.3369,
      "step": 67
    },
    {
      "epoch": 0.015454545454545455,
      "grad_norm": 0.0842682272195816,
      "learning_rate": 0.0001971331058020478,
      "loss": 0.3714,
      "step": 68
    },
    {
      "epoch": 0.015681818181818182,
      "grad_norm": 0.09159751981496811,
      "learning_rate": 0.00019708759954493744,
      "loss": 0.4256,
      "step": 69
    },
    {
      "epoch": 0.015909090909090907,
      "grad_norm": 0.093519426882267,
      "learning_rate": 0.0001970420932878271,
      "loss": 0.3816,
      "step": 70
    },
    {
      "epoch": 0.016136363636363636,
      "grad_norm": 0.07608913630247116,
      "learning_rate": 0.00019699658703071672,
      "loss": 0.3599,
      "step": 71
    },
    {
      "epoch": 0.016363636363636365,
      "grad_norm": 0.08850257843732834,
      "learning_rate": 0.00019695108077360637,
      "loss": 0.3878,
      "step": 72
    },
    {
      "epoch": 0.01659090909090909,
      "grad_norm": 0.06921189278364182,
      "learning_rate": 0.00019690557451649602,
      "loss": 0.3595,
      "step": 73
    },
    {
      "epoch": 0.01681818181818182,
      "grad_norm": 0.07835971564054489,
      "learning_rate": 0.00019686006825938567,
      "loss": 0.3831,
      "step": 74
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.0886501669883728,
      "learning_rate": 0.00019681456200227533,
      "loss": 0.4135,
      "step": 75
    },
    {
      "epoch": 0.017272727272727273,
      "grad_norm": 0.08811779320240021,
      "learning_rate": 0.00019676905574516498,
      "loss": 0.4594,
      "step": 76
    },
    {
      "epoch": 0.0175,
      "grad_norm": 0.08040676265954971,
      "learning_rate": 0.00019672354948805463,
      "loss": 0.3438,
      "step": 77
    },
    {
      "epoch": 0.017727272727272727,
      "grad_norm": 0.07586482912302017,
      "learning_rate": 0.00019667804323094426,
      "loss": 0.3938,
      "step": 78
    },
    {
      "epoch": 0.017954545454545456,
      "grad_norm": 0.07946701347827911,
      "learning_rate": 0.0001966325369738339,
      "loss": 0.3571,
      "step": 79
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 0.07428349554538727,
      "learning_rate": 0.00019658703071672356,
      "loss": 0.3837,
      "step": 80
    },
    {
      "epoch": 0.01840909090909091,
      "grad_norm": 0.08440966159105301,
      "learning_rate": 0.0001965415244596132,
      "loss": 0.3746,
      "step": 81
    },
    {
      "epoch": 0.018636363636363635,
      "grad_norm": 0.07434380799531937,
      "learning_rate": 0.00019649601820250284,
      "loss": 0.3394,
      "step": 82
    },
    {
      "epoch": 0.018863636363636364,
      "grad_norm": 0.06556101143360138,
      "learning_rate": 0.0001964505119453925,
      "loss": 0.3403,
      "step": 83
    },
    {
      "epoch": 0.019090909090909092,
      "grad_norm": 0.0670723170042038,
      "learning_rate": 0.00019640500568828215,
      "loss": 0.3438,
      "step": 84
    },
    {
      "epoch": 0.019318181818181818,
      "grad_norm": 0.07351026684045792,
      "learning_rate": 0.0001963594994311718,
      "loss": 0.3223,
      "step": 85
    },
    {
      "epoch": 0.019545454545454546,
      "grad_norm": 0.09580422937870026,
      "learning_rate": 0.00019631399317406145,
      "loss": 0.4042,
      "step": 86
    },
    {
      "epoch": 0.01977272727272727,
      "grad_norm": 0.09366362541913986,
      "learning_rate": 0.0001962684869169511,
      "loss": 0.45,
      "step": 87
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.06894698739051819,
      "learning_rate": 0.00019622298065984073,
      "loss": 0.3233,
      "step": 88
    },
    {
      "epoch": 0.020227272727272726,
      "grad_norm": 0.07522086799144745,
      "learning_rate": 0.00019617747440273039,
      "loss": 0.3228,
      "step": 89
    },
    {
      "epoch": 0.020454545454545454,
      "grad_norm": 0.07272551953792572,
      "learning_rate": 0.00019613196814562004,
      "loss": 0.3779,
      "step": 90
    },
    {
      "epoch": 0.020681818181818183,
      "grad_norm": 0.06931981444358826,
      "learning_rate": 0.00019608646188850966,
      "loss": 0.3298,
      "step": 91
    },
    {
      "epoch": 0.02090909090909091,
      "grad_norm": 0.09953733533620834,
      "learning_rate": 0.00019604095563139932,
      "loss": 0.3988,
      "step": 92
    },
    {
      "epoch": 0.021136363636363637,
      "grad_norm": 0.06477688997983932,
      "learning_rate": 0.00019599544937428897,
      "loss": 0.3211,
      "step": 93
    },
    {
      "epoch": 0.021363636363636362,
      "grad_norm": 0.06771562248468399,
      "learning_rate": 0.00019594994311717862,
      "loss": 0.3436,
      "step": 94
    },
    {
      "epoch": 0.02159090909090909,
      "grad_norm": 0.06511378288269043,
      "learning_rate": 0.00019590443686006828,
      "loss": 0.3276,
      "step": 95
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 0.06876669824123383,
      "learning_rate": 0.00019585893060295793,
      "loss": 0.3072,
      "step": 96
    },
    {
      "epoch": 0.022045454545454545,
      "grad_norm": 0.08212101459503174,
      "learning_rate": 0.00019581342434584758,
      "loss": 0.3948,
      "step": 97
    },
    {
      "epoch": 0.022272727272727274,
      "grad_norm": 0.04652820900082588,
      "learning_rate": 0.0001957679180887372,
      "loss": 0.3165,
      "step": 98
    },
    {
      "epoch": 0.0225,
      "grad_norm": 0.06844677031040192,
      "learning_rate": 0.00019572241183162686,
      "loss": 0.3645,
      "step": 99
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.07438177615404129,
      "learning_rate": 0.0001956769055745165,
      "loss": 0.329,
      "step": 100
    },
    {
      "epoch": 0.022954545454545453,
      "grad_norm": 0.05966833233833313,
      "learning_rate": 0.00019563139931740614,
      "loss": 0.362,
      "step": 101
    },
    {
      "epoch": 0.023181818181818182,
      "grad_norm": 0.07467912137508392,
      "learning_rate": 0.0001955858930602958,
      "loss": 0.4083,
      "step": 102
    },
    {
      "epoch": 0.02340909090909091,
      "grad_norm": 0.06976951658725739,
      "learning_rate": 0.00019554038680318544,
      "loss": 0.3992,
      "step": 103
    },
    {
      "epoch": 0.023636363636363636,
      "grad_norm": 0.06379973143339157,
      "learning_rate": 0.0001954948805460751,
      "loss": 0.2737,
      "step": 104
    },
    {
      "epoch": 0.023863636363636365,
      "grad_norm": 0.07330592721700668,
      "learning_rate": 0.00019544937428896475,
      "loss": 0.3534,
      "step": 105
    },
    {
      "epoch": 0.02409090909090909,
      "grad_norm": 0.056376054883003235,
      "learning_rate": 0.0001954038680318544,
      "loss": 0.3812,
      "step": 106
    },
    {
      "epoch": 0.02431818181818182,
      "grad_norm": 0.08460830897092819,
      "learning_rate": 0.00019535836177474406,
      "loss": 0.3729,
      "step": 107
    },
    {
      "epoch": 0.024545454545454544,
      "grad_norm": 0.04929930344223976,
      "learning_rate": 0.00019531285551763368,
      "loss": 0.3881,
      "step": 108
    },
    {
      "epoch": 0.024772727272727273,
      "grad_norm": 0.07843178510665894,
      "learning_rate": 0.00019526734926052333,
      "loss": 0.3292,
      "step": 109
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.07514087110757828,
      "learning_rate": 0.00019522184300341299,
      "loss": 0.4059,
      "step": 110
    },
    {
      "epoch": 0.025227272727272727,
      "grad_norm": 0.0708380714058876,
      "learning_rate": 0.0001951763367463026,
      "loss": 0.3484,
      "step": 111
    },
    {
      "epoch": 0.025454545454545455,
      "grad_norm": 0.09374292939901352,
      "learning_rate": 0.00019513083048919226,
      "loss": 0.392,
      "step": 112
    },
    {
      "epoch": 0.02568181818181818,
      "grad_norm": 0.0800919160246849,
      "learning_rate": 0.00019508532423208192,
      "loss": 0.4299,
      "step": 113
    },
    {
      "epoch": 0.02590909090909091,
      "grad_norm": 0.06834429502487183,
      "learning_rate": 0.00019503981797497157,
      "loss": 0.4027,
      "step": 114
    },
    {
      "epoch": 0.026136363636363635,
      "grad_norm": 0.09140557050704956,
      "learning_rate": 0.00019499431171786122,
      "loss": 0.3152,
      "step": 115
    },
    {
      "epoch": 0.026363636363636363,
      "grad_norm": 0.06075892969965935,
      "learning_rate": 0.00019494880546075088,
      "loss": 0.383,
      "step": 116
    },
    {
      "epoch": 0.026590909090909092,
      "grad_norm": 0.07017207145690918,
      "learning_rate": 0.0001949032992036405,
      "loss": 0.2985,
      "step": 117
    },
    {
      "epoch": 0.026818181818181817,
      "grad_norm": 0.07199474424123764,
      "learning_rate": 0.00019485779294653015,
      "loss": 0.3001,
      "step": 118
    },
    {
      "epoch": 0.027045454545454546,
      "grad_norm": 0.0679665207862854,
      "learning_rate": 0.0001948122866894198,
      "loss": 0.3266,
      "step": 119
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 0.10068900883197784,
      "learning_rate": 0.00019476678043230946,
      "loss": 0.3991,
      "step": 120
    },
    {
      "epoch": 0.0275,
      "grad_norm": 0.08029709756374359,
      "learning_rate": 0.00019472127417519909,
      "loss": 0.3728,
      "step": 121
    },
    {
      "epoch": 0.02772727272727273,
      "grad_norm": 0.07054473459720612,
      "learning_rate": 0.00019467576791808874,
      "loss": 0.3351,
      "step": 122
    },
    {
      "epoch": 0.027954545454545454,
      "grad_norm": 0.07432717084884644,
      "learning_rate": 0.0001946302616609784,
      "loss": 0.3482,
      "step": 123
    },
    {
      "epoch": 0.028181818181818183,
      "grad_norm": 0.06341076642274857,
      "learning_rate": 0.00019458475540386804,
      "loss": 0.3122,
      "step": 124
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.07806852459907532,
      "learning_rate": 0.0001945392491467577,
      "loss": 0.3495,
      "step": 125
    },
    {
      "epoch": 0.028636363636363637,
      "grad_norm": 0.07165726274251938,
      "learning_rate": 0.00019449374288964735,
      "loss": 0.3912,
      "step": 126
    },
    {
      "epoch": 0.028863636363636362,
      "grad_norm": 0.06661399453878403,
      "learning_rate": 0.00019444823663253698,
      "loss": 0.3841,
      "step": 127
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 0.07024721056222916,
      "learning_rate": 0.00019440273037542663,
      "loss": 0.3669,
      "step": 128
    },
    {
      "epoch": 0.02931818181818182,
      "grad_norm": 0.06235763803124428,
      "learning_rate": 0.00019435722411831628,
      "loss": 0.3683,
      "step": 129
    },
    {
      "epoch": 0.029545454545454545,
      "grad_norm": 0.06676219403743744,
      "learning_rate": 0.00019431171786120593,
      "loss": 0.2853,
      "step": 130
    },
    {
      "epoch": 0.029772727272727274,
      "grad_norm": 0.06562267988920212,
      "learning_rate": 0.00019426621160409556,
      "loss": 0.3558,
      "step": 131
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.07109113782644272,
      "learning_rate": 0.0001942207053469852,
      "loss": 0.3337,
      "step": 132
    },
    {
      "epoch": 0.030227272727272728,
      "grad_norm": 0.0557590015232563,
      "learning_rate": 0.00019417519908987487,
      "loss": 0.2473,
      "step": 133
    },
    {
      "epoch": 0.030454545454545453,
      "grad_norm": 0.049862559884786606,
      "learning_rate": 0.00019412969283276452,
      "loss": 0.3603,
      "step": 134
    },
    {
      "epoch": 0.03068181818181818,
      "grad_norm": 0.059394292533397675,
      "learning_rate": 0.00019408418657565417,
      "loss": 0.3513,
      "step": 135
    },
    {
      "epoch": 0.03090909090909091,
      "grad_norm": 0.053223248571157455,
      "learning_rate": 0.00019403868031854382,
      "loss": 0.3059,
      "step": 136
    },
    {
      "epoch": 0.031136363636363636,
      "grad_norm": 0.06313496083021164,
      "learning_rate": 0.00019399317406143345,
      "loss": 0.3693,
      "step": 137
    },
    {
      "epoch": 0.031363636363636364,
      "grad_norm": 0.06801935285329819,
      "learning_rate": 0.0001939476678043231,
      "loss": 0.4129,
      "step": 138
    },
    {
      "epoch": 0.03159090909090909,
      "grad_norm": 0.06246023252606392,
      "learning_rate": 0.00019390216154721276,
      "loss": 0.3396,
      "step": 139
    },
    {
      "epoch": 0.031818181818181815,
      "grad_norm": 0.06894106417894363,
      "learning_rate": 0.0001938566552901024,
      "loss": 0.3699,
      "step": 140
    },
    {
      "epoch": 0.032045454545454544,
      "grad_norm": 0.06517603993415833,
      "learning_rate": 0.00019381114903299203,
      "loss": 0.3329,
      "step": 141
    },
    {
      "epoch": 0.03227272727272727,
      "grad_norm": 0.05620145425200462,
      "learning_rate": 0.0001937656427758817,
      "loss": 0.29,
      "step": 142
    },
    {
      "epoch": 0.0325,
      "grad_norm": 0.08787811547517776,
      "learning_rate": 0.00019372013651877134,
      "loss": 0.4415,
      "step": 143
    },
    {
      "epoch": 0.03272727272727273,
      "grad_norm": 0.06131772696971893,
      "learning_rate": 0.000193674630261661,
      "loss": 0.3104,
      "step": 144
    },
    {
      "epoch": 0.03295454545454545,
      "grad_norm": 0.05860871449112892,
      "learning_rate": 0.00019362912400455065,
      "loss": 0.3425,
      "step": 145
    },
    {
      "epoch": 0.03318181818181818,
      "grad_norm": 0.05998973920941353,
      "learning_rate": 0.00019358361774744027,
      "loss": 0.262,
      "step": 146
    },
    {
      "epoch": 0.03340909090909091,
      "grad_norm": 0.0834483727812767,
      "learning_rate": 0.00019353811149032992,
      "loss": 0.4039,
      "step": 147
    },
    {
      "epoch": 0.03363636363636364,
      "grad_norm": 0.07726601511240005,
      "learning_rate": 0.00019349260523321958,
      "loss": 0.3994,
      "step": 148
    },
    {
      "epoch": 0.03386363636363637,
      "grad_norm": 0.05438241362571716,
      "learning_rate": 0.00019344709897610923,
      "loss": 0.3609,
      "step": 149
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.060505710542201996,
      "learning_rate": 0.00019340159271899888,
      "loss": 0.3292,
      "step": 150
    },
    {
      "epoch": 0.03431818181818182,
      "grad_norm": 0.07236941158771515,
      "learning_rate": 0.0001933560864618885,
      "loss": 0.3862,
      "step": 151
    },
    {
      "epoch": 0.034545454545454546,
      "grad_norm": 0.05794330686330795,
      "learning_rate": 0.00019331058020477816,
      "loss": 0.3482,
      "step": 152
    },
    {
      "epoch": 0.034772727272727275,
      "grad_norm": 0.05686696618795395,
      "learning_rate": 0.00019326507394766781,
      "loss": 0.3858,
      "step": 153
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.08095233142375946,
      "learning_rate": 0.00019321956769055747,
      "loss": 0.3735,
      "step": 154
    },
    {
      "epoch": 0.035227272727272725,
      "grad_norm": 0.09078076481819153,
      "learning_rate": 0.00019317406143344712,
      "loss": 0.4006,
      "step": 155
    },
    {
      "epoch": 0.035454545454545454,
      "grad_norm": 0.06459148973226547,
      "learning_rate": 0.00019312855517633675,
      "loss": 0.3301,
      "step": 156
    },
    {
      "epoch": 0.03568181818181818,
      "grad_norm": 0.06666135042905807,
      "learning_rate": 0.0001930830489192264,
      "loss": 0.3947,
      "step": 157
    },
    {
      "epoch": 0.03590909090909091,
      "grad_norm": 0.09065928310155869,
      "learning_rate": 0.00019303754266211605,
      "loss": 0.3663,
      "step": 158
    },
    {
      "epoch": 0.03613636363636363,
      "grad_norm": 0.06278455257415771,
      "learning_rate": 0.0001929920364050057,
      "loss": 0.3451,
      "step": 159
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.06701163947582245,
      "learning_rate": 0.00019294653014789536,
      "loss": 0.3367,
      "step": 160
    },
    {
      "epoch": 0.03659090909090909,
      "grad_norm": 0.05965646728873253,
      "learning_rate": 0.00019290102389078498,
      "loss": 0.3288,
      "step": 161
    },
    {
      "epoch": 0.03681818181818182,
      "grad_norm": 0.07862415164709091,
      "learning_rate": 0.00019285551763367463,
      "loss": 0.398,
      "step": 162
    },
    {
      "epoch": 0.03704545454545455,
      "grad_norm": 0.07096751779317856,
      "learning_rate": 0.0001928100113765643,
      "loss": 0.3608,
      "step": 163
    },
    {
      "epoch": 0.03727272727272727,
      "grad_norm": 0.09491382539272308,
      "learning_rate": 0.00019276450511945394,
      "loss": 0.4035,
      "step": 164
    },
    {
      "epoch": 0.0375,
      "grad_norm": 0.058224428445100784,
      "learning_rate": 0.0001927189988623436,
      "loss": 0.2915,
      "step": 165
    },
    {
      "epoch": 0.03772727272727273,
      "grad_norm": 0.05661878362298012,
      "learning_rate": 0.00019267349260523322,
      "loss": 0.3188,
      "step": 166
    },
    {
      "epoch": 0.037954545454545456,
      "grad_norm": 0.06353802978992462,
      "learning_rate": 0.00019262798634812287,
      "loss": 0.3623,
      "step": 167
    },
    {
      "epoch": 0.038181818181818185,
      "grad_norm": 0.07102194428443909,
      "learning_rate": 0.00019258248009101252,
      "loss": 0.3585,
      "step": 168
    },
    {
      "epoch": 0.03840909090909091,
      "grad_norm": 0.08376941829919815,
      "learning_rate": 0.00019253697383390218,
      "loss": 0.3215,
      "step": 169
    },
    {
      "epoch": 0.038636363636363635,
      "grad_norm": 0.06871546804904938,
      "learning_rate": 0.00019249146757679183,
      "loss": 0.3314,
      "step": 170
    },
    {
      "epoch": 0.038863636363636364,
      "grad_norm": 0.0606510192155838,
      "learning_rate": 0.00019244596131968146,
      "loss": 0.3726,
      "step": 171
    },
    {
      "epoch": 0.03909090909090909,
      "grad_norm": 0.06335197389125824,
      "learning_rate": 0.0001924004550625711,
      "loss": 0.399,
      "step": 172
    },
    {
      "epoch": 0.03931818181818182,
      "grad_norm": 0.05325044319033623,
      "learning_rate": 0.00019235494880546076,
      "loss": 0.2458,
      "step": 173
    },
    {
      "epoch": 0.03954545454545454,
      "grad_norm": 0.05998166278004646,
      "learning_rate": 0.00019230944254835041,
      "loss": 0.3107,
      "step": 174
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.05567270144820213,
      "learning_rate": 0.00019226393629124007,
      "loss": 0.2969,
      "step": 175
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.06554161757230759,
      "learning_rate": 0.0001922184300341297,
      "loss": 0.3137,
      "step": 176
    },
    {
      "epoch": 0.04022727272727273,
      "grad_norm": 0.08243762701749802,
      "learning_rate": 0.00019217292377701935,
      "loss": 0.4265,
      "step": 177
    },
    {
      "epoch": 0.04045454545454545,
      "grad_norm": 0.06550472229719162,
      "learning_rate": 0.000192127417519909,
      "loss": 0.3716,
      "step": 178
    },
    {
      "epoch": 0.04068181818181818,
      "grad_norm": 0.06431688368320465,
      "learning_rate": 0.00019208191126279865,
      "loss": 0.3315,
      "step": 179
    },
    {
      "epoch": 0.04090909090909091,
      "grad_norm": 0.04605208709836006,
      "learning_rate": 0.0001920364050056883,
      "loss": 0.2564,
      "step": 180
    },
    {
      "epoch": 0.04113636363636364,
      "grad_norm": 0.060621269047260284,
      "learning_rate": 0.00019199089874857793,
      "loss": 0.3198,
      "step": 181
    },
    {
      "epoch": 0.041363636363636366,
      "grad_norm": 0.06997399032115936,
      "learning_rate": 0.00019194539249146758,
      "loss": 0.3339,
      "step": 182
    },
    {
      "epoch": 0.04159090909090909,
      "grad_norm": 0.0798470601439476,
      "learning_rate": 0.00019189988623435724,
      "loss": 0.371,
      "step": 183
    },
    {
      "epoch": 0.04181818181818182,
      "grad_norm": 0.07429269701242447,
      "learning_rate": 0.0001918543799772469,
      "loss": 0.4104,
      "step": 184
    },
    {
      "epoch": 0.042045454545454546,
      "grad_norm": 0.07028881460428238,
      "learning_rate": 0.00019180887372013651,
      "loss": 0.3145,
      "step": 185
    },
    {
      "epoch": 0.042272727272727274,
      "grad_norm": 0.06278473883867264,
      "learning_rate": 0.00019176336746302617,
      "loss": 0.3727,
      "step": 186
    },
    {
      "epoch": 0.0425,
      "grad_norm": 0.07379573583602905,
      "learning_rate": 0.00019171786120591582,
      "loss": 0.3886,
      "step": 187
    },
    {
      "epoch": 0.042727272727272725,
      "grad_norm": 0.04174638167023659,
      "learning_rate": 0.00019167235494880547,
      "loss": 0.232,
      "step": 188
    },
    {
      "epoch": 0.042954545454545454,
      "grad_norm": 0.07095156610012054,
      "learning_rate": 0.00019162684869169513,
      "loss": 0.3197,
      "step": 189
    },
    {
      "epoch": 0.04318181818181818,
      "grad_norm": 0.05576963722705841,
      "learning_rate": 0.00019158134243458478,
      "loss": 0.3334,
      "step": 190
    },
    {
      "epoch": 0.04340909090909091,
      "grad_norm": 0.04908519983291626,
      "learning_rate": 0.0001915358361774744,
      "loss": 0.3451,
      "step": 191
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.07589255273342133,
      "learning_rate": 0.00019149032992036406,
      "loss": 0.3874,
      "step": 192
    },
    {
      "epoch": 0.04386363636363636,
      "grad_norm": 0.046867210417985916,
      "learning_rate": 0.0001914448236632537,
      "loss": 0.2871,
      "step": 193
    },
    {
      "epoch": 0.04409090909090909,
      "grad_norm": 0.05578349158167839,
      "learning_rate": 0.00019139931740614336,
      "loss": 0.2508,
      "step": 194
    },
    {
      "epoch": 0.04431818181818182,
      "grad_norm": 0.0778859332203865,
      "learning_rate": 0.000191353811149033,
      "loss": 0.4176,
      "step": 195
    },
    {
      "epoch": 0.04454545454545455,
      "grad_norm": 0.053020309656858444,
      "learning_rate": 0.00019130830489192264,
      "loss": 0.3378,
      "step": 196
    },
    {
      "epoch": 0.04477272727272727,
      "grad_norm": 0.06699839979410172,
      "learning_rate": 0.0001912627986348123,
      "loss": 0.356,
      "step": 197
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.07861863076686859,
      "learning_rate": 0.00019121729237770195,
      "loss": 0.4179,
      "step": 198
    },
    {
      "epoch": 0.04522727272727273,
      "grad_norm": 0.05042359605431557,
      "learning_rate": 0.0001911717861205916,
      "loss": 0.3011,
      "step": 199
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.06554067879915237,
      "learning_rate": 0.00019112627986348125,
      "loss": 0.3542,
      "step": 200
    },
    {
      "epoch": 0.045681818181818185,
      "grad_norm": 0.07131427526473999,
      "learning_rate": 0.00019108077360637088,
      "loss": 0.3716,
      "step": 201
    },
    {
      "epoch": 0.045909090909090906,
      "grad_norm": 0.06147327646613121,
      "learning_rate": 0.00019103526734926053,
      "loss": 0.3002,
      "step": 202
    },
    {
      "epoch": 0.046136363636363635,
      "grad_norm": 0.06199071556329727,
      "learning_rate": 0.00019098976109215018,
      "loss": 0.3662,
      "step": 203
    },
    {
      "epoch": 0.046363636363636364,
      "grad_norm": 0.058952223509550095,
      "learning_rate": 0.00019094425483503984,
      "loss": 0.3967,
      "step": 204
    },
    {
      "epoch": 0.04659090909090909,
      "grad_norm": 0.09795787930488586,
      "learning_rate": 0.00019089874857792946,
      "loss": 0.359,
      "step": 205
    },
    {
      "epoch": 0.04681818181818182,
      "grad_norm": 0.06651876121759415,
      "learning_rate": 0.00019085324232081911,
      "loss": 0.3508,
      "step": 206
    },
    {
      "epoch": 0.04704545454545454,
      "grad_norm": 0.05890844762325287,
      "learning_rate": 0.00019080773606370877,
      "loss": 0.334,
      "step": 207
    },
    {
      "epoch": 0.04727272727272727,
      "grad_norm": 0.05826595053076744,
      "learning_rate": 0.00019076222980659842,
      "loss": 0.358,
      "step": 208
    },
    {
      "epoch": 0.0475,
      "grad_norm": 0.05831390246748924,
      "learning_rate": 0.00019071672354948807,
      "loss": 0.3775,
      "step": 209
    },
    {
      "epoch": 0.04772727272727273,
      "grad_norm": 0.05108606442809105,
      "learning_rate": 0.00019067121729237773,
      "loss": 0.3129,
      "step": 210
    },
    {
      "epoch": 0.04795454545454545,
      "grad_norm": 0.07749366760253906,
      "learning_rate": 0.00019062571103526735,
      "loss": 0.3342,
      "step": 211
    },
    {
      "epoch": 0.04818181818181818,
      "grad_norm": 0.06167794018983841,
      "learning_rate": 0.000190580204778157,
      "loss": 0.3948,
      "step": 212
    },
    {
      "epoch": 0.04840909090909091,
      "grad_norm": 0.08156614750623703,
      "learning_rate": 0.00019053469852104666,
      "loss": 0.3393,
      "step": 213
    },
    {
      "epoch": 0.04863636363636364,
      "grad_norm": 0.056219011545181274,
      "learning_rate": 0.0001904891922639363,
      "loss": 0.3236,
      "step": 214
    },
    {
      "epoch": 0.048863636363636366,
      "grad_norm": 0.058977313339710236,
      "learning_rate": 0.00019044368600682594,
      "loss": 0.3425,
      "step": 215
    },
    {
      "epoch": 0.04909090909090909,
      "grad_norm": 0.08400088548660278,
      "learning_rate": 0.0001903981797497156,
      "loss": 0.3858,
      "step": 216
    },
    {
      "epoch": 0.04931818181818182,
      "grad_norm": 0.08909162878990173,
      "learning_rate": 0.00019035267349260524,
      "loss": 0.4069,
      "step": 217
    },
    {
      "epoch": 0.049545454545454545,
      "grad_norm": 0.05368777737021446,
      "learning_rate": 0.0001903071672354949,
      "loss": 0.2844,
      "step": 218
    },
    {
      "epoch": 0.049772727272727274,
      "grad_norm": 0.06547210365533829,
      "learning_rate": 0.00019026166097838455,
      "loss": 0.35,
      "step": 219
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.07236853986978531,
      "learning_rate": 0.0001902161547212742,
      "loss": 0.3626,
      "step": 220
    },
    {
      "epoch": 0.050227272727272725,
      "grad_norm": 0.07785093039274216,
      "learning_rate": 0.00019017064846416383,
      "loss": 0.3385,
      "step": 221
    },
    {
      "epoch": 0.05045454545454545,
      "grad_norm": 0.07804476469755173,
      "learning_rate": 0.00019012514220705348,
      "loss": 0.3729,
      "step": 222
    },
    {
      "epoch": 0.05068181818181818,
      "grad_norm": 0.06342072784900665,
      "learning_rate": 0.00019007963594994313,
      "loss": 0.3791,
      "step": 223
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.11797191202640533,
      "learning_rate": 0.00019003412969283276,
      "loss": 0.4548,
      "step": 224
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.0549025721848011,
      "learning_rate": 0.0001899886234357224,
      "loss": 0.3368,
      "step": 225
    },
    {
      "epoch": 0.05136363636363636,
      "grad_norm": 0.06929991394281387,
      "learning_rate": 0.00018994311717861206,
      "loss": 0.3999,
      "step": 226
    },
    {
      "epoch": 0.05159090909090909,
      "grad_norm": 0.05913428217172623,
      "learning_rate": 0.00018989761092150172,
      "loss": 0.3384,
      "step": 227
    },
    {
      "epoch": 0.05181818181818182,
      "grad_norm": 0.06352411955595016,
      "learning_rate": 0.00018985210466439137,
      "loss": 0.3684,
      "step": 228
    },
    {
      "epoch": 0.05204545454545455,
      "grad_norm": 0.0656827986240387,
      "learning_rate": 0.00018980659840728102,
      "loss": 0.3958,
      "step": 229
    },
    {
      "epoch": 0.05227272727272727,
      "grad_norm": 0.05775139108300209,
      "learning_rate": 0.00018976109215017067,
      "loss": 0.3722,
      "step": 230
    },
    {
      "epoch": 0.0525,
      "grad_norm": 0.05050124600529671,
      "learning_rate": 0.0001897155858930603,
      "loss": 0.2828,
      "step": 231
    },
    {
      "epoch": 0.05272727272727273,
      "grad_norm": 0.08886519819498062,
      "learning_rate": 0.00018967007963594995,
      "loss": 0.3941,
      "step": 232
    },
    {
      "epoch": 0.052954545454545456,
      "grad_norm": 0.0700606107711792,
      "learning_rate": 0.0001896245733788396,
      "loss": 0.3131,
      "step": 233
    },
    {
      "epoch": 0.053181818181818184,
      "grad_norm": 0.07516077160835266,
      "learning_rate": 0.00018957906712172923,
      "loss": 0.3343,
      "step": 234
    },
    {
      "epoch": 0.053409090909090906,
      "grad_norm": 0.07329991459846497,
      "learning_rate": 0.00018953356086461888,
      "loss": 0.4192,
      "step": 235
    },
    {
      "epoch": 0.053636363636363635,
      "grad_norm": 0.05742500722408295,
      "learning_rate": 0.00018948805460750854,
      "loss": 0.3598,
      "step": 236
    },
    {
      "epoch": 0.053863636363636364,
      "grad_norm": 0.06486418843269348,
      "learning_rate": 0.0001894425483503982,
      "loss": 0.3179,
      "step": 237
    },
    {
      "epoch": 0.05409090909090909,
      "grad_norm": 0.06652698665857315,
      "learning_rate": 0.00018939704209328784,
      "loss": 0.3379,
      "step": 238
    },
    {
      "epoch": 0.05431818181818182,
      "grad_norm": 0.08018311858177185,
      "learning_rate": 0.0001893515358361775,
      "loss": 0.316,
      "step": 239
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.04889434948563576,
      "learning_rate": 0.00018930602957906712,
      "loss": 0.3135,
      "step": 240
    },
    {
      "epoch": 0.05477272727272727,
      "grad_norm": 0.05238529294729233,
      "learning_rate": 0.00018926052332195677,
      "loss": 0.3251,
      "step": 241
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.07745719701051712,
      "learning_rate": 0.00018921501706484643,
      "loss": 0.3573,
      "step": 242
    },
    {
      "epoch": 0.05522727272727273,
      "grad_norm": 0.06807247549295425,
      "learning_rate": 0.00018916951080773608,
      "loss": 0.3772,
      "step": 243
    },
    {
      "epoch": 0.05545454545454546,
      "grad_norm": 0.06552942097187042,
      "learning_rate": 0.0001891240045506257,
      "loss": 0.3367,
      "step": 244
    },
    {
      "epoch": 0.05568181818181818,
      "grad_norm": 0.06414686143398285,
      "learning_rate": 0.00018907849829351536,
      "loss": 0.3492,
      "step": 245
    },
    {
      "epoch": 0.05590909090909091,
      "grad_norm": 0.04470270127058029,
      "learning_rate": 0.000189032992036405,
      "loss": 0.3242,
      "step": 246
    },
    {
      "epoch": 0.05613636363636364,
      "grad_norm": 0.051333699375391006,
      "learning_rate": 0.00018898748577929466,
      "loss": 0.3334,
      "step": 247
    },
    {
      "epoch": 0.056363636363636366,
      "grad_norm": 0.058295976370573044,
      "learning_rate": 0.00018894197952218432,
      "loss": 0.3661,
      "step": 248
    },
    {
      "epoch": 0.05659090909090909,
      "grad_norm": 0.06089434400200844,
      "learning_rate": 0.00018889647326507397,
      "loss": 0.3437,
      "step": 249
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.0700172707438469,
      "learning_rate": 0.0001888509670079636,
      "loss": 0.3262,
      "step": 250
    },
    {
      "epoch": 0.057045454545454545,
      "grad_norm": 0.07393556088209152,
      "learning_rate": 0.00018880546075085325,
      "loss": 0.3745,
      "step": 251
    },
    {
      "epoch": 0.057272727272727274,
      "grad_norm": 0.049629759043455124,
      "learning_rate": 0.0001887599544937429,
      "loss": 0.3184,
      "step": 252
    },
    {
      "epoch": 0.0575,
      "grad_norm": 0.049887765198946,
      "learning_rate": 0.00018871444823663253,
      "loss": 0.3451,
      "step": 253
    },
    {
      "epoch": 0.057727272727272724,
      "grad_norm": 0.06874776631593704,
      "learning_rate": 0.00018866894197952218,
      "loss": 0.3645,
      "step": 254
    },
    {
      "epoch": 0.05795454545454545,
      "grad_norm": 0.0678335502743721,
      "learning_rate": 0.00018862343572241183,
      "loss": 0.3604,
      "step": 255
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.0702473372220993,
      "learning_rate": 0.00018857792946530148,
      "loss": 0.4118,
      "step": 256
    },
    {
      "epoch": 0.05840909090909091,
      "grad_norm": 0.04643334820866585,
      "learning_rate": 0.00018853242320819114,
      "loss": 0.3204,
      "step": 257
    },
    {
      "epoch": 0.05863636363636364,
      "grad_norm": 0.059801798313856125,
      "learning_rate": 0.0001884869169510808,
      "loss": 0.3503,
      "step": 258
    },
    {
      "epoch": 0.05886363636363636,
      "grad_norm": 0.049428388476371765,
      "learning_rate": 0.00018844141069397044,
      "loss": 0.301,
      "step": 259
    },
    {
      "epoch": 0.05909090909090909,
      "grad_norm": 0.06760966777801514,
      "learning_rate": 0.00018839590443686007,
      "loss": 0.35,
      "step": 260
    },
    {
      "epoch": 0.05931818181818182,
      "grad_norm": 0.04700171947479248,
      "learning_rate": 0.00018835039817974972,
      "loss": 0.2932,
      "step": 261
    },
    {
      "epoch": 0.05954545454545455,
      "grad_norm": 0.06734611839056015,
      "learning_rate": 0.00018830489192263937,
      "loss": 0.3882,
      "step": 262
    },
    {
      "epoch": 0.059772727272727276,
      "grad_norm": 0.057850565761327744,
      "learning_rate": 0.000188259385665529,
      "loss": 0.3255,
      "step": 263
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.07376939803361893,
      "learning_rate": 0.00018821387940841865,
      "loss": 0.3814,
      "step": 264
    },
    {
      "epoch": 0.060227272727272727,
      "grad_norm": 0.053846053779125214,
      "learning_rate": 0.0001881683731513083,
      "loss": 0.3281,
      "step": 265
    },
    {
      "epoch": 0.060454545454545455,
      "grad_norm": 0.0724973976612091,
      "learning_rate": 0.00018812286689419796,
      "loss": 0.3703,
      "step": 266
    },
    {
      "epoch": 0.060681818181818184,
      "grad_norm": 0.06651509553194046,
      "learning_rate": 0.0001880773606370876,
      "loss": 0.4002,
      "step": 267
    },
    {
      "epoch": 0.060909090909090906,
      "grad_norm": 0.047983311116695404,
      "learning_rate": 0.00018803185437997726,
      "loss": 0.3397,
      "step": 268
    },
    {
      "epoch": 0.061136363636363635,
      "grad_norm": 0.06298086792230606,
      "learning_rate": 0.00018798634812286692,
      "loss": 0.4234,
      "step": 269
    },
    {
      "epoch": 0.06136363636363636,
      "grad_norm": 0.05371975898742676,
      "learning_rate": 0.00018794084186575654,
      "loss": 0.3412,
      "step": 270
    },
    {
      "epoch": 0.06159090909090909,
      "grad_norm": 0.05256347358226776,
      "learning_rate": 0.0001878953356086462,
      "loss": 0.2797,
      "step": 271
    },
    {
      "epoch": 0.06181818181818182,
      "grad_norm": 0.05623733997344971,
      "learning_rate": 0.00018784982935153585,
      "loss": 0.3607,
      "step": 272
    },
    {
      "epoch": 0.06204545454545454,
      "grad_norm": 0.05969402939081192,
      "learning_rate": 0.00018780432309442547,
      "loss": 0.3685,
      "step": 273
    },
    {
      "epoch": 0.06227272727272727,
      "grad_norm": 0.0661996603012085,
      "learning_rate": 0.00018775881683731513,
      "loss": 0.3888,
      "step": 274
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.05555962398648262,
      "learning_rate": 0.00018771331058020478,
      "loss": 0.3147,
      "step": 275
    },
    {
      "epoch": 0.06272727272727273,
      "grad_norm": 0.06367362290620804,
      "learning_rate": 0.00018766780432309443,
      "loss": 0.3458,
      "step": 276
    },
    {
      "epoch": 0.06295454545454546,
      "grad_norm": 0.06925500184297562,
      "learning_rate": 0.00018762229806598409,
      "loss": 0.3998,
      "step": 277
    },
    {
      "epoch": 0.06318181818181819,
      "grad_norm": 0.04492902755737305,
      "learning_rate": 0.00018757679180887374,
      "loss": 0.31,
      "step": 278
    },
    {
      "epoch": 0.06340909090909091,
      "grad_norm": 0.05792950838804245,
      "learning_rate": 0.0001875312855517634,
      "loss": 0.3781,
      "step": 279
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.05823434144258499,
      "learning_rate": 0.00018748577929465302,
      "loss": 0.3532,
      "step": 280
    },
    {
      "epoch": 0.06386363636363636,
      "grad_norm": 0.05601540580391884,
      "learning_rate": 0.00018744027303754267,
      "loss": 0.356,
      "step": 281
    },
    {
      "epoch": 0.06409090909090909,
      "grad_norm": 0.07912052422761917,
      "learning_rate": 0.00018739476678043232,
      "loss": 0.4209,
      "step": 282
    },
    {
      "epoch": 0.06431818181818182,
      "grad_norm": 0.057562101632356644,
      "learning_rate": 0.00018734926052332195,
      "loss": 0.3732,
      "step": 283
    },
    {
      "epoch": 0.06454545454545454,
      "grad_norm": 0.05628069490194321,
      "learning_rate": 0.0001873037542662116,
      "loss": 0.3454,
      "step": 284
    },
    {
      "epoch": 0.06477272727272727,
      "grad_norm": 0.0656595528125763,
      "learning_rate": 0.00018725824800910125,
      "loss": 0.3693,
      "step": 285
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.05250021442770958,
      "learning_rate": 0.0001872127417519909,
      "loss": 0.3188,
      "step": 286
    },
    {
      "epoch": 0.06522727272727273,
      "grad_norm": 0.05299752578139305,
      "learning_rate": 0.00018716723549488056,
      "loss": 0.3529,
      "step": 287
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.06954917311668396,
      "learning_rate": 0.0001871217292377702,
      "loss": 0.4178,
      "step": 288
    },
    {
      "epoch": 0.06568181818181819,
      "grad_norm": 0.050331227481365204,
      "learning_rate": 0.00018707622298065987,
      "loss": 0.2843,
      "step": 289
    },
    {
      "epoch": 0.0659090909090909,
      "grad_norm": 0.056997284293174744,
      "learning_rate": 0.0001870307167235495,
      "loss": 0.3583,
      "step": 290
    },
    {
      "epoch": 0.06613636363636363,
      "grad_norm": 0.05277497321367264,
      "learning_rate": 0.00018698521046643914,
      "loss": 0.3216,
      "step": 291
    },
    {
      "epoch": 0.06636363636363636,
      "grad_norm": 0.054318491369485855,
      "learning_rate": 0.00018693970420932877,
      "loss": 0.3929,
      "step": 292
    },
    {
      "epoch": 0.06659090909090909,
      "grad_norm": 0.07449901849031448,
      "learning_rate": 0.00018689419795221842,
      "loss": 0.4066,
      "step": 293
    },
    {
      "epoch": 0.06681818181818182,
      "grad_norm": 0.04061919450759888,
      "learning_rate": 0.00018684869169510808,
      "loss": 0.274,
      "step": 294
    },
    {
      "epoch": 0.06704545454545455,
      "grad_norm": 0.04693525657057762,
      "learning_rate": 0.00018680318543799773,
      "loss": 0.3059,
      "step": 295
    },
    {
      "epoch": 0.06727272727272728,
      "grad_norm": 0.04869629442691803,
      "learning_rate": 0.00018675767918088738,
      "loss": 0.3312,
      "step": 296
    },
    {
      "epoch": 0.0675,
      "grad_norm": 0.06189393997192383,
      "learning_rate": 0.00018671217292377703,
      "loss": 0.3788,
      "step": 297
    },
    {
      "epoch": 0.06772727272727273,
      "grad_norm": 0.05897223576903343,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.3297,
      "step": 298
    },
    {
      "epoch": 0.06795454545454545,
      "grad_norm": 0.06938318908214569,
      "learning_rate": 0.00018662116040955634,
      "loss": 0.3682,
      "step": 299
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.05221978947520256,
      "learning_rate": 0.00018657565415244596,
      "loss": 0.3669,
      "step": 300
    },
    {
      "epoch": 0.0684090909090909,
      "grad_norm": 0.05429336428642273,
      "learning_rate": 0.00018653014789533562,
      "loss": 0.3582,
      "step": 301
    },
    {
      "epoch": 0.06863636363636363,
      "grad_norm": 0.04778015613555908,
      "learning_rate": 0.00018648464163822524,
      "loss": 0.3018,
      "step": 302
    },
    {
      "epoch": 0.06886363636363636,
      "grad_norm": 0.06355077028274536,
      "learning_rate": 0.0001864391353811149,
      "loss": 0.3561,
      "step": 303
    },
    {
      "epoch": 0.06909090909090909,
      "grad_norm": 0.09303397685289383,
      "learning_rate": 0.00018639362912400455,
      "loss": 0.4112,
      "step": 304
    },
    {
      "epoch": 0.06931818181818182,
      "grad_norm": 0.048643965274095535,
      "learning_rate": 0.0001863481228668942,
      "loss": 0.3404,
      "step": 305
    },
    {
      "epoch": 0.06954545454545455,
      "grad_norm": 0.05611461400985718,
      "learning_rate": 0.00018630261660978385,
      "loss": 0.3303,
      "step": 306
    },
    {
      "epoch": 0.06977272727272728,
      "grad_norm": 0.06468231976032257,
      "learning_rate": 0.0001862571103526735,
      "loss": 0.2803,
      "step": 307
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.06781104952096939,
      "learning_rate": 0.00018621160409556316,
      "loss": 0.3638,
      "step": 308
    },
    {
      "epoch": 0.07022727272727272,
      "grad_norm": 0.06183740496635437,
      "learning_rate": 0.0001861660978384528,
      "loss": 0.333,
      "step": 309
    },
    {
      "epoch": 0.07045454545454545,
      "grad_norm": 0.054520800709724426,
      "learning_rate": 0.00018612059158134244,
      "loss": 0.3904,
      "step": 310
    },
    {
      "epoch": 0.07068181818181818,
      "grad_norm": 0.07450435310602188,
      "learning_rate": 0.0001860750853242321,
      "loss": 0.3857,
      "step": 311
    },
    {
      "epoch": 0.07090909090909091,
      "grad_norm": 0.049862246960401535,
      "learning_rate": 0.00018602957906712172,
      "loss": 0.3311,
      "step": 312
    },
    {
      "epoch": 0.07113636363636364,
      "grad_norm": 0.0763450488448143,
      "learning_rate": 0.00018598407281001137,
      "loss": 0.3376,
      "step": 313
    },
    {
      "epoch": 0.07136363636363637,
      "grad_norm": 0.05485435947775841,
      "learning_rate": 0.00018593856655290102,
      "loss": 0.4061,
      "step": 314
    },
    {
      "epoch": 0.0715909090909091,
      "grad_norm": 0.04947225749492645,
      "learning_rate": 0.00018589306029579068,
      "loss": 0.3244,
      "step": 315
    },
    {
      "epoch": 0.07181818181818182,
      "grad_norm": 0.05208340287208557,
      "learning_rate": 0.00018584755403868033,
      "loss": 0.2957,
      "step": 316
    },
    {
      "epoch": 0.07204545454545455,
      "grad_norm": 0.0679679811000824,
      "learning_rate": 0.00018580204778156998,
      "loss": 0.3918,
      "step": 317
    },
    {
      "epoch": 0.07227272727272727,
      "grad_norm": 0.05650695785880089,
      "learning_rate": 0.00018575654152445963,
      "loss": 0.3537,
      "step": 318
    },
    {
      "epoch": 0.0725,
      "grad_norm": 0.0630074217915535,
      "learning_rate": 0.0001857110352673493,
      "loss": 0.3675,
      "step": 319
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.06640052050352097,
      "learning_rate": 0.0001856655290102389,
      "loss": 0.3648,
      "step": 320
    },
    {
      "epoch": 0.07295454545454545,
      "grad_norm": 0.060985270887613297,
      "learning_rate": 0.00018562002275312857,
      "loss": 0.3312,
      "step": 321
    },
    {
      "epoch": 0.07318181818181818,
      "grad_norm": 0.05836831033229828,
      "learning_rate": 0.0001855745164960182,
      "loss": 0.3325,
      "step": 322
    },
    {
      "epoch": 0.07340909090909091,
      "grad_norm": 0.059331707656383514,
      "learning_rate": 0.00018552901023890784,
      "loss": 0.3386,
      "step": 323
    },
    {
      "epoch": 0.07363636363636364,
      "grad_norm": 0.07830832898616791,
      "learning_rate": 0.0001854835039817975,
      "loss": 0.3844,
      "step": 324
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.045595116913318634,
      "learning_rate": 0.00018543799772468715,
      "loss": 0.3137,
      "step": 325
    },
    {
      "epoch": 0.0740909090909091,
      "grad_norm": 0.07008963823318481,
      "learning_rate": 0.0001853924914675768,
      "loss": 0.3548,
      "step": 326
    },
    {
      "epoch": 0.07431818181818182,
      "grad_norm": 0.07493504881858826,
      "learning_rate": 0.00018534698521046646,
      "loss": 0.3323,
      "step": 327
    },
    {
      "epoch": 0.07454545454545454,
      "grad_norm": 0.04828701913356781,
      "learning_rate": 0.0001853014789533561,
      "loss": 0.2847,
      "step": 328
    },
    {
      "epoch": 0.07477272727272727,
      "grad_norm": 0.04781479761004448,
      "learning_rate": 0.00018525597269624576,
      "loss": 0.3185,
      "step": 329
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.06718601286411285,
      "learning_rate": 0.0001852104664391354,
      "loss": 0.4074,
      "step": 330
    },
    {
      "epoch": 0.07522727272727273,
      "grad_norm": 0.06405767798423767,
      "learning_rate": 0.00018516496018202504,
      "loss": 0.3942,
      "step": 331
    },
    {
      "epoch": 0.07545454545454545,
      "grad_norm": 0.0644848644733429,
      "learning_rate": 0.00018511945392491467,
      "loss": 0.367,
      "step": 332
    },
    {
      "epoch": 0.07568181818181818,
      "grad_norm": 0.06904548406600952,
      "learning_rate": 0.00018507394766780432,
      "loss": 0.3837,
      "step": 333
    },
    {
      "epoch": 0.07590909090909091,
      "grad_norm": 0.0511094368994236,
      "learning_rate": 0.00018502844141069397,
      "loss": 0.3101,
      "step": 334
    },
    {
      "epoch": 0.07613636363636364,
      "grad_norm": 0.0748281180858612,
      "learning_rate": 0.00018498293515358362,
      "loss": 0.3865,
      "step": 335
    },
    {
      "epoch": 0.07636363636363637,
      "grad_norm": 0.06199392303824425,
      "learning_rate": 0.00018493742889647328,
      "loss": 0.3368,
      "step": 336
    },
    {
      "epoch": 0.07659090909090908,
      "grad_norm": 0.05980556458234787,
      "learning_rate": 0.00018489192263936293,
      "loss": 0.3573,
      "step": 337
    },
    {
      "epoch": 0.07681818181818181,
      "grad_norm": 0.05399606376886368,
      "learning_rate": 0.00018484641638225258,
      "loss": 0.3404,
      "step": 338
    },
    {
      "epoch": 0.07704545454545454,
      "grad_norm": 0.0682339295744896,
      "learning_rate": 0.00018480091012514224,
      "loss": 0.3824,
      "step": 339
    },
    {
      "epoch": 0.07727272727272727,
      "grad_norm": 0.06125824525952339,
      "learning_rate": 0.00018475540386803186,
      "loss": 0.3301,
      "step": 340
    },
    {
      "epoch": 0.0775,
      "grad_norm": 0.0638417899608612,
      "learning_rate": 0.00018470989761092151,
      "loss": 0.3887,
      "step": 341
    },
    {
      "epoch": 0.07772727272727273,
      "grad_norm": 0.060136113315820694,
      "learning_rate": 0.00018466439135381114,
      "loss": 0.3206,
      "step": 342
    },
    {
      "epoch": 0.07795454545454546,
      "grad_norm": 0.06362151354551315,
      "learning_rate": 0.0001846188850967008,
      "loss": 0.333,
      "step": 343
    },
    {
      "epoch": 0.07818181818181819,
      "grad_norm": 0.06632666289806366,
      "learning_rate": 0.00018457337883959045,
      "loss": 0.3971,
      "step": 344
    },
    {
      "epoch": 0.07840909090909091,
      "grad_norm": 0.04698403552174568,
      "learning_rate": 0.0001845278725824801,
      "loss": 0.3047,
      "step": 345
    },
    {
      "epoch": 0.07863636363636364,
      "grad_norm": 0.06076575815677643,
      "learning_rate": 0.00018448236632536975,
      "loss": 0.2944,
      "step": 346
    },
    {
      "epoch": 0.07886363636363636,
      "grad_norm": 0.06461817771196365,
      "learning_rate": 0.0001844368600682594,
      "loss": 0.3607,
      "step": 347
    },
    {
      "epoch": 0.07909090909090909,
      "grad_norm": 0.05943220853805542,
      "learning_rate": 0.00018439135381114906,
      "loss": 0.3331,
      "step": 348
    },
    {
      "epoch": 0.07931818181818182,
      "grad_norm": 0.04814707115292549,
      "learning_rate": 0.0001843458475540387,
      "loss": 0.3776,
      "step": 349
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.05335039645433426,
      "learning_rate": 0.00018430034129692833,
      "loss": 0.3691,
      "step": 350
    },
    {
      "epoch": 0.07977272727272727,
      "grad_norm": 0.04392361268401146,
      "learning_rate": 0.000184254835039818,
      "loss": 0.2706,
      "step": 351
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.05365467444062233,
      "learning_rate": 0.0001842093287827076,
      "loss": 0.3878,
      "step": 352
    },
    {
      "epoch": 0.08022727272727273,
      "grad_norm": 0.050644513219594955,
      "learning_rate": 0.00018416382252559727,
      "loss": 0.3095,
      "step": 353
    },
    {
      "epoch": 0.08045454545454546,
      "grad_norm": 0.046484772115945816,
      "learning_rate": 0.00018411831626848692,
      "loss": 0.2683,
      "step": 354
    },
    {
      "epoch": 0.08068181818181819,
      "grad_norm": 0.045265376567840576,
      "learning_rate": 0.00018407281001137657,
      "loss": 0.2946,
      "step": 355
    },
    {
      "epoch": 0.0809090909090909,
      "grad_norm": 0.053030628710985184,
      "learning_rate": 0.00018402730375426622,
      "loss": 0.337,
      "step": 356
    },
    {
      "epoch": 0.08113636363636363,
      "grad_norm": 0.06730330735445023,
      "learning_rate": 0.00018398179749715588,
      "loss": 0.3971,
      "step": 357
    },
    {
      "epoch": 0.08136363636363636,
      "grad_norm": 0.05009976401925087,
      "learning_rate": 0.00018393629124004553,
      "loss": 0.3731,
      "step": 358
    },
    {
      "epoch": 0.08159090909090909,
      "grad_norm": 0.06351926177740097,
      "learning_rate": 0.00018389078498293518,
      "loss": 0.3616,
      "step": 359
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.07046020030975342,
      "learning_rate": 0.0001838452787258248,
      "loss": 0.3874,
      "step": 360
    },
    {
      "epoch": 0.08204545454545455,
      "grad_norm": 0.04964801296591759,
      "learning_rate": 0.00018379977246871446,
      "loss": 0.3326,
      "step": 361
    },
    {
      "epoch": 0.08227272727272728,
      "grad_norm": 0.05638054385781288,
      "learning_rate": 0.0001837542662116041,
      "loss": 0.3141,
      "step": 362
    },
    {
      "epoch": 0.0825,
      "grad_norm": 0.04976417496800423,
      "learning_rate": 0.00018370875995449374,
      "loss": 0.296,
      "step": 363
    },
    {
      "epoch": 0.08272727272727273,
      "grad_norm": 0.05541141703724861,
      "learning_rate": 0.0001836632536973834,
      "loss": 0.3269,
      "step": 364
    },
    {
      "epoch": 0.08295454545454546,
      "grad_norm": 0.04712510108947754,
      "learning_rate": 0.00018361774744027305,
      "loss": 0.2938,
      "step": 365
    },
    {
      "epoch": 0.08318181818181818,
      "grad_norm": 0.0760776624083519,
      "learning_rate": 0.0001835722411831627,
      "loss": 0.4384,
      "step": 366
    },
    {
      "epoch": 0.0834090909090909,
      "grad_norm": 0.054838553071022034,
      "learning_rate": 0.00018352673492605235,
      "loss": 0.3235,
      "step": 367
    },
    {
      "epoch": 0.08363636363636363,
      "grad_norm": 0.051773492246866226,
      "learning_rate": 0.000183481228668942,
      "loss": 0.2921,
      "step": 368
    },
    {
      "epoch": 0.08386363636363636,
      "grad_norm": 0.07863356918096542,
      "learning_rate": 0.00018343572241183166,
      "loss": 0.3843,
      "step": 369
    },
    {
      "epoch": 0.08409090909090909,
      "grad_norm": 0.06335171312093735,
      "learning_rate": 0.00018339021615472128,
      "loss": 0.3731,
      "step": 370
    },
    {
      "epoch": 0.08431818181818182,
      "grad_norm": 0.06293865293264389,
      "learning_rate": 0.00018334470989761094,
      "loss": 0.355,
      "step": 371
    },
    {
      "epoch": 0.08454545454545455,
      "grad_norm": 0.0797453224658966,
      "learning_rate": 0.00018329920364050056,
      "loss": 0.3448,
      "step": 372
    },
    {
      "epoch": 0.08477272727272728,
      "grad_norm": 0.06595376878976822,
      "learning_rate": 0.00018325369738339021,
      "loss": 0.344,
      "step": 373
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.06626773625612259,
      "learning_rate": 0.00018320819112627987,
      "loss": 0.3976,
      "step": 374
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.049016352742910385,
      "learning_rate": 0.00018316268486916952,
      "loss": 0.283,
      "step": 375
    },
    {
      "epoch": 0.08545454545454545,
      "grad_norm": 0.07497303187847137,
      "learning_rate": 0.00018311717861205917,
      "loss": 0.3257,
      "step": 376
    },
    {
      "epoch": 0.08568181818181818,
      "grad_norm": 0.06794065237045288,
      "learning_rate": 0.00018307167235494883,
      "loss": 0.3518,
      "step": 377
    },
    {
      "epoch": 0.08590909090909091,
      "grad_norm": 0.06082101911306381,
      "learning_rate": 0.00018302616609783848,
      "loss": 0.3383,
      "step": 378
    },
    {
      "epoch": 0.08613636363636364,
      "grad_norm": 0.05185623839497566,
      "learning_rate": 0.00018298065984072813,
      "loss": 0.3099,
      "step": 379
    },
    {
      "epoch": 0.08636363636363636,
      "grad_norm": 0.04557288810610771,
      "learning_rate": 0.00018293515358361776,
      "loss": 0.2421,
      "step": 380
    },
    {
      "epoch": 0.0865909090909091,
      "grad_norm": 0.05734207481145859,
      "learning_rate": 0.0001828896473265074,
      "loss": 0.3512,
      "step": 381
    },
    {
      "epoch": 0.08681818181818182,
      "grad_norm": 0.05303836241364479,
      "learning_rate": 0.00018284414106939704,
      "loss": 0.3629,
      "step": 382
    },
    {
      "epoch": 0.08704545454545455,
      "grad_norm": 0.05537203326821327,
      "learning_rate": 0.0001827986348122867,
      "loss": 0.327,
      "step": 383
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.05138049274682999,
      "learning_rate": 0.00018275312855517634,
      "loss": 0.3571,
      "step": 384
    },
    {
      "epoch": 0.0875,
      "grad_norm": 0.047112949192523956,
      "learning_rate": 0.000182707622298066,
      "loss": 0.3134,
      "step": 385
    },
    {
      "epoch": 0.08772727272727272,
      "grad_norm": 0.049319300800561905,
      "learning_rate": 0.00018266211604095565,
      "loss": 0.2888,
      "step": 386
    },
    {
      "epoch": 0.08795454545454545,
      "grad_norm": 0.08876416087150574,
      "learning_rate": 0.0001826166097838453,
      "loss": 0.3384,
      "step": 387
    },
    {
      "epoch": 0.08818181818181818,
      "grad_norm": 0.06045040115714073,
      "learning_rate": 0.00018257110352673495,
      "loss": 0.3278,
      "step": 388
    },
    {
      "epoch": 0.08840909090909091,
      "grad_norm": 0.05623570457100868,
      "learning_rate": 0.0001825255972696246,
      "loss": 0.3555,
      "step": 389
    },
    {
      "epoch": 0.08863636363636364,
      "grad_norm": 0.04623253270983696,
      "learning_rate": 0.00018248009101251423,
      "loss": 0.228,
      "step": 390
    },
    {
      "epoch": 0.08886363636363637,
      "grad_norm": 0.06788642704486847,
      "learning_rate": 0.00018243458475540386,
      "loss": 0.3231,
      "step": 391
    },
    {
      "epoch": 0.0890909090909091,
      "grad_norm": 0.0671856626868248,
      "learning_rate": 0.0001823890784982935,
      "loss": 0.3866,
      "step": 392
    },
    {
      "epoch": 0.08931818181818182,
      "grad_norm": 0.04060037061572075,
      "learning_rate": 0.00018234357224118316,
      "loss": 0.2732,
      "step": 393
    },
    {
      "epoch": 0.08954545454545454,
      "grad_norm": 0.06633857637643814,
      "learning_rate": 0.00018229806598407282,
      "loss": 0.3621,
      "step": 394
    },
    {
      "epoch": 0.08977272727272727,
      "grad_norm": 0.05408653989434242,
      "learning_rate": 0.00018225255972696247,
      "loss": 0.2564,
      "step": 395
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.05686313658952713,
      "learning_rate": 0.00018220705346985212,
      "loss": 0.3619,
      "step": 396
    },
    {
      "epoch": 0.09022727272727273,
      "grad_norm": 0.05252434313297272,
      "learning_rate": 0.00018216154721274177,
      "loss": 0.2566,
      "step": 397
    },
    {
      "epoch": 0.09045454545454545,
      "grad_norm": 0.0645257905125618,
      "learning_rate": 0.00018211604095563143,
      "loss": 0.4123,
      "step": 398
    },
    {
      "epoch": 0.09068181818181818,
      "grad_norm": 0.051257770508527756,
      "learning_rate": 0.00018207053469852105,
      "loss": 0.323,
      "step": 399
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.06319396942853928,
      "learning_rate": 0.0001820250284414107,
      "loss": 0.3765,
      "step": 400
    },
    {
      "epoch": 0.09113636363636364,
      "grad_norm": 0.05636661499738693,
      "learning_rate": 0.00018197952218430033,
      "loss": 0.319,
      "step": 401
    },
    {
      "epoch": 0.09136363636363637,
      "grad_norm": 0.06568755209445953,
      "learning_rate": 0.00018193401592718998,
      "loss": 0.3497,
      "step": 402
    },
    {
      "epoch": 0.0915909090909091,
      "grad_norm": 0.059914615005254745,
      "learning_rate": 0.00018188850967007964,
      "loss": 0.3725,
      "step": 403
    },
    {
      "epoch": 0.09181818181818181,
      "grad_norm": 0.04244052991271019,
      "learning_rate": 0.0001818430034129693,
      "loss": 0.3033,
      "step": 404
    },
    {
      "epoch": 0.09204545454545454,
      "grad_norm": 0.06326735019683838,
      "learning_rate": 0.00018179749715585894,
      "loss": 0.3373,
      "step": 405
    },
    {
      "epoch": 0.09227272727272727,
      "grad_norm": 0.06786724925041199,
      "learning_rate": 0.0001817519908987486,
      "loss": 0.3946,
      "step": 406
    },
    {
      "epoch": 0.0925,
      "grad_norm": 0.044108323752880096,
      "learning_rate": 0.00018170648464163825,
      "loss": 0.2895,
      "step": 407
    },
    {
      "epoch": 0.09272727272727273,
      "grad_norm": 0.05444852262735367,
      "learning_rate": 0.0001816609783845279,
      "loss": 0.3557,
      "step": 408
    },
    {
      "epoch": 0.09295454545454546,
      "grad_norm": 0.058795616030693054,
      "learning_rate": 0.00018161547212741753,
      "loss": 0.3758,
      "step": 409
    },
    {
      "epoch": 0.09318181818181819,
      "grad_norm": 0.0708850771188736,
      "learning_rate": 0.00018156996587030718,
      "loss": 0.3816,
      "step": 410
    },
    {
      "epoch": 0.09340909090909091,
      "grad_norm": 0.06272953748703003,
      "learning_rate": 0.0001815244596131968,
      "loss": 0.4135,
      "step": 411
    },
    {
      "epoch": 0.09363636363636364,
      "grad_norm": 0.03870053216814995,
      "learning_rate": 0.00018147895335608646,
      "loss": 0.2792,
      "step": 412
    },
    {
      "epoch": 0.09386363636363636,
      "grad_norm": 0.062147676944732666,
      "learning_rate": 0.0001814334470989761,
      "loss": 0.3183,
      "step": 413
    },
    {
      "epoch": 0.09409090909090909,
      "grad_norm": 0.06755338609218597,
      "learning_rate": 0.00018138794084186576,
      "loss": 0.3376,
      "step": 414
    },
    {
      "epoch": 0.09431818181818181,
      "grad_norm": 0.06125922128558159,
      "learning_rate": 0.00018134243458475542,
      "loss": 0.3664,
      "step": 415
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.04974981024861336,
      "learning_rate": 0.00018129692832764507,
      "loss": 0.2845,
      "step": 416
    },
    {
      "epoch": 0.09477272727272727,
      "grad_norm": 0.05072937533259392,
      "learning_rate": 0.00018125142207053472,
      "loss": 0.284,
      "step": 417
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.057851068675518036,
      "learning_rate": 0.00018120591581342437,
      "loss": 0.3116,
      "step": 418
    },
    {
      "epoch": 0.09522727272727273,
      "grad_norm": 0.04201078414916992,
      "learning_rate": 0.000181160409556314,
      "loss": 0.2637,
      "step": 419
    },
    {
      "epoch": 0.09545454545454546,
      "grad_norm": 0.04704837128520012,
      "learning_rate": 0.00018111490329920365,
      "loss": 0.3338,
      "step": 420
    },
    {
      "epoch": 0.09568181818181819,
      "grad_norm": 0.06882376968860626,
      "learning_rate": 0.00018106939704209328,
      "loss": 0.403,
      "step": 421
    },
    {
      "epoch": 0.0959090909090909,
      "grad_norm": 0.056562718003988266,
      "learning_rate": 0.00018102389078498293,
      "loss": 0.3633,
      "step": 422
    },
    {
      "epoch": 0.09613636363636363,
      "grad_norm": 0.06332030147314072,
      "learning_rate": 0.00018097838452787258,
      "loss": 0.3516,
      "step": 423
    },
    {
      "epoch": 0.09636363636363636,
      "grad_norm": 0.0509772002696991,
      "learning_rate": 0.00018093287827076224,
      "loss": 0.2806,
      "step": 424
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.06407227367162704,
      "learning_rate": 0.0001808873720136519,
      "loss": 0.3167,
      "step": 425
    },
    {
      "epoch": 0.09681818181818182,
      "grad_norm": 0.06669660657644272,
      "learning_rate": 0.00018084186575654154,
      "loss": 0.3779,
      "step": 426
    },
    {
      "epoch": 0.09704545454545455,
      "grad_norm": 0.08626802265644073,
      "learning_rate": 0.0001807963594994312,
      "loss": 0.3542,
      "step": 427
    },
    {
      "epoch": 0.09727272727272727,
      "grad_norm": 0.047750454396009445,
      "learning_rate": 0.00018075085324232082,
      "loss": 0.2737,
      "step": 428
    },
    {
      "epoch": 0.0975,
      "grad_norm": 0.0505741685628891,
      "learning_rate": 0.00018070534698521047,
      "loss": 0.2893,
      "step": 429
    },
    {
      "epoch": 0.09772727272727273,
      "grad_norm": 0.05496987700462341,
      "learning_rate": 0.00018065984072810013,
      "loss": 0.323,
      "step": 430
    },
    {
      "epoch": 0.09795454545454546,
      "grad_norm": 0.05047130957245827,
      "learning_rate": 0.00018061433447098975,
      "loss": 0.3316,
      "step": 431
    },
    {
      "epoch": 0.09818181818181818,
      "grad_norm": 0.05134263634681702,
      "learning_rate": 0.0001805688282138794,
      "loss": 0.3045,
      "step": 432
    },
    {
      "epoch": 0.0984090909090909,
      "grad_norm": 0.06546347588300705,
      "learning_rate": 0.00018052332195676906,
      "loss": 0.36,
      "step": 433
    },
    {
      "epoch": 0.09863636363636363,
      "grad_norm": 0.04929467290639877,
      "learning_rate": 0.0001804778156996587,
      "loss": 0.302,
      "step": 434
    },
    {
      "epoch": 0.09886363636363636,
      "grad_norm": 0.05996859073638916,
      "learning_rate": 0.00018043230944254836,
      "loss": 0.3469,
      "step": 435
    },
    {
      "epoch": 0.09909090909090909,
      "grad_norm": 0.04852202907204628,
      "learning_rate": 0.00018038680318543802,
      "loss": 0.3175,
      "step": 436
    },
    {
      "epoch": 0.09931818181818182,
      "grad_norm": 0.04900619387626648,
      "learning_rate": 0.00018034129692832767,
      "loss": 0.3206,
      "step": 437
    },
    {
      "epoch": 0.09954545454545455,
      "grad_norm": 0.055079810321331024,
      "learning_rate": 0.0001802957906712173,
      "loss": 0.3533,
      "step": 438
    },
    {
      "epoch": 0.09977272727272728,
      "grad_norm": 0.06453448534011841,
      "learning_rate": 0.00018025028441410695,
      "loss": 0.3995,
      "step": 439
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.048400383442640305,
      "learning_rate": 0.0001802047781569966,
      "loss": 0.322,
      "step": 440
    },
    {
      "epoch": 0.10022727272727272,
      "grad_norm": 0.04645123332738876,
      "learning_rate": 0.00018015927189988623,
      "loss": 0.2728,
      "step": 441
    },
    {
      "epoch": 0.10045454545454545,
      "grad_norm": 0.06393183022737503,
      "learning_rate": 0.00018011376564277588,
      "loss": 0.3908,
      "step": 442
    },
    {
      "epoch": 0.10068181818181818,
      "grad_norm": 0.06576705724000931,
      "learning_rate": 0.00018006825938566553,
      "loss": 0.3064,
      "step": 443
    },
    {
      "epoch": 0.1009090909090909,
      "grad_norm": 0.05006475746631622,
      "learning_rate": 0.00018002275312855518,
      "loss": 0.3159,
      "step": 444
    },
    {
      "epoch": 0.10113636363636364,
      "grad_norm": 0.041606832295656204,
      "learning_rate": 0.00017997724687144484,
      "loss": 0.2544,
      "step": 445
    },
    {
      "epoch": 0.10136363636363636,
      "grad_norm": 0.047395605593919754,
      "learning_rate": 0.0001799317406143345,
      "loss": 0.3207,
      "step": 446
    },
    {
      "epoch": 0.10159090909090909,
      "grad_norm": 0.04405288025736809,
      "learning_rate": 0.00017988623435722414,
      "loss": 0.2814,
      "step": 447
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.05298742279410362,
      "learning_rate": 0.00017984072810011377,
      "loss": 0.3271,
      "step": 448
    },
    {
      "epoch": 0.10204545454545455,
      "grad_norm": 0.0612906776368618,
      "learning_rate": 0.00017979522184300342,
      "loss": 0.3237,
      "step": 449
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.05722806602716446,
      "learning_rate": 0.00017974971558589307,
      "loss": 0.3404,
      "step": 450
    },
    {
      "epoch": 0.1025,
      "grad_norm": 0.0729404017329216,
      "learning_rate": 0.0001797042093287827,
      "loss": 0.4135,
      "step": 451
    },
    {
      "epoch": 0.10272727272727272,
      "grad_norm": 0.07039127498865128,
      "learning_rate": 0.00017965870307167235,
      "loss": 0.3701,
      "step": 452
    },
    {
      "epoch": 0.10295454545454545,
      "grad_norm": 0.05016703903675079,
      "learning_rate": 0.000179613196814562,
      "loss": 0.3115,
      "step": 453
    },
    {
      "epoch": 0.10318181818181818,
      "grad_norm": 0.04516303911805153,
      "learning_rate": 0.00017956769055745166,
      "loss": 0.3178,
      "step": 454
    },
    {
      "epoch": 0.10340909090909091,
      "grad_norm": 0.07465092092752457,
      "learning_rate": 0.0001795221843003413,
      "loss": 0.4185,
      "step": 455
    },
    {
      "epoch": 0.10363636363636364,
      "grad_norm": 0.05562557652592659,
      "learning_rate": 0.00017947667804323096,
      "loss": 0.3824,
      "step": 456
    },
    {
      "epoch": 0.10386363636363637,
      "grad_norm": 0.046506594866514206,
      "learning_rate": 0.00017943117178612062,
      "loss": 0.3244,
      "step": 457
    },
    {
      "epoch": 0.1040909090909091,
      "grad_norm": 0.05739615112543106,
      "learning_rate": 0.00017938566552901024,
      "loss": 0.3,
      "step": 458
    },
    {
      "epoch": 0.10431818181818182,
      "grad_norm": 0.04665132984519005,
      "learning_rate": 0.0001793401592718999,
      "loss": 0.3044,
      "step": 459
    },
    {
      "epoch": 0.10454545454545454,
      "grad_norm": 0.04875635728240013,
      "learning_rate": 0.00017929465301478955,
      "loss": 0.2916,
      "step": 460
    },
    {
      "epoch": 0.10477272727272727,
      "grad_norm": 0.059315308928489685,
      "learning_rate": 0.00017924914675767917,
      "loss": 0.3217,
      "step": 461
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.052390825003385544,
      "learning_rate": 0.00017920364050056883,
      "loss": 0.3353,
      "step": 462
    },
    {
      "epoch": 0.10522727272727272,
      "grad_norm": 0.04865780472755432,
      "learning_rate": 0.00017915813424345848,
      "loss": 0.3129,
      "step": 463
    },
    {
      "epoch": 0.10545454545454545,
      "grad_norm": 0.04936910793185234,
      "learning_rate": 0.00017911262798634813,
      "loss": 0.304,
      "step": 464
    },
    {
      "epoch": 0.10568181818181818,
      "grad_norm": 0.07107196003198624,
      "learning_rate": 0.00017906712172923779,
      "loss": 0.3834,
      "step": 465
    },
    {
      "epoch": 0.10590909090909091,
      "grad_norm": 0.04838815703988075,
      "learning_rate": 0.00017902161547212744,
      "loss": 0.3222,
      "step": 466
    },
    {
      "epoch": 0.10613636363636364,
      "grad_norm": 0.06602957844734192,
      "learning_rate": 0.00017897610921501706,
      "loss": 0.3405,
      "step": 467
    },
    {
      "epoch": 0.10636363636363637,
      "grad_norm": 0.06749571114778519,
      "learning_rate": 0.00017893060295790672,
      "loss": 0.3714,
      "step": 468
    },
    {
      "epoch": 0.1065909090909091,
      "grad_norm": 0.05099499970674515,
      "learning_rate": 0.00017888509670079637,
      "loss": 0.2705,
      "step": 469
    },
    {
      "epoch": 0.10681818181818181,
      "grad_norm": 0.06464849412441254,
      "learning_rate": 0.00017883959044368602,
      "loss": 0.3136,
      "step": 470
    },
    {
      "epoch": 0.10704545454545454,
      "grad_norm": 0.046003054827451706,
      "learning_rate": 0.00017879408418657565,
      "loss": 0.29,
      "step": 471
    },
    {
      "epoch": 0.10727272727272727,
      "grad_norm": 0.05447479709982872,
      "learning_rate": 0.0001787485779294653,
      "loss": 0.2891,
      "step": 472
    },
    {
      "epoch": 0.1075,
      "grad_norm": 0.06967628002166748,
      "learning_rate": 0.00017870307167235495,
      "loss": 0.3738,
      "step": 473
    },
    {
      "epoch": 0.10772727272727273,
      "grad_norm": 0.054162394255399704,
      "learning_rate": 0.0001786575654152446,
      "loss": 0.3033,
      "step": 474
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.06255701184272766,
      "learning_rate": 0.00017861205915813426,
      "loss": 0.3745,
      "step": 475
    },
    {
      "epoch": 0.10818181818181818,
      "grad_norm": 0.04162311926484108,
      "learning_rate": 0.0001785665529010239,
      "loss": 0.295,
      "step": 476
    },
    {
      "epoch": 0.10840909090909091,
      "grad_norm": 0.0643477737903595,
      "learning_rate": 0.00017852104664391354,
      "loss": 0.3845,
      "step": 477
    },
    {
      "epoch": 0.10863636363636364,
      "grad_norm": 0.06811483949422836,
      "learning_rate": 0.0001784755403868032,
      "loss": 0.397,
      "step": 478
    },
    {
      "epoch": 0.10886363636363636,
      "grad_norm": 0.06253533810377121,
      "learning_rate": 0.00017843003412969284,
      "loss": 0.349,
      "step": 479
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.06257100403308868,
      "learning_rate": 0.0001783845278725825,
      "loss": 0.3776,
      "step": 480
    },
    {
      "epoch": 0.10931818181818181,
      "grad_norm": 0.04759284481406212,
      "learning_rate": 0.00017833902161547212,
      "loss": 0.3192,
      "step": 481
    },
    {
      "epoch": 0.10954545454545454,
      "grad_norm": 0.09009284526109695,
      "learning_rate": 0.00017829351535836178,
      "loss": 0.4003,
      "step": 482
    },
    {
      "epoch": 0.10977272727272727,
      "grad_norm": 0.07113011181354523,
      "learning_rate": 0.00017824800910125143,
      "loss": 0.3692,
      "step": 483
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.062039367854595184,
      "learning_rate": 0.00017820250284414108,
      "loss": 0.3518,
      "step": 484
    },
    {
      "epoch": 0.11022727272727273,
      "grad_norm": 0.059538617730140686,
      "learning_rate": 0.00017815699658703073,
      "loss": 0.3461,
      "step": 485
    },
    {
      "epoch": 0.11045454545454546,
      "grad_norm": 0.08078610152006149,
      "learning_rate": 0.0001781114903299204,
      "loss": 0.3994,
      "step": 486
    },
    {
      "epoch": 0.11068181818181819,
      "grad_norm": 0.043665364384651184,
      "learning_rate": 0.00017806598407281,
      "loss": 0.2946,
      "step": 487
    },
    {
      "epoch": 0.11090909090909092,
      "grad_norm": 0.04464847967028618,
      "learning_rate": 0.00017802047781569967,
      "loss": 0.2919,
      "step": 488
    },
    {
      "epoch": 0.11113636363636363,
      "grad_norm": 0.0501101091504097,
      "learning_rate": 0.00017797497155858932,
      "loss": 0.3385,
      "step": 489
    },
    {
      "epoch": 0.11136363636363636,
      "grad_norm": 0.04533708840608597,
      "learning_rate": 0.00017792946530147897,
      "loss": 0.3076,
      "step": 490
    },
    {
      "epoch": 0.11159090909090909,
      "grad_norm": 0.05585330352187157,
      "learning_rate": 0.0001778839590443686,
      "loss": 0.3593,
      "step": 491
    },
    {
      "epoch": 0.11181818181818182,
      "grad_norm": 0.0668138712644577,
      "learning_rate": 0.00017783845278725825,
      "loss": 0.3862,
      "step": 492
    },
    {
      "epoch": 0.11204545454545455,
      "grad_norm": 0.0559220127761364,
      "learning_rate": 0.0001777929465301479,
      "loss": 0.4033,
      "step": 493
    },
    {
      "epoch": 0.11227272727272727,
      "grad_norm": 0.054483212530612946,
      "learning_rate": 0.00017774744027303755,
      "loss": 0.3315,
      "step": 494
    },
    {
      "epoch": 0.1125,
      "grad_norm": 0.050936587154865265,
      "learning_rate": 0.0001777019340159272,
      "loss": 0.3097,
      "step": 495
    },
    {
      "epoch": 0.11272727272727273,
      "grad_norm": 0.04861988127231598,
      "learning_rate": 0.00017765642775881686,
      "loss": 0.3224,
      "step": 496
    },
    {
      "epoch": 0.11295454545454546,
      "grad_norm": 0.04694347083568573,
      "learning_rate": 0.00017761092150170649,
      "loss": 0.3042,
      "step": 497
    },
    {
      "epoch": 0.11318181818181818,
      "grad_norm": 0.05131533741950989,
      "learning_rate": 0.00017756541524459614,
      "loss": 0.3798,
      "step": 498
    },
    {
      "epoch": 0.1134090909090909,
      "grad_norm": 0.05407940596342087,
      "learning_rate": 0.0001775199089874858,
      "loss": 0.3683,
      "step": 499
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.03856603801250458,
      "learning_rate": 0.00017747440273037544,
      "loss": 0.2366,
      "step": 500
    },
    {
      "epoch": 0.11386363636363636,
      "grad_norm": 0.04873974248766899,
      "learning_rate": 0.00017742889647326507,
      "loss": 0.2925,
      "step": 501
    },
    {
      "epoch": 0.11409090909090909,
      "grad_norm": 0.04812370613217354,
      "learning_rate": 0.00017738339021615472,
      "loss": 0.3701,
      "step": 502
    },
    {
      "epoch": 0.11431818181818182,
      "grad_norm": 0.053074974566698074,
      "learning_rate": 0.00017733788395904438,
      "loss": 0.3535,
      "step": 503
    },
    {
      "epoch": 0.11454545454545455,
      "grad_norm": 0.05523960664868355,
      "learning_rate": 0.00017729237770193403,
      "loss": 0.3643,
      "step": 504
    },
    {
      "epoch": 0.11477272727272728,
      "grad_norm": 0.05195546895265579,
      "learning_rate": 0.00017724687144482368,
      "loss": 0.404,
      "step": 505
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.04938383400440216,
      "learning_rate": 0.0001772013651877133,
      "loss": 0.3542,
      "step": 506
    },
    {
      "epoch": 0.11522727272727273,
      "grad_norm": 0.047885384410619736,
      "learning_rate": 0.00017715585893060296,
      "loss": 0.34,
      "step": 507
    },
    {
      "epoch": 0.11545454545454545,
      "grad_norm": 0.07191961258649826,
      "learning_rate": 0.0001771103526734926,
      "loss": 0.4057,
      "step": 508
    },
    {
      "epoch": 0.11568181818181818,
      "grad_norm": 0.06573770940303802,
      "learning_rate": 0.00017706484641638227,
      "loss": 0.3893,
      "step": 509
    },
    {
      "epoch": 0.1159090909090909,
      "grad_norm": 0.05400474742054939,
      "learning_rate": 0.00017701934015927192,
      "loss": 0.3932,
      "step": 510
    },
    {
      "epoch": 0.11613636363636363,
      "grad_norm": 0.04864051938056946,
      "learning_rate": 0.00017697383390216154,
      "loss": 0.3016,
      "step": 511
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.04285908862948418,
      "learning_rate": 0.0001769283276450512,
      "loss": 0.2577,
      "step": 512
    },
    {
      "epoch": 0.11659090909090909,
      "grad_norm": 0.06170567125082016,
      "learning_rate": 0.00017688282138794085,
      "loss": 0.3586,
      "step": 513
    },
    {
      "epoch": 0.11681818181818182,
      "grad_norm": 0.05586836487054825,
      "learning_rate": 0.0001768373151308305,
      "loss": 0.3426,
      "step": 514
    },
    {
      "epoch": 0.11704545454545455,
      "grad_norm": 0.0479804128408432,
      "learning_rate": 0.00017679180887372016,
      "loss": 0.3401,
      "step": 515
    },
    {
      "epoch": 0.11727272727272728,
      "grad_norm": 0.060473065823316574,
      "learning_rate": 0.00017674630261660978,
      "loss": 0.3597,
      "step": 516
    },
    {
      "epoch": 0.1175,
      "grad_norm": 0.040526632219552994,
      "learning_rate": 0.00017670079635949943,
      "loss": 0.2912,
      "step": 517
    },
    {
      "epoch": 0.11772727272727272,
      "grad_norm": 0.06097572669386864,
      "learning_rate": 0.0001766552901023891,
      "loss": 0.357,
      "step": 518
    },
    {
      "epoch": 0.11795454545454545,
      "grad_norm": 0.0635143518447876,
      "learning_rate": 0.00017660978384527874,
      "loss": 0.3846,
      "step": 519
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 0.0519946850836277,
      "learning_rate": 0.0001765642775881684,
      "loss": 0.3287,
      "step": 520
    },
    {
      "epoch": 0.11840909090909091,
      "grad_norm": 0.043528564274311066,
      "learning_rate": 0.00017651877133105802,
      "loss": 0.3022,
      "step": 521
    },
    {
      "epoch": 0.11863636363636364,
      "grad_norm": 0.04115309566259384,
      "learning_rate": 0.00017647326507394767,
      "loss": 0.3127,
      "step": 522
    },
    {
      "epoch": 0.11886363636363637,
      "grad_norm": 0.04090046137571335,
      "learning_rate": 0.00017642775881683732,
      "loss": 0.295,
      "step": 523
    },
    {
      "epoch": 0.1190909090909091,
      "grad_norm": 0.04554478079080582,
      "learning_rate": 0.00017638225255972698,
      "loss": 0.3205,
      "step": 524
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.0461648665368557,
      "learning_rate": 0.00017633674630261663,
      "loss": 0.341,
      "step": 525
    },
    {
      "epoch": 0.11954545454545455,
      "grad_norm": 0.04367884248495102,
      "learning_rate": 0.00017629124004550626,
      "loss": 0.3525,
      "step": 526
    },
    {
      "epoch": 0.11977272727272727,
      "grad_norm": 0.06184211000800133,
      "learning_rate": 0.0001762457337883959,
      "loss": 0.3987,
      "step": 527
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0571463443338871,
      "learning_rate": 0.00017620022753128556,
      "loss": 0.3316,
      "step": 528
    },
    {
      "epoch": 0.12022727272727272,
      "grad_norm": 0.05888042598962784,
      "learning_rate": 0.00017615472127417521,
      "loss": 0.3834,
      "step": 529
    },
    {
      "epoch": 0.12045454545454545,
      "grad_norm": 0.041799236088991165,
      "learning_rate": 0.00017610921501706487,
      "loss": 0.303,
      "step": 530
    },
    {
      "epoch": 0.12068181818181818,
      "grad_norm": 0.04720882698893547,
      "learning_rate": 0.0001760637087599545,
      "loss": 0.3138,
      "step": 531
    },
    {
      "epoch": 0.12090909090909091,
      "grad_norm": 0.06838241964578629,
      "learning_rate": 0.00017601820250284415,
      "loss": 0.3526,
      "step": 532
    },
    {
      "epoch": 0.12113636363636364,
      "grad_norm": 0.047062356024980545,
      "learning_rate": 0.0001759726962457338,
      "loss": 0.309,
      "step": 533
    },
    {
      "epoch": 0.12136363636363637,
      "grad_norm": 0.06649047881364822,
      "learning_rate": 0.00017592718998862345,
      "loss": 0.3659,
      "step": 534
    },
    {
      "epoch": 0.1215909090909091,
      "grad_norm": 0.04611698165535927,
      "learning_rate": 0.00017588168373151308,
      "loss": 0.3198,
      "step": 535
    },
    {
      "epoch": 0.12181818181818181,
      "grad_norm": 0.059690624475479126,
      "learning_rate": 0.00017583617747440273,
      "loss": 0.4155,
      "step": 536
    },
    {
      "epoch": 0.12204545454545454,
      "grad_norm": 0.061121463775634766,
      "learning_rate": 0.00017579067121729238,
      "loss": 0.3315,
      "step": 537
    },
    {
      "epoch": 0.12227272727272727,
      "grad_norm": 0.050696052610874176,
      "learning_rate": 0.00017574516496018203,
      "loss": 0.3069,
      "step": 538
    },
    {
      "epoch": 0.1225,
      "grad_norm": 0.05303850397467613,
      "learning_rate": 0.0001756996587030717,
      "loss": 0.3044,
      "step": 539
    },
    {
      "epoch": 0.12272727272727273,
      "grad_norm": 0.03772919252514839,
      "learning_rate": 0.00017565415244596134,
      "loss": 0.2731,
      "step": 540
    },
    {
      "epoch": 0.12295454545454546,
      "grad_norm": 0.0720243975520134,
      "learning_rate": 0.00017560864618885097,
      "loss": 0.3714,
      "step": 541
    },
    {
      "epoch": 0.12318181818181818,
      "grad_norm": 0.06448902934789658,
      "learning_rate": 0.00017556313993174062,
      "loss": 0.3276,
      "step": 542
    },
    {
      "epoch": 0.12340909090909091,
      "grad_norm": 0.0444146953523159,
      "learning_rate": 0.00017551763367463027,
      "loss": 0.3047,
      "step": 543
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.051417578011751175,
      "learning_rate": 0.00017547212741751992,
      "loss": 0.3524,
      "step": 544
    },
    {
      "epoch": 0.12386363636363637,
      "grad_norm": 0.054416827857494354,
      "learning_rate": 0.00017542662116040955,
      "loss": 0.3385,
      "step": 545
    },
    {
      "epoch": 0.12409090909090909,
      "grad_norm": 0.06362534314393997,
      "learning_rate": 0.0001753811149032992,
      "loss": 0.3353,
      "step": 546
    },
    {
      "epoch": 0.12431818181818181,
      "grad_norm": 0.05979186296463013,
      "learning_rate": 0.00017533560864618886,
      "loss": 0.3371,
      "step": 547
    },
    {
      "epoch": 0.12454545454545454,
      "grad_norm": 0.060951244086027145,
      "learning_rate": 0.0001752901023890785,
      "loss": 0.4137,
      "step": 548
    },
    {
      "epoch": 0.12477272727272727,
      "grad_norm": 0.061402805149555206,
      "learning_rate": 0.00017524459613196816,
      "loss": 0.4125,
      "step": 549
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.05218954756855965,
      "learning_rate": 0.00017519908987485781,
      "loss": 0.3451,
      "step": 550
    },
    {
      "epoch": 0.12522727272727271,
      "grad_norm": 0.03329073637723923,
      "learning_rate": 0.00017515358361774744,
      "loss": 0.2757,
      "step": 551
    },
    {
      "epoch": 0.12545454545454546,
      "grad_norm": 0.057966139167547226,
      "learning_rate": 0.0001751080773606371,
      "loss": 0.3924,
      "step": 552
    },
    {
      "epoch": 0.12568181818181817,
      "grad_norm": 0.06409616768360138,
      "learning_rate": 0.00017506257110352675,
      "loss": 0.4139,
      "step": 553
    },
    {
      "epoch": 0.12590909090909091,
      "grad_norm": 0.04970385506749153,
      "learning_rate": 0.0001750170648464164,
      "loss": 0.352,
      "step": 554
    },
    {
      "epoch": 0.12613636363636363,
      "grad_norm": 0.04743368178606033,
      "learning_rate": 0.00017497155858930602,
      "loss": 0.3293,
      "step": 555
    },
    {
      "epoch": 0.12636363636363637,
      "grad_norm": 0.053502585738897324,
      "learning_rate": 0.00017492605233219568,
      "loss": 0.3174,
      "step": 556
    },
    {
      "epoch": 0.1265909090909091,
      "grad_norm": 0.04553792625665665,
      "learning_rate": 0.00017488054607508533,
      "loss": 0.2885,
      "step": 557
    },
    {
      "epoch": 0.12681818181818183,
      "grad_norm": 0.06171794608235359,
      "learning_rate": 0.00017483503981797498,
      "loss": 0.382,
      "step": 558
    },
    {
      "epoch": 0.12704545454545454,
      "grad_norm": 0.045575592666864395,
      "learning_rate": 0.00017478953356086464,
      "loss": 0.3104,
      "step": 559
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.049325451254844666,
      "learning_rate": 0.0001747440273037543,
      "loss": 0.3107,
      "step": 560
    },
    {
      "epoch": 0.1275,
      "grad_norm": 0.049099087715148926,
      "learning_rate": 0.00017469852104664391,
      "loss": 0.317,
      "step": 561
    },
    {
      "epoch": 0.12772727272727272,
      "grad_norm": 0.06723972409963608,
      "learning_rate": 0.00017465301478953357,
      "loss": 0.3813,
      "step": 562
    },
    {
      "epoch": 0.12795454545454546,
      "grad_norm": 0.0628022700548172,
      "learning_rate": 0.00017460750853242322,
      "loss": 0.3577,
      "step": 563
    },
    {
      "epoch": 0.12818181818181817,
      "grad_norm": 0.06596698611974716,
      "learning_rate": 0.00017456200227531287,
      "loss": 0.4014,
      "step": 564
    },
    {
      "epoch": 0.12840909090909092,
      "grad_norm": 0.05174726992845535,
      "learning_rate": 0.0001745164960182025,
      "loss": 0.3351,
      "step": 565
    },
    {
      "epoch": 0.12863636363636363,
      "grad_norm": 0.040938884019851685,
      "learning_rate": 0.00017447098976109215,
      "loss": 0.325,
      "step": 566
    },
    {
      "epoch": 0.12886363636363637,
      "grad_norm": 0.05042309686541557,
      "learning_rate": 0.0001744254835039818,
      "loss": 0.3353,
      "step": 567
    },
    {
      "epoch": 0.1290909090909091,
      "grad_norm": 0.04879538342356682,
      "learning_rate": 0.00017437997724687146,
      "loss": 0.2638,
      "step": 568
    },
    {
      "epoch": 0.1293181818181818,
      "grad_norm": 0.042511891573667526,
      "learning_rate": 0.0001743344709897611,
      "loss": 0.2822,
      "step": 569
    },
    {
      "epoch": 0.12954545454545455,
      "grad_norm": 0.05759049579501152,
      "learning_rate": 0.00017428896473265076,
      "loss": 0.3459,
      "step": 570
    },
    {
      "epoch": 0.12977272727272726,
      "grad_norm": 0.042338527739048004,
      "learning_rate": 0.0001742434584755404,
      "loss": 0.3182,
      "step": 571
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.03986377269029617,
      "learning_rate": 0.00017419795221843004,
      "loss": 0.2949,
      "step": 572
    },
    {
      "epoch": 0.13022727272727272,
      "grad_norm": 0.05399760976433754,
      "learning_rate": 0.0001741524459613197,
      "loss": 0.3137,
      "step": 573
    },
    {
      "epoch": 0.13045454545454546,
      "grad_norm": 0.051244694739580154,
      "learning_rate": 0.00017410693970420932,
      "loss": 0.3416,
      "step": 574
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.054884638637304306,
      "learning_rate": 0.00017406143344709897,
      "loss": 0.3605,
      "step": 575
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.06936892122030258,
      "learning_rate": 0.00017401592718998863,
      "loss": 0.3171,
      "step": 576
    },
    {
      "epoch": 0.13113636363636363,
      "grad_norm": 0.05332188680768013,
      "learning_rate": 0.00017397042093287828,
      "loss": 0.2865,
      "step": 577
    },
    {
      "epoch": 0.13136363636363638,
      "grad_norm": 0.0783487930893898,
      "learning_rate": 0.00017392491467576793,
      "loss": 0.431,
      "step": 578
    },
    {
      "epoch": 0.1315909090909091,
      "grad_norm": 0.04774473235011101,
      "learning_rate": 0.00017387940841865758,
      "loss": 0.3207,
      "step": 579
    },
    {
      "epoch": 0.1318181818181818,
      "grad_norm": 0.08117112517356873,
      "learning_rate": 0.00017383390216154724,
      "loss": 0.4135,
      "step": 580
    },
    {
      "epoch": 0.13204545454545455,
      "grad_norm": 0.057862523943185806,
      "learning_rate": 0.00017378839590443686,
      "loss": 0.358,
      "step": 581
    },
    {
      "epoch": 0.13227272727272726,
      "grad_norm": 0.05769921839237213,
      "learning_rate": 0.00017374288964732652,
      "loss": 0.326,
      "step": 582
    },
    {
      "epoch": 0.1325,
      "grad_norm": 0.05046248808503151,
      "learning_rate": 0.00017369738339021617,
      "loss": 0.3239,
      "step": 583
    },
    {
      "epoch": 0.13272727272727272,
      "grad_norm": 0.05109000951051712,
      "learning_rate": 0.0001736518771331058,
      "loss": 0.3667,
      "step": 584
    },
    {
      "epoch": 0.13295454545454546,
      "grad_norm": 0.0737946555018425,
      "learning_rate": 0.00017360637087599545,
      "loss": 0.4177,
      "step": 585
    },
    {
      "epoch": 0.13318181818181818,
      "grad_norm": 0.05659298971295357,
      "learning_rate": 0.0001735608646188851,
      "loss": 0.3654,
      "step": 586
    },
    {
      "epoch": 0.13340909090909092,
      "grad_norm": 0.05074284225702286,
      "learning_rate": 0.00017351535836177475,
      "loss": 0.3509,
      "step": 587
    },
    {
      "epoch": 0.13363636363636364,
      "grad_norm": 0.07358963787555695,
      "learning_rate": 0.0001734698521046644,
      "loss": 0.3431,
      "step": 588
    },
    {
      "epoch": 0.13386363636363635,
      "grad_norm": 0.05778662487864494,
      "learning_rate": 0.00017342434584755406,
      "loss": 0.3877,
      "step": 589
    },
    {
      "epoch": 0.1340909090909091,
      "grad_norm": 0.04150396212935448,
      "learning_rate": 0.0001733788395904437,
      "loss": 0.291,
      "step": 590
    },
    {
      "epoch": 0.1343181818181818,
      "grad_norm": 0.07104428112506866,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4049,
      "step": 591
    },
    {
      "epoch": 0.13454545454545455,
      "grad_norm": 0.05652973800897598,
      "learning_rate": 0.000173287827076223,
      "loss": 0.3709,
      "step": 592
    },
    {
      "epoch": 0.13477272727272727,
      "grad_norm": 0.05565880611538887,
      "learning_rate": 0.00017324232081911264,
      "loss": 0.3679,
      "step": 593
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.0393863283097744,
      "learning_rate": 0.00017319681456200227,
      "loss": 0.3153,
      "step": 594
    },
    {
      "epoch": 0.13522727272727272,
      "grad_norm": 0.06309524178504944,
      "learning_rate": 0.00017315130830489192,
      "loss": 0.2934,
      "step": 595
    },
    {
      "epoch": 0.13545454545454547,
      "grad_norm": 0.07739437371492386,
      "learning_rate": 0.00017310580204778157,
      "loss": 0.452,
      "step": 596
    },
    {
      "epoch": 0.13568181818181818,
      "grad_norm": 0.06021957844495773,
      "learning_rate": 0.00017306029579067123,
      "loss": 0.3995,
      "step": 597
    },
    {
      "epoch": 0.1359090909090909,
      "grad_norm": 0.0585373193025589,
      "learning_rate": 0.00017301478953356088,
      "loss": 0.4031,
      "step": 598
    },
    {
      "epoch": 0.13613636363636364,
      "grad_norm": 0.06119629740715027,
      "learning_rate": 0.00017296928327645053,
      "loss": 0.3463,
      "step": 599
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.07305646687746048,
      "learning_rate": 0.00017292377701934018,
      "loss": 0.3873,
      "step": 600
    },
    {
      "epoch": 0.1365909090909091,
      "grad_norm": 0.04913812875747681,
      "learning_rate": 0.0001728782707622298,
      "loss": 0.3161,
      "step": 601
    },
    {
      "epoch": 0.1368181818181818,
      "grad_norm": 0.045002859085798264,
      "learning_rate": 0.00017283276450511946,
      "loss": 0.3155,
      "step": 602
    },
    {
      "epoch": 0.13704545454545455,
      "grad_norm": 0.05177612975239754,
      "learning_rate": 0.0001727872582480091,
      "loss": 0.3281,
      "step": 603
    },
    {
      "epoch": 0.13727272727272727,
      "grad_norm": 0.048753589391708374,
      "learning_rate": 0.00017274175199089874,
      "loss": 0.32,
      "step": 604
    },
    {
      "epoch": 0.1375,
      "grad_norm": 0.046667832881212234,
      "learning_rate": 0.0001726962457337884,
      "loss": 0.2926,
      "step": 605
    },
    {
      "epoch": 0.13772727272727273,
      "grad_norm": 0.05407164245843887,
      "learning_rate": 0.00017265073947667805,
      "loss": 0.3481,
      "step": 606
    },
    {
      "epoch": 0.13795454545454544,
      "grad_norm": 0.045325398445129395,
      "learning_rate": 0.0001726052332195677,
      "loss": 0.3181,
      "step": 607
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.0504896454513073,
      "learning_rate": 0.00017255972696245735,
      "loss": 0.3707,
      "step": 608
    },
    {
      "epoch": 0.1384090909090909,
      "grad_norm": 0.043808501213788986,
      "learning_rate": 0.000172514220705347,
      "loss": 0.3257,
      "step": 609
    },
    {
      "epoch": 0.13863636363636364,
      "grad_norm": 0.051435571163892746,
      "learning_rate": 0.00017246871444823666,
      "loss": 0.3359,
      "step": 610
    },
    {
      "epoch": 0.13886363636363636,
      "grad_norm": 0.046076904982328415,
      "learning_rate": 0.00017242320819112628,
      "loss": 0.3304,
      "step": 611
    },
    {
      "epoch": 0.1390909090909091,
      "grad_norm": 0.06081700697541237,
      "learning_rate": 0.00017237770193401594,
      "loss": 0.3203,
      "step": 612
    },
    {
      "epoch": 0.1393181818181818,
      "grad_norm": 0.05314023792743683,
      "learning_rate": 0.00017233219567690556,
      "loss": 0.3148,
      "step": 613
    },
    {
      "epoch": 0.13954545454545456,
      "grad_norm": 0.038438256829977036,
      "learning_rate": 0.00017228668941979522,
      "loss": 0.3044,
      "step": 614
    },
    {
      "epoch": 0.13977272727272727,
      "grad_norm": 0.05356582626700401,
      "learning_rate": 0.00017224118316268487,
      "loss": 0.3274,
      "step": 615
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.047853726893663406,
      "learning_rate": 0.00017219567690557452,
      "loss": 0.2949,
      "step": 616
    },
    {
      "epoch": 0.14022727272727273,
      "grad_norm": 0.04228794947266579,
      "learning_rate": 0.00017215017064846417,
      "loss": 0.3035,
      "step": 617
    },
    {
      "epoch": 0.14045454545454544,
      "grad_norm": 0.05854463577270508,
      "learning_rate": 0.00017210466439135383,
      "loss": 0.3495,
      "step": 618
    },
    {
      "epoch": 0.14068181818181819,
      "grad_norm": 0.053054433315992355,
      "learning_rate": 0.00017205915813424348,
      "loss": 0.3411,
      "step": 619
    },
    {
      "epoch": 0.1409090909090909,
      "grad_norm": 0.049055423587560654,
      "learning_rate": 0.00017201365187713313,
      "loss": 0.329,
      "step": 620
    },
    {
      "epoch": 0.14113636363636364,
      "grad_norm": 0.0430474653840065,
      "learning_rate": 0.00017196814562002276,
      "loss": 0.3093,
      "step": 621
    },
    {
      "epoch": 0.14136363636363636,
      "grad_norm": 0.051821883767843246,
      "learning_rate": 0.0001719226393629124,
      "loss": 0.38,
      "step": 622
    },
    {
      "epoch": 0.1415909090909091,
      "grad_norm": 0.057876504957675934,
      "learning_rate": 0.00017187713310580204,
      "loss": 0.3641,
      "step": 623
    },
    {
      "epoch": 0.14181818181818182,
      "grad_norm": 0.04312726855278015,
      "learning_rate": 0.0001718316268486917,
      "loss": 0.3107,
      "step": 624
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 0.046385280787944794,
      "learning_rate": 0.00017178612059158134,
      "loss": 0.2795,
      "step": 625
    },
    {
      "epoch": 0.14227272727272727,
      "grad_norm": 0.05152227729558945,
      "learning_rate": 0.000171740614334471,
      "loss": 0.3397,
      "step": 626
    },
    {
      "epoch": 0.1425,
      "grad_norm": 0.06464307010173798,
      "learning_rate": 0.00017169510807736065,
      "loss": 0.3995,
      "step": 627
    },
    {
      "epoch": 0.14272727272727273,
      "grad_norm": 0.050607819110155106,
      "learning_rate": 0.0001716496018202503,
      "loss": 0.3286,
      "step": 628
    },
    {
      "epoch": 0.14295454545454545,
      "grad_norm": 0.04270501807332039,
      "learning_rate": 0.00017160409556313995,
      "loss": 0.311,
      "step": 629
    },
    {
      "epoch": 0.1431818181818182,
      "grad_norm": 0.05256469175219536,
      "learning_rate": 0.0001715585893060296,
      "loss": 0.3289,
      "step": 630
    },
    {
      "epoch": 0.1434090909090909,
      "grad_norm": 0.060144394636154175,
      "learning_rate": 0.00017151308304891923,
      "loss": 0.4091,
      "step": 631
    },
    {
      "epoch": 0.14363636363636365,
      "grad_norm": 0.05262402072548866,
      "learning_rate": 0.00017146757679180888,
      "loss": 0.3646,
      "step": 632
    },
    {
      "epoch": 0.14386363636363636,
      "grad_norm": 0.04710402339696884,
      "learning_rate": 0.0001714220705346985,
      "loss": 0.3226,
      "step": 633
    },
    {
      "epoch": 0.1440909090909091,
      "grad_norm": 0.049272798001766205,
      "learning_rate": 0.00017137656427758816,
      "loss": 0.3624,
      "step": 634
    },
    {
      "epoch": 0.14431818181818182,
      "grad_norm": 0.04482449218630791,
      "learning_rate": 0.00017133105802047782,
      "loss": 0.3156,
      "step": 635
    },
    {
      "epoch": 0.14454545454545453,
      "grad_norm": 0.05434593930840492,
      "learning_rate": 0.00017128555176336747,
      "loss": 0.3398,
      "step": 636
    },
    {
      "epoch": 0.14477272727272728,
      "grad_norm": 0.049304936081171036,
      "learning_rate": 0.00017124004550625712,
      "loss": 0.3186,
      "step": 637
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.04786667227745056,
      "learning_rate": 0.00017119453924914677,
      "loss": 0.3292,
      "step": 638
    },
    {
      "epoch": 0.14522727272727273,
      "grad_norm": 0.0456882119178772,
      "learning_rate": 0.00017114903299203643,
      "loss": 0.3257,
      "step": 639
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.04663660749793053,
      "learning_rate": 0.00017110352673492608,
      "loss": 0.2635,
      "step": 640
    },
    {
      "epoch": 0.1456818181818182,
      "grad_norm": 0.04071159288287163,
      "learning_rate": 0.0001710580204778157,
      "loss": 0.3175,
      "step": 641
    },
    {
      "epoch": 0.1459090909090909,
      "grad_norm": 0.04430326074361801,
      "learning_rate": 0.00017101251422070533,
      "loss": 0.3078,
      "step": 642
    },
    {
      "epoch": 0.14613636363636365,
      "grad_norm": 0.05327078700065613,
      "learning_rate": 0.00017096700796359498,
      "loss": 0.3529,
      "step": 643
    },
    {
      "epoch": 0.14636363636363636,
      "grad_norm": 0.05894209071993828,
      "learning_rate": 0.00017092150170648464,
      "loss": 0.3358,
      "step": 644
    },
    {
      "epoch": 0.14659090909090908,
      "grad_norm": 0.058246828615665436,
      "learning_rate": 0.0001708759954493743,
      "loss": 0.3699,
      "step": 645
    },
    {
      "epoch": 0.14681818181818182,
      "grad_norm": 0.06031551957130432,
      "learning_rate": 0.00017083048919226394,
      "loss": 0.3793,
      "step": 646
    },
    {
      "epoch": 0.14704545454545453,
      "grad_norm": 0.05090085417032242,
      "learning_rate": 0.0001707849829351536,
      "loss": 0.3106,
      "step": 647
    },
    {
      "epoch": 0.14727272727272728,
      "grad_norm": 0.04874105751514435,
      "learning_rate": 0.00017073947667804325,
      "loss": 0.299,
      "step": 648
    },
    {
      "epoch": 0.1475,
      "grad_norm": 0.05022585019469261,
      "learning_rate": 0.0001706939704209329,
      "loss": 0.3184,
      "step": 649
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.06317684799432755,
      "learning_rate": 0.00017064846416382255,
      "loss": 0.3565,
      "step": 650
    },
    {
      "epoch": 0.14795454545454545,
      "grad_norm": 0.06075165420770645,
      "learning_rate": 0.00017060295790671218,
      "loss": 0.3699,
      "step": 651
    },
    {
      "epoch": 0.1481818181818182,
      "grad_norm": 0.07328740507364273,
      "learning_rate": 0.0001705574516496018,
      "loss": 0.356,
      "step": 652
    },
    {
      "epoch": 0.1484090909090909,
      "grad_norm": 0.06206810101866722,
      "learning_rate": 0.00017051194539249146,
      "loss": 0.4059,
      "step": 653
    },
    {
      "epoch": 0.14863636363636365,
      "grad_norm": 0.06260309368371964,
      "learning_rate": 0.0001704664391353811,
      "loss": 0.31,
      "step": 654
    },
    {
      "epoch": 0.14886363636363636,
      "grad_norm": 0.04896385222673416,
      "learning_rate": 0.00017042093287827076,
      "loss": 0.3764,
      "step": 655
    },
    {
      "epoch": 0.14909090909090908,
      "grad_norm": 0.05477331578731537,
      "learning_rate": 0.00017037542662116042,
      "loss": 0.3634,
      "step": 656
    },
    {
      "epoch": 0.14931818181818182,
      "grad_norm": 0.05315574258565903,
      "learning_rate": 0.00017032992036405007,
      "loss": 0.2815,
      "step": 657
    },
    {
      "epoch": 0.14954545454545454,
      "grad_norm": 0.0532134510576725,
      "learning_rate": 0.00017028441410693972,
      "loss": 0.3717,
      "step": 658
    },
    {
      "epoch": 0.14977272727272728,
      "grad_norm": 0.047621943056583405,
      "learning_rate": 0.00017023890784982938,
      "loss": 0.3022,
      "step": 659
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.04320667311549187,
      "learning_rate": 0.00017019340159271903,
      "loss": 0.3304,
      "step": 660
    },
    {
      "epoch": 0.15022727272727274,
      "grad_norm": 0.055900976061820984,
      "learning_rate": 0.00017014789533560865,
      "loss": 0.3158,
      "step": 661
    },
    {
      "epoch": 0.15045454545454545,
      "grad_norm": 0.04508920758962631,
      "learning_rate": 0.00017010238907849828,
      "loss": 0.3222,
      "step": 662
    },
    {
      "epoch": 0.1506818181818182,
      "grad_norm": 0.050054389983415604,
      "learning_rate": 0.00017005688282138793,
      "loss": 0.3709,
      "step": 663
    },
    {
      "epoch": 0.1509090909090909,
      "grad_norm": 0.04939066246151924,
      "learning_rate": 0.00017001137656427759,
      "loss": 0.3139,
      "step": 664
    },
    {
      "epoch": 0.15113636363636362,
      "grad_norm": 0.05826349928975105,
      "learning_rate": 0.00016996587030716724,
      "loss": 0.3522,
      "step": 665
    },
    {
      "epoch": 0.15136363636363637,
      "grad_norm": 0.07101041078567505,
      "learning_rate": 0.0001699203640500569,
      "loss": 0.3904,
      "step": 666
    },
    {
      "epoch": 0.15159090909090908,
      "grad_norm": 0.05062033608555794,
      "learning_rate": 0.00016987485779294654,
      "loss": 0.3472,
      "step": 667
    },
    {
      "epoch": 0.15181818181818182,
      "grad_norm": 0.042526017874479294,
      "learning_rate": 0.0001698293515358362,
      "loss": 0.332,
      "step": 668
    },
    {
      "epoch": 0.15204545454545454,
      "grad_norm": 0.05941377580165863,
      "learning_rate": 0.00016978384527872585,
      "loss": 0.363,
      "step": 669
    },
    {
      "epoch": 0.15227272727272728,
      "grad_norm": 0.08939093351364136,
      "learning_rate": 0.0001697383390216155,
      "loss": 0.3747,
      "step": 670
    },
    {
      "epoch": 0.1525,
      "grad_norm": 0.04039619863033295,
      "learning_rate": 0.00016969283276450513,
      "loss": 0.3103,
      "step": 671
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.05460066348314285,
      "learning_rate": 0.00016964732650739475,
      "loss": 0.3245,
      "step": 672
    },
    {
      "epoch": 0.15295454545454545,
      "grad_norm": 0.04589095711708069,
      "learning_rate": 0.0001696018202502844,
      "loss": 0.3294,
      "step": 673
    },
    {
      "epoch": 0.15318181818181817,
      "grad_norm": 0.04253242164850235,
      "learning_rate": 0.00016955631399317406,
      "loss": 0.2907,
      "step": 674
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.07318115234375,
      "learning_rate": 0.0001695108077360637,
      "loss": 0.4044,
      "step": 675
    },
    {
      "epoch": 0.15363636363636363,
      "grad_norm": 0.047347862273454666,
      "learning_rate": 0.00016946530147895337,
      "loss": 0.3784,
      "step": 676
    },
    {
      "epoch": 0.15386363636363637,
      "grad_norm": 0.07240953296422958,
      "learning_rate": 0.00016941979522184302,
      "loss": 0.3947,
      "step": 677
    },
    {
      "epoch": 0.15409090909090908,
      "grad_norm": 0.04459661245346069,
      "learning_rate": 0.00016937428896473267,
      "loss": 0.3651,
      "step": 678
    },
    {
      "epoch": 0.15431818181818183,
      "grad_norm": 0.050219763070344925,
      "learning_rate": 0.00016932878270762232,
      "loss": 0.3167,
      "step": 679
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.05519896373152733,
      "learning_rate": 0.00016928327645051198,
      "loss": 0.3012,
      "step": 680
    },
    {
      "epoch": 0.15477272727272728,
      "grad_norm": 0.04953693971037865,
      "learning_rate": 0.0001692377701934016,
      "loss": 0.321,
      "step": 681
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.04633258283138275,
      "learning_rate": 0.00016919226393629123,
      "loss": 0.3351,
      "step": 682
    },
    {
      "epoch": 0.1552272727272727,
      "grad_norm": 0.04900459945201874,
      "learning_rate": 0.00016914675767918088,
      "loss": 0.2967,
      "step": 683
    },
    {
      "epoch": 0.15545454545454546,
      "grad_norm": 0.04486610367894173,
      "learning_rate": 0.00016910125142207053,
      "loss": 0.3425,
      "step": 684
    },
    {
      "epoch": 0.15568181818181817,
      "grad_norm": 0.06709186732769012,
      "learning_rate": 0.00016905574516496019,
      "loss": 0.3698,
      "step": 685
    },
    {
      "epoch": 0.1559090909090909,
      "grad_norm": 0.05513433739542961,
      "learning_rate": 0.00016901023890784984,
      "loss": 0.3844,
      "step": 686
    },
    {
      "epoch": 0.15613636363636363,
      "grad_norm": 0.056900858879089355,
      "learning_rate": 0.0001689647326507395,
      "loss": 0.3506,
      "step": 687
    },
    {
      "epoch": 0.15636363636363637,
      "grad_norm": 0.051282886415719986,
      "learning_rate": 0.00016891922639362914,
      "loss": 0.3538,
      "step": 688
    },
    {
      "epoch": 0.1565909090909091,
      "grad_norm": 0.06552572548389435,
      "learning_rate": 0.0001688737201365188,
      "loss": 0.3841,
      "step": 689
    },
    {
      "epoch": 0.15681818181818183,
      "grad_norm": 0.0628509595990181,
      "learning_rate": 0.00016882821387940842,
      "loss": 0.3489,
      "step": 690
    },
    {
      "epoch": 0.15704545454545454,
      "grad_norm": 0.044930458068847656,
      "learning_rate": 0.00016878270762229808,
      "loss": 0.2778,
      "step": 691
    },
    {
      "epoch": 0.1572727272727273,
      "grad_norm": 0.04573524743318558,
      "learning_rate": 0.0001687372013651877,
      "loss": 0.3014,
      "step": 692
    },
    {
      "epoch": 0.1575,
      "grad_norm": 0.04799433797597885,
      "learning_rate": 0.00016869169510807735,
      "loss": 0.3565,
      "step": 693
    },
    {
      "epoch": 0.15772727272727272,
      "grad_norm": 0.043702226132154465,
      "learning_rate": 0.000168646188850967,
      "loss": 0.3064,
      "step": 694
    },
    {
      "epoch": 0.15795454545454546,
      "grad_norm": 0.048989418894052505,
      "learning_rate": 0.00016860068259385666,
      "loss": 0.3487,
      "step": 695
    },
    {
      "epoch": 0.15818181818181817,
      "grad_norm": 0.0508289560675621,
      "learning_rate": 0.0001685551763367463,
      "loss": 0.3847,
      "step": 696
    },
    {
      "epoch": 0.15840909090909092,
      "grad_norm": 0.05374765768647194,
      "learning_rate": 0.00016850967007963597,
      "loss": 0.3571,
      "step": 697
    },
    {
      "epoch": 0.15863636363636363,
      "grad_norm": 0.04379880800843239,
      "learning_rate": 0.00016846416382252562,
      "loss": 0.3604,
      "step": 698
    },
    {
      "epoch": 0.15886363636363637,
      "grad_norm": 0.06197090446949005,
      "learning_rate": 0.00016841865756541527,
      "loss": 0.3436,
      "step": 699
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.061489418148994446,
      "learning_rate": 0.0001683731513083049,
      "loss": 0.3716,
      "step": 700
    },
    {
      "epoch": 0.15931818181818183,
      "grad_norm": 0.046326104551553726,
      "learning_rate": 0.00016832764505119455,
      "loss": 0.3465,
      "step": 701
    },
    {
      "epoch": 0.15954545454545455,
      "grad_norm": 0.04271318390965462,
      "learning_rate": 0.00016828213879408418,
      "loss": 0.3334,
      "step": 702
    },
    {
      "epoch": 0.15977272727272726,
      "grad_norm": 0.04519316181540489,
      "learning_rate": 0.00016823663253697383,
      "loss": 0.3442,
      "step": 703
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.04407531023025513,
      "learning_rate": 0.00016819112627986348,
      "loss": 0.303,
      "step": 704
    },
    {
      "epoch": 0.16022727272727272,
      "grad_norm": 0.05025390163064003,
      "learning_rate": 0.00016814562002275313,
      "loss": 0.3822,
      "step": 705
    },
    {
      "epoch": 0.16045454545454546,
      "grad_norm": 0.05766066536307335,
      "learning_rate": 0.0001681001137656428,
      "loss": 0.3638,
      "step": 706
    },
    {
      "epoch": 0.16068181818181818,
      "grad_norm": 0.03592540696263313,
      "learning_rate": 0.00016805460750853244,
      "loss": 0.2968,
      "step": 707
    },
    {
      "epoch": 0.16090909090909092,
      "grad_norm": 0.0392569974064827,
      "learning_rate": 0.0001680091012514221,
      "loss": 0.3054,
      "step": 708
    },
    {
      "epoch": 0.16113636363636363,
      "grad_norm": 0.05409705266356468,
      "learning_rate": 0.00016796359499431175,
      "loss": 0.3976,
      "step": 709
    },
    {
      "epoch": 0.16136363636363638,
      "grad_norm": 0.0571899376809597,
      "learning_rate": 0.00016791808873720137,
      "loss": 0.4364,
      "step": 710
    },
    {
      "epoch": 0.1615909090909091,
      "grad_norm": 0.049775492399930954,
      "learning_rate": 0.00016787258248009102,
      "loss": 0.3651,
      "step": 711
    },
    {
      "epoch": 0.1618181818181818,
      "grad_norm": 0.054376307874917984,
      "learning_rate": 0.00016782707622298065,
      "loss": 0.3379,
      "step": 712
    },
    {
      "epoch": 0.16204545454545455,
      "grad_norm": 0.04361521825194359,
      "learning_rate": 0.0001677815699658703,
      "loss": 0.323,
      "step": 713
    },
    {
      "epoch": 0.16227272727272726,
      "grad_norm": 0.04477905482053757,
      "learning_rate": 0.00016773606370875996,
      "loss": 0.3283,
      "step": 714
    },
    {
      "epoch": 0.1625,
      "grad_norm": 0.050205297768116,
      "learning_rate": 0.0001676905574516496,
      "loss": 0.3298,
      "step": 715
    },
    {
      "epoch": 0.16272727272727272,
      "grad_norm": 0.041588637977838516,
      "learning_rate": 0.00016764505119453926,
      "loss": 0.291,
      "step": 716
    },
    {
      "epoch": 0.16295454545454546,
      "grad_norm": 0.047256167978048325,
      "learning_rate": 0.00016759954493742891,
      "loss": 0.3153,
      "step": 717
    },
    {
      "epoch": 0.16318181818181818,
      "grad_norm": 0.05178619921207428,
      "learning_rate": 0.00016755403868031857,
      "loss": 0.3576,
      "step": 718
    },
    {
      "epoch": 0.16340909090909092,
      "grad_norm": 0.04913974180817604,
      "learning_rate": 0.00016750853242320822,
      "loss": 0.3587,
      "step": 719
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.05529436469078064,
      "learning_rate": 0.00016746302616609785,
      "loss": 0.3891,
      "step": 720
    },
    {
      "epoch": 0.16386363636363635,
      "grad_norm": 0.062419790774583817,
      "learning_rate": 0.0001674175199089875,
      "loss": 0.4055,
      "step": 721
    },
    {
      "epoch": 0.1640909090909091,
      "grad_norm": 0.052793193608522415,
      "learning_rate": 0.00016737201365187712,
      "loss": 0.3211,
      "step": 722
    },
    {
      "epoch": 0.1643181818181818,
      "grad_norm": 0.04366283491253853,
      "learning_rate": 0.00016732650739476678,
      "loss": 0.3005,
      "step": 723
    },
    {
      "epoch": 0.16454545454545455,
      "grad_norm": 0.05479428544640541,
      "learning_rate": 0.00016728100113765643,
      "loss": 0.3048,
      "step": 724
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.06716248393058777,
      "learning_rate": 0.00016723549488054608,
      "loss": 0.4225,
      "step": 725
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.06333471834659576,
      "learning_rate": 0.00016718998862343573,
      "loss": 0.3854,
      "step": 726
    },
    {
      "epoch": 0.16522727272727272,
      "grad_norm": 0.04888129606842995,
      "learning_rate": 0.0001671444823663254,
      "loss": 0.3291,
      "step": 727
    },
    {
      "epoch": 0.16545454545454547,
      "grad_norm": 0.04863114655017853,
      "learning_rate": 0.00016709897610921504,
      "loss": 0.2907,
      "step": 728
    },
    {
      "epoch": 0.16568181818181818,
      "grad_norm": 0.06139031797647476,
      "learning_rate": 0.0001670534698521047,
      "loss": 0.3689,
      "step": 729
    },
    {
      "epoch": 0.16590909090909092,
      "grad_norm": 0.05587828531861305,
      "learning_rate": 0.00016700796359499432,
      "loss": 0.2882,
      "step": 730
    },
    {
      "epoch": 0.16613636363636364,
      "grad_norm": 0.06087227538228035,
      "learning_rate": 0.00016696245733788397,
      "loss": 0.4079,
      "step": 731
    },
    {
      "epoch": 0.16636363636363635,
      "grad_norm": 0.03791461139917374,
      "learning_rate": 0.0001669169510807736,
      "loss": 0.2537,
      "step": 732
    },
    {
      "epoch": 0.1665909090909091,
      "grad_norm": 0.05281712859869003,
      "learning_rate": 0.00016687144482366325,
      "loss": 0.3521,
      "step": 733
    },
    {
      "epoch": 0.1668181818181818,
      "grad_norm": 0.04719823598861694,
      "learning_rate": 0.0001668259385665529,
      "loss": 0.3214,
      "step": 734
    },
    {
      "epoch": 0.16704545454545455,
      "grad_norm": 0.061412662267684937,
      "learning_rate": 0.00016678043230944256,
      "loss": 0.3767,
      "step": 735
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.07336503267288208,
      "learning_rate": 0.0001667349260523322,
      "loss": 0.4001,
      "step": 736
    },
    {
      "epoch": 0.1675,
      "grad_norm": 0.04788077995181084,
      "learning_rate": 0.00016668941979522186,
      "loss": 0.3477,
      "step": 737
    },
    {
      "epoch": 0.16772727272727272,
      "grad_norm": 0.034000463783741,
      "learning_rate": 0.00016664391353811151,
      "loss": 0.2759,
      "step": 738
    },
    {
      "epoch": 0.16795454545454547,
      "grad_norm": 0.046261392533779144,
      "learning_rate": 0.00016659840728100117,
      "loss": 0.2938,
      "step": 739
    },
    {
      "epoch": 0.16818181818181818,
      "grad_norm": 0.04872245714068413,
      "learning_rate": 0.0001665529010238908,
      "loss": 0.3191,
      "step": 740
    },
    {
      "epoch": 0.1684090909090909,
      "grad_norm": 0.052715010941028595,
      "learning_rate": 0.00016650739476678045,
      "loss": 0.2954,
      "step": 741
    },
    {
      "epoch": 0.16863636363636364,
      "grad_norm": 0.062319837510585785,
      "learning_rate": 0.00016646188850967007,
      "loss": 0.4171,
      "step": 742
    },
    {
      "epoch": 0.16886363636363635,
      "grad_norm": 0.04400405287742615,
      "learning_rate": 0.00016641638225255972,
      "loss": 0.305,
      "step": 743
    },
    {
      "epoch": 0.1690909090909091,
      "grad_norm": 0.05880732089281082,
      "learning_rate": 0.00016637087599544938,
      "loss": 0.3432,
      "step": 744
    },
    {
      "epoch": 0.1693181818181818,
      "grad_norm": 0.05186164006590843,
      "learning_rate": 0.00016632536973833903,
      "loss": 0.3297,
      "step": 745
    },
    {
      "epoch": 0.16954545454545455,
      "grad_norm": 0.0438905693590641,
      "learning_rate": 0.00016627986348122868,
      "loss": 0.2893,
      "step": 746
    },
    {
      "epoch": 0.16977272727272727,
      "grad_norm": 0.055492036044597626,
      "learning_rate": 0.00016623435722411834,
      "loss": 0.367,
      "step": 747
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.05919848755002022,
      "learning_rate": 0.000166188850967008,
      "loss": 0.3358,
      "step": 748
    },
    {
      "epoch": 0.17022727272727273,
      "grad_norm": 0.04664396122097969,
      "learning_rate": 0.00016614334470989761,
      "loss": 0.3369,
      "step": 749
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 0.05963905528187752,
      "learning_rate": 0.00016609783845278727,
      "loss": 0.3317,
      "step": 750
    },
    {
      "epoch": 0.17068181818181818,
      "grad_norm": 0.059015557169914246,
      "learning_rate": 0.00016605233219567692,
      "loss": 0.363,
      "step": 751
    },
    {
      "epoch": 0.1709090909090909,
      "grad_norm": 0.047748394310474396,
      "learning_rate": 0.00016600682593856655,
      "loss": 0.3619,
      "step": 752
    },
    {
      "epoch": 0.17113636363636364,
      "grad_norm": 0.05795298516750336,
      "learning_rate": 0.0001659613196814562,
      "loss": 0.3665,
      "step": 753
    },
    {
      "epoch": 0.17136363636363636,
      "grad_norm": 0.04615363851189613,
      "learning_rate": 0.00016591581342434585,
      "loss": 0.2971,
      "step": 754
    },
    {
      "epoch": 0.1715909090909091,
      "grad_norm": 0.05229806900024414,
      "learning_rate": 0.0001658703071672355,
      "loss": 0.3701,
      "step": 755
    },
    {
      "epoch": 0.17181818181818181,
      "grad_norm": 0.054608941078186035,
      "learning_rate": 0.00016582480091012516,
      "loss": 0.3378,
      "step": 756
    },
    {
      "epoch": 0.17204545454545456,
      "grad_norm": 0.054356105625629425,
      "learning_rate": 0.0001657792946530148,
      "loss": 0.3518,
      "step": 757
    },
    {
      "epoch": 0.17227272727272727,
      "grad_norm": 0.05084051191806793,
      "learning_rate": 0.00016573378839590446,
      "loss": 0.3586,
      "step": 758
    },
    {
      "epoch": 0.1725,
      "grad_norm": 0.05115451291203499,
      "learning_rate": 0.0001656882821387941,
      "loss": 0.3466,
      "step": 759
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 0.03771866112947464,
      "learning_rate": 0.00016564277588168374,
      "loss": 0.293,
      "step": 760
    },
    {
      "epoch": 0.17295454545454544,
      "grad_norm": 0.05749303475022316,
      "learning_rate": 0.0001655972696245734,
      "loss": 0.3567,
      "step": 761
    },
    {
      "epoch": 0.1731818181818182,
      "grad_norm": 0.04397173598408699,
      "learning_rate": 0.00016555176336746302,
      "loss": 0.3558,
      "step": 762
    },
    {
      "epoch": 0.1734090909090909,
      "grad_norm": 0.05573895946145058,
      "learning_rate": 0.00016550625711035267,
      "loss": 0.3274,
      "step": 763
    },
    {
      "epoch": 0.17363636363636364,
      "grad_norm": 0.04954858124256134,
      "learning_rate": 0.00016546075085324233,
      "loss": 0.3163,
      "step": 764
    },
    {
      "epoch": 0.17386363636363636,
      "grad_norm": 0.04115207493305206,
      "learning_rate": 0.00016541524459613198,
      "loss": 0.2972,
      "step": 765
    },
    {
      "epoch": 0.1740909090909091,
      "grad_norm": 0.04579968750476837,
      "learning_rate": 0.00016536973833902163,
      "loss": 0.3237,
      "step": 766
    },
    {
      "epoch": 0.17431818181818182,
      "grad_norm": 0.05089529603719711,
      "learning_rate": 0.00016532423208191128,
      "loss": 0.3077,
      "step": 767
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.04252292588353157,
      "learning_rate": 0.00016527872582480094,
      "loss": 0.3221,
      "step": 768
    },
    {
      "epoch": 0.17477272727272727,
      "grad_norm": 0.04599340632557869,
      "learning_rate": 0.00016523321956769056,
      "loss": 0.2954,
      "step": 769
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.05692388489842415,
      "learning_rate": 0.00016518771331058022,
      "loss": 0.3739,
      "step": 770
    },
    {
      "epoch": 0.17522727272727273,
      "grad_norm": 0.04172151908278465,
      "learning_rate": 0.00016514220705346987,
      "loss": 0.3064,
      "step": 771
    },
    {
      "epoch": 0.17545454545454545,
      "grad_norm": 0.04848567396402359,
      "learning_rate": 0.0001650967007963595,
      "loss": 0.3471,
      "step": 772
    },
    {
      "epoch": 0.1756818181818182,
      "grad_norm": 0.04431639611721039,
      "learning_rate": 0.00016505119453924915,
      "loss": 0.3623,
      "step": 773
    },
    {
      "epoch": 0.1759090909090909,
      "grad_norm": 0.04839977249503136,
      "learning_rate": 0.0001650056882821388,
      "loss": 0.3547,
      "step": 774
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.057611074298620224,
      "learning_rate": 0.00016496018202502845,
      "loss": 0.2775,
      "step": 775
    },
    {
      "epoch": 0.17636363636363636,
      "grad_norm": 0.0465814583003521,
      "learning_rate": 0.0001649146757679181,
      "loss": 0.3743,
      "step": 776
    },
    {
      "epoch": 0.1765909090909091,
      "grad_norm": 0.04744240269064903,
      "learning_rate": 0.00016486916951080776,
      "loss": 0.304,
      "step": 777
    },
    {
      "epoch": 0.17681818181818182,
      "grad_norm": 0.04936050996184349,
      "learning_rate": 0.0001648236632536974,
      "loss": 0.319,
      "step": 778
    },
    {
      "epoch": 0.17704545454545453,
      "grad_norm": 0.054157156497240067,
      "learning_rate": 0.00016477815699658704,
      "loss": 0.3473,
      "step": 779
    },
    {
      "epoch": 0.17727272727272728,
      "grad_norm": 0.042026255279779434,
      "learning_rate": 0.0001647326507394767,
      "loss": 0.2847,
      "step": 780
    },
    {
      "epoch": 0.1775,
      "grad_norm": 0.05667484179139137,
      "learning_rate": 0.00016468714448236634,
      "loss": 0.3531,
      "step": 781
    },
    {
      "epoch": 0.17772727272727273,
      "grad_norm": 0.0480591356754303,
      "learning_rate": 0.00016464163822525597,
      "loss": 0.3373,
      "step": 782
    },
    {
      "epoch": 0.17795454545454545,
      "grad_norm": 0.055858634412288666,
      "learning_rate": 0.00016459613196814562,
      "loss": 0.34,
      "step": 783
    },
    {
      "epoch": 0.1781818181818182,
      "grad_norm": 0.052356746047735214,
      "learning_rate": 0.00016455062571103527,
      "loss": 0.3924,
      "step": 784
    },
    {
      "epoch": 0.1784090909090909,
      "grad_norm": 0.042978595942258835,
      "learning_rate": 0.00016450511945392493,
      "loss": 0.2685,
      "step": 785
    },
    {
      "epoch": 0.17863636363636365,
      "grad_norm": 0.0472528450191021,
      "learning_rate": 0.00016445961319681458,
      "loss": 0.2713,
      "step": 786
    },
    {
      "epoch": 0.17886363636363636,
      "grad_norm": 0.05753853917121887,
      "learning_rate": 0.00016441410693970423,
      "loss": 0.2869,
      "step": 787
    },
    {
      "epoch": 0.17909090909090908,
      "grad_norm": 0.06677425652742386,
      "learning_rate": 0.00016436860068259386,
      "loss": 0.3347,
      "step": 788
    },
    {
      "epoch": 0.17931818181818182,
      "grad_norm": 0.05280882492661476,
      "learning_rate": 0.0001643230944254835,
      "loss": 0.3533,
      "step": 789
    },
    {
      "epoch": 0.17954545454545454,
      "grad_norm": 0.04917595162987709,
      "learning_rate": 0.00016427758816837316,
      "loss": 0.3487,
      "step": 790
    },
    {
      "epoch": 0.17977272727272728,
      "grad_norm": 0.05697377398610115,
      "learning_rate": 0.00016423208191126282,
      "loss": 0.2847,
      "step": 791
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.056034982204437256,
      "learning_rate": 0.00016418657565415244,
      "loss": 0.3401,
      "step": 792
    },
    {
      "epoch": 0.18022727272727274,
      "grad_norm": 0.07267029583454132,
      "learning_rate": 0.0001641410693970421,
      "loss": 0.3743,
      "step": 793
    },
    {
      "epoch": 0.18045454545454545,
      "grad_norm": 0.0319678969681263,
      "learning_rate": 0.00016409556313993175,
      "loss": 0.2583,
      "step": 794
    },
    {
      "epoch": 0.1806818181818182,
      "grad_norm": 0.06679714471101761,
      "learning_rate": 0.0001640500568828214,
      "loss": 0.4081,
      "step": 795
    },
    {
      "epoch": 0.1809090909090909,
      "grad_norm": 0.053902510553598404,
      "learning_rate": 0.00016400455062571105,
      "loss": 0.3391,
      "step": 796
    },
    {
      "epoch": 0.18113636363636362,
      "grad_norm": 0.060669273138046265,
      "learning_rate": 0.0001639590443686007,
      "loss": 0.3317,
      "step": 797
    },
    {
      "epoch": 0.18136363636363637,
      "grad_norm": 0.060330696403980255,
      "learning_rate": 0.00016391353811149033,
      "loss": 0.3266,
      "step": 798
    },
    {
      "epoch": 0.18159090909090908,
      "grad_norm": 0.04535563662648201,
      "learning_rate": 0.00016386803185437998,
      "loss": 0.3544,
      "step": 799
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.03865009546279907,
      "learning_rate": 0.00016382252559726964,
      "loss": 0.3075,
      "step": 800
    },
    {
      "epoch": 0.18204545454545454,
      "grad_norm": 0.05783197656273842,
      "learning_rate": 0.0001637770193401593,
      "loss": 0.3807,
      "step": 801
    },
    {
      "epoch": 0.18227272727272728,
      "grad_norm": 0.042234744876623154,
      "learning_rate": 0.00016373151308304892,
      "loss": 0.333,
      "step": 802
    },
    {
      "epoch": 0.1825,
      "grad_norm": 0.04966728761792183,
      "learning_rate": 0.00016368600682593857,
      "loss": 0.273,
      "step": 803
    },
    {
      "epoch": 0.18272727272727274,
      "grad_norm": 0.04849299415946007,
      "learning_rate": 0.00016364050056882822,
      "loss": 0.3627,
      "step": 804
    },
    {
      "epoch": 0.18295454545454545,
      "grad_norm": 0.03936652094125748,
      "learning_rate": 0.00016359499431171787,
      "loss": 0.284,
      "step": 805
    },
    {
      "epoch": 0.1831818181818182,
      "grad_norm": 0.041521746665239334,
      "learning_rate": 0.00016354948805460753,
      "loss": 0.2941,
      "step": 806
    },
    {
      "epoch": 0.1834090909090909,
      "grad_norm": 0.051072463393211365,
      "learning_rate": 0.00016350398179749718,
      "loss": 0.3561,
      "step": 807
    },
    {
      "epoch": 0.18363636363636363,
      "grad_norm": 0.05672292411327362,
      "learning_rate": 0.0001634584755403868,
      "loss": 0.3185,
      "step": 808
    },
    {
      "epoch": 0.18386363636363637,
      "grad_norm": 0.06958356499671936,
      "learning_rate": 0.00016341296928327646,
      "loss": 0.4688,
      "step": 809
    },
    {
      "epoch": 0.18409090909090908,
      "grad_norm": 0.04928828030824661,
      "learning_rate": 0.0001633674630261661,
      "loss": 0.3269,
      "step": 810
    },
    {
      "epoch": 0.18431818181818183,
      "grad_norm": 0.06205107644200325,
      "learning_rate": 0.00016332195676905576,
      "loss": 0.3835,
      "step": 811
    },
    {
      "epoch": 0.18454545454545454,
      "grad_norm": 0.04386790096759796,
      "learning_rate": 0.0001632764505119454,
      "loss": 0.3058,
      "step": 812
    },
    {
      "epoch": 0.18477272727272728,
      "grad_norm": 0.05381979048252106,
      "learning_rate": 0.00016323094425483504,
      "loss": 0.3954,
      "step": 813
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.03334532678127289,
      "learning_rate": 0.0001631854379977247,
      "loss": 0.2653,
      "step": 814
    },
    {
      "epoch": 0.18522727272727274,
      "grad_norm": 0.054574623703956604,
      "learning_rate": 0.00016313993174061435,
      "loss": 0.3971,
      "step": 815
    },
    {
      "epoch": 0.18545454545454546,
      "grad_norm": 0.04033169150352478,
      "learning_rate": 0.000163094425483504,
      "loss": 0.3407,
      "step": 816
    },
    {
      "epoch": 0.18568181818181817,
      "grad_norm": 0.04686625301837921,
      "learning_rate": 0.00016304891922639363,
      "loss": 0.2862,
      "step": 817
    },
    {
      "epoch": 0.1859090909090909,
      "grad_norm": 0.05503065139055252,
      "learning_rate": 0.00016300341296928328,
      "loss": 0.3349,
      "step": 818
    },
    {
      "epoch": 0.18613636363636363,
      "grad_norm": 0.03989046812057495,
      "learning_rate": 0.00016295790671217293,
      "loss": 0.2935,
      "step": 819
    },
    {
      "epoch": 0.18636363636363637,
      "grad_norm": 0.04418236389756203,
      "learning_rate": 0.00016291240045506259,
      "loss": 0.2736,
      "step": 820
    },
    {
      "epoch": 0.18659090909090909,
      "grad_norm": 0.051985953003168106,
      "learning_rate": 0.00016286689419795224,
      "loss": 0.365,
      "step": 821
    },
    {
      "epoch": 0.18681818181818183,
      "grad_norm": 0.04776119813323021,
      "learning_rate": 0.00016282138794084186,
      "loss": 0.3599,
      "step": 822
    },
    {
      "epoch": 0.18704545454545454,
      "grad_norm": 0.053500521928071976,
      "learning_rate": 0.00016277588168373152,
      "loss": 0.3786,
      "step": 823
    },
    {
      "epoch": 0.18727272727272729,
      "grad_norm": 0.05331486091017723,
      "learning_rate": 0.00016273037542662117,
      "loss": 0.3411,
      "step": 824
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.048809152096509933,
      "learning_rate": 0.00016268486916951082,
      "loss": 0.3541,
      "step": 825
    },
    {
      "epoch": 0.18772727272727271,
      "grad_norm": 0.04398849233984947,
      "learning_rate": 0.00016263936291240047,
      "loss": 0.3603,
      "step": 826
    },
    {
      "epoch": 0.18795454545454546,
      "grad_norm": 0.05496080964803696,
      "learning_rate": 0.0001625938566552901,
      "loss": 0.399,
      "step": 827
    },
    {
      "epoch": 0.18818181818181817,
      "grad_norm": 0.03622755780816078,
      "learning_rate": 0.00016254835039817975,
      "loss": 0.2546,
      "step": 828
    },
    {
      "epoch": 0.18840909090909091,
      "grad_norm": 0.05767334997653961,
      "learning_rate": 0.0001625028441410694,
      "loss": 0.3443,
      "step": 829
    },
    {
      "epoch": 0.18863636363636363,
      "grad_norm": 0.05782254412770271,
      "learning_rate": 0.00016245733788395906,
      "loss": 0.3868,
      "step": 830
    },
    {
      "epoch": 0.18886363636363637,
      "grad_norm": 0.04116511344909668,
      "learning_rate": 0.0001624118316268487,
      "loss": 0.3261,
      "step": 831
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.03796049579977989,
      "learning_rate": 0.00016236632536973834,
      "loss": 0.2905,
      "step": 832
    },
    {
      "epoch": 0.18931818181818183,
      "grad_norm": 0.05940226465463638,
      "learning_rate": 0.000162320819112628,
      "loss": 0.3383,
      "step": 833
    },
    {
      "epoch": 0.18954545454545454,
      "grad_norm": 0.050653502345085144,
      "learning_rate": 0.00016227531285551764,
      "loss": 0.3368,
      "step": 834
    },
    {
      "epoch": 0.18977272727272726,
      "grad_norm": 0.05134756118059158,
      "learning_rate": 0.0001622298065984073,
      "loss": 0.3126,
      "step": 835
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.05295790731906891,
      "learning_rate": 0.00016218430034129695,
      "loss": 0.3759,
      "step": 836
    },
    {
      "epoch": 0.19022727272727272,
      "grad_norm": 0.06483079493045807,
      "learning_rate": 0.00016213879408418657,
      "loss": 0.4032,
      "step": 837
    },
    {
      "epoch": 0.19045454545454546,
      "grad_norm": 0.04286657273769379,
      "learning_rate": 0.00016209328782707623,
      "loss": 0.2898,
      "step": 838
    },
    {
      "epoch": 0.19068181818181817,
      "grad_norm": 0.045870088040828705,
      "learning_rate": 0.00016204778156996588,
      "loss": 0.3522,
      "step": 839
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.04462090879678726,
      "learning_rate": 0.00016200227531285553,
      "loss": 0.2998,
      "step": 840
    },
    {
      "epoch": 0.19113636363636363,
      "grad_norm": 0.05217643827199936,
      "learning_rate": 0.00016195676905574516,
      "loss": 0.3344,
      "step": 841
    },
    {
      "epoch": 0.19136363636363637,
      "grad_norm": 0.04849562793970108,
      "learning_rate": 0.0001619112627986348,
      "loss": 0.2963,
      "step": 842
    },
    {
      "epoch": 0.1915909090909091,
      "grad_norm": 0.059703949838876724,
      "learning_rate": 0.00016186575654152446,
      "loss": 0.4238,
      "step": 843
    },
    {
      "epoch": 0.1918181818181818,
      "grad_norm": 0.044806916266679764,
      "learning_rate": 0.00016182025028441412,
      "loss": 0.2788,
      "step": 844
    },
    {
      "epoch": 0.19204545454545455,
      "grad_norm": 0.03773590177297592,
      "learning_rate": 0.00016177474402730377,
      "loss": 0.2763,
      "step": 845
    },
    {
      "epoch": 0.19227272727272726,
      "grad_norm": 0.03956779092550278,
      "learning_rate": 0.00016172923777019342,
      "loss": 0.3031,
      "step": 846
    },
    {
      "epoch": 0.1925,
      "grad_norm": 0.04570518434047699,
      "learning_rate": 0.00016168373151308305,
      "loss": 0.2923,
      "step": 847
    },
    {
      "epoch": 0.19272727272727272,
      "grad_norm": 0.06912536919116974,
      "learning_rate": 0.0001616382252559727,
      "loss": 0.3292,
      "step": 848
    },
    {
      "epoch": 0.19295454545454546,
      "grad_norm": 0.057666122913360596,
      "learning_rate": 0.00016159271899886235,
      "loss": 0.3308,
      "step": 849
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.05332732945680618,
      "learning_rate": 0.000161547212741752,
      "loss": 0.3691,
      "step": 850
    },
    {
      "epoch": 0.19340909090909092,
      "grad_norm": 0.05723945423960686,
      "learning_rate": 0.00016150170648464163,
      "loss": 0.4126,
      "step": 851
    },
    {
      "epoch": 0.19363636363636363,
      "grad_norm": 0.0439126119017601,
      "learning_rate": 0.00016145620022753129,
      "loss": 0.318,
      "step": 852
    },
    {
      "epoch": 0.19386363636363638,
      "grad_norm": 0.04992842674255371,
      "learning_rate": 0.00016141069397042094,
      "loss": 0.3768,
      "step": 853
    },
    {
      "epoch": 0.1940909090909091,
      "grad_norm": 0.03436636924743652,
      "learning_rate": 0.0001613651877133106,
      "loss": 0.2728,
      "step": 854
    },
    {
      "epoch": 0.1943181818181818,
      "grad_norm": 0.04510318487882614,
      "learning_rate": 0.00016131968145620024,
      "loss": 0.2832,
      "step": 855
    },
    {
      "epoch": 0.19454545454545455,
      "grad_norm": 0.04611555114388466,
      "learning_rate": 0.00016127417519908987,
      "loss": 0.3007,
      "step": 856
    },
    {
      "epoch": 0.19477272727272726,
      "grad_norm": 0.044614020735025406,
      "learning_rate": 0.00016122866894197952,
      "loss": 0.3251,
      "step": 857
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.04877062886953354,
      "learning_rate": 0.00016118316268486918,
      "loss": 0.2894,
      "step": 858
    },
    {
      "epoch": 0.19522727272727272,
      "grad_norm": 0.054168496280908585,
      "learning_rate": 0.00016113765642775883,
      "loss": 0.3463,
      "step": 859
    },
    {
      "epoch": 0.19545454545454546,
      "grad_norm": 0.047166988253593445,
      "learning_rate": 0.00016109215017064848,
      "loss": 0.323,
      "step": 860
    },
    {
      "epoch": 0.19568181818181818,
      "grad_norm": 0.04480060562491417,
      "learning_rate": 0.0001610466439135381,
      "loss": 0.3203,
      "step": 861
    },
    {
      "epoch": 0.19590909090909092,
      "grad_norm": 0.051676079630851746,
      "learning_rate": 0.00016100113765642776,
      "loss": 0.3816,
      "step": 862
    },
    {
      "epoch": 0.19613636363636364,
      "grad_norm": 0.04818413034081459,
      "learning_rate": 0.0001609556313993174,
      "loss": 0.3923,
      "step": 863
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.04854915663599968,
      "learning_rate": 0.00016091012514220707,
      "loss": 0.3496,
      "step": 864
    },
    {
      "epoch": 0.1965909090909091,
      "grad_norm": 0.05714503675699234,
      "learning_rate": 0.00016086461888509672,
      "loss": 0.3765,
      "step": 865
    },
    {
      "epoch": 0.1968181818181818,
      "grad_norm": 0.04611428081989288,
      "learning_rate": 0.00016081911262798634,
      "loss": 0.3303,
      "step": 866
    },
    {
      "epoch": 0.19704545454545455,
      "grad_norm": 0.04934085160493851,
      "learning_rate": 0.000160773606370876,
      "loss": 0.3271,
      "step": 867
    },
    {
      "epoch": 0.19727272727272727,
      "grad_norm": 0.03729378804564476,
      "learning_rate": 0.00016072810011376565,
      "loss": 0.2869,
      "step": 868
    },
    {
      "epoch": 0.1975,
      "grad_norm": 0.07261484861373901,
      "learning_rate": 0.0001606825938566553,
      "loss": 0.3879,
      "step": 869
    },
    {
      "epoch": 0.19772727272727272,
      "grad_norm": 0.04495812952518463,
      "learning_rate": 0.00016063708759954495,
      "loss": 0.3409,
      "step": 870
    },
    {
      "epoch": 0.19795454545454547,
      "grad_norm": 0.05216356739401817,
      "learning_rate": 0.00016059158134243458,
      "loss": 0.3421,
      "step": 871
    },
    {
      "epoch": 0.19818181818181818,
      "grad_norm": 0.046042680740356445,
      "learning_rate": 0.00016054607508532423,
      "loss": 0.369,
      "step": 872
    },
    {
      "epoch": 0.1984090909090909,
      "grad_norm": 0.050636984407901764,
      "learning_rate": 0.00016050056882821389,
      "loss": 0.3505,
      "step": 873
    },
    {
      "epoch": 0.19863636363636364,
      "grad_norm": 0.0433967150747776,
      "learning_rate": 0.00016045506257110354,
      "loss": 0.3252,
      "step": 874
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.052617914974689484,
      "learning_rate": 0.0001604095563139932,
      "loss": 0.29,
      "step": 875
    },
    {
      "epoch": 0.1990909090909091,
      "grad_norm": 0.030154261738061905,
      "learning_rate": 0.00016036405005688282,
      "loss": 0.226,
      "step": 876
    },
    {
      "epoch": 0.1993181818181818,
      "grad_norm": 0.05841638892889023,
      "learning_rate": 0.00016031854379977247,
      "loss": 0.3821,
      "step": 877
    },
    {
      "epoch": 0.19954545454545455,
      "grad_norm": 0.042908720672130585,
      "learning_rate": 0.00016027303754266212,
      "loss": 0.2927,
      "step": 878
    },
    {
      "epoch": 0.19977272727272727,
      "grad_norm": 0.049835529178380966,
      "learning_rate": 0.00016022753128555178,
      "loss": 0.3399,
      "step": 879
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.04170897230505943,
      "learning_rate": 0.00016018202502844143,
      "loss": 0.3032,
      "step": 880
    },
    {
      "epoch": 0.20022727272727273,
      "grad_norm": 0.041583843529224396,
      "learning_rate": 0.00016013651877133105,
      "loss": 0.2789,
      "step": 881
    },
    {
      "epoch": 0.20045454545454544,
      "grad_norm": 0.048064641654491425,
      "learning_rate": 0.0001600910125142207,
      "loss": 0.3201,
      "step": 882
    },
    {
      "epoch": 0.20068181818181818,
      "grad_norm": 0.05315056070685387,
      "learning_rate": 0.00016004550625711036,
      "loss": 0.4271,
      "step": 883
    },
    {
      "epoch": 0.2009090909090909,
      "grad_norm": 0.03979136049747467,
      "learning_rate": 0.00016,
      "loss": 0.3004,
      "step": 884
    },
    {
      "epoch": 0.20113636363636364,
      "grad_norm": 0.061767373234033585,
      "learning_rate": 0.00015995449374288964,
      "loss": 0.3813,
      "step": 885
    },
    {
      "epoch": 0.20136363636363636,
      "grad_norm": 0.050917744636535645,
      "learning_rate": 0.0001599089874857793,
      "loss": 0.2942,
      "step": 886
    },
    {
      "epoch": 0.2015909090909091,
      "grad_norm": 0.04216175153851509,
      "learning_rate": 0.00015986348122866894,
      "loss": 0.3393,
      "step": 887
    },
    {
      "epoch": 0.2018181818181818,
      "grad_norm": 0.04269551858305931,
      "learning_rate": 0.0001598179749715586,
      "loss": 0.3274,
      "step": 888
    },
    {
      "epoch": 0.20204545454545456,
      "grad_norm": 0.051908306777477264,
      "learning_rate": 0.00015977246871444825,
      "loss": 0.3318,
      "step": 889
    },
    {
      "epoch": 0.20227272727272727,
      "grad_norm": 0.05359339714050293,
      "learning_rate": 0.0001597269624573379,
      "loss": 0.3565,
      "step": 890
    },
    {
      "epoch": 0.2025,
      "grad_norm": 0.0552242249250412,
      "learning_rate": 0.00015968145620022753,
      "loss": 0.3175,
      "step": 891
    },
    {
      "epoch": 0.20272727272727273,
      "grad_norm": 0.04491652920842171,
      "learning_rate": 0.00015963594994311718,
      "loss": 0.3446,
      "step": 892
    },
    {
      "epoch": 0.20295454545454544,
      "grad_norm": 0.05307132750749588,
      "learning_rate": 0.00015959044368600683,
      "loss": 0.3043,
      "step": 893
    },
    {
      "epoch": 0.20318181818181819,
      "grad_norm": 0.054033271968364716,
      "learning_rate": 0.0001595449374288965,
      "loss": 0.3931,
      "step": 894
    },
    {
      "epoch": 0.2034090909090909,
      "grad_norm": 0.054709892719984055,
      "learning_rate": 0.0001594994311717861,
      "loss": 0.3484,
      "step": 895
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.06394797563552856,
      "learning_rate": 0.00015945392491467577,
      "loss": 0.4376,
      "step": 896
    },
    {
      "epoch": 0.20386363636363636,
      "grad_norm": 0.039916496723890305,
      "learning_rate": 0.00015940841865756542,
      "loss": 0.3084,
      "step": 897
    },
    {
      "epoch": 0.2040909090909091,
      "grad_norm": 0.03305063024163246,
      "learning_rate": 0.00015936291240045507,
      "loss": 0.2099,
      "step": 898
    },
    {
      "epoch": 0.20431818181818182,
      "grad_norm": 0.04932524263858795,
      "learning_rate": 0.00015931740614334472,
      "loss": 0.333,
      "step": 899
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.052618712186813354,
      "learning_rate": 0.00015927189988623438,
      "loss": 0.3673,
      "step": 900
    },
    {
      "epoch": 0.20477272727272727,
      "grad_norm": 0.052039068192243576,
      "learning_rate": 0.000159226393629124,
      "loss": 0.3477,
      "step": 901
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.0500948540866375,
      "learning_rate": 0.00015918088737201366,
      "loss": 0.3888,
      "step": 902
    },
    {
      "epoch": 0.20522727272727273,
      "grad_norm": 0.034307800233364105,
      "learning_rate": 0.0001591353811149033,
      "loss": 0.2584,
      "step": 903
    },
    {
      "epoch": 0.20545454545454545,
      "grad_norm": 0.049041327089071274,
      "learning_rate": 0.00015908987485779296,
      "loss": 0.3744,
      "step": 904
    },
    {
      "epoch": 0.2056818181818182,
      "grad_norm": 0.05059454217553139,
      "learning_rate": 0.0001590443686006826,
      "loss": 0.3952,
      "step": 905
    },
    {
      "epoch": 0.2059090909090909,
      "grad_norm": 0.0545785129070282,
      "learning_rate": 0.00015899886234357224,
      "loss": 0.3918,
      "step": 906
    },
    {
      "epoch": 0.20613636363636365,
      "grad_norm": 0.043659087270498276,
      "learning_rate": 0.0001589533560864619,
      "loss": 0.344,
      "step": 907
    },
    {
      "epoch": 0.20636363636363636,
      "grad_norm": 0.05623982846736908,
      "learning_rate": 0.00015890784982935155,
      "loss": 0.3905,
      "step": 908
    },
    {
      "epoch": 0.2065909090909091,
      "grad_norm": 0.058380525559186935,
      "learning_rate": 0.0001588623435722412,
      "loss": 0.3323,
      "step": 909
    },
    {
      "epoch": 0.20681818181818182,
      "grad_norm": 0.04555663838982582,
      "learning_rate": 0.00015881683731513085,
      "loss": 0.3042,
      "step": 910
    },
    {
      "epoch": 0.20704545454545453,
      "grad_norm": 0.045715056359767914,
      "learning_rate": 0.00015877133105802048,
      "loss": 0.3627,
      "step": 911
    },
    {
      "epoch": 0.20727272727272728,
      "grad_norm": 0.055800944566726685,
      "learning_rate": 0.00015872582480091013,
      "loss": 0.3474,
      "step": 912
    },
    {
      "epoch": 0.2075,
      "grad_norm": 0.05466340109705925,
      "learning_rate": 0.00015868031854379978,
      "loss": 0.3684,
      "step": 913
    },
    {
      "epoch": 0.20772727272727273,
      "grad_norm": 0.050946418195962906,
      "learning_rate": 0.00015863481228668944,
      "loss": 0.3688,
      "step": 914
    },
    {
      "epoch": 0.20795454545454545,
      "grad_norm": 0.04021050035953522,
      "learning_rate": 0.00015858930602957906,
      "loss": 0.2976,
      "step": 915
    },
    {
      "epoch": 0.2081818181818182,
      "grad_norm": 0.047690171748399734,
      "learning_rate": 0.00015854379977246871,
      "loss": 0.3222,
      "step": 916
    },
    {
      "epoch": 0.2084090909090909,
      "grad_norm": 0.05362502858042717,
      "learning_rate": 0.00015849829351535837,
      "loss": 0.2945,
      "step": 917
    },
    {
      "epoch": 0.20863636363636365,
      "grad_norm": 0.04226234182715416,
      "learning_rate": 0.00015845278725824802,
      "loss": 0.3065,
      "step": 918
    },
    {
      "epoch": 0.20886363636363636,
      "grad_norm": 0.03650322183966637,
      "learning_rate": 0.00015840728100113767,
      "loss": 0.2911,
      "step": 919
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.04310676082968712,
      "learning_rate": 0.00015836177474402732,
      "loss": 0.3059,
      "step": 920
    },
    {
      "epoch": 0.20931818181818182,
      "grad_norm": 0.05565577745437622,
      "learning_rate": 0.00015831626848691695,
      "loss": 0.382,
      "step": 921
    },
    {
      "epoch": 0.20954545454545453,
      "grad_norm": 0.0511796697974205,
      "learning_rate": 0.0001582707622298066,
      "loss": 0.3637,
      "step": 922
    },
    {
      "epoch": 0.20977272727272728,
      "grad_norm": 0.04839792847633362,
      "learning_rate": 0.00015822525597269626,
      "loss": 0.3062,
      "step": 923
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.05430647358298302,
      "learning_rate": 0.00015817974971558588,
      "loss": 0.4132,
      "step": 924
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.03782240301370621,
      "learning_rate": 0.00015813424345847553,
      "loss": 0.2875,
      "step": 925
    },
    {
      "epoch": 0.21045454545454545,
      "grad_norm": 0.04436803609132767,
      "learning_rate": 0.0001580887372013652,
      "loss": 0.3572,
      "step": 926
    },
    {
      "epoch": 0.2106818181818182,
      "grad_norm": 0.04246139153838158,
      "learning_rate": 0.00015804323094425484,
      "loss": 0.271,
      "step": 927
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.04881512001156807,
      "learning_rate": 0.0001579977246871445,
      "loss": 0.3226,
      "step": 928
    },
    {
      "epoch": 0.21113636363636365,
      "grad_norm": 0.049583595246076584,
      "learning_rate": 0.00015795221843003415,
      "loss": 0.3506,
      "step": 929
    },
    {
      "epoch": 0.21136363636363636,
      "grad_norm": 0.04710213467478752,
      "learning_rate": 0.0001579067121729238,
      "loss": 0.2933,
      "step": 930
    },
    {
      "epoch": 0.21159090909090908,
      "grad_norm": 0.05744067579507828,
      "learning_rate": 0.00015786120591581342,
      "loss": 0.3393,
      "step": 931
    },
    {
      "epoch": 0.21181818181818182,
      "grad_norm": 0.05419589579105377,
      "learning_rate": 0.00015781569965870308,
      "loss": 0.3931,
      "step": 932
    },
    {
      "epoch": 0.21204545454545454,
      "grad_norm": 0.05086365342140198,
      "learning_rate": 0.00015777019340159273,
      "loss": 0.3411,
      "step": 933
    },
    {
      "epoch": 0.21227272727272728,
      "grad_norm": 0.04459264501929283,
      "learning_rate": 0.00015772468714448236,
      "loss": 0.3421,
      "step": 934
    },
    {
      "epoch": 0.2125,
      "grad_norm": 0.04762262478470802,
      "learning_rate": 0.000157679180887372,
      "loss": 0.3448,
      "step": 935
    },
    {
      "epoch": 0.21272727272727274,
      "grad_norm": 0.04373738169670105,
      "learning_rate": 0.00015763367463026166,
      "loss": 0.3248,
      "step": 936
    },
    {
      "epoch": 0.21295454545454545,
      "grad_norm": 0.05098768696188927,
      "learning_rate": 0.00015758816837315131,
      "loss": 0.3324,
      "step": 937
    },
    {
      "epoch": 0.2131818181818182,
      "grad_norm": 0.05924220383167267,
      "learning_rate": 0.00015754266211604097,
      "loss": 0.3673,
      "step": 938
    },
    {
      "epoch": 0.2134090909090909,
      "grad_norm": 0.04178894683718681,
      "learning_rate": 0.00015749715585893062,
      "loss": 0.2784,
      "step": 939
    },
    {
      "epoch": 0.21363636363636362,
      "grad_norm": 0.047769807279109955,
      "learning_rate": 0.00015745164960182027,
      "loss": 0.3346,
      "step": 940
    },
    {
      "epoch": 0.21386363636363637,
      "grad_norm": 0.039762694388628006,
      "learning_rate": 0.0001574061433447099,
      "loss": 0.3354,
      "step": 941
    },
    {
      "epoch": 0.21409090909090908,
      "grad_norm": 0.049808479845523834,
      "learning_rate": 0.00015736063708759955,
      "loss": 0.366,
      "step": 942
    },
    {
      "epoch": 0.21431818181818182,
      "grad_norm": 0.03640072047710419,
      "learning_rate": 0.0001573151308304892,
      "loss": 0.2802,
      "step": 943
    },
    {
      "epoch": 0.21454545454545454,
      "grad_norm": 0.04393555968999863,
      "learning_rate": 0.00015726962457337883,
      "loss": 0.2978,
      "step": 944
    },
    {
      "epoch": 0.21477272727272728,
      "grad_norm": 0.04133174940943718,
      "learning_rate": 0.00015722411831626848,
      "loss": 0.2903,
      "step": 945
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.058922480791807175,
      "learning_rate": 0.00015717861205915814,
      "loss": 0.3869,
      "step": 946
    },
    {
      "epoch": 0.21522727272727274,
      "grad_norm": 0.057792335748672485,
      "learning_rate": 0.0001571331058020478,
      "loss": 0.2958,
      "step": 947
    },
    {
      "epoch": 0.21545454545454545,
      "grad_norm": 0.04351675510406494,
      "learning_rate": 0.00015708759954493744,
      "loss": 0.309,
      "step": 948
    },
    {
      "epoch": 0.21568181818181817,
      "grad_norm": 0.0523410327732563,
      "learning_rate": 0.0001570420932878271,
      "loss": 0.3459,
      "step": 949
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.045415330678224564,
      "learning_rate": 0.00015699658703071675,
      "loss": 0.3316,
      "step": 950
    },
    {
      "epoch": 0.21613636363636363,
      "grad_norm": 0.052417952567338943,
      "learning_rate": 0.00015695108077360637,
      "loss": 0.3348,
      "step": 951
    },
    {
      "epoch": 0.21636363636363637,
      "grad_norm": 0.05386095866560936,
      "learning_rate": 0.00015690557451649603,
      "loss": 0.3537,
      "step": 952
    },
    {
      "epoch": 0.21659090909090908,
      "grad_norm": 0.04097946360707283,
      "learning_rate": 0.00015686006825938568,
      "loss": 0.2981,
      "step": 953
    },
    {
      "epoch": 0.21681818181818183,
      "grad_norm": 0.042024463415145874,
      "learning_rate": 0.0001568145620022753,
      "loss": 0.3292,
      "step": 954
    },
    {
      "epoch": 0.21704545454545454,
      "grad_norm": 0.034153155982494354,
      "learning_rate": 0.00015676905574516496,
      "loss": 0.2594,
      "step": 955
    },
    {
      "epoch": 0.21727272727272728,
      "grad_norm": 0.05807274207472801,
      "learning_rate": 0.0001567235494880546,
      "loss": 0.3828,
      "step": 956
    },
    {
      "epoch": 0.2175,
      "grad_norm": 0.04837659373879433,
      "learning_rate": 0.00015667804323094426,
      "loss": 0.3295,
      "step": 957
    },
    {
      "epoch": 0.2177272727272727,
      "grad_norm": 0.052157822996377945,
      "learning_rate": 0.00015663253697383392,
      "loss": 0.3377,
      "step": 958
    },
    {
      "epoch": 0.21795454545454546,
      "grad_norm": 0.04307360574603081,
      "learning_rate": 0.00015658703071672357,
      "loss": 0.3527,
      "step": 959
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.04680516943335533,
      "learning_rate": 0.00015654152445961322,
      "loss": 0.3466,
      "step": 960
    },
    {
      "epoch": 0.2184090909090909,
      "grad_norm": 0.03918924182653427,
      "learning_rate": 0.00015649601820250285,
      "loss": 0.2712,
      "step": 961
    },
    {
      "epoch": 0.21863636363636363,
      "grad_norm": 0.05245358124375343,
      "learning_rate": 0.0001564505119453925,
      "loss": 0.3653,
      "step": 962
    },
    {
      "epoch": 0.21886363636363637,
      "grad_norm": 0.04527681693434715,
      "learning_rate": 0.00015640500568828213,
      "loss": 0.364,
      "step": 963
    },
    {
      "epoch": 0.2190909090909091,
      "grad_norm": 0.04423471540212631,
      "learning_rate": 0.00015635949943117178,
      "loss": 0.2939,
      "step": 964
    },
    {
      "epoch": 0.21931818181818183,
      "grad_norm": 0.042325131595134735,
      "learning_rate": 0.00015631399317406143,
      "loss": 0.3126,
      "step": 965
    },
    {
      "epoch": 0.21954545454545454,
      "grad_norm": 0.054719310253858566,
      "learning_rate": 0.00015626848691695108,
      "loss": 0.3811,
      "step": 966
    },
    {
      "epoch": 0.2197727272727273,
      "grad_norm": 0.05900461599230766,
      "learning_rate": 0.00015622298065984074,
      "loss": 0.3454,
      "step": 967
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.044829003512859344,
      "learning_rate": 0.0001561774744027304,
      "loss": 0.3683,
      "step": 968
    },
    {
      "epoch": 0.22022727272727272,
      "grad_norm": 0.05735655501484871,
      "learning_rate": 0.00015613196814562004,
      "loss": 0.3642,
      "step": 969
    },
    {
      "epoch": 0.22045454545454546,
      "grad_norm": 0.04299527034163475,
      "learning_rate": 0.0001560864618885097,
      "loss": 0.2879,
      "step": 970
    },
    {
      "epoch": 0.22068181818181817,
      "grad_norm": 0.06081121414899826,
      "learning_rate": 0.00015604095563139932,
      "loss": 0.4024,
      "step": 971
    },
    {
      "epoch": 0.22090909090909092,
      "grad_norm": 0.043415702879428864,
      "learning_rate": 0.00015599544937428897,
      "loss": 0.3058,
      "step": 972
    },
    {
      "epoch": 0.22113636363636363,
      "grad_norm": 0.04329098016023636,
      "learning_rate": 0.0001559499431171786,
      "loss": 0.3333,
      "step": 973
    },
    {
      "epoch": 0.22136363636363637,
      "grad_norm": 0.035117410123348236,
      "learning_rate": 0.00015590443686006825,
      "loss": 0.2711,
      "step": 974
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.046584345400333405,
      "learning_rate": 0.0001558589306029579,
      "loss": 0.2886,
      "step": 975
    },
    {
      "epoch": 0.22181818181818183,
      "grad_norm": 0.04589708149433136,
      "learning_rate": 0.00015581342434584756,
      "loss": 0.2942,
      "step": 976
    },
    {
      "epoch": 0.22204545454545455,
      "grad_norm": 0.04865333437919617,
      "learning_rate": 0.0001557679180887372,
      "loss": 0.3401,
      "step": 977
    },
    {
      "epoch": 0.22227272727272726,
      "grad_norm": 0.05455043539404869,
      "learning_rate": 0.00015572241183162686,
      "loss": 0.3869,
      "step": 978
    },
    {
      "epoch": 0.2225,
      "grad_norm": 0.042597077786922455,
      "learning_rate": 0.00015567690557451652,
      "loss": 0.3535,
      "step": 979
    },
    {
      "epoch": 0.22272727272727272,
      "grad_norm": 0.05017559602856636,
      "learning_rate": 0.00015563139931740617,
      "loss": 0.3626,
      "step": 980
    },
    {
      "epoch": 0.22295454545454546,
      "grad_norm": 0.04776258021593094,
      "learning_rate": 0.0001555858930602958,
      "loss": 0.2806,
      "step": 981
    },
    {
      "epoch": 0.22318181818181818,
      "grad_norm": 0.03701011836528778,
      "learning_rate": 0.00015554038680318545,
      "loss": 0.3102,
      "step": 982
    },
    {
      "epoch": 0.22340909090909092,
      "grad_norm": 0.051466044038534164,
      "learning_rate": 0.00015549488054607507,
      "loss": 0.3417,
      "step": 983
    },
    {
      "epoch": 0.22363636363636363,
      "grad_norm": 0.056806813925504684,
      "learning_rate": 0.00015544937428896473,
      "loss": 0.3623,
      "step": 984
    },
    {
      "epoch": 0.22386363636363638,
      "grad_norm": 0.0703514888882637,
      "learning_rate": 0.00015540386803185438,
      "loss": 0.318,
      "step": 985
    },
    {
      "epoch": 0.2240909090909091,
      "grad_norm": 0.06919600814580917,
      "learning_rate": 0.00015535836177474403,
      "loss": 0.3662,
      "step": 986
    },
    {
      "epoch": 0.2243181818181818,
      "grad_norm": 0.05512407422065735,
      "learning_rate": 0.00015531285551763368,
      "loss": 0.3594,
      "step": 987
    },
    {
      "epoch": 0.22454545454545455,
      "grad_norm": 0.05396539345383644,
      "learning_rate": 0.00015526734926052334,
      "loss": 0.3384,
      "step": 988
    },
    {
      "epoch": 0.22477272727272726,
      "grad_norm": 0.06505182385444641,
      "learning_rate": 0.000155221843003413,
      "loss": 0.2761,
      "step": 989
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.04945303127169609,
      "learning_rate": 0.00015517633674630264,
      "loss": 0.3476,
      "step": 990
    },
    {
      "epoch": 0.22522727272727272,
      "grad_norm": 0.051517125219106674,
      "learning_rate": 0.00015513083048919227,
      "loss": 0.3203,
      "step": 991
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.04635387659072876,
      "learning_rate": 0.00015508532423208192,
      "loss": 0.3212,
      "step": 992
    },
    {
      "epoch": 0.22568181818181818,
      "grad_norm": 0.04785624518990517,
      "learning_rate": 0.00015503981797497155,
      "loss": 0.3551,
      "step": 993
    },
    {
      "epoch": 0.22590909090909092,
      "grad_norm": 0.04403620585799217,
      "learning_rate": 0.0001549943117178612,
      "loss": 0.3165,
      "step": 994
    },
    {
      "epoch": 0.22613636363636364,
      "grad_norm": 0.0426451712846756,
      "learning_rate": 0.00015494880546075085,
      "loss": 0.3094,
      "step": 995
    },
    {
      "epoch": 0.22636363636363635,
      "grad_norm": 0.04366220533847809,
      "learning_rate": 0.0001549032992036405,
      "loss": 0.3048,
      "step": 996
    },
    {
      "epoch": 0.2265909090909091,
      "grad_norm": 0.04993307963013649,
      "learning_rate": 0.00015485779294653016,
      "loss": 0.3705,
      "step": 997
    },
    {
      "epoch": 0.2268181818181818,
      "grad_norm": 0.03936310485005379,
      "learning_rate": 0.0001548122866894198,
      "loss": 0.2555,
      "step": 998
    },
    {
      "epoch": 0.22704545454545455,
      "grad_norm": 0.04796623811125755,
      "learning_rate": 0.00015476678043230946,
      "loss": 0.3407,
      "step": 999
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.05757635831832886,
      "learning_rate": 0.00015472127417519912,
      "loss": 0.3353,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 4400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.330195869982982e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
