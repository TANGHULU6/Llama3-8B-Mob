{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008,
      "grad_norm": 1.941324234008789,
      "learning_rate": 4e-05,
      "loss": 2.752,
      "step": 1
    },
    {
      "epoch": 0.0016,
      "grad_norm": 1.7332767248153687,
      "learning_rate": 8e-05,
      "loss": 2.6503,
      "step": 2
    },
    {
      "epoch": 0.0024,
      "grad_norm": 1.7960196733474731,
      "learning_rate": 0.00012,
      "loss": 2.7881,
      "step": 3
    },
    {
      "epoch": 0.0032,
      "grad_norm": 1.1861951351165771,
      "learning_rate": 0.00016,
      "loss": 2.5515,
      "step": 4
    },
    {
      "epoch": 0.004,
      "grad_norm": 3.3025026321411133,
      "learning_rate": 0.0002,
      "loss": 2.5282,
      "step": 5
    },
    {
      "epoch": 0.0048,
      "grad_norm": 1.8602603673934937,
      "learning_rate": 0.00019983935742971887,
      "loss": 2.3492,
      "step": 6
    },
    {
      "epoch": 0.0056,
      "grad_norm": 1.621707797050476,
      "learning_rate": 0.00019967871485943777,
      "loss": 2.3107,
      "step": 7
    },
    {
      "epoch": 0.0064,
      "grad_norm": 1.5794490575790405,
      "learning_rate": 0.00019951807228915663,
      "loss": 2.0091,
      "step": 8
    },
    {
      "epoch": 0.0072,
      "grad_norm": 1.0611001253128052,
      "learning_rate": 0.00019935742971887552,
      "loss": 1.8845,
      "step": 9
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.2688624858856201,
      "learning_rate": 0.0001991967871485944,
      "loss": 1.8038,
      "step": 10
    },
    {
      "epoch": 0.0088,
      "grad_norm": 1.3185248374938965,
      "learning_rate": 0.00019903614457831325,
      "loss": 1.7333,
      "step": 11
    },
    {
      "epoch": 0.0096,
      "grad_norm": 1.4532471895217896,
      "learning_rate": 0.00019887550200803214,
      "loss": 1.7498,
      "step": 12
    },
    {
      "epoch": 0.0104,
      "grad_norm": 0.9476293325424194,
      "learning_rate": 0.000198714859437751,
      "loss": 1.6931,
      "step": 13
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.9114360809326172,
      "learning_rate": 0.0001985542168674699,
      "loss": 1.6976,
      "step": 14
    },
    {
      "epoch": 0.012,
      "grad_norm": 1.6050965785980225,
      "learning_rate": 0.00019839357429718877,
      "loss": 1.6346,
      "step": 15
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.7870045900344849,
      "learning_rate": 0.00019823293172690763,
      "loss": 1.4849,
      "step": 16
    },
    {
      "epoch": 0.0136,
      "grad_norm": 1.452048420906067,
      "learning_rate": 0.00019807228915662652,
      "loss": 1.5989,
      "step": 17
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.9186365604400635,
      "learning_rate": 0.0001979116465863454,
      "loss": 1.4412,
      "step": 18
    },
    {
      "epoch": 0.0152,
      "grad_norm": 1.2634295225143433,
      "learning_rate": 0.00019775100401606425,
      "loss": 1.4461,
      "step": 19
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.0872528553009033,
      "learning_rate": 0.00019759036144578314,
      "loss": 1.2403,
      "step": 20
    },
    {
      "epoch": 0.0168,
      "grad_norm": 1.1629999876022339,
      "learning_rate": 0.000197429718875502,
      "loss": 1.3606,
      "step": 21
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.980350136756897,
      "learning_rate": 0.0001972690763052209,
      "loss": 1.2471,
      "step": 22
    },
    {
      "epoch": 0.0184,
      "grad_norm": 1.288614273071289,
      "learning_rate": 0.00019710843373493977,
      "loss": 1.1989,
      "step": 23
    },
    {
      "epoch": 0.0192,
      "grad_norm": 1.683009386062622,
      "learning_rate": 0.00019694779116465866,
      "loss": 1.1083,
      "step": 24
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.1450467109680176,
      "learning_rate": 0.00019678714859437752,
      "loss": 0.9803,
      "step": 25
    },
    {
      "epoch": 0.0208,
      "grad_norm": 2.3829190731048584,
      "learning_rate": 0.00019662650602409642,
      "loss": 1.0188,
      "step": 26
    },
    {
      "epoch": 0.0216,
      "grad_norm": 0.963970422744751,
      "learning_rate": 0.00019646586345381528,
      "loss": 0.9022,
      "step": 27
    },
    {
      "epoch": 0.0224,
      "grad_norm": 1.0515352487564087,
      "learning_rate": 0.00019630522088353415,
      "loss": 0.8448,
      "step": 28
    },
    {
      "epoch": 0.0232,
      "grad_norm": 1.8573294878005981,
      "learning_rate": 0.000196144578313253,
      "loss": 0.9113,
      "step": 29
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.4716150760650635,
      "learning_rate": 0.0001959839357429719,
      "loss": 0.8143,
      "step": 30
    },
    {
      "epoch": 0.0248,
      "grad_norm": 0.7217360734939575,
      "learning_rate": 0.00019582329317269077,
      "loss": 0.73,
      "step": 31
    },
    {
      "epoch": 0.0256,
      "grad_norm": 1.649575114250183,
      "learning_rate": 0.00019566265060240966,
      "loss": 0.7792,
      "step": 32
    },
    {
      "epoch": 0.0264,
      "grad_norm": 2.6351685523986816,
      "learning_rate": 0.00019550200803212852,
      "loss": 0.7384,
      "step": 33
    },
    {
      "epoch": 0.0272,
      "grad_norm": 1.2510908842086792,
      "learning_rate": 0.00019534136546184742,
      "loss": 0.6977,
      "step": 34
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.45769667625427246,
      "learning_rate": 0.00019518072289156628,
      "loss": 0.5825,
      "step": 35
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.5704997777938843,
      "learning_rate": 0.00019502008032128517,
      "loss": 0.6149,
      "step": 36
    },
    {
      "epoch": 0.0296,
      "grad_norm": 0.5680238604545593,
      "learning_rate": 0.000194859437751004,
      "loss": 0.5819,
      "step": 37
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.4144975543022156,
      "learning_rate": 0.0001946987951807229,
      "loss": 0.6504,
      "step": 38
    },
    {
      "epoch": 0.0312,
      "grad_norm": 0.4366830289363861,
      "learning_rate": 0.00019453815261044177,
      "loss": 0.5878,
      "step": 39
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.4782843291759491,
      "learning_rate": 0.00019437751004016066,
      "loss": 0.5859,
      "step": 40
    },
    {
      "epoch": 0.0328,
      "grad_norm": 0.43593868613243103,
      "learning_rate": 0.00019421686746987952,
      "loss": 0.7125,
      "step": 41
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.26006096601486206,
      "learning_rate": 0.00019405622489959842,
      "loss": 0.5556,
      "step": 42
    },
    {
      "epoch": 0.0344,
      "grad_norm": 0.3615660071372986,
      "learning_rate": 0.00019389558232931728,
      "loss": 0.6335,
      "step": 43
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.42500147223472595,
      "learning_rate": 0.00019373493975903617,
      "loss": 0.5899,
      "step": 44
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.36581432819366455,
      "learning_rate": 0.00019357429718875504,
      "loss": 0.6276,
      "step": 45
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.20403671264648438,
      "learning_rate": 0.0001934136546184739,
      "loss": 0.5555,
      "step": 46
    },
    {
      "epoch": 0.0376,
      "grad_norm": 0.17928065359592438,
      "learning_rate": 0.00019325301204819277,
      "loss": 0.5146,
      "step": 47
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.3546666204929352,
      "learning_rate": 0.00019309236947791166,
      "loss": 0.6305,
      "step": 48
    },
    {
      "epoch": 0.0392,
      "grad_norm": 0.3140902519226074,
      "learning_rate": 0.00019293172690763052,
      "loss": 0.5843,
      "step": 49
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.1891360878944397,
      "learning_rate": 0.00019277108433734942,
      "loss": 0.5587,
      "step": 50
    },
    {
      "epoch": 0.0408,
      "grad_norm": 0.3048390746116638,
      "learning_rate": 0.00019261044176706828,
      "loss": 0.5722,
      "step": 51
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.24148865044116974,
      "learning_rate": 0.00019244979919678717,
      "loss": 0.5373,
      "step": 52
    },
    {
      "epoch": 0.0424,
      "grad_norm": 0.12947136163711548,
      "learning_rate": 0.00019228915662650604,
      "loss": 0.524,
      "step": 53
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.22859278321266174,
      "learning_rate": 0.00019212851405622493,
      "loss": 0.5389,
      "step": 54
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.29262280464172363,
      "learning_rate": 0.00019196787148594377,
      "loss": 0.5682,
      "step": 55
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.16510549187660217,
      "learning_rate": 0.00019180722891566266,
      "loss": 0.5307,
      "step": 56
    },
    {
      "epoch": 0.0456,
      "grad_norm": 0.2364969700574875,
      "learning_rate": 0.00019164658634538152,
      "loss": 0.5257,
      "step": 57
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.19091682136058807,
      "learning_rate": 0.00019148594377510042,
      "loss": 0.562,
      "step": 58
    },
    {
      "epoch": 0.0472,
      "grad_norm": 0.15617014467716217,
      "learning_rate": 0.00019132530120481928,
      "loss": 0.5237,
      "step": 59
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.28973451256752014,
      "learning_rate": 0.00019116465863453817,
      "loss": 0.5164,
      "step": 60
    },
    {
      "epoch": 0.0488,
      "grad_norm": 0.17831726372241974,
      "learning_rate": 0.00019100401606425704,
      "loss": 0.5287,
      "step": 61
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.1744655966758728,
      "learning_rate": 0.00019084337349397593,
      "loss": 0.5407,
      "step": 62
    },
    {
      "epoch": 0.0504,
      "grad_norm": 0.23128552734851837,
      "learning_rate": 0.0001906827309236948,
      "loss": 0.5211,
      "step": 63
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.24889449775218964,
      "learning_rate": 0.00019052208835341369,
      "loss": 0.5272,
      "step": 64
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.2026108205318451,
      "learning_rate": 0.00019036144578313252,
      "loss": 0.569,
      "step": 65
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.2618114948272705,
      "learning_rate": 0.00019020080321285142,
      "loss": 0.5222,
      "step": 66
    },
    {
      "epoch": 0.0536,
      "grad_norm": 0.2662384808063507,
      "learning_rate": 0.00019004016064257028,
      "loss": 0.6583,
      "step": 67
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.16672225296497345,
      "learning_rate": 0.00018987951807228917,
      "loss": 0.5684,
      "step": 68
    },
    {
      "epoch": 0.0552,
      "grad_norm": 0.24147091805934906,
      "learning_rate": 0.00018971887550200804,
      "loss": 0.4919,
      "step": 69
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.25764378905296326,
      "learning_rate": 0.00018955823293172693,
      "loss": 0.574,
      "step": 70
    },
    {
      "epoch": 0.0568,
      "grad_norm": 0.1741090565919876,
      "learning_rate": 0.0001893975903614458,
      "loss": 0.4592,
      "step": 71
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.2410726696252823,
      "learning_rate": 0.00018923694779116469,
      "loss": 0.5779,
      "step": 72
    },
    {
      "epoch": 0.0584,
      "grad_norm": 0.225308358669281,
      "learning_rate": 0.00018907630522088355,
      "loss": 0.6039,
      "step": 73
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.21514193713665009,
      "learning_rate": 0.00018891566265060242,
      "loss": 0.5614,
      "step": 74
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.24643340706825256,
      "learning_rate": 0.00018875502008032128,
      "loss": 0.5252,
      "step": 75
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.10425075143575668,
      "learning_rate": 0.00018859437751004017,
      "loss": 0.5075,
      "step": 76
    },
    {
      "epoch": 0.0616,
      "grad_norm": 0.30816856026649475,
      "learning_rate": 0.00018843373493975904,
      "loss": 0.5519,
      "step": 77
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.18587513267993927,
      "learning_rate": 0.00018827309236947793,
      "loss": 0.5033,
      "step": 78
    },
    {
      "epoch": 0.0632,
      "grad_norm": 0.14557982981204987,
      "learning_rate": 0.0001881124497991968,
      "loss": 0.5435,
      "step": 79
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.21685940027236938,
      "learning_rate": 0.00018795180722891569,
      "loss": 0.5212,
      "step": 80
    },
    {
      "epoch": 0.0648,
      "grad_norm": 0.16854649782180786,
      "learning_rate": 0.00018779116465863455,
      "loss": 0.5461,
      "step": 81
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.196682408452034,
      "learning_rate": 0.00018763052208835344,
      "loss": 0.5512,
      "step": 82
    },
    {
      "epoch": 0.0664,
      "grad_norm": 0.13136343657970428,
      "learning_rate": 0.00018746987951807228,
      "loss": 0.5184,
      "step": 83
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.15909568965435028,
      "learning_rate": 0.00018730923694779117,
      "loss": 0.4869,
      "step": 84
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.2592766582965851,
      "learning_rate": 0.00018714859437751004,
      "loss": 0.6055,
      "step": 85
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.14349225163459778,
      "learning_rate": 0.00018698795180722893,
      "loss": 0.5411,
      "step": 86
    },
    {
      "epoch": 0.0696,
      "grad_norm": 0.15398819744586945,
      "learning_rate": 0.0001868273092369478,
      "loss": 0.4601,
      "step": 87
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.1917625367641449,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.5671,
      "step": 88
    },
    {
      "epoch": 0.0712,
      "grad_norm": 0.12152867764234543,
      "learning_rate": 0.00018650602409638555,
      "loss": 0.4685,
      "step": 89
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.16945461928844452,
      "learning_rate": 0.00018634538152610444,
      "loss": 0.5859,
      "step": 90
    },
    {
      "epoch": 0.0728,
      "grad_norm": 0.1564050316810608,
      "learning_rate": 0.0001861847389558233,
      "loss": 0.5461,
      "step": 91
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.13991890847682953,
      "learning_rate": 0.00018602409638554217,
      "loss": 0.4563,
      "step": 92
    },
    {
      "epoch": 0.0744,
      "grad_norm": 0.17850537598133087,
      "learning_rate": 0.00018586345381526104,
      "loss": 0.5653,
      "step": 93
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.13003551959991455,
      "learning_rate": 0.00018570281124497993,
      "loss": 0.485,
      "step": 94
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.17954277992248535,
      "learning_rate": 0.0001855421686746988,
      "loss": 0.5424,
      "step": 95
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.1812206208705902,
      "learning_rate": 0.0001853815261044177,
      "loss": 0.4204,
      "step": 96
    },
    {
      "epoch": 0.0776,
      "grad_norm": 0.12745656073093414,
      "learning_rate": 0.00018522088353413655,
      "loss": 0.4959,
      "step": 97
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.1340697705745697,
      "learning_rate": 0.00018506024096385544,
      "loss": 0.5047,
      "step": 98
    },
    {
      "epoch": 0.0792,
      "grad_norm": 0.1474214345216751,
      "learning_rate": 0.0001848995983935743,
      "loss": 0.4647,
      "step": 99
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.1372193843126297,
      "learning_rate": 0.0001847389558232932,
      "loss": 0.5236,
      "step": 100
    },
    {
      "epoch": 0.0808,
      "grad_norm": 0.13096606731414795,
      "learning_rate": 0.00018457831325301204,
      "loss": 0.5474,
      "step": 101
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.15031898021697998,
      "learning_rate": 0.00018441767068273093,
      "loss": 0.5698,
      "step": 102
    },
    {
      "epoch": 0.0824,
      "grad_norm": 0.15962275862693787,
      "learning_rate": 0.0001842570281124498,
      "loss": 0.4997,
      "step": 103
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.17350535094738007,
      "learning_rate": 0.0001840963855421687,
      "loss": 0.5165,
      "step": 104
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.1557052731513977,
      "learning_rate": 0.00018393574297188755,
      "loss": 0.5183,
      "step": 105
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.18565155565738678,
      "learning_rate": 0.00018377510040160644,
      "loss": 0.4683,
      "step": 106
    },
    {
      "epoch": 0.0856,
      "grad_norm": 0.193803608417511,
      "learning_rate": 0.0001836144578313253,
      "loss": 0.6201,
      "step": 107
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.23392528295516968,
      "learning_rate": 0.0001834538152610442,
      "loss": 0.5544,
      "step": 108
    },
    {
      "epoch": 0.0872,
      "grad_norm": 0.15205684304237366,
      "learning_rate": 0.00018329317269076307,
      "loss": 0.4771,
      "step": 109
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.15525127947330475,
      "learning_rate": 0.00018313253012048193,
      "loss": 0.4551,
      "step": 110
    },
    {
      "epoch": 0.0888,
      "grad_norm": 0.2049614042043686,
      "learning_rate": 0.0001829718875502008,
      "loss": 0.4716,
      "step": 111
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.13723190128803253,
      "learning_rate": 0.0001828112449799197,
      "loss": 0.4656,
      "step": 112
    },
    {
      "epoch": 0.0904,
      "grad_norm": 0.18983225524425507,
      "learning_rate": 0.00018265060240963855,
      "loss": 0.5376,
      "step": 113
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.18837760388851166,
      "learning_rate": 0.00018248995983935744,
      "loss": 0.6195,
      "step": 114
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.21289342641830444,
      "learning_rate": 0.0001823293172690763,
      "loss": 0.5643,
      "step": 115
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.21241886913776398,
      "learning_rate": 0.0001821686746987952,
      "loss": 0.5223,
      "step": 116
    },
    {
      "epoch": 0.0936,
      "grad_norm": 0.13318105041980743,
      "learning_rate": 0.00018200803212851407,
      "loss": 0.5028,
      "step": 117
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.1815022975206375,
      "learning_rate": 0.00018184738955823296,
      "loss": 0.5185,
      "step": 118
    },
    {
      "epoch": 0.0952,
      "grad_norm": 0.15303361415863037,
      "learning_rate": 0.0001816867469879518,
      "loss": 0.4804,
      "step": 119
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.12975865602493286,
      "learning_rate": 0.0001815261044176707,
      "loss": 0.4952,
      "step": 120
    },
    {
      "epoch": 0.0968,
      "grad_norm": 0.21118904650211334,
      "learning_rate": 0.00018136546184738955,
      "loss": 0.5008,
      "step": 121
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.20097766816616058,
      "learning_rate": 0.00018120481927710844,
      "loss": 0.5209,
      "step": 122
    },
    {
      "epoch": 0.0984,
      "grad_norm": 0.12750892341136932,
      "learning_rate": 0.0001810441767068273,
      "loss": 0.4812,
      "step": 123
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.2900545001029968,
      "learning_rate": 0.0001808835341365462,
      "loss": 0.5459,
      "step": 124
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.15927337110042572,
      "learning_rate": 0.00018072289156626507,
      "loss": 0.504,
      "step": 125
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.19007842242717743,
      "learning_rate": 0.00018056224899598396,
      "loss": 0.4812,
      "step": 126
    },
    {
      "epoch": 0.1016,
      "grad_norm": 0.16984927654266357,
      "learning_rate": 0.00018040160642570282,
      "loss": 0.5802,
      "step": 127
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.18895842134952545,
      "learning_rate": 0.0001802409638554217,
      "loss": 0.5632,
      "step": 128
    },
    {
      "epoch": 0.1032,
      "grad_norm": 0.19644992053508759,
      "learning_rate": 0.00018008032128514055,
      "loss": 0.5214,
      "step": 129
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.17447017133235931,
      "learning_rate": 0.00017991967871485944,
      "loss": 0.5065,
      "step": 130
    },
    {
      "epoch": 0.1048,
      "grad_norm": 0.1626010239124298,
      "learning_rate": 0.0001797590361445783,
      "loss": 0.5075,
      "step": 131
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.18524324893951416,
      "learning_rate": 0.0001795983935742972,
      "loss": 0.5008,
      "step": 132
    },
    {
      "epoch": 0.1064,
      "grad_norm": 0.15727326273918152,
      "learning_rate": 0.00017943775100401607,
      "loss": 0.4603,
      "step": 133
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.20182783901691437,
      "learning_rate": 0.00017927710843373496,
      "loss": 0.5579,
      "step": 134
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.21190425753593445,
      "learning_rate": 0.00017911646586345382,
      "loss": 0.5163,
      "step": 135
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.21216349303722382,
      "learning_rate": 0.00017895582329317271,
      "loss": 0.4799,
      "step": 136
    },
    {
      "epoch": 0.1096,
      "grad_norm": 0.12823493778705597,
      "learning_rate": 0.00017879518072289155,
      "loss": 0.5248,
      "step": 137
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.20794831216335297,
      "learning_rate": 0.00017863453815261044,
      "loss": 0.5484,
      "step": 138
    },
    {
      "epoch": 0.1112,
      "grad_norm": 0.15759164094924927,
      "learning_rate": 0.0001784738955823293,
      "loss": 0.4784,
      "step": 139
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.13906824588775635,
      "learning_rate": 0.0001783132530120482,
      "loss": 0.5113,
      "step": 140
    },
    {
      "epoch": 0.1128,
      "grad_norm": 0.12626518309116364,
      "learning_rate": 0.00017815261044176707,
      "loss": 0.4548,
      "step": 141
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.1400166004896164,
      "learning_rate": 0.00017799196787148596,
      "loss": 0.456,
      "step": 142
    },
    {
      "epoch": 0.1144,
      "grad_norm": 0.14976683259010315,
      "learning_rate": 0.00017783132530120482,
      "loss": 0.4908,
      "step": 143
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.14880728721618652,
      "learning_rate": 0.00017767068273092371,
      "loss": 0.4563,
      "step": 144
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.1554093360900879,
      "learning_rate": 0.00017751004016064258,
      "loss": 0.4493,
      "step": 145
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.18591852486133575,
      "learning_rate": 0.00017734939759036144,
      "loss": 0.5083,
      "step": 146
    },
    {
      "epoch": 0.1176,
      "grad_norm": 0.19841569662094116,
      "learning_rate": 0.0001771887550200803,
      "loss": 0.5832,
      "step": 147
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.12372736632823944,
      "learning_rate": 0.0001770281124497992,
      "loss": 0.5365,
      "step": 148
    },
    {
      "epoch": 0.1192,
      "grad_norm": 0.2691288888454437,
      "learning_rate": 0.00017686746987951807,
      "loss": 0.4528,
      "step": 149
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.14549323916435242,
      "learning_rate": 0.00017670682730923696,
      "loss": 0.5278,
      "step": 150
    },
    {
      "epoch": 0.1208,
      "grad_norm": 0.14432723820209503,
      "learning_rate": 0.00017654618473895582,
      "loss": 0.4191,
      "step": 151
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.26593828201293945,
      "learning_rate": 0.00017638554216867471,
      "loss": 0.5306,
      "step": 152
    },
    {
      "epoch": 0.1224,
      "grad_norm": 0.1641755998134613,
      "learning_rate": 0.00017622489959839358,
      "loss": 0.5156,
      "step": 153
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.23899373412132263,
      "learning_rate": 0.00017606425702811247,
      "loss": 0.4139,
      "step": 154
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.24600893259048462,
      "learning_rate": 0.00017590361445783134,
      "loss": 0.5322,
      "step": 155
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.19946891069412231,
      "learning_rate": 0.0001757429718875502,
      "loss": 0.4941,
      "step": 156
    },
    {
      "epoch": 0.1256,
      "grad_norm": 0.230889230966568,
      "learning_rate": 0.00017558232931726907,
      "loss": 0.4735,
      "step": 157
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.21162943542003632,
      "learning_rate": 0.00017542168674698796,
      "loss": 0.5809,
      "step": 158
    },
    {
      "epoch": 0.1272,
      "grad_norm": 0.22978875041007996,
      "learning_rate": 0.00017526104417670682,
      "loss": 0.4944,
      "step": 159
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.23015271127223969,
      "learning_rate": 0.00017510040160642571,
      "loss": 0.4956,
      "step": 160
    },
    {
      "epoch": 0.1288,
      "grad_norm": 0.19837267696857452,
      "learning_rate": 0.00017493975903614458,
      "loss": 0.571,
      "step": 161
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.19666633009910583,
      "learning_rate": 0.00017477911646586347,
      "loss": 0.5179,
      "step": 162
    },
    {
      "epoch": 0.1304,
      "grad_norm": 0.22441717982292175,
      "learning_rate": 0.00017461847389558234,
      "loss": 0.4849,
      "step": 163
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.13868561387062073,
      "learning_rate": 0.00017445783132530123,
      "loss": 0.4417,
      "step": 164
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.2527332007884979,
      "learning_rate": 0.0001742971887550201,
      "loss": 0.6099,
      "step": 165
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.1439509242773056,
      "learning_rate": 0.00017413654618473896,
      "loss": 0.491,
      "step": 166
    },
    {
      "epoch": 0.1336,
      "grad_norm": 0.15839964151382446,
      "learning_rate": 0.00017397590361445782,
      "loss": 0.4916,
      "step": 167
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.17436620593070984,
      "learning_rate": 0.00017381526104417671,
      "loss": 0.5741,
      "step": 168
    },
    {
      "epoch": 0.1352,
      "grad_norm": 0.10741580277681351,
      "learning_rate": 0.00017365461847389558,
      "loss": 0.4714,
      "step": 169
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.13119681179523468,
      "learning_rate": 0.00017349397590361447,
      "loss": 0.4623,
      "step": 170
    },
    {
      "epoch": 0.1368,
      "grad_norm": 0.11558403819799423,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4922,
      "step": 171
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.12917964160442352,
      "learning_rate": 0.00017317269076305223,
      "loss": 0.5108,
      "step": 172
    },
    {
      "epoch": 0.1384,
      "grad_norm": 0.16417180001735687,
      "learning_rate": 0.0001730120481927711,
      "loss": 0.4896,
      "step": 173
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.16088753938674927,
      "learning_rate": 0.00017285140562248996,
      "loss": 0.5023,
      "step": 174
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.14503170549869537,
      "learning_rate": 0.00017269076305220885,
      "loss": 0.5431,
      "step": 175
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.14389348030090332,
      "learning_rate": 0.00017253012048192771,
      "loss": 0.5747,
      "step": 176
    },
    {
      "epoch": 0.1416,
      "grad_norm": 0.18187740445137024,
      "learning_rate": 0.00017236947791164658,
      "loss": 0.5608,
      "step": 177
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.16277804970741272,
      "learning_rate": 0.00017220883534136547,
      "loss": 0.4647,
      "step": 178
    },
    {
      "epoch": 0.1432,
      "grad_norm": 0.13883566856384277,
      "learning_rate": 0.00017204819277108434,
      "loss": 0.5311,
      "step": 179
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.15728750824928284,
      "learning_rate": 0.00017188755020080323,
      "loss": 0.5459,
      "step": 180
    },
    {
      "epoch": 0.1448,
      "grad_norm": 0.13676059246063232,
      "learning_rate": 0.0001717269076305221,
      "loss": 0.4843,
      "step": 181
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.1354270875453949,
      "learning_rate": 0.00017156626506024099,
      "loss": 0.522,
      "step": 182
    },
    {
      "epoch": 0.1464,
      "grad_norm": 0.13968771696090698,
      "learning_rate": 0.00017140562248995985,
      "loss": 0.4229,
      "step": 183
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.11910255998373032,
      "learning_rate": 0.00017124497991967871,
      "loss": 0.4956,
      "step": 184
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.15137623250484467,
      "learning_rate": 0.0001710843373493976,
      "loss": 0.5157,
      "step": 185
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.1357930302619934,
      "learning_rate": 0.00017092369477911647,
      "loss": 0.5589,
      "step": 186
    },
    {
      "epoch": 0.1496,
      "grad_norm": 0.1315680742263794,
      "learning_rate": 0.00017076305220883536,
      "loss": 0.4788,
      "step": 187
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.14758452773094177,
      "learning_rate": 0.00017060240963855423,
      "loss": 0.4925,
      "step": 188
    },
    {
      "epoch": 0.1512,
      "grad_norm": 0.1200568825006485,
      "learning_rate": 0.0001704417670682731,
      "loss": 0.5101,
      "step": 189
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.13893119990825653,
      "learning_rate": 0.00017028112449799199,
      "loss": 0.5042,
      "step": 190
    },
    {
      "epoch": 0.1528,
      "grad_norm": 0.1342712938785553,
      "learning_rate": 0.00017012048192771085,
      "loss": 0.5243,
      "step": 191
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.12991876900196075,
      "learning_rate": 0.00016995983935742971,
      "loss": 0.431,
      "step": 192
    },
    {
      "epoch": 0.1544,
      "grad_norm": 0.14516614377498627,
      "learning_rate": 0.0001697991967871486,
      "loss": 0.5109,
      "step": 193
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.15527354180812836,
      "learning_rate": 0.00016963855421686747,
      "loss": 0.5175,
      "step": 194
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.17462898790836334,
      "learning_rate": 0.00016947791164658636,
      "loss": 0.4878,
      "step": 195
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.1769501119852066,
      "learning_rate": 0.00016931726907630523,
      "loss": 0.5403,
      "step": 196
    },
    {
      "epoch": 0.1576,
      "grad_norm": 0.15957415103912354,
      "learning_rate": 0.00016915662650602412,
      "loss": 0.5248,
      "step": 197
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.18585708737373352,
      "learning_rate": 0.00016899598393574299,
      "loss": 0.4441,
      "step": 198
    },
    {
      "epoch": 0.1592,
      "grad_norm": 0.18542715907096863,
      "learning_rate": 0.00016883534136546185,
      "loss": 0.5737,
      "step": 199
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.12313076108694077,
      "learning_rate": 0.00016867469879518074,
      "loss": 0.475,
      "step": 200
    },
    {
      "epoch": 0.1608,
      "grad_norm": 0.12632684409618378,
      "learning_rate": 0.0001685140562248996,
      "loss": 0.4218,
      "step": 201
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.1384943723678589,
      "learning_rate": 0.00016835341365461847,
      "loss": 0.4124,
      "step": 202
    },
    {
      "epoch": 0.1624,
      "grad_norm": 0.15199628472328186,
      "learning_rate": 0.00016819277108433736,
      "loss": 0.5305,
      "step": 203
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.1450003981590271,
      "learning_rate": 0.00016803212851405623,
      "loss": 0.479,
      "step": 204
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.2667602300643921,
      "learning_rate": 0.00016787148594377512,
      "loss": 0.5056,
      "step": 205
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.09203991293907166,
      "learning_rate": 0.00016771084337349399,
      "loss": 0.4249,
      "step": 206
    },
    {
      "epoch": 0.1656,
      "grad_norm": 0.2991757094860077,
      "learning_rate": 0.00016755020080321288,
      "loss": 0.5383,
      "step": 207
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.17227867245674133,
      "learning_rate": 0.00016738955823293174,
      "loss": 0.4941,
      "step": 208
    },
    {
      "epoch": 0.1672,
      "grad_norm": 0.13823769986629486,
      "learning_rate": 0.0001672289156626506,
      "loss": 0.5349,
      "step": 209
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.14983586966991425,
      "learning_rate": 0.00016706827309236947,
      "loss": 0.4356,
      "step": 210
    },
    {
      "epoch": 0.1688,
      "grad_norm": 0.15231658518314362,
      "learning_rate": 0.00016690763052208836,
      "loss": 0.5085,
      "step": 211
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.1412215530872345,
      "learning_rate": 0.00016674698795180723,
      "loss": 0.4252,
      "step": 212
    },
    {
      "epoch": 0.1704,
      "grad_norm": 0.11999603360891342,
      "learning_rate": 0.00016658634538152612,
      "loss": 0.4991,
      "step": 213
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.18846504390239716,
      "learning_rate": 0.00016642570281124499,
      "loss": 0.4877,
      "step": 214
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.17006219923496246,
      "learning_rate": 0.00016626506024096388,
      "loss": 0.5319,
      "step": 215
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.13713037967681885,
      "learning_rate": 0.00016610441767068274,
      "loss": 0.4456,
      "step": 216
    },
    {
      "epoch": 0.1736,
      "grad_norm": 0.25337573885917664,
      "learning_rate": 0.00016594377510040163,
      "loss": 0.5467,
      "step": 217
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.13771964609622955,
      "learning_rate": 0.0001657831325301205,
      "loss": 0.489,
      "step": 218
    },
    {
      "epoch": 0.1752,
      "grad_norm": 0.12303406745195389,
      "learning_rate": 0.00016562248995983936,
      "loss": 0.4498,
      "step": 219
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.13758894801139832,
      "learning_rate": 0.00016546184738955823,
      "loss": 0.4733,
      "step": 220
    },
    {
      "epoch": 0.1768,
      "grad_norm": 0.1379072219133377,
      "learning_rate": 0.00016530120481927712,
      "loss": 0.4534,
      "step": 221
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.1367827206850052,
      "learning_rate": 0.00016514056224899599,
      "loss": 0.4839,
      "step": 222
    },
    {
      "epoch": 0.1784,
      "grad_norm": 0.13524028658866882,
      "learning_rate": 0.00016497991967871488,
      "loss": 0.5062,
      "step": 223
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.14061260223388672,
      "learning_rate": 0.00016481927710843374,
      "loss": 0.5015,
      "step": 224
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.11246717721223831,
      "learning_rate": 0.00016465863453815263,
      "loss": 0.581,
      "step": 225
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.12819992005825043,
      "learning_rate": 0.0001644979919678715,
      "loss": 0.4902,
      "step": 226
    },
    {
      "epoch": 0.1816,
      "grad_norm": 0.1229761466383934,
      "learning_rate": 0.0001643373493975904,
      "loss": 0.5064,
      "step": 227
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.13272273540496826,
      "learning_rate": 0.00016417670682730923,
      "loss": 0.4974,
      "step": 228
    },
    {
      "epoch": 0.1832,
      "grad_norm": 0.1040247455239296,
      "learning_rate": 0.00016401606425702812,
      "loss": 0.4707,
      "step": 229
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.14711539447307587,
      "learning_rate": 0.00016385542168674699,
      "loss": 0.5477,
      "step": 230
    },
    {
      "epoch": 0.1848,
      "grad_norm": 0.1563195139169693,
      "learning_rate": 0.00016369477911646588,
      "loss": 0.5715,
      "step": 231
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.12796375155448914,
      "learning_rate": 0.00016353413654618474,
      "loss": 0.5335,
      "step": 232
    },
    {
      "epoch": 0.1864,
      "grad_norm": 0.13076534867286682,
      "learning_rate": 0.00016337349397590363,
      "loss": 0.4475,
      "step": 233
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.16574493050575256,
      "learning_rate": 0.0001632128514056225,
      "loss": 0.5459,
      "step": 234
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.14164406061172485,
      "learning_rate": 0.0001630522088353414,
      "loss": 0.5642,
      "step": 235
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.1378932148218155,
      "learning_rate": 0.00016289156626506026,
      "loss": 0.5464,
      "step": 236
    },
    {
      "epoch": 0.1896,
      "grad_norm": 0.1325736939907074,
      "learning_rate": 0.00016273092369477912,
      "loss": 0.5688,
      "step": 237
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.1439254730939865,
      "learning_rate": 0.00016257028112449799,
      "loss": 0.4897,
      "step": 238
    },
    {
      "epoch": 0.1912,
      "grad_norm": 0.1003589779138565,
      "learning_rate": 0.00016240963855421688,
      "loss": 0.5146,
      "step": 239
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.11221017688512802,
      "learning_rate": 0.00016224899598393574,
      "loss": 0.4834,
      "step": 240
    },
    {
      "epoch": 0.1928,
      "grad_norm": 0.14700821042060852,
      "learning_rate": 0.00016208835341365463,
      "loss": 0.4697,
      "step": 241
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.11809167265892029,
      "learning_rate": 0.0001619277108433735,
      "loss": 0.4768,
      "step": 242
    },
    {
      "epoch": 0.1944,
      "grad_norm": 0.13485923409461975,
      "learning_rate": 0.0001617670682730924,
      "loss": 0.4777,
      "step": 243
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.18821536004543304,
      "learning_rate": 0.00016160642570281126,
      "loss": 0.5516,
      "step": 244
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.12611427903175354,
      "learning_rate": 0.00016144578313253015,
      "loss": 0.5934,
      "step": 245
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.1525760143995285,
      "learning_rate": 0.00016128514056224899,
      "loss": 0.4538,
      "step": 246
    },
    {
      "epoch": 0.1976,
      "grad_norm": 0.12517401576042175,
      "learning_rate": 0.00016112449799196788,
      "loss": 0.4746,
      "step": 247
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.1394810676574707,
      "learning_rate": 0.00016096385542168674,
      "loss": 0.5044,
      "step": 248
    },
    {
      "epoch": 0.1992,
      "grad_norm": 0.12467650324106216,
      "learning_rate": 0.00016080321285140563,
      "loss": 0.4732,
      "step": 249
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.16997060179710388,
      "learning_rate": 0.0001606425702811245,
      "loss": 0.6336,
      "step": 250
    },
    {
      "epoch": 0.2008,
      "grad_norm": 0.10290290415287018,
      "learning_rate": 0.0001604819277108434,
      "loss": 0.4657,
      "step": 251
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.09954996407032013,
      "learning_rate": 0.00016032128514056226,
      "loss": 0.4863,
      "step": 252
    },
    {
      "epoch": 0.2024,
      "grad_norm": 0.1514117568731308,
      "learning_rate": 0.00016016064257028115,
      "loss": 0.5674,
      "step": 253
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.14140018820762634,
      "learning_rate": 0.00016,
      "loss": 0.4819,
      "step": 254
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.1422271728515625,
      "learning_rate": 0.00015983935742971888,
      "loss": 0.4641,
      "step": 255
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.2073243260383606,
      "learning_rate": 0.00015967871485943774,
      "loss": 0.5416,
      "step": 256
    },
    {
      "epoch": 0.2056,
      "grad_norm": 0.14334101974964142,
      "learning_rate": 0.00015951807228915663,
      "loss": 0.5264,
      "step": 257
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.14758847653865814,
      "learning_rate": 0.0001593574297188755,
      "loss": 0.4814,
      "step": 258
    },
    {
      "epoch": 0.2072,
      "grad_norm": 0.14874349534511566,
      "learning_rate": 0.0001591967871485944,
      "loss": 0.517,
      "step": 259
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.11460034549236298,
      "learning_rate": 0.00015903614457831326,
      "loss": 0.4911,
      "step": 260
    },
    {
      "epoch": 0.2088,
      "grad_norm": 0.12915468215942383,
      "learning_rate": 0.00015887550200803215,
      "loss": 0.4812,
      "step": 261
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.1403273642063141,
      "learning_rate": 0.000158714859437751,
      "loss": 0.4925,
      "step": 262
    },
    {
      "epoch": 0.2104,
      "grad_norm": 0.13195396959781647,
      "learning_rate": 0.0001585542168674699,
      "loss": 0.5044,
      "step": 263
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.13971373438835144,
      "learning_rate": 0.00015839357429718874,
      "loss": 0.489,
      "step": 264
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.17490024864673615,
      "learning_rate": 0.00015823293172690763,
      "loss": 0.5432,
      "step": 265
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.15205681324005127,
      "learning_rate": 0.0001580722891566265,
      "loss": 0.511,
      "step": 266
    },
    {
      "epoch": 0.2136,
      "grad_norm": 0.19073911011219025,
      "learning_rate": 0.0001579116465863454,
      "loss": 0.5866,
      "step": 267
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.1283777356147766,
      "learning_rate": 0.00015775100401606426,
      "loss": 0.4765,
      "step": 268
    },
    {
      "epoch": 0.2152,
      "grad_norm": 0.16130033135414124,
      "learning_rate": 0.00015759036144578315,
      "loss": 0.5359,
      "step": 269
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.11957390606403351,
      "learning_rate": 0.000157429718875502,
      "loss": 0.569,
      "step": 270
    },
    {
      "epoch": 0.2168,
      "grad_norm": 0.1405438929796219,
      "learning_rate": 0.0001572690763052209,
      "loss": 0.5412,
      "step": 271
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.1211351752281189,
      "learning_rate": 0.00015710843373493977,
      "loss": 0.5304,
      "step": 272
    },
    {
      "epoch": 0.2184,
      "grad_norm": 0.11422468721866608,
      "learning_rate": 0.00015694779116465866,
      "loss": 0.4431,
      "step": 273
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.10079476982355118,
      "learning_rate": 0.0001567871485943775,
      "loss": 0.4443,
      "step": 274
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.1517678052186966,
      "learning_rate": 0.0001566265060240964,
      "loss": 0.5187,
      "step": 275
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.13590426743030548,
      "learning_rate": 0.00015646586345381526,
      "loss": 0.5193,
      "step": 276
    },
    {
      "epoch": 0.2216,
      "grad_norm": 0.11251769214868546,
      "learning_rate": 0.00015630522088353415,
      "loss": 0.468,
      "step": 277
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.12393767386674881,
      "learning_rate": 0.000156144578313253,
      "loss": 0.4457,
      "step": 278
    },
    {
      "epoch": 0.2232,
      "grad_norm": 0.16574381291866302,
      "learning_rate": 0.0001559839357429719,
      "loss": 0.5097,
      "step": 279
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.12840361893177032,
      "learning_rate": 0.00015582329317269077,
      "loss": 0.4339,
      "step": 280
    },
    {
      "epoch": 0.2248,
      "grad_norm": 0.1273231953382492,
      "learning_rate": 0.00015566265060240966,
      "loss": 0.5075,
      "step": 281
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.11425511538982391,
      "learning_rate": 0.00015550200803212853,
      "loss": 0.5371,
      "step": 282
    },
    {
      "epoch": 0.2264,
      "grad_norm": 0.1064603328704834,
      "learning_rate": 0.0001553413654618474,
      "loss": 0.4736,
      "step": 283
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.11390958726406097,
      "learning_rate": 0.00015518072289156626,
      "loss": 0.4735,
      "step": 284
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.12802381813526154,
      "learning_rate": 0.00015502008032128515,
      "loss": 0.4964,
      "step": 285
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.12711894512176514,
      "learning_rate": 0.000154859437751004,
      "loss": 0.5393,
      "step": 286
    },
    {
      "epoch": 0.2296,
      "grad_norm": 0.14484970271587372,
      "learning_rate": 0.0001546987951807229,
      "loss": 0.526,
      "step": 287
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.14064733684062958,
      "learning_rate": 0.00015453815261044177,
      "loss": 0.561,
      "step": 288
    },
    {
      "epoch": 0.2312,
      "grad_norm": 0.11199415475130081,
      "learning_rate": 0.00015437751004016066,
      "loss": 0.4828,
      "step": 289
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.18842831254005432,
      "learning_rate": 0.00015421686746987953,
      "loss": 0.5959,
      "step": 290
    },
    {
      "epoch": 0.2328,
      "grad_norm": 0.145424947142601,
      "learning_rate": 0.00015405622489959842,
      "loss": 0.6047,
      "step": 291
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.1908039003610611,
      "learning_rate": 0.00015389558232931726,
      "loss": 0.5243,
      "step": 292
    },
    {
      "epoch": 0.2344,
      "grad_norm": 0.15801353752613068,
      "learning_rate": 0.00015373493975903615,
      "loss": 0.5114,
      "step": 293
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.28227850794792175,
      "learning_rate": 0.00015357429718875501,
      "loss": 0.5852,
      "step": 294
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.13431724905967712,
      "learning_rate": 0.0001534136546184739,
      "loss": 0.4713,
      "step": 295
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.13184873759746552,
      "learning_rate": 0.00015325301204819277,
      "loss": 0.5208,
      "step": 296
    },
    {
      "epoch": 0.2376,
      "grad_norm": 0.14489321410655975,
      "learning_rate": 0.00015309236947791166,
      "loss": 0.4867,
      "step": 297
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.15549485385417938,
      "learning_rate": 0.00015293172690763053,
      "loss": 0.5266,
      "step": 298
    },
    {
      "epoch": 0.2392,
      "grad_norm": 0.13148309290409088,
      "learning_rate": 0.00015277108433734942,
      "loss": 0.5269,
      "step": 299
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.10631942749023438,
      "learning_rate": 0.00015261044176706828,
      "loss": 0.5376,
      "step": 300
    },
    {
      "epoch": 0.2408,
      "grad_norm": 0.1441703587770462,
      "learning_rate": 0.00015244979919678715,
      "loss": 0.5141,
      "step": 301
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.14041106402873993,
      "learning_rate": 0.00015228915662650601,
      "loss": 0.4795,
      "step": 302
    },
    {
      "epoch": 0.2424,
      "grad_norm": 0.14060905575752258,
      "learning_rate": 0.0001521285140562249,
      "loss": 0.4681,
      "step": 303
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.14538908004760742,
      "learning_rate": 0.00015196787148594377,
      "loss": 0.501,
      "step": 304
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.11138054728507996,
      "learning_rate": 0.00015180722891566266,
      "loss": 0.5184,
      "step": 305
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.10783391445875168,
      "learning_rate": 0.00015164658634538153,
      "loss": 0.4652,
      "step": 306
    },
    {
      "epoch": 0.2456,
      "grad_norm": 0.12778566777706146,
      "learning_rate": 0.00015148594377510042,
      "loss": 0.4752,
      "step": 307
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.09369325637817383,
      "learning_rate": 0.00015132530120481928,
      "loss": 0.4745,
      "step": 308
    },
    {
      "epoch": 0.2472,
      "grad_norm": 0.1491166055202484,
      "learning_rate": 0.00015116465863453818,
      "loss": 0.5098,
      "step": 309
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.10092397779226303,
      "learning_rate": 0.00015100401606425701,
      "loss": 0.4806,
      "step": 310
    },
    {
      "epoch": 0.2488,
      "grad_norm": 0.10650447756052017,
      "learning_rate": 0.0001508433734939759,
      "loss": 0.4374,
      "step": 311
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.1586790233850479,
      "learning_rate": 0.00015068273092369477,
      "loss": 0.4703,
      "step": 312
    },
    {
      "epoch": 0.2504,
      "grad_norm": 0.11130639910697937,
      "learning_rate": 0.00015052208835341366,
      "loss": 0.4929,
      "step": 313
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.17083550989627838,
      "learning_rate": 0.00015036144578313253,
      "loss": 0.4921,
      "step": 314
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.11817184090614319,
      "learning_rate": 0.00015020080321285142,
      "loss": 0.4634,
      "step": 315
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.09470605105161667,
      "learning_rate": 0.00015004016064257028,
      "loss": 0.4859,
      "step": 316
    },
    {
      "epoch": 0.2536,
      "grad_norm": 0.20535019040107727,
      "learning_rate": 0.00014987951807228918,
      "loss": 0.5193,
      "step": 317
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.15144005417823792,
      "learning_rate": 0.00014971887550200804,
      "loss": 0.4702,
      "step": 318
    },
    {
      "epoch": 0.2552,
      "grad_norm": 0.15328684449195862,
      "learning_rate": 0.0001495582329317269,
      "loss": 0.5076,
      "step": 319
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.15188124775886536,
      "learning_rate": 0.00014939759036144577,
      "loss": 0.4806,
      "step": 320
    },
    {
      "epoch": 0.2568,
      "grad_norm": 0.09794263541698456,
      "learning_rate": 0.00014923694779116466,
      "loss": 0.4754,
      "step": 321
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.14065814018249512,
      "learning_rate": 0.00014907630522088353,
      "loss": 0.5581,
      "step": 322
    },
    {
      "epoch": 0.2584,
      "grad_norm": 0.16427262127399445,
      "learning_rate": 0.00014891566265060242,
      "loss": 0.4655,
      "step": 323
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.15315213799476624,
      "learning_rate": 0.00014875502008032128,
      "loss": 0.5025,
      "step": 324
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.12416176497936249,
      "learning_rate": 0.00014859437751004018,
      "loss": 0.5223,
      "step": 325
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.13869819045066833,
      "learning_rate": 0.00014843373493975904,
      "loss": 0.5118,
      "step": 326
    },
    {
      "epoch": 0.2616,
      "grad_norm": 0.14573624730110168,
      "learning_rate": 0.00014827309236947793,
      "loss": 0.5015,
      "step": 327
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.11648034304380417,
      "learning_rate": 0.00014811244979919677,
      "loss": 0.4758,
      "step": 328
    },
    {
      "epoch": 0.2632,
      "grad_norm": 0.11753533780574799,
      "learning_rate": 0.00014795180722891566,
      "loss": 0.5161,
      "step": 329
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.10458079725503922,
      "learning_rate": 0.00014779116465863453,
      "loss": 0.4507,
      "step": 330
    },
    {
      "epoch": 0.2648,
      "grad_norm": 0.1070442870259285,
      "learning_rate": 0.00014763052208835342,
      "loss": 0.4832,
      "step": 331
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.1294262707233429,
      "learning_rate": 0.00014746987951807228,
      "loss": 0.4612,
      "step": 332
    },
    {
      "epoch": 0.2664,
      "grad_norm": 0.1320263147354126,
      "learning_rate": 0.00014730923694779118,
      "loss": 0.5244,
      "step": 333
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.10904256999492645,
      "learning_rate": 0.00014714859437751004,
      "loss": 0.4918,
      "step": 334
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.16221925616264343,
      "learning_rate": 0.00014698795180722893,
      "loss": 0.4982,
      "step": 335
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.12812183797359467,
      "learning_rate": 0.0001468273092369478,
      "loss": 0.5587,
      "step": 336
    },
    {
      "epoch": 0.2696,
      "grad_norm": 0.20594188570976257,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.5634,
      "step": 337
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.10882114619016647,
      "learning_rate": 0.00014650602409638555,
      "loss": 0.4866,
      "step": 338
    },
    {
      "epoch": 0.2712,
      "grad_norm": 0.09235403686761856,
      "learning_rate": 0.00014634538152610442,
      "loss": 0.4453,
      "step": 339
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.14543350040912628,
      "learning_rate": 0.00014618473895582328,
      "loss": 0.4246,
      "step": 340
    },
    {
      "epoch": 0.2728,
      "grad_norm": 0.13408906757831573,
      "learning_rate": 0.00014602409638554218,
      "loss": 0.4453,
      "step": 341
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.16603682935237885,
      "learning_rate": 0.00014586345381526104,
      "loss": 0.5035,
      "step": 342
    },
    {
      "epoch": 0.2744,
      "grad_norm": 0.09760361164808273,
      "learning_rate": 0.00014570281124497993,
      "loss": 0.4574,
      "step": 343
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.14554762840270996,
      "learning_rate": 0.0001455421686746988,
      "loss": 0.5106,
      "step": 344
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.1627945452928543,
      "learning_rate": 0.0001453815261044177,
      "loss": 0.5082,
      "step": 345
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.13043348491191864,
      "learning_rate": 0.00014522088353413655,
      "loss": 0.5389,
      "step": 346
    },
    {
      "epoch": 0.2776,
      "grad_norm": 0.17354747653007507,
      "learning_rate": 0.00014506024096385542,
      "loss": 0.5121,
      "step": 347
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.10157424956560135,
      "learning_rate": 0.0001448995983935743,
      "loss": 0.435,
      "step": 348
    },
    {
      "epoch": 0.2792,
      "grad_norm": 0.12878796458244324,
      "learning_rate": 0.00014473895582329318,
      "loss": 0.476,
      "step": 349
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.11971824616193771,
      "learning_rate": 0.00014457831325301204,
      "loss": 0.4942,
      "step": 350
    },
    {
      "epoch": 0.2808,
      "grad_norm": 0.1554393172264099,
      "learning_rate": 0.00014441767068273093,
      "loss": 0.5111,
      "step": 351
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.11456134915351868,
      "learning_rate": 0.0001442570281124498,
      "loss": 0.4625,
      "step": 352
    },
    {
      "epoch": 0.2824,
      "grad_norm": 0.16754098236560822,
      "learning_rate": 0.0001440963855421687,
      "loss": 0.5429,
      "step": 353
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.13361409306526184,
      "learning_rate": 0.00014393574297188756,
      "loss": 0.3978,
      "step": 354
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.14138942956924438,
      "learning_rate": 0.00014377510040160642,
      "loss": 0.5055,
      "step": 355
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.15240679681301117,
      "learning_rate": 0.0001436144578313253,
      "loss": 0.4966,
      "step": 356
    },
    {
      "epoch": 0.2856,
      "grad_norm": 0.12172773480415344,
      "learning_rate": 0.00014345381526104418,
      "loss": 0.5009,
      "step": 357
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.14872828125953674,
      "learning_rate": 0.00014329317269076307,
      "loss": 0.5102,
      "step": 358
    },
    {
      "epoch": 0.2872,
      "grad_norm": 0.17000077664852142,
      "learning_rate": 0.00014313253012048193,
      "loss": 0.5252,
      "step": 359
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.1241719201207161,
      "learning_rate": 0.0001429718875502008,
      "loss": 0.4886,
      "step": 360
    },
    {
      "epoch": 0.2888,
      "grad_norm": 0.1359301507472992,
      "learning_rate": 0.0001428112449799197,
      "loss": 0.4613,
      "step": 361
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.14940646290779114,
      "learning_rate": 0.00014265060240963856,
      "loss": 0.426,
      "step": 362
    },
    {
      "epoch": 0.2904,
      "grad_norm": 0.13447009027004242,
      "learning_rate": 0.00014248995983935745,
      "loss": 0.5532,
      "step": 363
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.1325499564409256,
      "learning_rate": 0.0001423293172690763,
      "loss": 0.5086,
      "step": 364
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.214682936668396,
      "learning_rate": 0.00014216867469879518,
      "loss": 0.4452,
      "step": 365
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.13268506526947021,
      "learning_rate": 0.00014200803212851407,
      "loss": 0.4961,
      "step": 366
    },
    {
      "epoch": 0.2936,
      "grad_norm": 0.1623477041721344,
      "learning_rate": 0.00014184738955823293,
      "loss": 0.5265,
      "step": 367
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.13175636529922485,
      "learning_rate": 0.00014168674698795183,
      "loss": 0.5163,
      "step": 368
    },
    {
      "epoch": 0.2952,
      "grad_norm": 0.13719303905963898,
      "learning_rate": 0.0001415261044176707,
      "loss": 0.5449,
      "step": 369
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.17873899638652802,
      "learning_rate": 0.00014136546184738956,
      "loss": 0.5014,
      "step": 370
    },
    {
      "epoch": 0.2968,
      "grad_norm": 0.11260808259248734,
      "learning_rate": 0.00014120481927710845,
      "loss": 0.4743,
      "step": 371
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.14102815091609955,
      "learning_rate": 0.0001410441767068273,
      "loss": 0.5328,
      "step": 372
    },
    {
      "epoch": 0.2984,
      "grad_norm": 0.15604327619075775,
      "learning_rate": 0.0001408835341365462,
      "loss": 0.4412,
      "step": 373
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.18151336908340454,
      "learning_rate": 0.00014072289156626507,
      "loss": 0.5072,
      "step": 374
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.13750095665454865,
      "learning_rate": 0.00014056224899598393,
      "loss": 0.5326,
      "step": 375
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.10874955356121063,
      "learning_rate": 0.00014040160642570283,
      "loss": 0.4397,
      "step": 376
    },
    {
      "epoch": 0.3016,
      "grad_norm": 0.18464957177639008,
      "learning_rate": 0.0001402409638554217,
      "loss": 0.4802,
      "step": 377
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.1626199334859848,
      "learning_rate": 0.00014008032128514058,
      "loss": 0.4697,
      "step": 378
    },
    {
      "epoch": 0.3032,
      "grad_norm": 0.14606080949306488,
      "learning_rate": 0.00013991967871485945,
      "loss": 0.4728,
      "step": 379
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.19355034828186035,
      "learning_rate": 0.00013975903614457834,
      "loss": 0.5069,
      "step": 380
    },
    {
      "epoch": 0.3048,
      "grad_norm": 0.10869192332029343,
      "learning_rate": 0.0001395983935742972,
      "loss": 0.4144,
      "step": 381
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.11052564531564713,
      "learning_rate": 0.00013943775100401607,
      "loss": 0.5151,
      "step": 382
    },
    {
      "epoch": 0.3064,
      "grad_norm": 0.12199825793504715,
      "learning_rate": 0.00013927710843373493,
      "loss": 0.502,
      "step": 383
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.12953825294971466,
      "learning_rate": 0.00013911646586345383,
      "loss": 0.4524,
      "step": 384
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.14892993867397308,
      "learning_rate": 0.0001389558232931727,
      "loss": 0.5636,
      "step": 385
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.12146665900945663,
      "learning_rate": 0.00013879518072289158,
      "loss": 0.5273,
      "step": 386
    },
    {
      "epoch": 0.3096,
      "grad_norm": 0.13886158168315887,
      "learning_rate": 0.00013863453815261045,
      "loss": 0.4429,
      "step": 387
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.15585015714168549,
      "learning_rate": 0.00013847389558232934,
      "loss": 0.484,
      "step": 388
    },
    {
      "epoch": 0.3112,
      "grad_norm": 0.09534807503223419,
      "learning_rate": 0.0001383132530120482,
      "loss": 0.5125,
      "step": 389
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.16011078655719757,
      "learning_rate": 0.0001381526104417671,
      "loss": 0.4836,
      "step": 390
    },
    {
      "epoch": 0.3128,
      "grad_norm": 0.18073256313800812,
      "learning_rate": 0.00013799196787148596,
      "loss": 0.4249,
      "step": 391
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.12187714874744415,
      "learning_rate": 0.00013783132530120483,
      "loss": 0.4586,
      "step": 392
    },
    {
      "epoch": 0.3144,
      "grad_norm": 0.15822507441043854,
      "learning_rate": 0.0001376706827309237,
      "loss": 0.5127,
      "step": 393
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.14969941973686218,
      "learning_rate": 0.00013751004016064258,
      "loss": 0.4709,
      "step": 394
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.14236630499362946,
      "learning_rate": 0.00013734939759036145,
      "loss": 0.5114,
      "step": 395
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.11868630349636078,
      "learning_rate": 0.00013718875502008034,
      "loss": 0.4927,
      "step": 396
    },
    {
      "epoch": 0.3176,
      "grad_norm": 0.12686991691589355,
      "learning_rate": 0.0001370281124497992,
      "loss": 0.4131,
      "step": 397
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.1239229217171669,
      "learning_rate": 0.0001368674698795181,
      "loss": 0.4514,
      "step": 398
    },
    {
      "epoch": 0.3192,
      "grad_norm": 0.11924585700035095,
      "learning_rate": 0.00013670682730923696,
      "loss": 0.4221,
      "step": 399
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.08564293384552002,
      "learning_rate": 0.00013654618473895585,
      "loss": 0.4373,
      "step": 400
    },
    {
      "epoch": 0.3208,
      "grad_norm": 0.1436561942100525,
      "learning_rate": 0.0001363855421686747,
      "loss": 0.5477,
      "step": 401
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.1583041250705719,
      "learning_rate": 0.00013622489959839358,
      "loss": 0.4762,
      "step": 402
    },
    {
      "epoch": 0.3224,
      "grad_norm": 0.15051548182964325,
      "learning_rate": 0.00013606425702811245,
      "loss": 0.5521,
      "step": 403
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.1476561576128006,
      "learning_rate": 0.00013590361445783134,
      "loss": 0.4155,
      "step": 404
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.15965716540813446,
      "learning_rate": 0.0001357429718875502,
      "loss": 0.5667,
      "step": 405
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.14278820157051086,
      "learning_rate": 0.0001355823293172691,
      "loss": 0.4743,
      "step": 406
    },
    {
      "epoch": 0.3256,
      "grad_norm": 0.12323029339313507,
      "learning_rate": 0.00013542168674698796,
      "loss": 0.513,
      "step": 407
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.14317616820335388,
      "learning_rate": 0.00013526104417670685,
      "loss": 0.5527,
      "step": 408
    },
    {
      "epoch": 0.3272,
      "grad_norm": 0.1459268182516098,
      "learning_rate": 0.00013510040160642572,
      "loss": 0.4533,
      "step": 409
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.1997382938861847,
      "learning_rate": 0.00013493975903614458,
      "loss": 0.5517,
      "step": 410
    },
    {
      "epoch": 0.3288,
      "grad_norm": 0.15160265564918518,
      "learning_rate": 0.00013477911646586345,
      "loss": 0.5656,
      "step": 411
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.16655263304710388,
      "learning_rate": 0.00013461847389558234,
      "loss": 0.5263,
      "step": 412
    },
    {
      "epoch": 0.3304,
      "grad_norm": 0.13338890671730042,
      "learning_rate": 0.0001344578313253012,
      "loss": 0.5242,
      "step": 413
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.13966058194637299,
      "learning_rate": 0.0001342971887550201,
      "loss": 0.6014,
      "step": 414
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.12121088802814484,
      "learning_rate": 0.00013413654618473896,
      "loss": 0.5241,
      "step": 415
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.12544845044612885,
      "learning_rate": 0.00013397590361445785,
      "loss": 0.4428,
      "step": 416
    },
    {
      "epoch": 0.3336,
      "grad_norm": 0.11312253028154373,
      "learning_rate": 0.00013381526104417672,
      "loss": 0.5896,
      "step": 417
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.14312651753425598,
      "learning_rate": 0.0001336546184738956,
      "loss": 0.5463,
      "step": 418
    },
    {
      "epoch": 0.3352,
      "grad_norm": 0.11494959145784378,
      "learning_rate": 0.00013349397590361445,
      "loss": 0.4904,
      "step": 419
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1104433462023735,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.4712,
      "step": 420
    },
    {
      "epoch": 0.3368,
      "grad_norm": 0.10495848208665848,
      "learning_rate": 0.0001331726907630522,
      "loss": 0.4624,
      "step": 421
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.12345276027917862,
      "learning_rate": 0.0001330120481927711,
      "loss": 0.512,
      "step": 422
    },
    {
      "epoch": 0.3384,
      "grad_norm": 0.11625280976295471,
      "learning_rate": 0.00013285140562248996,
      "loss": 0.4953,
      "step": 423
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.09235560148954391,
      "learning_rate": 0.00013269076305220885,
      "loss": 0.4285,
      "step": 424
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.09924140572547913,
      "learning_rate": 0.00013253012048192772,
      "loss": 0.4833,
      "step": 425
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.09852057695388794,
      "learning_rate": 0.0001323694779116466,
      "loss": 0.4692,
      "step": 426
    },
    {
      "epoch": 0.3416,
      "grad_norm": 0.1240922063589096,
      "learning_rate": 0.00013220883534136547,
      "loss": 0.5282,
      "step": 427
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.10211552679538727,
      "learning_rate": 0.00013204819277108434,
      "loss": 0.4607,
      "step": 428
    },
    {
      "epoch": 0.3432,
      "grad_norm": 0.11214000731706619,
      "learning_rate": 0.0001318875502008032,
      "loss": 0.4522,
      "step": 429
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.11233930289745331,
      "learning_rate": 0.0001317269076305221,
      "loss": 0.4594,
      "step": 430
    },
    {
      "epoch": 0.3448,
      "grad_norm": 0.15934018790721893,
      "learning_rate": 0.00013156626506024096,
      "loss": 0.5402,
      "step": 431
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.09215739369392395,
      "learning_rate": 0.00013140562248995985,
      "loss": 0.4325,
      "step": 432
    },
    {
      "epoch": 0.3464,
      "grad_norm": 0.18278038501739502,
      "learning_rate": 0.00013124497991967872,
      "loss": 0.5084,
      "step": 433
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.11125212907791138,
      "learning_rate": 0.0001310843373493976,
      "loss": 0.3849,
      "step": 434
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.1343291848897934,
      "learning_rate": 0.00013092369477911648,
      "loss": 0.4676,
      "step": 435
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.1555168181657791,
      "learning_rate": 0.00013076305220883537,
      "loss": 0.5263,
      "step": 436
    },
    {
      "epoch": 0.3496,
      "grad_norm": 0.11222890764474869,
      "learning_rate": 0.0001306024096385542,
      "loss": 0.4858,
      "step": 437
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.1280864179134369,
      "learning_rate": 0.0001304417670682731,
      "loss": 0.5321,
      "step": 438
    },
    {
      "epoch": 0.3512,
      "grad_norm": 0.15732920169830322,
      "learning_rate": 0.00013028112449799196,
      "loss": 0.5716,
      "step": 439
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.14652392268180847,
      "learning_rate": 0.00013012048192771085,
      "loss": 0.5656,
      "step": 440
    },
    {
      "epoch": 0.3528,
      "grad_norm": 0.10851438343524933,
      "learning_rate": 0.00012995983935742972,
      "loss": 0.492,
      "step": 441
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.12644389271736145,
      "learning_rate": 0.0001297991967871486,
      "loss": 0.4531,
      "step": 442
    },
    {
      "epoch": 0.3544,
      "grad_norm": 0.12032592296600342,
      "learning_rate": 0.00012963855421686748,
      "loss": 0.509,
      "step": 443
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.10607510060071945,
      "learning_rate": 0.00012947791164658637,
      "loss": 0.4683,
      "step": 444
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.11731640249490738,
      "learning_rate": 0.00012931726907630523,
      "loss": 0.4925,
      "step": 445
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.08349202573299408,
      "learning_rate": 0.0001291566265060241,
      "loss": 0.4963,
      "step": 446
    },
    {
      "epoch": 0.3576,
      "grad_norm": 0.11429208517074585,
      "learning_rate": 0.00012899598393574296,
      "loss": 0.4765,
      "step": 447
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.1618610918521881,
      "learning_rate": 0.00012883534136546185,
      "loss": 0.4953,
      "step": 448
    },
    {
      "epoch": 0.3592,
      "grad_norm": 0.09481294453144073,
      "learning_rate": 0.00012867469879518072,
      "loss": 0.5025,
      "step": 449
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.10300091654062271,
      "learning_rate": 0.0001285140562248996,
      "loss": 0.4668,
      "step": 450
    },
    {
      "epoch": 0.3608,
      "grad_norm": 0.1424153745174408,
      "learning_rate": 0.00012835341365461848,
      "loss": 0.5036,
      "step": 451
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.10467083007097244,
      "learning_rate": 0.00012819277108433737,
      "loss": 0.5202,
      "step": 452
    },
    {
      "epoch": 0.3624,
      "grad_norm": 0.08802121132612228,
      "learning_rate": 0.00012803212851405623,
      "loss": 0.4564,
      "step": 453
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.13228994607925415,
      "learning_rate": 0.00012787148594377512,
      "loss": 0.5097,
      "step": 454
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.10729997605085373,
      "learning_rate": 0.00012771084337349396,
      "loss": 0.4718,
      "step": 455
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.11877725273370743,
      "learning_rate": 0.00012755020080321285,
      "loss": 0.5461,
      "step": 456
    },
    {
      "epoch": 0.3656,
      "grad_norm": 0.1288532316684723,
      "learning_rate": 0.00012738955823293172,
      "loss": 0.5506,
      "step": 457
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.11323190480470657,
      "learning_rate": 0.0001272289156626506,
      "loss": 0.5014,
      "step": 458
    },
    {
      "epoch": 0.3672,
      "grad_norm": 0.15634778141975403,
      "learning_rate": 0.00012706827309236948,
      "loss": 0.5705,
      "step": 459
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.13081595301628113,
      "learning_rate": 0.00012690763052208837,
      "loss": 0.4211,
      "step": 460
    },
    {
      "epoch": 0.3688,
      "grad_norm": 0.11833938956260681,
      "learning_rate": 0.00012674698795180723,
      "loss": 0.5569,
      "step": 461
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.07925142347812653,
      "learning_rate": 0.00012658634538152612,
      "loss": 0.4645,
      "step": 462
    },
    {
      "epoch": 0.3704,
      "grad_norm": 0.13969217240810394,
      "learning_rate": 0.000126425702811245,
      "loss": 0.4961,
      "step": 463
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.102584108710289,
      "learning_rate": 0.00012626506024096385,
      "loss": 0.4805,
      "step": 464
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.11708274483680725,
      "learning_rate": 0.00012610441767068272,
      "loss": 0.5092,
      "step": 465
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.12214212119579315,
      "learning_rate": 0.0001259437751004016,
      "loss": 0.5013,
      "step": 466
    },
    {
      "epoch": 0.3736,
      "grad_norm": 0.10202129930257797,
      "learning_rate": 0.00012578313253012048,
      "loss": 0.4631,
      "step": 467
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.10951036959886551,
      "learning_rate": 0.00012562248995983937,
      "loss": 0.5222,
      "step": 468
    },
    {
      "epoch": 0.3752,
      "grad_norm": 0.11080054938793182,
      "learning_rate": 0.00012546184738955823,
      "loss": 0.4803,
      "step": 469
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.1190124973654747,
      "learning_rate": 0.00012530120481927712,
      "loss": 0.5444,
      "step": 470
    },
    {
      "epoch": 0.3768,
      "grad_norm": 0.11260151118040085,
      "learning_rate": 0.000125140562248996,
      "loss": 0.4929,
      "step": 471
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.12430649250745773,
      "learning_rate": 0.00012497991967871488,
      "loss": 0.5408,
      "step": 472
    },
    {
      "epoch": 0.3784,
      "grad_norm": 0.12638334929943085,
      "learning_rate": 0.00012481927710843375,
      "loss": 0.4365,
      "step": 473
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.18023261427879333,
      "learning_rate": 0.0001246586345381526,
      "loss": 0.524,
      "step": 474
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.1323471963405609,
      "learning_rate": 0.00012449799196787148,
      "loss": 0.5183,
      "step": 475
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.1223708763718605,
      "learning_rate": 0.00012433734939759037,
      "loss": 0.4737,
      "step": 476
    },
    {
      "epoch": 0.3816,
      "grad_norm": 0.12727081775665283,
      "learning_rate": 0.00012417670682730923,
      "loss": 0.4961,
      "step": 477
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.11011364310979843,
      "learning_rate": 0.00012401606425702812,
      "loss": 0.4794,
      "step": 478
    },
    {
      "epoch": 0.3832,
      "grad_norm": 0.14199545979499817,
      "learning_rate": 0.000123855421686747,
      "loss": 0.4863,
      "step": 479
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.10818229615688324,
      "learning_rate": 0.00012369477911646588,
      "loss": 0.4751,
      "step": 480
    },
    {
      "epoch": 0.3848,
      "grad_norm": 0.09786406904459,
      "learning_rate": 0.00012353413654618475,
      "loss": 0.5546,
      "step": 481
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.09223214536905289,
      "learning_rate": 0.00012337349397590364,
      "loss": 0.4884,
      "step": 482
    },
    {
      "epoch": 0.3864,
      "grad_norm": 0.10466200113296509,
      "learning_rate": 0.00012321285140562248,
      "loss": 0.4885,
      "step": 483
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.1248093768954277,
      "learning_rate": 0.00012305220883534137,
      "loss": 0.466,
      "step": 484
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.12785027921199799,
      "learning_rate": 0.00012289156626506023,
      "loss": 0.4457,
      "step": 485
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.09236650913953781,
      "learning_rate": 0.00012273092369477912,
      "loss": 0.4388,
      "step": 486
    },
    {
      "epoch": 0.3896,
      "grad_norm": 0.11415068060159683,
      "learning_rate": 0.000122570281124498,
      "loss": 0.4604,
      "step": 487
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.11209790408611298,
      "learning_rate": 0.00012240963855421688,
      "loss": 0.4847,
      "step": 488
    },
    {
      "epoch": 0.3912,
      "grad_norm": 0.13767872750759125,
      "learning_rate": 0.00012224899598393575,
      "loss": 0.5743,
      "step": 489
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.0997677594423294,
      "learning_rate": 0.00012208835341365464,
      "loss": 0.4457,
      "step": 490
    },
    {
      "epoch": 0.3928,
      "grad_norm": 0.14399674534797668,
      "learning_rate": 0.00012192771084337352,
      "loss": 0.46,
      "step": 491
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.10053082555532455,
      "learning_rate": 0.00012176706827309237,
      "loss": 0.4925,
      "step": 492
    },
    {
      "epoch": 0.3944,
      "grad_norm": 0.1210375502705574,
      "learning_rate": 0.00012160642570281125,
      "loss": 0.5186,
      "step": 493
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.10348213464021683,
      "learning_rate": 0.00012144578313253012,
      "loss": 0.488,
      "step": 494
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.1435229331254959,
      "learning_rate": 0.000121285140562249,
      "loss": 0.5632,
      "step": 495
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.10657967627048492,
      "learning_rate": 0.00012112449799196788,
      "loss": 0.4915,
      "step": 496
    },
    {
      "epoch": 0.3976,
      "grad_norm": 0.10092423111200333,
      "learning_rate": 0.00012096385542168676,
      "loss": 0.5263,
      "step": 497
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.10261784493923187,
      "learning_rate": 0.00012080321285140564,
      "loss": 0.528,
      "step": 498
    },
    {
      "epoch": 0.3992,
      "grad_norm": 0.1111878827214241,
      "learning_rate": 0.00012064257028112452,
      "loss": 0.4513,
      "step": 499
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.11406494677066803,
      "learning_rate": 0.0001204819277108434,
      "loss": 0.4704,
      "step": 500
    },
    {
      "epoch": 0.4008,
      "grad_norm": 0.11011487245559692,
      "learning_rate": 0.00012032128514056225,
      "loss": 0.5114,
      "step": 501
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.10097750276327133,
      "learning_rate": 0.00012016064257028112,
      "loss": 0.5421,
      "step": 502
    },
    {
      "epoch": 0.4024,
      "grad_norm": 0.11563891172409058,
      "learning_rate": 0.00012,
      "loss": 0.501,
      "step": 503
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.09797266125679016,
      "learning_rate": 0.00011983935742971888,
      "loss": 0.4411,
      "step": 504
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.10539009422063828,
      "learning_rate": 0.00011967871485943776,
      "loss": 0.474,
      "step": 505
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.09576266258955002,
      "learning_rate": 0.00011951807228915664,
      "loss": 0.448,
      "step": 506
    },
    {
      "epoch": 0.4056,
      "grad_norm": 0.12518328428268433,
      "learning_rate": 0.00011935742971887552,
      "loss": 0.5556,
      "step": 507
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.10798518359661102,
      "learning_rate": 0.0001191967871485944,
      "loss": 0.5163,
      "step": 508
    },
    {
      "epoch": 0.4072,
      "grad_norm": 0.1290460228919983,
      "learning_rate": 0.00011903614457831327,
      "loss": 0.4078,
      "step": 509
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.11583729088306427,
      "learning_rate": 0.00011887550200803212,
      "loss": 0.4979,
      "step": 510
    },
    {
      "epoch": 0.4088,
      "grad_norm": 0.1153661385178566,
      "learning_rate": 0.000118714859437751,
      "loss": 0.4859,
      "step": 511
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.1157924085855484,
      "learning_rate": 0.00011855421686746988,
      "loss": 0.5394,
      "step": 512
    },
    {
      "epoch": 0.4104,
      "grad_norm": 0.1499260663986206,
      "learning_rate": 0.00011839357429718876,
      "loss": 0.4676,
      "step": 513
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.14711536467075348,
      "learning_rate": 0.00011823293172690764,
      "loss": 0.4974,
      "step": 514
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.1077151969075203,
      "learning_rate": 0.00011807228915662652,
      "loss": 0.4932,
      "step": 515
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.10291726887226105,
      "learning_rate": 0.0001179116465863454,
      "loss": 0.4585,
      "step": 516
    },
    {
      "epoch": 0.4136,
      "grad_norm": 0.1293715387582779,
      "learning_rate": 0.00011775100401606427,
      "loss": 0.4764,
      "step": 517
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.10051656514406204,
      "learning_rate": 0.00011759036144578315,
      "loss": 0.4745,
      "step": 518
    },
    {
      "epoch": 0.4152,
      "grad_norm": 0.13503403961658478,
      "learning_rate": 0.000117429718875502,
      "loss": 0.5136,
      "step": 519
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.10756786912679672,
      "learning_rate": 0.00011726907630522088,
      "loss": 0.4185,
      "step": 520
    },
    {
      "epoch": 0.4168,
      "grad_norm": 0.12417811155319214,
      "learning_rate": 0.00011710843373493976,
      "loss": 0.4532,
      "step": 521
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.12671832740306854,
      "learning_rate": 0.00011694779116465864,
      "loss": 0.471,
      "step": 522
    },
    {
      "epoch": 0.4184,
      "grad_norm": 0.126491978764534,
      "learning_rate": 0.00011678714859437752,
      "loss": 0.4934,
      "step": 523
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.12738503515720367,
      "learning_rate": 0.0001166265060240964,
      "loss": 0.4835,
      "step": 524
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.11313311755657196,
      "learning_rate": 0.00011646586345381527,
      "loss": 0.5644,
      "step": 525
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.1087779700756073,
      "learning_rate": 0.00011630522088353415,
      "loss": 0.4386,
      "step": 526
    },
    {
      "epoch": 0.4216,
      "grad_norm": 0.11648137122392654,
      "learning_rate": 0.00011614457831325303,
      "loss": 0.4935,
      "step": 527
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.11132007092237473,
      "learning_rate": 0.00011598393574297188,
      "loss": 0.45,
      "step": 528
    },
    {
      "epoch": 0.4232,
      "grad_norm": 0.09464289993047714,
      "learning_rate": 0.00011582329317269076,
      "loss": 0.4354,
      "step": 529
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.13674724102020264,
      "learning_rate": 0.00011566265060240964,
      "loss": 0.5363,
      "step": 530
    },
    {
      "epoch": 0.4248,
      "grad_norm": 0.07912842184305191,
      "learning_rate": 0.00011550200803212852,
      "loss": 0.4278,
      "step": 531
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.10669177770614624,
      "learning_rate": 0.0001153413654618474,
      "loss": 0.4744,
      "step": 532
    },
    {
      "epoch": 0.4264,
      "grad_norm": 0.10144595056772232,
      "learning_rate": 0.00011518072289156627,
      "loss": 0.5317,
      "step": 533
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.09844997525215149,
      "learning_rate": 0.00011502008032128515,
      "loss": 0.4541,
      "step": 534
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.12794671952724457,
      "learning_rate": 0.00011485943775100403,
      "loss": 0.5057,
      "step": 535
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.10328208655118942,
      "learning_rate": 0.00011469879518072291,
      "loss": 0.511,
      "step": 536
    },
    {
      "epoch": 0.4296,
      "grad_norm": 0.1145544946193695,
      "learning_rate": 0.00011453815261044176,
      "loss": 0.434,
      "step": 537
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.12157297879457474,
      "learning_rate": 0.00011437751004016064,
      "loss": 0.4569,
      "step": 538
    },
    {
      "epoch": 0.4312,
      "grad_norm": 0.1297057420015335,
      "learning_rate": 0.00011421686746987952,
      "loss": 0.5679,
      "step": 539
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.11132798343896866,
      "learning_rate": 0.0001140562248995984,
      "loss": 0.4632,
      "step": 540
    },
    {
      "epoch": 0.4328,
      "grad_norm": 0.11335862427949905,
      "learning_rate": 0.00011389558232931727,
      "loss": 0.4687,
      "step": 541
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.13023987412452698,
      "learning_rate": 0.00011373493975903615,
      "loss": 0.4695,
      "step": 542
    },
    {
      "epoch": 0.4344,
      "grad_norm": 0.1585882157087326,
      "learning_rate": 0.00011357429718875503,
      "loss": 0.5532,
      "step": 543
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.11351323872804642,
      "learning_rate": 0.00011341365461847391,
      "loss": 0.5252,
      "step": 544
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.1351500153541565,
      "learning_rate": 0.00011325301204819279,
      "loss": 0.4847,
      "step": 545
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.16049909591674805,
      "learning_rate": 0.00011309236947791164,
      "loss": 0.4689,
      "step": 546
    },
    {
      "epoch": 0.4376,
      "grad_norm": 0.12183976918458939,
      "learning_rate": 0.00011293172690763052,
      "loss": 0.496,
      "step": 547
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.13993699848651886,
      "learning_rate": 0.0001127710843373494,
      "loss": 0.5019,
      "step": 548
    },
    {
      "epoch": 0.4392,
      "grad_norm": 0.11073020845651627,
      "learning_rate": 0.00011261044176706827,
      "loss": 0.4594,
      "step": 549
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.11255818605422974,
      "learning_rate": 0.00011244979919678715,
      "loss": 0.4958,
      "step": 550
    },
    {
      "epoch": 0.4408,
      "grad_norm": 0.11649464070796967,
      "learning_rate": 0.00011228915662650603,
      "loss": 0.4518,
      "step": 551
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.09748630225658417,
      "learning_rate": 0.00011212851405622491,
      "loss": 0.5106,
      "step": 552
    },
    {
      "epoch": 0.4424,
      "grad_norm": 0.09902337938547134,
      "learning_rate": 0.00011196787148594379,
      "loss": 0.4326,
      "step": 553
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.1453341841697693,
      "learning_rate": 0.00011180722891566267,
      "loss": 0.5416,
      "step": 554
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.10158568620681763,
      "learning_rate": 0.00011164658634538152,
      "loss": 0.4493,
      "step": 555
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.09894977509975433,
      "learning_rate": 0.0001114859437751004,
      "loss": 0.4865,
      "step": 556
    },
    {
      "epoch": 0.4456,
      "grad_norm": 0.11554057151079178,
      "learning_rate": 0.00011132530120481927,
      "loss": 0.4731,
      "step": 557
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.12802566587924957,
      "learning_rate": 0.00011116465863453815,
      "loss": 0.5379,
      "step": 558
    },
    {
      "epoch": 0.4472,
      "grad_norm": 0.09636945277452469,
      "learning_rate": 0.00011100401606425703,
      "loss": 0.4339,
      "step": 559
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.127554789185524,
      "learning_rate": 0.00011084337349397591,
      "loss": 0.5124,
      "step": 560
    },
    {
      "epoch": 0.4488,
      "grad_norm": 0.10307540744543076,
      "learning_rate": 0.00011068273092369479,
      "loss": 0.4628,
      "step": 561
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.120032399892807,
      "learning_rate": 0.00011052208835341367,
      "loss": 0.5327,
      "step": 562
    },
    {
      "epoch": 0.4504,
      "grad_norm": 0.09606458991765976,
      "learning_rate": 0.00011036144578313254,
      "loss": 0.4472,
      "step": 563
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.10925350338220596,
      "learning_rate": 0.0001102008032128514,
      "loss": 0.5382,
      "step": 564
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.08590927720069885,
      "learning_rate": 0.00011004016064257027,
      "loss": 0.473,
      "step": 565
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.12420124560594559,
      "learning_rate": 0.00010987951807228915,
      "loss": 0.5027,
      "step": 566
    },
    {
      "epoch": 0.4536,
      "grad_norm": 0.1217193529009819,
      "learning_rate": 0.00010971887550200803,
      "loss": 0.4349,
      "step": 567
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.09225582331418991,
      "learning_rate": 0.00010955823293172691,
      "loss": 0.4146,
      "step": 568
    },
    {
      "epoch": 0.4552,
      "grad_norm": 0.13973914086818695,
      "learning_rate": 0.00010939759036144579,
      "loss": 0.5531,
      "step": 569
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.10750909149646759,
      "learning_rate": 0.00010923694779116467,
      "loss": 0.5071,
      "step": 570
    },
    {
      "epoch": 0.4568,
      "grad_norm": 0.14856787025928497,
      "learning_rate": 0.00010907630522088354,
      "loss": 0.493,
      "step": 571
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.13934928178787231,
      "learning_rate": 0.00010891566265060242,
      "loss": 0.49,
      "step": 572
    },
    {
      "epoch": 0.4584,
      "grad_norm": 0.11972140520811081,
      "learning_rate": 0.00010875502008032127,
      "loss": 0.5327,
      "step": 573
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.11666855216026306,
      "learning_rate": 0.00010859437751004015,
      "loss": 0.5254,
      "step": 574
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.10387075692415237,
      "learning_rate": 0.00010843373493975903,
      "loss": 0.4617,
      "step": 575
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.08950962871313095,
      "learning_rate": 0.00010827309236947791,
      "loss": 0.4801,
      "step": 576
    },
    {
      "epoch": 0.4616,
      "grad_norm": 0.13078680634498596,
      "learning_rate": 0.00010811244979919679,
      "loss": 0.431,
      "step": 577
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.09815850853919983,
      "learning_rate": 0.00010795180722891567,
      "loss": 0.428,
      "step": 578
    },
    {
      "epoch": 0.4632,
      "grad_norm": 0.09953360259532928,
      "learning_rate": 0.00010779116465863454,
      "loss": 0.513,
      "step": 579
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.08958287537097931,
      "learning_rate": 0.00010763052208835342,
      "loss": 0.4436,
      "step": 580
    },
    {
      "epoch": 0.4648,
      "grad_norm": 0.10623672604560852,
      "learning_rate": 0.0001074698795180723,
      "loss": 0.4816,
      "step": 581
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.11265277117490768,
      "learning_rate": 0.00010730923694779118,
      "loss": 0.4605,
      "step": 582
    },
    {
      "epoch": 0.4664,
      "grad_norm": 0.13286207616329193,
      "learning_rate": 0.00010714859437751003,
      "loss": 0.5807,
      "step": 583
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.12580923736095428,
      "learning_rate": 0.00010698795180722891,
      "loss": 0.5041,
      "step": 584
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.11488791555166245,
      "learning_rate": 0.00010682730923694779,
      "loss": 0.5004,
      "step": 585
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.1339333951473236,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.5241,
      "step": 586
    },
    {
      "epoch": 0.4696,
      "grad_norm": 0.13118599355220795,
      "learning_rate": 0.00010650602409638554,
      "loss": 0.5042,
      "step": 587
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.11723529547452927,
      "learning_rate": 0.00010634538152610442,
      "loss": 0.5236,
      "step": 588
    },
    {
      "epoch": 0.4712,
      "grad_norm": 0.0988781601190567,
      "learning_rate": 0.0001061847389558233,
      "loss": 0.5289,
      "step": 589
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.11618170142173767,
      "learning_rate": 0.00010602409638554218,
      "loss": 0.5335,
      "step": 590
    },
    {
      "epoch": 0.4728,
      "grad_norm": 0.10818197578191757,
      "learning_rate": 0.00010586345381526106,
      "loss": 0.4109,
      "step": 591
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.1468420922756195,
      "learning_rate": 0.00010570281124497991,
      "loss": 0.4578,
      "step": 592
    },
    {
      "epoch": 0.4744,
      "grad_norm": 0.10900918394327164,
      "learning_rate": 0.00010554216867469879,
      "loss": 0.528,
      "step": 593
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.13529784977436066,
      "learning_rate": 0.00010538152610441767,
      "loss": 0.5181,
      "step": 594
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.10326806455850601,
      "learning_rate": 0.00010522088353413654,
      "loss": 0.5302,
      "step": 595
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.09394128620624542,
      "learning_rate": 0.00010506024096385542,
      "loss": 0.4806,
      "step": 596
    },
    {
      "epoch": 0.4776,
      "grad_norm": 0.10005470365285873,
      "learning_rate": 0.0001048995983935743,
      "loss": 0.4506,
      "step": 597
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.12783074378967285,
      "learning_rate": 0.00010473895582329318,
      "loss": 0.5402,
      "step": 598
    },
    {
      "epoch": 0.4792,
      "grad_norm": 0.10841295123100281,
      "learning_rate": 0.00010457831325301206,
      "loss": 0.5207,
      "step": 599
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.11702478677034378,
      "learning_rate": 0.00010441767068273094,
      "loss": 0.4746,
      "step": 600
    },
    {
      "epoch": 0.4808,
      "grad_norm": 0.13486157357692719,
      "learning_rate": 0.0001042570281124498,
      "loss": 0.4862,
      "step": 601
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.11715016514062881,
      "learning_rate": 0.00010409638554216867,
      "loss": 0.5151,
      "step": 602
    },
    {
      "epoch": 0.4824,
      "grad_norm": 0.13198722898960114,
      "learning_rate": 0.00010393574297188754,
      "loss": 0.4738,
      "step": 603
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.10002795606851578,
      "learning_rate": 0.00010377510040160642,
      "loss": 0.4247,
      "step": 604
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.1056402176618576,
      "learning_rate": 0.0001036144578313253,
      "loss": 0.4416,
      "step": 605
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.10658781230449677,
      "learning_rate": 0.00010345381526104418,
      "loss": 0.4954,
      "step": 606
    },
    {
      "epoch": 0.4856,
      "grad_norm": 0.11027458310127258,
      "learning_rate": 0.00010329317269076306,
      "loss": 0.4859,
      "step": 607
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.1453198492527008,
      "learning_rate": 0.00010313253012048194,
      "loss": 0.4701,
      "step": 608
    },
    {
      "epoch": 0.4872,
      "grad_norm": 0.12252595275640488,
      "learning_rate": 0.00010297188755020082,
      "loss": 0.5411,
      "step": 609
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.17298553884029388,
      "learning_rate": 0.00010281124497991968,
      "loss": 0.4962,
      "step": 610
    },
    {
      "epoch": 0.4888,
      "grad_norm": 0.141980841755867,
      "learning_rate": 0.00010265060240963856,
      "loss": 0.4931,
      "step": 611
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.12394455075263977,
      "learning_rate": 0.00010248995983935742,
      "loss": 0.4359,
      "step": 612
    },
    {
      "epoch": 0.4904,
      "grad_norm": 0.11284812539815903,
      "learning_rate": 0.0001023293172690763,
      "loss": 0.4328,
      "step": 613
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.11615529656410217,
      "learning_rate": 0.00010216867469879518,
      "loss": 0.4768,
      "step": 614
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.11079400032758713,
      "learning_rate": 0.00010200803212851406,
      "loss": 0.4908,
      "step": 615
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.11279366910457611,
      "learning_rate": 0.00010184738955823294,
      "loss": 0.5022,
      "step": 616
    },
    {
      "epoch": 0.4936,
      "grad_norm": 0.12448957562446594,
      "learning_rate": 0.00010168674698795182,
      "loss": 0.4865,
      "step": 617
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.11074083298444748,
      "learning_rate": 0.0001015261044176707,
      "loss": 0.5847,
      "step": 618
    },
    {
      "epoch": 0.4952,
      "grad_norm": 0.1444678157567978,
      "learning_rate": 0.00010136546184738956,
      "loss": 0.5409,
      "step": 619
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.1004948690533638,
      "learning_rate": 0.00010120481927710844,
      "loss": 0.4232,
      "step": 620
    },
    {
      "epoch": 0.4968,
      "grad_norm": 0.17651353776454926,
      "learning_rate": 0.00010104417670682732,
      "loss": 0.5912,
      "step": 621
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.1575055718421936,
      "learning_rate": 0.0001008835341365462,
      "loss": 0.4188,
      "step": 622
    },
    {
      "epoch": 0.4984,
      "grad_norm": 0.10749685019254684,
      "learning_rate": 0.00010072289156626506,
      "loss": 0.4624,
      "step": 623
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.14234472811222076,
      "learning_rate": 0.00010056224899598394,
      "loss": 0.4754,
      "step": 624
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.11453142017126083,
      "learning_rate": 0.00010040160642570282,
      "loss": 0.425,
      "step": 625
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.10792006552219391,
      "learning_rate": 0.0001002409638554217,
      "loss": 0.4085,
      "step": 626
    },
    {
      "epoch": 0.5016,
      "grad_norm": 0.1210283637046814,
      "learning_rate": 0.00010008032128514057,
      "loss": 0.4771,
      "step": 627
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.13747470080852509,
      "learning_rate": 9.991967871485944e-05,
      "loss": 0.4869,
      "step": 628
    },
    {
      "epoch": 0.5032,
      "grad_norm": 0.09641784429550171,
      "learning_rate": 9.975903614457832e-05,
      "loss": 0.4799,
      "step": 629
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.1474318504333496,
      "learning_rate": 9.95983935742972e-05,
      "loss": 0.4893,
      "step": 630
    },
    {
      "epoch": 0.5048,
      "grad_norm": 0.1072789803147316,
      "learning_rate": 9.943775100401607e-05,
      "loss": 0.4861,
      "step": 631
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.1307854801416397,
      "learning_rate": 9.927710843373495e-05,
      "loss": 0.4675,
      "step": 632
    },
    {
      "epoch": 0.5064,
      "grad_norm": 0.1229768842458725,
      "learning_rate": 9.911646586345382e-05,
      "loss": 0.515,
      "step": 633
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.1334923356771469,
      "learning_rate": 9.89558232931727e-05,
      "loss": 0.535,
      "step": 634
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.10544546693563461,
      "learning_rate": 9.879518072289157e-05,
      "loss": 0.4337,
      "step": 635
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.15251193940639496,
      "learning_rate": 9.863453815261045e-05,
      "loss": 0.5248,
      "step": 636
    },
    {
      "epoch": 0.5096,
      "grad_norm": 0.12091584503650665,
      "learning_rate": 9.847389558232933e-05,
      "loss": 0.521,
      "step": 637
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.08757118135690689,
      "learning_rate": 9.831325301204821e-05,
      "loss": 0.4521,
      "step": 638
    },
    {
      "epoch": 0.5112,
      "grad_norm": 0.12412483245134354,
      "learning_rate": 9.815261044176707e-05,
      "loss": 0.4872,
      "step": 639
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.13149574398994446,
      "learning_rate": 9.799196787148595e-05,
      "loss": 0.4832,
      "step": 640
    },
    {
      "epoch": 0.5128,
      "grad_norm": 0.16535460948944092,
      "learning_rate": 9.783132530120483e-05,
      "loss": 0.5628,
      "step": 641
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.10083447396755219,
      "learning_rate": 9.767068273092371e-05,
      "loss": 0.4656,
      "step": 642
    },
    {
      "epoch": 0.5144,
      "grad_norm": 0.14278964698314667,
      "learning_rate": 9.751004016064259e-05,
      "loss": 0.4647,
      "step": 643
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.15331189334392548,
      "learning_rate": 9.734939759036145e-05,
      "loss": 0.4677,
      "step": 644
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.11957186460494995,
      "learning_rate": 9.718875502008033e-05,
      "loss": 0.4427,
      "step": 645
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.12696391344070435,
      "learning_rate": 9.702811244979921e-05,
      "loss": 0.4922,
      "step": 646
    },
    {
      "epoch": 0.5176,
      "grad_norm": 0.15713082253932953,
      "learning_rate": 9.686746987951809e-05,
      "loss": 0.4627,
      "step": 647
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.10605651140213013,
      "learning_rate": 9.670682730923695e-05,
      "loss": 0.4317,
      "step": 648
    },
    {
      "epoch": 0.5192,
      "grad_norm": 0.12028956413269043,
      "learning_rate": 9.654618473895583e-05,
      "loss": 0.5252,
      "step": 649
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.17624717950820923,
      "learning_rate": 9.638554216867471e-05,
      "loss": 0.5253,
      "step": 650
    },
    {
      "epoch": 0.5208,
      "grad_norm": 0.09561330080032349,
      "learning_rate": 9.622489959839359e-05,
      "loss": 0.4497,
      "step": 651
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.12713831663131714,
      "learning_rate": 9.606425702811246e-05,
      "loss": 0.5638,
      "step": 652
    },
    {
      "epoch": 0.5224,
      "grad_norm": 0.09927498549222946,
      "learning_rate": 9.590361445783133e-05,
      "loss": 0.4669,
      "step": 653
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.13061630725860596,
      "learning_rate": 9.574297188755021e-05,
      "loss": 0.4023,
      "step": 654
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.12229994684457779,
      "learning_rate": 9.558232931726909e-05,
      "loss": 0.5136,
      "step": 655
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.11873620748519897,
      "learning_rate": 9.542168674698796e-05,
      "loss": 0.4886,
      "step": 656
    },
    {
      "epoch": 0.5256,
      "grad_norm": 0.10892878472805023,
      "learning_rate": 9.526104417670684e-05,
      "loss": 0.4817,
      "step": 657
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.09200224280357361,
      "learning_rate": 9.510040160642571e-05,
      "loss": 0.4631,
      "step": 658
    },
    {
      "epoch": 0.5272,
      "grad_norm": 0.09714290499687195,
      "learning_rate": 9.493975903614459e-05,
      "loss": 0.3991,
      "step": 659
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.11474809795618057,
      "learning_rate": 9.477911646586346e-05,
      "loss": 0.561,
      "step": 660
    },
    {
      "epoch": 0.5288,
      "grad_norm": 0.09483303874731064,
      "learning_rate": 9.461847389558234e-05,
      "loss": 0.4783,
      "step": 661
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.11113468557596207,
      "learning_rate": 9.445783132530121e-05,
      "loss": 0.4741,
      "step": 662
    },
    {
      "epoch": 0.5304,
      "grad_norm": 0.09185149520635605,
      "learning_rate": 9.429718875502009e-05,
      "loss": 0.4275,
      "step": 663
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.09824016690254211,
      "learning_rate": 9.413654618473896e-05,
      "loss": 0.42,
      "step": 664
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.09245823323726654,
      "learning_rate": 9.397590361445784e-05,
      "loss": 0.4643,
      "step": 665
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.10028959810733795,
      "learning_rate": 9.381526104417672e-05,
      "loss": 0.4552,
      "step": 666
    },
    {
      "epoch": 0.5336,
      "grad_norm": 0.11695408821105957,
      "learning_rate": 9.365461847389559e-05,
      "loss": 0.4751,
      "step": 667
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.08434562385082245,
      "learning_rate": 9.349397590361446e-05,
      "loss": 0.43,
      "step": 668
    },
    {
      "epoch": 0.5352,
      "grad_norm": 0.11076831072568893,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.4703,
      "step": 669
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.11617434024810791,
      "learning_rate": 9.317269076305222e-05,
      "loss": 0.4891,
      "step": 670
    },
    {
      "epoch": 0.5368,
      "grad_norm": 0.11642688512802124,
      "learning_rate": 9.301204819277109e-05,
      "loss": 0.5465,
      "step": 671
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.13223424553871155,
      "learning_rate": 9.285140562248996e-05,
      "loss": 0.4828,
      "step": 672
    },
    {
      "epoch": 0.5384,
      "grad_norm": 0.11436827480792999,
      "learning_rate": 9.269076305220884e-05,
      "loss": 0.5041,
      "step": 673
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.08859289437532425,
      "learning_rate": 9.253012048192772e-05,
      "loss": 0.4724,
      "step": 674
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.13408029079437256,
      "learning_rate": 9.23694779116466e-05,
      "loss": 0.4865,
      "step": 675
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.1217900961637497,
      "learning_rate": 9.220883534136546e-05,
      "loss": 0.4099,
      "step": 676
    },
    {
      "epoch": 0.5416,
      "grad_norm": 0.10587403178215027,
      "learning_rate": 9.204819277108434e-05,
      "loss": 0.4372,
      "step": 677
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.09475435316562653,
      "learning_rate": 9.188755020080322e-05,
      "loss": 0.4518,
      "step": 678
    },
    {
      "epoch": 0.5432,
      "grad_norm": 0.10612226277589798,
      "learning_rate": 9.17269076305221e-05,
      "loss": 0.4367,
      "step": 679
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.10664577037096024,
      "learning_rate": 9.156626506024096e-05,
      "loss": 0.4915,
      "step": 680
    },
    {
      "epoch": 0.5448,
      "grad_norm": 0.08743905276060104,
      "learning_rate": 9.140562248995984e-05,
      "loss": 0.433,
      "step": 681
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.11389567703008652,
      "learning_rate": 9.124497991967872e-05,
      "loss": 0.4758,
      "step": 682
    },
    {
      "epoch": 0.5464,
      "grad_norm": 0.0865057036280632,
      "learning_rate": 9.10843373493976e-05,
      "loss": 0.5108,
      "step": 683
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.11709139496088028,
      "learning_rate": 9.092369477911648e-05,
      "loss": 0.5625,
      "step": 684
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.11756383627653122,
      "learning_rate": 9.076305220883534e-05,
      "loss": 0.4988,
      "step": 685
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.10504677891731262,
      "learning_rate": 9.060240963855422e-05,
      "loss": 0.4537,
      "step": 686
    },
    {
      "epoch": 0.5496,
      "grad_norm": 0.10976672172546387,
      "learning_rate": 9.04417670682731e-05,
      "loss": 0.4827,
      "step": 687
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.11414600163698196,
      "learning_rate": 9.028112449799198e-05,
      "loss": 0.4411,
      "step": 688
    },
    {
      "epoch": 0.5512,
      "grad_norm": 0.09575418382883072,
      "learning_rate": 9.012048192771084e-05,
      "loss": 0.4634,
      "step": 689
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.09026230126619339,
      "learning_rate": 8.995983935742972e-05,
      "loss": 0.513,
      "step": 690
    },
    {
      "epoch": 0.5528,
      "grad_norm": 0.1108793243765831,
      "learning_rate": 8.97991967871486e-05,
      "loss": 0.5416,
      "step": 691
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.13615942001342773,
      "learning_rate": 8.963855421686748e-05,
      "loss": 0.502,
      "step": 692
    },
    {
      "epoch": 0.5544,
      "grad_norm": 0.11611419171094894,
      "learning_rate": 8.947791164658636e-05,
      "loss": 0.4906,
      "step": 693
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.08765018731355667,
      "learning_rate": 8.931726907630522e-05,
      "loss": 0.424,
      "step": 694
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.12774454057216644,
      "learning_rate": 8.91566265060241e-05,
      "loss": 0.5028,
      "step": 695
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.10596276819705963,
      "learning_rate": 8.899598393574298e-05,
      "loss": 0.4759,
      "step": 696
    },
    {
      "epoch": 0.5576,
      "grad_norm": 0.10991460084915161,
      "learning_rate": 8.883534136546186e-05,
      "loss": 0.4734,
      "step": 697
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.10176622122526169,
      "learning_rate": 8.867469879518072e-05,
      "loss": 0.4935,
      "step": 698
    },
    {
      "epoch": 0.5592,
      "grad_norm": 0.10128366202116013,
      "learning_rate": 8.85140562248996e-05,
      "loss": 0.448,
      "step": 699
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.10781760513782501,
      "learning_rate": 8.835341365461848e-05,
      "loss": 0.4224,
      "step": 700
    },
    {
      "epoch": 0.5608,
      "grad_norm": 0.14077670872211456,
      "learning_rate": 8.819277108433736e-05,
      "loss": 0.4885,
      "step": 701
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.12084035575389862,
      "learning_rate": 8.803212851405624e-05,
      "loss": 0.5255,
      "step": 702
    },
    {
      "epoch": 0.5624,
      "grad_norm": 0.10550293326377869,
      "learning_rate": 8.78714859437751e-05,
      "loss": 0.4692,
      "step": 703
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.09784412384033203,
      "learning_rate": 8.771084337349398e-05,
      "loss": 0.4304,
      "step": 704
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.10043226927518845,
      "learning_rate": 8.755020080321286e-05,
      "loss": 0.4991,
      "step": 705
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.10295800864696503,
      "learning_rate": 8.738955823293174e-05,
      "loss": 0.4665,
      "step": 706
    },
    {
      "epoch": 0.5656,
      "grad_norm": 0.08678501099348068,
      "learning_rate": 8.722891566265061e-05,
      "loss": 0.4473,
      "step": 707
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.0969623550772667,
      "learning_rate": 8.706827309236948e-05,
      "loss": 0.5099,
      "step": 708
    },
    {
      "epoch": 0.5672,
      "grad_norm": 0.1027851477265358,
      "learning_rate": 8.690763052208836e-05,
      "loss": 0.4295,
      "step": 709
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.11643826961517334,
      "learning_rate": 8.674698795180724e-05,
      "loss": 0.4977,
      "step": 710
    },
    {
      "epoch": 0.5688,
      "grad_norm": 0.0886913612484932,
      "learning_rate": 8.658634538152611e-05,
      "loss": 0.477,
      "step": 711
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.11668624728918076,
      "learning_rate": 8.642570281124498e-05,
      "loss": 0.516,
      "step": 712
    },
    {
      "epoch": 0.5704,
      "grad_norm": 0.10582887381315231,
      "learning_rate": 8.626506024096386e-05,
      "loss": 0.4695,
      "step": 713
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.19665826857089996,
      "learning_rate": 8.610441767068274e-05,
      "loss": 0.5037,
      "step": 714
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.12855678796768188,
      "learning_rate": 8.594377510040161e-05,
      "loss": 0.4928,
      "step": 715
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.11778805404901505,
      "learning_rate": 8.578313253012049e-05,
      "loss": 0.4609,
      "step": 716
    },
    {
      "epoch": 0.5736,
      "grad_norm": 0.14105452597141266,
      "learning_rate": 8.562248995983936e-05,
      "loss": 0.4726,
      "step": 717
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.11787346005439758,
      "learning_rate": 8.546184738955824e-05,
      "loss": 0.5091,
      "step": 718
    },
    {
      "epoch": 0.5752,
      "grad_norm": 0.10624391585588455,
      "learning_rate": 8.530120481927711e-05,
      "loss": 0.4649,
      "step": 719
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.10870066285133362,
      "learning_rate": 8.514056224899599e-05,
      "loss": 0.4239,
      "step": 720
    },
    {
      "epoch": 0.5768,
      "grad_norm": 0.14816510677337646,
      "learning_rate": 8.497991967871486e-05,
      "loss": 0.5031,
      "step": 721
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.09728754311800003,
      "learning_rate": 8.481927710843374e-05,
      "loss": 0.4883,
      "step": 722
    },
    {
      "epoch": 0.5784,
      "grad_norm": 0.09567518532276154,
      "learning_rate": 8.465863453815261e-05,
      "loss": 0.4979,
      "step": 723
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.09471527487039566,
      "learning_rate": 8.449799196787149e-05,
      "loss": 0.3886,
      "step": 724
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.16048985719680786,
      "learning_rate": 8.433734939759037e-05,
      "loss": 0.4844,
      "step": 725
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.10939106345176697,
      "learning_rate": 8.417670682730924e-05,
      "loss": 0.4392,
      "step": 726
    },
    {
      "epoch": 0.5816,
      "grad_norm": 0.12242487072944641,
      "learning_rate": 8.401606425702811e-05,
      "loss": 0.542,
      "step": 727
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.11337235569953918,
      "learning_rate": 8.385542168674699e-05,
      "loss": 0.4568,
      "step": 728
    },
    {
      "epoch": 0.5832,
      "grad_norm": 0.125407412648201,
      "learning_rate": 8.369477911646587e-05,
      "loss": 0.4978,
      "step": 729
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.11396627128124237,
      "learning_rate": 8.353413654618474e-05,
      "loss": 0.4647,
      "step": 730
    },
    {
      "epoch": 0.5848,
      "grad_norm": 0.1160179153084755,
      "learning_rate": 8.337349397590361e-05,
      "loss": 0.5046,
      "step": 731
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.10416790097951889,
      "learning_rate": 8.321285140562249e-05,
      "loss": 0.3877,
      "step": 732
    },
    {
      "epoch": 0.5864,
      "grad_norm": 0.13720743358135223,
      "learning_rate": 8.305220883534137e-05,
      "loss": 0.5122,
      "step": 733
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.09713394939899445,
      "learning_rate": 8.289156626506025e-05,
      "loss": 0.5076,
      "step": 734
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.09586168825626373,
      "learning_rate": 8.273092369477911e-05,
      "loss": 0.4516,
      "step": 735
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.09988351911306381,
      "learning_rate": 8.257028112449799e-05,
      "loss": 0.4392,
      "step": 736
    },
    {
      "epoch": 0.5896,
      "grad_norm": 0.0990513488650322,
      "learning_rate": 8.240963855421687e-05,
      "loss": 0.4548,
      "step": 737
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.1143789291381836,
      "learning_rate": 8.224899598393575e-05,
      "loss": 0.4605,
      "step": 738
    },
    {
      "epoch": 0.5912,
      "grad_norm": 0.08981631696224213,
      "learning_rate": 8.208835341365461e-05,
      "loss": 0.4122,
      "step": 739
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.12119008600711823,
      "learning_rate": 8.192771084337349e-05,
      "loss": 0.4539,
      "step": 740
    },
    {
      "epoch": 0.5928,
      "grad_norm": 0.1094885990023613,
      "learning_rate": 8.176706827309237e-05,
      "loss": 0.4501,
      "step": 741
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.09413617849349976,
      "learning_rate": 8.160642570281125e-05,
      "loss": 0.4387,
      "step": 742
    },
    {
      "epoch": 0.5944,
      "grad_norm": 0.09447289258241653,
      "learning_rate": 8.144578313253013e-05,
      "loss": 0.4334,
      "step": 743
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.10656677931547165,
      "learning_rate": 8.128514056224899e-05,
      "loss": 0.4834,
      "step": 744
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.08056169748306274,
      "learning_rate": 8.112449799196787e-05,
      "loss": 0.436,
      "step": 745
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.08524682372808456,
      "learning_rate": 8.096385542168675e-05,
      "loss": 0.4357,
      "step": 746
    },
    {
      "epoch": 0.5976,
      "grad_norm": 0.15631386637687683,
      "learning_rate": 8.080321285140563e-05,
      "loss": 0.4505,
      "step": 747
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.08940406888723373,
      "learning_rate": 8.064257028112449e-05,
      "loss": 0.432,
      "step": 748
    },
    {
      "epoch": 0.5992,
      "grad_norm": 0.10068648308515549,
      "learning_rate": 8.048192771084337e-05,
      "loss": 0.5014,
      "step": 749
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.12162662297487259,
      "learning_rate": 8.032128514056225e-05,
      "loss": 0.4692,
      "step": 750
    },
    {
      "epoch": 0.6008,
      "grad_norm": 0.10513470321893692,
      "learning_rate": 8.016064257028113e-05,
      "loss": 0.4542,
      "step": 751
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.12313330173492432,
      "learning_rate": 8e-05,
      "loss": 0.5415,
      "step": 752
    },
    {
      "epoch": 0.6024,
      "grad_norm": 0.137748122215271,
      "learning_rate": 7.983935742971887e-05,
      "loss": 0.5641,
      "step": 753
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.09210006147623062,
      "learning_rate": 7.967871485943775e-05,
      "loss": 0.4195,
      "step": 754
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.10420618206262589,
      "learning_rate": 7.951807228915663e-05,
      "loss": 0.4578,
      "step": 755
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.09854063391685486,
      "learning_rate": 7.93574297188755e-05,
      "loss": 0.4983,
      "step": 756
    },
    {
      "epoch": 0.6056,
      "grad_norm": 0.09272221475839615,
      "learning_rate": 7.919678714859437e-05,
      "loss": 0.5013,
      "step": 757
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.125108540058136,
      "learning_rate": 7.903614457831325e-05,
      "loss": 0.4997,
      "step": 758
    },
    {
      "epoch": 0.6072,
      "grad_norm": 0.1037030890583992,
      "learning_rate": 7.887550200803213e-05,
      "loss": 0.4541,
      "step": 759
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.08894412219524384,
      "learning_rate": 7.8714859437751e-05,
      "loss": 0.4681,
      "step": 760
    },
    {
      "epoch": 0.6088,
      "grad_norm": 0.10299740731716156,
      "learning_rate": 7.855421686746989e-05,
      "loss": 0.513,
      "step": 761
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.11112968623638153,
      "learning_rate": 7.839357429718875e-05,
      "loss": 0.4844,
      "step": 762
    },
    {
      "epoch": 0.6104,
      "grad_norm": 0.10640543699264526,
      "learning_rate": 7.823293172690763e-05,
      "loss": 0.4541,
      "step": 763
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.09433278441429138,
      "learning_rate": 7.80722891566265e-05,
      "loss": 0.4276,
      "step": 764
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.1216011792421341,
      "learning_rate": 7.791164658634539e-05,
      "loss": 0.4522,
      "step": 765
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.10414974391460419,
      "learning_rate": 7.775100401606426e-05,
      "loss": 0.4953,
      "step": 766
    },
    {
      "epoch": 0.6136,
      "grad_norm": 0.11811921000480652,
      "learning_rate": 7.759036144578313e-05,
      "loss": 0.449,
      "step": 767
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.10081913322210312,
      "learning_rate": 7.7429718875502e-05,
      "loss": 0.5085,
      "step": 768
    },
    {
      "epoch": 0.6152,
      "grad_norm": 0.09761203080415726,
      "learning_rate": 7.726907630522089e-05,
      "loss": 0.473,
      "step": 769
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.13090312480926514,
      "learning_rate": 7.710843373493976e-05,
      "loss": 0.456,
      "step": 770
    },
    {
      "epoch": 0.6168,
      "grad_norm": 0.11335372924804688,
      "learning_rate": 7.694779116465863e-05,
      "loss": 0.4469,
      "step": 771
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.12202712148427963,
      "learning_rate": 7.678714859437751e-05,
      "loss": 0.5291,
      "step": 772
    },
    {
      "epoch": 0.6184,
      "grad_norm": 0.10388690233230591,
      "learning_rate": 7.662650602409639e-05,
      "loss": 0.4596,
      "step": 773
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.11448520421981812,
      "learning_rate": 7.646586345381526e-05,
      "loss": 0.4838,
      "step": 774
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.13050435483455658,
      "learning_rate": 7.630522088353414e-05,
      "loss": 0.5029,
      "step": 775
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.1025034636259079,
      "learning_rate": 7.614457831325301e-05,
      "loss": 0.4875,
      "step": 776
    },
    {
      "epoch": 0.6216,
      "grad_norm": 0.09090053290128708,
      "learning_rate": 7.598393574297189e-05,
      "loss": 0.4887,
      "step": 777
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.11909954249858856,
      "learning_rate": 7.582329317269076e-05,
      "loss": 0.458,
      "step": 778
    },
    {
      "epoch": 0.6232,
      "grad_norm": 0.10634135454893112,
      "learning_rate": 7.566265060240964e-05,
      "loss": 0.4782,
      "step": 779
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.10878236591815948,
      "learning_rate": 7.550200803212851e-05,
      "loss": 0.4859,
      "step": 780
    },
    {
      "epoch": 0.6248,
      "grad_norm": 0.10964029282331467,
      "learning_rate": 7.534136546184739e-05,
      "loss": 0.5246,
      "step": 781
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.11407871544361115,
      "learning_rate": 7.518072289156626e-05,
      "loss": 0.4709,
      "step": 782
    },
    {
      "epoch": 0.6264,
      "grad_norm": 0.11166344583034515,
      "learning_rate": 7.502008032128514e-05,
      "loss": 0.4753,
      "step": 783
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.10272267460823059,
      "learning_rate": 7.485943775100402e-05,
      "loss": 0.5298,
      "step": 784
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.08860224485397339,
      "learning_rate": 7.469879518072289e-05,
      "loss": 0.465,
      "step": 785
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.10783953219652176,
      "learning_rate": 7.453815261044176e-05,
      "loss": 0.518,
      "step": 786
    },
    {
      "epoch": 0.6296,
      "grad_norm": 0.11747146397829056,
      "learning_rate": 7.437751004016064e-05,
      "loss": 0.5224,
      "step": 787
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.11505219340324402,
      "learning_rate": 7.421686746987952e-05,
      "loss": 0.5045,
      "step": 788
    },
    {
      "epoch": 0.6312,
      "grad_norm": 0.09178605675697327,
      "learning_rate": 7.405622489959839e-05,
      "loss": 0.4777,
      "step": 789
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.09891543537378311,
      "learning_rate": 7.389558232931726e-05,
      "loss": 0.4881,
      "step": 790
    },
    {
      "epoch": 0.6328,
      "grad_norm": 0.09367542713880539,
      "learning_rate": 7.373493975903614e-05,
      "loss": 0.4201,
      "step": 791
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.13216857612133026,
      "learning_rate": 7.357429718875502e-05,
      "loss": 0.5147,
      "step": 792
    },
    {
      "epoch": 0.6344,
      "grad_norm": 0.12253890186548233,
      "learning_rate": 7.34136546184739e-05,
      "loss": 0.4554,
      "step": 793
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.1431037038564682,
      "learning_rate": 7.325301204819278e-05,
      "loss": 0.5504,
      "step": 794
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.11089546978473663,
      "learning_rate": 7.309236947791164e-05,
      "loss": 0.4262,
      "step": 795
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.14653900265693665,
      "learning_rate": 7.293172690763052e-05,
      "loss": 0.4915,
      "step": 796
    },
    {
      "epoch": 0.6376,
      "grad_norm": 0.11104849725961685,
      "learning_rate": 7.27710843373494e-05,
      "loss": 0.4977,
      "step": 797
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.12241095304489136,
      "learning_rate": 7.261044176706828e-05,
      "loss": 0.5416,
      "step": 798
    },
    {
      "epoch": 0.6392,
      "grad_norm": 0.12871234118938446,
      "learning_rate": 7.244979919678716e-05,
      "loss": 0.4852,
      "step": 799
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.11332140862941742,
      "learning_rate": 7.228915662650602e-05,
      "loss": 0.4976,
      "step": 800
    },
    {
      "epoch": 0.6408,
      "grad_norm": 0.10795821994543076,
      "learning_rate": 7.21285140562249e-05,
      "loss": 0.4598,
      "step": 801
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.19242200255393982,
      "learning_rate": 7.196787148594378e-05,
      "loss": 0.5085,
      "step": 802
    },
    {
      "epoch": 0.6424,
      "grad_norm": 0.0873708426952362,
      "learning_rate": 7.180722891566266e-05,
      "loss": 0.4507,
      "step": 803
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.12163491547107697,
      "learning_rate": 7.164658634538153e-05,
      "loss": 0.5314,
      "step": 804
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.11692342907190323,
      "learning_rate": 7.14859437751004e-05,
      "loss": 0.4991,
      "step": 805
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.13600319623947144,
      "learning_rate": 7.132530120481928e-05,
      "loss": 0.5768,
      "step": 806
    },
    {
      "epoch": 0.6456,
      "grad_norm": 0.09523335099220276,
      "learning_rate": 7.116465863453816e-05,
      "loss": 0.4803,
      "step": 807
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.11068008095026016,
      "learning_rate": 7.100401606425703e-05,
      "loss": 0.4998,
      "step": 808
    },
    {
      "epoch": 0.6472,
      "grad_norm": 0.15950143337249756,
      "learning_rate": 7.084337349397591e-05,
      "loss": 0.4688,
      "step": 809
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.14151781797409058,
      "learning_rate": 7.068273092369478e-05,
      "loss": 0.4793,
      "step": 810
    },
    {
      "epoch": 0.6488,
      "grad_norm": 0.09285901486873627,
      "learning_rate": 7.052208835341366e-05,
      "loss": 0.4536,
      "step": 811
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.13712890446186066,
      "learning_rate": 7.036144578313253e-05,
      "loss": 0.4966,
      "step": 812
    },
    {
      "epoch": 0.6504,
      "grad_norm": 0.1124717965722084,
      "learning_rate": 7.020080321285141e-05,
      "loss": 0.5209,
      "step": 813
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.13953152298927307,
      "learning_rate": 7.004016064257029e-05,
      "loss": 0.4861,
      "step": 814
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.10959184914827347,
      "learning_rate": 6.987951807228917e-05,
      "loss": 0.5099,
      "step": 815
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.11732440441846848,
      "learning_rate": 6.971887550200803e-05,
      "loss": 0.5407,
      "step": 816
    },
    {
      "epoch": 0.6536,
      "grad_norm": 0.11811047792434692,
      "learning_rate": 6.955823293172691e-05,
      "loss": 0.4645,
      "step": 817
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.09267845004796982,
      "learning_rate": 6.939759036144579e-05,
      "loss": 0.4506,
      "step": 818
    },
    {
      "epoch": 0.6552,
      "grad_norm": 0.12497903406620026,
      "learning_rate": 6.923694779116467e-05,
      "loss": 0.4936,
      "step": 819
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.13362573087215424,
      "learning_rate": 6.907630522088355e-05,
      "loss": 0.4927,
      "step": 820
    },
    {
      "epoch": 0.6568,
      "grad_norm": 0.09150061756372452,
      "learning_rate": 6.891566265060241e-05,
      "loss": 0.443,
      "step": 821
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.1078265830874443,
      "learning_rate": 6.875502008032129e-05,
      "loss": 0.4539,
      "step": 822
    },
    {
      "epoch": 0.6584,
      "grad_norm": 0.10353058576583862,
      "learning_rate": 6.859437751004017e-05,
      "loss": 0.4475,
      "step": 823
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.10515908151865005,
      "learning_rate": 6.843373493975905e-05,
      "loss": 0.5007,
      "step": 824
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.10976764559745789,
      "learning_rate": 6.827309236947793e-05,
      "loss": 0.489,
      "step": 825
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.09132584184408188,
      "learning_rate": 6.811244979919679e-05,
      "loss": 0.4792,
      "step": 826
    },
    {
      "epoch": 0.6616,
      "grad_norm": 0.11214404553174973,
      "learning_rate": 6.795180722891567e-05,
      "loss": 0.4843,
      "step": 827
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.13079196214675903,
      "learning_rate": 6.779116465863455e-05,
      "loss": 0.5008,
      "step": 828
    },
    {
      "epoch": 0.6632,
      "grad_norm": 0.08328912407159805,
      "learning_rate": 6.763052208835343e-05,
      "loss": 0.3965,
      "step": 829
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.11229658871889114,
      "learning_rate": 6.746987951807229e-05,
      "loss": 0.4912,
      "step": 830
    },
    {
      "epoch": 0.6648,
      "grad_norm": 0.11250285059213638,
      "learning_rate": 6.730923694779117e-05,
      "loss": 0.5061,
      "step": 831
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.10567785799503326,
      "learning_rate": 6.714859437751005e-05,
      "loss": 0.4819,
      "step": 832
    },
    {
      "epoch": 0.6664,
      "grad_norm": 0.11236647516489029,
      "learning_rate": 6.698795180722893e-05,
      "loss": 0.4871,
      "step": 833
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.12911349534988403,
      "learning_rate": 6.68273092369478e-05,
      "loss": 0.5433,
      "step": 834
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.10794056951999664,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.4277,
      "step": 835
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.0961592048406601,
      "learning_rate": 6.650602409638555e-05,
      "loss": 0.4619,
      "step": 836
    },
    {
      "epoch": 0.6696,
      "grad_norm": 0.09573223441839218,
      "learning_rate": 6.634538152610443e-05,
      "loss": 0.4415,
      "step": 837
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.11446923762559891,
      "learning_rate": 6.61847389558233e-05,
      "loss": 0.5222,
      "step": 838
    },
    {
      "epoch": 0.6712,
      "grad_norm": 0.07675093412399292,
      "learning_rate": 6.602409638554217e-05,
      "loss": 0.3913,
      "step": 839
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.11769133806228638,
      "learning_rate": 6.586345381526105e-05,
      "loss": 0.4534,
      "step": 840
    },
    {
      "epoch": 0.6728,
      "grad_norm": 0.10152817517518997,
      "learning_rate": 6.570281124497993e-05,
      "loss": 0.4973,
      "step": 841
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.08569761365652084,
      "learning_rate": 6.55421686746988e-05,
      "loss": 0.4875,
      "step": 842
    },
    {
      "epoch": 0.6744,
      "grad_norm": 0.09285473823547363,
      "learning_rate": 6.538152610441768e-05,
      "loss": 0.5023,
      "step": 843
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.10047479718923569,
      "learning_rate": 6.522088353413655e-05,
      "loss": 0.5155,
      "step": 844
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.09546620398759842,
      "learning_rate": 6.506024096385543e-05,
      "loss": 0.4926,
      "step": 845
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.1118999794125557,
      "learning_rate": 6.48995983935743e-05,
      "loss": 0.4966,
      "step": 846
    },
    {
      "epoch": 0.6776,
      "grad_norm": 0.0726458877325058,
      "learning_rate": 6.473895582329318e-05,
      "loss": 0.3718,
      "step": 847
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.09829149395227432,
      "learning_rate": 6.457831325301205e-05,
      "loss": 0.4817,
      "step": 848
    },
    {
      "epoch": 0.6792,
      "grad_norm": 0.10370348393917084,
      "learning_rate": 6.441767068273093e-05,
      "loss": 0.4213,
      "step": 849
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.1370488405227661,
      "learning_rate": 6.42570281124498e-05,
      "loss": 0.5081,
      "step": 850
    },
    {
      "epoch": 0.6808,
      "grad_norm": 0.11168081313371658,
      "learning_rate": 6.409638554216868e-05,
      "loss": 0.4966,
      "step": 851
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.12793509662151337,
      "learning_rate": 6.393574297188756e-05,
      "loss": 0.5743,
      "step": 852
    },
    {
      "epoch": 0.6824,
      "grad_norm": 0.13520659506320953,
      "learning_rate": 6.377510040160643e-05,
      "loss": 0.5197,
      "step": 853
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.1277017891407013,
      "learning_rate": 6.36144578313253e-05,
      "loss": 0.5209,
      "step": 854
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.11988008767366409,
      "learning_rate": 6.345381526104418e-05,
      "loss": 0.4917,
      "step": 855
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.13251447677612305,
      "learning_rate": 6.329317269076306e-05,
      "loss": 0.5619,
      "step": 856
    },
    {
      "epoch": 0.6856,
      "grad_norm": 0.09995885193347931,
      "learning_rate": 6.313253012048193e-05,
      "loss": 0.4331,
      "step": 857
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.10777714848518372,
      "learning_rate": 6.29718875502008e-05,
      "loss": 0.5307,
      "step": 858
    },
    {
      "epoch": 0.6872,
      "grad_norm": 0.08065460622310638,
      "learning_rate": 6.281124497991968e-05,
      "loss": 0.4361,
      "step": 859
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.1005324199795723,
      "learning_rate": 6.265060240963856e-05,
      "loss": 0.4308,
      "step": 860
    },
    {
      "epoch": 0.6888,
      "grad_norm": 0.12609006464481354,
      "learning_rate": 6.248995983935744e-05,
      "loss": 0.5318,
      "step": 861
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.11622948944568634,
      "learning_rate": 6.23293172690763e-05,
      "loss": 0.558,
      "step": 862
    },
    {
      "epoch": 0.6904,
      "grad_norm": 0.10177360475063324,
      "learning_rate": 6.216867469879518e-05,
      "loss": 0.4772,
      "step": 863
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.12575602531433105,
      "learning_rate": 6.200803212851406e-05,
      "loss": 0.4484,
      "step": 864
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.08902134746313095,
      "learning_rate": 6.184738955823294e-05,
      "loss": 0.4673,
      "step": 865
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.12699082493782043,
      "learning_rate": 6.168674698795182e-05,
      "loss": 0.4945,
      "step": 866
    },
    {
      "epoch": 0.6936,
      "grad_norm": 0.11471537500619888,
      "learning_rate": 6.152610441767068e-05,
      "loss": 0.4891,
      "step": 867
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.12152869254350662,
      "learning_rate": 6.136546184738956e-05,
      "loss": 0.4823,
      "step": 868
    },
    {
      "epoch": 0.6952,
      "grad_norm": 0.09688589721918106,
      "learning_rate": 6.120481927710844e-05,
      "loss": 0.4947,
      "step": 869
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.10482984036207199,
      "learning_rate": 6.104417670682732e-05,
      "loss": 0.4699,
      "step": 870
    },
    {
      "epoch": 0.6968,
      "grad_norm": 0.08551564812660217,
      "learning_rate": 6.0883534136546184e-05,
      "loss": 0.4397,
      "step": 871
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.11445095390081406,
      "learning_rate": 6.072289156626506e-05,
      "loss": 0.4963,
      "step": 872
    },
    {
      "epoch": 0.6984,
      "grad_norm": 0.10002540051937103,
      "learning_rate": 6.056224899598394e-05,
      "loss": 0.4707,
      "step": 873
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.10413425415754318,
      "learning_rate": 6.040160642570282e-05,
      "loss": 0.5296,
      "step": 874
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.09872613102197647,
      "learning_rate": 6.02409638554217e-05,
      "loss": 0.4635,
      "step": 875
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.10249559581279755,
      "learning_rate": 6.008032128514056e-05,
      "loss": 0.4982,
      "step": 876
    },
    {
      "epoch": 0.7016,
      "grad_norm": 0.11207075417041779,
      "learning_rate": 5.991967871485944e-05,
      "loss": 0.527,
      "step": 877
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.08746670186519623,
      "learning_rate": 5.975903614457832e-05,
      "loss": 0.428,
      "step": 878
    },
    {
      "epoch": 0.7032,
      "grad_norm": 0.09372904896736145,
      "learning_rate": 5.95983935742972e-05,
      "loss": 0.4832,
      "step": 879
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.08314260095357895,
      "learning_rate": 5.943775100401606e-05,
      "loss": 0.4591,
      "step": 880
    },
    {
      "epoch": 0.7048,
      "grad_norm": 0.12611635029315948,
      "learning_rate": 5.927710843373494e-05,
      "loss": 0.5241,
      "step": 881
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.09125303477048874,
      "learning_rate": 5.911646586345382e-05,
      "loss": 0.4936,
      "step": 882
    },
    {
      "epoch": 0.7064,
      "grad_norm": 0.11775407195091248,
      "learning_rate": 5.89558232931727e-05,
      "loss": 0.5598,
      "step": 883
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.09769311547279358,
      "learning_rate": 5.8795180722891576e-05,
      "loss": 0.4896,
      "step": 884
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.09227638691663742,
      "learning_rate": 5.863453815261044e-05,
      "loss": 0.4326,
      "step": 885
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.10676570236682892,
      "learning_rate": 5.847389558232932e-05,
      "loss": 0.4977,
      "step": 886
    },
    {
      "epoch": 0.7096,
      "grad_norm": 0.13536284863948822,
      "learning_rate": 5.83132530120482e-05,
      "loss": 0.4513,
      "step": 887
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.08585777878761292,
      "learning_rate": 5.8152610441767076e-05,
      "loss": 0.4325,
      "step": 888
    },
    {
      "epoch": 0.7112,
      "grad_norm": 0.1139865294098854,
      "learning_rate": 5.799196787148594e-05,
      "loss": 0.4618,
      "step": 889
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.10497510433197021,
      "learning_rate": 5.783132530120482e-05,
      "loss": 0.4166,
      "step": 890
    },
    {
      "epoch": 0.7128,
      "grad_norm": 0.1419491022825241,
      "learning_rate": 5.76706827309237e-05,
      "loss": 0.4742,
      "step": 891
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.13106869161128998,
      "learning_rate": 5.7510040160642576e-05,
      "loss": 0.5112,
      "step": 892
    },
    {
      "epoch": 0.7144,
      "grad_norm": 0.10713337361812592,
      "learning_rate": 5.7349397590361454e-05,
      "loss": 0.482,
      "step": 893
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.08877479285001755,
      "learning_rate": 5.718875502008032e-05,
      "loss": 0.5074,
      "step": 894
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.09858496487140656,
      "learning_rate": 5.70281124497992e-05,
      "loss": 0.4461,
      "step": 895
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.10170511156320572,
      "learning_rate": 5.6867469879518076e-05,
      "loss": 0.4692,
      "step": 896
    },
    {
      "epoch": 0.7176,
      "grad_norm": 0.09657211601734161,
      "learning_rate": 5.6706827309236955e-05,
      "loss": 0.4188,
      "step": 897
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.13929669559001923,
      "learning_rate": 5.654618473895582e-05,
      "loss": 0.4858,
      "step": 898
    },
    {
      "epoch": 0.7192,
      "grad_norm": 0.11505894362926483,
      "learning_rate": 5.63855421686747e-05,
      "loss": 0.4783,
      "step": 899
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.1193813756108284,
      "learning_rate": 5.6224899598393576e-05,
      "loss": 0.4171,
      "step": 900
    },
    {
      "epoch": 0.7208,
      "grad_norm": 0.12168873101472855,
      "learning_rate": 5.6064257028112455e-05,
      "loss": 0.497,
      "step": 901
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.12536291778087616,
      "learning_rate": 5.590361445783133e-05,
      "loss": 0.4582,
      "step": 902
    },
    {
      "epoch": 0.7224,
      "grad_norm": 0.11559290438890457,
      "learning_rate": 5.57429718875502e-05,
      "loss": 0.4325,
      "step": 903
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.11844436079263687,
      "learning_rate": 5.5582329317269076e-05,
      "loss": 0.5542,
      "step": 904
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.11935727298259735,
      "learning_rate": 5.5421686746987955e-05,
      "loss": 0.4672,
      "step": 905
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.09322020411491394,
      "learning_rate": 5.526104417670683e-05,
      "loss": 0.4228,
      "step": 906
    },
    {
      "epoch": 0.7256,
      "grad_norm": 0.09223926067352295,
      "learning_rate": 5.51004016064257e-05,
      "loss": 0.429,
      "step": 907
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.10102161765098572,
      "learning_rate": 5.4939759036144576e-05,
      "loss": 0.4877,
      "step": 908
    },
    {
      "epoch": 0.7272,
      "grad_norm": 0.09456540644168854,
      "learning_rate": 5.4779116465863455e-05,
      "loss": 0.4361,
      "step": 909
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.1312452107667923,
      "learning_rate": 5.461847389558233e-05,
      "loss": 0.5251,
      "step": 910
    },
    {
      "epoch": 0.7288,
      "grad_norm": 0.10304916650056839,
      "learning_rate": 5.445783132530121e-05,
      "loss": 0.4627,
      "step": 911
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.11697432398796082,
      "learning_rate": 5.4297188755020076e-05,
      "loss": 0.4528,
      "step": 912
    },
    {
      "epoch": 0.7304,
      "grad_norm": 0.08992739766836166,
      "learning_rate": 5.4136546184738955e-05,
      "loss": 0.4535,
      "step": 913
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.10907463729381561,
      "learning_rate": 5.397590361445783e-05,
      "loss": 0.4692,
      "step": 914
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.14243875443935394,
      "learning_rate": 5.381526104417671e-05,
      "loss": 0.5003,
      "step": 915
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.09349612891674042,
      "learning_rate": 5.365461847389559e-05,
      "loss": 0.5013,
      "step": 916
    },
    {
      "epoch": 0.7336,
      "grad_norm": 0.12211780995130539,
      "learning_rate": 5.3493975903614455e-05,
      "loss": 0.4918,
      "step": 917
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.10732684284448624,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.4839,
      "step": 918
    },
    {
      "epoch": 0.7352,
      "grad_norm": 0.1314370334148407,
      "learning_rate": 5.317269076305221e-05,
      "loss": 0.4686,
      "step": 919
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.10740600526332855,
      "learning_rate": 5.301204819277109e-05,
      "loss": 0.5064,
      "step": 920
    },
    {
      "epoch": 0.7368,
      "grad_norm": 0.09658735990524292,
      "learning_rate": 5.2851405622489955e-05,
      "loss": 0.473,
      "step": 921
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.0956924557685852,
      "learning_rate": 5.269076305220883e-05,
      "loss": 0.4637,
      "step": 922
    },
    {
      "epoch": 0.7384,
      "grad_norm": 0.0931069627404213,
      "learning_rate": 5.253012048192771e-05,
      "loss": 0.4771,
      "step": 923
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.0974125787615776,
      "learning_rate": 5.236947791164659e-05,
      "loss": 0.5071,
      "step": 924
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.09930874407291412,
      "learning_rate": 5.220883534136547e-05,
      "loss": 0.4747,
      "step": 925
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.08394239842891693,
      "learning_rate": 5.204819277108433e-05,
      "loss": 0.4558,
      "step": 926
    },
    {
      "epoch": 0.7416,
      "grad_norm": 0.08615977317094803,
      "learning_rate": 5.188755020080321e-05,
      "loss": 0.4736,
      "step": 927
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.10151076316833496,
      "learning_rate": 5.172690763052209e-05,
      "loss": 0.5011,
      "step": 928
    },
    {
      "epoch": 0.7432,
      "grad_norm": 0.0878298282623291,
      "learning_rate": 5.156626506024097e-05,
      "loss": 0.4586,
      "step": 929
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.10864976048469543,
      "learning_rate": 5.140562248995984e-05,
      "loss": 0.4531,
      "step": 930
    },
    {
      "epoch": 0.7448,
      "grad_norm": 0.09352067112922668,
      "learning_rate": 5.124497991967871e-05,
      "loss": 0.4735,
      "step": 931
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.13200180232524872,
      "learning_rate": 5.108433734939759e-05,
      "loss": 0.4676,
      "step": 932
    },
    {
      "epoch": 0.7464,
      "grad_norm": 0.11255086213350296,
      "learning_rate": 5.092369477911647e-05,
      "loss": 0.5165,
      "step": 933
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.09069664031267166,
      "learning_rate": 5.076305220883535e-05,
      "loss": 0.425,
      "step": 934
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.1288009136915207,
      "learning_rate": 5.060240963855422e-05,
      "loss": 0.4484,
      "step": 935
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.1277988702058792,
      "learning_rate": 5.04417670682731e-05,
      "loss": 0.5345,
      "step": 936
    },
    {
      "epoch": 0.7496,
      "grad_norm": 0.09223822504281998,
      "learning_rate": 5.028112449799197e-05,
      "loss": 0.4667,
      "step": 937
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.10068410634994507,
      "learning_rate": 5.012048192771085e-05,
      "loss": 0.4298,
      "step": 938
    },
    {
      "epoch": 0.7512,
      "grad_norm": 0.09313048422336578,
      "learning_rate": 4.995983935742972e-05,
      "loss": 0.4758,
      "step": 939
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.1279982477426529,
      "learning_rate": 4.97991967871486e-05,
      "loss": 0.5405,
      "step": 940
    },
    {
      "epoch": 0.7528,
      "grad_norm": 0.11784118413925171,
      "learning_rate": 4.9638554216867475e-05,
      "loss": 0.4583,
      "step": 941
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.11934930086135864,
      "learning_rate": 4.947791164658635e-05,
      "loss": 0.516,
      "step": 942
    },
    {
      "epoch": 0.7544,
      "grad_norm": 0.09514475613832474,
      "learning_rate": 4.9317269076305225e-05,
      "loss": 0.481,
      "step": 943
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.10187412798404694,
      "learning_rate": 4.9156626506024104e-05,
      "loss": 0.4949,
      "step": 944
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.10254120826721191,
      "learning_rate": 4.8995983935742975e-05,
      "loss": 0.5113,
      "step": 945
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.12352405488491058,
      "learning_rate": 4.8835341365461854e-05,
      "loss": 0.5053,
      "step": 946
    },
    {
      "epoch": 0.7576,
      "grad_norm": 0.09907654672861099,
      "learning_rate": 4.8674698795180725e-05,
      "loss": 0.5081,
      "step": 947
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.091077521443367,
      "learning_rate": 4.8514056224899604e-05,
      "loss": 0.4718,
      "step": 948
    },
    {
      "epoch": 0.7592,
      "grad_norm": 0.09803082793951035,
      "learning_rate": 4.8353413654618476e-05,
      "loss": 0.4334,
      "step": 949
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.09375768899917603,
      "learning_rate": 4.8192771084337354e-05,
      "loss": 0.4104,
      "step": 950
    },
    {
      "epoch": 0.7608,
      "grad_norm": 0.09282802045345306,
      "learning_rate": 4.803212851405623e-05,
      "loss": 0.4407,
      "step": 951
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.10455197840929031,
      "learning_rate": 4.7871485943775104e-05,
      "loss": 0.519,
      "step": 952
    },
    {
      "epoch": 0.7624,
      "grad_norm": 0.10423117130994797,
      "learning_rate": 4.771084337349398e-05,
      "loss": 0.4941,
      "step": 953
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.10390330851078033,
      "learning_rate": 4.7550200803212854e-05,
      "loss": 0.4443,
      "step": 954
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.08706572651863098,
      "learning_rate": 4.738955823293173e-05,
      "loss": 0.4618,
      "step": 955
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.1047988086938858,
      "learning_rate": 4.7228915662650604e-05,
      "loss": 0.5389,
      "step": 956
    },
    {
      "epoch": 0.7656,
      "grad_norm": 0.10451304167509079,
      "learning_rate": 4.706827309236948e-05,
      "loss": 0.4602,
      "step": 957
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.1085662990808487,
      "learning_rate": 4.690763052208836e-05,
      "loss": 0.481,
      "step": 958
    },
    {
      "epoch": 0.7672,
      "grad_norm": 0.09513380378484726,
      "learning_rate": 4.674698795180723e-05,
      "loss": 0.4412,
      "step": 959
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.11769349128007889,
      "learning_rate": 4.658634538152611e-05,
      "loss": 0.5178,
      "step": 960
    },
    {
      "epoch": 0.7688,
      "grad_norm": 0.09180514514446259,
      "learning_rate": 4.642570281124498e-05,
      "loss": 0.4667,
      "step": 961
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.10959535837173462,
      "learning_rate": 4.626506024096386e-05,
      "loss": 0.5214,
      "step": 962
    },
    {
      "epoch": 0.7704,
      "grad_norm": 0.09688473492860794,
      "learning_rate": 4.610441767068273e-05,
      "loss": 0.4895,
      "step": 963
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.09445284307003021,
      "learning_rate": 4.594377510040161e-05,
      "loss": 0.4546,
      "step": 964
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.12364725768566132,
      "learning_rate": 4.578313253012048e-05,
      "loss": 0.4789,
      "step": 965
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.09929939359426498,
      "learning_rate": 4.562248995983936e-05,
      "loss": 0.4508,
      "step": 966
    },
    {
      "epoch": 0.7736,
      "grad_norm": 0.10947273671627045,
      "learning_rate": 4.546184738955824e-05,
      "loss": 0.4678,
      "step": 967
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.13043202459812164,
      "learning_rate": 4.530120481927711e-05,
      "loss": 0.5357,
      "step": 968
    },
    {
      "epoch": 0.7752,
      "grad_norm": 0.09347863495349884,
      "learning_rate": 4.514056224899599e-05,
      "loss": 0.4408,
      "step": 969
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.12351908534765244,
      "learning_rate": 4.497991967871486e-05,
      "loss": 0.5879,
      "step": 970
    },
    {
      "epoch": 0.7768,
      "grad_norm": 0.1388445794582367,
      "learning_rate": 4.481927710843374e-05,
      "loss": 0.585,
      "step": 971
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.09480954706668854,
      "learning_rate": 4.465863453815261e-05,
      "loss": 0.4397,
      "step": 972
    },
    {
      "epoch": 0.7784,
      "grad_norm": 0.15420383214950562,
      "learning_rate": 4.449799196787149e-05,
      "loss": 0.443,
      "step": 973
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.09919373691082001,
      "learning_rate": 4.433734939759036e-05,
      "loss": 0.4769,
      "step": 974
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.10082028806209564,
      "learning_rate": 4.417670682730924e-05,
      "loss": 0.4668,
      "step": 975
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.10275644809007645,
      "learning_rate": 4.401606425702812e-05,
      "loss": 0.468,
      "step": 976
    },
    {
      "epoch": 0.7816,
      "grad_norm": 0.09045819193124771,
      "learning_rate": 4.385542168674699e-05,
      "loss": 0.4373,
      "step": 977
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.10059415549039841,
      "learning_rate": 4.369477911646587e-05,
      "loss": 0.5103,
      "step": 978
    },
    {
      "epoch": 0.7832,
      "grad_norm": 0.0991099402308464,
      "learning_rate": 4.353413654618474e-05,
      "loss": 0.4983,
      "step": 979
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.11825233697891235,
      "learning_rate": 4.337349397590362e-05,
      "loss": 0.4556,
      "step": 980
    },
    {
      "epoch": 0.7848,
      "grad_norm": 0.09148912131786346,
      "learning_rate": 4.321285140562249e-05,
      "loss": 0.5044,
      "step": 981
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.10712417960166931,
      "learning_rate": 4.305220883534137e-05,
      "loss": 0.4941,
      "step": 982
    },
    {
      "epoch": 0.7864,
      "grad_norm": 0.10206259042024612,
      "learning_rate": 4.2891566265060246e-05,
      "loss": 0.4734,
      "step": 983
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.0856347382068634,
      "learning_rate": 4.273092369477912e-05,
      "loss": 0.4256,
      "step": 984
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.10798379778862,
      "learning_rate": 4.2570281124497996e-05,
      "loss": 0.463,
      "step": 985
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.1260080337524414,
      "learning_rate": 4.240963855421687e-05,
      "loss": 0.5624,
      "step": 986
    },
    {
      "epoch": 0.7896,
      "grad_norm": 0.08655782043933868,
      "learning_rate": 4.2248995983935746e-05,
      "loss": 0.4649,
      "step": 987
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.13115963339805603,
      "learning_rate": 4.208835341365462e-05,
      "loss": 0.4615,
      "step": 988
    },
    {
      "epoch": 0.7912,
      "grad_norm": 0.14057227969169617,
      "learning_rate": 4.1927710843373496e-05,
      "loss": 0.4897,
      "step": 989
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.10657229274511337,
      "learning_rate": 4.176706827309237e-05,
      "loss": 0.4591,
      "step": 990
    },
    {
      "epoch": 0.7928,
      "grad_norm": 0.13718949258327484,
      "learning_rate": 4.1606425702811246e-05,
      "loss": 0.5287,
      "step": 991
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.11712926626205444,
      "learning_rate": 4.1445783132530125e-05,
      "loss": 0.4907,
      "step": 992
    },
    {
      "epoch": 0.7944,
      "grad_norm": 0.11180894076824188,
      "learning_rate": 4.1285140562248996e-05,
      "loss": 0.4492,
      "step": 993
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.1287088543176651,
      "learning_rate": 4.1124497991967875e-05,
      "loss": 0.4771,
      "step": 994
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.11727950721979141,
      "learning_rate": 4.0963855421686746e-05,
      "loss": 0.4693,
      "step": 995
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.10273710638284683,
      "learning_rate": 4.0803212851405625e-05,
      "loss": 0.4952,
      "step": 996
    },
    {
      "epoch": 0.7976,
      "grad_norm": 0.12066513299942017,
      "learning_rate": 4.0642570281124496e-05,
      "loss": 0.4625,
      "step": 997
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.10467080026865005,
      "learning_rate": 4.0481927710843375e-05,
      "loss": 0.4398,
      "step": 998
    },
    {
      "epoch": 0.7992,
      "grad_norm": 0.09202296286821365,
      "learning_rate": 4.0321285140562246e-05,
      "loss": 0.4389,
      "step": 999
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.11512041836977005,
      "learning_rate": 4.0160642570281125e-05,
      "loss": 0.4805,
      "step": 1000
    },
    {
      "epoch": 0.8008,
      "grad_norm": 0.09854777902364731,
      "learning_rate": 4e-05,
      "loss": 0.43,
      "step": 1001
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.16291049122810364,
      "learning_rate": 3.9839357429718875e-05,
      "loss": 0.5707,
      "step": 1002
    },
    {
      "epoch": 0.8024,
      "grad_norm": 0.08963684737682343,
      "learning_rate": 3.967871485943775e-05,
      "loss": 0.4602,
      "step": 1003
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.09842818230390549,
      "learning_rate": 3.9518072289156625e-05,
      "loss": 0.445,
      "step": 1004
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.11588279157876968,
      "learning_rate": 3.93574297188755e-05,
      "loss": 0.4875,
      "step": 1005
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.07390160858631134,
      "learning_rate": 3.9196787148594375e-05,
      "loss": 0.4011,
      "step": 1006
    },
    {
      "epoch": 0.8056,
      "grad_norm": 0.11805418133735657,
      "learning_rate": 3.903614457831325e-05,
      "loss": 0.4574,
      "step": 1007
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.1128387451171875,
      "learning_rate": 3.887550200803213e-05,
      "loss": 0.5414,
      "step": 1008
    },
    {
      "epoch": 0.8072,
      "grad_norm": 0.10813750326633453,
      "learning_rate": 3.8714859437751e-05,
      "loss": 0.4938,
      "step": 1009
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.09992881119251251,
      "learning_rate": 3.855421686746988e-05,
      "loss": 0.521,
      "step": 1010
    },
    {
      "epoch": 0.8088,
      "grad_norm": 0.11130319535732269,
      "learning_rate": 3.8393574297188753e-05,
      "loss": 0.4355,
      "step": 1011
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.08140555024147034,
      "learning_rate": 3.823293172690763e-05,
      "loss": 0.4262,
      "step": 1012
    },
    {
      "epoch": 0.8104,
      "grad_norm": 0.09676384180784225,
      "learning_rate": 3.8072289156626503e-05,
      "loss": 0.4659,
      "step": 1013
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.09696245938539505,
      "learning_rate": 3.791164658634538e-05,
      "loss": 0.4477,
      "step": 1014
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.13540269434452057,
      "learning_rate": 3.7751004016064253e-05,
      "loss": 0.4865,
      "step": 1015
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.08877073228359222,
      "learning_rate": 3.759036144578313e-05,
      "loss": 0.4674,
      "step": 1016
    },
    {
      "epoch": 0.8136,
      "grad_norm": 0.09497599303722382,
      "learning_rate": 3.742971887550201e-05,
      "loss": 0.4313,
      "step": 1017
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.10143701732158661,
      "learning_rate": 3.726907630522088e-05,
      "loss": 0.4799,
      "step": 1018
    },
    {
      "epoch": 0.8152,
      "grad_norm": 0.08236835896968842,
      "learning_rate": 3.710843373493976e-05,
      "loss": 0.3863,
      "step": 1019
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.1045350506901741,
      "learning_rate": 3.694779116465863e-05,
      "loss": 0.428,
      "step": 1020
    },
    {
      "epoch": 0.8168,
      "grad_norm": 0.11135508865118027,
      "learning_rate": 3.678714859437751e-05,
      "loss": 0.4571,
      "step": 1021
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.13667021691799164,
      "learning_rate": 3.662650602409639e-05,
      "loss": 0.5371,
      "step": 1022
    },
    {
      "epoch": 0.8184,
      "grad_norm": 0.09082347899675369,
      "learning_rate": 3.646586345381526e-05,
      "loss": 0.5268,
      "step": 1023
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.1286255270242691,
      "learning_rate": 3.630522088353414e-05,
      "loss": 0.5415,
      "step": 1024
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.10485266149044037,
      "learning_rate": 3.614457831325301e-05,
      "loss": 0.491,
      "step": 1025
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.10988353192806244,
      "learning_rate": 3.598393574297189e-05,
      "loss": 0.4732,
      "step": 1026
    },
    {
      "epoch": 0.8216,
      "grad_norm": 0.11836004257202148,
      "learning_rate": 3.582329317269077e-05,
      "loss": 0.5178,
      "step": 1027
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.09123828262090683,
      "learning_rate": 3.566265060240964e-05,
      "loss": 0.4508,
      "step": 1028
    },
    {
      "epoch": 0.8232,
      "grad_norm": 0.09082122147083282,
      "learning_rate": 3.550200803212852e-05,
      "loss": 0.4608,
      "step": 1029
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.09495491534471512,
      "learning_rate": 3.534136546184739e-05,
      "loss": 0.4672,
      "step": 1030
    },
    {
      "epoch": 0.8248,
      "grad_norm": 0.10334671288728714,
      "learning_rate": 3.518072289156627e-05,
      "loss": 0.4358,
      "step": 1031
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.1066695898771286,
      "learning_rate": 3.5020080321285146e-05,
      "loss": 0.4788,
      "step": 1032
    },
    {
      "epoch": 0.8264,
      "grad_norm": 0.12286414951086044,
      "learning_rate": 3.485943775100402e-05,
      "loss": 0.5575,
      "step": 1033
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.09905432164669037,
      "learning_rate": 3.4698795180722896e-05,
      "loss": 0.4945,
      "step": 1034
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.1083206906914711,
      "learning_rate": 3.4538152610441774e-05,
      "loss": 0.5207,
      "step": 1035
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.10899668186903,
      "learning_rate": 3.4377510040160646e-05,
      "loss": 0.4388,
      "step": 1036
    },
    {
      "epoch": 0.8296,
      "grad_norm": 0.10140711814165115,
      "learning_rate": 3.4216867469879524e-05,
      "loss": 0.503,
      "step": 1037
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.09345829486846924,
      "learning_rate": 3.4056224899598396e-05,
      "loss": 0.4735,
      "step": 1038
    },
    {
      "epoch": 0.8312,
      "grad_norm": 0.1034870520234108,
      "learning_rate": 3.3895582329317274e-05,
      "loss": 0.4603,
      "step": 1039
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.09563995897769928,
      "learning_rate": 3.3734939759036146e-05,
      "loss": 0.4682,
      "step": 1040
    },
    {
      "epoch": 0.8328,
      "grad_norm": 0.09394624829292297,
      "learning_rate": 3.3574297188755024e-05,
      "loss": 0.4366,
      "step": 1041
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.09766627848148346,
      "learning_rate": 3.34136546184739e-05,
      "loss": 0.4628,
      "step": 1042
    },
    {
      "epoch": 0.8344,
      "grad_norm": 0.10870817303657532,
      "learning_rate": 3.3253012048192774e-05,
      "loss": 0.471,
      "step": 1043
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.08893756568431854,
      "learning_rate": 3.309236947791165e-05,
      "loss": 0.5204,
      "step": 1044
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.09367219358682632,
      "learning_rate": 3.2931726907630524e-05,
      "loss": 0.4613,
      "step": 1045
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.10235219448804855,
      "learning_rate": 3.27710843373494e-05,
      "loss": 0.4374,
      "step": 1046
    },
    {
      "epoch": 0.8376,
      "grad_norm": 0.10985090583562851,
      "learning_rate": 3.2610441767068274e-05,
      "loss": 0.4757,
      "step": 1047
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.09222611039876938,
      "learning_rate": 3.244979919678715e-05,
      "loss": 0.4603,
      "step": 1048
    },
    {
      "epoch": 0.8392,
      "grad_norm": 0.10513116419315338,
      "learning_rate": 3.2289156626506024e-05,
      "loss": 0.46,
      "step": 1049
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.10409104824066162,
      "learning_rate": 3.21285140562249e-05,
      "loss": 0.4417,
      "step": 1050
    },
    {
      "epoch": 0.8408,
      "grad_norm": 0.10630180686712265,
      "learning_rate": 3.196787148594378e-05,
      "loss": 0.5029,
      "step": 1051
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.09931227564811707,
      "learning_rate": 3.180722891566265e-05,
      "loss": 0.4646,
      "step": 1052
    },
    {
      "epoch": 0.8424,
      "grad_norm": 0.09875653684139252,
      "learning_rate": 3.164658634538153e-05,
      "loss": 0.5251,
      "step": 1053
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.12171750515699387,
      "learning_rate": 3.14859437751004e-05,
      "loss": 0.5096,
      "step": 1054
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.09398624300956726,
      "learning_rate": 3.132530120481928e-05,
      "loss": 0.4361,
      "step": 1055
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.0973593220114708,
      "learning_rate": 3.116465863453815e-05,
      "loss": 0.4669,
      "step": 1056
    },
    {
      "epoch": 0.8456,
      "grad_norm": 0.10499123483896255,
      "learning_rate": 3.100401606425703e-05,
      "loss": 0.4891,
      "step": 1057
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.10372471064329147,
      "learning_rate": 3.084337349397591e-05,
      "loss": 0.4892,
      "step": 1058
    },
    {
      "epoch": 0.8472,
      "grad_norm": 0.11294291913509369,
      "learning_rate": 3.068273092369478e-05,
      "loss": 0.5546,
      "step": 1059
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.09603947401046753,
      "learning_rate": 3.052208835341366e-05,
      "loss": 0.4739,
      "step": 1060
    },
    {
      "epoch": 0.8488,
      "grad_norm": 0.11314012855291367,
      "learning_rate": 3.036144578313253e-05,
      "loss": 0.5179,
      "step": 1061
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.10174660384654999,
      "learning_rate": 3.020080321285141e-05,
      "loss": 0.4829,
      "step": 1062
    },
    {
      "epoch": 0.8504,
      "grad_norm": 0.11881547421216965,
      "learning_rate": 3.004016064257028e-05,
      "loss": 0.4543,
      "step": 1063
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.08715780079364777,
      "learning_rate": 2.987951807228916e-05,
      "loss": 0.4025,
      "step": 1064
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.10641885548830032,
      "learning_rate": 2.971887550200803e-05,
      "loss": 0.4754,
      "step": 1065
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.09976842999458313,
      "learning_rate": 2.955823293172691e-05,
      "loss": 0.4496,
      "step": 1066
    },
    {
      "epoch": 0.8536,
      "grad_norm": 0.09287154674530029,
      "learning_rate": 2.9397590361445788e-05,
      "loss": 0.4585,
      "step": 1067
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.12454808503389359,
      "learning_rate": 2.923694779116466e-05,
      "loss": 0.5239,
      "step": 1068
    },
    {
      "epoch": 0.8552,
      "grad_norm": 0.11059734970331192,
      "learning_rate": 2.9076305220883538e-05,
      "loss": 0.4844,
      "step": 1069
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.09529013931751251,
      "learning_rate": 2.891566265060241e-05,
      "loss": 0.4765,
      "step": 1070
    },
    {
      "epoch": 0.8568,
      "grad_norm": 0.10866256803274155,
      "learning_rate": 2.8755020080321288e-05,
      "loss": 0.5201,
      "step": 1071
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.08724898844957352,
      "learning_rate": 2.859437751004016e-05,
      "loss": 0.4078,
      "step": 1072
    },
    {
      "epoch": 0.8584,
      "grad_norm": 0.08777564018964767,
      "learning_rate": 2.8433734939759038e-05,
      "loss": 0.4673,
      "step": 1073
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.09868834912776947,
      "learning_rate": 2.827309236947791e-05,
      "loss": 0.5163,
      "step": 1074
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.09517238289117813,
      "learning_rate": 2.8112449799196788e-05,
      "loss": 0.483,
      "step": 1075
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.1086425930261612,
      "learning_rate": 2.7951807228915666e-05,
      "loss": 0.52,
      "step": 1076
    },
    {
      "epoch": 0.8616,
      "grad_norm": 0.12044834345579147,
      "learning_rate": 2.7791164658634538e-05,
      "loss": 0.4757,
      "step": 1077
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.11596739292144775,
      "learning_rate": 2.7630522088353417e-05,
      "loss": 0.4685,
      "step": 1078
    },
    {
      "epoch": 0.8632,
      "grad_norm": 0.0868334099650383,
      "learning_rate": 2.7469879518072288e-05,
      "loss": 0.4512,
      "step": 1079
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.11595134437084198,
      "learning_rate": 2.7309236947791167e-05,
      "loss": 0.4416,
      "step": 1080
    },
    {
      "epoch": 0.8648,
      "grad_norm": 0.10882436484098434,
      "learning_rate": 2.7148594377510038e-05,
      "loss": 0.4518,
      "step": 1081
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.10659284889698029,
      "learning_rate": 2.6987951807228917e-05,
      "loss": 0.4744,
      "step": 1082
    },
    {
      "epoch": 0.8664,
      "grad_norm": 0.09178513288497925,
      "learning_rate": 2.6827309236947795e-05,
      "loss": 0.4382,
      "step": 1083
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.09605992585420609,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.4943,
      "step": 1084
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.09744032472372055,
      "learning_rate": 2.6506024096385545e-05,
      "loss": 0.5407,
      "step": 1085
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.10100479423999786,
      "learning_rate": 2.6345381526104417e-05,
      "loss": 0.498,
      "step": 1086
    },
    {
      "epoch": 0.8696,
      "grad_norm": 0.14917708933353424,
      "learning_rate": 2.6184738955823295e-05,
      "loss": 0.5214,
      "step": 1087
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.09218742698431015,
      "learning_rate": 2.6024096385542167e-05,
      "loss": 0.4429,
      "step": 1088
    },
    {
      "epoch": 0.8712,
      "grad_norm": 0.10140866786241531,
      "learning_rate": 2.5863453815261045e-05,
      "loss": 0.5221,
      "step": 1089
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.1162266731262207,
      "learning_rate": 2.570281124497992e-05,
      "loss": 0.5513,
      "step": 1090
    },
    {
      "epoch": 0.8728,
      "grad_norm": 0.10808083415031433,
      "learning_rate": 2.5542168674698795e-05,
      "loss": 0.5157,
      "step": 1091
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.09586666524410248,
      "learning_rate": 2.5381526104417673e-05,
      "loss": 0.4685,
      "step": 1092
    },
    {
      "epoch": 0.8744,
      "grad_norm": 0.10095211863517761,
      "learning_rate": 2.522088353413655e-05,
      "loss": 0.4618,
      "step": 1093
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.13410988450050354,
      "learning_rate": 2.5060240963855423e-05,
      "loss": 0.5033,
      "step": 1094
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.09850195050239563,
      "learning_rate": 2.48995983935743e-05,
      "loss": 0.4674,
      "step": 1095
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.09220024943351746,
      "learning_rate": 2.4738955823293174e-05,
      "loss": 0.399,
      "step": 1096
    },
    {
      "epoch": 0.8776,
      "grad_norm": 0.10344909131526947,
      "learning_rate": 2.4578313253012052e-05,
      "loss": 0.429,
      "step": 1097
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.10639268159866333,
      "learning_rate": 2.4417670682730927e-05,
      "loss": 0.4339,
      "step": 1098
    },
    {
      "epoch": 0.8792,
      "grad_norm": 0.10870487242937088,
      "learning_rate": 2.4257028112449802e-05,
      "loss": 0.4755,
      "step": 1099
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.1263085901737213,
      "learning_rate": 2.4096385542168677e-05,
      "loss": 0.4489,
      "step": 1100
    },
    {
      "epoch": 0.8808,
      "grad_norm": 0.08934838324785233,
      "learning_rate": 2.3935742971887552e-05,
      "loss": 0.439,
      "step": 1101
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.11251365393400192,
      "learning_rate": 2.3775100401606427e-05,
      "loss": 0.4672,
      "step": 1102
    },
    {
      "epoch": 0.8824,
      "grad_norm": 0.08944893628358841,
      "learning_rate": 2.3614457831325302e-05,
      "loss": 0.4456,
      "step": 1103
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.12118861079216003,
      "learning_rate": 2.345381526104418e-05,
      "loss": 0.5159,
      "step": 1104
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.10350760072469711,
      "learning_rate": 2.3293172690763055e-05,
      "loss": 0.5149,
      "step": 1105
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.10679002106189728,
      "learning_rate": 2.313253012048193e-05,
      "loss": 0.4386,
      "step": 1106
    },
    {
      "epoch": 0.8856,
      "grad_norm": 0.11941543221473694,
      "learning_rate": 2.2971887550200805e-05,
      "loss": 0.4712,
      "step": 1107
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.09330563992261887,
      "learning_rate": 2.281124497991968e-05,
      "loss": 0.4561,
      "step": 1108
    },
    {
      "epoch": 0.8872,
      "grad_norm": 0.08860667794942856,
      "learning_rate": 2.2650602409638555e-05,
      "loss": 0.4529,
      "step": 1109
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.09562720358371735,
      "learning_rate": 2.248995983935743e-05,
      "loss": 0.4936,
      "step": 1110
    },
    {
      "epoch": 0.8888,
      "grad_norm": 0.09330959618091583,
      "learning_rate": 2.2329317269076305e-05,
      "loss": 0.523,
      "step": 1111
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.10642929375171661,
      "learning_rate": 2.216867469879518e-05,
      "loss": 0.4245,
      "step": 1112
    },
    {
      "epoch": 0.8904,
      "grad_norm": 0.08460406213998795,
      "learning_rate": 2.200803212851406e-05,
      "loss": 0.3931,
      "step": 1113
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.11736011505126953,
      "learning_rate": 2.1847389558232934e-05,
      "loss": 0.5083,
      "step": 1114
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.08330941200256348,
      "learning_rate": 2.168674698795181e-05,
      "loss": 0.4251,
      "step": 1115
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.09388340264558792,
      "learning_rate": 2.1526104417670684e-05,
      "loss": 0.4922,
      "step": 1116
    },
    {
      "epoch": 0.8936,
      "grad_norm": 0.08762329816818237,
      "learning_rate": 2.136546184738956e-05,
      "loss": 0.4282,
      "step": 1117
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.09312658756971359,
      "learning_rate": 2.1204819277108434e-05,
      "loss": 0.3966,
      "step": 1118
    },
    {
      "epoch": 0.8952,
      "grad_norm": 0.11492723971605301,
      "learning_rate": 2.104417670682731e-05,
      "loss": 0.4997,
      "step": 1119
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.10173816233873367,
      "learning_rate": 2.0883534136546184e-05,
      "loss": 0.486,
      "step": 1120
    },
    {
      "epoch": 0.8968,
      "grad_norm": 0.1189253106713295,
      "learning_rate": 2.0722891566265062e-05,
      "loss": 0.484,
      "step": 1121
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.11448600143194199,
      "learning_rate": 2.0562248995983937e-05,
      "loss": 0.4398,
      "step": 1122
    },
    {
      "epoch": 0.8984,
      "grad_norm": 0.10118219256401062,
      "learning_rate": 2.0401606425702812e-05,
      "loss": 0.4378,
      "step": 1123
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.11702583730220795,
      "learning_rate": 2.0240963855421687e-05,
      "loss": 0.5445,
      "step": 1124
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.1240898072719574,
      "learning_rate": 2.0080321285140562e-05,
      "loss": 0.5301,
      "step": 1125
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.0959317609667778,
      "learning_rate": 1.9919678714859437e-05,
      "loss": 0.4354,
      "step": 1126
    },
    {
      "epoch": 0.9016,
      "grad_norm": 0.11400824785232544,
      "learning_rate": 1.9759036144578312e-05,
      "loss": 0.4604,
      "step": 1127
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.11130241304636002,
      "learning_rate": 1.9598393574297187e-05,
      "loss": 0.4977,
      "step": 1128
    },
    {
      "epoch": 0.9032,
      "grad_norm": 0.13118837773799896,
      "learning_rate": 1.9437751004016066e-05,
      "loss": 0.4356,
      "step": 1129
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.11152736842632294,
      "learning_rate": 1.927710843373494e-05,
      "loss": 0.4966,
      "step": 1130
    },
    {
      "epoch": 0.9048,
      "grad_norm": 0.09368376433849335,
      "learning_rate": 1.9116465863453816e-05,
      "loss": 0.4043,
      "step": 1131
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.10420935600996017,
      "learning_rate": 1.895582329317269e-05,
      "loss": 0.4744,
      "step": 1132
    },
    {
      "epoch": 0.9064,
      "grad_norm": 0.11778143048286438,
      "learning_rate": 1.8795180722891566e-05,
      "loss": 0.5025,
      "step": 1133
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.08891104906797409,
      "learning_rate": 1.863453815261044e-05,
      "loss": 0.4791,
      "step": 1134
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.11586245149374008,
      "learning_rate": 1.8473895582329316e-05,
      "loss": 0.5222,
      "step": 1135
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.09088396281003952,
      "learning_rate": 1.8313253012048194e-05,
      "loss": 0.505,
      "step": 1136
    },
    {
      "epoch": 0.9096,
      "grad_norm": 0.11045239865779877,
      "learning_rate": 1.815261044176707e-05,
      "loss": 0.5105,
      "step": 1137
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.08246825635433197,
      "learning_rate": 1.7991967871485944e-05,
      "loss": 0.4624,
      "step": 1138
    },
    {
      "epoch": 0.9112,
      "grad_norm": 0.09297589957714081,
      "learning_rate": 1.783132530120482e-05,
      "loss": 0.4316,
      "step": 1139
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.10250982642173767,
      "learning_rate": 1.7670682730923694e-05,
      "loss": 0.4715,
      "step": 1140
    },
    {
      "epoch": 0.9128,
      "grad_norm": 0.09867683053016663,
      "learning_rate": 1.7510040160642573e-05,
      "loss": 0.4755,
      "step": 1141
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.10736393928527832,
      "learning_rate": 1.7349397590361448e-05,
      "loss": 0.5032,
      "step": 1142
    },
    {
      "epoch": 0.9144,
      "grad_norm": 0.12723855674266815,
      "learning_rate": 1.7188755020080323e-05,
      "loss": 0.4852,
      "step": 1143
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.1089639663696289,
      "learning_rate": 1.7028112449799198e-05,
      "loss": 0.4735,
      "step": 1144
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.08377136290073395,
      "learning_rate": 1.6867469879518073e-05,
      "loss": 0.4272,
      "step": 1145
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.07751896232366562,
      "learning_rate": 1.670682730923695e-05,
      "loss": 0.4426,
      "step": 1146
    },
    {
      "epoch": 0.9176,
      "grad_norm": 0.10497474670410156,
      "learning_rate": 1.6546184738955826e-05,
      "loss": 0.4132,
      "step": 1147
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.1186072826385498,
      "learning_rate": 1.63855421686747e-05,
      "loss": 0.5257,
      "step": 1148
    },
    {
      "epoch": 0.9192,
      "grad_norm": 0.11740414798259735,
      "learning_rate": 1.6224899598393576e-05,
      "loss": 0.5112,
      "step": 1149
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.10114149749279022,
      "learning_rate": 1.606425702811245e-05,
      "loss": 0.4567,
      "step": 1150
    },
    {
      "epoch": 0.9208,
      "grad_norm": 0.09690358489751816,
      "learning_rate": 1.5903614457831326e-05,
      "loss": 0.4907,
      "step": 1151
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.07918356359004974,
      "learning_rate": 1.57429718875502e-05,
      "loss": 0.4072,
      "step": 1152
    },
    {
      "epoch": 0.9224,
      "grad_norm": 0.08889159560203552,
      "learning_rate": 1.5582329317269076e-05,
      "loss": 0.4596,
      "step": 1153
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.09478002786636353,
      "learning_rate": 1.5421686746987955e-05,
      "loss": 0.4533,
      "step": 1154
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.08158540725708008,
      "learning_rate": 1.526104417670683e-05,
      "loss": 0.3747,
      "step": 1155
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.09768542647361755,
      "learning_rate": 1.5100401606425705e-05,
      "loss": 0.491,
      "step": 1156
    },
    {
      "epoch": 0.9256,
      "grad_norm": 0.08431423455476761,
      "learning_rate": 1.493975903614458e-05,
      "loss": 0.4203,
      "step": 1157
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.09468920528888702,
      "learning_rate": 1.4779116465863455e-05,
      "loss": 0.5143,
      "step": 1158
    },
    {
      "epoch": 0.9272,
      "grad_norm": 0.09542076289653778,
      "learning_rate": 1.461847389558233e-05,
      "loss": 0.4513,
      "step": 1159
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.10361167043447495,
      "learning_rate": 1.4457831325301205e-05,
      "loss": 0.5017,
      "step": 1160
    },
    {
      "epoch": 0.9288,
      "grad_norm": 0.09515765309333801,
      "learning_rate": 1.429718875502008e-05,
      "loss": 0.4624,
      "step": 1161
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.09591539204120636,
      "learning_rate": 1.4136546184738955e-05,
      "loss": 0.5078,
      "step": 1162
    },
    {
      "epoch": 0.9304,
      "grad_norm": 0.08453378081321716,
      "learning_rate": 1.3975903614457833e-05,
      "loss": 0.4308,
      "step": 1163
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.10483545809984207,
      "learning_rate": 1.3815261044176708e-05,
      "loss": 0.4807,
      "step": 1164
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.10504370927810669,
      "learning_rate": 1.3654618473895583e-05,
      "loss": 0.499,
      "step": 1165
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.10199170559644699,
      "learning_rate": 1.3493975903614458e-05,
      "loss": 0.496,
      "step": 1166
    },
    {
      "epoch": 0.9336,
      "grad_norm": 0.10032965242862701,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.5065,
      "step": 1167
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.08971337229013443,
      "learning_rate": 1.3172690763052208e-05,
      "loss": 0.3775,
      "step": 1168
    },
    {
      "epoch": 0.9352,
      "grad_norm": 0.09121722728013992,
      "learning_rate": 1.3012048192771083e-05,
      "loss": 0.4169,
      "step": 1169
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.09005043655633926,
      "learning_rate": 1.285140562248996e-05,
      "loss": 0.4507,
      "step": 1170
    },
    {
      "epoch": 0.9368,
      "grad_norm": 0.09041760861873627,
      "learning_rate": 1.2690763052208837e-05,
      "loss": 0.4594,
      "step": 1171
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.12372351437807083,
      "learning_rate": 1.2530120481927712e-05,
      "loss": 0.4539,
      "step": 1172
    },
    {
      "epoch": 0.9384,
      "grad_norm": 0.09779787063598633,
      "learning_rate": 1.2369477911646587e-05,
      "loss": 0.4718,
      "step": 1173
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.10386418551206589,
      "learning_rate": 1.2208835341365463e-05,
      "loss": 0.4883,
      "step": 1174
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.10511823743581772,
      "learning_rate": 1.2048192771084338e-05,
      "loss": 0.5274,
      "step": 1175
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.08755577355623245,
      "learning_rate": 1.1887550200803213e-05,
      "loss": 0.4458,
      "step": 1176
    },
    {
      "epoch": 0.9416,
      "grad_norm": 0.09984283894300461,
      "learning_rate": 1.172690763052209e-05,
      "loss": 0.4829,
      "step": 1177
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.09423269331455231,
      "learning_rate": 1.1566265060240965e-05,
      "loss": 0.4692,
      "step": 1178
    },
    {
      "epoch": 0.9432,
      "grad_norm": 0.0782926008105278,
      "learning_rate": 1.140562248995984e-05,
      "loss": 0.423,
      "step": 1179
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.11008154600858688,
      "learning_rate": 1.1244979919678715e-05,
      "loss": 0.46,
      "step": 1180
    },
    {
      "epoch": 0.9448,
      "grad_norm": 0.10485043376684189,
      "learning_rate": 1.108433734939759e-05,
      "loss": 0.4863,
      "step": 1181
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.11074686795473099,
      "learning_rate": 1.0923694779116467e-05,
      "loss": 0.4713,
      "step": 1182
    },
    {
      "epoch": 0.9464,
      "grad_norm": 0.09465394914150238,
      "learning_rate": 1.0763052208835342e-05,
      "loss": 0.4476,
      "step": 1183
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.096649169921875,
      "learning_rate": 1.0602409638554217e-05,
      "loss": 0.4792,
      "step": 1184
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.1205161064863205,
      "learning_rate": 1.0441767068273092e-05,
      "loss": 0.478,
      "step": 1185
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.11481805890798569,
      "learning_rate": 1.0281124497991969e-05,
      "loss": 0.5868,
      "step": 1186
    },
    {
      "epoch": 0.9496,
      "grad_norm": 0.1958872228860855,
      "learning_rate": 1.0120481927710844e-05,
      "loss": 0.4832,
      "step": 1187
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.11686497926712036,
      "learning_rate": 9.959839357429719e-06,
      "loss": 0.526,
      "step": 1188
    },
    {
      "epoch": 0.9512,
      "grad_norm": 0.11084715276956558,
      "learning_rate": 9.799196787148594e-06,
      "loss": 0.4939,
      "step": 1189
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.09418485313653946,
      "learning_rate": 9.63855421686747e-06,
      "loss": 0.4518,
      "step": 1190
    },
    {
      "epoch": 0.9528,
      "grad_norm": 0.09397059679031372,
      "learning_rate": 9.477911646586345e-06,
      "loss": 0.4564,
      "step": 1191
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.08589055389165878,
      "learning_rate": 9.31726907630522e-06,
      "loss": 0.4056,
      "step": 1192
    },
    {
      "epoch": 0.9544,
      "grad_norm": 0.08637724071741104,
      "learning_rate": 9.156626506024097e-06,
      "loss": 0.4274,
      "step": 1193
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.1036268025636673,
      "learning_rate": 8.995983935742972e-06,
      "loss": 0.4428,
      "step": 1194
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.09258488565683365,
      "learning_rate": 8.835341365461847e-06,
      "loss": 0.4894,
      "step": 1195
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.13165803253650665,
      "learning_rate": 8.674698795180724e-06,
      "loss": 0.5371,
      "step": 1196
    },
    {
      "epoch": 0.9576,
      "grad_norm": 0.11899255216121674,
      "learning_rate": 8.514056224899599e-06,
      "loss": 0.4765,
      "step": 1197
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.08967021852731705,
      "learning_rate": 8.353413654618476e-06,
      "loss": 0.4346,
      "step": 1198
    },
    {
      "epoch": 0.9592,
      "grad_norm": 0.09038665890693665,
      "learning_rate": 8.19277108433735e-06,
      "loss": 0.4608,
      "step": 1199
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.08473063260316849,
      "learning_rate": 8.032128514056226e-06,
      "loss": 0.4369,
      "step": 1200
    },
    {
      "epoch": 0.9608,
      "grad_norm": 0.08458661288022995,
      "learning_rate": 7.8714859437751e-06,
      "loss": 0.4586,
      "step": 1201
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.08869723230600357,
      "learning_rate": 7.710843373493977e-06,
      "loss": 0.4696,
      "step": 1202
    },
    {
      "epoch": 0.9624,
      "grad_norm": 0.09669504314661026,
      "learning_rate": 7.550200803212852e-06,
      "loss": 0.5107,
      "step": 1203
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.10444914549589157,
      "learning_rate": 7.389558232931727e-06,
      "loss": 0.4207,
      "step": 1204
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.08861453831195831,
      "learning_rate": 7.228915662650602e-06,
      "loss": 0.441,
      "step": 1205
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.14013522863388062,
      "learning_rate": 7.068273092369477e-06,
      "loss": 0.4683,
      "step": 1206
    },
    {
      "epoch": 0.9656,
      "grad_norm": 0.10225966572761536,
      "learning_rate": 6.907630522088354e-06,
      "loss": 0.4127,
      "step": 1207
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.10383506119251251,
      "learning_rate": 6.746987951807229e-06,
      "loss": 0.4961,
      "step": 1208
    },
    {
      "epoch": 0.9672,
      "grad_norm": 0.09871435910463333,
      "learning_rate": 6.586345381526104e-06,
      "loss": 0.4595,
      "step": 1209
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.09359748661518097,
      "learning_rate": 6.42570281124498e-06,
      "loss": 0.496,
      "step": 1210
    },
    {
      "epoch": 0.9688,
      "grad_norm": 0.08304426074028015,
      "learning_rate": 6.265060240963856e-06,
      "loss": 0.4154,
      "step": 1211
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.08748660981655121,
      "learning_rate": 6.104417670682732e-06,
      "loss": 0.4773,
      "step": 1212
    },
    {
      "epoch": 0.9704,
      "grad_norm": 0.07945548743009567,
      "learning_rate": 5.943775100401607e-06,
      "loss": 0.4583,
      "step": 1213
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.09299913793802261,
      "learning_rate": 5.783132530120483e-06,
      "loss": 0.4938,
      "step": 1214
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.11577487736940384,
      "learning_rate": 5.622489959839358e-06,
      "loss": 0.5401,
      "step": 1215
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.09529592096805573,
      "learning_rate": 5.4618473895582335e-06,
      "loss": 0.447,
      "step": 1216
    },
    {
      "epoch": 0.9736,
      "grad_norm": 0.09290872514247894,
      "learning_rate": 5.3012048192771085e-06,
      "loss": 0.4662,
      "step": 1217
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.08651406317949295,
      "learning_rate": 5.140562248995984e-06,
      "loss": 0.4696,
      "step": 1218
    },
    {
      "epoch": 0.9752,
      "grad_norm": 0.12730376422405243,
      "learning_rate": 4.979919678714859e-06,
      "loss": 0.5274,
      "step": 1219
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.10566023737192154,
      "learning_rate": 4.819277108433735e-06,
      "loss": 0.5422,
      "step": 1220
    },
    {
      "epoch": 0.9768,
      "grad_norm": 0.09596779942512512,
      "learning_rate": 4.65863453815261e-06,
      "loss": 0.5097,
      "step": 1221
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.10681507736444473,
      "learning_rate": 4.497991967871486e-06,
      "loss": 0.5234,
      "step": 1222
    },
    {
      "epoch": 0.9784,
      "grad_norm": 0.10094795376062393,
      "learning_rate": 4.337349397590362e-06,
      "loss": 0.4358,
      "step": 1223
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.09294305741786957,
      "learning_rate": 4.176706827309238e-06,
      "loss": 0.4653,
      "step": 1224
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.10395725071430206,
      "learning_rate": 4.016064257028113e-06,
      "loss": 0.4738,
      "step": 1225
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.07795455306768417,
      "learning_rate": 3.855421686746989e-06,
      "loss": 0.4194,
      "step": 1226
    },
    {
      "epoch": 0.9816,
      "grad_norm": 0.0792357325553894,
      "learning_rate": 3.6947791164658637e-06,
      "loss": 0.4289,
      "step": 1227
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.08935386687517166,
      "learning_rate": 3.5341365461847387e-06,
      "loss": 0.4532,
      "step": 1228
    },
    {
      "epoch": 0.9832,
      "grad_norm": 0.09914591163396835,
      "learning_rate": 3.3734939759036146e-06,
      "loss": 0.4432,
      "step": 1229
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.09273529797792435,
      "learning_rate": 3.21285140562249e-06,
      "loss": 0.4496,
      "step": 1230
    },
    {
      "epoch": 0.9848,
      "grad_norm": 0.09988696873188019,
      "learning_rate": 3.052208835341366e-06,
      "loss": 0.5082,
      "step": 1231
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.09469505399465561,
      "learning_rate": 2.8915662650602413e-06,
      "loss": 0.3998,
      "step": 1232
    },
    {
      "epoch": 0.9864,
      "grad_norm": 0.10762834548950195,
      "learning_rate": 2.7309236947791167e-06,
      "loss": 0.4612,
      "step": 1233
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.10447124391794205,
      "learning_rate": 2.570281124497992e-06,
      "loss": 0.535,
      "step": 1234
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.08833802491426468,
      "learning_rate": 2.4096385542168676e-06,
      "loss": 0.4619,
      "step": 1235
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.09641260653734207,
      "learning_rate": 2.248995983935743e-06,
      "loss": 0.5076,
      "step": 1236
    },
    {
      "epoch": 0.9896,
      "grad_norm": 0.10115733742713928,
      "learning_rate": 2.088353413654619e-06,
      "loss": 0.4495,
      "step": 1237
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.10822299867868423,
      "learning_rate": 1.9277108433734943e-06,
      "loss": 0.4637,
      "step": 1238
    },
    {
      "epoch": 0.9912,
      "grad_norm": 0.10169185698032379,
      "learning_rate": 1.7670682730923694e-06,
      "loss": 0.4777,
      "step": 1239
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.09713444858789444,
      "learning_rate": 1.606425702811245e-06,
      "loss": 0.4103,
      "step": 1240
    },
    {
      "epoch": 0.9928,
      "grad_norm": 0.10732577741146088,
      "learning_rate": 1.4457831325301207e-06,
      "loss": 0.5297,
      "step": 1241
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.10939237475395203,
      "learning_rate": 1.285140562248996e-06,
      "loss": 0.4677,
      "step": 1242
    },
    {
      "epoch": 0.9944,
      "grad_norm": 0.0977337434887886,
      "learning_rate": 1.1244979919678715e-06,
      "loss": 0.4387,
      "step": 1243
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.10719236731529236,
      "learning_rate": 9.638554216867472e-07,
      "loss": 0.4829,
      "step": 1244
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.09930461645126343,
      "learning_rate": 8.032128514056225e-07,
      "loss": 0.45,
      "step": 1245
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.09905656427145004,
      "learning_rate": 6.42570281124498e-07,
      "loss": 0.4882,
      "step": 1246
    },
    {
      "epoch": 0.9976,
      "grad_norm": 0.0899401307106018,
      "learning_rate": 4.819277108433736e-07,
      "loss": 0.3668,
      "step": 1247
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.08985860645771027,
      "learning_rate": 3.21285140562249e-07,
      "loss": 0.434,
      "step": 1248
    },
    {
      "epoch": 0.9992,
      "grad_norm": 0.09393785148859024,
      "learning_rate": 1.606425702811245e-07,
      "loss": 0.4159,
      "step": 1249
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.09254223108291626,
      "learning_rate": 0.0,
      "loss": 0.3923,
      "step": 1250
    }
  ],
  "logging_steps": 1,
  "max_steps": 1250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.345804000762921e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
