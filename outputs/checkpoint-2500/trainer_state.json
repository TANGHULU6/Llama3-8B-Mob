{
  "best_metric": 0.3183744251728058,
  "best_model_checkpoint": "outputs/checkpoint-1800",
  "epoch": 4.166666666666667,
  "eval_steps": 100,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 2.0955612659454346,
      "learning_rate": 4e-05,
      "loss": 1.9448,
      "step": 1
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 2.291522264480591,
      "learning_rate": 8e-05,
      "loss": 1.8955,
      "step": 2
    },
    {
      "epoch": 0.005,
      "grad_norm": 2.3751931190490723,
      "learning_rate": 0.00012,
      "loss": 1.9633,
      "step": 3
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.9432048797607422,
      "learning_rate": 0.00016,
      "loss": 1.7703,
      "step": 4
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.4780776500701904,
      "learning_rate": 0.0002,
      "loss": 1.5581,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.2533655166625977,
      "learning_rate": 0.00019988857938718665,
      "loss": 1.3527,
      "step": 6
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 1.636291265487671,
      "learning_rate": 0.00019977715877437326,
      "loss": 1.0732,
      "step": 7
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.666510820388794,
      "learning_rate": 0.0001996657381615599,
      "loss": 1.2106,
      "step": 8
    },
    {
      "epoch": 0.015,
      "grad_norm": 1.6698962450027466,
      "learning_rate": 0.00019955431754874653,
      "loss": 0.8514,
      "step": 9
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 2.109370708465576,
      "learning_rate": 0.00019944289693593316,
      "loss": 0.9609,
      "step": 10
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 1.8895906209945679,
      "learning_rate": 0.0001993314763231198,
      "loss": 0.6892,
      "step": 11
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8486143946647644,
      "learning_rate": 0.0001992200557103064,
      "loss": 0.4873,
      "step": 12
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.9126704931259155,
      "learning_rate": 0.00019910863509749305,
      "loss": 0.5162,
      "step": 13
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.47595322132110596,
      "learning_rate": 0.00019899721448467968,
      "loss": 0.6428,
      "step": 14
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.41951069235801697,
      "learning_rate": 0.0001988857938718663,
      "loss": 0.4553,
      "step": 15
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.49427658319473267,
      "learning_rate": 0.00019877437325905293,
      "loss": 0.6551,
      "step": 16
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.2967330813407898,
      "learning_rate": 0.00019866295264623957,
      "loss": 0.4602,
      "step": 17
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.2054484486579895,
      "learning_rate": 0.0001985515320334262,
      "loss": 0.4395,
      "step": 18
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.28449466824531555,
      "learning_rate": 0.00019844011142061284,
      "loss": 0.4793,
      "step": 19
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.34066563844680786,
      "learning_rate": 0.00019832869080779945,
      "loss": 0.5514,
      "step": 20
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.2806191146373749,
      "learning_rate": 0.00019821727019498608,
      "loss": 0.5207,
      "step": 21
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.1904837042093277,
      "learning_rate": 0.00019810584958217272,
      "loss": 0.4974,
      "step": 22
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.16037176549434662,
      "learning_rate": 0.00019799442896935933,
      "loss": 0.393,
      "step": 23
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.163355752825737,
      "learning_rate": 0.00019788300835654597,
      "loss": 0.4312,
      "step": 24
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.1961064487695694,
      "learning_rate": 0.00019777158774373258,
      "loss": 0.4412,
      "step": 25
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.19962693750858307,
      "learning_rate": 0.00019766016713091924,
      "loss": 0.5202,
      "step": 26
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.1513756513595581,
      "learning_rate": 0.00019754874651810588,
      "loss": 0.4442,
      "step": 27
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.1553286612033844,
      "learning_rate": 0.00019743732590529249,
      "loss": 0.4703,
      "step": 28
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.12882000207901,
      "learning_rate": 0.00019732590529247912,
      "loss": 0.3949,
      "step": 29
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15362459421157837,
      "learning_rate": 0.00019721448467966573,
      "loss": 0.4067,
      "step": 30
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.16636455059051514,
      "learning_rate": 0.00019710306406685237,
      "loss": 0.4751,
      "step": 31
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.10700325667858124,
      "learning_rate": 0.000196991643454039,
      "loss": 0.4901,
      "step": 32
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.10109906643629074,
      "learning_rate": 0.00019688022284122561,
      "loss": 0.2806,
      "step": 33
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.15828914940357208,
      "learning_rate": 0.00019676880222841228,
      "loss": 0.4163,
      "step": 34
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.1429501175880432,
      "learning_rate": 0.00019665738161559891,
      "loss": 0.3946,
      "step": 35
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.11291559785604477,
      "learning_rate": 0.00019654596100278552,
      "loss": 0.4201,
      "step": 36
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.11685312539339066,
      "learning_rate": 0.00019643454038997216,
      "loss": 0.3055,
      "step": 37
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.11478576809167862,
      "learning_rate": 0.00019632311977715877,
      "loss": 0.3478,
      "step": 38
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.11220988631248474,
      "learning_rate": 0.0001962116991643454,
      "loss": 0.3454,
      "step": 39
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.10664703696966171,
      "learning_rate": 0.00019610027855153204,
      "loss": 0.3251,
      "step": 40
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.10532573610544205,
      "learning_rate": 0.00019598885793871865,
      "loss": 0.3798,
      "step": 41
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.09253589063882828,
      "learning_rate": 0.00019587743732590532,
      "loss": 0.3477,
      "step": 42
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.10197603702545166,
      "learning_rate": 0.00019576601671309192,
      "loss": 0.3672,
      "step": 43
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.09146757423877716,
      "learning_rate": 0.00019565459610027856,
      "loss": 0.4197,
      "step": 44
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12180428951978683,
      "learning_rate": 0.0001955431754874652,
      "loss": 0.4203,
      "step": 45
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.11242137849330902,
      "learning_rate": 0.0001954317548746518,
      "loss": 0.3167,
      "step": 46
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.0971837043762207,
      "learning_rate": 0.00019532033426183844,
      "loss": 0.3995,
      "step": 47
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08444882929325104,
      "learning_rate": 0.00019520891364902508,
      "loss": 0.2908,
      "step": 48
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.09193655103445053,
      "learning_rate": 0.0001950974930362117,
      "loss": 0.3851,
      "step": 49
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.10707516968250275,
      "learning_rate": 0.00019498607242339835,
      "loss": 0.3579,
      "step": 50
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.08961743116378784,
      "learning_rate": 0.00019487465181058496,
      "loss": 0.3716,
      "step": 51
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.09596701711416245,
      "learning_rate": 0.0001947632311977716,
      "loss": 0.415,
      "step": 52
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.06666703522205353,
      "learning_rate": 0.00019465181058495824,
      "loss": 0.3567,
      "step": 53
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.11535973846912384,
      "learning_rate": 0.00019454038997214484,
      "loss": 0.4295,
      "step": 54
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.08375324308872223,
      "learning_rate": 0.00019442896935933148,
      "loss": 0.3019,
      "step": 55
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.10852738469839096,
      "learning_rate": 0.00019431754874651812,
      "loss": 0.3493,
      "step": 56
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.10233605653047562,
      "learning_rate": 0.00019420612813370473,
      "loss": 0.3831,
      "step": 57
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.13586613535881042,
      "learning_rate": 0.0001940947075208914,
      "loss": 0.4087,
      "step": 58
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.07681457698345184,
      "learning_rate": 0.000193983286908078,
      "loss": 0.3024,
      "step": 59
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.09336651116609573,
      "learning_rate": 0.00019387186629526464,
      "loss": 0.3808,
      "step": 60
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.06770402193069458,
      "learning_rate": 0.00019376044568245127,
      "loss": 0.2627,
      "step": 61
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.07679001986980438,
      "learning_rate": 0.00019364902506963788,
      "loss": 0.3938,
      "step": 62
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06754700839519501,
      "learning_rate": 0.00019353760445682452,
      "loss": 0.3716,
      "step": 63
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07708554714918137,
      "learning_rate": 0.00019342618384401116,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.08859172463417053,
      "learning_rate": 0.00019331476323119776,
      "loss": 0.3443,
      "step": 65
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.10104084014892578,
      "learning_rate": 0.00019320334261838443,
      "loss": 0.3372,
      "step": 66
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.07533049583435059,
      "learning_rate": 0.00019309192200557104,
      "loss": 0.3046,
      "step": 67
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.08598867803812027,
      "learning_rate": 0.00019298050139275767,
      "loss": 0.2944,
      "step": 68
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.12070752680301666,
      "learning_rate": 0.0001928690807799443,
      "loss": 0.3952,
      "step": 69
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.10526693612337112,
      "learning_rate": 0.00019275766016713092,
      "loss": 0.379,
      "step": 70
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.0892651155591011,
      "learning_rate": 0.00019264623955431756,
      "loss": 0.3193,
      "step": 71
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07309211045503616,
      "learning_rate": 0.0001925348189415042,
      "loss": 0.3395,
      "step": 72
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.06044447422027588,
      "learning_rate": 0.0001924233983286908,
      "loss": 0.2453,
      "step": 73
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.16050706803798676,
      "learning_rate": 0.00019231197771587747,
      "loss": 0.3868,
      "step": 74
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.07688120752573013,
      "learning_rate": 0.00019220055710306408,
      "loss": 0.325,
      "step": 75
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.17312116920948029,
      "learning_rate": 0.0001920891364902507,
      "loss": 0.3973,
      "step": 76
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.09058317542076111,
      "learning_rate": 0.00019197771587743735,
      "loss": 0.3466,
      "step": 77
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.08493395149707794,
      "learning_rate": 0.00019186629526462396,
      "loss": 0.3251,
      "step": 78
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.11874042451381683,
      "learning_rate": 0.0001917548746518106,
      "loss": 0.3859,
      "step": 79
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08727477490901947,
      "learning_rate": 0.0001916434540389972,
      "loss": 0.3329,
      "step": 80
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.08828488737344742,
      "learning_rate": 0.00019153203342618384,
      "loss": 0.3498,
      "step": 81
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.0929422378540039,
      "learning_rate": 0.0001914206128133705,
      "loss": 0.3477,
      "step": 82
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.06502547860145569,
      "learning_rate": 0.0001913091922005571,
      "loss": 0.2619,
      "step": 83
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0826447606086731,
      "learning_rate": 0.00019119777158774375,
      "loss": 0.375,
      "step": 84
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.06894782930612564,
      "learning_rate": 0.00019108635097493039,
      "loss": 0.384,
      "step": 85
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.08264435827732086,
      "learning_rate": 0.000190974930362117,
      "loss": 0.3475,
      "step": 86
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.08043172955513,
      "learning_rate": 0.00019086350974930363,
      "loss": 0.2602,
      "step": 87
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08756858110427856,
      "learning_rate": 0.00019075208913649024,
      "loss": 0.3371,
      "step": 88
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.07516686618328094,
      "learning_rate": 0.00019064066852367688,
      "loss": 0.3206,
      "step": 89
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06845497339963913,
      "learning_rate": 0.00019052924791086354,
      "loss": 0.2438,
      "step": 90
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.16434063017368317,
      "learning_rate": 0.00019041782729805015,
      "loss": 0.3756,
      "step": 91
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.08764452487230301,
      "learning_rate": 0.0001903064066852368,
      "loss": 0.364,
      "step": 92
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.09765373170375824,
      "learning_rate": 0.0001901949860724234,
      "loss": 0.336,
      "step": 93
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.12203431129455566,
      "learning_rate": 0.00019008356545961003,
      "loss": 0.3644,
      "step": 94
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.0851789191365242,
      "learning_rate": 0.00018997214484679667,
      "loss": 0.3209,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.0814284235239029,
      "learning_rate": 0.00018986072423398328,
      "loss": 0.3538,
      "step": 96
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.10511325299739838,
      "learning_rate": 0.00018974930362116992,
      "loss": 0.3208,
      "step": 97
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.06572487205266953,
      "learning_rate": 0.00018963788300835655,
      "loss": 0.3373,
      "step": 98
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.060700900852680206,
      "learning_rate": 0.0001895264623955432,
      "loss": 0.2413,
      "step": 99
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.09838373959064484,
      "learning_rate": 0.00018941504178272982,
      "loss": 0.4219,
      "step": 100
    },
    {
      "epoch": 0.16666666666666666,
      "eval_loss": 0.3487149178981781,
      "eval_runtime": 374.2756,
      "eval_samples_per_second": 0.267,
      "eval_steps_per_second": 0.267,
      "step": 100
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.07030830532312393,
      "learning_rate": 0.00018930362116991643,
      "loss": 0.2833,
      "step": 101
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.0894852876663208,
      "learning_rate": 0.00018919220055710307,
      "loss": 0.3532,
      "step": 102
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.08684878051280975,
      "learning_rate": 0.0001890807799442897,
      "loss": 0.346,
      "step": 103
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.06322134286165237,
      "learning_rate": 0.00018896935933147632,
      "loss": 0.2794,
      "step": 104
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.11456792056560516,
      "learning_rate": 0.00018885793871866295,
      "loss": 0.4254,
      "step": 105
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.08284913748502731,
      "learning_rate": 0.0001887465181058496,
      "loss": 0.3428,
      "step": 106
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.08064185827970505,
      "learning_rate": 0.00018863509749303623,
      "loss": 0.4691,
      "step": 107
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.07856238633394241,
      "learning_rate": 0.00018852367688022286,
      "loss": 0.402,
      "step": 108
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.11941109597682953,
      "learning_rate": 0.00018841225626740947,
      "loss": 0.3084,
      "step": 109
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.059156522154808044,
      "learning_rate": 0.0001883008356545961,
      "loss": 0.3936,
      "step": 110
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06889190524816513,
      "learning_rate": 0.00018818941504178274,
      "loss": 0.3664,
      "step": 111
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06391744315624237,
      "learning_rate": 0.00018807799442896935,
      "loss": 0.3049,
      "step": 112
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.07021020352840424,
      "learning_rate": 0.000187966573816156,
      "loss": 0.379,
      "step": 113
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.13152983784675598,
      "learning_rate": 0.00018785515320334263,
      "loss": 0.4045,
      "step": 114
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.08291923254728317,
      "learning_rate": 0.00018774373259052926,
      "loss": 0.3675,
      "step": 115
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.12651987373828888,
      "learning_rate": 0.0001876323119777159,
      "loss": 0.3512,
      "step": 116
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.06338682025671005,
      "learning_rate": 0.0001875208913649025,
      "loss": 0.3052,
      "step": 117
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.06231988966464996,
      "learning_rate": 0.00018740947075208915,
      "loss": 0.3496,
      "step": 118
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.092839315533638,
      "learning_rate": 0.00018729805013927578,
      "loss": 0.3263,
      "step": 119
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.05712943151593208,
      "learning_rate": 0.0001871866295264624,
      "loss": 0.2907,
      "step": 120
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.07494724541902542,
      "learning_rate": 0.00018707520891364903,
      "loss": 0.3746,
      "step": 121
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.06764266639947891,
      "learning_rate": 0.00018696378830083566,
      "loss": 0.34,
      "step": 122
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.05663382261991501,
      "learning_rate": 0.0001868523676880223,
      "loss": 0.2878,
      "step": 123
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.06405160576105118,
      "learning_rate": 0.00018674094707520894,
      "loss": 0.3435,
      "step": 124
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.07559706270694733,
      "learning_rate": 0.00018662952646239555,
      "loss": 0.3347,
      "step": 125
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.07337447255849838,
      "learning_rate": 0.00018651810584958218,
      "loss": 0.351,
      "step": 126
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.07447608560323715,
      "learning_rate": 0.00018640668523676882,
      "loss": 0.3073,
      "step": 127
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.06198855862021446,
      "learning_rate": 0.00018629526462395543,
      "loss": 0.2524,
      "step": 128
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.07246571034193039,
      "learning_rate": 0.00018618384401114207,
      "loss": 0.3869,
      "step": 129
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.09226162731647491,
      "learning_rate": 0.0001860724233983287,
      "loss": 0.4094,
      "step": 130
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09503162652254105,
      "learning_rate": 0.00018596100278551534,
      "loss": 0.4443,
      "step": 131
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.07748464494943619,
      "learning_rate": 0.00018584958217270198,
      "loss": 0.3511,
      "step": 132
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.05017251893877983,
      "learning_rate": 0.00018573816155988858,
      "loss": 0.295,
      "step": 133
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.05881548672914505,
      "learning_rate": 0.00018562674094707522,
      "loss": 0.3032,
      "step": 134
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.08307559788227081,
      "learning_rate": 0.00018551532033426183,
      "loss": 0.3153,
      "step": 135
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.05294783040881157,
      "learning_rate": 0.00018540389972144847,
      "loss": 0.299,
      "step": 136
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.08568192273378372,
      "learning_rate": 0.0001852924791086351,
      "loss": 0.3543,
      "step": 137
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.056592926383018494,
      "learning_rate": 0.00018518105849582174,
      "loss": 0.375,
      "step": 138
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.07708677649497986,
      "learning_rate": 0.00018506963788300838,
      "loss": 0.3557,
      "step": 139
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.08158115297555923,
      "learning_rate": 0.000184958217270195,
      "loss": 0.2725,
      "step": 140
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.0755927711725235,
      "learning_rate": 0.00018484679665738162,
      "loss": 0.3757,
      "step": 141
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.05458508059382439,
      "learning_rate": 0.00018473537604456826,
      "loss": 0.2738,
      "step": 142
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.06590596586465836,
      "learning_rate": 0.00018462395543175487,
      "loss": 0.3337,
      "step": 143
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.08819787949323654,
      "learning_rate": 0.0001845125348189415,
      "loss": 0.3993,
      "step": 144
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.07023905217647552,
      "learning_rate": 0.00018440111420612814,
      "loss": 0.3453,
      "step": 145
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.08446215838193893,
      "learning_rate": 0.00018428969359331478,
      "loss": 0.3801,
      "step": 146
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.08325417339801788,
      "learning_rate": 0.00018417827298050141,
      "loss": 0.3376,
      "step": 147
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.06384475529193878,
      "learning_rate": 0.00018406685236768802,
      "loss": 0.3087,
      "step": 148
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.09863974153995514,
      "learning_rate": 0.00018395543175487466,
      "loss": 0.3617,
      "step": 149
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.07659132778644562,
      "learning_rate": 0.0001838440111420613,
      "loss": 0.3664,
      "step": 150
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.07176845520734787,
      "learning_rate": 0.0001837325905292479,
      "loss": 0.3111,
      "step": 151
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.06705939024686813,
      "learning_rate": 0.00018362116991643454,
      "loss": 0.2932,
      "step": 152
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09360035508871078,
      "learning_rate": 0.00018350974930362118,
      "loss": 0.4418,
      "step": 153
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.08361534029245377,
      "learning_rate": 0.00018339832869080782,
      "loss": 0.361,
      "step": 154
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.07738277316093445,
      "learning_rate": 0.00018328690807799445,
      "loss": 0.3252,
      "step": 155
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.06970240920782089,
      "learning_rate": 0.00018317548746518106,
      "loss": 0.3677,
      "step": 156
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.07010390609502792,
      "learning_rate": 0.0001830640668523677,
      "loss": 0.296,
      "step": 157
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.05854480341076851,
      "learning_rate": 0.00018295264623955433,
      "loss": 0.3228,
      "step": 158
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.05452617257833481,
      "learning_rate": 0.00018284122562674094,
      "loss": 0.256,
      "step": 159
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.08169269561767578,
      "learning_rate": 0.00018272980501392758,
      "loss": 0.3414,
      "step": 160
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.06454888731241226,
      "learning_rate": 0.00018261838440111422,
      "loss": 0.3587,
      "step": 161
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.07409106940031052,
      "learning_rate": 0.00018250696378830085,
      "loss": 0.2825,
      "step": 162
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.058084022253751755,
      "learning_rate": 0.0001823955431754875,
      "loss": 0.2171,
      "step": 163
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.08975499123334885,
      "learning_rate": 0.0001822841225626741,
      "loss": 0.365,
      "step": 164
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.07376623153686523,
      "learning_rate": 0.00018217270194986074,
      "loss": 0.3336,
      "step": 165
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.06804781407117844,
      "learning_rate": 0.00018206128133704737,
      "loss": 0.3163,
      "step": 166
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.06362677365541458,
      "learning_rate": 0.00018194986072423398,
      "loss": 0.317,
      "step": 167
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06292019784450531,
      "learning_rate": 0.00018183844011142062,
      "loss": 0.2978,
      "step": 168
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.0647486001253128,
      "learning_rate": 0.00018172701949860725,
      "loss": 0.27,
      "step": 169
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.059582650661468506,
      "learning_rate": 0.00018161559888579386,
      "loss": 0.344,
      "step": 170
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.06418173015117645,
      "learning_rate": 0.00018150417827298053,
      "loss": 0.3143,
      "step": 171
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.05272924154996872,
      "learning_rate": 0.00018139275766016714,
      "loss": 0.282,
      "step": 172
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.05326687917113304,
      "learning_rate": 0.00018128133704735377,
      "loss": 0.2529,
      "step": 173
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.08164666593074799,
      "learning_rate": 0.0001811699164345404,
      "loss": 0.3672,
      "step": 174
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.08568466454744339,
      "learning_rate": 0.00018105849582172702,
      "loss": 0.3995,
      "step": 175
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.06509491801261902,
      "learning_rate": 0.00018094707520891366,
      "loss": 0.2809,
      "step": 176
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.06712925434112549,
      "learning_rate": 0.0001808356545961003,
      "loss": 0.2367,
      "step": 177
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.10027410835027695,
      "learning_rate": 0.0001807242339832869,
      "loss": 0.3808,
      "step": 178
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.07324101775884628,
      "learning_rate": 0.00018061281337047356,
      "loss": 0.3234,
      "step": 179
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10830274969339371,
      "learning_rate": 0.00018050139275766017,
      "loss": 0.4434,
      "step": 180
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.08461984992027283,
      "learning_rate": 0.0001803899721448468,
      "loss": 0.3819,
      "step": 181
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.06849800795316696,
      "learning_rate": 0.00018027855153203345,
      "loss": 0.3983,
      "step": 182
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.07148388773202896,
      "learning_rate": 0.00018016713091922006,
      "loss": 0.2975,
      "step": 183
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.06068180501461029,
      "learning_rate": 0.0001800557103064067,
      "loss": 0.3016,
      "step": 184
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.051959868520498276,
      "learning_rate": 0.0001799442896935933,
      "loss": 0.2419,
      "step": 185
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.07422163337469101,
      "learning_rate": 0.00017983286908077994,
      "loss": 0.2948,
      "step": 186
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.07162340730428696,
      "learning_rate": 0.0001797214484679666,
      "loss": 0.3059,
      "step": 187
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.06812291592359543,
      "learning_rate": 0.0001796100278551532,
      "loss": 0.3171,
      "step": 188
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.07255540788173676,
      "learning_rate": 0.00017949860724233985,
      "loss": 0.3323,
      "step": 189
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.0663914754986763,
      "learning_rate": 0.00017938718662952648,
      "loss": 0.2966,
      "step": 190
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.03991664573550224,
      "learning_rate": 0.0001792757660167131,
      "loss": 0.251,
      "step": 191
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.05767596513032913,
      "learning_rate": 0.00017916434540389973,
      "loss": 0.3607,
      "step": 192
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.05921991914510727,
      "learning_rate": 0.00017905292479108634,
      "loss": 0.3541,
      "step": 193
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.05089111626148224,
      "learning_rate": 0.00017894150417827298,
      "loss": 0.2928,
      "step": 194
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.05411629378795624,
      "learning_rate": 0.00017883008356545964,
      "loss": 0.3113,
      "step": 195
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.08924172818660736,
      "learning_rate": 0.00017871866295264625,
      "loss": 0.3164,
      "step": 196
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.06599845737218857,
      "learning_rate": 0.00017860724233983289,
      "loss": 0.3165,
      "step": 197
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.06638164818286896,
      "learning_rate": 0.0001784958217270195,
      "loss": 0.39,
      "step": 198
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06859147548675537,
      "learning_rate": 0.00017838440111420613,
      "loss": 0.3435,
      "step": 199
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05851481482386589,
      "learning_rate": 0.00017827298050139277,
      "loss": 0.2616,
      "step": 200
    },
    {
      "epoch": 0.3333333333333333,
      "eval_loss": 0.3388613164424896,
      "eval_runtime": 373.5218,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 200
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.06528528034687042,
      "learning_rate": 0.00017816155988857938,
      "loss": 0.3377,
      "step": 201
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.07327250391244888,
      "learning_rate": 0.00017805013927576601,
      "loss": 0.3787,
      "step": 202
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.0866728201508522,
      "learning_rate": 0.00017793871866295265,
      "loss": 0.3318,
      "step": 203
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.08070702105760574,
      "learning_rate": 0.0001778272980501393,
      "loss": 0.272,
      "step": 204
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.07358337938785553,
      "learning_rate": 0.00017771587743732592,
      "loss": 0.2816,
      "step": 205
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.11622414737939835,
      "learning_rate": 0.00017760445682451253,
      "loss": 0.3935,
      "step": 206
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.07202191650867462,
      "learning_rate": 0.00017749303621169917,
      "loss": 0.3054,
      "step": 207
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.06922760605812073,
      "learning_rate": 0.0001773816155988858,
      "loss": 0.2836,
      "step": 208
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.040258318185806274,
      "learning_rate": 0.00017727019498607242,
      "loss": 0.251,
      "step": 209
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06730931252241135,
      "learning_rate": 0.00017715877437325905,
      "loss": 0.3381,
      "step": 210
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.08930881321430206,
      "learning_rate": 0.0001770473537604457,
      "loss": 0.3098,
      "step": 211
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.08481532335281372,
      "learning_rate": 0.00017693593314763232,
      "loss": 0.3761,
      "step": 212
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.059320684522390366,
      "learning_rate": 0.00017682451253481896,
      "loss": 0.2541,
      "step": 213
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06912222504615784,
      "learning_rate": 0.00017671309192200557,
      "loss": 0.2664,
      "step": 214
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.08226578682661057,
      "learning_rate": 0.0001766016713091922,
      "loss": 0.3138,
      "step": 215
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.04609953239560127,
      "learning_rate": 0.00017649025069637884,
      "loss": 0.2542,
      "step": 216
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.06391782313585281,
      "learning_rate": 0.00017637883008356545,
      "loss": 0.3264,
      "step": 217
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.05829302966594696,
      "learning_rate": 0.0001762674094707521,
      "loss": 0.2933,
      "step": 218
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.06562326848506927,
      "learning_rate": 0.00017615598885793873,
      "loss": 0.2715,
      "step": 219
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.06454933434724808,
      "learning_rate": 0.00017604456824512536,
      "loss": 0.3263,
      "step": 220
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.05812375620007515,
      "learning_rate": 0.000175933147632312,
      "loss": 0.3219,
      "step": 221
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04714621976017952,
      "learning_rate": 0.0001758217270194986,
      "loss": 0.3001,
      "step": 222
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.058413684368133545,
      "learning_rate": 0.00017571030640668524,
      "loss": 0.3342,
      "step": 223
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.05853581801056862,
      "learning_rate": 0.00017559888579387188,
      "loss": 0.3559,
      "step": 224
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.06629899889230728,
      "learning_rate": 0.0001754874651810585,
      "loss": 0.3503,
      "step": 225
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.060716353356838226,
      "learning_rate": 0.00017537604456824513,
      "loss": 0.2432,
      "step": 226
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.0828184112906456,
      "learning_rate": 0.00017526462395543176,
      "loss": 0.3287,
      "step": 227
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09290502220392227,
      "learning_rate": 0.0001751532033426184,
      "loss": 0.4283,
      "step": 228
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.0796162560582161,
      "learning_rate": 0.00017504178272980504,
      "loss": 0.3038,
      "step": 229
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.059088610112667084,
      "learning_rate": 0.00017493036211699165,
      "loss": 0.2984,
      "step": 230
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06552764028310776,
      "learning_rate": 0.00017481894150417828,
      "loss": 0.28,
      "step": 231
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.07208864390850067,
      "learning_rate": 0.00017470752089136492,
      "loss": 0.3022,
      "step": 232
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.07033393532037735,
      "learning_rate": 0.00017459610027855153,
      "loss": 0.345,
      "step": 233
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.08407337963581085,
      "learning_rate": 0.00017448467966573816,
      "loss": 0.4327,
      "step": 234
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.037531256675720215,
      "learning_rate": 0.0001743732590529248,
      "loss": 0.2626,
      "step": 235
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.06134509667754173,
      "learning_rate": 0.00017426183844011144,
      "loss": 0.3523,
      "step": 236
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.07239819318056107,
      "learning_rate": 0.00017415041782729807,
      "loss": 0.3462,
      "step": 237
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.049917254596948624,
      "learning_rate": 0.00017403899721448468,
      "loss": 0.3035,
      "step": 238
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.06036359816789627,
      "learning_rate": 0.00017392757660167132,
      "loss": 0.2773,
      "step": 239
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07293415814638138,
      "learning_rate": 0.00017381615598885793,
      "loss": 0.2826,
      "step": 240
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.11930102854967117,
      "learning_rate": 0.00017370473537604457,
      "loss": 0.4427,
      "step": 241
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.06569141149520874,
      "learning_rate": 0.0001735933147632312,
      "loss": 0.3097,
      "step": 242
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.06904499232769012,
      "learning_rate": 0.00017348189415041784,
      "loss": 0.2963,
      "step": 243
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.07275153696537018,
      "learning_rate": 0.00017337047353760448,
      "loss": 0.3557,
      "step": 244
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.0644286647439003,
      "learning_rate": 0.0001732590529247911,
      "loss": 0.2972,
      "step": 245
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.09861116111278534,
      "learning_rate": 0.00017314763231197772,
      "loss": 0.426,
      "step": 246
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.06782294809818268,
      "learning_rate": 0.00017303621169916436,
      "loss": 0.4074,
      "step": 247
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.055203475058078766,
      "learning_rate": 0.00017292479108635097,
      "loss": 0.3725,
      "step": 248
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.1059175655245781,
      "learning_rate": 0.0001728133704735376,
      "loss": 0.4315,
      "step": 249
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.08250673860311508,
      "learning_rate": 0.00017270194986072424,
      "loss": 0.3823,
      "step": 250
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.06672301888465881,
      "learning_rate": 0.00017259052924791088,
      "loss": 0.272,
      "step": 251
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.07668539136648178,
      "learning_rate": 0.0001724791086350975,
      "loss": 0.3585,
      "step": 252
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.09261240810155869,
      "learning_rate": 0.00017236768802228412,
      "loss": 0.3171,
      "step": 253
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.09300730377435684,
      "learning_rate": 0.00017225626740947076,
      "loss": 0.3526,
      "step": 254
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.0796714648604393,
      "learning_rate": 0.0001721448467966574,
      "loss": 0.3038,
      "step": 255
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.06646404415369034,
      "learning_rate": 0.000172033426183844,
      "loss": 0.2964,
      "step": 256
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.06485851854085922,
      "learning_rate": 0.00017192200557103064,
      "loss": 0.3135,
      "step": 257
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.06937497109174728,
      "learning_rate": 0.00017181058495821728,
      "loss": 0.3272,
      "step": 258
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.07694260776042938,
      "learning_rate": 0.00017169916434540391,
      "loss": 0.3705,
      "step": 259
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.04648123309016228,
      "learning_rate": 0.00017158774373259055,
      "loss": 0.2886,
      "step": 260
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.056954897940158844,
      "learning_rate": 0.00017147632311977716,
      "loss": 0.3066,
      "step": 261
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.04178736358880997,
      "learning_rate": 0.0001713649025069638,
      "loss": 0.2673,
      "step": 262
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.08387766778469086,
      "learning_rate": 0.00017125348189415043,
      "loss": 0.3245,
      "step": 263
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.06390725821256638,
      "learning_rate": 0.00017114206128133704,
      "loss": 0.3462,
      "step": 264
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.07663051038980484,
      "learning_rate": 0.00017103064066852368,
      "loss": 0.371,
      "step": 265
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.06242416799068451,
      "learning_rate": 0.00017091922005571032,
      "loss": 0.3902,
      "step": 266
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.04411499202251434,
      "learning_rate": 0.00017080779944289695,
      "loss": 0.3499,
      "step": 267
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.048253074288368225,
      "learning_rate": 0.0001706963788300836,
      "loss": 0.3073,
      "step": 268
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.05330132693052292,
      "learning_rate": 0.0001705849582172702,
      "loss": 0.2782,
      "step": 269
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.09645810723304749,
      "learning_rate": 0.00017047353760445683,
      "loss": 0.3513,
      "step": 270
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.07553944736719131,
      "learning_rate": 0.00017036211699164347,
      "loss": 0.3487,
      "step": 271
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10264316946268082,
      "learning_rate": 0.00017025069637883008,
      "loss": 0.4436,
      "step": 272
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.05646580085158348,
      "learning_rate": 0.00017013927576601672,
      "loss": 0.3528,
      "step": 273
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.07130212336778641,
      "learning_rate": 0.00017002785515320335,
      "loss": 0.3375,
      "step": 274
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.1121247336268425,
      "learning_rate": 0.00016991643454039,
      "loss": 0.3786,
      "step": 275
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05661875754594803,
      "learning_rate": 0.00016980501392757663,
      "loss": 0.2849,
      "step": 276
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.06488969922065735,
      "learning_rate": 0.00016969359331476324,
      "loss": 0.3286,
      "step": 277
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.04874249920248985,
      "learning_rate": 0.00016958217270194987,
      "loss": 0.3338,
      "step": 278
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.049081817269325256,
      "learning_rate": 0.0001694707520891365,
      "loss": 0.3112,
      "step": 279
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.06580985337495804,
      "learning_rate": 0.00016935933147632312,
      "loss": 0.3001,
      "step": 280
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.055983323603868484,
      "learning_rate": 0.00016924791086350975,
      "loss": 0.312,
      "step": 281
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05061831325292587,
      "learning_rate": 0.0001691364902506964,
      "loss": 0.315,
      "step": 282
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.07002280652523041,
      "learning_rate": 0.00016902506963788303,
      "loss": 0.3657,
      "step": 283
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05577494204044342,
      "learning_rate": 0.00016891364902506966,
      "loss": 0.3993,
      "step": 284
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.0922563299536705,
      "learning_rate": 0.00016880222841225627,
      "loss": 0.3746,
      "step": 285
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.10654503852128983,
      "learning_rate": 0.0001686908077994429,
      "loss": 0.3481,
      "step": 286
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.0896528884768486,
      "learning_rate": 0.00016857938718662955,
      "loss": 0.3915,
      "step": 287
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.11687684059143066,
      "learning_rate": 0.00016846796657381616,
      "loss": 0.4647,
      "step": 288
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.05887802690267563,
      "learning_rate": 0.0001683565459610028,
      "loss": 0.3566,
      "step": 289
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.09831524640321732,
      "learning_rate": 0.0001682451253481894,
      "loss": 0.4483,
      "step": 290
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.0771913006901741,
      "learning_rate": 0.00016813370473537606,
      "loss": 0.3773,
      "step": 291
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.05990441143512726,
      "learning_rate": 0.0001680222841225627,
      "loss": 0.3402,
      "step": 292
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.07814719527959824,
      "learning_rate": 0.0001679108635097493,
      "loss": 0.4058,
      "step": 293
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05076230689883232,
      "learning_rate": 0.00016779944289693595,
      "loss": 0.3336,
      "step": 294
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.0958528146147728,
      "learning_rate": 0.00016768802228412256,
      "loss": 0.4436,
      "step": 295
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.060141295194625854,
      "learning_rate": 0.0001675766016713092,
      "loss": 0.4008,
      "step": 296
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.05116834118962288,
      "learning_rate": 0.00016746518105849583,
      "loss": 0.262,
      "step": 297
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.04933657869696617,
      "learning_rate": 0.00016735376044568244,
      "loss": 0.2721,
      "step": 298
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.07373249530792236,
      "learning_rate": 0.0001672423398328691,
      "loss": 0.355,
      "step": 299
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.044377684593200684,
      "learning_rate": 0.00016713091922005574,
      "loss": 0.2971,
      "step": 300
    },
    {
      "epoch": 0.5,
      "eval_loss": 0.33406704664230347,
      "eval_runtime": 373.8116,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 300
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.09329823404550552,
      "learning_rate": 0.00016701949860724235,
      "loss": 0.3936,
      "step": 301
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.05307592451572418,
      "learning_rate": 0.00016690807799442898,
      "loss": 0.2952,
      "step": 302
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.05269429087638855,
      "learning_rate": 0.0001667966573816156,
      "loss": 0.2697,
      "step": 303
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.07882646471261978,
      "learning_rate": 0.00016668523676880223,
      "loss": 0.3733,
      "step": 304
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.07247893512248993,
      "learning_rate": 0.00016657381615598887,
      "loss": 0.4261,
      "step": 305
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.05371604114770889,
      "learning_rate": 0.00016646239554317548,
      "loss": 0.3111,
      "step": 306
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.05024495720863342,
      "learning_rate": 0.00016635097493036214,
      "loss": 0.2737,
      "step": 307
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.06022166833281517,
      "learning_rate": 0.00016623955431754875,
      "loss": 0.2782,
      "step": 308
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.05292487516999245,
      "learning_rate": 0.00016612813370473539,
      "loss": 0.3065,
      "step": 309
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.056927524507045746,
      "learning_rate": 0.00016601671309192202,
      "loss": 0.3671,
      "step": 310
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.0786108672618866,
      "learning_rate": 0.00016590529247910863,
      "loss": 0.3963,
      "step": 311
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09610572457313538,
      "learning_rate": 0.00016579387186629527,
      "loss": 0.3851,
      "step": 312
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.05500410869717598,
      "learning_rate": 0.0001656824512534819,
      "loss": 0.3583,
      "step": 313
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.08582132309675217,
      "learning_rate": 0.00016557103064066851,
      "loss": 0.3898,
      "step": 314
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.061810631304979324,
      "learning_rate": 0.00016545961002785518,
      "loss": 0.2483,
      "step": 315
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.07196531444787979,
      "learning_rate": 0.0001653481894150418,
      "loss": 0.4125,
      "step": 316
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.06049725040793419,
      "learning_rate": 0.00016523676880222842,
      "loss": 0.3356,
      "step": 317
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.06734035164117813,
      "learning_rate": 0.00016512534818941506,
      "loss": 0.3966,
      "step": 318
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.05909775570034981,
      "learning_rate": 0.00016501392757660167,
      "loss": 0.2551,
      "step": 319
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.09914226830005646,
      "learning_rate": 0.0001649025069637883,
      "loss": 0.3654,
      "step": 320
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.06723151355981827,
      "learning_rate": 0.00016479108635097494,
      "loss": 0.2901,
      "step": 321
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.06519628316164017,
      "learning_rate": 0.00016467966573816155,
      "loss": 0.3625,
      "step": 322
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.04443543031811714,
      "learning_rate": 0.00016456824512534822,
      "loss": 0.2689,
      "step": 323
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.07195848226547241,
      "learning_rate": 0.00016445682451253482,
      "loss": 0.3749,
      "step": 324
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.05139874666929245,
      "learning_rate": 0.00016434540389972146,
      "loss": 0.3168,
      "step": 325
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.06149451434612274,
      "learning_rate": 0.0001642339832869081,
      "loss": 0.3361,
      "step": 326
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.059856116771698,
      "learning_rate": 0.0001641225626740947,
      "loss": 0.2833,
      "step": 327
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.060059476643800735,
      "learning_rate": 0.00016401114206128134,
      "loss": 0.3624,
      "step": 328
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.06225313991308212,
      "learning_rate": 0.00016389972144846798,
      "loss": 0.3619,
      "step": 329
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.0349864698946476,
      "learning_rate": 0.0001637883008356546,
      "loss": 0.209,
      "step": 330
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.05916865915060043,
      "learning_rate": 0.00016367688022284125,
      "loss": 0.336,
      "step": 331
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.043841492384672165,
      "learning_rate": 0.00016356545961002786,
      "loss": 0.3148,
      "step": 332
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.05800284445285797,
      "learning_rate": 0.0001634540389972145,
      "loss": 0.3651,
      "step": 333
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08612257242202759,
      "learning_rate": 0.00016334261838440114,
      "loss": 0.3809,
      "step": 334
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.05423552915453911,
      "learning_rate": 0.00016323119777158774,
      "loss": 0.3328,
      "step": 335
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.043566737323999405,
      "learning_rate": 0.00016311977715877438,
      "loss": 0.2274,
      "step": 336
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.0462743416428566,
      "learning_rate": 0.00016300835654596102,
      "loss": 0.312,
      "step": 337
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.06379109621047974,
      "learning_rate": 0.00016289693593314763,
      "loss": 0.3236,
      "step": 338
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.054661449044942856,
      "learning_rate": 0.00016278551532033426,
      "loss": 0.2697,
      "step": 339
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.044119060039520264,
      "learning_rate": 0.0001626740947075209,
      "loss": 0.3071,
      "step": 340
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.06531501561403275,
      "learning_rate": 0.00016256267409470754,
      "loss": 0.3056,
      "step": 341
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.07126521319150925,
      "learning_rate": 0.00016245125348189417,
      "loss": 0.3725,
      "step": 342
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.062475211918354034,
      "learning_rate": 0.00016233983286908078,
      "loss": 0.3481,
      "step": 343
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.051287248730659485,
      "learning_rate": 0.00016222841225626742,
      "loss": 0.3282,
      "step": 344
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.0753796398639679,
      "learning_rate": 0.00016211699164345403,
      "loss": 0.4105,
      "step": 345
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.06075955927371979,
      "learning_rate": 0.00016200557103064066,
      "loss": 0.2847,
      "step": 346
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.049342215061187744,
      "learning_rate": 0.0001618941504178273,
      "loss": 0.319,
      "step": 347
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.11758521944284439,
      "learning_rate": 0.00016178272980501394,
      "loss": 0.3552,
      "step": 348
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.06062491238117218,
      "learning_rate": 0.00016167130919220057,
      "loss": 0.3879,
      "step": 349
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.053289785981178284,
      "learning_rate": 0.0001615598885793872,
      "loss": 0.3227,
      "step": 350
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.046300258487463,
      "learning_rate": 0.00016144846796657382,
      "loss": 0.278,
      "step": 351
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.042093485593795776,
      "learning_rate": 0.00016133704735376046,
      "loss": 0.2906,
      "step": 352
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.0622057169675827,
      "learning_rate": 0.00016122562674094707,
      "loss": 0.3826,
      "step": 353
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.046517372131347656,
      "learning_rate": 0.0001611142061281337,
      "loss": 0.2846,
      "step": 354
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.07477085292339325,
      "learning_rate": 0.00016100278551532034,
      "loss": 0.39,
      "step": 355
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.05725261569023132,
      "learning_rate": 0.00016089136490250698,
      "loss": 0.3105,
      "step": 356
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.047877974808216095,
      "learning_rate": 0.0001607799442896936,
      "loss": 0.2642,
      "step": 357
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.06068779155611992,
      "learning_rate": 0.00016066852367688022,
      "loss": 0.3415,
      "step": 358
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.06766151636838913,
      "learning_rate": 0.00016055710306406686,
      "loss": 0.3771,
      "step": 359
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04824470356106758,
      "learning_rate": 0.0001604456824512535,
      "loss": 0.3413,
      "step": 360
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.05535799637436867,
      "learning_rate": 0.0001603342618384401,
      "loss": 0.3053,
      "step": 361
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.07063216716051102,
      "learning_rate": 0.00016022284122562674,
      "loss": 0.3687,
      "step": 362
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.03917135298252106,
      "learning_rate": 0.00016011142061281338,
      "loss": 0.2866,
      "step": 363
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.0627112165093422,
      "learning_rate": 0.00016,
      "loss": 0.2673,
      "step": 364
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.09194136410951614,
      "learning_rate": 0.00015988857938718665,
      "loss": 0.4174,
      "step": 365
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.07770700752735138,
      "learning_rate": 0.00015977715877437326,
      "loss": 0.3098,
      "step": 366
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.0638580247759819,
      "learning_rate": 0.0001596657381615599,
      "loss": 0.3463,
      "step": 367
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.06891981512308121,
      "learning_rate": 0.00015955431754874653,
      "loss": 0.3873,
      "step": 368
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.0637492835521698,
      "learning_rate": 0.00015944289693593314,
      "loss": 0.3125,
      "step": 369
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.06685901433229446,
      "learning_rate": 0.00015933147632311978,
      "loss": 0.291,
      "step": 370
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.06276644021272659,
      "learning_rate": 0.00015922005571030641,
      "loss": 0.3353,
      "step": 371
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.06120016798377037,
      "learning_rate": 0.00015910863509749305,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.06211041286587715,
      "learning_rate": 0.0001589972144846797,
      "loss": 0.3885,
      "step": 373
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.04900497943162918,
      "learning_rate": 0.0001588857938718663,
      "loss": 0.3883,
      "step": 374
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.07299406081438065,
      "learning_rate": 0.00015877437325905293,
      "loss": 0.2706,
      "step": 375
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.09120436757802963,
      "learning_rate": 0.00015866295264623957,
      "loss": 0.3739,
      "step": 376
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.07414333522319794,
      "learning_rate": 0.00015855153203342618,
      "loss": 0.3925,
      "step": 377
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.04861472547054291,
      "learning_rate": 0.00015844011142061282,
      "loss": 0.2753,
      "step": 378
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.10256479680538177,
      "learning_rate": 0.00015832869080779945,
      "loss": 0.3699,
      "step": 379
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.05784834176301956,
      "learning_rate": 0.0001582172701949861,
      "loss": 0.3029,
      "step": 380
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.05931473523378372,
      "learning_rate": 0.00015810584958217272,
      "loss": 0.2805,
      "step": 381
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.05126677453517914,
      "learning_rate": 0.00015799442896935933,
      "loss": 0.2928,
      "step": 382
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.09290463477373123,
      "learning_rate": 0.00015788300835654597,
      "loss": 0.3602,
      "step": 383
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10042349249124527,
      "learning_rate": 0.0001577715877437326,
      "loss": 0.4415,
      "step": 384
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.08114712685346603,
      "learning_rate": 0.00015766016713091922,
      "loss": 0.391,
      "step": 385
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.07912176847457886,
      "learning_rate": 0.00015754874651810585,
      "loss": 0.3671,
      "step": 386
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.06592220067977905,
      "learning_rate": 0.0001574373259052925,
      "loss": 0.3083,
      "step": 387
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.05964716896414757,
      "learning_rate": 0.00015732590529247913,
      "loss": 0.2582,
      "step": 388
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.06917083263397217,
      "learning_rate": 0.00015721448467966576,
      "loss": 0.3186,
      "step": 389
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.05812530592083931,
      "learning_rate": 0.00015710306406685237,
      "loss": 0.3,
      "step": 390
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.06314097344875336,
      "learning_rate": 0.000156991643454039,
      "loss": 0.2926,
      "step": 391
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.06183494254946709,
      "learning_rate": 0.00015688022284122564,
      "loss": 0.2942,
      "step": 392
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.05879906937479973,
      "learning_rate": 0.00015676880222841225,
      "loss": 0.2761,
      "step": 393
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.07158152759075165,
      "learning_rate": 0.0001566573816155989,
      "loss": 0.4515,
      "step": 394
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.05752398073673248,
      "learning_rate": 0.0001565459610027855,
      "loss": 0.3788,
      "step": 395
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06474988907575607,
      "learning_rate": 0.00015643454038997216,
      "loss": 0.3496,
      "step": 396
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.07224076986312866,
      "learning_rate": 0.0001563231197771588,
      "loss": 0.3171,
      "step": 397
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.04606259614229202,
      "learning_rate": 0.0001562116991643454,
      "loss": 0.2679,
      "step": 398
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.06618712842464447,
      "learning_rate": 0.00015610027855153205,
      "loss": 0.3408,
      "step": 399
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0727199912071228,
      "learning_rate": 0.00015598885793871866,
      "loss": 0.3694,
      "step": 400
    },
    {
      "epoch": 0.6666666666666666,
      "eval_loss": 0.33038070797920227,
      "eval_runtime": 373.7813,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 400
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.049671657383441925,
      "learning_rate": 0.0001558774373259053,
      "loss": 0.3638,
      "step": 401
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.0751660093665123,
      "learning_rate": 0.00015576601671309193,
      "loss": 0.3246,
      "step": 402
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.051771342754364014,
      "learning_rate": 0.00015565459610027854,
      "loss": 0.2599,
      "step": 403
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.06960935890674591,
      "learning_rate": 0.0001555431754874652,
      "loss": 0.2855,
      "step": 404
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.0638466402888298,
      "learning_rate": 0.00015543175487465184,
      "loss": 0.3257,
      "step": 405
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.06104127690196037,
      "learning_rate": 0.00015532033426183845,
      "loss": 0.2893,
      "step": 406
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.09004420042037964,
      "learning_rate": 0.00015520891364902508,
      "loss": 0.4522,
      "step": 407
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.07697346806526184,
      "learning_rate": 0.0001550974930362117,
      "loss": 0.2782,
      "step": 408
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.07419420033693314,
      "learning_rate": 0.00015498607242339833,
      "loss": 0.3739,
      "step": 409
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.055936019867658615,
      "learning_rate": 0.00015487465181058497,
      "loss": 0.3843,
      "step": 410
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.047740768641233444,
      "learning_rate": 0.00015476323119777158,
      "loss": 0.2755,
      "step": 411
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.04831160977482796,
      "learning_rate": 0.00015465181058495824,
      "loss": 0.2952,
      "step": 412
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.06364057958126068,
      "learning_rate": 0.00015454038997214485,
      "loss": 0.3322,
      "step": 413
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.05456951633095741,
      "learning_rate": 0.00015442896935933148,
      "loss": 0.3343,
      "step": 414
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.03701818734407425,
      "learning_rate": 0.00015431754874651812,
      "loss": 0.1905,
      "step": 415
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.06792543083429337,
      "learning_rate": 0.00015420612813370473,
      "loss": 0.3592,
      "step": 416
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.08207648247480392,
      "learning_rate": 0.00015409470752089137,
      "loss": 0.3399,
      "step": 417
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.06876149773597717,
      "learning_rate": 0.000153983286908078,
      "loss": 0.374,
      "step": 418
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.06378063559532166,
      "learning_rate": 0.0001538718662952646,
      "loss": 0.3097,
      "step": 419
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04361330345273018,
      "learning_rate": 0.00015376044568245128,
      "loss": 0.3016,
      "step": 420
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.05422496423125267,
      "learning_rate": 0.00015364902506963789,
      "loss": 0.4201,
      "step": 421
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.06403195112943649,
      "learning_rate": 0.00015353760445682452,
      "loss": 0.3504,
      "step": 422
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.05734192952513695,
      "learning_rate": 0.00015342618384401116,
      "loss": 0.3443,
      "step": 423
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06866508722305298,
      "learning_rate": 0.00015331476323119777,
      "loss": 0.3852,
      "step": 424
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.04271905496716499,
      "learning_rate": 0.0001532033426183844,
      "loss": 0.3524,
      "step": 425
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.056068915873765945,
      "learning_rate": 0.00015309192200557104,
      "loss": 0.2999,
      "step": 426
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.043030448257923126,
      "learning_rate": 0.00015298050139275765,
      "loss": 0.2591,
      "step": 427
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.0719999372959137,
      "learning_rate": 0.00015286908077994431,
      "loss": 0.325,
      "step": 428
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.05486292019486427,
      "learning_rate": 0.00015275766016713092,
      "loss": 0.3018,
      "step": 429
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.05790204182267189,
      "learning_rate": 0.00015264623955431756,
      "loss": 0.3348,
      "step": 430
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.0518096461892128,
      "learning_rate": 0.0001525348189415042,
      "loss": 0.3198,
      "step": 431
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06278590857982635,
      "learning_rate": 0.0001524233983286908,
      "loss": 0.3884,
      "step": 432
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.04729745164513588,
      "learning_rate": 0.00015231197771587744,
      "loss": 0.2791,
      "step": 433
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.06511681526899338,
      "learning_rate": 0.00015220055710306408,
      "loss": 0.3217,
      "step": 434
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05852040275931358,
      "learning_rate": 0.0001520891364902507,
      "loss": 0.3039,
      "step": 435
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.05285286903381348,
      "learning_rate": 0.00015197771587743735,
      "loss": 0.3112,
      "step": 436
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.0563475526869297,
      "learning_rate": 0.00015186629526462396,
      "loss": 0.3308,
      "step": 437
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.04962017387151718,
      "learning_rate": 0.0001517548746518106,
      "loss": 0.3074,
      "step": 438
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.06965400278568268,
      "learning_rate": 0.00015164345403899723,
      "loss": 0.3453,
      "step": 439
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03574811667203903,
      "learning_rate": 0.00015153203342618384,
      "loss": 0.2622,
      "step": 440
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09636316448450089,
      "learning_rate": 0.00015142061281337048,
      "loss": 0.3886,
      "step": 441
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.059329334646463394,
      "learning_rate": 0.00015130919220055712,
      "loss": 0.3156,
      "step": 442
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06609758734703064,
      "learning_rate": 0.00015119777158774373,
      "loss": 0.2453,
      "step": 443
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04046764224767685,
      "learning_rate": 0.0001510863509749304,
      "loss": 0.2396,
      "step": 444
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.10643579065799713,
      "learning_rate": 0.000150974930362117,
      "loss": 0.4159,
      "step": 445
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.06950733810663223,
      "learning_rate": 0.00015086350974930364,
      "loss": 0.3398,
      "step": 446
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.0766250416636467,
      "learning_rate": 0.00015075208913649027,
      "loss": 0.3328,
      "step": 447
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05973466485738754,
      "learning_rate": 0.00015064066852367688,
      "loss": 0.3382,
      "step": 448
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.04731937497854233,
      "learning_rate": 0.00015052924791086352,
      "loss": 0.2357,
      "step": 449
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.0641368106007576,
      "learning_rate": 0.00015041782729805013,
      "loss": 0.3222,
      "step": 450
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.08205905556678772,
      "learning_rate": 0.00015030640668523676,
      "loss": 0.3659,
      "step": 451
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06015029922127724,
      "learning_rate": 0.00015019498607242343,
      "loss": 0.28,
      "step": 452
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.049014464020729065,
      "learning_rate": 0.00015008356545961004,
      "loss": 0.2436,
      "step": 453
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.06603681296110153,
      "learning_rate": 0.00014997214484679667,
      "loss": 0.3804,
      "step": 454
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.07793335616588593,
      "learning_rate": 0.0001498607242339833,
      "loss": 0.294,
      "step": 455
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04767234995961189,
      "learning_rate": 0.00014974930362116992,
      "loss": 0.2779,
      "step": 456
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.056623298674821854,
      "learning_rate": 0.00014963788300835656,
      "loss": 0.3908,
      "step": 457
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.05769085884094238,
      "learning_rate": 0.00014952646239554316,
      "loss": 0.3466,
      "step": 458
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.04517456516623497,
      "learning_rate": 0.0001494150417827298,
      "loss": 0.2858,
      "step": 459
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.0452684722840786,
      "learning_rate": 0.00014930362116991646,
      "loss": 0.2877,
      "step": 460
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.053076423704624176,
      "learning_rate": 0.00014919220055710307,
      "loss": 0.3587,
      "step": 461
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.05435944348573685,
      "learning_rate": 0.0001490807799442897,
      "loss": 0.2822,
      "step": 462
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.05604596063494682,
      "learning_rate": 0.00014896935933147632,
      "loss": 0.3081,
      "step": 463
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.07164648920297623,
      "learning_rate": 0.00014885793871866296,
      "loss": 0.4099,
      "step": 464
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.06206504628062248,
      "learning_rate": 0.0001487465181058496,
      "loss": 0.3222,
      "step": 465
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.04434536397457123,
      "learning_rate": 0.0001486350974930362,
      "loss": 0.2555,
      "step": 466
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.042083725333213806,
      "learning_rate": 0.00014852367688022284,
      "loss": 0.2848,
      "step": 467
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04734901338815689,
      "learning_rate": 0.00014841225626740948,
      "loss": 0.3472,
      "step": 468
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.07734812796115875,
      "learning_rate": 0.0001483008356545961,
      "loss": 0.3498,
      "step": 469
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.04673198238015175,
      "learning_rate": 0.00014818941504178275,
      "loss": 0.3031,
      "step": 470
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.04137273505330086,
      "learning_rate": 0.00014807799442896936,
      "loss": 0.2605,
      "step": 471
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04433909058570862,
      "learning_rate": 0.000147966573816156,
      "loss": 0.298,
      "step": 472
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.051636550575494766,
      "learning_rate": 0.00014785515320334263,
      "loss": 0.316,
      "step": 473
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.06208198517560959,
      "learning_rate": 0.00014774373259052924,
      "loss": 0.3718,
      "step": 474
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.05418902263045311,
      "learning_rate": 0.00014763231197771588,
      "loss": 0.3801,
      "step": 475
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05110190063714981,
      "learning_rate": 0.0001475208913649025,
      "loss": 0.3136,
      "step": 476
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.09889218956232071,
      "learning_rate": 0.00014740947075208915,
      "loss": 0.3812,
      "step": 477
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.05322788283228874,
      "learning_rate": 0.00014729805013927579,
      "loss": 0.2818,
      "step": 478
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.05451244115829468,
      "learning_rate": 0.0001471866295264624,
      "loss": 0.3402,
      "step": 479
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.06887216866016388,
      "learning_rate": 0.00014707520891364903,
      "loss": 0.3994,
      "step": 480
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.0672229751944542,
      "learning_rate": 0.00014696378830083567,
      "loss": 0.4048,
      "step": 481
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.03675975278019905,
      "learning_rate": 0.00014685236768802228,
      "loss": 0.2377,
      "step": 482
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.04699351638555527,
      "learning_rate": 0.00014674094707520891,
      "loss": 0.2695,
      "step": 483
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.06801038235425949,
      "learning_rate": 0.00014662952646239555,
      "loss": 0.3598,
      "step": 484
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.06345156580209732,
      "learning_rate": 0.0001465181058495822,
      "loss": 0.3508,
      "step": 485
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.05789051949977875,
      "learning_rate": 0.00014640668523676882,
      "loss": 0.32,
      "step": 486
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.04386863857507706,
      "learning_rate": 0.00014629526462395543,
      "loss": 0.2561,
      "step": 487
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.05822966247797012,
      "learning_rate": 0.00014618384401114207,
      "loss": 0.2997,
      "step": 488
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.06740472465753555,
      "learning_rate": 0.0001460724233983287,
      "loss": 0.3412,
      "step": 489
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.04136982187628746,
      "learning_rate": 0.00014596100278551532,
      "loss": 0.2713,
      "step": 490
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.04848026484251022,
      "learning_rate": 0.00014584958217270195,
      "loss": 0.2985,
      "step": 491
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.0479574128985405,
      "learning_rate": 0.0001457381615598886,
      "loss": 0.3135,
      "step": 492
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.04878498986363411,
      "learning_rate": 0.00014562674094707522,
      "loss": 0.2961,
      "step": 493
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.06789479404687881,
      "learning_rate": 0.00014551532033426186,
      "loss": 0.3353,
      "step": 494
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.05490916594862938,
      "learning_rate": 0.00014540389972144847,
      "loss": 0.3405,
      "step": 495
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.0509500578045845,
      "learning_rate": 0.0001452924791086351,
      "loss": 0.2784,
      "step": 496
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.053391579538583755,
      "learning_rate": 0.00014518105849582174,
      "loss": 0.311,
      "step": 497
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.06824196875095367,
      "learning_rate": 0.00014506963788300835,
      "loss": 0.3328,
      "step": 498
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.04643777385354042,
      "learning_rate": 0.000144958217270195,
      "loss": 0.2726,
      "step": 499
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.051668308675289154,
      "learning_rate": 0.0001448467966573816,
      "loss": 0.2921,
      "step": 500
    },
    {
      "epoch": 0.8333333333333334,
      "eval_loss": 0.3281901478767395,
      "eval_runtime": 373.7636,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 500
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.060772743076086044,
      "learning_rate": 0.00014473537604456826,
      "loss": 0.2932,
      "step": 501
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.06085341051220894,
      "learning_rate": 0.0001446239554317549,
      "loss": 0.3707,
      "step": 502
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.07657663524150848,
      "learning_rate": 0.0001445125348189415,
      "loss": 0.3316,
      "step": 503
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.05329258367419243,
      "learning_rate": 0.00014440111420612814,
      "loss": 0.2978,
      "step": 504
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.07449754327535629,
      "learning_rate": 0.00014428969359331475,
      "loss": 0.3206,
      "step": 505
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.09915780276060104,
      "learning_rate": 0.0001441782729805014,
      "loss": 0.3772,
      "step": 506
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.04010171815752983,
      "learning_rate": 0.00014406685236768803,
      "loss": 0.2367,
      "step": 507
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.09146162867546082,
      "learning_rate": 0.00014395543175487464,
      "loss": 0.4259,
      "step": 508
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.03935272619128227,
      "learning_rate": 0.0001438440111420613,
      "loss": 0.2934,
      "step": 509
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.04444647580385208,
      "learning_rate": 0.00014373259052924794,
      "loss": 0.2709,
      "step": 510
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 0.04553202539682388,
      "learning_rate": 0.00014362116991643455,
      "loss": 0.2965,
      "step": 511
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.03921709582209587,
      "learning_rate": 0.00014350974930362118,
      "loss": 0.2571,
      "step": 512
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.06850571185350418,
      "learning_rate": 0.0001433983286908078,
      "loss": 0.3265,
      "step": 513
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.06620576232671738,
      "learning_rate": 0.00014328690807799443,
      "loss": 0.3389,
      "step": 514
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.054430171847343445,
      "learning_rate": 0.00014317548746518106,
      "loss": 0.2977,
      "step": 515
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.05139526352286339,
      "learning_rate": 0.00014306406685236767,
      "loss": 0.3128,
      "step": 516
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.07130229473114014,
      "learning_rate": 0.00014295264623955434,
      "loss": 0.3456,
      "step": 517
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.09925727546215057,
      "learning_rate": 0.00014284122562674095,
      "loss": 0.3533,
      "step": 518
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.05085983872413635,
      "learning_rate": 0.00014272980501392758,
      "loss": 0.3004,
      "step": 519
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04295116662979126,
      "learning_rate": 0.00014261838440111422,
      "loss": 0.2735,
      "step": 520
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 0.06154049560427666,
      "learning_rate": 0.00014250696378830083,
      "loss": 0.333,
      "step": 521
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.04609737545251846,
      "learning_rate": 0.00014239554317548747,
      "loss": 0.2828,
      "step": 522
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 0.06917523592710495,
      "learning_rate": 0.0001422841225626741,
      "loss": 0.3755,
      "step": 523
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.05453577637672424,
      "learning_rate": 0.0001421727019498607,
      "loss": 0.3307,
      "step": 524
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.0447324737906456,
      "learning_rate": 0.00014206128133704738,
      "loss": 0.2688,
      "step": 525
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.052297793328762054,
      "learning_rate": 0.00014194986072423398,
      "loss": 0.3794,
      "step": 526
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.05053732916712761,
      "learning_rate": 0.00014183844011142062,
      "loss": 0.2852,
      "step": 527
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.08157094568014145,
      "learning_rate": 0.00014172701949860726,
      "loss": 0.4197,
      "step": 528
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 0.04738731309771538,
      "learning_rate": 0.00014161559888579387,
      "loss": 0.2762,
      "step": 529
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.06784859299659729,
      "learning_rate": 0.0001415041782729805,
      "loss": 0.4156,
      "step": 530
    },
    {
      "epoch": 0.885,
      "grad_norm": 0.07192090898752213,
      "learning_rate": 0.00014139275766016714,
      "loss": 0.3342,
      "step": 531
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.08345683664083481,
      "learning_rate": 0.00014128133704735375,
      "loss": 0.3387,
      "step": 532
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 0.04191029071807861,
      "learning_rate": 0.0001411699164345404,
      "loss": 0.2741,
      "step": 533
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.05085219070315361,
      "learning_rate": 0.00014105849582172702,
      "loss": 0.2638,
      "step": 534
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.056857623159885406,
      "learning_rate": 0.00014094707520891366,
      "loss": 0.3194,
      "step": 535
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.06935551762580872,
      "learning_rate": 0.0001408356545961003,
      "loss": 0.3148,
      "step": 536
    },
    {
      "epoch": 0.895,
      "grad_norm": 0.06971725076436996,
      "learning_rate": 0.0001407242339832869,
      "loss": 0.3493,
      "step": 537
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.11990592628717422,
      "learning_rate": 0.00014061281337047354,
      "loss": 0.4268,
      "step": 538
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.054495323449373245,
      "learning_rate": 0.00014050139275766018,
      "loss": 0.3542,
      "step": 539
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.08286935836076736,
      "learning_rate": 0.0001403899721448468,
      "loss": 0.3491,
      "step": 540
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.07133619487285614,
      "learning_rate": 0.00014027855153203345,
      "loss": 0.3448,
      "step": 541
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.042325690388679504,
      "learning_rate": 0.00014016713091922006,
      "loss": 0.2781,
      "step": 542
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.032932549715042114,
      "learning_rate": 0.0001400557103064067,
      "loss": 0.2175,
      "step": 543
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.0688985288143158,
      "learning_rate": 0.00013994428969359333,
      "loss": 0.2599,
      "step": 544
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.05417398735880852,
      "learning_rate": 0.00013983286908077994,
      "loss": 0.2933,
      "step": 545
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.05415311083197594,
      "learning_rate": 0.00013972144846796658,
      "loss": 0.2908,
      "step": 546
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 0.05922441557049751,
      "learning_rate": 0.00013961002785515322,
      "loss": 0.3042,
      "step": 547
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.046018511056900024,
      "learning_rate": 0.00013949860724233982,
      "loss": 0.3232,
      "step": 548
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.0504499226808548,
      "learning_rate": 0.0001393871866295265,
      "loss": 0.2453,
      "step": 549
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.07001747936010361,
      "learning_rate": 0.0001392757660167131,
      "loss": 0.3062,
      "step": 550
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.060449179261922836,
      "learning_rate": 0.00013916434540389973,
      "loss": 0.3479,
      "step": 551
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.05341773480176926,
      "learning_rate": 0.00013905292479108637,
      "loss": 0.2807,
      "step": 552
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 0.04648570343852043,
      "learning_rate": 0.00013894150417827298,
      "loss": 0.2319,
      "step": 553
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 0.049530383199453354,
      "learning_rate": 0.00013883008356545962,
      "loss": 0.2977,
      "step": 554
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.04114844277501106,
      "learning_rate": 0.00013871866295264623,
      "loss": 0.2376,
      "step": 555
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04786466062068939,
      "learning_rate": 0.00013860724233983286,
      "loss": 0.2811,
      "step": 556
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.09112845361232758,
      "learning_rate": 0.00013849582172701953,
      "loss": 0.3976,
      "step": 557
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.04977407678961754,
      "learning_rate": 0.00013838440111420614,
      "loss": 0.2862,
      "step": 558
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 0.040610700845718384,
      "learning_rate": 0.00013827298050139277,
      "loss": 0.2682,
      "step": 559
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.07936462759971619,
      "learning_rate": 0.0001381615598885794,
      "loss": 0.3323,
      "step": 560
    },
    {
      "epoch": 0.935,
      "grad_norm": 0.05755506455898285,
      "learning_rate": 0.00013805013927576602,
      "loss": 0.3285,
      "step": 561
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.054356928914785385,
      "learning_rate": 0.00013793871866295265,
      "loss": 0.2217,
      "step": 562
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.048578329384326935,
      "learning_rate": 0.00013782729805013926,
      "loss": 0.3149,
      "step": 563
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.07088055461645126,
      "learning_rate": 0.0001377158774373259,
      "loss": 0.3515,
      "step": 564
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.05443679541349411,
      "learning_rate": 0.00013760445682451256,
      "loss": 0.3135,
      "step": 565
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.054265957325696945,
      "learning_rate": 0.00013749303621169917,
      "loss": 0.2861,
      "step": 566
    },
    {
      "epoch": 0.945,
      "grad_norm": 0.05600951611995697,
      "learning_rate": 0.0001373816155988858,
      "loss": 0.3121,
      "step": 567
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.045498721301555634,
      "learning_rate": 0.00013727019498607242,
      "loss": 0.2579,
      "step": 568
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 0.04574575647711754,
      "learning_rate": 0.00013715877437325906,
      "loss": 0.2609,
      "step": 569
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.038479067385196686,
      "learning_rate": 0.0001370473537604457,
      "loss": 0.2768,
      "step": 570
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.06350842863321304,
      "learning_rate": 0.0001369359331476323,
      "loss": 0.3884,
      "step": 571
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.05305676907300949,
      "learning_rate": 0.00013682451253481894,
      "loss": 0.305,
      "step": 572
    },
    {
      "epoch": 0.955,
      "grad_norm": 0.07867347449064255,
      "learning_rate": 0.00013671309192200557,
      "loss": 0.3895,
      "step": 573
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.04169553518295288,
      "learning_rate": 0.0001366016713091922,
      "loss": 0.2945,
      "step": 574
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.05505680665373802,
      "learning_rate": 0.00013649025069637885,
      "loss": 0.3148,
      "step": 575
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.048953715711832047,
      "learning_rate": 0.00013637883008356546,
      "loss": 0.3288,
      "step": 576
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.04455683007836342,
      "learning_rate": 0.0001362674094707521,
      "loss": 0.3008,
      "step": 577
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.042805854231119156,
      "learning_rate": 0.00013615598885793873,
      "loss": 0.3032,
      "step": 578
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.05032384768128395,
      "learning_rate": 0.00013604456824512534,
      "loss": 0.2927,
      "step": 579
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.05536658689379692,
      "learning_rate": 0.00013593314763231198,
      "loss": 0.2686,
      "step": 580
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.04495309293270111,
      "learning_rate": 0.0001358217270194986,
      "loss": 0.3429,
      "step": 581
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.04196522757411003,
      "learning_rate": 0.00013571030640668525,
      "loss": 0.2481,
      "step": 582
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.05041138082742691,
      "learning_rate": 0.00013559888579387188,
      "loss": 0.2846,
      "step": 583
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.04851841181516647,
      "learning_rate": 0.0001354874651810585,
      "loss": 0.303,
      "step": 584
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.05022190511226654,
      "learning_rate": 0.00013537604456824513,
      "loss": 0.3512,
      "step": 585
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.0567840076982975,
      "learning_rate": 0.00013526462395543177,
      "loss": 0.3161,
      "step": 586
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 0.059770599007606506,
      "learning_rate": 0.00013515320334261838,
      "loss": 0.3093,
      "step": 587
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.040113139897584915,
      "learning_rate": 0.000135041782729805,
      "loss": 0.2707,
      "step": 588
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.05541342496871948,
      "learning_rate": 0.00013493036211699165,
      "loss": 0.3682,
      "step": 589
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.048269204795360565,
      "learning_rate": 0.00013481894150417829,
      "loss": 0.3378,
      "step": 590
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.03636118397116661,
      "learning_rate": 0.00013470752089136492,
      "loss": 0.2207,
      "step": 591
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.09315942227840424,
      "learning_rate": 0.00013459610027855153,
      "loss": 0.3986,
      "step": 592
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.050616469234228134,
      "learning_rate": 0.00013448467966573817,
      "loss": 0.3024,
      "step": 593
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.08030980825424194,
      "learning_rate": 0.0001343732590529248,
      "loss": 0.2733,
      "step": 594
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.06223587691783905,
      "learning_rate": 0.00013426183844011141,
      "loss": 0.3,
      "step": 595
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.05033952370285988,
      "learning_rate": 0.00013415041782729805,
      "loss": 0.3118,
      "step": 596
    },
    {
      "epoch": 0.995,
      "grad_norm": 0.08084849268198013,
      "learning_rate": 0.0001340389972144847,
      "loss": 0.4004,
      "step": 597
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.07659777998924255,
      "learning_rate": 0.00013392757660167132,
      "loss": 0.3137,
      "step": 598
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.07017462700605392,
      "learning_rate": 0.00013381615598885796,
      "loss": 0.3972,
      "step": 599
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.05202601104974747,
      "learning_rate": 0.00013370473537604457,
      "loss": 0.3223,
      "step": 600
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3265322744846344,
      "eval_runtime": 373.6729,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 600
    },
    {
      "epoch": 1.0016666666666667,
      "grad_norm": 0.037545207887887955,
      "learning_rate": 0.0001335933147632312,
      "loss": 0.2446,
      "step": 601
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.08388081938028336,
      "learning_rate": 0.00013348189415041784,
      "loss": 0.3709,
      "step": 602
    },
    {
      "epoch": 1.005,
      "grad_norm": 0.06967391073703766,
      "learning_rate": 0.00013337047353760445,
      "loss": 0.3443,
      "step": 603
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.04108358919620514,
      "learning_rate": 0.0001332590529247911,
      "loss": 0.298,
      "step": 604
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.045270971953868866,
      "learning_rate": 0.00013314763231197772,
      "loss": 0.2757,
      "step": 605
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.0656428337097168,
      "learning_rate": 0.00013303621169916436,
      "loss": 0.3413,
      "step": 606
    },
    {
      "epoch": 1.0116666666666667,
      "grad_norm": 0.08023396134376526,
      "learning_rate": 0.000132924791086351,
      "loss": 0.4167,
      "step": 607
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.07055976986885071,
      "learning_rate": 0.0001328133704735376,
      "loss": 0.3589,
      "step": 608
    },
    {
      "epoch": 1.015,
      "grad_norm": 0.053702279925346375,
      "learning_rate": 0.00013270194986072424,
      "loss": 0.3138,
      "step": 609
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.05079668387770653,
      "learning_rate": 0.00013259052924791085,
      "loss": 0.2567,
      "step": 610
    },
    {
      "epoch": 1.0183333333333333,
      "grad_norm": 0.08691687881946564,
      "learning_rate": 0.0001324791086350975,
      "loss": 0.3413,
      "step": 611
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.05738988518714905,
      "learning_rate": 0.00013236768802228413,
      "loss": 0.3101,
      "step": 612
    },
    {
      "epoch": 1.0216666666666667,
      "grad_norm": 0.057901643216609955,
      "learning_rate": 0.00013225626740947076,
      "loss": 0.3374,
      "step": 613
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.0619928203523159,
      "learning_rate": 0.0001321448467966574,
      "loss": 0.3387,
      "step": 614
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.062190569937229156,
      "learning_rate": 0.00013203342618384404,
      "loss": 0.3594,
      "step": 615
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.043581511825323105,
      "learning_rate": 0.00013192200557103064,
      "loss": 0.2665,
      "step": 616
    },
    {
      "epoch": 1.0283333333333333,
      "grad_norm": 0.07001938670873642,
      "learning_rate": 0.00013181058495821728,
      "loss": 0.3811,
      "step": 617
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.053179021924734116,
      "learning_rate": 0.0001316991643454039,
      "loss": 0.3497,
      "step": 618
    },
    {
      "epoch": 1.0316666666666667,
      "grad_norm": 0.05566834285855293,
      "learning_rate": 0.00013158774373259053,
      "loss": 0.3463,
      "step": 619
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.057344164699316025,
      "learning_rate": 0.00013147632311977716,
      "loss": 0.2846,
      "step": 620
    },
    {
      "epoch": 1.035,
      "grad_norm": 0.1186261847615242,
      "learning_rate": 0.0001313649025069638,
      "loss": 0.3998,
      "step": 621
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.052325136959552765,
      "learning_rate": 0.00013125348189415044,
      "loss": 0.2836,
      "step": 622
    },
    {
      "epoch": 1.0383333333333333,
      "grad_norm": 0.049695033580064774,
      "learning_rate": 0.00013114206128133705,
      "loss": 0.2996,
      "step": 623
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.06760648638010025,
      "learning_rate": 0.00013103064066852368,
      "loss": 0.3445,
      "step": 624
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.04542970657348633,
      "learning_rate": 0.00013091922005571032,
      "loss": 0.3142,
      "step": 625
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.051388125866651535,
      "learning_rate": 0.00013080779944289693,
      "loss": 0.2817,
      "step": 626
    },
    {
      "epoch": 1.045,
      "grad_norm": 0.051026877015829086,
      "learning_rate": 0.00013069637883008356,
      "loss": 0.2982,
      "step": 627
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.05956486985087395,
      "learning_rate": 0.0001305849582172702,
      "loss": 0.362,
      "step": 628
    },
    {
      "epoch": 1.0483333333333333,
      "grad_norm": 0.044415973126888275,
      "learning_rate": 0.00013047353760445684,
      "loss": 0.2259,
      "step": 629
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.07529677450656891,
      "learning_rate": 0.00013036211699164347,
      "loss": 0.349,
      "step": 630
    },
    {
      "epoch": 1.0516666666666667,
      "grad_norm": 0.0490957573056221,
      "learning_rate": 0.00013025069637883008,
      "loss": 0.2862,
      "step": 631
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.04915102198719978,
      "learning_rate": 0.00013013927576601672,
      "loss": 0.3224,
      "step": 632
    },
    {
      "epoch": 1.055,
      "grad_norm": 0.04311239719390869,
      "learning_rate": 0.00013002785515320336,
      "loss": 0.2879,
      "step": 633
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.045873116701841354,
      "learning_rate": 0.00012991643454038997,
      "loss": 0.2648,
      "step": 634
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.05527368187904358,
      "learning_rate": 0.0001298050139275766,
      "loss": 0.3109,
      "step": 635
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.035854704678058624,
      "learning_rate": 0.00012969359331476324,
      "loss": 0.2428,
      "step": 636
    },
    {
      "epoch": 1.0616666666666668,
      "grad_norm": 0.06005294248461723,
      "learning_rate": 0.00012958217270194988,
      "loss": 0.343,
      "step": 637
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.05582824721932411,
      "learning_rate": 0.0001294707520891365,
      "loss": 0.3251,
      "step": 638
    },
    {
      "epoch": 1.065,
      "grad_norm": 0.050943125039339066,
      "learning_rate": 0.00012935933147632312,
      "loss": 0.2672,
      "step": 639
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.06783857941627502,
      "learning_rate": 0.00012924791086350976,
      "loss": 0.3331,
      "step": 640
    },
    {
      "epoch": 1.0683333333333334,
      "grad_norm": 0.05104564130306244,
      "learning_rate": 0.0001291364902506964,
      "loss": 0.3351,
      "step": 641
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.04604925960302353,
      "learning_rate": 0.000129025069637883,
      "loss": 0.2915,
      "step": 642
    },
    {
      "epoch": 1.0716666666666668,
      "grad_norm": 0.036190032958984375,
      "learning_rate": 0.00012891364902506964,
      "loss": 0.2309,
      "step": 643
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.05398710444569588,
      "learning_rate": 0.00012880222841225628,
      "loss": 0.265,
      "step": 644
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.042475681751966476,
      "learning_rate": 0.0001286908077994429,
      "loss": 0.2565,
      "step": 645
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.044244058430194855,
      "learning_rate": 0.00012857938718662955,
      "loss": 0.3151,
      "step": 646
    },
    {
      "epoch": 1.0783333333333334,
      "grad_norm": 0.050446126610040665,
      "learning_rate": 0.00012846796657381616,
      "loss": 0.321,
      "step": 647
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.04219895973801613,
      "learning_rate": 0.0001283565459610028,
      "loss": 0.3043,
      "step": 648
    },
    {
      "epoch": 1.0816666666666666,
      "grad_norm": 0.046174537390470505,
      "learning_rate": 0.00012824512534818943,
      "loss": 0.2782,
      "step": 649
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.05947514995932579,
      "learning_rate": 0.00012813370473537604,
      "loss": 0.3848,
      "step": 650
    },
    {
      "epoch": 1.085,
      "grad_norm": 0.04840335249900818,
      "learning_rate": 0.00012802228412256268,
      "loss": 0.3127,
      "step": 651
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.04292229190468788,
      "learning_rate": 0.00012791086350974931,
      "loss": 0.2783,
      "step": 652
    },
    {
      "epoch": 1.0883333333333334,
      "grad_norm": 0.0456630177795887,
      "learning_rate": 0.00012779944289693595,
      "loss": 0.3099,
      "step": 653
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.07853838801383972,
      "learning_rate": 0.0001276880222841226,
      "loss": 0.2929,
      "step": 654
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.05258505791425705,
      "learning_rate": 0.0001275766016713092,
      "loss": 0.2848,
      "step": 655
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.04929404705762863,
      "learning_rate": 0.00012746518105849583,
      "loss": 0.296,
      "step": 656
    },
    {
      "epoch": 1.095,
      "grad_norm": 0.06462109833955765,
      "learning_rate": 0.00012735376044568247,
      "loss": 0.3871,
      "step": 657
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.041407350450754166,
      "learning_rate": 0.00012724233983286908,
      "loss": 0.2242,
      "step": 658
    },
    {
      "epoch": 1.0983333333333334,
      "grad_norm": 0.04740098491311073,
      "learning_rate": 0.00012713091922005572,
      "loss": 0.3243,
      "step": 659
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.05279053747653961,
      "learning_rate": 0.00012701949860724232,
      "loss": 0.3107,
      "step": 660
    },
    {
      "epoch": 1.1016666666666666,
      "grad_norm": 0.05239010229706764,
      "learning_rate": 0.00012690807799442896,
      "loss": 0.2879,
      "step": 661
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.04420118406414986,
      "learning_rate": 0.00012679665738161562,
      "loss": 0.2619,
      "step": 662
    },
    {
      "epoch": 1.105,
      "grad_norm": 0.06074133142828941,
      "learning_rate": 0.00012668523676880223,
      "loss": 0.3508,
      "step": 663
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.04647393152117729,
      "learning_rate": 0.00012657381615598887,
      "loss": 0.3193,
      "step": 664
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.04810037836432457,
      "learning_rate": 0.0001264623955431755,
      "loss": 0.2773,
      "step": 665
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.05008118599653244,
      "learning_rate": 0.00012635097493036212,
      "loss": 0.3132,
      "step": 666
    },
    {
      "epoch": 1.1116666666666666,
      "grad_norm": 0.06312387436628342,
      "learning_rate": 0.00012623955431754875,
      "loss": 0.3909,
      "step": 667
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.04194491729140282,
      "learning_rate": 0.00012612813370473536,
      "loss": 0.2871,
      "step": 668
    },
    {
      "epoch": 1.115,
      "grad_norm": 0.07597023993730545,
      "learning_rate": 0.000126016713091922,
      "loss": 0.3848,
      "step": 669
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.06192773953080177,
      "learning_rate": 0.00012590529247910866,
      "loss": 0.3859,
      "step": 670
    },
    {
      "epoch": 1.1183333333333334,
      "grad_norm": 0.04794010892510414,
      "learning_rate": 0.00012579387186629527,
      "loss": 0.2845,
      "step": 671
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.04922955110669136,
      "learning_rate": 0.0001256824512534819,
      "loss": 0.3181,
      "step": 672
    },
    {
      "epoch": 1.1216666666666666,
      "grad_norm": 0.05445132032036781,
      "learning_rate": 0.00012557103064066852,
      "loss": 0.3717,
      "step": 673
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.049028895795345306,
      "learning_rate": 0.00012545961002785515,
      "loss": 0.3412,
      "step": 674
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.0510074719786644,
      "learning_rate": 0.0001253481894150418,
      "loss": 0.2722,
      "step": 675
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.05593949183821678,
      "learning_rate": 0.0001252367688022284,
      "loss": 0.3587,
      "step": 676
    },
    {
      "epoch": 1.1283333333333334,
      "grad_norm": 0.044265806674957275,
      "learning_rate": 0.00012512534818941504,
      "loss": 0.2558,
      "step": 677
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.044104140251874924,
      "learning_rate": 0.00012501392757660167,
      "loss": 0.2673,
      "step": 678
    },
    {
      "epoch": 1.1316666666666666,
      "grad_norm": 0.04890120029449463,
      "learning_rate": 0.0001249025069637883,
      "loss": 0.365,
      "step": 679
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.04026368260383606,
      "learning_rate": 0.00012479108635097495,
      "loss": 0.2255,
      "step": 680
    },
    {
      "epoch": 1.135,
      "grad_norm": 0.08276694267988205,
      "learning_rate": 0.00012467966573816156,
      "loss": 0.3267,
      "step": 681
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.049490395933389664,
      "learning_rate": 0.0001245682451253482,
      "loss": 0.2949,
      "step": 682
    },
    {
      "epoch": 1.1383333333333334,
      "grad_norm": 0.04618123918771744,
      "learning_rate": 0.00012445682451253483,
      "loss": 0.2721,
      "step": 683
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.06250430643558502,
      "learning_rate": 0.00012434540389972144,
      "loss": 0.3321,
      "step": 684
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.05551249533891678,
      "learning_rate": 0.00012423398328690807,
      "loss": 0.3071,
      "step": 685
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 0.06421687453985214,
      "learning_rate": 0.0001241225626740947,
      "loss": 0.3623,
      "step": 686
    },
    {
      "epoch": 1.145,
      "grad_norm": 0.04292492941021919,
      "learning_rate": 0.00012401114206128135,
      "loss": 0.2867,
      "step": 687
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.05317793786525726,
      "learning_rate": 0.00012389972144846798,
      "loss": 0.3102,
      "step": 688
    },
    {
      "epoch": 1.1483333333333334,
      "grad_norm": 0.04448207840323448,
      "learning_rate": 0.0001237883008356546,
      "loss": 0.2642,
      "step": 689
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.05929863080382347,
      "learning_rate": 0.00012367688022284123,
      "loss": 0.3923,
      "step": 690
    },
    {
      "epoch": 1.1516666666666666,
      "grad_norm": 0.04860999807715416,
      "learning_rate": 0.00012356545961002787,
      "loss": 0.2719,
      "step": 691
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.04562104120850563,
      "learning_rate": 0.00012345403899721448,
      "loss": 0.268,
      "step": 692
    },
    {
      "epoch": 1.155,
      "grad_norm": 0.05551109462976456,
      "learning_rate": 0.0001233426183844011,
      "loss": 0.3578,
      "step": 693
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.03982888534665108,
      "learning_rate": 0.00012323119777158775,
      "loss": 0.2445,
      "step": 694
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.04497479274868965,
      "learning_rate": 0.00012311977715877438,
      "loss": 0.2381,
      "step": 695
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.05202760547399521,
      "learning_rate": 0.00012300835654596102,
      "loss": 0.3109,
      "step": 696
    },
    {
      "epoch": 1.1616666666666666,
      "grad_norm": 0.05425313487648964,
      "learning_rate": 0.00012289693593314763,
      "loss": 0.3047,
      "step": 697
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.043152473866939545,
      "learning_rate": 0.00012278551532033427,
      "loss": 0.3205,
      "step": 698
    },
    {
      "epoch": 1.165,
      "grad_norm": 0.04097166657447815,
      "learning_rate": 0.0001226740947075209,
      "loss": 0.2996,
      "step": 699
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.05484712868928909,
      "learning_rate": 0.0001225626740947075,
      "loss": 0.2938,
      "step": 700
    },
    {
      "epoch": 1.1666666666666667,
      "eval_loss": 0.3249437212944031,
      "eval_runtime": 373.6794,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 700
    },
    {
      "epoch": 1.1683333333333334,
      "grad_norm": 0.04848772659897804,
      "learning_rate": 0.00012245125348189415,
      "loss": 0.3363,
      "step": 701
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.04571622982621193,
      "learning_rate": 0.00012233983286908079,
      "loss": 0.2947,
      "step": 702
    },
    {
      "epoch": 1.1716666666666666,
      "grad_norm": 0.05122512951493263,
      "learning_rate": 0.00012222841225626742,
      "loss": 0.2932,
      "step": 703
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.04593171924352646,
      "learning_rate": 0.00012211699164345406,
      "loss": 0.2591,
      "step": 704
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.059837035834789276,
      "learning_rate": 0.00012200557103064068,
      "loss": 0.337,
      "step": 705
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.055959466844797134,
      "learning_rate": 0.0001218941504178273,
      "loss": 0.3511,
      "step": 706
    },
    {
      "epoch": 1.1783333333333332,
      "grad_norm": 0.038410793989896774,
      "learning_rate": 0.00012178272980501393,
      "loss": 0.2529,
      "step": 707
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.06779003888368607,
      "learning_rate": 0.00012167130919220055,
      "loss": 0.3749,
      "step": 708
    },
    {
      "epoch": 1.1816666666666666,
      "grad_norm": 0.06086283177137375,
      "learning_rate": 0.00012155988857938719,
      "loss": 0.3096,
      "step": 709
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.06595747172832489,
      "learning_rate": 0.00012144846796657384,
      "loss": 0.3591,
      "step": 710
    },
    {
      "epoch": 1.185,
      "grad_norm": 0.054579734802246094,
      "learning_rate": 0.00012133704735376046,
      "loss": 0.3346,
      "step": 711
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.05444353446364403,
      "learning_rate": 0.00012122562674094708,
      "loss": 0.3249,
      "step": 712
    },
    {
      "epoch": 1.1883333333333332,
      "grad_norm": 0.05019025877118111,
      "learning_rate": 0.0001211142061281337,
      "loss": 0.3118,
      "step": 713
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.04386289417743683,
      "learning_rate": 0.00012100278551532034,
      "loss": 0.3329,
      "step": 714
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.050726648420095444,
      "learning_rate": 0.00012089136490250697,
      "loss": 0.3133,
      "step": 715
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.05836929753422737,
      "learning_rate": 0.00012077994428969359,
      "loss": 0.3348,
      "step": 716
    },
    {
      "epoch": 1.195,
      "grad_norm": 0.06014572083950043,
      "learning_rate": 0.00012066852367688022,
      "loss": 0.3553,
      "step": 717
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.0473906435072422,
      "learning_rate": 0.00012055710306406686,
      "loss": 0.3165,
      "step": 718
    },
    {
      "epoch": 1.1983333333333333,
      "grad_norm": 0.04099196940660477,
      "learning_rate": 0.0001204456824512535,
      "loss": 0.3445,
      "step": 719
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.051116038113832474,
      "learning_rate": 0.00012033426183844012,
      "loss": 0.3094,
      "step": 720
    },
    {
      "epoch": 1.2016666666666667,
      "grad_norm": 0.05735861882567406,
      "learning_rate": 0.00012022284122562674,
      "loss": 0.336,
      "step": 721
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.05691840872168541,
      "learning_rate": 0.00012011142061281338,
      "loss": 0.3509,
      "step": 722
    },
    {
      "epoch": 1.205,
      "grad_norm": 0.06288771331310272,
      "learning_rate": 0.00012,
      "loss": 0.3615,
      "step": 723
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.0397581048309803,
      "learning_rate": 0.00011988857938718663,
      "loss": 0.2513,
      "step": 724
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.059119731187820435,
      "learning_rate": 0.00011977715877437325,
      "loss": 0.3134,
      "step": 725
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.03636254742741585,
      "learning_rate": 0.0001196657381615599,
      "loss": 0.2961,
      "step": 726
    },
    {
      "epoch": 1.2116666666666667,
      "grad_norm": 0.038502391427755356,
      "learning_rate": 0.00011955431754874654,
      "loss": 0.235,
      "step": 727
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.07571970671415329,
      "learning_rate": 0.00011944289693593316,
      "loss": 0.3192,
      "step": 728
    },
    {
      "epoch": 1.215,
      "grad_norm": 0.04825633019208908,
      "learning_rate": 0.00011933147632311978,
      "loss": 0.2901,
      "step": 729
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.04018886014819145,
      "learning_rate": 0.0001192200557103064,
      "loss": 0.2673,
      "step": 730
    },
    {
      "epoch": 1.2183333333333333,
      "grad_norm": 0.04971102625131607,
      "learning_rate": 0.00011910863509749304,
      "loss": 0.3042,
      "step": 731
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.04221099615097046,
      "learning_rate": 0.00011899721448467966,
      "loss": 0.2827,
      "step": 732
    },
    {
      "epoch": 1.2216666666666667,
      "grad_norm": 0.06669100373983383,
      "learning_rate": 0.00011888579387186629,
      "loss": 0.3748,
      "step": 733
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.0487256795167923,
      "learning_rate": 0.00011877437325905294,
      "loss": 0.3002,
      "step": 734
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.05250631272792816,
      "learning_rate": 0.00011866295264623957,
      "loss": 0.3375,
      "step": 735
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.053504686802625656,
      "learning_rate": 0.0001185515320334262,
      "loss": 0.3242,
      "step": 736
    },
    {
      "epoch": 1.2283333333333333,
      "grad_norm": 0.061938539147377014,
      "learning_rate": 0.00011844011142061282,
      "loss": 0.3111,
      "step": 737
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.07530166953802109,
      "learning_rate": 0.00011832869080779944,
      "loss": 0.3552,
      "step": 738
    },
    {
      "epoch": 1.2316666666666667,
      "grad_norm": 0.05315600335597992,
      "learning_rate": 0.00011821727019498608,
      "loss": 0.2973,
      "step": 739
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.08840741962194443,
      "learning_rate": 0.0001181058495821727,
      "loss": 0.3165,
      "step": 740
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 0.06103053316473961,
      "learning_rate": 0.00011799442896935932,
      "loss": 0.3917,
      "step": 741
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 0.037969931960105896,
      "learning_rate": 0.00011788300835654597,
      "loss": 0.2706,
      "step": 742
    },
    {
      "epoch": 1.2383333333333333,
      "grad_norm": 0.05460311099886894,
      "learning_rate": 0.0001177715877437326,
      "loss": 0.3068,
      "step": 743
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.05513601377606392,
      "learning_rate": 0.00011766016713091923,
      "loss": 0.3605,
      "step": 744
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.0642230361700058,
      "learning_rate": 0.00011754874651810586,
      "loss": 0.3298,
      "step": 745
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.047271471470594406,
      "learning_rate": 0.00011743732590529248,
      "loss": 0.3171,
      "step": 746
    },
    {
      "epoch": 1.245,
      "grad_norm": 0.0600745715200901,
      "learning_rate": 0.00011732590529247912,
      "loss": 0.3494,
      "step": 747
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.045074548572301865,
      "learning_rate": 0.00011721448467966574,
      "loss": 0.2922,
      "step": 748
    },
    {
      "epoch": 1.2483333333333333,
      "grad_norm": 0.043050993233919144,
      "learning_rate": 0.00011710306406685236,
      "loss": 0.2634,
      "step": 749
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.08761952072381973,
      "learning_rate": 0.00011699164345403901,
      "loss": 0.4191,
      "step": 750
    },
    {
      "epoch": 1.2516666666666667,
      "grad_norm": 0.05572642385959625,
      "learning_rate": 0.00011688022284122563,
      "loss": 0.3562,
      "step": 751
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.047182947397232056,
      "learning_rate": 0.00011676880222841227,
      "loss": 0.3483,
      "step": 752
    },
    {
      "epoch": 1.255,
      "grad_norm": 0.0442778579890728,
      "learning_rate": 0.0001166573816155989,
      "loss": 0.2758,
      "step": 753
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.04973537102341652,
      "learning_rate": 0.00011654596100278552,
      "loss": 0.321,
      "step": 754
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.04727261886000633,
      "learning_rate": 0.00011643454038997214,
      "loss": 0.3255,
      "step": 755
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.0512387789785862,
      "learning_rate": 0.00011632311977715878,
      "loss": 0.2962,
      "step": 756
    },
    {
      "epoch": 1.2616666666666667,
      "grad_norm": 0.041468240320682526,
      "learning_rate": 0.0001162116991643454,
      "loss": 0.3404,
      "step": 757
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.054209623485803604,
      "learning_rate": 0.00011610027855153205,
      "loss": 0.3585,
      "step": 758
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 0.09473873674869537,
      "learning_rate": 0.00011598885793871867,
      "loss": 0.377,
      "step": 759
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.053749192506074905,
      "learning_rate": 0.00011587743732590531,
      "loss": 0.3183,
      "step": 760
    },
    {
      "epoch": 1.2683333333333333,
      "grad_norm": 0.04816459119319916,
      "learning_rate": 0.00011576601671309193,
      "loss": 0.3077,
      "step": 761
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.050339337438344955,
      "learning_rate": 0.00011565459610027855,
      "loss": 0.3323,
      "step": 762
    },
    {
      "epoch": 1.2716666666666667,
      "grad_norm": 0.04293170943856239,
      "learning_rate": 0.00011554317548746518,
      "loss": 0.3046,
      "step": 763
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.03964214399456978,
      "learning_rate": 0.00011543175487465181,
      "loss": 0.2726,
      "step": 764
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.053622957319021225,
      "learning_rate": 0.00011532033426183844,
      "loss": 0.3441,
      "step": 765
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.060688044875860214,
      "learning_rate": 0.00011520891364902509,
      "loss": 0.3651,
      "step": 766
    },
    {
      "epoch": 1.2783333333333333,
      "grad_norm": 0.04049067944288254,
      "learning_rate": 0.00011509749303621171,
      "loss": 0.2514,
      "step": 767
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.04279336705803871,
      "learning_rate": 0.00011498607242339833,
      "loss": 0.3028,
      "step": 768
    },
    {
      "epoch": 1.2816666666666667,
      "grad_norm": 0.03884007781744003,
      "learning_rate": 0.00011487465181058497,
      "loss": 0.2732,
      "step": 769
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.04321003705263138,
      "learning_rate": 0.00011476323119777159,
      "loss": 0.297,
      "step": 770
    },
    {
      "epoch": 1.285,
      "grad_norm": 0.04297022521495819,
      "learning_rate": 0.00011465181058495822,
      "loss": 0.3093,
      "step": 771
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.055295005440711975,
      "learning_rate": 0.00011454038997214485,
      "loss": 0.3599,
      "step": 772
    },
    {
      "epoch": 1.2883333333333333,
      "grad_norm": 0.04671638831496239,
      "learning_rate": 0.00011442896935933147,
      "loss": 0.3178,
      "step": 773
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.047454483807086945,
      "learning_rate": 0.00011431754874651812,
      "loss": 0.2839,
      "step": 774
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.04256737232208252,
      "learning_rate": 0.00011420612813370475,
      "loss": 0.2889,
      "step": 775
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.06473991274833679,
      "learning_rate": 0.00011409470752089137,
      "loss": 0.4239,
      "step": 776
    },
    {
      "epoch": 1.295,
      "grad_norm": 0.05683406814932823,
      "learning_rate": 0.00011398328690807801,
      "loss": 0.3071,
      "step": 777
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.05049443989992142,
      "learning_rate": 0.00011387186629526463,
      "loss": 0.3661,
      "step": 778
    },
    {
      "epoch": 1.2983333333333333,
      "grad_norm": 0.05286366119980812,
      "learning_rate": 0.00011376044568245125,
      "loss": 0.3027,
      "step": 779
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.0628724992275238,
      "learning_rate": 0.00011364902506963788,
      "loss": 0.405,
      "step": 780
    },
    {
      "epoch": 1.3016666666666667,
      "grad_norm": 0.05469696968793869,
      "learning_rate": 0.00011353760445682451,
      "loss": 0.3684,
      "step": 781
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.06705069541931152,
      "learning_rate": 0.00011342618384401116,
      "loss": 0.3751,
      "step": 782
    },
    {
      "epoch": 1.305,
      "grad_norm": 0.09377814084291458,
      "learning_rate": 0.00011331476323119779,
      "loss": 0.3359,
      "step": 783
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.048227708786726,
      "learning_rate": 0.00011320334261838441,
      "loss": 0.2308,
      "step": 784
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.049991101026535034,
      "learning_rate": 0.00011309192200557104,
      "loss": 0.3131,
      "step": 785
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.05291355028748512,
      "learning_rate": 0.00011298050139275767,
      "loss": 0.2895,
      "step": 786
    },
    {
      "epoch": 1.3116666666666665,
      "grad_norm": 0.04944576695561409,
      "learning_rate": 0.00011286908077994429,
      "loss": 0.2845,
      "step": 787
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.06204497069120407,
      "learning_rate": 0.00011275766016713091,
      "loss": 0.3096,
      "step": 788
    },
    {
      "epoch": 1.315,
      "grad_norm": 0.05431002005934715,
      "learning_rate": 0.00011264623955431755,
      "loss": 0.3263,
      "step": 789
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.04042857140302658,
      "learning_rate": 0.0001125348189415042,
      "loss": 0.262,
      "step": 790
    },
    {
      "epoch": 1.3183333333333334,
      "grad_norm": 0.06940200179815292,
      "learning_rate": 0.00011242339832869082,
      "loss": 0.3723,
      "step": 791
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.05204876884818077,
      "learning_rate": 0.00011231197771587745,
      "loss": 0.3301,
      "step": 792
    },
    {
      "epoch": 1.3216666666666668,
      "grad_norm": 0.040174610912799835,
      "learning_rate": 0.00011220055710306407,
      "loss": 0.2486,
      "step": 793
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.0440058633685112,
      "learning_rate": 0.0001120891364902507,
      "loss": 0.2952,
      "step": 794
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.08946824073791504,
      "learning_rate": 0.00011197771587743733,
      "loss": 0.36,
      "step": 795
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.0423639714717865,
      "learning_rate": 0.00011186629526462395,
      "loss": 0.2777,
      "step": 796
    },
    {
      "epoch": 1.3283333333333334,
      "grad_norm": 0.05461345985531807,
      "learning_rate": 0.00011175487465181059,
      "loss": 0.356,
      "step": 797
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.07325074076652527,
      "learning_rate": 0.00011164345403899722,
      "loss": 0.3202,
      "step": 798
    },
    {
      "epoch": 1.3316666666666666,
      "grad_norm": 0.05088663473725319,
      "learning_rate": 0.00011153203342618386,
      "loss": 0.3882,
      "step": 799
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.03988111764192581,
      "learning_rate": 0.00011142061281337048,
      "loss": 0.2689,
      "step": 800
    },
    {
      "epoch": 1.3333333333333333,
      "eval_loss": 0.3240317404270172,
      "eval_runtime": 373.4348,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 800
    },
    {
      "epoch": 1.335,
      "grad_norm": 0.057641930878162384,
      "learning_rate": 0.0001113091922005571,
      "loss": 0.3424,
      "step": 801
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.04876812547445297,
      "learning_rate": 0.00011119777158774374,
      "loss": 0.2695,
      "step": 802
    },
    {
      "epoch": 1.3383333333333334,
      "grad_norm": 0.046650100499391556,
      "learning_rate": 0.00011108635097493037,
      "loss": 0.3129,
      "step": 803
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.06726518273353577,
      "learning_rate": 0.00011097493036211699,
      "loss": 0.2987,
      "step": 804
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.04402477294206619,
      "learning_rate": 0.00011086350974930361,
      "loss": 0.2983,
      "step": 805
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.0868586078286171,
      "learning_rate": 0.00011075208913649026,
      "loss": 0.3249,
      "step": 806
    },
    {
      "epoch": 1.345,
      "grad_norm": 0.07503216713666916,
      "learning_rate": 0.0001106406685236769,
      "loss": 0.3765,
      "step": 807
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.06562774628400803,
      "learning_rate": 0.00011052924791086352,
      "loss": 0.315,
      "step": 808
    },
    {
      "epoch": 1.3483333333333334,
      "grad_norm": 0.04028520733118057,
      "learning_rate": 0.00011041782729805014,
      "loss": 0.2924,
      "step": 809
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.04509858042001724,
      "learning_rate": 0.00011030640668523677,
      "loss": 0.3295,
      "step": 810
    },
    {
      "epoch": 1.3516666666666666,
      "grad_norm": 0.06333824247121811,
      "learning_rate": 0.0001101949860724234,
      "loss": 0.3175,
      "step": 811
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.059569306671619415,
      "learning_rate": 0.00011008356545961003,
      "loss": 0.3195,
      "step": 812
    },
    {
      "epoch": 1.355,
      "grad_norm": 0.03712876886129379,
      "learning_rate": 0.00010997214484679665,
      "loss": 0.2937,
      "step": 813
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 0.04424058645963669,
      "learning_rate": 0.0001098607242339833,
      "loss": 0.309,
      "step": 814
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.05646035447716713,
      "learning_rate": 0.00010974930362116994,
      "loss": 0.3453,
      "step": 815
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.04758371412754059,
      "learning_rate": 0.00010963788300835656,
      "loss": 0.2891,
      "step": 816
    },
    {
      "epoch": 1.3616666666666668,
      "grad_norm": 0.05409663915634155,
      "learning_rate": 0.00010952646239554318,
      "loss": 0.3051,
      "step": 817
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.04837237671017647,
      "learning_rate": 0.0001094150417827298,
      "loss": 0.325,
      "step": 818
    },
    {
      "epoch": 1.365,
      "grad_norm": 0.03936009854078293,
      "learning_rate": 0.00010930362116991644,
      "loss": 0.2759,
      "step": 819
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.05668478459119797,
      "learning_rate": 0.00010919220055710306,
      "loss": 0.3206,
      "step": 820
    },
    {
      "epoch": 1.3683333333333334,
      "grad_norm": 0.06145121902227402,
      "learning_rate": 0.00010908077994428969,
      "loss": 0.3111,
      "step": 821
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.049643322825431824,
      "learning_rate": 0.00010896935933147632,
      "loss": 0.3238,
      "step": 822
    },
    {
      "epoch": 1.3716666666666666,
      "grad_norm": 0.04672195389866829,
      "learning_rate": 0.00010885793871866296,
      "loss": 0.2782,
      "step": 823
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.054078109562397,
      "learning_rate": 0.0001087465181058496,
      "loss": 0.317,
      "step": 824
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.08385499566793442,
      "learning_rate": 0.00010863509749303622,
      "loss": 0.3933,
      "step": 825
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.05065348371863365,
      "learning_rate": 0.00010852367688022284,
      "loss": 0.3226,
      "step": 826
    },
    {
      "epoch": 1.3783333333333334,
      "grad_norm": 0.06942520290613174,
      "learning_rate": 0.00010841225626740948,
      "loss": 0.3676,
      "step": 827
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.058958858251571655,
      "learning_rate": 0.0001083008356545961,
      "loss": 0.3398,
      "step": 828
    },
    {
      "epoch": 1.3816666666666666,
      "grad_norm": 0.0396934412419796,
      "learning_rate": 0.00010818941504178272,
      "loss": 0.2705,
      "step": 829
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.036017633974552155,
      "learning_rate": 0.00010807799442896935,
      "loss": 0.2296,
      "step": 830
    },
    {
      "epoch": 1.385,
      "grad_norm": 0.06789635866880417,
      "learning_rate": 0.000107966573816156,
      "loss": 0.4215,
      "step": 831
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.04464036226272583,
      "learning_rate": 0.00010785515320334263,
      "loss": 0.2796,
      "step": 832
    },
    {
      "epoch": 1.3883333333333332,
      "grad_norm": 0.038351599127054214,
      "learning_rate": 0.00010774373259052926,
      "loss": 0.2456,
      "step": 833
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.07164964079856873,
      "learning_rate": 0.00010763231197771588,
      "loss": 0.3818,
      "step": 834
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.054553356021642685,
      "learning_rate": 0.0001075208913649025,
      "loss": 0.2563,
      "step": 835
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.03486122563481331,
      "learning_rate": 0.00010740947075208914,
      "loss": 0.2593,
      "step": 836
    },
    {
      "epoch": 1.395,
      "grad_norm": 0.07681387662887573,
      "learning_rate": 0.00010729805013927576,
      "loss": 0.3424,
      "step": 837
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.08190979063510895,
      "learning_rate": 0.00010718662952646239,
      "loss": 0.3014,
      "step": 838
    },
    {
      "epoch": 1.3983333333333334,
      "grad_norm": 0.0377548523247242,
      "learning_rate": 0.00010707520891364904,
      "loss": 0.2923,
      "step": 839
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.04089314863085747,
      "learning_rate": 0.00010696378830083567,
      "loss": 0.2631,
      "step": 840
    },
    {
      "epoch": 1.4016666666666666,
      "grad_norm": 0.048210352659225464,
      "learning_rate": 0.0001068523676880223,
      "loss": 0.3364,
      "step": 841
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.06772482395172119,
      "learning_rate": 0.00010674094707520892,
      "loss": 0.3291,
      "step": 842
    },
    {
      "epoch": 1.405,
      "grad_norm": 0.043270956724882126,
      "learning_rate": 0.00010662952646239554,
      "loss": 0.304,
      "step": 843
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.04654589667916298,
      "learning_rate": 0.00010651810584958218,
      "loss": 0.3173,
      "step": 844
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.04368129372596741,
      "learning_rate": 0.0001064066852367688,
      "loss": 0.3324,
      "step": 845
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.048325065523386,
      "learning_rate": 0.00010629526462395542,
      "loss": 0.3019,
      "step": 846
    },
    {
      "epoch": 1.4116666666666666,
      "grad_norm": 0.04930404946208,
      "learning_rate": 0.00010618384401114207,
      "loss": 0.2915,
      "step": 847
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.05734408646821976,
      "learning_rate": 0.0001060724233983287,
      "loss": 0.2681,
      "step": 848
    },
    {
      "epoch": 1.415,
      "grad_norm": 0.0637495219707489,
      "learning_rate": 0.00010596100278551533,
      "loss": 0.3462,
      "step": 849
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.04086415842175484,
      "learning_rate": 0.00010584958217270196,
      "loss": 0.2709,
      "step": 850
    },
    {
      "epoch": 1.4183333333333334,
      "grad_norm": 0.052036724984645844,
      "learning_rate": 0.00010573816155988858,
      "loss": 0.2994,
      "step": 851
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.04468335583806038,
      "learning_rate": 0.00010562674094707521,
      "loss": 0.3165,
      "step": 852
    },
    {
      "epoch": 1.4216666666666666,
      "grad_norm": 0.03759390488266945,
      "learning_rate": 0.00010551532033426184,
      "loss": 0.2365,
      "step": 853
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.04838431254029274,
      "learning_rate": 0.00010540389972144846,
      "loss": 0.3243,
      "step": 854
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.07530464231967926,
      "learning_rate": 0.00010529247910863511,
      "loss": 0.4463,
      "step": 855
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.04751398414373398,
      "learning_rate": 0.00010518105849582173,
      "loss": 0.317,
      "step": 856
    },
    {
      "epoch": 1.4283333333333332,
      "grad_norm": 0.045988310128450394,
      "learning_rate": 0.00010506963788300837,
      "loss": 0.2684,
      "step": 857
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.06337545067071915,
      "learning_rate": 0.00010495821727019499,
      "loss": 0.3509,
      "step": 858
    },
    {
      "epoch": 1.4316666666666666,
      "grad_norm": 0.06959738582372665,
      "learning_rate": 0.00010484679665738162,
      "loss": 0.3598,
      "step": 859
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.04338691011071205,
      "learning_rate": 0.00010473537604456824,
      "loss": 0.2431,
      "step": 860
    },
    {
      "epoch": 1.435,
      "grad_norm": 0.045629557222127914,
      "learning_rate": 0.00010462395543175488,
      "loss": 0.3453,
      "step": 861
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.06907180696725845,
      "learning_rate": 0.0001045125348189415,
      "loss": 0.3901,
      "step": 862
    },
    {
      "epoch": 1.4383333333333335,
      "grad_norm": 0.05170123651623726,
      "learning_rate": 0.00010440111420612815,
      "loss": 0.3391,
      "step": 863
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.05284053087234497,
      "learning_rate": 0.00010428969359331477,
      "loss": 0.3646,
      "step": 864
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.047487154603004456,
      "learning_rate": 0.00010417827298050141,
      "loss": 0.3285,
      "step": 865
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 0.034828055649995804,
      "learning_rate": 0.00010406685236768803,
      "loss": 0.2746,
      "step": 866
    },
    {
      "epoch": 1.445,
      "grad_norm": 0.03969263657927513,
      "learning_rate": 0.00010395543175487465,
      "loss": 0.255,
      "step": 867
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.046008750796318054,
      "learning_rate": 0.00010384401114206128,
      "loss": 0.2987,
      "step": 868
    },
    {
      "epoch": 1.4483333333333333,
      "grad_norm": 0.05475010350346565,
      "learning_rate": 0.00010373259052924791,
      "loss": 0.3364,
      "step": 869
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.05276951193809509,
      "learning_rate": 0.00010362116991643454,
      "loss": 0.3285,
      "step": 870
    },
    {
      "epoch": 1.4516666666666667,
      "grad_norm": 0.0450415275990963,
      "learning_rate": 0.00010350974930362119,
      "loss": 0.3304,
      "step": 871
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.044550783932209015,
      "learning_rate": 0.00010339832869080781,
      "loss": 0.3144,
      "step": 872
    },
    {
      "epoch": 1.455,
      "grad_norm": 0.07092040777206421,
      "learning_rate": 0.00010328690807799443,
      "loss": 0.3759,
      "step": 873
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.09034174680709839,
      "learning_rate": 0.00010317548746518107,
      "loss": 0.3133,
      "step": 874
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.050970710813999176,
      "learning_rate": 0.00010306406685236769,
      "loss": 0.3221,
      "step": 875
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.06288238614797592,
      "learning_rate": 0.00010295264623955431,
      "loss": 0.3128,
      "step": 876
    },
    {
      "epoch": 1.4616666666666667,
      "grad_norm": 0.04106535017490387,
      "learning_rate": 0.00010284122562674095,
      "loss": 0.3279,
      "step": 877
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.04693116992712021,
      "learning_rate": 0.00010272980501392757,
      "loss": 0.3173,
      "step": 878
    },
    {
      "epoch": 1.465,
      "grad_norm": 0.05071249604225159,
      "learning_rate": 0.00010261838440111422,
      "loss": 0.2986,
      "step": 879
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.049158524721860886,
      "learning_rate": 0.00010250696378830085,
      "loss": 0.2869,
      "step": 880
    },
    {
      "epoch": 1.4683333333333333,
      "grad_norm": 0.047643452882766724,
      "learning_rate": 0.00010239554317548747,
      "loss": 0.3447,
      "step": 881
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.03984949365258217,
      "learning_rate": 0.0001022841225626741,
      "loss": 0.2787,
      "step": 882
    },
    {
      "epoch": 1.4716666666666667,
      "grad_norm": 0.05677858367562294,
      "learning_rate": 0.00010217270194986073,
      "loss": 0.317,
      "step": 883
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.06137717887759209,
      "learning_rate": 0.00010206128133704735,
      "loss": 0.2886,
      "step": 884
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.08303935080766678,
      "learning_rate": 0.00010194986072423397,
      "loss": 0.4525,
      "step": 885
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.07649409770965576,
      "learning_rate": 0.00010183844011142061,
      "loss": 0.313,
      "step": 886
    },
    {
      "epoch": 1.4783333333333333,
      "grad_norm": 0.05235839635133743,
      "learning_rate": 0.00010172701949860726,
      "loss": 0.3392,
      "step": 887
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.0328575000166893,
      "learning_rate": 0.00010161559888579388,
      "loss": 0.2203,
      "step": 888
    },
    {
      "epoch": 1.4816666666666667,
      "grad_norm": 0.0566297210752964,
      "learning_rate": 0.00010150417827298051,
      "loss": 0.3188,
      "step": 889
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.05074988678097725,
      "learning_rate": 0.00010139275766016714,
      "loss": 0.3088,
      "step": 890
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 0.05383668094873428,
      "learning_rate": 0.00010128133704735377,
      "loss": 0.367,
      "step": 891
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.04733072966337204,
      "learning_rate": 0.00010116991643454039,
      "loss": 0.3143,
      "step": 892
    },
    {
      "epoch": 1.4883333333333333,
      "grad_norm": 0.05412575975060463,
      "learning_rate": 0.00010105849582172701,
      "loss": 0.3076,
      "step": 893
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.06799665093421936,
      "learning_rate": 0.00010094707520891365,
      "loss": 0.2191,
      "step": 894
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.04516544193029404,
      "learning_rate": 0.0001008356545961003,
      "loss": 0.2922,
      "step": 895
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.04372014105319977,
      "learning_rate": 0.00010072423398328692,
      "loss": 0.2163,
      "step": 896
    },
    {
      "epoch": 1.495,
      "grad_norm": 0.05170045047998428,
      "learning_rate": 0.00010061281337047354,
      "loss": 0.3305,
      "step": 897
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.07278403639793396,
      "learning_rate": 0.00010050139275766017,
      "loss": 0.3428,
      "step": 898
    },
    {
      "epoch": 1.4983333333333333,
      "grad_norm": 0.06453842669725418,
      "learning_rate": 0.0001003899721448468,
      "loss": 0.3022,
      "step": 899
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.03192227706313133,
      "learning_rate": 0.00010027855153203343,
      "loss": 0.2279,
      "step": 900
    },
    {
      "epoch": 1.5,
      "eval_loss": 0.3231646716594696,
      "eval_runtime": 373.5018,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 900
    },
    {
      "epoch": 1.5016666666666667,
      "grad_norm": 0.08311466127634048,
      "learning_rate": 0.00010016713091922005,
      "loss": 0.3975,
      "step": 901
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.04909651726484299,
      "learning_rate": 0.00010005571030640669,
      "loss": 0.2727,
      "step": 902
    },
    {
      "epoch": 1.505,
      "grad_norm": 0.05146733671426773,
      "learning_rate": 9.994428969359332e-05,
      "loss": 0.3075,
      "step": 903
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.07472844421863556,
      "learning_rate": 9.983286908077995e-05,
      "loss": 0.4154,
      "step": 904
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.06042633578181267,
      "learning_rate": 9.972144846796658e-05,
      "loss": 0.2946,
      "step": 905
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.06991257518529892,
      "learning_rate": 9.96100278551532e-05,
      "loss": 0.3545,
      "step": 906
    },
    {
      "epoch": 1.5116666666666667,
      "grad_norm": 0.06294330954551697,
      "learning_rate": 9.949860724233984e-05,
      "loss": 0.3697,
      "step": 907
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.07625393569469452,
      "learning_rate": 9.938718662952646e-05,
      "loss": 0.4117,
      "step": 908
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.04371468350291252,
      "learning_rate": 9.92757660167131e-05,
      "loss": 0.2897,
      "step": 909
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.06273914128541946,
      "learning_rate": 9.916434540389972e-05,
      "loss": 0.3959,
      "step": 910
    },
    {
      "epoch": 1.5183333333333333,
      "grad_norm": 0.06369846314191818,
      "learning_rate": 9.905292479108636e-05,
      "loss": 0.2974,
      "step": 911
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.047940727323293686,
      "learning_rate": 9.894150417827298e-05,
      "loss": 0.3107,
      "step": 912
    },
    {
      "epoch": 1.5216666666666665,
      "grad_norm": 0.05870036780834198,
      "learning_rate": 9.883008356545962e-05,
      "loss": 0.3385,
      "step": 913
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.058333318680524826,
      "learning_rate": 9.871866295264624e-05,
      "loss": 0.3476,
      "step": 914
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.0731189027428627,
      "learning_rate": 9.860724233983287e-05,
      "loss": 0.3844,
      "step": 915
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.05347345024347305,
      "learning_rate": 9.84958217270195e-05,
      "loss": 0.2947,
      "step": 916
    },
    {
      "epoch": 1.5283333333333333,
      "grad_norm": 0.07489851862192154,
      "learning_rate": 9.838440111420614e-05,
      "loss": 0.3756,
      "step": 917
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.05343102291226387,
      "learning_rate": 9.827298050139276e-05,
      "loss": 0.3221,
      "step": 918
    },
    {
      "epoch": 1.5316666666666667,
      "grad_norm": 0.06133120507001877,
      "learning_rate": 9.816155988857938e-05,
      "loss": 0.3272,
      "step": 919
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.06399228423833847,
      "learning_rate": 9.805013927576602e-05,
      "loss": 0.4005,
      "step": 920
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 0.08236099779605865,
      "learning_rate": 9.793871866295266e-05,
      "loss": 0.4026,
      "step": 921
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.062330517917871475,
      "learning_rate": 9.782729805013928e-05,
      "loss": 0.3307,
      "step": 922
    },
    {
      "epoch": 1.5383333333333333,
      "grad_norm": 0.04514957219362259,
      "learning_rate": 9.77158774373259e-05,
      "loss": 0.3035,
      "step": 923
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.07358008623123169,
      "learning_rate": 9.760445682451254e-05,
      "loss": 0.3607,
      "step": 924
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.056394483894109726,
      "learning_rate": 9.749303621169918e-05,
      "loss": 0.3375,
      "step": 925
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 0.050410106778144836,
      "learning_rate": 9.73816155988858e-05,
      "loss": 0.3621,
      "step": 926
    },
    {
      "epoch": 1.545,
      "grad_norm": 0.05911073088645935,
      "learning_rate": 9.727019498607242e-05,
      "loss": 0.2999,
      "step": 927
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.09308454394340515,
      "learning_rate": 9.715877437325906e-05,
      "loss": 0.3752,
      "step": 928
    },
    {
      "epoch": 1.5483333333333333,
      "grad_norm": 0.05108194798231125,
      "learning_rate": 9.70473537604457e-05,
      "loss": 0.3351,
      "step": 929
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.038478974252939224,
      "learning_rate": 9.693593314763232e-05,
      "loss": 0.2626,
      "step": 930
    },
    {
      "epoch": 1.5516666666666667,
      "grad_norm": 0.05561017990112305,
      "learning_rate": 9.682451253481894e-05,
      "loss": 0.3155,
      "step": 931
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.058376245200634,
      "learning_rate": 9.671309192200558e-05,
      "loss": 0.3226,
      "step": 932
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 0.054469939321279526,
      "learning_rate": 9.660167130919221e-05,
      "loss": 0.2955,
      "step": 933
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.048533570021390915,
      "learning_rate": 9.649025069637884e-05,
      "loss": 0.3053,
      "step": 934
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.046764180064201355,
      "learning_rate": 9.637883008356546e-05,
      "loss": 0.3363,
      "step": 935
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05811616778373718,
      "learning_rate": 9.62674094707521e-05,
      "loss": 0.3371,
      "step": 936
    },
    {
      "epoch": 1.5616666666666665,
      "grad_norm": 0.05568284913897514,
      "learning_rate": 9.615598885793873e-05,
      "loss": 0.3529,
      "step": 937
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.058249302208423615,
      "learning_rate": 9.604456824512536e-05,
      "loss": 0.3825,
      "step": 938
    },
    {
      "epoch": 1.565,
      "grad_norm": 0.045777395367622375,
      "learning_rate": 9.593314763231198e-05,
      "loss": 0.309,
      "step": 939
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.053043436259031296,
      "learning_rate": 9.58217270194986e-05,
      "loss": 0.3832,
      "step": 940
    },
    {
      "epoch": 1.5683333333333334,
      "grad_norm": 0.08452142030000687,
      "learning_rate": 9.571030640668525e-05,
      "loss": 0.3818,
      "step": 941
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.06124549359083176,
      "learning_rate": 9.559888579387187e-05,
      "loss": 0.2691,
      "step": 942
    },
    {
      "epoch": 1.5716666666666668,
      "grad_norm": 0.05381093546748161,
      "learning_rate": 9.54874651810585e-05,
      "loss": 0.3318,
      "step": 943
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.04413091391324997,
      "learning_rate": 9.537604456824512e-05,
      "loss": 0.2974,
      "step": 944
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.07002463191747665,
      "learning_rate": 9.526462395543177e-05,
      "loss": 0.3627,
      "step": 945
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.057526927441358566,
      "learning_rate": 9.51532033426184e-05,
      "loss": 0.2648,
      "step": 946
    },
    {
      "epoch": 1.5783333333333334,
      "grad_norm": 0.03448295593261719,
      "learning_rate": 9.504178272980502e-05,
      "loss": 0.2861,
      "step": 947
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.04271212965250015,
      "learning_rate": 9.493036211699164e-05,
      "loss": 0.2355,
      "step": 948
    },
    {
      "epoch": 1.5816666666666666,
      "grad_norm": 0.048834070563316345,
      "learning_rate": 9.481894150417828e-05,
      "loss": 0.3116,
      "step": 949
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.032471898943185806,
      "learning_rate": 9.470752089136491e-05,
      "loss": 0.2525,
      "step": 950
    },
    {
      "epoch": 1.585,
      "grad_norm": 0.04324181377887726,
      "learning_rate": 9.459610027855154e-05,
      "loss": 0.3101,
      "step": 951
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.04475460946559906,
      "learning_rate": 9.448467966573816e-05,
      "loss": 0.3222,
      "step": 952
    },
    {
      "epoch": 1.5883333333333334,
      "grad_norm": 0.04748489707708359,
      "learning_rate": 9.43732590529248e-05,
      "loss": 0.3361,
      "step": 953
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.04902688413858414,
      "learning_rate": 9.426183844011143e-05,
      "loss": 0.3356,
      "step": 954
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.046897415071725845,
      "learning_rate": 9.415041782729805e-05,
      "loss": 0.3243,
      "step": 955
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.04333881661295891,
      "learning_rate": 9.403899721448468e-05,
      "loss": 0.2644,
      "step": 956
    },
    {
      "epoch": 1.595,
      "grad_norm": 0.04235534369945526,
      "learning_rate": 9.392757660167131e-05,
      "loss": 0.3333,
      "step": 957
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.05769545957446098,
      "learning_rate": 9.381615598885795e-05,
      "loss": 0.3247,
      "step": 958
    },
    {
      "epoch": 1.5983333333333334,
      "grad_norm": 0.046658098697662354,
      "learning_rate": 9.370473537604457e-05,
      "loss": 0.3516,
      "step": 959
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.06959307938814163,
      "learning_rate": 9.35933147632312e-05,
      "loss": 0.3301,
      "step": 960
    },
    {
      "epoch": 1.6016666666666666,
      "grad_norm": 0.07982617616653442,
      "learning_rate": 9.348189415041783e-05,
      "loss": 0.3099,
      "step": 961
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.053461212664842606,
      "learning_rate": 9.337047353760447e-05,
      "loss": 0.3332,
      "step": 962
    },
    {
      "epoch": 1.605,
      "grad_norm": 0.051453981548547745,
      "learning_rate": 9.325905292479109e-05,
      "loss": 0.3461,
      "step": 963
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.041737012565135956,
      "learning_rate": 9.314763231197771e-05,
      "loss": 0.2854,
      "step": 964
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.05738230049610138,
      "learning_rate": 9.303621169916435e-05,
      "loss": 0.3037,
      "step": 965
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.0427318811416626,
      "learning_rate": 9.292479108635099e-05,
      "loss": 0.3207,
      "step": 966
    },
    {
      "epoch": 1.6116666666666668,
      "grad_norm": 0.06356322020292282,
      "learning_rate": 9.281337047353761e-05,
      "loss": 0.3438,
      "step": 967
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.08278052508831024,
      "learning_rate": 9.270194986072423e-05,
      "loss": 0.3949,
      "step": 968
    },
    {
      "epoch": 1.615,
      "grad_norm": 0.04347321018576622,
      "learning_rate": 9.259052924791087e-05,
      "loss": 0.2636,
      "step": 969
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.050444383174180984,
      "learning_rate": 9.24791086350975e-05,
      "loss": 0.2684,
      "step": 970
    },
    {
      "epoch": 1.6183333333333332,
      "grad_norm": 0.048784345388412476,
      "learning_rate": 9.236768802228413e-05,
      "loss": 0.3078,
      "step": 971
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.07021452486515045,
      "learning_rate": 9.225626740947075e-05,
      "loss": 0.3033,
      "step": 972
    },
    {
      "epoch": 1.6216666666666666,
      "grad_norm": 0.04334842041134834,
      "learning_rate": 9.214484679665739e-05,
      "loss": 0.2498,
      "step": 973
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.05947504937648773,
      "learning_rate": 9.203342618384401e-05,
      "loss": 0.3875,
      "step": 974
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.05991059169173241,
      "learning_rate": 9.192200557103065e-05,
      "loss": 0.3127,
      "step": 975
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.040278319269418716,
      "learning_rate": 9.181058495821727e-05,
      "loss": 0.258,
      "step": 976
    },
    {
      "epoch": 1.6283333333333334,
      "grad_norm": 0.0503925159573555,
      "learning_rate": 9.169916434540391e-05,
      "loss": 0.2774,
      "step": 977
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.048469800502061844,
      "learning_rate": 9.158774373259053e-05,
      "loss": 0.334,
      "step": 978
    },
    {
      "epoch": 1.6316666666666668,
      "grad_norm": 0.03953427076339722,
      "learning_rate": 9.147632311977717e-05,
      "loss": 0.2564,
      "step": 979
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.0364089198410511,
      "learning_rate": 9.136490250696379e-05,
      "loss": 0.2619,
      "step": 980
    },
    {
      "epoch": 1.635,
      "grad_norm": 0.04198732599616051,
      "learning_rate": 9.125348189415043e-05,
      "loss": 0.3091,
      "step": 981
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.05511907860636711,
      "learning_rate": 9.114206128133705e-05,
      "loss": 0.3458,
      "step": 982
    },
    {
      "epoch": 1.6383333333333332,
      "grad_norm": 0.05258199945092201,
      "learning_rate": 9.103064066852369e-05,
      "loss": 0.3078,
      "step": 983
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.04512585327029228,
      "learning_rate": 9.091922005571031e-05,
      "loss": 0.3174,
      "step": 984
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.06326772272586823,
      "learning_rate": 9.080779944289693e-05,
      "loss": 0.3473,
      "step": 985
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.0529300831258297,
      "learning_rate": 9.069637883008357e-05,
      "loss": 0.3495,
      "step": 986
    },
    {
      "epoch": 1.645,
      "grad_norm": 0.04727138206362724,
      "learning_rate": 9.05849582172702e-05,
      "loss": 0.3098,
      "step": 987
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.07397496700286865,
      "learning_rate": 9.047353760445683e-05,
      "loss": 0.3076,
      "step": 988
    },
    {
      "epoch": 1.6483333333333334,
      "grad_norm": 0.04864749312400818,
      "learning_rate": 9.036211699164345e-05,
      "loss": 0.2394,
      "step": 989
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.044533081352710724,
      "learning_rate": 9.025069637883009e-05,
      "loss": 0.2835,
      "step": 990
    },
    {
      "epoch": 1.6516666666666666,
      "grad_norm": 0.054180171340703964,
      "learning_rate": 9.013927576601672e-05,
      "loss": 0.3572,
      "step": 991
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.03964740037918091,
      "learning_rate": 9.002785515320335e-05,
      "loss": 0.3126,
      "step": 992
    },
    {
      "epoch": 1.655,
      "grad_norm": 0.05723697692155838,
      "learning_rate": 8.991643454038997e-05,
      "loss": 0.338,
      "step": 993
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.060739897191524506,
      "learning_rate": 8.98050139275766e-05,
      "loss": 0.3024,
      "step": 994
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.04364289715886116,
      "learning_rate": 8.969359331476324e-05,
      "loss": 0.2868,
      "step": 995
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.03829342871904373,
      "learning_rate": 8.958217270194987e-05,
      "loss": 0.2455,
      "step": 996
    },
    {
      "epoch": 1.6616666666666666,
      "grad_norm": 0.07998751103878021,
      "learning_rate": 8.947075208913649e-05,
      "loss": 0.3685,
      "step": 997
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.03779842332005501,
      "learning_rate": 8.935933147632312e-05,
      "loss": 0.2718,
      "step": 998
    },
    {
      "epoch": 1.665,
      "grad_norm": 0.058501727879047394,
      "learning_rate": 8.924791086350975e-05,
      "loss": 0.316,
      "step": 999
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04681263864040375,
      "learning_rate": 8.913649025069638e-05,
      "loss": 0.2763,
      "step": 1000
    },
    {
      "epoch": 1.6666666666666665,
      "eval_loss": 0.3220631778240204,
      "eval_runtime": 373.5122,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1000
    },
    {
      "epoch": 1.6683333333333334,
      "grad_norm": 0.07085569947957993,
      "learning_rate": 8.902506963788301e-05,
      "loss": 0.3577,
      "step": 1001
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.06478603929281235,
      "learning_rate": 8.891364902506964e-05,
      "loss": 0.3906,
      "step": 1002
    },
    {
      "epoch": 1.6716666666666666,
      "grad_norm": 0.06864550709724426,
      "learning_rate": 8.880222841225627e-05,
      "loss": 0.3705,
      "step": 1003
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.04719690605998039,
      "learning_rate": 8.86908077994429e-05,
      "loss": 0.344,
      "step": 1004
    },
    {
      "epoch": 1.675,
      "grad_norm": 0.09088531136512756,
      "learning_rate": 8.857938718662953e-05,
      "loss": 0.3721,
      "step": 1005
    },
    {
      "epoch": 1.6766666666666667,
      "grad_norm": 0.09742141515016556,
      "learning_rate": 8.846796657381616e-05,
      "loss": 0.3712,
      "step": 1006
    },
    {
      "epoch": 1.6783333333333332,
      "grad_norm": 0.03908870741724968,
      "learning_rate": 8.835654596100279e-05,
      "loss": 0.2797,
      "step": 1007
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.03998957574367523,
      "learning_rate": 8.824512534818942e-05,
      "loss": 0.2921,
      "step": 1008
    },
    {
      "epoch": 1.6816666666666666,
      "grad_norm": 0.0421077162027359,
      "learning_rate": 8.813370473537604e-05,
      "loss": 0.2713,
      "step": 1009
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.05514073371887207,
      "learning_rate": 8.802228412256268e-05,
      "loss": 0.3459,
      "step": 1010
    },
    {
      "epoch": 1.685,
      "grad_norm": 0.048428285866975784,
      "learning_rate": 8.79108635097493e-05,
      "loss": 0.3187,
      "step": 1011
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.04380195960402489,
      "learning_rate": 8.779944289693594e-05,
      "loss": 0.285,
      "step": 1012
    },
    {
      "epoch": 1.6883333333333335,
      "grad_norm": 0.053142763674259186,
      "learning_rate": 8.768802228412256e-05,
      "loss": 0.3635,
      "step": 1013
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.05478207394480705,
      "learning_rate": 8.75766016713092e-05,
      "loss": 0.3758,
      "step": 1014
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.05076910927891731,
      "learning_rate": 8.746518105849582e-05,
      "loss": 0.3209,
      "step": 1015
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.030658623203635216,
      "learning_rate": 8.735376044568246e-05,
      "loss": 0.2124,
      "step": 1016
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 0.057109639048576355,
      "learning_rate": 8.724233983286908e-05,
      "loss": 0.2959,
      "step": 1017
    },
    {
      "epoch": 1.6966666666666668,
      "grad_norm": 0.04906706511974335,
      "learning_rate": 8.713091922005572e-05,
      "loss": 0.2626,
      "step": 1018
    },
    {
      "epoch": 1.6983333333333333,
      "grad_norm": 0.050157688558101654,
      "learning_rate": 8.701949860724234e-05,
      "loss": 0.2418,
      "step": 1019
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.053976695984601974,
      "learning_rate": 8.690807799442896e-05,
      "loss": 0.3202,
      "step": 1020
    },
    {
      "epoch": 1.7016666666666667,
      "grad_norm": 0.04405670985579491,
      "learning_rate": 8.67966573816156e-05,
      "loss": 0.2774,
      "step": 1021
    },
    {
      "epoch": 1.7033333333333334,
      "grad_norm": 0.066031314432621,
      "learning_rate": 8.668523676880224e-05,
      "loss": 0.3778,
      "step": 1022
    },
    {
      "epoch": 1.705,
      "grad_norm": 0.045106563717126846,
      "learning_rate": 8.657381615598886e-05,
      "loss": 0.2864,
      "step": 1023
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.05261269584298134,
      "learning_rate": 8.646239554317548e-05,
      "loss": 0.3422,
      "step": 1024
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.061535440385341644,
      "learning_rate": 8.635097493036212e-05,
      "loss": 0.354,
      "step": 1025
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.040535662323236465,
      "learning_rate": 8.623955431754876e-05,
      "loss": 0.307,
      "step": 1026
    },
    {
      "epoch": 1.7116666666666667,
      "grad_norm": 0.04058687388896942,
      "learning_rate": 8.612813370473538e-05,
      "loss": 0.2772,
      "step": 1027
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.05465413257479668,
      "learning_rate": 8.6016713091922e-05,
      "loss": 0.3865,
      "step": 1028
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 0.049970902502536774,
      "learning_rate": 8.590529247910864e-05,
      "loss": 0.2905,
      "step": 1029
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.05033727362751961,
      "learning_rate": 8.579387186629528e-05,
      "loss": 0.2018,
      "step": 1030
    },
    {
      "epoch": 1.7183333333333333,
      "grad_norm": 0.04867710545659065,
      "learning_rate": 8.56824512534819e-05,
      "loss": 0.2858,
      "step": 1031
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.0668291226029396,
      "learning_rate": 8.557103064066852e-05,
      "loss": 0.364,
      "step": 1032
    },
    {
      "epoch": 1.7216666666666667,
      "grad_norm": 0.05906341224908829,
      "learning_rate": 8.545961002785516e-05,
      "loss": 0.3295,
      "step": 1033
    },
    {
      "epoch": 1.7233333333333334,
      "grad_norm": 0.07651325315237045,
      "learning_rate": 8.53481894150418e-05,
      "loss": 0.3469,
      "step": 1034
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.04857095703482628,
      "learning_rate": 8.523676880222842e-05,
      "loss": 0.3142,
      "step": 1035
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.059669576585292816,
      "learning_rate": 8.512534818941504e-05,
      "loss": 0.3406,
      "step": 1036
    },
    {
      "epoch": 1.7283333333333335,
      "grad_norm": 0.057863347232341766,
      "learning_rate": 8.501392757660168e-05,
      "loss": 0.3478,
      "step": 1037
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.051596641540527344,
      "learning_rate": 8.490250696378831e-05,
      "loss": 0.2995,
      "step": 1038
    },
    {
      "epoch": 1.7316666666666667,
      "grad_norm": 0.05238925293087959,
      "learning_rate": 8.479108635097494e-05,
      "loss": 0.3014,
      "step": 1039
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.08119304478168488,
      "learning_rate": 8.467966573816156e-05,
      "loss": 0.3926,
      "step": 1040
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 0.055002421140670776,
      "learning_rate": 8.45682451253482e-05,
      "loss": 0.3548,
      "step": 1041
    },
    {
      "epoch": 1.7366666666666668,
      "grad_norm": 0.05420352891087532,
      "learning_rate": 8.445682451253483e-05,
      "loss": 0.3447,
      "step": 1042
    },
    {
      "epoch": 1.7383333333333333,
      "grad_norm": 0.0491153821349144,
      "learning_rate": 8.434540389972145e-05,
      "loss": 0.2802,
      "step": 1043
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.04026114195585251,
      "learning_rate": 8.423398328690808e-05,
      "loss": 0.2674,
      "step": 1044
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 0.049814388155937195,
      "learning_rate": 8.41225626740947e-05,
      "loss": 0.2916,
      "step": 1045
    },
    {
      "epoch": 1.7433333333333332,
      "grad_norm": 0.04157713055610657,
      "learning_rate": 8.401114206128135e-05,
      "loss": 0.3108,
      "step": 1046
    },
    {
      "epoch": 1.745,
      "grad_norm": 0.059172868728637695,
      "learning_rate": 8.389972144846797e-05,
      "loss": 0.3323,
      "step": 1047
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.07637706398963928,
      "learning_rate": 8.37883008356546e-05,
      "loss": 0.3773,
      "step": 1048
    },
    {
      "epoch": 1.7483333333333333,
      "grad_norm": 0.05768604204058647,
      "learning_rate": 8.367688022284122e-05,
      "loss": 0.3135,
      "step": 1049
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.037449564784765244,
      "learning_rate": 8.356545961002787e-05,
      "loss": 0.282,
      "step": 1050
    },
    {
      "epoch": 1.7516666666666667,
      "grad_norm": 0.06918846070766449,
      "learning_rate": 8.345403899721449e-05,
      "loss": 0.3042,
      "step": 1051
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.037327397614717484,
      "learning_rate": 8.334261838440112e-05,
      "loss": 0.2871,
      "step": 1052
    },
    {
      "epoch": 1.755,
      "grad_norm": 0.038553304970264435,
      "learning_rate": 8.323119777158774e-05,
      "loss": 0.2675,
      "step": 1053
    },
    {
      "epoch": 1.7566666666666668,
      "grad_norm": 0.08371641486883163,
      "learning_rate": 8.311977715877437e-05,
      "loss": 0.3224,
      "step": 1054
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.04039563238620758,
      "learning_rate": 8.300835654596101e-05,
      "loss": 0.2666,
      "step": 1055
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.051593661308288574,
      "learning_rate": 8.289693593314763e-05,
      "loss": 0.3018,
      "step": 1056
    },
    {
      "epoch": 1.7616666666666667,
      "grad_norm": 0.05394883081316948,
      "learning_rate": 8.278551532033426e-05,
      "loss": 0.2941,
      "step": 1057
    },
    {
      "epoch": 1.7633333333333332,
      "grad_norm": 0.052733805030584335,
      "learning_rate": 8.26740947075209e-05,
      "loss": 0.2792,
      "step": 1058
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 0.05602341890335083,
      "learning_rate": 8.256267409470753e-05,
      "loss": 0.3563,
      "step": 1059
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.04654848948121071,
      "learning_rate": 8.245125348189415e-05,
      "loss": 0.3973,
      "step": 1060
    },
    {
      "epoch": 1.7683333333333333,
      "grad_norm": 0.05538569763302803,
      "learning_rate": 8.233983286908078e-05,
      "loss": 0.3248,
      "step": 1061
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.038909751921892166,
      "learning_rate": 8.222841225626741e-05,
      "loss": 0.3168,
      "step": 1062
    },
    {
      "epoch": 1.7716666666666665,
      "grad_norm": 0.04581937938928604,
      "learning_rate": 8.211699164345405e-05,
      "loss": 0.3172,
      "step": 1063
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.039568401873111725,
      "learning_rate": 8.200557103064067e-05,
      "loss": 0.267,
      "step": 1064
    },
    {
      "epoch": 1.775,
      "grad_norm": 0.08366461843252182,
      "learning_rate": 8.18941504178273e-05,
      "loss": 0.3572,
      "step": 1065
    },
    {
      "epoch": 1.7766666666666666,
      "grad_norm": 0.040958721190690994,
      "learning_rate": 8.178272980501393e-05,
      "loss": 0.2671,
      "step": 1066
    },
    {
      "epoch": 1.7783333333333333,
      "grad_norm": 0.053148556500673294,
      "learning_rate": 8.167130919220057e-05,
      "loss": 0.3504,
      "step": 1067
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.053653452545404434,
      "learning_rate": 8.155988857938719e-05,
      "loss": 0.3303,
      "step": 1068
    },
    {
      "epoch": 1.7816666666666667,
      "grad_norm": 0.039743080735206604,
      "learning_rate": 8.144846796657381e-05,
      "loss": 0.2684,
      "step": 1069
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.03639829903841019,
      "learning_rate": 8.133704735376045e-05,
      "loss": 0.2615,
      "step": 1070
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 0.037467848509550095,
      "learning_rate": 8.122562674094709e-05,
      "loss": 0.2931,
      "step": 1071
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.05565938726067543,
      "learning_rate": 8.111420612813371e-05,
      "loss": 0.3577,
      "step": 1072
    },
    {
      "epoch": 1.7883333333333333,
      "grad_norm": 0.06557439267635345,
      "learning_rate": 8.100278551532033e-05,
      "loss": 0.3829,
      "step": 1073
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.07056461274623871,
      "learning_rate": 8.089136490250697e-05,
      "loss": 0.4024,
      "step": 1074
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.05761047825217247,
      "learning_rate": 8.07799442896936e-05,
      "loss": 0.29,
      "step": 1075
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.06583710759878159,
      "learning_rate": 8.066852367688023e-05,
      "loss": 0.3894,
      "step": 1076
    },
    {
      "epoch": 1.795,
      "grad_norm": 0.037544865161180496,
      "learning_rate": 8.055710306406685e-05,
      "loss": 0.2441,
      "step": 1077
    },
    {
      "epoch": 1.7966666666666666,
      "grad_norm": 0.04733952507376671,
      "learning_rate": 8.044568245125349e-05,
      "loss": 0.3119,
      "step": 1078
    },
    {
      "epoch": 1.7983333333333333,
      "grad_norm": 0.06008659675717354,
      "learning_rate": 8.033426183844011e-05,
      "loss": 0.326,
      "step": 1079
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.060504764318466187,
      "learning_rate": 8.022284122562675e-05,
      "loss": 0.3092,
      "step": 1080
    },
    {
      "epoch": 1.8016666666666667,
      "grad_norm": 0.04397258535027504,
      "learning_rate": 8.011142061281337e-05,
      "loss": 0.308,
      "step": 1081
    },
    {
      "epoch": 1.8033333333333332,
      "grad_norm": 0.039256297051906586,
      "learning_rate": 8e-05,
      "loss": 0.2455,
      "step": 1082
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 0.055752988904714584,
      "learning_rate": 7.988857938718663e-05,
      "loss": 0.3725,
      "step": 1083
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.047002725303173065,
      "learning_rate": 7.977715877437327e-05,
      "loss": 0.3347,
      "step": 1084
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 0.049420058727264404,
      "learning_rate": 7.966573816155989e-05,
      "loss": 0.3562,
      "step": 1085
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.07690422981977463,
      "learning_rate": 7.955431754874653e-05,
      "loss": 0.3612,
      "step": 1086
    },
    {
      "epoch": 1.8116666666666665,
      "grad_norm": 0.05377360060811043,
      "learning_rate": 7.944289693593315e-05,
      "loss": 0.3271,
      "step": 1087
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.04284443333745003,
      "learning_rate": 7.933147632311978e-05,
      "loss": 0.2685,
      "step": 1088
    },
    {
      "epoch": 1.815,
      "grad_norm": 0.051193393766880035,
      "learning_rate": 7.922005571030641e-05,
      "loss": 0.3022,
      "step": 1089
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.044064056128263474,
      "learning_rate": 7.910863509749304e-05,
      "loss": 0.3266,
      "step": 1090
    },
    {
      "epoch": 1.8183333333333334,
      "grad_norm": 0.04836930334568024,
      "learning_rate": 7.899721448467967e-05,
      "loss": 0.3104,
      "step": 1091
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.06750614941120148,
      "learning_rate": 7.88857938718663e-05,
      "loss": 0.3408,
      "step": 1092
    },
    {
      "epoch": 1.8216666666666668,
      "grad_norm": 0.039168402552604675,
      "learning_rate": 7.877437325905293e-05,
      "loss": 0.2928,
      "step": 1093
    },
    {
      "epoch": 1.8233333333333333,
      "grad_norm": 0.048351678997278214,
      "learning_rate": 7.866295264623956e-05,
      "loss": 0.3283,
      "step": 1094
    },
    {
      "epoch": 1.825,
      "grad_norm": 0.049547988921403885,
      "learning_rate": 7.855153203342619e-05,
      "loss": 0.2257,
      "step": 1095
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.04752562567591667,
      "learning_rate": 7.844011142061282e-05,
      "loss": 0.2747,
      "step": 1096
    },
    {
      "epoch": 1.8283333333333334,
      "grad_norm": 0.055682335048913956,
      "learning_rate": 7.832869080779945e-05,
      "loss": 0.3483,
      "step": 1097
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.044909216463565826,
      "learning_rate": 7.821727019498608e-05,
      "loss": 0.2348,
      "step": 1098
    },
    {
      "epoch": 1.8316666666666666,
      "grad_norm": 0.03840798884630203,
      "learning_rate": 7.81058495821727e-05,
      "loss": 0.2951,
      "step": 1099
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.0570337250828743,
      "learning_rate": 7.799442896935933e-05,
      "loss": 0.3471,
      "step": 1100
    },
    {
      "epoch": 1.8333333333333335,
      "eval_loss": 0.32139015197753906,
      "eval_runtime": 373.6022,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1100
    },
    {
      "epoch": 1.835,
      "grad_norm": 0.039956070482730865,
      "learning_rate": 7.788300835654596e-05,
      "loss": 0.2687,
      "step": 1101
    },
    {
      "epoch": 1.8366666666666667,
      "grad_norm": 0.05461916700005531,
      "learning_rate": 7.77715877437326e-05,
      "loss": 0.3152,
      "step": 1102
    },
    {
      "epoch": 1.8383333333333334,
      "grad_norm": 0.07117316871881485,
      "learning_rate": 7.766016713091922e-05,
      "loss": 0.3278,
      "step": 1103
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.06882954388856888,
      "learning_rate": 7.754874651810585e-05,
      "loss": 0.4192,
      "step": 1104
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 0.056025370955467224,
      "learning_rate": 7.743732590529248e-05,
      "loss": 0.3062,
      "step": 1105
    },
    {
      "epoch": 1.8433333333333333,
      "grad_norm": 0.05003223568201065,
      "learning_rate": 7.732590529247912e-05,
      "loss": 0.3024,
      "step": 1106
    },
    {
      "epoch": 1.845,
      "grad_norm": 0.05989714339375496,
      "learning_rate": 7.721448467966574e-05,
      "loss": 0.2549,
      "step": 1107
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.038612060248851776,
      "learning_rate": 7.710306406685237e-05,
      "loss": 0.2336,
      "step": 1108
    },
    {
      "epoch": 1.8483333333333334,
      "grad_norm": 0.06590210646390915,
      "learning_rate": 7.6991643454039e-05,
      "loss": 0.4017,
      "step": 1109
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.041503772139549255,
      "learning_rate": 7.688022284122564e-05,
      "loss": 0.2756,
      "step": 1110
    },
    {
      "epoch": 1.8516666666666666,
      "grad_norm": 0.06363305449485779,
      "learning_rate": 7.676880222841226e-05,
      "loss": 0.3553,
      "step": 1111
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.0680147111415863,
      "learning_rate": 7.665738161559888e-05,
      "loss": 0.328,
      "step": 1112
    },
    {
      "epoch": 1.855,
      "grad_norm": 0.0441366471350193,
      "learning_rate": 7.654596100278552e-05,
      "loss": 0.2788,
      "step": 1113
    },
    {
      "epoch": 1.8566666666666667,
      "grad_norm": 0.053791362792253494,
      "learning_rate": 7.643454038997216e-05,
      "loss": 0.3717,
      "step": 1114
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.039630841463804245,
      "learning_rate": 7.632311977715878e-05,
      "loss": 0.2374,
      "step": 1115
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.0536554753780365,
      "learning_rate": 7.62116991643454e-05,
      "loss": 0.3022,
      "step": 1116
    },
    {
      "epoch": 1.8616666666666668,
      "grad_norm": 0.04466424509882927,
      "learning_rate": 7.610027855153204e-05,
      "loss": 0.2894,
      "step": 1117
    },
    {
      "epoch": 1.8633333333333333,
      "grad_norm": 0.04519462585449219,
      "learning_rate": 7.598885793871868e-05,
      "loss": 0.2634,
      "step": 1118
    },
    {
      "epoch": 1.865,
      "grad_norm": 0.043722864240407944,
      "learning_rate": 7.58774373259053e-05,
      "loss": 0.2841,
      "step": 1119
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.03297045826911926,
      "learning_rate": 7.576601671309192e-05,
      "loss": 0.2088,
      "step": 1120
    },
    {
      "epoch": 1.8683333333333332,
      "grad_norm": 0.0791371688246727,
      "learning_rate": 7.565459610027856e-05,
      "loss": 0.3584,
      "step": 1121
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.05867009237408638,
      "learning_rate": 7.55431754874652e-05,
      "loss": 0.3474,
      "step": 1122
    },
    {
      "epoch": 1.8716666666666666,
      "grad_norm": 0.05260579288005829,
      "learning_rate": 7.543175487465182e-05,
      "loss": 0.3215,
      "step": 1123
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.038134459406137466,
      "learning_rate": 7.532033426183844e-05,
      "loss": 0.2712,
      "step": 1124
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.041534245014190674,
      "learning_rate": 7.520891364902506e-05,
      "loss": 0.2806,
      "step": 1125
    },
    {
      "epoch": 1.8766666666666667,
      "grad_norm": 0.04753369837999344,
      "learning_rate": 7.509749303621171e-05,
      "loss": 0.3032,
      "step": 1126
    },
    {
      "epoch": 1.8783333333333334,
      "grad_norm": 0.05900038406252861,
      "learning_rate": 7.498607242339834e-05,
      "loss": 0.3203,
      "step": 1127
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.04374930262565613,
      "learning_rate": 7.487465181058496e-05,
      "loss": 0.3044,
      "step": 1128
    },
    {
      "epoch": 1.8816666666666668,
      "grad_norm": 0.0397757813334465,
      "learning_rate": 7.476323119777158e-05,
      "loss": 0.2737,
      "step": 1129
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.049337953329086304,
      "learning_rate": 7.465181058495823e-05,
      "loss": 0.3064,
      "step": 1130
    },
    {
      "epoch": 1.885,
      "grad_norm": 0.04374752938747406,
      "learning_rate": 7.454038997214486e-05,
      "loss": 0.2839,
      "step": 1131
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.030827084556221962,
      "learning_rate": 7.442896935933148e-05,
      "loss": 0.25,
      "step": 1132
    },
    {
      "epoch": 1.8883333333333332,
      "grad_norm": 0.04772200807929039,
      "learning_rate": 7.43175487465181e-05,
      "loss": 0.308,
      "step": 1133
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.05332803726196289,
      "learning_rate": 7.420612813370474e-05,
      "loss": 0.3311,
      "step": 1134
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.07982104271650314,
      "learning_rate": 7.409470752089137e-05,
      "loss": 0.4451,
      "step": 1135
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.05805666744709015,
      "learning_rate": 7.3983286908078e-05,
      "loss": 0.3691,
      "step": 1136
    },
    {
      "epoch": 1.895,
      "grad_norm": 0.055468134582042694,
      "learning_rate": 7.387186629526462e-05,
      "loss": 0.34,
      "step": 1137
    },
    {
      "epoch": 1.8966666666666665,
      "grad_norm": 0.05011016130447388,
      "learning_rate": 7.376044568245126e-05,
      "loss": 0.3113,
      "step": 1138
    },
    {
      "epoch": 1.8983333333333334,
      "grad_norm": 0.043649762868881226,
      "learning_rate": 7.364902506963789e-05,
      "loss": 0.2623,
      "step": 1139
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.03532073274254799,
      "learning_rate": 7.353760445682452e-05,
      "loss": 0.2529,
      "step": 1140
    },
    {
      "epoch": 1.9016666666666666,
      "grad_norm": 0.04797568917274475,
      "learning_rate": 7.342618384401114e-05,
      "loss": 0.3109,
      "step": 1141
    },
    {
      "epoch": 1.9033333333333333,
      "grad_norm": 0.0748400092124939,
      "learning_rate": 7.331476323119778e-05,
      "loss": 0.2856,
      "step": 1142
    },
    {
      "epoch": 1.905,
      "grad_norm": 0.055701497942209244,
      "learning_rate": 7.320334261838441e-05,
      "loss": 0.2873,
      "step": 1143
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.05331743136048317,
      "learning_rate": 7.309192200557103e-05,
      "loss": 0.2613,
      "step": 1144
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 0.040177635848522186,
      "learning_rate": 7.298050139275766e-05,
      "loss": 0.2972,
      "step": 1145
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.03856652230024338,
      "learning_rate": 7.28690807799443e-05,
      "loss": 0.2944,
      "step": 1146
    },
    {
      "epoch": 1.9116666666666666,
      "grad_norm": 0.06419628858566284,
      "learning_rate": 7.275766016713093e-05,
      "loss": 0.3729,
      "step": 1147
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.058562688529491425,
      "learning_rate": 7.264623955431755e-05,
      "loss": 0.3752,
      "step": 1148
    },
    {
      "epoch": 1.915,
      "grad_norm": 0.06186780333518982,
      "learning_rate": 7.253481894150418e-05,
      "loss": 0.3664,
      "step": 1149
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.03859579563140869,
      "learning_rate": 7.24233983286908e-05,
      "loss": 0.2615,
      "step": 1150
    },
    {
      "epoch": 1.9183333333333334,
      "grad_norm": 0.0342753641307354,
      "learning_rate": 7.231197771587745e-05,
      "loss": 0.2752,
      "step": 1151
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.0599929615855217,
      "learning_rate": 7.220055710306407e-05,
      "loss": 0.3596,
      "step": 1152
    },
    {
      "epoch": 1.9216666666666666,
      "grad_norm": 0.05611804500222206,
      "learning_rate": 7.20891364902507e-05,
      "loss": 0.2843,
      "step": 1153
    },
    {
      "epoch": 1.9233333333333333,
      "grad_norm": 0.06090658903121948,
      "learning_rate": 7.197771587743732e-05,
      "loss": 0.2912,
      "step": 1154
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.04918806999921799,
      "learning_rate": 7.186629526462397e-05,
      "loss": 0.3737,
      "step": 1155
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.06336189061403275,
      "learning_rate": 7.175487465181059e-05,
      "loss": 0.348,
      "step": 1156
    },
    {
      "epoch": 1.9283333333333332,
      "grad_norm": 0.054586268961429596,
      "learning_rate": 7.164345403899721e-05,
      "loss": 0.3341,
      "step": 1157
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.0607214979827404,
      "learning_rate": 7.153203342618384e-05,
      "loss": 0.3244,
      "step": 1158
    },
    {
      "epoch": 1.9316666666666666,
      "grad_norm": 0.05204036086797714,
      "learning_rate": 7.142061281337047e-05,
      "loss": 0.3167,
      "step": 1159
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.06979598850011826,
      "learning_rate": 7.130919220055711e-05,
      "loss": 0.3857,
      "step": 1160
    },
    {
      "epoch": 1.935,
      "grad_norm": 0.04612233489751816,
      "learning_rate": 7.119777158774373e-05,
      "loss": 0.3342,
      "step": 1161
    },
    {
      "epoch": 1.9366666666666665,
      "grad_norm": 0.04412130266427994,
      "learning_rate": 7.108635097493036e-05,
      "loss": 0.2997,
      "step": 1162
    },
    {
      "epoch": 1.9383333333333335,
      "grad_norm": 0.046451177448034286,
      "learning_rate": 7.097493036211699e-05,
      "loss": 0.3081,
      "step": 1163
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.05968666449189186,
      "learning_rate": 7.086350974930363e-05,
      "loss": 0.3995,
      "step": 1164
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 0.051761694252491,
      "learning_rate": 7.075208913649025e-05,
      "loss": 0.3348,
      "step": 1165
    },
    {
      "epoch": 1.9433333333333334,
      "grad_norm": 0.05505632981657982,
      "learning_rate": 7.064066852367687e-05,
      "loss": 0.2955,
      "step": 1166
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 0.02490934729576111,
      "learning_rate": 7.052924791086351e-05,
      "loss": 0.1945,
      "step": 1167
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.053391456604003906,
      "learning_rate": 7.041782729805015e-05,
      "loss": 0.3253,
      "step": 1168
    },
    {
      "epoch": 1.9483333333333333,
      "grad_norm": 0.03841974213719368,
      "learning_rate": 7.030640668523677e-05,
      "loss": 0.265,
      "step": 1169
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.044301558285951614,
      "learning_rate": 7.01949860724234e-05,
      "loss": 0.2881,
      "step": 1170
    },
    {
      "epoch": 1.9516666666666667,
      "grad_norm": 0.07384896278381348,
      "learning_rate": 7.008356545961003e-05,
      "loss": 0.3335,
      "step": 1171
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.045259926468133926,
      "learning_rate": 6.997214484679667e-05,
      "loss": 0.3239,
      "step": 1172
    },
    {
      "epoch": 1.955,
      "grad_norm": 0.03684656694531441,
      "learning_rate": 6.986072423398329e-05,
      "loss": 0.2412,
      "step": 1173
    },
    {
      "epoch": 1.9566666666666666,
      "grad_norm": 0.038408178836107254,
      "learning_rate": 6.974930362116991e-05,
      "loss": 0.2791,
      "step": 1174
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.05049310624599457,
      "learning_rate": 6.963788300835655e-05,
      "loss": 0.3512,
      "step": 1175
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.03517109155654907,
      "learning_rate": 6.952646239554319e-05,
      "loss": 0.2493,
      "step": 1176
    },
    {
      "epoch": 1.9616666666666667,
      "grad_norm": 0.04136708378791809,
      "learning_rate": 6.941504178272981e-05,
      "loss": 0.2595,
      "step": 1177
    },
    {
      "epoch": 1.9633333333333334,
      "grad_norm": 0.07218920439481735,
      "learning_rate": 6.930362116991643e-05,
      "loss": 0.4046,
      "step": 1178
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 0.08492610603570938,
      "learning_rate": 6.919220055710307e-05,
      "loss": 0.3974,
      "step": 1179
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.05418262630701065,
      "learning_rate": 6.90807799442897e-05,
      "loss": 0.3055,
      "step": 1180
    },
    {
      "epoch": 1.9683333333333333,
      "grad_norm": 0.0503394678235054,
      "learning_rate": 6.896935933147633e-05,
      "loss": 0.3082,
      "step": 1181
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.042076535522937775,
      "learning_rate": 6.885793871866295e-05,
      "loss": 0.2733,
      "step": 1182
    },
    {
      "epoch": 1.9716666666666667,
      "grad_norm": 0.04861576855182648,
      "learning_rate": 6.874651810584959e-05,
      "loss": 0.342,
      "step": 1183
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.03602268919348717,
      "learning_rate": 6.863509749303621e-05,
      "loss": 0.2801,
      "step": 1184
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.033840883523225784,
      "learning_rate": 6.852367688022285e-05,
      "loss": 0.2123,
      "step": 1185
    },
    {
      "epoch": 1.9766666666666666,
      "grad_norm": 0.0330713614821434,
      "learning_rate": 6.841225626740947e-05,
      "loss": 0.2377,
      "step": 1186
    },
    {
      "epoch": 1.9783333333333335,
      "grad_norm": 0.05485839769244194,
      "learning_rate": 6.83008356545961e-05,
      "loss": 0.3309,
      "step": 1187
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.08233452588319778,
      "learning_rate": 6.818941504178273e-05,
      "loss": 0.3324,
      "step": 1188
    },
    {
      "epoch": 1.9816666666666667,
      "grad_norm": 0.05681740492582321,
      "learning_rate": 6.807799442896936e-05,
      "loss": 0.245,
      "step": 1189
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.07135338336229324,
      "learning_rate": 6.796657381615599e-05,
      "loss": 0.3694,
      "step": 1190
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 0.06960999220609665,
      "learning_rate": 6.785515320334262e-05,
      "loss": 0.3938,
      "step": 1191
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.06247348338365555,
      "learning_rate": 6.774373259052925e-05,
      "loss": 0.3872,
      "step": 1192
    },
    {
      "epoch": 1.9883333333333333,
      "grad_norm": 0.06307369470596313,
      "learning_rate": 6.763231197771588e-05,
      "loss": 0.3098,
      "step": 1193
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.06166960299015045,
      "learning_rate": 6.75208913649025e-05,
      "loss": 0.3393,
      "step": 1194
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 0.04023783653974533,
      "learning_rate": 6.740947075208914e-05,
      "loss": 0.2564,
      "step": 1195
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.051844604313373566,
      "learning_rate": 6.729805013927577e-05,
      "loss": 0.3222,
      "step": 1196
    },
    {
      "epoch": 1.995,
      "grad_norm": 0.048737820237874985,
      "learning_rate": 6.71866295264624e-05,
      "loss": 0.2834,
      "step": 1197
    },
    {
      "epoch": 1.9966666666666666,
      "grad_norm": 0.052716705948114395,
      "learning_rate": 6.707520891364903e-05,
      "loss": 0.3164,
      "step": 1198
    },
    {
      "epoch": 1.9983333333333333,
      "grad_norm": 0.04138531908392906,
      "learning_rate": 6.696378830083566e-05,
      "loss": 0.2897,
      "step": 1199
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.04967043548822403,
      "learning_rate": 6.685236768802228e-05,
      "loss": 0.3126,
      "step": 1200
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.3209705650806427,
      "eval_runtime": 373.6557,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1200
    },
    {
      "epoch": 2.0016666666666665,
      "grad_norm": 0.0407005250453949,
      "learning_rate": 6.674094707520892e-05,
      "loss": 0.2959,
      "step": 1201
    },
    {
      "epoch": 2.0033333333333334,
      "grad_norm": 0.05046853795647621,
      "learning_rate": 6.662952646239554e-05,
      "loss": 0.3223,
      "step": 1202
    },
    {
      "epoch": 2.005,
      "grad_norm": 0.03499262034893036,
      "learning_rate": 6.651810584958218e-05,
      "loss": 0.2761,
      "step": 1203
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.05014203116297722,
      "learning_rate": 6.64066852367688e-05,
      "loss": 0.2913,
      "step": 1204
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.09511170536279678,
      "learning_rate": 6.629526462395543e-05,
      "loss": 0.4162,
      "step": 1205
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.08506960421800613,
      "learning_rate": 6.618384401114206e-05,
      "loss": 0.3639,
      "step": 1206
    },
    {
      "epoch": 2.0116666666666667,
      "grad_norm": 0.04049472510814667,
      "learning_rate": 6.60724233983287e-05,
      "loss": 0.2592,
      "step": 1207
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.05180841684341431,
      "learning_rate": 6.596100278551532e-05,
      "loss": 0.286,
      "step": 1208
    },
    {
      "epoch": 2.015,
      "grad_norm": 0.05773412808775902,
      "learning_rate": 6.584958217270195e-05,
      "loss": 0.3139,
      "step": 1209
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.04725829139351845,
      "learning_rate": 6.573816155988858e-05,
      "loss": 0.2598,
      "step": 1210
    },
    {
      "epoch": 2.0183333333333335,
      "grad_norm": 0.04643702134490013,
      "learning_rate": 6.562674094707522e-05,
      "loss": 0.2674,
      "step": 1211
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.0490817166864872,
      "learning_rate": 6.551532033426184e-05,
      "loss": 0.2693,
      "step": 1212
    },
    {
      "epoch": 2.0216666666666665,
      "grad_norm": 0.06700317561626434,
      "learning_rate": 6.540389972144846e-05,
      "loss": 0.3524,
      "step": 1213
    },
    {
      "epoch": 2.0233333333333334,
      "grad_norm": 0.047703832387924194,
      "learning_rate": 6.52924791086351e-05,
      "loss": 0.2862,
      "step": 1214
    },
    {
      "epoch": 2.025,
      "grad_norm": 0.04658444970846176,
      "learning_rate": 6.518105849582174e-05,
      "loss": 0.3194,
      "step": 1215
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.05310896411538124,
      "learning_rate": 6.506963788300836e-05,
      "loss": 0.3221,
      "step": 1216
    },
    {
      "epoch": 2.0283333333333333,
      "grad_norm": 0.07135602086782455,
      "learning_rate": 6.495821727019498e-05,
      "loss": 0.348,
      "step": 1217
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.06830062717199326,
      "learning_rate": 6.484679665738162e-05,
      "loss": 0.3271,
      "step": 1218
    },
    {
      "epoch": 2.0316666666666667,
      "grad_norm": 0.059785228222608566,
      "learning_rate": 6.473537604456826e-05,
      "loss": 0.2839,
      "step": 1219
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.07451581954956055,
      "learning_rate": 6.462395543175488e-05,
      "loss": 0.3782,
      "step": 1220
    },
    {
      "epoch": 2.035,
      "grad_norm": 0.05150222033262253,
      "learning_rate": 6.45125348189415e-05,
      "loss": 0.2912,
      "step": 1221
    },
    {
      "epoch": 2.0366666666666666,
      "grad_norm": 0.050495509058237076,
      "learning_rate": 6.440111420612814e-05,
      "loss": 0.2599,
      "step": 1222
    },
    {
      "epoch": 2.038333333333333,
      "grad_norm": 0.05923295393586159,
      "learning_rate": 6.428969359331477e-05,
      "loss": 0.3092,
      "step": 1223
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.06040187552571297,
      "learning_rate": 6.41782729805014e-05,
      "loss": 0.2865,
      "step": 1224
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 0.05951881408691406,
      "learning_rate": 6.406685236768802e-05,
      "loss": 0.2982,
      "step": 1225
    },
    {
      "epoch": 2.0433333333333334,
      "grad_norm": 0.06327152997255325,
      "learning_rate": 6.395543175487466e-05,
      "loss": 0.3362,
      "step": 1226
    },
    {
      "epoch": 2.045,
      "grad_norm": 0.04942341893911362,
      "learning_rate": 6.38440111420613e-05,
      "loss": 0.3055,
      "step": 1227
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.05804983153939247,
      "learning_rate": 6.373259052924792e-05,
      "loss": 0.2889,
      "step": 1228
    },
    {
      "epoch": 2.0483333333333333,
      "grad_norm": 0.04934825375676155,
      "learning_rate": 6.362116991643454e-05,
      "loss": 0.3498,
      "step": 1229
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.051428891718387604,
      "learning_rate": 6.350974930362116e-05,
      "loss": 0.2739,
      "step": 1230
    },
    {
      "epoch": 2.0516666666666667,
      "grad_norm": 0.03629006817936897,
      "learning_rate": 6.339832869080781e-05,
      "loss": 0.2892,
      "step": 1231
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.0608680360019207,
      "learning_rate": 6.328690807799444e-05,
      "loss": 0.3376,
      "step": 1232
    },
    {
      "epoch": 2.055,
      "grad_norm": 0.07683080434799194,
      "learning_rate": 6.317548746518106e-05,
      "loss": 0.407,
      "step": 1233
    },
    {
      "epoch": 2.0566666666666666,
      "grad_norm": 0.08289474248886108,
      "learning_rate": 6.306406685236768e-05,
      "loss": 0.3374,
      "step": 1234
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.04473930224776268,
      "learning_rate": 6.295264623955433e-05,
      "loss": 0.2656,
      "step": 1235
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.03917067497968674,
      "learning_rate": 6.284122562674095e-05,
      "loss": 0.2639,
      "step": 1236
    },
    {
      "epoch": 2.0616666666666665,
      "grad_norm": 0.0576288141310215,
      "learning_rate": 6.272980501392758e-05,
      "loss": 0.3994,
      "step": 1237
    },
    {
      "epoch": 2.0633333333333335,
      "grad_norm": 0.04638929292559624,
      "learning_rate": 6.26183844011142e-05,
      "loss": 0.2354,
      "step": 1238
    },
    {
      "epoch": 2.065,
      "grad_norm": 0.04579152539372444,
      "learning_rate": 6.250696378830084e-05,
      "loss": 0.2811,
      "step": 1239
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.08506648242473602,
      "learning_rate": 6.239554317548747e-05,
      "loss": 0.3242,
      "step": 1240
    },
    {
      "epoch": 2.0683333333333334,
      "grad_norm": 0.06799248605966568,
      "learning_rate": 6.22841225626741e-05,
      "loss": 0.3585,
      "step": 1241
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.03649028018116951,
      "learning_rate": 6.217270194986072e-05,
      "loss": 0.2734,
      "step": 1242
    },
    {
      "epoch": 2.0716666666666668,
      "grad_norm": 0.06624673306941986,
      "learning_rate": 6.206128133704736e-05,
      "loss": 0.315,
      "step": 1243
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.05013921484351158,
      "learning_rate": 6.194986072423399e-05,
      "loss": 0.2556,
      "step": 1244
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.04889668896794319,
      "learning_rate": 6.183844011142061e-05,
      "loss": 0.2807,
      "step": 1245
    },
    {
      "epoch": 2.0766666666666667,
      "grad_norm": 0.04955616220831871,
      "learning_rate": 6.172701949860724e-05,
      "loss": 0.2898,
      "step": 1246
    },
    {
      "epoch": 2.078333333333333,
      "grad_norm": 0.0779874324798584,
      "learning_rate": 6.161559888579387e-05,
      "loss": 0.4326,
      "step": 1247
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.028741806745529175,
      "learning_rate": 6.150417827298051e-05,
      "loss": 0.222,
      "step": 1248
    },
    {
      "epoch": 2.0816666666666666,
      "grad_norm": 0.048552509397268295,
      "learning_rate": 6.139275766016713e-05,
      "loss": 0.2591,
      "step": 1249
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.06918494403362274,
      "learning_rate": 6.128133704735376e-05,
      "loss": 0.2724,
      "step": 1250
    },
    {
      "epoch": 2.085,
      "grad_norm": 0.0395839624106884,
      "learning_rate": 6.116991643454039e-05,
      "loss": 0.28,
      "step": 1251
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.04464644566178322,
      "learning_rate": 6.105849582172703e-05,
      "loss": 0.2818,
      "step": 1252
    },
    {
      "epoch": 2.0883333333333334,
      "grad_norm": 0.04318235069513321,
      "learning_rate": 6.094707520891365e-05,
      "loss": 0.2829,
      "step": 1253
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.04781635105609894,
      "learning_rate": 6.0835654596100275e-05,
      "loss": 0.2882,
      "step": 1254
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 0.04946117848157883,
      "learning_rate": 6.072423398328692e-05,
      "loss": 0.3315,
      "step": 1255
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.05562993884086609,
      "learning_rate": 6.061281337047354e-05,
      "loss": 0.3392,
      "step": 1256
    },
    {
      "epoch": 2.095,
      "grad_norm": 0.05542553961277008,
      "learning_rate": 6.050139275766017e-05,
      "loss": 0.3068,
      "step": 1257
    },
    {
      "epoch": 2.0966666666666667,
      "grad_norm": 0.04027304798364639,
      "learning_rate": 6.0389972144846794e-05,
      "loss": 0.2578,
      "step": 1258
    },
    {
      "epoch": 2.098333333333333,
      "grad_norm": 0.06204454228281975,
      "learning_rate": 6.027855153203343e-05,
      "loss": 0.394,
      "step": 1259
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.053169578313827515,
      "learning_rate": 6.016713091922006e-05,
      "loss": 0.3278,
      "step": 1260
    },
    {
      "epoch": 2.1016666666666666,
      "grad_norm": 0.05118163302540779,
      "learning_rate": 6.005571030640669e-05,
      "loss": 0.3642,
      "step": 1261
    },
    {
      "epoch": 2.1033333333333335,
      "grad_norm": 0.05617315694689751,
      "learning_rate": 5.994428969359331e-05,
      "loss": 0.3415,
      "step": 1262
    },
    {
      "epoch": 2.105,
      "grad_norm": 0.055213283747434616,
      "learning_rate": 5.983286908077995e-05,
      "loss": 0.3133,
      "step": 1263
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.04638250917196274,
      "learning_rate": 5.972144846796658e-05,
      "loss": 0.2564,
      "step": 1264
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.0408797413110733,
      "learning_rate": 5.96100278551532e-05,
      "loss": 0.2162,
      "step": 1265
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.05249958485364914,
      "learning_rate": 5.949860724233983e-05,
      "loss": 0.2869,
      "step": 1266
    },
    {
      "epoch": 2.111666666666667,
      "grad_norm": 0.04833338037133217,
      "learning_rate": 5.938718662952647e-05,
      "loss": 0.2936,
      "step": 1267
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.07411740720272064,
      "learning_rate": 5.92757660167131e-05,
      "loss": 0.3621,
      "step": 1268
    },
    {
      "epoch": 2.115,
      "grad_norm": 0.040167681872844696,
      "learning_rate": 5.916434540389972e-05,
      "loss": 0.2384,
      "step": 1269
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.045671552419662476,
      "learning_rate": 5.905292479108635e-05,
      "loss": 0.3164,
      "step": 1270
    },
    {
      "epoch": 2.118333333333333,
      "grad_norm": 0.045155927538871765,
      "learning_rate": 5.894150417827299e-05,
      "loss": 0.3256,
      "step": 1271
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.03627917543053627,
      "learning_rate": 5.883008356545962e-05,
      "loss": 0.2298,
      "step": 1272
    },
    {
      "epoch": 2.1216666666666666,
      "grad_norm": 0.04618803784251213,
      "learning_rate": 5.871866295264624e-05,
      "loss": 0.2625,
      "step": 1273
    },
    {
      "epoch": 2.1233333333333335,
      "grad_norm": 0.04439480975270271,
      "learning_rate": 5.860724233983287e-05,
      "loss": 0.2962,
      "step": 1274
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.10951309651136398,
      "learning_rate": 5.8495821727019506e-05,
      "loss": 0.389,
      "step": 1275
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.058789752423763275,
      "learning_rate": 5.8384401114206136e-05,
      "loss": 0.3568,
      "step": 1276
    },
    {
      "epoch": 2.1283333333333334,
      "grad_norm": 0.05015858635306358,
      "learning_rate": 5.827298050139276e-05,
      "loss": 0.3069,
      "step": 1277
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.04058518260717392,
      "learning_rate": 5.816155988857939e-05,
      "loss": 0.2512,
      "step": 1278
    },
    {
      "epoch": 2.131666666666667,
      "grad_norm": 0.06230128929018974,
      "learning_rate": 5.8050139275766025e-05,
      "loss": 0.3176,
      "step": 1279
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.059363726526498795,
      "learning_rate": 5.7938718662952654e-05,
      "loss": 0.3061,
      "step": 1280
    },
    {
      "epoch": 2.135,
      "grad_norm": 0.0390380322933197,
      "learning_rate": 5.782729805013928e-05,
      "loss": 0.2511,
      "step": 1281
    },
    {
      "epoch": 2.1366666666666667,
      "grad_norm": 0.04649966210126877,
      "learning_rate": 5.771587743732591e-05,
      "loss": 0.3042,
      "step": 1282
    },
    {
      "epoch": 2.138333333333333,
      "grad_norm": 0.04393063113093376,
      "learning_rate": 5.7604456824512544e-05,
      "loss": 0.2625,
      "step": 1283
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.04021163284778595,
      "learning_rate": 5.7493036211699167e-05,
      "loss": 0.242,
      "step": 1284
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 0.04700512811541557,
      "learning_rate": 5.7381615598885796e-05,
      "loss": 0.2933,
      "step": 1285
    },
    {
      "epoch": 2.1433333333333335,
      "grad_norm": 0.051543090492486954,
      "learning_rate": 5.7270194986072426e-05,
      "loss": 0.3391,
      "step": 1286
    },
    {
      "epoch": 2.145,
      "grad_norm": 0.07610370218753815,
      "learning_rate": 5.715877437325906e-05,
      "loss": 0.3198,
      "step": 1287
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.04297381266951561,
      "learning_rate": 5.7047353760445685e-05,
      "loss": 0.2958,
      "step": 1288
    },
    {
      "epoch": 2.1483333333333334,
      "grad_norm": 0.06201035529375076,
      "learning_rate": 5.6935933147632315e-05,
      "loss": 0.3351,
      "step": 1289
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.06196632236242294,
      "learning_rate": 5.682451253481894e-05,
      "loss": 0.3604,
      "step": 1290
    },
    {
      "epoch": 2.151666666666667,
      "grad_norm": 0.09168257564306259,
      "learning_rate": 5.671309192200558e-05,
      "loss": 0.357,
      "step": 1291
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.04834439978003502,
      "learning_rate": 5.6601671309192204e-05,
      "loss": 0.269,
      "step": 1292
    },
    {
      "epoch": 2.155,
      "grad_norm": 0.047708213329315186,
      "learning_rate": 5.6490250696378834e-05,
      "loss": 0.2736,
      "step": 1293
    },
    {
      "epoch": 2.1566666666666667,
      "grad_norm": 0.04304851219058037,
      "learning_rate": 5.637883008356546e-05,
      "loss": 0.3066,
      "step": 1294
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 0.0476490780711174,
      "learning_rate": 5.62674094707521e-05,
      "loss": 0.3063,
      "step": 1295
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.037564411759376526,
      "learning_rate": 5.615598885793872e-05,
      "loss": 0.2531,
      "step": 1296
    },
    {
      "epoch": 2.1616666666666666,
      "grad_norm": 0.0386945866048336,
      "learning_rate": 5.604456824512535e-05,
      "loss": 0.2528,
      "step": 1297
    },
    {
      "epoch": 2.163333333333333,
      "grad_norm": 0.049086060374975204,
      "learning_rate": 5.5933147632311976e-05,
      "loss": 0.3107,
      "step": 1298
    },
    {
      "epoch": 2.165,
      "grad_norm": 0.05495753139257431,
      "learning_rate": 5.582172701949861e-05,
      "loss": 0.3558,
      "step": 1299
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.03846137970685959,
      "learning_rate": 5.571030640668524e-05,
      "loss": 0.284,
      "step": 1300
    },
    {
      "epoch": 2.1666666666666665,
      "eval_loss": 0.32036784291267395,
      "eval_runtime": 373.5067,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1300
    },
    {
      "epoch": 2.1683333333333334,
      "grad_norm": 0.04390823841094971,
      "learning_rate": 5.559888579387187e-05,
      "loss": 0.2851,
      "step": 1301
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.04295789450407028,
      "learning_rate": 5.5487465181058494e-05,
      "loss": 0.2324,
      "step": 1302
    },
    {
      "epoch": 2.171666666666667,
      "grad_norm": 0.0544780008494854,
      "learning_rate": 5.537604456824513e-05,
      "loss": 0.3276,
      "step": 1303
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.06240323558449745,
      "learning_rate": 5.526462395543176e-05,
      "loss": 0.309,
      "step": 1304
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.04323374480009079,
      "learning_rate": 5.5153203342618384e-05,
      "loss": 0.3187,
      "step": 1305
    },
    {
      "epoch": 2.1766666666666667,
      "grad_norm": 0.03511737287044525,
      "learning_rate": 5.504178272980501e-05,
      "loss": 0.2616,
      "step": 1306
    },
    {
      "epoch": 2.1783333333333332,
      "grad_norm": 0.05280962586402893,
      "learning_rate": 5.493036211699165e-05,
      "loss": 0.3332,
      "step": 1307
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.05446241423487663,
      "learning_rate": 5.481894150417828e-05,
      "loss": 0.3704,
      "step": 1308
    },
    {
      "epoch": 2.1816666666666666,
      "grad_norm": 0.043858565390110016,
      "learning_rate": 5.47075208913649e-05,
      "loss": 0.3203,
      "step": 1309
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.065448097884655,
      "learning_rate": 5.459610027855153e-05,
      "loss": 0.3515,
      "step": 1310
    },
    {
      "epoch": 2.185,
      "grad_norm": 0.05817539989948273,
      "learning_rate": 5.448467966573816e-05,
      "loss": 0.324,
      "step": 1311
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.0471237450838089,
      "learning_rate": 5.43732590529248e-05,
      "loss": 0.3024,
      "step": 1312
    },
    {
      "epoch": 2.1883333333333335,
      "grad_norm": 0.028796203434467316,
      "learning_rate": 5.426183844011142e-05,
      "loss": 0.2221,
      "step": 1313
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.07597151398658752,
      "learning_rate": 5.415041782729805e-05,
      "loss": 0.3979,
      "step": 1314
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 0.06946378946304321,
      "learning_rate": 5.4038997214484674e-05,
      "loss": 0.4061,
      "step": 1315
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.07725159823894501,
      "learning_rate": 5.392757660167132e-05,
      "loss": 0.3594,
      "step": 1316
    },
    {
      "epoch": 2.195,
      "grad_norm": 0.048408735543489456,
      "learning_rate": 5.381615598885794e-05,
      "loss": 0.2626,
      "step": 1317
    },
    {
      "epoch": 2.1966666666666668,
      "grad_norm": 0.04520096257328987,
      "learning_rate": 5.370473537604457e-05,
      "loss": 0.3213,
      "step": 1318
    },
    {
      "epoch": 2.1983333333333333,
      "grad_norm": 0.0671725943684578,
      "learning_rate": 5.359331476323119e-05,
      "loss": 0.3525,
      "step": 1319
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.05163251981139183,
      "learning_rate": 5.3481894150417836e-05,
      "loss": 0.3265,
      "step": 1320
    },
    {
      "epoch": 2.2016666666666667,
      "grad_norm": 0.050987567752599716,
      "learning_rate": 5.337047353760446e-05,
      "loss": 0.3594,
      "step": 1321
    },
    {
      "epoch": 2.203333333333333,
      "grad_norm": 0.04127103090286255,
      "learning_rate": 5.325905292479109e-05,
      "loss": 0.301,
      "step": 1322
    },
    {
      "epoch": 2.205,
      "grad_norm": 0.05757347121834755,
      "learning_rate": 5.314763231197771e-05,
      "loss": 0.3646,
      "step": 1323
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.05433688685297966,
      "learning_rate": 5.303621169916435e-05,
      "loss": 0.3147,
      "step": 1324
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 0.07158727198839188,
      "learning_rate": 5.292479108635098e-05,
      "loss": 0.4275,
      "step": 1325
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.046951454132795334,
      "learning_rate": 5.281337047353761e-05,
      "loss": 0.3024,
      "step": 1326
    },
    {
      "epoch": 2.211666666666667,
      "grad_norm": 0.05740853399038315,
      "learning_rate": 5.270194986072423e-05,
      "loss": 0.3739,
      "step": 1327
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.07736902683973312,
      "learning_rate": 5.259052924791087e-05,
      "loss": 0.3733,
      "step": 1328
    },
    {
      "epoch": 2.215,
      "grad_norm": 0.05296056345105171,
      "learning_rate": 5.2479108635097497e-05,
      "loss": 0.3594,
      "step": 1329
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.07896334677934647,
      "learning_rate": 5.236768802228412e-05,
      "loss": 0.3018,
      "step": 1330
    },
    {
      "epoch": 2.2183333333333333,
      "grad_norm": 0.0622800812125206,
      "learning_rate": 5.225626740947075e-05,
      "loss": 0.3404,
      "step": 1331
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.033818554133176804,
      "learning_rate": 5.2144846796657386e-05,
      "loss": 0.246,
      "step": 1332
    },
    {
      "epoch": 2.2216666666666667,
      "grad_norm": 0.05386700853705406,
      "learning_rate": 5.2033426183844015e-05,
      "loss": 0.2884,
      "step": 1333
    },
    {
      "epoch": 2.223333333333333,
      "grad_norm": 0.06027147173881531,
      "learning_rate": 5.192200557103064e-05,
      "loss": 0.3389,
      "step": 1334
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.04384075105190277,
      "learning_rate": 5.181058495821727e-05,
      "loss": 0.3091,
      "step": 1335
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.1127052828669548,
      "learning_rate": 5.1699164345403904e-05,
      "loss": 0.4358,
      "step": 1336
    },
    {
      "epoch": 2.2283333333333335,
      "grad_norm": 0.05722548067569733,
      "learning_rate": 5.1587743732590534e-05,
      "loss": 0.3107,
      "step": 1337
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.05171726644039154,
      "learning_rate": 5.147632311977716e-05,
      "loss": 0.345,
      "step": 1338
    },
    {
      "epoch": 2.2316666666666665,
      "grad_norm": 0.06307576596736908,
      "learning_rate": 5.136490250696379e-05,
      "loss": 0.3682,
      "step": 1339
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.0464811809360981,
      "learning_rate": 5.125348189415042e-05,
      "loss": 0.2684,
      "step": 1340
    },
    {
      "epoch": 2.235,
      "grad_norm": 0.053933873772621155,
      "learning_rate": 5.114206128133705e-05,
      "loss": 0.2799,
      "step": 1341
    },
    {
      "epoch": 2.236666666666667,
      "grad_norm": 0.058260053396224976,
      "learning_rate": 5.1030640668523676e-05,
      "loss": 0.3214,
      "step": 1342
    },
    {
      "epoch": 2.2383333333333333,
      "grad_norm": 0.07201971858739853,
      "learning_rate": 5.0919220055710306e-05,
      "loss": 0.3411,
      "step": 1343
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.05166631191968918,
      "learning_rate": 5.080779944289694e-05,
      "loss": 0.2988,
      "step": 1344
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 0.0419587679207325,
      "learning_rate": 5.069637883008357e-05,
      "loss": 0.2642,
      "step": 1345
    },
    {
      "epoch": 2.243333333333333,
      "grad_norm": 0.052430711686611176,
      "learning_rate": 5.0584958217270195e-05,
      "loss": 0.3018,
      "step": 1346
    },
    {
      "epoch": 2.245,
      "grad_norm": 0.0429723784327507,
      "learning_rate": 5.0473537604456824e-05,
      "loss": 0.302,
      "step": 1347
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.050894588232040405,
      "learning_rate": 5.036211699164346e-05,
      "loss": 0.3135,
      "step": 1348
    },
    {
      "epoch": 2.2483333333333335,
      "grad_norm": 0.050591256469488144,
      "learning_rate": 5.0250696378830084e-05,
      "loss": 0.3054,
      "step": 1349
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.04097146540880203,
      "learning_rate": 5.0139275766016714e-05,
      "loss": 0.2726,
      "step": 1350
    },
    {
      "epoch": 2.2516666666666665,
      "grad_norm": 0.06555423885583878,
      "learning_rate": 5.002785515320334e-05,
      "loss": 0.3365,
      "step": 1351
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.04588885232806206,
      "learning_rate": 4.991643454038997e-05,
      "loss": 0.2963,
      "step": 1352
    },
    {
      "epoch": 2.255,
      "grad_norm": 0.044687654823064804,
      "learning_rate": 4.98050139275766e-05,
      "loss": 0.2602,
      "step": 1353
    },
    {
      "epoch": 2.256666666666667,
      "grad_norm": 0.04710475355386734,
      "learning_rate": 4.969359331476323e-05,
      "loss": 0.2875,
      "step": 1354
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.051271289587020874,
      "learning_rate": 4.958217270194986e-05,
      "loss": 0.2924,
      "step": 1355
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.04620908945798874,
      "learning_rate": 4.947075208913649e-05,
      "loss": 0.3443,
      "step": 1356
    },
    {
      "epoch": 2.2616666666666667,
      "grad_norm": 0.06785520166158676,
      "learning_rate": 4.935933147632312e-05,
      "loss": 0.3606,
      "step": 1357
    },
    {
      "epoch": 2.263333333333333,
      "grad_norm": 0.049830302596092224,
      "learning_rate": 4.924791086350975e-05,
      "loss": 0.33,
      "step": 1358
    },
    {
      "epoch": 2.265,
      "grad_norm": 0.045058537274599075,
      "learning_rate": 4.913649025069638e-05,
      "loss": 0.3361,
      "step": 1359
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.043794453144073486,
      "learning_rate": 4.902506963788301e-05,
      "loss": 0.3056,
      "step": 1360
    },
    {
      "epoch": 2.2683333333333335,
      "grad_norm": 0.04959990456700325,
      "learning_rate": 4.891364902506964e-05,
      "loss": 0.3399,
      "step": 1361
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.03161786496639252,
      "learning_rate": 4.880222841225627e-05,
      "loss": 0.2311,
      "step": 1362
    },
    {
      "epoch": 2.2716666666666665,
      "grad_norm": 0.06114078685641289,
      "learning_rate": 4.86908077994429e-05,
      "loss": 0.3468,
      "step": 1363
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.04805685207247734,
      "learning_rate": 4.857938718662953e-05,
      "loss": 0.2957,
      "step": 1364
    },
    {
      "epoch": 2.275,
      "grad_norm": 0.060297880321741104,
      "learning_rate": 4.846796657381616e-05,
      "loss": 0.3279,
      "step": 1365
    },
    {
      "epoch": 2.276666666666667,
      "grad_norm": 0.06647749990224838,
      "learning_rate": 4.835654596100279e-05,
      "loss": 0.3364,
      "step": 1366
    },
    {
      "epoch": 2.2783333333333333,
      "grad_norm": 0.06022699549794197,
      "learning_rate": 4.824512534818942e-05,
      "loss": 0.3286,
      "step": 1367
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.04562969505786896,
      "learning_rate": 4.813370473537605e-05,
      "loss": 0.3384,
      "step": 1368
    },
    {
      "epoch": 2.2816666666666667,
      "grad_norm": 0.03745248168706894,
      "learning_rate": 4.802228412256268e-05,
      "loss": 0.2301,
      "step": 1369
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.05121861770749092,
      "learning_rate": 4.79108635097493e-05,
      "loss": 0.3101,
      "step": 1370
    },
    {
      "epoch": 2.285,
      "grad_norm": 0.049903497099876404,
      "learning_rate": 4.779944289693594e-05,
      "loss": 0.3245,
      "step": 1371
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.05269569158554077,
      "learning_rate": 4.768802228412256e-05,
      "loss": 0.3359,
      "step": 1372
    },
    {
      "epoch": 2.288333333333333,
      "grad_norm": 0.058786455541849136,
      "learning_rate": 4.75766016713092e-05,
      "loss": 0.3859,
      "step": 1373
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.04777684062719345,
      "learning_rate": 4.746518105849582e-05,
      "loss": 0.3147,
      "step": 1374
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.05030466243624687,
      "learning_rate": 4.7353760445682456e-05,
      "loss": 0.3319,
      "step": 1375
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.044507380574941635,
      "learning_rate": 4.724233983286908e-05,
      "loss": 0.3263,
      "step": 1376
    },
    {
      "epoch": 2.295,
      "grad_norm": 0.03995537757873535,
      "learning_rate": 4.7130919220055716e-05,
      "loss": 0.2611,
      "step": 1377
    },
    {
      "epoch": 2.296666666666667,
      "grad_norm": 0.047087233513593674,
      "learning_rate": 4.701949860724234e-05,
      "loss": 0.2969,
      "step": 1378
    },
    {
      "epoch": 2.2983333333333333,
      "grad_norm": 0.06054156646132469,
      "learning_rate": 4.6908077994428975e-05,
      "loss": 0.3836,
      "step": 1379
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.04205109551548958,
      "learning_rate": 4.67966573816156e-05,
      "loss": 0.3506,
      "step": 1380
    },
    {
      "epoch": 2.3016666666666667,
      "grad_norm": 0.05390225723385811,
      "learning_rate": 4.6685236768802234e-05,
      "loss": 0.3128,
      "step": 1381
    },
    {
      "epoch": 2.3033333333333332,
      "grad_norm": 0.04639840126037598,
      "learning_rate": 4.657381615598886e-05,
      "loss": 0.2891,
      "step": 1382
    },
    {
      "epoch": 2.305,
      "grad_norm": 0.09639430791139603,
      "learning_rate": 4.6462395543175494e-05,
      "loss": 0.3807,
      "step": 1383
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.06929255276918411,
      "learning_rate": 4.635097493036212e-05,
      "loss": 0.3856,
      "step": 1384
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 0.05066024139523506,
      "learning_rate": 4.623955431754875e-05,
      "loss": 0.3785,
      "step": 1385
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.04456472024321556,
      "learning_rate": 4.6128133704735376e-05,
      "loss": 0.3035,
      "step": 1386
    },
    {
      "epoch": 2.3116666666666665,
      "grad_norm": 0.07715058326721191,
      "learning_rate": 4.6016713091922006e-05,
      "loss": 0.3276,
      "step": 1387
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.048166126012802124,
      "learning_rate": 4.5905292479108636e-05,
      "loss": 0.3686,
      "step": 1388
    },
    {
      "epoch": 2.315,
      "grad_norm": 0.04863511025905609,
      "learning_rate": 4.5793871866295265e-05,
      "loss": 0.3299,
      "step": 1389
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.05831924453377724,
      "learning_rate": 4.5682451253481895e-05,
      "loss": 0.295,
      "step": 1390
    },
    {
      "epoch": 2.3183333333333334,
      "grad_norm": 0.03508418798446655,
      "learning_rate": 4.5571030640668525e-05,
      "loss": 0.2196,
      "step": 1391
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.036704979836940765,
      "learning_rate": 4.5459610027855154e-05,
      "loss": 0.2675,
      "step": 1392
    },
    {
      "epoch": 2.3216666666666668,
      "grad_norm": 0.05562207102775574,
      "learning_rate": 4.5348189415041784e-05,
      "loss": 0.2795,
      "step": 1393
    },
    {
      "epoch": 2.3233333333333333,
      "grad_norm": 0.045106153935194016,
      "learning_rate": 4.5236768802228414e-05,
      "loss": 0.2641,
      "step": 1394
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.050169479101896286,
      "learning_rate": 4.5125348189415044e-05,
      "loss": 0.3014,
      "step": 1395
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.06871635466814041,
      "learning_rate": 4.501392757660167e-05,
      "loss": 0.2968,
      "step": 1396
    },
    {
      "epoch": 2.328333333333333,
      "grad_norm": 0.031573932617902756,
      "learning_rate": 4.49025069637883e-05,
      "loss": 0.2136,
      "step": 1397
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.05334972217679024,
      "learning_rate": 4.479108635097493e-05,
      "loss": 0.3501,
      "step": 1398
    },
    {
      "epoch": 2.3316666666666666,
      "grad_norm": 0.0339854434132576,
      "learning_rate": 4.467966573816156e-05,
      "loss": 0.2497,
      "step": 1399
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.06502052396535873,
      "learning_rate": 4.456824512534819e-05,
      "loss": 0.312,
      "step": 1400
    },
    {
      "epoch": 2.3333333333333335,
      "eval_loss": 0.32012027502059937,
      "eval_runtime": 373.4889,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1400
    },
    {
      "epoch": 2.335,
      "grad_norm": 0.04965037479996681,
      "learning_rate": 4.445682451253482e-05,
      "loss": 0.2949,
      "step": 1401
    },
    {
      "epoch": 2.336666666666667,
      "grad_norm": 0.05058502405881882,
      "learning_rate": 4.434540389972145e-05,
      "loss": 0.2967,
      "step": 1402
    },
    {
      "epoch": 2.3383333333333334,
      "grad_norm": 0.05852080509066582,
      "learning_rate": 4.423398328690808e-05,
      "loss": 0.3074,
      "step": 1403
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.03811778500676155,
      "learning_rate": 4.412256267409471e-05,
      "loss": 0.2825,
      "step": 1404
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.047390539199113846,
      "learning_rate": 4.401114206128134e-05,
      "loss": 0.301,
      "step": 1405
    },
    {
      "epoch": 2.3433333333333333,
      "grad_norm": 0.05347207933664322,
      "learning_rate": 4.389972144846797e-05,
      "loss": 0.358,
      "step": 1406
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 0.04978249967098236,
      "learning_rate": 4.37883008356546e-05,
      "loss": 0.3751,
      "step": 1407
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.041735634207725525,
      "learning_rate": 4.367688022284123e-05,
      "loss": 0.2726,
      "step": 1408
    },
    {
      "epoch": 2.348333333333333,
      "grad_norm": 0.03427409380674362,
      "learning_rate": 4.356545961002786e-05,
      "loss": 0.25,
      "step": 1409
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.060285117477178574,
      "learning_rate": 4.345403899721448e-05,
      "loss": 0.3536,
      "step": 1410
    },
    {
      "epoch": 2.3516666666666666,
      "grad_norm": 0.054794907569885254,
      "learning_rate": 4.334261838440112e-05,
      "loss": 0.3368,
      "step": 1411
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.06649518013000488,
      "learning_rate": 4.323119777158774e-05,
      "loss": 0.3071,
      "step": 1412
    },
    {
      "epoch": 2.355,
      "grad_norm": 0.03591005504131317,
      "learning_rate": 4.311977715877438e-05,
      "loss": 0.206,
      "step": 1413
    },
    {
      "epoch": 2.3566666666666665,
      "grad_norm": 0.08565974235534668,
      "learning_rate": 4.3008356545961e-05,
      "loss": 0.3705,
      "step": 1414
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 0.03660917282104492,
      "learning_rate": 4.289693593314764e-05,
      "loss": 0.2801,
      "step": 1415
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.04539922997355461,
      "learning_rate": 4.278551532033426e-05,
      "loss": 0.3017,
      "step": 1416
    },
    {
      "epoch": 2.361666666666667,
      "grad_norm": 0.0370650589466095,
      "learning_rate": 4.26740947075209e-05,
      "loss": 0.2629,
      "step": 1417
    },
    {
      "epoch": 2.3633333333333333,
      "grad_norm": 0.05450792610645294,
      "learning_rate": 4.256267409470752e-05,
      "loss": 0.3501,
      "step": 1418
    },
    {
      "epoch": 2.365,
      "grad_norm": 0.05377973988652229,
      "learning_rate": 4.2451253481894157e-05,
      "loss": 0.3207,
      "step": 1419
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.03876075893640518,
      "learning_rate": 4.233983286908078e-05,
      "loss": 0.2649,
      "step": 1420
    },
    {
      "epoch": 2.368333333333333,
      "grad_norm": 0.04438978061079979,
      "learning_rate": 4.2228412256267416e-05,
      "loss": 0.2701,
      "step": 1421
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.04075755551457405,
      "learning_rate": 4.211699164345404e-05,
      "loss": 0.2676,
      "step": 1422
    },
    {
      "epoch": 2.3716666666666666,
      "grad_norm": 0.05280226469039917,
      "learning_rate": 4.2005571030640675e-05,
      "loss": 0.2535,
      "step": 1423
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.04251730442047119,
      "learning_rate": 4.18941504178273e-05,
      "loss": 0.28,
      "step": 1424
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.04139016941189766,
      "learning_rate": 4.1782729805013935e-05,
      "loss": 0.2556,
      "step": 1425
    },
    {
      "epoch": 2.3766666666666665,
      "grad_norm": 0.051740508526563644,
      "learning_rate": 4.167130919220056e-05,
      "loss": 0.3354,
      "step": 1426
    },
    {
      "epoch": 2.3783333333333334,
      "grad_norm": 0.08636701107025146,
      "learning_rate": 4.155988857938719e-05,
      "loss": 0.3585,
      "step": 1427
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.06627444922924042,
      "learning_rate": 4.144846796657382e-05,
      "loss": 0.3454,
      "step": 1428
    },
    {
      "epoch": 2.381666666666667,
      "grad_norm": 0.04332584887742996,
      "learning_rate": 4.133704735376045e-05,
      "loss": 0.2917,
      "step": 1429
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.05122853443026543,
      "learning_rate": 4.1225626740947077e-05,
      "loss": 0.346,
      "step": 1430
    },
    {
      "epoch": 2.385,
      "grad_norm": 0.05963818356394768,
      "learning_rate": 4.1114206128133706e-05,
      "loss": 0.3448,
      "step": 1431
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.04039420560002327,
      "learning_rate": 4.1002785515320336e-05,
      "loss": 0.3156,
      "step": 1432
    },
    {
      "epoch": 2.388333333333333,
      "grad_norm": 0.04624377191066742,
      "learning_rate": 4.0891364902506966e-05,
      "loss": 0.257,
      "step": 1433
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.08895783871412277,
      "learning_rate": 4.0779944289693595e-05,
      "loss": 0.3317,
      "step": 1434
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.08151503652334213,
      "learning_rate": 4.0668523676880225e-05,
      "loss": 0.3518,
      "step": 1435
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.039446648210287094,
      "learning_rate": 4.0557103064066855e-05,
      "loss": 0.2547,
      "step": 1436
    },
    {
      "epoch": 2.395,
      "grad_norm": 0.059620000422000885,
      "learning_rate": 4.0445682451253484e-05,
      "loss": 0.3349,
      "step": 1437
    },
    {
      "epoch": 2.3966666666666665,
      "grad_norm": 0.04157932847738266,
      "learning_rate": 4.0334261838440114e-05,
      "loss": 0.2635,
      "step": 1438
    },
    {
      "epoch": 2.3983333333333334,
      "grad_norm": 0.03500598296523094,
      "learning_rate": 4.0222841225626744e-05,
      "loss": 0.2557,
      "step": 1439
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.04945174977183342,
      "learning_rate": 4.0111420612813374e-05,
      "loss": 0.3511,
      "step": 1440
    },
    {
      "epoch": 2.401666666666667,
      "grad_norm": 0.043488629162311554,
      "learning_rate": 4e-05,
      "loss": 0.311,
      "step": 1441
    },
    {
      "epoch": 2.4033333333333333,
      "grad_norm": 0.04319607838988304,
      "learning_rate": 3.988857938718663e-05,
      "loss": 0.2555,
      "step": 1442
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 0.034568656235933304,
      "learning_rate": 3.977715877437326e-05,
      "loss": 0.2276,
      "step": 1443
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.04133343696594238,
      "learning_rate": 3.966573816155989e-05,
      "loss": 0.2496,
      "step": 1444
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.03540710359811783,
      "learning_rate": 3.955431754874652e-05,
      "loss": 0.2557,
      "step": 1445
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.040977682918310165,
      "learning_rate": 3.944289693593315e-05,
      "loss": 0.284,
      "step": 1446
    },
    {
      "epoch": 2.4116666666666666,
      "grad_norm": 0.051338110119104385,
      "learning_rate": 3.933147632311978e-05,
      "loss": 0.3199,
      "step": 1447
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.03461872413754463,
      "learning_rate": 3.922005571030641e-05,
      "loss": 0.219,
      "step": 1448
    },
    {
      "epoch": 2.415,
      "grad_norm": 0.03847292810678482,
      "learning_rate": 3.910863509749304e-05,
      "loss": 0.2507,
      "step": 1449
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.03245954215526581,
      "learning_rate": 3.8997214484679664e-05,
      "loss": 0.191,
      "step": 1450
    },
    {
      "epoch": 2.4183333333333334,
      "grad_norm": 0.040062181651592255,
      "learning_rate": 3.88857938718663e-05,
      "loss": 0.2539,
      "step": 1451
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.08160033077001572,
      "learning_rate": 3.877437325905292e-05,
      "loss": 0.3277,
      "step": 1452
    },
    {
      "epoch": 2.421666666666667,
      "grad_norm": 0.04095526039600372,
      "learning_rate": 3.866295264623956e-05,
      "loss": 0.2861,
      "step": 1453
    },
    {
      "epoch": 2.4233333333333333,
      "grad_norm": 0.05381384864449501,
      "learning_rate": 3.855153203342618e-05,
      "loss": 0.3308,
      "step": 1454
    },
    {
      "epoch": 2.425,
      "grad_norm": 0.04430096223950386,
      "learning_rate": 3.844011142061282e-05,
      "loss": 0.3115,
      "step": 1455
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.04958468675613403,
      "learning_rate": 3.832869080779944e-05,
      "loss": 0.3021,
      "step": 1456
    },
    {
      "epoch": 2.4283333333333332,
      "grad_norm": 0.051727160811424255,
      "learning_rate": 3.821727019498608e-05,
      "loss": 0.3451,
      "step": 1457
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.035497672855854034,
      "learning_rate": 3.81058495821727e-05,
      "loss": 0.2497,
      "step": 1458
    },
    {
      "epoch": 2.4316666666666666,
      "grad_norm": 0.054572418332099915,
      "learning_rate": 3.799442896935934e-05,
      "loss": 0.2975,
      "step": 1459
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.04939005896449089,
      "learning_rate": 3.788300835654596e-05,
      "loss": 0.3376,
      "step": 1460
    },
    {
      "epoch": 2.435,
      "grad_norm": 0.05877692252397537,
      "learning_rate": 3.77715877437326e-05,
      "loss": 0.3058,
      "step": 1461
    },
    {
      "epoch": 2.4366666666666665,
      "grad_norm": 0.06428524106740952,
      "learning_rate": 3.766016713091922e-05,
      "loss": 0.338,
      "step": 1462
    },
    {
      "epoch": 2.4383333333333335,
      "grad_norm": 0.05005473271012306,
      "learning_rate": 3.754874651810586e-05,
      "loss": 0.3272,
      "step": 1463
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.04732503369450569,
      "learning_rate": 3.743732590529248e-05,
      "loss": 0.3003,
      "step": 1464
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 0.05266961827874184,
      "learning_rate": 3.7325905292479116e-05,
      "loss": 0.2718,
      "step": 1465
    },
    {
      "epoch": 2.4433333333333334,
      "grad_norm": 0.07062976807355881,
      "learning_rate": 3.721448467966574e-05,
      "loss": 0.3541,
      "step": 1466
    },
    {
      "epoch": 2.445,
      "grad_norm": 0.06100048869848251,
      "learning_rate": 3.710306406685237e-05,
      "loss": 0.3321,
      "step": 1467
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.04942381754517555,
      "learning_rate": 3.6991643454039e-05,
      "loss": 0.2875,
      "step": 1468
    },
    {
      "epoch": 2.4483333333333333,
      "grad_norm": 0.04500700160861015,
      "learning_rate": 3.688022284122563e-05,
      "loss": 0.3139,
      "step": 1469
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.04239075258374214,
      "learning_rate": 3.676880222841226e-05,
      "loss": 0.2796,
      "step": 1470
    },
    {
      "epoch": 2.4516666666666667,
      "grad_norm": 0.058747515082359314,
      "learning_rate": 3.665738161559889e-05,
      "loss": 0.3525,
      "step": 1471
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.07085835933685303,
      "learning_rate": 3.654596100278552e-05,
      "loss": 0.4124,
      "step": 1472
    },
    {
      "epoch": 2.455,
      "grad_norm": 0.062489744275808334,
      "learning_rate": 3.643454038997215e-05,
      "loss": 0.3928,
      "step": 1473
    },
    {
      "epoch": 2.4566666666666666,
      "grad_norm": 0.051710598170757294,
      "learning_rate": 3.632311977715878e-05,
      "loss": 0.3326,
      "step": 1474
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.04871952533721924,
      "learning_rate": 3.62116991643454e-05,
      "loss": 0.3328,
      "step": 1475
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.059727877378463745,
      "learning_rate": 3.6100278551532036e-05,
      "loss": 0.3296,
      "step": 1476
    },
    {
      "epoch": 2.461666666666667,
      "grad_norm": 0.053820982575416565,
      "learning_rate": 3.598885793871866e-05,
      "loss": 0.3063,
      "step": 1477
    },
    {
      "epoch": 2.4633333333333334,
      "grad_norm": 0.054226987063884735,
      "learning_rate": 3.5877437325905296e-05,
      "loss": 0.2444,
      "step": 1478
    },
    {
      "epoch": 2.465,
      "grad_norm": 0.0627044215798378,
      "learning_rate": 3.576601671309192e-05,
      "loss": 0.3566,
      "step": 1479
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.05718740448355675,
      "learning_rate": 3.5654596100278555e-05,
      "loss": 0.3296,
      "step": 1480
    },
    {
      "epoch": 2.4683333333333333,
      "grad_norm": 0.03427960351109505,
      "learning_rate": 3.554317548746518e-05,
      "loss": 0.2479,
      "step": 1481
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.0496165007352829,
      "learning_rate": 3.5431754874651814e-05,
      "loss": 0.3037,
      "step": 1482
    },
    {
      "epoch": 2.4716666666666667,
      "grad_norm": 0.05694787949323654,
      "learning_rate": 3.532033426183844e-05,
      "loss": 0.3324,
      "step": 1483
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.04263099282979965,
      "learning_rate": 3.5208913649025074e-05,
      "loss": 0.2443,
      "step": 1484
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.061483219265937805,
      "learning_rate": 3.50974930362117e-05,
      "loss": 0.3017,
      "step": 1485
    },
    {
      "epoch": 2.4766666666666666,
      "grad_norm": 0.05419601500034332,
      "learning_rate": 3.498607242339833e-05,
      "loss": 0.3072,
      "step": 1486
    },
    {
      "epoch": 2.4783333333333335,
      "grad_norm": 0.04493170604109764,
      "learning_rate": 3.4874651810584956e-05,
      "loss": 0.3067,
      "step": 1487
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.06333377212285995,
      "learning_rate": 3.476323119777159e-05,
      "loss": 0.3712,
      "step": 1488
    },
    {
      "epoch": 2.4816666666666665,
      "grad_norm": 0.05431203171610832,
      "learning_rate": 3.4651810584958216e-05,
      "loss": 0.3503,
      "step": 1489
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.05572059005498886,
      "learning_rate": 3.454038997214485e-05,
      "loss": 0.3293,
      "step": 1490
    },
    {
      "epoch": 2.485,
      "grad_norm": 0.05683689936995506,
      "learning_rate": 3.4428969359331475e-05,
      "loss": 0.3366,
      "step": 1491
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.06599829345941544,
      "learning_rate": 3.4317548746518105e-05,
      "loss": 0.3402,
      "step": 1492
    },
    {
      "epoch": 2.4883333333333333,
      "grad_norm": 0.08786585927009583,
      "learning_rate": 3.4206128133704734e-05,
      "loss": 0.3143,
      "step": 1493
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.07743130624294281,
      "learning_rate": 3.4094707520891364e-05,
      "loss": 0.3187,
      "step": 1494
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 0.05289245769381523,
      "learning_rate": 3.3983286908077994e-05,
      "loss": 0.3844,
      "step": 1495
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.0584622360765934,
      "learning_rate": 3.3871866295264624e-05,
      "loss": 0.343,
      "step": 1496
    },
    {
      "epoch": 2.495,
      "grad_norm": 0.05126229301095009,
      "learning_rate": 3.376044568245125e-05,
      "loss": 0.2541,
      "step": 1497
    },
    {
      "epoch": 2.4966666666666666,
      "grad_norm": 0.09105977416038513,
      "learning_rate": 3.364902506963788e-05,
      "loss": 0.4216,
      "step": 1498
    },
    {
      "epoch": 2.4983333333333335,
      "grad_norm": 0.050618186593055725,
      "learning_rate": 3.353760445682451e-05,
      "loss": 0.339,
      "step": 1499
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.0629943311214447,
      "learning_rate": 3.342618384401114e-05,
      "loss": 0.3833,
      "step": 1500
    },
    {
      "epoch": 2.5,
      "eval_loss": 0.3195089101791382,
      "eval_runtime": 373.2059,
      "eval_samples_per_second": 0.268,
      "eval_steps_per_second": 0.268,
      "step": 1500
    },
    {
      "epoch": 2.501666666666667,
      "grad_norm": 0.0527164600789547,
      "learning_rate": 3.331476323119777e-05,
      "loss": 0.3269,
      "step": 1501
    },
    {
      "epoch": 2.5033333333333334,
      "grad_norm": 0.039372265338897705,
      "learning_rate": 3.32033426183844e-05,
      "loss": 0.2824,
      "step": 1502
    },
    {
      "epoch": 2.505,
      "grad_norm": 0.046973925083875656,
      "learning_rate": 3.309192200557103e-05,
      "loss": 0.3486,
      "step": 1503
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.05611708015203476,
      "learning_rate": 3.298050139275766e-05,
      "loss": 0.3534,
      "step": 1504
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 0.05162171646952629,
      "learning_rate": 3.286908077994429e-05,
      "loss": 0.2737,
      "step": 1505
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.06981784105300903,
      "learning_rate": 3.275766016713092e-05,
      "loss": 0.3034,
      "step": 1506
    },
    {
      "epoch": 2.5116666666666667,
      "grad_norm": 0.03650907799601555,
      "learning_rate": 3.264623955431755e-05,
      "loss": 0.2682,
      "step": 1507
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.05091380700469017,
      "learning_rate": 3.253481894150418e-05,
      "loss": 0.2826,
      "step": 1508
    },
    {
      "epoch": 2.515,
      "grad_norm": 0.0453135222196579,
      "learning_rate": 3.242339832869081e-05,
      "loss": 0.3046,
      "step": 1509
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.07911663502454758,
      "learning_rate": 3.231197771587744e-05,
      "loss": 0.3197,
      "step": 1510
    },
    {
      "epoch": 2.5183333333333335,
      "grad_norm": 0.04890639707446098,
      "learning_rate": 3.220055710306407e-05,
      "loss": 0.3031,
      "step": 1511
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.044261109083890915,
      "learning_rate": 3.20891364902507e-05,
      "loss": 0.3086,
      "step": 1512
    },
    {
      "epoch": 2.5216666666666665,
      "grad_norm": 0.05195830017328262,
      "learning_rate": 3.197771587743733e-05,
      "loss": 0.2901,
      "step": 1513
    },
    {
      "epoch": 2.5233333333333334,
      "grad_norm": 0.05687936395406723,
      "learning_rate": 3.186629526462396e-05,
      "loss": 0.2663,
      "step": 1514
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.04945798218250275,
      "learning_rate": 3.175487465181058e-05,
      "loss": 0.3243,
      "step": 1515
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.04635507985949516,
      "learning_rate": 3.164345403899722e-05,
      "loss": 0.3258,
      "step": 1516
    },
    {
      "epoch": 2.5283333333333333,
      "grad_norm": 0.05599001422524452,
      "learning_rate": 3.153203342618384e-05,
      "loss": 0.3432,
      "step": 1517
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.09054287523031235,
      "learning_rate": 3.142061281337048e-05,
      "loss": 0.4127,
      "step": 1518
    },
    {
      "epoch": 2.5316666666666667,
      "grad_norm": 0.04945742338895798,
      "learning_rate": 3.13091922005571e-05,
      "loss": 0.3159,
      "step": 1519
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.051994554698467255,
      "learning_rate": 3.1197771587743737e-05,
      "loss": 0.2924,
      "step": 1520
    },
    {
      "epoch": 2.535,
      "grad_norm": 0.05484630912542343,
      "learning_rate": 3.108635097493036e-05,
      "loss": 0.2924,
      "step": 1521
    },
    {
      "epoch": 2.5366666666666666,
      "grad_norm": 0.04835979640483856,
      "learning_rate": 3.0974930362116996e-05,
      "loss": 0.324,
      "step": 1522
    },
    {
      "epoch": 2.538333333333333,
      "grad_norm": 0.03817827254533768,
      "learning_rate": 3.086350974930362e-05,
      "loss": 0.3005,
      "step": 1523
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.04746788740158081,
      "learning_rate": 3.0752089136490255e-05,
      "loss": 0.2488,
      "step": 1524
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 0.06972552090883255,
      "learning_rate": 3.064066852367688e-05,
      "loss": 0.2781,
      "step": 1525
    },
    {
      "epoch": 2.5433333333333334,
      "grad_norm": 0.05389454588294029,
      "learning_rate": 3.0529247910863515e-05,
      "loss": 0.2847,
      "step": 1526
    },
    {
      "epoch": 2.545,
      "grad_norm": 0.04710546135902405,
      "learning_rate": 3.0417827298050138e-05,
      "loss": 0.3069,
      "step": 1527
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.0555749386548996,
      "learning_rate": 3.030640668523677e-05,
      "loss": 0.3672,
      "step": 1528
    },
    {
      "epoch": 2.5483333333333333,
      "grad_norm": 0.04748361557722092,
      "learning_rate": 3.0194986072423397e-05,
      "loss": 0.2689,
      "step": 1529
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.04599102586507797,
      "learning_rate": 3.008356545961003e-05,
      "loss": 0.288,
      "step": 1530
    },
    {
      "epoch": 2.5516666666666667,
      "grad_norm": 0.03816261142492294,
      "learning_rate": 2.9972144846796656e-05,
      "loss": 0.2902,
      "step": 1531
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.05267556384205818,
      "learning_rate": 2.986072423398329e-05,
      "loss": 0.3118,
      "step": 1532
    },
    {
      "epoch": 2.555,
      "grad_norm": 0.04727037996053696,
      "learning_rate": 2.9749303621169916e-05,
      "loss": 0.2521,
      "step": 1533
    },
    {
      "epoch": 2.5566666666666666,
      "grad_norm": 0.04668213799595833,
      "learning_rate": 2.963788300835655e-05,
      "loss": 0.269,
      "step": 1534
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 0.05170217156410217,
      "learning_rate": 2.9526462395543175e-05,
      "loss": 0.3061,
      "step": 1535
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.06096075475215912,
      "learning_rate": 2.941504178272981e-05,
      "loss": 0.3315,
      "step": 1536
    },
    {
      "epoch": 2.5616666666666665,
      "grad_norm": 0.05536685138940811,
      "learning_rate": 2.9303621169916435e-05,
      "loss": 0.3564,
      "step": 1537
    },
    {
      "epoch": 2.5633333333333335,
      "grad_norm": 0.04970777779817581,
      "learning_rate": 2.9192200557103068e-05,
      "loss": 0.3425,
      "step": 1538
    },
    {
      "epoch": 2.565,
      "grad_norm": 0.07383934408426285,
      "learning_rate": 2.9080779944289694e-05,
      "loss": 0.3315,
      "step": 1539
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.046772345900535583,
      "learning_rate": 2.8969359331476327e-05,
      "loss": 0.2762,
      "step": 1540
    },
    {
      "epoch": 2.5683333333333334,
      "grad_norm": 0.04952071234583855,
      "learning_rate": 2.8857938718662954e-05,
      "loss": 0.3212,
      "step": 1541
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.03813836723566055,
      "learning_rate": 2.8746518105849583e-05,
      "loss": 0.2955,
      "step": 1542
    },
    {
      "epoch": 2.5716666666666668,
      "grad_norm": 0.05139094963669777,
      "learning_rate": 2.8635097493036213e-05,
      "loss": 0.262,
      "step": 1543
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.04574793949723244,
      "learning_rate": 2.8523676880222843e-05,
      "loss": 0.3088,
      "step": 1544
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.042457953095436096,
      "learning_rate": 2.841225626740947e-05,
      "loss": 0.2036,
      "step": 1545
    },
    {
      "epoch": 2.5766666666666667,
      "grad_norm": 0.07032553851604462,
      "learning_rate": 2.8300835654596102e-05,
      "loss": 0.3422,
      "step": 1546
    },
    {
      "epoch": 2.578333333333333,
      "grad_norm": 0.04627279192209244,
      "learning_rate": 2.818941504178273e-05,
      "loss": 0.2673,
      "step": 1547
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.06757767498493195,
      "learning_rate": 2.807799442896936e-05,
      "loss": 0.3701,
      "step": 1548
    },
    {
      "epoch": 2.5816666666666666,
      "grad_norm": 0.045686639845371246,
      "learning_rate": 2.7966573816155988e-05,
      "loss": 0.3308,
      "step": 1549
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.04503815993666649,
      "learning_rate": 2.785515320334262e-05,
      "loss": 0.2719,
      "step": 1550
    },
    {
      "epoch": 2.585,
      "grad_norm": 0.05297014117240906,
      "learning_rate": 2.7743732590529247e-05,
      "loss": 0.3289,
      "step": 1551
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.057273197919130325,
      "learning_rate": 2.763231197771588e-05,
      "loss": 0.3298,
      "step": 1552
    },
    {
      "epoch": 2.5883333333333334,
      "grad_norm": 0.03734060749411583,
      "learning_rate": 2.7520891364902507e-05,
      "loss": 0.2551,
      "step": 1553
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.04043074697256088,
      "learning_rate": 2.740947075208914e-05,
      "loss": 0.2609,
      "step": 1554
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 0.05275712534785271,
      "learning_rate": 2.7298050139275766e-05,
      "loss": 0.3558,
      "step": 1555
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.04864955320954323,
      "learning_rate": 2.71866295264624e-05,
      "loss": 0.303,
      "step": 1556
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 0.0591421015560627,
      "learning_rate": 2.7075208913649025e-05,
      "loss": 0.3186,
      "step": 1557
    },
    {
      "epoch": 2.5966666666666667,
      "grad_norm": 0.0436629056930542,
      "learning_rate": 2.696378830083566e-05,
      "loss": 0.2878,
      "step": 1558
    },
    {
      "epoch": 2.5983333333333336,
      "grad_norm": 0.050125617533922195,
      "learning_rate": 2.6852367688022285e-05,
      "loss": 0.3649,
      "step": 1559
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.04839489981532097,
      "learning_rate": 2.6740947075208918e-05,
      "loss": 0.2792,
      "step": 1560
    },
    {
      "epoch": 2.6016666666666666,
      "grad_norm": 0.06482480466365814,
      "learning_rate": 2.6629526462395544e-05,
      "loss": 0.3746,
      "step": 1561
    },
    {
      "epoch": 2.6033333333333335,
      "grad_norm": 0.044419702142477036,
      "learning_rate": 2.6518105849582174e-05,
      "loss": 0.3143,
      "step": 1562
    },
    {
      "epoch": 2.605,
      "grad_norm": 0.03639182075858116,
      "learning_rate": 2.6406685236768804e-05,
      "loss": 0.2702,
      "step": 1563
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.04861054942011833,
      "learning_rate": 2.6295264623955433e-05,
      "loss": 0.2289,
      "step": 1564
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 0.042176470160484314,
      "learning_rate": 2.618384401114206e-05,
      "loss": 0.2182,
      "step": 1565
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.06882858276367188,
      "learning_rate": 2.6072423398328693e-05,
      "loss": 0.4061,
      "step": 1566
    },
    {
      "epoch": 2.611666666666667,
      "grad_norm": 0.03691219538450241,
      "learning_rate": 2.596100278551532e-05,
      "loss": 0.269,
      "step": 1567
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.05598848685622215,
      "learning_rate": 2.5849582172701952e-05,
      "loss": 0.3418,
      "step": 1568
    },
    {
      "epoch": 2.615,
      "grad_norm": 0.04582440108060837,
      "learning_rate": 2.573816155988858e-05,
      "loss": 0.2608,
      "step": 1569
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.05222584679722786,
      "learning_rate": 2.562674094707521e-05,
      "loss": 0.3056,
      "step": 1570
    },
    {
      "epoch": 2.618333333333333,
      "grad_norm": 0.042796846479177475,
      "learning_rate": 2.5515320334261838e-05,
      "loss": 0.3016,
      "step": 1571
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.050656069070100784,
      "learning_rate": 2.540389972144847e-05,
      "loss": 0.3132,
      "step": 1572
    },
    {
      "epoch": 2.6216666666666666,
      "grad_norm": 0.04767369106411934,
      "learning_rate": 2.5292479108635097e-05,
      "loss": 0.325,
      "step": 1573
    },
    {
      "epoch": 2.623333333333333,
      "grad_norm": 0.03560958430171013,
      "learning_rate": 2.518105849582173e-05,
      "loss": 0.275,
      "step": 1574
    },
    {
      "epoch": 2.625,
      "grad_norm": 0.048757050186395645,
      "learning_rate": 2.5069637883008357e-05,
      "loss": 0.3757,
      "step": 1575
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.033947162330150604,
      "learning_rate": 2.4958217270194986e-05,
      "loss": 0.2585,
      "step": 1576
    },
    {
      "epoch": 2.6283333333333334,
      "grad_norm": 0.05348749831318855,
      "learning_rate": 2.4846796657381616e-05,
      "loss": 0.3352,
      "step": 1577
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.04925534874200821,
      "learning_rate": 2.4735376044568246e-05,
      "loss": 0.2777,
      "step": 1578
    },
    {
      "epoch": 2.631666666666667,
      "grad_norm": 0.07113868743181229,
      "learning_rate": 2.4623955431754876e-05,
      "loss": 0.2691,
      "step": 1579
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.040931493043899536,
      "learning_rate": 2.4512534818941505e-05,
      "loss": 0.267,
      "step": 1580
    },
    {
      "epoch": 2.635,
      "grad_norm": 0.06311175227165222,
      "learning_rate": 2.4401114206128135e-05,
      "loss": 0.3487,
      "step": 1581
    },
    {
      "epoch": 2.6366666666666667,
      "grad_norm": 0.060064028948545456,
      "learning_rate": 2.4289693593314765e-05,
      "loss": 0.3943,
      "step": 1582
    },
    {
      "epoch": 2.638333333333333,
      "grad_norm": 0.04066283628344536,
      "learning_rate": 2.4178272980501394e-05,
      "loss": 0.2902,
      "step": 1583
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.048939891159534454,
      "learning_rate": 2.4066852367688024e-05,
      "loss": 0.2845,
      "step": 1584
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 0.04493158310651779,
      "learning_rate": 2.395543175487465e-05,
      "loss": 0.2989,
      "step": 1585
    },
    {
      "epoch": 2.6433333333333335,
      "grad_norm": 0.04820992425084114,
      "learning_rate": 2.384401114206128e-05,
      "loss": 0.2946,
      "step": 1586
    },
    {
      "epoch": 2.645,
      "grad_norm": 0.03989992290735245,
      "learning_rate": 2.373259052924791e-05,
      "loss": 0.2514,
      "step": 1587
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.06647642701864243,
      "learning_rate": 2.362116991643454e-05,
      "loss": 0.3963,
      "step": 1588
    },
    {
      "epoch": 2.6483333333333334,
      "grad_norm": 0.0716092437505722,
      "learning_rate": 2.350974930362117e-05,
      "loss": 0.4135,
      "step": 1589
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.03725824132561684,
      "learning_rate": 2.33983286908078e-05,
      "loss": 0.2491,
      "step": 1590
    },
    {
      "epoch": 2.6516666666666664,
      "grad_norm": 0.03681214153766632,
      "learning_rate": 2.328690807799443e-05,
      "loss": 0.232,
      "step": 1591
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.04955782741308212,
      "learning_rate": 2.317548746518106e-05,
      "loss": 0.289,
      "step": 1592
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 0.036106791347265244,
      "learning_rate": 2.3064066852367688e-05,
      "loss": 0.2278,
      "step": 1593
    },
    {
      "epoch": 2.6566666666666667,
      "grad_norm": 0.04414523392915726,
      "learning_rate": 2.2952646239554318e-05,
      "loss": 0.263,
      "step": 1594
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 0.05343789979815483,
      "learning_rate": 2.2841225626740948e-05,
      "loss": 0.3271,
      "step": 1595
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.0472785085439682,
      "learning_rate": 2.2729805013927577e-05,
      "loss": 0.3201,
      "step": 1596
    },
    {
      "epoch": 2.6616666666666666,
      "grad_norm": 0.04776298999786377,
      "learning_rate": 2.2618384401114207e-05,
      "loss": 0.3016,
      "step": 1597
    },
    {
      "epoch": 2.663333333333333,
      "grad_norm": 0.04697604477405548,
      "learning_rate": 2.2506963788300837e-05,
      "loss": 0.3212,
      "step": 1598
    },
    {
      "epoch": 2.665,
      "grad_norm": 0.05854318290948868,
      "learning_rate": 2.2395543175487466e-05,
      "loss": 0.3209,
      "step": 1599
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.044749755412340164,
      "learning_rate": 2.2284122562674096e-05,
      "loss": 0.3192,
      "step": 1600
    },
    {
      "epoch": 2.6666666666666665,
      "eval_loss": 0.31889936327934265,
      "eval_runtime": 370.5637,
      "eval_samples_per_second": 0.27,
      "eval_steps_per_second": 0.27,
      "step": 1600
    },
    {
      "epoch": 2.6683333333333334,
      "grad_norm": 0.07841917872428894,
      "learning_rate": 2.2172701949860726e-05,
      "loss": 0.346,
      "step": 1601
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.045286938548088074,
      "learning_rate": 2.2061281337047355e-05,
      "loss": 0.2833,
      "step": 1602
    },
    {
      "epoch": 2.671666666666667,
      "grad_norm": 0.03972505033016205,
      "learning_rate": 2.1949860724233985e-05,
      "loss": 0.277,
      "step": 1603
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.07864516973495483,
      "learning_rate": 2.1838440111420615e-05,
      "loss": 0.3791,
      "step": 1604
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.056493502110242844,
      "learning_rate": 2.172701949860724e-05,
      "loss": 0.3629,
      "step": 1605
    },
    {
      "epoch": 2.6766666666666667,
      "grad_norm": 0.054622165858745575,
      "learning_rate": 2.161559888579387e-05,
      "loss": 0.325,
      "step": 1606
    },
    {
      "epoch": 2.6783333333333332,
      "grad_norm": 0.0480344220995903,
      "learning_rate": 2.15041782729805e-05,
      "loss": 0.3454,
      "step": 1607
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.04920703545212746,
      "learning_rate": 2.139275766016713e-05,
      "loss": 0.3334,
      "step": 1608
    },
    {
      "epoch": 2.6816666666666666,
      "grad_norm": 0.047040484845638275,
      "learning_rate": 2.128133704735376e-05,
      "loss": 0.2947,
      "step": 1609
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.055101893842220306,
      "learning_rate": 2.116991643454039e-05,
      "loss": 0.3608,
      "step": 1610
    },
    {
      "epoch": 2.685,
      "grad_norm": 0.0471007414162159,
      "learning_rate": 2.105849582172702e-05,
      "loss": 0.3366,
      "step": 1611
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.031810060143470764,
      "learning_rate": 2.094707520891365e-05,
      "loss": 0.2467,
      "step": 1612
    },
    {
      "epoch": 2.6883333333333335,
      "grad_norm": 0.050645630806684494,
      "learning_rate": 2.083565459610028e-05,
      "loss": 0.2915,
      "step": 1613
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.06462868303060532,
      "learning_rate": 2.072423398328691e-05,
      "loss": 0.2844,
      "step": 1614
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 0.06866754591464996,
      "learning_rate": 2.0612813370473538e-05,
      "loss": 0.3745,
      "step": 1615
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.06839333474636078,
      "learning_rate": 2.0501392757660168e-05,
      "loss": 0.3813,
      "step": 1616
    },
    {
      "epoch": 2.695,
      "grad_norm": 0.043393343687057495,
      "learning_rate": 2.0389972144846798e-05,
      "loss": 0.2714,
      "step": 1617
    },
    {
      "epoch": 2.6966666666666668,
      "grad_norm": 0.08940969407558441,
      "learning_rate": 2.0278551532033427e-05,
      "loss": 0.313,
      "step": 1618
    },
    {
      "epoch": 2.6983333333333333,
      "grad_norm": 0.05585027113556862,
      "learning_rate": 2.0167130919220057e-05,
      "loss": 0.254,
      "step": 1619
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.06343259662389755,
      "learning_rate": 2.0055710306406687e-05,
      "loss": 0.3033,
      "step": 1620
    },
    {
      "epoch": 2.7016666666666667,
      "grad_norm": 0.04000462219119072,
      "learning_rate": 1.9944289693593316e-05,
      "loss": 0.2531,
      "step": 1621
    },
    {
      "epoch": 2.703333333333333,
      "grad_norm": 0.053627852350473404,
      "learning_rate": 1.9832869080779946e-05,
      "loss": 0.3213,
      "step": 1622
    },
    {
      "epoch": 2.705,
      "grad_norm": 0.06977394223213196,
      "learning_rate": 1.9721448467966576e-05,
      "loss": 0.3957,
      "step": 1623
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.06906843185424805,
      "learning_rate": 1.9610027855153206e-05,
      "loss": 0.3425,
      "step": 1624
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 0.04796109348535538,
      "learning_rate": 1.9498607242339832e-05,
      "loss": 0.3214,
      "step": 1625
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.03675312176346779,
      "learning_rate": 1.938718662952646e-05,
      "loss": 0.2547,
      "step": 1626
    },
    {
      "epoch": 2.711666666666667,
      "grad_norm": 0.05025719851255417,
      "learning_rate": 1.927576601671309e-05,
      "loss": 0.3734,
      "step": 1627
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.04611407592892647,
      "learning_rate": 1.916434540389972e-05,
      "loss": 0.322,
      "step": 1628
    },
    {
      "epoch": 2.715,
      "grad_norm": 0.05419238284230232,
      "learning_rate": 1.905292479108635e-05,
      "loss": 0.3169,
      "step": 1629
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.04782490432262421,
      "learning_rate": 1.894150417827298e-05,
      "loss": 0.3266,
      "step": 1630
    },
    {
      "epoch": 2.7183333333333333,
      "grad_norm": 0.08977886289358139,
      "learning_rate": 1.883008356545961e-05,
      "loss": 0.3719,
      "step": 1631
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.08838954567909241,
      "learning_rate": 1.871866295264624e-05,
      "loss": 0.3728,
      "step": 1632
    },
    {
      "epoch": 2.7216666666666667,
      "grad_norm": 0.05162319168448448,
      "learning_rate": 1.860724233983287e-05,
      "loss": 0.2433,
      "step": 1633
    },
    {
      "epoch": 2.7233333333333336,
      "grad_norm": 0.05307112634181976,
      "learning_rate": 1.84958217270195e-05,
      "loss": 0.3211,
      "step": 1634
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.04667978733778,
      "learning_rate": 1.838440111420613e-05,
      "loss": 0.3332,
      "step": 1635
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.06595965474843979,
      "learning_rate": 1.827298050139276e-05,
      "loss": 0.4082,
      "step": 1636
    },
    {
      "epoch": 2.7283333333333335,
      "grad_norm": 0.0696399062871933,
      "learning_rate": 1.816155988857939e-05,
      "loss": 0.3158,
      "step": 1637
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.04408758878707886,
      "learning_rate": 1.8050139275766018e-05,
      "loss": 0.2709,
      "step": 1638
    },
    {
      "epoch": 2.7316666666666665,
      "grad_norm": 0.04047773778438568,
      "learning_rate": 1.7938718662952648e-05,
      "loss": 0.2941,
      "step": 1639
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.05584464594721794,
      "learning_rate": 1.7827298050139278e-05,
      "loss": 0.3448,
      "step": 1640
    },
    {
      "epoch": 2.735,
      "grad_norm": 0.047634925693273544,
      "learning_rate": 1.7715877437325907e-05,
      "loss": 0.3211,
      "step": 1641
    },
    {
      "epoch": 2.736666666666667,
      "grad_norm": 0.04067513346672058,
      "learning_rate": 1.7604456824512537e-05,
      "loss": 0.273,
      "step": 1642
    },
    {
      "epoch": 2.7383333333333333,
      "grad_norm": 0.06514227390289307,
      "learning_rate": 1.7493036211699167e-05,
      "loss": 0.286,
      "step": 1643
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.07289253175258636,
      "learning_rate": 1.7381615598885796e-05,
      "loss": 0.4047,
      "step": 1644
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 0.02992759272456169,
      "learning_rate": 1.7270194986072426e-05,
      "loss": 0.1774,
      "step": 1645
    },
    {
      "epoch": 2.743333333333333,
      "grad_norm": 0.04403075948357582,
      "learning_rate": 1.7158774373259052e-05,
      "loss": 0.305,
      "step": 1646
    },
    {
      "epoch": 2.745,
      "grad_norm": 0.08219115436077118,
      "learning_rate": 1.7047353760445682e-05,
      "loss": 0.2846,
      "step": 1647
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.036056485027074814,
      "learning_rate": 1.6935933147632312e-05,
      "loss": 0.2175,
      "step": 1648
    },
    {
      "epoch": 2.748333333333333,
      "grad_norm": 0.10812407732009888,
      "learning_rate": 1.682451253481894e-05,
      "loss": 0.3945,
      "step": 1649
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.05593469738960266,
      "learning_rate": 1.671309192200557e-05,
      "loss": 0.3377,
      "step": 1650
    },
    {
      "epoch": 2.751666666666667,
      "grad_norm": 0.039930008351802826,
      "learning_rate": 1.66016713091922e-05,
      "loss": 0.2252,
      "step": 1651
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.04164058715105057,
      "learning_rate": 1.649025069637883e-05,
      "loss": 0.2319,
      "step": 1652
    },
    {
      "epoch": 2.755,
      "grad_norm": 0.06749729067087173,
      "learning_rate": 1.637883008356546e-05,
      "loss": 0.2824,
      "step": 1653
    },
    {
      "epoch": 2.756666666666667,
      "grad_norm": 0.03603874146938324,
      "learning_rate": 1.626740947075209e-05,
      "loss": 0.237,
      "step": 1654
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 0.059520404785871506,
      "learning_rate": 1.615598885793872e-05,
      "loss": 0.3865,
      "step": 1655
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.04307981953024864,
      "learning_rate": 1.604456824512535e-05,
      "loss": 0.2556,
      "step": 1656
    },
    {
      "epoch": 2.7616666666666667,
      "grad_norm": 0.05187543109059334,
      "learning_rate": 1.593314763231198e-05,
      "loss": 0.3198,
      "step": 1657
    },
    {
      "epoch": 2.763333333333333,
      "grad_norm": 0.06437991559505463,
      "learning_rate": 1.582172701949861e-05,
      "loss": 0.3969,
      "step": 1658
    },
    {
      "epoch": 2.765,
      "grad_norm": 0.05796573683619499,
      "learning_rate": 1.571030640668524e-05,
      "loss": 0.2943,
      "step": 1659
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.04347410053014755,
      "learning_rate": 1.5598885793871868e-05,
      "loss": 0.2557,
      "step": 1660
    },
    {
      "epoch": 2.7683333333333335,
      "grad_norm": 0.05299137532711029,
      "learning_rate": 1.5487465181058498e-05,
      "loss": 0.3265,
      "step": 1661
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.07339876145124435,
      "learning_rate": 1.5376044568245128e-05,
      "loss": 0.3454,
      "step": 1662
    },
    {
      "epoch": 2.7716666666666665,
      "grad_norm": 0.04491593316197395,
      "learning_rate": 1.5264623955431757e-05,
      "loss": 0.2964,
      "step": 1663
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.0635295882821083,
      "learning_rate": 1.5153203342618385e-05,
      "loss": 0.3671,
      "step": 1664
    },
    {
      "epoch": 2.775,
      "grad_norm": 0.0446905717253685,
      "learning_rate": 1.5041782729805015e-05,
      "loss": 0.2907,
      "step": 1665
    },
    {
      "epoch": 2.7766666666666664,
      "grad_norm": 0.041968002915382385,
      "learning_rate": 1.4930362116991645e-05,
      "loss": 0.2746,
      "step": 1666
    },
    {
      "epoch": 2.7783333333333333,
      "grad_norm": 0.04943760111927986,
      "learning_rate": 1.4818941504178274e-05,
      "loss": 0.285,
      "step": 1667
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.06617343425750732,
      "learning_rate": 1.4707520891364904e-05,
      "loss": 0.3375,
      "step": 1668
    },
    {
      "epoch": 2.7816666666666667,
      "grad_norm": 0.05406142398715019,
      "learning_rate": 1.4596100278551534e-05,
      "loss": 0.31,
      "step": 1669
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.04421786591410637,
      "learning_rate": 1.4484679665738164e-05,
      "loss": 0.2776,
      "step": 1670
    },
    {
      "epoch": 2.785,
      "grad_norm": 0.04426104575395584,
      "learning_rate": 1.4373259052924792e-05,
      "loss": 0.2843,
      "step": 1671
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.05943593755364418,
      "learning_rate": 1.4261838440111421e-05,
      "loss": 0.3503,
      "step": 1672
    },
    {
      "epoch": 2.788333333333333,
      "grad_norm": 0.03623780608177185,
      "learning_rate": 1.4150417827298051e-05,
      "loss": 0.267,
      "step": 1673
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.04617195948958397,
      "learning_rate": 1.403899721448468e-05,
      "loss": 0.2817,
      "step": 1674
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 0.05532814562320709,
      "learning_rate": 1.392757660167131e-05,
      "loss": 0.2872,
      "step": 1675
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.04347327724099159,
      "learning_rate": 1.381615598885794e-05,
      "loss": 0.3211,
      "step": 1676
    },
    {
      "epoch": 2.795,
      "grad_norm": 0.06747595965862274,
      "learning_rate": 1.370473537604457e-05,
      "loss": 0.3169,
      "step": 1677
    },
    {
      "epoch": 2.796666666666667,
      "grad_norm": 0.08192235976457596,
      "learning_rate": 1.35933147632312e-05,
      "loss": 0.3595,
      "step": 1678
    },
    {
      "epoch": 2.7983333333333333,
      "grad_norm": 0.06024672091007233,
      "learning_rate": 1.348189415041783e-05,
      "loss": 0.3524,
      "step": 1679
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.050733283162117004,
      "learning_rate": 1.3370473537604459e-05,
      "loss": 0.3102,
      "step": 1680
    },
    {
      "epoch": 2.8016666666666667,
      "grad_norm": 0.04289516061544418,
      "learning_rate": 1.3259052924791087e-05,
      "loss": 0.2941,
      "step": 1681
    },
    {
      "epoch": 2.8033333333333332,
      "grad_norm": 0.05185234174132347,
      "learning_rate": 1.3147632311977717e-05,
      "loss": 0.2958,
      "step": 1682
    },
    {
      "epoch": 2.805,
      "grad_norm": 0.05637957155704498,
      "learning_rate": 1.3036211699164346e-05,
      "loss": 0.2862,
      "step": 1683
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.053833577781915665,
      "learning_rate": 1.2924791086350976e-05,
      "loss": 0.3262,
      "step": 1684
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 0.05234147980809212,
      "learning_rate": 1.2813370473537606e-05,
      "loss": 0.3272,
      "step": 1685
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.05480767413973808,
      "learning_rate": 1.2701949860724236e-05,
      "loss": 0.3277,
      "step": 1686
    },
    {
      "epoch": 2.8116666666666665,
      "grad_norm": 0.05240901932120323,
      "learning_rate": 1.2590529247910865e-05,
      "loss": 0.2928,
      "step": 1687
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.08169615268707275,
      "learning_rate": 1.2479108635097493e-05,
      "loss": 0.3804,
      "step": 1688
    },
    {
      "epoch": 2.815,
      "grad_norm": 0.05983424186706543,
      "learning_rate": 1.2367688022284123e-05,
      "loss": 0.3499,
      "step": 1689
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.041794680058956146,
      "learning_rate": 1.2256267409470753e-05,
      "loss": 0.2195,
      "step": 1690
    },
    {
      "epoch": 2.8183333333333334,
      "grad_norm": 0.04916771128773689,
      "learning_rate": 1.2144846796657382e-05,
      "loss": 0.3122,
      "step": 1691
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.03623218834400177,
      "learning_rate": 1.2033426183844012e-05,
      "loss": 0.2872,
      "step": 1692
    },
    {
      "epoch": 2.8216666666666668,
      "grad_norm": 0.04293137043714523,
      "learning_rate": 1.192200557103064e-05,
      "loss": 0.2752,
      "step": 1693
    },
    {
      "epoch": 2.8233333333333333,
      "grad_norm": 0.045497339218854904,
      "learning_rate": 1.181058495821727e-05,
      "loss": 0.282,
      "step": 1694
    },
    {
      "epoch": 2.825,
      "grad_norm": 0.05350889638066292,
      "learning_rate": 1.16991643454039e-05,
      "loss": 0.2806,
      "step": 1695
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.06188567727804184,
      "learning_rate": 1.158774373259053e-05,
      "loss": 0.329,
      "step": 1696
    },
    {
      "epoch": 2.828333333333333,
      "grad_norm": 0.04016122967004776,
      "learning_rate": 1.1476323119777159e-05,
      "loss": 0.2916,
      "step": 1697
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.037363745272159576,
      "learning_rate": 1.1364902506963789e-05,
      "loss": 0.2539,
      "step": 1698
    },
    {
      "epoch": 2.8316666666666666,
      "grad_norm": 0.061237823218107224,
      "learning_rate": 1.1253481894150418e-05,
      "loss": 0.3177,
      "step": 1699
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.051546361297369,
      "learning_rate": 1.1142061281337048e-05,
      "loss": 0.3871,
      "step": 1700
    },
    {
      "epoch": 2.8333333333333335,
      "eval_loss": 0.31863999366760254,
      "eval_runtime": 370.2006,
      "eval_samples_per_second": 0.27,
      "eval_steps_per_second": 0.27,
      "step": 1700
    },
    {
      "epoch": 2.835,
      "grad_norm": 0.05552847683429718,
      "learning_rate": 1.1030640668523678e-05,
      "loss": 0.379,
      "step": 1701
    },
    {
      "epoch": 2.836666666666667,
      "grad_norm": 0.05288122221827507,
      "learning_rate": 1.0919220055710307e-05,
      "loss": 0.2842,
      "step": 1702
    },
    {
      "epoch": 2.8383333333333334,
      "grad_norm": 0.04971173033118248,
      "learning_rate": 1.0807799442896935e-05,
      "loss": 0.3291,
      "step": 1703
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.038779210299253464,
      "learning_rate": 1.0696378830083565e-05,
      "loss": 0.2867,
      "step": 1704
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 0.04455331712961197,
      "learning_rate": 1.0584958217270195e-05,
      "loss": 0.2918,
      "step": 1705
    },
    {
      "epoch": 2.8433333333333333,
      "grad_norm": 0.052727244794368744,
      "learning_rate": 1.0473537604456825e-05,
      "loss": 0.3195,
      "step": 1706
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 0.050426680594682693,
      "learning_rate": 1.0362116991643454e-05,
      "loss": 0.3316,
      "step": 1707
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.08861155807971954,
      "learning_rate": 1.0250696378830084e-05,
      "loss": 0.3084,
      "step": 1708
    },
    {
      "epoch": 2.8483333333333336,
      "grad_norm": 0.04656172916293144,
      "learning_rate": 1.0139275766016714e-05,
      "loss": 0.2813,
      "step": 1709
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.04143676534295082,
      "learning_rate": 1.0027855153203343e-05,
      "loss": 0.3102,
      "step": 1710
    },
    {
      "epoch": 2.8516666666666666,
      "grad_norm": 0.06423648446798325,
      "learning_rate": 9.916434540389973e-06,
      "loss": 0.3566,
      "step": 1711
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.03766774386167526,
      "learning_rate": 9.805013927576603e-06,
      "loss": 0.2888,
      "step": 1712
    },
    {
      "epoch": 2.855,
      "grad_norm": 0.06415172666311264,
      "learning_rate": 9.69359331476323e-06,
      "loss": 0.3498,
      "step": 1713
    },
    {
      "epoch": 2.8566666666666665,
      "grad_norm": 0.07163826376199722,
      "learning_rate": 9.58217270194986e-06,
      "loss": 0.3803,
      "step": 1714
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.042581137269735336,
      "learning_rate": 9.47075208913649e-06,
      "loss": 0.3186,
      "step": 1715
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.06324424594640732,
      "learning_rate": 9.35933147632312e-06,
      "loss": 0.3823,
      "step": 1716
    },
    {
      "epoch": 2.861666666666667,
      "grad_norm": 0.0482647642493248,
      "learning_rate": 9.24791086350975e-06,
      "loss": 0.3151,
      "step": 1717
    },
    {
      "epoch": 2.8633333333333333,
      "grad_norm": 0.06015491113066673,
      "learning_rate": 9.13649025069638e-06,
      "loss": 0.3739,
      "step": 1718
    },
    {
      "epoch": 2.865,
      "grad_norm": 0.04849667474627495,
      "learning_rate": 9.025069637883009e-06,
      "loss": 0.3057,
      "step": 1719
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.06470870226621628,
      "learning_rate": 8.913649025069639e-06,
      "loss": 0.3942,
      "step": 1720
    },
    {
      "epoch": 2.868333333333333,
      "grad_norm": 0.04100726917386055,
      "learning_rate": 8.802228412256268e-06,
      "loss": 0.2986,
      "step": 1721
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.046374138444662094,
      "learning_rate": 8.690807799442898e-06,
      "loss": 0.3433,
      "step": 1722
    },
    {
      "epoch": 2.8716666666666666,
      "grad_norm": 0.0383656769990921,
      "learning_rate": 8.579387186629526e-06,
      "loss": 0.2584,
      "step": 1723
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.03602343425154686,
      "learning_rate": 8.467966573816156e-06,
      "loss": 0.2792,
      "step": 1724
    },
    {
      "epoch": 2.875,
      "grad_norm": 0.07500309497117996,
      "learning_rate": 8.356545961002786e-06,
      "loss": 0.376,
      "step": 1725
    },
    {
      "epoch": 2.876666666666667,
      "grad_norm": 0.046249501407146454,
      "learning_rate": 8.245125348189415e-06,
      "loss": 0.3009,
      "step": 1726
    },
    {
      "epoch": 2.8783333333333334,
      "grad_norm": 0.09334202110767365,
      "learning_rate": 8.133704735376045e-06,
      "loss": 0.379,
      "step": 1727
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.05367996171116829,
      "learning_rate": 8.022284122562675e-06,
      "loss": 0.287,
      "step": 1728
    },
    {
      "epoch": 2.881666666666667,
      "grad_norm": 0.04631255567073822,
      "learning_rate": 7.910863509749304e-06,
      "loss": 0.3322,
      "step": 1729
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.04200827702879906,
      "learning_rate": 7.799442896935934e-06,
      "loss": 0.2738,
      "step": 1730
    },
    {
      "epoch": 2.885,
      "grad_norm": 0.04162713140249252,
      "learning_rate": 7.688022284122564e-06,
      "loss": 0.3,
      "step": 1731
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.07843207567930222,
      "learning_rate": 7.576601671309193e-06,
      "loss": 0.3305,
      "step": 1732
    },
    {
      "epoch": 2.888333333333333,
      "grad_norm": 0.0439295619726181,
      "learning_rate": 7.465181058495822e-06,
      "loss": 0.2785,
      "step": 1733
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.04053365811705589,
      "learning_rate": 7.353760445682452e-06,
      "loss": 0.2483,
      "step": 1734
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 0.055006977170705795,
      "learning_rate": 7.242339832869082e-06,
      "loss": 0.3627,
      "step": 1735
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.059068191796541214,
      "learning_rate": 7.130919220055711e-06,
      "loss": 0.3351,
      "step": 1736
    },
    {
      "epoch": 2.895,
      "grad_norm": 0.03726230189204216,
      "learning_rate": 7.01949860724234e-06,
      "loss": 0.213,
      "step": 1737
    },
    {
      "epoch": 2.8966666666666665,
      "grad_norm": 0.046640556305646896,
      "learning_rate": 6.90807799442897e-06,
      "loss": 0.3071,
      "step": 1738
    },
    {
      "epoch": 2.8983333333333334,
      "grad_norm": 0.04800424352288246,
      "learning_rate": 6.7966573816156e-06,
      "loss": 0.277,
      "step": 1739
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.05597829073667526,
      "learning_rate": 6.6852367688022295e-06,
      "loss": 0.3451,
      "step": 1740
    },
    {
      "epoch": 2.9016666666666664,
      "grad_norm": 0.04292907938361168,
      "learning_rate": 6.573816155988858e-06,
      "loss": 0.311,
      "step": 1741
    },
    {
      "epoch": 2.9033333333333333,
      "grad_norm": 0.045306287705898285,
      "learning_rate": 6.462395543175488e-06,
      "loss": 0.3353,
      "step": 1742
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 0.0653376504778862,
      "learning_rate": 6.350974930362118e-06,
      "loss": 0.3478,
      "step": 1743
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.04711569845676422,
      "learning_rate": 6.239554317548747e-06,
      "loss": 0.3276,
      "step": 1744
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 0.04666072502732277,
      "learning_rate": 6.128133704735376e-06,
      "loss": 0.287,
      "step": 1745
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.05936276540160179,
      "learning_rate": 6.016713091922006e-06,
      "loss": 0.3442,
      "step": 1746
    },
    {
      "epoch": 2.9116666666666666,
      "grad_norm": 0.05769012123346329,
      "learning_rate": 5.905292479108635e-06,
      "loss": 0.3355,
      "step": 1747
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.06094273924827576,
      "learning_rate": 5.793871866295265e-06,
      "loss": 0.3235,
      "step": 1748
    },
    {
      "epoch": 2.915,
      "grad_norm": 0.0744546428322792,
      "learning_rate": 5.682451253481894e-06,
      "loss": 0.3336,
      "step": 1749
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.04877003654837608,
      "learning_rate": 5.571030640668524e-06,
      "loss": 0.3624,
      "step": 1750
    },
    {
      "epoch": 2.9183333333333334,
      "grad_norm": 0.0609775111079216,
      "learning_rate": 5.459610027855154e-06,
      "loss": 0.3404,
      "step": 1751
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.04475175589323044,
      "learning_rate": 5.348189415041783e-06,
      "loss": 0.3013,
      "step": 1752
    },
    {
      "epoch": 2.921666666666667,
      "grad_norm": 0.06886456906795502,
      "learning_rate": 5.236768802228412e-06,
      "loss": 0.3302,
      "step": 1753
    },
    {
      "epoch": 2.9233333333333333,
      "grad_norm": 0.04014822095632553,
      "learning_rate": 5.125348189415042e-06,
      "loss": 0.2793,
      "step": 1754
    },
    {
      "epoch": 2.925,
      "grad_norm": 0.04033178836107254,
      "learning_rate": 5.013927576601672e-06,
      "loss": 0.2289,
      "step": 1755
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.04932079091668129,
      "learning_rate": 4.902506963788301e-06,
      "loss": 0.3363,
      "step": 1756
    },
    {
      "epoch": 2.9283333333333332,
      "grad_norm": 0.06216413527727127,
      "learning_rate": 4.79108635097493e-06,
      "loss": 0.3084,
      "step": 1757
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.05808085575699806,
      "learning_rate": 4.67966573816156e-06,
      "loss": 0.3039,
      "step": 1758
    },
    {
      "epoch": 2.9316666666666666,
      "grad_norm": 0.044783685356378555,
      "learning_rate": 4.56824512534819e-06,
      "loss": 0.2571,
      "step": 1759
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.048510897904634476,
      "learning_rate": 4.456824512534819e-06,
      "loss": 0.329,
      "step": 1760
    },
    {
      "epoch": 2.935,
      "grad_norm": 0.04001861438155174,
      "learning_rate": 4.345403899721449e-06,
      "loss": 0.2867,
      "step": 1761
    },
    {
      "epoch": 2.9366666666666665,
      "grad_norm": 0.03906891122460365,
      "learning_rate": 4.233983286908078e-06,
      "loss": 0.3092,
      "step": 1762
    },
    {
      "epoch": 2.9383333333333335,
      "grad_norm": 0.030925868079066277,
      "learning_rate": 4.122562674094708e-06,
      "loss": 0.2348,
      "step": 1763
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.059919003397226334,
      "learning_rate": 4.011142061281337e-06,
      "loss": 0.3086,
      "step": 1764
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 0.04098975285887718,
      "learning_rate": 3.899721448467967e-06,
      "loss": 0.2881,
      "step": 1765
    },
    {
      "epoch": 2.9433333333333334,
      "grad_norm": 0.041266635060310364,
      "learning_rate": 3.7883008356545963e-06,
      "loss": 0.2987,
      "step": 1766
    },
    {
      "epoch": 2.945,
      "grad_norm": 0.04171426221728325,
      "learning_rate": 3.676880222841226e-06,
      "loss": 0.3108,
      "step": 1767
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.06242143735289574,
      "learning_rate": 3.5654596100278553e-06,
      "loss": 0.304,
      "step": 1768
    },
    {
      "epoch": 2.9483333333333333,
      "grad_norm": 0.04269048944115639,
      "learning_rate": 3.454038997214485e-06,
      "loss": 0.2399,
      "step": 1769
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.042269978672266006,
      "learning_rate": 3.3426183844011147e-06,
      "loss": 0.3287,
      "step": 1770
    },
    {
      "epoch": 2.9516666666666667,
      "grad_norm": 0.050749070942401886,
      "learning_rate": 3.231197771587744e-06,
      "loss": 0.3205,
      "step": 1771
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.050893668085336685,
      "learning_rate": 3.1197771587743733e-06,
      "loss": 0.3112,
      "step": 1772
    },
    {
      "epoch": 2.955,
      "grad_norm": 0.05196673423051834,
      "learning_rate": 3.008356545961003e-06,
      "loss": 0.3412,
      "step": 1773
    },
    {
      "epoch": 2.9566666666666666,
      "grad_norm": 0.05263397470116615,
      "learning_rate": 2.8969359331476323e-06,
      "loss": 0.3171,
      "step": 1774
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 0.0498538613319397,
      "learning_rate": 2.785515320334262e-06,
      "loss": 0.3006,
      "step": 1775
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.08356273174285889,
      "learning_rate": 2.6740947075208913e-06,
      "loss": 0.3086,
      "step": 1776
    },
    {
      "epoch": 2.961666666666667,
      "grad_norm": 0.07666011154651642,
      "learning_rate": 2.562674094707521e-06,
      "loss": 0.3409,
      "step": 1777
    },
    {
      "epoch": 2.9633333333333334,
      "grad_norm": 0.03982526436448097,
      "learning_rate": 2.4512534818941507e-06,
      "loss": 0.2749,
      "step": 1778
    },
    {
      "epoch": 2.965,
      "grad_norm": 0.04858333617448807,
      "learning_rate": 2.33983286908078e-06,
      "loss": 0.3272,
      "step": 1779
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.05663445219397545,
      "learning_rate": 2.2284122562674097e-06,
      "loss": 0.3361,
      "step": 1780
    },
    {
      "epoch": 2.9683333333333333,
      "grad_norm": 0.04293252155184746,
      "learning_rate": 2.116991643454039e-06,
      "loss": 0.3176,
      "step": 1781
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.03154852241277695,
      "learning_rate": 2.0055710306406687e-06,
      "loss": 0.2402,
      "step": 1782
    },
    {
      "epoch": 2.9716666666666667,
      "grad_norm": 0.0571243092417717,
      "learning_rate": 1.8941504178272982e-06,
      "loss": 0.2682,
      "step": 1783
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.06695286184549332,
      "learning_rate": 1.7827298050139277e-06,
      "loss": 0.2713,
      "step": 1784
    },
    {
      "epoch": 2.975,
      "grad_norm": 0.040786366909742355,
      "learning_rate": 1.6713091922005574e-06,
      "loss": 0.2635,
      "step": 1785
    },
    {
      "epoch": 2.9766666666666666,
      "grad_norm": 0.07258167862892151,
      "learning_rate": 1.5598885793871867e-06,
      "loss": 0.3416,
      "step": 1786
    },
    {
      "epoch": 2.9783333333333335,
      "grad_norm": 0.04503753408789635,
      "learning_rate": 1.4484679665738162e-06,
      "loss": 0.2507,
      "step": 1787
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.050209518522024155,
      "learning_rate": 1.3370473537604456e-06,
      "loss": 0.3522,
      "step": 1788
    },
    {
      "epoch": 2.9816666666666665,
      "grad_norm": 0.05936175957322121,
      "learning_rate": 1.2256267409470754e-06,
      "loss": 0.3592,
      "step": 1789
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.08109113574028015,
      "learning_rate": 1.1142061281337048e-06,
      "loss": 0.3216,
      "step": 1790
    },
    {
      "epoch": 2.985,
      "grad_norm": 0.05429208278656006,
      "learning_rate": 1.0027855153203343e-06,
      "loss": 0.3521,
      "step": 1791
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.04328843951225281,
      "learning_rate": 8.913649025069638e-07,
      "loss": 0.2903,
      "step": 1792
    },
    {
      "epoch": 2.9883333333333333,
      "grad_norm": 0.059405386447906494,
      "learning_rate": 7.799442896935933e-07,
      "loss": 0.2977,
      "step": 1793
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.04577487334609032,
      "learning_rate": 6.685236768802228e-07,
      "loss": 0.2947,
      "step": 1794
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 0.04067959263920784,
      "learning_rate": 5.571030640668524e-07,
      "loss": 0.2998,
      "step": 1795
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.06568128615617752,
      "learning_rate": 4.456824512534819e-07,
      "loss": 0.3817,
      "step": 1796
    },
    {
      "epoch": 2.995,
      "grad_norm": 0.08979300409555435,
      "learning_rate": 3.342618384401114e-07,
      "loss": 0.3152,
      "step": 1797
    },
    {
      "epoch": 2.9966666666666666,
      "grad_norm": 0.0351114496588707,
      "learning_rate": 2.2284122562674096e-07,
      "loss": 0.2575,
      "step": 1798
    },
    {
      "epoch": 2.998333333333333,
      "grad_norm": 0.04635537788271904,
      "learning_rate": 1.1142061281337048e-07,
      "loss": 0.3354,
      "step": 1799
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.048635732382535934,
      "learning_rate": 0.0,
      "loss": 0.2716,
      "step": 1800
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.3183744251728058,
      "eval_runtime": 370.3332,
      "eval_samples_per_second": 0.27,
      "eval_steps_per_second": 0.27,
      "step": 1800
    },
    {
      "epoch": 3.0016666666666665,
      "grad_norm": 0.046326663345098495,
      "learning_rate": 0.0001250886339937435,
      "loss": 0.3685,
      "step": 1801
    },
    {
      "epoch": 3.0033333333333334,
      "grad_norm": 0.03764420747756958,
      "learning_rate": 0.00012504692387904068,
      "loss": 0.2759,
      "step": 1802
    },
    {
      "epoch": 3.005,
      "grad_norm": 0.047275323420763016,
      "learning_rate": 0.00012500521376433786,
      "loss": 0.2961,
      "step": 1803
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.05649746209383011,
      "learning_rate": 0.00012496350364963506,
      "loss": 0.2857,
      "step": 1804
    },
    {
      "epoch": 3.0083333333333333,
      "grad_norm": 0.0638270378112793,
      "learning_rate": 0.00012492179353493224,
      "loss": 0.3216,
      "step": 1805
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.03739906847476959,
      "learning_rate": 0.00012488008342022942,
      "loss": 0.2065,
      "step": 1806
    },
    {
      "epoch": 3.0116666666666667,
      "grad_norm": 0.05797255411744118,
      "learning_rate": 0.0001248383733055266,
      "loss": 0.3134,
      "step": 1807
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.039846502244472504,
      "learning_rate": 0.00012479666319082378,
      "loss": 0.2526,
      "step": 1808
    },
    {
      "epoch": 3.015,
      "grad_norm": 0.061927199363708496,
      "learning_rate": 0.00012475495307612099,
      "loss": 0.304,
      "step": 1809
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 0.06275897473096848,
      "learning_rate": 0.00012471324296141817,
      "loss": 0.3175,
      "step": 1810
    },
    {
      "epoch": 3.0183333333333335,
      "grad_norm": 0.05911891534924507,
      "learning_rate": 0.00012467153284671535,
      "loss": 0.3253,
      "step": 1811
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.06630098819732666,
      "learning_rate": 0.00012462982273201252,
      "loss": 0.355,
      "step": 1812
    },
    {
      "epoch": 3.0216666666666665,
      "grad_norm": 0.05662908777594566,
      "learning_rate": 0.0001245881126173097,
      "loss": 0.3339,
      "step": 1813
    },
    {
      "epoch": 3.0233333333333334,
      "grad_norm": 0.07207849621772766,
      "learning_rate": 0.00012454640250260688,
      "loss": 0.256,
      "step": 1814
    },
    {
      "epoch": 3.025,
      "grad_norm": 0.043100107461214066,
      "learning_rate": 0.00012450469238790406,
      "loss": 0.2385,
      "step": 1815
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.06451291590929031,
      "learning_rate": 0.00012446298227320127,
      "loss": 0.2916,
      "step": 1816
    },
    {
      "epoch": 3.0283333333333333,
      "grad_norm": 0.07299347221851349,
      "learning_rate": 0.00012442127215849845,
      "loss": 0.3246,
      "step": 1817
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.06616150587797165,
      "learning_rate": 0.00012437956204379563,
      "loss": 0.3269,
      "step": 1818
    },
    {
      "epoch": 3.0316666666666667,
      "grad_norm": 0.08066970854997635,
      "learning_rate": 0.0001243378519290928,
      "loss": 0.3729,
      "step": 1819
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.06470867246389389,
      "learning_rate": 0.00012429614181439,
      "loss": 0.3178,
      "step": 1820
    },
    {
      "epoch": 3.035,
      "grad_norm": 0.057125262916088104,
      "learning_rate": 0.00012425443169968717,
      "loss": 0.329,
      "step": 1821
    },
    {
      "epoch": 3.0366666666666666,
      "grad_norm": 0.062257979065179825,
      "learning_rate": 0.00012421272158498435,
      "loss": 0.31,
      "step": 1822
    },
    {
      "epoch": 3.038333333333333,
      "grad_norm": 0.08560708910226822,
      "learning_rate": 0.00012417101147028153,
      "loss": 0.3372,
      "step": 1823
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.05603882297873497,
      "learning_rate": 0.00012412930135557873,
      "loss": 0.2849,
      "step": 1824
    },
    {
      "epoch": 3.0416666666666665,
      "grad_norm": 0.04685615748167038,
      "learning_rate": 0.0001240875912408759,
      "loss": 0.3004,
      "step": 1825
    },
    {
      "epoch": 3.0433333333333334,
      "grad_norm": 0.04796074703335762,
      "learning_rate": 0.0001240458811261731,
      "loss": 0.2923,
      "step": 1826
    },
    {
      "epoch": 3.045,
      "grad_norm": 0.05002216994762421,
      "learning_rate": 0.00012400417101147027,
      "loss": 0.2999,
      "step": 1827
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.05798459053039551,
      "learning_rate": 0.00012396246089676745,
      "loss": 0.3273,
      "step": 1828
    },
    {
      "epoch": 3.0483333333333333,
      "grad_norm": 0.05144047364592552,
      "learning_rate": 0.00012392075078206466,
      "loss": 0.3328,
      "step": 1829
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.04354139417409897,
      "learning_rate": 0.00012387904066736184,
      "loss": 0.255,
      "step": 1830
    },
    {
      "epoch": 3.0516666666666667,
      "grad_norm": 0.042884353548288345,
      "learning_rate": 0.00012383733055265902,
      "loss": 0.2508,
      "step": 1831
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.05167080834507942,
      "learning_rate": 0.0001237956204379562,
      "loss": 0.3152,
      "step": 1832
    },
    {
      "epoch": 3.055,
      "grad_norm": 0.0561695322394371,
      "learning_rate": 0.00012375391032325338,
      "loss": 0.3181,
      "step": 1833
    },
    {
      "epoch": 3.0566666666666666,
      "grad_norm": 0.05247094854712486,
      "learning_rate": 0.00012371220020855058,
      "loss": 0.3086,
      "step": 1834
    },
    {
      "epoch": 3.058333333333333,
      "grad_norm": 0.07067316770553589,
      "learning_rate": 0.00012367049009384776,
      "loss": 0.3651,
      "step": 1835
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.051379263401031494,
      "learning_rate": 0.00012362877997914494,
      "loss": 0.2798,
      "step": 1836
    },
    {
      "epoch": 3.0616666666666665,
      "grad_norm": 0.05453139543533325,
      "learning_rate": 0.00012358706986444212,
      "loss": 0.3325,
      "step": 1837
    },
    {
      "epoch": 3.0633333333333335,
      "grad_norm": 0.048803094774484634,
      "learning_rate": 0.0001235453597497393,
      "loss": 0.2828,
      "step": 1838
    },
    {
      "epoch": 3.065,
      "grad_norm": 0.05888117849826813,
      "learning_rate": 0.0001235036496350365,
      "loss": 0.3065,
      "step": 1839
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.04631095752120018,
      "learning_rate": 0.00012346193952033369,
      "loss": 0.2753,
      "step": 1840
    },
    {
      "epoch": 3.0683333333333334,
      "grad_norm": 0.07099584490060806,
      "learning_rate": 0.00012342022940563087,
      "loss": 0.3351,
      "step": 1841
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.08731292933225632,
      "learning_rate": 0.00012337851929092804,
      "loss": 0.388,
      "step": 1842
    },
    {
      "epoch": 3.0716666666666668,
      "grad_norm": 0.0725703313946724,
      "learning_rate": 0.00012333680917622522,
      "loss": 0.3122,
      "step": 1843
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.0610867477953434,
      "learning_rate": 0.00012329509906152243,
      "loss": 0.2596,
      "step": 1844
    },
    {
      "epoch": 3.075,
      "grad_norm": 0.06208733469247818,
      "learning_rate": 0.0001232533889468196,
      "loss": 0.3026,
      "step": 1845
    },
    {
      "epoch": 3.0766666666666667,
      "grad_norm": 0.06527504324913025,
      "learning_rate": 0.0001232116788321168,
      "loss": 0.2725,
      "step": 1846
    },
    {
      "epoch": 3.078333333333333,
      "grad_norm": 0.05644352361559868,
      "learning_rate": 0.00012316996871741397,
      "loss": 0.2797,
      "step": 1847
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.06533056497573853,
      "learning_rate": 0.00012312825860271115,
      "loss": 0.2951,
      "step": 1848
    },
    {
      "epoch": 3.0816666666666666,
      "grad_norm": 0.06593488156795502,
      "learning_rate": 0.00012308654848800836,
      "loss": 0.3149,
      "step": 1849
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 0.0492575541138649,
      "learning_rate": 0.00012304483837330553,
      "loss": 0.3058,
      "step": 1850
    },
    {
      "epoch": 3.085,
      "grad_norm": 0.0707104504108429,
      "learning_rate": 0.00012300312825860271,
      "loss": 0.3175,
      "step": 1851
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.11230939626693726,
      "learning_rate": 0.0001229614181438999,
      "loss": 0.3374,
      "step": 1852
    },
    {
      "epoch": 3.0883333333333334,
      "grad_norm": 0.047825418412685394,
      "learning_rate": 0.00012291970802919707,
      "loss": 0.2615,
      "step": 1853
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.0852755680680275,
      "learning_rate": 0.00012287799791449428,
      "loss": 0.3864,
      "step": 1854
    },
    {
      "epoch": 3.091666666666667,
      "grad_norm": 0.06490132212638855,
      "learning_rate": 0.00012283628779979146,
      "loss": 0.3048,
      "step": 1855
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.0448172390460968,
      "learning_rate": 0.00012279457768508864,
      "loss": 0.2399,
      "step": 1856
    },
    {
      "epoch": 3.095,
      "grad_norm": 0.043997153639793396,
      "learning_rate": 0.00012275286757038582,
      "loss": 0.2407,
      "step": 1857
    },
    {
      "epoch": 3.0966666666666667,
      "grad_norm": 0.10555030405521393,
      "learning_rate": 0.000122711157455683,
      "loss": 0.3688,
      "step": 1858
    },
    {
      "epoch": 3.098333333333333,
      "grad_norm": 0.06949686259031296,
      "learning_rate": 0.0001226694473409802,
      "loss": 0.3604,
      "step": 1859
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.08387400954961777,
      "learning_rate": 0.00012262773722627738,
      "loss": 0.324,
      "step": 1860
    },
    {
      "epoch": 3.1016666666666666,
      "grad_norm": 0.04616676643490791,
      "learning_rate": 0.00012258602711157456,
      "loss": 0.2667,
      "step": 1861
    },
    {
      "epoch": 3.1033333333333335,
      "grad_norm": 0.06358980387449265,
      "learning_rate": 0.00012254431699687174,
      "loss": 0.3498,
      "step": 1862
    },
    {
      "epoch": 3.105,
      "grad_norm": 0.06640362739562988,
      "learning_rate": 0.00012250260688216892,
      "loss": 0.2961,
      "step": 1863
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.05762182176113129,
      "learning_rate": 0.00012246089676746613,
      "loss": 0.2935,
      "step": 1864
    },
    {
      "epoch": 3.1083333333333334,
      "grad_norm": 0.059784162789583206,
      "learning_rate": 0.0001224191866527633,
      "loss": 0.3123,
      "step": 1865
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.06281501054763794,
      "learning_rate": 0.0001223774765380605,
      "loss": 0.3286,
      "step": 1866
    },
    {
      "epoch": 3.111666666666667,
      "grad_norm": 0.06352463364601135,
      "learning_rate": 0.00012233576642335767,
      "loss": 0.3252,
      "step": 1867
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.05363123118877411,
      "learning_rate": 0.00012229405630865485,
      "loss": 0.2917,
      "step": 1868
    },
    {
      "epoch": 3.115,
      "grad_norm": 0.05394616350531578,
      "learning_rate": 0.00012225234619395205,
      "loss": 0.2949,
      "step": 1869
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 0.04078693687915802,
      "learning_rate": 0.00012221063607924923,
      "loss": 0.2605,
      "step": 1870
    },
    {
      "epoch": 3.118333333333333,
      "grad_norm": 0.06301889568567276,
      "learning_rate": 0.0001221689259645464,
      "loss": 0.3655,
      "step": 1871
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.06253154575824738,
      "learning_rate": 0.0001221272158498436,
      "loss": 0.2936,
      "step": 1872
    },
    {
      "epoch": 3.1216666666666666,
      "grad_norm": 0.07955319434404373,
      "learning_rate": 0.00012208550573514077,
      "loss": 0.3297,
      "step": 1873
    },
    {
      "epoch": 3.1233333333333335,
      "grad_norm": 0.0708082988858223,
      "learning_rate": 0.00012204379562043798,
      "loss": 0.3328,
      "step": 1874
    },
    {
      "epoch": 3.125,
      "grad_norm": 0.054620347917079926,
      "learning_rate": 0.00012200208550573516,
      "loss": 0.2937,
      "step": 1875
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.048366788774728775,
      "learning_rate": 0.00012196037539103234,
      "loss": 0.2981,
      "step": 1876
    },
    {
      "epoch": 3.1283333333333334,
      "grad_norm": 0.059872716665267944,
      "learning_rate": 0.00012191866527632952,
      "loss": 0.2919,
      "step": 1877
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.1347782015800476,
      "learning_rate": 0.0001218769551616267,
      "loss": 0.3677,
      "step": 1878
    },
    {
      "epoch": 3.131666666666667,
      "grad_norm": 0.0535513199865818,
      "learning_rate": 0.0001218352450469239,
      "loss": 0.2793,
      "step": 1879
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.09644850343465805,
      "learning_rate": 0.00012179353493222108,
      "loss": 0.3853,
      "step": 1880
    },
    {
      "epoch": 3.135,
      "grad_norm": 0.07360448688268661,
      "learning_rate": 0.00012175182481751826,
      "loss": 0.335,
      "step": 1881
    },
    {
      "epoch": 3.1366666666666667,
      "grad_norm": 0.07144428044557571,
      "learning_rate": 0.00012171011470281544,
      "loss": 0.3204,
      "step": 1882
    },
    {
      "epoch": 3.138333333333333,
      "grad_norm": 0.054596468806266785,
      "learning_rate": 0.00012166840458811262,
      "loss": 0.2639,
      "step": 1883
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.05246453359723091,
      "learning_rate": 0.0001216266944734098,
      "loss": 0.2743,
      "step": 1884
    },
    {
      "epoch": 3.1416666666666666,
      "grad_norm": 0.0676392912864685,
      "learning_rate": 0.00012158498435870699,
      "loss": 0.3057,
      "step": 1885
    },
    {
      "epoch": 3.1433333333333335,
      "grad_norm": 0.08430808037519455,
      "learning_rate": 0.00012154327424400419,
      "loss": 0.4464,
      "step": 1886
    },
    {
      "epoch": 3.145,
      "grad_norm": 0.0637834221124649,
      "learning_rate": 0.00012150156412930137,
      "loss": 0.3362,
      "step": 1887
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.062017135322093964,
      "learning_rate": 0.00012145985401459854,
      "loss": 0.3075,
      "step": 1888
    },
    {
      "epoch": 3.1483333333333334,
      "grad_norm": 0.07864776253700256,
      "learning_rate": 0.00012141814389989572,
      "loss": 0.3499,
      "step": 1889
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.06989012658596039,
      "learning_rate": 0.00012137643378519292,
      "loss": 0.327,
      "step": 1890
    },
    {
      "epoch": 3.151666666666667,
      "grad_norm": 0.055578798055648804,
      "learning_rate": 0.0001213347236704901,
      "loss": 0.3148,
      "step": 1891
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.0688541829586029,
      "learning_rate": 0.00012129301355578728,
      "loss": 0.3726,
      "step": 1892
    },
    {
      "epoch": 3.155,
      "grad_norm": 0.06890922039747238,
      "learning_rate": 0.00012125130344108447,
      "loss": 0.3235,
      "step": 1893
    },
    {
      "epoch": 3.1566666666666667,
      "grad_norm": 0.06628227978944778,
      "learning_rate": 0.00012120959332638165,
      "loss": 0.3452,
      "step": 1894
    },
    {
      "epoch": 3.158333333333333,
      "grad_norm": 0.06667455285787582,
      "learning_rate": 0.00012116788321167884,
      "loss": 0.3318,
      "step": 1895
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.05289305001497269,
      "learning_rate": 0.00012112617309697602,
      "loss": 0.3107,
      "step": 1896
    },
    {
      "epoch": 3.1616666666666666,
      "grad_norm": 0.06335646659135818,
      "learning_rate": 0.0001210844629822732,
      "loss": 0.3498,
      "step": 1897
    },
    {
      "epoch": 3.163333333333333,
      "grad_norm": 0.058921605348587036,
      "learning_rate": 0.00012104275286757038,
      "loss": 0.3292,
      "step": 1898
    },
    {
      "epoch": 3.165,
      "grad_norm": 0.06560715287923813,
      "learning_rate": 0.00012100104275286756,
      "loss": 0.2757,
      "step": 1899
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.044742830097675323,
      "learning_rate": 0.00012095933263816477,
      "loss": 0.2754,
      "step": 1900
    },
    {
      "epoch": 3.1666666666666665,
      "eval_loss": 0.3213774859905243,
      "eval_runtime": 371.5134,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 1900
    },
    {
      "epoch": 3.1683333333333334,
      "grad_norm": 0.04192592576146126,
      "learning_rate": 0.00012091762252346195,
      "loss": 0.2626,
      "step": 1901
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.04168391600251198,
      "learning_rate": 0.00012087591240875913,
      "loss": 0.231,
      "step": 1902
    },
    {
      "epoch": 3.171666666666667,
      "grad_norm": 0.052857495844364166,
      "learning_rate": 0.0001208342022940563,
      "loss": 0.2893,
      "step": 1903
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.07415106147527695,
      "learning_rate": 0.00012079249217935348,
      "loss": 0.3717,
      "step": 1904
    },
    {
      "epoch": 3.175,
      "grad_norm": 0.04125596582889557,
      "learning_rate": 0.00012075078206465069,
      "loss": 0.2899,
      "step": 1905
    },
    {
      "epoch": 3.1766666666666667,
      "grad_norm": 0.06307455152273178,
      "learning_rate": 0.00012070907194994787,
      "loss": 0.3168,
      "step": 1906
    },
    {
      "epoch": 3.1783333333333332,
      "grad_norm": 0.04323277622461319,
      "learning_rate": 0.00012066736183524505,
      "loss": 0.251,
      "step": 1907
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.06875524669885635,
      "learning_rate": 0.00012062565172054223,
      "loss": 0.4054,
      "step": 1908
    },
    {
      "epoch": 3.1816666666666666,
      "grad_norm": 0.05740062892436981,
      "learning_rate": 0.00012058394160583941,
      "loss": 0.3161,
      "step": 1909
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 0.04621370509266853,
      "learning_rate": 0.00012054223149113662,
      "loss": 0.2902,
      "step": 1910
    },
    {
      "epoch": 3.185,
      "grad_norm": 0.038909912109375,
      "learning_rate": 0.0001205005213764338,
      "loss": 0.27,
      "step": 1911
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.05193958058953285,
      "learning_rate": 0.00012045881126173097,
      "loss": 0.2944,
      "step": 1912
    },
    {
      "epoch": 3.1883333333333335,
      "grad_norm": 0.053887929767370224,
      "learning_rate": 0.00012041710114702815,
      "loss": 0.3134,
      "step": 1913
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.05713546648621559,
      "learning_rate": 0.00012037539103232533,
      "loss": 0.3345,
      "step": 1914
    },
    {
      "epoch": 3.191666666666667,
      "grad_norm": 0.05089537054300308,
      "learning_rate": 0.00012033368091762254,
      "loss": 0.3333,
      "step": 1915
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.06436296552419662,
      "learning_rate": 0.00012029197080291972,
      "loss": 0.3493,
      "step": 1916
    },
    {
      "epoch": 3.195,
      "grad_norm": 0.06725156307220459,
      "learning_rate": 0.0001202502606882169,
      "loss": 0.3231,
      "step": 1917
    },
    {
      "epoch": 3.1966666666666668,
      "grad_norm": 0.06191232055425644,
      "learning_rate": 0.00012020855057351408,
      "loss": 0.3511,
      "step": 1918
    },
    {
      "epoch": 3.1983333333333333,
      "grad_norm": 0.05826999619603157,
      "learning_rate": 0.00012016684045881126,
      "loss": 0.3445,
      "step": 1919
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.042226046323776245,
      "learning_rate": 0.00012012513034410846,
      "loss": 0.2551,
      "step": 1920
    },
    {
      "epoch": 3.2016666666666667,
      "grad_norm": 0.04536837339401245,
      "learning_rate": 0.00012008342022940564,
      "loss": 0.2822,
      "step": 1921
    },
    {
      "epoch": 3.203333333333333,
      "grad_norm": 0.07702834159135818,
      "learning_rate": 0.00012004171011470282,
      "loss": 0.3734,
      "step": 1922
    },
    {
      "epoch": 3.205,
      "grad_norm": 0.08156614005565643,
      "learning_rate": 0.00012,
      "loss": 0.3605,
      "step": 1923
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.056073520332574844,
      "learning_rate": 0.00011995828988529718,
      "loss": 0.328,
      "step": 1924
    },
    {
      "epoch": 3.2083333333333335,
      "grad_norm": 0.05470710247755051,
      "learning_rate": 0.00011991657977059439,
      "loss": 0.2558,
      "step": 1925
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.07139065861701965,
      "learning_rate": 0.00011987486965589157,
      "loss": 0.3597,
      "step": 1926
    },
    {
      "epoch": 3.211666666666667,
      "grad_norm": 0.04271545261144638,
      "learning_rate": 0.00011983315954118875,
      "loss": 0.1969,
      "step": 1927
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.049109939485788345,
      "learning_rate": 0.00011979144942648593,
      "loss": 0.3203,
      "step": 1928
    },
    {
      "epoch": 3.215,
      "grad_norm": 0.03693458065390587,
      "learning_rate": 0.00011974973931178311,
      "loss": 0.2454,
      "step": 1929
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 0.0737045407295227,
      "learning_rate": 0.00011970802919708031,
      "loss": 0.4063,
      "step": 1930
    },
    {
      "epoch": 3.2183333333333333,
      "grad_norm": 0.050372011959552765,
      "learning_rate": 0.00011966631908237749,
      "loss": 0.3539,
      "step": 1931
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.040902357548475266,
      "learning_rate": 0.00011962460896767467,
      "loss": 0.2659,
      "step": 1932
    },
    {
      "epoch": 3.2216666666666667,
      "grad_norm": 0.06300743669271469,
      "learning_rate": 0.00011958289885297185,
      "loss": 0.3289,
      "step": 1933
    },
    {
      "epoch": 3.223333333333333,
      "grad_norm": 0.10003677755594254,
      "learning_rate": 0.00011954118873826903,
      "loss": 0.3033,
      "step": 1934
    },
    {
      "epoch": 3.225,
      "grad_norm": 0.05399703234434128,
      "learning_rate": 0.00011949947862356622,
      "loss": 0.364,
      "step": 1935
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.04632427543401718,
      "learning_rate": 0.0001194577685088634,
      "loss": 0.286,
      "step": 1936
    },
    {
      "epoch": 3.2283333333333335,
      "grad_norm": 0.057816118001937866,
      "learning_rate": 0.0001194160583941606,
      "loss": 0.3293,
      "step": 1937
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.04896409064531326,
      "learning_rate": 0.00011937434827945778,
      "loss": 0.3296,
      "step": 1938
    },
    {
      "epoch": 3.2316666666666665,
      "grad_norm": 0.055786412209272385,
      "learning_rate": 0.00011933263816475496,
      "loss": 0.319,
      "step": 1939
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.10717709362506866,
      "learning_rate": 0.00011929092805005215,
      "loss": 0.3506,
      "step": 1940
    },
    {
      "epoch": 3.235,
      "grad_norm": 0.06783679127693176,
      "learning_rate": 0.00011924921793534933,
      "loss": 0.3216,
      "step": 1941
    },
    {
      "epoch": 3.236666666666667,
      "grad_norm": 0.06375591456890106,
      "learning_rate": 0.00011920750782064651,
      "loss": 0.3553,
      "step": 1942
    },
    {
      "epoch": 3.2383333333333333,
      "grad_norm": 0.06313131749629974,
      "learning_rate": 0.00011916579770594369,
      "loss": 0.3367,
      "step": 1943
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.057238250970840454,
      "learning_rate": 0.00011912408759124087,
      "loss": 0.3156,
      "step": 1944
    },
    {
      "epoch": 3.2416666666666667,
      "grad_norm": 0.047254178673028946,
      "learning_rate": 0.00011908237747653807,
      "loss": 0.2875,
      "step": 1945
    },
    {
      "epoch": 3.243333333333333,
      "grad_norm": 0.09114142507314682,
      "learning_rate": 0.00011904066736183525,
      "loss": 0.3816,
      "step": 1946
    },
    {
      "epoch": 3.245,
      "grad_norm": 0.06048707291483879,
      "learning_rate": 0.00011899895724713243,
      "loss": 0.3361,
      "step": 1947
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.04650304839015007,
      "learning_rate": 0.00011895724713242961,
      "loss": 0.3207,
      "step": 1948
    },
    {
      "epoch": 3.2483333333333335,
      "grad_norm": 0.07079169154167175,
      "learning_rate": 0.00011891553701772679,
      "loss": 0.3909,
      "step": 1949
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.057282593101263046,
      "learning_rate": 0.000118873826903024,
      "loss": 0.3776,
      "step": 1950
    },
    {
      "epoch": 3.2516666666666665,
      "grad_norm": 0.04555864259600639,
      "learning_rate": 0.00011883211678832118,
      "loss": 0.327,
      "step": 1951
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.04497317969799042,
      "learning_rate": 0.00011879040667361836,
      "loss": 0.2804,
      "step": 1952
    },
    {
      "epoch": 3.255,
      "grad_norm": 0.052181463688611984,
      "learning_rate": 0.00011874869655891554,
      "loss": 0.2582,
      "step": 1953
    },
    {
      "epoch": 3.256666666666667,
      "grad_norm": 0.06603250652551651,
      "learning_rate": 0.00011870698644421272,
      "loss": 0.3451,
      "step": 1954
    },
    {
      "epoch": 3.2583333333333333,
      "grad_norm": 0.040492672473192215,
      "learning_rate": 0.00011866527632950992,
      "loss": 0.2465,
      "step": 1955
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.0497199222445488,
      "learning_rate": 0.0001186235662148071,
      "loss": 0.3352,
      "step": 1956
    },
    {
      "epoch": 3.2616666666666667,
      "grad_norm": 0.07001741975545883,
      "learning_rate": 0.00011858185610010428,
      "loss": 0.3514,
      "step": 1957
    },
    {
      "epoch": 3.263333333333333,
      "grad_norm": 0.057514555752277374,
      "learning_rate": 0.00011854014598540146,
      "loss": 0.3151,
      "step": 1958
    },
    {
      "epoch": 3.265,
      "grad_norm": 0.04000271484255791,
      "learning_rate": 0.00011849843587069864,
      "loss": 0.2352,
      "step": 1959
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.039074670523405075,
      "learning_rate": 0.00011845672575599585,
      "loss": 0.283,
      "step": 1960
    },
    {
      "epoch": 3.2683333333333335,
      "grad_norm": 0.0497979000210762,
      "learning_rate": 0.00011841501564129303,
      "loss": 0.2659,
      "step": 1961
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.055901698768138885,
      "learning_rate": 0.0001183733055265902,
      "loss": 0.3436,
      "step": 1962
    },
    {
      "epoch": 3.2716666666666665,
      "grad_norm": 0.049807317554950714,
      "learning_rate": 0.00011833159541188739,
      "loss": 0.2876,
      "step": 1963
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.05034976080060005,
      "learning_rate": 0.00011828988529718457,
      "loss": 0.2554,
      "step": 1964
    },
    {
      "epoch": 3.275,
      "grad_norm": 0.10107656568288803,
      "learning_rate": 0.00011824817518248177,
      "loss": 0.3568,
      "step": 1965
    },
    {
      "epoch": 3.276666666666667,
      "grad_norm": 0.05392756313085556,
      "learning_rate": 0.00011820646506777895,
      "loss": 0.2852,
      "step": 1966
    },
    {
      "epoch": 3.2783333333333333,
      "grad_norm": 0.08986011892557144,
      "learning_rate": 0.00011816475495307613,
      "loss": 0.3199,
      "step": 1967
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.04935908317565918,
      "learning_rate": 0.00011812304483837331,
      "loss": 0.3229,
      "step": 1968
    },
    {
      "epoch": 3.2816666666666667,
      "grad_norm": 0.07565964758396149,
      "learning_rate": 0.00011808133472367049,
      "loss": 0.3239,
      "step": 1969
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 0.07541945576667786,
      "learning_rate": 0.00011803962460896767,
      "loss": 0.3519,
      "step": 1970
    },
    {
      "epoch": 3.285,
      "grad_norm": 0.048292819410562515,
      "learning_rate": 0.00011799791449426488,
      "loss": 0.2799,
      "step": 1971
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.03467361629009247,
      "learning_rate": 0.00011795620437956206,
      "loss": 0.2701,
      "step": 1972
    },
    {
      "epoch": 3.288333333333333,
      "grad_norm": 0.055763375014066696,
      "learning_rate": 0.00011791449426485923,
      "loss": 0.3163,
      "step": 1973
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.038237087428569794,
      "learning_rate": 0.00011787278415015641,
      "loss": 0.2139,
      "step": 1974
    },
    {
      "epoch": 3.2916666666666665,
      "grad_norm": 0.05839398875832558,
      "learning_rate": 0.0001178310740354536,
      "loss": 0.3242,
      "step": 1975
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.04706251993775368,
      "learning_rate": 0.0001177893639207508,
      "loss": 0.3028,
      "step": 1976
    },
    {
      "epoch": 3.295,
      "grad_norm": 0.05025129020214081,
      "learning_rate": 0.00011774765380604798,
      "loss": 0.3161,
      "step": 1977
    },
    {
      "epoch": 3.296666666666667,
      "grad_norm": 0.04755397513508797,
      "learning_rate": 0.00011770594369134516,
      "loss": 0.2846,
      "step": 1978
    },
    {
      "epoch": 3.2983333333333333,
      "grad_norm": 0.06236930936574936,
      "learning_rate": 0.00011766423357664234,
      "loss": 0.3015,
      "step": 1979
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.046838220208883286,
      "learning_rate": 0.00011762252346193952,
      "loss": 0.2478,
      "step": 1980
    },
    {
      "epoch": 3.3016666666666667,
      "grad_norm": 0.05525437369942665,
      "learning_rate": 0.00011758081334723671,
      "loss": 0.3283,
      "step": 1981
    },
    {
      "epoch": 3.3033333333333332,
      "grad_norm": 0.06557133048772812,
      "learning_rate": 0.0001175391032325339,
      "loss": 0.3805,
      "step": 1982
    },
    {
      "epoch": 3.305,
      "grad_norm": 0.06148628890514374,
      "learning_rate": 0.00011749739311783108,
      "loss": 0.3543,
      "step": 1983
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.0559004470705986,
      "learning_rate": 0.00011745568300312826,
      "loss": 0.2545,
      "step": 1984
    },
    {
      "epoch": 3.3083333333333336,
      "grad_norm": 0.06611070781946182,
      "learning_rate": 0.00011741397288842544,
      "loss": 0.298,
      "step": 1985
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.06317707896232605,
      "learning_rate": 0.00011737226277372264,
      "loss": 0.3391,
      "step": 1986
    },
    {
      "epoch": 3.3116666666666665,
      "grad_norm": 0.048326943069696426,
      "learning_rate": 0.00011733055265901982,
      "loss": 0.2457,
      "step": 1987
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.05394357442855835,
      "learning_rate": 0.000117288842544317,
      "loss": 0.283,
      "step": 1988
    },
    {
      "epoch": 3.315,
      "grad_norm": 0.045775942504405975,
      "learning_rate": 0.00011724713242961419,
      "loss": 0.2667,
      "step": 1989
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 0.0449463427066803,
      "learning_rate": 0.00011720542231491137,
      "loss": 0.2192,
      "step": 1990
    },
    {
      "epoch": 3.3183333333333334,
      "grad_norm": 0.10026159137487411,
      "learning_rate": 0.00011716371220020856,
      "loss": 0.3895,
      "step": 1991
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.04766754433512688,
      "learning_rate": 0.00011712200208550574,
      "loss": 0.3047,
      "step": 1992
    },
    {
      "epoch": 3.3216666666666668,
      "grad_norm": 0.07889150083065033,
      "learning_rate": 0.00011708029197080292,
      "loss": 0.3583,
      "step": 1993
    },
    {
      "epoch": 3.3233333333333333,
      "grad_norm": 0.06903599947690964,
      "learning_rate": 0.0001170385818561001,
      "loss": 0.3037,
      "step": 1994
    },
    {
      "epoch": 3.325,
      "grad_norm": 0.061357706785202026,
      "learning_rate": 0.00011699687174139728,
      "loss": 0.2833,
      "step": 1995
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.06264040619134903,
      "learning_rate": 0.00011695516162669448,
      "loss": 0.333,
      "step": 1996
    },
    {
      "epoch": 3.328333333333333,
      "grad_norm": 0.053258266299963,
      "learning_rate": 0.00011691345151199166,
      "loss": 0.3478,
      "step": 1997
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.04473281651735306,
      "learning_rate": 0.00011687174139728884,
      "loss": 0.242,
      "step": 1998
    },
    {
      "epoch": 3.3316666666666666,
      "grad_norm": 0.07954924553632736,
      "learning_rate": 0.00011683003128258602,
      "loss": 0.3486,
      "step": 1999
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.046134043484926224,
      "learning_rate": 0.0001167883211678832,
      "loss": 0.3029,
      "step": 2000
    },
    {
      "epoch": 3.3333333333333335,
      "eval_loss": 0.3214721977710724,
      "eval_runtime": 371.6314,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 2000
    },
    {
      "epoch": 3.335,
      "grad_norm": 0.05293208360671997,
      "learning_rate": 0.00011674661105318041,
      "loss": 0.3008,
      "step": 2001
    },
    {
      "epoch": 3.336666666666667,
      "grad_norm": 0.049578964710235596,
      "learning_rate": 0.00011670490093847759,
      "loss": 0.2929,
      "step": 2002
    },
    {
      "epoch": 3.3383333333333334,
      "grad_norm": 0.05944085121154785,
      "learning_rate": 0.00011666319082377477,
      "loss": 0.3199,
      "step": 2003
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.047401659190654755,
      "learning_rate": 0.00011662148070907195,
      "loss": 0.2549,
      "step": 2004
    },
    {
      "epoch": 3.341666666666667,
      "grad_norm": 0.036717791110277176,
      "learning_rate": 0.00011657977059436913,
      "loss": 0.2128,
      "step": 2005
    },
    {
      "epoch": 3.3433333333333333,
      "grad_norm": 0.05925079062581062,
      "learning_rate": 0.00011653806047966633,
      "loss": 0.3614,
      "step": 2006
    },
    {
      "epoch": 3.3449999999999998,
      "grad_norm": 0.06325401365756989,
      "learning_rate": 0.00011649635036496351,
      "loss": 0.3304,
      "step": 2007
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.046179790049791336,
      "learning_rate": 0.00011645464025026069,
      "loss": 0.2732,
      "step": 2008
    },
    {
      "epoch": 3.348333333333333,
      "grad_norm": 0.05598052218556404,
      "learning_rate": 0.00011641293013555787,
      "loss": 0.3283,
      "step": 2009
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.041067250072956085,
      "learning_rate": 0.00011637122002085505,
      "loss": 0.2694,
      "step": 2010
    },
    {
      "epoch": 3.3516666666666666,
      "grad_norm": 0.051530856639146805,
      "learning_rate": 0.00011632950990615226,
      "loss": 0.3154,
      "step": 2011
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.08731094002723694,
      "learning_rate": 0.00011628779979144944,
      "loss": 0.4478,
      "step": 2012
    },
    {
      "epoch": 3.355,
      "grad_norm": 0.04189258813858032,
      "learning_rate": 0.00011624608967674662,
      "loss": 0.3155,
      "step": 2013
    },
    {
      "epoch": 3.3566666666666665,
      "grad_norm": 0.05196350812911987,
      "learning_rate": 0.0001162043795620438,
      "loss": 0.3069,
      "step": 2014
    },
    {
      "epoch": 3.3583333333333334,
      "grad_norm": 0.04721745848655701,
      "learning_rate": 0.00011616266944734098,
      "loss": 0.3173,
      "step": 2015
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.046596113592386246,
      "learning_rate": 0.00011612095933263818,
      "loss": 0.2867,
      "step": 2016
    },
    {
      "epoch": 3.361666666666667,
      "grad_norm": 0.04810766875743866,
      "learning_rate": 0.00011607924921793536,
      "loss": 0.2622,
      "step": 2017
    },
    {
      "epoch": 3.3633333333333333,
      "grad_norm": 0.04736747592687607,
      "learning_rate": 0.00011603753910323254,
      "loss": 0.3093,
      "step": 2018
    },
    {
      "epoch": 3.365,
      "grad_norm": 0.04764315485954285,
      "learning_rate": 0.00011599582898852972,
      "loss": 0.2969,
      "step": 2019
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.03989984095096588,
      "learning_rate": 0.0001159541188738269,
      "loss": 0.2276,
      "step": 2020
    },
    {
      "epoch": 3.368333333333333,
      "grad_norm": 0.03762546181678772,
      "learning_rate": 0.00011591240875912411,
      "loss": 0.2074,
      "step": 2021
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.07169823348522186,
      "learning_rate": 0.00011587069864442129,
      "loss": 0.3044,
      "step": 2022
    },
    {
      "epoch": 3.3716666666666666,
      "grad_norm": 0.07196886837482452,
      "learning_rate": 0.00011582898852971847,
      "loss": 0.3949,
      "step": 2023
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.046881649643182755,
      "learning_rate": 0.00011578727841501565,
      "loss": 0.2674,
      "step": 2024
    },
    {
      "epoch": 3.375,
      "grad_norm": 0.06907466799020767,
      "learning_rate": 0.00011574556830031283,
      "loss": 0.364,
      "step": 2025
    },
    {
      "epoch": 3.3766666666666665,
      "grad_norm": 0.04039497673511505,
      "learning_rate": 0.00011570385818561003,
      "loss": 0.2777,
      "step": 2026
    },
    {
      "epoch": 3.3783333333333334,
      "grad_norm": 0.05432536453008652,
      "learning_rate": 0.00011566214807090721,
      "loss": 0.3118,
      "step": 2027
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.057623088359832764,
      "learning_rate": 0.00011562043795620439,
      "loss": 0.3386,
      "step": 2028
    },
    {
      "epoch": 3.381666666666667,
      "grad_norm": 0.049519993364810944,
      "learning_rate": 0.00011557872784150157,
      "loss": 0.295,
      "step": 2029
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 0.058301545679569244,
      "learning_rate": 0.00011553701772679875,
      "loss": 0.2853,
      "step": 2030
    },
    {
      "epoch": 3.385,
      "grad_norm": 0.046777866780757904,
      "learning_rate": 0.00011549530761209594,
      "loss": 0.3,
      "step": 2031
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.042795777320861816,
      "learning_rate": 0.00011545359749739312,
      "loss": 0.2492,
      "step": 2032
    },
    {
      "epoch": 3.388333333333333,
      "grad_norm": 0.06683644652366638,
      "learning_rate": 0.00011541188738269032,
      "loss": 0.3135,
      "step": 2033
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.09519969671964645,
      "learning_rate": 0.0001153701772679875,
      "loss": 0.3387,
      "step": 2034
    },
    {
      "epoch": 3.3916666666666666,
      "grad_norm": 0.04775822535157204,
      "learning_rate": 0.00011532846715328467,
      "loss": 0.2629,
      "step": 2035
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.05231563746929169,
      "learning_rate": 0.00011528675703858187,
      "loss": 0.2822,
      "step": 2036
    },
    {
      "epoch": 3.395,
      "grad_norm": 0.042703915387392044,
      "learning_rate": 0.00011524504692387905,
      "loss": 0.284,
      "step": 2037
    },
    {
      "epoch": 3.3966666666666665,
      "grad_norm": 0.07001881301403046,
      "learning_rate": 0.00011520333680917623,
      "loss": 0.3171,
      "step": 2038
    },
    {
      "epoch": 3.3983333333333334,
      "grad_norm": 0.05570441856980324,
      "learning_rate": 0.0001151616266944734,
      "loss": 0.3302,
      "step": 2039
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.04895409196615219,
      "learning_rate": 0.0001151199165797706,
      "loss": 0.3172,
      "step": 2040
    },
    {
      "epoch": 3.401666666666667,
      "grad_norm": 0.05820000544190407,
      "learning_rate": 0.00011507820646506779,
      "loss": 0.328,
      "step": 2041
    },
    {
      "epoch": 3.4033333333333333,
      "grad_norm": 0.05645332485437393,
      "learning_rate": 0.00011503649635036497,
      "loss": 0.2893,
      "step": 2042
    },
    {
      "epoch": 3.4050000000000002,
      "grad_norm": 0.09246646612882614,
      "learning_rate": 0.00011499478623566215,
      "loss": 0.2555,
      "step": 2043
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.06348439306020737,
      "learning_rate": 0.00011495307612095933,
      "loss": 0.3128,
      "step": 2044
    },
    {
      "epoch": 3.408333333333333,
      "grad_norm": 0.10232742130756378,
      "learning_rate": 0.00011491136600625651,
      "loss": 0.3521,
      "step": 2045
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.061281122267246246,
      "learning_rate": 0.00011486965589155372,
      "loss": 0.3601,
      "step": 2046
    },
    {
      "epoch": 3.4116666666666666,
      "grad_norm": 0.06782541424036026,
      "learning_rate": 0.0001148279457768509,
      "loss": 0.3619,
      "step": 2047
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.05053892731666565,
      "learning_rate": 0.00011478623566214808,
      "loss": 0.2979,
      "step": 2048
    },
    {
      "epoch": 3.415,
      "grad_norm": 0.09431142359972,
      "learning_rate": 0.00011474452554744525,
      "loss": 0.3858,
      "step": 2049
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 0.05653287470340729,
      "learning_rate": 0.00011470281543274243,
      "loss": 0.3205,
      "step": 2050
    },
    {
      "epoch": 3.4183333333333334,
      "grad_norm": 0.05771791562438011,
      "learning_rate": 0.00011466110531803964,
      "loss": 0.3179,
      "step": 2051
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.05536162853240967,
      "learning_rate": 0.00011461939520333682,
      "loss": 0.3016,
      "step": 2052
    },
    {
      "epoch": 3.421666666666667,
      "grad_norm": 0.05361901596188545,
      "learning_rate": 0.000114577685088634,
      "loss": 0.3022,
      "step": 2053
    },
    {
      "epoch": 3.4233333333333333,
      "grad_norm": 0.041555870324373245,
      "learning_rate": 0.00011453597497393118,
      "loss": 0.2313,
      "step": 2054
    },
    {
      "epoch": 3.425,
      "grad_norm": 0.0547112412750721,
      "learning_rate": 0.00011449426485922836,
      "loss": 0.3539,
      "step": 2055
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.05181451514363289,
      "learning_rate": 0.00011445255474452554,
      "loss": 0.326,
      "step": 2056
    },
    {
      "epoch": 3.4283333333333332,
      "grad_norm": 0.05601291358470917,
      "learning_rate": 0.00011441084462982274,
      "loss": 0.3553,
      "step": 2057
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.05609818175435066,
      "learning_rate": 0.00011436913451511992,
      "loss": 0.357,
      "step": 2058
    },
    {
      "epoch": 3.4316666666666666,
      "grad_norm": 0.04825973883271217,
      "learning_rate": 0.0001143274244004171,
      "loss": 0.2877,
      "step": 2059
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.045210979878902435,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.2866,
      "step": 2060
    },
    {
      "epoch": 3.435,
      "grad_norm": 0.0637369230389595,
      "learning_rate": 0.00011424400417101146,
      "loss": 0.3278,
      "step": 2061
    },
    {
      "epoch": 3.4366666666666665,
      "grad_norm": 0.04903319850564003,
      "learning_rate": 0.00011420229405630867,
      "loss": 0.2791,
      "step": 2062
    },
    {
      "epoch": 3.4383333333333335,
      "grad_norm": 0.05579772964119911,
      "learning_rate": 0.00011416058394160585,
      "loss": 0.308,
      "step": 2063
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.04830937087535858,
      "learning_rate": 0.00011411887382690303,
      "loss": 0.3001,
      "step": 2064
    },
    {
      "epoch": 3.4416666666666664,
      "grad_norm": 0.0664907693862915,
      "learning_rate": 0.00011407716371220021,
      "loss": 0.3763,
      "step": 2065
    },
    {
      "epoch": 3.4433333333333334,
      "grad_norm": 0.05468945950269699,
      "learning_rate": 0.00011403545359749739,
      "loss": 0.3609,
      "step": 2066
    },
    {
      "epoch": 3.445,
      "grad_norm": 0.0562804713845253,
      "learning_rate": 0.0001139937434827946,
      "loss": 0.3175,
      "step": 2067
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.06094905734062195,
      "learning_rate": 0.00011395203336809177,
      "loss": 0.3289,
      "step": 2068
    },
    {
      "epoch": 3.4483333333333333,
      "grad_norm": 0.0826670303940773,
      "learning_rate": 0.00011391032325338895,
      "loss": 0.389,
      "step": 2069
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.05638198181986809,
      "learning_rate": 0.00011386861313868613,
      "loss": 0.2965,
      "step": 2070
    },
    {
      "epoch": 3.4516666666666667,
      "grad_norm": 0.042159050703048706,
      "learning_rate": 0.00011382690302398331,
      "loss": 0.2387,
      "step": 2071
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.053119149059057236,
      "learning_rate": 0.00011378519290928052,
      "loss": 0.3046,
      "step": 2072
    },
    {
      "epoch": 3.455,
      "grad_norm": 0.06120539829134941,
      "learning_rate": 0.0001137434827945777,
      "loss": 0.3288,
      "step": 2073
    },
    {
      "epoch": 3.4566666666666666,
      "grad_norm": 0.0430910661816597,
      "learning_rate": 0.00011370177267987488,
      "loss": 0.2471,
      "step": 2074
    },
    {
      "epoch": 3.4583333333333335,
      "grad_norm": 0.05122305825352669,
      "learning_rate": 0.00011366006256517206,
      "loss": 0.298,
      "step": 2075
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.05529133602976799,
      "learning_rate": 0.00011361835245046924,
      "loss": 0.3141,
      "step": 2076
    },
    {
      "epoch": 3.461666666666667,
      "grad_norm": 0.04737646505236626,
      "learning_rate": 0.00011357664233576644,
      "loss": 0.3251,
      "step": 2077
    },
    {
      "epoch": 3.4633333333333334,
      "grad_norm": 0.07248686254024506,
      "learning_rate": 0.00011353493222106362,
      "loss": 0.3671,
      "step": 2078
    },
    {
      "epoch": 3.465,
      "grad_norm": 0.0412755012512207,
      "learning_rate": 0.0001134932221063608,
      "loss": 0.2914,
      "step": 2079
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.04612692445516586,
      "learning_rate": 0.00011345151199165798,
      "loss": 0.2517,
      "step": 2080
    },
    {
      "epoch": 3.4683333333333333,
      "grad_norm": 0.04567829519510269,
      "learning_rate": 0.00011340980187695516,
      "loss": 0.266,
      "step": 2081
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 0.054789941757917404,
      "learning_rate": 0.00011336809176225235,
      "loss": 0.2476,
      "step": 2082
    },
    {
      "epoch": 3.4716666666666667,
      "grad_norm": 0.07023993134498596,
      "learning_rate": 0.00011332638164754953,
      "loss": 0.2887,
      "step": 2083
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.09852523356676102,
      "learning_rate": 0.00011328467153284673,
      "loss": 0.3282,
      "step": 2084
    },
    {
      "epoch": 3.475,
      "grad_norm": 0.0484321229159832,
      "learning_rate": 0.0001132429614181439,
      "loss": 0.25,
      "step": 2085
    },
    {
      "epoch": 3.4766666666666666,
      "grad_norm": 0.049442142248153687,
      "learning_rate": 0.00011320125130344109,
      "loss": 0.2771,
      "step": 2086
    },
    {
      "epoch": 3.4783333333333335,
      "grad_norm": 0.051477961242198944,
      "learning_rate": 0.00011315954118873828,
      "loss": 0.2916,
      "step": 2087
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.056634657084941864,
      "learning_rate": 0.00011311783107403546,
      "loss": 0.3111,
      "step": 2088
    },
    {
      "epoch": 3.4816666666666665,
      "grad_norm": 0.06994670629501343,
      "learning_rate": 0.00011307612095933264,
      "loss": 0.2873,
      "step": 2089
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 0.04409211874008179,
      "learning_rate": 0.00011303441084462982,
      "loss": 0.2599,
      "step": 2090
    },
    {
      "epoch": 3.485,
      "grad_norm": 0.07253221422433853,
      "learning_rate": 0.00011299270072992701,
      "loss": 0.3963,
      "step": 2091
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.048706963658332825,
      "learning_rate": 0.0001129509906152242,
      "loss": 0.3079,
      "step": 2092
    },
    {
      "epoch": 3.4883333333333333,
      "grad_norm": 0.08764307200908661,
      "learning_rate": 0.00011290928050052138,
      "loss": 0.3195,
      "step": 2093
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.0545576810836792,
      "learning_rate": 0.00011286757038581856,
      "loss": 0.3117,
      "step": 2094
    },
    {
      "epoch": 3.4916666666666667,
      "grad_norm": 0.05144375190138817,
      "learning_rate": 0.00011282586027111574,
      "loss": 0.2935,
      "step": 2095
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.05299406126141548,
      "learning_rate": 0.00011278415015641292,
      "loss": 0.2777,
      "step": 2096
    },
    {
      "epoch": 3.495,
      "grad_norm": 0.06336156278848648,
      "learning_rate": 0.00011274244004171013,
      "loss": 0.3246,
      "step": 2097
    },
    {
      "epoch": 3.4966666666666666,
      "grad_norm": 0.06219671294093132,
      "learning_rate": 0.00011270072992700731,
      "loss": 0.358,
      "step": 2098
    },
    {
      "epoch": 3.4983333333333335,
      "grad_norm": 0.044888172298669815,
      "learning_rate": 0.00011265901981230449,
      "loss": 0.2816,
      "step": 2099
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.03662409633398056,
      "learning_rate": 0.00011261730969760167,
      "loss": 0.2599,
      "step": 2100
    },
    {
      "epoch": 3.5,
      "eval_loss": 0.3205720782279968,
      "eval_runtime": 371.7736,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 2100
    },
    {
      "epoch": 3.501666666666667,
      "grad_norm": 0.06931786239147186,
      "learning_rate": 0.00011257559958289885,
      "loss": 0.3878,
      "step": 2101
    },
    {
      "epoch": 3.5033333333333334,
      "grad_norm": 0.0468922033905983,
      "learning_rate": 0.00011253388946819605,
      "loss": 0.2736,
      "step": 2102
    },
    {
      "epoch": 3.505,
      "grad_norm": 0.07861430943012238,
      "learning_rate": 0.00011249217935349323,
      "loss": 0.334,
      "step": 2103
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.07536041736602783,
      "learning_rate": 0.00011245046923879041,
      "loss": 0.3073,
      "step": 2104
    },
    {
      "epoch": 3.5083333333333333,
      "grad_norm": 0.04987848922610283,
      "learning_rate": 0.00011240875912408759,
      "loss": 0.311,
      "step": 2105
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.04907941445708275,
      "learning_rate": 0.00011236704900938477,
      "loss": 0.3017,
      "step": 2106
    },
    {
      "epoch": 3.5116666666666667,
      "grad_norm": 0.05365850403904915,
      "learning_rate": 0.00011232533889468198,
      "loss": 0.2624,
      "step": 2107
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.049010999500751495,
      "learning_rate": 0.00011228362877997916,
      "loss": 0.2682,
      "step": 2108
    },
    {
      "epoch": 3.515,
      "grad_norm": 0.04884723573923111,
      "learning_rate": 0.00011224191866527634,
      "loss": 0.2774,
      "step": 2109
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 0.05809545889496803,
      "learning_rate": 0.00011220020855057352,
      "loss": 0.3326,
      "step": 2110
    },
    {
      "epoch": 3.5183333333333335,
      "grad_norm": 0.06276586651802063,
      "learning_rate": 0.0001121584984358707,
      "loss": 0.3884,
      "step": 2111
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.05619725584983826,
      "learning_rate": 0.0001121167883211679,
      "loss": 0.3549,
      "step": 2112
    },
    {
      "epoch": 3.5216666666666665,
      "grad_norm": 0.054586295038461685,
      "learning_rate": 0.00011207507820646508,
      "loss": 0.2759,
      "step": 2113
    },
    {
      "epoch": 3.5233333333333334,
      "grad_norm": 0.05397654324769974,
      "learning_rate": 0.00011203336809176226,
      "loss": 0.4032,
      "step": 2114
    },
    {
      "epoch": 3.525,
      "grad_norm": 0.051778990775346756,
      "learning_rate": 0.00011199165797705944,
      "loss": 0.3519,
      "step": 2115
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.0740681067109108,
      "learning_rate": 0.00011194994786235662,
      "loss": 0.3873,
      "step": 2116
    },
    {
      "epoch": 3.5283333333333333,
      "grad_norm": 0.05533061549067497,
      "learning_rate": 0.00011190823774765383,
      "loss": 0.3367,
      "step": 2117
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 0.06350646913051605,
      "learning_rate": 0.000111866527632951,
      "loss": 0.3534,
      "step": 2118
    },
    {
      "epoch": 3.5316666666666667,
      "grad_norm": 0.05090070515871048,
      "learning_rate": 0.00011182481751824818,
      "loss": 0.3145,
      "step": 2119
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.05892186984419823,
      "learning_rate": 0.00011178310740354536,
      "loss": 0.3918,
      "step": 2120
    },
    {
      "epoch": 3.535,
      "grad_norm": 0.042280375957489014,
      "learning_rate": 0.00011174139728884254,
      "loss": 0.2862,
      "step": 2121
    },
    {
      "epoch": 3.5366666666666666,
      "grad_norm": 0.07863520085811615,
      "learning_rate": 0.00011169968717413975,
      "loss": 0.4161,
      "step": 2122
    },
    {
      "epoch": 3.538333333333333,
      "grad_norm": 0.06965412944555283,
      "learning_rate": 0.00011165797705943693,
      "loss": 0.3719,
      "step": 2123
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.04291622340679169,
      "learning_rate": 0.00011161626694473411,
      "loss": 0.3138,
      "step": 2124
    },
    {
      "epoch": 3.5416666666666665,
      "grad_norm": 0.05225519463419914,
      "learning_rate": 0.00011157455683003129,
      "loss": 0.3161,
      "step": 2125
    },
    {
      "epoch": 3.5433333333333334,
      "grad_norm": 0.0643591433763504,
      "learning_rate": 0.00011153284671532847,
      "loss": 0.3747,
      "step": 2126
    },
    {
      "epoch": 3.545,
      "grad_norm": 0.044865842908620834,
      "learning_rate": 0.00011149113660062566,
      "loss": 0.3437,
      "step": 2127
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.03936343267560005,
      "learning_rate": 0.00011144942648592285,
      "loss": 0.2976,
      "step": 2128
    },
    {
      "epoch": 3.5483333333333333,
      "grad_norm": 0.03876578435301781,
      "learning_rate": 0.00011140771637122003,
      "loss": 0.2784,
      "step": 2129
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.06155036389827728,
      "learning_rate": 0.00011136600625651721,
      "loss": 0.2635,
      "step": 2130
    },
    {
      "epoch": 3.5516666666666667,
      "grad_norm": 0.062332723289728165,
      "learning_rate": 0.00011132429614181439,
      "loss": 0.3415,
      "step": 2131
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.040779996663331985,
      "learning_rate": 0.00011128258602711159,
      "loss": 0.2362,
      "step": 2132
    },
    {
      "epoch": 3.555,
      "grad_norm": 0.05355333536863327,
      "learning_rate": 0.00011124087591240877,
      "loss": 0.4103,
      "step": 2133
    },
    {
      "epoch": 3.5566666666666666,
      "grad_norm": 0.05047301948070526,
      "learning_rate": 0.00011119916579770594,
      "loss": 0.3621,
      "step": 2134
    },
    {
      "epoch": 3.5583333333333336,
      "grad_norm": 0.06357595324516296,
      "learning_rate": 0.00011115745568300314,
      "loss": 0.3259,
      "step": 2135
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.04750993847846985,
      "learning_rate": 0.00011111574556830032,
      "loss": 0.2731,
      "step": 2136
    },
    {
      "epoch": 3.5616666666666665,
      "grad_norm": 0.055073849856853485,
      "learning_rate": 0.00011107403545359751,
      "loss": 0.304,
      "step": 2137
    },
    {
      "epoch": 3.5633333333333335,
      "grad_norm": 0.05231383070349693,
      "learning_rate": 0.00011103232533889469,
      "loss": 0.3222,
      "step": 2138
    },
    {
      "epoch": 3.565,
      "grad_norm": 0.04287088289856911,
      "learning_rate": 0.00011099061522419187,
      "loss": 0.269,
      "step": 2139
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.051787085831165314,
      "learning_rate": 0.00011094890510948905,
      "loss": 0.3098,
      "step": 2140
    },
    {
      "epoch": 3.5683333333333334,
      "grad_norm": 0.039851848036050797,
      "learning_rate": 0.00011090719499478623,
      "loss": 0.2663,
      "step": 2141
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.05554128810763359,
      "learning_rate": 0.00011086548488008342,
      "loss": 0.3422,
      "step": 2142
    },
    {
      "epoch": 3.5716666666666668,
      "grad_norm": 0.07850797474384308,
      "learning_rate": 0.00011082377476538061,
      "loss": 0.3138,
      "step": 2143
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.057605814188718796,
      "learning_rate": 0.0001107820646506778,
      "loss": 0.2816,
      "step": 2144
    },
    {
      "epoch": 3.575,
      "grad_norm": 0.07517246156930923,
      "learning_rate": 0.00011074035453597497,
      "loss": 0.4198,
      "step": 2145
    },
    {
      "epoch": 3.5766666666666667,
      "grad_norm": 0.043748997151851654,
      "learning_rate": 0.00011069864442127215,
      "loss": 0.2795,
      "step": 2146
    },
    {
      "epoch": 3.578333333333333,
      "grad_norm": 0.05212860554456711,
      "learning_rate": 0.00011065693430656933,
      "loss": 0.2779,
      "step": 2147
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.03705374896526337,
      "learning_rate": 0.00011061522419186654,
      "loss": 0.2586,
      "step": 2148
    },
    {
      "epoch": 3.5816666666666666,
      "grad_norm": 0.05545816197991371,
      "learning_rate": 0.00011057351407716372,
      "loss": 0.2908,
      "step": 2149
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 0.05452420935034752,
      "learning_rate": 0.0001105318039624609,
      "loss": 0.2994,
      "step": 2150
    },
    {
      "epoch": 3.585,
      "grad_norm": 0.046461254358291626,
      "learning_rate": 0.00011049009384775808,
      "loss": 0.3104,
      "step": 2151
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.03662402927875519,
      "learning_rate": 0.00011044838373305526,
      "loss": 0.2564,
      "step": 2152
    },
    {
      "epoch": 3.5883333333333334,
      "grad_norm": 0.050746336579322815,
      "learning_rate": 0.00011040667361835246,
      "loss": 0.3453,
      "step": 2153
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.07242312282323837,
      "learning_rate": 0.00011036496350364964,
      "loss": 0.2876,
      "step": 2154
    },
    {
      "epoch": 3.591666666666667,
      "grad_norm": 0.07653667777776718,
      "learning_rate": 0.00011032325338894682,
      "loss": 0.3241,
      "step": 2155
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.06029690429568291,
      "learning_rate": 0.000110281543274244,
      "loss": 0.3579,
      "step": 2156
    },
    {
      "epoch": 3.5949999999999998,
      "grad_norm": 0.04492969438433647,
      "learning_rate": 0.00011023983315954118,
      "loss": 0.1998,
      "step": 2157
    },
    {
      "epoch": 3.5966666666666667,
      "grad_norm": 0.05720745027065277,
      "learning_rate": 0.00011019812304483839,
      "loss": 0.3052,
      "step": 2158
    },
    {
      "epoch": 3.5983333333333336,
      "grad_norm": 0.07392260432243347,
      "learning_rate": 0.00011015641293013557,
      "loss": 0.3894,
      "step": 2159
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.05173616111278534,
      "learning_rate": 0.00011011470281543275,
      "loss": 0.3138,
      "step": 2160
    },
    {
      "epoch": 3.6016666666666666,
      "grad_norm": 0.05019362270832062,
      "learning_rate": 0.00011007299270072993,
      "loss": 0.3079,
      "step": 2161
    },
    {
      "epoch": 3.6033333333333335,
      "grad_norm": 0.06894797831773758,
      "learning_rate": 0.0001100312825860271,
      "loss": 0.3518,
      "step": 2162
    },
    {
      "epoch": 3.605,
      "grad_norm": 0.04892003536224365,
      "learning_rate": 0.00010998957247132431,
      "loss": 0.2662,
      "step": 2163
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.045374516397714615,
      "learning_rate": 0.00010994786235662149,
      "loss": 0.2958,
      "step": 2164
    },
    {
      "epoch": 3.6083333333333334,
      "grad_norm": 0.07918766885995865,
      "learning_rate": 0.00010990615224191867,
      "loss": 0.3563,
      "step": 2165
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.0408390536904335,
      "learning_rate": 0.00010986444212721585,
      "loss": 0.2531,
      "step": 2166
    },
    {
      "epoch": 3.611666666666667,
      "grad_norm": 0.0864640548825264,
      "learning_rate": 0.00010982273201251303,
      "loss": 0.3553,
      "step": 2167
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.03629495948553085,
      "learning_rate": 0.00010978102189781024,
      "loss": 0.2222,
      "step": 2168
    },
    {
      "epoch": 3.615,
      "grad_norm": 0.059184372425079346,
      "learning_rate": 0.00010973931178310742,
      "loss": 0.3968,
      "step": 2169
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 0.052229106426239014,
      "learning_rate": 0.0001096976016684046,
      "loss": 0.2904,
      "step": 2170
    },
    {
      "epoch": 3.618333333333333,
      "grad_norm": 0.05440022423863411,
      "learning_rate": 0.00010965589155370178,
      "loss": 0.2902,
      "step": 2171
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.037845440208911896,
      "learning_rate": 0.00010961418143899895,
      "loss": 0.218,
      "step": 2172
    },
    {
      "epoch": 3.6216666666666666,
      "grad_norm": 0.04210282862186432,
      "learning_rate": 0.00010957247132429616,
      "loss": 0.2821,
      "step": 2173
    },
    {
      "epoch": 3.623333333333333,
      "grad_norm": 0.06915769726037979,
      "learning_rate": 0.00010953076120959334,
      "loss": 0.3757,
      "step": 2174
    },
    {
      "epoch": 3.625,
      "grad_norm": 0.053802791982889175,
      "learning_rate": 0.00010948905109489052,
      "loss": 0.3001,
      "step": 2175
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.08546022325754166,
      "learning_rate": 0.0001094473409801877,
      "loss": 0.3596,
      "step": 2176
    },
    {
      "epoch": 3.6283333333333334,
      "grad_norm": 0.03714149072766304,
      "learning_rate": 0.00010940563086548488,
      "loss": 0.2232,
      "step": 2177
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.048101380467414856,
      "learning_rate": 0.00010936392075078207,
      "loss": 0.2887,
      "step": 2178
    },
    {
      "epoch": 3.631666666666667,
      "grad_norm": 0.04264995455741882,
      "learning_rate": 0.00010932221063607927,
      "loss": 0.2585,
      "step": 2179
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.06464427709579468,
      "learning_rate": 0.00010928050052137644,
      "loss": 0.3118,
      "step": 2180
    },
    {
      "epoch": 3.635,
      "grad_norm": 0.04988570138812065,
      "learning_rate": 0.00010923879040667362,
      "loss": 0.2551,
      "step": 2181
    },
    {
      "epoch": 3.6366666666666667,
      "grad_norm": 0.058940496295690536,
      "learning_rate": 0.0001091970802919708,
      "loss": 0.3605,
      "step": 2182
    },
    {
      "epoch": 3.638333333333333,
      "grad_norm": 0.04855046421289444,
      "learning_rate": 0.000109155370177268,
      "loss": 0.2818,
      "step": 2183
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.03422920033335686,
      "learning_rate": 0.00010911366006256518,
      "loss": 0.2453,
      "step": 2184
    },
    {
      "epoch": 3.6416666666666666,
      "grad_norm": 0.04131274297833443,
      "learning_rate": 0.00010907194994786236,
      "loss": 0.2799,
      "step": 2185
    },
    {
      "epoch": 3.6433333333333335,
      "grad_norm": 0.05338050052523613,
      "learning_rate": 0.00010903023983315954,
      "loss": 0.3294,
      "step": 2186
    },
    {
      "epoch": 3.645,
      "grad_norm": 0.04356081411242485,
      "learning_rate": 0.00010898852971845673,
      "loss": 0.2836,
      "step": 2187
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.044472306966781616,
      "learning_rate": 0.00010894681960375392,
      "loss": 0.3047,
      "step": 2188
    },
    {
      "epoch": 3.6483333333333334,
      "grad_norm": 0.05598163232207298,
      "learning_rate": 0.0001089051094890511,
      "loss": 0.3082,
      "step": 2189
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.04928706958889961,
      "learning_rate": 0.00010886339937434828,
      "loss": 0.3363,
      "step": 2190
    },
    {
      "epoch": 3.6516666666666664,
      "grad_norm": 0.03838256746530533,
      "learning_rate": 0.00010882168925964546,
      "loss": 0.2476,
      "step": 2191
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.043141212314367294,
      "learning_rate": 0.00010877997914494264,
      "loss": 0.2525,
      "step": 2192
    },
    {
      "epoch": 3.6550000000000002,
      "grad_norm": 0.0491400770843029,
      "learning_rate": 0.00010873826903023985,
      "loss": 0.2809,
      "step": 2193
    },
    {
      "epoch": 3.6566666666666667,
      "grad_norm": 0.05102771148085594,
      "learning_rate": 0.00010869655891553703,
      "loss": 0.2949,
      "step": 2194
    },
    {
      "epoch": 3.658333333333333,
      "grad_norm": 0.07737089693546295,
      "learning_rate": 0.0001086548488008342,
      "loss": 0.3671,
      "step": 2195
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.07383612543344498,
      "learning_rate": 0.00010861313868613138,
      "loss": 0.3722,
      "step": 2196
    },
    {
      "epoch": 3.6616666666666666,
      "grad_norm": 0.03802654892206192,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.2518,
      "step": 2197
    },
    {
      "epoch": 3.663333333333333,
      "grad_norm": 0.05617181956768036,
      "learning_rate": 0.00010852971845672577,
      "loss": 0.3487,
      "step": 2198
    },
    {
      "epoch": 3.665,
      "grad_norm": 0.05045614764094353,
      "learning_rate": 0.00010848800834202295,
      "loss": 0.2911,
      "step": 2199
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.05036511272192001,
      "learning_rate": 0.00010844629822732013,
      "loss": 0.2798,
      "step": 2200
    },
    {
      "epoch": 3.6666666666666665,
      "eval_loss": 0.31958454847335815,
      "eval_runtime": 371.6055,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 2200
    },
    {
      "epoch": 3.6683333333333334,
      "grad_norm": 0.05282978340983391,
      "learning_rate": 0.00010840458811261731,
      "loss": 0.2705,
      "step": 2201
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.03502843528985977,
      "learning_rate": 0.00010836287799791449,
      "loss": 0.2731,
      "step": 2202
    },
    {
      "epoch": 3.671666666666667,
      "grad_norm": 0.043954867869615555,
      "learning_rate": 0.0001083211678832117,
      "loss": 0.2452,
      "step": 2203
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.07298093289136887,
      "learning_rate": 0.00010827945776850887,
      "loss": 0.3213,
      "step": 2204
    },
    {
      "epoch": 3.675,
      "grad_norm": 0.04947080463171005,
      "learning_rate": 0.00010823774765380605,
      "loss": 0.287,
      "step": 2205
    },
    {
      "epoch": 3.6766666666666667,
      "grad_norm": 0.053989943116903305,
      "learning_rate": 0.00010819603753910323,
      "loss": 0.3178,
      "step": 2206
    },
    {
      "epoch": 3.6783333333333332,
      "grad_norm": 0.052596062421798706,
      "learning_rate": 0.00010815432742440041,
      "loss": 0.2801,
      "step": 2207
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.05093681067228317,
      "learning_rate": 0.00010811261730969762,
      "loss": 0.3084,
      "step": 2208
    },
    {
      "epoch": 3.6816666666666666,
      "grad_norm": 0.04353826493024826,
      "learning_rate": 0.0001080709071949948,
      "loss": 0.2612,
      "step": 2209
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 0.043846819549798965,
      "learning_rate": 0.00010802919708029198,
      "loss": 0.2463,
      "step": 2210
    },
    {
      "epoch": 3.685,
      "grad_norm": 0.07267297059297562,
      "learning_rate": 0.00010798748696558916,
      "loss": 0.3612,
      "step": 2211
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.03089437074959278,
      "learning_rate": 0.00010794577685088634,
      "loss": 0.2502,
      "step": 2212
    },
    {
      "epoch": 3.6883333333333335,
      "grad_norm": 0.04600229486823082,
      "learning_rate": 0.00010790406673618354,
      "loss": 0.2994,
      "step": 2213
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.035209670662879944,
      "learning_rate": 0.00010786235662148072,
      "loss": 0.2326,
      "step": 2214
    },
    {
      "epoch": 3.6916666666666664,
      "grad_norm": 0.03996715322136879,
      "learning_rate": 0.0001078206465067779,
      "loss": 0.2874,
      "step": 2215
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.04758670553565025,
      "learning_rate": 0.00010777893639207508,
      "loss": 0.3096,
      "step": 2216
    },
    {
      "epoch": 3.695,
      "grad_norm": 0.06673511117696762,
      "learning_rate": 0.00010773722627737226,
      "loss": 0.361,
      "step": 2217
    },
    {
      "epoch": 3.6966666666666668,
      "grad_norm": 0.04971318319439888,
      "learning_rate": 0.00010769551616266947,
      "loss": 0.2973,
      "step": 2218
    },
    {
      "epoch": 3.6983333333333333,
      "grad_norm": 0.04873505234718323,
      "learning_rate": 0.00010765380604796665,
      "loss": 0.3133,
      "step": 2219
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.051328904926776886,
      "learning_rate": 0.00010761209593326383,
      "loss": 0.3501,
      "step": 2220
    },
    {
      "epoch": 3.7016666666666667,
      "grad_norm": 0.040223464369773865,
      "learning_rate": 0.00010757038581856101,
      "loss": 0.2844,
      "step": 2221
    },
    {
      "epoch": 3.703333333333333,
      "grad_norm": 0.054285869002342224,
      "learning_rate": 0.00010752867570385819,
      "loss": 0.2937,
      "step": 2222
    },
    {
      "epoch": 3.705,
      "grad_norm": 0.07050414383411407,
      "learning_rate": 0.00010748696558915538,
      "loss": 0.343,
      "step": 2223
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.04641178250312805,
      "learning_rate": 0.00010744525547445257,
      "loss": 0.3318,
      "step": 2224
    },
    {
      "epoch": 3.7083333333333335,
      "grad_norm": 0.042934171855449677,
      "learning_rate": 0.00010740354535974975,
      "loss": 0.2727,
      "step": 2225
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.05458793789148331,
      "learning_rate": 0.00010736183524504693,
      "loss": 0.286,
      "step": 2226
    },
    {
      "epoch": 3.711666666666667,
      "grad_norm": 0.046842098236083984,
      "learning_rate": 0.00010732012513034411,
      "loss": 0.2804,
      "step": 2227
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.057778771966695786,
      "learning_rate": 0.0001072784150156413,
      "loss": 0.3671,
      "step": 2228
    },
    {
      "epoch": 3.715,
      "grad_norm": 0.041211750358343124,
      "learning_rate": 0.00010723670490093848,
      "loss": 0.2746,
      "step": 2229
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 0.044667478650808334,
      "learning_rate": 0.00010719499478623566,
      "loss": 0.2869,
      "step": 2230
    },
    {
      "epoch": 3.7183333333333333,
      "grad_norm": 0.04016343876719475,
      "learning_rate": 0.00010715328467153286,
      "loss": 0.3055,
      "step": 2231
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.05182823911309242,
      "learning_rate": 0.00010711157455683004,
      "loss": 0.2913,
      "step": 2232
    },
    {
      "epoch": 3.7216666666666667,
      "grad_norm": 0.048184286803007126,
      "learning_rate": 0.00010706986444212721,
      "loss": 0.3039,
      "step": 2233
    },
    {
      "epoch": 3.7233333333333336,
      "grad_norm": 0.08670658618211746,
      "learning_rate": 0.00010702815432742441,
      "loss": 0.3732,
      "step": 2234
    },
    {
      "epoch": 3.725,
      "grad_norm": 0.06250941008329391,
      "learning_rate": 0.00010698644421272159,
      "loss": 0.3315,
      "step": 2235
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.051087409257888794,
      "learning_rate": 0.00010694473409801877,
      "loss": 0.3145,
      "step": 2236
    },
    {
      "epoch": 3.7283333333333335,
      "grad_norm": 0.0808454379439354,
      "learning_rate": 0.00010690302398331595,
      "loss": 0.3302,
      "step": 2237
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.04246637225151062,
      "learning_rate": 0.00010686131386861314,
      "loss": 0.2913,
      "step": 2238
    },
    {
      "epoch": 3.7316666666666665,
      "grad_norm": 0.06087084487080574,
      "learning_rate": 0.00010681960375391033,
      "loss": 0.347,
      "step": 2239
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.055713340640068054,
      "learning_rate": 0.00010677789363920751,
      "loss": 0.3778,
      "step": 2240
    },
    {
      "epoch": 3.735,
      "grad_norm": 0.04175688698887825,
      "learning_rate": 0.00010673618352450469,
      "loss": 0.2305,
      "step": 2241
    },
    {
      "epoch": 3.736666666666667,
      "grad_norm": 0.05393463373184204,
      "learning_rate": 0.00010669447340980187,
      "loss": 0.3207,
      "step": 2242
    },
    {
      "epoch": 3.7383333333333333,
      "grad_norm": 0.04182640463113785,
      "learning_rate": 0.00010665276329509905,
      "loss": 0.3386,
      "step": 2243
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.04671623557806015,
      "learning_rate": 0.00010661105318039626,
      "loss": 0.2452,
      "step": 2244
    },
    {
      "epoch": 3.7416666666666667,
      "grad_norm": 0.052039727568626404,
      "learning_rate": 0.00010656934306569344,
      "loss": 0.3073,
      "step": 2245
    },
    {
      "epoch": 3.743333333333333,
      "grad_norm": 0.04659735783934593,
      "learning_rate": 0.00010652763295099062,
      "loss": 0.3259,
      "step": 2246
    },
    {
      "epoch": 3.745,
      "grad_norm": 0.06713239848613739,
      "learning_rate": 0.0001064859228362878,
      "loss": 0.3596,
      "step": 2247
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.06058453395962715,
      "learning_rate": 0.00010644421272158498,
      "loss": 0.348,
      "step": 2248
    },
    {
      "epoch": 3.748333333333333,
      "grad_norm": 0.0467560775578022,
      "learning_rate": 0.00010640250260688218,
      "loss": 0.3006,
      "step": 2249
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.06171844154596329,
      "learning_rate": 0.00010636079249217936,
      "loss": 0.3828,
      "step": 2250
    },
    {
      "epoch": 3.751666666666667,
      "grad_norm": 0.045438073575496674,
      "learning_rate": 0.00010631908237747654,
      "loss": 0.3409,
      "step": 2251
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.046074967831373215,
      "learning_rate": 0.00010627737226277372,
      "loss": 0.307,
      "step": 2252
    },
    {
      "epoch": 3.755,
      "grad_norm": 0.05261995270848274,
      "learning_rate": 0.0001062356621480709,
      "loss": 0.3281,
      "step": 2253
    },
    {
      "epoch": 3.756666666666667,
      "grad_norm": 0.04231778904795647,
      "learning_rate": 0.0001061939520333681,
      "loss": 0.2853,
      "step": 2254
    },
    {
      "epoch": 3.7583333333333333,
      "grad_norm": 0.046839356422424316,
      "learning_rate": 0.00010615224191866529,
      "loss": 0.2965,
      "step": 2255
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.06168091297149658,
      "learning_rate": 0.00010611053180396247,
      "loss": 0.3205,
      "step": 2256
    },
    {
      "epoch": 3.7616666666666667,
      "grad_norm": 0.0651545450091362,
      "learning_rate": 0.00010606882168925964,
      "loss": 0.3377,
      "step": 2257
    },
    {
      "epoch": 3.763333333333333,
      "grad_norm": 0.053112179040908813,
      "learning_rate": 0.00010602711157455682,
      "loss": 0.3242,
      "step": 2258
    },
    {
      "epoch": 3.765,
      "grad_norm": 0.04843002185225487,
      "learning_rate": 0.00010598540145985403,
      "loss": 0.3677,
      "step": 2259
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.04777032881975174,
      "learning_rate": 0.00010594369134515121,
      "loss": 0.2897,
      "step": 2260
    },
    {
      "epoch": 3.7683333333333335,
      "grad_norm": 0.04566241800785065,
      "learning_rate": 0.00010590198123044839,
      "loss": 0.2641,
      "step": 2261
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.05530976131558418,
      "learning_rate": 0.00010586027111574557,
      "loss": 0.3189,
      "step": 2262
    },
    {
      "epoch": 3.7716666666666665,
      "grad_norm": 0.07119346410036087,
      "learning_rate": 0.00010581856100104275,
      "loss": 0.3379,
      "step": 2263
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.06138281524181366,
      "learning_rate": 0.00010577685088633996,
      "loss": 0.3642,
      "step": 2264
    },
    {
      "epoch": 3.775,
      "grad_norm": 0.04519824683666229,
      "learning_rate": 0.00010573514077163713,
      "loss": 0.274,
      "step": 2265
    },
    {
      "epoch": 3.7766666666666664,
      "grad_norm": 0.049106184393167496,
      "learning_rate": 0.00010569343065693431,
      "loss": 0.3413,
      "step": 2266
    },
    {
      "epoch": 3.7783333333333333,
      "grad_norm": 0.0385318361222744,
      "learning_rate": 0.0001056517205422315,
      "loss": 0.2421,
      "step": 2267
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.04922354221343994,
      "learning_rate": 0.00010561001042752867,
      "loss": 0.3197,
      "step": 2268
    },
    {
      "epoch": 3.7816666666666667,
      "grad_norm": 0.0461236946284771,
      "learning_rate": 0.00010556830031282588,
      "loss": 0.3044,
      "step": 2269
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 0.047105200588703156,
      "learning_rate": 0.00010552659019812306,
      "loss": 0.2641,
      "step": 2270
    },
    {
      "epoch": 3.785,
      "grad_norm": 0.0899142399430275,
      "learning_rate": 0.00010548488008342024,
      "loss": 0.3412,
      "step": 2271
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.07742830365896225,
      "learning_rate": 0.00010544316996871742,
      "loss": 0.3403,
      "step": 2272
    },
    {
      "epoch": 3.788333333333333,
      "grad_norm": 0.03845285624265671,
      "learning_rate": 0.0001054014598540146,
      "loss": 0.2717,
      "step": 2273
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.05109265446662903,
      "learning_rate": 0.00010535974973931179,
      "loss": 0.311,
      "step": 2274
    },
    {
      "epoch": 3.7916666666666665,
      "grad_norm": 0.05624263361096382,
      "learning_rate": 0.00010531803962460898,
      "loss": 0.3289,
      "step": 2275
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.05943596735596657,
      "learning_rate": 0.00010527632950990616,
      "loss": 0.3197,
      "step": 2276
    },
    {
      "epoch": 3.795,
      "grad_norm": 0.04106228053569794,
      "learning_rate": 0.00010523461939520334,
      "loss": 0.3106,
      "step": 2277
    },
    {
      "epoch": 3.796666666666667,
      "grad_norm": 0.062106650322675705,
      "learning_rate": 0.00010519290928050052,
      "loss": 0.3819,
      "step": 2278
    },
    {
      "epoch": 3.7983333333333333,
      "grad_norm": 0.04019981250166893,
      "learning_rate": 0.00010515119916579772,
      "loss": 0.2728,
      "step": 2279
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.04225583374500275,
      "learning_rate": 0.0001051094890510949,
      "loss": 0.2597,
      "step": 2280
    },
    {
      "epoch": 3.8016666666666667,
      "grad_norm": 0.0597652830183506,
      "learning_rate": 0.00010506777893639207,
      "loss": 0.3144,
      "step": 2281
    },
    {
      "epoch": 3.8033333333333332,
      "grad_norm": 0.0721273422241211,
      "learning_rate": 0.00010502606882168927,
      "loss": 0.296,
      "step": 2282
    },
    {
      "epoch": 3.805,
      "grad_norm": 0.03593635559082031,
      "learning_rate": 0.00010498435870698645,
      "loss": 0.2666,
      "step": 2283
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.04483456164598465,
      "learning_rate": 0.00010494264859228364,
      "loss": 0.295,
      "step": 2284
    },
    {
      "epoch": 3.8083333333333336,
      "grad_norm": 0.06833245605230331,
      "learning_rate": 0.00010490093847758082,
      "loss": 0.2723,
      "step": 2285
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.05000440776348114,
      "learning_rate": 0.000104859228362878,
      "loss": 0.3607,
      "step": 2286
    },
    {
      "epoch": 3.8116666666666665,
      "grad_norm": 0.05052827298641205,
      "learning_rate": 0.00010481751824817518,
      "loss": 0.3392,
      "step": 2287
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.04429023340344429,
      "learning_rate": 0.00010477580813347236,
      "loss": 0.3182,
      "step": 2288
    },
    {
      "epoch": 3.815,
      "grad_norm": 0.04343811795115471,
      "learning_rate": 0.00010473409801876956,
      "loss": 0.2775,
      "step": 2289
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 0.05674101412296295,
      "learning_rate": 0.00010469238790406674,
      "loss": 0.3836,
      "step": 2290
    },
    {
      "epoch": 3.8183333333333334,
      "grad_norm": 0.09617886692285538,
      "learning_rate": 0.00010465067778936392,
      "loss": 0.3426,
      "step": 2291
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.056952498853206635,
      "learning_rate": 0.0001046089676746611,
      "loss": 0.3178,
      "step": 2292
    },
    {
      "epoch": 3.8216666666666668,
      "grad_norm": 0.04508087411522865,
      "learning_rate": 0.00010456725755995828,
      "loss": 0.3178,
      "step": 2293
    },
    {
      "epoch": 3.8233333333333333,
      "grad_norm": 0.08868487179279327,
      "learning_rate": 0.00010452554744525549,
      "loss": 0.4143,
      "step": 2294
    },
    {
      "epoch": 3.825,
      "grad_norm": 0.056043338030576706,
      "learning_rate": 0.00010448383733055267,
      "loss": 0.3393,
      "step": 2295
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.0709482878446579,
      "learning_rate": 0.00010444212721584985,
      "loss": 0.3442,
      "step": 2296
    },
    {
      "epoch": 3.828333333333333,
      "grad_norm": 0.0654367208480835,
      "learning_rate": 0.00010440041710114703,
      "loss": 0.2418,
      "step": 2297
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.05861096456646919,
      "learning_rate": 0.0001043587069864442,
      "loss": 0.3207,
      "step": 2298
    },
    {
      "epoch": 3.8316666666666666,
      "grad_norm": 0.0700758695602417,
      "learning_rate": 0.00010431699687174141,
      "loss": 0.3929,
      "step": 2299
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.042333122342824936,
      "learning_rate": 0.00010427528675703859,
      "loss": 0.2805,
      "step": 2300
    },
    {
      "epoch": 3.8333333333333335,
      "eval_loss": 0.31985801458358765,
      "eval_runtime": 371.641,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 2300
    },
    {
      "epoch": 3.835,
      "grad_norm": 0.03488902747631073,
      "learning_rate": 0.00010423357664233577,
      "loss": 0.2382,
      "step": 2301
    },
    {
      "epoch": 3.836666666666667,
      "grad_norm": 0.05344771221280098,
      "learning_rate": 0.00010419186652763295,
      "loss": 0.3447,
      "step": 2302
    },
    {
      "epoch": 3.8383333333333334,
      "grad_norm": 0.04820659011602402,
      "learning_rate": 0.00010415015641293013,
      "loss": 0.2781,
      "step": 2303
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.04358372464776039,
      "learning_rate": 0.00010410844629822734,
      "loss": 0.2706,
      "step": 2304
    },
    {
      "epoch": 3.841666666666667,
      "grad_norm": 0.056298550218343735,
      "learning_rate": 0.00010406673618352452,
      "loss": 0.3958,
      "step": 2305
    },
    {
      "epoch": 3.8433333333333333,
      "grad_norm": 0.046719878911972046,
      "learning_rate": 0.0001040250260688217,
      "loss": 0.3034,
      "step": 2306
    },
    {
      "epoch": 3.8449999999999998,
      "grad_norm": 0.05494691804051399,
      "learning_rate": 0.00010398331595411888,
      "loss": 0.2715,
      "step": 2307
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.06233029067516327,
      "learning_rate": 0.00010394160583941606,
      "loss": 0.3284,
      "step": 2308
    },
    {
      "epoch": 3.8483333333333336,
      "grad_norm": 0.03669464960694313,
      "learning_rate": 0.00010389989572471326,
      "loss": 0.2803,
      "step": 2309
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.07861983776092529,
      "learning_rate": 0.00010385818561001044,
      "loss": 0.3521,
      "step": 2310
    },
    {
      "epoch": 3.8516666666666666,
      "grad_norm": 0.04664392024278641,
      "learning_rate": 0.00010381647549530762,
      "loss": 0.301,
      "step": 2311
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.05025918409228325,
      "learning_rate": 0.0001037747653806048,
      "loss": 0.3268,
      "step": 2312
    },
    {
      "epoch": 3.855,
      "grad_norm": 0.05084432289004326,
      "learning_rate": 0.00010373305526590198,
      "loss": 0.3338,
      "step": 2313
    },
    {
      "epoch": 3.8566666666666665,
      "grad_norm": 0.05351841077208519,
      "learning_rate": 0.00010369134515119919,
      "loss": 0.3257,
      "step": 2314
    },
    {
      "epoch": 3.8583333333333334,
      "grad_norm": 0.07299081981182098,
      "learning_rate": 0.00010364963503649637,
      "loss": 0.4079,
      "step": 2315
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.04816671460866928,
      "learning_rate": 0.00010360792492179355,
      "loss": 0.3675,
      "step": 2316
    },
    {
      "epoch": 3.861666666666667,
      "grad_norm": 0.03696633130311966,
      "learning_rate": 0.00010356621480709073,
      "loss": 0.2501,
      "step": 2317
    },
    {
      "epoch": 3.8633333333333333,
      "grad_norm": 0.04825512319803238,
      "learning_rate": 0.0001035245046923879,
      "loss": 0.3079,
      "step": 2318
    },
    {
      "epoch": 3.865,
      "grad_norm": 0.04963638633489609,
      "learning_rate": 0.00010348279457768508,
      "loss": 0.3562,
      "step": 2319
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.038595858961343765,
      "learning_rate": 0.00010344108446298229,
      "loss": 0.2803,
      "step": 2320
    },
    {
      "epoch": 3.868333333333333,
      "grad_norm": 0.04510016366839409,
      "learning_rate": 0.00010339937434827947,
      "loss": 0.2753,
      "step": 2321
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.04440022632479668,
      "learning_rate": 0.00010335766423357665,
      "loss": 0.2818,
      "step": 2322
    },
    {
      "epoch": 3.8716666666666666,
      "grad_norm": 0.04264003410935402,
      "learning_rate": 0.00010331595411887383,
      "loss": 0.2723,
      "step": 2323
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.041280221194028854,
      "learning_rate": 0.00010327424400417101,
      "loss": 0.3155,
      "step": 2324
    },
    {
      "epoch": 3.875,
      "grad_norm": 0.050692860037088394,
      "learning_rate": 0.0001032325338894682,
      "loss": 0.2664,
      "step": 2325
    },
    {
      "epoch": 3.876666666666667,
      "grad_norm": 0.04412775859236717,
      "learning_rate": 0.0001031908237747654,
      "loss": 0.2855,
      "step": 2326
    },
    {
      "epoch": 3.8783333333333334,
      "grad_norm": 0.06403025984764099,
      "learning_rate": 0.00010314911366006257,
      "loss": 0.291,
      "step": 2327
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.049724698066711426,
      "learning_rate": 0.00010310740354535975,
      "loss": 0.3015,
      "step": 2328
    },
    {
      "epoch": 3.881666666666667,
      "grad_norm": 0.0452878437936306,
      "learning_rate": 0.00010306569343065693,
      "loss": 0.2784,
      "step": 2329
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 0.04992583766579628,
      "learning_rate": 0.00010302398331595413,
      "loss": 0.3562,
      "step": 2330
    },
    {
      "epoch": 3.885,
      "grad_norm": 0.04983311519026756,
      "learning_rate": 0.0001029822732012513,
      "loss": 0.325,
      "step": 2331
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.055268850177526474,
      "learning_rate": 0.00010294056308654849,
      "loss": 0.3124,
      "step": 2332
    },
    {
      "epoch": 3.888333333333333,
      "grad_norm": 0.05502535402774811,
      "learning_rate": 0.00010289885297184568,
      "loss": 0.309,
      "step": 2333
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.046393197029829025,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.277,
      "step": 2334
    },
    {
      "epoch": 3.8916666666666666,
      "grad_norm": 0.04674430936574936,
      "learning_rate": 0.00010281543274244005,
      "loss": 0.3026,
      "step": 2335
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.07257287949323654,
      "learning_rate": 0.00010277372262773723,
      "loss": 0.4384,
      "step": 2336
    },
    {
      "epoch": 3.895,
      "grad_norm": 0.05806434527039528,
      "learning_rate": 0.00010273201251303441,
      "loss": 0.3384,
      "step": 2337
    },
    {
      "epoch": 3.8966666666666665,
      "grad_norm": 0.05612991377711296,
      "learning_rate": 0.00010269030239833159,
      "loss": 0.3309,
      "step": 2338
    },
    {
      "epoch": 3.8983333333333334,
      "grad_norm": 0.056598689407110214,
      "learning_rate": 0.00010264859228362877,
      "loss": 0.3662,
      "step": 2339
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.03418118879199028,
      "learning_rate": 0.00010260688216892598,
      "loss": 0.2731,
      "step": 2340
    },
    {
      "epoch": 3.9016666666666664,
      "grad_norm": 0.06554391235113144,
      "learning_rate": 0.00010256517205422315,
      "loss": 0.4452,
      "step": 2341
    },
    {
      "epoch": 3.9033333333333333,
      "grad_norm": 0.09145708382129669,
      "learning_rate": 0.00010252346193952033,
      "loss": 0.362,
      "step": 2342
    },
    {
      "epoch": 3.9050000000000002,
      "grad_norm": 0.043819449841976166,
      "learning_rate": 0.00010248175182481751,
      "loss": 0.3236,
      "step": 2343
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.06955955922603607,
      "learning_rate": 0.0001024400417101147,
      "loss": 0.3527,
      "step": 2344
    },
    {
      "epoch": 3.908333333333333,
      "grad_norm": 0.04356991872191429,
      "learning_rate": 0.0001023983315954119,
      "loss": 0.2925,
      "step": 2345
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.04871281981468201,
      "learning_rate": 0.00010235662148070908,
      "loss": 0.3014,
      "step": 2346
    },
    {
      "epoch": 3.9116666666666666,
      "grad_norm": 0.04312913492321968,
      "learning_rate": 0.00010231491136600626,
      "loss": 0.2885,
      "step": 2347
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.05554298684000969,
      "learning_rate": 0.00010227320125130344,
      "loss": 0.3666,
      "step": 2348
    },
    {
      "epoch": 3.915,
      "grad_norm": 0.07823044061660767,
      "learning_rate": 0.00010223149113660062,
      "loss": 0.4211,
      "step": 2349
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 0.050566527992486954,
      "learning_rate": 0.00010218978102189782,
      "loss": 0.3452,
      "step": 2350
    },
    {
      "epoch": 3.9183333333333334,
      "grad_norm": 0.044750455766916275,
      "learning_rate": 0.000102148070907195,
      "loss": 0.3381,
      "step": 2351
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.056460753083229065,
      "learning_rate": 0.00010210636079249218,
      "loss": 0.3561,
      "step": 2352
    },
    {
      "epoch": 3.921666666666667,
      "grad_norm": 0.04660826176404953,
      "learning_rate": 0.00010206465067778936,
      "loss": 0.3083,
      "step": 2353
    },
    {
      "epoch": 3.9233333333333333,
      "grad_norm": 0.040229372680187225,
      "learning_rate": 0.00010202294056308654,
      "loss": 0.2526,
      "step": 2354
    },
    {
      "epoch": 3.925,
      "grad_norm": 0.0633140504360199,
      "learning_rate": 0.00010198123044838375,
      "loss": 0.3818,
      "step": 2355
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.07739020884037018,
      "learning_rate": 0.00010193952033368093,
      "loss": 0.3883,
      "step": 2356
    },
    {
      "epoch": 3.9283333333333332,
      "grad_norm": 0.044805124402046204,
      "learning_rate": 0.00010189781021897811,
      "loss": 0.2617,
      "step": 2357
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.0515134334564209,
      "learning_rate": 0.00010185610010427529,
      "loss": 0.3481,
      "step": 2358
    },
    {
      "epoch": 3.9316666666666666,
      "grad_norm": 0.0641586035490036,
      "learning_rate": 0.00010181438998957247,
      "loss": 0.3187,
      "step": 2359
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.06591623276472092,
      "learning_rate": 0.00010177267987486967,
      "loss": 0.3426,
      "step": 2360
    },
    {
      "epoch": 3.935,
      "grad_norm": 0.04467643052339554,
      "learning_rate": 0.00010173096976016685,
      "loss": 0.2533,
      "step": 2361
    },
    {
      "epoch": 3.9366666666666665,
      "grad_norm": 0.04480191320180893,
      "learning_rate": 0.00010168925964546403,
      "loss": 0.3086,
      "step": 2362
    },
    {
      "epoch": 3.9383333333333335,
      "grad_norm": 0.05488067492842674,
      "learning_rate": 0.00010164754953076121,
      "loss": 0.3941,
      "step": 2363
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.05030657351016998,
      "learning_rate": 0.00010160583941605839,
      "loss": 0.3291,
      "step": 2364
    },
    {
      "epoch": 3.9416666666666664,
      "grad_norm": 0.04196828603744507,
      "learning_rate": 0.0001015641293013556,
      "loss": 0.2432,
      "step": 2365
    },
    {
      "epoch": 3.9433333333333334,
      "grad_norm": 0.0445074699819088,
      "learning_rate": 0.00010152241918665278,
      "loss": 0.282,
      "step": 2366
    },
    {
      "epoch": 3.945,
      "grad_norm": 0.048986196517944336,
      "learning_rate": 0.00010148070907194996,
      "loss": 0.2892,
      "step": 2367
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.04347165301442146,
      "learning_rate": 0.00010143899895724714,
      "loss": 0.25,
      "step": 2368
    },
    {
      "epoch": 3.9483333333333333,
      "grad_norm": 0.04445802792906761,
      "learning_rate": 0.00010139728884254432,
      "loss": 0.2632,
      "step": 2369
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.03916536271572113,
      "learning_rate": 0.00010135557872784152,
      "loss": 0.2521,
      "step": 2370
    },
    {
      "epoch": 3.9516666666666667,
      "grad_norm": 0.04064390808343887,
      "learning_rate": 0.0001013138686131387,
      "loss": 0.2887,
      "step": 2371
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.04454411193728447,
      "learning_rate": 0.00010127215849843588,
      "loss": 0.3071,
      "step": 2372
    },
    {
      "epoch": 3.955,
      "grad_norm": 0.047475770115852356,
      "learning_rate": 0.00010123044838373306,
      "loss": 0.3023,
      "step": 2373
    },
    {
      "epoch": 3.9566666666666666,
      "grad_norm": 0.04481203854084015,
      "learning_rate": 0.00010118873826903024,
      "loss": 0.2687,
      "step": 2374
    },
    {
      "epoch": 3.9583333333333335,
      "grad_norm": 0.05212444067001343,
      "learning_rate": 0.00010114702815432743,
      "loss": 0.2838,
      "step": 2375
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.04006592929363251,
      "learning_rate": 0.00010110531803962461,
      "loss": 0.2686,
      "step": 2376
    },
    {
      "epoch": 3.961666666666667,
      "grad_norm": 0.05069146677851677,
      "learning_rate": 0.0001010636079249218,
      "loss": 0.3178,
      "step": 2377
    },
    {
      "epoch": 3.9633333333333334,
      "grad_norm": 0.0683351531624794,
      "learning_rate": 0.00010102189781021899,
      "loss": 0.3583,
      "step": 2378
    },
    {
      "epoch": 3.965,
      "grad_norm": 0.038313593715429306,
      "learning_rate": 0.00010098018769551616,
      "loss": 0.2343,
      "step": 2379
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.04579814150929451,
      "learning_rate": 0.00010093847758081336,
      "loss": 0.2841,
      "step": 2380
    },
    {
      "epoch": 3.9683333333333333,
      "grad_norm": 0.05817966163158417,
      "learning_rate": 0.00010089676746611054,
      "loss": 0.3192,
      "step": 2381
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 0.04519185051321983,
      "learning_rate": 0.00010085505735140772,
      "loss": 0.291,
      "step": 2382
    },
    {
      "epoch": 3.9716666666666667,
      "grad_norm": 0.05975272133946419,
      "learning_rate": 0.0001008133472367049,
      "loss": 0.3692,
      "step": 2383
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.0488375648856163,
      "learning_rate": 0.00010077163712200209,
      "loss": 0.3076,
      "step": 2384
    },
    {
      "epoch": 3.975,
      "grad_norm": 0.06930559873580933,
      "learning_rate": 0.00010072992700729928,
      "loss": 0.3014,
      "step": 2385
    },
    {
      "epoch": 3.9766666666666666,
      "grad_norm": 0.05154439061880112,
      "learning_rate": 0.00010068821689259646,
      "loss": 0.3321,
      "step": 2386
    },
    {
      "epoch": 3.9783333333333335,
      "grad_norm": 0.038412950932979584,
      "learning_rate": 0.00010064650677789364,
      "loss": 0.2312,
      "step": 2387
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.09518177062273026,
      "learning_rate": 0.00010060479666319082,
      "loss": 0.3891,
      "step": 2388
    },
    {
      "epoch": 3.9816666666666665,
      "grad_norm": 0.0484108068048954,
      "learning_rate": 0.000100563086548488,
      "loss": 0.3159,
      "step": 2389
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 0.03513121232390404,
      "learning_rate": 0.00010052137643378521,
      "loss": 0.2584,
      "step": 2390
    },
    {
      "epoch": 3.985,
      "grad_norm": 0.04078667238354683,
      "learning_rate": 0.00010047966631908239,
      "loss": 0.2872,
      "step": 2391
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.04308268427848816,
      "learning_rate": 0.00010043795620437957,
      "loss": 0.2853,
      "step": 2392
    },
    {
      "epoch": 3.9883333333333333,
      "grad_norm": 0.04402514547109604,
      "learning_rate": 0.00010039624608967675,
      "loss": 0.2827,
      "step": 2393
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.05570514127612114,
      "learning_rate": 0.00010035453597497392,
      "loss": 0.3429,
      "step": 2394
    },
    {
      "epoch": 3.9916666666666667,
      "grad_norm": 0.03750825300812721,
      "learning_rate": 0.00010031282586027113,
      "loss": 0.2693,
      "step": 2395
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.037781305611133575,
      "learning_rate": 0.00010027111574556831,
      "loss": 0.2793,
      "step": 2396
    },
    {
      "epoch": 3.995,
      "grad_norm": 0.08734200149774551,
      "learning_rate": 0.00010022940563086549,
      "loss": 0.3803,
      "step": 2397
    },
    {
      "epoch": 3.9966666666666666,
      "grad_norm": 0.03990530967712402,
      "learning_rate": 0.00010018769551616267,
      "loss": 0.2993,
      "step": 2398
    },
    {
      "epoch": 3.998333333333333,
      "grad_norm": 0.061058126389980316,
      "learning_rate": 0.00010014598540145985,
      "loss": 0.3431,
      "step": 2399
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.055897388607263565,
      "learning_rate": 0.00010010427528675706,
      "loss": 0.2827,
      "step": 2400
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.3189292252063751,
      "eval_runtime": 371.4884,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 2400
    },
    {
      "epoch": 4.001666666666667,
      "grad_norm": 0.04641279950737953,
      "learning_rate": 0.00010006256517205424,
      "loss": 0.2882,
      "step": 2401
    },
    {
      "epoch": 4.003333333333333,
      "grad_norm": 0.06778828054666519,
      "learning_rate": 0.00010002085505735142,
      "loss": 0.2774,
      "step": 2402
    },
    {
      "epoch": 4.005,
      "grad_norm": 0.09870221465826035,
      "learning_rate": 9.99791449426486e-05,
      "loss": 0.3824,
      "step": 2403
    },
    {
      "epoch": 4.006666666666667,
      "grad_norm": 0.06262515485286713,
      "learning_rate": 9.993743482794579e-05,
      "loss": 0.3465,
      "step": 2404
    },
    {
      "epoch": 4.008333333333334,
      "grad_norm": 0.05276002734899521,
      "learning_rate": 9.989572471324297e-05,
      "loss": 0.3539,
      "step": 2405
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.047447457909584045,
      "learning_rate": 9.985401459854015e-05,
      "loss": 0.3235,
      "step": 2406
    },
    {
      "epoch": 4.011666666666667,
      "grad_norm": 0.06287874281406403,
      "learning_rate": 9.981230448383734e-05,
      "loss": 0.2865,
      "step": 2407
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.0438864529132843,
      "learning_rate": 9.977059436913452e-05,
      "loss": 0.268,
      "step": 2408
    },
    {
      "epoch": 4.015,
      "grad_norm": 0.09272687882184982,
      "learning_rate": 9.972888425443171e-05,
      "loss": 0.2784,
      "step": 2409
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 0.04286789149045944,
      "learning_rate": 9.968717413972889e-05,
      "loss": 0.3106,
      "step": 2410
    },
    {
      "epoch": 4.0183333333333335,
      "grad_norm": 0.06949499249458313,
      "learning_rate": 9.964546402502607e-05,
      "loss": 0.3202,
      "step": 2411
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.07772856950759888,
      "learning_rate": 9.960375391032326e-05,
      "loss": 0.3075,
      "step": 2412
    },
    {
      "epoch": 4.0216666666666665,
      "grad_norm": 0.06306818127632141,
      "learning_rate": 9.956204379562044e-05,
      "loss": 0.3603,
      "step": 2413
    },
    {
      "epoch": 4.023333333333333,
      "grad_norm": 0.03443410247564316,
      "learning_rate": 9.952033368091764e-05,
      "loss": 0.2526,
      "step": 2414
    },
    {
      "epoch": 4.025,
      "grad_norm": 0.04401957243680954,
      "learning_rate": 9.947862356621482e-05,
      "loss": 0.2688,
      "step": 2415
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.04187379032373428,
      "learning_rate": 9.9436913451512e-05,
      "loss": 0.2493,
      "step": 2416
    },
    {
      "epoch": 4.028333333333333,
      "grad_norm": 0.06177285313606262,
      "learning_rate": 9.939520333680919e-05,
      "loss": 0.315,
      "step": 2417
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.057363852858543396,
      "learning_rate": 9.935349322210637e-05,
      "loss": 0.2841,
      "step": 2418
    },
    {
      "epoch": 4.031666666666666,
      "grad_norm": 0.04558102786540985,
      "learning_rate": 9.931178310740356e-05,
      "loss": 0.2941,
      "step": 2419
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.06927161663770676,
      "learning_rate": 9.927007299270074e-05,
      "loss": 0.3546,
      "step": 2420
    },
    {
      "epoch": 4.035,
      "grad_norm": 0.049558158963918686,
      "learning_rate": 9.922836287799792e-05,
      "loss": 0.1888,
      "step": 2421
    },
    {
      "epoch": 4.036666666666667,
      "grad_norm": 0.043817415833473206,
      "learning_rate": 9.918665276329511e-05,
      "loss": 0.299,
      "step": 2422
    },
    {
      "epoch": 4.038333333333333,
      "grad_norm": 0.040426645427942276,
      "learning_rate": 9.914494264859229e-05,
      "loss": 0.2352,
      "step": 2423
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.04554753750562668,
      "learning_rate": 9.910323253388947e-05,
      "loss": 0.2211,
      "step": 2424
    },
    {
      "epoch": 4.041666666666667,
      "grad_norm": 0.03539055213332176,
      "learning_rate": 9.906152241918667e-05,
      "loss": 0.2536,
      "step": 2425
    },
    {
      "epoch": 4.043333333333333,
      "grad_norm": 0.09560571610927582,
      "learning_rate": 9.901981230448384e-05,
      "loss": 0.3756,
      "step": 2426
    },
    {
      "epoch": 4.045,
      "grad_norm": 0.046732187271118164,
      "learning_rate": 9.897810218978102e-05,
      "loss": 0.269,
      "step": 2427
    },
    {
      "epoch": 4.046666666666667,
      "grad_norm": 0.08620501309633255,
      "learning_rate": 9.89363920750782e-05,
      "loss": 0.4011,
      "step": 2428
    },
    {
      "epoch": 4.048333333333333,
      "grad_norm": 0.05973418429493904,
      "learning_rate": 9.88946819603754e-05,
      "loss": 0.3171,
      "step": 2429
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.05621645599603653,
      "learning_rate": 9.885297184567258e-05,
      "loss": 0.3069,
      "step": 2430
    },
    {
      "epoch": 4.051666666666667,
      "grad_norm": 0.04713068902492523,
      "learning_rate": 9.881126173096976e-05,
      "loss": 0.2582,
      "step": 2431
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.06950278580188751,
      "learning_rate": 9.876955161626695e-05,
      "loss": 0.3432,
      "step": 2432
    },
    {
      "epoch": 4.055,
      "grad_norm": 0.04181160777807236,
      "learning_rate": 9.872784150156413e-05,
      "loss": 0.2493,
      "step": 2433
    },
    {
      "epoch": 4.056666666666667,
      "grad_norm": 0.044406820088624954,
      "learning_rate": 9.868613138686131e-05,
      "loss": 0.2802,
      "step": 2434
    },
    {
      "epoch": 4.058333333333334,
      "grad_norm": 0.046479739248752594,
      "learning_rate": 9.86444212721585e-05,
      "loss": 0.3043,
      "step": 2435
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.03750741481781006,
      "learning_rate": 9.860271115745568e-05,
      "loss": 0.2555,
      "step": 2436
    },
    {
      "epoch": 4.0616666666666665,
      "grad_norm": 0.04632921889424324,
      "learning_rate": 9.856100104275287e-05,
      "loss": 0.2446,
      "step": 2437
    },
    {
      "epoch": 4.0633333333333335,
      "grad_norm": 0.04049818590283394,
      "learning_rate": 9.851929092805005e-05,
      "loss": 0.2627,
      "step": 2438
    },
    {
      "epoch": 4.065,
      "grad_norm": 0.041338011622428894,
      "learning_rate": 9.847758081334723e-05,
      "loss": 0.2662,
      "step": 2439
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.05473647639155388,
      "learning_rate": 9.843587069864443e-05,
      "loss": 0.2987,
      "step": 2440
    },
    {
      "epoch": 4.068333333333333,
      "grad_norm": 0.03820285201072693,
      "learning_rate": 9.83941605839416e-05,
      "loss": 0.2532,
      "step": 2441
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.06214878708124161,
      "learning_rate": 9.83524504692388e-05,
      "loss": 0.285,
      "step": 2442
    },
    {
      "epoch": 4.071666666666666,
      "grad_norm": 0.04857408627867699,
      "learning_rate": 9.831074035453598e-05,
      "loss": 0.3351,
      "step": 2443
    },
    {
      "epoch": 4.073333333333333,
      "grad_norm": 0.05877280235290527,
      "learning_rate": 9.826903023983316e-05,
      "loss": 0.3214,
      "step": 2444
    },
    {
      "epoch": 4.075,
      "grad_norm": 0.07276508957147598,
      "learning_rate": 9.822732012513035e-05,
      "loss": 0.2995,
      "step": 2445
    },
    {
      "epoch": 4.076666666666666,
      "grad_norm": 0.10472798347473145,
      "learning_rate": 9.818561001042753e-05,
      "loss": 0.3106,
      "step": 2446
    },
    {
      "epoch": 4.078333333333333,
      "grad_norm": 0.05950162559747696,
      "learning_rate": 9.814389989572472e-05,
      "loss": 0.2969,
      "step": 2447
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.06080368906259537,
      "learning_rate": 9.81021897810219e-05,
      "loss": 0.3362,
      "step": 2448
    },
    {
      "epoch": 4.081666666666667,
      "grad_norm": 0.048201221972703934,
      "learning_rate": 9.806047966631908e-05,
      "loss": 0.2838,
      "step": 2449
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 0.045850418508052826,
      "learning_rate": 9.801876955161627e-05,
      "loss": 0.2876,
      "step": 2450
    },
    {
      "epoch": 4.085,
      "grad_norm": 0.056623995304107666,
      "learning_rate": 9.797705943691345e-05,
      "loss": 0.3695,
      "step": 2451
    },
    {
      "epoch": 4.086666666666667,
      "grad_norm": 0.051460787653923035,
      "learning_rate": 9.793534932221065e-05,
      "loss": 0.326,
      "step": 2452
    },
    {
      "epoch": 4.088333333333333,
      "grad_norm": 0.08597369492053986,
      "learning_rate": 9.789363920750783e-05,
      "loss": 0.3675,
      "step": 2453
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.0476987287402153,
      "learning_rate": 9.7851929092805e-05,
      "loss": 0.2544,
      "step": 2454
    },
    {
      "epoch": 4.091666666666667,
      "grad_norm": 0.05855461582541466,
      "learning_rate": 9.78102189781022e-05,
      "loss": 0.3236,
      "step": 2455
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.08181371539831161,
      "learning_rate": 9.776850886339938e-05,
      "loss": 0.2761,
      "step": 2456
    },
    {
      "epoch": 4.095,
      "grad_norm": 0.06922798603773117,
      "learning_rate": 9.772679874869657e-05,
      "loss": 0.2732,
      "step": 2457
    },
    {
      "epoch": 4.096666666666667,
      "grad_norm": 0.04699316993355751,
      "learning_rate": 9.768508863399375e-05,
      "loss": 0.2595,
      "step": 2458
    },
    {
      "epoch": 4.098333333333334,
      "grad_norm": 0.09747323393821716,
      "learning_rate": 9.764337851929093e-05,
      "loss": 0.3334,
      "step": 2459
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.040546394884586334,
      "learning_rate": 9.760166840458812e-05,
      "loss": 0.2356,
      "step": 2460
    },
    {
      "epoch": 4.101666666666667,
      "grad_norm": 0.05920076742768288,
      "learning_rate": 9.75599582898853e-05,
      "loss": 0.3013,
      "step": 2461
    },
    {
      "epoch": 4.1033333333333335,
      "grad_norm": 0.05077013373374939,
      "learning_rate": 9.75182481751825e-05,
      "loss": 0.2983,
      "step": 2462
    },
    {
      "epoch": 4.105,
      "grad_norm": 0.04920763894915581,
      "learning_rate": 9.747653806047968e-05,
      "loss": 0.2782,
      "step": 2463
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.06436929851770401,
      "learning_rate": 9.743482794577685e-05,
      "loss": 0.3273,
      "step": 2464
    },
    {
      "epoch": 4.108333333333333,
      "grad_norm": 0.045244064182043076,
      "learning_rate": 9.739311783107405e-05,
      "loss": 0.3121,
      "step": 2465
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.06308537721633911,
      "learning_rate": 9.735140771637123e-05,
      "loss": 0.25,
      "step": 2466
    },
    {
      "epoch": 4.111666666666666,
      "grad_norm": 0.06488551944494247,
      "learning_rate": 9.730969760166842e-05,
      "loss": 0.3544,
      "step": 2467
    },
    {
      "epoch": 4.113333333333333,
      "grad_norm": 0.05907280743122101,
      "learning_rate": 9.72679874869656e-05,
      "loss": 0.3734,
      "step": 2468
    },
    {
      "epoch": 4.115,
      "grad_norm": 0.08117642253637314,
      "learning_rate": 9.722627737226278e-05,
      "loss": 0.3962,
      "step": 2469
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 0.08047230541706085,
      "learning_rate": 9.718456725755997e-05,
      "loss": 0.3227,
      "step": 2470
    },
    {
      "epoch": 4.118333333333333,
      "grad_norm": 0.041312430053949356,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.268,
      "step": 2471
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.06841757148504257,
      "learning_rate": 9.710114702815433e-05,
      "loss": 0.3548,
      "step": 2472
    },
    {
      "epoch": 4.121666666666667,
      "grad_norm": 0.06200557202100754,
      "learning_rate": 9.705943691345152e-05,
      "loss": 0.3254,
      "step": 2473
    },
    {
      "epoch": 4.123333333333333,
      "grad_norm": 0.05345018580555916,
      "learning_rate": 9.70177267987487e-05,
      "loss": 0.286,
      "step": 2474
    },
    {
      "epoch": 4.125,
      "grad_norm": 0.06063448265194893,
      "learning_rate": 9.697601668404588e-05,
      "loss": 0.3032,
      "step": 2475
    },
    {
      "epoch": 4.126666666666667,
      "grad_norm": 0.04740395396947861,
      "learning_rate": 9.693430656934308e-05,
      "loss": 0.281,
      "step": 2476
    },
    {
      "epoch": 4.128333333333333,
      "grad_norm": 0.04164835810661316,
      "learning_rate": 9.689259645464026e-05,
      "loss": 0.2783,
      "step": 2477
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.05239054188132286,
      "learning_rate": 9.685088633993744e-05,
      "loss": 0.3338,
      "step": 2478
    },
    {
      "epoch": 4.131666666666667,
      "grad_norm": 0.042433541268110275,
      "learning_rate": 9.680917622523461e-05,
      "loss": 0.3112,
      "step": 2479
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.04118385165929794,
      "learning_rate": 9.676746611053181e-05,
      "loss": 0.3198,
      "step": 2480
    },
    {
      "epoch": 4.135,
      "grad_norm": 0.06629691272974014,
      "learning_rate": 9.672575599582899e-05,
      "loss": 0.3727,
      "step": 2481
    },
    {
      "epoch": 4.136666666666667,
      "grad_norm": 0.06316682696342468,
      "learning_rate": 9.668404588112617e-05,
      "loss": 0.3737,
      "step": 2482
    },
    {
      "epoch": 4.138333333333334,
      "grad_norm": 0.05740107223391533,
      "learning_rate": 9.664233576642336e-05,
      "loss": 0.3077,
      "step": 2483
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.053177472203969955,
      "learning_rate": 9.660062565172054e-05,
      "loss": 0.3609,
      "step": 2484
    },
    {
      "epoch": 4.141666666666667,
      "grad_norm": 0.050215888768434525,
      "learning_rate": 9.655891553701773e-05,
      "loss": 0.2888,
      "step": 2485
    },
    {
      "epoch": 4.1433333333333335,
      "grad_norm": 0.05358397960662842,
      "learning_rate": 9.651720542231491e-05,
      "loss": 0.3507,
      "step": 2486
    },
    {
      "epoch": 4.145,
      "grad_norm": 0.04788534343242645,
      "learning_rate": 9.647549530761209e-05,
      "loss": 0.311,
      "step": 2487
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.03945907577872276,
      "learning_rate": 9.643378519290928e-05,
      "loss": 0.2811,
      "step": 2488
    },
    {
      "epoch": 4.148333333333333,
      "grad_norm": 0.04892808571457863,
      "learning_rate": 9.639207507820646e-05,
      "loss": 0.3123,
      "step": 2489
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.045344799757003784,
      "learning_rate": 9.635036496350366e-05,
      "loss": 0.2946,
      "step": 2490
    },
    {
      "epoch": 4.151666666666666,
      "grad_norm": 0.043453969061374664,
      "learning_rate": 9.630865484880084e-05,
      "loss": 0.2575,
      "step": 2491
    },
    {
      "epoch": 4.153333333333333,
      "grad_norm": 0.04033798351883888,
      "learning_rate": 9.626694473409802e-05,
      "loss": 0.2635,
      "step": 2492
    },
    {
      "epoch": 4.155,
      "grad_norm": 0.06940322369337082,
      "learning_rate": 9.622523461939521e-05,
      "loss": 0.2906,
      "step": 2493
    },
    {
      "epoch": 4.156666666666666,
      "grad_norm": 0.05552423745393753,
      "learning_rate": 9.618352450469239e-05,
      "loss": 0.3295,
      "step": 2494
    },
    {
      "epoch": 4.158333333333333,
      "grad_norm": 0.04003457725048065,
      "learning_rate": 9.614181438998958e-05,
      "loss": 0.2642,
      "step": 2495
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.08230942487716675,
      "learning_rate": 9.610010427528676e-05,
      "loss": 0.3462,
      "step": 2496
    },
    {
      "epoch": 4.161666666666667,
      "grad_norm": 0.04453655704855919,
      "learning_rate": 9.605839416058394e-05,
      "loss": 0.2992,
      "step": 2497
    },
    {
      "epoch": 4.163333333333333,
      "grad_norm": 0.06085381284356117,
      "learning_rate": 9.601668404588113e-05,
      "loss": 0.3713,
      "step": 2498
    },
    {
      "epoch": 4.165,
      "grad_norm": 0.032147035002708435,
      "learning_rate": 9.597497393117831e-05,
      "loss": 0.2451,
      "step": 2499
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.06203083693981171,
      "learning_rate": 9.59332638164755e-05,
      "loss": 0.2648,
      "step": 2500
    },
    {
      "epoch": 4.166666666666667,
      "eval_loss": 0.31938040256500244,
      "eval_runtime": 371.5312,
      "eval_samples_per_second": 0.269,
      "eval_steps_per_second": 0.269,
      "step": 2500
    }
  ],
  "logging_steps": 1,
  "max_steps": 4800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 100,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.223283684286005e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
