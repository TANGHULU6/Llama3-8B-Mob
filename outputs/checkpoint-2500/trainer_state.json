{
  "best_metric": 0.35208961367607117,
  "best_model_checkpoint": "outputs/checkpoint-2500",
  "epoch": 0.7352941176470589,
  "eval_steps": 100,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002941176470588235,
      "grad_norm": 2.183479070663452,
      "learning_rate": 4e-05,
      "loss": 2.0034,
      "step": 1
    },
    {
      "epoch": 0.000588235294117647,
      "grad_norm": 2.10685658454895,
      "learning_rate": 8e-05,
      "loss": 2.1112,
      "step": 2
    },
    {
      "epoch": 0.0008823529411764706,
      "grad_norm": 2.0220677852630615,
      "learning_rate": 0.00012,
      "loss": 2.0415,
      "step": 3
    },
    {
      "epoch": 0.001176470588235294,
      "grad_norm": 1.8765255212783813,
      "learning_rate": 0.00016,
      "loss": 2.0435,
      "step": 4
    },
    {
      "epoch": 0.0014705882352941176,
      "grad_norm": 1.5902093648910522,
      "learning_rate": 0.0002,
      "loss": 1.6744,
      "step": 5
    },
    {
      "epoch": 0.0017647058823529412,
      "grad_norm": 2.2842509746551514,
      "learning_rate": 0.00019994108983799707,
      "loss": 1.2759,
      "step": 6
    },
    {
      "epoch": 0.002058823529411765,
      "grad_norm": 1.3071730136871338,
      "learning_rate": 0.00019988217967599413,
      "loss": 1.2103,
      "step": 7
    },
    {
      "epoch": 0.002352941176470588,
      "grad_norm": 1.2924339771270752,
      "learning_rate": 0.00019982326951399116,
      "loss": 1.1982,
      "step": 8
    },
    {
      "epoch": 0.0026470588235294116,
      "grad_norm": 1.2307484149932861,
      "learning_rate": 0.00019976435935198822,
      "loss": 1.0957,
      "step": 9
    },
    {
      "epoch": 0.0029411764705882353,
      "grad_norm": 1.2179538011550903,
      "learning_rate": 0.00019970544918998528,
      "loss": 0.998,
      "step": 10
    },
    {
      "epoch": 0.003235294117647059,
      "grad_norm": 2.353794574737549,
      "learning_rate": 0.00019964653902798234,
      "loss": 0.7913,
      "step": 11
    },
    {
      "epoch": 0.0035294117647058825,
      "grad_norm": 1.0379599332809448,
      "learning_rate": 0.0001995876288659794,
      "loss": 0.7102,
      "step": 12
    },
    {
      "epoch": 0.003823529411764706,
      "grad_norm": 0.931348443031311,
      "learning_rate": 0.00019952871870397644,
      "loss": 0.8079,
      "step": 13
    },
    {
      "epoch": 0.00411764705882353,
      "grad_norm": 0.5651299357414246,
      "learning_rate": 0.0001994698085419735,
      "loss": 0.5726,
      "step": 14
    },
    {
      "epoch": 0.004411764705882353,
      "grad_norm": 0.4931602478027344,
      "learning_rate": 0.00019941089837997056,
      "loss": 0.59,
      "step": 15
    },
    {
      "epoch": 0.004705882352941176,
      "grad_norm": 0.48339712619781494,
      "learning_rate": 0.00019935198821796762,
      "loss": 0.6296,
      "step": 16
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.4681629240512848,
      "learning_rate": 0.00019929307805596468,
      "loss": 0.6948,
      "step": 17
    },
    {
      "epoch": 0.005294117647058823,
      "grad_norm": 0.371240496635437,
      "learning_rate": 0.0001992341678939617,
      "loss": 0.6883,
      "step": 18
    },
    {
      "epoch": 0.005588235294117647,
      "grad_norm": 0.25134366750717163,
      "learning_rate": 0.00019917525773195877,
      "loss": 0.4542,
      "step": 19
    },
    {
      "epoch": 0.0058823529411764705,
      "grad_norm": 0.27983060479164124,
      "learning_rate": 0.00019911634756995583,
      "loss": 0.5234,
      "step": 20
    },
    {
      "epoch": 0.006176470588235294,
      "grad_norm": 0.23852011561393738,
      "learning_rate": 0.0001990574374079529,
      "loss": 0.5364,
      "step": 21
    },
    {
      "epoch": 0.006470588235294118,
      "grad_norm": 0.18029136955738068,
      "learning_rate": 0.00019899852724594995,
      "loss": 0.4398,
      "step": 22
    },
    {
      "epoch": 0.006764705882352941,
      "grad_norm": 0.23648658394813538,
      "learning_rate": 0.00019893961708394698,
      "loss": 0.5708,
      "step": 23
    },
    {
      "epoch": 0.007058823529411765,
      "grad_norm": 0.1452111452817917,
      "learning_rate": 0.00019888070692194404,
      "loss": 0.4328,
      "step": 24
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 0.29196223616600037,
      "learning_rate": 0.0001988217967599411,
      "loss": 0.5203,
      "step": 25
    },
    {
      "epoch": 0.007647058823529412,
      "grad_norm": 0.20188039541244507,
      "learning_rate": 0.00019876288659793816,
      "loss": 0.5585,
      "step": 26
    },
    {
      "epoch": 0.007941176470588234,
      "grad_norm": 0.12955594062805176,
      "learning_rate": 0.00019870397643593522,
      "loss": 0.4612,
      "step": 27
    },
    {
      "epoch": 0.00823529411764706,
      "grad_norm": 0.1844465285539627,
      "learning_rate": 0.00019864506627393226,
      "loss": 0.4544,
      "step": 28
    },
    {
      "epoch": 0.008529411764705883,
      "grad_norm": 0.18567843735218048,
      "learning_rate": 0.00019858615611192932,
      "loss": 0.4765,
      "step": 29
    },
    {
      "epoch": 0.008823529411764706,
      "grad_norm": 0.17821620404720306,
      "learning_rate": 0.00019852724594992638,
      "loss": 0.3675,
      "step": 30
    },
    {
      "epoch": 0.009117647058823529,
      "grad_norm": 0.17739641666412354,
      "learning_rate": 0.00019846833578792344,
      "loss": 0.3991,
      "step": 31
    },
    {
      "epoch": 0.009411764705882352,
      "grad_norm": 0.16542911529541016,
      "learning_rate": 0.0001984094256259205,
      "loss": 0.414,
      "step": 32
    },
    {
      "epoch": 0.009705882352941177,
      "grad_norm": 0.1985095590353012,
      "learning_rate": 0.00019835051546391753,
      "loss": 0.5053,
      "step": 33
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1879136860370636,
      "learning_rate": 0.0001982916053019146,
      "loss": 0.4769,
      "step": 34
    },
    {
      "epoch": 0.010294117647058823,
      "grad_norm": 0.18819700181484222,
      "learning_rate": 0.00019823269513991165,
      "loss": 0.3618,
      "step": 35
    },
    {
      "epoch": 0.010588235294117647,
      "grad_norm": 0.12826715409755707,
      "learning_rate": 0.0001981737849779087,
      "loss": 0.4915,
      "step": 36
    },
    {
      "epoch": 0.01088235294117647,
      "grad_norm": 0.1465553343296051,
      "learning_rate": 0.00019811487481590577,
      "loss": 0.3617,
      "step": 37
    },
    {
      "epoch": 0.011176470588235295,
      "grad_norm": 0.1761365383863449,
      "learning_rate": 0.0001980559646539028,
      "loss": 0.4842,
      "step": 38
    },
    {
      "epoch": 0.011470588235294118,
      "grad_norm": 0.09384077042341232,
      "learning_rate": 0.00019799705449189987,
      "loss": 0.2294,
      "step": 39
    },
    {
      "epoch": 0.011764705882352941,
      "grad_norm": 0.11776022613048553,
      "learning_rate": 0.00019793814432989693,
      "loss": 0.3746,
      "step": 40
    },
    {
      "epoch": 0.012058823529411764,
      "grad_norm": 0.13776753842830658,
      "learning_rate": 0.00019787923416789399,
      "loss": 0.4129,
      "step": 41
    },
    {
      "epoch": 0.012352941176470587,
      "grad_norm": 0.08935388177633286,
      "learning_rate": 0.00019782032400589105,
      "loss": 0.3549,
      "step": 42
    },
    {
      "epoch": 0.012647058823529412,
      "grad_norm": 0.15252627432346344,
      "learning_rate": 0.00019776141384388808,
      "loss": 0.4398,
      "step": 43
    },
    {
      "epoch": 0.012941176470588235,
      "grad_norm": 0.10281182825565338,
      "learning_rate": 0.00019770250368188514,
      "loss": 0.3606,
      "step": 44
    },
    {
      "epoch": 0.013235294117647059,
      "grad_norm": 0.09776261448860168,
      "learning_rate": 0.0001976435935198822,
      "loss": 0.4772,
      "step": 45
    },
    {
      "epoch": 0.013529411764705882,
      "grad_norm": 0.10491859167814255,
      "learning_rate": 0.00019758468335787926,
      "loss": 0.3781,
      "step": 46
    },
    {
      "epoch": 0.013823529411764707,
      "grad_norm": 0.1012224331498146,
      "learning_rate": 0.00019752577319587632,
      "loss": 0.4278,
      "step": 47
    },
    {
      "epoch": 0.01411764705882353,
      "grad_norm": 0.11251673102378845,
      "learning_rate": 0.00019746686303387335,
      "loss": 0.4952,
      "step": 48
    },
    {
      "epoch": 0.014411764705882353,
      "grad_norm": 0.08367883414030075,
      "learning_rate": 0.0001974079528718704,
      "loss": 0.4161,
      "step": 49
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 0.09331709891557693,
      "learning_rate": 0.00019734904270986747,
      "loss": 0.4508,
      "step": 50
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.08101513236761093,
      "learning_rate": 0.00019729013254786453,
      "loss": 0.3236,
      "step": 51
    },
    {
      "epoch": 0.015294117647058824,
      "grad_norm": 0.10727065801620483,
      "learning_rate": 0.0001972312223858616,
      "loss": 0.3601,
      "step": 52
    },
    {
      "epoch": 0.015588235294117648,
      "grad_norm": 0.10416439920663834,
      "learning_rate": 0.00019717231222385863,
      "loss": 0.4145,
      "step": 53
    },
    {
      "epoch": 0.01588235294117647,
      "grad_norm": 0.0757676288485527,
      "learning_rate": 0.0001971134020618557,
      "loss": 0.3368,
      "step": 54
    },
    {
      "epoch": 0.016176470588235296,
      "grad_norm": 0.09751800447702408,
      "learning_rate": 0.00019705449189985275,
      "loss": 0.4249,
      "step": 55
    },
    {
      "epoch": 0.01647058823529412,
      "grad_norm": 0.07901518046855927,
      "learning_rate": 0.0001969955817378498,
      "loss": 0.3693,
      "step": 56
    },
    {
      "epoch": 0.016764705882352942,
      "grad_norm": 0.0967986211180687,
      "learning_rate": 0.00019693667157584687,
      "loss": 0.4862,
      "step": 57
    },
    {
      "epoch": 0.017058823529411765,
      "grad_norm": 0.09867838025093079,
      "learning_rate": 0.0001968777614138439,
      "loss": 0.3648,
      "step": 58
    },
    {
      "epoch": 0.01735294117647059,
      "grad_norm": 0.06541481614112854,
      "learning_rate": 0.00019681885125184093,
      "loss": 0.3701,
      "step": 59
    },
    {
      "epoch": 0.01764705882352941,
      "grad_norm": 0.09128256142139435,
      "learning_rate": 0.000196759941089838,
      "loss": 0.4507,
      "step": 60
    },
    {
      "epoch": 0.017941176470588235,
      "grad_norm": 0.09447271376848221,
      "learning_rate": 0.00019670103092783505,
      "loss": 0.3778,
      "step": 61
    },
    {
      "epoch": 0.018235294117647058,
      "grad_norm": 0.08395741879940033,
      "learning_rate": 0.00019664212076583211,
      "loss": 0.3372,
      "step": 62
    },
    {
      "epoch": 0.01852941176470588,
      "grad_norm": 0.07432801276445389,
      "learning_rate": 0.00019658321060382915,
      "loss": 0.3504,
      "step": 63
    },
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 0.08092419803142548,
      "learning_rate": 0.0001965243004418262,
      "loss": 0.3852,
      "step": 64
    },
    {
      "epoch": 0.01911764705882353,
      "grad_norm": 0.07702334225177765,
      "learning_rate": 0.00019646539027982327,
      "loss": 0.3449,
      "step": 65
    },
    {
      "epoch": 0.019411764705882354,
      "grad_norm": 0.09544370323419571,
      "learning_rate": 0.00019640648011782033,
      "loss": 0.4421,
      "step": 66
    },
    {
      "epoch": 0.019705882352941177,
      "grad_norm": 0.079366534948349,
      "learning_rate": 0.0001963475699558174,
      "loss": 0.2845,
      "step": 67
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.08863312751054764,
      "learning_rate": 0.00019628865979381442,
      "loss": 0.3601,
      "step": 68
    },
    {
      "epoch": 0.020294117647058824,
      "grad_norm": 0.06770386546850204,
      "learning_rate": 0.00019622974963181148,
      "loss": 0.3229,
      "step": 69
    },
    {
      "epoch": 0.020588235294117647,
      "grad_norm": 0.08941573649644852,
      "learning_rate": 0.00019617083946980854,
      "loss": 0.2738,
      "step": 70
    },
    {
      "epoch": 0.02088235294117647,
      "grad_norm": 0.1214604452252388,
      "learning_rate": 0.0001961119293078056,
      "loss": 0.4371,
      "step": 71
    },
    {
      "epoch": 0.021176470588235293,
      "grad_norm": 0.07221110910177231,
      "learning_rate": 0.00019605301914580266,
      "loss": 0.3142,
      "step": 72
    },
    {
      "epoch": 0.021470588235294116,
      "grad_norm": 0.14565660059452057,
      "learning_rate": 0.0001959941089837997,
      "loss": 0.4053,
      "step": 73
    },
    {
      "epoch": 0.02176470588235294,
      "grad_norm": 0.09350073337554932,
      "learning_rate": 0.00019593519882179675,
      "loss": 0.4561,
      "step": 74
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 0.07255196571350098,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.4226,
      "step": 75
    },
    {
      "epoch": 0.02235294117647059,
      "grad_norm": 0.10246308892965317,
      "learning_rate": 0.00019581737849779087,
      "loss": 0.3564,
      "step": 76
    },
    {
      "epoch": 0.022647058823529412,
      "grad_norm": 0.09264907240867615,
      "learning_rate": 0.00019575846833578793,
      "loss": 0.3436,
      "step": 77
    },
    {
      "epoch": 0.022941176470588236,
      "grad_norm": 0.07179585099220276,
      "learning_rate": 0.00019569955817378497,
      "loss": 0.3345,
      "step": 78
    },
    {
      "epoch": 0.02323529411764706,
      "grad_norm": 0.07578697055578232,
      "learning_rate": 0.00019564064801178203,
      "loss": 0.3262,
      "step": 79
    },
    {
      "epoch": 0.023529411764705882,
      "grad_norm": 0.12121928483247757,
      "learning_rate": 0.0001955817378497791,
      "loss": 0.4263,
      "step": 80
    },
    {
      "epoch": 0.023823529411764705,
      "grad_norm": 0.09517159312963486,
      "learning_rate": 0.00019552282768777615,
      "loss": 0.3815,
      "step": 81
    },
    {
      "epoch": 0.02411764705882353,
      "grad_norm": 0.11090484261512756,
      "learning_rate": 0.0001954639175257732,
      "loss": 0.4297,
      "step": 82
    },
    {
      "epoch": 0.02441176470588235,
      "grad_norm": 0.15365923941135406,
      "learning_rate": 0.00019540500736377024,
      "loss": 0.4979,
      "step": 83
    },
    {
      "epoch": 0.024705882352941175,
      "grad_norm": 0.09497728943824768,
      "learning_rate": 0.0001953460972017673,
      "loss": 0.4349,
      "step": 84
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.08315237611532211,
      "learning_rate": 0.00019528718703976436,
      "loss": 0.2738,
      "step": 85
    },
    {
      "epoch": 0.025294117647058825,
      "grad_norm": 0.1224735751748085,
      "learning_rate": 0.00019522827687776142,
      "loss": 0.3724,
      "step": 86
    },
    {
      "epoch": 0.025588235294117648,
      "grad_norm": 0.11303207278251648,
      "learning_rate": 0.00019516936671575848,
      "loss": 0.4179,
      "step": 87
    },
    {
      "epoch": 0.02588235294117647,
      "grad_norm": 0.06620808690786362,
      "learning_rate": 0.00019511045655375552,
      "loss": 0.2997,
      "step": 88
    },
    {
      "epoch": 0.026176470588235294,
      "grad_norm": 0.07132145017385483,
      "learning_rate": 0.00019505154639175258,
      "loss": 0.3479,
      "step": 89
    },
    {
      "epoch": 0.026470588235294117,
      "grad_norm": 0.08084312826395035,
      "learning_rate": 0.00019499263622974964,
      "loss": 0.4186,
      "step": 90
    },
    {
      "epoch": 0.02676470588235294,
      "grad_norm": 0.09820722788572311,
      "learning_rate": 0.0001949337260677467,
      "loss": 0.4531,
      "step": 91
    },
    {
      "epoch": 0.027058823529411764,
      "grad_norm": 0.11209862679243088,
      "learning_rate": 0.00019487481590574376,
      "loss": 0.391,
      "step": 92
    },
    {
      "epoch": 0.027352941176470587,
      "grad_norm": 0.08833757042884827,
      "learning_rate": 0.0001948159057437408,
      "loss": 0.3612,
      "step": 93
    },
    {
      "epoch": 0.027647058823529413,
      "grad_norm": 0.07863058149814606,
      "learning_rate": 0.00019475699558173785,
      "loss": 0.3995,
      "step": 94
    },
    {
      "epoch": 0.027941176470588237,
      "grad_norm": 0.06074966490268707,
      "learning_rate": 0.0001946980854197349,
      "loss": 0.3514,
      "step": 95
    },
    {
      "epoch": 0.02823529411764706,
      "grad_norm": 0.08599737286567688,
      "learning_rate": 0.00019463917525773197,
      "loss": 0.3256,
      "step": 96
    },
    {
      "epoch": 0.028529411764705883,
      "grad_norm": 0.10413768887519836,
      "learning_rate": 0.00019458026509572903,
      "loss": 0.4315,
      "step": 97
    },
    {
      "epoch": 0.028823529411764706,
      "grad_norm": 0.08582927286624908,
      "learning_rate": 0.00019452135493372606,
      "loss": 0.4555,
      "step": 98
    },
    {
      "epoch": 0.02911764705882353,
      "grad_norm": 0.05906078964471817,
      "learning_rate": 0.00019446244477172312,
      "loss": 0.306,
      "step": 99
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.09747941046953201,
      "learning_rate": 0.00019440353460972018,
      "loss": 0.5009,
      "step": 100
    },
    {
      "epoch": 0.029411764705882353,
      "eval_loss": 0.3878893554210663,
      "eval_runtime": 214.9157,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 100
    },
    {
      "epoch": 0.029705882352941176,
      "grad_norm": 0.07448837161064148,
      "learning_rate": 0.00019434462444771724,
      "loss": 0.3754,
      "step": 101
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.0614958293735981,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.3578,
      "step": 102
    },
    {
      "epoch": 0.030294117647058822,
      "grad_norm": 0.06929472833871841,
      "learning_rate": 0.00019422680412371134,
      "loss": 0.3498,
      "step": 103
    },
    {
      "epoch": 0.03058823529411765,
      "grad_norm": 0.08569588512182236,
      "learning_rate": 0.0001941678939617084,
      "loss": 0.4205,
      "step": 104
    },
    {
      "epoch": 0.030882352941176472,
      "grad_norm": 0.0914229154586792,
      "learning_rate": 0.00019410898379970546,
      "loss": 0.38,
      "step": 105
    },
    {
      "epoch": 0.031176470588235295,
      "grad_norm": 0.07119389623403549,
      "learning_rate": 0.00019405007363770252,
      "loss": 0.4083,
      "step": 106
    },
    {
      "epoch": 0.03147058823529412,
      "grad_norm": 0.11529682576656342,
      "learning_rate": 0.00019399116347569958,
      "loss": 0.4223,
      "step": 107
    },
    {
      "epoch": 0.03176470588235294,
      "grad_norm": 0.09149236977100372,
      "learning_rate": 0.0001939322533136966,
      "loss": 0.3922,
      "step": 108
    },
    {
      "epoch": 0.032058823529411765,
      "grad_norm": 0.06520511209964752,
      "learning_rate": 0.00019387334315169367,
      "loss": 0.2844,
      "step": 109
    },
    {
      "epoch": 0.03235294117647059,
      "grad_norm": 0.10490236431360245,
      "learning_rate": 0.00019381443298969073,
      "loss": 0.3964,
      "step": 110
    },
    {
      "epoch": 0.03264705882352941,
      "grad_norm": 0.07780566066503525,
      "learning_rate": 0.0001937555228276878,
      "loss": 0.373,
      "step": 111
    },
    {
      "epoch": 0.03294117647058824,
      "grad_norm": 0.09160245209932327,
      "learning_rate": 0.00019369661266568485,
      "loss": 0.4141,
      "step": 112
    },
    {
      "epoch": 0.03323529411764706,
      "grad_norm": 0.1157829761505127,
      "learning_rate": 0.00019363770250368188,
      "loss": 0.3645,
      "step": 113
    },
    {
      "epoch": 0.033529411764705884,
      "grad_norm": 0.08872844278812408,
      "learning_rate": 0.00019357879234167894,
      "loss": 0.3769,
      "step": 114
    },
    {
      "epoch": 0.033823529411764704,
      "grad_norm": 0.09189071506261826,
      "learning_rate": 0.000193519882179676,
      "loss": 0.415,
      "step": 115
    },
    {
      "epoch": 0.03411764705882353,
      "grad_norm": 0.12543384730815887,
      "learning_rate": 0.00019346097201767306,
      "loss": 0.3818,
      "step": 116
    },
    {
      "epoch": 0.03441176470588235,
      "grad_norm": 0.09957624971866608,
      "learning_rate": 0.00019340206185567012,
      "loss": 0.4251,
      "step": 117
    },
    {
      "epoch": 0.03470588235294118,
      "grad_norm": 0.08664538711309433,
      "learning_rate": 0.00019334315169366716,
      "loss": 0.4353,
      "step": 118
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.0905040055513382,
      "learning_rate": 0.00019328424153166422,
      "loss": 0.4325,
      "step": 119
    },
    {
      "epoch": 0.03529411764705882,
      "grad_norm": 0.07393920421600342,
      "learning_rate": 0.00019322533136966128,
      "loss": 0.4034,
      "step": 120
    },
    {
      "epoch": 0.03558823529411765,
      "grad_norm": 0.10218611359596252,
      "learning_rate": 0.00019316642120765834,
      "loss": 0.3409,
      "step": 121
    },
    {
      "epoch": 0.03588235294117647,
      "grad_norm": 0.10684023052453995,
      "learning_rate": 0.0001931075110456554,
      "loss": 0.4893,
      "step": 122
    },
    {
      "epoch": 0.036176470588235296,
      "grad_norm": 0.07973593473434448,
      "learning_rate": 0.00019304860088365243,
      "loss": 0.3938,
      "step": 123
    },
    {
      "epoch": 0.036470588235294116,
      "grad_norm": 0.08641645312309265,
      "learning_rate": 0.0001929896907216495,
      "loss": 0.4374,
      "step": 124
    },
    {
      "epoch": 0.03676470588235294,
      "grad_norm": 0.07073718309402466,
      "learning_rate": 0.00019293078055964655,
      "loss": 0.3082,
      "step": 125
    },
    {
      "epoch": 0.03705882352941176,
      "grad_norm": 0.09762880951166153,
      "learning_rate": 0.0001928718703976436,
      "loss": 0.4032,
      "step": 126
    },
    {
      "epoch": 0.03735294117647059,
      "grad_norm": 0.07529620826244354,
      "learning_rate": 0.00019281296023564067,
      "loss": 0.3605,
      "step": 127
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 0.05729182809591293,
      "learning_rate": 0.0001927540500736377,
      "loss": 0.3305,
      "step": 128
    },
    {
      "epoch": 0.037941176470588235,
      "grad_norm": 0.07892298698425293,
      "learning_rate": 0.00019269513991163477,
      "loss": 0.4032,
      "step": 129
    },
    {
      "epoch": 0.03823529411764706,
      "grad_norm": 0.07929697632789612,
      "learning_rate": 0.00019263622974963183,
      "loss": 0.3707,
      "step": 130
    },
    {
      "epoch": 0.03852941176470588,
      "grad_norm": 0.06248617172241211,
      "learning_rate": 0.00019257731958762889,
      "loss": 0.324,
      "step": 131
    },
    {
      "epoch": 0.03882352941176471,
      "grad_norm": 0.09426065534353256,
      "learning_rate": 0.00019251840942562595,
      "loss": 0.4326,
      "step": 132
    },
    {
      "epoch": 0.03911764705882353,
      "grad_norm": 0.0804290696978569,
      "learning_rate": 0.00019245949926362298,
      "loss": 0.2912,
      "step": 133
    },
    {
      "epoch": 0.039411764705882354,
      "grad_norm": 0.0701073482632637,
      "learning_rate": 0.00019240058910162004,
      "loss": 0.4114,
      "step": 134
    },
    {
      "epoch": 0.039705882352941174,
      "grad_norm": 0.08479296416044235,
      "learning_rate": 0.0001923416789396171,
      "loss": 0.402,
      "step": 135
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.09177654981613159,
      "learning_rate": 0.00019228276877761416,
      "loss": 0.4206,
      "step": 136
    },
    {
      "epoch": 0.04029411764705882,
      "grad_norm": 0.0995887815952301,
      "learning_rate": 0.00019222385861561122,
      "loss": 0.3542,
      "step": 137
    },
    {
      "epoch": 0.04058823529411765,
      "grad_norm": 0.0686226338148117,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.2912,
      "step": 138
    },
    {
      "epoch": 0.040882352941176474,
      "grad_norm": 0.09901077300310135,
      "learning_rate": 0.0001921060382916053,
      "loss": 0.4222,
      "step": 139
    },
    {
      "epoch": 0.041176470588235294,
      "grad_norm": 0.06634445488452911,
      "learning_rate": 0.00019204712812960237,
      "loss": 0.3625,
      "step": 140
    },
    {
      "epoch": 0.04147058823529412,
      "grad_norm": 0.09192539751529694,
      "learning_rate": 0.00019198821796759943,
      "loss": 0.4323,
      "step": 141
    },
    {
      "epoch": 0.04176470588235294,
      "grad_norm": 0.0802953690290451,
      "learning_rate": 0.0001919293078055965,
      "loss": 0.4253,
      "step": 142
    },
    {
      "epoch": 0.04205882352941177,
      "grad_norm": 0.07491888105869293,
      "learning_rate": 0.00019187039764359353,
      "loss": 0.3895,
      "step": 143
    },
    {
      "epoch": 0.042352941176470586,
      "grad_norm": 0.09178746491670609,
      "learning_rate": 0.0001918114874815906,
      "loss": 0.412,
      "step": 144
    },
    {
      "epoch": 0.04264705882352941,
      "grad_norm": 0.08292875438928604,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.3632,
      "step": 145
    },
    {
      "epoch": 0.04294117647058823,
      "grad_norm": 0.09996288269758224,
      "learning_rate": 0.0001916936671575847,
      "loss": 0.4135,
      "step": 146
    },
    {
      "epoch": 0.04323529411764706,
      "grad_norm": 0.06329604983329773,
      "learning_rate": 0.00019163475699558177,
      "loss": 0.3524,
      "step": 147
    },
    {
      "epoch": 0.04352941176470588,
      "grad_norm": 0.08010169863700867,
      "learning_rate": 0.0001915758468335788,
      "loss": 0.3706,
      "step": 148
    },
    {
      "epoch": 0.043823529411764706,
      "grad_norm": 0.09632746875286102,
      "learning_rate": 0.00019151693667157586,
      "loss": 0.4426,
      "step": 149
    },
    {
      "epoch": 0.04411764705882353,
      "grad_norm": 0.10381979495286942,
      "learning_rate": 0.00019145802650957292,
      "loss": 0.3951,
      "step": 150
    },
    {
      "epoch": 0.04441176470588235,
      "grad_norm": 0.05651373043656349,
      "learning_rate": 0.00019139911634756998,
      "loss": 0.324,
      "step": 151
    },
    {
      "epoch": 0.04470588235294118,
      "grad_norm": 0.07263705134391785,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.3196,
      "step": 152
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.08887629210948944,
      "learning_rate": 0.00019128129602356407,
      "loss": 0.4209,
      "step": 153
    },
    {
      "epoch": 0.045294117647058825,
      "grad_norm": 0.08438466489315033,
      "learning_rate": 0.00019122238586156113,
      "loss": 0.4128,
      "step": 154
    },
    {
      "epoch": 0.045588235294117645,
      "grad_norm": 0.07210016995668411,
      "learning_rate": 0.0001911634756995582,
      "loss": 0.3699,
      "step": 155
    },
    {
      "epoch": 0.04588235294117647,
      "grad_norm": 0.05374811962246895,
      "learning_rate": 0.00019110456553755525,
      "loss": 0.3022,
      "step": 156
    },
    {
      "epoch": 0.04617647058823529,
      "grad_norm": 0.0962301567196846,
      "learning_rate": 0.00019104565537555231,
      "loss": 0.4716,
      "step": 157
    },
    {
      "epoch": 0.04647058823529412,
      "grad_norm": 0.0778447687625885,
      "learning_rate": 0.00019098674521354935,
      "loss": 0.3786,
      "step": 158
    },
    {
      "epoch": 0.046764705882352944,
      "grad_norm": 0.10694622248411179,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.422,
      "step": 159
    },
    {
      "epoch": 0.047058823529411764,
      "grad_norm": 0.06638095527887344,
      "learning_rate": 0.00019086892488954347,
      "loss": 0.2732,
      "step": 160
    },
    {
      "epoch": 0.04735294117647059,
      "grad_norm": 0.09084460139274597,
      "learning_rate": 0.00019081001472754053,
      "loss": 0.3637,
      "step": 161
    },
    {
      "epoch": 0.04764705882352941,
      "grad_norm": 0.08099769055843353,
      "learning_rate": 0.0001907511045655376,
      "loss": 0.3694,
      "step": 162
    },
    {
      "epoch": 0.04794117647058824,
      "grad_norm": 0.09244579821825027,
      "learning_rate": 0.00019069219440353462,
      "loss": 0.3655,
      "step": 163
    },
    {
      "epoch": 0.04823529411764706,
      "grad_norm": 0.09207990020513535,
      "learning_rate": 0.00019063328424153168,
      "loss": 0.3535,
      "step": 164
    },
    {
      "epoch": 0.04852941176470588,
      "grad_norm": 0.09165927767753601,
      "learning_rate": 0.00019057437407952871,
      "loss": 0.3486,
      "step": 165
    },
    {
      "epoch": 0.0488235294117647,
      "grad_norm": 0.10693888366222382,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.4196,
      "step": 166
    },
    {
      "epoch": 0.04911764705882353,
      "grad_norm": 0.08076412975788116,
      "learning_rate": 0.00019045655375552283,
      "loss": 0.3039,
      "step": 167
    },
    {
      "epoch": 0.04941176470588235,
      "grad_norm": 0.08076328784227371,
      "learning_rate": 0.00019039764359351987,
      "loss": 0.3357,
      "step": 168
    },
    {
      "epoch": 0.049705882352941176,
      "grad_norm": 0.07852445542812347,
      "learning_rate": 0.00019033873343151693,
      "loss": 0.3743,
      "step": 169
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.07005435228347778,
      "learning_rate": 0.000190279823269514,
      "loss": 0.327,
      "step": 170
    },
    {
      "epoch": 0.05029411764705882,
      "grad_norm": 0.07748187333345413,
      "learning_rate": 0.00019022091310751105,
      "loss": 0.4139,
      "step": 171
    },
    {
      "epoch": 0.05058823529411765,
      "grad_norm": 0.08679607510566711,
      "learning_rate": 0.0001901620029455081,
      "loss": 0.3866,
      "step": 172
    },
    {
      "epoch": 0.05088235294117647,
      "grad_norm": 0.07484240084886551,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.3319,
      "step": 173
    },
    {
      "epoch": 0.051176470588235295,
      "grad_norm": 0.08253172785043716,
      "learning_rate": 0.0001900441826215022,
      "loss": 0.3339,
      "step": 174
    },
    {
      "epoch": 0.051470588235294115,
      "grad_norm": 0.09325040131807327,
      "learning_rate": 0.00018998527245949926,
      "loss": 0.4256,
      "step": 175
    },
    {
      "epoch": 0.05176470588235294,
      "grad_norm": 0.08114452660083771,
      "learning_rate": 0.00018992636229749632,
      "loss": 0.3255,
      "step": 176
    },
    {
      "epoch": 0.05205882352941176,
      "grad_norm": 0.11641375720500946,
      "learning_rate": 0.00018986745213549338,
      "loss": 0.3524,
      "step": 177
    },
    {
      "epoch": 0.05235294117647059,
      "grad_norm": 0.08202917873859406,
      "learning_rate": 0.00018980854197349042,
      "loss": 0.3387,
      "step": 178
    },
    {
      "epoch": 0.052647058823529415,
      "grad_norm": 0.07000445574522018,
      "learning_rate": 0.00018974963181148748,
      "loss": 0.3619,
      "step": 179
    },
    {
      "epoch": 0.052941176470588235,
      "grad_norm": 0.10430999100208282,
      "learning_rate": 0.00018969072164948454,
      "loss": 0.3723,
      "step": 180
    },
    {
      "epoch": 0.05323529411764706,
      "grad_norm": 0.08108723163604736,
      "learning_rate": 0.0001896318114874816,
      "loss": 0.4293,
      "step": 181
    },
    {
      "epoch": 0.05352941176470588,
      "grad_norm": 0.07953565567731857,
      "learning_rate": 0.00018957290132547866,
      "loss": 0.3287,
      "step": 182
    },
    {
      "epoch": 0.05382352941176471,
      "grad_norm": 0.05877528712153435,
      "learning_rate": 0.0001895139911634757,
      "loss": 0.2636,
      "step": 183
    },
    {
      "epoch": 0.05411764705882353,
      "grad_norm": 0.0758645236492157,
      "learning_rate": 0.00018945508100147275,
      "loss": 0.367,
      "step": 184
    },
    {
      "epoch": 0.054411764705882354,
      "grad_norm": 0.07813616842031479,
      "learning_rate": 0.0001893961708394698,
      "loss": 0.3368,
      "step": 185
    },
    {
      "epoch": 0.054705882352941174,
      "grad_norm": 0.08332835137844086,
      "learning_rate": 0.00018933726067746687,
      "loss": 0.3504,
      "step": 186
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.11283726990222931,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.3934,
      "step": 187
    },
    {
      "epoch": 0.05529411764705883,
      "grad_norm": 0.059762921184301376,
      "learning_rate": 0.00018921944035346096,
      "loss": 0.3449,
      "step": 188
    },
    {
      "epoch": 0.05558823529411765,
      "grad_norm": 0.06143395975232124,
      "learning_rate": 0.00018916053019145802,
      "loss": 0.2979,
      "step": 189
    },
    {
      "epoch": 0.05588235294117647,
      "grad_norm": 0.08984668552875519,
      "learning_rate": 0.00018910162002945508,
      "loss": 0.4148,
      "step": 190
    },
    {
      "epoch": 0.05617647058823529,
      "grad_norm": 0.05973659083247185,
      "learning_rate": 0.00018904270986745214,
      "loss": 0.3196,
      "step": 191
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 0.06807804852724075,
      "learning_rate": 0.0001889837997054492,
      "loss": 0.4252,
      "step": 192
    },
    {
      "epoch": 0.05676470588235294,
      "grad_norm": 0.06614738702774048,
      "learning_rate": 0.00018892488954344624,
      "loss": 0.3819,
      "step": 193
    },
    {
      "epoch": 0.057058823529411766,
      "grad_norm": 0.07081478089094162,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.371,
      "step": 194
    },
    {
      "epoch": 0.057352941176470586,
      "grad_norm": 0.08730503916740417,
      "learning_rate": 0.00018880706921944036,
      "loss": 0.4878,
      "step": 195
    },
    {
      "epoch": 0.05764705882352941,
      "grad_norm": 0.08386127650737762,
      "learning_rate": 0.00018874815905743742,
      "loss": 0.4275,
      "step": 196
    },
    {
      "epoch": 0.05794117647058823,
      "grad_norm": 0.06872805953025818,
      "learning_rate": 0.00018868924889543448,
      "loss": 0.2907,
      "step": 197
    },
    {
      "epoch": 0.05823529411764706,
      "grad_norm": 0.07516340166330338,
      "learning_rate": 0.0001886303387334315,
      "loss": 0.3722,
      "step": 198
    },
    {
      "epoch": 0.058529411764705885,
      "grad_norm": 0.0641753226518631,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.3229,
      "step": 199
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.06973642855882645,
      "learning_rate": 0.00018851251840942563,
      "loss": 0.3445,
      "step": 200
    },
    {
      "epoch": 0.058823529411764705,
      "eval_loss": 0.3767182528972626,
      "eval_runtime": 214.4602,
      "eval_samples_per_second": 0.466,
      "eval_steps_per_second": 0.466,
      "step": 200
    },
    {
      "epoch": 0.05911764705882353,
      "grad_norm": 0.08166845887899399,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.3866,
      "step": 201
    },
    {
      "epoch": 0.05941176470588235,
      "grad_norm": 0.062053874135017395,
      "learning_rate": 0.00018839469808541975,
      "loss": 0.3172,
      "step": 202
    },
    {
      "epoch": 0.05970588235294118,
      "grad_norm": 0.061366621404886246,
      "learning_rate": 0.00018833578792341678,
      "loss": 0.3834,
      "step": 203
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.053111426532268524,
      "learning_rate": 0.00018827687776141384,
      "loss": 0.298,
      "step": 204
    },
    {
      "epoch": 0.060294117647058824,
      "grad_norm": 0.099363312125206,
      "learning_rate": 0.0001882179675994109,
      "loss": 0.3964,
      "step": 205
    },
    {
      "epoch": 0.060588235294117644,
      "grad_norm": 0.05475326254963875,
      "learning_rate": 0.00018815905743740796,
      "loss": 0.3463,
      "step": 206
    },
    {
      "epoch": 0.06088235294117647,
      "grad_norm": 0.06388437747955322,
      "learning_rate": 0.00018810014727540502,
      "loss": 0.3271,
      "step": 207
    },
    {
      "epoch": 0.0611764705882353,
      "grad_norm": 0.06116502359509468,
      "learning_rate": 0.00018804123711340206,
      "loss": 0.343,
      "step": 208
    },
    {
      "epoch": 0.06147058823529412,
      "grad_norm": 0.08131694793701172,
      "learning_rate": 0.00018798232695139912,
      "loss": 0.3793,
      "step": 209
    },
    {
      "epoch": 0.061764705882352944,
      "grad_norm": 0.06262039393186569,
      "learning_rate": 0.00018792341678939618,
      "loss": 0.3383,
      "step": 210
    },
    {
      "epoch": 0.062058823529411763,
      "grad_norm": 0.0843704417347908,
      "learning_rate": 0.00018786450662739324,
      "loss": 0.4044,
      "step": 211
    },
    {
      "epoch": 0.06235294117647059,
      "grad_norm": 0.0662040263414383,
      "learning_rate": 0.0001878055964653903,
      "loss": 0.3542,
      "step": 212
    },
    {
      "epoch": 0.06264705882352942,
      "grad_norm": 0.06551224738359451,
      "learning_rate": 0.00018774668630338733,
      "loss": 0.384,
      "step": 213
    },
    {
      "epoch": 0.06294117647058824,
      "grad_norm": 0.06674382090568542,
      "learning_rate": 0.0001876877761413844,
      "loss": 0.3074,
      "step": 214
    },
    {
      "epoch": 0.06323529411764706,
      "grad_norm": 0.08320005983114243,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.3714,
      "step": 215
    },
    {
      "epoch": 0.06352941176470588,
      "grad_norm": 0.052629657089710236,
      "learning_rate": 0.0001875699558173785,
      "loss": 0.3204,
      "step": 216
    },
    {
      "epoch": 0.06382352941176471,
      "grad_norm": 0.06657926738262177,
      "learning_rate": 0.00018751104565537557,
      "loss": 0.2796,
      "step": 217
    },
    {
      "epoch": 0.06411764705882353,
      "grad_norm": 0.06517869234085083,
      "learning_rate": 0.0001874521354933726,
      "loss": 0.3483,
      "step": 218
    },
    {
      "epoch": 0.06441176470588235,
      "grad_norm": 0.09810834378004074,
      "learning_rate": 0.00018739322533136967,
      "loss": 0.3366,
      "step": 219
    },
    {
      "epoch": 0.06470588235294118,
      "grad_norm": 0.10006564855575562,
      "learning_rate": 0.00018733431516936673,
      "loss": 0.4012,
      "step": 220
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.09300980716943741,
      "learning_rate": 0.00018727540500736379,
      "loss": 0.3651,
      "step": 221
    },
    {
      "epoch": 0.06529411764705882,
      "grad_norm": 0.06971597671508789,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.3294,
      "step": 222
    },
    {
      "epoch": 0.06558823529411764,
      "grad_norm": 0.0776345282793045,
      "learning_rate": 0.00018715758468335788,
      "loss": 0.3345,
      "step": 223
    },
    {
      "epoch": 0.06588235294117648,
      "grad_norm": 0.0927155613899231,
      "learning_rate": 0.00018709867452135494,
      "loss": 0.3162,
      "step": 224
    },
    {
      "epoch": 0.0661764705882353,
      "grad_norm": 0.08812511712312698,
      "learning_rate": 0.000187039764359352,
      "loss": 0.3909,
      "step": 225
    },
    {
      "epoch": 0.06647058823529411,
      "grad_norm": 0.07460417598485947,
      "learning_rate": 0.00018698085419734906,
      "loss": 0.3995,
      "step": 226
    },
    {
      "epoch": 0.06676470588235293,
      "grad_norm": 0.057078417390584946,
      "learning_rate": 0.00018692194403534612,
      "loss": 0.3695,
      "step": 227
    },
    {
      "epoch": 0.06705882352941177,
      "grad_norm": 0.05906906723976135,
      "learning_rate": 0.00018686303387334315,
      "loss": 0.3419,
      "step": 228
    },
    {
      "epoch": 0.06735294117647059,
      "grad_norm": 0.06583355367183685,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.2909,
      "step": 229
    },
    {
      "epoch": 0.06764705882352941,
      "grad_norm": 0.07396094501018524,
      "learning_rate": 0.00018674521354933727,
      "loss": 0.3711,
      "step": 230
    },
    {
      "epoch": 0.06794117647058824,
      "grad_norm": 0.08158345520496368,
      "learning_rate": 0.00018668630338733433,
      "loss": 0.3674,
      "step": 231
    },
    {
      "epoch": 0.06823529411764706,
      "grad_norm": 0.0752912163734436,
      "learning_rate": 0.0001866273932253314,
      "loss": 0.4318,
      "step": 232
    },
    {
      "epoch": 0.06852941176470588,
      "grad_norm": 0.10488764941692352,
      "learning_rate": 0.00018656848306332843,
      "loss": 0.3866,
      "step": 233
    },
    {
      "epoch": 0.0688235294117647,
      "grad_norm": 0.07578109204769135,
      "learning_rate": 0.0001865095729013255,
      "loss": 0.384,
      "step": 234
    },
    {
      "epoch": 0.06911764705882353,
      "grad_norm": 0.052479587495326996,
      "learning_rate": 0.00018645066273932255,
      "loss": 0.3382,
      "step": 235
    },
    {
      "epoch": 0.06941176470588235,
      "grad_norm": 0.06620831787586212,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.3808,
      "step": 236
    },
    {
      "epoch": 0.06970588235294117,
      "grad_norm": 0.07575883716344833,
      "learning_rate": 0.00018633284241531667,
      "loss": 0.3497,
      "step": 237
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0871863141655922,
      "learning_rate": 0.0001862739322533137,
      "loss": 0.4954,
      "step": 238
    },
    {
      "epoch": 0.07029411764705883,
      "grad_norm": 0.0836196318268776,
      "learning_rate": 0.00018621502209131076,
      "loss": 0.3722,
      "step": 239
    },
    {
      "epoch": 0.07058823529411765,
      "grad_norm": 0.07479219883680344,
      "learning_rate": 0.00018615611192930782,
      "loss": 0.4152,
      "step": 240
    },
    {
      "epoch": 0.07088235294117647,
      "grad_norm": 0.07516150176525116,
      "learning_rate": 0.00018609720176730488,
      "loss": 0.3162,
      "step": 241
    },
    {
      "epoch": 0.0711764705882353,
      "grad_norm": 0.06700415164232254,
      "learning_rate": 0.00018603829160530194,
      "loss": 0.3604,
      "step": 242
    },
    {
      "epoch": 0.07147058823529412,
      "grad_norm": 0.0815596655011177,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.3925,
      "step": 243
    },
    {
      "epoch": 0.07176470588235294,
      "grad_norm": 0.06797503679990768,
      "learning_rate": 0.00018592047128129603,
      "loss": 0.3939,
      "step": 244
    },
    {
      "epoch": 0.07205882352941176,
      "grad_norm": 0.08037151396274567,
      "learning_rate": 0.0001858615611192931,
      "loss": 0.363,
      "step": 245
    },
    {
      "epoch": 0.07235294117647059,
      "grad_norm": 0.10852646082639694,
      "learning_rate": 0.00018580265095729015,
      "loss": 0.4394,
      "step": 246
    },
    {
      "epoch": 0.07264705882352941,
      "grad_norm": 0.10037321597337723,
      "learning_rate": 0.00018574374079528721,
      "loss": 0.4556,
      "step": 247
    },
    {
      "epoch": 0.07294117647058823,
      "grad_norm": 0.062211133539676666,
      "learning_rate": 0.00018568483063328425,
      "loss": 0.3554,
      "step": 248
    },
    {
      "epoch": 0.07323529411764707,
      "grad_norm": 0.09802661091089249,
      "learning_rate": 0.0001856259204712813,
      "loss": 0.3845,
      "step": 249
    },
    {
      "epoch": 0.07352941176470588,
      "grad_norm": 0.10403288155794144,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.4759,
      "step": 250
    },
    {
      "epoch": 0.0738235294117647,
      "grad_norm": 0.055067576467990875,
      "learning_rate": 0.00018550810014727543,
      "loss": 0.2973,
      "step": 251
    },
    {
      "epoch": 0.07411764705882352,
      "grad_norm": 0.06348162144422531,
      "learning_rate": 0.0001854491899852725,
      "loss": 0.3869,
      "step": 252
    },
    {
      "epoch": 0.07441176470588236,
      "grad_norm": 0.06961240619421005,
      "learning_rate": 0.00018539027982326952,
      "loss": 0.3115,
      "step": 253
    },
    {
      "epoch": 0.07470588235294118,
      "grad_norm": 0.06145700812339783,
      "learning_rate": 0.00018533136966126658,
      "loss": 0.3156,
      "step": 254
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.08133311569690704,
      "learning_rate": 0.00018527245949926364,
      "loss": 0.4542,
      "step": 255
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 0.0609547384083271,
      "learning_rate": 0.0001852135493372607,
      "loss": 0.3222,
      "step": 256
    },
    {
      "epoch": 0.07558823529411765,
      "grad_norm": 0.04856938123703003,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.297,
      "step": 257
    },
    {
      "epoch": 0.07588235294117647,
      "grad_norm": 0.07120084017515182,
      "learning_rate": 0.0001850957290132548,
      "loss": 0.3647,
      "step": 258
    },
    {
      "epoch": 0.07617647058823529,
      "grad_norm": 0.08879172056913376,
      "learning_rate": 0.00018503681885125186,
      "loss": 0.3811,
      "step": 259
    },
    {
      "epoch": 0.07647058823529412,
      "grad_norm": 0.06976604461669922,
      "learning_rate": 0.00018497790868924892,
      "loss": 0.4262,
      "step": 260
    },
    {
      "epoch": 0.07676470588235294,
      "grad_norm": 0.07280811667442322,
      "learning_rate": 0.00018491899852724598,
      "loss": 0.4456,
      "step": 261
    },
    {
      "epoch": 0.07705882352941176,
      "grad_norm": 0.09368153661489487,
      "learning_rate": 0.00018486008836524304,
      "loss": 0.4654,
      "step": 262
    },
    {
      "epoch": 0.07735294117647058,
      "grad_norm": 0.05659034103155136,
      "learning_rate": 0.00018480117820324007,
      "loss": 0.3351,
      "step": 263
    },
    {
      "epoch": 0.07764705882352942,
      "grad_norm": 0.0724044144153595,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.3908,
      "step": 264
    },
    {
      "epoch": 0.07794117647058824,
      "grad_norm": 0.08915983140468597,
      "learning_rate": 0.0001846833578792342,
      "loss": 0.3803,
      "step": 265
    },
    {
      "epoch": 0.07823529411764706,
      "grad_norm": 0.07771384716033936,
      "learning_rate": 0.00018462444771723125,
      "loss": 0.4127,
      "step": 266
    },
    {
      "epoch": 0.07852941176470589,
      "grad_norm": 0.0986986979842186,
      "learning_rate": 0.0001845655375552283,
      "loss": 0.4261,
      "step": 267
    },
    {
      "epoch": 0.07882352941176471,
      "grad_norm": 0.0804448127746582,
      "learning_rate": 0.00018450662739322534,
      "loss": 0.3962,
      "step": 268
    },
    {
      "epoch": 0.07911764705882353,
      "grad_norm": 0.07400243729352951,
      "learning_rate": 0.0001844477172312224,
      "loss": 0.3748,
      "step": 269
    },
    {
      "epoch": 0.07941176470588235,
      "grad_norm": 0.09537741541862488,
      "learning_rate": 0.00018438880706921946,
      "loss": 0.3819,
      "step": 270
    },
    {
      "epoch": 0.07970588235294118,
      "grad_norm": 0.08744517713785172,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.4168,
      "step": 271
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.0631398856639862,
      "learning_rate": 0.00018427098674521356,
      "loss": 0.2908,
      "step": 272
    },
    {
      "epoch": 0.08029411764705882,
      "grad_norm": 0.07990700751543045,
      "learning_rate": 0.0001842120765832106,
      "loss": 0.325,
      "step": 273
    },
    {
      "epoch": 0.08058823529411764,
      "grad_norm": 0.09226884692907333,
      "learning_rate": 0.00018415316642120765,
      "loss": 0.3699,
      "step": 274
    },
    {
      "epoch": 0.08088235294117647,
      "grad_norm": 0.07983876019716263,
      "learning_rate": 0.0001840942562592047,
      "loss": 0.3971,
      "step": 275
    },
    {
      "epoch": 0.0811764705882353,
      "grad_norm": 0.0801749899983406,
      "learning_rate": 0.00018403534609720177,
      "loss": 0.3373,
      "step": 276
    },
    {
      "epoch": 0.08147058823529411,
      "grad_norm": 0.07723645865917206,
      "learning_rate": 0.00018397643593519883,
      "loss": 0.398,
      "step": 277
    },
    {
      "epoch": 0.08176470588235295,
      "grad_norm": 0.06696151196956635,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.3898,
      "step": 278
    },
    {
      "epoch": 0.08205882352941177,
      "grad_norm": 0.07642757147550583,
      "learning_rate": 0.00018385861561119292,
      "loss": 0.4008,
      "step": 279
    },
    {
      "epoch": 0.08235294117647059,
      "grad_norm": 0.08622708916664124,
      "learning_rate": 0.00018379970544918998,
      "loss": 0.4354,
      "step": 280
    },
    {
      "epoch": 0.0826470588235294,
      "grad_norm": 0.08928098529577255,
      "learning_rate": 0.00018374079528718704,
      "loss": 0.4553,
      "step": 281
    },
    {
      "epoch": 0.08294117647058824,
      "grad_norm": 0.09323769062757492,
      "learning_rate": 0.0001836818851251841,
      "loss": 0.4542,
      "step": 282
    },
    {
      "epoch": 0.08323529411764706,
      "grad_norm": 0.06147265434265137,
      "learning_rate": 0.00018362297496318114,
      "loss": 0.3943,
      "step": 283
    },
    {
      "epoch": 0.08352941176470588,
      "grad_norm": 0.060861315578222275,
      "learning_rate": 0.0001835640648011782,
      "loss": 0.3185,
      "step": 284
    },
    {
      "epoch": 0.0838235294117647,
      "grad_norm": 0.0663820207118988,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.3383,
      "step": 285
    },
    {
      "epoch": 0.08411764705882353,
      "grad_norm": 0.055125344544649124,
      "learning_rate": 0.00018344624447717232,
      "loss": 0.3278,
      "step": 286
    },
    {
      "epoch": 0.08441176470588235,
      "grad_norm": 0.07817421108484268,
      "learning_rate": 0.00018338733431516938,
      "loss": 0.423,
      "step": 287
    },
    {
      "epoch": 0.08470588235294117,
      "grad_norm": 0.061867907643318176,
      "learning_rate": 0.0001833284241531664,
      "loss": 0.3475,
      "step": 288
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.06432729214429855,
      "learning_rate": 0.00018326951399116347,
      "loss": 0.4151,
      "step": 289
    },
    {
      "epoch": 0.08529411764705883,
      "grad_norm": 0.08231685310602188,
      "learning_rate": 0.00018321060382916053,
      "loss": 0.4428,
      "step": 290
    },
    {
      "epoch": 0.08558823529411765,
      "grad_norm": 0.05390845611691475,
      "learning_rate": 0.0001831516936671576,
      "loss": 0.2827,
      "step": 291
    },
    {
      "epoch": 0.08588235294117647,
      "grad_norm": 0.10428059101104736,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.4806,
      "step": 292
    },
    {
      "epoch": 0.0861764705882353,
      "grad_norm": 0.06025645509362221,
      "learning_rate": 0.00018303387334315168,
      "loss": 0.3857,
      "step": 293
    },
    {
      "epoch": 0.08647058823529412,
      "grad_norm": 0.06423747539520264,
      "learning_rate": 0.00018297496318114874,
      "loss": 0.3793,
      "step": 294
    },
    {
      "epoch": 0.08676470588235294,
      "grad_norm": 0.06492647528648376,
      "learning_rate": 0.0001829160530191458,
      "loss": 0.3688,
      "step": 295
    },
    {
      "epoch": 0.08705882352941176,
      "grad_norm": 0.06362714618444443,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.4156,
      "step": 296
    },
    {
      "epoch": 0.08735294117647059,
      "grad_norm": 0.06653653830289841,
      "learning_rate": 0.00018279823269513992,
      "loss": 0.3548,
      "step": 297
    },
    {
      "epoch": 0.08764705882352941,
      "grad_norm": 0.07362311333417892,
      "learning_rate": 0.00018273932253313696,
      "loss": 0.3489,
      "step": 298
    },
    {
      "epoch": 0.08794117647058823,
      "grad_norm": 0.06643049418926239,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.3615,
      "step": 299
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.07043516635894775,
      "learning_rate": 0.00018262150220913108,
      "loss": 0.4047,
      "step": 300
    },
    {
      "epoch": 0.08823529411764706,
      "eval_loss": 0.3704615533351898,
      "eval_runtime": 214.0093,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 300
    },
    {
      "epoch": 0.08852941176470588,
      "grad_norm": 0.05111408233642578,
      "learning_rate": 0.00018256259204712814,
      "loss": 0.2938,
      "step": 301
    },
    {
      "epoch": 0.0888235294117647,
      "grad_norm": 0.06887197494506836,
      "learning_rate": 0.0001825036818851252,
      "loss": 0.39,
      "step": 302
    },
    {
      "epoch": 0.08911764705882352,
      "grad_norm": 0.0789952427148819,
      "learning_rate": 0.00018244477172312223,
      "loss": 0.3863,
      "step": 303
    },
    {
      "epoch": 0.08941176470588236,
      "grad_norm": 0.06443461030721664,
      "learning_rate": 0.0001823858615611193,
      "loss": 0.3641,
      "step": 304
    },
    {
      "epoch": 0.08970588235294118,
      "grad_norm": 0.060883525758981705,
      "learning_rate": 0.00018232695139911635,
      "loss": 0.3922,
      "step": 305
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.05561806261539459,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.3209,
      "step": 306
    },
    {
      "epoch": 0.09029411764705883,
      "grad_norm": 0.07831435650587082,
      "learning_rate": 0.00018220913107511047,
      "loss": 0.3631,
      "step": 307
    },
    {
      "epoch": 0.09058823529411765,
      "grad_norm": 0.0717497318983078,
      "learning_rate": 0.0001821502209131075,
      "loss": 0.36,
      "step": 308
    },
    {
      "epoch": 0.09088235294117647,
      "grad_norm": 0.07044396549463272,
      "learning_rate": 0.00018209131075110457,
      "loss": 0.3772,
      "step": 309
    },
    {
      "epoch": 0.09117647058823529,
      "grad_norm": 0.05202275514602661,
      "learning_rate": 0.00018203240058910163,
      "loss": 0.3335,
      "step": 310
    },
    {
      "epoch": 0.09147058823529412,
      "grad_norm": 0.07764929533004761,
      "learning_rate": 0.00018197349042709869,
      "loss": 0.408,
      "step": 311
    },
    {
      "epoch": 0.09176470588235294,
      "grad_norm": 0.06273847818374634,
      "learning_rate": 0.00018191458026509575,
      "loss": 0.357,
      "step": 312
    },
    {
      "epoch": 0.09205882352941176,
      "grad_norm": 0.0746827945113182,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.402,
      "step": 313
    },
    {
      "epoch": 0.09235294117647058,
      "grad_norm": 0.05298242345452309,
      "learning_rate": 0.00018179675994108984,
      "loss": 0.3659,
      "step": 314
    },
    {
      "epoch": 0.09264705882352942,
      "grad_norm": 0.06540913879871368,
      "learning_rate": 0.0001817378497790869,
      "loss": 0.345,
      "step": 315
    },
    {
      "epoch": 0.09294117647058824,
      "grad_norm": 0.06143077835440636,
      "learning_rate": 0.00018167893961708396,
      "loss": 0.3885,
      "step": 316
    },
    {
      "epoch": 0.09323529411764706,
      "grad_norm": 0.0803089290857315,
      "learning_rate": 0.00018162002945508102,
      "loss": 0.2972,
      "step": 317
    },
    {
      "epoch": 0.09352941176470589,
      "grad_norm": 0.056733958423137665,
      "learning_rate": 0.00018156111929307805,
      "loss": 0.3107,
      "step": 318
    },
    {
      "epoch": 0.09382352941176471,
      "grad_norm": 0.0864543467760086,
      "learning_rate": 0.0001815022091310751,
      "loss": 0.4175,
      "step": 319
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.05277983099222183,
      "learning_rate": 0.00018144329896907217,
      "loss": 0.3065,
      "step": 320
    },
    {
      "epoch": 0.09441176470588235,
      "grad_norm": 0.06177477166056633,
      "learning_rate": 0.00018138438880706923,
      "loss": 0.3307,
      "step": 321
    },
    {
      "epoch": 0.09470588235294118,
      "grad_norm": 0.0936097726225853,
      "learning_rate": 0.0001813254786450663,
      "loss": 0.444,
      "step": 322
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.06528394669294357,
      "learning_rate": 0.00018126656848306333,
      "loss": 0.3443,
      "step": 323
    },
    {
      "epoch": 0.09529411764705882,
      "grad_norm": 0.06071492284536362,
      "learning_rate": 0.00018120765832106039,
      "loss": 0.348,
      "step": 324
    },
    {
      "epoch": 0.09558823529411764,
      "grad_norm": 0.07457941025495529,
      "learning_rate": 0.00018114874815905745,
      "loss": 0.3457,
      "step": 325
    },
    {
      "epoch": 0.09588235294117647,
      "grad_norm": 0.08464271575212479,
      "learning_rate": 0.0001810898379970545,
      "loss": 0.4089,
      "step": 326
    },
    {
      "epoch": 0.0961764705882353,
      "grad_norm": 0.06049331650137901,
      "learning_rate": 0.00018103092783505157,
      "loss": 0.3713,
      "step": 327
    },
    {
      "epoch": 0.09647058823529411,
      "grad_norm": 0.06371386349201202,
      "learning_rate": 0.0001809720176730486,
      "loss": 0.3219,
      "step": 328
    },
    {
      "epoch": 0.09676470588235295,
      "grad_norm": 0.06541358679533005,
      "learning_rate": 0.00018091310751104566,
      "loss": 0.3225,
      "step": 329
    },
    {
      "epoch": 0.09705882352941177,
      "grad_norm": 0.06573724001646042,
      "learning_rate": 0.00018085419734904272,
      "loss": 0.3102,
      "step": 330
    },
    {
      "epoch": 0.09735294117647059,
      "grad_norm": 0.0672830119729042,
      "learning_rate": 0.00018079528718703978,
      "loss": 0.3864,
      "step": 331
    },
    {
      "epoch": 0.0976470588235294,
      "grad_norm": 0.0658344179391861,
      "learning_rate": 0.00018073637702503684,
      "loss": 0.3578,
      "step": 332
    },
    {
      "epoch": 0.09794117647058824,
      "grad_norm": 0.05132212117314339,
      "learning_rate": 0.00018067746686303387,
      "loss": 0.3227,
      "step": 333
    },
    {
      "epoch": 0.09823529411764706,
      "grad_norm": 0.07412703335285187,
      "learning_rate": 0.00018061855670103093,
      "loss": 0.3921,
      "step": 334
    },
    {
      "epoch": 0.09852941176470588,
      "grad_norm": 0.06318201124668121,
      "learning_rate": 0.000180559646539028,
      "loss": 0.3685,
      "step": 335
    },
    {
      "epoch": 0.0988235294117647,
      "grad_norm": 0.06001734733581543,
      "learning_rate": 0.00018050073637702505,
      "loss": 0.3365,
      "step": 336
    },
    {
      "epoch": 0.09911764705882353,
      "grad_norm": 0.06040819361805916,
      "learning_rate": 0.00018044182621502211,
      "loss": 0.404,
      "step": 337
    },
    {
      "epoch": 0.09941176470588235,
      "grad_norm": 0.06483824551105499,
      "learning_rate": 0.00018038291605301915,
      "loss": 0.3489,
      "step": 338
    },
    {
      "epoch": 0.09970588235294117,
      "grad_norm": 0.049011338502168655,
      "learning_rate": 0.0001803240058910162,
      "loss": 0.315,
      "step": 339
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.07122957706451416,
      "learning_rate": 0.00018026509572901327,
      "loss": 0.412,
      "step": 340
    },
    {
      "epoch": 0.10029411764705883,
      "grad_norm": 0.06241445615887642,
      "learning_rate": 0.00018020618556701033,
      "loss": 0.3668,
      "step": 341
    },
    {
      "epoch": 0.10058823529411764,
      "grad_norm": 0.07471606880426407,
      "learning_rate": 0.0001801472754050074,
      "loss": 0.3731,
      "step": 342
    },
    {
      "epoch": 0.10088235294117646,
      "grad_norm": 0.06179254874587059,
      "learning_rate": 0.00018008836524300442,
      "loss": 0.3401,
      "step": 343
    },
    {
      "epoch": 0.1011764705882353,
      "grad_norm": 0.07204656302928925,
      "learning_rate": 0.00018002945508100148,
      "loss": 0.4009,
      "step": 344
    },
    {
      "epoch": 0.10147058823529412,
      "grad_norm": 0.0760396420955658,
      "learning_rate": 0.00017997054491899854,
      "loss": 0.3671,
      "step": 345
    },
    {
      "epoch": 0.10176470588235294,
      "grad_norm": 0.05884910002350807,
      "learning_rate": 0.0001799116347569956,
      "loss": 0.4125,
      "step": 346
    },
    {
      "epoch": 0.10205882352941177,
      "grad_norm": 0.06462839990854263,
      "learning_rate": 0.00017985272459499266,
      "loss": 0.3126,
      "step": 347
    },
    {
      "epoch": 0.10235294117647059,
      "grad_norm": 0.07201676815748215,
      "learning_rate": 0.0001797938144329897,
      "loss": 0.31,
      "step": 348
    },
    {
      "epoch": 0.10264705882352941,
      "grad_norm": 0.0700809508562088,
      "learning_rate": 0.00017973490427098675,
      "loss": 0.343,
      "step": 349
    },
    {
      "epoch": 0.10294117647058823,
      "grad_norm": 0.05343497171998024,
      "learning_rate": 0.00017967599410898382,
      "loss": 0.3251,
      "step": 350
    },
    {
      "epoch": 0.10323529411764706,
      "grad_norm": 0.05584115907549858,
      "learning_rate": 0.00017961708394698088,
      "loss": 0.3171,
      "step": 351
    },
    {
      "epoch": 0.10352941176470588,
      "grad_norm": 0.05746942758560181,
      "learning_rate": 0.00017955817378497794,
      "loss": 0.3416,
      "step": 352
    },
    {
      "epoch": 0.1038235294117647,
      "grad_norm": 0.07215959578752518,
      "learning_rate": 0.00017949926362297497,
      "loss": 0.3658,
      "step": 353
    },
    {
      "epoch": 0.10411764705882352,
      "grad_norm": 0.07397723942995071,
      "learning_rate": 0.00017944035346097203,
      "loss": 0.4487,
      "step": 354
    },
    {
      "epoch": 0.10441176470588236,
      "grad_norm": 0.07747973501682281,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4236,
      "step": 355
    },
    {
      "epoch": 0.10470588235294118,
      "grad_norm": 0.09172757714986801,
      "learning_rate": 0.00017932253313696615,
      "loss": 0.4821,
      "step": 356
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.06078871712088585,
      "learning_rate": 0.0001792636229749632,
      "loss": 0.3594,
      "step": 357
    },
    {
      "epoch": 0.10529411764705883,
      "grad_norm": 0.058273136615753174,
      "learning_rate": 0.00017920471281296024,
      "loss": 0.355,
      "step": 358
    },
    {
      "epoch": 0.10558823529411765,
      "grad_norm": 0.0687500610947609,
      "learning_rate": 0.0001791458026509573,
      "loss": 0.428,
      "step": 359
    },
    {
      "epoch": 0.10588235294117647,
      "grad_norm": 0.06714075803756714,
      "learning_rate": 0.00017908689248895436,
      "loss": 0.432,
      "step": 360
    },
    {
      "epoch": 0.10617647058823529,
      "grad_norm": 0.07221890985965729,
      "learning_rate": 0.00017902798232695142,
      "loss": 0.4209,
      "step": 361
    },
    {
      "epoch": 0.10647058823529412,
      "grad_norm": 0.07603282481431961,
      "learning_rate": 0.00017896907216494848,
      "loss": 0.4382,
      "step": 362
    },
    {
      "epoch": 0.10676470588235294,
      "grad_norm": 0.07435841858386993,
      "learning_rate": 0.00017891016200294552,
      "loss": 0.3795,
      "step": 363
    },
    {
      "epoch": 0.10705882352941176,
      "grad_norm": 0.06485453248023987,
      "learning_rate": 0.00017885125184094258,
      "loss": 0.3296,
      "step": 364
    },
    {
      "epoch": 0.10735294117647058,
      "grad_norm": 0.05698046833276749,
      "learning_rate": 0.00017879234167893964,
      "loss": 0.3454,
      "step": 365
    },
    {
      "epoch": 0.10764705882352942,
      "grad_norm": 0.08076491951942444,
      "learning_rate": 0.0001787334315169367,
      "loss": 0.3827,
      "step": 366
    },
    {
      "epoch": 0.10794117647058823,
      "grad_norm": 0.06518575549125671,
      "learning_rate": 0.00017867452135493376,
      "loss": 0.334,
      "step": 367
    },
    {
      "epoch": 0.10823529411764705,
      "grad_norm": 0.08755042403936386,
      "learning_rate": 0.0001786156111929308,
      "loss": 0.4493,
      "step": 368
    },
    {
      "epoch": 0.10852941176470589,
      "grad_norm": 0.07141425460577011,
      "learning_rate": 0.00017855670103092785,
      "loss": 0.4216,
      "step": 369
    },
    {
      "epoch": 0.10882352941176471,
      "grad_norm": 0.07941947132349014,
      "learning_rate": 0.0001784977908689249,
      "loss": 0.4007,
      "step": 370
    },
    {
      "epoch": 0.10911764705882353,
      "grad_norm": 0.07058899849653244,
      "learning_rate": 0.00017843888070692197,
      "loss": 0.3645,
      "step": 371
    },
    {
      "epoch": 0.10941176470588235,
      "grad_norm": 0.07162158191204071,
      "learning_rate": 0.00017837997054491903,
      "loss": 0.3307,
      "step": 372
    },
    {
      "epoch": 0.10970588235294118,
      "grad_norm": 0.0682276040315628,
      "learning_rate": 0.00017832106038291606,
      "loss": 0.3817,
      "step": 373
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.06322090327739716,
      "learning_rate": 0.00017826215022091312,
      "loss": 0.341,
      "step": 374
    },
    {
      "epoch": 0.11029411764705882,
      "grad_norm": 0.07450313121080399,
      "learning_rate": 0.00017820324005891018,
      "loss": 0.372,
      "step": 375
    },
    {
      "epoch": 0.11058823529411765,
      "grad_norm": 0.0451170913875103,
      "learning_rate": 0.00017814432989690724,
      "loss": 0.2582,
      "step": 376
    },
    {
      "epoch": 0.11088235294117647,
      "grad_norm": 0.05825483798980713,
      "learning_rate": 0.00017808541973490428,
      "loss": 0.3824,
      "step": 377
    },
    {
      "epoch": 0.1111764705882353,
      "grad_norm": 0.09508311003446579,
      "learning_rate": 0.0001780265095729013,
      "loss": 0.3936,
      "step": 378
    },
    {
      "epoch": 0.11147058823529411,
      "grad_norm": 0.07506752014160156,
      "learning_rate": 0.00017796759941089837,
      "loss": 0.3861,
      "step": 379
    },
    {
      "epoch": 0.11176470588235295,
      "grad_norm": 0.047860223799943924,
      "learning_rate": 0.00017790868924889543,
      "loss": 0.3173,
      "step": 380
    },
    {
      "epoch": 0.11205882352941177,
      "grad_norm": 0.05295055732131004,
      "learning_rate": 0.0001778497790868925,
      "loss": 0.3199,
      "step": 381
    },
    {
      "epoch": 0.11235294117647059,
      "grad_norm": 0.0679042711853981,
      "learning_rate": 0.00017779086892488955,
      "loss": 0.367,
      "step": 382
    },
    {
      "epoch": 0.1126470588235294,
      "grad_norm": 0.08049646019935608,
      "learning_rate": 0.00017773195876288658,
      "loss": 0.3803,
      "step": 383
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.059739384800195694,
      "learning_rate": 0.00017767304860088364,
      "loss": 0.3174,
      "step": 384
    },
    {
      "epoch": 0.11323529411764706,
      "grad_norm": 0.06677518039941788,
      "learning_rate": 0.0001776141384388807,
      "loss": 0.4545,
      "step": 385
    },
    {
      "epoch": 0.11352941176470588,
      "grad_norm": 0.0665544793009758,
      "learning_rate": 0.00017755522827687776,
      "loss": 0.3212,
      "step": 386
    },
    {
      "epoch": 0.11382352941176471,
      "grad_norm": 0.05580659210681915,
      "learning_rate": 0.00017749631811487482,
      "loss": 0.3195,
      "step": 387
    },
    {
      "epoch": 0.11411764705882353,
      "grad_norm": 0.06076865643262863,
      "learning_rate": 0.00017743740795287186,
      "loss": 0.3935,
      "step": 388
    },
    {
      "epoch": 0.11441176470588235,
      "grad_norm": 0.055945150554180145,
      "learning_rate": 0.00017737849779086892,
      "loss": 0.3228,
      "step": 389
    },
    {
      "epoch": 0.11470588235294117,
      "grad_norm": 0.05191303789615631,
      "learning_rate": 0.00017731958762886598,
      "loss": 0.3207,
      "step": 390
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.050948645919561386,
      "learning_rate": 0.00017726067746686304,
      "loss": 0.3089,
      "step": 391
    },
    {
      "epoch": 0.11529411764705882,
      "grad_norm": 0.05824671685695648,
      "learning_rate": 0.0001772017673048601,
      "loss": 0.3184,
      "step": 392
    },
    {
      "epoch": 0.11558823529411764,
      "grad_norm": 0.06559012085199356,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3809,
      "step": 393
    },
    {
      "epoch": 0.11588235294117646,
      "grad_norm": 0.059053681790828705,
      "learning_rate": 0.0001770839469808542,
      "loss": 0.3867,
      "step": 394
    },
    {
      "epoch": 0.1161764705882353,
      "grad_norm": 0.08661926537752151,
      "learning_rate": 0.00017702503681885125,
      "loss": 0.4373,
      "step": 395
    },
    {
      "epoch": 0.11647058823529412,
      "grad_norm": 0.08061210811138153,
      "learning_rate": 0.0001769661266568483,
      "loss": 0.4645,
      "step": 396
    },
    {
      "epoch": 0.11676470588235294,
      "grad_norm": 0.05530845746397972,
      "learning_rate": 0.00017690721649484537,
      "loss": 0.3235,
      "step": 397
    },
    {
      "epoch": 0.11705882352941177,
      "grad_norm": 0.06628365814685822,
      "learning_rate": 0.0001768483063328424,
      "loss": 0.3643,
      "step": 398
    },
    {
      "epoch": 0.11735294117647059,
      "grad_norm": 0.08501642197370529,
      "learning_rate": 0.00017678939617083947,
      "loss": 0.3869,
      "step": 399
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.06242210045456886,
      "learning_rate": 0.00017673048600883653,
      "loss": 0.3576,
      "step": 400
    },
    {
      "epoch": 0.11764705882352941,
      "eval_loss": 0.3672531545162201,
      "eval_runtime": 213.7276,
      "eval_samples_per_second": 0.468,
      "eval_steps_per_second": 0.468,
      "step": 400
    },
    {
      "epoch": 0.11794117647058823,
      "grad_norm": 0.08263355493545532,
      "learning_rate": 0.00017667157584683359,
      "loss": 0.4616,
      "step": 401
    },
    {
      "epoch": 0.11823529411764706,
      "grad_norm": 0.07237403839826584,
      "learning_rate": 0.00017661266568483065,
      "loss": 0.474,
      "step": 402
    },
    {
      "epoch": 0.11852941176470588,
      "grad_norm": 0.07534389197826385,
      "learning_rate": 0.00017655375552282768,
      "loss": 0.3536,
      "step": 403
    },
    {
      "epoch": 0.1188235294117647,
      "grad_norm": 0.06584201753139496,
      "learning_rate": 0.00017649484536082474,
      "loss": 0.3358,
      "step": 404
    },
    {
      "epoch": 0.11911764705882352,
      "grad_norm": 0.06149284541606903,
      "learning_rate": 0.0001764359351988218,
      "loss": 0.3823,
      "step": 405
    },
    {
      "epoch": 0.11941176470588236,
      "grad_norm": 0.0789175033569336,
      "learning_rate": 0.00017637702503681886,
      "loss": 0.385,
      "step": 406
    },
    {
      "epoch": 0.11970588235294118,
      "grad_norm": 0.05479629337787628,
      "learning_rate": 0.00017631811487481592,
      "loss": 0.3309,
      "step": 407
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.07517760246992111,
      "learning_rate": 0.00017625920471281295,
      "loss": 0.4345,
      "step": 408
    },
    {
      "epoch": 0.12029411764705883,
      "grad_norm": 0.06628546863794327,
      "learning_rate": 0.00017620029455081,
      "loss": 0.3628,
      "step": 409
    },
    {
      "epoch": 0.12058823529411765,
      "grad_norm": 0.07795121520757675,
      "learning_rate": 0.00017614138438880707,
      "loss": 0.4178,
      "step": 410
    },
    {
      "epoch": 0.12088235294117647,
      "grad_norm": 0.06767774373292923,
      "learning_rate": 0.00017608247422680413,
      "loss": 0.3899,
      "step": 411
    },
    {
      "epoch": 0.12117647058823529,
      "grad_norm": 0.059194840490818024,
      "learning_rate": 0.0001760235640648012,
      "loss": 0.3724,
      "step": 412
    },
    {
      "epoch": 0.12147058823529412,
      "grad_norm": 0.06003563106060028,
      "learning_rate": 0.00017596465390279823,
      "loss": 0.4076,
      "step": 413
    },
    {
      "epoch": 0.12176470588235294,
      "grad_norm": 0.04255178943276405,
      "learning_rate": 0.00017590574374079529,
      "loss": 0.3021,
      "step": 414
    },
    {
      "epoch": 0.12205882352941176,
      "grad_norm": 0.07247471064329147,
      "learning_rate": 0.00017584683357879235,
      "loss": 0.3898,
      "step": 415
    },
    {
      "epoch": 0.1223529411764706,
      "grad_norm": 0.05953656882047653,
      "learning_rate": 0.0001757879234167894,
      "loss": 0.3128,
      "step": 416
    },
    {
      "epoch": 0.12264705882352941,
      "grad_norm": 0.08154992759227753,
      "learning_rate": 0.00017572901325478647,
      "loss": 0.4183,
      "step": 417
    },
    {
      "epoch": 0.12294117647058823,
      "grad_norm": 0.05401672422885895,
      "learning_rate": 0.0001756701030927835,
      "loss": 0.3215,
      "step": 418
    },
    {
      "epoch": 0.12323529411764705,
      "grad_norm": 0.05628965422511101,
      "learning_rate": 0.00017561119293078056,
      "loss": 0.3281,
      "step": 419
    },
    {
      "epoch": 0.12352941176470589,
      "grad_norm": 0.09957556426525116,
      "learning_rate": 0.00017555228276877762,
      "loss": 0.3042,
      "step": 420
    },
    {
      "epoch": 0.12382352941176471,
      "grad_norm": 0.07639549672603607,
      "learning_rate": 0.00017549337260677468,
      "loss": 0.4341,
      "step": 421
    },
    {
      "epoch": 0.12411764705882353,
      "grad_norm": 0.05212264135479927,
      "learning_rate": 0.00017543446244477174,
      "loss": 0.316,
      "step": 422
    },
    {
      "epoch": 0.12441176470588235,
      "grad_norm": 0.08201335370540619,
      "learning_rate": 0.00017537555228276877,
      "loss": 0.3606,
      "step": 423
    },
    {
      "epoch": 0.12470588235294118,
      "grad_norm": 0.0841822400689125,
      "learning_rate": 0.00017531664212076583,
      "loss": 0.4169,
      "step": 424
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06840043514966965,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4096,
      "step": 425
    },
    {
      "epoch": 0.12529411764705883,
      "grad_norm": 0.07148951292037964,
      "learning_rate": 0.00017519882179675995,
      "loss": 0.4096,
      "step": 426
    },
    {
      "epoch": 0.12558823529411764,
      "grad_norm": 0.06686822324991226,
      "learning_rate": 0.00017513991163475701,
      "loss": 0.3671,
      "step": 427
    },
    {
      "epoch": 0.12588235294117647,
      "grad_norm": 0.05390598624944687,
      "learning_rate": 0.00017508100147275405,
      "loss": 0.3129,
      "step": 428
    },
    {
      "epoch": 0.1261764705882353,
      "grad_norm": 0.05109141767024994,
      "learning_rate": 0.0001750220913107511,
      "loss": 0.2997,
      "step": 429
    },
    {
      "epoch": 0.1264705882352941,
      "grad_norm": 0.07828373461961746,
      "learning_rate": 0.00017496318114874817,
      "loss": 0.3868,
      "step": 430
    },
    {
      "epoch": 0.12676470588235295,
      "grad_norm": 0.061607904732227325,
      "learning_rate": 0.00017490427098674523,
      "loss": 0.3277,
      "step": 431
    },
    {
      "epoch": 0.12705882352941175,
      "grad_norm": 0.07689444720745087,
      "learning_rate": 0.0001748453608247423,
      "loss": 0.4161,
      "step": 432
    },
    {
      "epoch": 0.12735294117647059,
      "grad_norm": 0.06176171079277992,
      "learning_rate": 0.00017478645066273932,
      "loss": 0.3204,
      "step": 433
    },
    {
      "epoch": 0.12764705882352942,
      "grad_norm": 0.05509386956691742,
      "learning_rate": 0.00017472754050073638,
      "loss": 0.3386,
      "step": 434
    },
    {
      "epoch": 0.12794117647058822,
      "grad_norm": 0.05754280090332031,
      "learning_rate": 0.00017466863033873344,
      "loss": 0.3538,
      "step": 435
    },
    {
      "epoch": 0.12823529411764706,
      "grad_norm": 0.08163037151098251,
      "learning_rate": 0.0001746097201767305,
      "loss": 0.4038,
      "step": 436
    },
    {
      "epoch": 0.1285294117647059,
      "grad_norm": 0.05647306516766548,
      "learning_rate": 0.00017455081001472756,
      "loss": 0.3729,
      "step": 437
    },
    {
      "epoch": 0.1288235294117647,
      "grad_norm": 0.07209846377372742,
      "learning_rate": 0.0001744918998527246,
      "loss": 0.4259,
      "step": 438
    },
    {
      "epoch": 0.12911764705882353,
      "grad_norm": 0.06568286567926407,
      "learning_rate": 0.00017443298969072165,
      "loss": 0.3943,
      "step": 439
    },
    {
      "epoch": 0.12941176470588237,
      "grad_norm": 0.061060767620801926,
      "learning_rate": 0.00017437407952871872,
      "loss": 0.3711,
      "step": 440
    },
    {
      "epoch": 0.12970588235294117,
      "grad_norm": 0.062304235994815826,
      "learning_rate": 0.00017431516936671578,
      "loss": 0.3468,
      "step": 441
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.06511593610048294,
      "learning_rate": 0.00017425625920471284,
      "loss": 0.3523,
      "step": 442
    },
    {
      "epoch": 0.1302941176470588,
      "grad_norm": 0.07485492527484894,
      "learning_rate": 0.00017419734904270987,
      "loss": 0.3686,
      "step": 443
    },
    {
      "epoch": 0.13058823529411764,
      "grad_norm": 0.059787947684526443,
      "learning_rate": 0.00017413843888070693,
      "loss": 0.3702,
      "step": 444
    },
    {
      "epoch": 0.13088235294117648,
      "grad_norm": 0.061845812946558,
      "learning_rate": 0.000174079528718704,
      "loss": 0.3248,
      "step": 445
    },
    {
      "epoch": 0.13117647058823528,
      "grad_norm": 0.073362335562706,
      "learning_rate": 0.00017402061855670105,
      "loss": 0.4019,
      "step": 446
    },
    {
      "epoch": 0.13147058823529412,
      "grad_norm": 0.057457905262708664,
      "learning_rate": 0.0001739617083946981,
      "loss": 0.3218,
      "step": 447
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 0.05499434843659401,
      "learning_rate": 0.00017390279823269514,
      "loss": 0.3147,
      "step": 448
    },
    {
      "epoch": 0.13205882352941176,
      "grad_norm": 0.074238620698452,
      "learning_rate": 0.0001738438880706922,
      "loss": 0.3675,
      "step": 449
    },
    {
      "epoch": 0.1323529411764706,
      "grad_norm": 0.06017538905143738,
      "learning_rate": 0.00017378497790868926,
      "loss": 0.3322,
      "step": 450
    },
    {
      "epoch": 0.13264705882352942,
      "grad_norm": 0.06091383099555969,
      "learning_rate": 0.00017372606774668632,
      "loss": 0.4063,
      "step": 451
    },
    {
      "epoch": 0.13294117647058823,
      "grad_norm": 0.048693880438804626,
      "learning_rate": 0.00017366715758468338,
      "loss": 0.2808,
      "step": 452
    },
    {
      "epoch": 0.13323529411764706,
      "grad_norm": 0.06318904459476471,
      "learning_rate": 0.00017360824742268042,
      "loss": 0.3655,
      "step": 453
    },
    {
      "epoch": 0.13352941176470587,
      "grad_norm": 0.053866591304540634,
      "learning_rate": 0.00017354933726067748,
      "loss": 0.3069,
      "step": 454
    },
    {
      "epoch": 0.1338235294117647,
      "grad_norm": 0.05395478755235672,
      "learning_rate": 0.00017349042709867454,
      "loss": 0.3273,
      "step": 455
    },
    {
      "epoch": 0.13411764705882354,
      "grad_norm": 0.06352989375591278,
      "learning_rate": 0.0001734315169366716,
      "loss": 0.3202,
      "step": 456
    },
    {
      "epoch": 0.13441176470588234,
      "grad_norm": 0.05848497152328491,
      "learning_rate": 0.00017337260677466866,
      "loss": 0.3035,
      "step": 457
    },
    {
      "epoch": 0.13470588235294118,
      "grad_norm": 0.06433206796646118,
      "learning_rate": 0.0001733136966126657,
      "loss": 0.3535,
      "step": 458
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.0535602830350399,
      "learning_rate": 0.00017325478645066275,
      "loss": 0.3542,
      "step": 459
    },
    {
      "epoch": 0.13529411764705881,
      "grad_norm": 0.05265054106712341,
      "learning_rate": 0.0001731958762886598,
      "loss": 0.2825,
      "step": 460
    },
    {
      "epoch": 0.13558823529411765,
      "grad_norm": 0.0714925229549408,
      "learning_rate": 0.00017313696612665687,
      "loss": 0.3816,
      "step": 461
    },
    {
      "epoch": 0.13588235294117648,
      "grad_norm": 0.05201279744505882,
      "learning_rate": 0.00017307805596465393,
      "loss": 0.3201,
      "step": 462
    },
    {
      "epoch": 0.1361764705882353,
      "grad_norm": 0.06183996796607971,
      "learning_rate": 0.00017301914580265096,
      "loss": 0.307,
      "step": 463
    },
    {
      "epoch": 0.13647058823529412,
      "grad_norm": 0.06556003540754318,
      "learning_rate": 0.00017296023564064802,
      "loss": 0.34,
      "step": 464
    },
    {
      "epoch": 0.13676470588235295,
      "grad_norm": 0.05667487904429436,
      "learning_rate": 0.00017290132547864508,
      "loss": 0.3593,
      "step": 465
    },
    {
      "epoch": 0.13705882352941176,
      "grad_norm": 0.07059769332408905,
      "learning_rate": 0.00017284241531664214,
      "loss": 0.4527,
      "step": 466
    },
    {
      "epoch": 0.1373529411764706,
      "grad_norm": 0.05512050911784172,
      "learning_rate": 0.0001727835051546392,
      "loss": 0.3875,
      "step": 467
    },
    {
      "epoch": 0.1376470588235294,
      "grad_norm": 0.05575348809361458,
      "learning_rate": 0.00017272459499263624,
      "loss": 0.3994,
      "step": 468
    },
    {
      "epoch": 0.13794117647058823,
      "grad_norm": 0.07065296918153763,
      "learning_rate": 0.0001726656848306333,
      "loss": 0.3501,
      "step": 469
    },
    {
      "epoch": 0.13823529411764707,
      "grad_norm": 0.056007541716098785,
      "learning_rate": 0.00017260677466863036,
      "loss": 0.3254,
      "step": 470
    },
    {
      "epoch": 0.13852941176470587,
      "grad_norm": 0.04979860037565231,
      "learning_rate": 0.00017254786450662742,
      "loss": 0.3075,
      "step": 471
    },
    {
      "epoch": 0.1388235294117647,
      "grad_norm": 0.07049575448036194,
      "learning_rate": 0.00017248895434462448,
      "loss": 0.3535,
      "step": 472
    },
    {
      "epoch": 0.13911764705882354,
      "grad_norm": 0.057973578572273254,
      "learning_rate": 0.0001724300441826215,
      "loss": 0.2986,
      "step": 473
    },
    {
      "epoch": 0.13941176470588235,
      "grad_norm": 0.06545397639274597,
      "learning_rate": 0.00017237113402061857,
      "loss": 0.3819,
      "step": 474
    },
    {
      "epoch": 0.13970588235294118,
      "grad_norm": 0.056255120784044266,
      "learning_rate": 0.00017231222385861563,
      "loss": 0.3143,
      "step": 475
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.039822451770305634,
      "learning_rate": 0.0001722533136966127,
      "loss": 0.2702,
      "step": 476
    },
    {
      "epoch": 0.14029411764705882,
      "grad_norm": 0.05215808376669884,
      "learning_rate": 0.00017219440353460975,
      "loss": 0.4001,
      "step": 477
    },
    {
      "epoch": 0.14058823529411765,
      "grad_norm": 0.0583135187625885,
      "learning_rate": 0.00017213549337260678,
      "loss": 0.3764,
      "step": 478
    },
    {
      "epoch": 0.14088235294117646,
      "grad_norm": 0.06295765936374664,
      "learning_rate": 0.00017207658321060384,
      "loss": 0.3323,
      "step": 479
    },
    {
      "epoch": 0.1411764705882353,
      "grad_norm": 0.052351173013448715,
      "learning_rate": 0.0001720176730486009,
      "loss": 0.3386,
      "step": 480
    },
    {
      "epoch": 0.14147058823529413,
      "grad_norm": 0.07185954600572586,
      "learning_rate": 0.00017195876288659796,
      "loss": 0.3566,
      "step": 481
    },
    {
      "epoch": 0.14176470588235293,
      "grad_norm": 0.0632806196808815,
      "learning_rate": 0.00017189985272459503,
      "loss": 0.3787,
      "step": 482
    },
    {
      "epoch": 0.14205882352941177,
      "grad_norm": 0.06502800434827805,
      "learning_rate": 0.00017184094256259203,
      "loss": 0.3158,
      "step": 483
    },
    {
      "epoch": 0.1423529411764706,
      "grad_norm": 0.057220734655857086,
      "learning_rate": 0.0001717820324005891,
      "loss": 0.3649,
      "step": 484
    },
    {
      "epoch": 0.1426470588235294,
      "grad_norm": 0.05213073268532753,
      "learning_rate": 0.00017172312223858615,
      "loss": 0.3134,
      "step": 485
    },
    {
      "epoch": 0.14294117647058824,
      "grad_norm": 0.06543286144733429,
      "learning_rate": 0.0001716642120765832,
      "loss": 0.4079,
      "step": 486
    },
    {
      "epoch": 0.14323529411764707,
      "grad_norm": 0.05453117936849594,
      "learning_rate": 0.00017160530191458027,
      "loss": 0.3666,
      "step": 487
    },
    {
      "epoch": 0.14352941176470588,
      "grad_norm": 0.05235879123210907,
      "learning_rate": 0.0001715463917525773,
      "loss": 0.2937,
      "step": 488
    },
    {
      "epoch": 0.1438235294117647,
      "grad_norm": 0.05976506695151329,
      "learning_rate": 0.00017148748159057437,
      "loss": 0.385,
      "step": 489
    },
    {
      "epoch": 0.14411764705882352,
      "grad_norm": 0.07005610316991806,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4162,
      "step": 490
    },
    {
      "epoch": 0.14441176470588235,
      "grad_norm": 0.05024661868810654,
      "learning_rate": 0.00017136966126656849,
      "loss": 0.3276,
      "step": 491
    },
    {
      "epoch": 0.14470588235294118,
      "grad_norm": 0.07257386296987534,
      "learning_rate": 0.00017131075110456555,
      "loss": 0.3949,
      "step": 492
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.05447471886873245,
      "learning_rate": 0.00017125184094256258,
      "loss": 0.3068,
      "step": 493
    },
    {
      "epoch": 0.14529411764705882,
      "grad_norm": 0.059542350471019745,
      "learning_rate": 0.00017119293078055964,
      "loss": 0.4006,
      "step": 494
    },
    {
      "epoch": 0.14558823529411766,
      "grad_norm": 0.0608154721558094,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.3795,
      "step": 495
    },
    {
      "epoch": 0.14588235294117646,
      "grad_norm": 0.044919852167367935,
      "learning_rate": 0.00017107511045655376,
      "loss": 0.2894,
      "step": 496
    },
    {
      "epoch": 0.1461764705882353,
      "grad_norm": 0.049503348767757416,
      "learning_rate": 0.00017101620029455082,
      "loss": 0.3292,
      "step": 497
    },
    {
      "epoch": 0.14647058823529413,
      "grad_norm": 0.07854627817869186,
      "learning_rate": 0.00017095729013254785,
      "loss": 0.3632,
      "step": 498
    },
    {
      "epoch": 0.14676470588235294,
      "grad_norm": 0.05168467015028,
      "learning_rate": 0.0001708983799705449,
      "loss": 0.3386,
      "step": 499
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.05691979452967644,
      "learning_rate": 0.00017083946980854197,
      "loss": 0.3579,
      "step": 500
    },
    {
      "epoch": 0.14705882352941177,
      "eval_loss": 0.36545103788375854,
      "eval_runtime": 213.9326,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 500
    },
    {
      "epoch": 0.14735294117647058,
      "grad_norm": 0.06257742643356323,
      "learning_rate": 0.00017078055964653903,
      "loss": 0.3615,
      "step": 501
    },
    {
      "epoch": 0.1476470588235294,
      "grad_norm": 0.06772120296955109,
      "learning_rate": 0.0001707216494845361,
      "loss": 0.4368,
      "step": 502
    },
    {
      "epoch": 0.14794117647058824,
      "grad_norm": 0.05633125081658363,
      "learning_rate": 0.00017066273932253313,
      "loss": 0.2812,
      "step": 503
    },
    {
      "epoch": 0.14823529411764705,
      "grad_norm": 0.070951446890831,
      "learning_rate": 0.00017060382916053019,
      "loss": 0.3657,
      "step": 504
    },
    {
      "epoch": 0.14852941176470588,
      "grad_norm": 0.061112385243177414,
      "learning_rate": 0.00017054491899852725,
      "loss": 0.3246,
      "step": 505
    },
    {
      "epoch": 0.14882352941176472,
      "grad_norm": 0.06499598175287247,
      "learning_rate": 0.0001704860088365243,
      "loss": 0.4206,
      "step": 506
    },
    {
      "epoch": 0.14911764705882352,
      "grad_norm": 0.059075456112623215,
      "learning_rate": 0.00017042709867452137,
      "loss": 0.3916,
      "step": 507
    },
    {
      "epoch": 0.14941176470588236,
      "grad_norm": 0.06730490177869797,
      "learning_rate": 0.0001703681885125184,
      "loss": 0.4091,
      "step": 508
    },
    {
      "epoch": 0.1497058823529412,
      "grad_norm": 0.06274665147066116,
      "learning_rate": 0.00017030927835051546,
      "loss": 0.3465,
      "step": 509
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.06880743056535721,
      "learning_rate": 0.00017025036818851252,
      "loss": 0.3657,
      "step": 510
    },
    {
      "epoch": 0.15029411764705883,
      "grad_norm": 0.054245829582214355,
      "learning_rate": 0.00017019145802650958,
      "loss": 0.3499,
      "step": 511
    },
    {
      "epoch": 0.15058823529411763,
      "grad_norm": 0.0406309999525547,
      "learning_rate": 0.00017013254786450664,
      "loss": 0.2971,
      "step": 512
    },
    {
      "epoch": 0.15088235294117647,
      "grad_norm": 0.055706337094306946,
      "learning_rate": 0.00017007363770250367,
      "loss": 0.3311,
      "step": 513
    },
    {
      "epoch": 0.1511764705882353,
      "grad_norm": 0.05594045668840408,
      "learning_rate": 0.00017001472754050073,
      "loss": 0.3433,
      "step": 514
    },
    {
      "epoch": 0.1514705882352941,
      "grad_norm": 0.05443777143955231,
      "learning_rate": 0.0001699558173784978,
      "loss": 0.3296,
      "step": 515
    },
    {
      "epoch": 0.15176470588235294,
      "grad_norm": 0.06405218690633774,
      "learning_rate": 0.00016989690721649485,
      "loss": 0.3824,
      "step": 516
    },
    {
      "epoch": 0.15205882352941177,
      "grad_norm": 0.06635114550590515,
      "learning_rate": 0.00016983799705449191,
      "loss": 0.3242,
      "step": 517
    },
    {
      "epoch": 0.15235294117647058,
      "grad_norm": 0.0781254693865776,
      "learning_rate": 0.00016977908689248895,
      "loss": 0.3629,
      "step": 518
    },
    {
      "epoch": 0.1526470588235294,
      "grad_norm": 0.05647239461541176,
      "learning_rate": 0.000169720176730486,
      "loss": 0.2988,
      "step": 519
    },
    {
      "epoch": 0.15294117647058825,
      "grad_norm": 0.04966836795210838,
      "learning_rate": 0.00016966126656848307,
      "loss": 0.2591,
      "step": 520
    },
    {
      "epoch": 0.15323529411764705,
      "grad_norm": 0.08413435518741608,
      "learning_rate": 0.00016960235640648013,
      "loss": 0.3618,
      "step": 521
    },
    {
      "epoch": 0.1535294117647059,
      "grad_norm": 0.07128289341926575,
      "learning_rate": 0.0001695434462444772,
      "loss": 0.397,
      "step": 522
    },
    {
      "epoch": 0.1538235294117647,
      "grad_norm": 0.04600508511066437,
      "learning_rate": 0.00016948453608247422,
      "loss": 0.2954,
      "step": 523
    },
    {
      "epoch": 0.15411764705882353,
      "grad_norm": 0.0425855927169323,
      "learning_rate": 0.00016942562592047128,
      "loss": 0.3183,
      "step": 524
    },
    {
      "epoch": 0.15441176470588236,
      "grad_norm": 0.05441323295235634,
      "learning_rate": 0.00016936671575846834,
      "loss": 0.3023,
      "step": 525
    },
    {
      "epoch": 0.15470588235294117,
      "grad_norm": 0.06407584995031357,
      "learning_rate": 0.0001693078055964654,
      "loss": 0.2909,
      "step": 526
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.06265800446271896,
      "learning_rate": 0.00016924889543446246,
      "loss": 0.3713,
      "step": 527
    },
    {
      "epoch": 0.15529411764705883,
      "grad_norm": 0.05330798029899597,
      "learning_rate": 0.0001691899852724595,
      "loss": 0.3269,
      "step": 528
    },
    {
      "epoch": 0.15558823529411764,
      "grad_norm": 0.0772225558757782,
      "learning_rate": 0.00016913107511045655,
      "loss": 0.4335,
      "step": 529
    },
    {
      "epoch": 0.15588235294117647,
      "grad_norm": 0.05400877073407173,
      "learning_rate": 0.00016907216494845361,
      "loss": 0.2867,
      "step": 530
    },
    {
      "epoch": 0.1561764705882353,
      "grad_norm": 0.05162560194730759,
      "learning_rate": 0.00016901325478645068,
      "loss": 0.3734,
      "step": 531
    },
    {
      "epoch": 0.1564705882352941,
      "grad_norm": 0.058614373207092285,
      "learning_rate": 0.00016895434462444774,
      "loss": 0.397,
      "step": 532
    },
    {
      "epoch": 0.15676470588235294,
      "grad_norm": 0.04820419102907181,
      "learning_rate": 0.00016889543446244477,
      "loss": 0.3116,
      "step": 533
    },
    {
      "epoch": 0.15705882352941178,
      "grad_norm": 0.07222265750169754,
      "learning_rate": 0.00016883652430044183,
      "loss": 0.3625,
      "step": 534
    },
    {
      "epoch": 0.15735294117647058,
      "grad_norm": 0.05925459414720535,
      "learning_rate": 0.0001687776141384389,
      "loss": 0.3755,
      "step": 535
    },
    {
      "epoch": 0.15764705882352942,
      "grad_norm": 0.04997588321566582,
      "learning_rate": 0.00016871870397643595,
      "loss": 0.3209,
      "step": 536
    },
    {
      "epoch": 0.15794117647058822,
      "grad_norm": 0.05160374939441681,
      "learning_rate": 0.000168659793814433,
      "loss": 0.3561,
      "step": 537
    },
    {
      "epoch": 0.15823529411764706,
      "grad_norm": 0.04930102080106735,
      "learning_rate": 0.00016860088365243004,
      "loss": 0.3435,
      "step": 538
    },
    {
      "epoch": 0.1585294117647059,
      "grad_norm": 0.05635027959942818,
      "learning_rate": 0.0001685419734904271,
      "loss": 0.3482,
      "step": 539
    },
    {
      "epoch": 0.1588235294117647,
      "grad_norm": 0.054603248834609985,
      "learning_rate": 0.00016848306332842416,
      "loss": 0.3275,
      "step": 540
    },
    {
      "epoch": 0.15911764705882353,
      "grad_norm": 0.07488647103309631,
      "learning_rate": 0.00016842415316642122,
      "loss": 0.3595,
      "step": 541
    },
    {
      "epoch": 0.15941176470588236,
      "grad_norm": 0.06307503581047058,
      "learning_rate": 0.00016836524300441828,
      "loss": 0.2709,
      "step": 542
    },
    {
      "epoch": 0.15970588235294117,
      "grad_norm": 0.06321723014116287,
      "learning_rate": 0.00016830633284241532,
      "loss": 0.4018,
      "step": 543
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.060336921364068985,
      "learning_rate": 0.00016824742268041238,
      "loss": 0.3702,
      "step": 544
    },
    {
      "epoch": 0.16029411764705884,
      "grad_norm": 0.0429457351565361,
      "learning_rate": 0.00016818851251840944,
      "loss": 0.2679,
      "step": 545
    },
    {
      "epoch": 0.16058823529411764,
      "grad_norm": 0.06110946834087372,
      "learning_rate": 0.0001681296023564065,
      "loss": 0.357,
      "step": 546
    },
    {
      "epoch": 0.16088235294117648,
      "grad_norm": 0.045691847801208496,
      "learning_rate": 0.00016807069219440356,
      "loss": 0.2533,
      "step": 547
    },
    {
      "epoch": 0.16117647058823528,
      "grad_norm": 0.05680420994758606,
      "learning_rate": 0.0001680117820324006,
      "loss": 0.2963,
      "step": 548
    },
    {
      "epoch": 0.16147058823529412,
      "grad_norm": 0.057551540434360504,
      "learning_rate": 0.00016795287187039765,
      "loss": 0.3575,
      "step": 549
    },
    {
      "epoch": 0.16176470588235295,
      "grad_norm": 0.05316680297255516,
      "learning_rate": 0.0001678939617083947,
      "loss": 0.3866,
      "step": 550
    },
    {
      "epoch": 0.16205882352941176,
      "grad_norm": 0.04563960060477257,
      "learning_rate": 0.00016783505154639177,
      "loss": 0.3409,
      "step": 551
    },
    {
      "epoch": 0.1623529411764706,
      "grad_norm": 0.04845968633890152,
      "learning_rate": 0.00016777614138438883,
      "loss": 0.3106,
      "step": 552
    },
    {
      "epoch": 0.16264705882352942,
      "grad_norm": 0.06334744393825531,
      "learning_rate": 0.00016771723122238586,
      "loss": 0.268,
      "step": 553
    },
    {
      "epoch": 0.16294117647058823,
      "grad_norm": 0.0708044245839119,
      "learning_rate": 0.00016765832106038292,
      "loss": 0.4431,
      "step": 554
    },
    {
      "epoch": 0.16323529411764706,
      "grad_norm": 0.07076764851808548,
      "learning_rate": 0.00016759941089837998,
      "loss": 0.4225,
      "step": 555
    },
    {
      "epoch": 0.1635294117647059,
      "grad_norm": 0.05025637149810791,
      "learning_rate": 0.00016754050073637704,
      "loss": 0.3171,
      "step": 556
    },
    {
      "epoch": 0.1638235294117647,
      "grad_norm": 0.0685744658112526,
      "learning_rate": 0.0001674815905743741,
      "loss": 0.3236,
      "step": 557
    },
    {
      "epoch": 0.16411764705882353,
      "grad_norm": 0.0622173547744751,
      "learning_rate": 0.00016742268041237114,
      "loss": 0.378,
      "step": 558
    },
    {
      "epoch": 0.16441176470588234,
      "grad_norm": 0.06748336553573608,
      "learning_rate": 0.0001673637702503682,
      "loss": 0.3702,
      "step": 559
    },
    {
      "epoch": 0.16470588235294117,
      "grad_norm": 0.06239144131541252,
      "learning_rate": 0.00016730486008836526,
      "loss": 0.4086,
      "step": 560
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.06894143670797348,
      "learning_rate": 0.00016724594992636232,
      "loss": 0.3561,
      "step": 561
    },
    {
      "epoch": 0.1652941176470588,
      "grad_norm": 0.07150322943925858,
      "learning_rate": 0.00016718703976435938,
      "loss": 0.3583,
      "step": 562
    },
    {
      "epoch": 0.16558823529411765,
      "grad_norm": 0.06687574088573456,
      "learning_rate": 0.0001671281296023564,
      "loss": 0.4231,
      "step": 563
    },
    {
      "epoch": 0.16588235294117648,
      "grad_norm": 0.05152297019958496,
      "learning_rate": 0.00016706921944035347,
      "loss": 0.3372,
      "step": 564
    },
    {
      "epoch": 0.1661764705882353,
      "grad_norm": 0.05293889716267586,
      "learning_rate": 0.00016701030927835053,
      "loss": 0.338,
      "step": 565
    },
    {
      "epoch": 0.16647058823529412,
      "grad_norm": 0.06447193026542664,
      "learning_rate": 0.0001669513991163476,
      "loss": 0.4003,
      "step": 566
    },
    {
      "epoch": 0.16676470588235295,
      "grad_norm": 0.06149649992585182,
      "learning_rate": 0.00016689248895434465,
      "loss": 0.3622,
      "step": 567
    },
    {
      "epoch": 0.16705882352941176,
      "grad_norm": 0.05320175364613533,
      "learning_rate": 0.00016683357879234168,
      "loss": 0.3242,
      "step": 568
    },
    {
      "epoch": 0.1673529411764706,
      "grad_norm": 0.0722704604268074,
      "learning_rate": 0.00016677466863033874,
      "loss": 0.301,
      "step": 569
    },
    {
      "epoch": 0.1676470588235294,
      "grad_norm": 0.0751953125,
      "learning_rate": 0.0001667157584683358,
      "loss": 0.3208,
      "step": 570
    },
    {
      "epoch": 0.16794117647058823,
      "grad_norm": 0.0641278401017189,
      "learning_rate": 0.00016665684830633286,
      "loss": 0.4117,
      "step": 571
    },
    {
      "epoch": 0.16823529411764707,
      "grad_norm": 0.06084010750055313,
      "learning_rate": 0.00016659793814432993,
      "loss": 0.3652,
      "step": 572
    },
    {
      "epoch": 0.16852941176470587,
      "grad_norm": 0.07031506299972534,
      "learning_rate": 0.00016653902798232696,
      "loss": 0.3805,
      "step": 573
    },
    {
      "epoch": 0.1688235294117647,
      "grad_norm": 0.05292564257979393,
      "learning_rate": 0.00016648011782032402,
      "loss": 0.3311,
      "step": 574
    },
    {
      "epoch": 0.16911764705882354,
      "grad_norm": 0.06748942285776138,
      "learning_rate": 0.00016642120765832108,
      "loss": 0.409,
      "step": 575
    },
    {
      "epoch": 0.16941176470588235,
      "grad_norm": 0.06374265253543854,
      "learning_rate": 0.00016636229749631814,
      "loss": 0.3969,
      "step": 576
    },
    {
      "epoch": 0.16970588235294118,
      "grad_norm": 0.06359335780143738,
      "learning_rate": 0.0001663033873343152,
      "loss": 0.44,
      "step": 577
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.04644837602972984,
      "learning_rate": 0.00016624447717231223,
      "loss": 0.2698,
      "step": 578
    },
    {
      "epoch": 0.17029411764705882,
      "grad_norm": 0.054346539080142975,
      "learning_rate": 0.0001661855670103093,
      "loss": 0.4056,
      "step": 579
    },
    {
      "epoch": 0.17058823529411765,
      "grad_norm": 0.05096777155995369,
      "learning_rate": 0.00016612665684830635,
      "loss": 0.3561,
      "step": 580
    },
    {
      "epoch": 0.17088235294117646,
      "grad_norm": 0.0495946891605854,
      "learning_rate": 0.0001660677466863034,
      "loss": 0.2907,
      "step": 581
    },
    {
      "epoch": 0.1711764705882353,
      "grad_norm": 0.04551497846841812,
      "learning_rate": 0.00016600883652430047,
      "loss": 0.3388,
      "step": 582
    },
    {
      "epoch": 0.17147058823529412,
      "grad_norm": 0.056641411036252975,
      "learning_rate": 0.0001659499263622975,
      "loss": 0.3522,
      "step": 583
    },
    {
      "epoch": 0.17176470588235293,
      "grad_norm": 0.062166862189769745,
      "learning_rate": 0.00016589101620029457,
      "loss": 0.3824,
      "step": 584
    },
    {
      "epoch": 0.17205882352941176,
      "grad_norm": 0.052577801048755646,
      "learning_rate": 0.00016583210603829163,
      "loss": 0.3607,
      "step": 585
    },
    {
      "epoch": 0.1723529411764706,
      "grad_norm": 0.057921577244997025,
      "learning_rate": 0.00016577319587628869,
      "loss": 0.4349,
      "step": 586
    },
    {
      "epoch": 0.1726470588235294,
      "grad_norm": 0.06178458407521248,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.3902,
      "step": 587
    },
    {
      "epoch": 0.17294117647058824,
      "grad_norm": 0.03922653570771217,
      "learning_rate": 0.00016565537555228278,
      "loss": 0.2906,
      "step": 588
    },
    {
      "epoch": 0.17323529411764707,
      "grad_norm": 0.057879284024238586,
      "learning_rate": 0.0001655964653902798,
      "loss": 0.421,
      "step": 589
    },
    {
      "epoch": 0.17352941176470588,
      "grad_norm": 0.046928055584430695,
      "learning_rate": 0.00016553755522827687,
      "loss": 0.2935,
      "step": 590
    },
    {
      "epoch": 0.1738235294117647,
      "grad_norm": 0.04417013004422188,
      "learning_rate": 0.00016547864506627393,
      "loss": 0.2886,
      "step": 591
    },
    {
      "epoch": 0.17411764705882352,
      "grad_norm": 0.049289822578430176,
      "learning_rate": 0.000165419734904271,
      "loss": 0.3604,
      "step": 592
    },
    {
      "epoch": 0.17441176470588235,
      "grad_norm": 0.057384245097637177,
      "learning_rate": 0.00016536082474226803,
      "loss": 0.3671,
      "step": 593
    },
    {
      "epoch": 0.17470588235294118,
      "grad_norm": 0.06342916190624237,
      "learning_rate": 0.00016530191458026509,
      "loss": 0.3405,
      "step": 594
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.05093424394726753,
      "learning_rate": 0.00016524300441826215,
      "loss": 0.4079,
      "step": 595
    },
    {
      "epoch": 0.17529411764705882,
      "grad_norm": 0.06481397897005081,
      "learning_rate": 0.0001651840942562592,
      "loss": 0.382,
      "step": 596
    },
    {
      "epoch": 0.17558823529411766,
      "grad_norm": 0.059226565062999725,
      "learning_rate": 0.00016512518409425627,
      "loss": 0.3816,
      "step": 597
    },
    {
      "epoch": 0.17588235294117646,
      "grad_norm": 0.06437677890062332,
      "learning_rate": 0.0001650662739322533,
      "loss": 0.3638,
      "step": 598
    },
    {
      "epoch": 0.1761764705882353,
      "grad_norm": 0.06057155877351761,
      "learning_rate": 0.00016500736377025036,
      "loss": 0.3606,
      "step": 599
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.07591994851827621,
      "learning_rate": 0.00016494845360824742,
      "loss": 0.3755,
      "step": 600
    },
    {
      "epoch": 0.17647058823529413,
      "eval_loss": 0.36275482177734375,
      "eval_runtime": 213.6997,
      "eval_samples_per_second": 0.468,
      "eval_steps_per_second": 0.468,
      "step": 600
    },
    {
      "epoch": 0.17676470588235293,
      "grad_norm": 0.06091168895363808,
      "learning_rate": 0.00016488954344624448,
      "loss": 0.4202,
      "step": 601
    },
    {
      "epoch": 0.17705882352941177,
      "grad_norm": 0.07623398303985596,
      "learning_rate": 0.00016483063328424154,
      "loss": 0.4251,
      "step": 602
    },
    {
      "epoch": 0.1773529411764706,
      "grad_norm": 0.054351337254047394,
      "learning_rate": 0.00016477172312223857,
      "loss": 0.3425,
      "step": 603
    },
    {
      "epoch": 0.1776470588235294,
      "grad_norm": 0.055312998592853546,
      "learning_rate": 0.00016471281296023563,
      "loss": 0.3533,
      "step": 604
    },
    {
      "epoch": 0.17794117647058824,
      "grad_norm": 0.06219024583697319,
      "learning_rate": 0.0001646539027982327,
      "loss": 0.3499,
      "step": 605
    },
    {
      "epoch": 0.17823529411764705,
      "grad_norm": 0.06436453014612198,
      "learning_rate": 0.00016459499263622975,
      "loss": 0.4169,
      "step": 606
    },
    {
      "epoch": 0.17852941176470588,
      "grad_norm": 0.05423522740602493,
      "learning_rate": 0.00016453608247422681,
      "loss": 0.4158,
      "step": 607
    },
    {
      "epoch": 0.17882352941176471,
      "grad_norm": 0.07241040468215942,
      "learning_rate": 0.00016447717231222385,
      "loss": 0.4,
      "step": 608
    },
    {
      "epoch": 0.17911764705882352,
      "grad_norm": 0.04312172159552574,
      "learning_rate": 0.0001644182621502209,
      "loss": 0.2543,
      "step": 609
    },
    {
      "epoch": 0.17941176470588235,
      "grad_norm": 0.06916610151529312,
      "learning_rate": 0.00016435935198821797,
      "loss": 0.3779,
      "step": 610
    },
    {
      "epoch": 0.1797058823529412,
      "grad_norm": 0.0524308867752552,
      "learning_rate": 0.00016430044182621503,
      "loss": 0.2935,
      "step": 611
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.05730877444148064,
      "learning_rate": 0.0001642415316642121,
      "loss": 0.334,
      "step": 612
    },
    {
      "epoch": 0.18029411764705883,
      "grad_norm": 0.07678501307964325,
      "learning_rate": 0.00016418262150220912,
      "loss": 0.4058,
      "step": 613
    },
    {
      "epoch": 0.18058823529411766,
      "grad_norm": 0.05178012326359749,
      "learning_rate": 0.00016412371134020618,
      "loss": 0.381,
      "step": 614
    },
    {
      "epoch": 0.18088235294117647,
      "grad_norm": 0.05926099419593811,
      "learning_rate": 0.00016406480117820324,
      "loss": 0.4157,
      "step": 615
    },
    {
      "epoch": 0.1811764705882353,
      "grad_norm": 0.07076110690832138,
      "learning_rate": 0.0001640058910162003,
      "loss": 0.3461,
      "step": 616
    },
    {
      "epoch": 0.1814705882352941,
      "grad_norm": 0.06583809107542038,
      "learning_rate": 0.00016394698085419736,
      "loss": 0.3872,
      "step": 617
    },
    {
      "epoch": 0.18176470588235294,
      "grad_norm": 0.07160181552171707,
      "learning_rate": 0.0001638880706921944,
      "loss": 0.3967,
      "step": 618
    },
    {
      "epoch": 0.18205882352941177,
      "grad_norm": 0.048308733850717545,
      "learning_rate": 0.00016382916053019145,
      "loss": 0.3049,
      "step": 619
    },
    {
      "epoch": 0.18235294117647058,
      "grad_norm": 0.058495160192251205,
      "learning_rate": 0.00016377025036818851,
      "loss": 0.3789,
      "step": 620
    },
    {
      "epoch": 0.1826470588235294,
      "grad_norm": 0.04383595287799835,
      "learning_rate": 0.00016371134020618558,
      "loss": 0.3103,
      "step": 621
    },
    {
      "epoch": 0.18294117647058825,
      "grad_norm": 0.05879472196102142,
      "learning_rate": 0.00016365243004418264,
      "loss": 0.3974,
      "step": 622
    },
    {
      "epoch": 0.18323529411764705,
      "grad_norm": 0.0548536442220211,
      "learning_rate": 0.00016359351988217967,
      "loss": 0.3502,
      "step": 623
    },
    {
      "epoch": 0.18352941176470589,
      "grad_norm": 0.05755329132080078,
      "learning_rate": 0.00016353460972017673,
      "loss": 0.3722,
      "step": 624
    },
    {
      "epoch": 0.18382352941176472,
      "grad_norm": 0.04892938584089279,
      "learning_rate": 0.0001634756995581738,
      "loss": 0.3728,
      "step": 625
    },
    {
      "epoch": 0.18411764705882352,
      "grad_norm": 0.06282085925340652,
      "learning_rate": 0.00016341678939617085,
      "loss": 0.3796,
      "step": 626
    },
    {
      "epoch": 0.18441176470588236,
      "grad_norm": 0.07548104226589203,
      "learning_rate": 0.0001633578792341679,
      "loss": 0.348,
      "step": 627
    },
    {
      "epoch": 0.18470588235294116,
      "grad_norm": 0.060299310833215714,
      "learning_rate": 0.00016329896907216494,
      "loss": 0.3733,
      "step": 628
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.06596008688211441,
      "learning_rate": 0.000163240058910162,
      "loss": 0.3755,
      "step": 629
    },
    {
      "epoch": 0.18529411764705883,
      "grad_norm": 0.0637299120426178,
      "learning_rate": 0.00016318114874815906,
      "loss": 0.4158,
      "step": 630
    },
    {
      "epoch": 0.18558823529411764,
      "grad_norm": 0.046125706285238266,
      "learning_rate": 0.00016312223858615612,
      "loss": 0.2814,
      "step": 631
    },
    {
      "epoch": 0.18588235294117647,
      "grad_norm": 0.05006759986281395,
      "learning_rate": 0.00016306332842415318,
      "loss": 0.3306,
      "step": 632
    },
    {
      "epoch": 0.1861764705882353,
      "grad_norm": 0.061839934438467026,
      "learning_rate": 0.00016300441826215022,
      "loss": 0.3348,
      "step": 633
    },
    {
      "epoch": 0.1864705882352941,
      "grad_norm": 0.045693445950746536,
      "learning_rate": 0.00016294550810014728,
      "loss": 0.3086,
      "step": 634
    },
    {
      "epoch": 0.18676470588235294,
      "grad_norm": 0.04219239205121994,
      "learning_rate": 0.00016288659793814434,
      "loss": 0.2584,
      "step": 635
    },
    {
      "epoch": 0.18705882352941178,
      "grad_norm": 0.06212228536605835,
      "learning_rate": 0.0001628276877761414,
      "loss": 0.3693,
      "step": 636
    },
    {
      "epoch": 0.18735294117647058,
      "grad_norm": 0.09370388835668564,
      "learning_rate": 0.00016276877761413846,
      "loss": 0.4263,
      "step": 637
    },
    {
      "epoch": 0.18764705882352942,
      "grad_norm": 0.05876070633530617,
      "learning_rate": 0.0001627098674521355,
      "loss": 0.3344,
      "step": 638
    },
    {
      "epoch": 0.18794117647058822,
      "grad_norm": 0.05298371613025665,
      "learning_rate": 0.00016265095729013255,
      "loss": 0.2744,
      "step": 639
    },
    {
      "epoch": 0.18823529411764706,
      "grad_norm": 0.07696347683668137,
      "learning_rate": 0.0001625920471281296,
      "loss": 0.3967,
      "step": 640
    },
    {
      "epoch": 0.1885294117647059,
      "grad_norm": 0.06308316439390182,
      "learning_rate": 0.00016253313696612667,
      "loss": 0.3709,
      "step": 641
    },
    {
      "epoch": 0.1888235294117647,
      "grad_norm": 0.06144486740231514,
      "learning_rate": 0.00016247422680412373,
      "loss": 0.4177,
      "step": 642
    },
    {
      "epoch": 0.18911764705882353,
      "grad_norm": 0.07784160226583481,
      "learning_rate": 0.00016241531664212076,
      "loss": 0.3983,
      "step": 643
    },
    {
      "epoch": 0.18941176470588236,
      "grad_norm": 0.057016704231500626,
      "learning_rate": 0.00016235640648011782,
      "loss": 0.3166,
      "step": 644
    },
    {
      "epoch": 0.18970588235294117,
      "grad_norm": 0.08243582397699356,
      "learning_rate": 0.00016229749631811488,
      "loss": 0.374,
      "step": 645
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.061202920973300934,
      "learning_rate": 0.00016223858615611194,
      "loss": 0.3611,
      "step": 646
    },
    {
      "epoch": 0.19029411764705884,
      "grad_norm": 0.07230719923973083,
      "learning_rate": 0.000162179675994109,
      "loss": 0.4247,
      "step": 647
    },
    {
      "epoch": 0.19058823529411764,
      "grad_norm": 0.07778714597225189,
      "learning_rate": 0.00016212076583210604,
      "loss": 0.3991,
      "step": 648
    },
    {
      "epoch": 0.19088235294117648,
      "grad_norm": 0.06589809060096741,
      "learning_rate": 0.0001620618556701031,
      "loss": 0.3584,
      "step": 649
    },
    {
      "epoch": 0.19117647058823528,
      "grad_norm": 0.05107000842690468,
      "learning_rate": 0.00016200294550810016,
      "loss": 0.3666,
      "step": 650
    },
    {
      "epoch": 0.19147058823529411,
      "grad_norm": 0.0816480964422226,
      "learning_rate": 0.00016194403534609722,
      "loss": 0.389,
      "step": 651
    },
    {
      "epoch": 0.19176470588235295,
      "grad_norm": 0.07752407342195511,
      "learning_rate": 0.00016188512518409428,
      "loss": 0.3763,
      "step": 652
    },
    {
      "epoch": 0.19205882352941175,
      "grad_norm": 0.061488594859838486,
      "learning_rate": 0.0001618262150220913,
      "loss": 0.3669,
      "step": 653
    },
    {
      "epoch": 0.1923529411764706,
      "grad_norm": 0.06238222122192383,
      "learning_rate": 0.00016176730486008837,
      "loss": 0.3658,
      "step": 654
    },
    {
      "epoch": 0.19264705882352942,
      "grad_norm": 0.055362846702337265,
      "learning_rate": 0.00016170839469808543,
      "loss": 0.3294,
      "step": 655
    },
    {
      "epoch": 0.19294117647058823,
      "grad_norm": 0.03464547544717789,
      "learning_rate": 0.0001616494845360825,
      "loss": 0.1934,
      "step": 656
    },
    {
      "epoch": 0.19323529411764706,
      "grad_norm": 0.06280943006277084,
      "learning_rate": 0.00016159057437407955,
      "loss": 0.3296,
      "step": 657
    },
    {
      "epoch": 0.1935294117647059,
      "grad_norm": 0.08701840043067932,
      "learning_rate": 0.00016153166421207658,
      "loss": 0.3787,
      "step": 658
    },
    {
      "epoch": 0.1938235294117647,
      "grad_norm": 0.06059117242693901,
      "learning_rate": 0.00016147275405007364,
      "loss": 0.381,
      "step": 659
    },
    {
      "epoch": 0.19411764705882353,
      "grad_norm": 0.0558205284178257,
      "learning_rate": 0.0001614138438880707,
      "loss": 0.3544,
      "step": 660
    },
    {
      "epoch": 0.19441176470588234,
      "grad_norm": 0.07405561208724976,
      "learning_rate": 0.00016135493372606776,
      "loss": 0.399,
      "step": 661
    },
    {
      "epoch": 0.19470588235294117,
      "grad_norm": 0.06833494454622269,
      "learning_rate": 0.00016129602356406482,
      "loss": 0.2867,
      "step": 662
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.07515819370746613,
      "learning_rate": 0.00016123711340206186,
      "loss": 0.3996,
      "step": 663
    },
    {
      "epoch": 0.1952941176470588,
      "grad_norm": 0.06094181165099144,
      "learning_rate": 0.00016117820324005892,
      "loss": 0.3654,
      "step": 664
    },
    {
      "epoch": 0.19558823529411765,
      "grad_norm": 0.053042154759168625,
      "learning_rate": 0.00016111929307805598,
      "loss": 0.3563,
      "step": 665
    },
    {
      "epoch": 0.19588235294117648,
      "grad_norm": 0.06328105926513672,
      "learning_rate": 0.00016106038291605304,
      "loss": 0.3281,
      "step": 666
    },
    {
      "epoch": 0.19617647058823529,
      "grad_norm": 0.07407777011394501,
      "learning_rate": 0.0001610014727540501,
      "loss": 0.3632,
      "step": 667
    },
    {
      "epoch": 0.19647058823529412,
      "grad_norm": 0.08257772028446198,
      "learning_rate": 0.00016094256259204713,
      "loss": 0.3989,
      "step": 668
    },
    {
      "epoch": 0.19676470588235295,
      "grad_norm": 0.05990353599190712,
      "learning_rate": 0.0001608836524300442,
      "loss": 0.3657,
      "step": 669
    },
    {
      "epoch": 0.19705882352941176,
      "grad_norm": 0.043752070516347885,
      "learning_rate": 0.00016082474226804125,
      "loss": 0.3084,
      "step": 670
    },
    {
      "epoch": 0.1973529411764706,
      "grad_norm": 0.05810614302754402,
      "learning_rate": 0.0001607658321060383,
      "loss": 0.3596,
      "step": 671
    },
    {
      "epoch": 0.1976470588235294,
      "grad_norm": 0.08010239899158478,
      "learning_rate": 0.00016070692194403537,
      "loss": 0.3338,
      "step": 672
    },
    {
      "epoch": 0.19794117647058823,
      "grad_norm": 0.05676313862204552,
      "learning_rate": 0.0001606480117820324,
      "loss": 0.3452,
      "step": 673
    },
    {
      "epoch": 0.19823529411764707,
      "grad_norm": 0.048610348254442215,
      "learning_rate": 0.00016058910162002947,
      "loss": 0.3765,
      "step": 674
    },
    {
      "epoch": 0.19852941176470587,
      "grad_norm": 0.0690016895532608,
      "learning_rate": 0.00016053019145802653,
      "loss": 0.4122,
      "step": 675
    },
    {
      "epoch": 0.1988235294117647,
      "grad_norm": 0.07021305710077286,
      "learning_rate": 0.00016047128129602359,
      "loss": 0.4106,
      "step": 676
    },
    {
      "epoch": 0.19911764705882354,
      "grad_norm": 0.04867769405245781,
      "learning_rate": 0.00016041237113402065,
      "loss": 0.3399,
      "step": 677
    },
    {
      "epoch": 0.19941176470588234,
      "grad_norm": 0.04649893194437027,
      "learning_rate": 0.00016035346097201768,
      "loss": 0.3193,
      "step": 678
    },
    {
      "epoch": 0.19970588235294118,
      "grad_norm": 0.05922798439860344,
      "learning_rate": 0.00016029455081001474,
      "loss": 0.4132,
      "step": 679
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.060143593698740005,
      "learning_rate": 0.0001602356406480118,
      "loss": 0.3452,
      "step": 680
    },
    {
      "epoch": 0.20029411764705882,
      "grad_norm": 0.07750161737203598,
      "learning_rate": 0.00016017673048600886,
      "loss": 0.41,
      "step": 681
    },
    {
      "epoch": 0.20058823529411765,
      "grad_norm": 0.048696331679821014,
      "learning_rate": 0.00016011782032400592,
      "loss": 0.3681,
      "step": 682
    },
    {
      "epoch": 0.20088235294117648,
      "grad_norm": 0.04318804666399956,
      "learning_rate": 0.00016005891016200295,
      "loss": 0.3574,
      "step": 683
    },
    {
      "epoch": 0.2011764705882353,
      "grad_norm": 0.04688246548175812,
      "learning_rate": 0.00016,
      "loss": 0.2949,
      "step": 684
    },
    {
      "epoch": 0.20147058823529412,
      "grad_norm": 0.05895058065652847,
      "learning_rate": 0.00015994108983799707,
      "loss": 0.2987,
      "step": 685
    },
    {
      "epoch": 0.20176470588235293,
      "grad_norm": 0.060919392853975296,
      "learning_rate": 0.00015988217967599413,
      "loss": 0.3785,
      "step": 686
    },
    {
      "epoch": 0.20205882352941176,
      "grad_norm": 0.04055802524089813,
      "learning_rate": 0.0001598232695139912,
      "loss": 0.3251,
      "step": 687
    },
    {
      "epoch": 0.2023529411764706,
      "grad_norm": 0.04547673463821411,
      "learning_rate": 0.00015976435935198823,
      "loss": 0.298,
      "step": 688
    },
    {
      "epoch": 0.2026470588235294,
      "grad_norm": 0.04394799470901489,
      "learning_rate": 0.0001597054491899853,
      "loss": 0.3562,
      "step": 689
    },
    {
      "epoch": 0.20294117647058824,
      "grad_norm": 0.043902646750211716,
      "learning_rate": 0.00015964653902798235,
      "loss": 0.2803,
      "step": 690
    },
    {
      "epoch": 0.20323529411764707,
      "grad_norm": 0.048105254769325256,
      "learning_rate": 0.0001595876288659794,
      "loss": 0.2995,
      "step": 691
    },
    {
      "epoch": 0.20352941176470588,
      "grad_norm": 0.06300553679466248,
      "learning_rate": 0.00015952871870397647,
      "loss": 0.3867,
      "step": 692
    },
    {
      "epoch": 0.2038235294117647,
      "grad_norm": 0.05889274924993515,
      "learning_rate": 0.0001594698085419735,
      "loss": 0.4048,
      "step": 693
    },
    {
      "epoch": 0.20411764705882354,
      "grad_norm": 0.06334283947944641,
      "learning_rate": 0.00015941089837997056,
      "loss": 0.3586,
      "step": 694
    },
    {
      "epoch": 0.20441176470588235,
      "grad_norm": 0.051668889820575714,
      "learning_rate": 0.0001593519882179676,
      "loss": 0.3516,
      "step": 695
    },
    {
      "epoch": 0.20470588235294118,
      "grad_norm": 0.06799167394638062,
      "learning_rate": 0.00015929307805596465,
      "loss": 0.3773,
      "step": 696
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.0612388551235199,
      "learning_rate": 0.00015923416789396171,
      "loss": 0.4039,
      "step": 697
    },
    {
      "epoch": 0.20529411764705882,
      "grad_norm": 0.04965255409479141,
      "learning_rate": 0.00015917525773195875,
      "loss": 0.3389,
      "step": 698
    },
    {
      "epoch": 0.20558823529411765,
      "grad_norm": 0.05114796757698059,
      "learning_rate": 0.0001591163475699558,
      "loss": 0.3303,
      "step": 699
    },
    {
      "epoch": 0.20588235294117646,
      "grad_norm": 0.05628202483057976,
      "learning_rate": 0.00015905743740795287,
      "loss": 0.3742,
      "step": 700
    },
    {
      "epoch": 0.20588235294117646,
      "eval_loss": 0.361730694770813,
      "eval_runtime": 214.0831,
      "eval_samples_per_second": 0.467,
      "eval_steps_per_second": 0.467,
      "step": 700
    },
    {
      "epoch": 0.2061764705882353,
      "grad_norm": 0.05948459729552269,
      "learning_rate": 0.00015899852724594993,
      "loss": 0.3456,
      "step": 701
    },
    {
      "epoch": 0.20647058823529413,
      "grad_norm": 0.05347592011094093,
      "learning_rate": 0.000158939617083947,
      "loss": 0.3481,
      "step": 702
    },
    {
      "epoch": 0.20676470588235293,
      "grad_norm": 0.06056566908955574,
      "learning_rate": 0.00015888070692194402,
      "loss": 0.3424,
      "step": 703
    },
    {
      "epoch": 0.20705882352941177,
      "grad_norm": 0.05373469367623329,
      "learning_rate": 0.00015882179675994108,
      "loss": 0.3288,
      "step": 704
    },
    {
      "epoch": 0.2073529411764706,
      "grad_norm": 0.07597513496875763,
      "learning_rate": 0.00015876288659793814,
      "loss": 0.4784,
      "step": 705
    },
    {
      "epoch": 0.2076470588235294,
      "grad_norm": 0.06247251480817795,
      "learning_rate": 0.0001587039764359352,
      "loss": 0.3888,
      "step": 706
    },
    {
      "epoch": 0.20794117647058824,
      "grad_norm": 0.05527494102716446,
      "learning_rate": 0.00015864506627393226,
      "loss": 0.3633,
      "step": 707
    },
    {
      "epoch": 0.20823529411764705,
      "grad_norm": 0.048217445611953735,
      "learning_rate": 0.0001585861561119293,
      "loss": 0.2706,
      "step": 708
    },
    {
      "epoch": 0.20852941176470588,
      "grad_norm": 0.06454204022884369,
      "learning_rate": 0.00015852724594992635,
      "loss": 0.4145,
      "step": 709
    },
    {
      "epoch": 0.2088235294117647,
      "grad_norm": 0.041237641125917435,
      "learning_rate": 0.00015846833578792341,
      "loss": 0.2948,
      "step": 710
    },
    {
      "epoch": 0.20911764705882352,
      "grad_norm": 0.06086420267820358,
      "learning_rate": 0.00015840942562592047,
      "loss": 0.3185,
      "step": 711
    },
    {
      "epoch": 0.20941176470588235,
      "grad_norm": 0.06858322024345398,
      "learning_rate": 0.00015835051546391754,
      "loss": 0.3848,
      "step": 712
    },
    {
      "epoch": 0.2097058823529412,
      "grad_norm": 0.048601049929857254,
      "learning_rate": 0.00015829160530191457,
      "loss": 0.3518,
      "step": 713
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.04531221464276314,
      "learning_rate": 0.00015823269513991163,
      "loss": 0.331,
      "step": 714
    },
    {
      "epoch": 0.21029411764705883,
      "grad_norm": 0.058414533734321594,
      "learning_rate": 0.0001581737849779087,
      "loss": 0.3073,
      "step": 715
    },
    {
      "epoch": 0.21058823529411766,
      "grad_norm": 0.057046469300985336,
      "learning_rate": 0.00015811487481590575,
      "loss": 0.3519,
      "step": 716
    },
    {
      "epoch": 0.21088235294117647,
      "grad_norm": 0.06479691714048386,
      "learning_rate": 0.0001580559646539028,
      "loss": 0.3993,
      "step": 717
    },
    {
      "epoch": 0.2111764705882353,
      "grad_norm": 0.056813426315784454,
      "learning_rate": 0.00015799705449189984,
      "loss": 0.4067,
      "step": 718
    },
    {
      "epoch": 0.2114705882352941,
      "grad_norm": 0.04687907546758652,
      "learning_rate": 0.0001579381443298969,
      "loss": 0.3394,
      "step": 719
    },
    {
      "epoch": 0.21176470588235294,
      "grad_norm": 0.056302499026060104,
      "learning_rate": 0.00015787923416789396,
      "loss": 0.3572,
      "step": 720
    },
    {
      "epoch": 0.21205882352941177,
      "grad_norm": 0.04624346271157265,
      "learning_rate": 0.00015782032400589102,
      "loss": 0.2938,
      "step": 721
    },
    {
      "epoch": 0.21235294117647058,
      "grad_norm": 0.0647648349404335,
      "learning_rate": 0.00015776141384388808,
      "loss": 0.3502,
      "step": 722
    },
    {
      "epoch": 0.2126470588235294,
      "grad_norm": 0.06510554254055023,
      "learning_rate": 0.00015770250368188512,
      "loss": 0.3915,
      "step": 723
    },
    {
      "epoch": 0.21294117647058824,
      "grad_norm": 0.05733500048518181,
      "learning_rate": 0.00015764359351988218,
      "loss": 0.3201,
      "step": 724
    },
    {
      "epoch": 0.21323529411764705,
      "grad_norm": 0.048588450998067856,
      "learning_rate": 0.00015758468335787924,
      "loss": 0.3008,
      "step": 725
    },
    {
      "epoch": 0.21352941176470588,
      "grad_norm": 0.06594447046518326,
      "learning_rate": 0.0001575257731958763,
      "loss": 0.2949,
      "step": 726
    },
    {
      "epoch": 0.21382352941176472,
      "grad_norm": 0.06547994911670685,
      "learning_rate": 0.00015746686303387336,
      "loss": 0.3979,
      "step": 727
    },
    {
      "epoch": 0.21411764705882352,
      "grad_norm": 0.05505523830652237,
      "learning_rate": 0.0001574079528718704,
      "loss": 0.3866,
      "step": 728
    },
    {
      "epoch": 0.21441176470588236,
      "grad_norm": 0.06508039683103561,
      "learning_rate": 0.00015734904270986745,
      "loss": 0.4126,
      "step": 729
    },
    {
      "epoch": 0.21470588235294116,
      "grad_norm": 0.061068419367074966,
      "learning_rate": 0.0001572901325478645,
      "loss": 0.4058,
      "step": 730
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.05459418520331383,
      "learning_rate": 0.00015723122238586157,
      "loss": 0.3616,
      "step": 731
    },
    {
      "epoch": 0.21529411764705883,
      "grad_norm": 0.0439116507768631,
      "learning_rate": 0.00015717231222385863,
      "loss": 0.2966,
      "step": 732
    },
    {
      "epoch": 0.21558823529411764,
      "grad_norm": 0.07470805943012238,
      "learning_rate": 0.00015711340206185566,
      "loss": 0.419,
      "step": 733
    },
    {
      "epoch": 0.21588235294117647,
      "grad_norm": 0.053361207246780396,
      "learning_rate": 0.00015705449189985272,
      "loss": 0.3912,
      "step": 734
    },
    {
      "epoch": 0.2161764705882353,
      "grad_norm": 0.04712097719311714,
      "learning_rate": 0.00015699558173784978,
      "loss": 0.3297,
      "step": 735
    },
    {
      "epoch": 0.2164705882352941,
      "grad_norm": 0.053678762167692184,
      "learning_rate": 0.00015693667157584684,
      "loss": 0.3539,
      "step": 736
    },
    {
      "epoch": 0.21676470588235294,
      "grad_norm": 0.05292781442403793,
      "learning_rate": 0.0001568777614138439,
      "loss": 0.3822,
      "step": 737
    },
    {
      "epoch": 0.21705882352941178,
      "grad_norm": 0.050008561462163925,
      "learning_rate": 0.00015681885125184094,
      "loss": 0.3933,
      "step": 738
    },
    {
      "epoch": 0.21735294117647058,
      "grad_norm": 0.04230659827589989,
      "learning_rate": 0.000156759941089838,
      "loss": 0.33,
      "step": 739
    },
    {
      "epoch": 0.21764705882352942,
      "grad_norm": 0.03863969445228577,
      "learning_rate": 0.00015670103092783506,
      "loss": 0.2801,
      "step": 740
    },
    {
      "epoch": 0.21794117647058822,
      "grad_norm": 0.030913060531020164,
      "learning_rate": 0.00015664212076583212,
      "loss": 0.2478,
      "step": 741
    },
    {
      "epoch": 0.21823529411764706,
      "grad_norm": 0.06645620614290237,
      "learning_rate": 0.00015658321060382918,
      "loss": 0.4355,
      "step": 742
    },
    {
      "epoch": 0.2185294117647059,
      "grad_norm": 0.03781459107995033,
      "learning_rate": 0.0001565243004418262,
      "loss": 0.2876,
      "step": 743
    },
    {
      "epoch": 0.2188235294117647,
      "grad_norm": 0.05441611260175705,
      "learning_rate": 0.00015646539027982327,
      "loss": 0.3225,
      "step": 744
    },
    {
      "epoch": 0.21911764705882353,
      "grad_norm": 0.049760717898607254,
      "learning_rate": 0.00015640648011782033,
      "loss": 0.3591,
      "step": 745
    },
    {
      "epoch": 0.21941176470588236,
      "grad_norm": 0.06499160081148148,
      "learning_rate": 0.0001563475699558174,
      "loss": 0.4109,
      "step": 746
    },
    {
      "epoch": 0.21970588235294117,
      "grad_norm": 0.05624934285879135,
      "learning_rate": 0.00015628865979381445,
      "loss": 0.3165,
      "step": 747
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.04839985445141792,
      "learning_rate": 0.00015622974963181148,
      "loss": 0.346,
      "step": 748
    },
    {
      "epoch": 0.22029411764705883,
      "grad_norm": 0.03974880278110504,
      "learning_rate": 0.00015617083946980854,
      "loss": 0.2832,
      "step": 749
    },
    {
      "epoch": 0.22058823529411764,
      "grad_norm": 0.05842820182442665,
      "learning_rate": 0.0001561119293078056,
      "loss": 0.3802,
      "step": 750
    },
    {
      "epoch": 0.22088235294117647,
      "grad_norm": 0.04338705912232399,
      "learning_rate": 0.00015605301914580266,
      "loss": 0.3508,
      "step": 751
    },
    {
      "epoch": 0.2211764705882353,
      "grad_norm": 0.07235657423734665,
      "learning_rate": 0.00015599410898379972,
      "loss": 0.4419,
      "step": 752
    },
    {
      "epoch": 0.2214705882352941,
      "grad_norm": 0.053328510373830795,
      "learning_rate": 0.00015593519882179676,
      "loss": 0.3962,
      "step": 753
    },
    {
      "epoch": 0.22176470588235295,
      "grad_norm": 0.049274660646915436,
      "learning_rate": 0.00015587628865979382,
      "loss": 0.3595,
      "step": 754
    },
    {
      "epoch": 0.22205882352941175,
      "grad_norm": 0.05177650973200798,
      "learning_rate": 0.00015581737849779088,
      "loss": 0.3306,
      "step": 755
    },
    {
      "epoch": 0.2223529411764706,
      "grad_norm": 0.040614791214466095,
      "learning_rate": 0.00015575846833578794,
      "loss": 0.3338,
      "step": 756
    },
    {
      "epoch": 0.22264705882352942,
      "grad_norm": 0.07788284868001938,
      "learning_rate": 0.000155699558173785,
      "loss": 0.3596,
      "step": 757
    },
    {
      "epoch": 0.22294117647058823,
      "grad_norm": 0.06221330165863037,
      "learning_rate": 0.00015564064801178203,
      "loss": 0.3348,
      "step": 758
    },
    {
      "epoch": 0.22323529411764706,
      "grad_norm": 0.058369576930999756,
      "learning_rate": 0.0001555817378497791,
      "loss": 0.3426,
      "step": 759
    },
    {
      "epoch": 0.2235294117647059,
      "grad_norm": 0.046003568917512894,
      "learning_rate": 0.00015552282768777615,
      "loss": 0.3419,
      "step": 760
    },
    {
      "epoch": 0.2238235294117647,
      "grad_norm": 0.061616189777851105,
      "learning_rate": 0.0001554639175257732,
      "loss": 0.3606,
      "step": 761
    },
    {
      "epoch": 0.22411764705882353,
      "grad_norm": 0.0430360846221447,
      "learning_rate": 0.00015540500736377027,
      "loss": 0.3445,
      "step": 762
    },
    {
      "epoch": 0.22441176470588237,
      "grad_norm": 0.05893903225660324,
      "learning_rate": 0.0001553460972017673,
      "loss": 0.3413,
      "step": 763
    },
    {
      "epoch": 0.22470588235294117,
      "grad_norm": 0.05125729739665985,
      "learning_rate": 0.00015528718703976437,
      "loss": 0.395,
      "step": 764
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.04579239338636398,
      "learning_rate": 0.00015522827687776143,
      "loss": 0.3556,
      "step": 765
    },
    {
      "epoch": 0.2252941176470588,
      "grad_norm": 0.04881760850548744,
      "learning_rate": 0.00015516936671575849,
      "loss": 0.361,
      "step": 766
    },
    {
      "epoch": 0.22558823529411764,
      "grad_norm": 0.0862881988286972,
      "learning_rate": 0.00015511045655375555,
      "loss": 0.4207,
      "step": 767
    },
    {
      "epoch": 0.22588235294117648,
      "grad_norm": 0.05988311395049095,
      "learning_rate": 0.00015505154639175258,
      "loss": 0.3058,
      "step": 768
    },
    {
      "epoch": 0.22617647058823528,
      "grad_norm": 0.057993076741695404,
      "learning_rate": 0.00015499263622974964,
      "loss": 0.3631,
      "step": 769
    },
    {
      "epoch": 0.22647058823529412,
      "grad_norm": 0.06680333614349365,
      "learning_rate": 0.0001549337260677467,
      "loss": 0.3842,
      "step": 770
    },
    {
      "epoch": 0.22676470588235295,
      "grad_norm": 0.05164854973554611,
      "learning_rate": 0.00015487481590574376,
      "loss": 0.3587,
      "step": 771
    },
    {
      "epoch": 0.22705882352941176,
      "grad_norm": 0.051870714873075485,
      "learning_rate": 0.00015481590574374082,
      "loss": 0.301,
      "step": 772
    },
    {
      "epoch": 0.2273529411764706,
      "grad_norm": 0.0430571585893631,
      "learning_rate": 0.00015475699558173785,
      "loss": 0.2995,
      "step": 773
    },
    {
      "epoch": 0.22764705882352942,
      "grad_norm": 0.043372347950935364,
      "learning_rate": 0.0001546980854197349,
      "loss": 0.3693,
      "step": 774
    },
    {
      "epoch": 0.22794117647058823,
      "grad_norm": 0.05476806312799454,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.4043,
      "step": 775
    },
    {
      "epoch": 0.22823529411764706,
      "grad_norm": 0.04658502712845802,
      "learning_rate": 0.00015458026509572903,
      "loss": 0.3087,
      "step": 776
    },
    {
      "epoch": 0.22852941176470587,
      "grad_norm": 0.05170102417469025,
      "learning_rate": 0.0001545213549337261,
      "loss": 0.3543,
      "step": 777
    },
    {
      "epoch": 0.2288235294117647,
      "grad_norm": 0.050508178770542145,
      "learning_rate": 0.00015446244477172313,
      "loss": 0.2676,
      "step": 778
    },
    {
      "epoch": 0.22911764705882354,
      "grad_norm": 0.04520793631672859,
      "learning_rate": 0.0001544035346097202,
      "loss": 0.3243,
      "step": 779
    },
    {
      "epoch": 0.22941176470588234,
      "grad_norm": 0.044667504727840424,
      "learning_rate": 0.00015434462444771725,
      "loss": 0.2648,
      "step": 780
    },
    {
      "epoch": 0.22970588235294118,
      "grad_norm": 0.04910311847925186,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.3699,
      "step": 781
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.05556497722864151,
      "learning_rate": 0.00015422680412371137,
      "loss": 0.376,
      "step": 782
    },
    {
      "epoch": 0.23029411764705882,
      "grad_norm": 0.05940410867333412,
      "learning_rate": 0.0001541678939617084,
      "loss": 0.4036,
      "step": 783
    },
    {
      "epoch": 0.23058823529411765,
      "grad_norm": 0.05170710012316704,
      "learning_rate": 0.00015410898379970546,
      "loss": 0.3337,
      "step": 784
    },
    {
      "epoch": 0.23088235294117648,
      "grad_norm": 0.05576696619391441,
      "learning_rate": 0.00015405007363770252,
      "loss": 0.367,
      "step": 785
    },
    {
      "epoch": 0.2311764705882353,
      "grad_norm": 0.04955188184976578,
      "learning_rate": 0.00015399116347569958,
      "loss": 0.3255,
      "step": 786
    },
    {
      "epoch": 0.23147058823529412,
      "grad_norm": 0.052175652235746384,
      "learning_rate": 0.00015393225331369664,
      "loss": 0.3553,
      "step": 787
    },
    {
      "epoch": 0.23176470588235293,
      "grad_norm": 0.06152089312672615,
      "learning_rate": 0.00015387334315169367,
      "loss": 0.3847,
      "step": 788
    },
    {
      "epoch": 0.23205882352941176,
      "grad_norm": 0.055384524166584015,
      "learning_rate": 0.00015381443298969073,
      "loss": 0.3999,
      "step": 789
    },
    {
      "epoch": 0.2323529411764706,
      "grad_norm": 0.05208185315132141,
      "learning_rate": 0.0001537555228276878,
      "loss": 0.4138,
      "step": 790
    },
    {
      "epoch": 0.2326470588235294,
      "grad_norm": 0.054259367287158966,
      "learning_rate": 0.00015369661266568485,
      "loss": 0.3869,
      "step": 791
    },
    {
      "epoch": 0.23294117647058823,
      "grad_norm": 0.05241622403264046,
      "learning_rate": 0.00015363770250368191,
      "loss": 0.3169,
      "step": 792
    },
    {
      "epoch": 0.23323529411764707,
      "grad_norm": 0.0491480678319931,
      "learning_rate": 0.00015357879234167895,
      "loss": 0.387,
      "step": 793
    },
    {
      "epoch": 0.23352941176470587,
      "grad_norm": 0.057108499109745026,
      "learning_rate": 0.000153519882179676,
      "loss": 0.3523,
      "step": 794
    },
    {
      "epoch": 0.2338235294117647,
      "grad_norm": 0.053484465926885605,
      "learning_rate": 0.00015346097201767307,
      "loss": 0.3489,
      "step": 795
    },
    {
      "epoch": 0.23411764705882354,
      "grad_norm": 0.06675112992525101,
      "learning_rate": 0.00015340206185567013,
      "loss": 0.3654,
      "step": 796
    },
    {
      "epoch": 0.23441176470588235,
      "grad_norm": 0.053632237017154694,
      "learning_rate": 0.0001533431516936672,
      "loss": 0.3115,
      "step": 797
    },
    {
      "epoch": 0.23470588235294118,
      "grad_norm": 0.03884686157107353,
      "learning_rate": 0.00015328424153166422,
      "loss": 0.2709,
      "step": 798
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.05979926884174347,
      "learning_rate": 0.00015322533136966128,
      "loss": 0.3406,
      "step": 799
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.046870749443769455,
      "learning_rate": 0.00015316642120765834,
      "loss": 0.3245,
      "step": 800
    },
    {
      "epoch": 0.23529411764705882,
      "eval_loss": 0.36070603132247925,
      "eval_runtime": 214.9098,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 800
    },
    {
      "epoch": 0.23558823529411765,
      "grad_norm": 0.0703265517950058,
      "learning_rate": 0.00015310751104565537,
      "loss": 0.3716,
      "step": 801
    },
    {
      "epoch": 0.23588235294117646,
      "grad_norm": 0.0428219735622406,
      "learning_rate": 0.00015304860088365243,
      "loss": 0.3159,
      "step": 802
    },
    {
      "epoch": 0.2361764705882353,
      "grad_norm": 0.04364214837551117,
      "learning_rate": 0.00015298969072164947,
      "loss": 0.3344,
      "step": 803
    },
    {
      "epoch": 0.23647058823529413,
      "grad_norm": 0.0873560830950737,
      "learning_rate": 0.00015293078055964653,
      "loss": 0.4023,
      "step": 804
    },
    {
      "epoch": 0.23676470588235293,
      "grad_norm": 0.0571746900677681,
      "learning_rate": 0.0001528718703976436,
      "loss": 0.3798,
      "step": 805
    },
    {
      "epoch": 0.23705882352941177,
      "grad_norm": 0.06070447340607643,
      "learning_rate": 0.00015281296023564065,
      "loss": 0.3856,
      "step": 806
    },
    {
      "epoch": 0.2373529411764706,
      "grad_norm": 0.07413356006145477,
      "learning_rate": 0.0001527540500736377,
      "loss": 0.4296,
      "step": 807
    },
    {
      "epoch": 0.2376470588235294,
      "grad_norm": 0.05967044085264206,
      "learning_rate": 0.00015269513991163474,
      "loss": 0.3335,
      "step": 808
    },
    {
      "epoch": 0.23794117647058824,
      "grad_norm": 0.054330889135599136,
      "learning_rate": 0.0001526362297496318,
      "loss": 0.3731,
      "step": 809
    },
    {
      "epoch": 0.23823529411764705,
      "grad_norm": 0.05469048395752907,
      "learning_rate": 0.00015257731958762886,
      "loss": 0.3643,
      "step": 810
    },
    {
      "epoch": 0.23852941176470588,
      "grad_norm": 0.05839574337005615,
      "learning_rate": 0.00015251840942562592,
      "loss": 0.4054,
      "step": 811
    },
    {
      "epoch": 0.2388235294117647,
      "grad_norm": 0.06321358680725098,
      "learning_rate": 0.00015245949926362298,
      "loss": 0.3229,
      "step": 812
    },
    {
      "epoch": 0.23911764705882352,
      "grad_norm": 0.05567352473735809,
      "learning_rate": 0.00015240058910162002,
      "loss": 0.3676,
      "step": 813
    },
    {
      "epoch": 0.23941176470588235,
      "grad_norm": 0.04869804158806801,
      "learning_rate": 0.00015234167893961708,
      "loss": 0.3723,
      "step": 814
    },
    {
      "epoch": 0.23970588235294119,
      "grad_norm": 0.04963136464357376,
      "learning_rate": 0.00015228276877761414,
      "loss": 0.3282,
      "step": 815
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.04949852451682091,
      "learning_rate": 0.0001522238586156112,
      "loss": 0.3569,
      "step": 816
    },
    {
      "epoch": 0.24029411764705882,
      "grad_norm": 0.05772245675325394,
      "learning_rate": 0.00015216494845360826,
      "loss": 0.3151,
      "step": 817
    },
    {
      "epoch": 0.24058823529411766,
      "grad_norm": 0.06286825984716415,
      "learning_rate": 0.0001521060382916053,
      "loss": 0.356,
      "step": 818
    },
    {
      "epoch": 0.24088235294117646,
      "grad_norm": 0.04962434619665146,
      "learning_rate": 0.00015204712812960235,
      "loss": 0.3332,
      "step": 819
    },
    {
      "epoch": 0.2411764705882353,
      "grad_norm": 0.06126226484775543,
      "learning_rate": 0.0001519882179675994,
      "loss": 0.3859,
      "step": 820
    },
    {
      "epoch": 0.24147058823529413,
      "grad_norm": 0.06569595634937286,
      "learning_rate": 0.00015192930780559647,
      "loss": 0.3616,
      "step": 821
    },
    {
      "epoch": 0.24176470588235294,
      "grad_norm": 0.053810033947229385,
      "learning_rate": 0.00015187039764359353,
      "loss": 0.2952,
      "step": 822
    },
    {
      "epoch": 0.24205882352941177,
      "grad_norm": 0.07901117205619812,
      "learning_rate": 0.00015181148748159056,
      "loss": 0.3658,
      "step": 823
    },
    {
      "epoch": 0.24235294117647058,
      "grad_norm": 0.0573960617184639,
      "learning_rate": 0.00015175257731958762,
      "loss": 0.3566,
      "step": 824
    },
    {
      "epoch": 0.2426470588235294,
      "grad_norm": 0.0633251741528511,
      "learning_rate": 0.00015169366715758468,
      "loss": 0.3839,
      "step": 825
    },
    {
      "epoch": 0.24294117647058824,
      "grad_norm": 0.06958173960447311,
      "learning_rate": 0.00015163475699558174,
      "loss": 0.4815,
      "step": 826
    },
    {
      "epoch": 0.24323529411764705,
      "grad_norm": 0.061739731580019,
      "learning_rate": 0.0001515758468335788,
      "loss": 0.3624,
      "step": 827
    },
    {
      "epoch": 0.24352941176470588,
      "grad_norm": 0.0512876883149147,
      "learning_rate": 0.00015151693667157584,
      "loss": 0.3851,
      "step": 828
    },
    {
      "epoch": 0.24382352941176472,
      "grad_norm": 0.04965003579854965,
      "learning_rate": 0.0001514580265095729,
      "loss": 0.3836,
      "step": 829
    },
    {
      "epoch": 0.24411764705882352,
      "grad_norm": 0.03991016373038292,
      "learning_rate": 0.00015139911634756996,
      "loss": 0.2741,
      "step": 830
    },
    {
      "epoch": 0.24441176470588236,
      "grad_norm": 0.07161817699670792,
      "learning_rate": 0.00015134020618556702,
      "loss": 0.4191,
      "step": 831
    },
    {
      "epoch": 0.2447058823529412,
      "grad_norm": 0.05053377151489258,
      "learning_rate": 0.00015128129602356408,
      "loss": 0.3656,
      "step": 832
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.06886784732341766,
      "learning_rate": 0.0001512223858615611,
      "loss": 0.302,
      "step": 833
    },
    {
      "epoch": 0.24529411764705883,
      "grad_norm": 0.056182023137807846,
      "learning_rate": 0.00015116347569955817,
      "loss": 0.3499,
      "step": 834
    },
    {
      "epoch": 0.24558823529411763,
      "grad_norm": 0.06722873449325562,
      "learning_rate": 0.00015110456553755523,
      "loss": 0.3651,
      "step": 835
    },
    {
      "epoch": 0.24588235294117647,
      "grad_norm": 0.06928551197052002,
      "learning_rate": 0.0001510456553755523,
      "loss": 0.3442,
      "step": 836
    },
    {
      "epoch": 0.2461764705882353,
      "grad_norm": 0.04342685267329216,
      "learning_rate": 0.00015098674521354935,
      "loss": 0.3876,
      "step": 837
    },
    {
      "epoch": 0.2464705882352941,
      "grad_norm": 0.06129726767539978,
      "learning_rate": 0.00015092783505154638,
      "loss": 0.3739,
      "step": 838
    },
    {
      "epoch": 0.24676470588235294,
      "grad_norm": 0.0535118468105793,
      "learning_rate": 0.00015086892488954344,
      "loss": 0.3189,
      "step": 839
    },
    {
      "epoch": 0.24705882352941178,
      "grad_norm": 0.05913602560758591,
      "learning_rate": 0.0001508100147275405,
      "loss": 0.3226,
      "step": 840
    },
    {
      "epoch": 0.24735294117647058,
      "grad_norm": 0.06381912529468536,
      "learning_rate": 0.00015075110456553756,
      "loss": 0.4141,
      "step": 841
    },
    {
      "epoch": 0.24764705882352941,
      "grad_norm": 0.06733408570289612,
      "learning_rate": 0.00015069219440353462,
      "loss": 0.3329,
      "step": 842
    },
    {
      "epoch": 0.24794117647058825,
      "grad_norm": 0.05581289529800415,
      "learning_rate": 0.00015063328424153166,
      "loss": 0.3453,
      "step": 843
    },
    {
      "epoch": 0.24823529411764705,
      "grad_norm": 0.05781897157430649,
      "learning_rate": 0.00015057437407952872,
      "loss": 0.2994,
      "step": 844
    },
    {
      "epoch": 0.2485294117647059,
      "grad_norm": 0.060066986829042435,
      "learning_rate": 0.00015051546391752578,
      "loss": 0.3426,
      "step": 845
    },
    {
      "epoch": 0.2488235294117647,
      "grad_norm": 0.060956280678510666,
      "learning_rate": 0.00015045655375552284,
      "loss": 0.3351,
      "step": 846
    },
    {
      "epoch": 0.24911764705882353,
      "grad_norm": 0.053990401327610016,
      "learning_rate": 0.0001503976435935199,
      "loss": 0.3576,
      "step": 847
    },
    {
      "epoch": 0.24941176470588236,
      "grad_norm": 0.06148918345570564,
      "learning_rate": 0.00015033873343151693,
      "loss": 0.4017,
      "step": 848
    },
    {
      "epoch": 0.24970588235294117,
      "grad_norm": 0.05258846655488014,
      "learning_rate": 0.000150279823269514,
      "loss": 0.3335,
      "step": 849
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.06287950277328491,
      "learning_rate": 0.00015022091310751105,
      "loss": 0.4389,
      "step": 850
    },
    {
      "epoch": 0.25029411764705883,
      "grad_norm": 0.05930859223008156,
      "learning_rate": 0.0001501620029455081,
      "loss": 0.3823,
      "step": 851
    },
    {
      "epoch": 0.25058823529411767,
      "grad_norm": 0.05974937975406647,
      "learning_rate": 0.00015010309278350517,
      "loss": 0.3888,
      "step": 852
    },
    {
      "epoch": 0.25088235294117645,
      "grad_norm": 0.047430895268917084,
      "learning_rate": 0.0001500441826215022,
      "loss": 0.3934,
      "step": 853
    },
    {
      "epoch": 0.2511764705882353,
      "grad_norm": 0.05481480434536934,
      "learning_rate": 0.00014998527245949927,
      "loss": 0.3607,
      "step": 854
    },
    {
      "epoch": 0.2514705882352941,
      "grad_norm": 0.061703409999608994,
      "learning_rate": 0.00014992636229749633,
      "loss": 0.3602,
      "step": 855
    },
    {
      "epoch": 0.25176470588235295,
      "grad_norm": 0.05236892029643059,
      "learning_rate": 0.00014986745213549339,
      "loss": 0.3548,
      "step": 856
    },
    {
      "epoch": 0.2520588235294118,
      "grad_norm": 0.06785605102777481,
      "learning_rate": 0.00014980854197349045,
      "loss": 0.4444,
      "step": 857
    },
    {
      "epoch": 0.2523529411764706,
      "grad_norm": 0.0624309740960598,
      "learning_rate": 0.00014974963181148748,
      "loss": 0.397,
      "step": 858
    },
    {
      "epoch": 0.2526470588235294,
      "grad_norm": 0.05733226239681244,
      "learning_rate": 0.00014969072164948454,
      "loss": 0.3829,
      "step": 859
    },
    {
      "epoch": 0.2529411764705882,
      "grad_norm": 0.055794473737478256,
      "learning_rate": 0.0001496318114874816,
      "loss": 0.3355,
      "step": 860
    },
    {
      "epoch": 0.25323529411764706,
      "grad_norm": 0.04250465705990791,
      "learning_rate": 0.00014957290132547866,
      "loss": 0.2664,
      "step": 861
    },
    {
      "epoch": 0.2535294117647059,
      "grad_norm": 0.05331911891698837,
      "learning_rate": 0.00014951399116347572,
      "loss": 0.3417,
      "step": 862
    },
    {
      "epoch": 0.2538235294117647,
      "grad_norm": 0.05263544246554375,
      "learning_rate": 0.00014945508100147275,
      "loss": 0.3603,
      "step": 863
    },
    {
      "epoch": 0.2541176470588235,
      "grad_norm": 0.04400857165455818,
      "learning_rate": 0.0001493961708394698,
      "loss": 0.3568,
      "step": 864
    },
    {
      "epoch": 0.25441176470588234,
      "grad_norm": 0.05037229135632515,
      "learning_rate": 0.00014933726067746687,
      "loss": 0.4277,
      "step": 865
    },
    {
      "epoch": 0.25470588235294117,
      "grad_norm": 0.03948104754090309,
      "learning_rate": 0.00014927835051546393,
      "loss": 0.3081,
      "step": 866
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.052577387541532516,
      "learning_rate": 0.000149219440353461,
      "loss": 0.3495,
      "step": 867
    },
    {
      "epoch": 0.25529411764705884,
      "grad_norm": 0.05705944448709488,
      "learning_rate": 0.00014916053019145803,
      "loss": 0.41,
      "step": 868
    },
    {
      "epoch": 0.25558823529411767,
      "grad_norm": 0.051937736570835114,
      "learning_rate": 0.0001491016200294551,
      "loss": 0.3302,
      "step": 869
    },
    {
      "epoch": 0.25588235294117645,
      "grad_norm": 0.05710931494832039,
      "learning_rate": 0.00014904270986745215,
      "loss": 0.352,
      "step": 870
    },
    {
      "epoch": 0.2561764705882353,
      "grad_norm": 0.05482964590191841,
      "learning_rate": 0.0001489837997054492,
      "loss": 0.3548,
      "step": 871
    },
    {
      "epoch": 0.2564705882352941,
      "grad_norm": 0.06295016407966614,
      "learning_rate": 0.00014892488954344627,
      "loss": 0.3605,
      "step": 872
    },
    {
      "epoch": 0.25676470588235295,
      "grad_norm": 0.06074673309922218,
      "learning_rate": 0.0001488659793814433,
      "loss": 0.36,
      "step": 873
    },
    {
      "epoch": 0.2570588235294118,
      "grad_norm": 0.08104649186134338,
      "learning_rate": 0.00014880706921944036,
      "loss": 0.4031,
      "step": 874
    },
    {
      "epoch": 0.25735294117647056,
      "grad_norm": 0.0702260285615921,
      "learning_rate": 0.00014874815905743742,
      "loss": 0.4202,
      "step": 875
    },
    {
      "epoch": 0.2576470588235294,
      "grad_norm": 0.05512549355626106,
      "learning_rate": 0.00014868924889543448,
      "loss": 0.3606,
      "step": 876
    },
    {
      "epoch": 0.25794117647058823,
      "grad_norm": 0.046178918331861496,
      "learning_rate": 0.00014863033873343154,
      "loss": 0.3482,
      "step": 877
    },
    {
      "epoch": 0.25823529411764706,
      "grad_norm": 0.041780345141887665,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.3113,
      "step": 878
    },
    {
      "epoch": 0.2585294117647059,
      "grad_norm": 0.03710222616791725,
      "learning_rate": 0.00014851251840942563,
      "loss": 0.2505,
      "step": 879
    },
    {
      "epoch": 0.25882352941176473,
      "grad_norm": 0.055494584143161774,
      "learning_rate": 0.0001484536082474227,
      "loss": 0.3482,
      "step": 880
    },
    {
      "epoch": 0.2591176470588235,
      "grad_norm": 0.04734063521027565,
      "learning_rate": 0.00014839469808541975,
      "loss": 0.3332,
      "step": 881
    },
    {
      "epoch": 0.25941176470588234,
      "grad_norm": 0.05875244364142418,
      "learning_rate": 0.00014833578792341681,
      "loss": 0.3814,
      "step": 882
    },
    {
      "epoch": 0.2597058823529412,
      "grad_norm": 0.06047612428665161,
      "learning_rate": 0.00014827687776141385,
      "loss": 0.4207,
      "step": 883
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.053206901997327805,
      "learning_rate": 0.0001482179675994109,
      "loss": 0.3447,
      "step": 884
    },
    {
      "epoch": 0.26029411764705884,
      "grad_norm": 0.054574161767959595,
      "learning_rate": 0.00014815905743740797,
      "loss": 0.3899,
      "step": 885
    },
    {
      "epoch": 0.2605882352941176,
      "grad_norm": 0.055193495005369186,
      "learning_rate": 0.00014810014727540503,
      "loss": 0.3758,
      "step": 886
    },
    {
      "epoch": 0.26088235294117645,
      "grad_norm": 0.061861373484134674,
      "learning_rate": 0.0001480412371134021,
      "loss": 0.4514,
      "step": 887
    },
    {
      "epoch": 0.2611764705882353,
      "grad_norm": 0.06328039616346359,
      "learning_rate": 0.00014798232695139912,
      "loss": 0.3512,
      "step": 888
    },
    {
      "epoch": 0.2614705882352941,
      "grad_norm": 0.055281952023506165,
      "learning_rate": 0.00014792341678939618,
      "loss": 0.3483,
      "step": 889
    },
    {
      "epoch": 0.26176470588235295,
      "grad_norm": 0.050211429595947266,
      "learning_rate": 0.00014786450662739324,
      "loss": 0.2928,
      "step": 890
    },
    {
      "epoch": 0.2620588235294118,
      "grad_norm": 0.06085504963994026,
      "learning_rate": 0.0001478055964653903,
      "loss": 0.3739,
      "step": 891
    },
    {
      "epoch": 0.26235294117647057,
      "grad_norm": 0.050632916390895844,
      "learning_rate": 0.00014774668630338736,
      "loss": 0.297,
      "step": 892
    },
    {
      "epoch": 0.2626470588235294,
      "grad_norm": 0.0592765137553215,
      "learning_rate": 0.0001476877761413844,
      "loss": 0.3658,
      "step": 893
    },
    {
      "epoch": 0.26294117647058823,
      "grad_norm": 0.06128491833806038,
      "learning_rate": 0.00014762886597938146,
      "loss": 0.3383,
      "step": 894
    },
    {
      "epoch": 0.26323529411764707,
      "grad_norm": 0.050504181534051895,
      "learning_rate": 0.00014756995581737852,
      "loss": 0.3733,
      "step": 895
    },
    {
      "epoch": 0.2635294117647059,
      "grad_norm": 0.0494905561208725,
      "learning_rate": 0.00014751104565537558,
      "loss": 0.3046,
      "step": 896
    },
    {
      "epoch": 0.2638235294117647,
      "grad_norm": 0.0532960370182991,
      "learning_rate": 0.00014745213549337264,
      "loss": 0.3514,
      "step": 897
    },
    {
      "epoch": 0.2641176470588235,
      "grad_norm": 0.04775729775428772,
      "learning_rate": 0.00014739322533136967,
      "loss": 0.3666,
      "step": 898
    },
    {
      "epoch": 0.26441176470588235,
      "grad_norm": 0.05941201373934746,
      "learning_rate": 0.00014733431516936673,
      "loss": 0.3627,
      "step": 899
    },
    {
      "epoch": 0.2647058823529412,
      "grad_norm": 0.05919085443019867,
      "learning_rate": 0.0001472754050073638,
      "loss": 0.3586,
      "step": 900
    },
    {
      "epoch": 0.2647058823529412,
      "eval_loss": 0.36025717854499817,
      "eval_runtime": 215.1979,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 900
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.056728214025497437,
      "learning_rate": 0.00014721649484536085,
      "loss": 0.3747,
      "step": 901
    },
    {
      "epoch": 0.26529411764705885,
      "grad_norm": 0.05014482140541077,
      "learning_rate": 0.0001471575846833579,
      "loss": 0.3428,
      "step": 902
    },
    {
      "epoch": 0.2655882352941176,
      "grad_norm": 0.07538274675607681,
      "learning_rate": 0.00014709867452135494,
      "loss": 0.3977,
      "step": 903
    },
    {
      "epoch": 0.26588235294117646,
      "grad_norm": 0.0473269559442997,
      "learning_rate": 0.000147039764359352,
      "loss": 0.3414,
      "step": 904
    },
    {
      "epoch": 0.2661764705882353,
      "grad_norm": 0.05156073719263077,
      "learning_rate": 0.00014698085419734906,
      "loss": 0.3295,
      "step": 905
    },
    {
      "epoch": 0.2664705882352941,
      "grad_norm": 0.05122887343168259,
      "learning_rate": 0.00014692194403534612,
      "loss": 0.4378,
      "step": 906
    },
    {
      "epoch": 0.26676470588235296,
      "grad_norm": 0.05170300975441933,
      "learning_rate": 0.00014686303387334316,
      "loss": 0.3738,
      "step": 907
    },
    {
      "epoch": 0.26705882352941174,
      "grad_norm": 0.05372559651732445,
      "learning_rate": 0.0001468041237113402,
      "loss": 0.3749,
      "step": 908
    },
    {
      "epoch": 0.26735294117647057,
      "grad_norm": 0.0602579303085804,
      "learning_rate": 0.00014674521354933725,
      "loss": 0.4204,
      "step": 909
    },
    {
      "epoch": 0.2676470588235294,
      "grad_norm": 0.05305233597755432,
      "learning_rate": 0.0001466863033873343,
      "loss": 0.3255,
      "step": 910
    },
    {
      "epoch": 0.26794117647058824,
      "grad_norm": 0.04816712811589241,
      "learning_rate": 0.00014662739322533137,
      "loss": 0.3056,
      "step": 911
    },
    {
      "epoch": 0.26823529411764707,
      "grad_norm": 0.0460905060172081,
      "learning_rate": 0.00014656848306332843,
      "loss": 0.3052,
      "step": 912
    },
    {
      "epoch": 0.2685294117647059,
      "grad_norm": 0.05712074786424637,
      "learning_rate": 0.00014650957290132546,
      "loss": 0.3281,
      "step": 913
    },
    {
      "epoch": 0.2688235294117647,
      "grad_norm": 0.08309024572372437,
      "learning_rate": 0.00014645066273932252,
      "loss": 0.3701,
      "step": 914
    },
    {
      "epoch": 0.2691176470588235,
      "grad_norm": 0.05143461003899574,
      "learning_rate": 0.00014639175257731958,
      "loss": 0.338,
      "step": 915
    },
    {
      "epoch": 0.26941176470588235,
      "grad_norm": 0.05507894977927208,
      "learning_rate": 0.00014633284241531664,
      "loss": 0.3412,
      "step": 916
    },
    {
      "epoch": 0.2697058823529412,
      "grad_norm": 0.05125855654478073,
      "learning_rate": 0.0001462739322533137,
      "loss": 0.3154,
      "step": 917
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.04735909402370453,
      "learning_rate": 0.00014621502209131074,
      "loss": 0.3006,
      "step": 918
    },
    {
      "epoch": 0.2702941176470588,
      "grad_norm": 0.05605161190032959,
      "learning_rate": 0.0001461561119293078,
      "loss": 0.346,
      "step": 919
    },
    {
      "epoch": 0.27058823529411763,
      "grad_norm": 0.05249663069844246,
      "learning_rate": 0.00014609720176730486,
      "loss": 0.368,
      "step": 920
    },
    {
      "epoch": 0.27088235294117646,
      "grad_norm": 0.04319138079881668,
      "learning_rate": 0.00014603829160530192,
      "loss": 0.36,
      "step": 921
    },
    {
      "epoch": 0.2711764705882353,
      "grad_norm": 0.05004463717341423,
      "learning_rate": 0.00014597938144329898,
      "loss": 0.3404,
      "step": 922
    },
    {
      "epoch": 0.27147058823529413,
      "grad_norm": 0.05729815736413002,
      "learning_rate": 0.000145920471281296,
      "loss": 0.3441,
      "step": 923
    },
    {
      "epoch": 0.27176470588235296,
      "grad_norm": 0.05888151377439499,
      "learning_rate": 0.00014586156111929307,
      "loss": 0.4088,
      "step": 924
    },
    {
      "epoch": 0.27205882352941174,
      "grad_norm": 0.04524515941739082,
      "learning_rate": 0.00014580265095729013,
      "loss": 0.3354,
      "step": 925
    },
    {
      "epoch": 0.2723529411764706,
      "grad_norm": 0.060338981449604034,
      "learning_rate": 0.0001457437407952872,
      "loss": 0.4297,
      "step": 926
    },
    {
      "epoch": 0.2726470588235294,
      "grad_norm": 0.06957358866930008,
      "learning_rate": 0.00014568483063328425,
      "loss": 0.3951,
      "step": 927
    },
    {
      "epoch": 0.27294117647058824,
      "grad_norm": 0.053953204303979874,
      "learning_rate": 0.00014562592047128128,
      "loss": 0.3193,
      "step": 928
    },
    {
      "epoch": 0.2732352941176471,
      "grad_norm": 0.05325806513428688,
      "learning_rate": 0.00014556701030927834,
      "loss": 0.3681,
      "step": 929
    },
    {
      "epoch": 0.2735294117647059,
      "grad_norm": 0.05422458052635193,
      "learning_rate": 0.0001455081001472754,
      "loss": 0.3372,
      "step": 930
    },
    {
      "epoch": 0.2738235294117647,
      "grad_norm": 0.0421563945710659,
      "learning_rate": 0.00014544918998527246,
      "loss": 0.3249,
      "step": 931
    },
    {
      "epoch": 0.2741176470588235,
      "grad_norm": 0.055413078516721725,
      "learning_rate": 0.00014539027982326952,
      "loss": 0.3667,
      "step": 932
    },
    {
      "epoch": 0.27441176470588236,
      "grad_norm": 0.05874481424689293,
      "learning_rate": 0.00014533136966126656,
      "loss": 0.3452,
      "step": 933
    },
    {
      "epoch": 0.2747058823529412,
      "grad_norm": 0.0545656755566597,
      "learning_rate": 0.00014527245949926362,
      "loss": 0.3275,
      "step": 934
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.053286924958229065,
      "learning_rate": 0.00014521354933726068,
      "loss": 0.3573,
      "step": 935
    },
    {
      "epoch": 0.2752941176470588,
      "grad_norm": 0.05394110083580017,
      "learning_rate": 0.00014515463917525774,
      "loss": 0.4114,
      "step": 936
    },
    {
      "epoch": 0.27558823529411763,
      "grad_norm": 0.04163404554128647,
      "learning_rate": 0.0001450957290132548,
      "loss": 0.3224,
      "step": 937
    },
    {
      "epoch": 0.27588235294117647,
      "grad_norm": 0.05688483268022537,
      "learning_rate": 0.00014503681885125183,
      "loss": 0.3346,
      "step": 938
    },
    {
      "epoch": 0.2761764705882353,
      "grad_norm": 0.05022352188825607,
      "learning_rate": 0.0001449779086892489,
      "loss": 0.3978,
      "step": 939
    },
    {
      "epoch": 0.27647058823529413,
      "grad_norm": 0.05576276406645775,
      "learning_rate": 0.00014491899852724595,
      "loss": 0.3776,
      "step": 940
    },
    {
      "epoch": 0.27676470588235297,
      "grad_norm": 0.046128999441862106,
      "learning_rate": 0.000144860088365243,
      "loss": 0.3798,
      "step": 941
    },
    {
      "epoch": 0.27705882352941175,
      "grad_norm": 0.05310777947306633,
      "learning_rate": 0.00014480117820324007,
      "loss": 0.3392,
      "step": 942
    },
    {
      "epoch": 0.2773529411764706,
      "grad_norm": 0.04054052755236626,
      "learning_rate": 0.0001447422680412371,
      "loss": 0.2662,
      "step": 943
    },
    {
      "epoch": 0.2776470588235294,
      "grad_norm": 0.06393079459667206,
      "learning_rate": 0.00014468335787923417,
      "loss": 0.4191,
      "step": 944
    },
    {
      "epoch": 0.27794117647058825,
      "grad_norm": 0.0385395810008049,
      "learning_rate": 0.00014462444771723123,
      "loss": 0.2813,
      "step": 945
    },
    {
      "epoch": 0.2782352941176471,
      "grad_norm": 0.05538781359791756,
      "learning_rate": 0.00014456553755522829,
      "loss": 0.357,
      "step": 946
    },
    {
      "epoch": 0.27852941176470586,
      "grad_norm": 0.0629754588007927,
      "learning_rate": 0.00014450662739322535,
      "loss": 0.3709,
      "step": 947
    },
    {
      "epoch": 0.2788235294117647,
      "grad_norm": 0.0660441517829895,
      "learning_rate": 0.00014444771723122238,
      "loss": 0.4383,
      "step": 948
    },
    {
      "epoch": 0.2791176470588235,
      "grad_norm": 0.05180821195244789,
      "learning_rate": 0.00014438880706921944,
      "loss": 0.3433,
      "step": 949
    },
    {
      "epoch": 0.27941176470588236,
      "grad_norm": 0.06394598633050919,
      "learning_rate": 0.0001443298969072165,
      "loss": 0.3705,
      "step": 950
    },
    {
      "epoch": 0.2797058823529412,
      "grad_norm": 0.0662921667098999,
      "learning_rate": 0.00014427098674521356,
      "loss": 0.4069,
      "step": 951
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.06407994776964188,
      "learning_rate": 0.00014421207658321062,
      "loss": 0.4387,
      "step": 952
    },
    {
      "epoch": 0.2802941176470588,
      "grad_norm": 0.0465533584356308,
      "learning_rate": 0.00014415316642120765,
      "loss": 0.3562,
      "step": 953
    },
    {
      "epoch": 0.28058823529411764,
      "grad_norm": 0.06705782562494278,
      "learning_rate": 0.0001440942562592047,
      "loss": 0.426,
      "step": 954
    },
    {
      "epoch": 0.28088235294117647,
      "grad_norm": 0.04978872463107109,
      "learning_rate": 0.00014403534609720177,
      "loss": 0.3007,
      "step": 955
    },
    {
      "epoch": 0.2811764705882353,
      "grad_norm": 0.043267663568258286,
      "learning_rate": 0.00014397643593519883,
      "loss": 0.4004,
      "step": 956
    },
    {
      "epoch": 0.28147058823529414,
      "grad_norm": 0.04667340964078903,
      "learning_rate": 0.0001439175257731959,
      "loss": 0.3197,
      "step": 957
    },
    {
      "epoch": 0.2817647058823529,
      "grad_norm": 0.040911514312028885,
      "learning_rate": 0.00014385861561119293,
      "loss": 0.2781,
      "step": 958
    },
    {
      "epoch": 0.28205882352941175,
      "grad_norm": 0.06391698867082596,
      "learning_rate": 0.00014379970544919,
      "loss": 0.3694,
      "step": 959
    },
    {
      "epoch": 0.2823529411764706,
      "grad_norm": 0.049885302782058716,
      "learning_rate": 0.00014374079528718705,
      "loss": 0.4171,
      "step": 960
    },
    {
      "epoch": 0.2826470588235294,
      "grad_norm": 0.04847703501582146,
      "learning_rate": 0.0001436818851251841,
      "loss": 0.3557,
      "step": 961
    },
    {
      "epoch": 0.28294117647058825,
      "grad_norm": 0.05565766990184784,
      "learning_rate": 0.00014362297496318117,
      "loss": 0.4008,
      "step": 962
    },
    {
      "epoch": 0.2832352941176471,
      "grad_norm": 0.0435057133436203,
      "learning_rate": 0.0001435640648011782,
      "loss": 0.3425,
      "step": 963
    },
    {
      "epoch": 0.28352941176470586,
      "grad_norm": 0.047882210463285446,
      "learning_rate": 0.00014350515463917526,
      "loss": 0.3754,
      "step": 964
    },
    {
      "epoch": 0.2838235294117647,
      "grad_norm": 0.05104188248515129,
      "learning_rate": 0.00014344624447717232,
      "loss": 0.3734,
      "step": 965
    },
    {
      "epoch": 0.28411764705882353,
      "grad_norm": 0.04731910303235054,
      "learning_rate": 0.00014338733431516938,
      "loss": 0.3495,
      "step": 966
    },
    {
      "epoch": 0.28441176470588236,
      "grad_norm": 0.07234266400337219,
      "learning_rate": 0.00014332842415316644,
      "loss": 0.4218,
      "step": 967
    },
    {
      "epoch": 0.2847058823529412,
      "grad_norm": 0.04335230588912964,
      "learning_rate": 0.00014326951399116347,
      "loss": 0.2492,
      "step": 968
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.04967648163437843,
      "learning_rate": 0.00014321060382916053,
      "loss": 0.3128,
      "step": 969
    },
    {
      "epoch": 0.2852941176470588,
      "grad_norm": 0.05885560065507889,
      "learning_rate": 0.0001431516936671576,
      "loss": 0.3867,
      "step": 970
    },
    {
      "epoch": 0.28558823529411764,
      "grad_norm": 0.07137142866849899,
      "learning_rate": 0.00014309278350515465,
      "loss": 0.4366,
      "step": 971
    },
    {
      "epoch": 0.2858823529411765,
      "grad_norm": 0.06220686808228493,
      "learning_rate": 0.00014303387334315171,
      "loss": 0.4251,
      "step": 972
    },
    {
      "epoch": 0.2861764705882353,
      "grad_norm": 0.06086564436554909,
      "learning_rate": 0.00014297496318114875,
      "loss": 0.3885,
      "step": 973
    },
    {
      "epoch": 0.28647058823529414,
      "grad_norm": 0.047263890504837036,
      "learning_rate": 0.0001429160530191458,
      "loss": 0.3149,
      "step": 974
    },
    {
      "epoch": 0.2867647058823529,
      "grad_norm": 0.059443868696689606,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3687,
      "step": 975
    },
    {
      "epoch": 0.28705882352941176,
      "grad_norm": 0.05369756370782852,
      "learning_rate": 0.00014279823269513993,
      "loss": 0.3525,
      "step": 976
    },
    {
      "epoch": 0.2873529411764706,
      "grad_norm": 0.05691470578312874,
      "learning_rate": 0.000142739322533137,
      "loss": 0.4081,
      "step": 977
    },
    {
      "epoch": 0.2876470588235294,
      "grad_norm": 0.047678809612989426,
      "learning_rate": 0.00014268041237113402,
      "loss": 0.3412,
      "step": 978
    },
    {
      "epoch": 0.28794117647058826,
      "grad_norm": 0.06309922784566879,
      "learning_rate": 0.00014262150220913108,
      "loss": 0.3825,
      "step": 979
    },
    {
      "epoch": 0.28823529411764703,
      "grad_norm": 0.05055781081318855,
      "learning_rate": 0.00014256259204712814,
      "loss": 0.353,
      "step": 980
    },
    {
      "epoch": 0.28852941176470587,
      "grad_norm": 0.06261890381574631,
      "learning_rate": 0.0001425036818851252,
      "loss": 0.3442,
      "step": 981
    },
    {
      "epoch": 0.2888235294117647,
      "grad_norm": 0.05280494689941406,
      "learning_rate": 0.00014244477172312226,
      "loss": 0.4284,
      "step": 982
    },
    {
      "epoch": 0.28911764705882353,
      "grad_norm": 0.05149334296584129,
      "learning_rate": 0.0001423858615611193,
      "loss": 0.3717,
      "step": 983
    },
    {
      "epoch": 0.28941176470588237,
      "grad_norm": 0.04419086501002312,
      "learning_rate": 0.00014232695139911636,
      "loss": 0.3878,
      "step": 984
    },
    {
      "epoch": 0.2897058823529412,
      "grad_norm": 0.07101210206747055,
      "learning_rate": 0.00014226804123711342,
      "loss": 0.4357,
      "step": 985
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.04253876209259033,
      "learning_rate": 0.00014220913107511048,
      "loss": 0.3181,
      "step": 986
    },
    {
      "epoch": 0.2902941176470588,
      "grad_norm": 0.04713556170463562,
      "learning_rate": 0.00014215022091310754,
      "loss": 0.3433,
      "step": 987
    },
    {
      "epoch": 0.29058823529411765,
      "grad_norm": 0.06763221323490143,
      "learning_rate": 0.00014209131075110457,
      "loss": 0.3908,
      "step": 988
    },
    {
      "epoch": 0.2908823529411765,
      "grad_norm": 0.04204615578055382,
      "learning_rate": 0.00014203240058910163,
      "loss": 0.3423,
      "step": 989
    },
    {
      "epoch": 0.2911764705882353,
      "grad_norm": 0.046757813543081284,
      "learning_rate": 0.0001419734904270987,
      "loss": 0.3219,
      "step": 990
    },
    {
      "epoch": 0.2914705882352941,
      "grad_norm": 0.0587739534676075,
      "learning_rate": 0.00014191458026509575,
      "loss": 0.4156,
      "step": 991
    },
    {
      "epoch": 0.2917647058823529,
      "grad_norm": 0.04404037445783615,
      "learning_rate": 0.0001418556701030928,
      "loss": 0.3708,
      "step": 992
    },
    {
      "epoch": 0.29205882352941176,
      "grad_norm": 0.04753689467906952,
      "learning_rate": 0.00014179675994108984,
      "loss": 0.2676,
      "step": 993
    },
    {
      "epoch": 0.2923529411764706,
      "grad_norm": 0.05095509812235832,
      "learning_rate": 0.0001417378497790869,
      "loss": 0.3852,
      "step": 994
    },
    {
      "epoch": 0.2926470588235294,
      "grad_norm": 0.055156245827674866,
      "learning_rate": 0.00014167893961708396,
      "loss": 0.3565,
      "step": 995
    },
    {
      "epoch": 0.29294117647058826,
      "grad_norm": 0.04383677616715431,
      "learning_rate": 0.00014162002945508102,
      "loss": 0.3255,
      "step": 996
    },
    {
      "epoch": 0.29323529411764704,
      "grad_norm": 0.048199232667684555,
      "learning_rate": 0.00014156111929307808,
      "loss": 0.3829,
      "step": 997
    },
    {
      "epoch": 0.29352941176470587,
      "grad_norm": 0.048286356031894684,
      "learning_rate": 0.00014150220913107512,
      "loss": 0.3312,
      "step": 998
    },
    {
      "epoch": 0.2938235294117647,
      "grad_norm": 0.051367830485105515,
      "learning_rate": 0.00014144329896907218,
      "loss": 0.3189,
      "step": 999
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.04175925627350807,
      "learning_rate": 0.00014138438880706924,
      "loss": 0.313,
      "step": 1000
    },
    {
      "epoch": 0.29411764705882354,
      "eval_loss": 0.3591006100177765,
      "eval_runtime": 215.1292,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1000
    },
    {
      "epoch": 0.2944117647058824,
      "grad_norm": 0.051591262221336365,
      "learning_rate": 0.0001413254786450663,
      "loss": 0.3876,
      "step": 1001
    },
    {
      "epoch": 0.29470588235294115,
      "grad_norm": 0.05844810605049133,
      "learning_rate": 0.00014126656848306336,
      "loss": 0.4081,
      "step": 1002
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.05298194661736488,
      "learning_rate": 0.0001412076583210604,
      "loss": 0.2929,
      "step": 1003
    },
    {
      "epoch": 0.2952941176470588,
      "grad_norm": 0.04877354949712753,
      "learning_rate": 0.00014114874815905745,
      "loss": 0.373,
      "step": 1004
    },
    {
      "epoch": 0.29558823529411765,
      "grad_norm": 0.05304316058754921,
      "learning_rate": 0.0001410898379970545,
      "loss": 0.3388,
      "step": 1005
    },
    {
      "epoch": 0.2958823529411765,
      "grad_norm": 0.05452723801136017,
      "learning_rate": 0.00014103092783505157,
      "loss": 0.307,
      "step": 1006
    },
    {
      "epoch": 0.2961764705882353,
      "grad_norm": 0.04721319302916527,
      "learning_rate": 0.00014097201767304863,
      "loss": 0.3033,
      "step": 1007
    },
    {
      "epoch": 0.2964705882352941,
      "grad_norm": 0.04567733779549599,
      "learning_rate": 0.00014091310751104566,
      "loss": 0.2704,
      "step": 1008
    },
    {
      "epoch": 0.29676470588235293,
      "grad_norm": 0.06240099295973778,
      "learning_rate": 0.00014085419734904272,
      "loss": 0.3543,
      "step": 1009
    },
    {
      "epoch": 0.29705882352941176,
      "grad_norm": 0.07420143485069275,
      "learning_rate": 0.00014079528718703978,
      "loss": 0.4096,
      "step": 1010
    },
    {
      "epoch": 0.2973529411764706,
      "grad_norm": 0.07037311792373657,
      "learning_rate": 0.00014073637702503684,
      "loss": 0.3753,
      "step": 1011
    },
    {
      "epoch": 0.29764705882352943,
      "grad_norm": 0.04381735995411873,
      "learning_rate": 0.0001406774668630339,
      "loss": 0.306,
      "step": 1012
    },
    {
      "epoch": 0.2979411764705882,
      "grad_norm": 0.05807105079293251,
      "learning_rate": 0.0001406185567010309,
      "loss": 0.3382,
      "step": 1013
    },
    {
      "epoch": 0.29823529411764704,
      "grad_norm": 0.04848058521747589,
      "learning_rate": 0.00014055964653902797,
      "loss": 0.3621,
      "step": 1014
    },
    {
      "epoch": 0.2985294117647059,
      "grad_norm": 0.05834103375673294,
      "learning_rate": 0.00014050073637702503,
      "loss": 0.3835,
      "step": 1015
    },
    {
      "epoch": 0.2988235294117647,
      "grad_norm": 0.04603530466556549,
      "learning_rate": 0.0001404418262150221,
      "loss": 0.3321,
      "step": 1016
    },
    {
      "epoch": 0.29911764705882354,
      "grad_norm": 0.052594445645809174,
      "learning_rate": 0.00014038291605301915,
      "loss": 0.3704,
      "step": 1017
    },
    {
      "epoch": 0.2994117647058824,
      "grad_norm": 0.04902760311961174,
      "learning_rate": 0.00014032400589101618,
      "loss": 0.3647,
      "step": 1018
    },
    {
      "epoch": 0.29970588235294116,
      "grad_norm": 0.06467926502227783,
      "learning_rate": 0.00014026509572901324,
      "loss": 0.3414,
      "step": 1019
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.05172251537442207,
      "learning_rate": 0.0001402061855670103,
      "loss": 0.3582,
      "step": 1020
    },
    {
      "epoch": 0.3002941176470588,
      "grad_norm": 0.05596764013171196,
      "learning_rate": 0.00014014727540500736,
      "loss": 0.3337,
      "step": 1021
    },
    {
      "epoch": 0.30058823529411766,
      "grad_norm": 0.054255541414022446,
      "learning_rate": 0.00014008836524300442,
      "loss": 0.3563,
      "step": 1022
    },
    {
      "epoch": 0.3008823529411765,
      "grad_norm": 0.05098652094602585,
      "learning_rate": 0.00014002945508100146,
      "loss": 0.3665,
      "step": 1023
    },
    {
      "epoch": 0.30117647058823527,
      "grad_norm": 0.04350297152996063,
      "learning_rate": 0.00013997054491899852,
      "loss": 0.2897,
      "step": 1024
    },
    {
      "epoch": 0.3014705882352941,
      "grad_norm": 0.0477239154279232,
      "learning_rate": 0.00013991163475699558,
      "loss": 0.4063,
      "step": 1025
    },
    {
      "epoch": 0.30176470588235293,
      "grad_norm": 0.05367223918437958,
      "learning_rate": 0.00013985272459499264,
      "loss": 0.3725,
      "step": 1026
    },
    {
      "epoch": 0.30205882352941177,
      "grad_norm": 0.05794167146086693,
      "learning_rate": 0.0001397938144329897,
      "loss": 0.3659,
      "step": 1027
    },
    {
      "epoch": 0.3023529411764706,
      "grad_norm": 0.06229998916387558,
      "learning_rate": 0.00013973490427098673,
      "loss": 0.3934,
      "step": 1028
    },
    {
      "epoch": 0.30264705882352944,
      "grad_norm": 0.05271994695067406,
      "learning_rate": 0.0001396759941089838,
      "loss": 0.4074,
      "step": 1029
    },
    {
      "epoch": 0.3029411764705882,
      "grad_norm": 0.07008574903011322,
      "learning_rate": 0.00013961708394698085,
      "loss": 0.3787,
      "step": 1030
    },
    {
      "epoch": 0.30323529411764705,
      "grad_norm": 0.038220200687646866,
      "learning_rate": 0.0001395581737849779,
      "loss": 0.2741,
      "step": 1031
    },
    {
      "epoch": 0.3035294117647059,
      "grad_norm": 0.045519907027482986,
      "learning_rate": 0.00013949926362297497,
      "loss": 0.2932,
      "step": 1032
    },
    {
      "epoch": 0.3038235294117647,
      "grad_norm": 0.06562769412994385,
      "learning_rate": 0.000139440353460972,
      "loss": 0.3709,
      "step": 1033
    },
    {
      "epoch": 0.30411764705882355,
      "grad_norm": 0.050264906138181686,
      "learning_rate": 0.00013938144329896907,
      "loss": 0.3279,
      "step": 1034
    },
    {
      "epoch": 0.3044117647058823,
      "grad_norm": 0.05049460381269455,
      "learning_rate": 0.00013932253313696613,
      "loss": 0.3674,
      "step": 1035
    },
    {
      "epoch": 0.30470588235294116,
      "grad_norm": 0.06520510464906693,
      "learning_rate": 0.00013926362297496319,
      "loss": 0.3745,
      "step": 1036
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.04368053376674652,
      "learning_rate": 0.00013920471281296025,
      "loss": 0.2897,
      "step": 1037
    },
    {
      "epoch": 0.3052941176470588,
      "grad_norm": 0.0319485142827034,
      "learning_rate": 0.00013914580265095728,
      "loss": 0.227,
      "step": 1038
    },
    {
      "epoch": 0.30558823529411766,
      "grad_norm": 0.05107998847961426,
      "learning_rate": 0.00013908689248895434,
      "loss": 0.3901,
      "step": 1039
    },
    {
      "epoch": 0.3058823529411765,
      "grad_norm": 0.0476672388613224,
      "learning_rate": 0.0001390279823269514,
      "loss": 0.3354,
      "step": 1040
    },
    {
      "epoch": 0.30617647058823527,
      "grad_norm": 0.0639825239777565,
      "learning_rate": 0.00013896907216494846,
      "loss": 0.4201,
      "step": 1041
    },
    {
      "epoch": 0.3064705882352941,
      "grad_norm": 0.04959540814161301,
      "learning_rate": 0.00013891016200294552,
      "loss": 0.3673,
      "step": 1042
    },
    {
      "epoch": 0.30676470588235294,
      "grad_norm": 0.04500806704163551,
      "learning_rate": 0.00013885125184094255,
      "loss": 0.2886,
      "step": 1043
    },
    {
      "epoch": 0.3070588235294118,
      "grad_norm": 0.04842762276530266,
      "learning_rate": 0.0001387923416789396,
      "loss": 0.3256,
      "step": 1044
    },
    {
      "epoch": 0.3073529411764706,
      "grad_norm": 0.06260630488395691,
      "learning_rate": 0.00013873343151693667,
      "loss": 0.4375,
      "step": 1045
    },
    {
      "epoch": 0.3076470588235294,
      "grad_norm": 0.059424012899398804,
      "learning_rate": 0.00013867452135493373,
      "loss": 0.3867,
      "step": 1046
    },
    {
      "epoch": 0.3079411764705882,
      "grad_norm": 0.04370175302028656,
      "learning_rate": 0.0001386156111929308,
      "loss": 0.3694,
      "step": 1047
    },
    {
      "epoch": 0.30823529411764705,
      "grad_norm": 0.07215604931116104,
      "learning_rate": 0.00013855670103092783,
      "loss": 0.409,
      "step": 1048
    },
    {
      "epoch": 0.3085294117647059,
      "grad_norm": 0.06284046918153763,
      "learning_rate": 0.00013849779086892489,
      "loss": 0.329,
      "step": 1049
    },
    {
      "epoch": 0.3088235294117647,
      "grad_norm": 0.050512541085481644,
      "learning_rate": 0.00013843888070692195,
      "loss": 0.3324,
      "step": 1050
    },
    {
      "epoch": 0.30911764705882355,
      "grad_norm": 0.04752623289823532,
      "learning_rate": 0.000138379970544919,
      "loss": 0.2828,
      "step": 1051
    },
    {
      "epoch": 0.30941176470588233,
      "grad_norm": 0.05138843506574631,
      "learning_rate": 0.00013832106038291607,
      "loss": 0.3432,
      "step": 1052
    },
    {
      "epoch": 0.30970588235294116,
      "grad_norm": 0.08374598622322083,
      "learning_rate": 0.0001382621502209131,
      "loss": 0.3523,
      "step": 1053
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.058244165033102036,
      "learning_rate": 0.00013820324005891016,
      "loss": 0.4205,
      "step": 1054
    },
    {
      "epoch": 0.31029411764705883,
      "grad_norm": 0.05952610448002815,
      "learning_rate": 0.00013814432989690722,
      "loss": 0.3581,
      "step": 1055
    },
    {
      "epoch": 0.31058823529411766,
      "grad_norm": 0.05606214702129364,
      "learning_rate": 0.00013808541973490428,
      "loss": 0.4293,
      "step": 1056
    },
    {
      "epoch": 0.31088235294117644,
      "grad_norm": 0.050698552280664444,
      "learning_rate": 0.00013802650957290134,
      "loss": 0.3841,
      "step": 1057
    },
    {
      "epoch": 0.3111764705882353,
      "grad_norm": 0.058532942086458206,
      "learning_rate": 0.00013796759941089837,
      "loss": 0.3806,
      "step": 1058
    },
    {
      "epoch": 0.3114705882352941,
      "grad_norm": 0.09046195447444916,
      "learning_rate": 0.00013790868924889543,
      "loss": 0.3688,
      "step": 1059
    },
    {
      "epoch": 0.31176470588235294,
      "grad_norm": 0.055358078330755234,
      "learning_rate": 0.0001378497790868925,
      "loss": 0.3646,
      "step": 1060
    },
    {
      "epoch": 0.3120588235294118,
      "grad_norm": 0.04910806939005852,
      "learning_rate": 0.00013779086892488955,
      "loss": 0.3441,
      "step": 1061
    },
    {
      "epoch": 0.3123529411764706,
      "grad_norm": 0.04647079110145569,
      "learning_rate": 0.00013773195876288661,
      "loss": 0.3265,
      "step": 1062
    },
    {
      "epoch": 0.3126470588235294,
      "grad_norm": 0.04868427291512489,
      "learning_rate": 0.00013767304860088365,
      "loss": 0.3263,
      "step": 1063
    },
    {
      "epoch": 0.3129411764705882,
      "grad_norm": 0.040066108107566833,
      "learning_rate": 0.0001376141384388807,
      "loss": 0.3055,
      "step": 1064
    },
    {
      "epoch": 0.31323529411764706,
      "grad_norm": 0.048818159848451614,
      "learning_rate": 0.00013755522827687777,
      "loss": 0.3703,
      "step": 1065
    },
    {
      "epoch": 0.3135294117647059,
      "grad_norm": 0.058080777525901794,
      "learning_rate": 0.00013749631811487483,
      "loss": 0.3727,
      "step": 1066
    },
    {
      "epoch": 0.3138235294117647,
      "grad_norm": 0.04944861680269241,
      "learning_rate": 0.0001374374079528719,
      "loss": 0.3745,
      "step": 1067
    },
    {
      "epoch": 0.31411764705882356,
      "grad_norm": 0.045673079788684845,
      "learning_rate": 0.00013737849779086892,
      "loss": 0.3313,
      "step": 1068
    },
    {
      "epoch": 0.31441176470588234,
      "grad_norm": 0.042419202625751495,
      "learning_rate": 0.00013731958762886598,
      "loss": 0.3521,
      "step": 1069
    },
    {
      "epoch": 0.31470588235294117,
      "grad_norm": 0.04238913580775261,
      "learning_rate": 0.00013726067746686304,
      "loss": 0.2823,
      "step": 1070
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.04315204173326492,
      "learning_rate": 0.0001372017673048601,
      "loss": 0.3461,
      "step": 1071
    },
    {
      "epoch": 0.31529411764705884,
      "grad_norm": 0.05500626191496849,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.3611,
      "step": 1072
    },
    {
      "epoch": 0.31558823529411767,
      "grad_norm": 0.06926807016134262,
      "learning_rate": 0.0001370839469808542,
      "loss": 0.3737,
      "step": 1073
    },
    {
      "epoch": 0.31588235294117645,
      "grad_norm": 0.04234514385461807,
      "learning_rate": 0.00013702503681885126,
      "loss": 0.3418,
      "step": 1074
    },
    {
      "epoch": 0.3161764705882353,
      "grad_norm": 0.05765678733587265,
      "learning_rate": 0.00013696612665684832,
      "loss": 0.3962,
      "step": 1075
    },
    {
      "epoch": 0.3164705882352941,
      "grad_norm": 0.04844558238983154,
      "learning_rate": 0.00013690721649484538,
      "loss": 0.357,
      "step": 1076
    },
    {
      "epoch": 0.31676470588235295,
      "grad_norm": 0.04993690550327301,
      "learning_rate": 0.00013684830633284244,
      "loss": 0.3311,
      "step": 1077
    },
    {
      "epoch": 0.3170588235294118,
      "grad_norm": 0.051376596093177795,
      "learning_rate": 0.00013678939617083947,
      "loss": 0.3553,
      "step": 1078
    },
    {
      "epoch": 0.3173529411764706,
      "grad_norm": 0.052805617451667786,
      "learning_rate": 0.00013673048600883653,
      "loss": 0.2832,
      "step": 1079
    },
    {
      "epoch": 0.3176470588235294,
      "grad_norm": 0.052873555570840836,
      "learning_rate": 0.0001366715758468336,
      "loss": 0.406,
      "step": 1080
    },
    {
      "epoch": 0.3179411764705882,
      "grad_norm": 0.06516626477241516,
      "learning_rate": 0.00013661266568483065,
      "loss": 0.3909,
      "step": 1081
    },
    {
      "epoch": 0.31823529411764706,
      "grad_norm": 0.05466814711689949,
      "learning_rate": 0.0001365537555228277,
      "loss": 0.2939,
      "step": 1082
    },
    {
      "epoch": 0.3185294117647059,
      "grad_norm": 0.048912253230810165,
      "learning_rate": 0.00013649484536082474,
      "loss": 0.3676,
      "step": 1083
    },
    {
      "epoch": 0.31882352941176473,
      "grad_norm": 0.055120840668678284,
      "learning_rate": 0.0001364359351988218,
      "loss": 0.3733,
      "step": 1084
    },
    {
      "epoch": 0.3191176470588235,
      "grad_norm": 0.0456012599170208,
      "learning_rate": 0.00013637702503681886,
      "loss": 0.2736,
      "step": 1085
    },
    {
      "epoch": 0.31941176470588234,
      "grad_norm": 0.059227537363767624,
      "learning_rate": 0.00013631811487481592,
      "loss": 0.3617,
      "step": 1086
    },
    {
      "epoch": 0.3197058823529412,
      "grad_norm": 0.04676665738224983,
      "learning_rate": 0.00013625920471281298,
      "loss": 0.2848,
      "step": 1087
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.04604864493012428,
      "learning_rate": 0.00013620029455081002,
      "loss": 0.3654,
      "step": 1088
    },
    {
      "epoch": 0.32029411764705884,
      "grad_norm": 0.05289626866579056,
      "learning_rate": 0.00013614138438880708,
      "loss": 0.3793,
      "step": 1089
    },
    {
      "epoch": 0.3205882352941177,
      "grad_norm": 0.05551222711801529,
      "learning_rate": 0.00013608247422680414,
      "loss": 0.3497,
      "step": 1090
    },
    {
      "epoch": 0.32088235294117645,
      "grad_norm": 0.049391310662031174,
      "learning_rate": 0.0001360235640648012,
      "loss": 0.3648,
      "step": 1091
    },
    {
      "epoch": 0.3211764705882353,
      "grad_norm": 0.04672539234161377,
      "learning_rate": 0.00013596465390279826,
      "loss": 0.3032,
      "step": 1092
    },
    {
      "epoch": 0.3214705882352941,
      "grad_norm": 0.05588151142001152,
      "learning_rate": 0.0001359057437407953,
      "loss": 0.3466,
      "step": 1093
    },
    {
      "epoch": 0.32176470588235295,
      "grad_norm": 0.058099497109651566,
      "learning_rate": 0.00013584683357879235,
      "loss": 0.4186,
      "step": 1094
    },
    {
      "epoch": 0.3220588235294118,
      "grad_norm": 0.06302514672279358,
      "learning_rate": 0.0001357879234167894,
      "loss": 0.3768,
      "step": 1095
    },
    {
      "epoch": 0.32235294117647056,
      "grad_norm": 0.056245457381010056,
      "learning_rate": 0.00013572901325478647,
      "loss": 0.4022,
      "step": 1096
    },
    {
      "epoch": 0.3226470588235294,
      "grad_norm": 0.05731814727187157,
      "learning_rate": 0.00013567010309278353,
      "loss": 0.369,
      "step": 1097
    },
    {
      "epoch": 0.32294117647058823,
      "grad_norm": 0.054962627589702606,
      "learning_rate": 0.00013561119293078056,
      "loss": 0.3979,
      "step": 1098
    },
    {
      "epoch": 0.32323529411764707,
      "grad_norm": 0.04959545284509659,
      "learning_rate": 0.00013555228276877762,
      "loss": 0.3188,
      "step": 1099
    },
    {
      "epoch": 0.3235294117647059,
      "grad_norm": 0.05491848662495613,
      "learning_rate": 0.00013549337260677468,
      "loss": 0.3656,
      "step": 1100
    },
    {
      "epoch": 0.3235294117647059,
      "eval_loss": 0.3581154942512512,
      "eval_runtime": 214.8984,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1100
    },
    {
      "epoch": 0.32382352941176473,
      "grad_norm": 0.060841191560029984,
      "learning_rate": 0.00013543446244477174,
      "loss": 0.2597,
      "step": 1101
    },
    {
      "epoch": 0.3241176470588235,
      "grad_norm": 0.05782413110136986,
      "learning_rate": 0.0001353755522827688,
      "loss": 0.2931,
      "step": 1102
    },
    {
      "epoch": 0.32441176470588234,
      "grad_norm": 0.050143249332904816,
      "learning_rate": 0.00013531664212076584,
      "loss": 0.3299,
      "step": 1103
    },
    {
      "epoch": 0.3247058823529412,
      "grad_norm": 0.06045601889491081,
      "learning_rate": 0.0001352577319587629,
      "loss": 0.3496,
      "step": 1104
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.04244425147771835,
      "learning_rate": 0.00013519882179675996,
      "loss": 0.2823,
      "step": 1105
    },
    {
      "epoch": 0.32529411764705884,
      "grad_norm": 0.054346468299627304,
      "learning_rate": 0.00013513991163475702,
      "loss": 0.3298,
      "step": 1106
    },
    {
      "epoch": 0.3255882352941176,
      "grad_norm": 0.05542978644371033,
      "learning_rate": 0.00013508100147275408,
      "loss": 0.3519,
      "step": 1107
    },
    {
      "epoch": 0.32588235294117646,
      "grad_norm": 0.040928639471530914,
      "learning_rate": 0.0001350220913107511,
      "loss": 0.3207,
      "step": 1108
    },
    {
      "epoch": 0.3261764705882353,
      "grad_norm": 0.0512186735868454,
      "learning_rate": 0.00013496318114874817,
      "loss": 0.3964,
      "step": 1109
    },
    {
      "epoch": 0.3264705882352941,
      "grad_norm": 0.05464067682623863,
      "learning_rate": 0.00013490427098674523,
      "loss": 0.3645,
      "step": 1110
    },
    {
      "epoch": 0.32676470588235296,
      "grad_norm": 0.06166354939341545,
      "learning_rate": 0.0001348453608247423,
      "loss": 0.3833,
      "step": 1111
    },
    {
      "epoch": 0.3270588235294118,
      "grad_norm": 0.05063244327902794,
      "learning_rate": 0.00013478645066273935,
      "loss": 0.3649,
      "step": 1112
    },
    {
      "epoch": 0.32735294117647057,
      "grad_norm": 0.045847468078136444,
      "learning_rate": 0.00013472754050073638,
      "loss": 0.3291,
      "step": 1113
    },
    {
      "epoch": 0.3276470588235294,
      "grad_norm": 0.0513523630797863,
      "learning_rate": 0.00013466863033873344,
      "loss": 0.3137,
      "step": 1114
    },
    {
      "epoch": 0.32794117647058824,
      "grad_norm": 0.08801085501909256,
      "learning_rate": 0.0001346097201767305,
      "loss": 0.4855,
      "step": 1115
    },
    {
      "epoch": 0.32823529411764707,
      "grad_norm": 0.05960071086883545,
      "learning_rate": 0.00013455081001472757,
      "loss": 0.3733,
      "step": 1116
    },
    {
      "epoch": 0.3285294117647059,
      "grad_norm": 0.053401753306388855,
      "learning_rate": 0.00013449189985272463,
      "loss": 0.339,
      "step": 1117
    },
    {
      "epoch": 0.3288235294117647,
      "grad_norm": 0.05458609387278557,
      "learning_rate": 0.00013443298969072166,
      "loss": 0.38,
      "step": 1118
    },
    {
      "epoch": 0.3291176470588235,
      "grad_norm": 0.06582891941070557,
      "learning_rate": 0.0001343740795287187,
      "loss": 0.3342,
      "step": 1119
    },
    {
      "epoch": 0.32941176470588235,
      "grad_norm": 0.0510503314435482,
      "learning_rate": 0.00013431516936671575,
      "loss": 0.3787,
      "step": 1120
    },
    {
      "epoch": 0.3297058823529412,
      "grad_norm": 0.05124008655548096,
      "learning_rate": 0.0001342562592047128,
      "loss": 0.3111,
      "step": 1121
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.05622609332203865,
      "learning_rate": 0.00013419734904270987,
      "loss": 0.3584,
      "step": 1122
    },
    {
      "epoch": 0.33029411764705885,
      "grad_norm": 0.058525361120700836,
      "learning_rate": 0.0001341384388807069,
      "loss": 0.341,
      "step": 1123
    },
    {
      "epoch": 0.3305882352941176,
      "grad_norm": 0.04863717034459114,
      "learning_rate": 0.00013407952871870397,
      "loss": 0.3112,
      "step": 1124
    },
    {
      "epoch": 0.33088235294117646,
      "grad_norm": 0.042498908936977386,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.32,
      "step": 1125
    },
    {
      "epoch": 0.3311764705882353,
      "grad_norm": 0.03817171975970268,
      "learning_rate": 0.00013396170839469809,
      "loss": 0.343,
      "step": 1126
    },
    {
      "epoch": 0.33147058823529413,
      "grad_norm": 0.0660669207572937,
      "learning_rate": 0.00013390279823269515,
      "loss": 0.4667,
      "step": 1127
    },
    {
      "epoch": 0.33176470588235296,
      "grad_norm": 0.043125398457050323,
      "learning_rate": 0.00013384388807069218,
      "loss": 0.2945,
      "step": 1128
    },
    {
      "epoch": 0.33205882352941174,
      "grad_norm": 0.05159172788262367,
      "learning_rate": 0.00013378497790868924,
      "loss": 0.3019,
      "step": 1129
    },
    {
      "epoch": 0.3323529411764706,
      "grad_norm": 0.0468907468020916,
      "learning_rate": 0.0001337260677466863,
      "loss": 0.3463,
      "step": 1130
    },
    {
      "epoch": 0.3326470588235294,
      "grad_norm": 0.09934356808662415,
      "learning_rate": 0.00013366715758468336,
      "loss": 0.44,
      "step": 1131
    },
    {
      "epoch": 0.33294117647058824,
      "grad_norm": 0.04134361073374748,
      "learning_rate": 0.00013360824742268042,
      "loss": 0.3747,
      "step": 1132
    },
    {
      "epoch": 0.3332352941176471,
      "grad_norm": 0.048714105039834976,
      "learning_rate": 0.00013354933726067745,
      "loss": 0.3575,
      "step": 1133
    },
    {
      "epoch": 0.3335294117647059,
      "grad_norm": 0.03767428547143936,
      "learning_rate": 0.0001334904270986745,
      "loss": 0.3219,
      "step": 1134
    },
    {
      "epoch": 0.3338235294117647,
      "grad_norm": 0.05452403426170349,
      "learning_rate": 0.00013343151693667157,
      "loss": 0.3954,
      "step": 1135
    },
    {
      "epoch": 0.3341176470588235,
      "grad_norm": 0.04922841489315033,
      "learning_rate": 0.00013337260677466863,
      "loss": 0.3246,
      "step": 1136
    },
    {
      "epoch": 0.33441176470588235,
      "grad_norm": 0.04751508682966232,
      "learning_rate": 0.0001333136966126657,
      "loss": 0.3619,
      "step": 1137
    },
    {
      "epoch": 0.3347058823529412,
      "grad_norm": 0.041572824120521545,
      "learning_rate": 0.00013325478645066273,
      "loss": 0.2759,
      "step": 1138
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.05829545855522156,
      "learning_rate": 0.00013319587628865979,
      "loss": 0.4076,
      "step": 1139
    },
    {
      "epoch": 0.3352941176470588,
      "grad_norm": 0.0635799840092659,
      "learning_rate": 0.00013313696612665685,
      "loss": 0.4215,
      "step": 1140
    },
    {
      "epoch": 0.33558823529411763,
      "grad_norm": 0.05547849461436272,
      "learning_rate": 0.0001330780559646539,
      "loss": 0.3762,
      "step": 1141
    },
    {
      "epoch": 0.33588235294117647,
      "grad_norm": 0.043614696711301804,
      "learning_rate": 0.00013301914580265097,
      "loss": 0.3309,
      "step": 1142
    },
    {
      "epoch": 0.3361764705882353,
      "grad_norm": 0.04247620329260826,
      "learning_rate": 0.000132960235640648,
      "loss": 0.337,
      "step": 1143
    },
    {
      "epoch": 0.33647058823529413,
      "grad_norm": 0.04477175325155258,
      "learning_rate": 0.00013290132547864506,
      "loss": 0.3443,
      "step": 1144
    },
    {
      "epoch": 0.33676470588235297,
      "grad_norm": 0.05270840600132942,
      "learning_rate": 0.00013284241531664212,
      "loss": 0.3797,
      "step": 1145
    },
    {
      "epoch": 0.33705882352941174,
      "grad_norm": 0.04806090146303177,
      "learning_rate": 0.00013278350515463918,
      "loss": 0.388,
      "step": 1146
    },
    {
      "epoch": 0.3373529411764706,
      "grad_norm": 0.04853607714176178,
      "learning_rate": 0.00013272459499263624,
      "loss": 0.2886,
      "step": 1147
    },
    {
      "epoch": 0.3376470588235294,
      "grad_norm": 0.043064557015895844,
      "learning_rate": 0.00013266568483063327,
      "loss": 0.3436,
      "step": 1148
    },
    {
      "epoch": 0.33794117647058824,
      "grad_norm": 0.05424869805574417,
      "learning_rate": 0.00013260677466863033,
      "loss": 0.3282,
      "step": 1149
    },
    {
      "epoch": 0.3382352941176471,
      "grad_norm": 0.06425879895687103,
      "learning_rate": 0.0001325478645066274,
      "loss": 0.3582,
      "step": 1150
    },
    {
      "epoch": 0.33852941176470586,
      "grad_norm": 0.054306965321302414,
      "learning_rate": 0.00013248895434462445,
      "loss": 0.3673,
      "step": 1151
    },
    {
      "epoch": 0.3388235294117647,
      "grad_norm": 0.06780901551246643,
      "learning_rate": 0.00013243004418262151,
      "loss": 0.3489,
      "step": 1152
    },
    {
      "epoch": 0.3391176470588235,
      "grad_norm": 0.051383644342422485,
      "learning_rate": 0.00013237113402061855,
      "loss": 0.3514,
      "step": 1153
    },
    {
      "epoch": 0.33941176470588236,
      "grad_norm": 0.07987073808908463,
      "learning_rate": 0.0001323122238586156,
      "loss": 0.3968,
      "step": 1154
    },
    {
      "epoch": 0.3397058823529412,
      "grad_norm": 0.0539771169424057,
      "learning_rate": 0.00013225331369661267,
      "loss": 0.3745,
      "step": 1155
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.03240249305963516,
      "learning_rate": 0.00013219440353460973,
      "loss": 0.2377,
      "step": 1156
    },
    {
      "epoch": 0.3402941176470588,
      "grad_norm": 0.04802228510379791,
      "learning_rate": 0.0001321354933726068,
      "loss": 0.304,
      "step": 1157
    },
    {
      "epoch": 0.34058823529411764,
      "grad_norm": 0.05189390853047371,
      "learning_rate": 0.00013207658321060382,
      "loss": 0.3323,
      "step": 1158
    },
    {
      "epoch": 0.34088235294117647,
      "grad_norm": 0.0628160908818245,
      "learning_rate": 0.00013201767304860088,
      "loss": 0.4102,
      "step": 1159
    },
    {
      "epoch": 0.3411764705882353,
      "grad_norm": 0.060788534581661224,
      "learning_rate": 0.00013195876288659794,
      "loss": 0.3762,
      "step": 1160
    },
    {
      "epoch": 0.34147058823529414,
      "grad_norm": 0.05360953509807587,
      "learning_rate": 0.000131899852724595,
      "loss": 0.3682,
      "step": 1161
    },
    {
      "epoch": 0.3417647058823529,
      "grad_norm": 0.04791836440563202,
      "learning_rate": 0.00013184094256259206,
      "loss": 0.3304,
      "step": 1162
    },
    {
      "epoch": 0.34205882352941175,
      "grad_norm": 0.05625937134027481,
      "learning_rate": 0.0001317820324005891,
      "loss": 0.3774,
      "step": 1163
    },
    {
      "epoch": 0.3423529411764706,
      "grad_norm": 0.0462304949760437,
      "learning_rate": 0.00013172312223858615,
      "loss": 0.3383,
      "step": 1164
    },
    {
      "epoch": 0.3426470588235294,
      "grad_norm": 0.04596276953816414,
      "learning_rate": 0.00013166421207658322,
      "loss": 0.3435,
      "step": 1165
    },
    {
      "epoch": 0.34294117647058825,
      "grad_norm": 0.039368484169244766,
      "learning_rate": 0.00013160530191458028,
      "loss": 0.2565,
      "step": 1166
    },
    {
      "epoch": 0.3432352941176471,
      "grad_norm": 0.05051527917385101,
      "learning_rate": 0.00013154639175257734,
      "loss": 0.3663,
      "step": 1167
    },
    {
      "epoch": 0.34352941176470586,
      "grad_norm": 0.04265039414167404,
      "learning_rate": 0.00013148748159057437,
      "loss": 0.3255,
      "step": 1168
    },
    {
      "epoch": 0.3438235294117647,
      "grad_norm": 0.04583904892206192,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.3551,
      "step": 1169
    },
    {
      "epoch": 0.34411764705882353,
      "grad_norm": 0.04362077638506889,
      "learning_rate": 0.0001313696612665685,
      "loss": 0.3243,
      "step": 1170
    },
    {
      "epoch": 0.34441176470588236,
      "grad_norm": 0.047426991164684296,
      "learning_rate": 0.00013131075110456555,
      "loss": 0.3289,
      "step": 1171
    },
    {
      "epoch": 0.3447058823529412,
      "grad_norm": 0.04730711504817009,
      "learning_rate": 0.0001312518409425626,
      "loss": 0.3269,
      "step": 1172
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.05111146718263626,
      "learning_rate": 0.00013119293078055964,
      "loss": 0.3581,
      "step": 1173
    },
    {
      "epoch": 0.3452941176470588,
      "grad_norm": 0.050252318382263184,
      "learning_rate": 0.0001311340206185567,
      "loss": 0.3422,
      "step": 1174
    },
    {
      "epoch": 0.34558823529411764,
      "grad_norm": 0.04708684980869293,
      "learning_rate": 0.00013107511045655376,
      "loss": 0.3696,
      "step": 1175
    },
    {
      "epoch": 0.3458823529411765,
      "grad_norm": 0.06348949670791626,
      "learning_rate": 0.00013101620029455082,
      "loss": 0.3719,
      "step": 1176
    },
    {
      "epoch": 0.3461764705882353,
      "grad_norm": 0.038008108735084534,
      "learning_rate": 0.00013095729013254788,
      "loss": 0.2803,
      "step": 1177
    },
    {
      "epoch": 0.34647058823529414,
      "grad_norm": 0.039894115179777145,
      "learning_rate": 0.00013089837997054492,
      "loss": 0.2906,
      "step": 1178
    },
    {
      "epoch": 0.3467647058823529,
      "grad_norm": 0.05815289914608002,
      "learning_rate": 0.00013083946980854198,
      "loss": 0.3621,
      "step": 1179
    },
    {
      "epoch": 0.34705882352941175,
      "grad_norm": 0.06782546639442444,
      "learning_rate": 0.00013078055964653904,
      "loss": 0.3599,
      "step": 1180
    },
    {
      "epoch": 0.3473529411764706,
      "grad_norm": 0.04992103949189186,
      "learning_rate": 0.0001307216494845361,
      "loss": 0.3308,
      "step": 1181
    },
    {
      "epoch": 0.3476470588235294,
      "grad_norm": 0.054422684013843536,
      "learning_rate": 0.00013066273932253316,
      "loss": 0.4176,
      "step": 1182
    },
    {
      "epoch": 0.34794117647058825,
      "grad_norm": 0.046637579798698425,
      "learning_rate": 0.0001306038291605302,
      "loss": 0.3671,
      "step": 1183
    },
    {
      "epoch": 0.34823529411764703,
      "grad_norm": 0.05252611264586449,
      "learning_rate": 0.00013054491899852725,
      "loss": 0.3589,
      "step": 1184
    },
    {
      "epoch": 0.34852941176470587,
      "grad_norm": 0.05455933138728142,
      "learning_rate": 0.0001304860088365243,
      "loss": 0.3772,
      "step": 1185
    },
    {
      "epoch": 0.3488235294117647,
      "grad_norm": 0.056457776576280594,
      "learning_rate": 0.00013042709867452137,
      "loss": 0.3732,
      "step": 1186
    },
    {
      "epoch": 0.34911764705882353,
      "grad_norm": 0.06405091285705566,
      "learning_rate": 0.00013036818851251843,
      "loss": 0.4421,
      "step": 1187
    },
    {
      "epoch": 0.34941176470588237,
      "grad_norm": 0.04818575456738472,
      "learning_rate": 0.00013030927835051546,
      "loss": 0.3322,
      "step": 1188
    },
    {
      "epoch": 0.3497058823529412,
      "grad_norm": 0.0436028353869915,
      "learning_rate": 0.00013025036818851252,
      "loss": 0.2928,
      "step": 1189
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.06273920834064484,
      "learning_rate": 0.00013019145802650958,
      "loss": 0.3812,
      "step": 1190
    },
    {
      "epoch": 0.3502941176470588,
      "grad_norm": 0.04493565857410431,
      "learning_rate": 0.00013013254786450664,
      "loss": 0.3184,
      "step": 1191
    },
    {
      "epoch": 0.35058823529411764,
      "grad_norm": 0.05724247545003891,
      "learning_rate": 0.0001300736377025037,
      "loss": 0.3803,
      "step": 1192
    },
    {
      "epoch": 0.3508823529411765,
      "grad_norm": 0.06601449847221375,
      "learning_rate": 0.00013001472754050074,
      "loss": 0.4094,
      "step": 1193
    },
    {
      "epoch": 0.3511764705882353,
      "grad_norm": 0.05499592795968056,
      "learning_rate": 0.0001299558173784978,
      "loss": 0.3876,
      "step": 1194
    },
    {
      "epoch": 0.3514705882352941,
      "grad_norm": 0.04843682423233986,
      "learning_rate": 0.00012989690721649486,
      "loss": 0.3678,
      "step": 1195
    },
    {
      "epoch": 0.3517647058823529,
      "grad_norm": 0.058475229889154434,
      "learning_rate": 0.00012983799705449192,
      "loss": 0.3068,
      "step": 1196
    },
    {
      "epoch": 0.35205882352941176,
      "grad_norm": 0.049743667244911194,
      "learning_rate": 0.00012977908689248898,
      "loss": 0.3328,
      "step": 1197
    },
    {
      "epoch": 0.3523529411764706,
      "grad_norm": 0.0543099120259285,
      "learning_rate": 0.000129720176730486,
      "loss": 0.3653,
      "step": 1198
    },
    {
      "epoch": 0.3526470588235294,
      "grad_norm": 0.054477859288454056,
      "learning_rate": 0.00012966126656848307,
      "loss": 0.3195,
      "step": 1199
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.05431341379880905,
      "learning_rate": 0.00012960235640648013,
      "loss": 0.3871,
      "step": 1200
    },
    {
      "epoch": 0.35294117647058826,
      "eval_loss": 0.3578689694404602,
      "eval_runtime": 215.0203,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1200
    },
    {
      "epoch": 0.35323529411764704,
      "grad_norm": 0.05612337589263916,
      "learning_rate": 0.0001295434462444772,
      "loss": 0.3462,
      "step": 1201
    },
    {
      "epoch": 0.35352941176470587,
      "grad_norm": 0.05593489855527878,
      "learning_rate": 0.00012948453608247425,
      "loss": 0.3878,
      "step": 1202
    },
    {
      "epoch": 0.3538235294117647,
      "grad_norm": 0.05170774087309837,
      "learning_rate": 0.00012942562592047128,
      "loss": 0.3339,
      "step": 1203
    },
    {
      "epoch": 0.35411764705882354,
      "grad_norm": 0.05669974163174629,
      "learning_rate": 0.00012936671575846834,
      "loss": 0.3242,
      "step": 1204
    },
    {
      "epoch": 0.35441176470588237,
      "grad_norm": 0.038242511451244354,
      "learning_rate": 0.0001293078055964654,
      "loss": 0.2387,
      "step": 1205
    },
    {
      "epoch": 0.3547058823529412,
      "grad_norm": 0.05158767104148865,
      "learning_rate": 0.00012924889543446247,
      "loss": 0.3643,
      "step": 1206
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.047482699155807495,
      "learning_rate": 0.00012918998527245953,
      "loss": 0.3252,
      "step": 1207
    },
    {
      "epoch": 0.3552941176470588,
      "grad_norm": 0.050700925290584564,
      "learning_rate": 0.00012913107511045656,
      "loss": 0.367,
      "step": 1208
    },
    {
      "epoch": 0.35558823529411765,
      "grad_norm": 0.049844950437545776,
      "learning_rate": 0.00012907216494845362,
      "loss": 0.3341,
      "step": 1209
    },
    {
      "epoch": 0.3558823529411765,
      "grad_norm": 0.05538234859704971,
      "learning_rate": 0.00012901325478645068,
      "loss": 0.4436,
      "step": 1210
    },
    {
      "epoch": 0.3561764705882353,
      "grad_norm": 0.059633154422044754,
      "learning_rate": 0.00012895434462444774,
      "loss": 0.3628,
      "step": 1211
    },
    {
      "epoch": 0.3564705882352941,
      "grad_norm": 0.04025227203965187,
      "learning_rate": 0.0001288954344624448,
      "loss": 0.2922,
      "step": 1212
    },
    {
      "epoch": 0.35676470588235293,
      "grad_norm": 0.061965212225914,
      "learning_rate": 0.00012883652430044183,
      "loss": 0.3856,
      "step": 1213
    },
    {
      "epoch": 0.35705882352941176,
      "grad_norm": 0.05434237793087959,
      "learning_rate": 0.0001287776141384389,
      "loss": 0.3506,
      "step": 1214
    },
    {
      "epoch": 0.3573529411764706,
      "grad_norm": 0.07430143654346466,
      "learning_rate": 0.00012871870397643595,
      "loss": 0.4225,
      "step": 1215
    },
    {
      "epoch": 0.35764705882352943,
      "grad_norm": 0.05231545865535736,
      "learning_rate": 0.000128659793814433,
      "loss": 0.3408,
      "step": 1216
    },
    {
      "epoch": 0.35794117647058826,
      "grad_norm": 0.04871430993080139,
      "learning_rate": 0.00012860088365243007,
      "loss": 0.3267,
      "step": 1217
    },
    {
      "epoch": 0.35823529411764704,
      "grad_norm": 0.05013696849346161,
      "learning_rate": 0.0001285419734904271,
      "loss": 0.3675,
      "step": 1218
    },
    {
      "epoch": 0.3585294117647059,
      "grad_norm": 0.05929703637957573,
      "learning_rate": 0.00012848306332842417,
      "loss": 0.3984,
      "step": 1219
    },
    {
      "epoch": 0.3588235294117647,
      "grad_norm": 0.049690864980220795,
      "learning_rate": 0.00012842415316642123,
      "loss": 0.3409,
      "step": 1220
    },
    {
      "epoch": 0.35911764705882354,
      "grad_norm": 0.05115607753396034,
      "learning_rate": 0.00012836524300441829,
      "loss": 0.4023,
      "step": 1221
    },
    {
      "epoch": 0.3594117647058824,
      "grad_norm": 0.04360247403383255,
      "learning_rate": 0.00012830633284241535,
      "loss": 0.3666,
      "step": 1222
    },
    {
      "epoch": 0.35970588235294115,
      "grad_norm": 0.059131965041160583,
      "learning_rate": 0.00012824742268041238,
      "loss": 0.3581,
      "step": 1223
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.04159872606396675,
      "learning_rate": 0.00012818851251840944,
      "loss": 0.3347,
      "step": 1224
    },
    {
      "epoch": 0.3602941176470588,
      "grad_norm": 0.06740151345729828,
      "learning_rate": 0.0001281296023564065,
      "loss": 0.3808,
      "step": 1225
    },
    {
      "epoch": 0.36058823529411765,
      "grad_norm": 0.040865253657102585,
      "learning_rate": 0.00012807069219440353,
      "loss": 0.2779,
      "step": 1226
    },
    {
      "epoch": 0.3608823529411765,
      "grad_norm": 0.05046997219324112,
      "learning_rate": 0.0001280117820324006,
      "loss": 0.3619,
      "step": 1227
    },
    {
      "epoch": 0.3611764705882353,
      "grad_norm": 0.05985623225569725,
      "learning_rate": 0.00012795287187039763,
      "loss": 0.3638,
      "step": 1228
    },
    {
      "epoch": 0.3614705882352941,
      "grad_norm": 0.045666299760341644,
      "learning_rate": 0.00012789396170839469,
      "loss": 0.3264,
      "step": 1229
    },
    {
      "epoch": 0.36176470588235293,
      "grad_norm": 0.052918385714292526,
      "learning_rate": 0.00012783505154639175,
      "loss": 0.3751,
      "step": 1230
    },
    {
      "epoch": 0.36205882352941177,
      "grad_norm": 0.04437274485826492,
      "learning_rate": 0.0001277761413843888,
      "loss": 0.3176,
      "step": 1231
    },
    {
      "epoch": 0.3623529411764706,
      "grad_norm": 0.0401371493935585,
      "learning_rate": 0.00012771723122238587,
      "loss": 0.3435,
      "step": 1232
    },
    {
      "epoch": 0.36264705882352943,
      "grad_norm": 0.046056028455495834,
      "learning_rate": 0.0001276583210603829,
      "loss": 0.2926,
      "step": 1233
    },
    {
      "epoch": 0.3629411764705882,
      "grad_norm": 0.06220937892794609,
      "learning_rate": 0.00012759941089837996,
      "loss": 0.4164,
      "step": 1234
    },
    {
      "epoch": 0.36323529411764705,
      "grad_norm": 0.04205590486526489,
      "learning_rate": 0.00012754050073637702,
      "loss": 0.3201,
      "step": 1235
    },
    {
      "epoch": 0.3635294117647059,
      "grad_norm": 0.048191435635089874,
      "learning_rate": 0.00012748159057437408,
      "loss": 0.4112,
      "step": 1236
    },
    {
      "epoch": 0.3638235294117647,
      "grad_norm": 0.05335724353790283,
      "learning_rate": 0.00012742268041237114,
      "loss": 0.3529,
      "step": 1237
    },
    {
      "epoch": 0.36411764705882355,
      "grad_norm": 0.04148417338728905,
      "learning_rate": 0.00012736377025036817,
      "loss": 0.2793,
      "step": 1238
    },
    {
      "epoch": 0.3644117647058824,
      "grad_norm": 0.049284785985946655,
      "learning_rate": 0.00012730486008836523,
      "loss": 0.3264,
      "step": 1239
    },
    {
      "epoch": 0.36470588235294116,
      "grad_norm": 0.048924870789051056,
      "learning_rate": 0.0001272459499263623,
      "loss": 0.3747,
      "step": 1240
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.05956423655152321,
      "learning_rate": 0.00012718703976435935,
      "loss": 0.424,
      "step": 1241
    },
    {
      "epoch": 0.3652941176470588,
      "grad_norm": 0.04703722521662712,
      "learning_rate": 0.00012712812960235641,
      "loss": 0.3892,
      "step": 1242
    },
    {
      "epoch": 0.36558823529411766,
      "grad_norm": 0.04680611565709114,
      "learning_rate": 0.00012706921944035345,
      "loss": 0.303,
      "step": 1243
    },
    {
      "epoch": 0.3658823529411765,
      "grad_norm": 0.06876662373542786,
      "learning_rate": 0.0001270103092783505,
      "loss": 0.3599,
      "step": 1244
    },
    {
      "epoch": 0.36617647058823527,
      "grad_norm": 0.057138074189424515,
      "learning_rate": 0.00012695139911634757,
      "loss": 0.3422,
      "step": 1245
    },
    {
      "epoch": 0.3664705882352941,
      "grad_norm": 0.05650115758180618,
      "learning_rate": 0.00012689248895434463,
      "loss": 0.3995,
      "step": 1246
    },
    {
      "epoch": 0.36676470588235294,
      "grad_norm": 0.07055284082889557,
      "learning_rate": 0.0001268335787923417,
      "loss": 0.4487,
      "step": 1247
    },
    {
      "epoch": 0.36705882352941177,
      "grad_norm": 0.07371333986520767,
      "learning_rate": 0.00012677466863033872,
      "loss": 0.4266,
      "step": 1248
    },
    {
      "epoch": 0.3673529411764706,
      "grad_norm": 0.0647057592868805,
      "learning_rate": 0.00012671575846833578,
      "loss": 0.374,
      "step": 1249
    },
    {
      "epoch": 0.36764705882352944,
      "grad_norm": 0.0517519973218441,
      "learning_rate": 0.00012665684830633284,
      "loss": 0.3716,
      "step": 1250
    },
    {
      "epoch": 0.3679411764705882,
      "grad_norm": 0.06566080451011658,
      "learning_rate": 0.0001265979381443299,
      "loss": 0.3913,
      "step": 1251
    },
    {
      "epoch": 0.36823529411764705,
      "grad_norm": 0.04742696136236191,
      "learning_rate": 0.00012653902798232696,
      "loss": 0.3249,
      "step": 1252
    },
    {
      "epoch": 0.3685294117647059,
      "grad_norm": 0.04594354331493378,
      "learning_rate": 0.000126480117820324,
      "loss": 0.2667,
      "step": 1253
    },
    {
      "epoch": 0.3688235294117647,
      "grad_norm": 0.05378134921193123,
      "learning_rate": 0.00012642120765832105,
      "loss": 0.3577,
      "step": 1254
    },
    {
      "epoch": 0.36911764705882355,
      "grad_norm": 0.05394124239683151,
      "learning_rate": 0.00012636229749631812,
      "loss": 0.3335,
      "step": 1255
    },
    {
      "epoch": 0.36941176470588233,
      "grad_norm": 0.046751853078603745,
      "learning_rate": 0.00012630338733431518,
      "loss": 0.3456,
      "step": 1256
    },
    {
      "epoch": 0.36970588235294116,
      "grad_norm": 0.055029045790433884,
      "learning_rate": 0.00012624447717231224,
      "loss": 0.3559,
      "step": 1257
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04299620911478996,
      "learning_rate": 0.00012618556701030927,
      "loss": 0.2829,
      "step": 1258
    },
    {
      "epoch": 0.37029411764705883,
      "grad_norm": 0.043918292969465256,
      "learning_rate": 0.00012612665684830633,
      "loss": 0.2994,
      "step": 1259
    },
    {
      "epoch": 0.37058823529411766,
      "grad_norm": 0.054976657032966614,
      "learning_rate": 0.0001260677466863034,
      "loss": 0.3551,
      "step": 1260
    },
    {
      "epoch": 0.3708823529411765,
      "grad_norm": 0.07367368042469025,
      "learning_rate": 0.00012600883652430045,
      "loss": 0.4302,
      "step": 1261
    },
    {
      "epoch": 0.3711764705882353,
      "grad_norm": 0.05191940441727638,
      "learning_rate": 0.0001259499263622975,
      "loss": 0.3783,
      "step": 1262
    },
    {
      "epoch": 0.3714705882352941,
      "grad_norm": 0.046866804361343384,
      "learning_rate": 0.00012589101620029454,
      "loss": 0.3351,
      "step": 1263
    },
    {
      "epoch": 0.37176470588235294,
      "grad_norm": 0.06573686748743057,
      "learning_rate": 0.0001258321060382916,
      "loss": 0.348,
      "step": 1264
    },
    {
      "epoch": 0.3720588235294118,
      "grad_norm": 0.04800538346171379,
      "learning_rate": 0.00012577319587628866,
      "loss": 0.3733,
      "step": 1265
    },
    {
      "epoch": 0.3723529411764706,
      "grad_norm": 0.05403474345803261,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.3977,
      "step": 1266
    },
    {
      "epoch": 0.3726470588235294,
      "grad_norm": 0.06811253726482391,
      "learning_rate": 0.00012565537555228278,
      "loss": 0.4195,
      "step": 1267
    },
    {
      "epoch": 0.3729411764705882,
      "grad_norm": 0.045033764094114304,
      "learning_rate": 0.00012559646539027982,
      "loss": 0.3159,
      "step": 1268
    },
    {
      "epoch": 0.37323529411764705,
      "grad_norm": 0.04985898733139038,
      "learning_rate": 0.00012553755522827688,
      "loss": 0.3573,
      "step": 1269
    },
    {
      "epoch": 0.3735294117647059,
      "grad_norm": 0.05793865770101547,
      "learning_rate": 0.00012547864506627394,
      "loss": 0.3607,
      "step": 1270
    },
    {
      "epoch": 0.3738235294117647,
      "grad_norm": 0.053910013288259506,
      "learning_rate": 0.000125419734904271,
      "loss": 0.353,
      "step": 1271
    },
    {
      "epoch": 0.37411764705882355,
      "grad_norm": 0.05294743925333023,
      "learning_rate": 0.00012536082474226806,
      "loss": 0.3677,
      "step": 1272
    },
    {
      "epoch": 0.37441176470588233,
      "grad_norm": 0.04266868159174919,
      "learning_rate": 0.0001253019145802651,
      "loss": 0.3302,
      "step": 1273
    },
    {
      "epoch": 0.37470588235294117,
      "grad_norm": 0.047914858907461166,
      "learning_rate": 0.00012524300441826215,
      "loss": 0.3719,
      "step": 1274
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.043540775775909424,
      "learning_rate": 0.0001251840942562592,
      "loss": 0.3159,
      "step": 1275
    },
    {
      "epoch": 0.37529411764705883,
      "grad_norm": 0.05985075235366821,
      "learning_rate": 0.00012512518409425627,
      "loss": 0.4526,
      "step": 1276
    },
    {
      "epoch": 0.37558823529411767,
      "grad_norm": 0.044367995113134384,
      "learning_rate": 0.00012506627393225333,
      "loss": 0.3296,
      "step": 1277
    },
    {
      "epoch": 0.37588235294117645,
      "grad_norm": 0.047462813556194305,
      "learning_rate": 0.00012500736377025036,
      "loss": 0.3895,
      "step": 1278
    },
    {
      "epoch": 0.3761764705882353,
      "grad_norm": 0.044791851192712784,
      "learning_rate": 0.00012494845360824742,
      "loss": 0.307,
      "step": 1279
    },
    {
      "epoch": 0.3764705882352941,
      "grad_norm": 0.05473348870873451,
      "learning_rate": 0.00012488954344624448,
      "loss": 0.3785,
      "step": 1280
    },
    {
      "epoch": 0.37676470588235295,
      "grad_norm": 0.05407163128256798,
      "learning_rate": 0.00012483063328424154,
      "loss": 0.359,
      "step": 1281
    },
    {
      "epoch": 0.3770588235294118,
      "grad_norm": 0.04632994905114174,
      "learning_rate": 0.0001247717231222386,
      "loss": 0.3361,
      "step": 1282
    },
    {
      "epoch": 0.3773529411764706,
      "grad_norm": 0.04580370709300041,
      "learning_rate": 0.00012471281296023564,
      "loss": 0.3544,
      "step": 1283
    },
    {
      "epoch": 0.3776470588235294,
      "grad_norm": 0.05669739469885826,
      "learning_rate": 0.0001246539027982327,
      "loss": 0.4295,
      "step": 1284
    },
    {
      "epoch": 0.3779411764705882,
      "grad_norm": 0.04398402199149132,
      "learning_rate": 0.00012459499263622976,
      "loss": 0.312,
      "step": 1285
    },
    {
      "epoch": 0.37823529411764706,
      "grad_norm": 0.06459106504917145,
      "learning_rate": 0.00012453608247422682,
      "loss": 0.3803,
      "step": 1286
    },
    {
      "epoch": 0.3785294117647059,
      "grad_norm": 0.03704247996211052,
      "learning_rate": 0.00012447717231222388,
      "loss": 0.2848,
      "step": 1287
    },
    {
      "epoch": 0.3788235294117647,
      "grad_norm": 0.06009516492486,
      "learning_rate": 0.0001244182621502209,
      "loss": 0.3577,
      "step": 1288
    },
    {
      "epoch": 0.3791176470588235,
      "grad_norm": 0.053305983543395996,
      "learning_rate": 0.00012435935198821797,
      "loss": 0.3817,
      "step": 1289
    },
    {
      "epoch": 0.37941176470588234,
      "grad_norm": 0.043128881603479385,
      "learning_rate": 0.00012430044182621503,
      "loss": 0.3544,
      "step": 1290
    },
    {
      "epoch": 0.37970588235294117,
      "grad_norm": 0.04540223255753517,
      "learning_rate": 0.0001242415316642121,
      "loss": 0.3139,
      "step": 1291
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.050651755183935165,
      "learning_rate": 0.00012418262150220915,
      "loss": 0.375,
      "step": 1292
    },
    {
      "epoch": 0.38029411764705884,
      "grad_norm": 0.057679902762174606,
      "learning_rate": 0.00012412371134020618,
      "loss": 0.422,
      "step": 1293
    },
    {
      "epoch": 0.38058823529411767,
      "grad_norm": 0.0467391274869442,
      "learning_rate": 0.00012406480117820324,
      "loss": 0.3239,
      "step": 1294
    },
    {
      "epoch": 0.38088235294117645,
      "grad_norm": 0.0463053397834301,
      "learning_rate": 0.0001240058910162003,
      "loss": 0.3072,
      "step": 1295
    },
    {
      "epoch": 0.3811764705882353,
      "grad_norm": 0.03564709797501564,
      "learning_rate": 0.00012394698085419736,
      "loss": 0.2927,
      "step": 1296
    },
    {
      "epoch": 0.3814705882352941,
      "grad_norm": 0.042259495705366135,
      "learning_rate": 0.00012388807069219443,
      "loss": 0.3455,
      "step": 1297
    },
    {
      "epoch": 0.38176470588235295,
      "grad_norm": 0.062223244458436966,
      "learning_rate": 0.00012382916053019146,
      "loss": 0.3889,
      "step": 1298
    },
    {
      "epoch": 0.3820588235294118,
      "grad_norm": 0.03891083225607872,
      "learning_rate": 0.00012377025036818852,
      "loss": 0.341,
      "step": 1299
    },
    {
      "epoch": 0.38235294117647056,
      "grad_norm": 0.04057096689939499,
      "learning_rate": 0.00012371134020618558,
      "loss": 0.3384,
      "step": 1300
    },
    {
      "epoch": 0.38235294117647056,
      "eval_loss": 0.3573998510837555,
      "eval_runtime": 214.9819,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1300
    },
    {
      "epoch": 0.3826470588235294,
      "grad_norm": 0.05427822098135948,
      "learning_rate": 0.00012365243004418264,
      "loss": 0.3945,
      "step": 1301
    },
    {
      "epoch": 0.38294117647058823,
      "grad_norm": 0.05495625361800194,
      "learning_rate": 0.0001235935198821797,
      "loss": 0.3631,
      "step": 1302
    },
    {
      "epoch": 0.38323529411764706,
      "grad_norm": 0.05675226449966431,
      "learning_rate": 0.00012353460972017673,
      "loss": 0.344,
      "step": 1303
    },
    {
      "epoch": 0.3835294117647059,
      "grad_norm": 0.05390354245901108,
      "learning_rate": 0.0001234756995581738,
      "loss": 0.3948,
      "step": 1304
    },
    {
      "epoch": 0.38382352941176473,
      "grad_norm": 0.0680784359574318,
      "learning_rate": 0.00012341678939617085,
      "loss": 0.4396,
      "step": 1305
    },
    {
      "epoch": 0.3841176470588235,
      "grad_norm": 0.0624089278280735,
      "learning_rate": 0.0001233578792341679,
      "loss": 0.3995,
      "step": 1306
    },
    {
      "epoch": 0.38441176470588234,
      "grad_norm": 0.037145357578992844,
      "learning_rate": 0.00012329896907216497,
      "loss": 0.2783,
      "step": 1307
    },
    {
      "epoch": 0.3847058823529412,
      "grad_norm": 0.03985507786273956,
      "learning_rate": 0.000123240058910162,
      "loss": 0.2912,
      "step": 1308
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.04936208203434944,
      "learning_rate": 0.00012318114874815907,
      "loss": 0.3435,
      "step": 1309
    },
    {
      "epoch": 0.38529411764705884,
      "grad_norm": 0.05017708241939545,
      "learning_rate": 0.00012312223858615613,
      "loss": 0.3843,
      "step": 1310
    },
    {
      "epoch": 0.3855882352941176,
      "grad_norm": 0.03305354341864586,
      "learning_rate": 0.00012306332842415319,
      "loss": 0.2579,
      "step": 1311
    },
    {
      "epoch": 0.38588235294117645,
      "grad_norm": 0.05752619355916977,
      "learning_rate": 0.00012300441826215025,
      "loss": 0.3594,
      "step": 1312
    },
    {
      "epoch": 0.3861764705882353,
      "grad_norm": 0.04651322588324547,
      "learning_rate": 0.00012294550810014728,
      "loss": 0.3208,
      "step": 1313
    },
    {
      "epoch": 0.3864705882352941,
      "grad_norm": 0.04646375775337219,
      "learning_rate": 0.00012288659793814434,
      "loss": 0.3574,
      "step": 1314
    },
    {
      "epoch": 0.38676470588235295,
      "grad_norm": 0.05326000973582268,
      "learning_rate": 0.0001228276877761414,
      "loss": 0.3665,
      "step": 1315
    },
    {
      "epoch": 0.3870588235294118,
      "grad_norm": 0.04762781038880348,
      "learning_rate": 0.00012276877761413846,
      "loss": 0.368,
      "step": 1316
    },
    {
      "epoch": 0.38735294117647057,
      "grad_norm": 0.05842554196715355,
      "learning_rate": 0.00012270986745213552,
      "loss": 0.4174,
      "step": 1317
    },
    {
      "epoch": 0.3876470588235294,
      "grad_norm": 0.05039726942777634,
      "learning_rate": 0.00012265095729013255,
      "loss": 0.3597,
      "step": 1318
    },
    {
      "epoch": 0.38794117647058823,
      "grad_norm": 0.048867132514715195,
      "learning_rate": 0.0001225920471281296,
      "loss": 0.3751,
      "step": 1319
    },
    {
      "epoch": 0.38823529411764707,
      "grad_norm": 0.05045283958315849,
      "learning_rate": 0.00012253313696612667,
      "loss": 0.3755,
      "step": 1320
    },
    {
      "epoch": 0.3885294117647059,
      "grad_norm": 0.05624552443623543,
      "learning_rate": 0.00012247422680412373,
      "loss": 0.386,
      "step": 1321
    },
    {
      "epoch": 0.3888235294117647,
      "grad_norm": 0.06283754855394363,
      "learning_rate": 0.0001224153166421208,
      "loss": 0.367,
      "step": 1322
    },
    {
      "epoch": 0.3891176470588235,
      "grad_norm": 0.047938425093889236,
      "learning_rate": 0.00012235640648011783,
      "loss": 0.3577,
      "step": 1323
    },
    {
      "epoch": 0.38941176470588235,
      "grad_norm": 0.055969301611185074,
      "learning_rate": 0.0001222974963181149,
      "loss": 0.3629,
      "step": 1324
    },
    {
      "epoch": 0.3897058823529412,
      "grad_norm": 0.06310264021158218,
      "learning_rate": 0.00012223858615611195,
      "loss": 0.3821,
      "step": 1325
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.042859386652708054,
      "learning_rate": 0.000122179675994109,
      "loss": 0.3136,
      "step": 1326
    },
    {
      "epoch": 0.39029411764705885,
      "grad_norm": 0.05375557765364647,
      "learning_rate": 0.00012212076583210607,
      "loss": 0.4027,
      "step": 1327
    },
    {
      "epoch": 0.3905882352941176,
      "grad_norm": 0.05072581395506859,
      "learning_rate": 0.00012206185567010311,
      "loss": 0.3859,
      "step": 1328
    },
    {
      "epoch": 0.39088235294117646,
      "grad_norm": 0.051452286541461945,
      "learning_rate": 0.00012200294550810016,
      "loss": 0.3451,
      "step": 1329
    },
    {
      "epoch": 0.3911764705882353,
      "grad_norm": 0.07947725057601929,
      "learning_rate": 0.00012194403534609722,
      "loss": 0.4392,
      "step": 1330
    },
    {
      "epoch": 0.3914705882352941,
      "grad_norm": 0.04293189197778702,
      "learning_rate": 0.00012188512518409427,
      "loss": 0.3444,
      "step": 1331
    },
    {
      "epoch": 0.39176470588235296,
      "grad_norm": 0.051394179463386536,
      "learning_rate": 0.0001218262150220913,
      "loss": 0.3695,
      "step": 1332
    },
    {
      "epoch": 0.39205882352941174,
      "grad_norm": 0.0504554845392704,
      "learning_rate": 0.00012176730486008836,
      "loss": 0.3613,
      "step": 1333
    },
    {
      "epoch": 0.39235294117647057,
      "grad_norm": 0.04974290728569031,
      "learning_rate": 0.00012170839469808542,
      "loss": 0.386,
      "step": 1334
    },
    {
      "epoch": 0.3926470588235294,
      "grad_norm": 0.05117402598261833,
      "learning_rate": 0.00012164948453608247,
      "loss": 0.4092,
      "step": 1335
    },
    {
      "epoch": 0.39294117647058824,
      "grad_norm": 0.04851390793919563,
      "learning_rate": 0.00012159057437407953,
      "loss": 0.3862,
      "step": 1336
    },
    {
      "epoch": 0.39323529411764707,
      "grad_norm": 0.04160236567258835,
      "learning_rate": 0.00012153166421207657,
      "loss": 0.3192,
      "step": 1337
    },
    {
      "epoch": 0.3935294117647059,
      "grad_norm": 0.06943189352750778,
      "learning_rate": 0.00012147275405007363,
      "loss": 0.4419,
      "step": 1338
    },
    {
      "epoch": 0.3938235294117647,
      "grad_norm": 0.0646464005112648,
      "learning_rate": 0.0001214138438880707,
      "loss": 0.4499,
      "step": 1339
    },
    {
      "epoch": 0.3941176470588235,
      "grad_norm": 0.04562922194600105,
      "learning_rate": 0.00012135493372606774,
      "loss": 0.3301,
      "step": 1340
    },
    {
      "epoch": 0.39441176470588235,
      "grad_norm": 0.03986398130655289,
      "learning_rate": 0.0001212960235640648,
      "loss": 0.2751,
      "step": 1341
    },
    {
      "epoch": 0.3947058823529412,
      "grad_norm": 0.06359890103340149,
      "learning_rate": 0.00012123711340206185,
      "loss": 0.3676,
      "step": 1342
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.049072153866291046,
      "learning_rate": 0.00012117820324005891,
      "loss": 0.3236,
      "step": 1343
    },
    {
      "epoch": 0.3952941176470588,
      "grad_norm": 0.0649639442563057,
      "learning_rate": 0.00012111929307805597,
      "loss": 0.36,
      "step": 1344
    },
    {
      "epoch": 0.39558823529411763,
      "grad_norm": 0.06171058490872383,
      "learning_rate": 0.00012106038291605301,
      "loss": 0.3942,
      "step": 1345
    },
    {
      "epoch": 0.39588235294117646,
      "grad_norm": 0.03820634260773659,
      "learning_rate": 0.00012100147275405008,
      "loss": 0.2892,
      "step": 1346
    },
    {
      "epoch": 0.3961764705882353,
      "grad_norm": 0.06211497634649277,
      "learning_rate": 0.00012094256259204712,
      "loss": 0.4122,
      "step": 1347
    },
    {
      "epoch": 0.39647058823529413,
      "grad_norm": 0.057267144322395325,
      "learning_rate": 0.00012088365243004418,
      "loss": 0.3969,
      "step": 1348
    },
    {
      "epoch": 0.39676470588235296,
      "grad_norm": 0.05225009471178055,
      "learning_rate": 0.00012082474226804124,
      "loss": 0.3604,
      "step": 1349
    },
    {
      "epoch": 0.39705882352941174,
      "grad_norm": 0.04754442349076271,
      "learning_rate": 0.00012076583210603829,
      "loss": 0.4045,
      "step": 1350
    },
    {
      "epoch": 0.3973529411764706,
      "grad_norm": 0.04741108417510986,
      "learning_rate": 0.00012070692194403535,
      "loss": 0.3202,
      "step": 1351
    },
    {
      "epoch": 0.3976470588235294,
      "grad_norm": 0.04239587485790253,
      "learning_rate": 0.0001206480117820324,
      "loss": 0.354,
      "step": 1352
    },
    {
      "epoch": 0.39794117647058824,
      "grad_norm": 0.04027119651436806,
      "learning_rate": 0.00012058910162002946,
      "loss": 0.2822,
      "step": 1353
    },
    {
      "epoch": 0.3982352941176471,
      "grad_norm": 0.04223267361521721,
      "learning_rate": 0.00012053019145802652,
      "loss": 0.341,
      "step": 1354
    },
    {
      "epoch": 0.3985294117647059,
      "grad_norm": 0.044186413288116455,
      "learning_rate": 0.00012047128129602356,
      "loss": 0.2704,
      "step": 1355
    },
    {
      "epoch": 0.3988235294117647,
      "grad_norm": 0.06058436259627342,
      "learning_rate": 0.00012041237113402062,
      "loss": 0.4137,
      "step": 1356
    },
    {
      "epoch": 0.3991176470588235,
      "grad_norm": 0.04846618324518204,
      "learning_rate": 0.00012035346097201767,
      "loss": 0.2911,
      "step": 1357
    },
    {
      "epoch": 0.39941176470588236,
      "grad_norm": 0.05125468224287033,
      "learning_rate": 0.00012029455081001473,
      "loss": 0.3359,
      "step": 1358
    },
    {
      "epoch": 0.3997058823529412,
      "grad_norm": 0.04562591016292572,
      "learning_rate": 0.00012023564064801178,
      "loss": 0.3383,
      "step": 1359
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.05275029316544533,
      "learning_rate": 0.00012017673048600884,
      "loss": 0.3683,
      "step": 1360
    },
    {
      "epoch": 0.4002941176470588,
      "grad_norm": 0.044708628207445145,
      "learning_rate": 0.0001201178203240059,
      "loss": 0.3483,
      "step": 1361
    },
    {
      "epoch": 0.40058823529411763,
      "grad_norm": 0.05151382088661194,
      "learning_rate": 0.00012005891016200294,
      "loss": 0.4231,
      "step": 1362
    },
    {
      "epoch": 0.40088235294117647,
      "grad_norm": 0.06292923539876938,
      "learning_rate": 0.00012,
      "loss": 0.4159,
      "step": 1363
    },
    {
      "epoch": 0.4011764705882353,
      "grad_norm": 0.054363466799259186,
      "learning_rate": 0.00011994108983799705,
      "loss": 0.2893,
      "step": 1364
    },
    {
      "epoch": 0.40147058823529413,
      "grad_norm": 0.05365629866719246,
      "learning_rate": 0.00011988217967599411,
      "loss": 0.3673,
      "step": 1365
    },
    {
      "epoch": 0.40176470588235297,
      "grad_norm": 0.05142178758978844,
      "learning_rate": 0.00011982326951399117,
      "loss": 0.3996,
      "step": 1366
    },
    {
      "epoch": 0.40205882352941175,
      "grad_norm": 0.05022203549742699,
      "learning_rate": 0.00011976435935198822,
      "loss": 0.3153,
      "step": 1367
    },
    {
      "epoch": 0.4023529411764706,
      "grad_norm": 0.050639402121305466,
      "learning_rate": 0.00011970544918998528,
      "loss": 0.3316,
      "step": 1368
    },
    {
      "epoch": 0.4026470588235294,
      "grad_norm": 0.045551594346761703,
      "learning_rate": 0.00011964653902798232,
      "loss": 0.3238,
      "step": 1369
    },
    {
      "epoch": 0.40294117647058825,
      "grad_norm": 0.05725470185279846,
      "learning_rate": 0.00011958762886597938,
      "loss": 0.3979,
      "step": 1370
    },
    {
      "epoch": 0.4032352941176471,
      "grad_norm": 0.041019637137651443,
      "learning_rate": 0.00011952871870397644,
      "loss": 0.322,
      "step": 1371
    },
    {
      "epoch": 0.40352941176470586,
      "grad_norm": 0.038840241730213165,
      "learning_rate": 0.00011946980854197349,
      "loss": 0.3317,
      "step": 1372
    },
    {
      "epoch": 0.4038235294117647,
      "grad_norm": 0.06554783135652542,
      "learning_rate": 0.00011941089837997055,
      "loss": 0.4131,
      "step": 1373
    },
    {
      "epoch": 0.4041176470588235,
      "grad_norm": 0.061385683715343475,
      "learning_rate": 0.0001193519882179676,
      "loss": 0.3657,
      "step": 1374
    },
    {
      "epoch": 0.40441176470588236,
      "grad_norm": 0.06637674570083618,
      "learning_rate": 0.00011929307805596466,
      "loss": 0.4114,
      "step": 1375
    },
    {
      "epoch": 0.4047058823529412,
      "grad_norm": 0.05040501803159714,
      "learning_rate": 0.00011923416789396172,
      "loss": 0.3593,
      "step": 1376
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.044764019548892975,
      "learning_rate": 0.00011917525773195876,
      "loss": 0.3261,
      "step": 1377
    },
    {
      "epoch": 0.4052941176470588,
      "grad_norm": 0.04309243708848953,
      "learning_rate": 0.00011911634756995582,
      "loss": 0.3362,
      "step": 1378
    },
    {
      "epoch": 0.40558823529411764,
      "grad_norm": 0.04293927550315857,
      "learning_rate": 0.00011905743740795287,
      "loss": 0.3158,
      "step": 1379
    },
    {
      "epoch": 0.40588235294117647,
      "grad_norm": 0.06824080646038055,
      "learning_rate": 0.00011899852724594993,
      "loss": 0.4154,
      "step": 1380
    },
    {
      "epoch": 0.4061764705882353,
      "grad_norm": 0.04876857250928879,
      "learning_rate": 0.00011893961708394699,
      "loss": 0.3614,
      "step": 1381
    },
    {
      "epoch": 0.40647058823529414,
      "grad_norm": 0.04890661686658859,
      "learning_rate": 0.00011888070692194404,
      "loss": 0.3739,
      "step": 1382
    },
    {
      "epoch": 0.4067647058823529,
      "grad_norm": 0.044223979115486145,
      "learning_rate": 0.0001188217967599411,
      "loss": 0.3289,
      "step": 1383
    },
    {
      "epoch": 0.40705882352941175,
      "grad_norm": 0.06071609631180763,
      "learning_rate": 0.00011876288659793814,
      "loss": 0.4001,
      "step": 1384
    },
    {
      "epoch": 0.4073529411764706,
      "grad_norm": 0.05139601230621338,
      "learning_rate": 0.0001187039764359352,
      "loss": 0.3603,
      "step": 1385
    },
    {
      "epoch": 0.4076470588235294,
      "grad_norm": 0.048182811588048935,
      "learning_rate": 0.00011864506627393226,
      "loss": 0.3267,
      "step": 1386
    },
    {
      "epoch": 0.40794117647058825,
      "grad_norm": 0.04559943079948425,
      "learning_rate": 0.00011858615611192931,
      "loss": 0.3092,
      "step": 1387
    },
    {
      "epoch": 0.4082352941176471,
      "grad_norm": 0.05604960024356842,
      "learning_rate": 0.00011852724594992637,
      "loss": 0.3637,
      "step": 1388
    },
    {
      "epoch": 0.40852941176470586,
      "grad_norm": 0.04470619559288025,
      "learning_rate": 0.00011846833578792342,
      "loss": 0.3295,
      "step": 1389
    },
    {
      "epoch": 0.4088235294117647,
      "grad_norm": 0.05306799337267876,
      "learning_rate": 0.00011840942562592048,
      "loss": 0.3955,
      "step": 1390
    },
    {
      "epoch": 0.40911764705882353,
      "grad_norm": 0.056474752724170685,
      "learning_rate": 0.00011835051546391754,
      "loss": 0.369,
      "step": 1391
    },
    {
      "epoch": 0.40941176470588236,
      "grad_norm": 0.04651063680648804,
      "learning_rate": 0.00011829160530191459,
      "loss": 0.3253,
      "step": 1392
    },
    {
      "epoch": 0.4097058823529412,
      "grad_norm": 0.057954829186201096,
      "learning_rate": 0.00011823269513991165,
      "loss": 0.431,
      "step": 1393
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.04268980026245117,
      "learning_rate": 0.00011817378497790869,
      "loss": 0.3504,
      "step": 1394
    },
    {
      "epoch": 0.4102941176470588,
      "grad_norm": 0.056998055428266525,
      "learning_rate": 0.00011811487481590575,
      "loss": 0.3878,
      "step": 1395
    },
    {
      "epoch": 0.41058823529411764,
      "grad_norm": 0.05483836680650711,
      "learning_rate": 0.00011805596465390281,
      "loss": 0.3976,
      "step": 1396
    },
    {
      "epoch": 0.4108823529411765,
      "grad_norm": 0.0504285991191864,
      "learning_rate": 0.00011799705449189986,
      "loss": 0.3843,
      "step": 1397
    },
    {
      "epoch": 0.4111764705882353,
      "grad_norm": 0.0429946705698967,
      "learning_rate": 0.00011793814432989692,
      "loss": 0.3039,
      "step": 1398
    },
    {
      "epoch": 0.41147058823529414,
      "grad_norm": 0.03648083284497261,
      "learning_rate": 0.00011787923416789397,
      "loss": 0.2639,
      "step": 1399
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.04350374639034271,
      "learning_rate": 0.00011782032400589103,
      "loss": 0.2742,
      "step": 1400
    },
    {
      "epoch": 0.4117647058823529,
      "eval_loss": 0.3569006323814392,
      "eval_runtime": 214.9714,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1400
    },
    {
      "epoch": 0.41205882352941176,
      "grad_norm": 0.07458766549825668,
      "learning_rate": 0.00011776141384388809,
      "loss": 0.3827,
      "step": 1401
    },
    {
      "epoch": 0.4123529411764706,
      "grad_norm": 0.043710850179195404,
      "learning_rate": 0.00011770250368188513,
      "loss": 0.3197,
      "step": 1402
    },
    {
      "epoch": 0.4126470588235294,
      "grad_norm": 0.05306949466466904,
      "learning_rate": 0.00011764359351988219,
      "loss": 0.3749,
      "step": 1403
    },
    {
      "epoch": 0.41294117647058826,
      "grad_norm": 0.05690343305468559,
      "learning_rate": 0.00011758468335787924,
      "loss": 0.3432,
      "step": 1404
    },
    {
      "epoch": 0.41323529411764703,
      "grad_norm": 0.058218665421009064,
      "learning_rate": 0.0001175257731958763,
      "loss": 0.3233,
      "step": 1405
    },
    {
      "epoch": 0.41352941176470587,
      "grad_norm": 0.05485540255904198,
      "learning_rate": 0.00011746686303387336,
      "loss": 0.373,
      "step": 1406
    },
    {
      "epoch": 0.4138235294117647,
      "grad_norm": 0.06812147796154022,
      "learning_rate": 0.0001174079528718704,
      "loss": 0.401,
      "step": 1407
    },
    {
      "epoch": 0.41411764705882353,
      "grad_norm": 0.06258100271224976,
      "learning_rate": 0.00011734904270986747,
      "loss": 0.3895,
      "step": 1408
    },
    {
      "epoch": 0.41441176470588237,
      "grad_norm": 0.047856543213129044,
      "learning_rate": 0.00011729013254786451,
      "loss": 0.3583,
      "step": 1409
    },
    {
      "epoch": 0.4147058823529412,
      "grad_norm": 0.03744266554713249,
      "learning_rate": 0.00011723122238586157,
      "loss": 0.3124,
      "step": 1410
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.0475785993039608,
      "learning_rate": 0.00011717231222385863,
      "loss": 0.3573,
      "step": 1411
    },
    {
      "epoch": 0.4152941176470588,
      "grad_norm": 0.039432935416698456,
      "learning_rate": 0.00011711340206185568,
      "loss": 0.2859,
      "step": 1412
    },
    {
      "epoch": 0.41558823529411765,
      "grad_norm": 0.05227135494351387,
      "learning_rate": 0.00011705449189985274,
      "loss": 0.3846,
      "step": 1413
    },
    {
      "epoch": 0.4158823529411765,
      "grad_norm": 0.05332094803452492,
      "learning_rate": 0.00011699558173784979,
      "loss": 0.4047,
      "step": 1414
    },
    {
      "epoch": 0.4161764705882353,
      "grad_norm": 0.04937996342778206,
      "learning_rate": 0.00011693667157584685,
      "loss": 0.3596,
      "step": 1415
    },
    {
      "epoch": 0.4164705882352941,
      "grad_norm": 0.050593115389347076,
      "learning_rate": 0.0001168777614138439,
      "loss": 0.3995,
      "step": 1416
    },
    {
      "epoch": 0.4167647058823529,
      "grad_norm": 0.05611787363886833,
      "learning_rate": 0.00011681885125184095,
      "loss": 0.3386,
      "step": 1417
    },
    {
      "epoch": 0.41705882352941176,
      "grad_norm": 0.04736638069152832,
      "learning_rate": 0.00011675994108983801,
      "loss": 0.3713,
      "step": 1418
    },
    {
      "epoch": 0.4173529411764706,
      "grad_norm": 0.04788510128855705,
      "learning_rate": 0.00011670103092783506,
      "loss": 0.3835,
      "step": 1419
    },
    {
      "epoch": 0.4176470588235294,
      "grad_norm": 0.0423266664147377,
      "learning_rate": 0.00011664212076583212,
      "loss": 0.3236,
      "step": 1420
    },
    {
      "epoch": 0.41794117647058826,
      "grad_norm": 0.057038407772779465,
      "learning_rate": 0.00011658321060382917,
      "loss": 0.41,
      "step": 1421
    },
    {
      "epoch": 0.41823529411764704,
      "grad_norm": 0.045573167502880096,
      "learning_rate": 0.00011652430044182623,
      "loss": 0.3659,
      "step": 1422
    },
    {
      "epoch": 0.41852941176470587,
      "grad_norm": 0.03999052196741104,
      "learning_rate": 0.00011646539027982329,
      "loss": 0.3491,
      "step": 1423
    },
    {
      "epoch": 0.4188235294117647,
      "grad_norm": 0.04251812770962715,
      "learning_rate": 0.00011640648011782033,
      "loss": 0.2533,
      "step": 1424
    },
    {
      "epoch": 0.41911764705882354,
      "grad_norm": 0.05124450847506523,
      "learning_rate": 0.0001163475699558174,
      "loss": 0.4325,
      "step": 1425
    },
    {
      "epoch": 0.4194117647058824,
      "grad_norm": 0.05424467474222183,
      "learning_rate": 0.00011628865979381444,
      "loss": 0.4006,
      "step": 1426
    },
    {
      "epoch": 0.41970588235294115,
      "grad_norm": 0.047938816249370575,
      "learning_rate": 0.0001162297496318115,
      "loss": 0.3322,
      "step": 1427
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.044476523995399475,
      "learning_rate": 0.00011617083946980856,
      "loss": 0.3122,
      "step": 1428
    },
    {
      "epoch": 0.4202941176470588,
      "grad_norm": 0.06259079277515411,
      "learning_rate": 0.00011611192930780561,
      "loss": 0.3383,
      "step": 1429
    },
    {
      "epoch": 0.42058823529411765,
      "grad_norm": 0.04609962925314903,
      "learning_rate": 0.00011605301914580267,
      "loss": 0.3761,
      "step": 1430
    },
    {
      "epoch": 0.4208823529411765,
      "grad_norm": 0.04458559304475784,
      "learning_rate": 0.00011599410898379971,
      "loss": 0.3515,
      "step": 1431
    },
    {
      "epoch": 0.4211764705882353,
      "grad_norm": 0.043507181107997894,
      "learning_rate": 0.00011593519882179677,
      "loss": 0.2688,
      "step": 1432
    },
    {
      "epoch": 0.4214705882352941,
      "grad_norm": 0.05503888055682182,
      "learning_rate": 0.00011587628865979384,
      "loss": 0.3382,
      "step": 1433
    },
    {
      "epoch": 0.42176470588235293,
      "grad_norm": 0.04582468047738075,
      "learning_rate": 0.00011581737849779088,
      "loss": 0.3373,
      "step": 1434
    },
    {
      "epoch": 0.42205882352941176,
      "grad_norm": 0.054588206112384796,
      "learning_rate": 0.00011575846833578794,
      "loss": 0.3856,
      "step": 1435
    },
    {
      "epoch": 0.4223529411764706,
      "grad_norm": 0.053621381521224976,
      "learning_rate": 0.00011569955817378499,
      "loss": 0.36,
      "step": 1436
    },
    {
      "epoch": 0.42264705882352943,
      "grad_norm": 0.04988783225417137,
      "learning_rate": 0.00011564064801178205,
      "loss": 0.2902,
      "step": 1437
    },
    {
      "epoch": 0.4229411764705882,
      "grad_norm": 0.04767896607518196,
      "learning_rate": 0.00011558173784977908,
      "loss": 0.3645,
      "step": 1438
    },
    {
      "epoch": 0.42323529411764704,
      "grad_norm": 0.047525037080049515,
      "learning_rate": 0.00011552282768777614,
      "loss": 0.3512,
      "step": 1439
    },
    {
      "epoch": 0.4235294117647059,
      "grad_norm": 0.04808172583580017,
      "learning_rate": 0.00011546391752577319,
      "loss": 0.329,
      "step": 1440
    },
    {
      "epoch": 0.4238235294117647,
      "grad_norm": 0.04395170882344246,
      "learning_rate": 0.00011540500736377025,
      "loss": 0.3579,
      "step": 1441
    },
    {
      "epoch": 0.42411764705882354,
      "grad_norm": 0.04004097729921341,
      "learning_rate": 0.0001153460972017673,
      "loss": 0.2977,
      "step": 1442
    },
    {
      "epoch": 0.4244117647058824,
      "grad_norm": 0.05595221370458603,
      "learning_rate": 0.00011528718703976436,
      "loss": 0.4217,
      "step": 1443
    },
    {
      "epoch": 0.42470588235294116,
      "grad_norm": 0.054234784096479416,
      "learning_rate": 0.00011522827687776142,
      "loss": 0.384,
      "step": 1444
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.043437324464321136,
      "learning_rate": 0.00011516936671575846,
      "loss": 0.343,
      "step": 1445
    },
    {
      "epoch": 0.4252941176470588,
      "grad_norm": 0.03511124476790428,
      "learning_rate": 0.00011511045655375552,
      "loss": 0.2739,
      "step": 1446
    },
    {
      "epoch": 0.42558823529411766,
      "grad_norm": 0.04205278679728508,
      "learning_rate": 0.00011505154639175257,
      "loss": 0.2734,
      "step": 1447
    },
    {
      "epoch": 0.4258823529411765,
      "grad_norm": 0.03931418061256409,
      "learning_rate": 0.00011499263622974963,
      "loss": 0.3427,
      "step": 1448
    },
    {
      "epoch": 0.42617647058823527,
      "grad_norm": 0.04164159297943115,
      "learning_rate": 0.00011493372606774669,
      "loss": 0.3156,
      "step": 1449
    },
    {
      "epoch": 0.4264705882352941,
      "grad_norm": 0.05470561981201172,
      "learning_rate": 0.00011487481590574374,
      "loss": 0.3763,
      "step": 1450
    },
    {
      "epoch": 0.42676470588235293,
      "grad_norm": 0.03529087081551552,
      "learning_rate": 0.0001148159057437408,
      "loss": 0.2658,
      "step": 1451
    },
    {
      "epoch": 0.42705882352941177,
      "grad_norm": 0.04991048201918602,
      "learning_rate": 0.00011475699558173784,
      "loss": 0.3047,
      "step": 1452
    },
    {
      "epoch": 0.4273529411764706,
      "grad_norm": 0.05847854167222977,
      "learning_rate": 0.0001146980854197349,
      "loss": 0.3271,
      "step": 1453
    },
    {
      "epoch": 0.42764705882352944,
      "grad_norm": 0.06347766518592834,
      "learning_rate": 0.00011463917525773196,
      "loss": 0.3757,
      "step": 1454
    },
    {
      "epoch": 0.4279411764705882,
      "grad_norm": 0.05963263288140297,
      "learning_rate": 0.00011458026509572901,
      "loss": 0.4113,
      "step": 1455
    },
    {
      "epoch": 0.42823529411764705,
      "grad_norm": 0.04550037533044815,
      "learning_rate": 0.00011452135493372607,
      "loss": 0.3557,
      "step": 1456
    },
    {
      "epoch": 0.4285294117647059,
      "grad_norm": 0.044173356145620346,
      "learning_rate": 0.00011446244477172312,
      "loss": 0.3029,
      "step": 1457
    },
    {
      "epoch": 0.4288235294117647,
      "grad_norm": 0.06303786486387253,
      "learning_rate": 0.00011440353460972018,
      "loss": 0.3945,
      "step": 1458
    },
    {
      "epoch": 0.42911764705882355,
      "grad_norm": 0.05114191398024559,
      "learning_rate": 0.00011434462444771724,
      "loss": 0.3468,
      "step": 1459
    },
    {
      "epoch": 0.4294117647058823,
      "grad_norm": 0.05063660070300102,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.3676,
      "step": 1460
    },
    {
      "epoch": 0.42970588235294116,
      "grad_norm": 0.05120294913649559,
      "learning_rate": 0.00011422680412371134,
      "loss": 0.401,
      "step": 1461
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.05055505409836769,
      "learning_rate": 0.00011416789396170839,
      "loss": 0.3999,
      "step": 1462
    },
    {
      "epoch": 0.4302941176470588,
      "grad_norm": 0.06068051978945732,
      "learning_rate": 0.00011410898379970545,
      "loss": 0.3892,
      "step": 1463
    },
    {
      "epoch": 0.43058823529411766,
      "grad_norm": 0.050354793667793274,
      "learning_rate": 0.0001140500736377025,
      "loss": 0.349,
      "step": 1464
    },
    {
      "epoch": 0.4308823529411765,
      "grad_norm": 0.042131051421165466,
      "learning_rate": 0.00011399116347569956,
      "loss": 0.2673,
      "step": 1465
    },
    {
      "epoch": 0.43117647058823527,
      "grad_norm": 0.038415875285863876,
      "learning_rate": 0.00011393225331369662,
      "loss": 0.3332,
      "step": 1466
    },
    {
      "epoch": 0.4314705882352941,
      "grad_norm": 0.04174242168664932,
      "learning_rate": 0.00011387334315169366,
      "loss": 0.3568,
      "step": 1467
    },
    {
      "epoch": 0.43176470588235294,
      "grad_norm": 0.04415922239422798,
      "learning_rate": 0.00011381443298969072,
      "loss": 0.3693,
      "step": 1468
    },
    {
      "epoch": 0.4320588235294118,
      "grad_norm": 0.040057793259620667,
      "learning_rate": 0.00011375552282768777,
      "loss": 0.2996,
      "step": 1469
    },
    {
      "epoch": 0.4323529411764706,
      "grad_norm": 0.04788050055503845,
      "learning_rate": 0.00011369661266568483,
      "loss": 0.3377,
      "step": 1470
    },
    {
      "epoch": 0.4326470588235294,
      "grad_norm": 0.05230429768562317,
      "learning_rate": 0.00011363770250368189,
      "loss": 0.4103,
      "step": 1471
    },
    {
      "epoch": 0.4329411764705882,
      "grad_norm": 0.04937032610177994,
      "learning_rate": 0.00011357879234167894,
      "loss": 0.3588,
      "step": 1472
    },
    {
      "epoch": 0.43323529411764705,
      "grad_norm": 0.04389195889234543,
      "learning_rate": 0.000113519882179676,
      "loss": 0.2986,
      "step": 1473
    },
    {
      "epoch": 0.4335294117647059,
      "grad_norm": 0.0464177131652832,
      "learning_rate": 0.00011346097201767304,
      "loss": 0.3412,
      "step": 1474
    },
    {
      "epoch": 0.4338235294117647,
      "grad_norm": 0.045647576451301575,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.3591,
      "step": 1475
    },
    {
      "epoch": 0.43411764705882355,
      "grad_norm": 0.04733160138130188,
      "learning_rate": 0.00011334315169366716,
      "loss": 0.3643,
      "step": 1476
    },
    {
      "epoch": 0.43441176470588233,
      "grad_norm": 0.05155586823821068,
      "learning_rate": 0.00011328424153166421,
      "loss": 0.3303,
      "step": 1477
    },
    {
      "epoch": 0.43470588235294116,
      "grad_norm": 0.04916726425290108,
      "learning_rate": 0.00011322533136966127,
      "loss": 0.379,
      "step": 1478
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.03938120976090431,
      "learning_rate": 0.00011316642120765832,
      "loss": 0.2725,
      "step": 1479
    },
    {
      "epoch": 0.43529411764705883,
      "grad_norm": 0.04570867866277695,
      "learning_rate": 0.00011310751104565538,
      "loss": 0.3713,
      "step": 1480
    },
    {
      "epoch": 0.43558823529411766,
      "grad_norm": 0.04126943275332451,
      "learning_rate": 0.00011304860088365244,
      "loss": 0.3266,
      "step": 1481
    },
    {
      "epoch": 0.43588235294117644,
      "grad_norm": 0.04315617308020592,
      "learning_rate": 0.00011298969072164949,
      "loss": 0.3476,
      "step": 1482
    },
    {
      "epoch": 0.4361764705882353,
      "grad_norm": 0.04201637580990791,
      "learning_rate": 0.00011293078055964655,
      "loss": 0.3418,
      "step": 1483
    },
    {
      "epoch": 0.4364705882352941,
      "grad_norm": 0.03902050852775574,
      "learning_rate": 0.00011287187039764359,
      "loss": 0.3317,
      "step": 1484
    },
    {
      "epoch": 0.43676470588235294,
      "grad_norm": 0.04993387311697006,
      "learning_rate": 0.00011281296023564065,
      "loss": 0.3522,
      "step": 1485
    },
    {
      "epoch": 0.4370588235294118,
      "grad_norm": 0.045794688165187836,
      "learning_rate": 0.00011275405007363771,
      "loss": 0.4014,
      "step": 1486
    },
    {
      "epoch": 0.4373529411764706,
      "grad_norm": 0.06758589297533035,
      "learning_rate": 0.00011269513991163476,
      "loss": 0.458,
      "step": 1487
    },
    {
      "epoch": 0.4376470588235294,
      "grad_norm": 0.047969575971364975,
      "learning_rate": 0.00011263622974963182,
      "loss": 0.3804,
      "step": 1488
    },
    {
      "epoch": 0.4379411764705882,
      "grad_norm": 0.04207945987582207,
      "learning_rate": 0.00011257731958762887,
      "loss": 0.3226,
      "step": 1489
    },
    {
      "epoch": 0.43823529411764706,
      "grad_norm": 0.050753869116306305,
      "learning_rate": 0.00011251840942562593,
      "loss": 0.3817,
      "step": 1490
    },
    {
      "epoch": 0.4385294117647059,
      "grad_norm": 0.046424079686403275,
      "learning_rate": 0.00011245949926362299,
      "loss": 0.3474,
      "step": 1491
    },
    {
      "epoch": 0.4388235294117647,
      "grad_norm": 0.047728583216667175,
      "learning_rate": 0.00011240058910162003,
      "loss": 0.3749,
      "step": 1492
    },
    {
      "epoch": 0.43911764705882356,
      "grad_norm": 0.05758187174797058,
      "learning_rate": 0.00011234167893961709,
      "loss": 0.3822,
      "step": 1493
    },
    {
      "epoch": 0.43941176470588234,
      "grad_norm": 0.03489355370402336,
      "learning_rate": 0.00011228276877761414,
      "loss": 0.3257,
      "step": 1494
    },
    {
      "epoch": 0.43970588235294117,
      "grad_norm": 0.04968120530247688,
      "learning_rate": 0.0001122238586156112,
      "loss": 0.4335,
      "step": 1495
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.04561382904648781,
      "learning_rate": 0.00011216494845360826,
      "loss": 0.3638,
      "step": 1496
    },
    {
      "epoch": 0.44029411764705884,
      "grad_norm": 0.04203074052929878,
      "learning_rate": 0.0001121060382916053,
      "loss": 0.291,
      "step": 1497
    },
    {
      "epoch": 0.44058823529411767,
      "grad_norm": 0.05255045369267464,
      "learning_rate": 0.00011204712812960237,
      "loss": 0.3573,
      "step": 1498
    },
    {
      "epoch": 0.44088235294117645,
      "grad_norm": 0.0453685037791729,
      "learning_rate": 0.00011198821796759941,
      "loss": 0.3538,
      "step": 1499
    },
    {
      "epoch": 0.4411764705882353,
      "grad_norm": 0.04543042927980423,
      "learning_rate": 0.00011192930780559647,
      "loss": 0.3126,
      "step": 1500
    },
    {
      "epoch": 0.4411764705882353,
      "eval_loss": 0.35577255487442017,
      "eval_runtime": 215.0566,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1500
    },
    {
      "epoch": 0.4414705882352941,
      "grad_norm": 0.05047621205449104,
      "learning_rate": 0.00011187039764359353,
      "loss": 0.3417,
      "step": 1501
    },
    {
      "epoch": 0.44176470588235295,
      "grad_norm": 0.03938969597220421,
      "learning_rate": 0.00011181148748159058,
      "loss": 0.2971,
      "step": 1502
    },
    {
      "epoch": 0.4420588235294118,
      "grad_norm": 0.05975763872265816,
      "learning_rate": 0.00011175257731958764,
      "loss": 0.3409,
      "step": 1503
    },
    {
      "epoch": 0.4423529411764706,
      "grad_norm": 0.04659176617860794,
      "learning_rate": 0.00011169366715758469,
      "loss": 0.3623,
      "step": 1504
    },
    {
      "epoch": 0.4426470588235294,
      "grad_norm": 0.04382623732089996,
      "learning_rate": 0.00011163475699558175,
      "loss": 0.3661,
      "step": 1505
    },
    {
      "epoch": 0.4429411764705882,
      "grad_norm": 0.04652060568332672,
      "learning_rate": 0.00011157584683357881,
      "loss": 0.3047,
      "step": 1506
    },
    {
      "epoch": 0.44323529411764706,
      "grad_norm": 0.0461694560945034,
      "learning_rate": 0.00011151693667157585,
      "loss": 0.3551,
      "step": 1507
    },
    {
      "epoch": 0.4435294117647059,
      "grad_norm": 0.07965178042650223,
      "learning_rate": 0.00011145802650957291,
      "loss": 0.382,
      "step": 1508
    },
    {
      "epoch": 0.44382352941176473,
      "grad_norm": 0.03963971510529518,
      "learning_rate": 0.00011139911634756996,
      "loss": 0.3286,
      "step": 1509
    },
    {
      "epoch": 0.4441176470588235,
      "grad_norm": 0.05175135284662247,
      "learning_rate": 0.00011134020618556702,
      "loss": 0.3451,
      "step": 1510
    },
    {
      "epoch": 0.44441176470588234,
      "grad_norm": 0.03990388661623001,
      "learning_rate": 0.00011128129602356408,
      "loss": 0.3225,
      "step": 1511
    },
    {
      "epoch": 0.4447058823529412,
      "grad_norm": 0.04993777349591255,
      "learning_rate": 0.00011122238586156113,
      "loss": 0.3751,
      "step": 1512
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.041866909712553024,
      "learning_rate": 0.00011116347569955819,
      "loss": 0.3111,
      "step": 1513
    },
    {
      "epoch": 0.44529411764705884,
      "grad_norm": 0.05114729329943657,
      "learning_rate": 0.00011110456553755523,
      "loss": 0.4107,
      "step": 1514
    },
    {
      "epoch": 0.4455882352941177,
      "grad_norm": 0.042058851569890976,
      "learning_rate": 0.0001110456553755523,
      "loss": 0.3367,
      "step": 1515
    },
    {
      "epoch": 0.44588235294117645,
      "grad_norm": 0.04709388315677643,
      "learning_rate": 0.00011098674521354935,
      "loss": 0.3212,
      "step": 1516
    },
    {
      "epoch": 0.4461764705882353,
      "grad_norm": 0.05930517613887787,
      "learning_rate": 0.0001109278350515464,
      "loss": 0.3863,
      "step": 1517
    },
    {
      "epoch": 0.4464705882352941,
      "grad_norm": 0.05467585474252701,
      "learning_rate": 0.00011086892488954346,
      "loss": 0.3546,
      "step": 1518
    },
    {
      "epoch": 0.44676470588235295,
      "grad_norm": 0.03955887630581856,
      "learning_rate": 0.00011081001472754051,
      "loss": 0.32,
      "step": 1519
    },
    {
      "epoch": 0.4470588235294118,
      "grad_norm": 0.05607407167553902,
      "learning_rate": 0.00011075110456553757,
      "loss": 0.3713,
      "step": 1520
    },
    {
      "epoch": 0.44735294117647056,
      "grad_norm": 0.06131068244576454,
      "learning_rate": 0.00011069219440353461,
      "loss": 0.4011,
      "step": 1521
    },
    {
      "epoch": 0.4476470588235294,
      "grad_norm": 0.05761842429637909,
      "learning_rate": 0.00011063328424153167,
      "loss": 0.3645,
      "step": 1522
    },
    {
      "epoch": 0.44794117647058823,
      "grad_norm": 0.04051276296377182,
      "learning_rate": 0.00011057437407952874,
      "loss": 0.4106,
      "step": 1523
    },
    {
      "epoch": 0.44823529411764707,
      "grad_norm": 0.0394602008163929,
      "learning_rate": 0.00011051546391752578,
      "loss": 0.3245,
      "step": 1524
    },
    {
      "epoch": 0.4485294117647059,
      "grad_norm": 0.056172020733356476,
      "learning_rate": 0.00011045655375552284,
      "loss": 0.4084,
      "step": 1525
    },
    {
      "epoch": 0.44882352941176473,
      "grad_norm": 0.06924355775117874,
      "learning_rate": 0.00011039764359351989,
      "loss": 0.3444,
      "step": 1526
    },
    {
      "epoch": 0.4491176470588235,
      "grad_norm": 0.0676816999912262,
      "learning_rate": 0.00011033873343151695,
      "loss": 0.4273,
      "step": 1527
    },
    {
      "epoch": 0.44941176470588234,
      "grad_norm": 0.051533643156290054,
      "learning_rate": 0.00011027982326951401,
      "loss": 0.3484,
      "step": 1528
    },
    {
      "epoch": 0.4497058823529412,
      "grad_norm": 0.05038343369960785,
      "learning_rate": 0.00011022091310751106,
      "loss": 0.3979,
      "step": 1529
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.060940392315387726,
      "learning_rate": 0.00011016200294550812,
      "loss": 0.4234,
      "step": 1530
    },
    {
      "epoch": 0.45029411764705884,
      "grad_norm": 0.07136587798595428,
      "learning_rate": 0.00011010309278350516,
      "loss": 0.3639,
      "step": 1531
    },
    {
      "epoch": 0.4505882352941176,
      "grad_norm": 0.05903846397995949,
      "learning_rate": 0.00011004418262150222,
      "loss": 0.3605,
      "step": 1532
    },
    {
      "epoch": 0.45088235294117646,
      "grad_norm": 0.0585053451359272,
      "learning_rate": 0.00010998527245949928,
      "loss": 0.3558,
      "step": 1533
    },
    {
      "epoch": 0.4511764705882353,
      "grad_norm": 0.0552399717271328,
      "learning_rate": 0.00010992636229749633,
      "loss": 0.3434,
      "step": 1534
    },
    {
      "epoch": 0.4514705882352941,
      "grad_norm": 0.037142474204301834,
      "learning_rate": 0.00010986745213549339,
      "loss": 0.2522,
      "step": 1535
    },
    {
      "epoch": 0.45176470588235296,
      "grad_norm": 0.06180565804243088,
      "learning_rate": 0.00010980854197349044,
      "loss": 0.3777,
      "step": 1536
    },
    {
      "epoch": 0.4520588235294118,
      "grad_norm": 0.04483265429735184,
      "learning_rate": 0.0001097496318114875,
      "loss": 0.3346,
      "step": 1537
    },
    {
      "epoch": 0.45235294117647057,
      "grad_norm": 0.06104700639843941,
      "learning_rate": 0.00010969072164948456,
      "loss": 0.3666,
      "step": 1538
    },
    {
      "epoch": 0.4526470588235294,
      "grad_norm": 0.049549516290426254,
      "learning_rate": 0.0001096318114874816,
      "loss": 0.3262,
      "step": 1539
    },
    {
      "epoch": 0.45294117647058824,
      "grad_norm": 0.050015825778245926,
      "learning_rate": 0.00010957290132547866,
      "loss": 0.3384,
      "step": 1540
    },
    {
      "epoch": 0.45323529411764707,
      "grad_norm": 0.0609889030456543,
      "learning_rate": 0.00010951399116347571,
      "loss": 0.4021,
      "step": 1541
    },
    {
      "epoch": 0.4535294117647059,
      "grad_norm": 0.05243980139493942,
      "learning_rate": 0.00010945508100147277,
      "loss": 0.3282,
      "step": 1542
    },
    {
      "epoch": 0.4538235294117647,
      "grad_norm": 0.044156163930892944,
      "learning_rate": 0.00010939617083946983,
      "loss": 0.3248,
      "step": 1543
    },
    {
      "epoch": 0.4541176470588235,
      "grad_norm": 0.0447097048163414,
      "learning_rate": 0.00010933726067746686,
      "loss": 0.286,
      "step": 1544
    },
    {
      "epoch": 0.45441176470588235,
      "grad_norm": 0.05780167505145073,
      "learning_rate": 0.00010927835051546391,
      "loss": 0.3582,
      "step": 1545
    },
    {
      "epoch": 0.4547058823529412,
      "grad_norm": 0.040962256491184235,
      "learning_rate": 0.00010921944035346097,
      "loss": 0.3314,
      "step": 1546
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.046962495893239975,
      "learning_rate": 0.00010916053019145802,
      "loss": 0.3109,
      "step": 1547
    },
    {
      "epoch": 0.45529411764705885,
      "grad_norm": 0.032504603266716,
      "learning_rate": 0.00010910162002945508,
      "loss": 0.2652,
      "step": 1548
    },
    {
      "epoch": 0.4555882352941176,
      "grad_norm": 0.0438128300011158,
      "learning_rate": 0.00010904270986745214,
      "loss": 0.3135,
      "step": 1549
    },
    {
      "epoch": 0.45588235294117646,
      "grad_norm": 0.036934562027454376,
      "learning_rate": 0.00010898379970544918,
      "loss": 0.3158,
      "step": 1550
    },
    {
      "epoch": 0.4561764705882353,
      "grad_norm": 0.043571434915065765,
      "learning_rate": 0.00010892488954344624,
      "loss": 0.3111,
      "step": 1551
    },
    {
      "epoch": 0.45647058823529413,
      "grad_norm": 0.05435504764318466,
      "learning_rate": 0.00010886597938144329,
      "loss": 0.3389,
      "step": 1552
    },
    {
      "epoch": 0.45676470588235296,
      "grad_norm": 0.05362159386277199,
      "learning_rate": 0.00010880706921944035,
      "loss": 0.3636,
      "step": 1553
    },
    {
      "epoch": 0.45705882352941174,
      "grad_norm": 0.05193367227911949,
      "learning_rate": 0.00010874815905743741,
      "loss": 0.3743,
      "step": 1554
    },
    {
      "epoch": 0.4573529411764706,
      "grad_norm": 0.038116391748189926,
      "learning_rate": 0.00010868924889543446,
      "loss": 0.273,
      "step": 1555
    },
    {
      "epoch": 0.4576470588235294,
      "grad_norm": 0.04862749204039574,
      "learning_rate": 0.00010863033873343152,
      "loss": 0.3577,
      "step": 1556
    },
    {
      "epoch": 0.45794117647058824,
      "grad_norm": 0.03746433183550835,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.3134,
      "step": 1557
    },
    {
      "epoch": 0.4582352941176471,
      "grad_norm": 0.043618589639663696,
      "learning_rate": 0.00010851251840942562,
      "loss": 0.3539,
      "step": 1558
    },
    {
      "epoch": 0.4585294117647059,
      "grad_norm": 0.0575152151286602,
      "learning_rate": 0.00010845360824742268,
      "loss": 0.4455,
      "step": 1559
    },
    {
      "epoch": 0.4588235294117647,
      "grad_norm": 0.06640410423278809,
      "learning_rate": 0.00010839469808541973,
      "loss": 0.4181,
      "step": 1560
    },
    {
      "epoch": 0.4591176470588235,
      "grad_norm": 0.06133562698960304,
      "learning_rate": 0.00010833578792341679,
      "loss": 0.3172,
      "step": 1561
    },
    {
      "epoch": 0.45941176470588235,
      "grad_norm": 0.05035899206995964,
      "learning_rate": 0.00010827687776141384,
      "loss": 0.3743,
      "step": 1562
    },
    {
      "epoch": 0.4597058823529412,
      "grad_norm": 0.03488511964678764,
      "learning_rate": 0.0001082179675994109,
      "loss": 0.2684,
      "step": 1563
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05320039764046669,
      "learning_rate": 0.00010815905743740796,
      "loss": 0.3513,
      "step": 1564
    },
    {
      "epoch": 0.4602941176470588,
      "grad_norm": 0.04653910547494888,
      "learning_rate": 0.000108100147275405,
      "loss": 0.3217,
      "step": 1565
    },
    {
      "epoch": 0.46058823529411763,
      "grad_norm": 0.06979376077651978,
      "learning_rate": 0.00010804123711340206,
      "loss": 0.4037,
      "step": 1566
    },
    {
      "epoch": 0.46088235294117647,
      "grad_norm": 0.04450828582048416,
      "learning_rate": 0.00010798232695139911,
      "loss": 0.3072,
      "step": 1567
    },
    {
      "epoch": 0.4611764705882353,
      "grad_norm": 0.040792159736156464,
      "learning_rate": 0.00010792341678939617,
      "loss": 0.2711,
      "step": 1568
    },
    {
      "epoch": 0.46147058823529413,
      "grad_norm": 0.06058608368039131,
      "learning_rate": 0.00010786450662739322,
      "loss": 0.3834,
      "step": 1569
    },
    {
      "epoch": 0.46176470588235297,
      "grad_norm": 0.05934825912117958,
      "learning_rate": 0.00010780559646539028,
      "loss": 0.3944,
      "step": 1570
    },
    {
      "epoch": 0.46205882352941174,
      "grad_norm": 0.05712960287928581,
      "learning_rate": 0.00010774668630338734,
      "loss": 0.3131,
      "step": 1571
    },
    {
      "epoch": 0.4623529411764706,
      "grad_norm": 0.06168792024254799,
      "learning_rate": 0.00010768777614138439,
      "loss": 0.3975,
      "step": 1572
    },
    {
      "epoch": 0.4626470588235294,
      "grad_norm": 0.056472260504961014,
      "learning_rate": 0.00010762886597938145,
      "loss": 0.3883,
      "step": 1573
    },
    {
      "epoch": 0.46294117647058824,
      "grad_norm": 0.040869034826755524,
      "learning_rate": 0.00010756995581737849,
      "loss": 0.3453,
      "step": 1574
    },
    {
      "epoch": 0.4632352941176471,
      "grad_norm": 0.0409223809838295,
      "learning_rate": 0.00010751104565537555,
      "loss": 0.3228,
      "step": 1575
    },
    {
      "epoch": 0.46352941176470586,
      "grad_norm": 0.031191492453217506,
      "learning_rate": 0.00010745213549337261,
      "loss": 0.2592,
      "step": 1576
    },
    {
      "epoch": 0.4638235294117647,
      "grad_norm": 0.052316587418317795,
      "learning_rate": 0.00010739322533136966,
      "loss": 0.3996,
      "step": 1577
    },
    {
      "epoch": 0.4641176470588235,
      "grad_norm": 0.043100278824567795,
      "learning_rate": 0.00010733431516936672,
      "loss": 0.329,
      "step": 1578
    },
    {
      "epoch": 0.46441176470588236,
      "grad_norm": 0.04168938845396042,
      "learning_rate": 0.00010727540500736377,
      "loss": 0.3319,
      "step": 1579
    },
    {
      "epoch": 0.4647058823529412,
      "grad_norm": 0.05572114884853363,
      "learning_rate": 0.00010721649484536083,
      "loss": 0.407,
      "step": 1580
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.049271728843450546,
      "learning_rate": 0.00010715758468335789,
      "loss": 0.3722,
      "step": 1581
    },
    {
      "epoch": 0.4652941176470588,
      "grad_norm": 0.04736950248479843,
      "learning_rate": 0.00010709867452135493,
      "loss": 0.3698,
      "step": 1582
    },
    {
      "epoch": 0.46558823529411764,
      "grad_norm": 0.05382229760289192,
      "learning_rate": 0.00010703976435935199,
      "loss": 0.3566,
      "step": 1583
    },
    {
      "epoch": 0.46588235294117647,
      "grad_norm": 0.06560253351926804,
      "learning_rate": 0.00010698085419734904,
      "loss": 0.3787,
      "step": 1584
    },
    {
      "epoch": 0.4661764705882353,
      "grad_norm": 0.053363699465990067,
      "learning_rate": 0.0001069219440353461,
      "loss": 0.3464,
      "step": 1585
    },
    {
      "epoch": 0.46647058823529414,
      "grad_norm": 0.053018566220998764,
      "learning_rate": 0.00010686303387334316,
      "loss": 0.4156,
      "step": 1586
    },
    {
      "epoch": 0.4667647058823529,
      "grad_norm": 0.07254452258348465,
      "learning_rate": 0.0001068041237113402,
      "loss": 0.4519,
      "step": 1587
    },
    {
      "epoch": 0.46705882352941175,
      "grad_norm": 0.044374000281095505,
      "learning_rate": 0.00010674521354933727,
      "loss": 0.3188,
      "step": 1588
    },
    {
      "epoch": 0.4673529411764706,
      "grad_norm": 0.04369311407208443,
      "learning_rate": 0.00010668630338733431,
      "loss": 0.2725,
      "step": 1589
    },
    {
      "epoch": 0.4676470588235294,
      "grad_norm": 0.05070335045456886,
      "learning_rate": 0.00010662739322533137,
      "loss": 0.3779,
      "step": 1590
    },
    {
      "epoch": 0.46794117647058825,
      "grad_norm": 0.041317302733659744,
      "learning_rate": 0.00010656848306332843,
      "loss": 0.2719,
      "step": 1591
    },
    {
      "epoch": 0.4682352941176471,
      "grad_norm": 0.04460809752345085,
      "learning_rate": 0.00010650957290132548,
      "loss": 0.3339,
      "step": 1592
    },
    {
      "epoch": 0.46852941176470586,
      "grad_norm": 0.04398029297590256,
      "learning_rate": 0.00010645066273932254,
      "loss": 0.3457,
      "step": 1593
    },
    {
      "epoch": 0.4688235294117647,
      "grad_norm": 0.04637724533677101,
      "learning_rate": 0.00010639175257731959,
      "loss": 0.3442,
      "step": 1594
    },
    {
      "epoch": 0.46911764705882353,
      "grad_norm": 0.05469703674316406,
      "learning_rate": 0.00010633284241531665,
      "loss": 0.352,
      "step": 1595
    },
    {
      "epoch": 0.46941176470588236,
      "grad_norm": 0.06203627213835716,
      "learning_rate": 0.00010627393225331371,
      "loss": 0.3773,
      "step": 1596
    },
    {
      "epoch": 0.4697058823529412,
      "grad_norm": 0.037151917815208435,
      "learning_rate": 0.00010621502209131075,
      "loss": 0.3166,
      "step": 1597
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.05706686154007912,
      "learning_rate": 0.00010615611192930781,
      "loss": 0.2987,
      "step": 1598
    },
    {
      "epoch": 0.4702941176470588,
      "grad_norm": 0.057312872260808945,
      "learning_rate": 0.00010609720176730486,
      "loss": 0.367,
      "step": 1599
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.04095660522580147,
      "learning_rate": 0.00010603829160530192,
      "loss": 0.3329,
      "step": 1600
    },
    {
      "epoch": 0.47058823529411764,
      "eval_loss": 0.3555690050125122,
      "eval_runtime": 214.9628,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1600
    },
    {
      "epoch": 0.4708823529411765,
      "grad_norm": 0.048445556312799454,
      "learning_rate": 0.00010597938144329898,
      "loss": 0.2633,
      "step": 1601
    },
    {
      "epoch": 0.4711764705882353,
      "grad_norm": 0.046356234699487686,
      "learning_rate": 0.00010592047128129603,
      "loss": 0.3413,
      "step": 1602
    },
    {
      "epoch": 0.47147058823529414,
      "grad_norm": 0.0574142150580883,
      "learning_rate": 0.00010586156111929309,
      "loss": 0.4089,
      "step": 1603
    },
    {
      "epoch": 0.4717647058823529,
      "grad_norm": 0.046472761780023575,
      "learning_rate": 0.00010580265095729013,
      "loss": 0.362,
      "step": 1604
    },
    {
      "epoch": 0.47205882352941175,
      "grad_norm": 0.047714561223983765,
      "learning_rate": 0.0001057437407952872,
      "loss": 0.3926,
      "step": 1605
    },
    {
      "epoch": 0.4723529411764706,
      "grad_norm": 0.045674532651901245,
      "learning_rate": 0.00010568483063328425,
      "loss": 0.3254,
      "step": 1606
    },
    {
      "epoch": 0.4726470588235294,
      "grad_norm": 0.0554666705429554,
      "learning_rate": 0.0001056259204712813,
      "loss": 0.3995,
      "step": 1607
    },
    {
      "epoch": 0.47294117647058825,
      "grad_norm": 0.039893414825201035,
      "learning_rate": 0.00010556701030927836,
      "loss": 0.3469,
      "step": 1608
    },
    {
      "epoch": 0.47323529411764703,
      "grad_norm": 0.04168045520782471,
      "learning_rate": 0.00010550810014727541,
      "loss": 0.3215,
      "step": 1609
    },
    {
      "epoch": 0.47352941176470587,
      "grad_norm": 0.04886326193809509,
      "learning_rate": 0.00010544918998527247,
      "loss": 0.3069,
      "step": 1610
    },
    {
      "epoch": 0.4738235294117647,
      "grad_norm": 0.054157234728336334,
      "learning_rate": 0.00010539027982326953,
      "loss": 0.4099,
      "step": 1611
    },
    {
      "epoch": 0.47411764705882353,
      "grad_norm": 0.05427350848913193,
      "learning_rate": 0.00010533136966126657,
      "loss": 0.3485,
      "step": 1612
    },
    {
      "epoch": 0.47441176470588237,
      "grad_norm": 0.054546091705560684,
      "learning_rate": 0.00010527245949926363,
      "loss": 0.3828,
      "step": 1613
    },
    {
      "epoch": 0.4747058823529412,
      "grad_norm": 0.04334356635808945,
      "learning_rate": 0.00010521354933726068,
      "loss": 0.3125,
      "step": 1614
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.04830650985240936,
      "learning_rate": 0.00010515463917525774,
      "loss": 0.344,
      "step": 1615
    },
    {
      "epoch": 0.4752941176470588,
      "grad_norm": 0.05471000820398331,
      "learning_rate": 0.0001050957290132548,
      "loss": 0.3402,
      "step": 1616
    },
    {
      "epoch": 0.47558823529411764,
      "grad_norm": 0.05408671870827675,
      "learning_rate": 0.00010503681885125185,
      "loss": 0.3718,
      "step": 1617
    },
    {
      "epoch": 0.4758823529411765,
      "grad_norm": 0.04185596480965614,
      "learning_rate": 0.00010497790868924891,
      "loss": 0.3274,
      "step": 1618
    },
    {
      "epoch": 0.4761764705882353,
      "grad_norm": 0.042849767953157425,
      "learning_rate": 0.00010491899852724596,
      "loss": 0.3027,
      "step": 1619
    },
    {
      "epoch": 0.4764705882352941,
      "grad_norm": 0.04217338562011719,
      "learning_rate": 0.00010486008836524302,
      "loss": 0.3426,
      "step": 1620
    },
    {
      "epoch": 0.4767647058823529,
      "grad_norm": 0.06060248240828514,
      "learning_rate": 0.00010480117820324008,
      "loss": 0.3877,
      "step": 1621
    },
    {
      "epoch": 0.47705882352941176,
      "grad_norm": 0.05066188797354698,
      "learning_rate": 0.00010474226804123712,
      "loss": 0.3396,
      "step": 1622
    },
    {
      "epoch": 0.4773529411764706,
      "grad_norm": 0.03744193911552429,
      "learning_rate": 0.00010468335787923418,
      "loss": 0.3298,
      "step": 1623
    },
    {
      "epoch": 0.4776470588235294,
      "grad_norm": 0.05589960888028145,
      "learning_rate": 0.00010462444771723123,
      "loss": 0.3651,
      "step": 1624
    },
    {
      "epoch": 0.47794117647058826,
      "grad_norm": 0.043705105781555176,
      "learning_rate": 0.00010456553755522829,
      "loss": 0.3393,
      "step": 1625
    },
    {
      "epoch": 0.47823529411764704,
      "grad_norm": 0.05330341309309006,
      "learning_rate": 0.00010450662739322534,
      "loss": 0.3781,
      "step": 1626
    },
    {
      "epoch": 0.47852941176470587,
      "grad_norm": 0.04740685597062111,
      "learning_rate": 0.0001044477172312224,
      "loss": 0.3775,
      "step": 1627
    },
    {
      "epoch": 0.4788235294117647,
      "grad_norm": 0.05736404284834862,
      "learning_rate": 0.00010438880706921946,
      "loss": 0.3414,
      "step": 1628
    },
    {
      "epoch": 0.47911764705882354,
      "grad_norm": 0.05087268352508545,
      "learning_rate": 0.0001043298969072165,
      "loss": 0.3074,
      "step": 1629
    },
    {
      "epoch": 0.47941176470588237,
      "grad_norm": 0.03436039388179779,
      "learning_rate": 0.00010427098674521356,
      "loss": 0.2601,
      "step": 1630
    },
    {
      "epoch": 0.4797058823529412,
      "grad_norm": 0.042932309210300446,
      "learning_rate": 0.00010421207658321061,
      "loss": 0.2927,
      "step": 1631
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.04075950011610985,
      "learning_rate": 0.00010415316642120767,
      "loss": 0.3388,
      "step": 1632
    },
    {
      "epoch": 0.4802941176470588,
      "grad_norm": 0.056044597178697586,
      "learning_rate": 0.00010409425625920473,
      "loss": 0.368,
      "step": 1633
    },
    {
      "epoch": 0.48058823529411765,
      "grad_norm": 0.051501404494047165,
      "learning_rate": 0.00010403534609720178,
      "loss": 0.4019,
      "step": 1634
    },
    {
      "epoch": 0.4808823529411765,
      "grad_norm": 0.03871605917811394,
      "learning_rate": 0.00010397643593519884,
      "loss": 0.3052,
      "step": 1635
    },
    {
      "epoch": 0.4811764705882353,
      "grad_norm": 0.04688509553670883,
      "learning_rate": 0.00010391752577319588,
      "loss": 0.3483,
      "step": 1636
    },
    {
      "epoch": 0.4814705882352941,
      "grad_norm": 0.03869866207242012,
      "learning_rate": 0.00010385861561119294,
      "loss": 0.2934,
      "step": 1637
    },
    {
      "epoch": 0.48176470588235293,
      "grad_norm": 0.044096074998378754,
      "learning_rate": 0.00010379970544919,
      "loss": 0.2894,
      "step": 1638
    },
    {
      "epoch": 0.48205882352941176,
      "grad_norm": 0.04826722666621208,
      "learning_rate": 0.00010374079528718705,
      "loss": 0.3332,
      "step": 1639
    },
    {
      "epoch": 0.4823529411764706,
      "grad_norm": 0.055096231400966644,
      "learning_rate": 0.00010368188512518411,
      "loss": 0.394,
      "step": 1640
    },
    {
      "epoch": 0.48264705882352943,
      "grad_norm": 0.051034409552812576,
      "learning_rate": 0.00010362297496318116,
      "loss": 0.3434,
      "step": 1641
    },
    {
      "epoch": 0.48294117647058826,
      "grad_norm": 0.03885288164019585,
      "learning_rate": 0.00010356406480117822,
      "loss": 0.3472,
      "step": 1642
    },
    {
      "epoch": 0.48323529411764704,
      "grad_norm": 0.05485723912715912,
      "learning_rate": 0.00010350515463917528,
      "loss": 0.3523,
      "step": 1643
    },
    {
      "epoch": 0.4835294117647059,
      "grad_norm": 0.04105374589562416,
      "learning_rate": 0.00010344624447717232,
      "loss": 0.2819,
      "step": 1644
    },
    {
      "epoch": 0.4838235294117647,
      "grad_norm": 0.05515086278319359,
      "learning_rate": 0.00010338733431516938,
      "loss": 0.4233,
      "step": 1645
    },
    {
      "epoch": 0.48411764705882354,
      "grad_norm": 0.04849065840244293,
      "learning_rate": 0.00010332842415316643,
      "loss": 0.3019,
      "step": 1646
    },
    {
      "epoch": 0.4844117647058824,
      "grad_norm": 0.043450016528367996,
      "learning_rate": 0.00010326951399116349,
      "loss": 0.3172,
      "step": 1647
    },
    {
      "epoch": 0.48470588235294115,
      "grad_norm": 0.05025358125567436,
      "learning_rate": 0.00010321060382916055,
      "loss": 0.392,
      "step": 1648
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.050870563834905624,
      "learning_rate": 0.0001031516936671576,
      "loss": 0.3244,
      "step": 1649
    },
    {
      "epoch": 0.4852941176470588,
      "grad_norm": 0.054472196847200394,
      "learning_rate": 0.00010309278350515463,
      "loss": 0.3317,
      "step": 1650
    },
    {
      "epoch": 0.48558823529411765,
      "grad_norm": 0.0507936105132103,
      "learning_rate": 0.00010303387334315169,
      "loss": 0.3794,
      "step": 1651
    },
    {
      "epoch": 0.4858823529411765,
      "grad_norm": 0.04330074414610863,
      "learning_rate": 0.00010297496318114874,
      "loss": 0.3448,
      "step": 1652
    },
    {
      "epoch": 0.4861764705882353,
      "grad_norm": 0.04971326142549515,
      "learning_rate": 0.0001029160530191458,
      "loss": 0.3597,
      "step": 1653
    },
    {
      "epoch": 0.4864705882352941,
      "grad_norm": 0.0464293509721756,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.3077,
      "step": 1654
    },
    {
      "epoch": 0.48676470588235293,
      "grad_norm": 0.05592853203415871,
      "learning_rate": 0.0001027982326951399,
      "loss": 0.428,
      "step": 1655
    },
    {
      "epoch": 0.48705882352941177,
      "grad_norm": 0.035026147961616516,
      "learning_rate": 0.00010273932253313696,
      "loss": 0.312,
      "step": 1656
    },
    {
      "epoch": 0.4873529411764706,
      "grad_norm": 0.0548536591231823,
      "learning_rate": 0.00010268041237113401,
      "loss": 0.3848,
      "step": 1657
    },
    {
      "epoch": 0.48764705882352943,
      "grad_norm": 0.04426930099725723,
      "learning_rate": 0.00010262150220913107,
      "loss": 0.36,
      "step": 1658
    },
    {
      "epoch": 0.4879411764705882,
      "grad_norm": 0.04997008666396141,
      "learning_rate": 0.00010256259204712813,
      "loss": 0.3496,
      "step": 1659
    },
    {
      "epoch": 0.48823529411764705,
      "grad_norm": 0.045691315084695816,
      "learning_rate": 0.00010250368188512518,
      "loss": 0.372,
      "step": 1660
    },
    {
      "epoch": 0.4885294117647059,
      "grad_norm": 0.054668281227350235,
      "learning_rate": 0.00010244477172312224,
      "loss": 0.4319,
      "step": 1661
    },
    {
      "epoch": 0.4888235294117647,
      "grad_norm": 0.04888008162379265,
      "learning_rate": 0.00010238586156111928,
      "loss": 0.2933,
      "step": 1662
    },
    {
      "epoch": 0.48911764705882355,
      "grad_norm": 0.049913033843040466,
      "learning_rate": 0.00010232695139911635,
      "loss": 0.357,
      "step": 1663
    },
    {
      "epoch": 0.4894117647058824,
      "grad_norm": 0.05260984227061272,
      "learning_rate": 0.0001022680412371134,
      "loss": 0.3469,
      "step": 1664
    },
    {
      "epoch": 0.48970588235294116,
      "grad_norm": 0.060492802411317825,
      "learning_rate": 0.00010220913107511045,
      "loss": 0.3798,
      "step": 1665
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.05047009885311127,
      "learning_rate": 0.00010215022091310751,
      "loss": 0.3381,
      "step": 1666
    },
    {
      "epoch": 0.4902941176470588,
      "grad_norm": 0.04366239532828331,
      "learning_rate": 0.00010209131075110456,
      "loss": 0.3594,
      "step": 1667
    },
    {
      "epoch": 0.49058823529411766,
      "grad_norm": 0.05151171237230301,
      "learning_rate": 0.00010203240058910162,
      "loss": 0.3736,
      "step": 1668
    },
    {
      "epoch": 0.4908823529411765,
      "grad_norm": 0.041654590517282486,
      "learning_rate": 0.00010197349042709868,
      "loss": 0.2933,
      "step": 1669
    },
    {
      "epoch": 0.49117647058823527,
      "grad_norm": 0.049820102751255035,
      "learning_rate": 0.00010191458026509573,
      "loss": 0.4069,
      "step": 1670
    },
    {
      "epoch": 0.4914705882352941,
      "grad_norm": 0.0446120984852314,
      "learning_rate": 0.00010185567010309279,
      "loss": 0.3206,
      "step": 1671
    },
    {
      "epoch": 0.49176470588235294,
      "grad_norm": 0.05005880072712898,
      "learning_rate": 0.00010179675994108983,
      "loss": 0.3266,
      "step": 1672
    },
    {
      "epoch": 0.49205882352941177,
      "grad_norm": 0.05879750847816467,
      "learning_rate": 0.00010173784977908689,
      "loss": 0.3828,
      "step": 1673
    },
    {
      "epoch": 0.4923529411764706,
      "grad_norm": 0.04136925935745239,
      "learning_rate": 0.00010167893961708394,
      "loss": 0.327,
      "step": 1674
    },
    {
      "epoch": 0.49264705882352944,
      "grad_norm": 0.05228739604353905,
      "learning_rate": 0.000101620029455081,
      "loss": 0.3542,
      "step": 1675
    },
    {
      "epoch": 0.4929411764705882,
      "grad_norm": 0.054262127727270126,
      "learning_rate": 0.00010156111929307806,
      "loss": 0.386,
      "step": 1676
    },
    {
      "epoch": 0.49323529411764705,
      "grad_norm": 0.052707113325595856,
      "learning_rate": 0.0001015022091310751,
      "loss": 0.3498,
      "step": 1677
    },
    {
      "epoch": 0.4935294117647059,
      "grad_norm": 0.03860966116189957,
      "learning_rate": 0.00010144329896907217,
      "loss": 0.3167,
      "step": 1678
    },
    {
      "epoch": 0.4938235294117647,
      "grad_norm": 0.052963536232709885,
      "learning_rate": 0.00010138438880706921,
      "loss": 0.37,
      "step": 1679
    },
    {
      "epoch": 0.49411764705882355,
      "grad_norm": 0.05554391071200371,
      "learning_rate": 0.00010132547864506627,
      "loss": 0.341,
      "step": 1680
    },
    {
      "epoch": 0.49441176470588233,
      "grad_norm": 0.04981335997581482,
      "learning_rate": 0.00010126656848306333,
      "loss": 0.3397,
      "step": 1681
    },
    {
      "epoch": 0.49470588235294116,
      "grad_norm": 0.053373727947473526,
      "learning_rate": 0.00010120765832106038,
      "loss": 0.4034,
      "step": 1682
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.060170598328113556,
      "learning_rate": 0.00010114874815905744,
      "loss": 0.4147,
      "step": 1683
    },
    {
      "epoch": 0.49529411764705883,
      "grad_norm": 0.03401954844594002,
      "learning_rate": 0.00010108983799705449,
      "loss": 0.2745,
      "step": 1684
    },
    {
      "epoch": 0.49558823529411766,
      "grad_norm": 0.05787290260195732,
      "learning_rate": 0.00010103092783505155,
      "loss": 0.3488,
      "step": 1685
    },
    {
      "epoch": 0.4958823529411765,
      "grad_norm": 0.04667576029896736,
      "learning_rate": 0.00010097201767304861,
      "loss": 0.3782,
      "step": 1686
    },
    {
      "epoch": 0.4961764705882353,
      "grad_norm": 0.047468796372413635,
      "learning_rate": 0.00010091310751104565,
      "loss": 0.3528,
      "step": 1687
    },
    {
      "epoch": 0.4964705882352941,
      "grad_norm": 0.05794264003634453,
      "learning_rate": 0.00010085419734904271,
      "loss": 0.3653,
      "step": 1688
    },
    {
      "epoch": 0.49676470588235294,
      "grad_norm": 0.053292494267225266,
      "learning_rate": 0.00010079528718703976,
      "loss": 0.3866,
      "step": 1689
    },
    {
      "epoch": 0.4970588235294118,
      "grad_norm": 0.057759370654821396,
      "learning_rate": 0.00010073637702503682,
      "loss": 0.4035,
      "step": 1690
    },
    {
      "epoch": 0.4973529411764706,
      "grad_norm": 0.0388956144452095,
      "learning_rate": 0.00010067746686303388,
      "loss": 0.32,
      "step": 1691
    },
    {
      "epoch": 0.4976470588235294,
      "grad_norm": 0.052862562239170074,
      "learning_rate": 0.00010061855670103093,
      "loss": 0.3723,
      "step": 1692
    },
    {
      "epoch": 0.4979411764705882,
      "grad_norm": 0.058789849281311035,
      "learning_rate": 0.00010055964653902799,
      "loss": 0.3437,
      "step": 1693
    },
    {
      "epoch": 0.49823529411764705,
      "grad_norm": 0.062365349382162094,
      "learning_rate": 0.00010050073637702503,
      "loss": 0.3957,
      "step": 1694
    },
    {
      "epoch": 0.4985294117647059,
      "grad_norm": 0.0442822240293026,
      "learning_rate": 0.0001004418262150221,
      "loss": 0.3091,
      "step": 1695
    },
    {
      "epoch": 0.4988235294117647,
      "grad_norm": 0.05481655150651932,
      "learning_rate": 0.00010038291605301915,
      "loss": 0.3461,
      "step": 1696
    },
    {
      "epoch": 0.49911764705882355,
      "grad_norm": 0.0440036877989769,
      "learning_rate": 0.0001003240058910162,
      "loss": 0.326,
      "step": 1697
    },
    {
      "epoch": 0.49941176470588233,
      "grad_norm": 0.06950852274894714,
      "learning_rate": 0.00010026509572901326,
      "loss": 0.3919,
      "step": 1698
    },
    {
      "epoch": 0.49970588235294117,
      "grad_norm": 0.05627838894724846,
      "learning_rate": 0.00010020618556701031,
      "loss": 0.3738,
      "step": 1699
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.045114997774362564,
      "learning_rate": 0.00010014727540500737,
      "loss": 0.3285,
      "step": 1700
    },
    {
      "epoch": 0.5,
      "eval_loss": 0.35528963804244995,
      "eval_runtime": 215.0753,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1700
    },
    {
      "epoch": 0.5002941176470588,
      "grad_norm": 0.04323615878820419,
      "learning_rate": 0.00010008836524300443,
      "loss": 0.3188,
      "step": 1701
    },
    {
      "epoch": 0.5005882352941177,
      "grad_norm": 0.05653177201747894,
      "learning_rate": 0.00010002945508100147,
      "loss": 0.3413,
      "step": 1702
    },
    {
      "epoch": 0.5008823529411764,
      "grad_norm": 0.04271835833787918,
      "learning_rate": 9.997054491899853e-05,
      "loss": 0.2875,
      "step": 1703
    },
    {
      "epoch": 0.5011764705882353,
      "grad_norm": 0.04797825217247009,
      "learning_rate": 9.991163475699558e-05,
      "loss": 0.3269,
      "step": 1704
    },
    {
      "epoch": 0.5014705882352941,
      "grad_norm": 0.04457899183034897,
      "learning_rate": 9.985272459499264e-05,
      "loss": 0.3511,
      "step": 1705
    },
    {
      "epoch": 0.5017647058823529,
      "grad_norm": 0.040974829345941544,
      "learning_rate": 9.97938144329897e-05,
      "loss": 0.3234,
      "step": 1706
    },
    {
      "epoch": 0.5020588235294118,
      "grad_norm": 0.06038548797369003,
      "learning_rate": 9.973490427098675e-05,
      "loss": 0.3641,
      "step": 1707
    },
    {
      "epoch": 0.5023529411764706,
      "grad_norm": 0.04733579605817795,
      "learning_rate": 9.967599410898381e-05,
      "loss": 0.2851,
      "step": 1708
    },
    {
      "epoch": 0.5026470588235294,
      "grad_norm": 0.040367092937231064,
      "learning_rate": 9.961708394698086e-05,
      "loss": 0.2728,
      "step": 1709
    },
    {
      "epoch": 0.5029411764705882,
      "grad_norm": 0.05374953895807266,
      "learning_rate": 9.955817378497792e-05,
      "loss": 0.3439,
      "step": 1710
    },
    {
      "epoch": 0.5032352941176471,
      "grad_norm": 0.05966408923268318,
      "learning_rate": 9.949926362297498e-05,
      "loss": 0.4065,
      "step": 1711
    },
    {
      "epoch": 0.5035294117647059,
      "grad_norm": 0.0435771606862545,
      "learning_rate": 9.944035346097202e-05,
      "loss": 0.3396,
      "step": 1712
    },
    {
      "epoch": 0.5038235294117647,
      "grad_norm": 0.05170166492462158,
      "learning_rate": 9.938144329896908e-05,
      "loss": 0.3518,
      "step": 1713
    },
    {
      "epoch": 0.5041176470588236,
      "grad_norm": 0.03891848772764206,
      "learning_rate": 9.932253313696613e-05,
      "loss": 0.2898,
      "step": 1714
    },
    {
      "epoch": 0.5044117647058823,
      "grad_norm": 0.05158078297972679,
      "learning_rate": 9.926362297496319e-05,
      "loss": 0.3223,
      "step": 1715
    },
    {
      "epoch": 0.5047058823529412,
      "grad_norm": 0.04308953881263733,
      "learning_rate": 9.920471281296025e-05,
      "loss": 0.3485,
      "step": 1716
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.04909822344779968,
      "learning_rate": 9.91458026509573e-05,
      "loss": 0.3594,
      "step": 1717
    },
    {
      "epoch": 0.5052941176470588,
      "grad_norm": 0.05065998062491417,
      "learning_rate": 9.908689248895436e-05,
      "loss": 0.3415,
      "step": 1718
    },
    {
      "epoch": 0.5055882352941177,
      "grad_norm": 0.05327775701880455,
      "learning_rate": 9.90279823269514e-05,
      "loss": 0.3774,
      "step": 1719
    },
    {
      "epoch": 0.5058823529411764,
      "grad_norm": 0.0507163405418396,
      "learning_rate": 9.896907216494846e-05,
      "loss": 0.4178,
      "step": 1720
    },
    {
      "epoch": 0.5061764705882353,
      "grad_norm": 0.04634875804185867,
      "learning_rate": 9.891016200294552e-05,
      "loss": 0.3613,
      "step": 1721
    },
    {
      "epoch": 0.5064705882352941,
      "grad_norm": 0.0418376661837101,
      "learning_rate": 9.885125184094257e-05,
      "loss": 0.3381,
      "step": 1722
    },
    {
      "epoch": 0.5067647058823529,
      "grad_norm": 0.03720816969871521,
      "learning_rate": 9.879234167893963e-05,
      "loss": 0.3247,
      "step": 1723
    },
    {
      "epoch": 0.5070588235294118,
      "grad_norm": 0.05689702555537224,
      "learning_rate": 9.873343151693668e-05,
      "loss": 0.4139,
      "step": 1724
    },
    {
      "epoch": 0.5073529411764706,
      "grad_norm": 0.050654202699661255,
      "learning_rate": 9.867452135493374e-05,
      "loss": 0.3802,
      "step": 1725
    },
    {
      "epoch": 0.5076470588235295,
      "grad_norm": 0.034517500549554825,
      "learning_rate": 9.86156111929308e-05,
      "loss": 0.2757,
      "step": 1726
    },
    {
      "epoch": 0.5079411764705882,
      "grad_norm": 0.03805077448487282,
      "learning_rate": 9.855670103092784e-05,
      "loss": 0.2589,
      "step": 1727
    },
    {
      "epoch": 0.508235294117647,
      "grad_norm": 0.04507686570286751,
      "learning_rate": 9.84977908689249e-05,
      "loss": 0.3742,
      "step": 1728
    },
    {
      "epoch": 0.5085294117647059,
      "grad_norm": 0.05193423107266426,
      "learning_rate": 9.843888070692195e-05,
      "loss": 0.4184,
      "step": 1729
    },
    {
      "epoch": 0.5088235294117647,
      "grad_norm": 0.0409892313182354,
      "learning_rate": 9.8379970544919e-05,
      "loss": 0.3492,
      "step": 1730
    },
    {
      "epoch": 0.5091176470588236,
      "grad_norm": 0.04336053505539894,
      "learning_rate": 9.832106038291606e-05,
      "loss": 0.3597,
      "step": 1731
    },
    {
      "epoch": 0.5094117647058823,
      "grad_norm": 0.04290398582816124,
      "learning_rate": 9.82621502209131e-05,
      "loss": 0.3144,
      "step": 1732
    },
    {
      "epoch": 0.5097058823529412,
      "grad_norm": 0.038508687168359756,
      "learning_rate": 9.820324005891016e-05,
      "loss": 0.3767,
      "step": 1733
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.045742955058813095,
      "learning_rate": 9.814432989690721e-05,
      "loss": 0.3202,
      "step": 1734
    },
    {
      "epoch": 0.5102941176470588,
      "grad_norm": 0.03248167037963867,
      "learning_rate": 9.808541973490427e-05,
      "loss": 0.279,
      "step": 1735
    },
    {
      "epoch": 0.5105882352941177,
      "grad_norm": 0.051418982446193695,
      "learning_rate": 9.802650957290133e-05,
      "loss": 0.3537,
      "step": 1736
    },
    {
      "epoch": 0.5108823529411765,
      "grad_norm": 0.046674855053424835,
      "learning_rate": 9.796759941089838e-05,
      "loss": 0.3439,
      "step": 1737
    },
    {
      "epoch": 0.5111764705882353,
      "grad_norm": 0.05389679968357086,
      "learning_rate": 9.790868924889544e-05,
      "loss": 0.3652,
      "step": 1738
    },
    {
      "epoch": 0.5114705882352941,
      "grad_norm": 0.037138596177101135,
      "learning_rate": 9.784977908689248e-05,
      "loss": 0.319,
      "step": 1739
    },
    {
      "epoch": 0.5117647058823529,
      "grad_norm": 0.041218943893909454,
      "learning_rate": 9.779086892488954e-05,
      "loss": 0.3122,
      "step": 1740
    },
    {
      "epoch": 0.5120588235294118,
      "grad_norm": 0.04889868199825287,
      "learning_rate": 9.77319587628866e-05,
      "loss": 0.3353,
      "step": 1741
    },
    {
      "epoch": 0.5123529411764706,
      "grad_norm": 0.053639840334653854,
      "learning_rate": 9.767304860088365e-05,
      "loss": 0.3298,
      "step": 1742
    },
    {
      "epoch": 0.5126470588235295,
      "grad_norm": 0.06352218985557556,
      "learning_rate": 9.761413843888071e-05,
      "loss": 0.4135,
      "step": 1743
    },
    {
      "epoch": 0.5129411764705882,
      "grad_norm": 0.05876927077770233,
      "learning_rate": 9.755522827687776e-05,
      "loss": 0.4238,
      "step": 1744
    },
    {
      "epoch": 0.513235294117647,
      "grad_norm": 0.05370986461639404,
      "learning_rate": 9.749631811487482e-05,
      "loss": 0.3622,
      "step": 1745
    },
    {
      "epoch": 0.5135294117647059,
      "grad_norm": 0.04798521101474762,
      "learning_rate": 9.743740795287188e-05,
      "loss": 0.3176,
      "step": 1746
    },
    {
      "epoch": 0.5138235294117647,
      "grad_norm": 0.05262324586510658,
      "learning_rate": 9.737849779086892e-05,
      "loss": 0.3486,
      "step": 1747
    },
    {
      "epoch": 0.5141176470588236,
      "grad_norm": 0.04498549923300743,
      "learning_rate": 9.731958762886598e-05,
      "loss": 0.3448,
      "step": 1748
    },
    {
      "epoch": 0.5144117647058823,
      "grad_norm": 0.03680448234081268,
      "learning_rate": 9.726067746686303e-05,
      "loss": 0.3139,
      "step": 1749
    },
    {
      "epoch": 0.5147058823529411,
      "grad_norm": 0.04901891574263573,
      "learning_rate": 9.720176730486009e-05,
      "loss": 0.3522,
      "step": 1750
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.042397573590278625,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.3332,
      "step": 1751
    },
    {
      "epoch": 0.5152941176470588,
      "grad_norm": 0.04661472514271736,
      "learning_rate": 9.70839469808542e-05,
      "loss": 0.3121,
      "step": 1752
    },
    {
      "epoch": 0.5155882352941177,
      "grad_norm": 0.0575062520802021,
      "learning_rate": 9.702503681885126e-05,
      "loss": 0.3187,
      "step": 1753
    },
    {
      "epoch": 0.5158823529411765,
      "grad_norm": 0.0459429956972599,
      "learning_rate": 9.69661266568483e-05,
      "loss": 0.3963,
      "step": 1754
    },
    {
      "epoch": 0.5161764705882353,
      "grad_norm": 0.0461001992225647,
      "learning_rate": 9.690721649484537e-05,
      "loss": 0.3113,
      "step": 1755
    },
    {
      "epoch": 0.5164705882352941,
      "grad_norm": 0.05365348234772682,
      "learning_rate": 9.684830633284243e-05,
      "loss": 0.3908,
      "step": 1756
    },
    {
      "epoch": 0.5167647058823529,
      "grad_norm": 0.04436089098453522,
      "learning_rate": 9.678939617083947e-05,
      "loss": 0.32,
      "step": 1757
    },
    {
      "epoch": 0.5170588235294118,
      "grad_norm": 0.053900472819805145,
      "learning_rate": 9.673048600883653e-05,
      "loss": 0.4005,
      "step": 1758
    },
    {
      "epoch": 0.5173529411764706,
      "grad_norm": 0.053410548716783524,
      "learning_rate": 9.667157584683358e-05,
      "loss": 0.3517,
      "step": 1759
    },
    {
      "epoch": 0.5176470588235295,
      "grad_norm": 0.06466490030288696,
      "learning_rate": 9.661266568483064e-05,
      "loss": 0.4464,
      "step": 1760
    },
    {
      "epoch": 0.5179411764705882,
      "grad_norm": 0.05513342469930649,
      "learning_rate": 9.65537555228277e-05,
      "loss": 0.2956,
      "step": 1761
    },
    {
      "epoch": 0.518235294117647,
      "grad_norm": 0.044702596962451935,
      "learning_rate": 9.649484536082475e-05,
      "loss": 0.3273,
      "step": 1762
    },
    {
      "epoch": 0.5185294117647059,
      "grad_norm": 0.05088043212890625,
      "learning_rate": 9.64359351988218e-05,
      "loss": 0.3413,
      "step": 1763
    },
    {
      "epoch": 0.5188235294117647,
      "grad_norm": 0.052786532789468765,
      "learning_rate": 9.637702503681885e-05,
      "loss": 0.3442,
      "step": 1764
    },
    {
      "epoch": 0.5191176470588236,
      "grad_norm": 0.06992347538471222,
      "learning_rate": 9.631811487481591e-05,
      "loss": 0.4266,
      "step": 1765
    },
    {
      "epoch": 0.5194117647058824,
      "grad_norm": 0.04022379592061043,
      "learning_rate": 9.625920471281297e-05,
      "loss": 0.2999,
      "step": 1766
    },
    {
      "epoch": 0.5197058823529411,
      "grad_norm": 0.04938333109021187,
      "learning_rate": 9.620029455081002e-05,
      "loss": 0.3427,
      "step": 1767
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.0501822754740715,
      "learning_rate": 9.614138438880708e-05,
      "loss": 0.3095,
      "step": 1768
    },
    {
      "epoch": 0.5202941176470588,
      "grad_norm": 0.048496976494789124,
      "learning_rate": 9.608247422680413e-05,
      "loss": 0.3348,
      "step": 1769
    },
    {
      "epoch": 0.5205882352941177,
      "grad_norm": 0.05005212500691414,
      "learning_rate": 9.602356406480119e-05,
      "loss": 0.3156,
      "step": 1770
    },
    {
      "epoch": 0.5208823529411765,
      "grad_norm": 0.05137670040130615,
      "learning_rate": 9.596465390279825e-05,
      "loss": 0.3021,
      "step": 1771
    },
    {
      "epoch": 0.5211764705882352,
      "grad_norm": 0.04659993201494217,
      "learning_rate": 9.59057437407953e-05,
      "loss": 0.3185,
      "step": 1772
    },
    {
      "epoch": 0.5214705882352941,
      "grad_norm": 0.05081995949149132,
      "learning_rate": 9.584683357879235e-05,
      "loss": 0.4097,
      "step": 1773
    },
    {
      "epoch": 0.5217647058823529,
      "grad_norm": 0.06404828280210495,
      "learning_rate": 9.57879234167894e-05,
      "loss": 0.3997,
      "step": 1774
    },
    {
      "epoch": 0.5220588235294118,
      "grad_norm": 0.04402944818139076,
      "learning_rate": 9.572901325478646e-05,
      "loss": 0.3402,
      "step": 1775
    },
    {
      "epoch": 0.5223529411764706,
      "grad_norm": 0.04632234200835228,
      "learning_rate": 9.567010309278352e-05,
      "loss": 0.3479,
      "step": 1776
    },
    {
      "epoch": 0.5226470588235295,
      "grad_norm": 0.04806354641914368,
      "learning_rate": 9.561119293078057e-05,
      "loss": 0.2885,
      "step": 1777
    },
    {
      "epoch": 0.5229411764705882,
      "grad_norm": 0.04296409338712692,
      "learning_rate": 9.555228276877763e-05,
      "loss": 0.2892,
      "step": 1778
    },
    {
      "epoch": 0.523235294117647,
      "grad_norm": 0.052339792251586914,
      "learning_rate": 9.549337260677467e-05,
      "loss": 0.3787,
      "step": 1779
    },
    {
      "epoch": 0.5235294117647059,
      "grad_norm": 0.036271799355745316,
      "learning_rate": 9.543446244477173e-05,
      "loss": 0.2918,
      "step": 1780
    },
    {
      "epoch": 0.5238235294117647,
      "grad_norm": 0.048107631504535675,
      "learning_rate": 9.53755522827688e-05,
      "loss": 0.3957,
      "step": 1781
    },
    {
      "epoch": 0.5241176470588236,
      "grad_norm": 0.04877346381545067,
      "learning_rate": 9.531664212076584e-05,
      "loss": 0.337,
      "step": 1782
    },
    {
      "epoch": 0.5244117647058824,
      "grad_norm": 0.049068327993154526,
      "learning_rate": 9.525773195876289e-05,
      "loss": 0.2973,
      "step": 1783
    },
    {
      "epoch": 0.5247058823529411,
      "grad_norm": 0.0537990964949131,
      "learning_rate": 9.519882179675993e-05,
      "loss": 0.3945,
      "step": 1784
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.0447751022875309,
      "learning_rate": 9.5139911634757e-05,
      "loss": 0.4061,
      "step": 1785
    },
    {
      "epoch": 0.5252941176470588,
      "grad_norm": 0.0533231683075428,
      "learning_rate": 9.508100147275405e-05,
      "loss": 0.3693,
      "step": 1786
    },
    {
      "epoch": 0.5255882352941177,
      "grad_norm": 0.05664095655083656,
      "learning_rate": 9.50220913107511e-05,
      "loss": 0.3641,
      "step": 1787
    },
    {
      "epoch": 0.5258823529411765,
      "grad_norm": 0.046374958008527756,
      "learning_rate": 9.496318114874816e-05,
      "loss": 0.3091,
      "step": 1788
    },
    {
      "epoch": 0.5261764705882352,
      "grad_norm": 0.040690258145332336,
      "learning_rate": 9.490427098674521e-05,
      "loss": 0.3004,
      "step": 1789
    },
    {
      "epoch": 0.5264705882352941,
      "grad_norm": 0.04289671406149864,
      "learning_rate": 9.484536082474227e-05,
      "loss": 0.3243,
      "step": 1790
    },
    {
      "epoch": 0.5267647058823529,
      "grad_norm": 0.041610363870859146,
      "learning_rate": 9.478645066273933e-05,
      "loss": 0.3561,
      "step": 1791
    },
    {
      "epoch": 0.5270588235294118,
      "grad_norm": 0.0369388721883297,
      "learning_rate": 9.472754050073637e-05,
      "loss": 0.2433,
      "step": 1792
    },
    {
      "epoch": 0.5273529411764706,
      "grad_norm": 0.044694047421216965,
      "learning_rate": 9.466863033873343e-05,
      "loss": 0.3097,
      "step": 1793
    },
    {
      "epoch": 0.5276470588235294,
      "grad_norm": 0.03771381825208664,
      "learning_rate": 9.460972017673048e-05,
      "loss": 0.249,
      "step": 1794
    },
    {
      "epoch": 0.5279411764705882,
      "grad_norm": 0.05496547743678093,
      "learning_rate": 9.455081001472754e-05,
      "loss": 0.3592,
      "step": 1795
    },
    {
      "epoch": 0.528235294117647,
      "grad_norm": 0.03788748383522034,
      "learning_rate": 9.44918998527246e-05,
      "loss": 0.3299,
      "step": 1796
    },
    {
      "epoch": 0.5285294117647059,
      "grad_norm": 0.04291585832834244,
      "learning_rate": 9.443298969072165e-05,
      "loss": 0.2854,
      "step": 1797
    },
    {
      "epoch": 0.5288235294117647,
      "grad_norm": 0.05119067430496216,
      "learning_rate": 9.437407952871871e-05,
      "loss": 0.3709,
      "step": 1798
    },
    {
      "epoch": 0.5291176470588236,
      "grad_norm": 0.05103572830557823,
      "learning_rate": 9.431516936671576e-05,
      "loss": 0.391,
      "step": 1799
    },
    {
      "epoch": 0.5294117647058824,
      "grad_norm": 0.038661304861307144,
      "learning_rate": 9.425625920471282e-05,
      "loss": 0.3196,
      "step": 1800
    },
    {
      "epoch": 0.5294117647058824,
      "eval_loss": 0.354735791683197,
      "eval_runtime": 215.0362,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1800
    },
    {
      "epoch": 0.5297058823529411,
      "grad_norm": 0.04663600027561188,
      "learning_rate": 9.419734904270988e-05,
      "loss": 0.3483,
      "step": 1801
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.046556901186704636,
      "learning_rate": 9.413843888070692e-05,
      "loss": 0.402,
      "step": 1802
    },
    {
      "epoch": 0.5302941176470588,
      "grad_norm": 0.0578414723277092,
      "learning_rate": 9.407952871870398e-05,
      "loss": 0.3455,
      "step": 1803
    },
    {
      "epoch": 0.5305882352941177,
      "grad_norm": 0.0468086376786232,
      "learning_rate": 9.402061855670103e-05,
      "loss": 0.3011,
      "step": 1804
    },
    {
      "epoch": 0.5308823529411765,
      "grad_norm": 0.048519033938646317,
      "learning_rate": 9.396170839469809e-05,
      "loss": 0.3296,
      "step": 1805
    },
    {
      "epoch": 0.5311764705882352,
      "grad_norm": 0.06479869037866592,
      "learning_rate": 9.390279823269515e-05,
      "loss": 0.4281,
      "step": 1806
    },
    {
      "epoch": 0.5314705882352941,
      "grad_norm": 0.06008301302790642,
      "learning_rate": 9.38438880706922e-05,
      "loss": 0.4246,
      "step": 1807
    },
    {
      "epoch": 0.5317647058823529,
      "grad_norm": 0.05513978749513626,
      "learning_rate": 9.378497790868926e-05,
      "loss": 0.3841,
      "step": 1808
    },
    {
      "epoch": 0.5320588235294118,
      "grad_norm": 0.03571990504860878,
      "learning_rate": 9.37260677466863e-05,
      "loss": 0.2365,
      "step": 1809
    },
    {
      "epoch": 0.5323529411764706,
      "grad_norm": 0.04401388391852379,
      "learning_rate": 9.366715758468336e-05,
      "loss": 0.3475,
      "step": 1810
    },
    {
      "epoch": 0.5326470588235294,
      "grad_norm": 0.03750721737742424,
      "learning_rate": 9.360824742268042e-05,
      "loss": 0.3306,
      "step": 1811
    },
    {
      "epoch": 0.5329411764705883,
      "grad_norm": 0.04099415987730026,
      "learning_rate": 9.354933726067747e-05,
      "loss": 0.3961,
      "step": 1812
    },
    {
      "epoch": 0.533235294117647,
      "grad_norm": 0.05096830800175667,
      "learning_rate": 9.349042709867453e-05,
      "loss": 0.3441,
      "step": 1813
    },
    {
      "epoch": 0.5335294117647059,
      "grad_norm": 0.04097587242722511,
      "learning_rate": 9.343151693667158e-05,
      "loss": 0.2731,
      "step": 1814
    },
    {
      "epoch": 0.5338235294117647,
      "grad_norm": 0.045405805110931396,
      "learning_rate": 9.337260677466864e-05,
      "loss": 0.3669,
      "step": 1815
    },
    {
      "epoch": 0.5341176470588235,
      "grad_norm": 0.04693211242556572,
      "learning_rate": 9.33136966126657e-05,
      "loss": 0.3298,
      "step": 1816
    },
    {
      "epoch": 0.5344117647058824,
      "grad_norm": 0.042806077748537064,
      "learning_rate": 9.325478645066274e-05,
      "loss": 0.3425,
      "step": 1817
    },
    {
      "epoch": 0.5347058823529411,
      "grad_norm": 0.04029611125588417,
      "learning_rate": 9.31958762886598e-05,
      "loss": 0.3311,
      "step": 1818
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.044577062129974365,
      "learning_rate": 9.313696612665685e-05,
      "loss": 0.3377,
      "step": 1819
    },
    {
      "epoch": 0.5352941176470588,
      "grad_norm": 0.0634269192814827,
      "learning_rate": 9.307805596465391e-05,
      "loss": 0.3916,
      "step": 1820
    },
    {
      "epoch": 0.5355882352941177,
      "grad_norm": 0.05173816904425621,
      "learning_rate": 9.301914580265097e-05,
      "loss": 0.3084,
      "step": 1821
    },
    {
      "epoch": 0.5358823529411765,
      "grad_norm": 0.044208329170942307,
      "learning_rate": 9.296023564064802e-05,
      "loss": 0.3118,
      "step": 1822
    },
    {
      "epoch": 0.5361764705882353,
      "grad_norm": 0.05173271521925926,
      "learning_rate": 9.290132547864508e-05,
      "loss": 0.3616,
      "step": 1823
    },
    {
      "epoch": 0.5364705882352941,
      "grad_norm": 0.04538016393780708,
      "learning_rate": 9.284241531664212e-05,
      "loss": 0.362,
      "step": 1824
    },
    {
      "epoch": 0.5367647058823529,
      "grad_norm": 0.0431702546775341,
      "learning_rate": 9.278350515463918e-05,
      "loss": 0.3576,
      "step": 1825
    },
    {
      "epoch": 0.5370588235294118,
      "grad_norm": 0.04864157736301422,
      "learning_rate": 9.272459499263624e-05,
      "loss": 0.3619,
      "step": 1826
    },
    {
      "epoch": 0.5373529411764706,
      "grad_norm": 0.042712897062301636,
      "learning_rate": 9.266568483063329e-05,
      "loss": 0.3192,
      "step": 1827
    },
    {
      "epoch": 0.5376470588235294,
      "grad_norm": 0.04584601894021034,
      "learning_rate": 9.260677466863035e-05,
      "loss": 0.3311,
      "step": 1828
    },
    {
      "epoch": 0.5379411764705883,
      "grad_norm": 0.050582028925418854,
      "learning_rate": 9.25478645066274e-05,
      "loss": 0.2798,
      "step": 1829
    },
    {
      "epoch": 0.538235294117647,
      "grad_norm": 0.03501608595252037,
      "learning_rate": 9.248895434462446e-05,
      "loss": 0.2731,
      "step": 1830
    },
    {
      "epoch": 0.5385294117647059,
      "grad_norm": 0.05501287803053856,
      "learning_rate": 9.243004418262152e-05,
      "loss": 0.2966,
      "step": 1831
    },
    {
      "epoch": 0.5388235294117647,
      "grad_norm": 0.05966725945472717,
      "learning_rate": 9.237113402061856e-05,
      "loss": 0.4023,
      "step": 1832
    },
    {
      "epoch": 0.5391176470588235,
      "grad_norm": 0.05176457390189171,
      "learning_rate": 9.231222385861562e-05,
      "loss": 0.3673,
      "step": 1833
    },
    {
      "epoch": 0.5394117647058824,
      "grad_norm": 0.053075965493917465,
      "learning_rate": 9.225331369661267e-05,
      "loss": 0.3448,
      "step": 1834
    },
    {
      "epoch": 0.5397058823529411,
      "grad_norm": 0.04204459488391876,
      "learning_rate": 9.219440353460973e-05,
      "loss": 0.3439,
      "step": 1835
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.04556887969374657,
      "learning_rate": 9.213549337260678e-05,
      "loss": 0.316,
      "step": 1836
    },
    {
      "epoch": 0.5402941176470588,
      "grad_norm": 0.047464217990636826,
      "learning_rate": 9.207658321060382e-05,
      "loss": 0.3463,
      "step": 1837
    },
    {
      "epoch": 0.5405882352941176,
      "grad_norm": 0.04160198196768761,
      "learning_rate": 9.201767304860088e-05,
      "loss": 0.3071,
      "step": 1838
    },
    {
      "epoch": 0.5408823529411765,
      "grad_norm": 0.0413566529750824,
      "learning_rate": 9.195876288659793e-05,
      "loss": 0.2878,
      "step": 1839
    },
    {
      "epoch": 0.5411764705882353,
      "grad_norm": 0.0407385528087616,
      "learning_rate": 9.189985272459499e-05,
      "loss": 0.3029,
      "step": 1840
    },
    {
      "epoch": 0.5414705882352941,
      "grad_norm": 0.04538820683956146,
      "learning_rate": 9.184094256259205e-05,
      "loss": 0.3385,
      "step": 1841
    },
    {
      "epoch": 0.5417647058823529,
      "grad_norm": 0.0615985170006752,
      "learning_rate": 9.17820324005891e-05,
      "loss": 0.4313,
      "step": 1842
    },
    {
      "epoch": 0.5420588235294118,
      "grad_norm": 0.04653133079409599,
      "learning_rate": 9.172312223858616e-05,
      "loss": 0.2931,
      "step": 1843
    },
    {
      "epoch": 0.5423529411764706,
      "grad_norm": 0.04441971331834793,
      "learning_rate": 9.16642120765832e-05,
      "loss": 0.363,
      "step": 1844
    },
    {
      "epoch": 0.5426470588235294,
      "grad_norm": 0.040075596421957016,
      "learning_rate": 9.160530191458027e-05,
      "loss": 0.3009,
      "step": 1845
    },
    {
      "epoch": 0.5429411764705883,
      "grad_norm": 0.037282511591911316,
      "learning_rate": 9.154639175257733e-05,
      "loss": 0.2544,
      "step": 1846
    },
    {
      "epoch": 0.543235294117647,
      "grad_norm": 0.0422031506896019,
      "learning_rate": 9.148748159057437e-05,
      "loss": 0.3853,
      "step": 1847
    },
    {
      "epoch": 0.5435294117647059,
      "grad_norm": 0.049369268119335175,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.3869,
      "step": 1848
    },
    {
      "epoch": 0.5438235294117647,
      "grad_norm": 0.051845189183950424,
      "learning_rate": 9.136966126656848e-05,
      "loss": 0.3876,
      "step": 1849
    },
    {
      "epoch": 0.5441176470588235,
      "grad_norm": 0.048428021371364594,
      "learning_rate": 9.131075110456554e-05,
      "loss": 0.3477,
      "step": 1850
    },
    {
      "epoch": 0.5444117647058824,
      "grad_norm": 0.06003117561340332,
      "learning_rate": 9.12518409425626e-05,
      "loss": 0.3337,
      "step": 1851
    },
    {
      "epoch": 0.5447058823529412,
      "grad_norm": 0.06034212186932564,
      "learning_rate": 9.119293078055965e-05,
      "loss": 0.362,
      "step": 1852
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.04489065706729889,
      "learning_rate": 9.11340206185567e-05,
      "loss": 0.3422,
      "step": 1853
    },
    {
      "epoch": 0.5452941176470588,
      "grad_norm": 0.04905321076512337,
      "learning_rate": 9.107511045655375e-05,
      "loss": 0.3374,
      "step": 1854
    },
    {
      "epoch": 0.5455882352941176,
      "grad_norm": 0.055135879665613174,
      "learning_rate": 9.101620029455081e-05,
      "loss": 0.3117,
      "step": 1855
    },
    {
      "epoch": 0.5458823529411765,
      "grad_norm": 0.0425938181579113,
      "learning_rate": 9.095729013254787e-05,
      "loss": 0.3267,
      "step": 1856
    },
    {
      "epoch": 0.5461764705882353,
      "grad_norm": 0.0552813820540905,
      "learning_rate": 9.089837997054492e-05,
      "loss": 0.33,
      "step": 1857
    },
    {
      "epoch": 0.5464705882352942,
      "grad_norm": 0.03949713706970215,
      "learning_rate": 9.083946980854198e-05,
      "loss": 0.2709,
      "step": 1858
    },
    {
      "epoch": 0.5467647058823529,
      "grad_norm": 0.0498775951564312,
      "learning_rate": 9.078055964653903e-05,
      "loss": 0.3703,
      "step": 1859
    },
    {
      "epoch": 0.5470588235294118,
      "grad_norm": 0.06749560683965683,
      "learning_rate": 9.072164948453609e-05,
      "loss": 0.4816,
      "step": 1860
    },
    {
      "epoch": 0.5473529411764706,
      "grad_norm": 0.053641755133867264,
      "learning_rate": 9.066273932253315e-05,
      "loss": 0.3573,
      "step": 1861
    },
    {
      "epoch": 0.5476470588235294,
      "grad_norm": 0.05622876062989235,
      "learning_rate": 9.060382916053019e-05,
      "loss": 0.376,
      "step": 1862
    },
    {
      "epoch": 0.5479411764705883,
      "grad_norm": 0.04321601986885071,
      "learning_rate": 9.054491899852725e-05,
      "loss": 0.3316,
      "step": 1863
    },
    {
      "epoch": 0.548235294117647,
      "grad_norm": 0.055059414356946945,
      "learning_rate": 9.04860088365243e-05,
      "loss": 0.4211,
      "step": 1864
    },
    {
      "epoch": 0.5485294117647059,
      "grad_norm": 0.04477987065911293,
      "learning_rate": 9.042709867452136e-05,
      "loss": 0.3514,
      "step": 1865
    },
    {
      "epoch": 0.5488235294117647,
      "grad_norm": 0.04636155813932419,
      "learning_rate": 9.036818851251842e-05,
      "loss": 0.3677,
      "step": 1866
    },
    {
      "epoch": 0.5491176470588235,
      "grad_norm": 0.05439474806189537,
      "learning_rate": 9.030927835051547e-05,
      "loss": 0.407,
      "step": 1867
    },
    {
      "epoch": 0.5494117647058824,
      "grad_norm": 0.048285529017448425,
      "learning_rate": 9.025036818851253e-05,
      "loss": 0.3331,
      "step": 1868
    },
    {
      "epoch": 0.5497058823529412,
      "grad_norm": 0.05072841793298721,
      "learning_rate": 9.019145802650957e-05,
      "loss": 0.3621,
      "step": 1869
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.03995278477668762,
      "learning_rate": 9.013254786450663e-05,
      "loss": 0.331,
      "step": 1870
    },
    {
      "epoch": 0.5502941176470588,
      "grad_norm": 0.05012073367834091,
      "learning_rate": 9.00736377025037e-05,
      "loss": 0.3388,
      "step": 1871
    },
    {
      "epoch": 0.5505882352941176,
      "grad_norm": 0.04864496737718582,
      "learning_rate": 9.001472754050074e-05,
      "loss": 0.4085,
      "step": 1872
    },
    {
      "epoch": 0.5508823529411765,
      "grad_norm": 0.044518351554870605,
      "learning_rate": 8.99558173784978e-05,
      "loss": 0.341,
      "step": 1873
    },
    {
      "epoch": 0.5511764705882353,
      "grad_norm": 0.03861990198493004,
      "learning_rate": 8.989690721649485e-05,
      "loss": 0.3426,
      "step": 1874
    },
    {
      "epoch": 0.5514705882352942,
      "grad_norm": 0.048988427966833115,
      "learning_rate": 8.983799705449191e-05,
      "loss": 0.3369,
      "step": 1875
    },
    {
      "epoch": 0.5517647058823529,
      "grad_norm": 0.04743443801999092,
      "learning_rate": 8.977908689248897e-05,
      "loss": 0.3399,
      "step": 1876
    },
    {
      "epoch": 0.5520588235294117,
      "grad_norm": 0.036383286118507385,
      "learning_rate": 8.972017673048601e-05,
      "loss": 0.2991,
      "step": 1877
    },
    {
      "epoch": 0.5523529411764706,
      "grad_norm": 0.052748698741197586,
      "learning_rate": 8.966126656848307e-05,
      "loss": 0.3531,
      "step": 1878
    },
    {
      "epoch": 0.5526470588235294,
      "grad_norm": 0.05606452375650406,
      "learning_rate": 8.960235640648012e-05,
      "loss": 0.4254,
      "step": 1879
    },
    {
      "epoch": 0.5529411764705883,
      "grad_norm": 0.04573902487754822,
      "learning_rate": 8.954344624447718e-05,
      "loss": 0.3698,
      "step": 1880
    },
    {
      "epoch": 0.553235294117647,
      "grad_norm": 0.052257124334573746,
      "learning_rate": 8.948453608247424e-05,
      "loss": 0.3784,
      "step": 1881
    },
    {
      "epoch": 0.5535294117647059,
      "grad_norm": 0.05450078099966049,
      "learning_rate": 8.942562592047129e-05,
      "loss": 0.3917,
      "step": 1882
    },
    {
      "epoch": 0.5538235294117647,
      "grad_norm": 0.04227297008037567,
      "learning_rate": 8.936671575846835e-05,
      "loss": 0.3571,
      "step": 1883
    },
    {
      "epoch": 0.5541176470588235,
      "grad_norm": 0.04336343705654144,
      "learning_rate": 8.93078055964654e-05,
      "loss": 0.3582,
      "step": 1884
    },
    {
      "epoch": 0.5544117647058824,
      "grad_norm": 0.055460549890995026,
      "learning_rate": 8.924889543446246e-05,
      "loss": 0.3808,
      "step": 1885
    },
    {
      "epoch": 0.5547058823529412,
      "grad_norm": 0.04190743714570999,
      "learning_rate": 8.918998527245952e-05,
      "loss": 0.2746,
      "step": 1886
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.04778246209025383,
      "learning_rate": 8.913107511045656e-05,
      "loss": 0.273,
      "step": 1887
    },
    {
      "epoch": 0.5552941176470588,
      "grad_norm": 0.048150479793548584,
      "learning_rate": 8.907216494845362e-05,
      "loss": 0.3418,
      "step": 1888
    },
    {
      "epoch": 0.5555882352941176,
      "grad_norm": 0.060752954334020615,
      "learning_rate": 8.901325478645066e-05,
      "loss": 0.4094,
      "step": 1889
    },
    {
      "epoch": 0.5558823529411765,
      "grad_norm": 0.06227699667215347,
      "learning_rate": 8.895434462444772e-05,
      "loss": 0.4254,
      "step": 1890
    },
    {
      "epoch": 0.5561764705882353,
      "grad_norm": 0.04677232354879379,
      "learning_rate": 8.889543446244478e-05,
      "loss": 0.3567,
      "step": 1891
    },
    {
      "epoch": 0.5564705882352942,
      "grad_norm": 0.04238835722208023,
      "learning_rate": 8.883652430044182e-05,
      "loss": 0.2937,
      "step": 1892
    },
    {
      "epoch": 0.5567647058823529,
      "grad_norm": 0.0606326125562191,
      "learning_rate": 8.877761413843888e-05,
      "loss": 0.4023,
      "step": 1893
    },
    {
      "epoch": 0.5570588235294117,
      "grad_norm": 0.038650453090667725,
      "learning_rate": 8.871870397643593e-05,
      "loss": 0.2753,
      "step": 1894
    },
    {
      "epoch": 0.5573529411764706,
      "grad_norm": 0.05009415000677109,
      "learning_rate": 8.865979381443299e-05,
      "loss": 0.3382,
      "step": 1895
    },
    {
      "epoch": 0.5576470588235294,
      "grad_norm": 0.03973177447915077,
      "learning_rate": 8.860088365243005e-05,
      "loss": 0.2728,
      "step": 1896
    },
    {
      "epoch": 0.5579411764705883,
      "grad_norm": 0.05194797366857529,
      "learning_rate": 8.85419734904271e-05,
      "loss": 0.4034,
      "step": 1897
    },
    {
      "epoch": 0.558235294117647,
      "grad_norm": 0.043556079268455505,
      "learning_rate": 8.848306332842416e-05,
      "loss": 0.3074,
      "step": 1898
    },
    {
      "epoch": 0.5585294117647058,
      "grad_norm": 0.05351043865084648,
      "learning_rate": 8.84241531664212e-05,
      "loss": 0.4,
      "step": 1899
    },
    {
      "epoch": 0.5588235294117647,
      "grad_norm": 0.06298115849494934,
      "learning_rate": 8.836524300441826e-05,
      "loss": 0.4011,
      "step": 1900
    },
    {
      "epoch": 0.5588235294117647,
      "eval_loss": 0.3541489839553833,
      "eval_runtime": 215.1454,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 1900
    },
    {
      "epoch": 0.5591176470588235,
      "grad_norm": 0.0494181290268898,
      "learning_rate": 8.830633284241532e-05,
      "loss": 0.4113,
      "step": 1901
    },
    {
      "epoch": 0.5594117647058824,
      "grad_norm": 0.048658475279808044,
      "learning_rate": 8.824742268041237e-05,
      "loss": 0.3433,
      "step": 1902
    },
    {
      "epoch": 0.5597058823529412,
      "grad_norm": 0.05034078285098076,
      "learning_rate": 8.818851251840943e-05,
      "loss": 0.2918,
      "step": 1903
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.04760800674557686,
      "learning_rate": 8.812960235640648e-05,
      "loss": 0.3399,
      "step": 1904
    },
    {
      "epoch": 0.5602941176470588,
      "grad_norm": 0.04796388000249863,
      "learning_rate": 8.807069219440354e-05,
      "loss": 0.3458,
      "step": 1905
    },
    {
      "epoch": 0.5605882352941176,
      "grad_norm": 0.053976524621248245,
      "learning_rate": 8.80117820324006e-05,
      "loss": 0.4393,
      "step": 1906
    },
    {
      "epoch": 0.5608823529411765,
      "grad_norm": 0.04775578901171684,
      "learning_rate": 8.795287187039764e-05,
      "loss": 0.3305,
      "step": 1907
    },
    {
      "epoch": 0.5611764705882353,
      "grad_norm": 0.04433073848485947,
      "learning_rate": 8.78939617083947e-05,
      "loss": 0.3296,
      "step": 1908
    },
    {
      "epoch": 0.5614705882352942,
      "grad_norm": 0.06168816238641739,
      "learning_rate": 8.783505154639175e-05,
      "loss": 0.4104,
      "step": 1909
    },
    {
      "epoch": 0.5617647058823529,
      "grad_norm": 0.043356429785490036,
      "learning_rate": 8.777614138438881e-05,
      "loss": 0.397,
      "step": 1910
    },
    {
      "epoch": 0.5620588235294117,
      "grad_norm": 0.0479293018579483,
      "learning_rate": 8.771723122238587e-05,
      "loss": 0.3948,
      "step": 1911
    },
    {
      "epoch": 0.5623529411764706,
      "grad_norm": 0.04815453290939331,
      "learning_rate": 8.765832106038292e-05,
      "loss": 0.3812,
      "step": 1912
    },
    {
      "epoch": 0.5626470588235294,
      "grad_norm": 0.040274884551763535,
      "learning_rate": 8.759941089837998e-05,
      "loss": 0.3147,
      "step": 1913
    },
    {
      "epoch": 0.5629411764705883,
      "grad_norm": 0.049300666898489,
      "learning_rate": 8.754050073637702e-05,
      "loss": 0.353,
      "step": 1914
    },
    {
      "epoch": 0.5632352941176471,
      "grad_norm": 0.04325628653168678,
      "learning_rate": 8.748159057437408e-05,
      "loss": 0.3918,
      "step": 1915
    },
    {
      "epoch": 0.5635294117647058,
      "grad_norm": 0.0460246242582798,
      "learning_rate": 8.742268041237114e-05,
      "loss": 0.3558,
      "step": 1916
    },
    {
      "epoch": 0.5638235294117647,
      "grad_norm": 0.05993739515542984,
      "learning_rate": 8.736377025036819e-05,
      "loss": 0.3838,
      "step": 1917
    },
    {
      "epoch": 0.5641176470588235,
      "grad_norm": 0.0638018473982811,
      "learning_rate": 8.730486008836525e-05,
      "loss": 0.4331,
      "step": 1918
    },
    {
      "epoch": 0.5644117647058824,
      "grad_norm": 0.06168200075626373,
      "learning_rate": 8.72459499263623e-05,
      "loss": 0.4258,
      "step": 1919
    },
    {
      "epoch": 0.5647058823529412,
      "grad_norm": 0.047590307891368866,
      "learning_rate": 8.718703976435936e-05,
      "loss": 0.3088,
      "step": 1920
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.040233395993709564,
      "learning_rate": 8.712812960235642e-05,
      "loss": 0.2798,
      "step": 1921
    },
    {
      "epoch": 0.5652941176470588,
      "grad_norm": 0.05408810079097748,
      "learning_rate": 8.706921944035346e-05,
      "loss": 0.3817,
      "step": 1922
    },
    {
      "epoch": 0.5655882352941176,
      "grad_norm": 0.049048081040382385,
      "learning_rate": 8.701030927835052e-05,
      "loss": 0.3056,
      "step": 1923
    },
    {
      "epoch": 0.5658823529411765,
      "grad_norm": 0.05497991666197777,
      "learning_rate": 8.695139911634757e-05,
      "loss": 0.3976,
      "step": 1924
    },
    {
      "epoch": 0.5661764705882353,
      "grad_norm": 0.03588224574923515,
      "learning_rate": 8.689248895434463e-05,
      "loss": 0.3341,
      "step": 1925
    },
    {
      "epoch": 0.5664705882352942,
      "grad_norm": 0.056190356612205505,
      "learning_rate": 8.683357879234169e-05,
      "loss": 0.4038,
      "step": 1926
    },
    {
      "epoch": 0.566764705882353,
      "grad_norm": 0.05025533214211464,
      "learning_rate": 8.677466863033874e-05,
      "loss": 0.3646,
      "step": 1927
    },
    {
      "epoch": 0.5670588235294117,
      "grad_norm": 0.060820482671260834,
      "learning_rate": 8.67157584683358e-05,
      "loss": 0.4338,
      "step": 1928
    },
    {
      "epoch": 0.5673529411764706,
      "grad_norm": 0.038887619972229004,
      "learning_rate": 8.665684830633284e-05,
      "loss": 0.318,
      "step": 1929
    },
    {
      "epoch": 0.5676470588235294,
      "grad_norm": 0.04239388927817345,
      "learning_rate": 8.65979381443299e-05,
      "loss": 0.3768,
      "step": 1930
    },
    {
      "epoch": 0.5679411764705883,
      "grad_norm": 0.049315113574266434,
      "learning_rate": 8.653902798232697e-05,
      "loss": 0.3558,
      "step": 1931
    },
    {
      "epoch": 0.5682352941176471,
      "grad_norm": 0.037775907665491104,
      "learning_rate": 8.648011782032401e-05,
      "loss": 0.3141,
      "step": 1932
    },
    {
      "epoch": 0.5685294117647058,
      "grad_norm": 0.060911644250154495,
      "learning_rate": 8.642120765832107e-05,
      "loss": 0.3916,
      "step": 1933
    },
    {
      "epoch": 0.5688235294117647,
      "grad_norm": 0.04002375900745392,
      "learning_rate": 8.636229749631812e-05,
      "loss": 0.2945,
      "step": 1934
    },
    {
      "epoch": 0.5691176470588235,
      "grad_norm": 0.05747006833553314,
      "learning_rate": 8.630338733431518e-05,
      "loss": 0.4242,
      "step": 1935
    },
    {
      "epoch": 0.5694117647058824,
      "grad_norm": 0.05277176946401596,
      "learning_rate": 8.624447717231224e-05,
      "loss": 0.403,
      "step": 1936
    },
    {
      "epoch": 0.5697058823529412,
      "grad_norm": 0.049593422561883926,
      "learning_rate": 8.618556701030929e-05,
      "loss": 0.3392,
      "step": 1937
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.0492636002600193,
      "learning_rate": 8.612665684830635e-05,
      "loss": 0.4157,
      "step": 1938
    },
    {
      "epoch": 0.5702941176470588,
      "grad_norm": 0.07399068772792816,
      "learning_rate": 8.606774668630339e-05,
      "loss": 0.3392,
      "step": 1939
    },
    {
      "epoch": 0.5705882352941176,
      "grad_norm": 0.05529350787401199,
      "learning_rate": 8.600883652430045e-05,
      "loss": 0.3803,
      "step": 1940
    },
    {
      "epoch": 0.5708823529411765,
      "grad_norm": 0.035624947398900986,
      "learning_rate": 8.594992636229751e-05,
      "loss": 0.2576,
      "step": 1941
    },
    {
      "epoch": 0.5711764705882353,
      "grad_norm": 0.04065055027604103,
      "learning_rate": 8.589101620029455e-05,
      "loss": 0.309,
      "step": 1942
    },
    {
      "epoch": 0.5714705882352941,
      "grad_norm": 0.050908755511045456,
      "learning_rate": 8.58321060382916e-05,
      "loss": 0.3503,
      "step": 1943
    },
    {
      "epoch": 0.571764705882353,
      "grad_norm": 0.041843488812446594,
      "learning_rate": 8.577319587628865e-05,
      "loss": 0.2741,
      "step": 1944
    },
    {
      "epoch": 0.5720588235294117,
      "grad_norm": 0.05306628346443176,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.3847,
      "step": 1945
    },
    {
      "epoch": 0.5723529411764706,
      "grad_norm": 0.0460236594080925,
      "learning_rate": 8.565537555228277e-05,
      "loss": 0.3398,
      "step": 1946
    },
    {
      "epoch": 0.5726470588235294,
      "grad_norm": 0.055077746510505676,
      "learning_rate": 8.559646539027982e-05,
      "loss": 0.3826,
      "step": 1947
    },
    {
      "epoch": 0.5729411764705883,
      "grad_norm": 0.04378444328904152,
      "learning_rate": 8.553755522827688e-05,
      "loss": 0.3422,
      "step": 1948
    },
    {
      "epoch": 0.5732352941176471,
      "grad_norm": 0.05307929962873459,
      "learning_rate": 8.547864506627393e-05,
      "loss": 0.3262,
      "step": 1949
    },
    {
      "epoch": 0.5735294117647058,
      "grad_norm": 0.04229868948459625,
      "learning_rate": 8.541973490427099e-05,
      "loss": 0.2829,
      "step": 1950
    },
    {
      "epoch": 0.5738235294117647,
      "grad_norm": 0.042962364852428436,
      "learning_rate": 8.536082474226805e-05,
      "loss": 0.3104,
      "step": 1951
    },
    {
      "epoch": 0.5741176470588235,
      "grad_norm": 0.05691291764378548,
      "learning_rate": 8.530191458026509e-05,
      "loss": 0.3878,
      "step": 1952
    },
    {
      "epoch": 0.5744117647058824,
      "grad_norm": 0.04601665958762169,
      "learning_rate": 8.524300441826215e-05,
      "loss": 0.3579,
      "step": 1953
    },
    {
      "epoch": 0.5747058823529412,
      "grad_norm": 0.059234678745269775,
      "learning_rate": 8.51840942562592e-05,
      "loss": 0.3804,
      "step": 1954
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.041287295520305634,
      "learning_rate": 8.512518409425626e-05,
      "loss": 0.3215,
      "step": 1955
    },
    {
      "epoch": 0.5752941176470588,
      "grad_norm": 0.04395680874586105,
      "learning_rate": 8.506627393225332e-05,
      "loss": 0.2839,
      "step": 1956
    },
    {
      "epoch": 0.5755882352941176,
      "grad_norm": 0.03847332298755646,
      "learning_rate": 8.500736377025037e-05,
      "loss": 0.2873,
      "step": 1957
    },
    {
      "epoch": 0.5758823529411765,
      "grad_norm": 0.038385454565286636,
      "learning_rate": 8.494845360824743e-05,
      "loss": 0.2623,
      "step": 1958
    },
    {
      "epoch": 0.5761764705882353,
      "grad_norm": 0.05705936253070831,
      "learning_rate": 8.488954344624447e-05,
      "loss": 0.3536,
      "step": 1959
    },
    {
      "epoch": 0.5764705882352941,
      "grad_norm": 0.045291975140571594,
      "learning_rate": 8.483063328424153e-05,
      "loss": 0.337,
      "step": 1960
    },
    {
      "epoch": 0.576764705882353,
      "grad_norm": 0.053036898374557495,
      "learning_rate": 8.47717231222386e-05,
      "loss": 0.3768,
      "step": 1961
    },
    {
      "epoch": 0.5770588235294117,
      "grad_norm": 0.05502954125404358,
      "learning_rate": 8.471281296023564e-05,
      "loss": 0.3432,
      "step": 1962
    },
    {
      "epoch": 0.5773529411764706,
      "grad_norm": 0.042302168905735016,
      "learning_rate": 8.46539027982327e-05,
      "loss": 0.3373,
      "step": 1963
    },
    {
      "epoch": 0.5776470588235294,
      "grad_norm": 0.04176574945449829,
      "learning_rate": 8.459499263622975e-05,
      "loss": 0.3282,
      "step": 1964
    },
    {
      "epoch": 0.5779411764705882,
      "grad_norm": 0.04233730956912041,
      "learning_rate": 8.453608247422681e-05,
      "loss": 0.3117,
      "step": 1965
    },
    {
      "epoch": 0.5782352941176471,
      "grad_norm": 0.05095972120761871,
      "learning_rate": 8.447717231222387e-05,
      "loss": 0.3636,
      "step": 1966
    },
    {
      "epoch": 0.5785294117647058,
      "grad_norm": 0.05490601435303688,
      "learning_rate": 8.441826215022091e-05,
      "loss": 0.2721,
      "step": 1967
    },
    {
      "epoch": 0.5788235294117647,
      "grad_norm": 0.03800354525446892,
      "learning_rate": 8.435935198821797e-05,
      "loss": 0.3006,
      "step": 1968
    },
    {
      "epoch": 0.5791176470588235,
      "grad_norm": 0.04566212743520737,
      "learning_rate": 8.430044182621502e-05,
      "loss": 0.3448,
      "step": 1969
    },
    {
      "epoch": 0.5794117647058824,
      "grad_norm": 0.043957795947790146,
      "learning_rate": 8.424153166421208e-05,
      "loss": 0.3405,
      "step": 1970
    },
    {
      "epoch": 0.5797058823529412,
      "grad_norm": 0.04097147285938263,
      "learning_rate": 8.418262150220914e-05,
      "loss": 0.3497,
      "step": 1971
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.05753789842128754,
      "learning_rate": 8.412371134020619e-05,
      "loss": 0.376,
      "step": 1972
    },
    {
      "epoch": 0.5802941176470588,
      "grad_norm": 0.036959823220968246,
      "learning_rate": 8.406480117820325e-05,
      "loss": 0.305,
      "step": 1973
    },
    {
      "epoch": 0.5805882352941176,
      "grad_norm": 0.0532904677093029,
      "learning_rate": 8.40058910162003e-05,
      "loss": 0.3925,
      "step": 1974
    },
    {
      "epoch": 0.5808823529411765,
      "grad_norm": 0.05071989819407463,
      "learning_rate": 8.394698085419735e-05,
      "loss": 0.3819,
      "step": 1975
    },
    {
      "epoch": 0.5811764705882353,
      "grad_norm": 0.030278034508228302,
      "learning_rate": 8.388807069219442e-05,
      "loss": 0.2813,
      "step": 1976
    },
    {
      "epoch": 0.5814705882352941,
      "grad_norm": 0.036401160061359406,
      "learning_rate": 8.382916053019146e-05,
      "loss": 0.3077,
      "step": 1977
    },
    {
      "epoch": 0.581764705882353,
      "grad_norm": 0.035056740045547485,
      "learning_rate": 8.377025036818852e-05,
      "loss": 0.3062,
      "step": 1978
    },
    {
      "epoch": 0.5820588235294117,
      "grad_norm": 0.05686913803219795,
      "learning_rate": 8.371134020618557e-05,
      "loss": 0.3598,
      "step": 1979
    },
    {
      "epoch": 0.5823529411764706,
      "grad_norm": 0.048840269446372986,
      "learning_rate": 8.365243004418263e-05,
      "loss": 0.3524,
      "step": 1980
    },
    {
      "epoch": 0.5826470588235294,
      "grad_norm": 0.046510666608810425,
      "learning_rate": 8.359351988217969e-05,
      "loss": 0.4071,
      "step": 1981
    },
    {
      "epoch": 0.5829411764705882,
      "grad_norm": 0.05060427263379097,
      "learning_rate": 8.353460972017674e-05,
      "loss": 0.3443,
      "step": 1982
    },
    {
      "epoch": 0.5832352941176471,
      "grad_norm": 0.04469769448041916,
      "learning_rate": 8.34756995581738e-05,
      "loss": 0.3294,
      "step": 1983
    },
    {
      "epoch": 0.5835294117647059,
      "grad_norm": 0.06398685276508331,
      "learning_rate": 8.341678939617084e-05,
      "loss": 0.3982,
      "step": 1984
    },
    {
      "epoch": 0.5838235294117647,
      "grad_norm": 0.04674175754189491,
      "learning_rate": 8.33578792341679e-05,
      "loss": 0.3892,
      "step": 1985
    },
    {
      "epoch": 0.5841176470588235,
      "grad_norm": 0.050583381205797195,
      "learning_rate": 8.329896907216496e-05,
      "loss": 0.2631,
      "step": 1986
    },
    {
      "epoch": 0.5844117647058824,
      "grad_norm": 0.043134890496730804,
      "learning_rate": 8.324005891016201e-05,
      "loss": 0.3972,
      "step": 1987
    },
    {
      "epoch": 0.5847058823529412,
      "grad_norm": 0.060467422008514404,
      "learning_rate": 8.318114874815907e-05,
      "loss": 0.401,
      "step": 1988
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.05336218699812889,
      "learning_rate": 8.312223858615612e-05,
      "loss": 0.416,
      "step": 1989
    },
    {
      "epoch": 0.5852941176470589,
      "grad_norm": 0.04729130119085312,
      "learning_rate": 8.306332842415318e-05,
      "loss": 0.3268,
      "step": 1990
    },
    {
      "epoch": 0.5855882352941176,
      "grad_norm": 0.04605159908533096,
      "learning_rate": 8.300441826215024e-05,
      "loss": 0.362,
      "step": 1991
    },
    {
      "epoch": 0.5858823529411765,
      "grad_norm": 0.054356250911951065,
      "learning_rate": 8.294550810014728e-05,
      "loss": 0.4036,
      "step": 1992
    },
    {
      "epoch": 0.5861764705882353,
      "grad_norm": 0.04549213871359825,
      "learning_rate": 8.288659793814434e-05,
      "loss": 0.3408,
      "step": 1993
    },
    {
      "epoch": 0.5864705882352941,
      "grad_norm": 0.037033211439847946,
      "learning_rate": 8.282768777614139e-05,
      "loss": 0.3009,
      "step": 1994
    },
    {
      "epoch": 0.586764705882353,
      "grad_norm": 0.033868156373500824,
      "learning_rate": 8.276877761413844e-05,
      "loss": 0.2962,
      "step": 1995
    },
    {
      "epoch": 0.5870588235294117,
      "grad_norm": 0.052434030920267105,
      "learning_rate": 8.27098674521355e-05,
      "loss": 0.3142,
      "step": 1996
    },
    {
      "epoch": 0.5873529411764706,
      "grad_norm": 0.052188824862241745,
      "learning_rate": 8.265095729013254e-05,
      "loss": 0.3931,
      "step": 1997
    },
    {
      "epoch": 0.5876470588235294,
      "grad_norm": 0.04896083101630211,
      "learning_rate": 8.25920471281296e-05,
      "loss": 0.3613,
      "step": 1998
    },
    {
      "epoch": 0.5879411764705882,
      "grad_norm": 0.04862763360142708,
      "learning_rate": 8.253313696612665e-05,
      "loss": 0.3755,
      "step": 1999
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.03956785425543785,
      "learning_rate": 8.247422680412371e-05,
      "loss": 0.3419,
      "step": 2000
    },
    {
      "epoch": 0.5882352941176471,
      "eval_loss": 0.3534516990184784,
      "eval_runtime": 215.0137,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 2000
    },
    {
      "epoch": 0.5885294117647059,
      "grad_norm": 0.08007106184959412,
      "learning_rate": 8.241531664212077e-05,
      "loss": 0.3157,
      "step": 2001
    },
    {
      "epoch": 0.5888235294117647,
      "grad_norm": 0.04880348592996597,
      "learning_rate": 8.235640648011782e-05,
      "loss": 0.4,
      "step": 2002
    },
    {
      "epoch": 0.5891176470588235,
      "grad_norm": 0.05238376185297966,
      "learning_rate": 8.229749631811488e-05,
      "loss": 0.344,
      "step": 2003
    },
    {
      "epoch": 0.5894117647058823,
      "grad_norm": 0.05038890987634659,
      "learning_rate": 8.223858615611192e-05,
      "loss": 0.3716,
      "step": 2004
    },
    {
      "epoch": 0.5897058823529412,
      "grad_norm": 0.04929036274552345,
      "learning_rate": 8.217967599410898e-05,
      "loss": 0.3766,
      "step": 2005
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.051485154777765274,
      "learning_rate": 8.212076583210604e-05,
      "loss": 0.359,
      "step": 2006
    },
    {
      "epoch": 0.5902941176470589,
      "grad_norm": 0.0472106970846653,
      "learning_rate": 8.206185567010309e-05,
      "loss": 0.3455,
      "step": 2007
    },
    {
      "epoch": 0.5905882352941176,
      "grad_norm": 0.04126075655221939,
      "learning_rate": 8.200294550810015e-05,
      "loss": 0.2692,
      "step": 2008
    },
    {
      "epoch": 0.5908823529411765,
      "grad_norm": 0.04324346035718918,
      "learning_rate": 8.19440353460972e-05,
      "loss": 0.2433,
      "step": 2009
    },
    {
      "epoch": 0.5911764705882353,
      "grad_norm": 0.05978575348854065,
      "learning_rate": 8.188512518409426e-05,
      "loss": 0.3535,
      "step": 2010
    },
    {
      "epoch": 0.5914705882352941,
      "grad_norm": 0.04392977058887482,
      "learning_rate": 8.182621502209132e-05,
      "loss": 0.2685,
      "step": 2011
    },
    {
      "epoch": 0.591764705882353,
      "grad_norm": 0.045548126101493835,
      "learning_rate": 8.176730486008836e-05,
      "loss": 0.2982,
      "step": 2012
    },
    {
      "epoch": 0.5920588235294117,
      "grad_norm": 0.04893549159169197,
      "learning_rate": 8.170839469808542e-05,
      "loss": 0.3491,
      "step": 2013
    },
    {
      "epoch": 0.5923529411764706,
      "grad_norm": 0.06265737116336823,
      "learning_rate": 8.164948453608247e-05,
      "loss": 0.4111,
      "step": 2014
    },
    {
      "epoch": 0.5926470588235294,
      "grad_norm": 0.04757094010710716,
      "learning_rate": 8.159057437407953e-05,
      "loss": 0.3795,
      "step": 2015
    },
    {
      "epoch": 0.5929411764705882,
      "grad_norm": 0.06168097257614136,
      "learning_rate": 8.153166421207659e-05,
      "loss": 0.3885,
      "step": 2016
    },
    {
      "epoch": 0.5932352941176471,
      "grad_norm": 0.058187030255794525,
      "learning_rate": 8.147275405007364e-05,
      "loss": 0.3629,
      "step": 2017
    },
    {
      "epoch": 0.5935294117647059,
      "grad_norm": 0.05137164890766144,
      "learning_rate": 8.14138438880707e-05,
      "loss": 0.3435,
      "step": 2018
    },
    {
      "epoch": 0.5938235294117648,
      "grad_norm": 0.05793642997741699,
      "learning_rate": 8.135493372606774e-05,
      "loss": 0.4044,
      "step": 2019
    },
    {
      "epoch": 0.5941176470588235,
      "grad_norm": 0.06286374479532242,
      "learning_rate": 8.12960235640648e-05,
      "loss": 0.362,
      "step": 2020
    },
    {
      "epoch": 0.5944117647058823,
      "grad_norm": 0.05905544385313988,
      "learning_rate": 8.123711340206187e-05,
      "loss": 0.4363,
      "step": 2021
    },
    {
      "epoch": 0.5947058823529412,
      "grad_norm": 0.037786729633808136,
      "learning_rate": 8.117820324005891e-05,
      "loss": 0.2585,
      "step": 2022
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.057399436831474304,
      "learning_rate": 8.111929307805597e-05,
      "loss": 0.3694,
      "step": 2023
    },
    {
      "epoch": 0.5952941176470589,
      "grad_norm": 0.03852524608373642,
      "learning_rate": 8.106038291605302e-05,
      "loss": 0.3504,
      "step": 2024
    },
    {
      "epoch": 0.5955882352941176,
      "grad_norm": 0.04400065541267395,
      "learning_rate": 8.100147275405008e-05,
      "loss": 0.4066,
      "step": 2025
    },
    {
      "epoch": 0.5958823529411764,
      "grad_norm": 0.05736970156431198,
      "learning_rate": 8.094256259204714e-05,
      "loss": 0.4464,
      "step": 2026
    },
    {
      "epoch": 0.5961764705882353,
      "grad_norm": 0.04169071838259697,
      "learning_rate": 8.088365243004419e-05,
      "loss": 0.2747,
      "step": 2027
    },
    {
      "epoch": 0.5964705882352941,
      "grad_norm": 0.058144617825746536,
      "learning_rate": 8.082474226804125e-05,
      "loss": 0.4054,
      "step": 2028
    },
    {
      "epoch": 0.596764705882353,
      "grad_norm": 0.06744340062141418,
      "learning_rate": 8.076583210603829e-05,
      "loss": 0.3564,
      "step": 2029
    },
    {
      "epoch": 0.5970588235294118,
      "grad_norm": 0.05113789439201355,
      "learning_rate": 8.070692194403535e-05,
      "loss": 0.3412,
      "step": 2030
    },
    {
      "epoch": 0.5973529411764706,
      "grad_norm": 0.050988759845495224,
      "learning_rate": 8.064801178203241e-05,
      "loss": 0.3321,
      "step": 2031
    },
    {
      "epoch": 0.5976470588235294,
      "grad_norm": 0.05167427286505699,
      "learning_rate": 8.058910162002946e-05,
      "loss": 0.3851,
      "step": 2032
    },
    {
      "epoch": 0.5979411764705882,
      "grad_norm": 0.059736624360084534,
      "learning_rate": 8.053019145802652e-05,
      "loss": 0.3626,
      "step": 2033
    },
    {
      "epoch": 0.5982352941176471,
      "grad_norm": 0.046355802565813065,
      "learning_rate": 8.047128129602357e-05,
      "loss": 0.2832,
      "step": 2034
    },
    {
      "epoch": 0.5985294117647059,
      "grad_norm": 0.045304398983716965,
      "learning_rate": 8.041237113402063e-05,
      "loss": 0.3386,
      "step": 2035
    },
    {
      "epoch": 0.5988235294117648,
      "grad_norm": 0.056560926139354706,
      "learning_rate": 8.035346097201769e-05,
      "loss": 0.363,
      "step": 2036
    },
    {
      "epoch": 0.5991176470588235,
      "grad_norm": 0.05823887512087822,
      "learning_rate": 8.029455081001473e-05,
      "loss": 0.3399,
      "step": 2037
    },
    {
      "epoch": 0.5994117647058823,
      "grad_norm": 0.0498640201985836,
      "learning_rate": 8.023564064801179e-05,
      "loss": 0.3579,
      "step": 2038
    },
    {
      "epoch": 0.5997058823529412,
      "grad_norm": 0.04093145579099655,
      "learning_rate": 8.017673048600884e-05,
      "loss": 0.3526,
      "step": 2039
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.04457459598779678,
      "learning_rate": 8.01178203240059e-05,
      "loss": 0.324,
      "step": 2040
    },
    {
      "epoch": 0.6002941176470589,
      "grad_norm": 0.04852496087551117,
      "learning_rate": 8.005891016200296e-05,
      "loss": 0.3339,
      "step": 2041
    },
    {
      "epoch": 0.6005882352941176,
      "grad_norm": 0.05269456282258034,
      "learning_rate": 8e-05,
      "loss": 0.3334,
      "step": 2042
    },
    {
      "epoch": 0.6008823529411764,
      "grad_norm": 0.06658697873353958,
      "learning_rate": 7.994108983799707e-05,
      "loss": 0.392,
      "step": 2043
    },
    {
      "epoch": 0.6011764705882353,
      "grad_norm": 0.044806186109781265,
      "learning_rate": 7.988217967599411e-05,
      "loss": 0.3012,
      "step": 2044
    },
    {
      "epoch": 0.6014705882352941,
      "grad_norm": 0.06194525584578514,
      "learning_rate": 7.982326951399117e-05,
      "loss": 0.3777,
      "step": 2045
    },
    {
      "epoch": 0.601764705882353,
      "grad_norm": 0.049104757606983185,
      "learning_rate": 7.976435935198823e-05,
      "loss": 0.3594,
      "step": 2046
    },
    {
      "epoch": 0.6020588235294118,
      "grad_norm": 0.048611003905534744,
      "learning_rate": 7.970544918998528e-05,
      "loss": 0.3148,
      "step": 2047
    },
    {
      "epoch": 0.6023529411764705,
      "grad_norm": 0.060652002692222595,
      "learning_rate": 7.964653902798233e-05,
      "loss": 0.3728,
      "step": 2048
    },
    {
      "epoch": 0.6026470588235294,
      "grad_norm": 0.042654916644096375,
      "learning_rate": 7.958762886597937e-05,
      "loss": 0.2605,
      "step": 2049
    },
    {
      "epoch": 0.6029411764705882,
      "grad_norm": 0.05899283289909363,
      "learning_rate": 7.952871870397643e-05,
      "loss": 0.3715,
      "step": 2050
    },
    {
      "epoch": 0.6032352941176471,
      "grad_norm": 0.04928595572710037,
      "learning_rate": 7.94698085419735e-05,
      "loss": 0.3352,
      "step": 2051
    },
    {
      "epoch": 0.6035294117647059,
      "grad_norm": 0.04423507675528526,
      "learning_rate": 7.941089837997054e-05,
      "loss": 0.3124,
      "step": 2052
    },
    {
      "epoch": 0.6038235294117648,
      "grad_norm": 0.05728420987725258,
      "learning_rate": 7.93519882179676e-05,
      "loss": 0.3961,
      "step": 2053
    },
    {
      "epoch": 0.6041176470588235,
      "grad_norm": 0.048474013805389404,
      "learning_rate": 7.929307805596465e-05,
      "loss": 0.3745,
      "step": 2054
    },
    {
      "epoch": 0.6044117647058823,
      "grad_norm": 0.047051507979631424,
      "learning_rate": 7.923416789396171e-05,
      "loss": 0.3432,
      "step": 2055
    },
    {
      "epoch": 0.6047058823529412,
      "grad_norm": 0.06228096783161163,
      "learning_rate": 7.917525773195877e-05,
      "loss": 0.3729,
      "step": 2056
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.06175258383154869,
      "learning_rate": 7.911634756995581e-05,
      "loss": 0.4147,
      "step": 2057
    },
    {
      "epoch": 0.6052941176470589,
      "grad_norm": 0.04330616444349289,
      "learning_rate": 7.905743740795287e-05,
      "loss": 0.3359,
      "step": 2058
    },
    {
      "epoch": 0.6055882352941176,
      "grad_norm": 0.057967666536569595,
      "learning_rate": 7.899852724594992e-05,
      "loss": 0.3404,
      "step": 2059
    },
    {
      "epoch": 0.6058823529411764,
      "grad_norm": 0.04735414311289787,
      "learning_rate": 7.893961708394698e-05,
      "loss": 0.2737,
      "step": 2060
    },
    {
      "epoch": 0.6061764705882353,
      "grad_norm": 0.04661096632480621,
      "learning_rate": 7.888070692194404e-05,
      "loss": 0.304,
      "step": 2061
    },
    {
      "epoch": 0.6064705882352941,
      "grad_norm": 0.03316580504179001,
      "learning_rate": 7.882179675994109e-05,
      "loss": 0.2258,
      "step": 2062
    },
    {
      "epoch": 0.606764705882353,
      "grad_norm": 0.03544285520911217,
      "learning_rate": 7.876288659793815e-05,
      "loss": 0.2647,
      "step": 2063
    },
    {
      "epoch": 0.6070588235294118,
      "grad_norm": 0.051622424274683,
      "learning_rate": 7.87039764359352e-05,
      "loss": 0.2744,
      "step": 2064
    },
    {
      "epoch": 0.6073529411764705,
      "grad_norm": 0.05752810090780258,
      "learning_rate": 7.864506627393225e-05,
      "loss": 0.3324,
      "step": 2065
    },
    {
      "epoch": 0.6076470588235294,
      "grad_norm": 0.05535097047686577,
      "learning_rate": 7.858615611192931e-05,
      "loss": 0.3677,
      "step": 2066
    },
    {
      "epoch": 0.6079411764705882,
      "grad_norm": 0.057833489030599594,
      "learning_rate": 7.852724594992636e-05,
      "loss": 0.409,
      "step": 2067
    },
    {
      "epoch": 0.6082352941176471,
      "grad_norm": 0.0479150116443634,
      "learning_rate": 7.846833578792342e-05,
      "loss": 0.3404,
      "step": 2068
    },
    {
      "epoch": 0.6085294117647059,
      "grad_norm": 0.041219405829906464,
      "learning_rate": 7.840942562592047e-05,
      "loss": 0.3687,
      "step": 2069
    },
    {
      "epoch": 0.6088235294117647,
      "grad_norm": 0.05248310789465904,
      "learning_rate": 7.835051546391753e-05,
      "loss": 0.3868,
      "step": 2070
    },
    {
      "epoch": 0.6091176470588235,
      "grad_norm": 0.05020814761519432,
      "learning_rate": 7.829160530191459e-05,
      "loss": 0.3611,
      "step": 2071
    },
    {
      "epoch": 0.6094117647058823,
      "grad_norm": 0.05838680639863014,
      "learning_rate": 7.823269513991164e-05,
      "loss": 0.3655,
      "step": 2072
    },
    {
      "epoch": 0.6097058823529412,
      "grad_norm": 0.04708879441022873,
      "learning_rate": 7.81737849779087e-05,
      "loss": 0.3175,
      "step": 2073
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.045376650989055634,
      "learning_rate": 7.811487481590574e-05,
      "loss": 0.3511,
      "step": 2074
    },
    {
      "epoch": 0.6102941176470589,
      "grad_norm": 0.04860186204314232,
      "learning_rate": 7.80559646539028e-05,
      "loss": 0.3706,
      "step": 2075
    },
    {
      "epoch": 0.6105882352941177,
      "grad_norm": 0.04058702662587166,
      "learning_rate": 7.799705449189986e-05,
      "loss": 0.3017,
      "step": 2076
    },
    {
      "epoch": 0.6108823529411764,
      "grad_norm": 0.04373373091220856,
      "learning_rate": 7.793814432989691e-05,
      "loss": 0.3326,
      "step": 2077
    },
    {
      "epoch": 0.6111764705882353,
      "grad_norm": 0.046844568103551865,
      "learning_rate": 7.787923416789397e-05,
      "loss": 0.2683,
      "step": 2078
    },
    {
      "epoch": 0.6114705882352941,
      "grad_norm": 0.04328661039471626,
      "learning_rate": 7.782032400589102e-05,
      "loss": 0.3025,
      "step": 2079
    },
    {
      "epoch": 0.611764705882353,
      "grad_norm": 0.047542866319417953,
      "learning_rate": 7.776141384388808e-05,
      "loss": 0.3234,
      "step": 2080
    },
    {
      "epoch": 0.6120588235294118,
      "grad_norm": 0.06645889580249786,
      "learning_rate": 7.770250368188514e-05,
      "loss": 0.3664,
      "step": 2081
    },
    {
      "epoch": 0.6123529411764705,
      "grad_norm": 0.05878058820962906,
      "learning_rate": 7.764359351988218e-05,
      "loss": 0.4089,
      "step": 2082
    },
    {
      "epoch": 0.6126470588235294,
      "grad_norm": 0.04402511194348335,
      "learning_rate": 7.758468335787924e-05,
      "loss": 0.3547,
      "step": 2083
    },
    {
      "epoch": 0.6129411764705882,
      "grad_norm": 0.04273422807455063,
      "learning_rate": 7.752577319587629e-05,
      "loss": 0.335,
      "step": 2084
    },
    {
      "epoch": 0.6132352941176471,
      "grad_norm": 0.059213872998952866,
      "learning_rate": 7.746686303387335e-05,
      "loss": 0.3335,
      "step": 2085
    },
    {
      "epoch": 0.6135294117647059,
      "grad_norm": 0.06442298740148544,
      "learning_rate": 7.740795287187041e-05,
      "loss": 0.328,
      "step": 2086
    },
    {
      "epoch": 0.6138235294117647,
      "grad_norm": 0.04669850319623947,
      "learning_rate": 7.734904270986746e-05,
      "loss": 0.3339,
      "step": 2087
    },
    {
      "epoch": 0.6141176470588235,
      "grad_norm": 0.03576946631073952,
      "learning_rate": 7.729013254786452e-05,
      "loss": 0.3154,
      "step": 2088
    },
    {
      "epoch": 0.6144117647058823,
      "grad_norm": 0.04102576896548271,
      "learning_rate": 7.723122238586156e-05,
      "loss": 0.2993,
      "step": 2089
    },
    {
      "epoch": 0.6147058823529412,
      "grad_norm": 0.05995301902294159,
      "learning_rate": 7.717231222385862e-05,
      "loss": 0.4242,
      "step": 2090
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.041493430733680725,
      "learning_rate": 7.711340206185568e-05,
      "loss": 0.3158,
      "step": 2091
    },
    {
      "epoch": 0.6152941176470588,
      "grad_norm": 0.05936909094452858,
      "learning_rate": 7.705449189985273e-05,
      "loss": 0.4058,
      "step": 2092
    },
    {
      "epoch": 0.6155882352941177,
      "grad_norm": 0.05039326846599579,
      "learning_rate": 7.699558173784979e-05,
      "loss": 0.3382,
      "step": 2093
    },
    {
      "epoch": 0.6158823529411764,
      "grad_norm": 0.04213752970099449,
      "learning_rate": 7.693667157584684e-05,
      "loss": 0.3429,
      "step": 2094
    },
    {
      "epoch": 0.6161764705882353,
      "grad_norm": 0.05497639998793602,
      "learning_rate": 7.68777614138439e-05,
      "loss": 0.331,
      "step": 2095
    },
    {
      "epoch": 0.6164705882352941,
      "grad_norm": 0.05368391424417496,
      "learning_rate": 7.681885125184096e-05,
      "loss": 0.3893,
      "step": 2096
    },
    {
      "epoch": 0.616764705882353,
      "grad_norm": 0.054291460663080215,
      "learning_rate": 7.6759941089838e-05,
      "loss": 0.3525,
      "step": 2097
    },
    {
      "epoch": 0.6170588235294118,
      "grad_norm": 0.057917144149541855,
      "learning_rate": 7.670103092783506e-05,
      "loss": 0.3729,
      "step": 2098
    },
    {
      "epoch": 0.6173529411764705,
      "grad_norm": 0.04953235760331154,
      "learning_rate": 7.664212076583211e-05,
      "loss": 0.3859,
      "step": 2099
    },
    {
      "epoch": 0.6176470588235294,
      "grad_norm": 0.04276634752750397,
      "learning_rate": 7.658321060382917e-05,
      "loss": 0.3354,
      "step": 2100
    },
    {
      "epoch": 0.6176470588235294,
      "eval_loss": 0.3534764051437378,
      "eval_runtime": 215.0224,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 2100
    },
    {
      "epoch": 0.6179411764705882,
      "grad_norm": 0.04155172035098076,
      "learning_rate": 7.652430044182622e-05,
      "loss": 0.3401,
      "step": 2101
    },
    {
      "epoch": 0.6182352941176471,
      "grad_norm": 0.043698374181985855,
      "learning_rate": 7.646539027982326e-05,
      "loss": 0.3197,
      "step": 2102
    },
    {
      "epoch": 0.6185294117647059,
      "grad_norm": 0.04591674730181694,
      "learning_rate": 7.640648011782032e-05,
      "loss": 0.2897,
      "step": 2103
    },
    {
      "epoch": 0.6188235294117647,
      "grad_norm": 0.0543784499168396,
      "learning_rate": 7.634756995581737e-05,
      "loss": 0.3991,
      "step": 2104
    },
    {
      "epoch": 0.6191176470588236,
      "grad_norm": 0.03665482997894287,
      "learning_rate": 7.628865979381443e-05,
      "loss": 0.3071,
      "step": 2105
    },
    {
      "epoch": 0.6194117647058823,
      "grad_norm": 0.06070850044488907,
      "learning_rate": 7.622974963181149e-05,
      "loss": 0.389,
      "step": 2106
    },
    {
      "epoch": 0.6197058823529412,
      "grad_norm": 0.040925316512584686,
      "learning_rate": 7.617083946980854e-05,
      "loss": 0.3169,
      "step": 2107
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.03421996906399727,
      "learning_rate": 7.61119293078056e-05,
      "loss": 0.2826,
      "step": 2108
    },
    {
      "epoch": 0.6202941176470588,
      "grad_norm": 0.042490556836128235,
      "learning_rate": 7.605301914580264e-05,
      "loss": 0.3551,
      "step": 2109
    },
    {
      "epoch": 0.6205882352941177,
      "grad_norm": 0.05357617512345314,
      "learning_rate": 7.59941089837997e-05,
      "loss": 0.4037,
      "step": 2110
    },
    {
      "epoch": 0.6208823529411764,
      "grad_norm": 0.04278821125626564,
      "learning_rate": 7.593519882179676e-05,
      "loss": 0.334,
      "step": 2111
    },
    {
      "epoch": 0.6211764705882353,
      "grad_norm": 0.04137003421783447,
      "learning_rate": 7.587628865979381e-05,
      "loss": 0.3175,
      "step": 2112
    },
    {
      "epoch": 0.6214705882352941,
      "grad_norm": 0.036953914910554886,
      "learning_rate": 7.581737849779087e-05,
      "loss": 0.2681,
      "step": 2113
    },
    {
      "epoch": 0.6217647058823529,
      "grad_norm": 0.04175521433353424,
      "learning_rate": 7.575846833578792e-05,
      "loss": 0.3206,
      "step": 2114
    },
    {
      "epoch": 0.6220588235294118,
      "grad_norm": 0.04877163842320442,
      "learning_rate": 7.569955817378498e-05,
      "loss": 0.3225,
      "step": 2115
    },
    {
      "epoch": 0.6223529411764706,
      "grad_norm": 0.0504722073674202,
      "learning_rate": 7.564064801178204e-05,
      "loss": 0.381,
      "step": 2116
    },
    {
      "epoch": 0.6226470588235294,
      "grad_norm": 0.05380384624004364,
      "learning_rate": 7.558173784977909e-05,
      "loss": 0.3907,
      "step": 2117
    },
    {
      "epoch": 0.6229411764705882,
      "grad_norm": 0.04255110025405884,
      "learning_rate": 7.552282768777615e-05,
      "loss": 0.2706,
      "step": 2118
    },
    {
      "epoch": 0.6232352941176471,
      "grad_norm": 0.04862241446971893,
      "learning_rate": 7.546391752577319e-05,
      "loss": 0.416,
      "step": 2119
    },
    {
      "epoch": 0.6235294117647059,
      "grad_norm": 0.04078393802046776,
      "learning_rate": 7.540500736377025e-05,
      "loss": 0.3595,
      "step": 2120
    },
    {
      "epoch": 0.6238235294117647,
      "grad_norm": 0.06350640952587128,
      "learning_rate": 7.534609720176731e-05,
      "loss": 0.3461,
      "step": 2121
    },
    {
      "epoch": 0.6241176470588236,
      "grad_norm": 0.048709459602832794,
      "learning_rate": 7.528718703976436e-05,
      "loss": 0.3502,
      "step": 2122
    },
    {
      "epoch": 0.6244117647058823,
      "grad_norm": 0.04524727165699005,
      "learning_rate": 7.522827687776142e-05,
      "loss": 0.3131,
      "step": 2123
    },
    {
      "epoch": 0.6247058823529412,
      "grad_norm": 0.04980868101119995,
      "learning_rate": 7.516936671575847e-05,
      "loss": 0.3825,
      "step": 2124
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.04129970073699951,
      "learning_rate": 7.511045655375553e-05,
      "loss": 0.3061,
      "step": 2125
    },
    {
      "epoch": 0.6252941176470588,
      "grad_norm": 0.04749351367354393,
      "learning_rate": 7.505154639175259e-05,
      "loss": 0.2977,
      "step": 2126
    },
    {
      "epoch": 0.6255882352941177,
      "grad_norm": 0.05905672535300255,
      "learning_rate": 7.499263622974963e-05,
      "loss": 0.3697,
      "step": 2127
    },
    {
      "epoch": 0.6258823529411764,
      "grad_norm": 0.03944738581776619,
      "learning_rate": 7.493372606774669e-05,
      "loss": 0.2914,
      "step": 2128
    },
    {
      "epoch": 0.6261764705882353,
      "grad_norm": 0.059250395745038986,
      "learning_rate": 7.487481590574374e-05,
      "loss": 0.386,
      "step": 2129
    },
    {
      "epoch": 0.6264705882352941,
      "grad_norm": 0.05074477195739746,
      "learning_rate": 7.48159057437408e-05,
      "loss": 0.343,
      "step": 2130
    },
    {
      "epoch": 0.6267647058823529,
      "grad_norm": 0.03702859580516815,
      "learning_rate": 7.475699558173786e-05,
      "loss": 0.2993,
      "step": 2131
    },
    {
      "epoch": 0.6270588235294118,
      "grad_norm": 0.044660817831754684,
      "learning_rate": 7.46980854197349e-05,
      "loss": 0.3229,
      "step": 2132
    },
    {
      "epoch": 0.6273529411764706,
      "grad_norm": 0.038595061749219894,
      "learning_rate": 7.463917525773197e-05,
      "loss": 0.3271,
      "step": 2133
    },
    {
      "epoch": 0.6276470588235294,
      "grad_norm": 0.06955642253160477,
      "learning_rate": 7.458026509572901e-05,
      "loss": 0.4072,
      "step": 2134
    },
    {
      "epoch": 0.6279411764705882,
      "grad_norm": 0.04902977868914604,
      "learning_rate": 7.452135493372607e-05,
      "loss": 0.3674,
      "step": 2135
    },
    {
      "epoch": 0.6282352941176471,
      "grad_norm": 0.04198145866394043,
      "learning_rate": 7.446244477172313e-05,
      "loss": 0.3494,
      "step": 2136
    },
    {
      "epoch": 0.6285294117647059,
      "grad_norm": 0.053792569786310196,
      "learning_rate": 7.440353460972018e-05,
      "loss": 0.4052,
      "step": 2137
    },
    {
      "epoch": 0.6288235294117647,
      "grad_norm": 0.049765292555093765,
      "learning_rate": 7.434462444771724e-05,
      "loss": 0.3375,
      "step": 2138
    },
    {
      "epoch": 0.6291176470588236,
      "grad_norm": 0.038161903619766235,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.3003,
      "step": 2139
    },
    {
      "epoch": 0.6294117647058823,
      "grad_norm": 0.047957222908735275,
      "learning_rate": 7.422680412371135e-05,
      "loss": 0.4074,
      "step": 2140
    },
    {
      "epoch": 0.6297058823529412,
      "grad_norm": 0.04026848077774048,
      "learning_rate": 7.416789396170841e-05,
      "loss": 0.3363,
      "step": 2141
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.04097098484635353,
      "learning_rate": 7.410898379970545e-05,
      "loss": 0.3852,
      "step": 2142
    },
    {
      "epoch": 0.6302941176470588,
      "grad_norm": 0.04393375292420387,
      "learning_rate": 7.405007363770251e-05,
      "loss": 0.3024,
      "step": 2143
    },
    {
      "epoch": 0.6305882352941177,
      "grad_norm": 0.0490136593580246,
      "learning_rate": 7.399116347569956e-05,
      "loss": 0.3692,
      "step": 2144
    },
    {
      "epoch": 0.6308823529411764,
      "grad_norm": 0.043226439505815506,
      "learning_rate": 7.393225331369662e-05,
      "loss": 0.2932,
      "step": 2145
    },
    {
      "epoch": 0.6311764705882353,
      "grad_norm": 0.0419573150575161,
      "learning_rate": 7.387334315169368e-05,
      "loss": 0.3157,
      "step": 2146
    },
    {
      "epoch": 0.6314705882352941,
      "grad_norm": 0.0420064739882946,
      "learning_rate": 7.381443298969073e-05,
      "loss": 0.3363,
      "step": 2147
    },
    {
      "epoch": 0.6317647058823529,
      "grad_norm": 0.04367470741271973,
      "learning_rate": 7.375552282768779e-05,
      "loss": 0.2511,
      "step": 2148
    },
    {
      "epoch": 0.6320588235294118,
      "grad_norm": 0.04316267743706703,
      "learning_rate": 7.369661266568483e-05,
      "loss": 0.3583,
      "step": 2149
    },
    {
      "epoch": 0.6323529411764706,
      "grad_norm": 0.04091127589344978,
      "learning_rate": 7.36377025036819e-05,
      "loss": 0.3531,
      "step": 2150
    },
    {
      "epoch": 0.6326470588235295,
      "grad_norm": 0.03947354480624199,
      "learning_rate": 7.357879234167895e-05,
      "loss": 0.3147,
      "step": 2151
    },
    {
      "epoch": 0.6329411764705882,
      "grad_norm": 0.04999437928199768,
      "learning_rate": 7.3519882179676e-05,
      "loss": 0.3658,
      "step": 2152
    },
    {
      "epoch": 0.633235294117647,
      "grad_norm": 0.0544465146958828,
      "learning_rate": 7.346097201767306e-05,
      "loss": 0.4002,
      "step": 2153
    },
    {
      "epoch": 0.6335294117647059,
      "grad_norm": 0.04138043522834778,
      "learning_rate": 7.34020618556701e-05,
      "loss": 0.329,
      "step": 2154
    },
    {
      "epoch": 0.6338235294117647,
      "grad_norm": 0.043849553912878036,
      "learning_rate": 7.334315169366715e-05,
      "loss": 0.3065,
      "step": 2155
    },
    {
      "epoch": 0.6341176470588236,
      "grad_norm": 0.05635060742497444,
      "learning_rate": 7.328424153166421e-05,
      "loss": 0.4016,
      "step": 2156
    },
    {
      "epoch": 0.6344117647058823,
      "grad_norm": 0.05600282549858093,
      "learning_rate": 7.322533136966126e-05,
      "loss": 0.3434,
      "step": 2157
    },
    {
      "epoch": 0.6347058823529412,
      "grad_norm": 0.0342528410255909,
      "learning_rate": 7.316642120765832e-05,
      "loss": 0.2712,
      "step": 2158
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.04427991062402725,
      "learning_rate": 7.310751104565537e-05,
      "loss": 0.4085,
      "step": 2159
    },
    {
      "epoch": 0.6352941176470588,
      "grad_norm": 0.041920389980077744,
      "learning_rate": 7.304860088365243e-05,
      "loss": 0.331,
      "step": 2160
    },
    {
      "epoch": 0.6355882352941177,
      "grad_norm": 0.03618263080716133,
      "learning_rate": 7.298969072164949e-05,
      "loss": 0.3048,
      "step": 2161
    },
    {
      "epoch": 0.6358823529411765,
      "grad_norm": 0.054422397166490555,
      "learning_rate": 7.293078055964654e-05,
      "loss": 0.3596,
      "step": 2162
    },
    {
      "epoch": 0.6361764705882353,
      "grad_norm": 0.052660662680864334,
      "learning_rate": 7.28718703976436e-05,
      "loss": 0.4066,
      "step": 2163
    },
    {
      "epoch": 0.6364705882352941,
      "grad_norm": 0.0412047915160656,
      "learning_rate": 7.281296023564064e-05,
      "loss": 0.3034,
      "step": 2164
    },
    {
      "epoch": 0.6367647058823529,
      "grad_norm": 0.040625497698783875,
      "learning_rate": 7.27540500736377e-05,
      "loss": 0.3557,
      "step": 2165
    },
    {
      "epoch": 0.6370588235294118,
      "grad_norm": 0.06082993000745773,
      "learning_rate": 7.269513991163476e-05,
      "loss": 0.3927,
      "step": 2166
    },
    {
      "epoch": 0.6373529411764706,
      "grad_norm": 0.03306960687041283,
      "learning_rate": 7.263622974963181e-05,
      "loss": 0.2327,
      "step": 2167
    },
    {
      "epoch": 0.6376470588235295,
      "grad_norm": 0.038695983588695526,
      "learning_rate": 7.257731958762887e-05,
      "loss": 0.3523,
      "step": 2168
    },
    {
      "epoch": 0.6379411764705882,
      "grad_norm": 0.04760926961898804,
      "learning_rate": 7.251840942562592e-05,
      "loss": 0.3948,
      "step": 2169
    },
    {
      "epoch": 0.638235294117647,
      "grad_norm": 0.05059647560119629,
      "learning_rate": 7.245949926362298e-05,
      "loss": 0.3095,
      "step": 2170
    },
    {
      "epoch": 0.6385294117647059,
      "grad_norm": 0.05369235575199127,
      "learning_rate": 7.240058910162004e-05,
      "loss": 0.3822,
      "step": 2171
    },
    {
      "epoch": 0.6388235294117647,
      "grad_norm": 0.041345398873090744,
      "learning_rate": 7.234167893961708e-05,
      "loss": 0.3029,
      "step": 2172
    },
    {
      "epoch": 0.6391176470588236,
      "grad_norm": 0.05283002927899361,
      "learning_rate": 7.228276877761414e-05,
      "loss": 0.3691,
      "step": 2173
    },
    {
      "epoch": 0.6394117647058823,
      "grad_norm": 0.04373466968536377,
      "learning_rate": 7.222385861561119e-05,
      "loss": 0.3731,
      "step": 2174
    },
    {
      "epoch": 0.6397058823529411,
      "grad_norm": 0.03868809714913368,
      "learning_rate": 7.216494845360825e-05,
      "loss": 0.3083,
      "step": 2175
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.048817820847034454,
      "learning_rate": 7.210603829160531e-05,
      "loss": 0.3543,
      "step": 2176
    },
    {
      "epoch": 0.6402941176470588,
      "grad_norm": 0.052159544080495834,
      "learning_rate": 7.204712812960236e-05,
      "loss": 0.3504,
      "step": 2177
    },
    {
      "epoch": 0.6405882352941177,
      "grad_norm": 0.0503532700240612,
      "learning_rate": 7.198821796759942e-05,
      "loss": 0.4097,
      "step": 2178
    },
    {
      "epoch": 0.6408823529411765,
      "grad_norm": 0.05262477695941925,
      "learning_rate": 7.192930780559646e-05,
      "loss": 0.339,
      "step": 2179
    },
    {
      "epoch": 0.6411764705882353,
      "grad_norm": 0.047808632254600525,
      "learning_rate": 7.187039764359352e-05,
      "loss": 0.2768,
      "step": 2180
    },
    {
      "epoch": 0.6414705882352941,
      "grad_norm": 0.05010617524385452,
      "learning_rate": 7.181148748159058e-05,
      "loss": 0.3933,
      "step": 2181
    },
    {
      "epoch": 0.6417647058823529,
      "grad_norm": 0.05531347543001175,
      "learning_rate": 7.175257731958763e-05,
      "loss": 0.4085,
      "step": 2182
    },
    {
      "epoch": 0.6420588235294118,
      "grad_norm": 0.04091707244515419,
      "learning_rate": 7.169366715758469e-05,
      "loss": 0.327,
      "step": 2183
    },
    {
      "epoch": 0.6423529411764706,
      "grad_norm": 0.03681588172912598,
      "learning_rate": 7.163475699558174e-05,
      "loss": 0.301,
      "step": 2184
    },
    {
      "epoch": 0.6426470588235295,
      "grad_norm": 0.04061538353562355,
      "learning_rate": 7.15758468335788e-05,
      "loss": 0.3776,
      "step": 2185
    },
    {
      "epoch": 0.6429411764705882,
      "grad_norm": 0.044503942131996155,
      "learning_rate": 7.151693667157586e-05,
      "loss": 0.3507,
      "step": 2186
    },
    {
      "epoch": 0.643235294117647,
      "grad_norm": 0.037266287952661514,
      "learning_rate": 7.14580265095729e-05,
      "loss": 0.2857,
      "step": 2187
    },
    {
      "epoch": 0.6435294117647059,
      "grad_norm": 0.05218048021197319,
      "learning_rate": 7.139911634756996e-05,
      "loss": 0.3912,
      "step": 2188
    },
    {
      "epoch": 0.6438235294117647,
      "grad_norm": 0.04831937700510025,
      "learning_rate": 7.134020618556701e-05,
      "loss": 0.3793,
      "step": 2189
    },
    {
      "epoch": 0.6441176470588236,
      "grad_norm": 0.03881898522377014,
      "learning_rate": 7.128129602356407e-05,
      "loss": 0.3035,
      "step": 2190
    },
    {
      "epoch": 0.6444117647058824,
      "grad_norm": 0.039119426161050797,
      "learning_rate": 7.122238586156113e-05,
      "loss": 0.3031,
      "step": 2191
    },
    {
      "epoch": 0.6447058823529411,
      "grad_norm": 0.04771527647972107,
      "learning_rate": 7.116347569955818e-05,
      "loss": 0.3662,
      "step": 2192
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.05228780582547188,
      "learning_rate": 7.110456553755524e-05,
      "loss": 0.3654,
      "step": 2193
    },
    {
      "epoch": 0.6452941176470588,
      "grad_norm": 0.0490148700773716,
      "learning_rate": 7.104565537555228e-05,
      "loss": 0.3318,
      "step": 2194
    },
    {
      "epoch": 0.6455882352941177,
      "grad_norm": 0.041326023638248444,
      "learning_rate": 7.098674521354934e-05,
      "loss": 0.3104,
      "step": 2195
    },
    {
      "epoch": 0.6458823529411765,
      "grad_norm": 0.04334895685315132,
      "learning_rate": 7.09278350515464e-05,
      "loss": 0.3603,
      "step": 2196
    },
    {
      "epoch": 0.6461764705882352,
      "grad_norm": 0.056428372859954834,
      "learning_rate": 7.086892488954345e-05,
      "loss": 0.3902,
      "step": 2197
    },
    {
      "epoch": 0.6464705882352941,
      "grad_norm": 0.07016345113515854,
      "learning_rate": 7.081001472754051e-05,
      "loss": 0.4037,
      "step": 2198
    },
    {
      "epoch": 0.6467647058823529,
      "grad_norm": 0.04581325873732567,
      "learning_rate": 7.075110456553756e-05,
      "loss": 0.347,
      "step": 2199
    },
    {
      "epoch": 0.6470588235294118,
      "grad_norm": 0.047395043075084686,
      "learning_rate": 7.069219440353462e-05,
      "loss": 0.3676,
      "step": 2200
    },
    {
      "epoch": 0.6470588235294118,
      "eval_loss": 0.3530646860599518,
      "eval_runtime": 214.9869,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 2200
    },
    {
      "epoch": 0.6473529411764706,
      "grad_norm": 0.04729752615094185,
      "learning_rate": 7.063328424153168e-05,
      "loss": 0.3693,
      "step": 2201
    },
    {
      "epoch": 0.6476470588235295,
      "grad_norm": 0.047937165945768356,
      "learning_rate": 7.057437407952873e-05,
      "loss": 0.4147,
      "step": 2202
    },
    {
      "epoch": 0.6479411764705882,
      "grad_norm": 0.04685954377055168,
      "learning_rate": 7.051546391752579e-05,
      "loss": 0.3635,
      "step": 2203
    },
    {
      "epoch": 0.648235294117647,
      "grad_norm": 0.060719504952430725,
      "learning_rate": 7.045655375552283e-05,
      "loss": 0.4803,
      "step": 2204
    },
    {
      "epoch": 0.6485294117647059,
      "grad_norm": 0.05371445417404175,
      "learning_rate": 7.039764359351989e-05,
      "loss": 0.3999,
      "step": 2205
    },
    {
      "epoch": 0.6488235294117647,
      "grad_norm": 0.04827180877327919,
      "learning_rate": 7.033873343151695e-05,
      "loss": 0.3364,
      "step": 2206
    },
    {
      "epoch": 0.6491176470588236,
      "grad_norm": 0.05287761613726616,
      "learning_rate": 7.027982326951399e-05,
      "loss": 0.3723,
      "step": 2207
    },
    {
      "epoch": 0.6494117647058824,
      "grad_norm": 0.05071207135915756,
      "learning_rate": 7.022091310751105e-05,
      "loss": 0.339,
      "step": 2208
    },
    {
      "epoch": 0.6497058823529411,
      "grad_norm": 0.049767304211854935,
      "learning_rate": 7.016200294550809e-05,
      "loss": 0.286,
      "step": 2209
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.04573359712958336,
      "learning_rate": 7.010309278350515e-05,
      "loss": 0.3016,
      "step": 2210
    },
    {
      "epoch": 0.6502941176470588,
      "grad_norm": 0.055387936532497406,
      "learning_rate": 7.004418262150221e-05,
      "loss": 0.3831,
      "step": 2211
    },
    {
      "epoch": 0.6505882352941177,
      "grad_norm": 0.04935291036963463,
      "learning_rate": 6.998527245949926e-05,
      "loss": 0.3125,
      "step": 2212
    },
    {
      "epoch": 0.6508823529411765,
      "grad_norm": 0.054282013326883316,
      "learning_rate": 6.992636229749632e-05,
      "loss": 0.3891,
      "step": 2213
    },
    {
      "epoch": 0.6511764705882352,
      "grad_norm": 0.041351545602083206,
      "learning_rate": 6.986745213549337e-05,
      "loss": 0.342,
      "step": 2214
    },
    {
      "epoch": 0.6514705882352941,
      "grad_norm": 0.0484379343688488,
      "learning_rate": 6.980854197349043e-05,
      "loss": 0.3779,
      "step": 2215
    },
    {
      "epoch": 0.6517647058823529,
      "grad_norm": 0.034574251621961594,
      "learning_rate": 6.974963181148749e-05,
      "loss": 0.2756,
      "step": 2216
    },
    {
      "epoch": 0.6520588235294118,
      "grad_norm": 0.03877989202737808,
      "learning_rate": 6.969072164948453e-05,
      "loss": 0.3047,
      "step": 2217
    },
    {
      "epoch": 0.6523529411764706,
      "grad_norm": 0.02906850166618824,
      "learning_rate": 6.963181148748159e-05,
      "loss": 0.2246,
      "step": 2218
    },
    {
      "epoch": 0.6526470588235294,
      "grad_norm": 0.04812713339924812,
      "learning_rate": 6.957290132547864e-05,
      "loss": 0.4117,
      "step": 2219
    },
    {
      "epoch": 0.6529411764705882,
      "grad_norm": 0.05266563594341278,
      "learning_rate": 6.95139911634757e-05,
      "loss": 0.3734,
      "step": 2220
    },
    {
      "epoch": 0.653235294117647,
      "grad_norm": 0.060656215995550156,
      "learning_rate": 6.945508100147276e-05,
      "loss": 0.3733,
      "step": 2221
    },
    {
      "epoch": 0.6535294117647059,
      "grad_norm": 0.04444660618901253,
      "learning_rate": 6.93961708394698e-05,
      "loss": 0.3855,
      "step": 2222
    },
    {
      "epoch": 0.6538235294117647,
      "grad_norm": 0.03995945304632187,
      "learning_rate": 6.933726067746687e-05,
      "loss": 0.3595,
      "step": 2223
    },
    {
      "epoch": 0.6541176470588236,
      "grad_norm": 0.04781213775277138,
      "learning_rate": 6.927835051546391e-05,
      "loss": 0.3829,
      "step": 2224
    },
    {
      "epoch": 0.6544117647058824,
      "grad_norm": 0.042446110397577286,
      "learning_rate": 6.921944035346097e-05,
      "loss": 0.3297,
      "step": 2225
    },
    {
      "epoch": 0.6547058823529411,
      "grad_norm": 0.04320085048675537,
      "learning_rate": 6.916053019145803e-05,
      "loss": 0.337,
      "step": 2226
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.0606694370508194,
      "learning_rate": 6.910162002945508e-05,
      "loss": 0.4403,
      "step": 2227
    },
    {
      "epoch": 0.6552941176470588,
      "grad_norm": 0.047455787658691406,
      "learning_rate": 6.904270986745214e-05,
      "loss": 0.378,
      "step": 2228
    },
    {
      "epoch": 0.6555882352941177,
      "grad_norm": 0.045587025582790375,
      "learning_rate": 6.898379970544919e-05,
      "loss": 0.4081,
      "step": 2229
    },
    {
      "epoch": 0.6558823529411765,
      "grad_norm": 0.033525463193655014,
      "learning_rate": 6.892488954344625e-05,
      "loss": 0.3077,
      "step": 2230
    },
    {
      "epoch": 0.6561764705882352,
      "grad_norm": 0.04624728858470917,
      "learning_rate": 6.886597938144331e-05,
      "loss": 0.3801,
      "step": 2231
    },
    {
      "epoch": 0.6564705882352941,
      "grad_norm": 0.04876461997628212,
      "learning_rate": 6.880706921944035e-05,
      "loss": 0.3287,
      "step": 2232
    },
    {
      "epoch": 0.6567647058823529,
      "grad_norm": 0.04429437965154648,
      "learning_rate": 6.874815905743741e-05,
      "loss": 0.3704,
      "step": 2233
    },
    {
      "epoch": 0.6570588235294118,
      "grad_norm": 0.03993796184659004,
      "learning_rate": 6.868924889543446e-05,
      "loss": 0.2965,
      "step": 2234
    },
    {
      "epoch": 0.6573529411764706,
      "grad_norm": 0.041623491793870926,
      "learning_rate": 6.863033873343152e-05,
      "loss": 0.3099,
      "step": 2235
    },
    {
      "epoch": 0.6576470588235294,
      "grad_norm": 0.060110755264759064,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.3959,
      "step": 2236
    },
    {
      "epoch": 0.6579411764705883,
      "grad_norm": 0.056909121572971344,
      "learning_rate": 6.851251840942563e-05,
      "loss": 0.3863,
      "step": 2237
    },
    {
      "epoch": 0.658235294117647,
      "grad_norm": 0.039657190442085266,
      "learning_rate": 6.845360824742269e-05,
      "loss": 0.2788,
      "step": 2238
    },
    {
      "epoch": 0.6585294117647059,
      "grad_norm": 0.049306344240903854,
      "learning_rate": 6.839469808541973e-05,
      "loss": 0.4602,
      "step": 2239
    },
    {
      "epoch": 0.6588235294117647,
      "grad_norm": 0.05535971373319626,
      "learning_rate": 6.83357879234168e-05,
      "loss": 0.352,
      "step": 2240
    },
    {
      "epoch": 0.6591176470588235,
      "grad_norm": 0.06604823470115662,
      "learning_rate": 6.827687776141385e-05,
      "loss": 0.4438,
      "step": 2241
    },
    {
      "epoch": 0.6594117647058824,
      "grad_norm": 0.04544205218553543,
      "learning_rate": 6.82179675994109e-05,
      "loss": 0.3459,
      "step": 2242
    },
    {
      "epoch": 0.6597058823529411,
      "grad_norm": 0.049410056322813034,
      "learning_rate": 6.815905743740796e-05,
      "loss": 0.3618,
      "step": 2243
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.03853505477309227,
      "learning_rate": 6.810014727540501e-05,
      "loss": 0.2885,
      "step": 2244
    },
    {
      "epoch": 0.6602941176470588,
      "grad_norm": 0.03569396212697029,
      "learning_rate": 6.804123711340207e-05,
      "loss": 0.317,
      "step": 2245
    },
    {
      "epoch": 0.6605882352941177,
      "grad_norm": 0.052684418857097626,
      "learning_rate": 6.798232695139913e-05,
      "loss": 0.3824,
      "step": 2246
    },
    {
      "epoch": 0.6608823529411765,
      "grad_norm": 0.03850993514060974,
      "learning_rate": 6.792341678939617e-05,
      "loss": 0.2797,
      "step": 2247
    },
    {
      "epoch": 0.6611764705882353,
      "grad_norm": 0.0486142672598362,
      "learning_rate": 6.786450662739324e-05,
      "loss": 0.3767,
      "step": 2248
    },
    {
      "epoch": 0.6614705882352941,
      "grad_norm": 0.04695849120616913,
      "learning_rate": 6.780559646539028e-05,
      "loss": 0.3739,
      "step": 2249
    },
    {
      "epoch": 0.6617647058823529,
      "grad_norm": 0.04297341778874397,
      "learning_rate": 6.774668630338734e-05,
      "loss": 0.271,
      "step": 2250
    },
    {
      "epoch": 0.6620588235294118,
      "grad_norm": 0.045693133026361465,
      "learning_rate": 6.76877761413844e-05,
      "loss": 0.3685,
      "step": 2251
    },
    {
      "epoch": 0.6623529411764706,
      "grad_norm": 0.0494963601231575,
      "learning_rate": 6.762886597938145e-05,
      "loss": 0.3567,
      "step": 2252
    },
    {
      "epoch": 0.6626470588235294,
      "grad_norm": 0.035083431750535965,
      "learning_rate": 6.756995581737851e-05,
      "loss": 0.2496,
      "step": 2253
    },
    {
      "epoch": 0.6629411764705883,
      "grad_norm": 0.04389122128486633,
      "learning_rate": 6.751104565537556e-05,
      "loss": 0.3857,
      "step": 2254
    },
    {
      "epoch": 0.663235294117647,
      "grad_norm": 0.05071960389614105,
      "learning_rate": 6.745213549337262e-05,
      "loss": 0.3192,
      "step": 2255
    },
    {
      "epoch": 0.6635294117647059,
      "grad_norm": 0.0431019626557827,
      "learning_rate": 6.739322533136968e-05,
      "loss": 0.3248,
      "step": 2256
    },
    {
      "epoch": 0.6638235294117647,
      "grad_norm": 0.045183416455984116,
      "learning_rate": 6.733431516936672e-05,
      "loss": 0.3477,
      "step": 2257
    },
    {
      "epoch": 0.6641176470588235,
      "grad_norm": 0.062268029898405075,
      "learning_rate": 6.727540500736378e-05,
      "loss": 0.357,
      "step": 2258
    },
    {
      "epoch": 0.6644117647058824,
      "grad_norm": 0.0449410118162632,
      "learning_rate": 6.721649484536083e-05,
      "loss": 0.288,
      "step": 2259
    },
    {
      "epoch": 0.6647058823529411,
      "grad_norm": 0.05368579924106598,
      "learning_rate": 6.715758468335788e-05,
      "loss": 0.3675,
      "step": 2260
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.03833475708961487,
      "learning_rate": 6.709867452135494e-05,
      "loss": 0.3044,
      "step": 2261
    },
    {
      "epoch": 0.6652941176470588,
      "grad_norm": 0.03522751107811928,
      "learning_rate": 6.703976435935198e-05,
      "loss": 0.3045,
      "step": 2262
    },
    {
      "epoch": 0.6655882352941176,
      "grad_norm": 0.048915717750787735,
      "learning_rate": 6.698085419734904e-05,
      "loss": 0.3514,
      "step": 2263
    },
    {
      "epoch": 0.6658823529411765,
      "grad_norm": 0.050575897097587585,
      "learning_rate": 6.692194403534609e-05,
      "loss": 0.3321,
      "step": 2264
    },
    {
      "epoch": 0.6661764705882353,
      "grad_norm": 0.03771240636706352,
      "learning_rate": 6.686303387334315e-05,
      "loss": 0.3309,
      "step": 2265
    },
    {
      "epoch": 0.6664705882352941,
      "grad_norm": 0.06139042600989342,
      "learning_rate": 6.680412371134021e-05,
      "loss": 0.4018,
      "step": 2266
    },
    {
      "epoch": 0.6667647058823529,
      "grad_norm": 0.049530647695064545,
      "learning_rate": 6.674521354933726e-05,
      "loss": 0.3298,
      "step": 2267
    },
    {
      "epoch": 0.6670588235294118,
      "grad_norm": 0.05610969290137291,
      "learning_rate": 6.668630338733432e-05,
      "loss": 0.3753,
      "step": 2268
    },
    {
      "epoch": 0.6673529411764706,
      "grad_norm": 0.06522420793771744,
      "learning_rate": 6.662739322533136e-05,
      "loss": 0.4692,
      "step": 2269
    },
    {
      "epoch": 0.6676470588235294,
      "grad_norm": 0.03893105313181877,
      "learning_rate": 6.656848306332842e-05,
      "loss": 0.2969,
      "step": 2270
    },
    {
      "epoch": 0.6679411764705883,
      "grad_norm": 0.07742875814437866,
      "learning_rate": 6.650957290132548e-05,
      "loss": 0.463,
      "step": 2271
    },
    {
      "epoch": 0.668235294117647,
      "grad_norm": 0.03377201408147812,
      "learning_rate": 6.645066273932253e-05,
      "loss": 0.2676,
      "step": 2272
    },
    {
      "epoch": 0.6685294117647059,
      "grad_norm": 0.038513410836458206,
      "learning_rate": 6.639175257731959e-05,
      "loss": 0.36,
      "step": 2273
    },
    {
      "epoch": 0.6688235294117647,
      "grad_norm": 0.05109480023384094,
      "learning_rate": 6.633284241531664e-05,
      "loss": 0.3123,
      "step": 2274
    },
    {
      "epoch": 0.6691176470588235,
      "grad_norm": 0.043595705181360245,
      "learning_rate": 6.62739322533137e-05,
      "loss": 0.3293,
      "step": 2275
    },
    {
      "epoch": 0.6694117647058824,
      "grad_norm": 0.057802923023700714,
      "learning_rate": 6.621502209131076e-05,
      "loss": 0.3366,
      "step": 2276
    },
    {
      "epoch": 0.6697058823529412,
      "grad_norm": 0.038530245423316956,
      "learning_rate": 6.61561119293078e-05,
      "loss": 0.3463,
      "step": 2277
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.04922235384583473,
      "learning_rate": 6.609720176730486e-05,
      "loss": 0.3075,
      "step": 2278
    },
    {
      "epoch": 0.6702941176470588,
      "grad_norm": 0.05288402736186981,
      "learning_rate": 6.603829160530191e-05,
      "loss": 0.3415,
      "step": 2279
    },
    {
      "epoch": 0.6705882352941176,
      "grad_norm": 0.051469575613737106,
      "learning_rate": 6.597938144329897e-05,
      "loss": 0.4129,
      "step": 2280
    },
    {
      "epoch": 0.6708823529411765,
      "grad_norm": 0.04894820973277092,
      "learning_rate": 6.592047128129603e-05,
      "loss": 0.3147,
      "step": 2281
    },
    {
      "epoch": 0.6711764705882353,
      "grad_norm": 0.055269211530685425,
      "learning_rate": 6.586156111929308e-05,
      "loss": 0.3416,
      "step": 2282
    },
    {
      "epoch": 0.6714705882352942,
      "grad_norm": 0.03964458778500557,
      "learning_rate": 6.580265095729014e-05,
      "loss": 0.2872,
      "step": 2283
    },
    {
      "epoch": 0.6717647058823529,
      "grad_norm": 0.04718461260199547,
      "learning_rate": 6.574374079528718e-05,
      "loss": 0.3704,
      "step": 2284
    },
    {
      "epoch": 0.6720588235294118,
      "grad_norm": 0.05207355320453644,
      "learning_rate": 6.568483063328424e-05,
      "loss": 0.3498,
      "step": 2285
    },
    {
      "epoch": 0.6723529411764706,
      "grad_norm": 0.042991217225790024,
      "learning_rate": 6.56259204712813e-05,
      "loss": 0.3734,
      "step": 2286
    },
    {
      "epoch": 0.6726470588235294,
      "grad_norm": 0.04199201241135597,
      "learning_rate": 6.556701030927835e-05,
      "loss": 0.3091,
      "step": 2287
    },
    {
      "epoch": 0.6729411764705883,
      "grad_norm": 0.05315385013818741,
      "learning_rate": 6.550810014727541e-05,
      "loss": 0.3572,
      "step": 2288
    },
    {
      "epoch": 0.673235294117647,
      "grad_norm": 0.038296911865472794,
      "learning_rate": 6.544918998527246e-05,
      "loss": 0.3099,
      "step": 2289
    },
    {
      "epoch": 0.6735294117647059,
      "grad_norm": 0.058804187923669815,
      "learning_rate": 6.539027982326952e-05,
      "loss": 0.3988,
      "step": 2290
    },
    {
      "epoch": 0.6738235294117647,
      "grad_norm": 0.03643321990966797,
      "learning_rate": 6.533136966126658e-05,
      "loss": 0.3235,
      "step": 2291
    },
    {
      "epoch": 0.6741176470588235,
      "grad_norm": 0.052071135491132736,
      "learning_rate": 6.527245949926362e-05,
      "loss": 0.4106,
      "step": 2292
    },
    {
      "epoch": 0.6744117647058824,
      "grad_norm": 0.05566755682229996,
      "learning_rate": 6.521354933726069e-05,
      "loss": 0.3486,
      "step": 2293
    },
    {
      "epoch": 0.6747058823529412,
      "grad_norm": 0.041111402213573456,
      "learning_rate": 6.515463917525773e-05,
      "loss": 0.2755,
      "step": 2294
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.03802427276968956,
      "learning_rate": 6.509572901325479e-05,
      "loss": 0.2657,
      "step": 2295
    },
    {
      "epoch": 0.6752941176470588,
      "grad_norm": 0.04926957190036774,
      "learning_rate": 6.503681885125185e-05,
      "loss": 0.3084,
      "step": 2296
    },
    {
      "epoch": 0.6755882352941176,
      "grad_norm": 0.04457085579633713,
      "learning_rate": 6.49779086892489e-05,
      "loss": 0.3116,
      "step": 2297
    },
    {
      "epoch": 0.6758823529411765,
      "grad_norm": 0.03859340026974678,
      "learning_rate": 6.491899852724596e-05,
      "loss": 0.2929,
      "step": 2298
    },
    {
      "epoch": 0.6761764705882353,
      "grad_norm": 0.04297390952706337,
      "learning_rate": 6.4860088365243e-05,
      "loss": 0.3269,
      "step": 2299
    },
    {
      "epoch": 0.6764705882352942,
      "grad_norm": 0.05674198642373085,
      "learning_rate": 6.480117820324007e-05,
      "loss": 0.4071,
      "step": 2300
    },
    {
      "epoch": 0.6764705882352942,
      "eval_loss": 0.3528716564178467,
      "eval_runtime": 214.9594,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 2300
    },
    {
      "epoch": 0.6767647058823529,
      "grad_norm": 0.05233298987150192,
      "learning_rate": 6.474226804123713e-05,
      "loss": 0.3947,
      "step": 2301
    },
    {
      "epoch": 0.6770588235294117,
      "grad_norm": 0.057522181421518326,
      "learning_rate": 6.468335787923417e-05,
      "loss": 0.4299,
      "step": 2302
    },
    {
      "epoch": 0.6773529411764706,
      "grad_norm": 0.05728248879313469,
      "learning_rate": 6.462444771723123e-05,
      "loss": 0.3292,
      "step": 2303
    },
    {
      "epoch": 0.6776470588235294,
      "grad_norm": 0.05298919975757599,
      "learning_rate": 6.456553755522828e-05,
      "loss": 0.3734,
      "step": 2304
    },
    {
      "epoch": 0.6779411764705883,
      "grad_norm": 0.044204771518707275,
      "learning_rate": 6.450662739322534e-05,
      "loss": 0.3705,
      "step": 2305
    },
    {
      "epoch": 0.678235294117647,
      "grad_norm": 0.051419731229543686,
      "learning_rate": 6.44477172312224e-05,
      "loss": 0.3808,
      "step": 2306
    },
    {
      "epoch": 0.6785294117647059,
      "grad_norm": 0.039484210312366486,
      "learning_rate": 6.438880706921945e-05,
      "loss": 0.3482,
      "step": 2307
    },
    {
      "epoch": 0.6788235294117647,
      "grad_norm": 0.06307525932788849,
      "learning_rate": 6.43298969072165e-05,
      "loss": 0.404,
      "step": 2308
    },
    {
      "epoch": 0.6791176470588235,
      "grad_norm": 0.037984441965818405,
      "learning_rate": 6.427098674521355e-05,
      "loss": 0.2977,
      "step": 2309
    },
    {
      "epoch": 0.6794117647058824,
      "grad_norm": 0.04186432808637619,
      "learning_rate": 6.421207658321061e-05,
      "loss": 0.3,
      "step": 2310
    },
    {
      "epoch": 0.6797058823529412,
      "grad_norm": 0.04610756039619446,
      "learning_rate": 6.415316642120767e-05,
      "loss": 0.385,
      "step": 2311
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.05421149730682373,
      "learning_rate": 6.409425625920472e-05,
      "loss": 0.4189,
      "step": 2312
    },
    {
      "epoch": 0.6802941176470588,
      "grad_norm": 0.044659573584795,
      "learning_rate": 6.403534609720177e-05,
      "loss": 0.3449,
      "step": 2313
    },
    {
      "epoch": 0.6805882352941176,
      "grad_norm": 0.041503507643938065,
      "learning_rate": 6.397643593519881e-05,
      "loss": 0.3022,
      "step": 2314
    },
    {
      "epoch": 0.6808823529411765,
      "grad_norm": 0.053009163588285446,
      "learning_rate": 6.391752577319587e-05,
      "loss": 0.3706,
      "step": 2315
    },
    {
      "epoch": 0.6811764705882353,
      "grad_norm": 0.03915414214134216,
      "learning_rate": 6.385861561119293e-05,
      "loss": 0.3212,
      "step": 2316
    },
    {
      "epoch": 0.6814705882352942,
      "grad_norm": 0.05673316866159439,
      "learning_rate": 6.379970544918998e-05,
      "loss": 0.3751,
      "step": 2317
    },
    {
      "epoch": 0.6817647058823529,
      "grad_norm": 0.046392105519771576,
      "learning_rate": 6.374079528718704e-05,
      "loss": 0.3593,
      "step": 2318
    },
    {
      "epoch": 0.6820588235294117,
      "grad_norm": 0.04357335343956947,
      "learning_rate": 6.368188512518409e-05,
      "loss": 0.2946,
      "step": 2319
    },
    {
      "epoch": 0.6823529411764706,
      "grad_norm": 0.04414307698607445,
      "learning_rate": 6.362297496318115e-05,
      "loss": 0.3579,
      "step": 2320
    },
    {
      "epoch": 0.6826470588235294,
      "grad_norm": 0.04962610453367233,
      "learning_rate": 6.356406480117821e-05,
      "loss": 0.3943,
      "step": 2321
    },
    {
      "epoch": 0.6829411764705883,
      "grad_norm": 0.05716044083237648,
      "learning_rate": 6.350515463917525e-05,
      "loss": 0.3385,
      "step": 2322
    },
    {
      "epoch": 0.683235294117647,
      "grad_norm": 0.042205099016427994,
      "learning_rate": 6.344624447717231e-05,
      "loss": 0.2662,
      "step": 2323
    },
    {
      "epoch": 0.6835294117647058,
      "grad_norm": 0.057954367250204086,
      "learning_rate": 6.338733431516936e-05,
      "loss": 0.3965,
      "step": 2324
    },
    {
      "epoch": 0.6838235294117647,
      "grad_norm": 0.046068109571933746,
      "learning_rate": 6.332842415316642e-05,
      "loss": 0.3437,
      "step": 2325
    },
    {
      "epoch": 0.6841176470588235,
      "grad_norm": 0.033994052559137344,
      "learning_rate": 6.326951399116348e-05,
      "loss": 0.2911,
      "step": 2326
    },
    {
      "epoch": 0.6844117647058824,
      "grad_norm": 0.05899028107523918,
      "learning_rate": 6.321060382916053e-05,
      "loss": 0.3661,
      "step": 2327
    },
    {
      "epoch": 0.6847058823529412,
      "grad_norm": 0.04349448159337044,
      "learning_rate": 6.315169366715759e-05,
      "loss": 0.3005,
      "step": 2328
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.040972474962472916,
      "learning_rate": 6.309278350515463e-05,
      "loss": 0.3226,
      "step": 2329
    },
    {
      "epoch": 0.6852941176470588,
      "grad_norm": 0.04075390473008156,
      "learning_rate": 6.30338733431517e-05,
      "loss": 0.2978,
      "step": 2330
    },
    {
      "epoch": 0.6855882352941176,
      "grad_norm": 0.0522967092692852,
      "learning_rate": 6.297496318114875e-05,
      "loss": 0.2797,
      "step": 2331
    },
    {
      "epoch": 0.6858823529411765,
      "grad_norm": 0.048911720514297485,
      "learning_rate": 6.29160530191458e-05,
      "loss": 0.3433,
      "step": 2332
    },
    {
      "epoch": 0.6861764705882353,
      "grad_norm": 0.06321289390325546,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.3972,
      "step": 2333
    },
    {
      "epoch": 0.6864705882352942,
      "grad_norm": 0.050637174397706985,
      "learning_rate": 6.279823269513991e-05,
      "loss": 0.4054,
      "step": 2334
    },
    {
      "epoch": 0.6867647058823529,
      "grad_norm": 0.04607279971241951,
      "learning_rate": 6.273932253313697e-05,
      "loss": 0.3548,
      "step": 2335
    },
    {
      "epoch": 0.6870588235294117,
      "grad_norm": 0.06474243104457855,
      "learning_rate": 6.268041237113403e-05,
      "loss": 0.4191,
      "step": 2336
    },
    {
      "epoch": 0.6873529411764706,
      "grad_norm": 0.05138590559363365,
      "learning_rate": 6.262150220913107e-05,
      "loss": 0.3777,
      "step": 2337
    },
    {
      "epoch": 0.6876470588235294,
      "grad_norm": 0.04292004182934761,
      "learning_rate": 6.256259204712814e-05,
      "loss": 0.288,
      "step": 2338
    },
    {
      "epoch": 0.6879411764705883,
      "grad_norm": 0.03156419098377228,
      "learning_rate": 6.250368188512518e-05,
      "loss": 0.1996,
      "step": 2339
    },
    {
      "epoch": 0.6882352941176471,
      "grad_norm": 0.05718201398849487,
      "learning_rate": 6.244477172312224e-05,
      "loss": 0.407,
      "step": 2340
    },
    {
      "epoch": 0.6885294117647058,
      "grad_norm": 0.04992750287055969,
      "learning_rate": 6.23858615611193e-05,
      "loss": 0.4008,
      "step": 2341
    },
    {
      "epoch": 0.6888235294117647,
      "grad_norm": 0.04544374719262123,
      "learning_rate": 6.232695139911635e-05,
      "loss": 0.3473,
      "step": 2342
    },
    {
      "epoch": 0.6891176470588235,
      "grad_norm": 0.05876942723989487,
      "learning_rate": 6.226804123711341e-05,
      "loss": 0.3661,
      "step": 2343
    },
    {
      "epoch": 0.6894117647058824,
      "grad_norm": 0.042766474187374115,
      "learning_rate": 6.220913107511046e-05,
      "loss": 0.3151,
      "step": 2344
    },
    {
      "epoch": 0.6897058823529412,
      "grad_norm": 0.04648792743682861,
      "learning_rate": 6.215022091310752e-05,
      "loss": 0.3854,
      "step": 2345
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.045354556292295456,
      "learning_rate": 6.209131075110458e-05,
      "loss": 0.3902,
      "step": 2346
    },
    {
      "epoch": 0.6902941176470588,
      "grad_norm": 0.0539974644780159,
      "learning_rate": 6.203240058910162e-05,
      "loss": 0.3884,
      "step": 2347
    },
    {
      "epoch": 0.6905882352941176,
      "grad_norm": 0.04271921142935753,
      "learning_rate": 6.197349042709868e-05,
      "loss": 0.2659,
      "step": 2348
    },
    {
      "epoch": 0.6908823529411765,
      "grad_norm": 0.04586779326200485,
      "learning_rate": 6.191458026509573e-05,
      "loss": 0.3057,
      "step": 2349
    },
    {
      "epoch": 0.6911764705882353,
      "grad_norm": 0.05341547355055809,
      "learning_rate": 6.185567010309279e-05,
      "loss": 0.3316,
      "step": 2350
    },
    {
      "epoch": 0.6914705882352942,
      "grad_norm": 0.04107147827744484,
      "learning_rate": 6.179675994108985e-05,
      "loss": 0.3432,
      "step": 2351
    },
    {
      "epoch": 0.691764705882353,
      "grad_norm": 0.04951293766498566,
      "learning_rate": 6.17378497790869e-05,
      "loss": 0.3552,
      "step": 2352
    },
    {
      "epoch": 0.6920588235294117,
      "grad_norm": 0.0379570834338665,
      "learning_rate": 6.167893961708396e-05,
      "loss": 0.3385,
      "step": 2353
    },
    {
      "epoch": 0.6923529411764706,
      "grad_norm": 0.03656318038702011,
      "learning_rate": 6.1620029455081e-05,
      "loss": 0.303,
      "step": 2354
    },
    {
      "epoch": 0.6926470588235294,
      "grad_norm": 0.03733883798122406,
      "learning_rate": 6.156111929307806e-05,
      "loss": 0.3141,
      "step": 2355
    },
    {
      "epoch": 0.6929411764705883,
      "grad_norm": 0.04349987208843231,
      "learning_rate": 6.150220913107512e-05,
      "loss": 0.3635,
      "step": 2356
    },
    {
      "epoch": 0.6932352941176471,
      "grad_norm": 0.03732750937342644,
      "learning_rate": 6.144329896907217e-05,
      "loss": 0.3229,
      "step": 2357
    },
    {
      "epoch": 0.6935294117647058,
      "grad_norm": 0.04807717725634575,
      "learning_rate": 6.138438880706923e-05,
      "loss": 0.3904,
      "step": 2358
    },
    {
      "epoch": 0.6938235294117647,
      "grad_norm": 0.04428679123520851,
      "learning_rate": 6.132547864506628e-05,
      "loss": 0.3433,
      "step": 2359
    },
    {
      "epoch": 0.6941176470588235,
      "grad_norm": 0.05895836651325226,
      "learning_rate": 6.126656848306334e-05,
      "loss": 0.3903,
      "step": 2360
    },
    {
      "epoch": 0.6944117647058824,
      "grad_norm": 0.038600821048021317,
      "learning_rate": 6.12076583210604e-05,
      "loss": 0.3361,
      "step": 2361
    },
    {
      "epoch": 0.6947058823529412,
      "grad_norm": 0.04455438256263733,
      "learning_rate": 6.114874815905744e-05,
      "loss": 0.3813,
      "step": 2362
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.047260161489248276,
      "learning_rate": 6.10898379970545e-05,
      "loss": 0.2753,
      "step": 2363
    },
    {
      "epoch": 0.6952941176470588,
      "grad_norm": 0.03765135258436203,
      "learning_rate": 6.103092783505156e-05,
      "loss": 0.3034,
      "step": 2364
    },
    {
      "epoch": 0.6955882352941176,
      "grad_norm": 0.04588793218135834,
      "learning_rate": 6.097201767304861e-05,
      "loss": 0.3577,
      "step": 2365
    },
    {
      "epoch": 0.6958823529411765,
      "grad_norm": 0.06639675050973892,
      "learning_rate": 6.091310751104565e-05,
      "loss": 0.3637,
      "step": 2366
    },
    {
      "epoch": 0.6961764705882353,
      "grad_norm": 0.04159185290336609,
      "learning_rate": 6.085419734904271e-05,
      "loss": 0.3637,
      "step": 2367
    },
    {
      "epoch": 0.6964705882352941,
      "grad_norm": 0.04172774404287338,
      "learning_rate": 6.0795287187039764e-05,
      "loss": 0.3456,
      "step": 2368
    },
    {
      "epoch": 0.696764705882353,
      "grad_norm": 0.04241340607404709,
      "learning_rate": 6.073637702503682e-05,
      "loss": 0.3062,
      "step": 2369
    },
    {
      "epoch": 0.6970588235294117,
      "grad_norm": 0.0627988949418068,
      "learning_rate": 6.067746686303387e-05,
      "loss": 0.387,
      "step": 2370
    },
    {
      "epoch": 0.6973529411764706,
      "grad_norm": 0.0620054192841053,
      "learning_rate": 6.0618556701030924e-05,
      "loss": 0.4364,
      "step": 2371
    },
    {
      "epoch": 0.6976470588235294,
      "grad_norm": 0.05014701560139656,
      "learning_rate": 6.0559646539027984e-05,
      "loss": 0.2829,
      "step": 2372
    },
    {
      "epoch": 0.6979411764705883,
      "grad_norm": 0.04032795503735542,
      "learning_rate": 6.050073637702504e-05,
      "loss": 0.323,
      "step": 2373
    },
    {
      "epoch": 0.6982352941176471,
      "grad_norm": 0.049068160355091095,
      "learning_rate": 6.044182621502209e-05,
      "loss": 0.3187,
      "step": 2374
    },
    {
      "epoch": 0.6985294117647058,
      "grad_norm": 0.05593946948647499,
      "learning_rate": 6.0382916053019144e-05,
      "loss": 0.3998,
      "step": 2375
    },
    {
      "epoch": 0.6988235294117647,
      "grad_norm": 0.05089335888624191,
      "learning_rate": 6.03240058910162e-05,
      "loss": 0.3369,
      "step": 2376
    },
    {
      "epoch": 0.6991176470588235,
      "grad_norm": 0.055621035397052765,
      "learning_rate": 6.026509572901326e-05,
      "loss": 0.367,
      "step": 2377
    },
    {
      "epoch": 0.6994117647058824,
      "grad_norm": 0.043481312692165375,
      "learning_rate": 6.020618556701031e-05,
      "loss": 0.3353,
      "step": 2378
    },
    {
      "epoch": 0.6997058823529412,
      "grad_norm": 0.03966890275478363,
      "learning_rate": 6.0147275405007365e-05,
      "loss": 0.3305,
      "step": 2379
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.06218257546424866,
      "learning_rate": 6.008836524300442e-05,
      "loss": 0.4461,
      "step": 2380
    },
    {
      "epoch": 0.7002941176470588,
      "grad_norm": 0.04907030984759331,
      "learning_rate": 6.002945508100147e-05,
      "loss": 0.385,
      "step": 2381
    },
    {
      "epoch": 0.7005882352941176,
      "grad_norm": 0.049420520663261414,
      "learning_rate": 5.9970544918998525e-05,
      "loss": 0.415,
      "step": 2382
    },
    {
      "epoch": 0.7008823529411765,
      "grad_norm": 0.03750571236014366,
      "learning_rate": 5.9911634756995585e-05,
      "loss": 0.3368,
      "step": 2383
    },
    {
      "epoch": 0.7011764705882353,
      "grad_norm": 0.035152386873960495,
      "learning_rate": 5.985272459499264e-05,
      "loss": 0.3059,
      "step": 2384
    },
    {
      "epoch": 0.7014705882352941,
      "grad_norm": 0.046973224729299545,
      "learning_rate": 5.979381443298969e-05,
      "loss": 0.3317,
      "step": 2385
    },
    {
      "epoch": 0.701764705882353,
      "grad_norm": 0.05907149240374565,
      "learning_rate": 5.9734904270986745e-05,
      "loss": 0.406,
      "step": 2386
    },
    {
      "epoch": 0.7020588235294117,
      "grad_norm": 0.04020584002137184,
      "learning_rate": 5.96759941089838e-05,
      "loss": 0.3297,
      "step": 2387
    },
    {
      "epoch": 0.7023529411764706,
      "grad_norm": 0.06261269003152847,
      "learning_rate": 5.961708394698086e-05,
      "loss": 0.377,
      "step": 2388
    },
    {
      "epoch": 0.7026470588235294,
      "grad_norm": 0.04072629660367966,
      "learning_rate": 5.955817378497791e-05,
      "loss": 0.2944,
      "step": 2389
    },
    {
      "epoch": 0.7029411764705882,
      "grad_norm": 0.06370297074317932,
      "learning_rate": 5.9499263622974965e-05,
      "loss": 0.3513,
      "step": 2390
    },
    {
      "epoch": 0.7032352941176471,
      "grad_norm": 0.03914403170347214,
      "learning_rate": 5.944035346097202e-05,
      "loss": 0.3159,
      "step": 2391
    },
    {
      "epoch": 0.7035294117647058,
      "grad_norm": 0.04187550023198128,
      "learning_rate": 5.938144329896907e-05,
      "loss": 0.2767,
      "step": 2392
    },
    {
      "epoch": 0.7038235294117647,
      "grad_norm": 0.05873176082968712,
      "learning_rate": 5.932253313696613e-05,
      "loss": 0.3878,
      "step": 2393
    },
    {
      "epoch": 0.7041176470588235,
      "grad_norm": 0.053043004125356674,
      "learning_rate": 5.9263622974963186e-05,
      "loss": 0.3424,
      "step": 2394
    },
    {
      "epoch": 0.7044117647058824,
      "grad_norm": 0.04473119601607323,
      "learning_rate": 5.920471281296024e-05,
      "loss": 0.3451,
      "step": 2395
    },
    {
      "epoch": 0.7047058823529412,
      "grad_norm": 0.045636534690856934,
      "learning_rate": 5.914580265095729e-05,
      "loss": 0.3567,
      "step": 2396
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.051253676414489746,
      "learning_rate": 5.9086892488954346e-05,
      "loss": 0.4072,
      "step": 2397
    },
    {
      "epoch": 0.7052941176470588,
      "grad_norm": 0.04157358407974243,
      "learning_rate": 5.9027982326951406e-05,
      "loss": 0.3167,
      "step": 2398
    },
    {
      "epoch": 0.7055882352941176,
      "grad_norm": 0.041106585413217545,
      "learning_rate": 5.896907216494846e-05,
      "loss": 0.3397,
      "step": 2399
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.0476282574236393,
      "learning_rate": 5.891016200294551e-05,
      "loss": 0.3034,
      "step": 2400
    },
    {
      "epoch": 0.7058823529411765,
      "eval_loss": 0.3524744510650635,
      "eval_runtime": 214.9845,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 2400
    },
    {
      "epoch": 0.7061764705882353,
      "grad_norm": 0.04453331232070923,
      "learning_rate": 5.8851251840942566e-05,
      "loss": 0.3376,
      "step": 2401
    },
    {
      "epoch": 0.7064705882352941,
      "grad_norm": 0.08176279067993164,
      "learning_rate": 5.879234167893962e-05,
      "loss": 0.4223,
      "step": 2402
    },
    {
      "epoch": 0.706764705882353,
      "grad_norm": 0.03538886830210686,
      "learning_rate": 5.873343151693668e-05,
      "loss": 0.297,
      "step": 2403
    },
    {
      "epoch": 0.7070588235294117,
      "grad_norm": 0.04043501988053322,
      "learning_rate": 5.867452135493373e-05,
      "loss": 0.3197,
      "step": 2404
    },
    {
      "epoch": 0.7073529411764706,
      "grad_norm": 0.049811333417892456,
      "learning_rate": 5.861561119293079e-05,
      "loss": 0.3581,
      "step": 2405
    },
    {
      "epoch": 0.7076470588235294,
      "grad_norm": 0.03900117054581642,
      "learning_rate": 5.855670103092784e-05,
      "loss": 0.3547,
      "step": 2406
    },
    {
      "epoch": 0.7079411764705882,
      "grad_norm": 0.056850772351026535,
      "learning_rate": 5.8497790868924893e-05,
      "loss": 0.4224,
      "step": 2407
    },
    {
      "epoch": 0.7082352941176471,
      "grad_norm": 0.04110278934240341,
      "learning_rate": 5.843888070692195e-05,
      "loss": 0.3016,
      "step": 2408
    },
    {
      "epoch": 0.7085294117647059,
      "grad_norm": 0.03902683034539223,
      "learning_rate": 5.837997054491901e-05,
      "loss": 0.3639,
      "step": 2409
    },
    {
      "epoch": 0.7088235294117647,
      "grad_norm": 0.039197880774736404,
      "learning_rate": 5.832106038291606e-05,
      "loss": 0.3067,
      "step": 2410
    },
    {
      "epoch": 0.7091176470588235,
      "grad_norm": 0.043189335614442825,
      "learning_rate": 5.8262150220913114e-05,
      "loss": 0.3447,
      "step": 2411
    },
    {
      "epoch": 0.7094117647058824,
      "grad_norm": 0.040966324508190155,
      "learning_rate": 5.820324005891017e-05,
      "loss": 0.3429,
      "step": 2412
    },
    {
      "epoch": 0.7097058823529412,
      "grad_norm": 0.042864665389060974,
      "learning_rate": 5.814432989690722e-05,
      "loss": 0.3581,
      "step": 2413
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.043393563479185104,
      "learning_rate": 5.808541973490428e-05,
      "loss": 0.3124,
      "step": 2414
    },
    {
      "epoch": 0.7102941176470589,
      "grad_norm": 0.03794321045279503,
      "learning_rate": 5.8026509572901334e-05,
      "loss": 0.3349,
      "step": 2415
    },
    {
      "epoch": 0.7105882352941176,
      "grad_norm": 0.04093584045767784,
      "learning_rate": 5.796759941089839e-05,
      "loss": 0.3406,
      "step": 2416
    },
    {
      "epoch": 0.7108823529411765,
      "grad_norm": 0.03994106873869896,
      "learning_rate": 5.790868924889544e-05,
      "loss": 0.2833,
      "step": 2417
    },
    {
      "epoch": 0.7111764705882353,
      "grad_norm": 0.04780333861708641,
      "learning_rate": 5.7849779086892494e-05,
      "loss": 0.3507,
      "step": 2418
    },
    {
      "epoch": 0.7114705882352941,
      "grad_norm": 0.05344962701201439,
      "learning_rate": 5.779086892488954e-05,
      "loss": 0.3725,
      "step": 2419
    },
    {
      "epoch": 0.711764705882353,
      "grad_norm": 0.04321284592151642,
      "learning_rate": 5.7731958762886594e-05,
      "loss": 0.3367,
      "step": 2420
    },
    {
      "epoch": 0.7120588235294117,
      "grad_norm": 0.061037737876176834,
      "learning_rate": 5.767304860088365e-05,
      "loss": 0.3157,
      "step": 2421
    },
    {
      "epoch": 0.7123529411764706,
      "grad_norm": 0.037639617919921875,
      "learning_rate": 5.761413843888071e-05,
      "loss": 0.3443,
      "step": 2422
    },
    {
      "epoch": 0.7126470588235294,
      "grad_norm": 0.035922396928071976,
      "learning_rate": 5.755522827687776e-05,
      "loss": 0.297,
      "step": 2423
    },
    {
      "epoch": 0.7129411764705882,
      "grad_norm": 0.04316660761833191,
      "learning_rate": 5.7496318114874815e-05,
      "loss": 0.3426,
      "step": 2424
    },
    {
      "epoch": 0.7132352941176471,
      "grad_norm": 0.05304773151874542,
      "learning_rate": 5.743740795287187e-05,
      "loss": 0.4025,
      "step": 2425
    },
    {
      "epoch": 0.7135294117647059,
      "grad_norm": 0.04692716896533966,
      "learning_rate": 5.737849779086892e-05,
      "loss": 0.3905,
      "step": 2426
    },
    {
      "epoch": 0.7138235294117647,
      "grad_norm": 0.04552413523197174,
      "learning_rate": 5.731958762886598e-05,
      "loss": 0.299,
      "step": 2427
    },
    {
      "epoch": 0.7141176470588235,
      "grad_norm": 0.04341253265738487,
      "learning_rate": 5.7260677466863035e-05,
      "loss": 0.2962,
      "step": 2428
    },
    {
      "epoch": 0.7144117647058823,
      "grad_norm": 0.058899953961372375,
      "learning_rate": 5.720176730486009e-05,
      "loss": 0.4026,
      "step": 2429
    },
    {
      "epoch": 0.7147058823529412,
      "grad_norm": 0.047856539487838745,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.3221,
      "step": 2430
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.04838323965668678,
      "learning_rate": 5.7083946980854195e-05,
      "loss": 0.2922,
      "step": 2431
    },
    {
      "epoch": 0.7152941176470589,
      "grad_norm": 0.04851826652884483,
      "learning_rate": 5.702503681885125e-05,
      "loss": 0.3468,
      "step": 2432
    },
    {
      "epoch": 0.7155882352941176,
      "grad_norm": 0.041203707456588745,
      "learning_rate": 5.696612665684831e-05,
      "loss": 0.2817,
      "step": 2433
    },
    {
      "epoch": 0.7158823529411765,
      "grad_norm": 0.0471319817006588,
      "learning_rate": 5.690721649484536e-05,
      "loss": 0.3708,
      "step": 2434
    },
    {
      "epoch": 0.7161764705882353,
      "grad_norm": 0.05034887045621872,
      "learning_rate": 5.6848306332842415e-05,
      "loss": 0.3333,
      "step": 2435
    },
    {
      "epoch": 0.7164705882352941,
      "grad_norm": 0.052046485245227814,
      "learning_rate": 5.678939617083947e-05,
      "loss": 0.3412,
      "step": 2436
    },
    {
      "epoch": 0.716764705882353,
      "grad_norm": 0.03700189292430878,
      "learning_rate": 5.673048600883652e-05,
      "loss": 0.307,
      "step": 2437
    },
    {
      "epoch": 0.7170588235294117,
      "grad_norm": 0.05109848082065582,
      "learning_rate": 5.667157584683358e-05,
      "loss": 0.4238,
      "step": 2438
    },
    {
      "epoch": 0.7173529411764706,
      "grad_norm": 0.052732642740011215,
      "learning_rate": 5.6612665684830636e-05,
      "loss": 0.3811,
      "step": 2439
    },
    {
      "epoch": 0.7176470588235294,
      "grad_norm": 0.04830155894160271,
      "learning_rate": 5.655375552282769e-05,
      "loss": 0.3662,
      "step": 2440
    },
    {
      "epoch": 0.7179411764705882,
      "grad_norm": 0.04435200244188309,
      "learning_rate": 5.649484536082474e-05,
      "loss": 0.3226,
      "step": 2441
    },
    {
      "epoch": 0.7182352941176471,
      "grad_norm": 0.04022759944200516,
      "learning_rate": 5.6435935198821796e-05,
      "loss": 0.3122,
      "step": 2442
    },
    {
      "epoch": 0.7185294117647059,
      "grad_norm": 0.04673885926604271,
      "learning_rate": 5.6377025036818856e-05,
      "loss": 0.3701,
      "step": 2443
    },
    {
      "epoch": 0.7188235294117648,
      "grad_norm": 0.04066424071788788,
      "learning_rate": 5.631811487481591e-05,
      "loss": 0.3449,
      "step": 2444
    },
    {
      "epoch": 0.7191176470588235,
      "grad_norm": 0.04883071035146713,
      "learning_rate": 5.625920471281296e-05,
      "loss": 0.4264,
      "step": 2445
    },
    {
      "epoch": 0.7194117647058823,
      "grad_norm": 0.0493464469909668,
      "learning_rate": 5.6200294550810016e-05,
      "loss": 0.3495,
      "step": 2446
    },
    {
      "epoch": 0.7197058823529412,
      "grad_norm": 0.055875055491924286,
      "learning_rate": 5.614138438880707e-05,
      "loss": 0.4704,
      "step": 2447
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.036159999668598175,
      "learning_rate": 5.608247422680413e-05,
      "loss": 0.2753,
      "step": 2448
    },
    {
      "epoch": 0.7202941176470589,
      "grad_norm": 0.04383213073015213,
      "learning_rate": 5.602356406480118e-05,
      "loss": 0.3247,
      "step": 2449
    },
    {
      "epoch": 0.7205882352941176,
      "grad_norm": 0.03310908377170563,
      "learning_rate": 5.596465390279824e-05,
      "loss": 0.2494,
      "step": 2450
    },
    {
      "epoch": 0.7208823529411764,
      "grad_norm": 0.0471324697136879,
      "learning_rate": 5.590574374079529e-05,
      "loss": 0.328,
      "step": 2451
    },
    {
      "epoch": 0.7211764705882353,
      "grad_norm": 0.04562816023826599,
      "learning_rate": 5.5846833578792343e-05,
      "loss": 0.3521,
      "step": 2452
    },
    {
      "epoch": 0.7214705882352941,
      "grad_norm": 0.043095655739307404,
      "learning_rate": 5.5787923416789404e-05,
      "loss": 0.3636,
      "step": 2453
    },
    {
      "epoch": 0.721764705882353,
      "grad_norm": 0.04644377529621124,
      "learning_rate": 5.572901325478646e-05,
      "loss": 0.349,
      "step": 2454
    },
    {
      "epoch": 0.7220588235294118,
      "grad_norm": 0.04899050295352936,
      "learning_rate": 5.567010309278351e-05,
      "loss": 0.417,
      "step": 2455
    },
    {
      "epoch": 0.7223529411764706,
      "grad_norm": 0.038928885012865067,
      "learning_rate": 5.5611192930780564e-05,
      "loss": 0.3144,
      "step": 2456
    },
    {
      "epoch": 0.7226470588235294,
      "grad_norm": 0.06368932127952576,
      "learning_rate": 5.555228276877762e-05,
      "loss": 0.3827,
      "step": 2457
    },
    {
      "epoch": 0.7229411764705882,
      "grad_norm": 0.046918343752622604,
      "learning_rate": 5.549337260677468e-05,
      "loss": 0.321,
      "step": 2458
    },
    {
      "epoch": 0.7232352941176471,
      "grad_norm": 0.04757833480834961,
      "learning_rate": 5.543446244477173e-05,
      "loss": 0.3575,
      "step": 2459
    },
    {
      "epoch": 0.7235294117647059,
      "grad_norm": 0.05038994178175926,
      "learning_rate": 5.5375552282768784e-05,
      "loss": 0.3612,
      "step": 2460
    },
    {
      "epoch": 0.7238235294117648,
      "grad_norm": 0.03747410699725151,
      "learning_rate": 5.531664212076584e-05,
      "loss": 0.3459,
      "step": 2461
    },
    {
      "epoch": 0.7241176470588235,
      "grad_norm": 0.048635344952344894,
      "learning_rate": 5.525773195876289e-05,
      "loss": 0.3343,
      "step": 2462
    },
    {
      "epoch": 0.7244117647058823,
      "grad_norm": 0.043079689145088196,
      "learning_rate": 5.5198821796759944e-05,
      "loss": 0.346,
      "step": 2463
    },
    {
      "epoch": 0.7247058823529412,
      "grad_norm": 0.04237363860011101,
      "learning_rate": 5.5139911634757004e-05,
      "loss": 0.2981,
      "step": 2464
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.05207251012325287,
      "learning_rate": 5.508100147275406e-05,
      "loss": 0.4003,
      "step": 2465
    },
    {
      "epoch": 0.7252941176470589,
      "grad_norm": 0.05209410563111305,
      "learning_rate": 5.502209131075111e-05,
      "loss": 0.3374,
      "step": 2466
    },
    {
      "epoch": 0.7255882352941176,
      "grad_norm": 0.04921712353825569,
      "learning_rate": 5.4963181148748165e-05,
      "loss": 0.421,
      "step": 2467
    },
    {
      "epoch": 0.7258823529411764,
      "grad_norm": 0.04980650171637535,
      "learning_rate": 5.490427098674522e-05,
      "loss": 0.4151,
      "step": 2468
    },
    {
      "epoch": 0.7261764705882353,
      "grad_norm": 0.04453970119357109,
      "learning_rate": 5.484536082474228e-05,
      "loss": 0.3906,
      "step": 2469
    },
    {
      "epoch": 0.7264705882352941,
      "grad_norm": 0.03367532417178154,
      "learning_rate": 5.478645066273933e-05,
      "loss": 0.242,
      "step": 2470
    },
    {
      "epoch": 0.726764705882353,
      "grad_norm": 0.043572891503572464,
      "learning_rate": 5.4727540500736385e-05,
      "loss": 0.376,
      "step": 2471
    },
    {
      "epoch": 0.7270588235294118,
      "grad_norm": 0.03879748657345772,
      "learning_rate": 5.466863033873343e-05,
      "loss": 0.3289,
      "step": 2472
    },
    {
      "epoch": 0.7273529411764705,
      "grad_norm": 0.05659238621592522,
      "learning_rate": 5.4609720176730485e-05,
      "loss": 0.3667,
      "step": 2473
    },
    {
      "epoch": 0.7276470588235294,
      "grad_norm": 0.057039447128772736,
      "learning_rate": 5.455081001472754e-05,
      "loss": 0.3658,
      "step": 2474
    },
    {
      "epoch": 0.7279411764705882,
      "grad_norm": 0.04639853909611702,
      "learning_rate": 5.449189985272459e-05,
      "loss": 0.3322,
      "step": 2475
    },
    {
      "epoch": 0.7282352941176471,
      "grad_norm": 0.03952757269144058,
      "learning_rate": 5.4432989690721645e-05,
      "loss": 0.3041,
      "step": 2476
    },
    {
      "epoch": 0.7285294117647059,
      "grad_norm": 0.050503700971603394,
      "learning_rate": 5.4374079528718705e-05,
      "loss": 0.3641,
      "step": 2477
    },
    {
      "epoch": 0.7288235294117648,
      "grad_norm": 0.047948528081178665,
      "learning_rate": 5.431516936671576e-05,
      "loss": 0.3423,
      "step": 2478
    },
    {
      "epoch": 0.7291176470588235,
      "grad_norm": 0.043107204139232635,
      "learning_rate": 5.425625920471281e-05,
      "loss": 0.326,
      "step": 2479
    },
    {
      "epoch": 0.7294117647058823,
      "grad_norm": 0.04117533937096596,
      "learning_rate": 5.4197349042709865e-05,
      "loss": 0.2809,
      "step": 2480
    },
    {
      "epoch": 0.7297058823529412,
      "grad_norm": 0.0614127516746521,
      "learning_rate": 5.413843888070692e-05,
      "loss": 0.3229,
      "step": 2481
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.061018820852041245,
      "learning_rate": 5.407952871870398e-05,
      "loss": 0.4214,
      "step": 2482
    },
    {
      "epoch": 0.7302941176470589,
      "grad_norm": 0.06948083639144897,
      "learning_rate": 5.402061855670103e-05,
      "loss": 0.3377,
      "step": 2483
    },
    {
      "epoch": 0.7305882352941176,
      "grad_norm": 0.038829464465379715,
      "learning_rate": 5.3961708394698086e-05,
      "loss": 0.3054,
      "step": 2484
    },
    {
      "epoch": 0.7308823529411764,
      "grad_norm": 0.06574341654777527,
      "learning_rate": 5.390279823269514e-05,
      "loss": 0.4046,
      "step": 2485
    },
    {
      "epoch": 0.7311764705882353,
      "grad_norm": 0.04692427068948746,
      "learning_rate": 5.384388807069219e-05,
      "loss": 0.3189,
      "step": 2486
    },
    {
      "epoch": 0.7314705882352941,
      "grad_norm": 0.054159898310899734,
      "learning_rate": 5.3784977908689246e-05,
      "loss": 0.3835,
      "step": 2487
    },
    {
      "epoch": 0.731764705882353,
      "grad_norm": 0.041354451328516006,
      "learning_rate": 5.3726067746686306e-05,
      "loss": 0.32,
      "step": 2488
    },
    {
      "epoch": 0.7320588235294118,
      "grad_norm": 0.048041392117738724,
      "learning_rate": 5.366715758468336e-05,
      "loss": 0.3305,
      "step": 2489
    },
    {
      "epoch": 0.7323529411764705,
      "grad_norm": 0.047168247401714325,
      "learning_rate": 5.360824742268041e-05,
      "loss": 0.3596,
      "step": 2490
    },
    {
      "epoch": 0.7326470588235294,
      "grad_norm": 0.04315079748630524,
      "learning_rate": 5.3549337260677466e-05,
      "loss": 0.2849,
      "step": 2491
    },
    {
      "epoch": 0.7329411764705882,
      "grad_norm": 0.057821258902549744,
      "learning_rate": 5.349042709867452e-05,
      "loss": 0.3683,
      "step": 2492
    },
    {
      "epoch": 0.7332352941176471,
      "grad_norm": 0.039485324174165726,
      "learning_rate": 5.343151693667158e-05,
      "loss": 0.3079,
      "step": 2493
    },
    {
      "epoch": 0.7335294117647059,
      "grad_norm": 0.05536584183573723,
      "learning_rate": 5.337260677466863e-05,
      "loss": 0.4474,
      "step": 2494
    },
    {
      "epoch": 0.7338235294117647,
      "grad_norm": 0.04237242788076401,
      "learning_rate": 5.3313696612665687e-05,
      "loss": 0.2753,
      "step": 2495
    },
    {
      "epoch": 0.7341176470588235,
      "grad_norm": 0.0499337762594223,
      "learning_rate": 5.325478645066274e-05,
      "loss": 0.3702,
      "step": 2496
    },
    {
      "epoch": 0.7344117647058823,
      "grad_norm": 0.050150733441114426,
      "learning_rate": 5.319587628865979e-05,
      "loss": 0.4133,
      "step": 2497
    },
    {
      "epoch": 0.7347058823529412,
      "grad_norm": 0.04704694077372551,
      "learning_rate": 5.3136966126656854e-05,
      "loss": 0.2684,
      "step": 2498
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.047764044255018234,
      "learning_rate": 5.307805596465391e-05,
      "loss": 0.3028,
      "step": 2499
    },
    {
      "epoch": 0.7352941176470589,
      "grad_norm": 0.05392345413565636,
      "learning_rate": 5.301914580265096e-05,
      "loss": 0.3894,
      "step": 2500
    },
    {
      "epoch": 0.7352941176470589,
      "eval_loss": 0.35208961367607117,
      "eval_runtime": 215.1939,
      "eval_samples_per_second": 0.465,
      "eval_steps_per_second": 0.465,
      "step": 2500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.494529881689883e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
