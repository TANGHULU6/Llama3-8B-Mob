{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0969227041434456,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 3.876908165737824e-05,
      "grad_norm": 2.9230403900146484,
      "learning_rate": 4e-05,
      "loss": 3.5254,
      "step": 1
    },
    {
      "epoch": 7.753816331475648e-05,
      "grad_norm": 2.9728610515594482,
      "learning_rate": 8e-05,
      "loss": 3.501,
      "step": 2
    },
    {
      "epoch": 0.00011630724497213472,
      "grad_norm": 2.602348804473877,
      "learning_rate": 0.00012,
      "loss": 3.3968,
      "step": 3
    },
    {
      "epoch": 0.00015507632662951296,
      "grad_norm": 1.888668417930603,
      "learning_rate": 0.00016,
      "loss": 3.2776,
      "step": 4
    },
    {
      "epoch": 0.0001938454082868912,
      "grad_norm": 2.0576820373535156,
      "learning_rate": 0.0002,
      "loss": 3.1686,
      "step": 5
    },
    {
      "epoch": 0.00023261448994426945,
      "grad_norm": 2.360640287399292,
      "learning_rate": 0.00019999224445478518,
      "loss": 2.915,
      "step": 6
    },
    {
      "epoch": 0.0002713835716016477,
      "grad_norm": 2.034036636352539,
      "learning_rate": 0.00019998448890957036,
      "loss": 2.6996,
      "step": 7
    },
    {
      "epoch": 0.00031015265325902593,
      "grad_norm": 1.719345211982727,
      "learning_rate": 0.00019997673336435553,
      "loss": 2.5063,
      "step": 8
    },
    {
      "epoch": 0.0003489217349164042,
      "grad_norm": 1.3464865684509277,
      "learning_rate": 0.0001999689778191407,
      "loss": 2.3007,
      "step": 9
    },
    {
      "epoch": 0.0003876908165737824,
      "grad_norm": 1.3057857751846313,
      "learning_rate": 0.00019996122227392587,
      "loss": 2.1478,
      "step": 10
    },
    {
      "epoch": 0.00042645989823116064,
      "grad_norm": 1.516593337059021,
      "learning_rate": 0.00019995346672871105,
      "loss": 1.8987,
      "step": 11
    },
    {
      "epoch": 0.0004652289798885389,
      "grad_norm": 2.7924110889434814,
      "learning_rate": 0.00019994571118349622,
      "loss": 1.8408,
      "step": 12
    },
    {
      "epoch": 0.0005039980615459171,
      "grad_norm": 1.7557216882705688,
      "learning_rate": 0.00019993795563828136,
      "loss": 1.7127,
      "step": 13
    },
    {
      "epoch": 0.0005427671432032953,
      "grad_norm": 1.2475104331970215,
      "learning_rate": 0.00019993020009306656,
      "loss": 1.7299,
      "step": 14
    },
    {
      "epoch": 0.0005815362248606736,
      "grad_norm": 2.00046706199646,
      "learning_rate": 0.0001999224445478517,
      "loss": 1.6083,
      "step": 15
    },
    {
      "epoch": 0.0006203053065180519,
      "grad_norm": 1.4280766248703003,
      "learning_rate": 0.0001999146890026369,
      "loss": 1.3585,
      "step": 16
    },
    {
      "epoch": 0.0006590743881754301,
      "grad_norm": 1.5705692768096924,
      "learning_rate": 0.00019990693345742206,
      "loss": 1.3294,
      "step": 17
    },
    {
      "epoch": 0.0006978434698328084,
      "grad_norm": 2.0808141231536865,
      "learning_rate": 0.00019989917791220726,
      "loss": 1.2745,
      "step": 18
    },
    {
      "epoch": 0.0007366125514901866,
      "grad_norm": 1.1891632080078125,
      "learning_rate": 0.0001998914223669924,
      "loss": 1.1397,
      "step": 19
    },
    {
      "epoch": 0.0007753816331475648,
      "grad_norm": 2.031670570373535,
      "learning_rate": 0.00019988366682177757,
      "loss": 0.9842,
      "step": 20
    },
    {
      "epoch": 0.000814150714804943,
      "grad_norm": 1.32050621509552,
      "learning_rate": 0.00019987591127656275,
      "loss": 0.9566,
      "step": 21
    },
    {
      "epoch": 0.0008529197964623213,
      "grad_norm": 1.065791130065918,
      "learning_rate": 0.00019986815573134792,
      "loss": 0.9263,
      "step": 22
    },
    {
      "epoch": 0.0008916888781196995,
      "grad_norm": 0.8470771908760071,
      "learning_rate": 0.0001998604001861331,
      "loss": 0.9322,
      "step": 23
    },
    {
      "epoch": 0.0009304579597770778,
      "grad_norm": 1.1364352703094482,
      "learning_rate": 0.00019985264464091827,
      "loss": 0.9283,
      "step": 24
    },
    {
      "epoch": 0.000969227041434456,
      "grad_norm": 0.9388750195503235,
      "learning_rate": 0.00019984488909570344,
      "loss": 0.8513,
      "step": 25
    },
    {
      "epoch": 0.0010079961230918342,
      "grad_norm": 0.8084384202957153,
      "learning_rate": 0.0001998371335504886,
      "loss": 0.9695,
      "step": 26
    },
    {
      "epoch": 0.0010467652047492124,
      "grad_norm": 0.622874915599823,
      "learning_rate": 0.00019982937800527378,
      "loss": 0.9047,
      "step": 27
    },
    {
      "epoch": 0.0010855342864065907,
      "grad_norm": 0.5236457586288452,
      "learning_rate": 0.00019982162246005896,
      "loss": 0.8968,
      "step": 28
    },
    {
      "epoch": 0.001124303368063969,
      "grad_norm": 0.7239035367965698,
      "learning_rate": 0.00019981386691484413,
      "loss": 0.9109,
      "step": 29
    },
    {
      "epoch": 0.0011630724497213472,
      "grad_norm": 0.6791359782218933,
      "learning_rate": 0.0001998061113696293,
      "loss": 0.9015,
      "step": 30
    },
    {
      "epoch": 0.0012018415313787255,
      "grad_norm": 1.4358574151992798,
      "learning_rate": 0.00019979835582441448,
      "loss": 0.9787,
      "step": 31
    },
    {
      "epoch": 0.0012406106130361037,
      "grad_norm": 0.7053666710853577,
      "learning_rate": 0.00019979060027919965,
      "loss": 0.8346,
      "step": 32
    },
    {
      "epoch": 0.001279379694693482,
      "grad_norm": 0.7978454232215881,
      "learning_rate": 0.00019978284473398482,
      "loss": 0.9314,
      "step": 33
    },
    {
      "epoch": 0.0013181487763508602,
      "grad_norm": 1.066017746925354,
      "learning_rate": 0.00019977508918876997,
      "loss": 0.8579,
      "step": 34
    },
    {
      "epoch": 0.0013569178580082385,
      "grad_norm": 0.6724333167076111,
      "learning_rate": 0.00019976733364355517,
      "loss": 0.8359,
      "step": 35
    },
    {
      "epoch": 0.0013956869396656167,
      "grad_norm": 0.6842663884162903,
      "learning_rate": 0.0001997595780983403,
      "loss": 0.9626,
      "step": 36
    },
    {
      "epoch": 0.001434456021322995,
      "grad_norm": 0.7215273976325989,
      "learning_rate": 0.0001997518225531255,
      "loss": 0.8302,
      "step": 37
    },
    {
      "epoch": 0.0014732251029803732,
      "grad_norm": 1.0165963172912598,
      "learning_rate": 0.00019974406700791066,
      "loss": 0.7839,
      "step": 38
    },
    {
      "epoch": 0.0015119941846377515,
      "grad_norm": 0.7261216044425964,
      "learning_rate": 0.00019973631146269586,
      "loss": 0.7824,
      "step": 39
    },
    {
      "epoch": 0.0015507632662951295,
      "grad_norm": 1.400816798210144,
      "learning_rate": 0.000199728555917481,
      "loss": 0.8116,
      "step": 40
    },
    {
      "epoch": 0.0015895323479525078,
      "grad_norm": 1.2097454071044922,
      "learning_rate": 0.00019972080037226618,
      "loss": 0.825,
      "step": 41
    },
    {
      "epoch": 0.001628301429609886,
      "grad_norm": 2.607693910598755,
      "learning_rate": 0.00019971304482705135,
      "loss": 0.7427,
      "step": 42
    },
    {
      "epoch": 0.0016670705112672643,
      "grad_norm": 1.8811224699020386,
      "learning_rate": 0.00019970528928183652,
      "loss": 0.7219,
      "step": 43
    },
    {
      "epoch": 0.0017058395929246426,
      "grad_norm": 1.5005086660385132,
      "learning_rate": 0.0001996975337366217,
      "loss": 0.6583,
      "step": 44
    },
    {
      "epoch": 0.0017446086745820208,
      "grad_norm": 1.2151750326156616,
      "learning_rate": 0.00019968977819140687,
      "loss": 0.5819,
      "step": 45
    },
    {
      "epoch": 0.001783377756239399,
      "grad_norm": 1.1026912927627563,
      "learning_rate": 0.00019968202264619204,
      "loss": 0.7084,
      "step": 46
    },
    {
      "epoch": 0.0018221468378967773,
      "grad_norm": 1.3144177198410034,
      "learning_rate": 0.0001996742671009772,
      "loss": 0.6346,
      "step": 47
    },
    {
      "epoch": 0.0018609159195541556,
      "grad_norm": 3.076739549636841,
      "learning_rate": 0.00019966651155576236,
      "loss": 0.6793,
      "step": 48
    },
    {
      "epoch": 0.0018996850012115338,
      "grad_norm": 2.344365358352661,
      "learning_rate": 0.00019965875601054756,
      "loss": 0.7315,
      "step": 49
    },
    {
      "epoch": 0.001938454082868912,
      "grad_norm": 2.0453948974609375,
      "learning_rate": 0.0001996510004653327,
      "loss": 0.539,
      "step": 50
    },
    {
      "epoch": 0.0019772231645262903,
      "grad_norm": 1.2270336151123047,
      "learning_rate": 0.0001996432449201179,
      "loss": 0.5726,
      "step": 51
    },
    {
      "epoch": 0.0020159922461836684,
      "grad_norm": 1.0815093517303467,
      "learning_rate": 0.00019963548937490305,
      "loss": 0.6521,
      "step": 52
    },
    {
      "epoch": 0.002054761327841047,
      "grad_norm": 1.9334309101104736,
      "learning_rate": 0.00019962773382968825,
      "loss": 0.6541,
      "step": 53
    },
    {
      "epoch": 0.002093530409498425,
      "grad_norm": 1.8983532190322876,
      "learning_rate": 0.0001996199782844734,
      "loss": 0.4936,
      "step": 54
    },
    {
      "epoch": 0.0021322994911558034,
      "grad_norm": 1.2337671518325806,
      "learning_rate": 0.00019961222273925857,
      "loss": 0.6425,
      "step": 55
    },
    {
      "epoch": 0.0021710685728131814,
      "grad_norm": 1.8018003702163696,
      "learning_rate": 0.00019960446719404374,
      "loss": 0.4729,
      "step": 56
    },
    {
      "epoch": 0.00220983765447056,
      "grad_norm": 1.631045937538147,
      "learning_rate": 0.00019959671164882891,
      "loss": 0.5205,
      "step": 57
    },
    {
      "epoch": 0.002248606736127938,
      "grad_norm": 1.0424281358718872,
      "learning_rate": 0.0001995889561036141,
      "loss": 0.3898,
      "step": 58
    },
    {
      "epoch": 0.0022873758177853164,
      "grad_norm": 1.7853654623031616,
      "learning_rate": 0.00019958120055839926,
      "loss": 0.5223,
      "step": 59
    },
    {
      "epoch": 0.0023261448994426944,
      "grad_norm": 1.6207019090652466,
      "learning_rate": 0.00019957344501318443,
      "loss": 0.5722,
      "step": 60
    },
    {
      "epoch": 0.002364913981100073,
      "grad_norm": 0.8296762704849243,
      "learning_rate": 0.0001995656894679696,
      "loss": 0.5489,
      "step": 61
    },
    {
      "epoch": 0.002403683062757451,
      "grad_norm": 0.4752282500267029,
      "learning_rate": 0.00019955793392275478,
      "loss": 0.5042,
      "step": 62
    },
    {
      "epoch": 0.002442452144414829,
      "grad_norm": 0.6248577833175659,
      "learning_rate": 0.00019955017837753995,
      "loss": 0.4223,
      "step": 63
    },
    {
      "epoch": 0.0024812212260722074,
      "grad_norm": 0.7937059998512268,
      "learning_rate": 0.00019954242283232512,
      "loss": 0.6155,
      "step": 64
    },
    {
      "epoch": 0.0025199903077295855,
      "grad_norm": 0.7163397669792175,
      "learning_rate": 0.0001995346672871103,
      "loss": 0.4599,
      "step": 65
    },
    {
      "epoch": 0.002558759389386964,
      "grad_norm": 0.5749192833900452,
      "learning_rate": 0.00019952691174189547,
      "loss": 0.5089,
      "step": 66
    },
    {
      "epoch": 0.002597528471044342,
      "grad_norm": 0.6175609827041626,
      "learning_rate": 0.00019951915619668064,
      "loss": 0.377,
      "step": 67
    },
    {
      "epoch": 0.0026362975527017204,
      "grad_norm": 0.4376225173473358,
      "learning_rate": 0.00019951140065146582,
      "loss": 0.4471,
      "step": 68
    },
    {
      "epoch": 0.0026750666343590985,
      "grad_norm": 0.6264302730560303,
      "learning_rate": 0.00019950364510625096,
      "loss": 0.4308,
      "step": 69
    },
    {
      "epoch": 0.002713835716016477,
      "grad_norm": 0.45756563544273376,
      "learning_rate": 0.00019949588956103616,
      "loss": 0.4173,
      "step": 70
    },
    {
      "epoch": 0.002752604797673855,
      "grad_norm": 0.4236374795436859,
      "learning_rate": 0.0001994881340158213,
      "loss": 0.4116,
      "step": 71
    },
    {
      "epoch": 0.0027913738793312335,
      "grad_norm": 0.6917219758033752,
      "learning_rate": 0.0001994803784706065,
      "loss": 0.4681,
      "step": 72
    },
    {
      "epoch": 0.0028301429609886115,
      "grad_norm": 0.5528842210769653,
      "learning_rate": 0.00019947262292539165,
      "loss": 0.5669,
      "step": 73
    },
    {
      "epoch": 0.00286891204264599,
      "grad_norm": 0.4958662986755371,
      "learning_rate": 0.00019946486738017685,
      "loss": 0.4476,
      "step": 74
    },
    {
      "epoch": 0.002907681124303368,
      "grad_norm": 0.6449024677276611,
      "learning_rate": 0.000199457111834962,
      "loss": 0.3656,
      "step": 75
    },
    {
      "epoch": 0.0029464502059607465,
      "grad_norm": 0.33932071924209595,
      "learning_rate": 0.00019944935628974717,
      "loss": 0.4241,
      "step": 76
    },
    {
      "epoch": 0.0029852192876181245,
      "grad_norm": 0.41205376386642456,
      "learning_rate": 0.00019944160074453234,
      "loss": 0.3988,
      "step": 77
    },
    {
      "epoch": 0.003023988369275503,
      "grad_norm": 0.4457258880138397,
      "learning_rate": 0.00019943384519931752,
      "loss": 0.3866,
      "step": 78
    },
    {
      "epoch": 0.003062757450932881,
      "grad_norm": 0.47480031847953796,
      "learning_rate": 0.0001994260896541027,
      "loss": 0.4328,
      "step": 79
    },
    {
      "epoch": 0.003101526532590259,
      "grad_norm": 0.3692890703678131,
      "learning_rate": 0.00019941833410888786,
      "loss": 0.4129,
      "step": 80
    },
    {
      "epoch": 0.0031402956142476375,
      "grad_norm": 0.5300651788711548,
      "learning_rate": 0.00019941057856367303,
      "loss": 0.3446,
      "step": 81
    },
    {
      "epoch": 0.0031790646959050156,
      "grad_norm": 0.38094469904899597,
      "learning_rate": 0.0001994028230184582,
      "loss": 0.3706,
      "step": 82
    },
    {
      "epoch": 0.003217833777562394,
      "grad_norm": 0.44736218452453613,
      "learning_rate": 0.00019939506747324338,
      "loss": 0.396,
      "step": 83
    },
    {
      "epoch": 0.003256602859219772,
      "grad_norm": 0.5638589859008789,
      "learning_rate": 0.00019938731192802855,
      "loss": 0.3678,
      "step": 84
    },
    {
      "epoch": 0.0032953719408771506,
      "grad_norm": 0.35411328077316284,
      "learning_rate": 0.00019937955638281373,
      "loss": 0.3928,
      "step": 85
    },
    {
      "epoch": 0.0033341410225345286,
      "grad_norm": 0.8422765135765076,
      "learning_rate": 0.0001993718008375989,
      "loss": 0.496,
      "step": 86
    },
    {
      "epoch": 0.003372910104191907,
      "grad_norm": 0.381354421377182,
      "learning_rate": 0.00019936404529238407,
      "loss": 0.4217,
      "step": 87
    },
    {
      "epoch": 0.003411679185849285,
      "grad_norm": 0.41054025292396545,
      "learning_rate": 0.00019935628974716924,
      "loss": 0.4166,
      "step": 88
    },
    {
      "epoch": 0.0034504482675066636,
      "grad_norm": 0.4218270778656006,
      "learning_rate": 0.00019934853420195442,
      "loss": 0.4048,
      "step": 89
    },
    {
      "epoch": 0.0034892173491640416,
      "grad_norm": 0.37158849835395813,
      "learning_rate": 0.00019934077865673956,
      "loss": 0.3391,
      "step": 90
    },
    {
      "epoch": 0.00352798643082142,
      "grad_norm": 0.3715416193008423,
      "learning_rate": 0.00019933302311152476,
      "loss": 0.4155,
      "step": 91
    },
    {
      "epoch": 0.003566755512478798,
      "grad_norm": 0.5655447840690613,
      "learning_rate": 0.0001993252675663099,
      "loss": 0.4163,
      "step": 92
    },
    {
      "epoch": 0.0036055245941361766,
      "grad_norm": 0.39139705896377563,
      "learning_rate": 0.0001993175120210951,
      "loss": 0.4716,
      "step": 93
    },
    {
      "epoch": 0.0036442936757935546,
      "grad_norm": 0.4194222688674927,
      "learning_rate": 0.00019930975647588025,
      "loss": 0.3666,
      "step": 94
    },
    {
      "epoch": 0.0036830627574509327,
      "grad_norm": 0.30195069313049316,
      "learning_rate": 0.00019930200093066545,
      "loss": 0.47,
      "step": 95
    },
    {
      "epoch": 0.003721831839108311,
      "grad_norm": 0.2908288240432739,
      "learning_rate": 0.0001992942453854506,
      "loss": 0.3742,
      "step": 96
    },
    {
      "epoch": 0.003760600920765689,
      "grad_norm": 0.4658166766166687,
      "learning_rate": 0.00019928648984023577,
      "loss": 0.3734,
      "step": 97
    },
    {
      "epoch": 0.0037993700024230677,
      "grad_norm": 0.2506216764450073,
      "learning_rate": 0.00019927873429502095,
      "loss": 0.3404,
      "step": 98
    },
    {
      "epoch": 0.0038381390840804457,
      "grad_norm": 0.34372252225875854,
      "learning_rate": 0.00019927097874980612,
      "loss": 0.3836,
      "step": 99
    },
    {
      "epoch": 0.003876908165737824,
      "grad_norm": 0.35475262999534607,
      "learning_rate": 0.0001992632232045913,
      "loss": 0.4652,
      "step": 100
    },
    {
      "epoch": 0.003915677247395203,
      "grad_norm": 0.3077355921268463,
      "learning_rate": 0.00019925546765937646,
      "loss": 0.5019,
      "step": 101
    },
    {
      "epoch": 0.003954446329052581,
      "grad_norm": 0.35015109181404114,
      "learning_rate": 0.00019924771211416164,
      "loss": 0.3415,
      "step": 102
    },
    {
      "epoch": 0.003993215410709959,
      "grad_norm": 0.3876751661300659,
      "learning_rate": 0.0001992399565689468,
      "loss": 0.3819,
      "step": 103
    },
    {
      "epoch": 0.004031984492367337,
      "grad_norm": 0.31146731972694397,
      "learning_rate": 0.00019923220102373195,
      "loss": 0.336,
      "step": 104
    },
    {
      "epoch": 0.004070753574024716,
      "grad_norm": 0.22270101308822632,
      "learning_rate": 0.00019922444547851715,
      "loss": 0.36,
      "step": 105
    },
    {
      "epoch": 0.004109522655682094,
      "grad_norm": 0.26990437507629395,
      "learning_rate": 0.00019921668993330233,
      "loss": 0.4258,
      "step": 106
    },
    {
      "epoch": 0.004148291737339472,
      "grad_norm": 0.32246652245521545,
      "learning_rate": 0.0001992089343880875,
      "loss": 0.4101,
      "step": 107
    },
    {
      "epoch": 0.00418706081899685,
      "grad_norm": 0.3357292711734772,
      "learning_rate": 0.00019920117884287267,
      "loss": 0.4459,
      "step": 108
    },
    {
      "epoch": 0.004225829900654229,
      "grad_norm": 0.21544921398162842,
      "learning_rate": 0.00019919342329765785,
      "loss": 0.387,
      "step": 109
    },
    {
      "epoch": 0.004264598982311607,
      "grad_norm": 0.31889063119888306,
      "learning_rate": 0.00019918566775244302,
      "loss": 0.5532,
      "step": 110
    },
    {
      "epoch": 0.004303368063968985,
      "grad_norm": 0.3546103537082672,
      "learning_rate": 0.00019917791220722816,
      "loss": 0.4151,
      "step": 111
    },
    {
      "epoch": 0.004342137145626363,
      "grad_norm": 0.22958186268806458,
      "learning_rate": 0.00019917015666201336,
      "loss": 0.3035,
      "step": 112
    },
    {
      "epoch": 0.004380906227283741,
      "grad_norm": 0.20088817179203033,
      "learning_rate": 0.0001991624011167985,
      "loss": 0.399,
      "step": 113
    },
    {
      "epoch": 0.00441967530894112,
      "grad_norm": 0.33017146587371826,
      "learning_rate": 0.0001991546455715837,
      "loss": 0.4771,
      "step": 114
    },
    {
      "epoch": 0.004458444390598498,
      "grad_norm": 0.31551429629325867,
      "learning_rate": 0.00019914689002636886,
      "loss": 0.4771,
      "step": 115
    },
    {
      "epoch": 0.004497213472255876,
      "grad_norm": 0.2243560403585434,
      "learning_rate": 0.00019913913448115406,
      "loss": 0.3735,
      "step": 116
    },
    {
      "epoch": 0.004535982553913254,
      "grad_norm": 0.2509166896343231,
      "learning_rate": 0.0001991313789359392,
      "loss": 0.345,
      "step": 117
    },
    {
      "epoch": 0.004574751635570633,
      "grad_norm": 0.2468937635421753,
      "learning_rate": 0.00019912362339072437,
      "loss": 0.3935,
      "step": 118
    },
    {
      "epoch": 0.004613520717228011,
      "grad_norm": 0.20808769762516022,
      "learning_rate": 0.00019911586784550955,
      "loss": 0.4382,
      "step": 119
    },
    {
      "epoch": 0.004652289798885389,
      "grad_norm": 0.33053871989250183,
      "learning_rate": 0.00019910811230029472,
      "loss": 0.3894,
      "step": 120
    },
    {
      "epoch": 0.004691058880542767,
      "grad_norm": 0.2554713189601898,
      "learning_rate": 0.0001991003567550799,
      "loss": 0.3369,
      "step": 121
    },
    {
      "epoch": 0.004729827962200146,
      "grad_norm": 0.3124626576900482,
      "learning_rate": 0.00019909260120986507,
      "loss": 0.458,
      "step": 122
    },
    {
      "epoch": 0.004768597043857524,
      "grad_norm": 0.25137564539909363,
      "learning_rate": 0.00019908484566465024,
      "loss": 0.3361,
      "step": 123
    },
    {
      "epoch": 0.004807366125514902,
      "grad_norm": 0.2105439156293869,
      "learning_rate": 0.0001990770901194354,
      "loss": 0.3712,
      "step": 124
    },
    {
      "epoch": 0.00484613520717228,
      "grad_norm": 0.2221527397632599,
      "learning_rate": 0.00019906933457422056,
      "loss": 0.3082,
      "step": 125
    },
    {
      "epoch": 0.004884904288829658,
      "grad_norm": 0.4388672113418579,
      "learning_rate": 0.00019906157902900576,
      "loss": 0.4884,
      "step": 126
    },
    {
      "epoch": 0.004923673370487037,
      "grad_norm": 0.25225573778152466,
      "learning_rate": 0.0001990538234837909,
      "loss": 0.3389,
      "step": 127
    },
    {
      "epoch": 0.004962442452144415,
      "grad_norm": 0.2689926326274872,
      "learning_rate": 0.0001990460679385761,
      "loss": 0.3869,
      "step": 128
    },
    {
      "epoch": 0.005001211533801793,
      "grad_norm": 0.21025776863098145,
      "learning_rate": 0.00019903831239336125,
      "loss": 0.3295,
      "step": 129
    },
    {
      "epoch": 0.005039980615459171,
      "grad_norm": 0.28117847442626953,
      "learning_rate": 0.00019903055684814645,
      "loss": 0.3619,
      "step": 130
    },
    {
      "epoch": 0.00507874969711655,
      "grad_norm": 0.24307838082313538,
      "learning_rate": 0.0001990228013029316,
      "loss": 0.3603,
      "step": 131
    },
    {
      "epoch": 0.005117518778773928,
      "grad_norm": 0.19048719108104706,
      "learning_rate": 0.00019901504575771677,
      "loss": 0.3613,
      "step": 132
    },
    {
      "epoch": 0.005156287860431306,
      "grad_norm": 0.2224605828523636,
      "learning_rate": 0.00019900729021250194,
      "loss": 0.402,
      "step": 133
    },
    {
      "epoch": 0.005195056942088684,
      "grad_norm": 0.23811571300029755,
      "learning_rate": 0.0001989995346672871,
      "loss": 0.439,
      "step": 134
    },
    {
      "epoch": 0.005233826023746063,
      "grad_norm": 0.24107198417186737,
      "learning_rate": 0.00019899177912207228,
      "loss": 0.3484,
      "step": 135
    },
    {
      "epoch": 0.005272595105403441,
      "grad_norm": 0.2605302929878235,
      "learning_rate": 0.00019898402357685746,
      "loss": 0.4287,
      "step": 136
    },
    {
      "epoch": 0.005311364187060819,
      "grad_norm": 0.21079882979393005,
      "learning_rate": 0.00019897626803164263,
      "loss": 0.3634,
      "step": 137
    },
    {
      "epoch": 0.005350133268718197,
      "grad_norm": 0.20935696363449097,
      "learning_rate": 0.0001989685124864278,
      "loss": 0.4098,
      "step": 138
    },
    {
      "epoch": 0.005388902350375576,
      "grad_norm": 0.32826367020606995,
      "learning_rate": 0.00019896075694121298,
      "loss": 0.355,
      "step": 139
    },
    {
      "epoch": 0.005427671432032954,
      "grad_norm": 0.2559847831726074,
      "learning_rate": 0.00019895300139599815,
      "loss": 0.387,
      "step": 140
    },
    {
      "epoch": 0.005466440513690332,
      "grad_norm": 0.22332607209682465,
      "learning_rate": 0.00019894524585078332,
      "loss": 0.4326,
      "step": 141
    },
    {
      "epoch": 0.00550520959534771,
      "grad_norm": 0.23543040454387665,
      "learning_rate": 0.0001989374903055685,
      "loss": 0.3601,
      "step": 142
    },
    {
      "epoch": 0.005543978677005088,
      "grad_norm": 0.22014300525188446,
      "learning_rate": 0.00019892973476035367,
      "loss": 0.3377,
      "step": 143
    },
    {
      "epoch": 0.005582747758662467,
      "grad_norm": 0.19583342969417572,
      "learning_rate": 0.00019892197921513884,
      "loss": 0.3768,
      "step": 144
    },
    {
      "epoch": 0.005621516840319845,
      "grad_norm": 0.18667078018188477,
      "learning_rate": 0.000198914223669924,
      "loss": 0.3089,
      "step": 145
    },
    {
      "epoch": 0.005660285921977223,
      "grad_norm": 0.2764764428138733,
      "learning_rate": 0.00019890646812470916,
      "loss": 0.3909,
      "step": 146
    },
    {
      "epoch": 0.005699055003634601,
      "grad_norm": 0.2581457793712616,
      "learning_rate": 0.00019889871257949436,
      "loss": 0.3749,
      "step": 147
    },
    {
      "epoch": 0.00573782408529198,
      "grad_norm": 0.23061729967594147,
      "learning_rate": 0.0001988909570342795,
      "loss": 0.3478,
      "step": 148
    },
    {
      "epoch": 0.005776593166949358,
      "grad_norm": 0.2585600018501282,
      "learning_rate": 0.0001988832014890647,
      "loss": 0.3833,
      "step": 149
    },
    {
      "epoch": 0.005815362248606736,
      "grad_norm": 0.24284476041793823,
      "learning_rate": 0.00019887544594384985,
      "loss": 0.3863,
      "step": 150
    },
    {
      "epoch": 0.005854131330264114,
      "grad_norm": 0.2630809545516968,
      "learning_rate": 0.00019886769039863505,
      "loss": 0.4452,
      "step": 151
    },
    {
      "epoch": 0.005892900411921493,
      "grad_norm": 0.30988144874572754,
      "learning_rate": 0.0001988599348534202,
      "loss": 0.4985,
      "step": 152
    },
    {
      "epoch": 0.005931669493578871,
      "grad_norm": 0.2957611083984375,
      "learning_rate": 0.00019885217930820537,
      "loss": 0.3851,
      "step": 153
    },
    {
      "epoch": 0.005970438575236249,
      "grad_norm": 0.17687276005744934,
      "learning_rate": 0.00019884442376299054,
      "loss": 0.4051,
      "step": 154
    },
    {
      "epoch": 0.006009207656893627,
      "grad_norm": 0.18523411452770233,
      "learning_rate": 0.00019883666821777571,
      "loss": 0.297,
      "step": 155
    },
    {
      "epoch": 0.006047976738551006,
      "grad_norm": 0.2911287844181061,
      "learning_rate": 0.0001988289126725609,
      "loss": 0.3899,
      "step": 156
    },
    {
      "epoch": 0.006086745820208384,
      "grad_norm": 0.265298068523407,
      "learning_rate": 0.00019882115712734606,
      "loss": 0.4147,
      "step": 157
    },
    {
      "epoch": 0.006125514901865762,
      "grad_norm": 0.19885624945163727,
      "learning_rate": 0.00019881340158213123,
      "loss": 0.3762,
      "step": 158
    },
    {
      "epoch": 0.00616428398352314,
      "grad_norm": 0.2756243050098419,
      "learning_rate": 0.0001988056460369164,
      "loss": 0.3785,
      "step": 159
    },
    {
      "epoch": 0.006203053065180518,
      "grad_norm": 0.29343855381011963,
      "learning_rate": 0.00019879789049170158,
      "loss": 0.3712,
      "step": 160
    },
    {
      "epoch": 0.006241822146837897,
      "grad_norm": 0.25586599111557007,
      "learning_rate": 0.00019879013494648675,
      "loss": 0.3969,
      "step": 161
    },
    {
      "epoch": 0.006280591228495275,
      "grad_norm": 0.3250058889389038,
      "learning_rate": 0.00019878237940127192,
      "loss": 0.4723,
      "step": 162
    },
    {
      "epoch": 0.006319360310152653,
      "grad_norm": 0.2455916404724121,
      "learning_rate": 0.0001987746238560571,
      "loss": 0.3313,
      "step": 163
    },
    {
      "epoch": 0.006358129391810031,
      "grad_norm": 0.2899685204029083,
      "learning_rate": 0.00019876686831084227,
      "loss": 0.3853,
      "step": 164
    },
    {
      "epoch": 0.00639689847346741,
      "grad_norm": 0.2092159390449524,
      "learning_rate": 0.00019875911276562744,
      "loss": 0.3279,
      "step": 165
    },
    {
      "epoch": 0.006435667555124788,
      "grad_norm": 0.3109884560108185,
      "learning_rate": 0.00019875135722041261,
      "loss": 0.4469,
      "step": 166
    },
    {
      "epoch": 0.006474436636782166,
      "grad_norm": 0.3385792374610901,
      "learning_rate": 0.00019874360167519776,
      "loss": 0.382,
      "step": 167
    },
    {
      "epoch": 0.006513205718439544,
      "grad_norm": 0.24273033440113068,
      "learning_rate": 0.00019873584612998296,
      "loss": 0.3674,
      "step": 168
    },
    {
      "epoch": 0.006551974800096923,
      "grad_norm": 0.22174479067325592,
      "learning_rate": 0.0001987280905847681,
      "loss": 0.2885,
      "step": 169
    },
    {
      "epoch": 0.006590743881754301,
      "grad_norm": 0.2220771163702011,
      "learning_rate": 0.0001987203350395533,
      "loss": 0.2828,
      "step": 170
    },
    {
      "epoch": 0.006629512963411679,
      "grad_norm": 0.22016362845897675,
      "learning_rate": 0.00019871257949433845,
      "loss": 0.3072,
      "step": 171
    },
    {
      "epoch": 0.006668282045069057,
      "grad_norm": 0.2534507215023041,
      "learning_rate": 0.00019870482394912365,
      "loss": 0.3672,
      "step": 172
    },
    {
      "epoch": 0.006707051126726436,
      "grad_norm": 0.2035534530878067,
      "learning_rate": 0.0001986970684039088,
      "loss": 0.3944,
      "step": 173
    },
    {
      "epoch": 0.006745820208383814,
      "grad_norm": 0.26666587591171265,
      "learning_rate": 0.00019868931285869397,
      "loss": 0.4368,
      "step": 174
    },
    {
      "epoch": 0.006784589290041192,
      "grad_norm": 1.4885716438293457,
      "learning_rate": 0.00019868155731347914,
      "loss": 0.4004,
      "step": 175
    },
    {
      "epoch": 0.00682335837169857,
      "grad_norm": 0.2214568555355072,
      "learning_rate": 0.00019867380176826432,
      "loss": 0.3292,
      "step": 176
    },
    {
      "epoch": 0.006862127453355948,
      "grad_norm": 3.1462607383728027,
      "learning_rate": 0.0001986660462230495,
      "loss": 0.3781,
      "step": 177
    },
    {
      "epoch": 0.006900896535013327,
      "grad_norm": 0.24755346775054932,
      "learning_rate": 0.00019865829067783466,
      "loss": 0.4209,
      "step": 178
    },
    {
      "epoch": 0.006939665616670705,
      "grad_norm": 0.17932401597499847,
      "learning_rate": 0.00019865053513261983,
      "loss": 0.3562,
      "step": 179
    },
    {
      "epoch": 0.006978434698328083,
      "grad_norm": 0.2353314757347107,
      "learning_rate": 0.000198642779587405,
      "loss": 0.4042,
      "step": 180
    },
    {
      "epoch": 0.007017203779985461,
      "grad_norm": 0.2786005735397339,
      "learning_rate": 0.00019863502404219015,
      "loss": 0.4013,
      "step": 181
    },
    {
      "epoch": 0.00705597286164284,
      "grad_norm": 0.36645349860191345,
      "learning_rate": 0.00019862726849697535,
      "loss": 0.4744,
      "step": 182
    },
    {
      "epoch": 0.007094741943300218,
      "grad_norm": 0.2079298496246338,
      "learning_rate": 0.0001986195129517605,
      "loss": 0.3746,
      "step": 183
    },
    {
      "epoch": 0.007133511024957596,
      "grad_norm": 0.21631605923175812,
      "learning_rate": 0.0001986117574065457,
      "loss": 0.4156,
      "step": 184
    },
    {
      "epoch": 0.007172280106614974,
      "grad_norm": 0.28707534074783325,
      "learning_rate": 0.00019860400186133084,
      "loss": 0.3969,
      "step": 185
    },
    {
      "epoch": 0.007211049188272353,
      "grad_norm": 0.2143794596195221,
      "learning_rate": 0.00019859624631611604,
      "loss": 0.3813,
      "step": 186
    },
    {
      "epoch": 0.007249818269929731,
      "grad_norm": 0.21132655441761017,
      "learning_rate": 0.00019858849077090122,
      "loss": 0.3391,
      "step": 187
    },
    {
      "epoch": 0.007288587351587109,
      "grad_norm": 0.22449682652950287,
      "learning_rate": 0.00019858073522568636,
      "loss": 0.3404,
      "step": 188
    },
    {
      "epoch": 0.007327356433244487,
      "grad_norm": 0.25435206294059753,
      "learning_rate": 0.00019857297968047156,
      "loss": 0.3927,
      "step": 189
    },
    {
      "epoch": 0.007366125514901865,
      "grad_norm": 0.2559340298175812,
      "learning_rate": 0.0001985652241352567,
      "loss": 0.3802,
      "step": 190
    },
    {
      "epoch": 0.007404894596559244,
      "grad_norm": 0.20444971323013306,
      "learning_rate": 0.0001985574685900419,
      "loss": 0.3824,
      "step": 191
    },
    {
      "epoch": 0.007443663678216622,
      "grad_norm": 0.15645471215248108,
      "learning_rate": 0.00019854971304482705,
      "loss": 0.3501,
      "step": 192
    },
    {
      "epoch": 0.007482432759874,
      "grad_norm": 0.29072433710098267,
      "learning_rate": 0.00019854195749961225,
      "loss": 0.3813,
      "step": 193
    },
    {
      "epoch": 0.007521201841531378,
      "grad_norm": 0.21071089804172516,
      "learning_rate": 0.0001985342019543974,
      "loss": 0.349,
      "step": 194
    },
    {
      "epoch": 0.007559970923188757,
      "grad_norm": 0.2314879149198532,
      "learning_rate": 0.00019852644640918257,
      "loss": 0.4425,
      "step": 195
    },
    {
      "epoch": 0.007598740004846135,
      "grad_norm": 0.20747314393520355,
      "learning_rate": 0.00019851869086396774,
      "loss": 0.2847,
      "step": 196
    },
    {
      "epoch": 0.007637509086503513,
      "grad_norm": 0.23883546888828278,
      "learning_rate": 0.00019851093531875292,
      "loss": 0.4661,
      "step": 197
    },
    {
      "epoch": 0.007676278168160891,
      "grad_norm": 0.28168734908103943,
      "learning_rate": 0.0001985031797735381,
      "loss": 0.426,
      "step": 198
    },
    {
      "epoch": 0.00771504724981827,
      "grad_norm": 0.23158499598503113,
      "learning_rate": 0.00019849542422832326,
      "loss": 0.4451,
      "step": 199
    },
    {
      "epoch": 0.007753816331475648,
      "grad_norm": 0.2524152994155884,
      "learning_rate": 0.00019848766868310844,
      "loss": 0.4426,
      "step": 200
    },
    {
      "epoch": 0.007792585413133026,
      "grad_norm": 0.319896399974823,
      "learning_rate": 0.0001984799131378936,
      "loss": 0.4565,
      "step": 201
    },
    {
      "epoch": 0.007831354494790405,
      "grad_norm": 0.21280167996883392,
      "learning_rate": 0.00019847215759267875,
      "loss": 0.3817,
      "step": 202
    },
    {
      "epoch": 0.007870123576447783,
      "grad_norm": 0.2225416898727417,
      "learning_rate": 0.00019846440204746395,
      "loss": 0.4552,
      "step": 203
    },
    {
      "epoch": 0.007908892658105161,
      "grad_norm": 0.2604549527168274,
      "learning_rate": 0.0001984566465022491,
      "loss": 0.4121,
      "step": 204
    },
    {
      "epoch": 0.00794766173976254,
      "grad_norm": 0.19879518449306488,
      "learning_rate": 0.0001984488909570343,
      "loss": 0.3646,
      "step": 205
    },
    {
      "epoch": 0.007986430821419917,
      "grad_norm": 0.22521816194057465,
      "learning_rate": 0.00019844113541181945,
      "loss": 0.4652,
      "step": 206
    },
    {
      "epoch": 0.008025199903077295,
      "grad_norm": 0.22883793711662292,
      "learning_rate": 0.00019843337986660465,
      "loss": 0.3474,
      "step": 207
    },
    {
      "epoch": 0.008063968984734673,
      "grad_norm": 0.21625067293643951,
      "learning_rate": 0.0001984256243213898,
      "loss": 0.4296,
      "step": 208
    },
    {
      "epoch": 0.008102738066392052,
      "grad_norm": 0.18735408782958984,
      "learning_rate": 0.00019841786877617496,
      "loss": 0.3738,
      "step": 209
    },
    {
      "epoch": 0.008141507148049431,
      "grad_norm": 0.2535339593887329,
      "learning_rate": 0.00019841011323096014,
      "loss": 0.4173,
      "step": 210
    },
    {
      "epoch": 0.00818027622970681,
      "grad_norm": 0.15875427424907684,
      "learning_rate": 0.0001984023576857453,
      "loss": 0.3334,
      "step": 211
    },
    {
      "epoch": 0.008219045311364187,
      "grad_norm": 0.1572163999080658,
      "learning_rate": 0.00019839460214053048,
      "loss": 0.3304,
      "step": 212
    },
    {
      "epoch": 0.008257814393021565,
      "grad_norm": 0.22806194424629211,
      "learning_rate": 0.00019838684659531566,
      "loss": 0.3865,
      "step": 213
    },
    {
      "epoch": 0.008296583474678943,
      "grad_norm": 0.18296799063682556,
      "learning_rate": 0.00019837909105010083,
      "loss": 0.337,
      "step": 214
    },
    {
      "epoch": 0.008335352556336321,
      "grad_norm": 0.17683517932891846,
      "learning_rate": 0.000198371335504886,
      "loss": 0.3905,
      "step": 215
    },
    {
      "epoch": 0.0083741216379937,
      "grad_norm": 0.2188284546136856,
      "learning_rate": 0.00019836357995967117,
      "loss": 0.3723,
      "step": 216
    },
    {
      "epoch": 0.008412890719651078,
      "grad_norm": 0.16540655493736267,
      "learning_rate": 0.00019835582441445635,
      "loss": 0.246,
      "step": 217
    },
    {
      "epoch": 0.008451659801308457,
      "grad_norm": 0.20147579908370972,
      "learning_rate": 0.00019834806886924152,
      "loss": 0.4248,
      "step": 218
    },
    {
      "epoch": 0.008490428882965835,
      "grad_norm": 0.17072437703609467,
      "learning_rate": 0.0001983403133240267,
      "loss": 0.3281,
      "step": 219
    },
    {
      "epoch": 0.008529197964623213,
      "grad_norm": 0.23376506567001343,
      "learning_rate": 0.00019833255777881186,
      "loss": 0.414,
      "step": 220
    },
    {
      "epoch": 0.008567967046280591,
      "grad_norm": 0.20413747429847717,
      "learning_rate": 0.00019832480223359704,
      "loss": 0.3961,
      "step": 221
    },
    {
      "epoch": 0.00860673612793797,
      "grad_norm": 0.23441222310066223,
      "learning_rate": 0.0001983170466883822,
      "loss": 0.3363,
      "step": 222
    },
    {
      "epoch": 0.008645505209595348,
      "grad_norm": 0.1757137030363083,
      "learning_rate": 0.00019830929114316736,
      "loss": 0.3592,
      "step": 223
    },
    {
      "epoch": 0.008684274291252726,
      "grad_norm": 0.17620986700057983,
      "learning_rate": 0.00019830153559795256,
      "loss": 0.3542,
      "step": 224
    },
    {
      "epoch": 0.008723043372910104,
      "grad_norm": 0.19212448596954346,
      "learning_rate": 0.0001982937800527377,
      "loss": 0.3742,
      "step": 225
    },
    {
      "epoch": 0.008761812454567482,
      "grad_norm": 0.14896930754184723,
      "learning_rate": 0.0001982860245075229,
      "loss": 0.3273,
      "step": 226
    },
    {
      "epoch": 0.008800581536224861,
      "grad_norm": 0.1456153839826584,
      "learning_rate": 0.00019827826896230805,
      "loss": 0.2953,
      "step": 227
    },
    {
      "epoch": 0.00883935061788224,
      "grad_norm": 0.17887063324451447,
      "learning_rate": 0.00019827051341709325,
      "loss": 0.3625,
      "step": 228
    },
    {
      "epoch": 0.008878119699539617,
      "grad_norm": 0.1582985818386078,
      "learning_rate": 0.0001982627578718784,
      "loss": 0.3813,
      "step": 229
    },
    {
      "epoch": 0.008916888781196996,
      "grad_norm": 0.1736868917942047,
      "learning_rate": 0.00019825500232666357,
      "loss": 0.3342,
      "step": 230
    },
    {
      "epoch": 0.008955657862854374,
      "grad_norm": 0.12840498983860016,
      "learning_rate": 0.00019824724678144874,
      "loss": 0.2974,
      "step": 231
    },
    {
      "epoch": 0.008994426944511752,
      "grad_norm": 0.2300778478384018,
      "learning_rate": 0.0001982394912362339,
      "loss": 0.4068,
      "step": 232
    },
    {
      "epoch": 0.00903319602616913,
      "grad_norm": 0.1750321090221405,
      "learning_rate": 0.00019823173569101908,
      "loss": 0.3147,
      "step": 233
    },
    {
      "epoch": 0.009071965107826508,
      "grad_norm": 0.19468194246292114,
      "learning_rate": 0.00019822398014580426,
      "loss": 0.3456,
      "step": 234
    },
    {
      "epoch": 0.009110734189483887,
      "grad_norm": 0.1704874038696289,
      "learning_rate": 0.00019821622460058943,
      "loss": 0.3335,
      "step": 235
    },
    {
      "epoch": 0.009149503271141265,
      "grad_norm": 0.1997767686843872,
      "learning_rate": 0.0001982084690553746,
      "loss": 0.3981,
      "step": 236
    },
    {
      "epoch": 0.009188272352798644,
      "grad_norm": 0.29413455724716187,
      "learning_rate": 0.00019820071351015978,
      "loss": 0.4936,
      "step": 237
    },
    {
      "epoch": 0.009227041434456022,
      "grad_norm": 0.18746864795684814,
      "learning_rate": 0.00019819295796494495,
      "loss": 0.3378,
      "step": 238
    },
    {
      "epoch": 0.0092658105161134,
      "grad_norm": 0.23890095949172974,
      "learning_rate": 0.00019818520241973012,
      "loss": 0.4311,
      "step": 239
    },
    {
      "epoch": 0.009304579597770778,
      "grad_norm": 0.2671896517276764,
      "learning_rate": 0.0001981774468745153,
      "loss": 0.4022,
      "step": 240
    },
    {
      "epoch": 0.009343348679428156,
      "grad_norm": 0.23823219537734985,
      "learning_rate": 0.00019816969132930047,
      "loss": 0.4079,
      "step": 241
    },
    {
      "epoch": 0.009382117761085534,
      "grad_norm": 0.17543238401412964,
      "learning_rate": 0.00019816193578408564,
      "loss": 0.2953,
      "step": 242
    },
    {
      "epoch": 0.009420886842742912,
      "grad_norm": 0.2410370409488678,
      "learning_rate": 0.0001981541802388708,
      "loss": 0.3029,
      "step": 243
    },
    {
      "epoch": 0.009459655924400292,
      "grad_norm": 0.21385610103607178,
      "learning_rate": 0.00019814642469365596,
      "loss": 0.4091,
      "step": 244
    },
    {
      "epoch": 0.00949842500605767,
      "grad_norm": 0.28087151050567627,
      "learning_rate": 0.00019813866914844116,
      "loss": 0.3801,
      "step": 245
    },
    {
      "epoch": 0.009537194087715048,
      "grad_norm": 0.16606636345386505,
      "learning_rate": 0.0001981309136032263,
      "loss": 0.3553,
      "step": 246
    },
    {
      "epoch": 0.009575963169372426,
      "grad_norm": 0.17693157494068146,
      "learning_rate": 0.0001981231580580115,
      "loss": 0.3527,
      "step": 247
    },
    {
      "epoch": 0.009614732251029804,
      "grad_norm": 0.26819562911987305,
      "learning_rate": 0.00019811540251279665,
      "loss": 0.3776,
      "step": 248
    },
    {
      "epoch": 0.009653501332687182,
      "grad_norm": 0.16641798615455627,
      "learning_rate": 0.00019810764696758185,
      "loss": 0.3592,
      "step": 249
    },
    {
      "epoch": 0.00969227041434456,
      "grad_norm": 0.2717999815940857,
      "learning_rate": 0.000198099891422367,
      "loss": 0.3219,
      "step": 250
    },
    {
      "epoch": 0.009731039496001938,
      "grad_norm": 0.22350554168224335,
      "learning_rate": 0.00019809213587715217,
      "loss": 0.3677,
      "step": 251
    },
    {
      "epoch": 0.009769808577659316,
      "grad_norm": 0.21572169661521912,
      "learning_rate": 0.00019808438033193734,
      "loss": 0.4451,
      "step": 252
    },
    {
      "epoch": 0.009808577659316696,
      "grad_norm": 0.20602978765964508,
      "learning_rate": 0.0001980766247867225,
      "loss": 0.3234,
      "step": 253
    },
    {
      "epoch": 0.009847346740974074,
      "grad_norm": 0.22651544213294983,
      "learning_rate": 0.00019806886924150769,
      "loss": 0.4344,
      "step": 254
    },
    {
      "epoch": 0.009886115822631452,
      "grad_norm": 0.18123123049736023,
      "learning_rate": 0.00019806111369629286,
      "loss": 0.347,
      "step": 255
    },
    {
      "epoch": 0.00992488490428883,
      "grad_norm": 0.26282376050949097,
      "learning_rate": 0.00019805335815107803,
      "loss": 0.4966,
      "step": 256
    },
    {
      "epoch": 0.009963653985946208,
      "grad_norm": 0.21470052003860474,
      "learning_rate": 0.0001980456026058632,
      "loss": 0.3697,
      "step": 257
    },
    {
      "epoch": 0.010002423067603586,
      "grad_norm": 0.2005261927843094,
      "learning_rate": 0.00019803784706064835,
      "loss": 0.2692,
      "step": 258
    },
    {
      "epoch": 0.010041192149260964,
      "grad_norm": 0.19036979973316193,
      "learning_rate": 0.00019803009151543355,
      "loss": 0.359,
      "step": 259
    },
    {
      "epoch": 0.010079961230918342,
      "grad_norm": 0.1930702030658722,
      "learning_rate": 0.0001980223359702187,
      "loss": 0.4043,
      "step": 260
    },
    {
      "epoch": 0.010118730312575722,
      "grad_norm": 0.13818514347076416,
      "learning_rate": 0.0001980145804250039,
      "loss": 0.3037,
      "step": 261
    },
    {
      "epoch": 0.0101574993942331,
      "grad_norm": 0.15924029052257538,
      "learning_rate": 0.00019800682487978904,
      "loss": 0.2793,
      "step": 262
    },
    {
      "epoch": 0.010196268475890478,
      "grad_norm": 0.15902143716812134,
      "learning_rate": 0.00019799906933457424,
      "loss": 0.3191,
      "step": 263
    },
    {
      "epoch": 0.010235037557547856,
      "grad_norm": 0.21369120478630066,
      "learning_rate": 0.0001979913137893594,
      "loss": 0.4198,
      "step": 264
    },
    {
      "epoch": 0.010273806639205234,
      "grad_norm": 0.17183227837085724,
      "learning_rate": 0.00019798355824414456,
      "loss": 0.3457,
      "step": 265
    },
    {
      "epoch": 0.010312575720862612,
      "grad_norm": 0.14809849858283997,
      "learning_rate": 0.00019797580269892976,
      "loss": 0.368,
      "step": 266
    },
    {
      "epoch": 0.01035134480251999,
      "grad_norm": 0.17257201671600342,
      "learning_rate": 0.0001979680471537149,
      "loss": 0.3516,
      "step": 267
    },
    {
      "epoch": 0.010390113884177368,
      "grad_norm": 0.17178680002689362,
      "learning_rate": 0.0001979602916085001,
      "loss": 0.3955,
      "step": 268
    },
    {
      "epoch": 0.010428882965834746,
      "grad_norm": 0.1562902182340622,
      "learning_rate": 0.00019795253606328525,
      "loss": 0.3737,
      "step": 269
    },
    {
      "epoch": 0.010467652047492126,
      "grad_norm": 0.15938642621040344,
      "learning_rate": 0.00019794478051807045,
      "loss": 0.41,
      "step": 270
    },
    {
      "epoch": 0.010506421129149504,
      "grad_norm": 0.14168888330459595,
      "learning_rate": 0.0001979370249728556,
      "loss": 0.3174,
      "step": 271
    },
    {
      "epoch": 0.010545190210806882,
      "grad_norm": 0.1937994360923767,
      "learning_rate": 0.00019792926942764077,
      "loss": 0.4053,
      "step": 272
    },
    {
      "epoch": 0.01058395929246426,
      "grad_norm": 0.14505666494369507,
      "learning_rate": 0.00019792151388242594,
      "loss": 0.2953,
      "step": 273
    },
    {
      "epoch": 0.010622728374121638,
      "grad_norm": 0.18066340684890747,
      "learning_rate": 0.00019791375833721112,
      "loss": 0.4077,
      "step": 274
    },
    {
      "epoch": 0.010661497455779016,
      "grad_norm": 0.1607230007648468,
      "learning_rate": 0.0001979060027919963,
      "loss": 0.405,
      "step": 275
    },
    {
      "epoch": 0.010700266537436394,
      "grad_norm": 0.22398890554904938,
      "learning_rate": 0.00019789824724678146,
      "loss": 0.4078,
      "step": 276
    },
    {
      "epoch": 0.010739035619093772,
      "grad_norm": 0.17724472284317017,
      "learning_rate": 0.00019789049170156663,
      "loss": 0.3958,
      "step": 277
    },
    {
      "epoch": 0.010777804700751152,
      "grad_norm": 0.15473636984825134,
      "learning_rate": 0.0001978827361563518,
      "loss": 0.427,
      "step": 278
    },
    {
      "epoch": 0.01081657378240853,
      "grad_norm": 0.18028514087200165,
      "learning_rate": 0.00019787498061113695,
      "loss": 0.2848,
      "step": 279
    },
    {
      "epoch": 0.010855342864065908,
      "grad_norm": 0.16773951053619385,
      "learning_rate": 0.00019786722506592215,
      "loss": 0.2831,
      "step": 280
    },
    {
      "epoch": 0.010894111945723286,
      "grad_norm": 0.14374513924121857,
      "learning_rate": 0.0001978594695207073,
      "loss": 0.3855,
      "step": 281
    },
    {
      "epoch": 0.010932881027380664,
      "grad_norm": 0.17668862640857697,
      "learning_rate": 0.0001978517139754925,
      "loss": 0.3661,
      "step": 282
    },
    {
      "epoch": 0.010971650109038042,
      "grad_norm": 0.2290666699409485,
      "learning_rate": 0.00019784395843027764,
      "loss": 0.4541,
      "step": 283
    },
    {
      "epoch": 0.01101041919069542,
      "grad_norm": 0.13845233619213104,
      "learning_rate": 0.00019783620288506284,
      "loss": 0.3,
      "step": 284
    },
    {
      "epoch": 0.011049188272352798,
      "grad_norm": 0.1722291111946106,
      "learning_rate": 0.000197828447339848,
      "loss": 0.3855,
      "step": 285
    },
    {
      "epoch": 0.011087957354010176,
      "grad_norm": 0.1882779598236084,
      "learning_rate": 0.00019782069179463316,
      "loss": 0.3737,
      "step": 286
    },
    {
      "epoch": 0.011126726435667556,
      "grad_norm": 0.18851841986179352,
      "learning_rate": 0.00019781293624941833,
      "loss": 0.3622,
      "step": 287
    },
    {
      "epoch": 0.011165495517324934,
      "grad_norm": 0.19004085659980774,
      "learning_rate": 0.0001978051807042035,
      "loss": 0.3477,
      "step": 288
    },
    {
      "epoch": 0.011204264598982312,
      "grad_norm": 0.1555006206035614,
      "learning_rate": 0.00019779742515898868,
      "loss": 0.3086,
      "step": 289
    },
    {
      "epoch": 0.01124303368063969,
      "grad_norm": 0.18072089552879333,
      "learning_rate": 0.00019778966961377385,
      "loss": 0.3378,
      "step": 290
    },
    {
      "epoch": 0.011281802762297068,
      "grad_norm": 0.1425275206565857,
      "learning_rate": 0.00019778191406855903,
      "loss": 0.3014,
      "step": 291
    },
    {
      "epoch": 0.011320571843954446,
      "grad_norm": 0.17995955049991608,
      "learning_rate": 0.0001977741585233442,
      "loss": 0.3221,
      "step": 292
    },
    {
      "epoch": 0.011359340925611824,
      "grad_norm": 0.21315564215183258,
      "learning_rate": 0.00019776640297812937,
      "loss": 0.4809,
      "step": 293
    },
    {
      "epoch": 0.011398110007269202,
      "grad_norm": 0.1716470867395401,
      "learning_rate": 0.00019775864743291454,
      "loss": 0.4263,
      "step": 294
    },
    {
      "epoch": 0.011436879088926582,
      "grad_norm": 0.17753319442272186,
      "learning_rate": 0.00019775089188769972,
      "loss": 0.306,
      "step": 295
    },
    {
      "epoch": 0.01147564817058396,
      "grad_norm": 0.1918880045413971,
      "learning_rate": 0.0001977431363424849,
      "loss": 0.404,
      "step": 296
    },
    {
      "epoch": 0.011514417252241338,
      "grad_norm": 0.1563969999551773,
      "learning_rate": 0.00019773538079727006,
      "loss": 0.3539,
      "step": 297
    },
    {
      "epoch": 0.011553186333898716,
      "grad_norm": 0.1823503077030182,
      "learning_rate": 0.00019772762525205524,
      "loss": 0.413,
      "step": 298
    },
    {
      "epoch": 0.011591955415556094,
      "grad_norm": 0.15847617387771606,
      "learning_rate": 0.0001977198697068404,
      "loss": 0.3811,
      "step": 299
    },
    {
      "epoch": 0.011630724497213472,
      "grad_norm": 0.17075276374816895,
      "learning_rate": 0.00019771211416162555,
      "loss": 0.3632,
      "step": 300
    },
    {
      "epoch": 0.01166949357887085,
      "grad_norm": 0.1365727186203003,
      "learning_rate": 0.00019770435861641075,
      "loss": 0.3045,
      "step": 301
    },
    {
      "epoch": 0.011708262660528228,
      "grad_norm": 0.18656589090824127,
      "learning_rate": 0.0001976966030711959,
      "loss": 0.3121,
      "step": 302
    },
    {
      "epoch": 0.011747031742185606,
      "grad_norm": 0.14196452498435974,
      "learning_rate": 0.0001976888475259811,
      "loss": 0.2838,
      "step": 303
    },
    {
      "epoch": 0.011785800823842986,
      "grad_norm": 0.1519457995891571,
      "learning_rate": 0.00019768109198076625,
      "loss": 0.3095,
      "step": 304
    },
    {
      "epoch": 0.011824569905500364,
      "grad_norm": 0.16806429624557495,
      "learning_rate": 0.00019767333643555145,
      "loss": 0.3477,
      "step": 305
    },
    {
      "epoch": 0.011863338987157742,
      "grad_norm": 0.18827693164348602,
      "learning_rate": 0.0001976655808903366,
      "loss": 0.358,
      "step": 306
    },
    {
      "epoch": 0.01190210806881512,
      "grad_norm": 0.1746499240398407,
      "learning_rate": 0.00019765782534512176,
      "loss": 0.4075,
      "step": 307
    },
    {
      "epoch": 0.011940877150472498,
      "grad_norm": 0.16340157389640808,
      "learning_rate": 0.00019765006979990694,
      "loss": 0.2728,
      "step": 308
    },
    {
      "epoch": 0.011979646232129876,
      "grad_norm": 0.16414311528205872,
      "learning_rate": 0.0001976423142546921,
      "loss": 0.4277,
      "step": 309
    },
    {
      "epoch": 0.012018415313787254,
      "grad_norm": 0.15941451489925385,
      "learning_rate": 0.00019763455870947728,
      "loss": 0.3637,
      "step": 310
    },
    {
      "epoch": 0.012057184395444632,
      "grad_norm": 0.15505185723304749,
      "learning_rate": 0.00019762680316426245,
      "loss": 0.3993,
      "step": 311
    },
    {
      "epoch": 0.012095953477102012,
      "grad_norm": 0.14807192981243134,
      "learning_rate": 0.00019761904761904763,
      "loss": 0.4166,
      "step": 312
    },
    {
      "epoch": 0.01213472255875939,
      "grad_norm": 0.18796958029270172,
      "learning_rate": 0.0001976112920738328,
      "loss": 0.3833,
      "step": 313
    },
    {
      "epoch": 0.012173491640416768,
      "grad_norm": 0.2849912941455841,
      "learning_rate": 0.00019760353652861797,
      "loss": 0.4802,
      "step": 314
    },
    {
      "epoch": 0.012212260722074146,
      "grad_norm": 0.15811723470687866,
      "learning_rate": 0.00019759578098340315,
      "loss": 0.4112,
      "step": 315
    },
    {
      "epoch": 0.012251029803731524,
      "grad_norm": 0.20007836818695068,
      "learning_rate": 0.00019758802543818832,
      "loss": 0.4202,
      "step": 316
    },
    {
      "epoch": 0.012289798885388902,
      "grad_norm": 0.2096620351076126,
      "learning_rate": 0.0001975802698929735,
      "loss": 0.2618,
      "step": 317
    },
    {
      "epoch": 0.01232856796704628,
      "grad_norm": 0.16916342079639435,
      "learning_rate": 0.00019757251434775866,
      "loss": 0.2976,
      "step": 318
    },
    {
      "epoch": 0.012367337048703658,
      "grad_norm": 0.1328880935907364,
      "learning_rate": 0.00019756475880254384,
      "loss": 0.2735,
      "step": 319
    },
    {
      "epoch": 0.012406106130361036,
      "grad_norm": 0.2563544511795044,
      "learning_rate": 0.000197557003257329,
      "loss": 0.4316,
      "step": 320
    },
    {
      "epoch": 0.012444875212018416,
      "grad_norm": 0.15550367534160614,
      "learning_rate": 0.00019754924771211416,
      "loss": 0.322,
      "step": 321
    },
    {
      "epoch": 0.012483644293675794,
      "grad_norm": 0.18695387244224548,
      "learning_rate": 0.00019754149216689936,
      "loss": 0.3976,
      "step": 322
    },
    {
      "epoch": 0.012522413375333172,
      "grad_norm": 0.17756637930870056,
      "learning_rate": 0.0001975337366216845,
      "loss": 0.3794,
      "step": 323
    },
    {
      "epoch": 0.01256118245699055,
      "grad_norm": 0.14004594087600708,
      "learning_rate": 0.0001975259810764697,
      "loss": 0.3057,
      "step": 324
    },
    {
      "epoch": 0.012599951538647928,
      "grad_norm": 0.1699274182319641,
      "learning_rate": 0.00019751822553125485,
      "loss": 0.3011,
      "step": 325
    },
    {
      "epoch": 0.012638720620305306,
      "grad_norm": 0.1379086822271347,
      "learning_rate": 0.00019751046998604005,
      "loss": 0.3239,
      "step": 326
    },
    {
      "epoch": 0.012677489701962684,
      "grad_norm": 0.19186082482337952,
      "learning_rate": 0.0001975027144408252,
      "loss": 0.4043,
      "step": 327
    },
    {
      "epoch": 0.012716258783620062,
      "grad_norm": 0.17940974235534668,
      "learning_rate": 0.00019749495889561037,
      "loss": 0.3993,
      "step": 328
    },
    {
      "epoch": 0.012755027865277442,
      "grad_norm": 0.13812488317489624,
      "learning_rate": 0.00019748720335039554,
      "loss": 0.3167,
      "step": 329
    },
    {
      "epoch": 0.01279379694693482,
      "grad_norm": 0.1958157867193222,
      "learning_rate": 0.0001974794478051807,
      "loss": 0.413,
      "step": 330
    },
    {
      "epoch": 0.012832566028592198,
      "grad_norm": 0.1377459615468979,
      "learning_rate": 0.00019747169225996588,
      "loss": 0.326,
      "step": 331
    },
    {
      "epoch": 0.012871335110249576,
      "grad_norm": 0.16171003878116608,
      "learning_rate": 0.00019746393671475106,
      "loss": 0.3766,
      "step": 332
    },
    {
      "epoch": 0.012910104191906954,
      "grad_norm": 0.163371279835701,
      "learning_rate": 0.00019745618116953623,
      "loss": 0.3899,
      "step": 333
    },
    {
      "epoch": 0.012948873273564332,
      "grad_norm": 0.16726039350032806,
      "learning_rate": 0.0001974484256243214,
      "loss": 0.3175,
      "step": 334
    },
    {
      "epoch": 0.01298764235522171,
      "grad_norm": 0.16827479004859924,
      "learning_rate": 0.00019744067007910655,
      "loss": 0.3356,
      "step": 335
    },
    {
      "epoch": 0.013026411436879088,
      "grad_norm": 0.17647625505924225,
      "learning_rate": 0.00019743291453389175,
      "loss": 0.4804,
      "step": 336
    },
    {
      "epoch": 0.013065180518536466,
      "grad_norm": 0.17954511940479279,
      "learning_rate": 0.0001974251589886769,
      "loss": 0.386,
      "step": 337
    },
    {
      "epoch": 0.013103949600193846,
      "grad_norm": 0.13161879777908325,
      "learning_rate": 0.0001974174034434621,
      "loss": 0.2799,
      "step": 338
    },
    {
      "epoch": 0.013142718681851224,
      "grad_norm": 0.14247940480709076,
      "learning_rate": 0.00019740964789824724,
      "loss": 0.2743,
      "step": 339
    },
    {
      "epoch": 0.013181487763508602,
      "grad_norm": 0.16297179460525513,
      "learning_rate": 0.00019740189235303244,
      "loss": 0.384,
      "step": 340
    },
    {
      "epoch": 0.01322025684516598,
      "grad_norm": 0.09923382848501205,
      "learning_rate": 0.00019739413680781758,
      "loss": 0.2412,
      "step": 341
    },
    {
      "epoch": 0.013259025926823358,
      "grad_norm": 0.1338622272014618,
      "learning_rate": 0.00019738638126260276,
      "loss": 0.3233,
      "step": 342
    },
    {
      "epoch": 0.013297795008480736,
      "grad_norm": 0.1586993783712387,
      "learning_rate": 0.00019737862571738793,
      "loss": 0.2131,
      "step": 343
    },
    {
      "epoch": 0.013336564090138114,
      "grad_norm": 0.16750630736351013,
      "learning_rate": 0.0001973708701721731,
      "loss": 0.3975,
      "step": 344
    },
    {
      "epoch": 0.013375333171795492,
      "grad_norm": 0.17013856768608093,
      "learning_rate": 0.0001973631146269583,
      "loss": 0.3769,
      "step": 345
    },
    {
      "epoch": 0.013414102253452872,
      "grad_norm": 0.16270804405212402,
      "learning_rate": 0.00019735535908174345,
      "loss": 0.3886,
      "step": 346
    },
    {
      "epoch": 0.01345287133511025,
      "grad_norm": 0.1447736620903015,
      "learning_rate": 0.00019734760353652865,
      "loss": 0.4385,
      "step": 347
    },
    {
      "epoch": 0.013491640416767628,
      "grad_norm": 0.14677630364894867,
      "learning_rate": 0.0001973398479913138,
      "loss": 0.2727,
      "step": 348
    },
    {
      "epoch": 0.013530409498425006,
      "grad_norm": 0.1441754847764969,
      "learning_rate": 0.00019733209244609897,
      "loss": 0.3208,
      "step": 349
    },
    {
      "epoch": 0.013569178580082384,
      "grad_norm": 0.1403016895055771,
      "learning_rate": 0.00019732433690088414,
      "loss": 0.3165,
      "step": 350
    },
    {
      "epoch": 0.013607947661739762,
      "grad_norm": 0.1476401537656784,
      "learning_rate": 0.0001973165813556693,
      "loss": 0.369,
      "step": 351
    },
    {
      "epoch": 0.01364671674339714,
      "grad_norm": 0.17325538396835327,
      "learning_rate": 0.00019730882581045449,
      "loss": 0.4067,
      "step": 352
    },
    {
      "epoch": 0.013685485825054518,
      "grad_norm": 0.15439319610595703,
      "learning_rate": 0.00019730107026523966,
      "loss": 0.4298,
      "step": 353
    },
    {
      "epoch": 0.013724254906711897,
      "grad_norm": 0.14925047755241394,
      "learning_rate": 0.00019729331472002483,
      "loss": 0.4455,
      "step": 354
    },
    {
      "epoch": 0.013763023988369276,
      "grad_norm": 0.19611449539661407,
      "learning_rate": 0.00019728555917481,
      "loss": 0.4139,
      "step": 355
    },
    {
      "epoch": 0.013801793070026654,
      "grad_norm": 0.14050205051898956,
      "learning_rate": 0.00019727780362959515,
      "loss": 0.2917,
      "step": 356
    },
    {
      "epoch": 0.013840562151684032,
      "grad_norm": 0.17629007995128632,
      "learning_rate": 0.00019727004808438035,
      "loss": 0.3285,
      "step": 357
    },
    {
      "epoch": 0.01387933123334141,
      "grad_norm": 0.1293446123600006,
      "learning_rate": 0.0001972622925391655,
      "loss": 0.3552,
      "step": 358
    },
    {
      "epoch": 0.013918100314998788,
      "grad_norm": 0.1502116620540619,
      "learning_rate": 0.0001972545369939507,
      "loss": 0.3638,
      "step": 359
    },
    {
      "epoch": 0.013956869396656166,
      "grad_norm": 0.16742493212223053,
      "learning_rate": 0.00019724678144873584,
      "loss": 0.3653,
      "step": 360
    },
    {
      "epoch": 0.013995638478313545,
      "grad_norm": 0.13336917757987976,
      "learning_rate": 0.00019723902590352104,
      "loss": 0.3348,
      "step": 361
    },
    {
      "epoch": 0.014034407559970923,
      "grad_norm": 0.1996009796857834,
      "learning_rate": 0.0001972312703583062,
      "loss": 0.5099,
      "step": 362
    },
    {
      "epoch": 0.0140731766416283,
      "grad_norm": 0.1458614319562912,
      "learning_rate": 0.00019722351481309136,
      "loss": 0.335,
      "step": 363
    },
    {
      "epoch": 0.01411194572328568,
      "grad_norm": 0.1723356693983078,
      "learning_rate": 0.00019721575926787653,
      "loss": 0.2735,
      "step": 364
    },
    {
      "epoch": 0.014150714804943058,
      "grad_norm": 0.12598104774951935,
      "learning_rate": 0.0001972080037226617,
      "loss": 0.3295,
      "step": 365
    },
    {
      "epoch": 0.014189483886600436,
      "grad_norm": 0.1391986459493637,
      "learning_rate": 0.00019720024817744688,
      "loss": 0.3673,
      "step": 366
    },
    {
      "epoch": 0.014228252968257814,
      "grad_norm": 0.1247745156288147,
      "learning_rate": 0.00019719249263223205,
      "loss": 0.3172,
      "step": 367
    },
    {
      "epoch": 0.014267022049915193,
      "grad_norm": 0.1664644032716751,
      "learning_rate": 0.00019718473708701722,
      "loss": 0.3376,
      "step": 368
    },
    {
      "epoch": 0.01430579113157257,
      "grad_norm": 0.15393756330013275,
      "learning_rate": 0.0001971769815418024,
      "loss": 0.3282,
      "step": 369
    },
    {
      "epoch": 0.014344560213229949,
      "grad_norm": 0.24614785611629486,
      "learning_rate": 0.00019716922599658757,
      "loss": 0.3735,
      "step": 370
    },
    {
      "epoch": 0.014383329294887327,
      "grad_norm": 0.13313700258731842,
      "learning_rate": 0.00019716147045137274,
      "loss": 0.3337,
      "step": 371
    },
    {
      "epoch": 0.014422098376544706,
      "grad_norm": 0.16321057081222534,
      "learning_rate": 0.00019715371490615791,
      "loss": 0.3106,
      "step": 372
    },
    {
      "epoch": 0.014460867458202084,
      "grad_norm": 0.21115976572036743,
      "learning_rate": 0.0001971459593609431,
      "loss": 0.4267,
      "step": 373
    },
    {
      "epoch": 0.014499636539859462,
      "grad_norm": 0.1320957988500595,
      "learning_rate": 0.00019713820381572826,
      "loss": 0.2987,
      "step": 374
    },
    {
      "epoch": 0.01453840562151684,
      "grad_norm": 0.1355506032705307,
      "learning_rate": 0.00019713044827051343,
      "loss": 0.3048,
      "step": 375
    },
    {
      "epoch": 0.014577174703174219,
      "grad_norm": 0.1460687518119812,
      "learning_rate": 0.0001971226927252986,
      "loss": 0.2872,
      "step": 376
    },
    {
      "epoch": 0.014615943784831597,
      "grad_norm": 0.174779012799263,
      "learning_rate": 0.00019711493718008375,
      "loss": 0.3562,
      "step": 377
    },
    {
      "epoch": 0.014654712866488975,
      "grad_norm": 0.19002993404865265,
      "learning_rate": 0.00019710718163486895,
      "loss": 0.3962,
      "step": 378
    },
    {
      "epoch": 0.014693481948146353,
      "grad_norm": 0.1637079268693924,
      "learning_rate": 0.0001970994260896541,
      "loss": 0.295,
      "step": 379
    },
    {
      "epoch": 0.01473225102980373,
      "grad_norm": 0.14877000451087952,
      "learning_rate": 0.0001970916705444393,
      "loss": 0.2763,
      "step": 380
    },
    {
      "epoch": 0.01477102011146111,
      "grad_norm": 0.1653047353029251,
      "learning_rate": 0.00019708391499922444,
      "loss": 0.3783,
      "step": 381
    },
    {
      "epoch": 0.014809789193118489,
      "grad_norm": 0.12878736853599548,
      "learning_rate": 0.00019707615945400964,
      "loss": 0.3046,
      "step": 382
    },
    {
      "epoch": 0.014848558274775867,
      "grad_norm": 0.1508573442697525,
      "learning_rate": 0.0001970684039087948,
      "loss": 0.3313,
      "step": 383
    },
    {
      "epoch": 0.014887327356433245,
      "grad_norm": 0.18111349642276764,
      "learning_rate": 0.00019706064836357996,
      "loss": 0.325,
      "step": 384
    },
    {
      "epoch": 0.014926096438090623,
      "grad_norm": 0.16527347266674042,
      "learning_rate": 0.00019705289281836513,
      "loss": 0.3714,
      "step": 385
    },
    {
      "epoch": 0.014964865519748,
      "grad_norm": 0.12838663160800934,
      "learning_rate": 0.0001970451372731503,
      "loss": 0.3371,
      "step": 386
    },
    {
      "epoch": 0.015003634601405379,
      "grad_norm": 0.14168483018875122,
      "learning_rate": 0.00019703738172793548,
      "loss": 0.3218,
      "step": 387
    },
    {
      "epoch": 0.015042403683062757,
      "grad_norm": 0.1287270486354828,
      "learning_rate": 0.00019702962618272065,
      "loss": 0.3169,
      "step": 388
    },
    {
      "epoch": 0.015081172764720137,
      "grad_norm": 0.1544041931629181,
      "learning_rate": 0.00019702187063750583,
      "loss": 0.2958,
      "step": 389
    },
    {
      "epoch": 0.015119941846377515,
      "grad_norm": 0.1667313128709793,
      "learning_rate": 0.000197014115092291,
      "loss": 0.4009,
      "step": 390
    },
    {
      "epoch": 0.015158710928034893,
      "grad_norm": 0.12452993541955948,
      "learning_rate": 0.00019700635954707617,
      "loss": 0.2814,
      "step": 391
    },
    {
      "epoch": 0.01519748000969227,
      "grad_norm": 0.1428566575050354,
      "learning_rate": 0.00019699860400186134,
      "loss": 0.3935,
      "step": 392
    },
    {
      "epoch": 0.015236249091349649,
      "grad_norm": 0.12138844281435013,
      "learning_rate": 0.00019699084845664652,
      "loss": 0.2686,
      "step": 393
    },
    {
      "epoch": 0.015275018173007027,
      "grad_norm": 0.14477959275245667,
      "learning_rate": 0.0001969830929114317,
      "loss": 0.3188,
      "step": 394
    },
    {
      "epoch": 0.015313787254664405,
      "grad_norm": 0.13903096318244934,
      "learning_rate": 0.00019697533736621686,
      "loss": 0.3875,
      "step": 395
    },
    {
      "epoch": 0.015352556336321783,
      "grad_norm": 0.10525523126125336,
      "learning_rate": 0.00019696758182100203,
      "loss": 0.2283,
      "step": 396
    },
    {
      "epoch": 0.01539132541797916,
      "grad_norm": 0.1606525480747223,
      "learning_rate": 0.0001969598262757872,
      "loss": 0.4378,
      "step": 397
    },
    {
      "epoch": 0.01543009449963654,
      "grad_norm": 0.1394253969192505,
      "learning_rate": 0.00019695207073057235,
      "loss": 0.3907,
      "step": 398
    },
    {
      "epoch": 0.015468863581293919,
      "grad_norm": 0.11906446516513824,
      "learning_rate": 0.00019694431518535755,
      "loss": 0.2909,
      "step": 399
    },
    {
      "epoch": 0.015507632662951297,
      "grad_norm": 0.16384343802928925,
      "learning_rate": 0.0001969365596401427,
      "loss": 0.2721,
      "step": 400
    },
    {
      "epoch": 0.015546401744608675,
      "grad_norm": 0.12959982454776764,
      "learning_rate": 0.0001969288040949279,
      "loss": 0.3299,
      "step": 401
    },
    {
      "epoch": 0.015585170826266053,
      "grad_norm": 0.14808742702007294,
      "learning_rate": 0.00019692104854971304,
      "loss": 0.3806,
      "step": 402
    },
    {
      "epoch": 0.01562393990792343,
      "grad_norm": 0.15015818178653717,
      "learning_rate": 0.00019691329300449824,
      "loss": 0.3465,
      "step": 403
    },
    {
      "epoch": 0.01566270898958081,
      "grad_norm": 0.13531392812728882,
      "learning_rate": 0.0001969055374592834,
      "loss": 0.3146,
      "step": 404
    },
    {
      "epoch": 0.015701478071238187,
      "grad_norm": 0.1363358497619629,
      "learning_rate": 0.00019689778191406856,
      "loss": 0.3018,
      "step": 405
    },
    {
      "epoch": 0.015740247152895567,
      "grad_norm": 0.1440751552581787,
      "learning_rate": 0.00019689002636885374,
      "loss": 0.3852,
      "step": 406
    },
    {
      "epoch": 0.015779016234552943,
      "grad_norm": 0.168474018573761,
      "learning_rate": 0.0001968822708236389,
      "loss": 0.4273,
      "step": 407
    },
    {
      "epoch": 0.015817785316210323,
      "grad_norm": 0.1505265235900879,
      "learning_rate": 0.00019687451527842408,
      "loss": 0.3417,
      "step": 408
    },
    {
      "epoch": 0.0158565543978677,
      "grad_norm": 0.14399905502796173,
      "learning_rate": 0.00019686675973320925,
      "loss": 0.254,
      "step": 409
    },
    {
      "epoch": 0.01589532347952508,
      "grad_norm": 0.12960729002952576,
      "learning_rate": 0.00019685900418799443,
      "loss": 0.326,
      "step": 410
    },
    {
      "epoch": 0.01593409256118246,
      "grad_norm": 0.145662322640419,
      "learning_rate": 0.0001968512486427796,
      "loss": 0.3734,
      "step": 411
    },
    {
      "epoch": 0.015972861642839835,
      "grad_norm": 0.13401231169700623,
      "learning_rate": 0.00019684349309756475,
      "loss": 0.4027,
      "step": 412
    },
    {
      "epoch": 0.016011630724497215,
      "grad_norm": 0.13621477782726288,
      "learning_rate": 0.00019683573755234995,
      "loss": 0.3533,
      "step": 413
    },
    {
      "epoch": 0.01605039980615459,
      "grad_norm": 0.13191908597946167,
      "learning_rate": 0.0001968279820071351,
      "loss": 0.3434,
      "step": 414
    },
    {
      "epoch": 0.01608916888781197,
      "grad_norm": 0.16295069456100464,
      "learning_rate": 0.0001968202264619203,
      "loss": 0.3627,
      "step": 415
    },
    {
      "epoch": 0.016127937969469347,
      "grad_norm": 0.16902285814285278,
      "learning_rate": 0.00019681247091670544,
      "loss": 0.3346,
      "step": 416
    },
    {
      "epoch": 0.016166707051126727,
      "grad_norm": 0.12611564993858337,
      "learning_rate": 0.00019680471537149064,
      "loss": 0.2957,
      "step": 417
    },
    {
      "epoch": 0.016205476132784103,
      "grad_norm": 0.1521373987197876,
      "learning_rate": 0.00019679695982627578,
      "loss": 0.4249,
      "step": 418
    },
    {
      "epoch": 0.016244245214441483,
      "grad_norm": 0.13296061754226685,
      "learning_rate": 0.00019678920428106096,
      "loss": 0.397,
      "step": 419
    },
    {
      "epoch": 0.016283014296098863,
      "grad_norm": 0.15465140342712402,
      "learning_rate": 0.00019678144873584613,
      "loss": 0.4008,
      "step": 420
    },
    {
      "epoch": 0.01632178337775624,
      "grad_norm": 0.14004071056842804,
      "learning_rate": 0.0001967736931906313,
      "loss": 0.2918,
      "step": 421
    },
    {
      "epoch": 0.01636055245941362,
      "grad_norm": 0.1837671399116516,
      "learning_rate": 0.00019676593764541647,
      "loss": 0.4286,
      "step": 422
    },
    {
      "epoch": 0.016399321541070995,
      "grad_norm": 0.19393673539161682,
      "learning_rate": 0.00019675818210020165,
      "loss": 0.4437,
      "step": 423
    },
    {
      "epoch": 0.016438090622728375,
      "grad_norm": 0.16608259081840515,
      "learning_rate": 0.00019675042655498685,
      "loss": 0.4126,
      "step": 424
    },
    {
      "epoch": 0.01647685970438575,
      "grad_norm": 0.14514431357383728,
      "learning_rate": 0.000196742671009772,
      "loss": 0.327,
      "step": 425
    },
    {
      "epoch": 0.01651562878604313,
      "grad_norm": 0.12400224804878235,
      "learning_rate": 0.00019673491546455716,
      "loss": 0.2874,
      "step": 426
    },
    {
      "epoch": 0.016554397867700507,
      "grad_norm": 0.1985125094652176,
      "learning_rate": 0.00019672715991934234,
      "loss": 0.2595,
      "step": 427
    },
    {
      "epoch": 0.016593166949357887,
      "grad_norm": 0.14098939299583435,
      "learning_rate": 0.0001967194043741275,
      "loss": 0.3202,
      "step": 428
    },
    {
      "epoch": 0.016631936031015267,
      "grad_norm": 0.17842087149620056,
      "learning_rate": 0.00019671164882891268,
      "loss": 0.378,
      "step": 429
    },
    {
      "epoch": 0.016670705112672643,
      "grad_norm": 0.19144096970558167,
      "learning_rate": 0.00019670389328369786,
      "loss": 0.3635,
      "step": 430
    },
    {
      "epoch": 0.016709474194330023,
      "grad_norm": 0.16658827662467957,
      "learning_rate": 0.00019669613773848303,
      "loss": 0.3491,
      "step": 431
    },
    {
      "epoch": 0.0167482432759874,
      "grad_norm": 0.1181446984410286,
      "learning_rate": 0.0001966883821932682,
      "loss": 0.3126,
      "step": 432
    },
    {
      "epoch": 0.01678701235764478,
      "grad_norm": 0.11732371151447296,
      "learning_rate": 0.00019668062664805335,
      "loss": 0.343,
      "step": 433
    },
    {
      "epoch": 0.016825781439302155,
      "grad_norm": 0.17194682359695435,
      "learning_rate": 0.00019667287110283855,
      "loss": 0.3137,
      "step": 434
    },
    {
      "epoch": 0.016864550520959535,
      "grad_norm": 0.18140262365341187,
      "learning_rate": 0.0001966651155576237,
      "loss": 0.3288,
      "step": 435
    },
    {
      "epoch": 0.016903319602616915,
      "grad_norm": 0.21123430132865906,
      "learning_rate": 0.0001966573600124089,
      "loss": 0.3513,
      "step": 436
    },
    {
      "epoch": 0.01694208868427429,
      "grad_norm": 0.10980615764856339,
      "learning_rate": 0.00019664960446719404,
      "loss": 0.3195,
      "step": 437
    },
    {
      "epoch": 0.01698085776593167,
      "grad_norm": 0.19458700716495514,
      "learning_rate": 0.00019664184892197924,
      "loss": 0.4076,
      "step": 438
    },
    {
      "epoch": 0.017019626847589047,
      "grad_norm": 0.11625851690769196,
      "learning_rate": 0.00019663409337676438,
      "loss": 0.2444,
      "step": 439
    },
    {
      "epoch": 0.017058395929246427,
      "grad_norm": 0.15956972539424896,
      "learning_rate": 0.00019662633783154956,
      "loss": 0.3963,
      "step": 440
    },
    {
      "epoch": 0.017097165010903803,
      "grad_norm": 0.1335020214319229,
      "learning_rate": 0.00019661858228633473,
      "loss": 0.3055,
      "step": 441
    },
    {
      "epoch": 0.017135934092561183,
      "grad_norm": 0.15638770163059235,
      "learning_rate": 0.0001966108267411199,
      "loss": 0.3921,
      "step": 442
    },
    {
      "epoch": 0.01717470317421856,
      "grad_norm": 0.12148343026638031,
      "learning_rate": 0.00019660307119590508,
      "loss": 0.2952,
      "step": 443
    },
    {
      "epoch": 0.01721347225587594,
      "grad_norm": 0.22213637828826904,
      "learning_rate": 0.00019659531565069025,
      "loss": 0.317,
      "step": 444
    },
    {
      "epoch": 0.01725224133753332,
      "grad_norm": 0.1285373717546463,
      "learning_rate": 0.00019658756010547542,
      "loss": 0.3892,
      "step": 445
    },
    {
      "epoch": 0.017291010419190695,
      "grad_norm": 0.12641629576683044,
      "learning_rate": 0.0001965798045602606,
      "loss": 0.4013,
      "step": 446
    },
    {
      "epoch": 0.017329779500848075,
      "grad_norm": 0.17710454761981964,
      "learning_rate": 0.00019657204901504577,
      "loss": 0.3584,
      "step": 447
    },
    {
      "epoch": 0.01736854858250545,
      "grad_norm": 0.1602262556552887,
      "learning_rate": 0.00019656429346983094,
      "loss": 0.2915,
      "step": 448
    },
    {
      "epoch": 0.01740731766416283,
      "grad_norm": 0.1707475781440735,
      "learning_rate": 0.0001965565379246161,
      "loss": 0.355,
      "step": 449
    },
    {
      "epoch": 0.017446086745820207,
      "grad_norm": 0.11489633470773697,
      "learning_rate": 0.00019654878237940129,
      "loss": 0.3115,
      "step": 450
    },
    {
      "epoch": 0.017484855827477587,
      "grad_norm": 0.23644787073135376,
      "learning_rate": 0.00019654102683418646,
      "loss": 0.2952,
      "step": 451
    },
    {
      "epoch": 0.017523624909134963,
      "grad_norm": 0.1150301918387413,
      "learning_rate": 0.00019653327128897163,
      "loss": 0.2402,
      "step": 452
    },
    {
      "epoch": 0.017562393990792343,
      "grad_norm": 0.18920862674713135,
      "learning_rate": 0.0001965255157437568,
      "loss": 0.3637,
      "step": 453
    },
    {
      "epoch": 0.017601163072449723,
      "grad_norm": 0.15430313348770142,
      "learning_rate": 0.00019651776019854195,
      "loss": 0.4022,
      "step": 454
    },
    {
      "epoch": 0.0176399321541071,
      "grad_norm": 0.1298331469297409,
      "learning_rate": 0.00019651000465332715,
      "loss": 0.3911,
      "step": 455
    },
    {
      "epoch": 0.01767870123576448,
      "grad_norm": 0.19578342139720917,
      "learning_rate": 0.0001965022491081123,
      "loss": 0.4099,
      "step": 456
    },
    {
      "epoch": 0.017717470317421855,
      "grad_norm": 0.14448004961013794,
      "learning_rate": 0.0001964944935628975,
      "loss": 0.3414,
      "step": 457
    },
    {
      "epoch": 0.017756239399079235,
      "grad_norm": 0.15065518021583557,
      "learning_rate": 0.00019648673801768264,
      "loss": 0.3323,
      "step": 458
    },
    {
      "epoch": 0.01779500848073661,
      "grad_norm": 0.1625320464372635,
      "learning_rate": 0.00019647898247246784,
      "loss": 0.3804,
      "step": 459
    },
    {
      "epoch": 0.01783377756239399,
      "grad_norm": 0.13624584674835205,
      "learning_rate": 0.00019647122692725299,
      "loss": 0.3658,
      "step": 460
    },
    {
      "epoch": 0.017872546644051367,
      "grad_norm": 0.15552262961864471,
      "learning_rate": 0.00019646347138203816,
      "loss": 0.3745,
      "step": 461
    },
    {
      "epoch": 0.017911315725708747,
      "grad_norm": 0.13408325612545013,
      "learning_rate": 0.00019645571583682333,
      "loss": 0.336,
      "step": 462
    },
    {
      "epoch": 0.017950084807366127,
      "grad_norm": 0.16611824929714203,
      "learning_rate": 0.0001964479602916085,
      "loss": 0.3877,
      "step": 463
    },
    {
      "epoch": 0.017988853889023503,
      "grad_norm": 0.20148736238479614,
      "learning_rate": 0.00019644020474639368,
      "loss": 0.4303,
      "step": 464
    },
    {
      "epoch": 0.018027622970680883,
      "grad_norm": 0.18641529977321625,
      "learning_rate": 0.00019643244920117885,
      "loss": 0.4074,
      "step": 465
    },
    {
      "epoch": 0.01806639205233826,
      "grad_norm": 0.1860334575176239,
      "learning_rate": 0.00019642469365596402,
      "loss": 0.2978,
      "step": 466
    },
    {
      "epoch": 0.01810516113399564,
      "grad_norm": 0.12042317539453506,
      "learning_rate": 0.0001964169381107492,
      "loss": 0.3415,
      "step": 467
    },
    {
      "epoch": 0.018143930215653015,
      "grad_norm": 0.14925208687782288,
      "learning_rate": 0.00019640918256553437,
      "loss": 0.3002,
      "step": 468
    },
    {
      "epoch": 0.018182699297310395,
      "grad_norm": 0.14147423207759857,
      "learning_rate": 0.00019640142702031954,
      "loss": 0.3785,
      "step": 469
    },
    {
      "epoch": 0.018221468378967775,
      "grad_norm": 0.1361871361732483,
      "learning_rate": 0.00019639367147510471,
      "loss": 0.3277,
      "step": 470
    },
    {
      "epoch": 0.01826023746062515,
      "grad_norm": 0.13133303821086884,
      "learning_rate": 0.0001963859159298899,
      "loss": 0.3599,
      "step": 471
    },
    {
      "epoch": 0.01829900654228253,
      "grad_norm": 0.1453704833984375,
      "learning_rate": 0.00019637816038467506,
      "loss": 0.2955,
      "step": 472
    },
    {
      "epoch": 0.018337775623939907,
      "grad_norm": 0.13043160736560822,
      "learning_rate": 0.00019637040483946023,
      "loss": 0.3153,
      "step": 473
    },
    {
      "epoch": 0.018376544705597287,
      "grad_norm": 0.1674705445766449,
      "learning_rate": 0.0001963626492942454,
      "loss": 0.3681,
      "step": 474
    },
    {
      "epoch": 0.018415313787254663,
      "grad_norm": 0.17125336825847626,
      "learning_rate": 0.00019635489374903058,
      "loss": 0.4033,
      "step": 475
    },
    {
      "epoch": 0.018454082868912043,
      "grad_norm": 0.13353514671325684,
      "learning_rate": 0.00019634713820381575,
      "loss": 0.4049,
      "step": 476
    },
    {
      "epoch": 0.01849285195056942,
      "grad_norm": 0.14961327612400055,
      "learning_rate": 0.0001963393826586009,
      "loss": 0.338,
      "step": 477
    },
    {
      "epoch": 0.0185316210322268,
      "grad_norm": 0.12232797592878342,
      "learning_rate": 0.0001963316271133861,
      "loss": 0.3474,
      "step": 478
    },
    {
      "epoch": 0.01857039011388418,
      "grad_norm": 0.1387988179922104,
      "learning_rate": 0.00019632387156817124,
      "loss": 0.3184,
      "step": 479
    },
    {
      "epoch": 0.018609159195541555,
      "grad_norm": 0.14245088398456573,
      "learning_rate": 0.00019631611602295644,
      "loss": 0.3307,
      "step": 480
    },
    {
      "epoch": 0.018647928277198935,
      "grad_norm": 0.12236102670431137,
      "learning_rate": 0.0001963083604777416,
      "loss": 0.4003,
      "step": 481
    },
    {
      "epoch": 0.01868669735885631,
      "grad_norm": 0.12563689053058624,
      "learning_rate": 0.0001963006049325268,
      "loss": 0.3761,
      "step": 482
    },
    {
      "epoch": 0.01872546644051369,
      "grad_norm": 0.1455814242362976,
      "learning_rate": 0.00019629284938731193,
      "loss": 0.4315,
      "step": 483
    },
    {
      "epoch": 0.018764235522171067,
      "grad_norm": 0.15698201954364777,
      "learning_rate": 0.0001962850938420971,
      "loss": 0.4024,
      "step": 484
    },
    {
      "epoch": 0.018803004603828447,
      "grad_norm": 0.13749755918979645,
      "learning_rate": 0.00019627733829688228,
      "loss": 0.4304,
      "step": 485
    },
    {
      "epoch": 0.018841773685485824,
      "grad_norm": 0.12770070135593414,
      "learning_rate": 0.00019626958275166745,
      "loss": 0.3543,
      "step": 486
    },
    {
      "epoch": 0.018880542767143203,
      "grad_norm": 0.14369884133338928,
      "learning_rate": 0.00019626182720645262,
      "loss": 0.4076,
      "step": 487
    },
    {
      "epoch": 0.018919311848800583,
      "grad_norm": 0.15312813222408295,
      "learning_rate": 0.0001962540716612378,
      "loss": 0.4115,
      "step": 488
    },
    {
      "epoch": 0.01895808093045796,
      "grad_norm": 0.13087573647499084,
      "learning_rate": 0.00019624631611602297,
      "loss": 0.3201,
      "step": 489
    },
    {
      "epoch": 0.01899685001211534,
      "grad_norm": 0.16908030211925507,
      "learning_rate": 0.00019623856057080814,
      "loss": 0.4052,
      "step": 490
    },
    {
      "epoch": 0.019035619093772715,
      "grad_norm": 0.12254291772842407,
      "learning_rate": 0.0001962308050255933,
      "loss": 0.3118,
      "step": 491
    },
    {
      "epoch": 0.019074388175430095,
      "grad_norm": 0.09603819996118546,
      "learning_rate": 0.0001962230494803785,
      "loss": 0.216,
      "step": 492
    },
    {
      "epoch": 0.01911315725708747,
      "grad_norm": 0.11954992264509201,
      "learning_rate": 0.00019621529393516363,
      "loss": 0.2884,
      "step": 493
    },
    {
      "epoch": 0.01915192633874485,
      "grad_norm": 0.14627595245838165,
      "learning_rate": 0.00019620753838994883,
      "loss": 0.381,
      "step": 494
    },
    {
      "epoch": 0.019190695420402228,
      "grad_norm": 0.15289384126663208,
      "learning_rate": 0.00019619978284473398,
      "loss": 0.3847,
      "step": 495
    },
    {
      "epoch": 0.019229464502059607,
      "grad_norm": 0.16771847009658813,
      "learning_rate": 0.00019619202729951918,
      "loss": 0.4231,
      "step": 496
    },
    {
      "epoch": 0.019268233583716987,
      "grad_norm": 0.1541203111410141,
      "learning_rate": 0.00019618427175430433,
      "loss": 0.462,
      "step": 497
    },
    {
      "epoch": 0.019307002665374363,
      "grad_norm": 0.14206314086914062,
      "learning_rate": 0.0001961765162090895,
      "loss": 0.463,
      "step": 498
    },
    {
      "epoch": 0.019345771747031743,
      "grad_norm": 0.1531120389699936,
      "learning_rate": 0.00019616876066387467,
      "loss": 0.2751,
      "step": 499
    },
    {
      "epoch": 0.01938454082868912,
      "grad_norm": 0.1362907737493515,
      "learning_rate": 0.00019616100511865984,
      "loss": 0.3358,
      "step": 500
    },
    {
      "epoch": 0.0194233099103465,
      "grad_norm": 0.1308339387178421,
      "learning_rate": 0.00019615324957344502,
      "loss": 0.4206,
      "step": 501
    },
    {
      "epoch": 0.019462078992003876,
      "grad_norm": 0.12440216541290283,
      "learning_rate": 0.0001961454940282302,
      "loss": 0.3088,
      "step": 502
    },
    {
      "epoch": 0.019500848073661255,
      "grad_norm": 0.14742709696292877,
      "learning_rate": 0.0001961377384830154,
      "loss": 0.3706,
      "step": 503
    },
    {
      "epoch": 0.01953961715531863,
      "grad_norm": 0.17309924960136414,
      "learning_rate": 0.00019612998293780054,
      "loss": 0.3825,
      "step": 504
    },
    {
      "epoch": 0.01957838623697601,
      "grad_norm": 0.11520370841026306,
      "learning_rate": 0.0001961222273925857,
      "loss": 0.2865,
      "step": 505
    },
    {
      "epoch": 0.01961715531863339,
      "grad_norm": 0.13414233922958374,
      "learning_rate": 0.00019611447184737088,
      "loss": 0.3256,
      "step": 506
    },
    {
      "epoch": 0.019655924400290768,
      "grad_norm": 0.13777656853199005,
      "learning_rate": 0.00019610671630215605,
      "loss": 0.4037,
      "step": 507
    },
    {
      "epoch": 0.019694693481948147,
      "grad_norm": 0.16367879509925842,
      "learning_rate": 0.00019609896075694123,
      "loss": 0.397,
      "step": 508
    },
    {
      "epoch": 0.019733462563605524,
      "grad_norm": 0.1250070184469223,
      "learning_rate": 0.0001960912052117264,
      "loss": 0.3084,
      "step": 509
    },
    {
      "epoch": 0.019772231645262903,
      "grad_norm": 0.12609577178955078,
      "learning_rate": 0.00019608344966651157,
      "loss": 0.363,
      "step": 510
    },
    {
      "epoch": 0.01981100072692028,
      "grad_norm": 0.18983499705791473,
      "learning_rate": 0.00019607569412129675,
      "loss": 0.4316,
      "step": 511
    },
    {
      "epoch": 0.01984976980857766,
      "grad_norm": 0.12098043411970139,
      "learning_rate": 0.0001960679385760819,
      "loss": 0.3096,
      "step": 512
    },
    {
      "epoch": 0.01988853889023504,
      "grad_norm": 0.13141795992851257,
      "learning_rate": 0.0001960601830308671,
      "loss": 0.3682,
      "step": 513
    },
    {
      "epoch": 0.019927307971892416,
      "grad_norm": 0.1479913890361786,
      "learning_rate": 0.00019605242748565224,
      "loss": 0.395,
      "step": 514
    },
    {
      "epoch": 0.019966077053549795,
      "grad_norm": 0.12864726781845093,
      "learning_rate": 0.00019604467194043744,
      "loss": 0.2816,
      "step": 515
    },
    {
      "epoch": 0.02000484613520717,
      "grad_norm": 0.13275468349456787,
      "learning_rate": 0.00019603691639522258,
      "loss": 0.3444,
      "step": 516
    },
    {
      "epoch": 0.02004361521686455,
      "grad_norm": 0.1497001349925995,
      "learning_rate": 0.00019602916085000778,
      "loss": 0.3916,
      "step": 517
    },
    {
      "epoch": 0.020082384298521928,
      "grad_norm": 0.14230145514011383,
      "learning_rate": 0.00019602140530479293,
      "loss": 0.3455,
      "step": 518
    },
    {
      "epoch": 0.020121153380179307,
      "grad_norm": 0.18756361305713654,
      "learning_rate": 0.0001960136497595781,
      "loss": 0.4038,
      "step": 519
    },
    {
      "epoch": 0.020159922461836684,
      "grad_norm": 0.12030191719532013,
      "learning_rate": 0.00019600589421436327,
      "loss": 0.3443,
      "step": 520
    },
    {
      "epoch": 0.020198691543494064,
      "grad_norm": 0.12642276287078857,
      "learning_rate": 0.00019599813866914845,
      "loss": 0.2994,
      "step": 521
    },
    {
      "epoch": 0.020237460625151443,
      "grad_norm": 0.14097754657268524,
      "learning_rate": 0.00019599038312393362,
      "loss": 0.3421,
      "step": 522
    },
    {
      "epoch": 0.02027622970680882,
      "grad_norm": 0.1735939383506775,
      "learning_rate": 0.0001959826275787188,
      "loss": 0.46,
      "step": 523
    },
    {
      "epoch": 0.0203149987884662,
      "grad_norm": 0.1437433809041977,
      "learning_rate": 0.00019597487203350396,
      "loss": 0.4079,
      "step": 524
    },
    {
      "epoch": 0.020353767870123576,
      "grad_norm": 0.13783778250217438,
      "learning_rate": 0.00019596711648828914,
      "loss": 0.2767,
      "step": 525
    },
    {
      "epoch": 0.020392536951780955,
      "grad_norm": 0.16083037853240967,
      "learning_rate": 0.0001959593609430743,
      "loss": 0.4328,
      "step": 526
    },
    {
      "epoch": 0.020431306033438332,
      "grad_norm": 0.1532863974571228,
      "learning_rate": 0.00019595160539785948,
      "loss": 0.3776,
      "step": 527
    },
    {
      "epoch": 0.02047007511509571,
      "grad_norm": 0.17312666773796082,
      "learning_rate": 0.00019594384985264466,
      "loss": 0.3426,
      "step": 528
    },
    {
      "epoch": 0.020508844196753088,
      "grad_norm": 0.13133639097213745,
      "learning_rate": 0.00019593609430742983,
      "loss": 0.3644,
      "step": 529
    },
    {
      "epoch": 0.020547613278410468,
      "grad_norm": 0.17243625223636627,
      "learning_rate": 0.000195928338762215,
      "loss": 0.3709,
      "step": 530
    },
    {
      "epoch": 0.020586382360067847,
      "grad_norm": 0.18095728754997253,
      "learning_rate": 0.00019592058321700017,
      "loss": 0.2908,
      "step": 531
    },
    {
      "epoch": 0.020625151441725224,
      "grad_norm": 0.16339357197284698,
      "learning_rate": 0.00019591282767178535,
      "loss": 0.4088,
      "step": 532
    },
    {
      "epoch": 0.020663920523382603,
      "grad_norm": 0.12654350697994232,
      "learning_rate": 0.0001959050721265705,
      "loss": 0.3954,
      "step": 533
    },
    {
      "epoch": 0.02070268960503998,
      "grad_norm": 0.1298525482416153,
      "learning_rate": 0.0001958973165813557,
      "loss": 0.2782,
      "step": 534
    },
    {
      "epoch": 0.02074145868669736,
      "grad_norm": 0.14341552555561066,
      "learning_rate": 0.00019588956103614084,
      "loss": 0.3579,
      "step": 535
    },
    {
      "epoch": 0.020780227768354736,
      "grad_norm": 0.15922871232032776,
      "learning_rate": 0.00019588180549092604,
      "loss": 0.4608,
      "step": 536
    },
    {
      "epoch": 0.020818996850012116,
      "grad_norm": 0.12792560458183289,
      "learning_rate": 0.00019587404994571118,
      "loss": 0.2889,
      "step": 537
    },
    {
      "epoch": 0.020857765931669492,
      "grad_norm": 0.1475749909877777,
      "learning_rate": 0.00019586629440049638,
      "loss": 0.3236,
      "step": 538
    },
    {
      "epoch": 0.02089653501332687,
      "grad_norm": 0.14816170930862427,
      "learning_rate": 0.00019585853885528153,
      "loss": 0.3278,
      "step": 539
    },
    {
      "epoch": 0.02093530409498425,
      "grad_norm": 0.1923484355211258,
      "learning_rate": 0.0001958507833100667,
      "loss": 0.4004,
      "step": 540
    },
    {
      "epoch": 0.020974073176641628,
      "grad_norm": 0.14266391098499298,
      "learning_rate": 0.00019584302776485188,
      "loss": 0.3505,
      "step": 541
    },
    {
      "epoch": 0.021012842258299008,
      "grad_norm": 0.13322880864143372,
      "learning_rate": 0.00019583527221963705,
      "loss": 0.4029,
      "step": 542
    },
    {
      "epoch": 0.021051611339956384,
      "grad_norm": 0.164333313703537,
      "learning_rate": 0.00019582751667442222,
      "loss": 0.3505,
      "step": 543
    },
    {
      "epoch": 0.021090380421613764,
      "grad_norm": 0.17077764868736267,
      "learning_rate": 0.0001958197611292074,
      "loss": 0.3289,
      "step": 544
    },
    {
      "epoch": 0.02112914950327114,
      "grad_norm": 0.12227551639080048,
      "learning_rate": 0.00019581200558399257,
      "loss": 0.327,
      "step": 545
    },
    {
      "epoch": 0.02116791858492852,
      "grad_norm": 0.10787495225667953,
      "learning_rate": 0.00019580425003877774,
      "loss": 0.2478,
      "step": 546
    },
    {
      "epoch": 0.0212066876665859,
      "grad_norm": 0.2120770812034607,
      "learning_rate": 0.0001957964944935629,
      "loss": 0.4913,
      "step": 547
    },
    {
      "epoch": 0.021245456748243276,
      "grad_norm": 0.14822717010974884,
      "learning_rate": 0.00019578873894834808,
      "loss": 0.4111,
      "step": 548
    },
    {
      "epoch": 0.021284225829900656,
      "grad_norm": 0.15608283877372742,
      "learning_rate": 0.00019578098340313326,
      "loss": 0.3318,
      "step": 549
    },
    {
      "epoch": 0.021322994911558032,
      "grad_norm": 0.1402905285358429,
      "learning_rate": 0.00019577322785791843,
      "loss": 0.3252,
      "step": 550
    },
    {
      "epoch": 0.02136176399321541,
      "grad_norm": 0.12583139538764954,
      "learning_rate": 0.0001957654723127036,
      "loss": 0.3645,
      "step": 551
    },
    {
      "epoch": 0.021400533074872788,
      "grad_norm": 0.1477106362581253,
      "learning_rate": 0.00019575771676748878,
      "loss": 0.2729,
      "step": 552
    },
    {
      "epoch": 0.021439302156530168,
      "grad_norm": 0.16832514107227325,
      "learning_rate": 0.00019574996122227395,
      "loss": 0.3308,
      "step": 553
    },
    {
      "epoch": 0.021478071238187544,
      "grad_norm": 0.13375407457351685,
      "learning_rate": 0.0001957422056770591,
      "loss": 0.3161,
      "step": 554
    },
    {
      "epoch": 0.021516840319844924,
      "grad_norm": 0.1808338761329651,
      "learning_rate": 0.0001957344501318443,
      "loss": 0.3363,
      "step": 555
    },
    {
      "epoch": 0.021555609401502304,
      "grad_norm": 0.160701721906662,
      "learning_rate": 0.00019572669458662944,
      "loss": 0.3375,
      "step": 556
    },
    {
      "epoch": 0.02159437848315968,
      "grad_norm": 0.13458579778671265,
      "learning_rate": 0.00019571893904141464,
      "loss": 0.3182,
      "step": 557
    },
    {
      "epoch": 0.02163314756481706,
      "grad_norm": 0.17249923944473267,
      "learning_rate": 0.00019571118349619979,
      "loss": 0.4176,
      "step": 558
    },
    {
      "epoch": 0.021671916646474436,
      "grad_norm": 0.13681551814079285,
      "learning_rate": 0.00019570342795098499,
      "loss": 0.3064,
      "step": 559
    },
    {
      "epoch": 0.021710685728131816,
      "grad_norm": 0.14876124262809753,
      "learning_rate": 0.00019569567240577013,
      "loss": 0.4152,
      "step": 560
    },
    {
      "epoch": 0.021749454809789192,
      "grad_norm": 0.13656890392303467,
      "learning_rate": 0.0001956879168605553,
      "loss": 0.3293,
      "step": 561
    },
    {
      "epoch": 0.02178822389144657,
      "grad_norm": 0.15880084037780762,
      "learning_rate": 0.00019568016131534048,
      "loss": 0.4029,
      "step": 562
    },
    {
      "epoch": 0.021826992973103948,
      "grad_norm": 0.14270128309726715,
      "learning_rate": 0.00019567240577012565,
      "loss": 0.3167,
      "step": 563
    },
    {
      "epoch": 0.021865762054761328,
      "grad_norm": 0.19180063903331757,
      "learning_rate": 0.00019566465022491082,
      "loss": 0.3028,
      "step": 564
    },
    {
      "epoch": 0.021904531136418708,
      "grad_norm": 0.12258367985486984,
      "learning_rate": 0.000195656894679696,
      "loss": 0.3139,
      "step": 565
    },
    {
      "epoch": 0.021943300218076084,
      "grad_norm": 0.11452396214008331,
      "learning_rate": 0.00019564913913448117,
      "loss": 0.251,
      "step": 566
    },
    {
      "epoch": 0.021982069299733464,
      "grad_norm": 0.14361853897571564,
      "learning_rate": 0.00019564138358926634,
      "loss": 0.336,
      "step": 567
    },
    {
      "epoch": 0.02202083838139084,
      "grad_norm": 0.1515485644340515,
      "learning_rate": 0.0001956336280440515,
      "loss": 0.3697,
      "step": 568
    },
    {
      "epoch": 0.02205960746304822,
      "grad_norm": 0.9391334056854248,
      "learning_rate": 0.0001956258724988367,
      "loss": 0.3715,
      "step": 569
    },
    {
      "epoch": 0.022098376544705596,
      "grad_norm": 1.8353686332702637,
      "learning_rate": 0.00019561811695362183,
      "loss": 0.3687,
      "step": 570
    },
    {
      "epoch": 0.022137145626362976,
      "grad_norm": 1.3706902265548706,
      "learning_rate": 0.00019561036140840703,
      "loss": 0.4642,
      "step": 571
    },
    {
      "epoch": 0.022175914708020352,
      "grad_norm": 1.1550265550613403,
      "learning_rate": 0.00019560260586319218,
      "loss": 0.4143,
      "step": 572
    },
    {
      "epoch": 0.022214683789677732,
      "grad_norm": 0.509951114654541,
      "learning_rate": 0.00019559485031797738,
      "loss": 0.3327,
      "step": 573
    },
    {
      "epoch": 0.02225345287133511,
      "grad_norm": 0.5013702511787415,
      "learning_rate": 0.00019558709477276252,
      "loss": 0.3342,
      "step": 574
    },
    {
      "epoch": 0.022292221952992488,
      "grad_norm": 0.3059904873371124,
      "learning_rate": 0.0001955793392275477,
      "loss": 0.3877,
      "step": 575
    },
    {
      "epoch": 0.022330991034649868,
      "grad_norm": 0.202401801943779,
      "learning_rate": 0.00019557158368233287,
      "loss": 0.3863,
      "step": 576
    },
    {
      "epoch": 0.022369760116307244,
      "grad_norm": 0.20941194891929626,
      "learning_rate": 0.00019556382813711804,
      "loss": 0.3351,
      "step": 577
    },
    {
      "epoch": 0.022408529197964624,
      "grad_norm": 0.4104011654853821,
      "learning_rate": 0.00019555607259190321,
      "loss": 0.3836,
      "step": 578
    },
    {
      "epoch": 0.022447298279622,
      "grad_norm": 0.7336627840995789,
      "learning_rate": 0.0001955483170466884,
      "loss": 0.4236,
      "step": 579
    },
    {
      "epoch": 0.02248606736127938,
      "grad_norm": 0.37783563137054443,
      "learning_rate": 0.00019554056150147356,
      "loss": 0.4472,
      "step": 580
    },
    {
      "epoch": 0.02252483644293676,
      "grad_norm": 0.2758208215236664,
      "learning_rate": 0.00019553280595625873,
      "loss": 0.4315,
      "step": 581
    },
    {
      "epoch": 0.022563605524594136,
      "grad_norm": 0.2904183566570282,
      "learning_rate": 0.0001955250504110439,
      "loss": 0.3712,
      "step": 582
    },
    {
      "epoch": 0.022602374606251516,
      "grad_norm": 0.20140543580055237,
      "learning_rate": 0.00019551729486582908,
      "loss": 0.3585,
      "step": 583
    },
    {
      "epoch": 0.022641143687908892,
      "grad_norm": 0.21985921263694763,
      "learning_rate": 0.00019550953932061425,
      "loss": 0.3842,
      "step": 584
    },
    {
      "epoch": 0.022679912769566272,
      "grad_norm": 0.16363483667373657,
      "learning_rate": 0.00019550178377539942,
      "loss": 0.3463,
      "step": 585
    },
    {
      "epoch": 0.022718681851223648,
      "grad_norm": 0.14894932508468628,
      "learning_rate": 0.0001954940282301846,
      "loss": 0.3239,
      "step": 586
    },
    {
      "epoch": 0.022757450932881028,
      "grad_norm": 0.14155693352222443,
      "learning_rate": 0.00019548627268496977,
      "loss": 0.3524,
      "step": 587
    },
    {
      "epoch": 0.022796220014538404,
      "grad_norm": 0.17409126460552216,
      "learning_rate": 0.00019547851713975494,
      "loss": 0.364,
      "step": 588
    },
    {
      "epoch": 0.022834989096195784,
      "grad_norm": 0.13523845374584198,
      "learning_rate": 0.0001954707615945401,
      "loss": 0.341,
      "step": 589
    },
    {
      "epoch": 0.022873758177853164,
      "grad_norm": 0.1435757577419281,
      "learning_rate": 0.0001954630060493253,
      "loss": 0.3215,
      "step": 590
    },
    {
      "epoch": 0.02291252725951054,
      "grad_norm": 0.13787634670734406,
      "learning_rate": 0.00019545525050411043,
      "loss": 0.3713,
      "step": 591
    },
    {
      "epoch": 0.02295129634116792,
      "grad_norm": 0.12746509909629822,
      "learning_rate": 0.00019544749495889563,
      "loss": 0.347,
      "step": 592
    },
    {
      "epoch": 0.022990065422825296,
      "grad_norm": 0.17007026076316833,
      "learning_rate": 0.00019543973941368078,
      "loss": 0.3104,
      "step": 593
    },
    {
      "epoch": 0.023028834504482676,
      "grad_norm": 0.14676226675510406,
      "learning_rate": 0.00019543198386846598,
      "loss": 0.3754,
      "step": 594
    },
    {
      "epoch": 0.023067603586140052,
      "grad_norm": 0.16150449216365814,
      "learning_rate": 0.00019542422832325113,
      "loss": 0.322,
      "step": 595
    },
    {
      "epoch": 0.023106372667797432,
      "grad_norm": 0.12332477420568466,
      "learning_rate": 0.0001954164727780363,
      "loss": 0.2539,
      "step": 596
    },
    {
      "epoch": 0.023145141749454808,
      "grad_norm": 0.11435279250144958,
      "learning_rate": 0.00019540871723282147,
      "loss": 0.2386,
      "step": 597
    },
    {
      "epoch": 0.023183910831112188,
      "grad_norm": 0.15886913239955902,
      "learning_rate": 0.00019540096168760664,
      "loss": 0.4188,
      "step": 598
    },
    {
      "epoch": 0.023222679912769568,
      "grad_norm": 0.1929069310426712,
      "learning_rate": 0.00019539320614239182,
      "loss": 0.4831,
      "step": 599
    },
    {
      "epoch": 0.023261448994426944,
      "grad_norm": 0.14406147599220276,
      "learning_rate": 0.000195385450597177,
      "loss": 0.3404,
      "step": 600
    },
    {
      "epoch": 0.023300218076084324,
      "grad_norm": 0.15912429988384247,
      "learning_rate": 0.00019537769505196216,
      "loss": 0.4249,
      "step": 601
    },
    {
      "epoch": 0.0233389871577417,
      "grad_norm": 0.13816098868846893,
      "learning_rate": 0.00019536993950674733,
      "loss": 0.3067,
      "step": 602
    },
    {
      "epoch": 0.02337775623939908,
      "grad_norm": 0.1368275135755539,
      "learning_rate": 0.0001953621839615325,
      "loss": 0.3286,
      "step": 603
    },
    {
      "epoch": 0.023416525321056456,
      "grad_norm": 0.1689123511314392,
      "learning_rate": 0.00019535442841631768,
      "loss": 0.3293,
      "step": 604
    },
    {
      "epoch": 0.023455294402713836,
      "grad_norm": 0.11681830137968063,
      "learning_rate": 0.00019534667287110285,
      "loss": 0.2835,
      "step": 605
    },
    {
      "epoch": 0.023494063484371212,
      "grad_norm": 0.2824833393096924,
      "learning_rate": 0.00019533891732588803,
      "loss": 0.3601,
      "step": 606
    },
    {
      "epoch": 0.023532832566028592,
      "grad_norm": 0.14289884269237518,
      "learning_rate": 0.0001953311617806732,
      "loss": 0.3335,
      "step": 607
    },
    {
      "epoch": 0.023571601647685972,
      "grad_norm": 0.2678700387477875,
      "learning_rate": 0.00019532340623545837,
      "loss": 0.2871,
      "step": 608
    },
    {
      "epoch": 0.023610370729343348,
      "grad_norm": 0.13805721700191498,
      "learning_rate": 0.00019531565069024354,
      "loss": 0.3739,
      "step": 609
    },
    {
      "epoch": 0.023649139811000728,
      "grad_norm": 0.1062786728143692,
      "learning_rate": 0.0001953078951450287,
      "loss": 0.2719,
      "step": 610
    },
    {
      "epoch": 0.023687908892658104,
      "grad_norm": 0.15580390393733978,
      "learning_rate": 0.0001953001395998139,
      "loss": 0.307,
      "step": 611
    },
    {
      "epoch": 0.023726677974315484,
      "grad_norm": 0.16413190960884094,
      "learning_rate": 0.00019529238405459904,
      "loss": 0.2902,
      "step": 612
    },
    {
      "epoch": 0.02376544705597286,
      "grad_norm": 0.14230580627918243,
      "learning_rate": 0.00019528462850938424,
      "loss": 0.2791,
      "step": 613
    },
    {
      "epoch": 0.02380421613763024,
      "grad_norm": 0.19018952548503876,
      "learning_rate": 0.00019527687296416938,
      "loss": 0.4349,
      "step": 614
    },
    {
      "epoch": 0.023842985219287616,
      "grad_norm": 0.14707177877426147,
      "learning_rate": 0.00019526911741895458,
      "loss": 0.3461,
      "step": 615
    },
    {
      "epoch": 0.023881754300944996,
      "grad_norm": 0.13823722302913666,
      "learning_rate": 0.00019526136187373973,
      "loss": 0.3332,
      "step": 616
    },
    {
      "epoch": 0.023920523382602376,
      "grad_norm": 0.17502743005752563,
      "learning_rate": 0.0001952536063285249,
      "loss": 0.3794,
      "step": 617
    },
    {
      "epoch": 0.023959292464259752,
      "grad_norm": 0.16534532606601715,
      "learning_rate": 0.00019524585078331007,
      "loss": 0.3735,
      "step": 618
    },
    {
      "epoch": 0.023998061545917132,
      "grad_norm": 0.13790617883205414,
      "learning_rate": 0.00019523809523809525,
      "loss": 0.3402,
      "step": 619
    },
    {
      "epoch": 0.02403683062757451,
      "grad_norm": 0.10798073559999466,
      "learning_rate": 0.00019523033969288042,
      "loss": 0.2566,
      "step": 620
    },
    {
      "epoch": 0.024075599709231888,
      "grad_norm": 0.12738962471485138,
      "learning_rate": 0.0001952225841476656,
      "loss": 0.3025,
      "step": 621
    },
    {
      "epoch": 0.024114368790889264,
      "grad_norm": 0.14039160311222076,
      "learning_rate": 0.00019521482860245076,
      "loss": 0.3105,
      "step": 622
    },
    {
      "epoch": 0.024153137872546644,
      "grad_norm": 0.14337095618247986,
      "learning_rate": 0.00019520707305723594,
      "loss": 0.3483,
      "step": 623
    },
    {
      "epoch": 0.024191906954204024,
      "grad_norm": 0.19594360888004303,
      "learning_rate": 0.00019519931751202108,
      "loss": 0.4179,
      "step": 624
    },
    {
      "epoch": 0.0242306760358614,
      "grad_norm": 0.14699982106685638,
      "learning_rate": 0.00019519156196680628,
      "loss": 0.2671,
      "step": 625
    },
    {
      "epoch": 0.02426944511751878,
      "grad_norm": 0.12292362004518509,
      "learning_rate": 0.00019518380642159146,
      "loss": 0.3385,
      "step": 626
    },
    {
      "epoch": 0.024308214199176156,
      "grad_norm": 0.1570376306772232,
      "learning_rate": 0.00019517605087637663,
      "loss": 0.36,
      "step": 627
    },
    {
      "epoch": 0.024346983280833536,
      "grad_norm": 0.16306377947330475,
      "learning_rate": 0.0001951682953311618,
      "loss": 0.4046,
      "step": 628
    },
    {
      "epoch": 0.024385752362490912,
      "grad_norm": 0.13903959095478058,
      "learning_rate": 0.00019516053978594697,
      "loss": 0.3848,
      "step": 629
    },
    {
      "epoch": 0.024424521444148292,
      "grad_norm": 0.10529884696006775,
      "learning_rate": 0.00019515278424073215,
      "loss": 0.2881,
      "step": 630
    },
    {
      "epoch": 0.02446329052580567,
      "grad_norm": 0.13262340426445007,
      "learning_rate": 0.0001951450286955173,
      "loss": 0.3196,
      "step": 631
    },
    {
      "epoch": 0.024502059607463048,
      "grad_norm": 0.120638407766819,
      "learning_rate": 0.0001951372731503025,
      "loss": 0.3393,
      "step": 632
    },
    {
      "epoch": 0.024540828689120428,
      "grad_norm": 0.11723420023918152,
      "learning_rate": 0.00019512951760508764,
      "loss": 0.3085,
      "step": 633
    },
    {
      "epoch": 0.024579597770777804,
      "grad_norm": 0.1314477175474167,
      "learning_rate": 0.00019512176205987284,
      "loss": 0.3251,
      "step": 634
    },
    {
      "epoch": 0.024618366852435184,
      "grad_norm": 0.17395444214344025,
      "learning_rate": 0.00019511400651465798,
      "loss": 0.3446,
      "step": 635
    },
    {
      "epoch": 0.02465713593409256,
      "grad_norm": 0.15411558747291565,
      "learning_rate": 0.00019510625096944318,
      "loss": 0.3059,
      "step": 636
    },
    {
      "epoch": 0.02469590501574994,
      "grad_norm": 0.14704738557338715,
      "learning_rate": 0.00019509849542422833,
      "loss": 0.3413,
      "step": 637
    },
    {
      "epoch": 0.024734674097407316,
      "grad_norm": 0.1162613183259964,
      "learning_rate": 0.0001950907398790135,
      "loss": 0.3152,
      "step": 638
    },
    {
      "epoch": 0.024773443179064696,
      "grad_norm": 0.17963512241840363,
      "learning_rate": 0.00019508298433379867,
      "loss": 0.3228,
      "step": 639
    },
    {
      "epoch": 0.024812212260722073,
      "grad_norm": 0.15124431252479553,
      "learning_rate": 0.00019507522878858385,
      "loss": 0.408,
      "step": 640
    },
    {
      "epoch": 0.024850981342379452,
      "grad_norm": 0.13263171911239624,
      "learning_rate": 0.00019506747324336902,
      "loss": 0.3432,
      "step": 641
    },
    {
      "epoch": 0.024889750424036832,
      "grad_norm": 0.1430506855249405,
      "learning_rate": 0.0001950597176981542,
      "loss": 0.3659,
      "step": 642
    },
    {
      "epoch": 0.02492851950569421,
      "grad_norm": 0.11329846829175949,
      "learning_rate": 0.00019505196215293937,
      "loss": 0.3736,
      "step": 643
    },
    {
      "epoch": 0.024967288587351588,
      "grad_norm": 0.1095627024769783,
      "learning_rate": 0.00019504420660772454,
      "loss": 0.219,
      "step": 644
    },
    {
      "epoch": 0.025006057669008964,
      "grad_norm": 0.1244802325963974,
      "learning_rate": 0.00019503645106250968,
      "loss": 0.3388,
      "step": 645
    },
    {
      "epoch": 0.025044826750666344,
      "grad_norm": 0.19948188960552216,
      "learning_rate": 0.00019502869551729488,
      "loss": 0.4291,
      "step": 646
    },
    {
      "epoch": 0.02508359583232372,
      "grad_norm": 0.14367417991161346,
      "learning_rate": 0.00019502093997208003,
      "loss": 0.3599,
      "step": 647
    },
    {
      "epoch": 0.0251223649139811,
      "grad_norm": 0.14436271786689758,
      "learning_rate": 0.00019501318442686523,
      "loss": 0.309,
      "step": 648
    },
    {
      "epoch": 0.025161133995638477,
      "grad_norm": 0.14636613428592682,
      "learning_rate": 0.00019500542888165038,
      "loss": 0.3612,
      "step": 649
    },
    {
      "epoch": 0.025199903077295856,
      "grad_norm": 0.13061213493347168,
      "learning_rate": 0.00019499767333643558,
      "loss": 0.397,
      "step": 650
    },
    {
      "epoch": 0.025238672158953236,
      "grad_norm": 0.18870125710964203,
      "learning_rate": 0.00019498991779122072,
      "loss": 0.3053,
      "step": 651
    },
    {
      "epoch": 0.025277441240610612,
      "grad_norm": 0.14134030044078827,
      "learning_rate": 0.0001949821622460059,
      "loss": 0.3768,
      "step": 652
    },
    {
      "epoch": 0.025316210322267992,
      "grad_norm": 0.10310565680265427,
      "learning_rate": 0.00019497440670079107,
      "loss": 0.2463,
      "step": 653
    },
    {
      "epoch": 0.02535497940392537,
      "grad_norm": 0.12161462008953094,
      "learning_rate": 0.00019496665115557624,
      "loss": 0.3629,
      "step": 654
    },
    {
      "epoch": 0.02539374848558275,
      "grad_norm": 0.14353173971176147,
      "learning_rate": 0.0001949588956103614,
      "loss": 0.355,
      "step": 655
    },
    {
      "epoch": 0.025432517567240125,
      "grad_norm": 0.1453709751367569,
      "learning_rate": 0.00019495114006514659,
      "loss": 0.3612,
      "step": 656
    },
    {
      "epoch": 0.025471286648897504,
      "grad_norm": 0.1242619976401329,
      "learning_rate": 0.00019494338451993176,
      "loss": 0.2684,
      "step": 657
    },
    {
      "epoch": 0.025510055730554884,
      "grad_norm": 0.1471596211194992,
      "learning_rate": 0.00019493562897471693,
      "loss": 0.3435,
      "step": 658
    },
    {
      "epoch": 0.02554882481221226,
      "grad_norm": 0.12913718819618225,
      "learning_rate": 0.0001949278734295021,
      "loss": 0.3304,
      "step": 659
    },
    {
      "epoch": 0.02558759389386964,
      "grad_norm": 0.14096295833587646,
      "learning_rate": 0.00019492011788428728,
      "loss": 0.3015,
      "step": 660
    },
    {
      "epoch": 0.025626362975527017,
      "grad_norm": 0.1536055952310562,
      "learning_rate": 0.00019491236233907245,
      "loss": 0.3761,
      "step": 661
    },
    {
      "epoch": 0.025665132057184396,
      "grad_norm": 0.13636817038059235,
      "learning_rate": 0.00019490460679385762,
      "loss": 0.3202,
      "step": 662
    },
    {
      "epoch": 0.025703901138841773,
      "grad_norm": 0.14299120008945465,
      "learning_rate": 0.0001948968512486428,
      "loss": 0.3506,
      "step": 663
    },
    {
      "epoch": 0.025742670220499152,
      "grad_norm": 0.1567169576883316,
      "learning_rate": 0.00019488909570342797,
      "loss": 0.419,
      "step": 664
    },
    {
      "epoch": 0.02578143930215653,
      "grad_norm": 0.13163630664348602,
      "learning_rate": 0.00019488134015821314,
      "loss": 0.3771,
      "step": 665
    },
    {
      "epoch": 0.02582020838381391,
      "grad_norm": 0.11811084300279617,
      "learning_rate": 0.00019487358461299829,
      "loss": 0.2622,
      "step": 666
    },
    {
      "epoch": 0.025858977465471288,
      "grad_norm": 0.13206157088279724,
      "learning_rate": 0.00019486582906778349,
      "loss": 0.3733,
      "step": 667
    },
    {
      "epoch": 0.025897746547128665,
      "grad_norm": 0.17198975384235382,
      "learning_rate": 0.00019485807352256863,
      "loss": 0.3838,
      "step": 668
    },
    {
      "epoch": 0.025936515628786044,
      "grad_norm": 0.14025801420211792,
      "learning_rate": 0.00019485031797735383,
      "loss": 0.3279,
      "step": 669
    },
    {
      "epoch": 0.02597528471044342,
      "grad_norm": 0.14524196088314056,
      "learning_rate": 0.00019484256243213898,
      "loss": 0.3956,
      "step": 670
    },
    {
      "epoch": 0.0260140537921008,
      "grad_norm": 0.171369731426239,
      "learning_rate": 0.00019483480688692418,
      "loss": 0.3884,
      "step": 671
    },
    {
      "epoch": 0.026052822873758177,
      "grad_norm": 0.14057689905166626,
      "learning_rate": 0.00019482705134170932,
      "loss": 0.3122,
      "step": 672
    },
    {
      "epoch": 0.026091591955415556,
      "grad_norm": 0.11704333126544952,
      "learning_rate": 0.0001948192957964945,
      "loss": 0.2717,
      "step": 673
    },
    {
      "epoch": 0.026130361037072933,
      "grad_norm": 0.15853378176689148,
      "learning_rate": 0.00019481154025127967,
      "loss": 0.4561,
      "step": 674
    },
    {
      "epoch": 0.026169130118730313,
      "grad_norm": 0.15986555814743042,
      "learning_rate": 0.00019480378470606484,
      "loss": 0.3698,
      "step": 675
    },
    {
      "epoch": 0.026207899200387692,
      "grad_norm": 0.21201330423355103,
      "learning_rate": 0.00019479602916085001,
      "loss": 0.3644,
      "step": 676
    },
    {
      "epoch": 0.02624666828204507,
      "grad_norm": 0.14610984921455383,
      "learning_rate": 0.0001947882736156352,
      "loss": 0.3386,
      "step": 677
    },
    {
      "epoch": 0.02628543736370245,
      "grad_norm": 0.1655457615852356,
      "learning_rate": 0.00019478051807042036,
      "loss": 0.4625,
      "step": 678
    },
    {
      "epoch": 0.026324206445359825,
      "grad_norm": 0.29047563672065735,
      "learning_rate": 0.00019477276252520553,
      "loss": 0.3383,
      "step": 679
    },
    {
      "epoch": 0.026362975527017204,
      "grad_norm": 0.1609141230583191,
      "learning_rate": 0.0001947650069799907,
      "loss": 0.3517,
      "step": 680
    },
    {
      "epoch": 0.02640174460867458,
      "grad_norm": 0.15208369493484497,
      "learning_rate": 0.00019475725143477588,
      "loss": 0.3451,
      "step": 681
    },
    {
      "epoch": 0.02644051369033196,
      "grad_norm": 0.13443222641944885,
      "learning_rate": 0.00019474949588956105,
      "loss": 0.325,
      "step": 682
    },
    {
      "epoch": 0.026479282771989337,
      "grad_norm": 0.22076444327831268,
      "learning_rate": 0.00019474174034434622,
      "loss": 0.453,
      "step": 683
    },
    {
      "epoch": 0.026518051853646717,
      "grad_norm": 0.14659824967384338,
      "learning_rate": 0.0001947339847991314,
      "loss": 0.3662,
      "step": 684
    },
    {
      "epoch": 0.026556820935304096,
      "grad_norm": 0.15056480467319489,
      "learning_rate": 0.00019472622925391657,
      "loss": 0.2978,
      "step": 685
    },
    {
      "epoch": 0.026595590016961473,
      "grad_norm": 0.11042409390211105,
      "learning_rate": 0.00019471847370870174,
      "loss": 0.3017,
      "step": 686
    },
    {
      "epoch": 0.026634359098618852,
      "grad_norm": 0.15722766518592834,
      "learning_rate": 0.0001947107181634869,
      "loss": 0.4367,
      "step": 687
    },
    {
      "epoch": 0.02667312818027623,
      "grad_norm": 0.16413244605064392,
      "learning_rate": 0.0001947029626182721,
      "loss": 0.2729,
      "step": 688
    },
    {
      "epoch": 0.02671189726193361,
      "grad_norm": 0.13052162528038025,
      "learning_rate": 0.00019469520707305723,
      "loss": 0.3847,
      "step": 689
    },
    {
      "epoch": 0.026750666343590985,
      "grad_norm": 0.11671365797519684,
      "learning_rate": 0.00019468745152784243,
      "loss": 0.3387,
      "step": 690
    },
    {
      "epoch": 0.026789435425248365,
      "grad_norm": 0.15717360377311707,
      "learning_rate": 0.00019467969598262758,
      "loss": 0.3345,
      "step": 691
    },
    {
      "epoch": 0.026828204506905744,
      "grad_norm": 0.11671122163534164,
      "learning_rate": 0.00019467194043741278,
      "loss": 0.2723,
      "step": 692
    },
    {
      "epoch": 0.02686697358856312,
      "grad_norm": 0.1834949404001236,
      "learning_rate": 0.00019466418489219792,
      "loss": 0.3149,
      "step": 693
    },
    {
      "epoch": 0.0269057426702205,
      "grad_norm": 0.1521560549736023,
      "learning_rate": 0.0001946564293469831,
      "loss": 0.3507,
      "step": 694
    },
    {
      "epoch": 0.026944511751877877,
      "grad_norm": 0.3263343572616577,
      "learning_rate": 0.00019464867380176827,
      "loss": 0.3839,
      "step": 695
    },
    {
      "epoch": 0.026983280833535257,
      "grad_norm": 0.14289487898349762,
      "learning_rate": 0.00019464091825655344,
      "loss": 0.3251,
      "step": 696
    },
    {
      "epoch": 0.027022049915192633,
      "grad_norm": 0.1466882973909378,
      "learning_rate": 0.00019463316271133862,
      "loss": 0.4031,
      "step": 697
    },
    {
      "epoch": 0.027060818996850013,
      "grad_norm": 0.173664852976799,
      "learning_rate": 0.0001946254071661238,
      "loss": 0.3918,
      "step": 698
    },
    {
      "epoch": 0.02709958807850739,
      "grad_norm": 0.3488028049468994,
      "learning_rate": 0.00019461765162090896,
      "loss": 0.3445,
      "step": 699
    },
    {
      "epoch": 0.02713835716016477,
      "grad_norm": 0.14075805246829987,
      "learning_rate": 0.00019460989607569413,
      "loss": 0.3639,
      "step": 700
    },
    {
      "epoch": 0.02717712624182215,
      "grad_norm": 0.1685851514339447,
      "learning_rate": 0.00019460214053047928,
      "loss": 0.2941,
      "step": 701
    },
    {
      "epoch": 0.027215895323479525,
      "grad_norm": 0.17793361842632294,
      "learning_rate": 0.00019459438498526448,
      "loss": 0.3262,
      "step": 702
    },
    {
      "epoch": 0.027254664405136905,
      "grad_norm": 0.23459628224372864,
      "learning_rate": 0.00019458662944004963,
      "loss": 0.4432,
      "step": 703
    },
    {
      "epoch": 0.02729343348679428,
      "grad_norm": 0.20118007063865662,
      "learning_rate": 0.00019457887389483483,
      "loss": 0.4388,
      "step": 704
    },
    {
      "epoch": 0.02733220256845166,
      "grad_norm": 0.16348223388195038,
      "learning_rate": 0.00019457111834962,
      "loss": 0.3313,
      "step": 705
    },
    {
      "epoch": 0.027370971650109037,
      "grad_norm": 0.15311217308044434,
      "learning_rate": 0.00019456336280440517,
      "loss": 0.3044,
      "step": 706
    },
    {
      "epoch": 0.027409740731766417,
      "grad_norm": 0.17977260053157806,
      "learning_rate": 0.00019455560725919034,
      "loss": 0.3338,
      "step": 707
    },
    {
      "epoch": 0.027448509813423793,
      "grad_norm": 0.14893856644630432,
      "learning_rate": 0.0001945478517139755,
      "loss": 0.377,
      "step": 708
    },
    {
      "epoch": 0.027487278895081173,
      "grad_norm": 0.16728457808494568,
      "learning_rate": 0.0001945400961687607,
      "loss": 0.3934,
      "step": 709
    },
    {
      "epoch": 0.027526047976738553,
      "grad_norm": 0.14982229471206665,
      "learning_rate": 0.00019453234062354584,
      "loss": 0.3288,
      "step": 710
    },
    {
      "epoch": 0.02756481705839593,
      "grad_norm": 0.16108989715576172,
      "learning_rate": 0.00019452458507833104,
      "loss": 0.3632,
      "step": 711
    },
    {
      "epoch": 0.02760358614005331,
      "grad_norm": 0.1427665650844574,
      "learning_rate": 0.00019451682953311618,
      "loss": 0.3213,
      "step": 712
    },
    {
      "epoch": 0.027642355221710685,
      "grad_norm": 0.16945527493953705,
      "learning_rate": 0.00019450907398790138,
      "loss": 0.4473,
      "step": 713
    },
    {
      "epoch": 0.027681124303368065,
      "grad_norm": 0.13440439105033875,
      "learning_rate": 0.00019450131844268653,
      "loss": 0.3259,
      "step": 714
    },
    {
      "epoch": 0.02771989338502544,
      "grad_norm": 0.1424192637205124,
      "learning_rate": 0.0001944935628974717,
      "loss": 0.3594,
      "step": 715
    },
    {
      "epoch": 0.02775866246668282,
      "grad_norm": 0.1256193071603775,
      "learning_rate": 0.00019448580735225687,
      "loss": 0.3969,
      "step": 716
    },
    {
      "epoch": 0.027797431548340197,
      "grad_norm": 0.14561088383197784,
      "learning_rate": 0.00019447805180704205,
      "loss": 0.2955,
      "step": 717
    },
    {
      "epoch": 0.027836200629997577,
      "grad_norm": 0.1556336134672165,
      "learning_rate": 0.00019447029626182722,
      "loss": 0.3335,
      "step": 718
    },
    {
      "epoch": 0.027874969711654957,
      "grad_norm": 0.16120177507400513,
      "learning_rate": 0.0001944625407166124,
      "loss": 0.3851,
      "step": 719
    },
    {
      "epoch": 0.027913738793312333,
      "grad_norm": 0.1111195757985115,
      "learning_rate": 0.00019445478517139756,
      "loss": 0.259,
      "step": 720
    },
    {
      "epoch": 0.027952507874969713,
      "grad_norm": 0.1587584763765335,
      "learning_rate": 0.00019444702962618274,
      "loss": 0.3276,
      "step": 721
    },
    {
      "epoch": 0.02799127695662709,
      "grad_norm": 0.1578490138053894,
      "learning_rate": 0.00019443927408096788,
      "loss": 0.3656,
      "step": 722
    },
    {
      "epoch": 0.02803004603828447,
      "grad_norm": 0.13586750626564026,
      "learning_rate": 0.00019443151853575308,
      "loss": 0.3499,
      "step": 723
    },
    {
      "epoch": 0.028068815119941845,
      "grad_norm": 0.11539575457572937,
      "learning_rate": 0.00019442376299053823,
      "loss": 0.2859,
      "step": 724
    },
    {
      "epoch": 0.028107584201599225,
      "grad_norm": 0.18767140805721283,
      "learning_rate": 0.00019441600744532343,
      "loss": 0.4262,
      "step": 725
    },
    {
      "epoch": 0.0281463532832566,
      "grad_norm": 0.16470061242580414,
      "learning_rate": 0.00019440825190010857,
      "loss": 0.3957,
      "step": 726
    },
    {
      "epoch": 0.02818512236491398,
      "grad_norm": 0.15745604038238525,
      "learning_rate": 0.00019440049635489377,
      "loss": 0.4233,
      "step": 727
    },
    {
      "epoch": 0.02822389144657136,
      "grad_norm": 0.1588202863931656,
      "learning_rate": 0.00019439274080967892,
      "loss": 0.3959,
      "step": 728
    },
    {
      "epoch": 0.028262660528228737,
      "grad_norm": 0.15335285663604736,
      "learning_rate": 0.0001943849852644641,
      "loss": 0.2596,
      "step": 729
    },
    {
      "epoch": 0.028301429609886117,
      "grad_norm": 0.12144558876752853,
      "learning_rate": 0.00019437722971924926,
      "loss": 0.3145,
      "step": 730
    },
    {
      "epoch": 0.028340198691543493,
      "grad_norm": 0.14693588018417358,
      "learning_rate": 0.00019436947417403444,
      "loss": 0.3763,
      "step": 731
    },
    {
      "epoch": 0.028378967773200873,
      "grad_norm": 0.10730589926242828,
      "learning_rate": 0.0001943617186288196,
      "loss": 0.2696,
      "step": 732
    },
    {
      "epoch": 0.02841773685485825,
      "grad_norm": 0.14851371943950653,
      "learning_rate": 0.00019435396308360478,
      "loss": 0.3837,
      "step": 733
    },
    {
      "epoch": 0.02845650593651563,
      "grad_norm": 0.11991468816995621,
      "learning_rate": 0.00019434620753838996,
      "loss": 0.2821,
      "step": 734
    },
    {
      "epoch": 0.02849527501817301,
      "grad_norm": 0.15144699811935425,
      "learning_rate": 0.00019433845199317513,
      "loss": 0.3985,
      "step": 735
    },
    {
      "epoch": 0.028534044099830385,
      "grad_norm": 0.1369335651397705,
      "learning_rate": 0.0001943306964479603,
      "loss": 0.3161,
      "step": 736
    },
    {
      "epoch": 0.028572813181487765,
      "grad_norm": 0.1304178237915039,
      "learning_rate": 0.00019432294090274547,
      "loss": 0.3789,
      "step": 737
    },
    {
      "epoch": 0.02861158226314514,
      "grad_norm": 0.16952620446681976,
      "learning_rate": 0.00019431518535753065,
      "loss": 0.3692,
      "step": 738
    },
    {
      "epoch": 0.02865035134480252,
      "grad_norm": 0.15759386122226715,
      "learning_rate": 0.00019430742981231582,
      "loss": 0.343,
      "step": 739
    },
    {
      "epoch": 0.028689120426459897,
      "grad_norm": 0.14076033234596252,
      "learning_rate": 0.000194299674267101,
      "loss": 0.3017,
      "step": 740
    },
    {
      "epoch": 0.028727889508117277,
      "grad_norm": 0.13666193187236786,
      "learning_rate": 0.00019429191872188617,
      "loss": 0.3402,
      "step": 741
    },
    {
      "epoch": 0.028766658589774653,
      "grad_norm": 0.13885383307933807,
      "learning_rate": 0.00019428416317667134,
      "loss": 0.2926,
      "step": 742
    },
    {
      "epoch": 0.028805427671432033,
      "grad_norm": 0.13617195188999176,
      "learning_rate": 0.00019427640763145648,
      "loss": 0.3489,
      "step": 743
    },
    {
      "epoch": 0.028844196753089413,
      "grad_norm": 0.12854012846946716,
      "learning_rate": 0.00019426865208624168,
      "loss": 0.2953,
      "step": 744
    },
    {
      "epoch": 0.02888296583474679,
      "grad_norm": 0.13885194063186646,
      "learning_rate": 0.00019426089654102683,
      "loss": 0.3189,
      "step": 745
    },
    {
      "epoch": 0.02892173491640417,
      "grad_norm": 0.15223351120948792,
      "learning_rate": 0.00019425314099581203,
      "loss": 0.3747,
      "step": 746
    },
    {
      "epoch": 0.028960503998061545,
      "grad_norm": 0.11393503099679947,
      "learning_rate": 0.00019424538545059718,
      "loss": 0.2488,
      "step": 747
    },
    {
      "epoch": 0.028999273079718925,
      "grad_norm": 0.09188877046108246,
      "learning_rate": 0.00019423762990538237,
      "loss": 0.2483,
      "step": 748
    },
    {
      "epoch": 0.0290380421613763,
      "grad_norm": 0.15332770347595215,
      "learning_rate": 0.00019422987436016752,
      "loss": 0.2972,
      "step": 749
    },
    {
      "epoch": 0.02907681124303368,
      "grad_norm": 0.13914336264133453,
      "learning_rate": 0.0001942221188149527,
      "loss": 0.3029,
      "step": 750
    },
    {
      "epoch": 0.029115580324691057,
      "grad_norm": 0.13293831050395966,
      "learning_rate": 0.00019421436326973787,
      "loss": 0.3473,
      "step": 751
    },
    {
      "epoch": 0.029154349406348437,
      "grad_norm": 0.14073552191257477,
      "learning_rate": 0.00019420660772452304,
      "loss": 0.242,
      "step": 752
    },
    {
      "epoch": 0.029193118488005817,
      "grad_norm": 0.15943776071071625,
      "learning_rate": 0.0001941988521793082,
      "loss": 0.3217,
      "step": 753
    },
    {
      "epoch": 0.029231887569663193,
      "grad_norm": 0.14959822595119476,
      "learning_rate": 0.00019419109663409338,
      "loss": 0.4036,
      "step": 754
    },
    {
      "epoch": 0.029270656651320573,
      "grad_norm": 0.15571707487106323,
      "learning_rate": 0.00019418334108887856,
      "loss": 0.3371,
      "step": 755
    },
    {
      "epoch": 0.02930942573297795,
      "grad_norm": 0.12794242799282074,
      "learning_rate": 0.00019417558554366373,
      "loss": 0.2784,
      "step": 756
    },
    {
      "epoch": 0.02934819481463533,
      "grad_norm": 0.1406203657388687,
      "learning_rate": 0.0001941678299984489,
      "loss": 0.2766,
      "step": 757
    },
    {
      "epoch": 0.029386963896292705,
      "grad_norm": 0.14880146086215973,
      "learning_rate": 0.00019416007445323408,
      "loss": 0.3556,
      "step": 758
    },
    {
      "epoch": 0.029425732977950085,
      "grad_norm": 0.1430426836013794,
      "learning_rate": 0.00019415231890801925,
      "loss": 0.3495,
      "step": 759
    },
    {
      "epoch": 0.02946450205960746,
      "grad_norm": 0.12985996901988983,
      "learning_rate": 0.00019414456336280442,
      "loss": 0.3485,
      "step": 760
    },
    {
      "epoch": 0.02950327114126484,
      "grad_norm": 0.1447828710079193,
      "learning_rate": 0.0001941368078175896,
      "loss": 0.3512,
      "step": 761
    },
    {
      "epoch": 0.02954204022292222,
      "grad_norm": 0.1409195214509964,
      "learning_rate": 0.00019412905227237477,
      "loss": 0.3189,
      "step": 762
    },
    {
      "epoch": 0.029580809304579597,
      "grad_norm": 0.11343153566122055,
      "learning_rate": 0.00019412129672715994,
      "loss": 0.3164,
      "step": 763
    },
    {
      "epoch": 0.029619578386236977,
      "grad_norm": 0.12111203372478485,
      "learning_rate": 0.00019411354118194509,
      "loss": 0.3126,
      "step": 764
    },
    {
      "epoch": 0.029658347467894353,
      "grad_norm": 0.13961444795131683,
      "learning_rate": 0.00019410578563673029,
      "loss": 0.2991,
      "step": 765
    },
    {
      "epoch": 0.029697116549551733,
      "grad_norm": 0.14520049095153809,
      "learning_rate": 0.00019409803009151543,
      "loss": 0.3512,
      "step": 766
    },
    {
      "epoch": 0.02973588563120911,
      "grad_norm": 0.11342813819646835,
      "learning_rate": 0.00019409027454630063,
      "loss": 0.2725,
      "step": 767
    },
    {
      "epoch": 0.02977465471286649,
      "grad_norm": 0.1532905101776123,
      "learning_rate": 0.00019408251900108578,
      "loss": 0.3638,
      "step": 768
    },
    {
      "epoch": 0.02981342379452387,
      "grad_norm": 0.13135695457458496,
      "learning_rate": 0.00019407476345587098,
      "loss": 0.32,
      "step": 769
    },
    {
      "epoch": 0.029852192876181245,
      "grad_norm": 0.1524440497159958,
      "learning_rate": 0.00019406700791065612,
      "loss": 0.3192,
      "step": 770
    },
    {
      "epoch": 0.029890961957838625,
      "grad_norm": 0.14860109984874725,
      "learning_rate": 0.0001940592523654413,
      "loss": 0.398,
      "step": 771
    },
    {
      "epoch": 0.029929731039496,
      "grad_norm": 0.1722787767648697,
      "learning_rate": 0.00019405149682022647,
      "loss": 0.3966,
      "step": 772
    },
    {
      "epoch": 0.02996850012115338,
      "grad_norm": 0.1535925716161728,
      "learning_rate": 0.00019404374127501164,
      "loss": 0.3416,
      "step": 773
    },
    {
      "epoch": 0.030007269202810757,
      "grad_norm": 0.13479819893836975,
      "learning_rate": 0.00019403598572979681,
      "loss": 0.2852,
      "step": 774
    },
    {
      "epoch": 0.030046038284468137,
      "grad_norm": 0.14476878941059113,
      "learning_rate": 0.000194028230184582,
      "loss": 0.3234,
      "step": 775
    },
    {
      "epoch": 0.030084807366125513,
      "grad_norm": 0.15290749073028564,
      "learning_rate": 0.00019402047463936716,
      "loss": 0.4112,
      "step": 776
    },
    {
      "epoch": 0.030123576447782893,
      "grad_norm": 0.16335515677928925,
      "learning_rate": 0.00019401271909415233,
      "loss": 0.4145,
      "step": 777
    },
    {
      "epoch": 0.030162345529440273,
      "grad_norm": 0.1364612728357315,
      "learning_rate": 0.00019400496354893748,
      "loss": 0.3406,
      "step": 778
    },
    {
      "epoch": 0.03020111461109765,
      "grad_norm": 0.14381858706474304,
      "learning_rate": 0.00019399720800372268,
      "loss": 0.374,
      "step": 779
    },
    {
      "epoch": 0.03023988369275503,
      "grad_norm": 0.12395752966403961,
      "learning_rate": 0.00019398945245850782,
      "loss": 0.383,
      "step": 780
    },
    {
      "epoch": 0.030278652774412405,
      "grad_norm": 0.16195173561573029,
      "learning_rate": 0.00019398169691329302,
      "loss": 0.4168,
      "step": 781
    },
    {
      "epoch": 0.030317421856069785,
      "grad_norm": 0.16055835783481598,
      "learning_rate": 0.00019397394136807817,
      "loss": 0.3961,
      "step": 782
    },
    {
      "epoch": 0.03035619093772716,
      "grad_norm": 0.13111206889152527,
      "learning_rate": 0.00019396618582286337,
      "loss": 0.3473,
      "step": 783
    },
    {
      "epoch": 0.03039496001938454,
      "grad_norm": 0.10464950650930405,
      "learning_rate": 0.00019395843027764854,
      "loss": 0.2896,
      "step": 784
    },
    {
      "epoch": 0.030433729101041918,
      "grad_norm": 0.1111006960272789,
      "learning_rate": 0.0001939506747324337,
      "loss": 0.304,
      "step": 785
    },
    {
      "epoch": 0.030472498182699297,
      "grad_norm": 0.13271547853946686,
      "learning_rate": 0.0001939429191872189,
      "loss": 0.3707,
      "step": 786
    },
    {
      "epoch": 0.030511267264356677,
      "grad_norm": 0.1170705109834671,
      "learning_rate": 0.00019393516364200403,
      "loss": 0.2838,
      "step": 787
    },
    {
      "epoch": 0.030550036346014053,
      "grad_norm": 0.13570144772529602,
      "learning_rate": 0.00019392740809678923,
      "loss": 0.3984,
      "step": 788
    },
    {
      "epoch": 0.030588805427671433,
      "grad_norm": 0.1302831470966339,
      "learning_rate": 0.00019391965255157438,
      "loss": 0.3612,
      "step": 789
    },
    {
      "epoch": 0.03062757450932881,
      "grad_norm": 0.12234552949666977,
      "learning_rate": 0.00019391189700635958,
      "loss": 0.3311,
      "step": 790
    },
    {
      "epoch": 0.03066634359098619,
      "grad_norm": 0.1160445436835289,
      "learning_rate": 0.00019390414146114472,
      "loss": 0.2627,
      "step": 791
    },
    {
      "epoch": 0.030705112672643566,
      "grad_norm": 0.13718800246715546,
      "learning_rate": 0.0001938963859159299,
      "loss": 0.4019,
      "step": 792
    },
    {
      "epoch": 0.030743881754300945,
      "grad_norm": 0.10986029356718063,
      "learning_rate": 0.00019388863037071507,
      "loss": 0.2954,
      "step": 793
    },
    {
      "epoch": 0.03078265083595832,
      "grad_norm": 0.11896625906229019,
      "learning_rate": 0.00019388087482550024,
      "loss": 0.3163,
      "step": 794
    },
    {
      "epoch": 0.0308214199176157,
      "grad_norm": 0.1533733755350113,
      "learning_rate": 0.00019387311928028542,
      "loss": 0.3999,
      "step": 795
    },
    {
      "epoch": 0.03086018899927308,
      "grad_norm": 0.15066683292388916,
      "learning_rate": 0.0001938653637350706,
      "loss": 0.3511,
      "step": 796
    },
    {
      "epoch": 0.030898958080930457,
      "grad_norm": 0.11850325763225555,
      "learning_rate": 0.00019385760818985576,
      "loss": 0.2715,
      "step": 797
    },
    {
      "epoch": 0.030937727162587837,
      "grad_norm": 0.1499381810426712,
      "learning_rate": 0.00019384985264464093,
      "loss": 0.4427,
      "step": 798
    },
    {
      "epoch": 0.030976496244245214,
      "grad_norm": 0.13061849772930145,
      "learning_rate": 0.00019384209709942608,
      "loss": 0.3469,
      "step": 799
    },
    {
      "epoch": 0.031015265325902593,
      "grad_norm": 0.1737978458404541,
      "learning_rate": 0.00019383434155421128,
      "loss": 0.415,
      "step": 800
    },
    {
      "epoch": 0.03105403440755997,
      "grad_norm": 0.1525367647409439,
      "learning_rate": 0.00019382658600899643,
      "loss": 0.3333,
      "step": 801
    },
    {
      "epoch": 0.03109280348921735,
      "grad_norm": 0.1475008726119995,
      "learning_rate": 0.00019381883046378163,
      "loss": 0.3796,
      "step": 802
    },
    {
      "epoch": 0.031131572570874726,
      "grad_norm": 0.12405146658420563,
      "learning_rate": 0.00019381107491856677,
      "loss": 0.2696,
      "step": 803
    },
    {
      "epoch": 0.031170341652532105,
      "grad_norm": 0.15872743725776672,
      "learning_rate": 0.00019380331937335197,
      "loss": 0.4452,
      "step": 804
    },
    {
      "epoch": 0.031209110734189485,
      "grad_norm": 0.1332738995552063,
      "learning_rate": 0.00019379556382813712,
      "loss": 0.3619,
      "step": 805
    },
    {
      "epoch": 0.03124787981584686,
      "grad_norm": 0.14465591311454773,
      "learning_rate": 0.0001937878082829223,
      "loss": 0.2951,
      "step": 806
    },
    {
      "epoch": 0.03128664889750424,
      "grad_norm": 0.12897107005119324,
      "learning_rate": 0.00019378005273770746,
      "loss": 0.3164,
      "step": 807
    },
    {
      "epoch": 0.03132541797916162,
      "grad_norm": 0.15160737931728363,
      "learning_rate": 0.00019377229719249263,
      "loss": 0.3481,
      "step": 808
    },
    {
      "epoch": 0.031364187060819,
      "grad_norm": 0.1570264995098114,
      "learning_rate": 0.0001937645416472778,
      "loss": 0.3958,
      "step": 809
    },
    {
      "epoch": 0.031402956142476374,
      "grad_norm": 0.14227689802646637,
      "learning_rate": 0.00019375678610206298,
      "loss": 0.2821,
      "step": 810
    },
    {
      "epoch": 0.03144172522413375,
      "grad_norm": 0.13741613924503326,
      "learning_rate": 0.00019374903055684815,
      "loss": 0.3303,
      "step": 811
    },
    {
      "epoch": 0.03148049430579113,
      "grad_norm": 0.10923609137535095,
      "learning_rate": 0.00019374127501163333,
      "loss": 0.2689,
      "step": 812
    },
    {
      "epoch": 0.03151926338744851,
      "grad_norm": 0.1430056393146515,
      "learning_rate": 0.0001937335194664185,
      "loss": 0.3411,
      "step": 813
    },
    {
      "epoch": 0.031558032469105886,
      "grad_norm": 0.18997591733932495,
      "learning_rate": 0.00019372576392120367,
      "loss": 0.3321,
      "step": 814
    },
    {
      "epoch": 0.03159680155076327,
      "grad_norm": 0.13114984333515167,
      "learning_rate": 0.00019371800837598884,
      "loss": 0.3053,
      "step": 815
    },
    {
      "epoch": 0.031635570632420645,
      "grad_norm": 0.11083045601844788,
      "learning_rate": 0.00019371025283077402,
      "loss": 0.212,
      "step": 816
    },
    {
      "epoch": 0.03167433971407802,
      "grad_norm": 0.14100335538387299,
      "learning_rate": 0.0001937024972855592,
      "loss": 0.3409,
      "step": 817
    },
    {
      "epoch": 0.0317131087957354,
      "grad_norm": 0.1491110920906067,
      "learning_rate": 0.00019369474174034436,
      "loss": 0.3385,
      "step": 818
    },
    {
      "epoch": 0.03175187787739278,
      "grad_norm": 0.12198185175657272,
      "learning_rate": 0.00019368698619512954,
      "loss": 0.3156,
      "step": 819
    },
    {
      "epoch": 0.03179064695905016,
      "grad_norm": 0.15058554708957672,
      "learning_rate": 0.00019367923064991468,
      "loss": 0.3604,
      "step": 820
    },
    {
      "epoch": 0.031829416040707534,
      "grad_norm": 0.12448665499687195,
      "learning_rate": 0.00019367147510469988,
      "loss": 0.3531,
      "step": 821
    },
    {
      "epoch": 0.03186818512236492,
      "grad_norm": 0.1388682872056961,
      "learning_rate": 0.00019366371955948503,
      "loss": 0.351,
      "step": 822
    },
    {
      "epoch": 0.03190695420402229,
      "grad_norm": 0.14601382613182068,
      "learning_rate": 0.00019365596401427023,
      "loss": 0.3534,
      "step": 823
    },
    {
      "epoch": 0.03194572328567967,
      "grad_norm": 0.1706010401248932,
      "learning_rate": 0.00019364820846905537,
      "loss": 0.3229,
      "step": 824
    },
    {
      "epoch": 0.031984492367337046,
      "grad_norm": 0.14511820673942566,
      "learning_rate": 0.00019364045292384057,
      "loss": 0.3218,
      "step": 825
    },
    {
      "epoch": 0.03202326144899443,
      "grad_norm": 0.12673726677894592,
      "learning_rate": 0.00019363269737862572,
      "loss": 0.3151,
      "step": 826
    },
    {
      "epoch": 0.032062030530651806,
      "grad_norm": 0.14350451529026031,
      "learning_rate": 0.0001936249418334109,
      "loss": 0.3332,
      "step": 827
    },
    {
      "epoch": 0.03210079961230918,
      "grad_norm": 0.145944282412529,
      "learning_rate": 0.00019361718628819606,
      "loss": 0.2886,
      "step": 828
    },
    {
      "epoch": 0.032139568693966565,
      "grad_norm": 0.14873869717121124,
      "learning_rate": 0.00019360943074298124,
      "loss": 0.363,
      "step": 829
    },
    {
      "epoch": 0.03217833777562394,
      "grad_norm": 0.13960890471935272,
      "learning_rate": 0.0001936016751977664,
      "loss": 0.3264,
      "step": 830
    },
    {
      "epoch": 0.03221710685728132,
      "grad_norm": 0.153490349650383,
      "learning_rate": 0.00019359391965255158,
      "loss": 0.397,
      "step": 831
    },
    {
      "epoch": 0.032255875938938694,
      "grad_norm": 0.13975480198860168,
      "learning_rate": 0.00019358616410733676,
      "loss": 0.4115,
      "step": 832
    },
    {
      "epoch": 0.03229464502059608,
      "grad_norm": 0.14018258452415466,
      "learning_rate": 0.00019357840856212193,
      "loss": 0.2688,
      "step": 833
    },
    {
      "epoch": 0.032333414102253454,
      "grad_norm": 0.12935326993465424,
      "learning_rate": 0.0001935706530169071,
      "loss": 0.2882,
      "step": 834
    },
    {
      "epoch": 0.03237218318391083,
      "grad_norm": 0.1376933753490448,
      "learning_rate": 0.00019356289747169227,
      "loss": 0.3446,
      "step": 835
    },
    {
      "epoch": 0.032410952265568206,
      "grad_norm": 0.12239096313714981,
      "learning_rate": 0.00019355514192647745,
      "loss": 0.244,
      "step": 836
    },
    {
      "epoch": 0.03244972134722559,
      "grad_norm": 0.16403579711914062,
      "learning_rate": 0.00019354738638126262,
      "loss": 0.385,
      "step": 837
    },
    {
      "epoch": 0.032488490428882966,
      "grad_norm": 0.1525702327489853,
      "learning_rate": 0.0001935396308360478,
      "loss": 0.3593,
      "step": 838
    },
    {
      "epoch": 0.03252725951054034,
      "grad_norm": 0.1646185964345932,
      "learning_rate": 0.00019353187529083296,
      "loss": 0.2968,
      "step": 839
    },
    {
      "epoch": 0.032566028592197725,
      "grad_norm": 0.1332615464925766,
      "learning_rate": 0.00019352411974561814,
      "loss": 0.2553,
      "step": 840
    },
    {
      "epoch": 0.0326047976738551,
      "grad_norm": 0.15526556968688965,
      "learning_rate": 0.00019351636420040328,
      "loss": 0.3502,
      "step": 841
    },
    {
      "epoch": 0.03264356675551248,
      "grad_norm": 0.16160443425178528,
      "learning_rate": 0.00019350860865518848,
      "loss": 0.4514,
      "step": 842
    },
    {
      "epoch": 0.032682335837169854,
      "grad_norm": 0.1561393141746521,
      "learning_rate": 0.00019350085310997363,
      "loss": 0.3825,
      "step": 843
    },
    {
      "epoch": 0.03272110491882724,
      "grad_norm": 0.17530420422554016,
      "learning_rate": 0.00019349309756475883,
      "loss": 0.351,
      "step": 844
    },
    {
      "epoch": 0.032759874000484614,
      "grad_norm": 0.15632277727127075,
      "learning_rate": 0.00019348534201954397,
      "loss": 0.3882,
      "step": 845
    },
    {
      "epoch": 0.03279864308214199,
      "grad_norm": 0.139304056763649,
      "learning_rate": 0.00019347758647432917,
      "loss": 0.2989,
      "step": 846
    },
    {
      "epoch": 0.03283741216379937,
      "grad_norm": 0.15528415143489838,
      "learning_rate": 0.00019346983092911432,
      "loss": 0.3923,
      "step": 847
    },
    {
      "epoch": 0.03287618124545675,
      "grad_norm": 0.1323522925376892,
      "learning_rate": 0.0001934620753838995,
      "loss": 0.3164,
      "step": 848
    },
    {
      "epoch": 0.032914950327114126,
      "grad_norm": 0.23862828314304352,
      "learning_rate": 0.00019345431983868467,
      "loss": 0.3775,
      "step": 849
    },
    {
      "epoch": 0.0329537194087715,
      "grad_norm": 0.15224497020244598,
      "learning_rate": 0.00019344656429346984,
      "loss": 0.3438,
      "step": 850
    },
    {
      "epoch": 0.032992488490428885,
      "grad_norm": 0.16642503440380096,
      "learning_rate": 0.000193438808748255,
      "loss": 0.407,
      "step": 851
    },
    {
      "epoch": 0.03303125757208626,
      "grad_norm": 0.17703013122081757,
      "learning_rate": 0.00019343105320304018,
      "loss": 0.3064,
      "step": 852
    },
    {
      "epoch": 0.03307002665374364,
      "grad_norm": 0.14384225010871887,
      "learning_rate": 0.00019342329765782536,
      "loss": 0.3835,
      "step": 853
    },
    {
      "epoch": 0.033108795735401014,
      "grad_norm": 0.14357881247997284,
      "learning_rate": 0.00019341554211261053,
      "loss": 0.3272,
      "step": 854
    },
    {
      "epoch": 0.0331475648170584,
      "grad_norm": 0.17309506237506866,
      "learning_rate": 0.00019340778656739568,
      "loss": 0.3695,
      "step": 855
    },
    {
      "epoch": 0.033186333898715774,
      "grad_norm": 0.1707196980714798,
      "learning_rate": 0.00019340003102218088,
      "loss": 0.3957,
      "step": 856
    },
    {
      "epoch": 0.03322510298037315,
      "grad_norm": 0.1601928472518921,
      "learning_rate": 0.00019339227547696602,
      "loss": 0.3924,
      "step": 857
    },
    {
      "epoch": 0.03326387206203053,
      "grad_norm": 0.11283578723669052,
      "learning_rate": 0.00019338451993175122,
      "loss": 0.2254,
      "step": 858
    },
    {
      "epoch": 0.03330264114368791,
      "grad_norm": 0.1654360443353653,
      "learning_rate": 0.00019337676438653637,
      "loss": 0.3826,
      "step": 859
    },
    {
      "epoch": 0.033341410225345286,
      "grad_norm": 0.16355270147323608,
      "learning_rate": 0.00019336900884132157,
      "loss": 0.4071,
      "step": 860
    },
    {
      "epoch": 0.03338017930700266,
      "grad_norm": 0.1664850115776062,
      "learning_rate": 0.0001933612532961067,
      "loss": 0.393,
      "step": 861
    },
    {
      "epoch": 0.033418948388660046,
      "grad_norm": 0.12651577591896057,
      "learning_rate": 0.00019335349775089189,
      "loss": 0.2597,
      "step": 862
    },
    {
      "epoch": 0.03345771747031742,
      "grad_norm": 0.1808202564716339,
      "learning_rate": 0.00019334574220567709,
      "loss": 0.519,
      "step": 863
    },
    {
      "epoch": 0.0334964865519748,
      "grad_norm": 0.13950994610786438,
      "learning_rate": 0.00019333798666046223,
      "loss": 0.3099,
      "step": 864
    },
    {
      "epoch": 0.03353525563363218,
      "grad_norm": 0.1583147495985031,
      "learning_rate": 0.00019333023111524743,
      "loss": 0.4591,
      "step": 865
    },
    {
      "epoch": 0.03357402471528956,
      "grad_norm": 0.12358883768320084,
      "learning_rate": 0.00019332247557003258,
      "loss": 0.326,
      "step": 866
    },
    {
      "epoch": 0.033612793796946934,
      "grad_norm": 0.17066402733325958,
      "learning_rate": 0.00019331472002481778,
      "loss": 0.4482,
      "step": 867
    },
    {
      "epoch": 0.03365156287860431,
      "grad_norm": 0.17997927963733673,
      "learning_rate": 0.00019330696447960292,
      "loss": 0.4558,
      "step": 868
    },
    {
      "epoch": 0.033690331960261694,
      "grad_norm": 0.15257245302200317,
      "learning_rate": 0.0001932992089343881,
      "loss": 0.307,
      "step": 869
    },
    {
      "epoch": 0.03372910104191907,
      "grad_norm": 0.12448501586914062,
      "learning_rate": 0.00019329145338917327,
      "loss": 0.3506,
      "step": 870
    },
    {
      "epoch": 0.033767870123576446,
      "grad_norm": 0.1929914653301239,
      "learning_rate": 0.00019328369784395844,
      "loss": 0.3076,
      "step": 871
    },
    {
      "epoch": 0.03380663920523383,
      "grad_norm": 0.1644866019487381,
      "learning_rate": 0.0001932759422987436,
      "loss": 0.345,
      "step": 872
    },
    {
      "epoch": 0.033845408286891206,
      "grad_norm": 0.12283150851726532,
      "learning_rate": 0.00019326818675352879,
      "loss": 0.2692,
      "step": 873
    },
    {
      "epoch": 0.03388417736854858,
      "grad_norm": 0.14834171533584595,
      "learning_rate": 0.00019326043120831396,
      "loss": 0.3516,
      "step": 874
    },
    {
      "epoch": 0.03392294645020596,
      "grad_norm": 0.1640624701976776,
      "learning_rate": 0.00019325267566309913,
      "loss": 0.3654,
      "step": 875
    },
    {
      "epoch": 0.03396171553186334,
      "grad_norm": 0.1411774605512619,
      "learning_rate": 0.00019324492011788428,
      "loss": 0.3207,
      "step": 876
    },
    {
      "epoch": 0.03400048461352072,
      "grad_norm": 0.17533232271671295,
      "learning_rate": 0.00019323716457266948,
      "loss": 0.3911,
      "step": 877
    },
    {
      "epoch": 0.034039253695178094,
      "grad_norm": 0.1914886236190796,
      "learning_rate": 0.00019322940902745462,
      "loss": 0.4059,
      "step": 878
    },
    {
      "epoch": 0.03407802277683547,
      "grad_norm": 0.17702162265777588,
      "learning_rate": 0.00019322165348223982,
      "loss": 0.3812,
      "step": 879
    },
    {
      "epoch": 0.034116791858492854,
      "grad_norm": 0.16096484661102295,
      "learning_rate": 0.00019321389793702497,
      "loss": 0.35,
      "step": 880
    },
    {
      "epoch": 0.03415556094015023,
      "grad_norm": 0.17081744968891144,
      "learning_rate": 0.00019320614239181017,
      "loss": 0.3423,
      "step": 881
    },
    {
      "epoch": 0.034194330021807606,
      "grad_norm": 0.17524804174900055,
      "learning_rate": 0.00019319838684659531,
      "loss": 0.3916,
      "step": 882
    },
    {
      "epoch": 0.03423309910346499,
      "grad_norm": 0.13314391672611237,
      "learning_rate": 0.0001931906313013805,
      "loss": 0.3181,
      "step": 883
    },
    {
      "epoch": 0.034271868185122366,
      "grad_norm": 0.1459713578224182,
      "learning_rate": 0.00019318287575616566,
      "loss": 0.2922,
      "step": 884
    },
    {
      "epoch": 0.03431063726677974,
      "grad_norm": 0.13754677772521973,
      "learning_rate": 0.00019317512021095083,
      "loss": 0.3234,
      "step": 885
    },
    {
      "epoch": 0.03434940634843712,
      "grad_norm": 0.10992977023124695,
      "learning_rate": 0.000193167364665736,
      "loss": 0.1899,
      "step": 886
    },
    {
      "epoch": 0.0343881754300945,
      "grad_norm": 0.16857585310935974,
      "learning_rate": 0.00019315960912052118,
      "loss": 0.3499,
      "step": 887
    },
    {
      "epoch": 0.03442694451175188,
      "grad_norm": 0.17438319325447083,
      "learning_rate": 0.00019315185357530635,
      "loss": 0.3766,
      "step": 888
    },
    {
      "epoch": 0.034465713593409254,
      "grad_norm": 0.1461031585931778,
      "learning_rate": 0.00019314409803009152,
      "loss": 0.3607,
      "step": 889
    },
    {
      "epoch": 0.03450448267506664,
      "grad_norm": 0.14572231471538544,
      "learning_rate": 0.0001931363424848767,
      "loss": 0.3618,
      "step": 890
    },
    {
      "epoch": 0.034543251756724014,
      "grad_norm": 0.16701506078243256,
      "learning_rate": 0.00019312858693966187,
      "loss": 0.3707,
      "step": 891
    },
    {
      "epoch": 0.03458202083838139,
      "grad_norm": 0.13021449744701385,
      "learning_rate": 0.00019312083139444704,
      "loss": 0.2644,
      "step": 892
    },
    {
      "epoch": 0.034620789920038766,
      "grad_norm": 0.14766669273376465,
      "learning_rate": 0.00019311307584923222,
      "loss": 0.3518,
      "step": 893
    },
    {
      "epoch": 0.03465955900169615,
      "grad_norm": 0.12665268778800964,
      "learning_rate": 0.0001931053203040174,
      "loss": 0.2572,
      "step": 894
    },
    {
      "epoch": 0.034698328083353526,
      "grad_norm": 0.14547976851463318,
      "learning_rate": 0.00019309756475880256,
      "loss": 0.3179,
      "step": 895
    },
    {
      "epoch": 0.0347370971650109,
      "grad_norm": 0.13895027339458466,
      "learning_rate": 0.00019308980921358773,
      "loss": 0.3013,
      "step": 896
    },
    {
      "epoch": 0.03477586624666828,
      "grad_norm": 0.14943788945674896,
      "learning_rate": 0.00019308205366837288,
      "loss": 0.3239,
      "step": 897
    },
    {
      "epoch": 0.03481463532832566,
      "grad_norm": 0.12250302731990814,
      "learning_rate": 0.00019307429812315808,
      "loss": 0.2519,
      "step": 898
    },
    {
      "epoch": 0.03485340440998304,
      "grad_norm": 0.14216984808444977,
      "learning_rate": 0.00019306654257794322,
      "loss": 0.2741,
      "step": 899
    },
    {
      "epoch": 0.034892173491640414,
      "grad_norm": 0.1566479653120041,
      "learning_rate": 0.00019305878703272842,
      "loss": 0.3355,
      "step": 900
    },
    {
      "epoch": 0.0349309425732978,
      "grad_norm": 0.1406208723783493,
      "learning_rate": 0.00019305103148751357,
      "loss": 0.2891,
      "step": 901
    },
    {
      "epoch": 0.034969711654955174,
      "grad_norm": 0.1729278266429901,
      "learning_rate": 0.00019304327594229877,
      "loss": 0.2664,
      "step": 902
    },
    {
      "epoch": 0.03500848073661255,
      "grad_norm": 0.13524429500102997,
      "learning_rate": 0.00019303552039708392,
      "loss": 0.3418,
      "step": 903
    },
    {
      "epoch": 0.03504724981826993,
      "grad_norm": 0.1537805199623108,
      "learning_rate": 0.0001930277648518691,
      "loss": 0.3819,
      "step": 904
    },
    {
      "epoch": 0.03508601889992731,
      "grad_norm": 0.15207625925540924,
      "learning_rate": 0.00019302000930665426,
      "loss": 0.3544,
      "step": 905
    },
    {
      "epoch": 0.035124787981584686,
      "grad_norm": 0.13587911427021027,
      "learning_rate": 0.00019301225376143943,
      "loss": 0.2848,
      "step": 906
    },
    {
      "epoch": 0.03516355706324206,
      "grad_norm": 0.13870298862457275,
      "learning_rate": 0.0001930044982162246,
      "loss": 0.3286,
      "step": 907
    },
    {
      "epoch": 0.035202326144899446,
      "grad_norm": 0.13917893171310425,
      "learning_rate": 0.00019299674267100978,
      "loss": 0.3199,
      "step": 908
    },
    {
      "epoch": 0.03524109522655682,
      "grad_norm": 0.13581031560897827,
      "learning_rate": 0.00019298898712579495,
      "loss": 0.3136,
      "step": 909
    },
    {
      "epoch": 0.0352798643082142,
      "grad_norm": 0.17068599164485931,
      "learning_rate": 0.00019298123158058013,
      "loss": 0.3469,
      "step": 910
    },
    {
      "epoch": 0.035318633389871575,
      "grad_norm": 0.131046324968338,
      "learning_rate": 0.0001929734760353653,
      "loss": 0.3248,
      "step": 911
    },
    {
      "epoch": 0.03535740247152896,
      "grad_norm": 0.1594708263874054,
      "learning_rate": 0.00019296572049015047,
      "loss": 0.3419,
      "step": 912
    },
    {
      "epoch": 0.035396171553186334,
      "grad_norm": 0.11170542240142822,
      "learning_rate": 0.00019295796494493564,
      "loss": 0.3038,
      "step": 913
    },
    {
      "epoch": 0.03543494063484371,
      "grad_norm": 0.23525109887123108,
      "learning_rate": 0.00019295020939972082,
      "loss": 0.3272,
      "step": 914
    },
    {
      "epoch": 0.035473709716501094,
      "grad_norm": 0.1287388652563095,
      "learning_rate": 0.000192942453854506,
      "loss": 0.3078,
      "step": 915
    },
    {
      "epoch": 0.03551247879815847,
      "grad_norm": 0.14810311794281006,
      "learning_rate": 0.00019293469830929116,
      "loss": 0.2912,
      "step": 916
    },
    {
      "epoch": 0.035551247879815846,
      "grad_norm": 0.14017188549041748,
      "learning_rate": 0.00019292694276407634,
      "loss": 0.3401,
      "step": 917
    },
    {
      "epoch": 0.03559001696147322,
      "grad_norm": 0.1867048740386963,
      "learning_rate": 0.00019291918721886148,
      "loss": 0.3678,
      "step": 918
    },
    {
      "epoch": 0.035628786043130606,
      "grad_norm": 0.1679278314113617,
      "learning_rate": 0.00019291143167364668,
      "loss": 0.369,
      "step": 919
    },
    {
      "epoch": 0.03566755512478798,
      "grad_norm": 0.17613014578819275,
      "learning_rate": 0.00019290367612843183,
      "loss": 0.4006,
      "step": 920
    },
    {
      "epoch": 0.03570632420644536,
      "grad_norm": 0.13778789341449738,
      "learning_rate": 0.00019289592058321703,
      "loss": 0.2786,
      "step": 921
    },
    {
      "epoch": 0.035745093288102735,
      "grad_norm": 0.16514448821544647,
      "learning_rate": 0.00019288816503800217,
      "loss": 0.3153,
      "step": 922
    },
    {
      "epoch": 0.03578386236976012,
      "grad_norm": 0.14452511072158813,
      "learning_rate": 0.00019288040949278737,
      "loss": 0.2672,
      "step": 923
    },
    {
      "epoch": 0.035822631451417494,
      "grad_norm": 0.17976520955562592,
      "learning_rate": 0.00019287265394757252,
      "loss": 0.3954,
      "step": 924
    },
    {
      "epoch": 0.03586140053307487,
      "grad_norm": 0.12289900332689285,
      "learning_rate": 0.0001928648984023577,
      "loss": 0.3437,
      "step": 925
    },
    {
      "epoch": 0.035900169614732254,
      "grad_norm": 0.1324232518672943,
      "learning_rate": 0.00019285714285714286,
      "loss": 0.3235,
      "step": 926
    },
    {
      "epoch": 0.03593893869638963,
      "grad_norm": 0.13548357784748077,
      "learning_rate": 0.00019284938731192804,
      "loss": 0.3373,
      "step": 927
    },
    {
      "epoch": 0.035977707778047006,
      "grad_norm": 0.16543996334075928,
      "learning_rate": 0.0001928416317667132,
      "loss": 0.4035,
      "step": 928
    },
    {
      "epoch": 0.03601647685970438,
      "grad_norm": 0.1430216282606125,
      "learning_rate": 0.00019283387622149838,
      "loss": 0.3517,
      "step": 929
    },
    {
      "epoch": 0.036055245941361766,
      "grad_norm": 0.13254888355731964,
      "learning_rate": 0.00019282612067628355,
      "loss": 0.3086,
      "step": 930
    },
    {
      "epoch": 0.03609401502301914,
      "grad_norm": 0.17714722454547882,
      "learning_rate": 0.00019281836513106873,
      "loss": 0.371,
      "step": 931
    },
    {
      "epoch": 0.03613278410467652,
      "grad_norm": 0.18527327477931976,
      "learning_rate": 0.00019281060958585387,
      "loss": 0.3776,
      "step": 932
    },
    {
      "epoch": 0.0361715531863339,
      "grad_norm": 0.1359756886959076,
      "learning_rate": 0.00019280285404063907,
      "loss": 0.3309,
      "step": 933
    },
    {
      "epoch": 0.03621032226799128,
      "grad_norm": 0.14166051149368286,
      "learning_rate": 0.00019279509849542422,
      "loss": 0.3855,
      "step": 934
    },
    {
      "epoch": 0.036249091349648654,
      "grad_norm": 0.14816197752952576,
      "learning_rate": 0.00019278734295020942,
      "loss": 0.3527,
      "step": 935
    },
    {
      "epoch": 0.03628786043130603,
      "grad_norm": 0.10959473997354507,
      "learning_rate": 0.00019277958740499456,
      "loss": 0.2404,
      "step": 936
    },
    {
      "epoch": 0.036326629512963414,
      "grad_norm": 0.1778821498155594,
      "learning_rate": 0.00019277183185977976,
      "loss": 0.429,
      "step": 937
    },
    {
      "epoch": 0.03636539859462079,
      "grad_norm": 0.1441332846879959,
      "learning_rate": 0.0001927640763145649,
      "loss": 0.373,
      "step": 938
    },
    {
      "epoch": 0.03640416767627817,
      "grad_norm": 0.12437808513641357,
      "learning_rate": 0.00019275632076935008,
      "loss": 0.3136,
      "step": 939
    },
    {
      "epoch": 0.03644293675793555,
      "grad_norm": 0.14220908284187317,
      "learning_rate": 0.00019274856522413526,
      "loss": 0.3309,
      "step": 940
    },
    {
      "epoch": 0.036481705839592926,
      "grad_norm": 0.11195690184831619,
      "learning_rate": 0.00019274080967892043,
      "loss": 0.2454,
      "step": 941
    },
    {
      "epoch": 0.0365204749212503,
      "grad_norm": 0.1323036253452301,
      "learning_rate": 0.00019273305413370563,
      "loss": 0.324,
      "step": 942
    },
    {
      "epoch": 0.03655924400290768,
      "grad_norm": 0.12876258790493011,
      "learning_rate": 0.00019272529858849077,
      "loss": 0.3405,
      "step": 943
    },
    {
      "epoch": 0.03659801308456506,
      "grad_norm": 0.12248507887125015,
      "learning_rate": 0.00019271754304327597,
      "loss": 0.2852,
      "step": 944
    },
    {
      "epoch": 0.03663678216622244,
      "grad_norm": 0.12737126648426056,
      "learning_rate": 0.00019270978749806112,
      "loss": 0.3295,
      "step": 945
    },
    {
      "epoch": 0.036675551247879815,
      "grad_norm": 0.12943904101848602,
      "learning_rate": 0.0001927020319528463,
      "loss": 0.2939,
      "step": 946
    },
    {
      "epoch": 0.03671432032953719,
      "grad_norm": 0.1617288440465927,
      "learning_rate": 0.00019269427640763147,
      "loss": 0.301,
      "step": 947
    },
    {
      "epoch": 0.036753089411194574,
      "grad_norm": 0.13073022663593292,
      "learning_rate": 0.00019268652086241664,
      "loss": 0.3164,
      "step": 948
    },
    {
      "epoch": 0.03679185849285195,
      "grad_norm": 0.1815042495727539,
      "learning_rate": 0.0001926787653172018,
      "loss": 0.322,
      "step": 949
    },
    {
      "epoch": 0.03683062757450933,
      "grad_norm": 0.17300401628017426,
      "learning_rate": 0.00019267100977198698,
      "loss": 0.3189,
      "step": 950
    },
    {
      "epoch": 0.03686939665616671,
      "grad_norm": 0.1860598921775818,
      "learning_rate": 0.00019266325422677216,
      "loss": 0.3835,
      "step": 951
    },
    {
      "epoch": 0.036908165737824086,
      "grad_norm": 0.16607725620269775,
      "learning_rate": 0.00019265549868155733,
      "loss": 0.3312,
      "step": 952
    },
    {
      "epoch": 0.03694693481948146,
      "grad_norm": 0.15772613883018494,
      "learning_rate": 0.00019264774313634248,
      "loss": 0.371,
      "step": 953
    },
    {
      "epoch": 0.03698570390113884,
      "grad_norm": 0.15743419528007507,
      "learning_rate": 0.00019263998759112767,
      "loss": 0.3115,
      "step": 954
    },
    {
      "epoch": 0.03702447298279622,
      "grad_norm": 0.15479891002178192,
      "learning_rate": 0.00019263223204591282,
      "loss": 0.3269,
      "step": 955
    },
    {
      "epoch": 0.0370632420644536,
      "grad_norm": 0.1377086043357849,
      "learning_rate": 0.00019262447650069802,
      "loss": 0.3185,
      "step": 956
    },
    {
      "epoch": 0.037102011146110975,
      "grad_norm": 0.17909018695354462,
      "learning_rate": 0.00019261672095548317,
      "loss": 0.3804,
      "step": 957
    },
    {
      "epoch": 0.03714078022776836,
      "grad_norm": 0.14542177319526672,
      "learning_rate": 0.00019260896541026837,
      "loss": 0.3088,
      "step": 958
    },
    {
      "epoch": 0.037179549309425734,
      "grad_norm": 0.15711043775081635,
      "learning_rate": 0.0001926012098650535,
      "loss": 0.3209,
      "step": 959
    },
    {
      "epoch": 0.03721831839108311,
      "grad_norm": 0.13193729519844055,
      "learning_rate": 0.00019259345431983868,
      "loss": 0.2707,
      "step": 960
    },
    {
      "epoch": 0.03725708747274049,
      "grad_norm": 0.281897634267807,
      "learning_rate": 0.00019258569877462386,
      "loss": 0.2839,
      "step": 961
    },
    {
      "epoch": 0.03729585655439787,
      "grad_norm": 0.16159451007843018,
      "learning_rate": 0.00019257794322940903,
      "loss": 0.4133,
      "step": 962
    },
    {
      "epoch": 0.037334625636055246,
      "grad_norm": 0.1601019650697708,
      "learning_rate": 0.0001925701876841942,
      "loss": 0.3695,
      "step": 963
    },
    {
      "epoch": 0.03737339471771262,
      "grad_norm": 0.23631414771080017,
      "learning_rate": 0.00019256243213897938,
      "loss": 0.3649,
      "step": 964
    },
    {
      "epoch": 0.03741216379937,
      "grad_norm": 0.1748684048652649,
      "learning_rate": 0.00019255467659376455,
      "loss": 0.3442,
      "step": 965
    },
    {
      "epoch": 0.03745093288102738,
      "grad_norm": 0.12431918829679489,
      "learning_rate": 0.00019254692104854972,
      "loss": 0.2348,
      "step": 966
    },
    {
      "epoch": 0.03748970196268476,
      "grad_norm": 0.1322316825389862,
      "learning_rate": 0.0001925391655033349,
      "loss": 0.2684,
      "step": 967
    },
    {
      "epoch": 0.037528471044342135,
      "grad_norm": 0.20313848555088043,
      "learning_rate": 0.00019253140995812007,
      "loss": 0.3953,
      "step": 968
    },
    {
      "epoch": 0.03756724012599952,
      "grad_norm": 0.10008793324232101,
      "learning_rate": 0.00019252365441290524,
      "loss": 0.2614,
      "step": 969
    },
    {
      "epoch": 0.037606009207656894,
      "grad_norm": 0.15135496854782104,
      "learning_rate": 0.0001925158988676904,
      "loss": 0.2916,
      "step": 970
    },
    {
      "epoch": 0.03764477828931427,
      "grad_norm": 0.11701161414384842,
      "learning_rate": 0.00019250814332247559,
      "loss": 0.2662,
      "step": 971
    },
    {
      "epoch": 0.03768354737097165,
      "grad_norm": 0.120465487241745,
      "learning_rate": 0.00019250038777726076,
      "loss": 0.2943,
      "step": 972
    },
    {
      "epoch": 0.03772231645262903,
      "grad_norm": 0.15339045226573944,
      "learning_rate": 0.00019249263223204593,
      "loss": 0.3757,
      "step": 973
    },
    {
      "epoch": 0.03776108553428641,
      "grad_norm": 0.1412520408630371,
      "learning_rate": 0.00019248487668683108,
      "loss": 0.3153,
      "step": 974
    },
    {
      "epoch": 0.03779985461594378,
      "grad_norm": 0.14359411597251892,
      "learning_rate": 0.00019247712114161628,
      "loss": 0.2987,
      "step": 975
    },
    {
      "epoch": 0.037838623697601166,
      "grad_norm": 0.26551762223243713,
      "learning_rate": 0.00019246936559640142,
      "loss": 0.3376,
      "step": 976
    },
    {
      "epoch": 0.03787739277925854,
      "grad_norm": 0.1527782529592514,
      "learning_rate": 0.00019246161005118662,
      "loss": 0.3518,
      "step": 977
    },
    {
      "epoch": 0.03791616186091592,
      "grad_norm": 0.19303084909915924,
      "learning_rate": 0.00019245385450597177,
      "loss": 0.3975,
      "step": 978
    },
    {
      "epoch": 0.037954930942573295,
      "grad_norm": 0.17645010352134705,
      "learning_rate": 0.00019244609896075697,
      "loss": 0.3538,
      "step": 979
    },
    {
      "epoch": 0.03799370002423068,
      "grad_norm": 0.15810085833072662,
      "learning_rate": 0.00019243834341554211,
      "loss": 0.3629,
      "step": 980
    },
    {
      "epoch": 0.038032469105888055,
      "grad_norm": 0.14370878040790558,
      "learning_rate": 0.0001924305878703273,
      "loss": 0.2722,
      "step": 981
    },
    {
      "epoch": 0.03807123818754543,
      "grad_norm": 0.2535851001739502,
      "learning_rate": 0.00019242283232511246,
      "loss": 0.3435,
      "step": 982
    },
    {
      "epoch": 0.038110007269202814,
      "grad_norm": 0.1965339183807373,
      "learning_rate": 0.00019241507677989763,
      "loss": 0.4184,
      "step": 983
    },
    {
      "epoch": 0.03814877635086019,
      "grad_norm": 0.14829392731189728,
      "learning_rate": 0.0001924073212346828,
      "loss": 0.2617,
      "step": 984
    },
    {
      "epoch": 0.03818754543251757,
      "grad_norm": 0.13344329595565796,
      "learning_rate": 0.00019239956568946798,
      "loss": 0.3316,
      "step": 985
    },
    {
      "epoch": 0.03822631451417494,
      "grad_norm": 0.15886908769607544,
      "learning_rate": 0.00019239181014425315,
      "loss": 0.3516,
      "step": 986
    },
    {
      "epoch": 0.038265083595832326,
      "grad_norm": 0.13207028806209564,
      "learning_rate": 0.00019238405459903832,
      "loss": 0.3184,
      "step": 987
    },
    {
      "epoch": 0.0383038526774897,
      "grad_norm": 0.11880747973918915,
      "learning_rate": 0.0001923762990538235,
      "loss": 0.2834,
      "step": 988
    },
    {
      "epoch": 0.03834262175914708,
      "grad_norm": 0.17069698870182037,
      "learning_rate": 0.00019236854350860867,
      "loss": 0.4087,
      "step": 989
    },
    {
      "epoch": 0.038381390840804455,
      "grad_norm": 0.17211465537548065,
      "learning_rate": 0.00019236078796339384,
      "loss": 0.4278,
      "step": 990
    },
    {
      "epoch": 0.03842015992246184,
      "grad_norm": 0.17018133401870728,
      "learning_rate": 0.00019235303241817901,
      "loss": 0.4113,
      "step": 991
    },
    {
      "epoch": 0.038458929004119215,
      "grad_norm": 0.18388600647449493,
      "learning_rate": 0.0001923452768729642,
      "loss": 0.3894,
      "step": 992
    },
    {
      "epoch": 0.03849769808577659,
      "grad_norm": 0.18226005136966705,
      "learning_rate": 0.00019233752132774936,
      "loss": 0.3254,
      "step": 993
    },
    {
      "epoch": 0.038536467167433974,
      "grad_norm": 0.20131605863571167,
      "learning_rate": 0.00019232976578253453,
      "loss": 0.4279,
      "step": 994
    },
    {
      "epoch": 0.03857523624909135,
      "grad_norm": 0.2003948986530304,
      "learning_rate": 0.00019232201023731968,
      "loss": 0.4104,
      "step": 995
    },
    {
      "epoch": 0.03861400533074873,
      "grad_norm": 0.15467366576194763,
      "learning_rate": 0.00019231425469210488,
      "loss": 0.3137,
      "step": 996
    },
    {
      "epoch": 0.0386527744124061,
      "grad_norm": 0.1341279000043869,
      "learning_rate": 0.00019230649914689002,
      "loss": 0.2858,
      "step": 997
    },
    {
      "epoch": 0.038691543494063486,
      "grad_norm": 0.12605643272399902,
      "learning_rate": 0.00019229874360167522,
      "loss": 0.278,
      "step": 998
    },
    {
      "epoch": 0.03873031257572086,
      "grad_norm": 0.12419694662094116,
      "learning_rate": 0.00019229098805646037,
      "loss": 0.2918,
      "step": 999
    },
    {
      "epoch": 0.03876908165737824,
      "grad_norm": 0.22856441140174866,
      "learning_rate": 0.00019228323251124557,
      "loss": 0.3878,
      "step": 1000
    },
    {
      "epoch": 0.03880785073903562,
      "grad_norm": 0.13353388011455536,
      "learning_rate": 0.00019227547696603072,
      "loss": 0.2861,
      "step": 1001
    },
    {
      "epoch": 0.038846619820693,
      "grad_norm": 0.11295311897993088,
      "learning_rate": 0.0001922677214208159,
      "loss": 0.2951,
      "step": 1002
    },
    {
      "epoch": 0.038885388902350375,
      "grad_norm": 0.13158029317855835,
      "learning_rate": 0.00019225996587560106,
      "loss": 0.2875,
      "step": 1003
    },
    {
      "epoch": 0.03892415798400775,
      "grad_norm": 0.1787683516740799,
      "learning_rate": 0.00019225221033038623,
      "loss": 0.3904,
      "step": 1004
    },
    {
      "epoch": 0.038962927065665134,
      "grad_norm": 0.13487949967384338,
      "learning_rate": 0.0001922444547851714,
      "loss": 0.2508,
      "step": 1005
    },
    {
      "epoch": 0.03900169614732251,
      "grad_norm": 0.13409531116485596,
      "learning_rate": 0.00019223669923995658,
      "loss": 0.2958,
      "step": 1006
    },
    {
      "epoch": 0.03904046522897989,
      "grad_norm": 0.13283568620681763,
      "learning_rate": 0.00019222894369474175,
      "loss": 0.3071,
      "step": 1007
    },
    {
      "epoch": 0.03907923431063726,
      "grad_norm": 0.1168748289346695,
      "learning_rate": 0.00019222118814952693,
      "loss": 0.2324,
      "step": 1008
    },
    {
      "epoch": 0.03911800339229465,
      "grad_norm": 0.16088542342185974,
      "learning_rate": 0.00019221343260431207,
      "loss": 0.3299,
      "step": 1009
    },
    {
      "epoch": 0.03915677247395202,
      "grad_norm": 0.16127623617649078,
      "learning_rate": 0.00019220567705909727,
      "loss": 0.3408,
      "step": 1010
    },
    {
      "epoch": 0.0391955415556094,
      "grad_norm": 0.1736142486333847,
      "learning_rate": 0.00019219792151388242,
      "loss": 0.3909,
      "step": 1011
    },
    {
      "epoch": 0.03923431063726678,
      "grad_norm": 0.19272111356258392,
      "learning_rate": 0.00019219016596866762,
      "loss": 0.3667,
      "step": 1012
    },
    {
      "epoch": 0.03927307971892416,
      "grad_norm": 0.14777877926826477,
      "learning_rate": 0.00019218241042345276,
      "loss": 0.2812,
      "step": 1013
    },
    {
      "epoch": 0.039311848800581535,
      "grad_norm": 0.1424110233783722,
      "learning_rate": 0.00019217465487823796,
      "loss": 0.3329,
      "step": 1014
    },
    {
      "epoch": 0.03935061788223891,
      "grad_norm": 0.13956259191036224,
      "learning_rate": 0.0001921668993330231,
      "loss": 0.3194,
      "step": 1015
    },
    {
      "epoch": 0.039389386963896295,
      "grad_norm": 0.1498570442199707,
      "learning_rate": 0.00019215914378780828,
      "loss": 0.3495,
      "step": 1016
    },
    {
      "epoch": 0.03942815604555367,
      "grad_norm": 0.11568794399499893,
      "learning_rate": 0.00019215138824259345,
      "loss": 0.2426,
      "step": 1017
    },
    {
      "epoch": 0.03946692512721105,
      "grad_norm": 0.16301488876342773,
      "learning_rate": 0.00019214363269737863,
      "loss": 0.3456,
      "step": 1018
    },
    {
      "epoch": 0.03950569420886843,
      "grad_norm": 0.16110548377037048,
      "learning_rate": 0.0001921358771521638,
      "loss": 0.2855,
      "step": 1019
    },
    {
      "epoch": 0.03954446329052581,
      "grad_norm": 0.16815093159675598,
      "learning_rate": 0.00019212812160694897,
      "loss": 0.3503,
      "step": 1020
    },
    {
      "epoch": 0.03958323237218318,
      "grad_norm": 0.1719040721654892,
      "learning_rate": 0.00019212036606173417,
      "loss": 0.2567,
      "step": 1021
    },
    {
      "epoch": 0.03962200145384056,
      "grad_norm": 0.17457394301891327,
      "learning_rate": 0.00019211261051651932,
      "loss": 0.3042,
      "step": 1022
    },
    {
      "epoch": 0.03966077053549794,
      "grad_norm": 0.1695326417684555,
      "learning_rate": 0.0001921048549713045,
      "loss": 0.3834,
      "step": 1023
    },
    {
      "epoch": 0.03969953961715532,
      "grad_norm": 0.15245692431926727,
      "learning_rate": 0.00019209709942608966,
      "loss": 0.3678,
      "step": 1024
    },
    {
      "epoch": 0.039738308698812695,
      "grad_norm": 0.13434317708015442,
      "learning_rate": 0.00019208934388087484,
      "loss": 0.259,
      "step": 1025
    },
    {
      "epoch": 0.03977707778047008,
      "grad_norm": 0.1453741043806076,
      "learning_rate": 0.00019208158833566,
      "loss": 0.3281,
      "step": 1026
    },
    {
      "epoch": 0.039815846862127455,
      "grad_norm": 0.11964968591928482,
      "learning_rate": 0.00019207383279044518,
      "loss": 0.2832,
      "step": 1027
    },
    {
      "epoch": 0.03985461594378483,
      "grad_norm": 0.15541790425777435,
      "learning_rate": 0.00019206607724523035,
      "loss": 0.324,
      "step": 1028
    },
    {
      "epoch": 0.03989338502544221,
      "grad_norm": 0.16196350753307343,
      "learning_rate": 0.00019205832170001553,
      "loss": 0.2835,
      "step": 1029
    },
    {
      "epoch": 0.03993215410709959,
      "grad_norm": 0.15348230302333832,
      "learning_rate": 0.00019205056615480067,
      "loss": 0.3788,
      "step": 1030
    },
    {
      "epoch": 0.03997092318875697,
      "grad_norm": 0.15305796265602112,
      "learning_rate": 0.00019204281060958587,
      "loss": 0.3569,
      "step": 1031
    },
    {
      "epoch": 0.04000969227041434,
      "grad_norm": 0.18953508138656616,
      "learning_rate": 0.00019203505506437102,
      "loss": 0.4193,
      "step": 1032
    },
    {
      "epoch": 0.04004846135207172,
      "grad_norm": 0.18976332247257233,
      "learning_rate": 0.00019202729951915622,
      "loss": 0.3626,
      "step": 1033
    },
    {
      "epoch": 0.0400872304337291,
      "grad_norm": 0.18555615842342377,
      "learning_rate": 0.00019201954397394136,
      "loss": 0.3542,
      "step": 1034
    },
    {
      "epoch": 0.04012599951538648,
      "grad_norm": 0.16781295835971832,
      "learning_rate": 0.00019201178842872656,
      "loss": 0.3086,
      "step": 1035
    },
    {
      "epoch": 0.040164768597043855,
      "grad_norm": 0.14670316874980927,
      "learning_rate": 0.0001920040328835117,
      "loss": 0.3343,
      "step": 1036
    },
    {
      "epoch": 0.04020353767870124,
      "grad_norm": 0.2055506557226181,
      "learning_rate": 0.00019199627733829688,
      "loss": 0.3866,
      "step": 1037
    },
    {
      "epoch": 0.040242306760358615,
      "grad_norm": 0.16219983994960785,
      "learning_rate": 0.00019198852179308206,
      "loss": 0.3301,
      "step": 1038
    },
    {
      "epoch": 0.04028107584201599,
      "grad_norm": 0.16163961589336395,
      "learning_rate": 0.00019198076624786723,
      "loss": 0.3707,
      "step": 1039
    },
    {
      "epoch": 0.04031984492367337,
      "grad_norm": 0.14547328650951385,
      "learning_rate": 0.0001919730107026524,
      "loss": 0.3142,
      "step": 1040
    },
    {
      "epoch": 0.04035861400533075,
      "grad_norm": 0.16481375694274902,
      "learning_rate": 0.00019196525515743757,
      "loss": 0.3428,
      "step": 1041
    },
    {
      "epoch": 0.04039738308698813,
      "grad_norm": 0.15138556063175201,
      "learning_rate": 0.00019195749961222275,
      "loss": 0.3256,
      "step": 1042
    },
    {
      "epoch": 0.0404361521686455,
      "grad_norm": 0.18569444119930267,
      "learning_rate": 0.00019194974406700792,
      "loss": 0.3462,
      "step": 1043
    },
    {
      "epoch": 0.04047492125030289,
      "grad_norm": 0.18199671804904938,
      "learning_rate": 0.0001919419885217931,
      "loss": 0.3145,
      "step": 1044
    },
    {
      "epoch": 0.04051369033196026,
      "grad_norm": 0.19228462874889374,
      "learning_rate": 0.00019193423297657826,
      "loss": 0.3262,
      "step": 1045
    },
    {
      "epoch": 0.04055245941361764,
      "grad_norm": 0.1738521307706833,
      "learning_rate": 0.00019192647743136344,
      "loss": 0.3533,
      "step": 1046
    },
    {
      "epoch": 0.040591228495275015,
      "grad_norm": 0.15258139371871948,
      "learning_rate": 0.0001919187218861486,
      "loss": 0.3024,
      "step": 1047
    },
    {
      "epoch": 0.0406299975769324,
      "grad_norm": 0.12504377961158752,
      "learning_rate": 0.00019191096634093378,
      "loss": 0.2696,
      "step": 1048
    },
    {
      "epoch": 0.040668766658589775,
      "grad_norm": 0.16052867472171783,
      "learning_rate": 0.00019190321079571896,
      "loss": 0.3193,
      "step": 1049
    },
    {
      "epoch": 0.04070753574024715,
      "grad_norm": 0.13636918365955353,
      "learning_rate": 0.00019189545525050413,
      "loss": 0.2973,
      "step": 1050
    },
    {
      "epoch": 0.040746304821904535,
      "grad_norm": 0.12767580151557922,
      "learning_rate": 0.00019188769970528927,
      "loss": 0.2838,
      "step": 1051
    },
    {
      "epoch": 0.04078507390356191,
      "grad_norm": 0.1617906093597412,
      "learning_rate": 0.00019187994416007447,
      "loss": 0.364,
      "step": 1052
    },
    {
      "epoch": 0.04082384298521929,
      "grad_norm": 0.19329161942005157,
      "learning_rate": 0.00019187218861485962,
      "loss": 0.4176,
      "step": 1053
    },
    {
      "epoch": 0.040862612066876663,
      "grad_norm": 0.17092235386371613,
      "learning_rate": 0.00019186443306964482,
      "loss": 0.3915,
      "step": 1054
    },
    {
      "epoch": 0.04090138114853405,
      "grad_norm": 0.13639821112155914,
      "learning_rate": 0.00019185667752442997,
      "loss": 0.2689,
      "step": 1055
    },
    {
      "epoch": 0.04094015023019142,
      "grad_norm": 0.17618237435817719,
      "learning_rate": 0.00019184892197921517,
      "loss": 0.368,
      "step": 1056
    },
    {
      "epoch": 0.0409789193118488,
      "grad_norm": 0.16319647431373596,
      "learning_rate": 0.0001918411664340003,
      "loss": 0.2766,
      "step": 1057
    },
    {
      "epoch": 0.041017688393506176,
      "grad_norm": 0.18136827647686005,
      "learning_rate": 0.00019183341088878548,
      "loss": 0.3814,
      "step": 1058
    },
    {
      "epoch": 0.04105645747516356,
      "grad_norm": 0.1845402717590332,
      "learning_rate": 0.00019182565534357066,
      "loss": 0.3889,
      "step": 1059
    },
    {
      "epoch": 0.041095226556820935,
      "grad_norm": 0.167554572224617,
      "learning_rate": 0.00019181789979835583,
      "loss": 0.3667,
      "step": 1060
    },
    {
      "epoch": 0.04113399563847831,
      "grad_norm": 0.1613660603761673,
      "learning_rate": 0.000191810144253141,
      "loss": 0.3345,
      "step": 1061
    },
    {
      "epoch": 0.041172764720135695,
      "grad_norm": 0.1594947725534439,
      "learning_rate": 0.00019180238870792618,
      "loss": 0.4203,
      "step": 1062
    },
    {
      "epoch": 0.04121153380179307,
      "grad_norm": 0.1384793519973755,
      "learning_rate": 0.00019179463316271135,
      "loss": 0.2942,
      "step": 1063
    },
    {
      "epoch": 0.04125030288345045,
      "grad_norm": 0.14732691645622253,
      "learning_rate": 0.00019178687761749652,
      "loss": 0.3591,
      "step": 1064
    },
    {
      "epoch": 0.041289071965107824,
      "grad_norm": 0.16610577702522278,
      "learning_rate": 0.0001917791220722817,
      "loss": 0.3779,
      "step": 1065
    },
    {
      "epoch": 0.04132784104676521,
      "grad_norm": 0.1405896097421646,
      "learning_rate": 0.00019177136652706687,
      "loss": 0.3037,
      "step": 1066
    },
    {
      "epoch": 0.04136661012842258,
      "grad_norm": 0.1425531804561615,
      "learning_rate": 0.00019176361098185204,
      "loss": 0.3285,
      "step": 1067
    },
    {
      "epoch": 0.04140537921007996,
      "grad_norm": 0.13546128571033478,
      "learning_rate": 0.0001917558554366372,
      "loss": 0.3317,
      "step": 1068
    },
    {
      "epoch": 0.04144414829173734,
      "grad_norm": 0.13762274384498596,
      "learning_rate": 0.00019174809989142239,
      "loss": 0.27,
      "step": 1069
    },
    {
      "epoch": 0.04148291737339472,
      "grad_norm": 0.12552410364151,
      "learning_rate": 0.00019174034434620756,
      "loss": 0.2865,
      "step": 1070
    },
    {
      "epoch": 0.041521686455052095,
      "grad_norm": 0.15934009850025177,
      "learning_rate": 0.00019173258880099273,
      "loss": 0.3336,
      "step": 1071
    },
    {
      "epoch": 0.04156045553670947,
      "grad_norm": 0.12123747169971466,
      "learning_rate": 0.00019172483325577788,
      "loss": 0.2623,
      "step": 1072
    },
    {
      "epoch": 0.041599224618366855,
      "grad_norm": 0.15967392921447754,
      "learning_rate": 0.00019171707771056308,
      "loss": 0.2757,
      "step": 1073
    },
    {
      "epoch": 0.04163799370002423,
      "grad_norm": 0.1879553347826004,
      "learning_rate": 0.00019170932216534822,
      "loss": 0.3827,
      "step": 1074
    },
    {
      "epoch": 0.04167676278168161,
      "grad_norm": 0.19079145789146423,
      "learning_rate": 0.00019170156662013342,
      "loss": 0.3358,
      "step": 1075
    },
    {
      "epoch": 0.041715531863338984,
      "grad_norm": 0.20137323439121246,
      "learning_rate": 0.00019169381107491857,
      "loss": 0.3428,
      "step": 1076
    },
    {
      "epoch": 0.04175430094499637,
      "grad_norm": 0.1618637591600418,
      "learning_rate": 0.00019168605552970377,
      "loss": 0.3219,
      "step": 1077
    },
    {
      "epoch": 0.04179307002665374,
      "grad_norm": 0.16398265957832336,
      "learning_rate": 0.0001916782999844889,
      "loss": 0.4023,
      "step": 1078
    },
    {
      "epoch": 0.04183183910831112,
      "grad_norm": 0.13263991475105286,
      "learning_rate": 0.00019167054443927409,
      "loss": 0.321,
      "step": 1079
    },
    {
      "epoch": 0.0418706081899685,
      "grad_norm": 0.15464623272418976,
      "learning_rate": 0.00019166278889405926,
      "loss": 0.3325,
      "step": 1080
    },
    {
      "epoch": 0.04190937727162588,
      "grad_norm": 0.23382443189620972,
      "learning_rate": 0.00019165503334884443,
      "loss": 0.4543,
      "step": 1081
    },
    {
      "epoch": 0.041948146353283255,
      "grad_norm": 0.1443822830915451,
      "learning_rate": 0.0001916472778036296,
      "loss": 0.3165,
      "step": 1082
    },
    {
      "epoch": 0.04198691543494063,
      "grad_norm": 0.13798658549785614,
      "learning_rate": 0.00019163952225841478,
      "loss": 0.2719,
      "step": 1083
    },
    {
      "epoch": 0.042025684516598015,
      "grad_norm": 0.16328410804271698,
      "learning_rate": 0.00019163176671319995,
      "loss": 0.2915,
      "step": 1084
    },
    {
      "epoch": 0.04206445359825539,
      "grad_norm": 0.18903955817222595,
      "learning_rate": 0.00019162401116798512,
      "loss": 0.3094,
      "step": 1085
    },
    {
      "epoch": 0.04210322267991277,
      "grad_norm": 0.23899923264980316,
      "learning_rate": 0.00019161625562277027,
      "loss": 0.4026,
      "step": 1086
    },
    {
      "epoch": 0.04214199176157015,
      "grad_norm": 0.17976140975952148,
      "learning_rate": 0.00019160850007755547,
      "loss": 0.2996,
      "step": 1087
    },
    {
      "epoch": 0.04218076084322753,
      "grad_norm": 0.17994314432144165,
      "learning_rate": 0.00019160074453234061,
      "loss": 0.3235,
      "step": 1088
    },
    {
      "epoch": 0.042219529924884903,
      "grad_norm": 0.16266053915023804,
      "learning_rate": 0.00019159298898712581,
      "loss": 0.3412,
      "step": 1089
    },
    {
      "epoch": 0.04225829900654228,
      "grad_norm": 0.22308336198329926,
      "learning_rate": 0.00019158523344191096,
      "loss": 0.4684,
      "step": 1090
    },
    {
      "epoch": 0.04229706808819966,
      "grad_norm": 0.1783304214477539,
      "learning_rate": 0.00019157747789669616,
      "loss": 0.3828,
      "step": 1091
    },
    {
      "epoch": 0.04233583716985704,
      "grad_norm": 0.15982161462306976,
      "learning_rate": 0.0001915697223514813,
      "loss": 0.3547,
      "step": 1092
    },
    {
      "epoch": 0.042374606251514416,
      "grad_norm": 0.164137601852417,
      "learning_rate": 0.00019156196680626648,
      "loss": 0.3203,
      "step": 1093
    },
    {
      "epoch": 0.0424133753331718,
      "grad_norm": 0.15366464853286743,
      "learning_rate": 0.00019155421126105165,
      "loss": 0.3408,
      "step": 1094
    },
    {
      "epoch": 0.042452144414829175,
      "grad_norm": 0.14403480291366577,
      "learning_rate": 0.00019154645571583682,
      "loss": 0.2815,
      "step": 1095
    },
    {
      "epoch": 0.04249091349648655,
      "grad_norm": 0.1630687117576599,
      "learning_rate": 0.000191538700170622,
      "loss": 0.2718,
      "step": 1096
    },
    {
      "epoch": 0.04252968257814393,
      "grad_norm": 0.13907194137573242,
      "learning_rate": 0.00019153094462540717,
      "loss": 0.2926,
      "step": 1097
    },
    {
      "epoch": 0.04256845165980131,
      "grad_norm": 0.19967307150363922,
      "learning_rate": 0.00019152318908019234,
      "loss": 0.4065,
      "step": 1098
    },
    {
      "epoch": 0.04260722074145869,
      "grad_norm": 0.19042466580867767,
      "learning_rate": 0.00019151543353497752,
      "loss": 0.3817,
      "step": 1099
    },
    {
      "epoch": 0.042645989823116064,
      "grad_norm": 0.13326314091682434,
      "learning_rate": 0.0001915076779897627,
      "loss": 0.2755,
      "step": 1100
    },
    {
      "epoch": 0.04268475890477344,
      "grad_norm": 0.15846943855285645,
      "learning_rate": 0.00019149992244454786,
      "loss": 0.3548,
      "step": 1101
    },
    {
      "epoch": 0.04272352798643082,
      "grad_norm": 0.15626652538776398,
      "learning_rate": 0.00019149216689933303,
      "loss": 0.2554,
      "step": 1102
    },
    {
      "epoch": 0.0427622970680882,
      "grad_norm": 0.1267220675945282,
      "learning_rate": 0.0001914844113541182,
      "loss": 0.2818,
      "step": 1103
    },
    {
      "epoch": 0.042801066149745576,
      "grad_norm": 0.14643938839435577,
      "learning_rate": 0.00019147665580890338,
      "loss": 0.3657,
      "step": 1104
    },
    {
      "epoch": 0.04283983523140296,
      "grad_norm": 0.19341781735420227,
      "learning_rate": 0.00019146890026368855,
      "loss": 0.4374,
      "step": 1105
    },
    {
      "epoch": 0.042878604313060335,
      "grad_norm": 0.14704203605651855,
      "learning_rate": 0.00019146114471847372,
      "loss": 0.3385,
      "step": 1106
    },
    {
      "epoch": 0.04291737339471771,
      "grad_norm": 0.1424056738615036,
      "learning_rate": 0.00019145338917325887,
      "loss": 0.2631,
      "step": 1107
    },
    {
      "epoch": 0.04295614247637509,
      "grad_norm": 0.1883256584405899,
      "learning_rate": 0.00019144563362804407,
      "loss": 0.4034,
      "step": 1108
    },
    {
      "epoch": 0.04299491155803247,
      "grad_norm": 0.19935545325279236,
      "learning_rate": 0.00019143787808282922,
      "loss": 0.3137,
      "step": 1109
    },
    {
      "epoch": 0.04303368063968985,
      "grad_norm": 0.16736409068107605,
      "learning_rate": 0.00019143012253761442,
      "loss": 0.2922,
      "step": 1110
    },
    {
      "epoch": 0.043072449721347224,
      "grad_norm": 0.17065085470676422,
      "learning_rate": 0.00019142236699239956,
      "loss": 0.2752,
      "step": 1111
    },
    {
      "epoch": 0.04311121880300461,
      "grad_norm": 0.18953119218349457,
      "learning_rate": 0.00019141461144718476,
      "loss": 0.3081,
      "step": 1112
    },
    {
      "epoch": 0.04314998788466198,
      "grad_norm": 0.21999220550060272,
      "learning_rate": 0.0001914068559019699,
      "loss": 0.3333,
      "step": 1113
    },
    {
      "epoch": 0.04318875696631936,
      "grad_norm": 0.1802578717470169,
      "learning_rate": 0.00019139910035675508,
      "loss": 0.3268,
      "step": 1114
    },
    {
      "epoch": 0.043227526047976736,
      "grad_norm": 0.18953540921211243,
      "learning_rate": 0.00019139134481154025,
      "loss": 0.3651,
      "step": 1115
    },
    {
      "epoch": 0.04326629512963412,
      "grad_norm": 0.1695447564125061,
      "learning_rate": 0.00019138358926632543,
      "loss": 0.2381,
      "step": 1116
    },
    {
      "epoch": 0.043305064211291495,
      "grad_norm": 0.16902075707912445,
      "learning_rate": 0.0001913758337211106,
      "loss": 0.3153,
      "step": 1117
    },
    {
      "epoch": 0.04334383329294887,
      "grad_norm": 0.2031126320362091,
      "learning_rate": 0.00019136807817589577,
      "loss": 0.3139,
      "step": 1118
    },
    {
      "epoch": 0.04338260237460625,
      "grad_norm": 0.17308056354522705,
      "learning_rate": 0.00019136032263068094,
      "loss": 0.2768,
      "step": 1119
    },
    {
      "epoch": 0.04342137145626363,
      "grad_norm": 0.1643604189157486,
      "learning_rate": 0.00019135256708546612,
      "loss": 0.309,
      "step": 1120
    },
    {
      "epoch": 0.04346014053792101,
      "grad_norm": 0.2067638635635376,
      "learning_rate": 0.0001913448115402513,
      "loss": 0.3963,
      "step": 1121
    },
    {
      "epoch": 0.043498909619578384,
      "grad_norm": 0.2261684536933899,
      "learning_rate": 0.00019133705599503646,
      "loss": 0.3844,
      "step": 1122
    },
    {
      "epoch": 0.04353767870123577,
      "grad_norm": 0.1483713835477829,
      "learning_rate": 0.00019132930044982164,
      "loss": 0.2828,
      "step": 1123
    },
    {
      "epoch": 0.04357644778289314,
      "grad_norm": 0.14597751200199127,
      "learning_rate": 0.0001913215449046068,
      "loss": 0.2561,
      "step": 1124
    },
    {
      "epoch": 0.04361521686455052,
      "grad_norm": 0.1852516233921051,
      "learning_rate": 0.00019131378935939198,
      "loss": 0.3391,
      "step": 1125
    },
    {
      "epoch": 0.043653985946207896,
      "grad_norm": 0.181377574801445,
      "learning_rate": 0.00019130603381417715,
      "loss": 0.297,
      "step": 1126
    },
    {
      "epoch": 0.04369275502786528,
      "grad_norm": 0.1315283179283142,
      "learning_rate": 0.00019129827826896233,
      "loss": 0.2758,
      "step": 1127
    },
    {
      "epoch": 0.043731524109522656,
      "grad_norm": 0.13891907036304474,
      "learning_rate": 0.00019129052272374747,
      "loss": 0.2786,
      "step": 1128
    },
    {
      "epoch": 0.04377029319118003,
      "grad_norm": 0.1515732854604721,
      "learning_rate": 0.00019128276717853267,
      "loss": 0.3246,
      "step": 1129
    },
    {
      "epoch": 0.043809062272837415,
      "grad_norm": 0.1691371351480484,
      "learning_rate": 0.00019127501163331782,
      "loss": 0.3187,
      "step": 1130
    },
    {
      "epoch": 0.04384783135449479,
      "grad_norm": 0.14646607637405396,
      "learning_rate": 0.00019126725608810302,
      "loss": 0.3053,
      "step": 1131
    },
    {
      "epoch": 0.04388660043615217,
      "grad_norm": 0.15248270332813263,
      "learning_rate": 0.00019125950054288816,
      "loss": 0.3444,
      "step": 1132
    },
    {
      "epoch": 0.043925369517809544,
      "grad_norm": 0.15062406659126282,
      "learning_rate": 0.00019125174499767336,
      "loss": 0.3216,
      "step": 1133
    },
    {
      "epoch": 0.04396413859946693,
      "grad_norm": 0.16923372447490692,
      "learning_rate": 0.0001912439894524585,
      "loss": 0.3636,
      "step": 1134
    },
    {
      "epoch": 0.044002907681124304,
      "grad_norm": 0.1827515959739685,
      "learning_rate": 0.00019123623390724368,
      "loss": 0.3321,
      "step": 1135
    },
    {
      "epoch": 0.04404167676278168,
      "grad_norm": 0.14202824234962463,
      "learning_rate": 0.00019122847836202885,
      "loss": 0.3497,
      "step": 1136
    },
    {
      "epoch": 0.04408044584443906,
      "grad_norm": 0.18012894690036774,
      "learning_rate": 0.00019122072281681403,
      "loss": 0.4236,
      "step": 1137
    },
    {
      "epoch": 0.04411921492609644,
      "grad_norm": 0.14476805925369263,
      "learning_rate": 0.0001912129672715992,
      "loss": 0.3042,
      "step": 1138
    },
    {
      "epoch": 0.044157984007753816,
      "grad_norm": 0.18126830458641052,
      "learning_rate": 0.00019120521172638437,
      "loss": 0.4051,
      "step": 1139
    },
    {
      "epoch": 0.04419675308941119,
      "grad_norm": 0.18405751883983612,
      "learning_rate": 0.00019119745618116955,
      "loss": 0.368,
      "step": 1140
    },
    {
      "epoch": 0.044235522171068575,
      "grad_norm": 0.14543843269348145,
      "learning_rate": 0.00019118970063595472,
      "loss": 0.3372,
      "step": 1141
    },
    {
      "epoch": 0.04427429125272595,
      "grad_norm": 0.16340717673301697,
      "learning_rate": 0.00019118194509073986,
      "loss": 0.378,
      "step": 1142
    },
    {
      "epoch": 0.04431306033438333,
      "grad_norm": 0.16402341425418854,
      "learning_rate": 0.00019117418954552506,
      "loss": 0.3526,
      "step": 1143
    },
    {
      "epoch": 0.044351829416040704,
      "grad_norm": 0.13830998539924622,
      "learning_rate": 0.00019116643400031024,
      "loss": 0.2558,
      "step": 1144
    },
    {
      "epoch": 0.04439059849769809,
      "grad_norm": 0.21245776116847992,
      "learning_rate": 0.0001911586784550954,
      "loss": 0.3726,
      "step": 1145
    },
    {
      "epoch": 0.044429367579355464,
      "grad_norm": 0.18136253952980042,
      "learning_rate": 0.00019115092290988058,
      "loss": 0.3156,
      "step": 1146
    },
    {
      "epoch": 0.04446813666101284,
      "grad_norm": 0.1653892695903778,
      "learning_rate": 0.00019114316736466576,
      "loss": 0.2642,
      "step": 1147
    },
    {
      "epoch": 0.04450690574267022,
      "grad_norm": 0.17614924907684326,
      "learning_rate": 0.00019113541181945093,
      "loss": 0.3063,
      "step": 1148
    },
    {
      "epoch": 0.0445456748243276,
      "grad_norm": 0.17524731159210205,
      "learning_rate": 0.00019112765627423607,
      "loss": 0.2956,
      "step": 1149
    },
    {
      "epoch": 0.044584443905984976,
      "grad_norm": 0.2261160910129547,
      "learning_rate": 0.00019111990072902127,
      "loss": 0.3796,
      "step": 1150
    },
    {
      "epoch": 0.04462321298764235,
      "grad_norm": 0.15345199406147003,
      "learning_rate": 0.00019111214518380642,
      "loss": 0.3305,
      "step": 1151
    },
    {
      "epoch": 0.044661982069299735,
      "grad_norm": 0.1851859986782074,
      "learning_rate": 0.00019110438963859162,
      "loss": 0.3787,
      "step": 1152
    },
    {
      "epoch": 0.04470075115095711,
      "grad_norm": 0.1663854420185089,
      "learning_rate": 0.00019109663409337677,
      "loss": 0.3456,
      "step": 1153
    },
    {
      "epoch": 0.04473952023261449,
      "grad_norm": 0.1866789013147354,
      "learning_rate": 0.00019108887854816197,
      "loss": 0.37,
      "step": 1154
    },
    {
      "epoch": 0.04477828931427187,
      "grad_norm": 0.17122863233089447,
      "learning_rate": 0.0001910811230029471,
      "loss": 0.3211,
      "step": 1155
    },
    {
      "epoch": 0.04481705839592925,
      "grad_norm": 0.17975790798664093,
      "learning_rate": 0.00019107336745773228,
      "loss": 0.374,
      "step": 1156
    },
    {
      "epoch": 0.044855827477586624,
      "grad_norm": 0.23324134945869446,
      "learning_rate": 0.00019106561191251746,
      "loss": 0.4246,
      "step": 1157
    },
    {
      "epoch": 0.044894596559244,
      "grad_norm": 0.1883704960346222,
      "learning_rate": 0.00019105785636730263,
      "loss": 0.3318,
      "step": 1158
    },
    {
      "epoch": 0.04493336564090138,
      "grad_norm": 0.13780996203422546,
      "learning_rate": 0.0001910501008220878,
      "loss": 0.2443,
      "step": 1159
    },
    {
      "epoch": 0.04497213472255876,
      "grad_norm": 0.18694563210010529,
      "learning_rate": 0.00019104234527687297,
      "loss": 0.3302,
      "step": 1160
    },
    {
      "epoch": 0.045010903804216136,
      "grad_norm": 0.17016269266605377,
      "learning_rate": 0.00019103458973165815,
      "loss": 0.3044,
      "step": 1161
    },
    {
      "epoch": 0.04504967288587352,
      "grad_norm": 0.20010565221309662,
      "learning_rate": 0.00019102683418644332,
      "loss": 0.3702,
      "step": 1162
    },
    {
      "epoch": 0.045088441967530896,
      "grad_norm": 0.18462170660495758,
      "learning_rate": 0.00019101907864122847,
      "loss": 0.3037,
      "step": 1163
    },
    {
      "epoch": 0.04512721104918827,
      "grad_norm": 0.20821352303028107,
      "learning_rate": 0.00019101132309601367,
      "loss": 0.4146,
      "step": 1164
    },
    {
      "epoch": 0.04516598013084565,
      "grad_norm": 0.16295351088047028,
      "learning_rate": 0.0001910035675507988,
      "loss": 0.2942,
      "step": 1165
    },
    {
      "epoch": 0.04520474921250303,
      "grad_norm": 0.18280866742134094,
      "learning_rate": 0.000190995812005584,
      "loss": 0.4192,
      "step": 1166
    },
    {
      "epoch": 0.04524351829416041,
      "grad_norm": 0.18831279873847961,
      "learning_rate": 0.00019098805646036916,
      "loss": 0.3784,
      "step": 1167
    },
    {
      "epoch": 0.045282287375817784,
      "grad_norm": 0.12369431555271149,
      "learning_rate": 0.00019098030091515436,
      "loss": 0.2612,
      "step": 1168
    },
    {
      "epoch": 0.04532105645747516,
      "grad_norm": 0.14056652784347534,
      "learning_rate": 0.0001909725453699395,
      "loss": 0.2578,
      "step": 1169
    },
    {
      "epoch": 0.045359825539132544,
      "grad_norm": 0.20832692086696625,
      "learning_rate": 0.00019096478982472468,
      "loss": 0.4141,
      "step": 1170
    },
    {
      "epoch": 0.04539859462078992,
      "grad_norm": 0.1784866750240326,
      "learning_rate": 0.00019095703427950985,
      "loss": 0.299,
      "step": 1171
    },
    {
      "epoch": 0.045437363702447296,
      "grad_norm": 0.1603546291589737,
      "learning_rate": 0.00019094927873429502,
      "loss": 0.3836,
      "step": 1172
    },
    {
      "epoch": 0.04547613278410468,
      "grad_norm": 0.16057856380939484,
      "learning_rate": 0.0001909415231890802,
      "loss": 0.2939,
      "step": 1173
    },
    {
      "epoch": 0.045514901865762056,
      "grad_norm": 0.1453256905078888,
      "learning_rate": 0.00019093376764386537,
      "loss": 0.2972,
      "step": 1174
    },
    {
      "epoch": 0.04555367094741943,
      "grad_norm": 0.18573474884033203,
      "learning_rate": 0.00019092601209865054,
      "loss": 0.3387,
      "step": 1175
    },
    {
      "epoch": 0.04559244002907681,
      "grad_norm": 0.1485198587179184,
      "learning_rate": 0.0001909182565534357,
      "loss": 0.2597,
      "step": 1176
    },
    {
      "epoch": 0.04563120911073419,
      "grad_norm": 0.19327397644519806,
      "learning_rate": 0.00019091050100822089,
      "loss": 0.3403,
      "step": 1177
    },
    {
      "epoch": 0.04566997819239157,
      "grad_norm": 0.15629464387893677,
      "learning_rate": 0.00019090274546300606,
      "loss": 0.2838,
      "step": 1178
    },
    {
      "epoch": 0.045708747274048944,
      "grad_norm": 0.15821552276611328,
      "learning_rate": 0.00019089498991779123,
      "loss": 0.3078,
      "step": 1179
    },
    {
      "epoch": 0.04574751635570633,
      "grad_norm": 0.1646910458803177,
      "learning_rate": 0.0001908872343725764,
      "loss": 0.3456,
      "step": 1180
    },
    {
      "epoch": 0.045786285437363704,
      "grad_norm": 0.1714419424533844,
      "learning_rate": 0.00019087947882736158,
      "loss": 0.2777,
      "step": 1181
    },
    {
      "epoch": 0.04582505451902108,
      "grad_norm": 0.18264442682266235,
      "learning_rate": 0.00019087172328214675,
      "loss": 0.3686,
      "step": 1182
    },
    {
      "epoch": 0.045863823600678456,
      "grad_norm": 0.17462724447250366,
      "learning_rate": 0.00019086396773693192,
      "loss": 0.3577,
      "step": 1183
    },
    {
      "epoch": 0.04590259268233584,
      "grad_norm": 0.17441558837890625,
      "learning_rate": 0.00019085621219171707,
      "loss": 0.3245,
      "step": 1184
    },
    {
      "epoch": 0.045941361763993216,
      "grad_norm": 0.1968897432088852,
      "learning_rate": 0.00019084845664650227,
      "loss": 0.4219,
      "step": 1185
    },
    {
      "epoch": 0.04598013084565059,
      "grad_norm": 0.1715601086616516,
      "learning_rate": 0.00019084070110128741,
      "loss": 0.3365,
      "step": 1186
    },
    {
      "epoch": 0.04601889992730797,
      "grad_norm": 0.18349061906337738,
      "learning_rate": 0.00019083294555607261,
      "loss": 0.2865,
      "step": 1187
    },
    {
      "epoch": 0.04605766900896535,
      "grad_norm": 0.17372560501098633,
      "learning_rate": 0.00019082519001085776,
      "loss": 0.3144,
      "step": 1188
    },
    {
      "epoch": 0.04609643809062273,
      "grad_norm": 0.19983164966106415,
      "learning_rate": 0.00019081743446564296,
      "loss": 0.3401,
      "step": 1189
    },
    {
      "epoch": 0.046135207172280104,
      "grad_norm": 0.1670221984386444,
      "learning_rate": 0.0001908096789204281,
      "loss": 0.2709,
      "step": 1190
    },
    {
      "epoch": 0.04617397625393749,
      "grad_norm": 0.18382607400417328,
      "learning_rate": 0.00019080192337521328,
      "loss": 0.3334,
      "step": 1191
    },
    {
      "epoch": 0.046212745335594864,
      "grad_norm": 0.14860588312149048,
      "learning_rate": 0.00019079416782999845,
      "loss": 0.2649,
      "step": 1192
    },
    {
      "epoch": 0.04625151441725224,
      "grad_norm": 0.1762741357088089,
      "learning_rate": 0.00019078641228478362,
      "loss": 0.2918,
      "step": 1193
    },
    {
      "epoch": 0.046290283498909617,
      "grad_norm": 0.20165151357650757,
      "learning_rate": 0.0001907786567395688,
      "loss": 0.3423,
      "step": 1194
    },
    {
      "epoch": 0.046329052580567,
      "grad_norm": 0.13829493522644043,
      "learning_rate": 0.00019077090119435397,
      "loss": 0.2943,
      "step": 1195
    },
    {
      "epoch": 0.046367821662224376,
      "grad_norm": 0.20361852645874023,
      "learning_rate": 0.00019076314564913914,
      "loss": 0.3327,
      "step": 1196
    },
    {
      "epoch": 0.04640659074388175,
      "grad_norm": 0.16492486000061035,
      "learning_rate": 0.00019075539010392431,
      "loss": 0.344,
      "step": 1197
    },
    {
      "epoch": 0.046445359825539136,
      "grad_norm": 0.16831545531749725,
      "learning_rate": 0.0001907476345587095,
      "loss": 0.3158,
      "step": 1198
    },
    {
      "epoch": 0.04648412890719651,
      "grad_norm": 0.12779103219509125,
      "learning_rate": 0.00019073987901349466,
      "loss": 0.2605,
      "step": 1199
    },
    {
      "epoch": 0.04652289798885389,
      "grad_norm": 0.1884717494249344,
      "learning_rate": 0.00019073212346827983,
      "loss": 0.3698,
      "step": 1200
    },
    {
      "epoch": 0.046561667070511265,
      "grad_norm": 0.18541070818901062,
      "learning_rate": 0.000190724367923065,
      "loss": 0.3172,
      "step": 1201
    },
    {
      "epoch": 0.04660043615216865,
      "grad_norm": 0.20121148228645325,
      "learning_rate": 0.00019071661237785018,
      "loss": 0.4148,
      "step": 1202
    },
    {
      "epoch": 0.046639205233826024,
      "grad_norm": 0.20140397548675537,
      "learning_rate": 0.00019070885683263535,
      "loss": 0.4491,
      "step": 1203
    },
    {
      "epoch": 0.0466779743154834,
      "grad_norm": 0.16330796480178833,
      "learning_rate": 0.00019070110128742052,
      "loss": 0.323,
      "step": 1204
    },
    {
      "epoch": 0.046716743397140784,
      "grad_norm": 0.15619583427906036,
      "learning_rate": 0.00019069334574220567,
      "loss": 0.3242,
      "step": 1205
    },
    {
      "epoch": 0.04675551247879816,
      "grad_norm": 0.1509726345539093,
      "learning_rate": 0.00019068559019699087,
      "loss": 0.2928,
      "step": 1206
    },
    {
      "epoch": 0.046794281560455536,
      "grad_norm": 0.15200656652450562,
      "learning_rate": 0.00019067783465177602,
      "loss": 0.3298,
      "step": 1207
    },
    {
      "epoch": 0.04683305064211291,
      "grad_norm": 0.15050141513347626,
      "learning_rate": 0.00019067007910656122,
      "loss": 0.315,
      "step": 1208
    },
    {
      "epoch": 0.046871819723770296,
      "grad_norm": 0.19363924860954285,
      "learning_rate": 0.00019066232356134636,
      "loss": 0.3923,
      "step": 1209
    },
    {
      "epoch": 0.04691058880542767,
      "grad_norm": 0.20724570751190186,
      "learning_rate": 0.00019065456801613156,
      "loss": 0.3184,
      "step": 1210
    },
    {
      "epoch": 0.04694935788708505,
      "grad_norm": 0.14111831784248352,
      "learning_rate": 0.0001906468124709167,
      "loss": 0.3101,
      "step": 1211
    },
    {
      "epoch": 0.046988126968742425,
      "grad_norm": 0.1760024130344391,
      "learning_rate": 0.00019063905692570188,
      "loss": 0.3627,
      "step": 1212
    },
    {
      "epoch": 0.04702689605039981,
      "grad_norm": 0.13067398965358734,
      "learning_rate": 0.00019063130138048705,
      "loss": 0.286,
      "step": 1213
    },
    {
      "epoch": 0.047065665132057184,
      "grad_norm": 0.15021567046642303,
      "learning_rate": 0.00019062354583527223,
      "loss": 0.3414,
      "step": 1214
    },
    {
      "epoch": 0.04710443421371456,
      "grad_norm": 0.16869066655635834,
      "learning_rate": 0.0001906157902900574,
      "loss": 0.3421,
      "step": 1215
    },
    {
      "epoch": 0.047143203295371944,
      "grad_norm": 0.15927186608314514,
      "learning_rate": 0.00019060803474484257,
      "loss": 0.2904,
      "step": 1216
    },
    {
      "epoch": 0.04718197237702932,
      "grad_norm": 0.19063429534435272,
      "learning_rate": 0.00019060027919962774,
      "loss": 0.3565,
      "step": 1217
    },
    {
      "epoch": 0.047220741458686696,
      "grad_norm": 0.17181266844272614,
      "learning_rate": 0.00019059252365441292,
      "loss": 0.3006,
      "step": 1218
    },
    {
      "epoch": 0.04725951054034407,
      "grad_norm": 0.18197354674339294,
      "learning_rate": 0.00019058476810919806,
      "loss": 0.3342,
      "step": 1219
    },
    {
      "epoch": 0.047298279622001456,
      "grad_norm": 0.18005476891994476,
      "learning_rate": 0.00019057701256398326,
      "loss": 0.3465,
      "step": 1220
    },
    {
      "epoch": 0.04733704870365883,
      "grad_norm": 0.17664557695388794,
      "learning_rate": 0.0001905692570187684,
      "loss": 0.3291,
      "step": 1221
    },
    {
      "epoch": 0.04737581778531621,
      "grad_norm": 0.16410888731479645,
      "learning_rate": 0.0001905615014735536,
      "loss": 0.3332,
      "step": 1222
    },
    {
      "epoch": 0.04741458686697359,
      "grad_norm": 0.137213334441185,
      "learning_rate": 0.00019055374592833878,
      "loss": 0.2663,
      "step": 1223
    },
    {
      "epoch": 0.04745335594863097,
      "grad_norm": 0.20488937199115753,
      "learning_rate": 0.00019054599038312395,
      "loss": 0.3635,
      "step": 1224
    },
    {
      "epoch": 0.047492125030288344,
      "grad_norm": 0.1392371505498886,
      "learning_rate": 0.00019053823483790913,
      "loss": 0.2616,
      "step": 1225
    },
    {
      "epoch": 0.04753089411194572,
      "grad_norm": 0.23253117501735687,
      "learning_rate": 0.00019053047929269427,
      "loss": 0.3758,
      "step": 1226
    },
    {
      "epoch": 0.047569663193603104,
      "grad_norm": 0.16906534135341644,
      "learning_rate": 0.00019052272374747947,
      "loss": 0.384,
      "step": 1227
    },
    {
      "epoch": 0.04760843227526048,
      "grad_norm": 0.16117817163467407,
      "learning_rate": 0.00019051496820226462,
      "loss": 0.2887,
      "step": 1228
    },
    {
      "epoch": 0.047647201356917857,
      "grad_norm": 0.19880981743335724,
      "learning_rate": 0.00019050721265704982,
      "loss": 0.3505,
      "step": 1229
    },
    {
      "epoch": 0.04768597043857523,
      "grad_norm": 0.21933481097221375,
      "learning_rate": 0.00019049945711183496,
      "loss": 0.4264,
      "step": 1230
    },
    {
      "epoch": 0.047724739520232616,
      "grad_norm": 0.1768646538257599,
      "learning_rate": 0.00019049170156662016,
      "loss": 0.3187,
      "step": 1231
    },
    {
      "epoch": 0.04776350860188999,
      "grad_norm": 0.18963463604450226,
      "learning_rate": 0.0001904839460214053,
      "loss": 0.3547,
      "step": 1232
    },
    {
      "epoch": 0.04780227768354737,
      "grad_norm": 0.16548210382461548,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.2659,
      "step": 1233
    },
    {
      "epoch": 0.04784104676520475,
      "grad_norm": 0.15871888399124146,
      "learning_rate": 0.00019046843493097565,
      "loss": 0.309,
      "step": 1234
    },
    {
      "epoch": 0.04787981584686213,
      "grad_norm": 0.16460128128528595,
      "learning_rate": 0.00019046067938576083,
      "loss": 0.3349,
      "step": 1235
    },
    {
      "epoch": 0.047918584928519505,
      "grad_norm": 0.17455609142780304,
      "learning_rate": 0.000190452923840546,
      "loss": 0.4004,
      "step": 1236
    },
    {
      "epoch": 0.04795735401017688,
      "grad_norm": 0.15920047461986542,
      "learning_rate": 0.00019044516829533117,
      "loss": 0.305,
      "step": 1237
    },
    {
      "epoch": 0.047996123091834264,
      "grad_norm": 0.19471164047718048,
      "learning_rate": 0.00019043741275011635,
      "loss": 0.3167,
      "step": 1238
    },
    {
      "epoch": 0.04803489217349164,
      "grad_norm": 0.20185013115406036,
      "learning_rate": 0.00019042965720490152,
      "loss": 0.3272,
      "step": 1239
    },
    {
      "epoch": 0.04807366125514902,
      "grad_norm": 0.18657146394252777,
      "learning_rate": 0.00019042190165968666,
      "loss": 0.2936,
      "step": 1240
    },
    {
      "epoch": 0.0481124303368064,
      "grad_norm": 0.22405126690864563,
      "learning_rate": 0.00019041414611447186,
      "loss": 0.3333,
      "step": 1241
    },
    {
      "epoch": 0.048151199418463776,
      "grad_norm": 0.2220880538225174,
      "learning_rate": 0.000190406390569257,
      "loss": 0.3593,
      "step": 1242
    },
    {
      "epoch": 0.04818996850012115,
      "grad_norm": 0.21182411909103394,
      "learning_rate": 0.0001903986350240422,
      "loss": 0.3177,
      "step": 1243
    },
    {
      "epoch": 0.04822873758177853,
      "grad_norm": 0.30651721358299255,
      "learning_rate": 0.00019039087947882736,
      "loss": 0.4116,
      "step": 1244
    },
    {
      "epoch": 0.04826750666343591,
      "grad_norm": 0.19227014482021332,
      "learning_rate": 0.00019038312393361256,
      "loss": 0.3079,
      "step": 1245
    },
    {
      "epoch": 0.04830627574509329,
      "grad_norm": 0.17001092433929443,
      "learning_rate": 0.0001903753683883977,
      "loss": 0.3188,
      "step": 1246
    },
    {
      "epoch": 0.048345044826750665,
      "grad_norm": 0.1826089471578598,
      "learning_rate": 0.00019036761284318287,
      "loss": 0.3569,
      "step": 1247
    },
    {
      "epoch": 0.04838381390840805,
      "grad_norm": 0.15501080453395844,
      "learning_rate": 0.00019035985729796805,
      "loss": 0.3126,
      "step": 1248
    },
    {
      "epoch": 0.048422582990065424,
      "grad_norm": 0.1633838266134262,
      "learning_rate": 0.00019035210175275322,
      "loss": 0.3314,
      "step": 1249
    },
    {
      "epoch": 0.0484613520717228,
      "grad_norm": 0.13066717982292175,
      "learning_rate": 0.0001903443462075384,
      "loss": 0.2864,
      "step": 1250
    },
    {
      "epoch": 0.04850012115338018,
      "grad_norm": 0.16148705780506134,
      "learning_rate": 0.00019033659066232356,
      "loss": 0.3157,
      "step": 1251
    },
    {
      "epoch": 0.04853889023503756,
      "grad_norm": 0.1460016667842865,
      "learning_rate": 0.00019032883511710874,
      "loss": 0.227,
      "step": 1252
    },
    {
      "epoch": 0.048577659316694936,
      "grad_norm": 0.1448584347963333,
      "learning_rate": 0.0001903210795718939,
      "loss": 0.2594,
      "step": 1253
    },
    {
      "epoch": 0.04861642839835231,
      "grad_norm": 0.18028372526168823,
      "learning_rate": 0.00019031332402667908,
      "loss": 0.3227,
      "step": 1254
    },
    {
      "epoch": 0.04865519748000969,
      "grad_norm": 0.12801364064216614,
      "learning_rate": 0.00019030556848146426,
      "loss": 0.2253,
      "step": 1255
    },
    {
      "epoch": 0.04869396656166707,
      "grad_norm": 0.1451917290687561,
      "learning_rate": 0.00019029781293624943,
      "loss": 0.2245,
      "step": 1256
    },
    {
      "epoch": 0.04873273564332445,
      "grad_norm": 0.21071840822696686,
      "learning_rate": 0.0001902900573910346,
      "loss": 0.3586,
      "step": 1257
    },
    {
      "epoch": 0.048771504724981825,
      "grad_norm": 0.19537267088890076,
      "learning_rate": 0.00019028230184581977,
      "loss": 0.3494,
      "step": 1258
    },
    {
      "epoch": 0.04881027380663921,
      "grad_norm": 0.14903704822063446,
      "learning_rate": 0.00019027454630060495,
      "loss": 0.2698,
      "step": 1259
    },
    {
      "epoch": 0.048849042888296584,
      "grad_norm": 0.15669582784175873,
      "learning_rate": 0.00019026679075539012,
      "loss": 0.256,
      "step": 1260
    },
    {
      "epoch": 0.04888781196995396,
      "grad_norm": 0.16662058234214783,
      "learning_rate": 0.00019025903521017527,
      "loss": 0.3303,
      "step": 1261
    },
    {
      "epoch": 0.04892658105161134,
      "grad_norm": 0.18479591608047485,
      "learning_rate": 0.00019025127966496047,
      "loss": 0.356,
      "step": 1262
    },
    {
      "epoch": 0.04896535013326872,
      "grad_norm": 0.17089518904685974,
      "learning_rate": 0.0001902435241197456,
      "loss": 0.2737,
      "step": 1263
    },
    {
      "epoch": 0.049004119214926097,
      "grad_norm": 0.18044589459896088,
      "learning_rate": 0.0001902357685745308,
      "loss": 0.3076,
      "step": 1264
    },
    {
      "epoch": 0.04904288829658347,
      "grad_norm": 0.15476714074611664,
      "learning_rate": 0.00019022801302931596,
      "loss": 0.3051,
      "step": 1265
    },
    {
      "epoch": 0.049081657378240856,
      "grad_norm": 0.1581023782491684,
      "learning_rate": 0.00019022025748410116,
      "loss": 0.3468,
      "step": 1266
    },
    {
      "epoch": 0.04912042645989823,
      "grad_norm": 0.21263669431209564,
      "learning_rate": 0.0001902125019388863,
      "loss": 0.3542,
      "step": 1267
    },
    {
      "epoch": 0.04915919554155561,
      "grad_norm": 0.15817980468273163,
      "learning_rate": 0.00019020474639367148,
      "loss": 0.3177,
      "step": 1268
    },
    {
      "epoch": 0.049197964623212985,
      "grad_norm": 0.13836415112018585,
      "learning_rate": 0.00019019699084845665,
      "loss": 0.2751,
      "step": 1269
    },
    {
      "epoch": 0.04923673370487037,
      "grad_norm": 0.13790181279182434,
      "learning_rate": 0.00019018923530324182,
      "loss": 0.2537,
      "step": 1270
    },
    {
      "epoch": 0.049275502786527745,
      "grad_norm": 0.24695490300655365,
      "learning_rate": 0.000190181479758027,
      "loss": 0.3889,
      "step": 1271
    },
    {
      "epoch": 0.04931427186818512,
      "grad_norm": 0.2560611665248871,
      "learning_rate": 0.00019017372421281217,
      "loss": 0.3819,
      "step": 1272
    },
    {
      "epoch": 0.049353040949842504,
      "grad_norm": 0.17487794160842896,
      "learning_rate": 0.00019016596866759734,
      "loss": 0.3021,
      "step": 1273
    },
    {
      "epoch": 0.04939181003149988,
      "grad_norm": 0.15912558138370514,
      "learning_rate": 0.0001901582131223825,
      "loss": 0.228,
      "step": 1274
    },
    {
      "epoch": 0.04943057911315726,
      "grad_norm": 0.1908804178237915,
      "learning_rate": 0.00019015045757716769,
      "loss": 0.3405,
      "step": 1275
    },
    {
      "epoch": 0.04946934819481463,
      "grad_norm": 0.20279784500598907,
      "learning_rate": 0.00019014270203195286,
      "loss": 0.3839,
      "step": 1276
    },
    {
      "epoch": 0.049508117276472016,
      "grad_norm": 0.17023122310638428,
      "learning_rate": 0.00019013494648673803,
      "loss": 0.2987,
      "step": 1277
    },
    {
      "epoch": 0.04954688635812939,
      "grad_norm": 0.2061876654624939,
      "learning_rate": 0.0001901271909415232,
      "loss": 0.3415,
      "step": 1278
    },
    {
      "epoch": 0.04958565543978677,
      "grad_norm": 0.18997205793857574,
      "learning_rate": 0.00019011943539630838,
      "loss": 0.3403,
      "step": 1279
    },
    {
      "epoch": 0.049624424521444145,
      "grad_norm": 0.15664133429527283,
      "learning_rate": 0.00019011167985109355,
      "loss": 0.2962,
      "step": 1280
    },
    {
      "epoch": 0.04966319360310153,
      "grad_norm": 0.2181759774684906,
      "learning_rate": 0.00019010392430587872,
      "loss": 0.3467,
      "step": 1281
    },
    {
      "epoch": 0.049701962684758905,
      "grad_norm": 0.17885245382785797,
      "learning_rate": 0.00019009616876066387,
      "loss": 0.3009,
      "step": 1282
    },
    {
      "epoch": 0.04974073176641628,
      "grad_norm": 0.22574836015701294,
      "learning_rate": 0.00019008841321544907,
      "loss": 0.4163,
      "step": 1283
    },
    {
      "epoch": 0.049779500848073664,
      "grad_norm": 0.1776547133922577,
      "learning_rate": 0.0001900806576702342,
      "loss": 0.2929,
      "step": 1284
    },
    {
      "epoch": 0.04981826992973104,
      "grad_norm": 0.14629323780536652,
      "learning_rate": 0.0001900729021250194,
      "loss": 0.2501,
      "step": 1285
    },
    {
      "epoch": 0.04985703901138842,
      "grad_norm": 0.20735597610473633,
      "learning_rate": 0.00019006514657980456,
      "loss": 0.3469,
      "step": 1286
    },
    {
      "epoch": 0.04989580809304579,
      "grad_norm": 0.17530275881290436,
      "learning_rate": 0.00019005739103458976,
      "loss": 0.3091,
      "step": 1287
    },
    {
      "epoch": 0.049934577174703176,
      "grad_norm": 0.2001752108335495,
      "learning_rate": 0.0001900496354893749,
      "loss": 0.3616,
      "step": 1288
    },
    {
      "epoch": 0.04997334625636055,
      "grad_norm": 0.12920932471752167,
      "learning_rate": 0.00019004187994416008,
      "loss": 0.2333,
      "step": 1289
    },
    {
      "epoch": 0.05001211533801793,
      "grad_norm": 0.16093991696834564,
      "learning_rate": 0.00019003412439894525,
      "loss": 0.315,
      "step": 1290
    },
    {
      "epoch": 0.05005088441967531,
      "grad_norm": 0.1751129925251007,
      "learning_rate": 0.00019002636885373042,
      "loss": 0.3429,
      "step": 1291
    },
    {
      "epoch": 0.05008965350133269,
      "grad_norm": 0.17275142669677734,
      "learning_rate": 0.0001900186133085156,
      "loss": 0.2898,
      "step": 1292
    },
    {
      "epoch": 0.050128422582990065,
      "grad_norm": 0.12404058128595352,
      "learning_rate": 0.00019001085776330077,
      "loss": 0.2254,
      "step": 1293
    },
    {
      "epoch": 0.05016719166464744,
      "grad_norm": 0.1675277203321457,
      "learning_rate": 0.00019000310221808594,
      "loss": 0.3449,
      "step": 1294
    },
    {
      "epoch": 0.050205960746304824,
      "grad_norm": 0.16546915471553802,
      "learning_rate": 0.00018999534667287111,
      "loss": 0.3062,
      "step": 1295
    },
    {
      "epoch": 0.0502447298279622,
      "grad_norm": 0.18585695326328278,
      "learning_rate": 0.00018998759112765626,
      "loss": 0.3337,
      "step": 1296
    },
    {
      "epoch": 0.05028349890961958,
      "grad_norm": 0.1957339197397232,
      "learning_rate": 0.00018997983558244146,
      "loss": 0.3784,
      "step": 1297
    },
    {
      "epoch": 0.05032226799127695,
      "grad_norm": 0.1831463724374771,
      "learning_rate": 0.0001899720800372266,
      "loss": 0.3306,
      "step": 1298
    },
    {
      "epoch": 0.050361037072934337,
      "grad_norm": 0.17590637505054474,
      "learning_rate": 0.0001899643244920118,
      "loss": 0.2673,
      "step": 1299
    },
    {
      "epoch": 0.05039980615459171,
      "grad_norm": 0.24586331844329834,
      "learning_rate": 0.00018995656894679695,
      "loss": 0.3372,
      "step": 1300
    },
    {
      "epoch": 0.05043857523624909,
      "grad_norm": 0.20410355925559998,
      "learning_rate": 0.00018994881340158215,
      "loss": 0.3335,
      "step": 1301
    },
    {
      "epoch": 0.05047734431790647,
      "grad_norm": 0.24012665450572968,
      "learning_rate": 0.00018994105785636732,
      "loss": 0.4014,
      "step": 1302
    },
    {
      "epoch": 0.05051611339956385,
      "grad_norm": 0.13543759286403656,
      "learning_rate": 0.00018993330231115247,
      "loss": 0.2501,
      "step": 1303
    },
    {
      "epoch": 0.050554882481221225,
      "grad_norm": 0.1981927752494812,
      "learning_rate": 0.00018992554676593767,
      "loss": 0.3358,
      "step": 1304
    },
    {
      "epoch": 0.0505936515628786,
      "grad_norm": 0.20860391855239868,
      "learning_rate": 0.00018991779122072282,
      "loss": 0.3958,
      "step": 1305
    },
    {
      "epoch": 0.050632420644535985,
      "grad_norm": 0.18827468156814575,
      "learning_rate": 0.00018991003567550802,
      "loss": 0.3607,
      "step": 1306
    },
    {
      "epoch": 0.05067118972619336,
      "grad_norm": 0.13993577659130096,
      "learning_rate": 0.00018990228013029316,
      "loss": 0.281,
      "step": 1307
    },
    {
      "epoch": 0.05070995880785074,
      "grad_norm": 0.16178344190120697,
      "learning_rate": 0.00018989452458507836,
      "loss": 0.3035,
      "step": 1308
    },
    {
      "epoch": 0.05074872788950812,
      "grad_norm": 0.14175787568092346,
      "learning_rate": 0.0001898867690398635,
      "loss": 0.246,
      "step": 1309
    },
    {
      "epoch": 0.0507874969711655,
      "grad_norm": 0.2209191918373108,
      "learning_rate": 0.00018987901349464868,
      "loss": 0.4196,
      "step": 1310
    },
    {
      "epoch": 0.05082626605282287,
      "grad_norm": 0.177951380610466,
      "learning_rate": 0.00018987125794943385,
      "loss": 0.3329,
      "step": 1311
    },
    {
      "epoch": 0.05086503513448025,
      "grad_norm": 0.18734030425548553,
      "learning_rate": 0.00018986350240421902,
      "loss": 0.3162,
      "step": 1312
    },
    {
      "epoch": 0.05090380421613763,
      "grad_norm": 0.170694038271904,
      "learning_rate": 0.0001898557468590042,
      "loss": 0.312,
      "step": 1313
    },
    {
      "epoch": 0.05094257329779501,
      "grad_norm": 0.19002793729305267,
      "learning_rate": 0.00018984799131378937,
      "loss": 0.314,
      "step": 1314
    },
    {
      "epoch": 0.050981342379452385,
      "grad_norm": 0.16991806030273438,
      "learning_rate": 0.00018984023576857454,
      "loss": 0.2899,
      "step": 1315
    },
    {
      "epoch": 0.05102011146110977,
      "grad_norm": 0.16940492391586304,
      "learning_rate": 0.00018983248022335972,
      "loss": 0.308,
      "step": 1316
    },
    {
      "epoch": 0.051058880542767145,
      "grad_norm": 0.2242240458726883,
      "learning_rate": 0.00018982472467814486,
      "loss": 0.3441,
      "step": 1317
    },
    {
      "epoch": 0.05109764962442452,
      "grad_norm": 0.1627425104379654,
      "learning_rate": 0.00018981696913293006,
      "loss": 0.2514,
      "step": 1318
    },
    {
      "epoch": 0.0511364187060819,
      "grad_norm": 0.1515997052192688,
      "learning_rate": 0.0001898092135877152,
      "loss": 0.244,
      "step": 1319
    },
    {
      "epoch": 0.05117518778773928,
      "grad_norm": 0.1635696142911911,
      "learning_rate": 0.0001898014580425004,
      "loss": 0.2999,
      "step": 1320
    },
    {
      "epoch": 0.05121395686939666,
      "grad_norm": 0.22844088077545166,
      "learning_rate": 0.00018979370249728555,
      "loss": 0.4236,
      "step": 1321
    },
    {
      "epoch": 0.05125272595105403,
      "grad_norm": 0.17745840549468994,
      "learning_rate": 0.00018978594695207075,
      "loss": 0.3013,
      "step": 1322
    },
    {
      "epoch": 0.05129149503271141,
      "grad_norm": 0.18008261919021606,
      "learning_rate": 0.0001897781914068559,
      "loss": 0.2831,
      "step": 1323
    },
    {
      "epoch": 0.05133026411436879,
      "grad_norm": 0.20957383513450623,
      "learning_rate": 0.00018977043586164107,
      "loss": 0.3259,
      "step": 1324
    },
    {
      "epoch": 0.05136903319602617,
      "grad_norm": 0.18979234993457794,
      "learning_rate": 0.00018976268031642624,
      "loss": 0.2777,
      "step": 1325
    },
    {
      "epoch": 0.051407802277683545,
      "grad_norm": 0.15106958150863647,
      "learning_rate": 0.00018975492477121142,
      "loss": 0.2403,
      "step": 1326
    },
    {
      "epoch": 0.05144657135934093,
      "grad_norm": 0.18468241393566132,
      "learning_rate": 0.0001897471692259966,
      "loss": 0.2979,
      "step": 1327
    },
    {
      "epoch": 0.051485340440998305,
      "grad_norm": 0.155146524310112,
      "learning_rate": 0.00018973941368078176,
      "loss": 0.2419,
      "step": 1328
    },
    {
      "epoch": 0.05152410952265568,
      "grad_norm": 0.15052299201488495,
      "learning_rate": 0.00018973165813556694,
      "loss": 0.2255,
      "step": 1329
    },
    {
      "epoch": 0.05156287860431306,
      "grad_norm": 0.20551973581314087,
      "learning_rate": 0.0001897239025903521,
      "loss": 0.295,
      "step": 1330
    },
    {
      "epoch": 0.05160164768597044,
      "grad_norm": 0.14707614481449127,
      "learning_rate": 0.00018971614704513728,
      "loss": 0.2557,
      "step": 1331
    },
    {
      "epoch": 0.05164041676762782,
      "grad_norm": 0.16039146482944489,
      "learning_rate": 0.00018970839149992245,
      "loss": 0.2481,
      "step": 1332
    },
    {
      "epoch": 0.05167918584928519,
      "grad_norm": 0.17816220223903656,
      "learning_rate": 0.00018970063595470763,
      "loss": 0.2913,
      "step": 1333
    },
    {
      "epoch": 0.051717954930942577,
      "grad_norm": 0.17388388514518738,
      "learning_rate": 0.0001896928804094928,
      "loss": 0.2739,
      "step": 1334
    },
    {
      "epoch": 0.05175672401259995,
      "grad_norm": 0.16724155843257904,
      "learning_rate": 0.00018968512486427797,
      "loss": 0.2599,
      "step": 1335
    },
    {
      "epoch": 0.05179549309425733,
      "grad_norm": 0.15556475520133972,
      "learning_rate": 0.00018967736931906315,
      "loss": 0.2845,
      "step": 1336
    },
    {
      "epoch": 0.051834262175914705,
      "grad_norm": 0.20936445891857147,
      "learning_rate": 0.00018966961377384832,
      "loss": 0.3902,
      "step": 1337
    },
    {
      "epoch": 0.05187303125757209,
      "grad_norm": 0.15699343383312225,
      "learning_rate": 0.00018966185822863346,
      "loss": 0.2966,
      "step": 1338
    },
    {
      "epoch": 0.051911800339229465,
      "grad_norm": 0.21308015286922455,
      "learning_rate": 0.00018965410268341866,
      "loss": 0.3595,
      "step": 1339
    },
    {
      "epoch": 0.05195056942088684,
      "grad_norm": 0.16551658511161804,
      "learning_rate": 0.0001896463471382038,
      "loss": 0.3381,
      "step": 1340
    },
    {
      "epoch": 0.05198933850254422,
      "grad_norm": 0.16302041709423065,
      "learning_rate": 0.000189638591592989,
      "loss": 0.3288,
      "step": 1341
    },
    {
      "epoch": 0.0520281075842016,
      "grad_norm": 0.1772579699754715,
      "learning_rate": 0.00018963083604777415,
      "loss": 0.3028,
      "step": 1342
    },
    {
      "epoch": 0.05206687666585898,
      "grad_norm": 0.19723311066627502,
      "learning_rate": 0.00018962308050255935,
      "loss": 0.3586,
      "step": 1343
    },
    {
      "epoch": 0.05210564574751635,
      "grad_norm": 0.14796856045722961,
      "learning_rate": 0.0001896153249573445,
      "loss": 0.2652,
      "step": 1344
    },
    {
      "epoch": 0.05214441482917374,
      "grad_norm": 0.1689416468143463,
      "learning_rate": 0.00018960756941212967,
      "loss": 0.2443,
      "step": 1345
    },
    {
      "epoch": 0.05218318391083111,
      "grad_norm": 0.18280380964279175,
      "learning_rate": 0.00018959981386691485,
      "loss": 0.2492,
      "step": 1346
    },
    {
      "epoch": 0.05222195299248849,
      "grad_norm": 0.21145905554294586,
      "learning_rate": 0.00018959205832170002,
      "loss": 0.2558,
      "step": 1347
    },
    {
      "epoch": 0.052260722074145866,
      "grad_norm": 0.21578148007392883,
      "learning_rate": 0.0001895843027764852,
      "loss": 0.3504,
      "step": 1348
    },
    {
      "epoch": 0.05229949115580325,
      "grad_norm": 0.17066830396652222,
      "learning_rate": 0.00018957654723127036,
      "loss": 0.2193,
      "step": 1349
    },
    {
      "epoch": 0.052338260237460625,
      "grad_norm": 0.20327846705913544,
      "learning_rate": 0.00018956879168605554,
      "loss": 0.2986,
      "step": 1350
    },
    {
      "epoch": 0.052377029319118,
      "grad_norm": 0.2529442310333252,
      "learning_rate": 0.0001895610361408407,
      "loss": 0.3823,
      "step": 1351
    },
    {
      "epoch": 0.052415798400775385,
      "grad_norm": 0.23669663071632385,
      "learning_rate": 0.00018955328059562588,
      "loss": 0.32,
      "step": 1352
    },
    {
      "epoch": 0.05245456748243276,
      "grad_norm": 0.22779057919979095,
      "learning_rate": 0.00018954552505041106,
      "loss": 0.3349,
      "step": 1353
    },
    {
      "epoch": 0.05249333656409014,
      "grad_norm": 0.19589386880397797,
      "learning_rate": 0.00018953776950519623,
      "loss": 0.3107,
      "step": 1354
    },
    {
      "epoch": 0.052532105645747514,
      "grad_norm": 0.26917433738708496,
      "learning_rate": 0.0001895300139599814,
      "loss": 0.3535,
      "step": 1355
    },
    {
      "epoch": 0.0525708747274049,
      "grad_norm": 0.1939793825149536,
      "learning_rate": 0.00018952225841476657,
      "loss": 0.3001,
      "step": 1356
    },
    {
      "epoch": 0.05260964380906227,
      "grad_norm": 0.16025611758232117,
      "learning_rate": 0.00018951450286955175,
      "loss": 0.3595,
      "step": 1357
    },
    {
      "epoch": 0.05264841289071965,
      "grad_norm": 0.17203302681446075,
      "learning_rate": 0.00018950674732433692,
      "loss": 0.3418,
      "step": 1358
    },
    {
      "epoch": 0.05268718197237703,
      "grad_norm": 0.14415614306926727,
      "learning_rate": 0.00018949899177912207,
      "loss": 0.2325,
      "step": 1359
    },
    {
      "epoch": 0.05272595105403441,
      "grad_norm": 0.16397510468959808,
      "learning_rate": 0.00018949123623390727,
      "loss": 0.3177,
      "step": 1360
    },
    {
      "epoch": 0.052764720135691785,
      "grad_norm": 0.16007602214813232,
      "learning_rate": 0.0001894834806886924,
      "loss": 0.3187,
      "step": 1361
    },
    {
      "epoch": 0.05280348921734916,
      "grad_norm": 0.14972439408302307,
      "learning_rate": 0.0001894757251434776,
      "loss": 0.2623,
      "step": 1362
    },
    {
      "epoch": 0.052842258299006545,
      "grad_norm": 0.1434333473443985,
      "learning_rate": 0.00018946796959826276,
      "loss": 0.2587,
      "step": 1363
    },
    {
      "epoch": 0.05288102738066392,
      "grad_norm": 0.18780848383903503,
      "learning_rate": 0.00018946021405304796,
      "loss": 0.2996,
      "step": 1364
    },
    {
      "epoch": 0.0529197964623213,
      "grad_norm": 0.20752181112766266,
      "learning_rate": 0.0001894524585078331,
      "loss": 0.3087,
      "step": 1365
    },
    {
      "epoch": 0.052958565543978674,
      "grad_norm": 0.12826867401599884,
      "learning_rate": 0.00018944470296261827,
      "loss": 0.203,
      "step": 1366
    },
    {
      "epoch": 0.05299733462563606,
      "grad_norm": 0.23416803777217865,
      "learning_rate": 0.00018943694741740345,
      "loss": 0.3768,
      "step": 1367
    },
    {
      "epoch": 0.05303610370729343,
      "grad_norm": 0.16843317449092865,
      "learning_rate": 0.00018942919187218862,
      "loss": 0.2871,
      "step": 1368
    },
    {
      "epoch": 0.05307487278895081,
      "grad_norm": 0.22923406958580017,
      "learning_rate": 0.0001894214363269738,
      "loss": 0.3743,
      "step": 1369
    },
    {
      "epoch": 0.05311364187060819,
      "grad_norm": 0.20202332735061646,
      "learning_rate": 0.00018941368078175897,
      "loss": 0.2635,
      "step": 1370
    },
    {
      "epoch": 0.05315241095226557,
      "grad_norm": 0.20325644314289093,
      "learning_rate": 0.00018940592523654414,
      "loss": 0.3078,
      "step": 1371
    },
    {
      "epoch": 0.053191180033922945,
      "grad_norm": 0.2458471953868866,
      "learning_rate": 0.0001893981696913293,
      "loss": 0.3957,
      "step": 1372
    },
    {
      "epoch": 0.05322994911558032,
      "grad_norm": 0.2075236290693283,
      "learning_rate": 0.00018939041414611446,
      "loss": 0.2985,
      "step": 1373
    },
    {
      "epoch": 0.053268718197237705,
      "grad_norm": 0.2033800631761551,
      "learning_rate": 0.00018938265860089966,
      "loss": 0.2894,
      "step": 1374
    },
    {
      "epoch": 0.05330748727889508,
      "grad_norm": 0.23658409714698792,
      "learning_rate": 0.0001893749030556848,
      "loss": 0.3675,
      "step": 1375
    },
    {
      "epoch": 0.05334625636055246,
      "grad_norm": 0.2150202840566635,
      "learning_rate": 0.00018936714751047,
      "loss": 0.3404,
      "step": 1376
    },
    {
      "epoch": 0.05338502544220984,
      "grad_norm": 0.2176586389541626,
      "learning_rate": 0.00018935939196525515,
      "loss": 0.3474,
      "step": 1377
    },
    {
      "epoch": 0.05342379452386722,
      "grad_norm": 0.18821991980075836,
      "learning_rate": 0.00018935163642004035,
      "loss": 0.2688,
      "step": 1378
    },
    {
      "epoch": 0.05346256360552459,
      "grad_norm": 0.234449565410614,
      "learning_rate": 0.0001893438808748255,
      "loss": 0.3183,
      "step": 1379
    },
    {
      "epoch": 0.05350133268718197,
      "grad_norm": 0.20574167370796204,
      "learning_rate": 0.00018933612532961067,
      "loss": 0.3623,
      "step": 1380
    },
    {
      "epoch": 0.05354010176883935,
      "grad_norm": 0.20114897191524506,
      "learning_rate": 0.00018932836978439587,
      "loss": 0.3689,
      "step": 1381
    },
    {
      "epoch": 0.05357887085049673,
      "grad_norm": 0.20351974666118622,
      "learning_rate": 0.000189320614239181,
      "loss": 0.3797,
      "step": 1382
    },
    {
      "epoch": 0.053617639932154106,
      "grad_norm": 0.1368079036474228,
      "learning_rate": 0.0001893128586939662,
      "loss": 0.2303,
      "step": 1383
    },
    {
      "epoch": 0.05365640901381149,
      "grad_norm": 0.17817911505699158,
      "learning_rate": 0.00018930510314875136,
      "loss": 0.2844,
      "step": 1384
    },
    {
      "epoch": 0.053695178095468865,
      "grad_norm": 0.18145756423473358,
      "learning_rate": 0.00018929734760353656,
      "loss": 0.3076,
      "step": 1385
    },
    {
      "epoch": 0.05373394717712624,
      "grad_norm": 0.2045907974243164,
      "learning_rate": 0.0001892895920583217,
      "loss": 0.2969,
      "step": 1386
    },
    {
      "epoch": 0.05377271625878362,
      "grad_norm": 0.2344735711812973,
      "learning_rate": 0.0001892818365131069,
      "loss": 0.3496,
      "step": 1387
    },
    {
      "epoch": 0.053811485340441,
      "grad_norm": 0.21367940306663513,
      "learning_rate": 0.00018927408096789205,
      "loss": 0.2809,
      "step": 1388
    },
    {
      "epoch": 0.05385025442209838,
      "grad_norm": 0.24980802834033966,
      "learning_rate": 0.00018926632542267722,
      "loss": 0.3566,
      "step": 1389
    },
    {
      "epoch": 0.053889023503755754,
      "grad_norm": 0.23350927233695984,
      "learning_rate": 0.0001892585698774624,
      "loss": 0.2845,
      "step": 1390
    },
    {
      "epoch": 0.05392779258541313,
      "grad_norm": 0.2522452473640442,
      "learning_rate": 0.00018925081433224757,
      "loss": 0.256,
      "step": 1391
    },
    {
      "epoch": 0.05396656166707051,
      "grad_norm": 0.2158960998058319,
      "learning_rate": 0.00018924305878703274,
      "loss": 0.3346,
      "step": 1392
    },
    {
      "epoch": 0.05400533074872789,
      "grad_norm": 0.15956881642341614,
      "learning_rate": 0.00018923530324181791,
      "loss": 0.2294,
      "step": 1393
    },
    {
      "epoch": 0.054044099830385266,
      "grad_norm": 0.1909416913986206,
      "learning_rate": 0.0001892275476966031,
      "loss": 0.3434,
      "step": 1394
    },
    {
      "epoch": 0.05408286891204265,
      "grad_norm": 0.1329880952835083,
      "learning_rate": 0.00018921979215138826,
      "loss": 0.2807,
      "step": 1395
    },
    {
      "epoch": 0.054121637993700025,
      "grad_norm": 0.17696791887283325,
      "learning_rate": 0.0001892120366061734,
      "loss": 0.3615,
      "step": 1396
    },
    {
      "epoch": 0.0541604070753574,
      "grad_norm": 0.1617465615272522,
      "learning_rate": 0.0001892042810609586,
      "loss": 0.2967,
      "step": 1397
    },
    {
      "epoch": 0.05419917615701478,
      "grad_norm": 0.16582277417182922,
      "learning_rate": 0.00018919652551574375,
      "loss": 0.2826,
      "step": 1398
    },
    {
      "epoch": 0.05423794523867216,
      "grad_norm": 0.1981358379125595,
      "learning_rate": 0.00018918876997052895,
      "loss": 0.3398,
      "step": 1399
    },
    {
      "epoch": 0.05427671432032954,
      "grad_norm": 0.19727906584739685,
      "learning_rate": 0.0001891810144253141,
      "loss": 0.3463,
      "step": 1400
    },
    {
      "epoch": 0.054315483401986914,
      "grad_norm": 0.17561165988445282,
      "learning_rate": 0.0001891732588800993,
      "loss": 0.2992,
      "step": 1401
    },
    {
      "epoch": 0.0543542524836443,
      "grad_norm": 0.15695297718048096,
      "learning_rate": 0.00018916550333488444,
      "loss": 0.2373,
      "step": 1402
    },
    {
      "epoch": 0.05439302156530167,
      "grad_norm": 0.17241403460502625,
      "learning_rate": 0.00018915774778966961,
      "loss": 0.2718,
      "step": 1403
    },
    {
      "epoch": 0.05443179064695905,
      "grad_norm": 0.18658863008022308,
      "learning_rate": 0.0001891499922444548,
      "loss": 0.306,
      "step": 1404
    },
    {
      "epoch": 0.054470559728616426,
      "grad_norm": 0.2585308253765106,
      "learning_rate": 0.00018914223669923996,
      "loss": 0.4245,
      "step": 1405
    },
    {
      "epoch": 0.05450932881027381,
      "grad_norm": 0.18255086243152618,
      "learning_rate": 0.00018913448115402513,
      "loss": 0.3119,
      "step": 1406
    },
    {
      "epoch": 0.054548097891931185,
      "grad_norm": 0.22600263357162476,
      "learning_rate": 0.0001891267256088103,
      "loss": 0.4171,
      "step": 1407
    },
    {
      "epoch": 0.05458686697358856,
      "grad_norm": 0.2158760279417038,
      "learning_rate": 0.00018911897006359548,
      "loss": 0.2269,
      "step": 1408
    },
    {
      "epoch": 0.05462563605524594,
      "grad_norm": 0.1594916135072708,
      "learning_rate": 0.00018911121451838065,
      "loss": 0.362,
      "step": 1409
    },
    {
      "epoch": 0.05466440513690332,
      "grad_norm": 0.22543011605739594,
      "learning_rate": 0.00018910345897316582,
      "loss": 0.3163,
      "step": 1410
    },
    {
      "epoch": 0.0547031742185607,
      "grad_norm": 0.13784946501255035,
      "learning_rate": 0.000189095703427951,
      "loss": 0.2454,
      "step": 1411
    },
    {
      "epoch": 0.054741943300218074,
      "grad_norm": 0.1492028683423996,
      "learning_rate": 0.00018908794788273617,
      "loss": 0.343,
      "step": 1412
    },
    {
      "epoch": 0.05478071238187546,
      "grad_norm": 0.17985109984874725,
      "learning_rate": 0.00018908019233752134,
      "loss": 0.3285,
      "step": 1413
    },
    {
      "epoch": 0.05481948146353283,
      "grad_norm": 0.17583374679088593,
      "learning_rate": 0.00018907243679230652,
      "loss": 0.2803,
      "step": 1414
    },
    {
      "epoch": 0.05485825054519021,
      "grad_norm": 0.20695646107196808,
      "learning_rate": 0.0001890646812470917,
      "loss": 0.3354,
      "step": 1415
    },
    {
      "epoch": 0.054897019626847586,
      "grad_norm": 0.20717164874076843,
      "learning_rate": 0.00018905692570187686,
      "loss": 0.3136,
      "step": 1416
    },
    {
      "epoch": 0.05493578870850497,
      "grad_norm": 0.1636844277381897,
      "learning_rate": 0.000189049170156662,
      "loss": 0.2292,
      "step": 1417
    },
    {
      "epoch": 0.054974557790162346,
      "grad_norm": 0.20398342609405518,
      "learning_rate": 0.0001890414146114472,
      "loss": 0.2981,
      "step": 1418
    },
    {
      "epoch": 0.05501332687181972,
      "grad_norm": 0.19948488473892212,
      "learning_rate": 0.00018903365906623235,
      "loss": 0.2981,
      "step": 1419
    },
    {
      "epoch": 0.055052095953477105,
      "grad_norm": 0.1989673227071762,
      "learning_rate": 0.00018902590352101755,
      "loss": 0.2881,
      "step": 1420
    },
    {
      "epoch": 0.05509086503513448,
      "grad_norm": 0.17202065885066986,
      "learning_rate": 0.0001890181479758027,
      "loss": 0.2394,
      "step": 1421
    },
    {
      "epoch": 0.05512963411679186,
      "grad_norm": 0.23608863353729248,
      "learning_rate": 0.0001890103924305879,
      "loss": 0.2859,
      "step": 1422
    },
    {
      "epoch": 0.055168403198449234,
      "grad_norm": 0.197374165058136,
      "learning_rate": 0.00018900263688537304,
      "loss": 0.2566,
      "step": 1423
    },
    {
      "epoch": 0.05520717228010662,
      "grad_norm": 0.2717880308628082,
      "learning_rate": 0.00018899488134015822,
      "loss": 0.4285,
      "step": 1424
    },
    {
      "epoch": 0.055245941361763994,
      "grad_norm": 0.18742436170578003,
      "learning_rate": 0.0001889871257949434,
      "loss": 0.2937,
      "step": 1425
    },
    {
      "epoch": 0.05528471044342137,
      "grad_norm": 0.18468771874904633,
      "learning_rate": 0.00018897937024972856,
      "loss": 0.2782,
      "step": 1426
    },
    {
      "epoch": 0.05532347952507875,
      "grad_norm": 0.19010260701179504,
      "learning_rate": 0.00018897161470451373,
      "loss": 0.3066,
      "step": 1427
    },
    {
      "epoch": 0.05536224860673613,
      "grad_norm": 0.23756074905395508,
      "learning_rate": 0.0001889638591592989,
      "loss": 0.284,
      "step": 1428
    },
    {
      "epoch": 0.055401017688393506,
      "grad_norm": 0.22752335667610168,
      "learning_rate": 0.00018895610361408408,
      "loss": 0.3333,
      "step": 1429
    },
    {
      "epoch": 0.05543978677005088,
      "grad_norm": 0.19717687368392944,
      "learning_rate": 0.00018894834806886925,
      "loss": 0.3421,
      "step": 1430
    },
    {
      "epoch": 0.055478555851708265,
      "grad_norm": 0.19330056011676788,
      "learning_rate": 0.00018894059252365443,
      "loss": 0.266,
      "step": 1431
    },
    {
      "epoch": 0.05551732493336564,
      "grad_norm": 0.18976359069347382,
      "learning_rate": 0.0001889328369784396,
      "loss": 0.3007,
      "step": 1432
    },
    {
      "epoch": 0.05555609401502302,
      "grad_norm": 0.18390481173992157,
      "learning_rate": 0.00018892508143322477,
      "loss": 0.2642,
      "step": 1433
    },
    {
      "epoch": 0.055594863096680394,
      "grad_norm": 0.24295777082443237,
      "learning_rate": 0.00018891732588800994,
      "loss": 0.3097,
      "step": 1434
    },
    {
      "epoch": 0.05563363217833778,
      "grad_norm": 0.21120306849479675,
      "learning_rate": 0.00018890957034279512,
      "loss": 0.3071,
      "step": 1435
    },
    {
      "epoch": 0.055672401259995154,
      "grad_norm": 0.23674391210079193,
      "learning_rate": 0.0001889018147975803,
      "loss": 0.2429,
      "step": 1436
    },
    {
      "epoch": 0.05571117034165253,
      "grad_norm": 0.1947356015443802,
      "learning_rate": 0.00018889405925236546,
      "loss": 0.2969,
      "step": 1437
    },
    {
      "epoch": 0.05574993942330991,
      "grad_norm": 0.25112608075141907,
      "learning_rate": 0.0001888863037071506,
      "loss": 0.3137,
      "step": 1438
    },
    {
      "epoch": 0.05578870850496729,
      "grad_norm": 0.18128472566604614,
      "learning_rate": 0.0001888785481619358,
      "loss": 0.3039,
      "step": 1439
    },
    {
      "epoch": 0.055827477586624666,
      "grad_norm": 0.14829061925411224,
      "learning_rate": 0.00018887079261672095,
      "loss": 0.2426,
      "step": 1440
    },
    {
      "epoch": 0.05586624666828204,
      "grad_norm": 0.20314206182956696,
      "learning_rate": 0.00018886303707150615,
      "loss": 0.2875,
      "step": 1441
    },
    {
      "epoch": 0.055905015749939425,
      "grad_norm": 0.2090146392583847,
      "learning_rate": 0.0001888552815262913,
      "loss": 0.3236,
      "step": 1442
    },
    {
      "epoch": 0.0559437848315968,
      "grad_norm": 0.20519885420799255,
      "learning_rate": 0.0001888475259810765,
      "loss": 0.2442,
      "step": 1443
    },
    {
      "epoch": 0.05598255391325418,
      "grad_norm": 0.28762173652648926,
      "learning_rate": 0.00018883977043586165,
      "loss": 0.3943,
      "step": 1444
    },
    {
      "epoch": 0.05602132299491156,
      "grad_norm": 0.18879619240760803,
      "learning_rate": 0.00018883201489064682,
      "loss": 0.2665,
      "step": 1445
    },
    {
      "epoch": 0.05606009207656894,
      "grad_norm": 0.2192763090133667,
      "learning_rate": 0.000188824259345432,
      "loss": 0.2675,
      "step": 1446
    },
    {
      "epoch": 0.056098861158226314,
      "grad_norm": 0.2894446551799774,
      "learning_rate": 0.00018881650380021716,
      "loss": 0.2972,
      "step": 1447
    },
    {
      "epoch": 0.05613763023988369,
      "grad_norm": 0.27178284525871277,
      "learning_rate": 0.00018880874825500234,
      "loss": 0.3622,
      "step": 1448
    },
    {
      "epoch": 0.05617639932154107,
      "grad_norm": 0.21440182626247406,
      "learning_rate": 0.0001888009927097875,
      "loss": 0.3124,
      "step": 1449
    },
    {
      "epoch": 0.05621516840319845,
      "grad_norm": 0.2191777527332306,
      "learning_rate": 0.00018879323716457268,
      "loss": 0.3207,
      "step": 1450
    },
    {
      "epoch": 0.056253937484855826,
      "grad_norm": 0.1854175180196762,
      "learning_rate": 0.00018878548161935786,
      "loss": 0.3194,
      "step": 1451
    },
    {
      "epoch": 0.0562927065665132,
      "grad_norm": 0.19878287613391876,
      "learning_rate": 0.000188777726074143,
      "loss": 0.3591,
      "step": 1452
    },
    {
      "epoch": 0.056331475648170586,
      "grad_norm": 0.1937853842973709,
      "learning_rate": 0.0001887699705289282,
      "loss": 0.3316,
      "step": 1453
    },
    {
      "epoch": 0.05637024472982796,
      "grad_norm": 0.15454483032226562,
      "learning_rate": 0.00018876221498371335,
      "loss": 0.246,
      "step": 1454
    },
    {
      "epoch": 0.05640901381148534,
      "grad_norm": 0.15625861287117004,
      "learning_rate": 0.00018875445943849855,
      "loss": 0.2933,
      "step": 1455
    },
    {
      "epoch": 0.05644778289314272,
      "grad_norm": 0.15011276304721832,
      "learning_rate": 0.0001887467038932837,
      "loss": 0.2266,
      "step": 1456
    },
    {
      "epoch": 0.0564865519748001,
      "grad_norm": 0.23505859076976776,
      "learning_rate": 0.0001887389483480689,
      "loss": 0.3989,
      "step": 1457
    },
    {
      "epoch": 0.056525321056457474,
      "grad_norm": 0.21374914050102234,
      "learning_rate": 0.00018873119280285404,
      "loss": 0.2827,
      "step": 1458
    },
    {
      "epoch": 0.05656409013811485,
      "grad_norm": 0.21210822463035583,
      "learning_rate": 0.0001887234372576392,
      "loss": 0.3238,
      "step": 1459
    },
    {
      "epoch": 0.056602859219772234,
      "grad_norm": 0.2017086297273636,
      "learning_rate": 0.0001887156817124244,
      "loss": 0.312,
      "step": 1460
    },
    {
      "epoch": 0.05664162830142961,
      "grad_norm": 0.18037021160125732,
      "learning_rate": 0.00018870792616720956,
      "loss": 0.2458,
      "step": 1461
    },
    {
      "epoch": 0.056680397383086986,
      "grad_norm": 0.23326140642166138,
      "learning_rate": 0.00018870017062199476,
      "loss": 0.3623,
      "step": 1462
    },
    {
      "epoch": 0.05671916646474437,
      "grad_norm": 0.20418982207775116,
      "learning_rate": 0.0001886924150767799,
      "loss": 0.3326,
      "step": 1463
    },
    {
      "epoch": 0.056757935546401746,
      "grad_norm": 0.20623236894607544,
      "learning_rate": 0.0001886846595315651,
      "loss": 0.2984,
      "step": 1464
    },
    {
      "epoch": 0.05679670462805912,
      "grad_norm": 0.23427072167396545,
      "learning_rate": 0.00018867690398635025,
      "loss": 0.4006,
      "step": 1465
    },
    {
      "epoch": 0.0568354737097165,
      "grad_norm": 0.17063787579536438,
      "learning_rate": 0.00018866914844113542,
      "loss": 0.2723,
      "step": 1466
    },
    {
      "epoch": 0.05687424279137388,
      "grad_norm": 0.19662374258041382,
      "learning_rate": 0.0001886613928959206,
      "loss": 0.335,
      "step": 1467
    },
    {
      "epoch": 0.05691301187303126,
      "grad_norm": 0.22839714586734772,
      "learning_rate": 0.00018865363735070577,
      "loss": 0.3358,
      "step": 1468
    },
    {
      "epoch": 0.056951780954688634,
      "grad_norm": 0.16599412262439728,
      "learning_rate": 0.00018864588180549094,
      "loss": 0.2472,
      "step": 1469
    },
    {
      "epoch": 0.05699055003634602,
      "grad_norm": 0.20197799801826477,
      "learning_rate": 0.0001886381262602761,
      "loss": 0.3121,
      "step": 1470
    },
    {
      "epoch": 0.057029319118003394,
      "grad_norm": 0.16446377336978912,
      "learning_rate": 0.00018863037071506128,
      "loss": 0.2651,
      "step": 1471
    },
    {
      "epoch": 0.05706808819966077,
      "grad_norm": 0.24076944589614868,
      "learning_rate": 0.00018862261516984646,
      "loss": 0.2913,
      "step": 1472
    },
    {
      "epoch": 0.057106857281318146,
      "grad_norm": 0.30987924337387085,
      "learning_rate": 0.0001886148596246316,
      "loss": 0.3656,
      "step": 1473
    },
    {
      "epoch": 0.05714562636297553,
      "grad_norm": 0.2554345428943634,
      "learning_rate": 0.0001886071040794168,
      "loss": 0.3705,
      "step": 1474
    },
    {
      "epoch": 0.057184395444632906,
      "grad_norm": 0.15628565847873688,
      "learning_rate": 0.00018859934853420195,
      "loss": 0.2307,
      "step": 1475
    },
    {
      "epoch": 0.05722316452629028,
      "grad_norm": 0.1806262582540512,
      "learning_rate": 0.00018859159298898715,
      "loss": 0.3003,
      "step": 1476
    },
    {
      "epoch": 0.05726193360794766,
      "grad_norm": 0.20776838064193726,
      "learning_rate": 0.0001885838374437723,
      "loss": 0.3138,
      "step": 1477
    },
    {
      "epoch": 0.05730070268960504,
      "grad_norm": 0.19051994383335114,
      "learning_rate": 0.0001885760818985575,
      "loss": 0.2895,
      "step": 1478
    },
    {
      "epoch": 0.05733947177126242,
      "grad_norm": 0.21753840148448944,
      "learning_rate": 0.00018856832635334264,
      "loss": 0.3718,
      "step": 1479
    },
    {
      "epoch": 0.057378240852919794,
      "grad_norm": 0.14692917466163635,
      "learning_rate": 0.0001885605708081278,
      "loss": 0.2212,
      "step": 1480
    },
    {
      "epoch": 0.05741700993457718,
      "grad_norm": 0.19357147812843323,
      "learning_rate": 0.00018855281526291299,
      "loss": 0.2678,
      "step": 1481
    },
    {
      "epoch": 0.057455779016234554,
      "grad_norm": 0.18643149733543396,
      "learning_rate": 0.00018854505971769816,
      "loss": 0.3082,
      "step": 1482
    },
    {
      "epoch": 0.05749454809789193,
      "grad_norm": 0.21336336433887482,
      "learning_rate": 0.00018853730417248333,
      "loss": 0.288,
      "step": 1483
    },
    {
      "epoch": 0.057533317179549306,
      "grad_norm": 0.20105569064617157,
      "learning_rate": 0.0001885295486272685,
      "loss": 0.3082,
      "step": 1484
    },
    {
      "epoch": 0.05757208626120669,
      "grad_norm": 0.2347669005393982,
      "learning_rate": 0.00018852179308205368,
      "loss": 0.3413,
      "step": 1485
    },
    {
      "epoch": 0.057610855342864066,
      "grad_norm": 0.19608043134212494,
      "learning_rate": 0.00018851403753683885,
      "loss": 0.3126,
      "step": 1486
    },
    {
      "epoch": 0.05764962442452144,
      "grad_norm": 0.1488513946533203,
      "learning_rate": 0.00018850628199162402,
      "loss": 0.2117,
      "step": 1487
    },
    {
      "epoch": 0.057688393506178826,
      "grad_norm": 0.26026907563209534,
      "learning_rate": 0.0001884985264464092,
      "loss": 0.3393,
      "step": 1488
    },
    {
      "epoch": 0.0577271625878362,
      "grad_norm": 0.2394944578409195,
      "learning_rate": 0.00018849077090119437,
      "loss": 0.3582,
      "step": 1489
    },
    {
      "epoch": 0.05776593166949358,
      "grad_norm": 0.21052028238773346,
      "learning_rate": 0.00018848301535597954,
      "loss": 0.2814,
      "step": 1490
    },
    {
      "epoch": 0.057804700751150954,
      "grad_norm": 0.20522424578666687,
      "learning_rate": 0.0001884752598107647,
      "loss": 0.251,
      "step": 1491
    },
    {
      "epoch": 0.05784346983280834,
      "grad_norm": 0.21351952850818634,
      "learning_rate": 0.00018846750426554989,
      "loss": 0.3359,
      "step": 1492
    },
    {
      "epoch": 0.057882238914465714,
      "grad_norm": 0.18160541355609894,
      "learning_rate": 0.00018845974872033506,
      "loss": 0.3247,
      "step": 1493
    },
    {
      "epoch": 0.05792100799612309,
      "grad_norm": 0.16872131824493408,
      "learning_rate": 0.0001884519931751202,
      "loss": 0.2926,
      "step": 1494
    },
    {
      "epoch": 0.05795977707778047,
      "grad_norm": 0.21096043288707733,
      "learning_rate": 0.0001884442376299054,
      "loss": 0.3346,
      "step": 1495
    },
    {
      "epoch": 0.05799854615943785,
      "grad_norm": 0.28068262338638306,
      "learning_rate": 0.00018843648208469055,
      "loss": 0.4588,
      "step": 1496
    },
    {
      "epoch": 0.058037315241095226,
      "grad_norm": 0.15142908692359924,
      "learning_rate": 0.00018842872653947575,
      "loss": 0.2499,
      "step": 1497
    },
    {
      "epoch": 0.0580760843227526,
      "grad_norm": 0.16465206444263458,
      "learning_rate": 0.0001884209709942609,
      "loss": 0.2617,
      "step": 1498
    },
    {
      "epoch": 0.058114853404409986,
      "grad_norm": 0.22718006372451782,
      "learning_rate": 0.0001884132154490461,
      "loss": 0.3084,
      "step": 1499
    },
    {
      "epoch": 0.05815362248606736,
      "grad_norm": 0.17782579362392426,
      "learning_rate": 0.00018840545990383124,
      "loss": 0.3056,
      "step": 1500
    },
    {
      "epoch": 0.05819239156772474,
      "grad_norm": 0.2268139123916626,
      "learning_rate": 0.00018839770435861641,
      "loss": 0.3159,
      "step": 1501
    },
    {
      "epoch": 0.058231160649382115,
      "grad_norm": 0.19731204211711884,
      "learning_rate": 0.0001883899488134016,
      "loss": 0.3037,
      "step": 1502
    },
    {
      "epoch": 0.0582699297310395,
      "grad_norm": 0.1725049912929535,
      "learning_rate": 0.00018838219326818676,
      "loss": 0.2742,
      "step": 1503
    },
    {
      "epoch": 0.058308698812696874,
      "grad_norm": 0.22431516647338867,
      "learning_rate": 0.00018837443772297193,
      "loss": 0.3314,
      "step": 1504
    },
    {
      "epoch": 0.05834746789435425,
      "grad_norm": 0.1688654124736786,
      "learning_rate": 0.0001883666821777571,
      "loss": 0.2496,
      "step": 1505
    },
    {
      "epoch": 0.058386236976011634,
      "grad_norm": 0.20682726800441742,
      "learning_rate": 0.00018835892663254228,
      "loss": 0.3498,
      "step": 1506
    },
    {
      "epoch": 0.05842500605766901,
      "grad_norm": 0.22468449175357819,
      "learning_rate": 0.00018835117108732745,
      "loss": 0.3533,
      "step": 1507
    },
    {
      "epoch": 0.058463775139326386,
      "grad_norm": 0.17563816905021667,
      "learning_rate": 0.00018834341554211262,
      "loss": 0.3007,
      "step": 1508
    },
    {
      "epoch": 0.05850254422098376,
      "grad_norm": 0.1726967692375183,
      "learning_rate": 0.0001883356599968978,
      "loss": 0.266,
      "step": 1509
    },
    {
      "epoch": 0.058541313302641146,
      "grad_norm": 0.19334325194358826,
      "learning_rate": 0.00018832790445168297,
      "loss": 0.2571,
      "step": 1510
    },
    {
      "epoch": 0.05858008238429852,
      "grad_norm": 0.20928464829921722,
      "learning_rate": 0.00018832014890646814,
      "loss": 0.3002,
      "step": 1511
    },
    {
      "epoch": 0.0586188514659559,
      "grad_norm": 0.21252210438251495,
      "learning_rate": 0.00018831239336125332,
      "loss": 0.3219,
      "step": 1512
    },
    {
      "epoch": 0.05865762054761328,
      "grad_norm": 0.23880477249622345,
      "learning_rate": 0.0001883046378160385,
      "loss": 0.2942,
      "step": 1513
    },
    {
      "epoch": 0.05869638962927066,
      "grad_norm": 0.1961803287267685,
      "learning_rate": 0.00018829688227082366,
      "loss": 0.2889,
      "step": 1514
    },
    {
      "epoch": 0.058735158710928034,
      "grad_norm": 0.2196294516324997,
      "learning_rate": 0.0001882891267256088,
      "loss": 0.2875,
      "step": 1515
    },
    {
      "epoch": 0.05877392779258541,
      "grad_norm": 0.19398953020572662,
      "learning_rate": 0.000188281371180394,
      "loss": 0.242,
      "step": 1516
    },
    {
      "epoch": 0.058812696874242794,
      "grad_norm": 0.2519818842411041,
      "learning_rate": 0.00018827361563517915,
      "loss": 0.3171,
      "step": 1517
    },
    {
      "epoch": 0.05885146595590017,
      "grad_norm": 0.2346494346857071,
      "learning_rate": 0.00018826586008996435,
      "loss": 0.3776,
      "step": 1518
    },
    {
      "epoch": 0.058890235037557546,
      "grad_norm": 0.18265070021152496,
      "learning_rate": 0.0001882581045447495,
      "loss": 0.3331,
      "step": 1519
    },
    {
      "epoch": 0.05892900411921492,
      "grad_norm": 0.22284996509552002,
      "learning_rate": 0.0001882503489995347,
      "loss": 0.337,
      "step": 1520
    },
    {
      "epoch": 0.058967773200872306,
      "grad_norm": 0.19316312670707703,
      "learning_rate": 0.00018824259345431984,
      "loss": 0.3181,
      "step": 1521
    },
    {
      "epoch": 0.05900654228252968,
      "grad_norm": 0.19182857871055603,
      "learning_rate": 0.00018823483790910502,
      "loss": 0.3421,
      "step": 1522
    },
    {
      "epoch": 0.05904531136418706,
      "grad_norm": 0.15464144945144653,
      "learning_rate": 0.0001882270823638902,
      "loss": 0.2279,
      "step": 1523
    },
    {
      "epoch": 0.05908408044584444,
      "grad_norm": 0.1633245050907135,
      "learning_rate": 0.00018821932681867536,
      "loss": 0.2907,
      "step": 1524
    },
    {
      "epoch": 0.05912284952750182,
      "grad_norm": 0.17352445423603058,
      "learning_rate": 0.00018821157127346053,
      "loss": 0.281,
      "step": 1525
    },
    {
      "epoch": 0.059161618609159194,
      "grad_norm": 0.19839739799499512,
      "learning_rate": 0.0001882038157282457,
      "loss": 0.3376,
      "step": 1526
    },
    {
      "epoch": 0.05920038769081657,
      "grad_norm": 0.19539718329906464,
      "learning_rate": 0.00018819606018303088,
      "loss": 0.2844,
      "step": 1527
    },
    {
      "epoch": 0.059239156772473954,
      "grad_norm": 0.2417004257440567,
      "learning_rate": 0.00018818830463781605,
      "loss": 0.3022,
      "step": 1528
    },
    {
      "epoch": 0.05927792585413133,
      "grad_norm": 0.2729608118534088,
      "learning_rate": 0.0001881805490926012,
      "loss": 0.3135,
      "step": 1529
    },
    {
      "epoch": 0.05931669493578871,
      "grad_norm": 0.2648843824863434,
      "learning_rate": 0.0001881727935473864,
      "loss": 0.3434,
      "step": 1530
    },
    {
      "epoch": 0.05935546401744609,
      "grad_norm": 0.19104129076004028,
      "learning_rate": 0.00018816503800217154,
      "loss": 0.244,
      "step": 1531
    },
    {
      "epoch": 0.059394233099103466,
      "grad_norm": 0.1663535237312317,
      "learning_rate": 0.00018815728245695674,
      "loss": 0.2357,
      "step": 1532
    },
    {
      "epoch": 0.05943300218076084,
      "grad_norm": 0.20994623005390167,
      "learning_rate": 0.0001881495269117419,
      "loss": 0.3224,
      "step": 1533
    },
    {
      "epoch": 0.05947177126241822,
      "grad_norm": 0.18009167909622192,
      "learning_rate": 0.0001881417713665271,
      "loss": 0.2744,
      "step": 1534
    },
    {
      "epoch": 0.0595105403440756,
      "grad_norm": 0.17196306586265564,
      "learning_rate": 0.00018813401582131224,
      "loss": 0.2693,
      "step": 1535
    },
    {
      "epoch": 0.05954930942573298,
      "grad_norm": 0.20289947092533112,
      "learning_rate": 0.0001881262602760974,
      "loss": 0.3481,
      "step": 1536
    },
    {
      "epoch": 0.059588078507390355,
      "grad_norm": 0.18762050569057465,
      "learning_rate": 0.00018811850473088258,
      "loss": 0.3187,
      "step": 1537
    },
    {
      "epoch": 0.05962684758904774,
      "grad_norm": 0.1541372388601303,
      "learning_rate": 0.00018811074918566775,
      "loss": 0.2693,
      "step": 1538
    },
    {
      "epoch": 0.059665616670705114,
      "grad_norm": 0.18411974608898163,
      "learning_rate": 0.00018810299364045295,
      "loss": 0.2796,
      "step": 1539
    },
    {
      "epoch": 0.05970438575236249,
      "grad_norm": 0.1840285211801529,
      "learning_rate": 0.0001880952380952381,
      "loss": 0.2795,
      "step": 1540
    },
    {
      "epoch": 0.05974315483401987,
      "grad_norm": 0.19012309610843658,
      "learning_rate": 0.0001880874825500233,
      "loss": 0.2558,
      "step": 1541
    },
    {
      "epoch": 0.05978192391567725,
      "grad_norm": 0.19912582635879517,
      "learning_rate": 0.00018807972700480845,
      "loss": 0.3096,
      "step": 1542
    },
    {
      "epoch": 0.059820692997334626,
      "grad_norm": 0.18273481726646423,
      "learning_rate": 0.00018807197145959362,
      "loss": 0.2398,
      "step": 1543
    },
    {
      "epoch": 0.059859462078992,
      "grad_norm": 0.21625395119190216,
      "learning_rate": 0.0001880642159143788,
      "loss": 0.3514,
      "step": 1544
    },
    {
      "epoch": 0.05989823116064938,
      "grad_norm": 0.21166163682937622,
      "learning_rate": 0.00018805646036916396,
      "loss": 0.3174,
      "step": 1545
    },
    {
      "epoch": 0.05993700024230676,
      "grad_norm": 0.19591662287712097,
      "learning_rate": 0.00018804870482394914,
      "loss": 0.2687,
      "step": 1546
    },
    {
      "epoch": 0.05997576932396414,
      "grad_norm": 0.19347871840000153,
      "learning_rate": 0.0001880409492787343,
      "loss": 0.2477,
      "step": 1547
    },
    {
      "epoch": 0.060014538405621515,
      "grad_norm": 0.17367945611476898,
      "learning_rate": 0.00018803319373351948,
      "loss": 0.2591,
      "step": 1548
    },
    {
      "epoch": 0.0600533074872789,
      "grad_norm": 0.21217061579227448,
      "learning_rate": 0.00018802543818830465,
      "loss": 0.2887,
      "step": 1549
    },
    {
      "epoch": 0.060092076568936274,
      "grad_norm": 0.24143674969673157,
      "learning_rate": 0.0001880176826430898,
      "loss": 0.3837,
      "step": 1550
    },
    {
      "epoch": 0.06013084565059365,
      "grad_norm": 0.21763424575328827,
      "learning_rate": 0.000188009927097875,
      "loss": 0.2636,
      "step": 1551
    },
    {
      "epoch": 0.06016961473225103,
      "grad_norm": 0.1946503072977066,
      "learning_rate": 0.00018800217155266015,
      "loss": 0.2732,
      "step": 1552
    },
    {
      "epoch": 0.06020838381390841,
      "grad_norm": 0.19519615173339844,
      "learning_rate": 0.00018799441600744535,
      "loss": 0.3155,
      "step": 1553
    },
    {
      "epoch": 0.060247152895565786,
      "grad_norm": 0.16476650536060333,
      "learning_rate": 0.0001879866604622305,
      "loss": 0.2476,
      "step": 1554
    },
    {
      "epoch": 0.06028592197722316,
      "grad_norm": 0.23012028634548187,
      "learning_rate": 0.0001879789049170157,
      "loss": 0.2823,
      "step": 1555
    },
    {
      "epoch": 0.060324691058880546,
      "grad_norm": 0.23561792075634003,
      "learning_rate": 0.00018797114937180084,
      "loss": 0.3279,
      "step": 1556
    },
    {
      "epoch": 0.06036346014053792,
      "grad_norm": 0.33115023374557495,
      "learning_rate": 0.000187963393826586,
      "loss": 0.3326,
      "step": 1557
    },
    {
      "epoch": 0.0604022292221953,
      "grad_norm": 0.26063817739486694,
      "learning_rate": 0.00018795563828137118,
      "loss": 0.2459,
      "step": 1558
    },
    {
      "epoch": 0.060440998303852675,
      "grad_norm": 0.20977316796779633,
      "learning_rate": 0.00018794788273615636,
      "loss": 0.2576,
      "step": 1559
    },
    {
      "epoch": 0.06047976738551006,
      "grad_norm": 0.2187204211950302,
      "learning_rate": 0.00018794012719094153,
      "loss": 0.259,
      "step": 1560
    },
    {
      "epoch": 0.060518536467167434,
      "grad_norm": 0.2936207354068756,
      "learning_rate": 0.0001879323716457267,
      "loss": 0.2912,
      "step": 1561
    },
    {
      "epoch": 0.06055730554882481,
      "grad_norm": 0.27998557686805725,
      "learning_rate": 0.00018792461610051187,
      "loss": 0.3226,
      "step": 1562
    },
    {
      "epoch": 0.06059607463048219,
      "grad_norm": 0.31494221091270447,
      "learning_rate": 0.00018791686055529705,
      "loss": 0.2782,
      "step": 1563
    },
    {
      "epoch": 0.06063484371213957,
      "grad_norm": 0.20845815539360046,
      "learning_rate": 0.00018790910501008222,
      "loss": 0.2823,
      "step": 1564
    },
    {
      "epoch": 0.06067361279379695,
      "grad_norm": 0.2629190981388092,
      "learning_rate": 0.0001879013494648674,
      "loss": 0.3639,
      "step": 1565
    },
    {
      "epoch": 0.06071238187545432,
      "grad_norm": 0.1832033395767212,
      "learning_rate": 0.00018789359391965257,
      "loss": 0.2708,
      "step": 1566
    },
    {
      "epoch": 0.060751150957111706,
      "grad_norm": 0.19159522652626038,
      "learning_rate": 0.00018788583837443774,
      "loss": 0.2931,
      "step": 1567
    },
    {
      "epoch": 0.06078992003876908,
      "grad_norm": 0.16892081499099731,
      "learning_rate": 0.0001878780828292229,
      "loss": 0.3002,
      "step": 1568
    },
    {
      "epoch": 0.06082868912042646,
      "grad_norm": 0.14281176030635834,
      "learning_rate": 0.00018787032728400808,
      "loss": 0.2216,
      "step": 1569
    },
    {
      "epoch": 0.060867458202083835,
      "grad_norm": 0.18307864665985107,
      "learning_rate": 0.00018786257173879326,
      "loss": 0.2309,
      "step": 1570
    },
    {
      "epoch": 0.06090622728374122,
      "grad_norm": 0.17241434752941132,
      "learning_rate": 0.0001878548161935784,
      "loss": 0.2358,
      "step": 1571
    },
    {
      "epoch": 0.060944996365398595,
      "grad_norm": 0.23037223517894745,
      "learning_rate": 0.0001878470606483636,
      "loss": 0.3289,
      "step": 1572
    },
    {
      "epoch": 0.06098376544705597,
      "grad_norm": 0.1918107122182846,
      "learning_rate": 0.00018783930510314875,
      "loss": 0.245,
      "step": 1573
    },
    {
      "epoch": 0.061022534528713354,
      "grad_norm": 0.20964543521404266,
      "learning_rate": 0.00018783154955793395,
      "loss": 0.2709,
      "step": 1574
    },
    {
      "epoch": 0.06106130361037073,
      "grad_norm": 0.27697500586509705,
      "learning_rate": 0.0001878237940127191,
      "loss": 0.3756,
      "step": 1575
    },
    {
      "epoch": 0.06110007269202811,
      "grad_norm": 0.24549053609371185,
      "learning_rate": 0.0001878160384675043,
      "loss": 0.3152,
      "step": 1576
    },
    {
      "epoch": 0.06113884177368548,
      "grad_norm": 0.2360323667526245,
      "learning_rate": 0.00018780828292228944,
      "loss": 0.3172,
      "step": 1577
    },
    {
      "epoch": 0.061177610855342866,
      "grad_norm": 0.20888172090053558,
      "learning_rate": 0.0001878005273770746,
      "loss": 0.2861,
      "step": 1578
    },
    {
      "epoch": 0.06121637993700024,
      "grad_norm": 0.18744121491909027,
      "learning_rate": 0.00018779277183185978,
      "loss": 0.2701,
      "step": 1579
    },
    {
      "epoch": 0.06125514901865762,
      "grad_norm": 0.2680114209651947,
      "learning_rate": 0.00018778501628664496,
      "loss": 0.3302,
      "step": 1580
    },
    {
      "epoch": 0.061293918100315,
      "grad_norm": 0.2620087265968323,
      "learning_rate": 0.00018777726074143013,
      "loss": 0.3689,
      "step": 1581
    },
    {
      "epoch": 0.06133268718197238,
      "grad_norm": 0.24668467044830322,
      "learning_rate": 0.0001877695051962153,
      "loss": 0.2175,
      "step": 1582
    },
    {
      "epoch": 0.061371456263629755,
      "grad_norm": 0.2143821120262146,
      "learning_rate": 0.00018776174965100048,
      "loss": 0.2818,
      "step": 1583
    },
    {
      "epoch": 0.06141022534528713,
      "grad_norm": 0.23515692353248596,
      "learning_rate": 0.00018775399410578565,
      "loss": 0.2966,
      "step": 1584
    },
    {
      "epoch": 0.061448994426944514,
      "grad_norm": 0.17812709510326385,
      "learning_rate": 0.00018774623856057082,
      "loss": 0.2722,
      "step": 1585
    },
    {
      "epoch": 0.06148776350860189,
      "grad_norm": 0.2824242115020752,
      "learning_rate": 0.000187738483015356,
      "loss": 0.3935,
      "step": 1586
    },
    {
      "epoch": 0.06152653259025927,
      "grad_norm": 0.24675041437149048,
      "learning_rate": 0.00018773072747014117,
      "loss": 0.3436,
      "step": 1587
    },
    {
      "epoch": 0.06156530167191664,
      "grad_norm": 0.21974369883537292,
      "learning_rate": 0.00018772297192492634,
      "loss": 0.3008,
      "step": 1588
    },
    {
      "epoch": 0.061604070753574026,
      "grad_norm": 0.278337687253952,
      "learning_rate": 0.0001877152163797115,
      "loss": 0.3915,
      "step": 1589
    },
    {
      "epoch": 0.0616428398352314,
      "grad_norm": 0.19885237514972687,
      "learning_rate": 0.00018770746083449669,
      "loss": 0.3025,
      "step": 1590
    },
    {
      "epoch": 0.06168160891688878,
      "grad_norm": 0.2502850592136383,
      "learning_rate": 0.00018769970528928186,
      "loss": 0.3921,
      "step": 1591
    },
    {
      "epoch": 0.06172037799854616,
      "grad_norm": 0.25032538175582886,
      "learning_rate": 0.000187691949744067,
      "loss": 0.3082,
      "step": 1592
    },
    {
      "epoch": 0.06175914708020354,
      "grad_norm": 0.1966782659292221,
      "learning_rate": 0.0001876841941988522,
      "loss": 0.26,
      "step": 1593
    },
    {
      "epoch": 0.061797916161860915,
      "grad_norm": 0.22405262291431427,
      "learning_rate": 0.00018767643865363735,
      "loss": 0.3313,
      "step": 1594
    },
    {
      "epoch": 0.06183668524351829,
      "grad_norm": 0.24844978749752045,
      "learning_rate": 0.00018766868310842255,
      "loss": 0.3188,
      "step": 1595
    },
    {
      "epoch": 0.061875454325175674,
      "grad_norm": 0.28221914172172546,
      "learning_rate": 0.0001876609275632077,
      "loss": 0.3843,
      "step": 1596
    },
    {
      "epoch": 0.06191422340683305,
      "grad_norm": 0.1989627331495285,
      "learning_rate": 0.0001876531720179929,
      "loss": 0.2902,
      "step": 1597
    },
    {
      "epoch": 0.06195299248849043,
      "grad_norm": 0.21191054582595825,
      "learning_rate": 0.00018764541647277804,
      "loss": 0.2936,
      "step": 1598
    },
    {
      "epoch": 0.06199176157014781,
      "grad_norm": 0.20323199033737183,
      "learning_rate": 0.00018763766092756321,
      "loss": 0.2582,
      "step": 1599
    },
    {
      "epoch": 0.06203053065180519,
      "grad_norm": 0.2552703320980072,
      "learning_rate": 0.0001876299053823484,
      "loss": 0.3173,
      "step": 1600
    },
    {
      "epoch": 0.06206929973346256,
      "grad_norm": 0.2086367905139923,
      "learning_rate": 0.00018762214983713356,
      "loss": 0.3076,
      "step": 1601
    },
    {
      "epoch": 0.06210806881511994,
      "grad_norm": 0.1921670287847519,
      "learning_rate": 0.00018761439429191873,
      "loss": 0.2636,
      "step": 1602
    },
    {
      "epoch": 0.06214683789677732,
      "grad_norm": 0.21618406474590302,
      "learning_rate": 0.0001876066387467039,
      "loss": 0.2826,
      "step": 1603
    },
    {
      "epoch": 0.0621856069784347,
      "grad_norm": 0.1942652314901352,
      "learning_rate": 0.00018759888320148908,
      "loss": 0.29,
      "step": 1604
    },
    {
      "epoch": 0.062224376060092075,
      "grad_norm": 0.2203644961118698,
      "learning_rate": 0.00018759112765627425,
      "loss": 0.3151,
      "step": 1605
    },
    {
      "epoch": 0.06226314514174945,
      "grad_norm": 0.16370752453804016,
      "learning_rate": 0.0001875833721110594,
      "loss": 0.2628,
      "step": 1606
    },
    {
      "epoch": 0.062301914223406835,
      "grad_norm": 0.26791900396347046,
      "learning_rate": 0.0001875756165658446,
      "loss": 0.3622,
      "step": 1607
    },
    {
      "epoch": 0.06234068330506421,
      "grad_norm": 0.28316226601600647,
      "learning_rate": 0.00018756786102062974,
      "loss": 0.3636,
      "step": 1608
    },
    {
      "epoch": 0.06237945238672159,
      "grad_norm": 0.2300776094198227,
      "learning_rate": 0.00018756010547541494,
      "loss": 0.3238,
      "step": 1609
    },
    {
      "epoch": 0.06241822146837897,
      "grad_norm": 0.21892763674259186,
      "learning_rate": 0.0001875523499302001,
      "loss": 0.3363,
      "step": 1610
    },
    {
      "epoch": 0.06245699055003635,
      "grad_norm": 0.17032675445079803,
      "learning_rate": 0.0001875445943849853,
      "loss": 0.2584,
      "step": 1611
    },
    {
      "epoch": 0.06249575963169372,
      "grad_norm": 0.23925964534282684,
      "learning_rate": 0.00018753683883977043,
      "loss": 0.2932,
      "step": 1612
    },
    {
      "epoch": 0.0625345287133511,
      "grad_norm": 0.2147558182477951,
      "learning_rate": 0.0001875290832945556,
      "loss": 0.2371,
      "step": 1613
    },
    {
      "epoch": 0.06257329779500848,
      "grad_norm": 0.23906001448631287,
      "learning_rate": 0.00018752132774934078,
      "loss": 0.2962,
      "step": 1614
    },
    {
      "epoch": 0.06261206687666586,
      "grad_norm": 0.1725405603647232,
      "learning_rate": 0.00018751357220412595,
      "loss": 0.2554,
      "step": 1615
    },
    {
      "epoch": 0.06265083595832324,
      "grad_norm": 0.26040613651275635,
      "learning_rate": 0.00018750581665891112,
      "loss": 0.3015,
      "step": 1616
    },
    {
      "epoch": 0.06268960503998061,
      "grad_norm": 0.22339341044425964,
      "learning_rate": 0.0001874980611136963,
      "loss": 0.3,
      "step": 1617
    },
    {
      "epoch": 0.062728374121638,
      "grad_norm": 0.19303855299949646,
      "learning_rate": 0.0001874903055684815,
      "loss": 0.2291,
      "step": 1618
    },
    {
      "epoch": 0.06276714320329538,
      "grad_norm": 0.2805996835231781,
      "learning_rate": 0.00018748255002326664,
      "loss": 0.3314,
      "step": 1619
    },
    {
      "epoch": 0.06280591228495275,
      "grad_norm": 0.16726826131343842,
      "learning_rate": 0.00018747479447805182,
      "loss": 0.2293,
      "step": 1620
    },
    {
      "epoch": 0.06284468136661013,
      "grad_norm": 0.2156970202922821,
      "learning_rate": 0.000187467038932837,
      "loss": 0.3019,
      "step": 1621
    },
    {
      "epoch": 0.0628834504482675,
      "grad_norm": 0.2649688720703125,
      "learning_rate": 0.00018745928338762216,
      "loss": 0.3327,
      "step": 1622
    },
    {
      "epoch": 0.06292221952992488,
      "grad_norm": 0.19864952564239502,
      "learning_rate": 0.00018745152784240733,
      "loss": 0.2546,
      "step": 1623
    },
    {
      "epoch": 0.06296098861158227,
      "grad_norm": 0.22100766003131866,
      "learning_rate": 0.0001874437722971925,
      "loss": 0.2888,
      "step": 1624
    },
    {
      "epoch": 0.06299975769323964,
      "grad_norm": 0.18079690635204315,
      "learning_rate": 0.00018743601675197768,
      "loss": 0.2637,
      "step": 1625
    },
    {
      "epoch": 0.06303852677489702,
      "grad_norm": 0.24253202974796295,
      "learning_rate": 0.00018742826120676285,
      "loss": 0.3132,
      "step": 1626
    },
    {
      "epoch": 0.0630772958565544,
      "grad_norm": 0.19489644467830658,
      "learning_rate": 0.000187420505661548,
      "loss": 0.257,
      "step": 1627
    },
    {
      "epoch": 0.06311606493821177,
      "grad_norm": 0.2509344220161438,
      "learning_rate": 0.0001874127501163332,
      "loss": 0.3397,
      "step": 1628
    },
    {
      "epoch": 0.06315483401986915,
      "grad_norm": 0.25841203331947327,
      "learning_rate": 0.00018740499457111834,
      "loss": 0.3015,
      "step": 1629
    },
    {
      "epoch": 0.06319360310152654,
      "grad_norm": 0.24317868053913116,
      "learning_rate": 0.00018739723902590354,
      "loss": 0.2456,
      "step": 1630
    },
    {
      "epoch": 0.06323237218318391,
      "grad_norm": 0.22533239424228668,
      "learning_rate": 0.0001873894834806887,
      "loss": 0.2871,
      "step": 1631
    },
    {
      "epoch": 0.06327114126484129,
      "grad_norm": 0.26378071308135986,
      "learning_rate": 0.0001873817279354739,
      "loss": 0.387,
      "step": 1632
    },
    {
      "epoch": 0.06330991034649867,
      "grad_norm": 0.26709842681884766,
      "learning_rate": 0.00018737397239025903,
      "loss": 0.352,
      "step": 1633
    },
    {
      "epoch": 0.06334867942815604,
      "grad_norm": 0.23448699712753296,
      "learning_rate": 0.0001873662168450442,
      "loss": 0.2991,
      "step": 1634
    },
    {
      "epoch": 0.06338744850981343,
      "grad_norm": 0.21004757285118103,
      "learning_rate": 0.00018735846129982938,
      "loss": 0.2739,
      "step": 1635
    },
    {
      "epoch": 0.0634262175914708,
      "grad_norm": 0.2325962483882904,
      "learning_rate": 0.00018735070575461455,
      "loss": 0.251,
      "step": 1636
    },
    {
      "epoch": 0.06346498667312818,
      "grad_norm": 0.24282629787921906,
      "learning_rate": 0.00018734295020939973,
      "loss": 0.2763,
      "step": 1637
    },
    {
      "epoch": 0.06350375575478556,
      "grad_norm": 0.2330755889415741,
      "learning_rate": 0.0001873351946641849,
      "loss": 0.2594,
      "step": 1638
    },
    {
      "epoch": 0.06354252483644293,
      "grad_norm": 0.3263648748397827,
      "learning_rate": 0.00018732743911897007,
      "loss": 0.346,
      "step": 1639
    },
    {
      "epoch": 0.06358129391810032,
      "grad_norm": 0.2078729271888733,
      "learning_rate": 0.00018731968357375524,
      "loss": 0.2285,
      "step": 1640
    },
    {
      "epoch": 0.0636200629997577,
      "grad_norm": 0.20934653282165527,
      "learning_rate": 0.00018731192802854042,
      "loss": 0.2569,
      "step": 1641
    },
    {
      "epoch": 0.06365883208141507,
      "grad_norm": 0.2553001940250397,
      "learning_rate": 0.0001873041724833256,
      "loss": 0.2904,
      "step": 1642
    },
    {
      "epoch": 0.06369760116307245,
      "grad_norm": 0.29271185398101807,
      "learning_rate": 0.00018729641693811076,
      "loss": 0.3511,
      "step": 1643
    },
    {
      "epoch": 0.06373637024472983,
      "grad_norm": 0.246208056807518,
      "learning_rate": 0.00018728866139289594,
      "loss": 0.3091,
      "step": 1644
    },
    {
      "epoch": 0.0637751393263872,
      "grad_norm": 0.22033683955669403,
      "learning_rate": 0.0001872809058476811,
      "loss": 0.2879,
      "step": 1645
    },
    {
      "epoch": 0.06381390840804459,
      "grad_norm": 0.22975456714630127,
      "learning_rate": 0.00018727315030246628,
      "loss": 0.2903,
      "step": 1646
    },
    {
      "epoch": 0.06385267748970196,
      "grad_norm": 0.16086038947105408,
      "learning_rate": 0.00018726539475725145,
      "loss": 0.2039,
      "step": 1647
    },
    {
      "epoch": 0.06389144657135934,
      "grad_norm": 0.25215721130371094,
      "learning_rate": 0.0001872576392120366,
      "loss": 0.3271,
      "step": 1648
    },
    {
      "epoch": 0.06393021565301672,
      "grad_norm": 0.23706664144992828,
      "learning_rate": 0.0001872498836668218,
      "loss": 0.3352,
      "step": 1649
    },
    {
      "epoch": 0.06396898473467409,
      "grad_norm": 0.18778525292873383,
      "learning_rate": 0.00018724212812160695,
      "loss": 0.216,
      "step": 1650
    },
    {
      "epoch": 0.06400775381633148,
      "grad_norm": 0.2531261146068573,
      "learning_rate": 0.00018723437257639215,
      "loss": 0.3277,
      "step": 1651
    },
    {
      "epoch": 0.06404652289798886,
      "grad_norm": 0.21943531930446625,
      "learning_rate": 0.0001872266170311773,
      "loss": 0.2532,
      "step": 1652
    },
    {
      "epoch": 0.06408529197964623,
      "grad_norm": 0.23756344616413116,
      "learning_rate": 0.0001872188614859625,
      "loss": 0.2919,
      "step": 1653
    },
    {
      "epoch": 0.06412406106130361,
      "grad_norm": 0.23133541643619537,
      "learning_rate": 0.00018721110594074764,
      "loss": 0.2399,
      "step": 1654
    },
    {
      "epoch": 0.064162830142961,
      "grad_norm": 0.20756734907627106,
      "learning_rate": 0.0001872033503955328,
      "loss": 0.216,
      "step": 1655
    },
    {
      "epoch": 0.06420159922461836,
      "grad_norm": 0.20582596957683563,
      "learning_rate": 0.00018719559485031798,
      "loss": 0.2661,
      "step": 1656
    },
    {
      "epoch": 0.06424036830627575,
      "grad_norm": 0.2655220031738281,
      "learning_rate": 0.00018718783930510316,
      "loss": 0.2615,
      "step": 1657
    },
    {
      "epoch": 0.06427913738793313,
      "grad_norm": 0.28063300251960754,
      "learning_rate": 0.00018718008375988833,
      "loss": 0.3024,
      "step": 1658
    },
    {
      "epoch": 0.0643179064695905,
      "grad_norm": 0.2919977307319641,
      "learning_rate": 0.0001871723282146735,
      "loss": 0.2674,
      "step": 1659
    },
    {
      "epoch": 0.06435667555124788,
      "grad_norm": 0.3327515423297882,
      "learning_rate": 0.00018716457266945867,
      "loss": 0.3269,
      "step": 1660
    },
    {
      "epoch": 0.06439544463290525,
      "grad_norm": 0.20629893243312836,
      "learning_rate": 0.00018715681712424385,
      "loss": 0.2341,
      "step": 1661
    },
    {
      "epoch": 0.06443421371456264,
      "grad_norm": 0.296203076839447,
      "learning_rate": 0.00018714906157902902,
      "loss": 0.3122,
      "step": 1662
    },
    {
      "epoch": 0.06447298279622002,
      "grad_norm": 0.23635217547416687,
      "learning_rate": 0.0001871413060338142,
      "loss": 0.2667,
      "step": 1663
    },
    {
      "epoch": 0.06451175187787739,
      "grad_norm": 0.2758612334728241,
      "learning_rate": 0.00018713355048859936,
      "loss": 0.2671,
      "step": 1664
    },
    {
      "epoch": 0.06455052095953477,
      "grad_norm": 0.26180922985076904,
      "learning_rate": 0.00018712579494338454,
      "loss": 0.318,
      "step": 1665
    },
    {
      "epoch": 0.06458929004119215,
      "grad_norm": 0.16801583766937256,
      "learning_rate": 0.0001871180393981697,
      "loss": 0.196,
      "step": 1666
    },
    {
      "epoch": 0.06462805912284952,
      "grad_norm": 0.3210265040397644,
      "learning_rate": 0.00018711028385295488,
      "loss": 0.3645,
      "step": 1667
    },
    {
      "epoch": 0.06466682820450691,
      "grad_norm": 0.26629912853240967,
      "learning_rate": 0.00018710252830774006,
      "loss": 0.2529,
      "step": 1668
    },
    {
      "epoch": 0.06470559728616429,
      "grad_norm": 0.2915992736816406,
      "learning_rate": 0.0001870947727625252,
      "loss": 0.3299,
      "step": 1669
    },
    {
      "epoch": 0.06474436636782166,
      "grad_norm": 0.22840552031993866,
      "learning_rate": 0.0001870870172173104,
      "loss": 0.2479,
      "step": 1670
    },
    {
      "epoch": 0.06478313544947904,
      "grad_norm": 0.19719460606575012,
      "learning_rate": 0.00018707926167209555,
      "loss": 0.2512,
      "step": 1671
    },
    {
      "epoch": 0.06482190453113641,
      "grad_norm": 0.1898377388715744,
      "learning_rate": 0.00018707150612688075,
      "loss": 0.2246,
      "step": 1672
    },
    {
      "epoch": 0.0648606736127938,
      "grad_norm": 0.18478348851203918,
      "learning_rate": 0.0001870637505816659,
      "loss": 0.2416,
      "step": 1673
    },
    {
      "epoch": 0.06489944269445118,
      "grad_norm": 0.2028384804725647,
      "learning_rate": 0.0001870559950364511,
      "loss": 0.2559,
      "step": 1674
    },
    {
      "epoch": 0.06493821177610855,
      "grad_norm": 0.22454506158828735,
      "learning_rate": 0.00018704823949123624,
      "loss": 0.2565,
      "step": 1675
    },
    {
      "epoch": 0.06497698085776593,
      "grad_norm": 0.21751277148723602,
      "learning_rate": 0.0001870404839460214,
      "loss": 0.2096,
      "step": 1676
    },
    {
      "epoch": 0.06501574993942331,
      "grad_norm": 0.2653978765010834,
      "learning_rate": 0.00018703272840080658,
      "loss": 0.2751,
      "step": 1677
    },
    {
      "epoch": 0.06505451902108068,
      "grad_norm": 0.2761109173297882,
      "learning_rate": 0.00018702497285559176,
      "loss": 0.3309,
      "step": 1678
    },
    {
      "epoch": 0.06509328810273807,
      "grad_norm": 0.2335660457611084,
      "learning_rate": 0.00018701721731037693,
      "loss": 0.2487,
      "step": 1679
    },
    {
      "epoch": 0.06513205718439545,
      "grad_norm": 0.19184477627277374,
      "learning_rate": 0.0001870094617651621,
      "loss": 0.1928,
      "step": 1680
    },
    {
      "epoch": 0.06517082626605282,
      "grad_norm": 0.29284119606018066,
      "learning_rate": 0.00018700170621994728,
      "loss": 0.3181,
      "step": 1681
    },
    {
      "epoch": 0.0652095953477102,
      "grad_norm": 0.21952995657920837,
      "learning_rate": 0.00018699395067473245,
      "loss": 0.238,
      "step": 1682
    },
    {
      "epoch": 0.06524836442936759,
      "grad_norm": 0.28159844875335693,
      "learning_rate": 0.0001869861951295176,
      "loss": 0.3429,
      "step": 1683
    },
    {
      "epoch": 0.06528713351102496,
      "grad_norm": 0.25854483246803284,
      "learning_rate": 0.0001869784395843028,
      "loss": 0.3297,
      "step": 1684
    },
    {
      "epoch": 0.06532590259268234,
      "grad_norm": 0.21375912427902222,
      "learning_rate": 0.00018697068403908794,
      "loss": 0.2946,
      "step": 1685
    },
    {
      "epoch": 0.06536467167433971,
      "grad_norm": 0.2119208127260208,
      "learning_rate": 0.00018696292849387314,
      "loss": 0.2654,
      "step": 1686
    },
    {
      "epoch": 0.06540344075599709,
      "grad_norm": 0.2549656927585602,
      "learning_rate": 0.00018695517294865829,
      "loss": 0.349,
      "step": 1687
    },
    {
      "epoch": 0.06544220983765447,
      "grad_norm": 0.16879785060882568,
      "learning_rate": 0.00018694741740344349,
      "loss": 0.2276,
      "step": 1688
    },
    {
      "epoch": 0.06548097891931184,
      "grad_norm": 0.18856580555438995,
      "learning_rate": 0.00018693966185822863,
      "loss": 0.2935,
      "step": 1689
    },
    {
      "epoch": 0.06551974800096923,
      "grad_norm": 0.2495112121105194,
      "learning_rate": 0.0001869319063130138,
      "loss": 0.2833,
      "step": 1690
    },
    {
      "epoch": 0.06555851708262661,
      "grad_norm": 0.2615143358707428,
      "learning_rate": 0.00018692415076779898,
      "loss": 0.3655,
      "step": 1691
    },
    {
      "epoch": 0.06559728616428398,
      "grad_norm": 0.24677978456020355,
      "learning_rate": 0.00018691639522258415,
      "loss": 0.3034,
      "step": 1692
    },
    {
      "epoch": 0.06563605524594136,
      "grad_norm": 0.25500041246414185,
      "learning_rate": 0.00018690863967736932,
      "loss": 0.305,
      "step": 1693
    },
    {
      "epoch": 0.06567482432759875,
      "grad_norm": 0.40294814109802246,
      "learning_rate": 0.0001869008841321545,
      "loss": 0.3405,
      "step": 1694
    },
    {
      "epoch": 0.06571359340925612,
      "grad_norm": 0.18187950551509857,
      "learning_rate": 0.00018689312858693967,
      "loss": 0.293,
      "step": 1695
    },
    {
      "epoch": 0.0657523624909135,
      "grad_norm": 0.2744314670562744,
      "learning_rate": 0.00018688537304172484,
      "loss": 0.3876,
      "step": 1696
    },
    {
      "epoch": 0.06579113157257087,
      "grad_norm": 0.22355924546718597,
      "learning_rate": 0.00018687761749651,
      "loss": 0.2409,
      "step": 1697
    },
    {
      "epoch": 0.06582990065422825,
      "grad_norm": 0.24770870804786682,
      "learning_rate": 0.00018686986195129519,
      "loss": 0.2928,
      "step": 1698
    },
    {
      "epoch": 0.06586866973588563,
      "grad_norm": 0.20409053564071655,
      "learning_rate": 0.00018686210640608036,
      "loss": 0.2254,
      "step": 1699
    },
    {
      "epoch": 0.065907438817543,
      "grad_norm": 0.24334560334682465,
      "learning_rate": 0.00018685435086086553,
      "loss": 0.2635,
      "step": 1700
    },
    {
      "epoch": 0.06594620789920039,
      "grad_norm": 0.2430592030286789,
      "learning_rate": 0.0001868465953156507,
      "loss": 0.2677,
      "step": 1701
    },
    {
      "epoch": 0.06598497698085777,
      "grad_norm": 0.20540811121463776,
      "learning_rate": 0.00018683883977043588,
      "loss": 0.2712,
      "step": 1702
    },
    {
      "epoch": 0.06602374606251514,
      "grad_norm": 0.24762749671936035,
      "learning_rate": 0.00018683108422522105,
      "loss": 0.312,
      "step": 1703
    },
    {
      "epoch": 0.06606251514417252,
      "grad_norm": 0.24636560678482056,
      "learning_rate": 0.0001868233286800062,
      "loss": 0.3222,
      "step": 1704
    },
    {
      "epoch": 0.0661012842258299,
      "grad_norm": 0.22298884391784668,
      "learning_rate": 0.0001868155731347914,
      "loss": 0.2911,
      "step": 1705
    },
    {
      "epoch": 0.06614005330748728,
      "grad_norm": 0.26168009638786316,
      "learning_rate": 0.00018680781758957654,
      "loss": 0.3458,
      "step": 1706
    },
    {
      "epoch": 0.06617882238914466,
      "grad_norm": 0.248213529586792,
      "learning_rate": 0.00018680006204436174,
      "loss": 0.3084,
      "step": 1707
    },
    {
      "epoch": 0.06621759147080203,
      "grad_norm": 0.22401340305805206,
      "learning_rate": 0.0001867923064991469,
      "loss": 0.2349,
      "step": 1708
    },
    {
      "epoch": 0.06625636055245941,
      "grad_norm": 0.23395615816116333,
      "learning_rate": 0.0001867845509539321,
      "loss": 0.2651,
      "step": 1709
    },
    {
      "epoch": 0.0662951296341168,
      "grad_norm": 0.3242175877094269,
      "learning_rate": 0.00018677679540871723,
      "loss": 0.3328,
      "step": 1710
    },
    {
      "epoch": 0.06633389871577416,
      "grad_norm": 0.2624073326587677,
      "learning_rate": 0.0001867690398635024,
      "loss": 0.3024,
      "step": 1711
    },
    {
      "epoch": 0.06637266779743155,
      "grad_norm": 0.23994940519332886,
      "learning_rate": 0.00018676128431828758,
      "loss": 0.2943,
      "step": 1712
    },
    {
      "epoch": 0.06641143687908893,
      "grad_norm": 0.2284735143184662,
      "learning_rate": 0.00018675352877307275,
      "loss": 0.2388,
      "step": 1713
    },
    {
      "epoch": 0.0664502059607463,
      "grad_norm": 0.2265206128358841,
      "learning_rate": 0.00018674577322785792,
      "loss": 0.2943,
      "step": 1714
    },
    {
      "epoch": 0.06648897504240368,
      "grad_norm": 0.19666099548339844,
      "learning_rate": 0.0001867380176826431,
      "loss": 0.2033,
      "step": 1715
    },
    {
      "epoch": 0.06652774412406107,
      "grad_norm": 0.17328496277332306,
      "learning_rate": 0.00018673026213742827,
      "loss": 0.2373,
      "step": 1716
    },
    {
      "epoch": 0.06656651320571844,
      "grad_norm": 0.18392814695835114,
      "learning_rate": 0.00018672250659221344,
      "loss": 0.2459,
      "step": 1717
    },
    {
      "epoch": 0.06660528228737582,
      "grad_norm": 0.1811067909002304,
      "learning_rate": 0.00018671475104699862,
      "loss": 0.2228,
      "step": 1718
    },
    {
      "epoch": 0.0666440513690332,
      "grad_norm": 0.22813132405281067,
      "learning_rate": 0.0001867069955017838,
      "loss": 0.2709,
      "step": 1719
    },
    {
      "epoch": 0.06668282045069057,
      "grad_norm": 0.24454019963741302,
      "learning_rate": 0.00018669923995656896,
      "loss": 0.2513,
      "step": 1720
    },
    {
      "epoch": 0.06672158953234796,
      "grad_norm": 0.3021988272666931,
      "learning_rate": 0.00018669148441135413,
      "loss": 0.3224,
      "step": 1721
    },
    {
      "epoch": 0.06676035861400532,
      "grad_norm": 0.30627861618995667,
      "learning_rate": 0.0001866837288661393,
      "loss": 0.2968,
      "step": 1722
    },
    {
      "epoch": 0.06679912769566271,
      "grad_norm": 0.32968926429748535,
      "learning_rate": 0.00018667597332092448,
      "loss": 0.2997,
      "step": 1723
    },
    {
      "epoch": 0.06683789677732009,
      "grad_norm": 0.2834336757659912,
      "learning_rate": 0.00018666821777570965,
      "loss": 0.3315,
      "step": 1724
    },
    {
      "epoch": 0.06687666585897746,
      "grad_norm": 0.3195471167564392,
      "learning_rate": 0.0001866604622304948,
      "loss": 0.3174,
      "step": 1725
    },
    {
      "epoch": 0.06691543494063484,
      "grad_norm": 0.22074773907661438,
      "learning_rate": 0.00018665270668528,
      "loss": 0.2565,
      "step": 1726
    },
    {
      "epoch": 0.06695420402229223,
      "grad_norm": 0.21128009259700775,
      "learning_rate": 0.00018664495114006514,
      "loss": 0.2301,
      "step": 1727
    },
    {
      "epoch": 0.0669929731039496,
      "grad_norm": 0.23494970798492432,
      "learning_rate": 0.00018663719559485034,
      "loss": 0.2554,
      "step": 1728
    },
    {
      "epoch": 0.06703174218560698,
      "grad_norm": 0.24756105244159698,
      "learning_rate": 0.0001866294400496355,
      "loss": 0.2989,
      "step": 1729
    },
    {
      "epoch": 0.06707051126726436,
      "grad_norm": 0.22527426481246948,
      "learning_rate": 0.0001866216845044207,
      "loss": 0.325,
      "step": 1730
    },
    {
      "epoch": 0.06710928034892173,
      "grad_norm": 0.26975125074386597,
      "learning_rate": 0.00018661392895920583,
      "loss": 0.3347,
      "step": 1731
    },
    {
      "epoch": 0.06714804943057912,
      "grad_norm": 0.2191968411207199,
      "learning_rate": 0.000186606173413991,
      "loss": 0.2861,
      "step": 1732
    },
    {
      "epoch": 0.06718681851223648,
      "grad_norm": 0.18829552829265594,
      "learning_rate": 0.00018659841786877618,
      "loss": 0.2508,
      "step": 1733
    },
    {
      "epoch": 0.06722558759389387,
      "grad_norm": 0.22303859889507294,
      "learning_rate": 0.00018659066232356135,
      "loss": 0.2748,
      "step": 1734
    },
    {
      "epoch": 0.06726435667555125,
      "grad_norm": 0.20867203176021576,
      "learning_rate": 0.00018658290677834653,
      "loss": 0.2466,
      "step": 1735
    },
    {
      "epoch": 0.06730312575720862,
      "grad_norm": 0.23211310803890228,
      "learning_rate": 0.0001865751512331317,
      "loss": 0.3052,
      "step": 1736
    },
    {
      "epoch": 0.067341894838866,
      "grad_norm": 0.2504420876502991,
      "learning_rate": 0.00018656739568791687,
      "loss": 0.3103,
      "step": 1737
    },
    {
      "epoch": 0.06738066392052339,
      "grad_norm": 0.26604610681533813,
      "learning_rate": 0.00018655964014270204,
      "loss": 0.3364,
      "step": 1738
    },
    {
      "epoch": 0.06741943300218076,
      "grad_norm": 0.2126794010400772,
      "learning_rate": 0.0001865518845974872,
      "loss": 0.2434,
      "step": 1739
    },
    {
      "epoch": 0.06745820208383814,
      "grad_norm": 0.2651456594467163,
      "learning_rate": 0.0001865441290522724,
      "loss": 0.2767,
      "step": 1740
    },
    {
      "epoch": 0.06749697116549552,
      "grad_norm": 0.2277381271123886,
      "learning_rate": 0.00018653637350705756,
      "loss": 0.216,
      "step": 1741
    },
    {
      "epoch": 0.06753574024715289,
      "grad_norm": 0.19079801440238953,
      "learning_rate": 0.00018652861796184274,
      "loss": 0.2075,
      "step": 1742
    },
    {
      "epoch": 0.06757450932881028,
      "grad_norm": 0.299988329410553,
      "learning_rate": 0.0001865208624166279,
      "loss": 0.2754,
      "step": 1743
    },
    {
      "epoch": 0.06761327841046766,
      "grad_norm": 0.2922667860984802,
      "learning_rate": 0.00018651310687141308,
      "loss": 0.2955,
      "step": 1744
    },
    {
      "epoch": 0.06765204749212503,
      "grad_norm": 0.28161728382110596,
      "learning_rate": 0.00018650535132619825,
      "loss": 0.2537,
      "step": 1745
    },
    {
      "epoch": 0.06769081657378241,
      "grad_norm": 0.3637188971042633,
      "learning_rate": 0.0001864975957809834,
      "loss": 0.3469,
      "step": 1746
    },
    {
      "epoch": 0.06772958565543978,
      "grad_norm": 0.2672199606895447,
      "learning_rate": 0.0001864898402357686,
      "loss": 0.36,
      "step": 1747
    },
    {
      "epoch": 0.06776835473709716,
      "grad_norm": 0.20607426762580872,
      "learning_rate": 0.00018648208469055375,
      "loss": 0.2452,
      "step": 1748
    },
    {
      "epoch": 0.06780712381875455,
      "grad_norm": 0.25786900520324707,
      "learning_rate": 0.00018647432914533894,
      "loss": 0.2691,
      "step": 1749
    },
    {
      "epoch": 0.06784589290041192,
      "grad_norm": 0.32500118017196655,
      "learning_rate": 0.0001864665736001241,
      "loss": 0.3843,
      "step": 1750
    },
    {
      "epoch": 0.0678846619820693,
      "grad_norm": 0.2680351734161377,
      "learning_rate": 0.0001864588180549093,
      "loss": 0.3825,
      "step": 1751
    },
    {
      "epoch": 0.06792343106372668,
      "grad_norm": 0.2038753777742386,
      "learning_rate": 0.00018645106250969444,
      "loss": 0.296,
      "step": 1752
    },
    {
      "epoch": 0.06796220014538405,
      "grad_norm": 0.18561215698719025,
      "learning_rate": 0.0001864433069644796,
      "loss": 0.2808,
      "step": 1753
    },
    {
      "epoch": 0.06800096922704144,
      "grad_norm": 0.2174217849969864,
      "learning_rate": 0.00018643555141926478,
      "loss": 0.2753,
      "step": 1754
    },
    {
      "epoch": 0.06803973830869882,
      "grad_norm": 0.2286020964384079,
      "learning_rate": 0.00018642779587404995,
      "loss": 0.3357,
      "step": 1755
    },
    {
      "epoch": 0.06807850739035619,
      "grad_norm": 0.21668733656406403,
      "learning_rate": 0.00018642004032883513,
      "loss": 0.2614,
      "step": 1756
    },
    {
      "epoch": 0.06811727647201357,
      "grad_norm": 0.1810242235660553,
      "learning_rate": 0.0001864122847836203,
      "loss": 0.2215,
      "step": 1757
    },
    {
      "epoch": 0.06815604555367094,
      "grad_norm": 0.2248181253671646,
      "learning_rate": 0.00018640452923840547,
      "loss": 0.2524,
      "step": 1758
    },
    {
      "epoch": 0.06819481463532832,
      "grad_norm": 0.25550565123558044,
      "learning_rate": 0.00018639677369319065,
      "loss": 0.2475,
      "step": 1759
    },
    {
      "epoch": 0.06823358371698571,
      "grad_norm": 0.2555575966835022,
      "learning_rate": 0.0001863890181479758,
      "loss": 0.2873,
      "step": 1760
    },
    {
      "epoch": 0.06827235279864308,
      "grad_norm": 0.2546035051345825,
      "learning_rate": 0.000186381262602761,
      "loss": 0.257,
      "step": 1761
    },
    {
      "epoch": 0.06831112188030046,
      "grad_norm": 0.3280123174190521,
      "learning_rate": 0.00018637350705754614,
      "loss": 0.3013,
      "step": 1762
    },
    {
      "epoch": 0.06834989096195784,
      "grad_norm": 0.2526850998401642,
      "learning_rate": 0.00018636575151233134,
      "loss": 0.2427,
      "step": 1763
    },
    {
      "epoch": 0.06838866004361521,
      "grad_norm": 0.23345617949962616,
      "learning_rate": 0.00018635799596711648,
      "loss": 0.2209,
      "step": 1764
    },
    {
      "epoch": 0.0684274291252726,
      "grad_norm": 0.24776796996593475,
      "learning_rate": 0.00018635024042190168,
      "loss": 0.2951,
      "step": 1765
    },
    {
      "epoch": 0.06846619820692998,
      "grad_norm": 0.23435084521770477,
      "learning_rate": 0.00018634248487668683,
      "loss": 0.2543,
      "step": 1766
    },
    {
      "epoch": 0.06850496728858735,
      "grad_norm": 0.24376311898231506,
      "learning_rate": 0.000186334729331472,
      "loss": 0.2812,
      "step": 1767
    },
    {
      "epoch": 0.06854373637024473,
      "grad_norm": 0.21214498579502106,
      "learning_rate": 0.00018632697378625717,
      "loss": 0.2267,
      "step": 1768
    },
    {
      "epoch": 0.06858250545190211,
      "grad_norm": 0.28737834095954895,
      "learning_rate": 0.00018631921824104235,
      "loss": 0.2924,
      "step": 1769
    },
    {
      "epoch": 0.06862127453355948,
      "grad_norm": 0.2654016613960266,
      "learning_rate": 0.00018631146269582752,
      "loss": 0.2784,
      "step": 1770
    },
    {
      "epoch": 0.06866004361521687,
      "grad_norm": 0.27723345160484314,
      "learning_rate": 0.0001863037071506127,
      "loss": 0.303,
      "step": 1771
    },
    {
      "epoch": 0.06869881269687424,
      "grad_norm": 0.2962455153465271,
      "learning_rate": 0.00018629595160539787,
      "loss": 0.2835,
      "step": 1772
    },
    {
      "epoch": 0.06873758177853162,
      "grad_norm": 0.37413305044174194,
      "learning_rate": 0.00018628819606018304,
      "loss": 0.3803,
      "step": 1773
    },
    {
      "epoch": 0.068776350860189,
      "grad_norm": 0.25119319558143616,
      "learning_rate": 0.0001862804405149682,
      "loss": 0.3307,
      "step": 1774
    },
    {
      "epoch": 0.06881511994184637,
      "grad_norm": 0.2297118753194809,
      "learning_rate": 0.00018627268496975338,
      "loss": 0.2835,
      "step": 1775
    },
    {
      "epoch": 0.06885388902350376,
      "grad_norm": 0.21467137336730957,
      "learning_rate": 0.00018626492942453856,
      "loss": 0.2779,
      "step": 1776
    },
    {
      "epoch": 0.06889265810516114,
      "grad_norm": 0.23531056940555573,
      "learning_rate": 0.00018625717387932373,
      "loss": 0.2974,
      "step": 1777
    },
    {
      "epoch": 0.06893142718681851,
      "grad_norm": 0.36457112431526184,
      "learning_rate": 0.0001862494183341089,
      "loss": 0.3972,
      "step": 1778
    },
    {
      "epoch": 0.06897019626847589,
      "grad_norm": 0.19711123406887054,
      "learning_rate": 0.00018624166278889407,
      "loss": 0.258,
      "step": 1779
    },
    {
      "epoch": 0.06900896535013328,
      "grad_norm": 0.18871763348579407,
      "learning_rate": 0.00018623390724367925,
      "loss": 0.2863,
      "step": 1780
    },
    {
      "epoch": 0.06904773443179064,
      "grad_norm": 0.20367585122585297,
      "learning_rate": 0.0001862261516984644,
      "loss": 0.2882,
      "step": 1781
    },
    {
      "epoch": 0.06908650351344803,
      "grad_norm": 0.1734454482793808,
      "learning_rate": 0.0001862183961532496,
      "loss": 0.2557,
      "step": 1782
    },
    {
      "epoch": 0.0691252725951054,
      "grad_norm": 0.2271697074174881,
      "learning_rate": 0.00018621064060803474,
      "loss": 0.2357,
      "step": 1783
    },
    {
      "epoch": 0.06916404167676278,
      "grad_norm": 0.22271138429641724,
      "learning_rate": 0.00018620288506281994,
      "loss": 0.2313,
      "step": 1784
    },
    {
      "epoch": 0.06920281075842016,
      "grad_norm": 0.2012520283460617,
      "learning_rate": 0.00018619512951760508,
      "loss": 0.2411,
      "step": 1785
    },
    {
      "epoch": 0.06924157984007753,
      "grad_norm": 0.29709291458129883,
      "learning_rate": 0.00018618737397239028,
      "loss": 0.3176,
      "step": 1786
    },
    {
      "epoch": 0.06928034892173492,
      "grad_norm": 0.20484000444412231,
      "learning_rate": 0.00018617961842717543,
      "loss": 0.2284,
      "step": 1787
    },
    {
      "epoch": 0.0693191180033923,
      "grad_norm": 0.28984546661376953,
      "learning_rate": 0.0001861718628819606,
      "loss": 0.2836,
      "step": 1788
    },
    {
      "epoch": 0.06935788708504967,
      "grad_norm": 0.3177304267883301,
      "learning_rate": 0.00018616410733674578,
      "loss": 0.3371,
      "step": 1789
    },
    {
      "epoch": 0.06939665616670705,
      "grad_norm": 0.253711998462677,
      "learning_rate": 0.00018615635179153095,
      "loss": 0.271,
      "step": 1790
    },
    {
      "epoch": 0.06943542524836444,
      "grad_norm": 0.2424374222755432,
      "learning_rate": 0.00018614859624631612,
      "loss": 0.2447,
      "step": 1791
    },
    {
      "epoch": 0.0694741943300218,
      "grad_norm": 0.22649334371089935,
      "learning_rate": 0.0001861408407011013,
      "loss": 0.2706,
      "step": 1792
    },
    {
      "epoch": 0.06951296341167919,
      "grad_norm": 0.2102464735507965,
      "learning_rate": 0.00018613308515588647,
      "loss": 0.2186,
      "step": 1793
    },
    {
      "epoch": 0.06955173249333656,
      "grad_norm": 0.2098294049501419,
      "learning_rate": 0.00018612532961067164,
      "loss": 0.226,
      "step": 1794
    },
    {
      "epoch": 0.06959050157499394,
      "grad_norm": 0.24888275563716888,
      "learning_rate": 0.0001861175740654568,
      "loss": 0.3054,
      "step": 1795
    },
    {
      "epoch": 0.06962927065665132,
      "grad_norm": 0.18768689036369324,
      "learning_rate": 0.00018610981852024199,
      "loss": 0.2013,
      "step": 1796
    },
    {
      "epoch": 0.06966803973830869,
      "grad_norm": 0.23311588168144226,
      "learning_rate": 0.00018610206297502716,
      "loss": 0.2654,
      "step": 1797
    },
    {
      "epoch": 0.06970680881996608,
      "grad_norm": 0.2477208375930786,
      "learning_rate": 0.00018609430742981233,
      "loss": 0.3007,
      "step": 1798
    },
    {
      "epoch": 0.06974557790162346,
      "grad_norm": 0.30051693320274353,
      "learning_rate": 0.0001860865518845975,
      "loss": 0.2858,
      "step": 1799
    },
    {
      "epoch": 0.06978434698328083,
      "grad_norm": 0.2753455340862274,
      "learning_rate": 0.00018607879633938268,
      "loss": 0.3039,
      "step": 1800
    },
    {
      "epoch": 0.06982311606493821,
      "grad_norm": 0.3140665292739868,
      "learning_rate": 0.00018607104079416785,
      "loss": 0.3034,
      "step": 1801
    },
    {
      "epoch": 0.0698618851465956,
      "grad_norm": 0.23078659176826477,
      "learning_rate": 0.000186063285248953,
      "loss": 0.2292,
      "step": 1802
    },
    {
      "epoch": 0.06990065422825296,
      "grad_norm": 0.3067777156829834,
      "learning_rate": 0.0001860555297037382,
      "loss": 0.2862,
      "step": 1803
    },
    {
      "epoch": 0.06993942330991035,
      "grad_norm": 0.21677815914154053,
      "learning_rate": 0.00018604777415852334,
      "loss": 0.2115,
      "step": 1804
    },
    {
      "epoch": 0.06997819239156773,
      "grad_norm": 0.259775847196579,
      "learning_rate": 0.00018604001861330854,
      "loss": 0.2629,
      "step": 1805
    },
    {
      "epoch": 0.0700169614732251,
      "grad_norm": 0.23242506384849548,
      "learning_rate": 0.0001860322630680937,
      "loss": 0.2375,
      "step": 1806
    },
    {
      "epoch": 0.07005573055488248,
      "grad_norm": 0.28712254762649536,
      "learning_rate": 0.00018602450752287889,
      "loss": 0.2962,
      "step": 1807
    },
    {
      "epoch": 0.07009449963653985,
      "grad_norm": 0.32976871728897095,
      "learning_rate": 0.00018601675197766403,
      "loss": 0.3228,
      "step": 1808
    },
    {
      "epoch": 0.07013326871819724,
      "grad_norm": 0.3288671672344208,
      "learning_rate": 0.0001860089964324492,
      "loss": 0.2996,
      "step": 1809
    },
    {
      "epoch": 0.07017203779985462,
      "grad_norm": 0.2497781366109848,
      "learning_rate": 0.00018600124088723438,
      "loss": 0.252,
      "step": 1810
    },
    {
      "epoch": 0.07021080688151199,
      "grad_norm": 0.23350228369235992,
      "learning_rate": 0.00018599348534201955,
      "loss": 0.2201,
      "step": 1811
    },
    {
      "epoch": 0.07024957596316937,
      "grad_norm": 0.30471038818359375,
      "learning_rate": 0.00018598572979680472,
      "loss": 0.2713,
      "step": 1812
    },
    {
      "epoch": 0.07028834504482676,
      "grad_norm": 0.24624115228652954,
      "learning_rate": 0.0001859779742515899,
      "loss": 0.2131,
      "step": 1813
    },
    {
      "epoch": 0.07032711412648412,
      "grad_norm": 0.24708232283592224,
      "learning_rate": 0.00018597021870637507,
      "loss": 0.2605,
      "step": 1814
    },
    {
      "epoch": 0.07036588320814151,
      "grad_norm": 0.25121942162513733,
      "learning_rate": 0.00018596246316116024,
      "loss": 0.294,
      "step": 1815
    },
    {
      "epoch": 0.07040465228979889,
      "grad_norm": 0.19758807122707367,
      "learning_rate": 0.0001859547076159454,
      "loss": 0.2442,
      "step": 1816
    },
    {
      "epoch": 0.07044342137145626,
      "grad_norm": 0.37123045325279236,
      "learning_rate": 0.0001859469520707306,
      "loss": 0.3804,
      "step": 1817
    },
    {
      "epoch": 0.07048219045311364,
      "grad_norm": 0.24467943608760834,
      "learning_rate": 0.00018593919652551573,
      "loss": 0.2382,
      "step": 1818
    },
    {
      "epoch": 0.07052095953477101,
      "grad_norm": 0.21710412204265594,
      "learning_rate": 0.00018593144098030093,
      "loss": 0.264,
      "step": 1819
    },
    {
      "epoch": 0.0705597286164284,
      "grad_norm": 0.2321401685476303,
      "learning_rate": 0.0001859236854350861,
      "loss": 0.2745,
      "step": 1820
    },
    {
      "epoch": 0.07059849769808578,
      "grad_norm": 0.26936668157577515,
      "learning_rate": 0.00018591592988987128,
      "loss": 0.2895,
      "step": 1821
    },
    {
      "epoch": 0.07063726677974315,
      "grad_norm": 0.2349502593278885,
      "learning_rate": 0.00018590817434465645,
      "loss": 0.2493,
      "step": 1822
    },
    {
      "epoch": 0.07067603586140053,
      "grad_norm": 0.23762862384319305,
      "learning_rate": 0.0001859004187994416,
      "loss": 0.2446,
      "step": 1823
    },
    {
      "epoch": 0.07071480494305792,
      "grad_norm": 0.1844540387392044,
      "learning_rate": 0.0001858926632542268,
      "loss": 0.1954,
      "step": 1824
    },
    {
      "epoch": 0.07075357402471529,
      "grad_norm": 0.28582727909088135,
      "learning_rate": 0.00018588490770901194,
      "loss": 0.3065,
      "step": 1825
    },
    {
      "epoch": 0.07079234310637267,
      "grad_norm": 0.2412651777267456,
      "learning_rate": 0.00018587715216379714,
      "loss": 0.2469,
      "step": 1826
    },
    {
      "epoch": 0.07083111218803005,
      "grad_norm": 0.32884687185287476,
      "learning_rate": 0.0001858693966185823,
      "loss": 0.3012,
      "step": 1827
    },
    {
      "epoch": 0.07086988126968742,
      "grad_norm": 0.29662972688674927,
      "learning_rate": 0.0001858616410733675,
      "loss": 0.2521,
      "step": 1828
    },
    {
      "epoch": 0.0709086503513448,
      "grad_norm": 0.28003308176994324,
      "learning_rate": 0.00018585388552815263,
      "loss": 0.2114,
      "step": 1829
    },
    {
      "epoch": 0.07094741943300219,
      "grad_norm": 0.29989466071128845,
      "learning_rate": 0.0001858461299829378,
      "loss": 0.2422,
      "step": 1830
    },
    {
      "epoch": 0.07098618851465956,
      "grad_norm": 0.3913407027721405,
      "learning_rate": 0.00018583837443772298,
      "loss": 0.3247,
      "step": 1831
    },
    {
      "epoch": 0.07102495759631694,
      "grad_norm": 0.3610263466835022,
      "learning_rate": 0.00018583061889250815,
      "loss": 0.332,
      "step": 1832
    },
    {
      "epoch": 0.07106372667797431,
      "grad_norm": 0.27625077962875366,
      "learning_rate": 0.00018582286334729333,
      "loss": 0.2746,
      "step": 1833
    },
    {
      "epoch": 0.07110249575963169,
      "grad_norm": 0.24602638185024261,
      "learning_rate": 0.0001858151078020785,
      "loss": 0.2566,
      "step": 1834
    },
    {
      "epoch": 0.07114126484128908,
      "grad_norm": 0.26982006430625916,
      "learning_rate": 0.00018580735225686367,
      "loss": 0.3335,
      "step": 1835
    },
    {
      "epoch": 0.07118003392294645,
      "grad_norm": 0.2664927542209625,
      "learning_rate": 0.00018579959671164884,
      "loss": 0.3081,
      "step": 1836
    },
    {
      "epoch": 0.07121880300460383,
      "grad_norm": 0.23742476105690002,
      "learning_rate": 0.000185791841166434,
      "loss": 0.2629,
      "step": 1837
    },
    {
      "epoch": 0.07125757208626121,
      "grad_norm": 0.2005058228969574,
      "learning_rate": 0.0001857840856212192,
      "loss": 0.2283,
      "step": 1838
    },
    {
      "epoch": 0.07129634116791858,
      "grad_norm": 0.19846540689468384,
      "learning_rate": 0.00018577633007600433,
      "loss": 0.2251,
      "step": 1839
    },
    {
      "epoch": 0.07133511024957596,
      "grad_norm": 0.2037762552499771,
      "learning_rate": 0.00018576857453078953,
      "loss": 0.2413,
      "step": 1840
    },
    {
      "epoch": 0.07137387933123335,
      "grad_norm": 0.33314505219459534,
      "learning_rate": 0.00018576081898557468,
      "loss": 0.3341,
      "step": 1841
    },
    {
      "epoch": 0.07141264841289072,
      "grad_norm": 0.3296930193901062,
      "learning_rate": 0.00018575306344035988,
      "loss": 0.3482,
      "step": 1842
    },
    {
      "epoch": 0.0714514174945481,
      "grad_norm": 0.20631863176822662,
      "learning_rate": 0.00018574530789514503,
      "loss": 0.2413,
      "step": 1843
    },
    {
      "epoch": 0.07149018657620547,
      "grad_norm": 0.3189554214477539,
      "learning_rate": 0.0001857375523499302,
      "loss": 0.2997,
      "step": 1844
    },
    {
      "epoch": 0.07152895565786285,
      "grad_norm": 0.24871307611465454,
      "learning_rate": 0.00018572979680471537,
      "loss": 0.2544,
      "step": 1845
    },
    {
      "epoch": 0.07156772473952024,
      "grad_norm": 0.26846539974212646,
      "learning_rate": 0.00018572204125950054,
      "loss": 0.2576,
      "step": 1846
    },
    {
      "epoch": 0.0716064938211776,
      "grad_norm": 0.2959567904472351,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.2633,
      "step": 1847
    },
    {
      "epoch": 0.07164526290283499,
      "grad_norm": 0.31336671113967896,
      "learning_rate": 0.0001857065301690709,
      "loss": 0.2514,
      "step": 1848
    },
    {
      "epoch": 0.07168403198449237,
      "grad_norm": 0.29088982939720154,
      "learning_rate": 0.00018569877462385606,
      "loss": 0.2471,
      "step": 1849
    },
    {
      "epoch": 0.07172280106614974,
      "grad_norm": 0.26505047082901,
      "learning_rate": 0.00018569101907864124,
      "loss": 0.2724,
      "step": 1850
    },
    {
      "epoch": 0.07176157014780712,
      "grad_norm": 0.3164748251438141,
      "learning_rate": 0.0001856832635334264,
      "loss": 0.2779,
      "step": 1851
    },
    {
      "epoch": 0.07180033922946451,
      "grad_norm": 0.2617391049861908,
      "learning_rate": 0.00018567550798821158,
      "loss": 0.2073,
      "step": 1852
    },
    {
      "epoch": 0.07183910831112188,
      "grad_norm": 0.28469815850257874,
      "learning_rate": 0.00018566775244299675,
      "loss": 0.2918,
      "step": 1853
    },
    {
      "epoch": 0.07187787739277926,
      "grad_norm": 0.29739904403686523,
      "learning_rate": 0.00018565999689778193,
      "loss": 0.3226,
      "step": 1854
    },
    {
      "epoch": 0.07191664647443664,
      "grad_norm": 0.23245947062969208,
      "learning_rate": 0.0001856522413525671,
      "loss": 0.2802,
      "step": 1855
    },
    {
      "epoch": 0.07195541555609401,
      "grad_norm": 0.2036193311214447,
      "learning_rate": 0.00018564448580735227,
      "loss": 0.2461,
      "step": 1856
    },
    {
      "epoch": 0.0719941846377514,
      "grad_norm": 0.2250671088695526,
      "learning_rate": 0.00018563673026213745,
      "loss": 0.2398,
      "step": 1857
    },
    {
      "epoch": 0.07203295371940877,
      "grad_norm": 0.21487939357757568,
      "learning_rate": 0.0001856289747169226,
      "loss": 0.2552,
      "step": 1858
    },
    {
      "epoch": 0.07207172280106615,
      "grad_norm": 0.22107934951782227,
      "learning_rate": 0.0001856212191717078,
      "loss": 0.2624,
      "step": 1859
    },
    {
      "epoch": 0.07211049188272353,
      "grad_norm": 0.2445676624774933,
      "learning_rate": 0.00018561346362649294,
      "loss": 0.3016,
      "step": 1860
    },
    {
      "epoch": 0.0721492609643809,
      "grad_norm": 0.21173177659511566,
      "learning_rate": 0.00018560570808127814,
      "loss": 0.2159,
      "step": 1861
    },
    {
      "epoch": 0.07218803004603828,
      "grad_norm": 0.26696234941482544,
      "learning_rate": 0.00018559795253606328,
      "loss": 0.2908,
      "step": 1862
    },
    {
      "epoch": 0.07222679912769567,
      "grad_norm": 0.26629140973091125,
      "learning_rate": 0.00018559019699084848,
      "loss": 0.2519,
      "step": 1863
    },
    {
      "epoch": 0.07226556820935304,
      "grad_norm": 0.30039212107658386,
      "learning_rate": 0.00018558244144563363,
      "loss": 0.3139,
      "step": 1864
    },
    {
      "epoch": 0.07230433729101042,
      "grad_norm": 0.24175599217414856,
      "learning_rate": 0.0001855746859004188,
      "loss": 0.2212,
      "step": 1865
    },
    {
      "epoch": 0.0723431063726678,
      "grad_norm": 0.20150001347064972,
      "learning_rate": 0.00018556693035520397,
      "loss": 0.1958,
      "step": 1866
    },
    {
      "epoch": 0.07238187545432517,
      "grad_norm": 0.21932587027549744,
      "learning_rate": 0.00018555917480998915,
      "loss": 0.2544,
      "step": 1867
    },
    {
      "epoch": 0.07242064453598256,
      "grad_norm": 0.21037693321704865,
      "learning_rate": 0.00018555141926477432,
      "loss": 0.2093,
      "step": 1868
    },
    {
      "epoch": 0.07245941361763993,
      "grad_norm": 0.2865290343761444,
      "learning_rate": 0.0001855436637195595,
      "loss": 0.3439,
      "step": 1869
    },
    {
      "epoch": 0.07249818269929731,
      "grad_norm": 0.21921928226947784,
      "learning_rate": 0.00018553590817434466,
      "loss": 0.2727,
      "step": 1870
    },
    {
      "epoch": 0.07253695178095469,
      "grad_norm": 0.3273702561855316,
      "learning_rate": 0.00018552815262912984,
      "loss": 0.2669,
      "step": 1871
    },
    {
      "epoch": 0.07257572086261206,
      "grad_norm": 0.2674447000026703,
      "learning_rate": 0.000185520397083915,
      "loss": 0.2423,
      "step": 1872
    },
    {
      "epoch": 0.07261448994426944,
      "grad_norm": 0.27383992075920105,
      "learning_rate": 0.00018551264153870018,
      "loss": 0.2183,
      "step": 1873
    },
    {
      "epoch": 0.07265325902592683,
      "grad_norm": 0.37918347120285034,
      "learning_rate": 0.00018550488599348536,
      "loss": 0.3058,
      "step": 1874
    },
    {
      "epoch": 0.0726920281075842,
      "grad_norm": 0.2441255748271942,
      "learning_rate": 0.00018549713044827053,
      "loss": 0.1813,
      "step": 1875
    },
    {
      "epoch": 0.07273079718924158,
      "grad_norm": 0.33111923933029175,
      "learning_rate": 0.0001854893749030557,
      "loss": 0.3044,
      "step": 1876
    },
    {
      "epoch": 0.07276956627089896,
      "grad_norm": 0.2907407581806183,
      "learning_rate": 0.00018548161935784087,
      "loss": 0.2952,
      "step": 1877
    },
    {
      "epoch": 0.07280833535255633,
      "grad_norm": 0.261069655418396,
      "learning_rate": 0.00018547386381262605,
      "loss": 0.2738,
      "step": 1878
    },
    {
      "epoch": 0.07284710443421372,
      "grad_norm": 0.2596657872200012,
      "learning_rate": 0.0001854661082674112,
      "loss": 0.2917,
      "step": 1879
    },
    {
      "epoch": 0.0728858735158711,
      "grad_norm": 0.2591628432273865,
      "learning_rate": 0.0001854583527221964,
      "loss": 0.305,
      "step": 1880
    },
    {
      "epoch": 0.07292464259752847,
      "grad_norm": 0.24054668843746185,
      "learning_rate": 0.00018545059717698154,
      "loss": 0.2437,
      "step": 1881
    },
    {
      "epoch": 0.07296341167918585,
      "grad_norm": 0.2597227990627289,
      "learning_rate": 0.00018544284163176674,
      "loss": 0.2408,
      "step": 1882
    },
    {
      "epoch": 0.07300218076084322,
      "grad_norm": 0.2942802608013153,
      "learning_rate": 0.00018543508608655188,
      "loss": 0.35,
      "step": 1883
    },
    {
      "epoch": 0.0730409498425006,
      "grad_norm": 0.23458272218704224,
      "learning_rate": 0.00018542733054133708,
      "loss": 0.2293,
      "step": 1884
    },
    {
      "epoch": 0.07307971892415799,
      "grad_norm": 0.2350122183561325,
      "learning_rate": 0.00018541957499612223,
      "loss": 0.2318,
      "step": 1885
    },
    {
      "epoch": 0.07311848800581536,
      "grad_norm": 0.25620537996292114,
      "learning_rate": 0.0001854118194509074,
      "loss": 0.2477,
      "step": 1886
    },
    {
      "epoch": 0.07315725708747274,
      "grad_norm": 0.2994065284729004,
      "learning_rate": 0.00018540406390569258,
      "loss": 0.2626,
      "step": 1887
    },
    {
      "epoch": 0.07319602616913012,
      "grad_norm": 0.3092086613178253,
      "learning_rate": 0.00018539630836047775,
      "loss": 0.2823,
      "step": 1888
    },
    {
      "epoch": 0.0732347952507875,
      "grad_norm": 0.3008871376514435,
      "learning_rate": 0.00018538855281526292,
      "loss": 0.3117,
      "step": 1889
    },
    {
      "epoch": 0.07327356433244488,
      "grad_norm": 0.29515665769577026,
      "learning_rate": 0.0001853807972700481,
      "loss": 0.3181,
      "step": 1890
    },
    {
      "epoch": 0.07331233341410226,
      "grad_norm": 0.22193807363510132,
      "learning_rate": 0.00018537304172483327,
      "loss": 0.2395,
      "step": 1891
    },
    {
      "epoch": 0.07335110249575963,
      "grad_norm": 0.25054872035980225,
      "learning_rate": 0.00018536528617961844,
      "loss": 0.217,
      "step": 1892
    },
    {
      "epoch": 0.07338987157741701,
      "grad_norm": 0.3554899990558624,
      "learning_rate": 0.00018535753063440359,
      "loss": 0.367,
      "step": 1893
    },
    {
      "epoch": 0.07342864065907438,
      "grad_norm": 0.3406619727611542,
      "learning_rate": 0.00018534977508918879,
      "loss": 0.3387,
      "step": 1894
    },
    {
      "epoch": 0.07346740974073176,
      "grad_norm": 0.265360027551651,
      "learning_rate": 0.00018534201954397393,
      "loss": 0.2288,
      "step": 1895
    },
    {
      "epoch": 0.07350617882238915,
      "grad_norm": 0.28926533460617065,
      "learning_rate": 0.00018533426399875913,
      "loss": 0.2746,
      "step": 1896
    },
    {
      "epoch": 0.07354494790404652,
      "grad_norm": 0.26073968410491943,
      "learning_rate": 0.00018532650845354428,
      "loss": 0.2555,
      "step": 1897
    },
    {
      "epoch": 0.0735837169857039,
      "grad_norm": 0.2581103444099426,
      "learning_rate": 0.00018531875290832948,
      "loss": 0.2361,
      "step": 1898
    },
    {
      "epoch": 0.07362248606736128,
      "grad_norm": 0.23617155849933624,
      "learning_rate": 0.00018531099736311465,
      "loss": 0.2451,
      "step": 1899
    },
    {
      "epoch": 0.07366125514901865,
      "grad_norm": 0.25421375036239624,
      "learning_rate": 0.0001853032418178998,
      "loss": 0.252,
      "step": 1900
    },
    {
      "epoch": 0.07370002423067604,
      "grad_norm": 0.2281671017408371,
      "learning_rate": 0.000185295486272685,
      "loss": 0.1903,
      "step": 1901
    },
    {
      "epoch": 0.07373879331233342,
      "grad_norm": 0.27492523193359375,
      "learning_rate": 0.00018528773072747014,
      "loss": 0.2428,
      "step": 1902
    },
    {
      "epoch": 0.07377756239399079,
      "grad_norm": 0.2294924408197403,
      "learning_rate": 0.00018527997518225534,
      "loss": 0.2261,
      "step": 1903
    },
    {
      "epoch": 0.07381633147564817,
      "grad_norm": 0.29306262731552124,
      "learning_rate": 0.00018527221963704049,
      "loss": 0.3113,
      "step": 1904
    },
    {
      "epoch": 0.07385510055730554,
      "grad_norm": 0.26451241970062256,
      "learning_rate": 0.00018526446409182569,
      "loss": 0.2749,
      "step": 1905
    },
    {
      "epoch": 0.07389386963896293,
      "grad_norm": 0.2541757822036743,
      "learning_rate": 0.00018525670854661083,
      "loss": 0.2882,
      "step": 1906
    },
    {
      "epoch": 0.07393263872062031,
      "grad_norm": 0.24802856147289276,
      "learning_rate": 0.000185248953001396,
      "loss": 0.2881,
      "step": 1907
    },
    {
      "epoch": 0.07397140780227768,
      "grad_norm": 0.22380280494689941,
      "learning_rate": 0.00018524119745618118,
      "loss": 0.2208,
      "step": 1908
    },
    {
      "epoch": 0.07401017688393506,
      "grad_norm": 0.3083789348602295,
      "learning_rate": 0.00018523344191096635,
      "loss": 0.3291,
      "step": 1909
    },
    {
      "epoch": 0.07404894596559244,
      "grad_norm": 0.2453755885362625,
      "learning_rate": 0.00018522568636575152,
      "loss": 0.2358,
      "step": 1910
    },
    {
      "epoch": 0.07408771504724981,
      "grad_norm": 0.24692760407924652,
      "learning_rate": 0.0001852179308205367,
      "loss": 0.2538,
      "step": 1911
    },
    {
      "epoch": 0.0741264841289072,
      "grad_norm": 0.3238418698310852,
      "learning_rate": 0.00018521017527532187,
      "loss": 0.3048,
      "step": 1912
    },
    {
      "epoch": 0.07416525321056458,
      "grad_norm": 0.3197022080421448,
      "learning_rate": 0.00018520241973010704,
      "loss": 0.286,
      "step": 1913
    },
    {
      "epoch": 0.07420402229222195,
      "grad_norm": 0.24462710320949554,
      "learning_rate": 0.0001851946641848922,
      "loss": 0.2394,
      "step": 1914
    },
    {
      "epoch": 0.07424279137387933,
      "grad_norm": 0.28358712792396545,
      "learning_rate": 0.0001851869086396774,
      "loss": 0.2445,
      "step": 1915
    },
    {
      "epoch": 0.07428156045553672,
      "grad_norm": 0.1968732625246048,
      "learning_rate": 0.00018517915309446253,
      "loss": 0.1946,
      "step": 1916
    },
    {
      "epoch": 0.07432032953719409,
      "grad_norm": 0.2727065980434418,
      "learning_rate": 0.00018517139754924773,
      "loss": 0.2735,
      "step": 1917
    },
    {
      "epoch": 0.07435909861885147,
      "grad_norm": 0.24923673272132874,
      "learning_rate": 0.00018516364200403288,
      "loss": 0.243,
      "step": 1918
    },
    {
      "epoch": 0.07439786770050884,
      "grad_norm": 0.2688484787940979,
      "learning_rate": 0.00018515588645881808,
      "loss": 0.2666,
      "step": 1919
    },
    {
      "epoch": 0.07443663678216622,
      "grad_norm": 0.24436131119728088,
      "learning_rate": 0.00018514813091360322,
      "loss": 0.2756,
      "step": 1920
    },
    {
      "epoch": 0.0744754058638236,
      "grad_norm": 0.21759195625782013,
      "learning_rate": 0.0001851403753683884,
      "loss": 0.1997,
      "step": 1921
    },
    {
      "epoch": 0.07451417494548097,
      "grad_norm": 0.2920955419540405,
      "learning_rate": 0.00018513261982317357,
      "loss": 0.2592,
      "step": 1922
    },
    {
      "epoch": 0.07455294402713836,
      "grad_norm": 0.3047550320625305,
      "learning_rate": 0.00018512486427795874,
      "loss": 0.2744,
      "step": 1923
    },
    {
      "epoch": 0.07459171310879574,
      "grad_norm": 0.265664279460907,
      "learning_rate": 0.00018511710873274392,
      "loss": 0.2223,
      "step": 1924
    },
    {
      "epoch": 0.07463048219045311,
      "grad_norm": 0.2793257236480713,
      "learning_rate": 0.0001851093531875291,
      "loss": 0.2226,
      "step": 1925
    },
    {
      "epoch": 0.07466925127211049,
      "grad_norm": 0.19983623921871185,
      "learning_rate": 0.00018510159764231426,
      "loss": 0.2236,
      "step": 1926
    },
    {
      "epoch": 0.07470802035376788,
      "grad_norm": 0.37565845251083374,
      "learning_rate": 0.00018509384209709943,
      "loss": 0.282,
      "step": 1927
    },
    {
      "epoch": 0.07474678943542525,
      "grad_norm": 0.36269256472587585,
      "learning_rate": 0.0001850860865518846,
      "loss": 0.3194,
      "step": 1928
    },
    {
      "epoch": 0.07478555851708263,
      "grad_norm": 0.30326297879219055,
      "learning_rate": 0.00018507833100666978,
      "loss": 0.2605,
      "step": 1929
    },
    {
      "epoch": 0.07482432759874,
      "grad_norm": 0.33748289942741394,
      "learning_rate": 0.00018507057546145495,
      "loss": 0.3173,
      "step": 1930
    },
    {
      "epoch": 0.07486309668039738,
      "grad_norm": 0.3436301052570343,
      "learning_rate": 0.00018506281991624012,
      "loss": 0.2904,
      "step": 1931
    },
    {
      "epoch": 0.07490186576205476,
      "grad_norm": 0.2824835479259491,
      "learning_rate": 0.0001850550643710253,
      "loss": 0.2753,
      "step": 1932
    },
    {
      "epoch": 0.07494063484371213,
      "grad_norm": 0.2674463391304016,
      "learning_rate": 0.00018504730882581047,
      "loss": 0.2335,
      "step": 1933
    },
    {
      "epoch": 0.07497940392536952,
      "grad_norm": 0.224977508187294,
      "learning_rate": 0.00018503955328059564,
      "loss": 0.1902,
      "step": 1934
    },
    {
      "epoch": 0.0750181730070269,
      "grad_norm": 0.27023640275001526,
      "learning_rate": 0.0001850317977353808,
      "loss": 0.2167,
      "step": 1935
    },
    {
      "epoch": 0.07505694208868427,
      "grad_norm": 0.2537762522697449,
      "learning_rate": 0.000185024042190166,
      "loss": 0.2177,
      "step": 1936
    },
    {
      "epoch": 0.07509571117034165,
      "grad_norm": 0.22149528563022614,
      "learning_rate": 0.00018501628664495113,
      "loss": 0.1912,
      "step": 1937
    },
    {
      "epoch": 0.07513448025199904,
      "grad_norm": 0.305677592754364,
      "learning_rate": 0.00018500853109973633,
      "loss": 0.2149,
      "step": 1938
    },
    {
      "epoch": 0.0751732493336564,
      "grad_norm": 0.27343663573265076,
      "learning_rate": 0.00018500077555452148,
      "loss": 0.2607,
      "step": 1939
    },
    {
      "epoch": 0.07521201841531379,
      "grad_norm": 0.28392842411994934,
      "learning_rate": 0.00018499302000930668,
      "loss": 0.2639,
      "step": 1940
    },
    {
      "epoch": 0.07525078749697117,
      "grad_norm": 0.3286098837852478,
      "learning_rate": 0.00018498526446409183,
      "loss": 0.2939,
      "step": 1941
    },
    {
      "epoch": 0.07528955657862854,
      "grad_norm": 0.2870616912841797,
      "learning_rate": 0.000184977508918877,
      "loss": 0.243,
      "step": 1942
    },
    {
      "epoch": 0.07532832566028592,
      "grad_norm": 0.34178197383880615,
      "learning_rate": 0.00018496975337366217,
      "loss": 0.2486,
      "step": 1943
    },
    {
      "epoch": 0.0753670947419433,
      "grad_norm": 0.2497745007276535,
      "learning_rate": 0.00018496199782844734,
      "loss": 0.202,
      "step": 1944
    },
    {
      "epoch": 0.07540586382360068,
      "grad_norm": 0.24265538156032562,
      "learning_rate": 0.00018495424228323252,
      "loss": 0.2394,
      "step": 1945
    },
    {
      "epoch": 0.07544463290525806,
      "grad_norm": 0.2429477870464325,
      "learning_rate": 0.0001849464867380177,
      "loss": 0.1741,
      "step": 1946
    },
    {
      "epoch": 0.07548340198691543,
      "grad_norm": 0.36351215839385986,
      "learning_rate": 0.00018493873119280286,
      "loss": 0.2285,
      "step": 1947
    },
    {
      "epoch": 0.07552217106857281,
      "grad_norm": 0.3128373622894287,
      "learning_rate": 0.00018493097564758804,
      "loss": 0.2672,
      "step": 1948
    },
    {
      "epoch": 0.0755609401502302,
      "grad_norm": 0.3531453609466553,
      "learning_rate": 0.0001849232201023732,
      "loss": 0.3156,
      "step": 1949
    },
    {
      "epoch": 0.07559970923188757,
      "grad_norm": 0.25149500370025635,
      "learning_rate": 0.00018491546455715838,
      "loss": 0.2094,
      "step": 1950
    },
    {
      "epoch": 0.07563847831354495,
      "grad_norm": 0.23368261754512787,
      "learning_rate": 0.00018490770901194355,
      "loss": 0.2105,
      "step": 1951
    },
    {
      "epoch": 0.07567724739520233,
      "grad_norm": 0.3186008930206299,
      "learning_rate": 0.00018489995346672873,
      "loss": 0.2041,
      "step": 1952
    },
    {
      "epoch": 0.0757160164768597,
      "grad_norm": 0.4063701033592224,
      "learning_rate": 0.0001848921979215139,
      "loss": 0.2822,
      "step": 1953
    },
    {
      "epoch": 0.07575478555851708,
      "grad_norm": 0.2931073009967804,
      "learning_rate": 0.00018488444237629907,
      "loss": 0.2625,
      "step": 1954
    },
    {
      "epoch": 0.07579355464017445,
      "grad_norm": 0.2845739424228668,
      "learning_rate": 0.00018487668683108424,
      "loss": 0.279,
      "step": 1955
    },
    {
      "epoch": 0.07583232372183184,
      "grad_norm": 0.23238249123096466,
      "learning_rate": 0.0001848689312858694,
      "loss": 0.2251,
      "step": 1956
    },
    {
      "epoch": 0.07587109280348922,
      "grad_norm": 0.7456677556037903,
      "learning_rate": 0.0001848611757406546,
      "loss": 0.2517,
      "step": 1957
    },
    {
      "epoch": 0.07590986188514659,
      "grad_norm": 0.33450502157211304,
      "learning_rate": 0.00018485342019543974,
      "loss": 0.2708,
      "step": 1958
    },
    {
      "epoch": 0.07594863096680397,
      "grad_norm": 0.2533995807170868,
      "learning_rate": 0.00018484566465022494,
      "loss": 0.2354,
      "step": 1959
    },
    {
      "epoch": 0.07598740004846136,
      "grad_norm": 0.30484652519226074,
      "learning_rate": 0.00018483790910501008,
      "loss": 0.242,
      "step": 1960
    },
    {
      "epoch": 0.07602616913011873,
      "grad_norm": 0.2764492332935333,
      "learning_rate": 0.00018483015355979528,
      "loss": 0.2076,
      "step": 1961
    },
    {
      "epoch": 0.07606493821177611,
      "grad_norm": 0.24780716001987457,
      "learning_rate": 0.00018482239801458043,
      "loss": 0.2312,
      "step": 1962
    },
    {
      "epoch": 0.07610370729343349,
      "grad_norm": 0.30227258801460266,
      "learning_rate": 0.0001848146424693656,
      "loss": 0.2893,
      "step": 1963
    },
    {
      "epoch": 0.07614247637509086,
      "grad_norm": 0.31068024039268494,
      "learning_rate": 0.00018480688692415077,
      "loss": 0.3144,
      "step": 1964
    },
    {
      "epoch": 0.07618124545674824,
      "grad_norm": 0.32769259810447693,
      "learning_rate": 0.00018479913137893595,
      "loss": 0.2896,
      "step": 1965
    },
    {
      "epoch": 0.07622001453840563,
      "grad_norm": 0.24120356142520905,
      "learning_rate": 0.00018479137583372112,
      "loss": 0.2597,
      "step": 1966
    },
    {
      "epoch": 0.076258783620063,
      "grad_norm": 0.34408465027809143,
      "learning_rate": 0.0001847836202885063,
      "loss": 0.3197,
      "step": 1967
    },
    {
      "epoch": 0.07629755270172038,
      "grad_norm": 0.22431915998458862,
      "learning_rate": 0.00018477586474329146,
      "loss": 0.2318,
      "step": 1968
    },
    {
      "epoch": 0.07633632178337775,
      "grad_norm": 0.27687349915504456,
      "learning_rate": 0.00018476810919807664,
      "loss": 0.2554,
      "step": 1969
    },
    {
      "epoch": 0.07637509086503513,
      "grad_norm": 0.2771669328212738,
      "learning_rate": 0.00018476035365286178,
      "loss": 0.2744,
      "step": 1970
    },
    {
      "epoch": 0.07641385994669252,
      "grad_norm": 0.22166451811790466,
      "learning_rate": 0.00018475259810764698,
      "loss": 0.1977,
      "step": 1971
    },
    {
      "epoch": 0.07645262902834989,
      "grad_norm": 0.24139492213726044,
      "learning_rate": 0.00018474484256243213,
      "loss": 0.2153,
      "step": 1972
    },
    {
      "epoch": 0.07649139811000727,
      "grad_norm": 0.3202435374259949,
      "learning_rate": 0.00018473708701721733,
      "loss": 0.3011,
      "step": 1973
    },
    {
      "epoch": 0.07653016719166465,
      "grad_norm": 0.26247748732566833,
      "learning_rate": 0.00018472933147200247,
      "loss": 0.2499,
      "step": 1974
    },
    {
      "epoch": 0.07656893627332202,
      "grad_norm": 0.2989700734615326,
      "learning_rate": 0.00018472157592678767,
      "loss": 0.2919,
      "step": 1975
    },
    {
      "epoch": 0.0766077053549794,
      "grad_norm": 0.23393647372722626,
      "learning_rate": 0.00018471382038157282,
      "loss": 0.1997,
      "step": 1976
    },
    {
      "epoch": 0.07664647443663679,
      "grad_norm": 0.3318268954753876,
      "learning_rate": 0.000184706064836358,
      "loss": 0.2622,
      "step": 1977
    },
    {
      "epoch": 0.07668524351829416,
      "grad_norm": 0.2774631083011627,
      "learning_rate": 0.0001846983092911432,
      "loss": 0.1873,
      "step": 1978
    },
    {
      "epoch": 0.07672401259995154,
      "grad_norm": 0.3082433342933655,
      "learning_rate": 0.00018469055374592834,
      "loss": 0.2511,
      "step": 1979
    },
    {
      "epoch": 0.07676278168160891,
      "grad_norm": 0.2976533770561218,
      "learning_rate": 0.00018468279820071354,
      "loss": 0.2364,
      "step": 1980
    },
    {
      "epoch": 0.0768015507632663,
      "grad_norm": 0.5042532086372375,
      "learning_rate": 0.00018467504265549868,
      "loss": 0.3617,
      "step": 1981
    },
    {
      "epoch": 0.07684031984492368,
      "grad_norm": 0.32101762294769287,
      "learning_rate": 0.00018466728711028388,
      "loss": 0.2203,
      "step": 1982
    },
    {
      "epoch": 0.07687908892658105,
      "grad_norm": 0.28425076603889465,
      "learning_rate": 0.00018465953156506903,
      "loss": 0.22,
      "step": 1983
    },
    {
      "epoch": 0.07691785800823843,
      "grad_norm": 0.3970434367656708,
      "learning_rate": 0.0001846517760198542,
      "loss": 0.3514,
      "step": 1984
    },
    {
      "epoch": 0.07695662708989581,
      "grad_norm": 0.27003908157348633,
      "learning_rate": 0.00018464402047463937,
      "loss": 0.239,
      "step": 1985
    },
    {
      "epoch": 0.07699539617155318,
      "grad_norm": 0.24099038541316986,
      "learning_rate": 0.00018463626492942455,
      "loss": 0.2286,
      "step": 1986
    },
    {
      "epoch": 0.07703416525321057,
      "grad_norm": 0.29011210799217224,
      "learning_rate": 0.00018462850938420972,
      "loss": 0.2383,
      "step": 1987
    },
    {
      "epoch": 0.07707293433486795,
      "grad_norm": 0.24745529890060425,
      "learning_rate": 0.0001846207538389949,
      "loss": 0.2141,
      "step": 1988
    },
    {
      "epoch": 0.07711170341652532,
      "grad_norm": 0.24066932499408722,
      "learning_rate": 0.00018461299829378007,
      "loss": 0.2046,
      "step": 1989
    },
    {
      "epoch": 0.0771504724981827,
      "grad_norm": 0.2375435084104538,
      "learning_rate": 0.00018460524274856524,
      "loss": 0.2207,
      "step": 1990
    },
    {
      "epoch": 0.07718924157984008,
      "grad_norm": 0.3262627422809601,
      "learning_rate": 0.00018459748720335038,
      "loss": 0.2831,
      "step": 1991
    },
    {
      "epoch": 0.07722801066149745,
      "grad_norm": 0.2811242341995239,
      "learning_rate": 0.00018458973165813558,
      "loss": 0.2264,
      "step": 1992
    },
    {
      "epoch": 0.07726677974315484,
      "grad_norm": 0.2959141433238983,
      "learning_rate": 0.00018458197611292073,
      "loss": 0.2303,
      "step": 1993
    },
    {
      "epoch": 0.0773055488248122,
      "grad_norm": 0.3778800964355469,
      "learning_rate": 0.00018457422056770593,
      "loss": 0.3219,
      "step": 1994
    },
    {
      "epoch": 0.07734431790646959,
      "grad_norm": 0.31679093837738037,
      "learning_rate": 0.00018456646502249108,
      "loss": 0.265,
      "step": 1995
    },
    {
      "epoch": 0.07738308698812697,
      "grad_norm": 0.37115171551704407,
      "learning_rate": 0.00018455870947727628,
      "loss": 0.2705,
      "step": 1996
    },
    {
      "epoch": 0.07742185606978434,
      "grad_norm": 0.31078818440437317,
      "learning_rate": 0.00018455095393206142,
      "loss": 0.2626,
      "step": 1997
    },
    {
      "epoch": 0.07746062515144173,
      "grad_norm": 0.30243149399757385,
      "learning_rate": 0.0001845431983868466,
      "loss": 0.3064,
      "step": 1998
    },
    {
      "epoch": 0.07749939423309911,
      "grad_norm": 0.28174033761024475,
      "learning_rate": 0.00018453544284163177,
      "loss": 0.234,
      "step": 1999
    },
    {
      "epoch": 0.07753816331475648,
      "grad_norm": 0.2615840435028076,
      "learning_rate": 0.00018452768729641694,
      "loss": 0.2431,
      "step": 2000
    },
    {
      "epoch": 0.07757693239641386,
      "grad_norm": 0.22232197225093842,
      "learning_rate": 0.0001845199317512021,
      "loss": 0.2138,
      "step": 2001
    },
    {
      "epoch": 0.07761570147807124,
      "grad_norm": 0.21735718846321106,
      "learning_rate": 0.00018451217620598729,
      "loss": 0.1943,
      "step": 2002
    },
    {
      "epoch": 0.07765447055972861,
      "grad_norm": 0.2784954905509949,
      "learning_rate": 0.00018450442066077246,
      "loss": 0.247,
      "step": 2003
    },
    {
      "epoch": 0.077693239641386,
      "grad_norm": 0.25826194882392883,
      "learning_rate": 0.00018449666511555763,
      "loss": 0.2046,
      "step": 2004
    },
    {
      "epoch": 0.07773200872304337,
      "grad_norm": 0.38921424746513367,
      "learning_rate": 0.0001844889095703428,
      "loss": 0.3796,
      "step": 2005
    },
    {
      "epoch": 0.07777077780470075,
      "grad_norm": 0.3231884837150574,
      "learning_rate": 0.00018448115402512798,
      "loss": 0.2453,
      "step": 2006
    },
    {
      "epoch": 0.07780954688635813,
      "grad_norm": 0.2228299230337143,
      "learning_rate": 0.00018447339847991315,
      "loss": 0.1846,
      "step": 2007
    },
    {
      "epoch": 0.0778483159680155,
      "grad_norm": 0.3119843900203705,
      "learning_rate": 0.00018446564293469832,
      "loss": 0.2454,
      "step": 2008
    },
    {
      "epoch": 0.07788708504967289,
      "grad_norm": 0.32207831740379333,
      "learning_rate": 0.0001844578873894835,
      "loss": 0.2296,
      "step": 2009
    },
    {
      "epoch": 0.07792585413133027,
      "grad_norm": 0.2617672085762024,
      "learning_rate": 0.00018445013184426867,
      "loss": 0.2273,
      "step": 2010
    },
    {
      "epoch": 0.07796462321298764,
      "grad_norm": 0.3208692669868469,
      "learning_rate": 0.00018444237629905384,
      "loss": 0.2307,
      "step": 2011
    },
    {
      "epoch": 0.07800339229464502,
      "grad_norm": 0.3105791211128235,
      "learning_rate": 0.000184434620753839,
      "loss": 0.2258,
      "step": 2012
    },
    {
      "epoch": 0.0780421613763024,
      "grad_norm": 0.292217493057251,
      "learning_rate": 0.00018442686520862419,
      "loss": 0.2557,
      "step": 2013
    },
    {
      "epoch": 0.07808093045795977,
      "grad_norm": 0.28131935000419617,
      "learning_rate": 0.00018441910966340933,
      "loss": 0.2104,
      "step": 2014
    },
    {
      "epoch": 0.07811969953961716,
      "grad_norm": 0.27456021308898926,
      "learning_rate": 0.00018441135411819453,
      "loss": 0.1886,
      "step": 2015
    },
    {
      "epoch": 0.07815846862127453,
      "grad_norm": 0.2650863826274872,
      "learning_rate": 0.00018440359857297968,
      "loss": 0.195,
      "step": 2016
    },
    {
      "epoch": 0.07819723770293191,
      "grad_norm": 0.22250168025493622,
      "learning_rate": 0.00018439584302776488,
      "loss": 0.1435,
      "step": 2017
    },
    {
      "epoch": 0.0782360067845893,
      "grad_norm": 0.3240942358970642,
      "learning_rate": 0.00018438808748255002,
      "loss": 0.244,
      "step": 2018
    },
    {
      "epoch": 0.07827477586624666,
      "grad_norm": 0.3918350636959076,
      "learning_rate": 0.0001843803319373352,
      "loss": 0.3167,
      "step": 2019
    },
    {
      "epoch": 0.07831354494790405,
      "grad_norm": 0.30922219157218933,
      "learning_rate": 0.00018437257639212037,
      "loss": 0.2479,
      "step": 2020
    },
    {
      "epoch": 0.07835231402956143,
      "grad_norm": 0.38193467259407043,
      "learning_rate": 0.00018436482084690554,
      "loss": 0.3187,
      "step": 2021
    },
    {
      "epoch": 0.0783910831112188,
      "grad_norm": 0.28934136033058167,
      "learning_rate": 0.00018435706530169071,
      "loss": 0.2453,
      "step": 2022
    },
    {
      "epoch": 0.07842985219287618,
      "grad_norm": 0.36771416664123535,
      "learning_rate": 0.0001843493097564759,
      "loss": 0.2787,
      "step": 2023
    },
    {
      "epoch": 0.07846862127453356,
      "grad_norm": 0.26685354113578796,
      "learning_rate": 0.00018434155421126106,
      "loss": 0.22,
      "step": 2024
    },
    {
      "epoch": 0.07850739035619093,
      "grad_norm": 0.3426089882850647,
      "learning_rate": 0.00018433379866604623,
      "loss": 0.2742,
      "step": 2025
    },
    {
      "epoch": 0.07854615943784832,
      "grad_norm": 0.35473117232322693,
      "learning_rate": 0.0001843260431208314,
      "loss": 0.2836,
      "step": 2026
    },
    {
      "epoch": 0.0785849285195057,
      "grad_norm": 0.30634748935699463,
      "learning_rate": 0.00018431828757561658,
      "loss": 0.2581,
      "step": 2027
    },
    {
      "epoch": 0.07862369760116307,
      "grad_norm": 0.33257755637168884,
      "learning_rate": 0.00018431053203040175,
      "loss": 0.3053,
      "step": 2028
    },
    {
      "epoch": 0.07866246668282045,
      "grad_norm": 0.33119505643844604,
      "learning_rate": 0.00018430277648518692,
      "loss": 0.2568,
      "step": 2029
    },
    {
      "epoch": 0.07870123576447782,
      "grad_norm": 0.28210052847862244,
      "learning_rate": 0.0001842950209399721,
      "loss": 0.2386,
      "step": 2030
    },
    {
      "epoch": 0.0787400048461352,
      "grad_norm": 0.252821683883667,
      "learning_rate": 0.00018428726539475727,
      "loss": 0.234,
      "step": 2031
    },
    {
      "epoch": 0.07877877392779259,
      "grad_norm": 0.24882714450359344,
      "learning_rate": 0.00018427950984954244,
      "loss": 0.2095,
      "step": 2032
    },
    {
      "epoch": 0.07881754300944996,
      "grad_norm": 0.2631865441799164,
      "learning_rate": 0.0001842717543043276,
      "loss": 0.2325,
      "step": 2033
    },
    {
      "epoch": 0.07885631209110734,
      "grad_norm": 0.31420445442199707,
      "learning_rate": 0.0001842639987591128,
      "loss": 0.2373,
      "step": 2034
    },
    {
      "epoch": 0.07889508117276472,
      "grad_norm": 0.32976481318473816,
      "learning_rate": 0.00018425624321389793,
      "loss": 0.2473,
      "step": 2035
    },
    {
      "epoch": 0.0789338502544221,
      "grad_norm": 0.31156545877456665,
      "learning_rate": 0.00018424848766868313,
      "loss": 0.2682,
      "step": 2036
    },
    {
      "epoch": 0.07897261933607948,
      "grad_norm": 0.3602465093135834,
      "learning_rate": 0.00018424073212346828,
      "loss": 0.2829,
      "step": 2037
    },
    {
      "epoch": 0.07901138841773686,
      "grad_norm": 0.30339428782463074,
      "learning_rate": 0.00018423297657825348,
      "loss": 0.2951,
      "step": 2038
    },
    {
      "epoch": 0.07905015749939423,
      "grad_norm": 0.24983912706375122,
      "learning_rate": 0.00018422522103303863,
      "loss": 0.2439,
      "step": 2039
    },
    {
      "epoch": 0.07908892658105161,
      "grad_norm": 0.3626314401626587,
      "learning_rate": 0.0001842174654878238,
      "loss": 0.2892,
      "step": 2040
    },
    {
      "epoch": 0.07912769566270898,
      "grad_norm": 0.27798405289649963,
      "learning_rate": 0.00018420970994260897,
      "loss": 0.2229,
      "step": 2041
    },
    {
      "epoch": 0.07916646474436637,
      "grad_norm": 0.35065001249313354,
      "learning_rate": 0.00018420195439739414,
      "loss": 0.3155,
      "step": 2042
    },
    {
      "epoch": 0.07920523382602375,
      "grad_norm": 0.2944674491882324,
      "learning_rate": 0.00018419419885217932,
      "loss": 0.2047,
      "step": 2043
    },
    {
      "epoch": 0.07924400290768112,
      "grad_norm": 0.2450106292963028,
      "learning_rate": 0.0001841864433069645,
      "loss": 0.2407,
      "step": 2044
    },
    {
      "epoch": 0.0792827719893385,
      "grad_norm": 0.23328641057014465,
      "learning_rate": 0.00018417868776174966,
      "loss": 0.1764,
      "step": 2045
    },
    {
      "epoch": 0.07932154107099589,
      "grad_norm": 0.2022436112165451,
      "learning_rate": 0.00018417093221653483,
      "loss": 0.2229,
      "step": 2046
    },
    {
      "epoch": 0.07936031015265325,
      "grad_norm": 0.22328288853168488,
      "learning_rate": 0.00018416317667131998,
      "loss": 0.2185,
      "step": 2047
    },
    {
      "epoch": 0.07939907923431064,
      "grad_norm": 0.2627248764038086,
      "learning_rate": 0.00018415542112610518,
      "loss": 0.2433,
      "step": 2048
    },
    {
      "epoch": 0.07943784831596802,
      "grad_norm": 0.22857145965099335,
      "learning_rate": 0.00018414766558089033,
      "loss": 0.1841,
      "step": 2049
    },
    {
      "epoch": 0.07947661739762539,
      "grad_norm": 0.28618648648262024,
      "learning_rate": 0.00018413991003567553,
      "loss": 0.2091,
      "step": 2050
    },
    {
      "epoch": 0.07951538647928277,
      "grad_norm": 0.3339337110519409,
      "learning_rate": 0.00018413215449046067,
      "loss": 0.2934,
      "step": 2051
    },
    {
      "epoch": 0.07955415556094016,
      "grad_norm": 0.36383959650993347,
      "learning_rate": 0.00018412439894524587,
      "loss": 0.2902,
      "step": 2052
    },
    {
      "epoch": 0.07959292464259753,
      "grad_norm": 0.2736988365650177,
      "learning_rate": 0.00018411664340003102,
      "loss": 0.2402,
      "step": 2053
    },
    {
      "epoch": 0.07963169372425491,
      "grad_norm": 0.2989915907382965,
      "learning_rate": 0.0001841088878548162,
      "loss": 0.2152,
      "step": 2054
    },
    {
      "epoch": 0.07967046280591228,
      "grad_norm": 0.3308209180831909,
      "learning_rate": 0.00018410113230960136,
      "loss": 0.2356,
      "step": 2055
    },
    {
      "epoch": 0.07970923188756966,
      "grad_norm": 0.280186265707016,
      "learning_rate": 0.00018409337676438654,
      "loss": 0.243,
      "step": 2056
    },
    {
      "epoch": 0.07974800096922705,
      "grad_norm": 0.2894231379032135,
      "learning_rate": 0.00018408562121917174,
      "loss": 0.2439,
      "step": 2057
    },
    {
      "epoch": 0.07978677005088441,
      "grad_norm": 0.33441999554634094,
      "learning_rate": 0.00018407786567395688,
      "loss": 0.2675,
      "step": 2058
    },
    {
      "epoch": 0.0798255391325418,
      "grad_norm": 0.28186386823654175,
      "learning_rate": 0.00018407011012874208,
      "loss": 0.2544,
      "step": 2059
    },
    {
      "epoch": 0.07986430821419918,
      "grad_norm": 0.3119441568851471,
      "learning_rate": 0.00018406235458352723,
      "loss": 0.2093,
      "step": 2060
    },
    {
      "epoch": 0.07990307729585655,
      "grad_norm": 0.34940585494041443,
      "learning_rate": 0.0001840545990383124,
      "loss": 0.2639,
      "step": 2061
    },
    {
      "epoch": 0.07994184637751393,
      "grad_norm": 0.2959667146205902,
      "learning_rate": 0.00018404684349309757,
      "loss": 0.237,
      "step": 2062
    },
    {
      "epoch": 0.07998061545917132,
      "grad_norm": 0.256835013628006,
      "learning_rate": 0.00018403908794788275,
      "loss": 0.1896,
      "step": 2063
    },
    {
      "epoch": 0.08001938454082869,
      "grad_norm": 0.31647664308547974,
      "learning_rate": 0.00018403133240266792,
      "loss": 0.2353,
      "step": 2064
    },
    {
      "epoch": 0.08005815362248607,
      "grad_norm": 0.3498857021331787,
      "learning_rate": 0.0001840235768574531,
      "loss": 0.2932,
      "step": 2065
    },
    {
      "epoch": 0.08009692270414344,
      "grad_norm": 0.518442690372467,
      "learning_rate": 0.00018401582131223826,
      "loss": 0.313,
      "step": 2066
    },
    {
      "epoch": 0.08013569178580082,
      "grad_norm": 0.3162190616130829,
      "learning_rate": 0.00018400806576702344,
      "loss": 0.2641,
      "step": 2067
    },
    {
      "epoch": 0.0801744608674582,
      "grad_norm": 0.24169938266277313,
      "learning_rate": 0.00018400031022180858,
      "loss": 0.2112,
      "step": 2068
    },
    {
      "epoch": 0.08021322994911557,
      "grad_norm": 0.2819792628288269,
      "learning_rate": 0.00018399255467659378,
      "loss": 0.2665,
      "step": 2069
    },
    {
      "epoch": 0.08025199903077296,
      "grad_norm": 0.28094205260276794,
      "learning_rate": 0.00018398479913137893,
      "loss": 0.2114,
      "step": 2070
    },
    {
      "epoch": 0.08029076811243034,
      "grad_norm": 0.22333256900310516,
      "learning_rate": 0.00018397704358616413,
      "loss": 0.1944,
      "step": 2071
    },
    {
      "epoch": 0.08032953719408771,
      "grad_norm": 0.31597381830215454,
      "learning_rate": 0.00018396928804094927,
      "loss": 0.2847,
      "step": 2072
    },
    {
      "epoch": 0.0803683062757451,
      "grad_norm": 0.3171074688434601,
      "learning_rate": 0.00018396153249573447,
      "loss": 0.2438,
      "step": 2073
    },
    {
      "epoch": 0.08040707535740248,
      "grad_norm": 0.3242817521095276,
      "learning_rate": 0.00018395377695051962,
      "loss": 0.2818,
      "step": 2074
    },
    {
      "epoch": 0.08044584443905985,
      "grad_norm": 0.39851123094558716,
      "learning_rate": 0.0001839460214053048,
      "loss": 0.2719,
      "step": 2075
    },
    {
      "epoch": 0.08048461352071723,
      "grad_norm": 0.28420791029930115,
      "learning_rate": 0.00018393826586008996,
      "loss": 0.2077,
      "step": 2076
    },
    {
      "epoch": 0.08052338260237461,
      "grad_norm": 0.28797924518585205,
      "learning_rate": 0.00018393051031487514,
      "loss": 0.2044,
      "step": 2077
    },
    {
      "epoch": 0.08056215168403198,
      "grad_norm": 0.37158918380737305,
      "learning_rate": 0.0001839227547696603,
      "loss": 0.287,
      "step": 2078
    },
    {
      "epoch": 0.08060092076568937,
      "grad_norm": 0.3309451639652252,
      "learning_rate": 0.00018391499922444548,
      "loss": 0.2246,
      "step": 2079
    },
    {
      "epoch": 0.08063968984734673,
      "grad_norm": 0.33598822355270386,
      "learning_rate": 0.00018390724367923066,
      "loss": 0.2505,
      "step": 2080
    },
    {
      "epoch": 0.08067845892900412,
      "grad_norm": 0.2742200493812561,
      "learning_rate": 0.00018389948813401583,
      "loss": 0.2204,
      "step": 2081
    },
    {
      "epoch": 0.0807172280106615,
      "grad_norm": 0.2845686972141266,
      "learning_rate": 0.000183891732588801,
      "loss": 0.1999,
      "step": 2082
    },
    {
      "epoch": 0.08075599709231887,
      "grad_norm": 0.3938598334789276,
      "learning_rate": 0.00018388397704358617,
      "loss": 0.242,
      "step": 2083
    },
    {
      "epoch": 0.08079476617397625,
      "grad_norm": 0.3400435745716095,
      "learning_rate": 0.00018387622149837135,
      "loss": 0.2221,
      "step": 2084
    },
    {
      "epoch": 0.08083353525563364,
      "grad_norm": 0.2855897843837738,
      "learning_rate": 0.00018386846595315652,
      "loss": 0.2034,
      "step": 2085
    },
    {
      "epoch": 0.080872304337291,
      "grad_norm": 0.4024435579776764,
      "learning_rate": 0.0001838607104079417,
      "loss": 0.2705,
      "step": 2086
    },
    {
      "epoch": 0.08091107341894839,
      "grad_norm": 0.350206196308136,
      "learning_rate": 0.00018385295486272687,
      "loss": 0.2101,
      "step": 2087
    },
    {
      "epoch": 0.08094984250060577,
      "grad_norm": 0.3042347729206085,
      "learning_rate": 0.00018384519931751204,
      "loss": 0.1668,
      "step": 2088
    },
    {
      "epoch": 0.08098861158226314,
      "grad_norm": 0.3407781422138214,
      "learning_rate": 0.00018383744377229718,
      "loss": 0.2282,
      "step": 2089
    },
    {
      "epoch": 0.08102738066392053,
      "grad_norm": 0.4188114106655121,
      "learning_rate": 0.00018382968822708238,
      "loss": 0.2953,
      "step": 2090
    },
    {
      "epoch": 0.0810661497455779,
      "grad_norm": 0.305578351020813,
      "learning_rate": 0.00018382193268186753,
      "loss": 0.2315,
      "step": 2091
    },
    {
      "epoch": 0.08110491882723528,
      "grad_norm": 0.36230140924453735,
      "learning_rate": 0.00018381417713665273,
      "loss": 0.2353,
      "step": 2092
    },
    {
      "epoch": 0.08114368790889266,
      "grad_norm": 0.29064810276031494,
      "learning_rate": 0.00018380642159143788,
      "loss": 0.2257,
      "step": 2093
    },
    {
      "epoch": 0.08118245699055003,
      "grad_norm": 0.30602267384529114,
      "learning_rate": 0.00018379866604622308,
      "loss": 0.1937,
      "step": 2094
    },
    {
      "epoch": 0.08122122607220741,
      "grad_norm": 0.36347317695617676,
      "learning_rate": 0.00018379091050100822,
      "loss": 0.2997,
      "step": 2095
    },
    {
      "epoch": 0.0812599951538648,
      "grad_norm": 0.2358579933643341,
      "learning_rate": 0.0001837831549557934,
      "loss": 0.1626,
      "step": 2096
    },
    {
      "epoch": 0.08129876423552217,
      "grad_norm": 0.26839542388916016,
      "learning_rate": 0.00018377539941057857,
      "loss": 0.1953,
      "step": 2097
    },
    {
      "epoch": 0.08133753331717955,
      "grad_norm": 0.27796897292137146,
      "learning_rate": 0.00018376764386536374,
      "loss": 0.2223,
      "step": 2098
    },
    {
      "epoch": 0.08137630239883693,
      "grad_norm": 0.23711664974689484,
      "learning_rate": 0.0001837598883201489,
      "loss": 0.1859,
      "step": 2099
    },
    {
      "epoch": 0.0814150714804943,
      "grad_norm": 0.32736220955848694,
      "learning_rate": 0.00018375213277493409,
      "loss": 0.2425,
      "step": 2100
    },
    {
      "epoch": 0.08145384056215169,
      "grad_norm": 0.34451478719711304,
      "learning_rate": 0.00018374437722971926,
      "loss": 0.2636,
      "step": 2101
    },
    {
      "epoch": 0.08149260964380907,
      "grad_norm": 0.28252559900283813,
      "learning_rate": 0.00018373662168450443,
      "loss": 0.2303,
      "step": 2102
    },
    {
      "epoch": 0.08153137872546644,
      "grad_norm": 0.3200644850730896,
      "learning_rate": 0.0001837288661392896,
      "loss": 0.2616,
      "step": 2103
    },
    {
      "epoch": 0.08157014780712382,
      "grad_norm": 0.32443591952323914,
      "learning_rate": 0.00018372111059407478,
      "loss": 0.2722,
      "step": 2104
    },
    {
      "epoch": 0.08160891688878119,
      "grad_norm": 0.3759651780128479,
      "learning_rate": 0.00018371335504885995,
      "loss": 0.3051,
      "step": 2105
    },
    {
      "epoch": 0.08164768597043857,
      "grad_norm": 0.32043537497520447,
      "learning_rate": 0.00018370559950364512,
      "loss": 0.2763,
      "step": 2106
    },
    {
      "epoch": 0.08168645505209596,
      "grad_norm": 0.25598734617233276,
      "learning_rate": 0.0001836978439584303,
      "loss": 0.2257,
      "step": 2107
    },
    {
      "epoch": 0.08172522413375333,
      "grad_norm": 0.3239327669143677,
      "learning_rate": 0.00018369008841321547,
      "loss": 0.2832,
      "step": 2108
    },
    {
      "epoch": 0.08176399321541071,
      "grad_norm": 0.2801454961299896,
      "learning_rate": 0.00018368233286800064,
      "loss": 0.2468,
      "step": 2109
    },
    {
      "epoch": 0.0818027622970681,
      "grad_norm": 0.24219921231269836,
      "learning_rate": 0.00018367457732278579,
      "loss": 0.1787,
      "step": 2110
    },
    {
      "epoch": 0.08184153137872546,
      "grad_norm": 0.26315978169441223,
      "learning_rate": 0.00018366682177757099,
      "loss": 0.2364,
      "step": 2111
    },
    {
      "epoch": 0.08188030046038285,
      "grad_norm": 0.30915942788124084,
      "learning_rate": 0.00018365906623235613,
      "loss": 0.264,
      "step": 2112
    },
    {
      "epoch": 0.08191906954204023,
      "grad_norm": 0.26906275749206543,
      "learning_rate": 0.00018365131068714133,
      "loss": 0.2082,
      "step": 2113
    },
    {
      "epoch": 0.0819578386236976,
      "grad_norm": 0.32787975668907166,
      "learning_rate": 0.00018364355514192648,
      "loss": 0.2624,
      "step": 2114
    },
    {
      "epoch": 0.08199660770535498,
      "grad_norm": 0.3545720875263214,
      "learning_rate": 0.00018363579959671168,
      "loss": 0.2704,
      "step": 2115
    },
    {
      "epoch": 0.08203537678701235,
      "grad_norm": 0.32010647654533386,
      "learning_rate": 0.00018362804405149682,
      "loss": 0.2214,
      "step": 2116
    },
    {
      "epoch": 0.08207414586866973,
      "grad_norm": 0.32650160789489746,
      "learning_rate": 0.000183620288506282,
      "loss": 0.2446,
      "step": 2117
    },
    {
      "epoch": 0.08211291495032712,
      "grad_norm": 0.24548164010047913,
      "learning_rate": 0.00018361253296106717,
      "loss": 0.1662,
      "step": 2118
    },
    {
      "epoch": 0.08215168403198449,
      "grad_norm": 0.3463744819164276,
      "learning_rate": 0.00018360477741585234,
      "loss": 0.2903,
      "step": 2119
    },
    {
      "epoch": 0.08219045311364187,
      "grad_norm": 0.36282700300216675,
      "learning_rate": 0.00018359702187063751,
      "loss": 0.3056,
      "step": 2120
    },
    {
      "epoch": 0.08222922219529925,
      "grad_norm": 0.25799426436424255,
      "learning_rate": 0.0001835892663254227,
      "loss": 0.2031,
      "step": 2121
    },
    {
      "epoch": 0.08226799127695662,
      "grad_norm": 0.28868579864501953,
      "learning_rate": 0.00018358151078020786,
      "loss": 0.2171,
      "step": 2122
    },
    {
      "epoch": 0.082306760358614,
      "grad_norm": 0.2978714406490326,
      "learning_rate": 0.00018357375523499303,
      "loss": 0.2145,
      "step": 2123
    },
    {
      "epoch": 0.08234552944027139,
      "grad_norm": 0.28280186653137207,
      "learning_rate": 0.00018356599968977818,
      "loss": 0.2086,
      "step": 2124
    },
    {
      "epoch": 0.08238429852192876,
      "grad_norm": 0.36151647567749023,
      "learning_rate": 0.00018355824414456338,
      "loss": 0.2744,
      "step": 2125
    },
    {
      "epoch": 0.08242306760358614,
      "grad_norm": 0.2789982259273529,
      "learning_rate": 0.00018355048859934852,
      "loss": 0.2149,
      "step": 2126
    },
    {
      "epoch": 0.08246183668524351,
      "grad_norm": 0.5387896299362183,
      "learning_rate": 0.00018354273305413372,
      "loss": 0.3137,
      "step": 2127
    },
    {
      "epoch": 0.0825006057669009,
      "grad_norm": 0.31972721219062805,
      "learning_rate": 0.00018353497750891887,
      "loss": 0.2452,
      "step": 2128
    },
    {
      "epoch": 0.08253937484855828,
      "grad_norm": 0.2560039162635803,
      "learning_rate": 0.00018352722196370407,
      "loss": 0.1762,
      "step": 2129
    },
    {
      "epoch": 0.08257814393021565,
      "grad_norm": 0.36728671193122864,
      "learning_rate": 0.00018351946641848922,
      "loss": 0.2686,
      "step": 2130
    },
    {
      "epoch": 0.08261691301187303,
      "grad_norm": 0.2723909914493561,
      "learning_rate": 0.0001835117108732744,
      "loss": 0.2093,
      "step": 2131
    },
    {
      "epoch": 0.08265568209353041,
      "grad_norm": 0.2744981050491333,
      "learning_rate": 0.00018350395532805956,
      "loss": 0.2015,
      "step": 2132
    },
    {
      "epoch": 0.08269445117518778,
      "grad_norm": 0.3729780912399292,
      "learning_rate": 0.00018349619978284473,
      "loss": 0.2647,
      "step": 2133
    },
    {
      "epoch": 0.08273322025684517,
      "grad_norm": 0.25485238432884216,
      "learning_rate": 0.0001834884442376299,
      "loss": 0.2016,
      "step": 2134
    },
    {
      "epoch": 0.08277198933850255,
      "grad_norm": 0.3736717700958252,
      "learning_rate": 0.00018348068869241508,
      "loss": 0.2422,
      "step": 2135
    },
    {
      "epoch": 0.08281075842015992,
      "grad_norm": 0.36077550053596497,
      "learning_rate": 0.00018347293314720028,
      "loss": 0.2788,
      "step": 2136
    },
    {
      "epoch": 0.0828495275018173,
      "grad_norm": 0.25250521302223206,
      "learning_rate": 0.00018346517760198542,
      "loss": 0.2015,
      "step": 2137
    },
    {
      "epoch": 0.08288829658347469,
      "grad_norm": 0.31442469358444214,
      "learning_rate": 0.0001834574220567706,
      "loss": 0.2579,
      "step": 2138
    },
    {
      "epoch": 0.08292706566513205,
      "grad_norm": 0.37849077582359314,
      "learning_rate": 0.00018344966651155577,
      "loss": 0.2762,
      "step": 2139
    },
    {
      "epoch": 0.08296583474678944,
      "grad_norm": 0.3000822365283966,
      "learning_rate": 0.00018344191096634094,
      "loss": 0.2267,
      "step": 2140
    },
    {
      "epoch": 0.08300460382844681,
      "grad_norm": 0.2932770550251007,
      "learning_rate": 0.00018343415542112612,
      "loss": 0.2466,
      "step": 2141
    },
    {
      "epoch": 0.08304337291010419,
      "grad_norm": 0.3526036739349365,
      "learning_rate": 0.0001834263998759113,
      "loss": 0.2843,
      "step": 2142
    },
    {
      "epoch": 0.08308214199176157,
      "grad_norm": 0.20553924143314362,
      "learning_rate": 0.00018341864433069646,
      "loss": 0.2098,
      "step": 2143
    },
    {
      "epoch": 0.08312091107341894,
      "grad_norm": 0.25007328391075134,
      "learning_rate": 0.00018341088878548163,
      "loss": 0.2319,
      "step": 2144
    },
    {
      "epoch": 0.08315968015507633,
      "grad_norm": 0.2907940149307251,
      "learning_rate": 0.00018340313324026678,
      "loss": 0.1936,
      "step": 2145
    },
    {
      "epoch": 0.08319844923673371,
      "grad_norm": 0.3409484028816223,
      "learning_rate": 0.00018339537769505198,
      "loss": 0.2517,
      "step": 2146
    },
    {
      "epoch": 0.08323721831839108,
      "grad_norm": 0.281546026468277,
      "learning_rate": 0.00018338762214983713,
      "loss": 0.1948,
      "step": 2147
    },
    {
      "epoch": 0.08327598740004846,
      "grad_norm": 0.3816224932670593,
      "learning_rate": 0.00018337986660462233,
      "loss": 0.321,
      "step": 2148
    },
    {
      "epoch": 0.08331475648170585,
      "grad_norm": 0.34648653864860535,
      "learning_rate": 0.00018337211105940747,
      "loss": 0.2756,
      "step": 2149
    },
    {
      "epoch": 0.08335352556336321,
      "grad_norm": 0.3074461817741394,
      "learning_rate": 0.00018336435551419267,
      "loss": 0.2452,
      "step": 2150
    },
    {
      "epoch": 0.0833922946450206,
      "grad_norm": 0.3241651952266693,
      "learning_rate": 0.00018335659996897782,
      "loss": 0.2347,
      "step": 2151
    },
    {
      "epoch": 0.08343106372667797,
      "grad_norm": 0.27353203296661377,
      "learning_rate": 0.000183348844423763,
      "loss": 0.2143,
      "step": 2152
    },
    {
      "epoch": 0.08346983280833535,
      "grad_norm": 0.2225918024778366,
      "learning_rate": 0.00018334108887854816,
      "loss": 0.1597,
      "step": 2153
    },
    {
      "epoch": 0.08350860188999273,
      "grad_norm": 0.23147350549697876,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.1885,
      "step": 2154
    },
    {
      "epoch": 0.0835473709716501,
      "grad_norm": 0.37511178851127625,
      "learning_rate": 0.0001833255777881185,
      "loss": 0.3144,
      "step": 2155
    },
    {
      "epoch": 0.08358614005330749,
      "grad_norm": 0.3477049469947815,
      "learning_rate": 0.00018331782224290368,
      "loss": 0.2934,
      "step": 2156
    },
    {
      "epoch": 0.08362490913496487,
      "grad_norm": 0.29822462797164917,
      "learning_rate": 0.00018331006669768885,
      "loss": 0.2216,
      "step": 2157
    },
    {
      "epoch": 0.08366367821662224,
      "grad_norm": 0.2403685450553894,
      "learning_rate": 0.00018330231115247403,
      "loss": 0.2017,
      "step": 2158
    },
    {
      "epoch": 0.08370244729827962,
      "grad_norm": 0.27886220812797546,
      "learning_rate": 0.0001832945556072592,
      "loss": 0.1816,
      "step": 2159
    },
    {
      "epoch": 0.083741216379937,
      "grad_norm": 0.27123165130615234,
      "learning_rate": 0.00018328680006204437,
      "loss": 0.2082,
      "step": 2160
    },
    {
      "epoch": 0.08377998546159438,
      "grad_norm": 0.23994071781635284,
      "learning_rate": 0.00018327904451682954,
      "loss": 0.1834,
      "step": 2161
    },
    {
      "epoch": 0.08381875454325176,
      "grad_norm": 0.3682482838630676,
      "learning_rate": 0.00018327128897161472,
      "loss": 0.2814,
      "step": 2162
    },
    {
      "epoch": 0.08385752362490914,
      "grad_norm": 0.32166197896003723,
      "learning_rate": 0.0001832635334263999,
      "loss": 0.2353,
      "step": 2163
    },
    {
      "epoch": 0.08389629270656651,
      "grad_norm": 0.3181692361831665,
      "learning_rate": 0.00018325577788118506,
      "loss": 0.1849,
      "step": 2164
    },
    {
      "epoch": 0.0839350617882239,
      "grad_norm": 0.32332083582878113,
      "learning_rate": 0.00018324802233597024,
      "loss": 0.2429,
      "step": 2165
    },
    {
      "epoch": 0.08397383086988126,
      "grad_norm": 0.25926902890205383,
      "learning_rate": 0.00018324026679075538,
      "loss": 0.1874,
      "step": 2166
    },
    {
      "epoch": 0.08401259995153865,
      "grad_norm": 0.37281009554862976,
      "learning_rate": 0.00018323251124554058,
      "loss": 0.2718,
      "step": 2167
    },
    {
      "epoch": 0.08405136903319603,
      "grad_norm": 0.38203129172325134,
      "learning_rate": 0.00018322475570032573,
      "loss": 0.2693,
      "step": 2168
    },
    {
      "epoch": 0.0840901381148534,
      "grad_norm": 0.3625793159008026,
      "learning_rate": 0.00018321700015511093,
      "loss": 0.2778,
      "step": 2169
    },
    {
      "epoch": 0.08412890719651078,
      "grad_norm": 0.2501477599143982,
      "learning_rate": 0.00018320924460989607,
      "loss": 0.1889,
      "step": 2170
    },
    {
      "epoch": 0.08416767627816817,
      "grad_norm": 0.276074081659317,
      "learning_rate": 0.00018320148906468127,
      "loss": 0.2434,
      "step": 2171
    },
    {
      "epoch": 0.08420644535982554,
      "grad_norm": 0.21494650840759277,
      "learning_rate": 0.00018319373351946642,
      "loss": 0.2251,
      "step": 2172
    },
    {
      "epoch": 0.08424521444148292,
      "grad_norm": 0.2120656967163086,
      "learning_rate": 0.0001831859779742516,
      "loss": 0.204,
      "step": 2173
    },
    {
      "epoch": 0.0842839835231403,
      "grad_norm": 0.27366167306900024,
      "learning_rate": 0.00018317822242903676,
      "loss": 0.2525,
      "step": 2174
    },
    {
      "epoch": 0.08432275260479767,
      "grad_norm": 0.3087995648384094,
      "learning_rate": 0.00018317046688382194,
      "loss": 0.2746,
      "step": 2175
    },
    {
      "epoch": 0.08436152168645505,
      "grad_norm": 0.2634357810020447,
      "learning_rate": 0.0001831627113386071,
      "loss": 0.2299,
      "step": 2176
    },
    {
      "epoch": 0.08440029076811242,
      "grad_norm": 0.267975777387619,
      "learning_rate": 0.00018315495579339228,
      "loss": 0.2024,
      "step": 2177
    },
    {
      "epoch": 0.08443905984976981,
      "grad_norm": 0.34123438596725464,
      "learning_rate": 0.00018314720024817746,
      "loss": 0.2461,
      "step": 2178
    },
    {
      "epoch": 0.08447782893142719,
      "grad_norm": 0.36386099457740784,
      "learning_rate": 0.00018313944470296263,
      "loss": 0.2632,
      "step": 2179
    },
    {
      "epoch": 0.08451659801308456,
      "grad_norm": 0.4159248471260071,
      "learning_rate": 0.0001831316891577478,
      "loss": 0.2832,
      "step": 2180
    },
    {
      "epoch": 0.08455536709474194,
      "grad_norm": 0.3254714608192444,
      "learning_rate": 0.00018312393361253297,
      "loss": 0.1862,
      "step": 2181
    },
    {
      "epoch": 0.08459413617639933,
      "grad_norm": 0.35196635127067566,
      "learning_rate": 0.00018311617806731815,
      "loss": 0.2216,
      "step": 2182
    },
    {
      "epoch": 0.0846329052580567,
      "grad_norm": 0.34743931889533997,
      "learning_rate": 0.00018310842252210332,
      "loss": 0.2591,
      "step": 2183
    },
    {
      "epoch": 0.08467167433971408,
      "grad_norm": 0.350993275642395,
      "learning_rate": 0.0001831006669768885,
      "loss": 0.2243,
      "step": 2184
    },
    {
      "epoch": 0.08471044342137146,
      "grad_norm": 0.3508949875831604,
      "learning_rate": 0.00018309291143167367,
      "loss": 0.2826,
      "step": 2185
    },
    {
      "epoch": 0.08474921250302883,
      "grad_norm": 0.35129430890083313,
      "learning_rate": 0.00018308515588645884,
      "loss": 0.2682,
      "step": 2186
    },
    {
      "epoch": 0.08478798158468621,
      "grad_norm": 0.2572302222251892,
      "learning_rate": 0.00018307740034124398,
      "loss": 0.2198,
      "step": 2187
    },
    {
      "epoch": 0.0848267506663436,
      "grad_norm": 0.26086166501045227,
      "learning_rate": 0.00018306964479602918,
      "loss": 0.2215,
      "step": 2188
    },
    {
      "epoch": 0.08486551974800097,
      "grad_norm": 0.194442018866539,
      "learning_rate": 0.00018306188925081433,
      "loss": 0.1484,
      "step": 2189
    },
    {
      "epoch": 0.08490428882965835,
      "grad_norm": 0.29517990350723267,
      "learning_rate": 0.00018305413370559953,
      "loss": 0.2198,
      "step": 2190
    },
    {
      "epoch": 0.08494305791131572,
      "grad_norm": 0.30163711309432983,
      "learning_rate": 0.00018304637816038467,
      "loss": 0.2669,
      "step": 2191
    },
    {
      "epoch": 0.0849818269929731,
      "grad_norm": 0.16508837044239044,
      "learning_rate": 0.00018303862261516987,
      "loss": 0.1716,
      "step": 2192
    },
    {
      "epoch": 0.08502059607463049,
      "grad_norm": 0.2547047436237335,
      "learning_rate": 0.00018303086706995502,
      "loss": 0.2052,
      "step": 2193
    },
    {
      "epoch": 0.08505936515628786,
      "grad_norm": 0.2868663966655731,
      "learning_rate": 0.0001830231115247402,
      "loss": 0.2146,
      "step": 2194
    },
    {
      "epoch": 0.08509813423794524,
      "grad_norm": 0.3843502402305603,
      "learning_rate": 0.00018301535597952537,
      "loss": 0.2657,
      "step": 2195
    },
    {
      "epoch": 0.08513690331960262,
      "grad_norm": 0.3241342306137085,
      "learning_rate": 0.00018300760043431054,
      "loss": 0.2141,
      "step": 2196
    },
    {
      "epoch": 0.08517567240125999,
      "grad_norm": 0.3866936266422272,
      "learning_rate": 0.0001829998448890957,
      "loss": 0.2631,
      "step": 2197
    },
    {
      "epoch": 0.08521444148291737,
      "grad_norm": 0.3047422766685486,
      "learning_rate": 0.00018299208934388088,
      "loss": 0.1757,
      "step": 2198
    },
    {
      "epoch": 0.08525321056457476,
      "grad_norm": 0.2933928072452545,
      "learning_rate": 0.00018298433379866606,
      "loss": 0.1979,
      "step": 2199
    },
    {
      "epoch": 0.08529197964623213,
      "grad_norm": 0.361745148897171,
      "learning_rate": 0.00018297657825345123,
      "loss": 0.2204,
      "step": 2200
    },
    {
      "epoch": 0.08533074872788951,
      "grad_norm": 0.2756848633289337,
      "learning_rate": 0.00018296882270823638,
      "loss": 0.2053,
      "step": 2201
    },
    {
      "epoch": 0.08536951780954688,
      "grad_norm": 0.38564103841781616,
      "learning_rate": 0.00018296106716302158,
      "loss": 0.2642,
      "step": 2202
    },
    {
      "epoch": 0.08540828689120426,
      "grad_norm": 0.33667826652526855,
      "learning_rate": 0.00018295331161780672,
      "loss": 0.2455,
      "step": 2203
    },
    {
      "epoch": 0.08544705597286165,
      "grad_norm": 0.3172564208507538,
      "learning_rate": 0.00018294555607259192,
      "loss": 0.2026,
      "step": 2204
    },
    {
      "epoch": 0.08548582505451902,
      "grad_norm": 0.3314269185066223,
      "learning_rate": 0.00018293780052737707,
      "loss": 0.2405,
      "step": 2205
    },
    {
      "epoch": 0.0855245941361764,
      "grad_norm": 0.35640159249305725,
      "learning_rate": 0.00018293004498216227,
      "loss": 0.2662,
      "step": 2206
    },
    {
      "epoch": 0.08556336321783378,
      "grad_norm": 0.3121861517429352,
      "learning_rate": 0.0001829222894369474,
      "loss": 0.2558,
      "step": 2207
    },
    {
      "epoch": 0.08560213229949115,
      "grad_norm": 0.2585955560207367,
      "learning_rate": 0.00018291453389173259,
      "loss": 0.1967,
      "step": 2208
    },
    {
      "epoch": 0.08564090138114853,
      "grad_norm": 0.3471306562423706,
      "learning_rate": 0.00018290677834651776,
      "loss": 0.1781,
      "step": 2209
    },
    {
      "epoch": 0.08567967046280592,
      "grad_norm": 0.29569104313850403,
      "learning_rate": 0.00018289902280130293,
      "loss": 0.2194,
      "step": 2210
    },
    {
      "epoch": 0.08571843954446329,
      "grad_norm": 0.34243398904800415,
      "learning_rate": 0.0001828912672560881,
      "loss": 0.1995,
      "step": 2211
    },
    {
      "epoch": 0.08575720862612067,
      "grad_norm": 0.4385584890842438,
      "learning_rate": 0.00018288351171087328,
      "loss": 0.2795,
      "step": 2212
    },
    {
      "epoch": 0.08579597770777805,
      "grad_norm": 0.35588619112968445,
      "learning_rate": 0.00018287575616565845,
      "loss": 0.2349,
      "step": 2213
    },
    {
      "epoch": 0.08583474678943542,
      "grad_norm": 0.3737349510192871,
      "learning_rate": 0.00018286800062044362,
      "loss": 0.2051,
      "step": 2214
    },
    {
      "epoch": 0.0858735158710928,
      "grad_norm": 0.41680383682250977,
      "learning_rate": 0.0001828602450752288,
      "loss": 0.2246,
      "step": 2215
    },
    {
      "epoch": 0.08591228495275018,
      "grad_norm": 0.35831892490386963,
      "learning_rate": 0.00018285248953001397,
      "loss": 0.2311,
      "step": 2216
    },
    {
      "epoch": 0.08595105403440756,
      "grad_norm": 0.38622400164604187,
      "learning_rate": 0.00018284473398479914,
      "loss": 0.2395,
      "step": 2217
    },
    {
      "epoch": 0.08598982311606494,
      "grad_norm": 0.379254549741745,
      "learning_rate": 0.00018283697843958431,
      "loss": 0.1928,
      "step": 2218
    },
    {
      "epoch": 0.08602859219772231,
      "grad_norm": 0.4194527566432953,
      "learning_rate": 0.0001828292228943695,
      "loss": 0.3101,
      "step": 2219
    },
    {
      "epoch": 0.0860673612793797,
      "grad_norm": 0.28719207644462585,
      "learning_rate": 0.00018282146734915466,
      "loss": 0.2221,
      "step": 2220
    },
    {
      "epoch": 0.08610613036103708,
      "grad_norm": 0.3627876341342926,
      "learning_rate": 0.00018281371180393983,
      "loss": 0.2743,
      "step": 2221
    },
    {
      "epoch": 0.08614489944269445,
      "grad_norm": 0.35308077931404114,
      "learning_rate": 0.00018280595625872498,
      "loss": 0.2876,
      "step": 2222
    },
    {
      "epoch": 0.08618366852435183,
      "grad_norm": 0.4416384696960449,
      "learning_rate": 0.00018279820071351018,
      "loss": 0.2619,
      "step": 2223
    },
    {
      "epoch": 0.08622243760600921,
      "grad_norm": 0.3560083210468292,
      "learning_rate": 0.00018279044516829532,
      "loss": 0.2711,
      "step": 2224
    },
    {
      "epoch": 0.08626120668766658,
      "grad_norm": 0.30085301399230957,
      "learning_rate": 0.00018278268962308052,
      "loss": 0.2025,
      "step": 2225
    },
    {
      "epoch": 0.08629997576932397,
      "grad_norm": 0.19293272495269775,
      "learning_rate": 0.00018277493407786567,
      "loss": 0.1659,
      "step": 2226
    },
    {
      "epoch": 0.08633874485098134,
      "grad_norm": 0.2295909821987152,
      "learning_rate": 0.00018276717853265087,
      "loss": 0.1949,
      "step": 2227
    },
    {
      "epoch": 0.08637751393263872,
      "grad_norm": 0.3150966763496399,
      "learning_rate": 0.00018275942298743601,
      "loss": 0.2077,
      "step": 2228
    },
    {
      "epoch": 0.0864162830142961,
      "grad_norm": 0.41396066546440125,
      "learning_rate": 0.0001827516674422212,
      "loss": 0.2235,
      "step": 2229
    },
    {
      "epoch": 0.08645505209595347,
      "grad_norm": 0.3250897526741028,
      "learning_rate": 0.00018274391189700636,
      "loss": 0.2242,
      "step": 2230
    },
    {
      "epoch": 0.08649382117761086,
      "grad_norm": 0.3666510581970215,
      "learning_rate": 0.00018273615635179153,
      "loss": 0.258,
      "step": 2231
    },
    {
      "epoch": 0.08653259025926824,
      "grad_norm": 0.40332818031311035,
      "learning_rate": 0.0001827284008065767,
      "loss": 0.2089,
      "step": 2232
    },
    {
      "epoch": 0.08657135934092561,
      "grad_norm": 0.3647313416004181,
      "learning_rate": 0.00018272064526136188,
      "loss": 0.2116,
      "step": 2233
    },
    {
      "epoch": 0.08661012842258299,
      "grad_norm": 0.34518471360206604,
      "learning_rate": 0.00018271288971614705,
      "loss": 0.2394,
      "step": 2234
    },
    {
      "epoch": 0.08664889750424037,
      "grad_norm": 0.33065035939216614,
      "learning_rate": 0.00018270513417093222,
      "loss": 0.2348,
      "step": 2235
    },
    {
      "epoch": 0.08668766658589774,
      "grad_norm": 0.34603947401046753,
      "learning_rate": 0.0001826973786257174,
      "loss": 0.2615,
      "step": 2236
    },
    {
      "epoch": 0.08672643566755513,
      "grad_norm": 0.294737309217453,
      "learning_rate": 0.00018268962308050257,
      "loss": 0.2045,
      "step": 2237
    },
    {
      "epoch": 0.0867652047492125,
      "grad_norm": 0.3323507308959961,
      "learning_rate": 0.00018268186753528774,
      "loss": 0.1903,
      "step": 2238
    },
    {
      "epoch": 0.08680397383086988,
      "grad_norm": 0.3458355665206909,
      "learning_rate": 0.00018267411199007292,
      "loss": 0.1965,
      "step": 2239
    },
    {
      "epoch": 0.08684274291252726,
      "grad_norm": 0.28971824049949646,
      "learning_rate": 0.0001826663564448581,
      "loss": 0.1785,
      "step": 2240
    },
    {
      "epoch": 0.08688151199418463,
      "grad_norm": 0.31260544061660767,
      "learning_rate": 0.00018265860089964326,
      "loss": 0.1924,
      "step": 2241
    },
    {
      "epoch": 0.08692028107584202,
      "grad_norm": 0.43543145060539246,
      "learning_rate": 0.00018265084535442843,
      "loss": 0.2743,
      "step": 2242
    },
    {
      "epoch": 0.0869590501574994,
      "grad_norm": 0.3871290981769562,
      "learning_rate": 0.00018264308980921358,
      "loss": 0.2414,
      "step": 2243
    },
    {
      "epoch": 0.08699781923915677,
      "grad_norm": 0.32747817039489746,
      "learning_rate": 0.00018263533426399878,
      "loss": 0.1996,
      "step": 2244
    },
    {
      "epoch": 0.08703658832081415,
      "grad_norm": 0.33760324120521545,
      "learning_rate": 0.00018262757871878393,
      "loss": 0.1844,
      "step": 2245
    },
    {
      "epoch": 0.08707535740247153,
      "grad_norm": 0.4109220802783966,
      "learning_rate": 0.00018261982317356913,
      "loss": 0.2341,
      "step": 2246
    },
    {
      "epoch": 0.0871141264841289,
      "grad_norm": 0.22940944135189056,
      "learning_rate": 0.00018261206762835427,
      "loss": 0.1786,
      "step": 2247
    },
    {
      "epoch": 0.08715289556578629,
      "grad_norm": 0.30206435918807983,
      "learning_rate": 0.00018260431208313947,
      "loss": 0.1981,
      "step": 2248
    },
    {
      "epoch": 0.08719166464744367,
      "grad_norm": 0.3342219591140747,
      "learning_rate": 0.00018259655653792462,
      "loss": 0.1821,
      "step": 2249
    },
    {
      "epoch": 0.08723043372910104,
      "grad_norm": 0.30190303921699524,
      "learning_rate": 0.0001825888009927098,
      "loss": 0.2125,
      "step": 2250
    },
    {
      "epoch": 0.08726920281075842,
      "grad_norm": 0.3440879285335541,
      "learning_rate": 0.00018258104544749496,
      "loss": 0.1972,
      "step": 2251
    },
    {
      "epoch": 0.08730797189241579,
      "grad_norm": 0.3615349531173706,
      "learning_rate": 0.00018257328990228013,
      "loss": 0.2581,
      "step": 2252
    },
    {
      "epoch": 0.08734674097407318,
      "grad_norm": 0.2816137671470642,
      "learning_rate": 0.0001825655343570653,
      "loss": 0.1805,
      "step": 2253
    },
    {
      "epoch": 0.08738551005573056,
      "grad_norm": 0.40775611996650696,
      "learning_rate": 0.00018255777881185048,
      "loss": 0.2572,
      "step": 2254
    },
    {
      "epoch": 0.08742427913738793,
      "grad_norm": 0.40630024671554565,
      "learning_rate": 0.00018255002326663565,
      "loss": 0.2887,
      "step": 2255
    },
    {
      "epoch": 0.08746304821904531,
      "grad_norm": 0.3532054126262665,
      "learning_rate": 0.00018254226772142083,
      "loss": 0.2348,
      "step": 2256
    },
    {
      "epoch": 0.0875018173007027,
      "grad_norm": 0.3956059515476227,
      "learning_rate": 0.00018253451217620597,
      "loss": 0.3012,
      "step": 2257
    },
    {
      "epoch": 0.08754058638236006,
      "grad_norm": 0.28856754302978516,
      "learning_rate": 0.00018252675663099117,
      "loss": 0.262,
      "step": 2258
    },
    {
      "epoch": 0.08757935546401745,
      "grad_norm": 0.28014472126960754,
      "learning_rate": 0.00018251900108577634,
      "loss": 0.2523,
      "step": 2259
    },
    {
      "epoch": 0.08761812454567483,
      "grad_norm": 0.30816707015037537,
      "learning_rate": 0.00018251124554056152,
      "loss": 0.2579,
      "step": 2260
    },
    {
      "epoch": 0.0876568936273322,
      "grad_norm": 0.3007943332195282,
      "learning_rate": 0.0001825034899953467,
      "loss": 0.2494,
      "step": 2261
    },
    {
      "epoch": 0.08769566270898958,
      "grad_norm": 0.24912963807582855,
      "learning_rate": 0.00018249573445013186,
      "loss": 0.2344,
      "step": 2262
    },
    {
      "epoch": 0.08773443179064695,
      "grad_norm": 0.25433531403541565,
      "learning_rate": 0.00018248797890491704,
      "loss": 0.2097,
      "step": 2263
    },
    {
      "epoch": 0.08777320087230434,
      "grad_norm": 0.22340449690818787,
      "learning_rate": 0.00018248022335970218,
      "loss": 0.1953,
      "step": 2264
    },
    {
      "epoch": 0.08781196995396172,
      "grad_norm": 0.3341294825077057,
      "learning_rate": 0.00018247246781448738,
      "loss": 0.2711,
      "step": 2265
    },
    {
      "epoch": 0.08785073903561909,
      "grad_norm": 0.26116251945495605,
      "learning_rate": 0.00018246471226927253,
      "loss": 0.1858,
      "step": 2266
    },
    {
      "epoch": 0.08788950811727647,
      "grad_norm": 0.2498766928911209,
      "learning_rate": 0.00018245695672405773,
      "loss": 0.2021,
      "step": 2267
    },
    {
      "epoch": 0.08792827719893385,
      "grad_norm": 0.3768240809440613,
      "learning_rate": 0.00018244920117884287,
      "loss": 0.2295,
      "step": 2268
    },
    {
      "epoch": 0.08796704628059122,
      "grad_norm": 0.2930147647857666,
      "learning_rate": 0.00018244144563362807,
      "loss": 0.2093,
      "step": 2269
    },
    {
      "epoch": 0.08800581536224861,
      "grad_norm": 0.21003876626491547,
      "learning_rate": 0.00018243369008841322,
      "loss": 0.1665,
      "step": 2270
    },
    {
      "epoch": 0.08804458444390599,
      "grad_norm": 0.4710082411766052,
      "learning_rate": 0.0001824259345431984,
      "loss": 0.3413,
      "step": 2271
    },
    {
      "epoch": 0.08808335352556336,
      "grad_norm": 0.31040769815444946,
      "learning_rate": 0.00018241817899798356,
      "loss": 0.2244,
      "step": 2272
    },
    {
      "epoch": 0.08812212260722074,
      "grad_norm": 0.22496016323566437,
      "learning_rate": 0.00018241042345276874,
      "loss": 0.1561,
      "step": 2273
    },
    {
      "epoch": 0.08816089168887813,
      "grad_norm": 0.2726987302303314,
      "learning_rate": 0.0001824026679075539,
      "loss": 0.1805,
      "step": 2274
    },
    {
      "epoch": 0.0881996607705355,
      "grad_norm": 0.2513890266418457,
      "learning_rate": 0.00018239491236233908,
      "loss": 0.213,
      "step": 2275
    },
    {
      "epoch": 0.08823842985219288,
      "grad_norm": 0.34968456625938416,
      "learning_rate": 0.00018238715681712426,
      "loss": 0.2572,
      "step": 2276
    },
    {
      "epoch": 0.08827719893385025,
      "grad_norm": 0.23990121483802795,
      "learning_rate": 0.00018237940127190943,
      "loss": 0.1768,
      "step": 2277
    },
    {
      "epoch": 0.08831596801550763,
      "grad_norm": 0.3433034121990204,
      "learning_rate": 0.00018237164572669457,
      "loss": 0.2211,
      "step": 2278
    },
    {
      "epoch": 0.08835473709716501,
      "grad_norm": 0.32854369282722473,
      "learning_rate": 0.00018236389018147977,
      "loss": 0.2237,
      "step": 2279
    },
    {
      "epoch": 0.08839350617882238,
      "grad_norm": 0.2914881408214569,
      "learning_rate": 0.00018235613463626492,
      "loss": 0.209,
      "step": 2280
    },
    {
      "epoch": 0.08843227526047977,
      "grad_norm": 0.31305375695228577,
      "learning_rate": 0.00018234837909105012,
      "loss": 0.2024,
      "step": 2281
    },
    {
      "epoch": 0.08847104434213715,
      "grad_norm": 0.3428128957748413,
      "learning_rate": 0.00018234062354583526,
      "loss": 0.1981,
      "step": 2282
    },
    {
      "epoch": 0.08850981342379452,
      "grad_norm": 0.28000763058662415,
      "learning_rate": 0.00018233286800062046,
      "loss": 0.1779,
      "step": 2283
    },
    {
      "epoch": 0.0885485825054519,
      "grad_norm": 0.42098572850227356,
      "learning_rate": 0.0001823251124554056,
      "loss": 0.2488,
      "step": 2284
    },
    {
      "epoch": 0.08858735158710929,
      "grad_norm": 0.368711918592453,
      "learning_rate": 0.00018231735691019078,
      "loss": 0.2079,
      "step": 2285
    },
    {
      "epoch": 0.08862612066876666,
      "grad_norm": 0.2627553343772888,
      "learning_rate": 0.00018230960136497596,
      "loss": 0.1582,
      "step": 2286
    },
    {
      "epoch": 0.08866488975042404,
      "grad_norm": 0.37047329545021057,
      "learning_rate": 0.00018230184581976113,
      "loss": 0.2456,
      "step": 2287
    },
    {
      "epoch": 0.08870365883208141,
      "grad_norm": 0.26339855790138245,
      "learning_rate": 0.0001822940902745463,
      "loss": 0.2067,
      "step": 2288
    },
    {
      "epoch": 0.08874242791373879,
      "grad_norm": 0.30315566062927246,
      "learning_rate": 0.00018228633472933147,
      "loss": 0.2104,
      "step": 2289
    },
    {
      "epoch": 0.08878119699539617,
      "grad_norm": 0.3027389943599701,
      "learning_rate": 0.00018227857918411665,
      "loss": 0.2368,
      "step": 2290
    },
    {
      "epoch": 0.08881996607705354,
      "grad_norm": 0.34488844871520996,
      "learning_rate": 0.00018227082363890182,
      "loss": 0.2504,
      "step": 2291
    },
    {
      "epoch": 0.08885873515871093,
      "grad_norm": 0.26082468032836914,
      "learning_rate": 0.000182263068093687,
      "loss": 0.1957,
      "step": 2292
    },
    {
      "epoch": 0.08889750424036831,
      "grad_norm": 0.2970971465110779,
      "learning_rate": 0.00018225531254847217,
      "loss": 0.1935,
      "step": 2293
    },
    {
      "epoch": 0.08893627332202568,
      "grad_norm": 0.28376781940460205,
      "learning_rate": 0.00018224755700325734,
      "loss": 0.1781,
      "step": 2294
    },
    {
      "epoch": 0.08897504240368306,
      "grad_norm": 0.328107088804245,
      "learning_rate": 0.0001822398014580425,
      "loss": 0.1969,
      "step": 2295
    },
    {
      "epoch": 0.08901381148534045,
      "grad_norm": 0.33114728331565857,
      "learning_rate": 0.00018223204591282768,
      "loss": 0.2241,
      "step": 2296
    },
    {
      "epoch": 0.08905258056699782,
      "grad_norm": 0.3196774423122406,
      "learning_rate": 0.00018222429036761286,
      "loss": 0.175,
      "step": 2297
    },
    {
      "epoch": 0.0890913496486552,
      "grad_norm": 0.2986453175544739,
      "learning_rate": 0.00018221653482239803,
      "loss": 0.1765,
      "step": 2298
    },
    {
      "epoch": 0.08913011873031258,
      "grad_norm": 0.2692025601863861,
      "learning_rate": 0.00018220877927718318,
      "loss": 0.1617,
      "step": 2299
    },
    {
      "epoch": 0.08916888781196995,
      "grad_norm": 0.3489021360874176,
      "learning_rate": 0.00018220102373196838,
      "loss": 0.2123,
      "step": 2300
    },
    {
      "epoch": 0.08920765689362734,
      "grad_norm": 0.24104934930801392,
      "learning_rate": 0.00018219326818675352,
      "loss": 0.148,
      "step": 2301
    },
    {
      "epoch": 0.0892464259752847,
      "grad_norm": 0.3807287812232971,
      "learning_rate": 0.00018218551264153872,
      "loss": 0.246,
      "step": 2302
    },
    {
      "epoch": 0.08928519505694209,
      "grad_norm": 0.45535847544670105,
      "learning_rate": 0.00018217775709632387,
      "loss": 0.2574,
      "step": 2303
    },
    {
      "epoch": 0.08932396413859947,
      "grad_norm": 0.35158106684684753,
      "learning_rate": 0.00018217000155110907,
      "loss": 0.2213,
      "step": 2304
    },
    {
      "epoch": 0.08936273322025684,
      "grad_norm": 0.34631064534187317,
      "learning_rate": 0.0001821622460058942,
      "loss": 0.2074,
      "step": 2305
    },
    {
      "epoch": 0.08940150230191422,
      "grad_norm": 0.2569408714771271,
      "learning_rate": 0.00018215449046067939,
      "loss": 0.1673,
      "step": 2306
    },
    {
      "epoch": 0.0894402713835716,
      "grad_norm": 0.25132715702056885,
      "learning_rate": 0.00018214673491546456,
      "loss": 0.2009,
      "step": 2307
    },
    {
      "epoch": 0.08947904046522898,
      "grad_norm": 0.3333874046802521,
      "learning_rate": 0.00018213897937024973,
      "loss": 0.1852,
      "step": 2308
    },
    {
      "epoch": 0.08951780954688636,
      "grad_norm": 0.4042209982872009,
      "learning_rate": 0.0001821312238250349,
      "loss": 0.2691,
      "step": 2309
    },
    {
      "epoch": 0.08955657862854374,
      "grad_norm": 0.3705311715602875,
      "learning_rate": 0.00018212346827982008,
      "loss": 0.2205,
      "step": 2310
    },
    {
      "epoch": 0.08959534771020111,
      "grad_norm": 0.3105775713920593,
      "learning_rate": 0.00018211571273460525,
      "loss": 0.206,
      "step": 2311
    },
    {
      "epoch": 0.0896341167918585,
      "grad_norm": 0.32571208477020264,
      "learning_rate": 0.00018210795718939042,
      "loss": 0.2283,
      "step": 2312
    },
    {
      "epoch": 0.08967288587351586,
      "grad_norm": 0.25288596749305725,
      "learning_rate": 0.0001821002016441756,
      "loss": 0.1411,
      "step": 2313
    },
    {
      "epoch": 0.08971165495517325,
      "grad_norm": 0.4406491816043854,
      "learning_rate": 0.00018209244609896077,
      "loss": 0.2536,
      "step": 2314
    },
    {
      "epoch": 0.08975042403683063,
      "grad_norm": 0.3556556701660156,
      "learning_rate": 0.00018208469055374594,
      "loss": 0.2249,
      "step": 2315
    },
    {
      "epoch": 0.089789193118488,
      "grad_norm": 0.22572897374629974,
      "learning_rate": 0.0001820769350085311,
      "loss": 0.1754,
      "step": 2316
    },
    {
      "epoch": 0.08982796220014538,
      "grad_norm": 0.3350091874599457,
      "learning_rate": 0.00018206917946331629,
      "loss": 0.1804,
      "step": 2317
    },
    {
      "epoch": 0.08986673128180277,
      "grad_norm": 0.31155574321746826,
      "learning_rate": 0.00018206142391810146,
      "loss": 0.1935,
      "step": 2318
    },
    {
      "epoch": 0.08990550036346014,
      "grad_norm": 0.43520545959472656,
      "learning_rate": 0.00018205366837288663,
      "loss": 0.2184,
      "step": 2319
    },
    {
      "epoch": 0.08994426944511752,
      "grad_norm": 0.359338641166687,
      "learning_rate": 0.0001820459128276718,
      "loss": 0.2422,
      "step": 2320
    },
    {
      "epoch": 0.0899830385267749,
      "grad_norm": 0.2606334388256073,
      "learning_rate": 0.00018203815728245698,
      "loss": 0.2012,
      "step": 2321
    },
    {
      "epoch": 0.09002180760843227,
      "grad_norm": 0.3211071491241455,
      "learning_rate": 0.00018203040173724212,
      "loss": 0.1569,
      "step": 2322
    },
    {
      "epoch": 0.09006057669008966,
      "grad_norm": 0.3716304898262024,
      "learning_rate": 0.00018202264619202732,
      "loss": 0.2161,
      "step": 2323
    },
    {
      "epoch": 0.09009934577174704,
      "grad_norm": 0.3376169800758362,
      "learning_rate": 0.00018201489064681247,
      "loss": 0.2365,
      "step": 2324
    },
    {
      "epoch": 0.09013811485340441,
      "grad_norm": 0.32220786809921265,
      "learning_rate": 0.00018200713510159767,
      "loss": 0.1784,
      "step": 2325
    },
    {
      "epoch": 0.09017688393506179,
      "grad_norm": 0.33622899651527405,
      "learning_rate": 0.00018199937955638281,
      "loss": 0.1907,
      "step": 2326
    },
    {
      "epoch": 0.09021565301671916,
      "grad_norm": 0.3981262445449829,
      "learning_rate": 0.00018199162401116801,
      "loss": 0.2792,
      "step": 2327
    },
    {
      "epoch": 0.09025442209837654,
      "grad_norm": 0.320310115814209,
      "learning_rate": 0.00018198386846595316,
      "loss": 0.1679,
      "step": 2328
    },
    {
      "epoch": 0.09029319118003393,
      "grad_norm": 0.38925856351852417,
      "learning_rate": 0.00018197611292073833,
      "loss": 0.1789,
      "step": 2329
    },
    {
      "epoch": 0.0903319602616913,
      "grad_norm": 0.4378639757633209,
      "learning_rate": 0.0001819683573755235,
      "loss": 0.2867,
      "step": 2330
    },
    {
      "epoch": 0.09037072934334868,
      "grad_norm": 0.2670220732688904,
      "learning_rate": 0.00018196060183030868,
      "loss": 0.1841,
      "step": 2331
    },
    {
      "epoch": 0.09040949842500606,
      "grad_norm": 0.28118687868118286,
      "learning_rate": 0.00018195284628509385,
      "loss": 0.1548,
      "step": 2332
    },
    {
      "epoch": 0.09044826750666343,
      "grad_norm": 0.27047669887542725,
      "learning_rate": 0.00018194509073987902,
      "loss": 0.197,
      "step": 2333
    },
    {
      "epoch": 0.09048703658832082,
      "grad_norm": 0.28183794021606445,
      "learning_rate": 0.0001819373351946642,
      "loss": 0.2383,
      "step": 2334
    },
    {
      "epoch": 0.0905258056699782,
      "grad_norm": 0.32951387763023376,
      "learning_rate": 0.00018192957964944937,
      "loss": 0.2271,
      "step": 2335
    },
    {
      "epoch": 0.09056457475163557,
      "grad_norm": 0.3104686439037323,
      "learning_rate": 0.00018192182410423452,
      "loss": 0.259,
      "step": 2336
    },
    {
      "epoch": 0.09060334383329295,
      "grad_norm": 0.25965288281440735,
      "learning_rate": 0.00018191406855901972,
      "loss": 0.1871,
      "step": 2337
    },
    {
      "epoch": 0.09064211291495032,
      "grad_norm": 0.4356499910354614,
      "learning_rate": 0.0001819063130138049,
      "loss": 0.2631,
      "step": 2338
    },
    {
      "epoch": 0.0906808819966077,
      "grad_norm": 0.31546103954315186,
      "learning_rate": 0.00018189855746859006,
      "loss": 0.187,
      "step": 2339
    },
    {
      "epoch": 0.09071965107826509,
      "grad_norm": 0.2674682140350342,
      "learning_rate": 0.00018189080192337523,
      "loss": 0.1885,
      "step": 2340
    },
    {
      "epoch": 0.09075842015992246,
      "grad_norm": 0.3219728469848633,
      "learning_rate": 0.0001818830463781604,
      "loss": 0.1801,
      "step": 2341
    },
    {
      "epoch": 0.09079718924157984,
      "grad_norm": 0.4253798723220825,
      "learning_rate": 0.00018187529083294558,
      "loss": 0.2007,
      "step": 2342
    },
    {
      "epoch": 0.09083595832323722,
      "grad_norm": 0.3393506407737732,
      "learning_rate": 0.00018186753528773072,
      "loss": 0.1897,
      "step": 2343
    },
    {
      "epoch": 0.09087472740489459,
      "grad_norm": 0.45421236753463745,
      "learning_rate": 0.00018185977974251592,
      "loss": 0.2986,
      "step": 2344
    },
    {
      "epoch": 0.09091349648655198,
      "grad_norm": 0.333370566368103,
      "learning_rate": 0.00018185202419730107,
      "loss": 0.2089,
      "step": 2345
    },
    {
      "epoch": 0.09095226556820936,
      "grad_norm": 0.3152455687522888,
      "learning_rate": 0.00018184426865208627,
      "loss": 0.1899,
      "step": 2346
    },
    {
      "epoch": 0.09099103464986673,
      "grad_norm": 0.39164409041404724,
      "learning_rate": 0.00018183651310687142,
      "loss": 0.222,
      "step": 2347
    },
    {
      "epoch": 0.09102980373152411,
      "grad_norm": 0.39717361330986023,
      "learning_rate": 0.00018182875756165662,
      "loss": 0.1785,
      "step": 2348
    },
    {
      "epoch": 0.09106857281318148,
      "grad_norm": 0.3999367952346802,
      "learning_rate": 0.00018182100201644176,
      "loss": 0.2458,
      "step": 2349
    },
    {
      "epoch": 0.09110734189483886,
      "grad_norm": 0.3852429986000061,
      "learning_rate": 0.00018181324647122693,
      "loss": 0.226,
      "step": 2350
    },
    {
      "epoch": 0.09114611097649625,
      "grad_norm": 0.25061824917793274,
      "learning_rate": 0.0001818054909260121,
      "loss": 0.136,
      "step": 2351
    },
    {
      "epoch": 0.09118488005815362,
      "grad_norm": 0.4042721390724182,
      "learning_rate": 0.00018179773538079728,
      "loss": 0.2473,
      "step": 2352
    },
    {
      "epoch": 0.091223649139811,
      "grad_norm": 0.310467392206192,
      "learning_rate": 0.00018178997983558245,
      "loss": 0.1951,
      "step": 2353
    },
    {
      "epoch": 0.09126241822146838,
      "grad_norm": 0.35975366830825806,
      "learning_rate": 0.00018178222429036763,
      "loss": 0.2335,
      "step": 2354
    },
    {
      "epoch": 0.09130118730312575,
      "grad_norm": 0.3652350604534149,
      "learning_rate": 0.0001817744687451528,
      "loss": 0.2229,
      "step": 2355
    },
    {
      "epoch": 0.09133995638478314,
      "grad_norm": 0.3418947160243988,
      "learning_rate": 0.00018176671319993797,
      "loss": 0.1804,
      "step": 2356
    },
    {
      "epoch": 0.09137872546644052,
      "grad_norm": 0.28310108184814453,
      "learning_rate": 0.00018175895765472312,
      "loss": 0.159,
      "step": 2357
    },
    {
      "epoch": 0.09141749454809789,
      "grad_norm": 0.49863094091415405,
      "learning_rate": 0.00018175120210950832,
      "loss": 0.3032,
      "step": 2358
    },
    {
      "epoch": 0.09145626362975527,
      "grad_norm": 0.34503650665283203,
      "learning_rate": 0.00018174344656429346,
      "loss": 0.1792,
      "step": 2359
    },
    {
      "epoch": 0.09149503271141265,
      "grad_norm": 0.3903096616268158,
      "learning_rate": 0.00018173569101907866,
      "loss": 0.2697,
      "step": 2360
    },
    {
      "epoch": 0.09153380179307002,
      "grad_norm": 0.24419601261615753,
      "learning_rate": 0.0001817279354738638,
      "loss": 0.1646,
      "step": 2361
    },
    {
      "epoch": 0.09157257087472741,
      "grad_norm": 0.26797693967819214,
      "learning_rate": 0.000181720179928649,
      "loss": 0.204,
      "step": 2362
    },
    {
      "epoch": 0.09161133995638478,
      "grad_norm": 0.3920711576938629,
      "learning_rate": 0.00018171242438343415,
      "loss": 0.2685,
      "step": 2363
    },
    {
      "epoch": 0.09165010903804216,
      "grad_norm": 0.22955037653446198,
      "learning_rate": 0.00018170466883821933,
      "loss": 0.1665,
      "step": 2364
    },
    {
      "epoch": 0.09168887811969954,
      "grad_norm": 0.29439985752105713,
      "learning_rate": 0.0001816969132930045,
      "loss": 0.1852,
      "step": 2365
    },
    {
      "epoch": 0.09172764720135691,
      "grad_norm": 0.2960139513015747,
      "learning_rate": 0.00018168915774778967,
      "loss": 0.1697,
      "step": 2366
    },
    {
      "epoch": 0.0917664162830143,
      "grad_norm": 0.30735328793525696,
      "learning_rate": 0.00018168140220257485,
      "loss": 0.2245,
      "step": 2367
    },
    {
      "epoch": 0.09180518536467168,
      "grad_norm": 0.3107769191265106,
      "learning_rate": 0.00018167364665736002,
      "loss": 0.2012,
      "step": 2368
    },
    {
      "epoch": 0.09184395444632905,
      "grad_norm": 0.43129003047943115,
      "learning_rate": 0.0001816658911121452,
      "loss": 0.2183,
      "step": 2369
    },
    {
      "epoch": 0.09188272352798643,
      "grad_norm": 0.2965131998062134,
      "learning_rate": 0.00018165813556693036,
      "loss": 0.1734,
      "step": 2370
    },
    {
      "epoch": 0.09192149260964382,
      "grad_norm": 0.29618021845817566,
      "learning_rate": 0.00018165038002171554,
      "loss": 0.1583,
      "step": 2371
    },
    {
      "epoch": 0.09196026169130118,
      "grad_norm": 0.44751083850860596,
      "learning_rate": 0.0001816426244765007,
      "loss": 0.2719,
      "step": 2372
    },
    {
      "epoch": 0.09199903077295857,
      "grad_norm": 0.2863399386405945,
      "learning_rate": 0.00018163486893128588,
      "loss": 0.1547,
      "step": 2373
    },
    {
      "epoch": 0.09203779985461594,
      "grad_norm": 0.3436492085456848,
      "learning_rate": 0.00018162711338607105,
      "loss": 0.2079,
      "step": 2374
    },
    {
      "epoch": 0.09207656893627332,
      "grad_norm": 0.35465526580810547,
      "learning_rate": 0.00018161935784085623,
      "loss": 0.217,
      "step": 2375
    },
    {
      "epoch": 0.0921153380179307,
      "grad_norm": 0.3831879794597626,
      "learning_rate": 0.0001816116022956414,
      "loss": 0.2419,
      "step": 2376
    },
    {
      "epoch": 0.09215410709958807,
      "grad_norm": 0.28780269622802734,
      "learning_rate": 0.00018160384675042657,
      "loss": 0.2369,
      "step": 2377
    },
    {
      "epoch": 0.09219287618124546,
      "grad_norm": 0.27506837248802185,
      "learning_rate": 0.00018159609120521172,
      "loss": 0.1635,
      "step": 2378
    },
    {
      "epoch": 0.09223164526290284,
      "grad_norm": 0.2614684998989105,
      "learning_rate": 0.00018158833565999692,
      "loss": 0.163,
      "step": 2379
    },
    {
      "epoch": 0.09227041434456021,
      "grad_norm": 0.25696930289268494,
      "learning_rate": 0.00018158058011478206,
      "loss": 0.179,
      "step": 2380
    },
    {
      "epoch": 0.09230918342621759,
      "grad_norm": 0.30901017785072327,
      "learning_rate": 0.00018157282456956726,
      "loss": 0.1901,
      "step": 2381
    },
    {
      "epoch": 0.09234795250787498,
      "grad_norm": 0.3161519467830658,
      "learning_rate": 0.0001815650690243524,
      "loss": 0.2296,
      "step": 2382
    },
    {
      "epoch": 0.09238672158953234,
      "grad_norm": 0.3603658378124237,
      "learning_rate": 0.0001815573134791376,
      "loss": 0.2205,
      "step": 2383
    },
    {
      "epoch": 0.09242549067118973,
      "grad_norm": 0.26814478635787964,
      "learning_rate": 0.00018154955793392276,
      "loss": 0.1843,
      "step": 2384
    },
    {
      "epoch": 0.09246425975284711,
      "grad_norm": 0.3345937132835388,
      "learning_rate": 0.00018154180238870793,
      "loss": 0.2097,
      "step": 2385
    },
    {
      "epoch": 0.09250302883450448,
      "grad_norm": 0.3578421175479889,
      "learning_rate": 0.0001815340468434931,
      "loss": 0.209,
      "step": 2386
    },
    {
      "epoch": 0.09254179791616186,
      "grad_norm": 0.4437567889690399,
      "learning_rate": 0.00018152629129827827,
      "loss": 0.2661,
      "step": 2387
    },
    {
      "epoch": 0.09258056699781923,
      "grad_norm": 0.3142712116241455,
      "learning_rate": 0.00018151853575306345,
      "loss": 0.2019,
      "step": 2388
    },
    {
      "epoch": 0.09261933607947662,
      "grad_norm": 0.37538987398147583,
      "learning_rate": 0.00018151078020784862,
      "loss": 0.2052,
      "step": 2389
    },
    {
      "epoch": 0.092658105161134,
      "grad_norm": 0.357534795999527,
      "learning_rate": 0.0001815030246626338,
      "loss": 0.1889,
      "step": 2390
    },
    {
      "epoch": 0.09269687424279137,
      "grad_norm": 0.3747004568576813,
      "learning_rate": 0.00018149526911741897,
      "loss": 0.2329,
      "step": 2391
    },
    {
      "epoch": 0.09273564332444875,
      "grad_norm": 0.3763623833656311,
      "learning_rate": 0.00018148751357220414,
      "loss": 0.2281,
      "step": 2392
    },
    {
      "epoch": 0.09277441240610614,
      "grad_norm": 0.336575448513031,
      "learning_rate": 0.0001814797580269893,
      "loss": 0.2013,
      "step": 2393
    },
    {
      "epoch": 0.0928131814877635,
      "grad_norm": 0.23398804664611816,
      "learning_rate": 0.00018147200248177448,
      "loss": 0.1443,
      "step": 2394
    },
    {
      "epoch": 0.09285195056942089,
      "grad_norm": 0.37961146235466003,
      "learning_rate": 0.00018146424693655966,
      "loss": 0.2297,
      "step": 2395
    },
    {
      "epoch": 0.09289071965107827,
      "grad_norm": 0.4582189917564392,
      "learning_rate": 0.00018145649139134483,
      "loss": 0.2214,
      "step": 2396
    },
    {
      "epoch": 0.09292948873273564,
      "grad_norm": 0.36640989780426025,
      "learning_rate": 0.00018144873584613,
      "loss": 0.2366,
      "step": 2397
    },
    {
      "epoch": 0.09296825781439302,
      "grad_norm": 0.39432090520858765,
      "learning_rate": 0.00018144098030091517,
      "loss": 0.2059,
      "step": 2398
    },
    {
      "epoch": 0.0930070268960504,
      "grad_norm": 0.29735487699508667,
      "learning_rate": 0.00018143322475570032,
      "loss": 0.1508,
      "step": 2399
    },
    {
      "epoch": 0.09304579597770778,
      "grad_norm": 0.3699287474155426,
      "learning_rate": 0.00018142546921048552,
      "loss": 0.2049,
      "step": 2400
    },
    {
      "epoch": 0.09308456505936516,
      "grad_norm": 0.287981778383255,
      "learning_rate": 0.00018141771366527067,
      "loss": 0.1726,
      "step": 2401
    },
    {
      "epoch": 0.09312333414102253,
      "grad_norm": 0.3490707576274872,
      "learning_rate": 0.00018140995812005587,
      "loss": 0.168,
      "step": 2402
    },
    {
      "epoch": 0.09316210322267991,
      "grad_norm": 0.3702709972858429,
      "learning_rate": 0.000181402202574841,
      "loss": 0.1919,
      "step": 2403
    },
    {
      "epoch": 0.0932008723043373,
      "grad_norm": 0.41646015644073486,
      "learning_rate": 0.0001813944470296262,
      "loss": 0.2237,
      "step": 2404
    },
    {
      "epoch": 0.09323964138599466,
      "grad_norm": 0.4919864237308502,
      "learning_rate": 0.00018138669148441136,
      "loss": 0.2303,
      "step": 2405
    },
    {
      "epoch": 0.09327841046765205,
      "grad_norm": 0.3054509162902832,
      "learning_rate": 0.00018137893593919653,
      "loss": 0.1559,
      "step": 2406
    },
    {
      "epoch": 0.09331717954930943,
      "grad_norm": 0.3796851336956024,
      "learning_rate": 0.0001813711803939817,
      "loss": 0.1983,
      "step": 2407
    },
    {
      "epoch": 0.0933559486309668,
      "grad_norm": 0.40777167677879333,
      "learning_rate": 0.00018136342484876688,
      "loss": 0.2506,
      "step": 2408
    },
    {
      "epoch": 0.09339471771262418,
      "grad_norm": 0.32182392477989197,
      "learning_rate": 0.00018135566930355205,
      "loss": 0.2181,
      "step": 2409
    },
    {
      "epoch": 0.09343348679428157,
      "grad_norm": 0.21776141226291656,
      "learning_rate": 0.00018134791375833722,
      "loss": 0.1474,
      "step": 2410
    },
    {
      "epoch": 0.09347225587593894,
      "grad_norm": 0.2629771828651428,
      "learning_rate": 0.0001813401582131224,
      "loss": 0.1847,
      "step": 2411
    },
    {
      "epoch": 0.09351102495759632,
      "grad_norm": 0.3110746443271637,
      "learning_rate": 0.00018133240266790757,
      "loss": 0.2082,
      "step": 2412
    },
    {
      "epoch": 0.09354979403925369,
      "grad_norm": 0.30385640263557434,
      "learning_rate": 0.0001813246471226927,
      "loss": 0.1988,
      "step": 2413
    },
    {
      "epoch": 0.09358856312091107,
      "grad_norm": 0.31056439876556396,
      "learning_rate": 0.0001813168915774779,
      "loss": 0.1657,
      "step": 2414
    },
    {
      "epoch": 0.09362733220256846,
      "grad_norm": 0.40763619542121887,
      "learning_rate": 0.00018130913603226306,
      "loss": 0.2404,
      "step": 2415
    },
    {
      "epoch": 0.09366610128422583,
      "grad_norm": 0.3366019129753113,
      "learning_rate": 0.00018130138048704826,
      "loss": 0.1801,
      "step": 2416
    },
    {
      "epoch": 0.09370487036588321,
      "grad_norm": 0.4264638423919678,
      "learning_rate": 0.00018129362494183343,
      "loss": 0.2342,
      "step": 2417
    },
    {
      "epoch": 0.09374363944754059,
      "grad_norm": 0.460551917552948,
      "learning_rate": 0.0001812858693966186,
      "loss": 0.2494,
      "step": 2418
    },
    {
      "epoch": 0.09378240852919796,
      "grad_norm": 0.28117698431015015,
      "learning_rate": 0.00018127811385140378,
      "loss": 0.1505,
      "step": 2419
    },
    {
      "epoch": 0.09382117761085534,
      "grad_norm": 0.4114231467247009,
      "learning_rate": 0.00018127035830618892,
      "loss": 0.1944,
      "step": 2420
    },
    {
      "epoch": 0.09385994669251273,
      "grad_norm": 0.32027173042297363,
      "learning_rate": 0.00018126260276097412,
      "loss": 0.1905,
      "step": 2421
    },
    {
      "epoch": 0.0938987157741701,
      "grad_norm": 0.28439101576805115,
      "learning_rate": 0.00018125484721575927,
      "loss": 0.1778,
      "step": 2422
    },
    {
      "epoch": 0.09393748485582748,
      "grad_norm": 0.45159250497817993,
      "learning_rate": 0.00018124709167054447,
      "loss": 0.2409,
      "step": 2423
    },
    {
      "epoch": 0.09397625393748485,
      "grad_norm": 0.3048710227012634,
      "learning_rate": 0.00018123933612532961,
      "loss": 0.1834,
      "step": 2424
    },
    {
      "epoch": 0.09401502301914223,
      "grad_norm": 0.2852828800678253,
      "learning_rate": 0.00018123158058011481,
      "loss": 0.1449,
      "step": 2425
    },
    {
      "epoch": 0.09405379210079962,
      "grad_norm": 0.38274532556533813,
      "learning_rate": 0.00018122382503489996,
      "loss": 0.2509,
      "step": 2426
    },
    {
      "epoch": 0.09409256118245699,
      "grad_norm": 0.3058629631996155,
      "learning_rate": 0.00018121606948968513,
      "loss": 0.1691,
      "step": 2427
    },
    {
      "epoch": 0.09413133026411437,
      "grad_norm": 0.3551523685455322,
      "learning_rate": 0.0001812083139444703,
      "loss": 0.2062,
      "step": 2428
    },
    {
      "epoch": 0.09417009934577175,
      "grad_norm": 0.2573353350162506,
      "learning_rate": 0.00018120055839925548,
      "loss": 0.1495,
      "step": 2429
    },
    {
      "epoch": 0.09420886842742912,
      "grad_norm": 0.40748071670532227,
      "learning_rate": 0.00018119280285404065,
      "loss": 0.1822,
      "step": 2430
    },
    {
      "epoch": 0.0942476375090865,
      "grad_norm": 0.4199608564376831,
      "learning_rate": 0.00018118504730882582,
      "loss": 0.2274,
      "step": 2431
    },
    {
      "epoch": 0.09428640659074389,
      "grad_norm": 0.4017734229564667,
      "learning_rate": 0.000181177291763611,
      "loss": 0.2065,
      "step": 2432
    },
    {
      "epoch": 0.09432517567240126,
      "grad_norm": 0.3768734633922577,
      "learning_rate": 0.00018116953621839617,
      "loss": 0.201,
      "step": 2433
    },
    {
      "epoch": 0.09436394475405864,
      "grad_norm": 0.3900549113750458,
      "learning_rate": 0.00018116178067318131,
      "loss": 0.1842,
      "step": 2434
    },
    {
      "epoch": 0.09440271383571602,
      "grad_norm": 0.3106303811073303,
      "learning_rate": 0.00018115402512796651,
      "loss": 0.163,
      "step": 2435
    },
    {
      "epoch": 0.09444148291737339,
      "grad_norm": 0.4656856656074524,
      "learning_rate": 0.00018114626958275166,
      "loss": 0.2639,
      "step": 2436
    },
    {
      "epoch": 0.09448025199903078,
      "grad_norm": 0.36297059059143066,
      "learning_rate": 0.00018113851403753686,
      "loss": 0.1785,
      "step": 2437
    },
    {
      "epoch": 0.09451902108068815,
      "grad_norm": 0.33101579546928406,
      "learning_rate": 0.000181130758492322,
      "loss": 0.1722,
      "step": 2438
    },
    {
      "epoch": 0.09455779016234553,
      "grad_norm": 0.4341283440589905,
      "learning_rate": 0.0001811230029471072,
      "loss": 0.2581,
      "step": 2439
    },
    {
      "epoch": 0.09459655924400291,
      "grad_norm": 0.30956536531448364,
      "learning_rate": 0.00018111524740189235,
      "loss": 0.1867,
      "step": 2440
    },
    {
      "epoch": 0.09463532832566028,
      "grad_norm": 0.37712472677230835,
      "learning_rate": 0.00018110749185667752,
      "loss": 0.1824,
      "step": 2441
    },
    {
      "epoch": 0.09467409740731766,
      "grad_norm": 0.28997427225112915,
      "learning_rate": 0.0001810997363114627,
      "loss": 0.1462,
      "step": 2442
    },
    {
      "epoch": 0.09471286648897505,
      "grad_norm": 0.38645175099372864,
      "learning_rate": 0.00018109198076624787,
      "loss": 0.1994,
      "step": 2443
    },
    {
      "epoch": 0.09475163557063242,
      "grad_norm": 0.42235422134399414,
      "learning_rate": 0.00018108422522103304,
      "loss": 0.2301,
      "step": 2444
    },
    {
      "epoch": 0.0947904046522898,
      "grad_norm": 0.27767738699913025,
      "learning_rate": 0.00018107646967581822,
      "loss": 0.1332,
      "step": 2445
    },
    {
      "epoch": 0.09482917373394718,
      "grad_norm": 0.297079473733902,
      "learning_rate": 0.0001810687141306034,
      "loss": 0.1651,
      "step": 2446
    },
    {
      "epoch": 0.09486794281560455,
      "grad_norm": 0.36850079894065857,
      "learning_rate": 0.00018106095858538856,
      "loss": 0.2214,
      "step": 2447
    },
    {
      "epoch": 0.09490671189726194,
      "grad_norm": 0.348237544298172,
      "learning_rate": 0.00018105320304017373,
      "loss": 0.1879,
      "step": 2448
    },
    {
      "epoch": 0.0949454809789193,
      "grad_norm": 0.3895414173603058,
      "learning_rate": 0.0001810454474949589,
      "loss": 0.2025,
      "step": 2449
    },
    {
      "epoch": 0.09498425006057669,
      "grad_norm": 0.3640536367893219,
      "learning_rate": 0.00018103769194974408,
      "loss": 0.2123,
      "step": 2450
    },
    {
      "epoch": 0.09502301914223407,
      "grad_norm": 0.37640199065208435,
      "learning_rate": 0.00018102993640452925,
      "loss": 0.2101,
      "step": 2451
    },
    {
      "epoch": 0.09506178822389144,
      "grad_norm": 0.42742568254470825,
      "learning_rate": 0.00018102218085931443,
      "loss": 0.2051,
      "step": 2452
    },
    {
      "epoch": 0.09510055730554882,
      "grad_norm": 0.3259127736091614,
      "learning_rate": 0.0001810144253140996,
      "loss": 0.1749,
      "step": 2453
    },
    {
      "epoch": 0.09513932638720621,
      "grad_norm": 0.2995041310787201,
      "learning_rate": 0.00018100666976888477,
      "loss": 0.1803,
      "step": 2454
    },
    {
      "epoch": 0.09517809546886358,
      "grad_norm": 0.42720240354537964,
      "learning_rate": 0.00018099891422366992,
      "loss": 0.2203,
      "step": 2455
    },
    {
      "epoch": 0.09521686455052096,
      "grad_norm": 0.3467028737068176,
      "learning_rate": 0.00018099115867845512,
      "loss": 0.2221,
      "step": 2456
    },
    {
      "epoch": 0.09525563363217834,
      "grad_norm": 0.29351067543029785,
      "learning_rate": 0.00018098340313324026,
      "loss": 0.1722,
      "step": 2457
    },
    {
      "epoch": 0.09529440271383571,
      "grad_norm": 0.3804624080657959,
      "learning_rate": 0.00018097564758802546,
      "loss": 0.251,
      "step": 2458
    },
    {
      "epoch": 0.0953331717954931,
      "grad_norm": 0.34030455350875854,
      "learning_rate": 0.0001809678920428106,
      "loss": 0.1861,
      "step": 2459
    },
    {
      "epoch": 0.09537194087715047,
      "grad_norm": 0.3026289939880371,
      "learning_rate": 0.0001809601364975958,
      "loss": 0.2152,
      "step": 2460
    },
    {
      "epoch": 0.09541070995880785,
      "grad_norm": 0.4654986560344696,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.2744,
      "step": 2461
    },
    {
      "epoch": 0.09544947904046523,
      "grad_norm": 0.4178730547428131,
      "learning_rate": 0.00018094462540716613,
      "loss": 0.2458,
      "step": 2462
    },
    {
      "epoch": 0.0954882481221226,
      "grad_norm": 0.35699138045310974,
      "learning_rate": 0.0001809368698619513,
      "loss": 0.1986,
      "step": 2463
    },
    {
      "epoch": 0.09552701720377998,
      "grad_norm": 0.30338016152381897,
      "learning_rate": 0.00018092911431673647,
      "loss": 0.1818,
      "step": 2464
    },
    {
      "epoch": 0.09556578628543737,
      "grad_norm": 0.2844836413860321,
      "learning_rate": 0.00018092135877152164,
      "loss": 0.1417,
      "step": 2465
    },
    {
      "epoch": 0.09560455536709474,
      "grad_norm": 0.31178057193756104,
      "learning_rate": 0.00018091360322630682,
      "loss": 0.147,
      "step": 2466
    },
    {
      "epoch": 0.09564332444875212,
      "grad_norm": 0.33322465419769287,
      "learning_rate": 0.000180905847681092,
      "loss": 0.1796,
      "step": 2467
    },
    {
      "epoch": 0.0956820935304095,
      "grad_norm": 0.38003551959991455,
      "learning_rate": 0.00018089809213587716,
      "loss": 0.1958,
      "step": 2468
    },
    {
      "epoch": 0.09572086261206687,
      "grad_norm": 0.4804418981075287,
      "learning_rate": 0.00018089033659066234,
      "loss": 0.2412,
      "step": 2469
    },
    {
      "epoch": 0.09575963169372426,
      "grad_norm": 0.40189361572265625,
      "learning_rate": 0.0001808825810454475,
      "loss": 0.1977,
      "step": 2470
    },
    {
      "epoch": 0.09579840077538164,
      "grad_norm": 0.2980917990207672,
      "learning_rate": 0.00018087482550023268,
      "loss": 0.1801,
      "step": 2471
    },
    {
      "epoch": 0.09583716985703901,
      "grad_norm": 0.36476975679397583,
      "learning_rate": 0.00018086706995501785,
      "loss": 0.2007,
      "step": 2472
    },
    {
      "epoch": 0.09587593893869639,
      "grad_norm": 0.2878924608230591,
      "learning_rate": 0.00018085931440980303,
      "loss": 0.1662,
      "step": 2473
    },
    {
      "epoch": 0.09591470802035376,
      "grad_norm": 0.3621548116207123,
      "learning_rate": 0.0001808515588645882,
      "loss": 0.2015,
      "step": 2474
    },
    {
      "epoch": 0.09595347710201114,
      "grad_norm": 0.2947962284088135,
      "learning_rate": 0.00018084380331937337,
      "loss": 0.1581,
      "step": 2475
    },
    {
      "epoch": 0.09599224618366853,
      "grad_norm": 0.3526119291782379,
      "learning_rate": 0.00018083604777415852,
      "loss": 0.2092,
      "step": 2476
    },
    {
      "epoch": 0.0960310152653259,
      "grad_norm": 0.2855086922645569,
      "learning_rate": 0.00018082829222894372,
      "loss": 0.15,
      "step": 2477
    },
    {
      "epoch": 0.09606978434698328,
      "grad_norm": 0.28013837337493896,
      "learning_rate": 0.00018082053668372886,
      "loss": 0.1402,
      "step": 2478
    },
    {
      "epoch": 0.09610855342864066,
      "grad_norm": 0.24760058522224426,
      "learning_rate": 0.00018081278113851406,
      "loss": 0.1356,
      "step": 2479
    },
    {
      "epoch": 0.09614732251029803,
      "grad_norm": 0.45518773794174194,
      "learning_rate": 0.0001808050255932992,
      "loss": 0.2353,
      "step": 2480
    },
    {
      "epoch": 0.09618609159195542,
      "grad_norm": 0.4927019774913788,
      "learning_rate": 0.0001807972700480844,
      "loss": 0.2696,
      "step": 2481
    },
    {
      "epoch": 0.0962248606736128,
      "grad_norm": 0.3170301020145416,
      "learning_rate": 0.00018078951450286956,
      "loss": 0.1716,
      "step": 2482
    },
    {
      "epoch": 0.09626362975527017,
      "grad_norm": 0.44986632466316223,
      "learning_rate": 0.00018078175895765473,
      "loss": 0.2767,
      "step": 2483
    },
    {
      "epoch": 0.09630239883692755,
      "grad_norm": 0.3538062274456024,
      "learning_rate": 0.0001807740034124399,
      "loss": 0.197,
      "step": 2484
    },
    {
      "epoch": 0.09634116791858492,
      "grad_norm": 0.34073516726493835,
      "learning_rate": 0.00018076624786722507,
      "loss": 0.1807,
      "step": 2485
    },
    {
      "epoch": 0.0963799370002423,
      "grad_norm": 0.3694955110549927,
      "learning_rate": 0.00018075849232201025,
      "loss": 0.2188,
      "step": 2486
    },
    {
      "epoch": 0.09641870608189969,
      "grad_norm": 0.3774366080760956,
      "learning_rate": 0.00018075073677679542,
      "loss": 0.2029,
      "step": 2487
    },
    {
      "epoch": 0.09645747516355706,
      "grad_norm": 0.30146974325180054,
      "learning_rate": 0.0001807429812315806,
      "loss": 0.1521,
      "step": 2488
    },
    {
      "epoch": 0.09649624424521444,
      "grad_norm": 0.2959839999675751,
      "learning_rate": 0.00018073522568636576,
      "loss": 0.1576,
      "step": 2489
    },
    {
      "epoch": 0.09653501332687182,
      "grad_norm": 0.37126559019088745,
      "learning_rate": 0.0001807274701411509,
      "loss": 0.2311,
      "step": 2490
    },
    {
      "epoch": 0.0965737824085292,
      "grad_norm": 0.3045230209827423,
      "learning_rate": 0.0001807197145959361,
      "loss": 0.18,
      "step": 2491
    },
    {
      "epoch": 0.09661255149018658,
      "grad_norm": 0.37629643082618713,
      "learning_rate": 0.00018071195905072126,
      "loss": 0.2373,
      "step": 2492
    },
    {
      "epoch": 0.09665132057184396,
      "grad_norm": 0.29669448733329773,
      "learning_rate": 0.00018070420350550646,
      "loss": 0.1625,
      "step": 2493
    },
    {
      "epoch": 0.09669008965350133,
      "grad_norm": 0.32838183641433716,
      "learning_rate": 0.0001806964479602916,
      "loss": 0.175,
      "step": 2494
    },
    {
      "epoch": 0.09672885873515871,
      "grad_norm": 0.29174792766571045,
      "learning_rate": 0.0001806886924150768,
      "loss": 0.1514,
      "step": 2495
    },
    {
      "epoch": 0.0967676278168161,
      "grad_norm": 0.40270617604255676,
      "learning_rate": 0.00018068093686986197,
      "loss": 0.2279,
      "step": 2496
    },
    {
      "epoch": 0.09680639689847347,
      "grad_norm": 0.32675790786743164,
      "learning_rate": 0.00018067318132464712,
      "loss": 0.2012,
      "step": 2497
    },
    {
      "epoch": 0.09684516598013085,
      "grad_norm": 0.3215668201446533,
      "learning_rate": 0.00018066542577943232,
      "loss": 0.1809,
      "step": 2498
    },
    {
      "epoch": 0.09688393506178822,
      "grad_norm": 0.450560599565506,
      "learning_rate": 0.00018065767023421747,
      "loss": 0.285,
      "step": 2499
    },
    {
      "epoch": 0.0969227041434456,
      "grad_norm": 0.26753613352775574,
      "learning_rate": 0.00018064991468900267,
      "loss": 0.1614,
      "step": 2500
    }
  ],
  "logging_steps": 1,
  "max_steps": 25793,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.2766898479104e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
