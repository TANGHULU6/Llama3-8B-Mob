
2024-09-16 14:50:28,491 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-16 14:50:41,732 INFO Model and tokenizer initialized successfully.
2024-09-16 14:50:41,733 INFO Starting inference from index 600 to 799 on dataset datasetD_test_3000-5999.json
2024-09-16 14:50:41,733 INFO Loading dataset from datasetD_test_3000-5999.json...
2024-09-16 14:50:42,860 INFO Dataset loaded with 3000 conversations.
2024-09-16 14:50:42,863 INFO Loaded dataset datasetD_test_3000-5999.json with 3000 conversations
2024-09-16 14:50:42,864 INFO Selected subset of dataset from index 600 to 800
Map: 100%|███████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2576.76 examples/s]
2024-09-16 14:50:42,948 INFO Applied formatting prompts function to dataset
2024-09-16 14:50:42,948 INFO Total conversations to process: 200
2024-09-16 14:50:42,948 INFO Processing conversation 600/799
2024-09-16 14:50:42,965 INFO Input sequence length: torch.Size([1, 6757])
2024-09-16 14:55:18,190 INFO User 3600 processed in 275.24s
2024-09-16 14:55:18,192 INFO Processing conversation 601/799
2024-09-16 14:55:18,212 INFO Input sequence length: torch.Size([1, 9949])
2024-09-16 15:01:58,366 INFO User 3601 processed in 400.17s
2024-09-16 15:01:58,368 INFO Processing conversation 602/799
2024-09-16 15:01:58,387 INFO Input sequence length: torch.Size([1, 11813])
2024-09-16 15:13:15,334 INFO User 3602 processed in 676.97s
2024-09-16 15:13:15,335 INFO Processing conversation 603/799
2024-09-16 15:13:15,368 INFO Input sequence length: torch.Size([1, 19221])
2024-09-16 15:59:02,287 INFO User 3603 processed in 2746.95s
2024-09-16 15:59:02,289 INFO Processing conversation 604/799
2024-09-16 15:59:02,308 INFO Input sequence length: torch.Size([1, 10885])
2024-09-16 16:15:43,679 INFO User 3604 processed in 1001.39s
2024-09-16 16:15:43,681 INFO Processing conversation 605/799
