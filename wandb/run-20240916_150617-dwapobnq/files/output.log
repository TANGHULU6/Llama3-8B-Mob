
2024-09-16 15:06:21,825 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-16 15:06:41,366 INFO Model and tokenizer initialized successfully.
2024-09-16 15:06:41,367 INFO Starting inference from index 1000 to 1199 on dataset datasetD_test_3000-5999.json
2024-09-16 15:06:41,367 INFO Loading dataset from datasetD_test_3000-5999.json...
2024-09-16 15:06:42,583 INFO Dataset loaded with 3000 conversations.
2024-09-16 15:06:42,585 INFO Loaded dataset datasetD_test_3000-5999.json with 3000 conversations
2024-09-16 15:06:42,587 INFO Selected subset of dataset from index 1000 to 1200
Map: 100%|███████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2249.08 examples/s]
2024-09-16 15:06:42,686 INFO Applied formatting prompts function to dataset
2024-09-16 15:06:42,686 INFO Total conversations to process: 200
2024-09-16 15:06:42,686 INFO Processing conversation 1000/1199
2024-09-16 15:06:42,712 INFO Input sequence length: torch.Size([1, 14853])
2024-09-16 15:35:15,765 INFO User 4001 processed in 1713.08s
2024-09-16 15:35:15,767 INFO Processing conversation 1001/1199
2024-09-16 15:35:15,782 INFO Input sequence length: torch.Size([1, 9013])
2024-09-16 15:46:12,235 INFO User 4000 processed in 656.47s
2024-09-16 15:46:12,236 INFO Processing conversation 1002/1199
2024-09-16 15:46:12,258 INFO Input sequence length: torch.Size([1, 13109])
2024-09-16 16:02:36,917 INFO User 4002 processed in 984.68s
2024-09-16 16:02:36,919 INFO Processing conversation 1003/1199
2024-09-16 16:02:36,936 INFO Input sequence length: torch.Size([1, 10189])
2024-09-16 16:17:37,681 INFO User 4003 processed in 900.76s
2024-09-16 16:17:37,683 INFO Processing conversation 1004/1199
2024-09-16 16:17:37,697 INFO Input sequence length: torch.Size([1, 7485])
2024-09-16 16:25:48,476 INFO User 4004 processed in 490.79s
2024-09-16 16:25:48,477 INFO Processing conversation 1005/1199
