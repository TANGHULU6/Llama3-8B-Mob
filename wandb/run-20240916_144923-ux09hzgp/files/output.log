
2024-09-16 14:49:27,516 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-16 14:49:39,163 INFO Model and tokenizer initialized successfully.
2024-09-16 14:49:39,165 INFO Starting inference from index 400 to 599 on dataset datasetD_test_3000-5999.json
2024-09-16 14:49:39,165 INFO Loading dataset from datasetD_test_3000-5999.json...
2024-09-16 14:49:40,288 INFO Dataset loaded with 3000 conversations.
2024-09-16 14:49:40,291 INFO Loaded dataset datasetD_test_3000-5999.json with 3000 conversations
2024-09-16 14:49:40,292 INFO Selected subset of dataset from index 400 to 600
Map: 100%|███████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2849.16 examples/s]
2024-09-16 14:49:40,369 INFO Applied formatting prompts function to dataset
2024-09-16 14:49:40,369 INFO Total conversations to process: 200
2024-09-16 14:49:40,369 INFO Processing conversation 400/599
2024-09-16 14:49:40,396 INFO Input sequence length: torch.Size([1, 9869])
2024-09-16 14:58:32,174 INFO User 3400 processed in 531.81s
2024-09-16 14:58:32,176 INFO Processing conversation 401/599
2024-09-16 14:58:32,209 INFO Input sequence length: torch.Size([1, 19749])
2024-09-16 15:30:15,326 INFO User 3401 processed in 1903.15s
2024-09-16 15:30:15,328 INFO Processing conversation 402/599
2024-09-16 15:30:15,357 INFO Input sequence length: torch.Size([1, 17693])
2024-09-16 16:05:11,038 INFO User 3402 processed in 2095.71s
2024-09-16 16:05:11,039 INFO Processing conversation 403/599
2024-09-16 16:05:11,061 INFO Input sequence length: torch.Size([1, 12757])
2024-09-16 16:24:03,542 INFO User 3403 processed in 1132.50s
2024-09-16 16:24:03,544 INFO Processing conversation 404/599
