[34m[1mwandb[39m[22m: Downloading large artifact model-First:v10, 249.60MB. 11 files...
[34m[1mwandb[39m[22m:   11 of 11 files downloaded.
Done. 0:0:0.4
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 1 | Gradient Accumulation steps = 4
\        /    Total batch size = 4 | Total steps = 250
 "-____-"     Number of trainable parameters = 41,943,040
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory:
	eval_steps: 50 (from args) != 500 (from trainer_state.json)
Traceback (most recent call last):
  File "/home/geo_llm/llama3-finetune/HuMob_wandb.py", line 166, in <module>
    trainer.train(resume_from_checkpoint=artifact_dir)
  File "<string>", line 126, in train
  File "<string>", line 286, in _fast_inner_training_loop
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 461, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 508, in call_event
    result = getattr(callback, event)(
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 680, in on_train_begin
    assert args.load_best_model_at_end, "EarlyStoppingCallback requires load_best_model_at_end = True"
AssertionError: EarlyStoppingCallback requires load_best_model_at_end = True