
2024-09-14 23:28:05,829 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Traceback (most recent call last):
  File "/home/geo_llm/llama3-finetune/main.py", line 2, in <module>
    from Infer import run_inference
  File "/home/geo_llm/llama3-finetune/Infer.py", line 25, in <module>
    model, tokenizer = FastLanguageModel.from_pretrained(
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py", line 172, in from_pretrained
    model, tokenizer = dispatch_model.from_pretrained(
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py", line 1253, in from_pretrained
    raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
RuntimeError: Unsloth currently does not support multi GPU setups - but we are working on it!