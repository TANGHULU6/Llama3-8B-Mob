
2024-09-16 12:50:23,957 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-16 12:50:33,488 INFO Model and tokenizer initialized successfully.
2024-09-16 12:50:33,489 INFO Starting inference from index 514 to 514 on dataset datasetB_test_22000-24999.json
2024-09-16 12:50:33,489 INFO Loading dataset from datasetB_test_22000-24999.json...
2024-09-16 12:50:34,122 INFO Dataset loaded with 3000 conversations.
2024-09-16 12:50:34,124 INFO Loaded dataset datasetB_test_22000-24999.json with 3000 conversations
2024-09-16 12:50:34,125 INFO Selected subset of dataset from index 514 to 515
Map: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.51 examples/s]
2024-09-16 12:50:34,152 INFO Applied formatting prompts function to dataset
2024-09-16 12:50:34,152 INFO Total conversations to process: 1
2024-09-16 12:50:34,152 INFO Processing conversation 514/514
2024-09-16 12:50:34,163 INFO Input sequence length: torch.Size([1, 4909])
2024-09-16 12:51:31,001 INFO User 22514 processed in 56.85s
2024-09-16 12:51:31,002 INFO Failed conversations: []
2024-09-16 12:51:31,619 INFO Saving results to generated_datasetB_test_22000-24999_range(514, 515).csv.gz
2024-09-16 12:51:31,620 INFO Results saved to generated_datasetB_test_22000-24999_range(514, 515).csv.gz