[34m[1mwandb[39m[22m: Downloading large artifact model-C_eval_loss:epoch_1.0, 250.02MB. 11 files...
[34m[1mwandb[39m[22m:   11 of 11 files downloaded.
Done. 0:0:0.6
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 13,600 | Num Epochs = 3
O^O/ \_/ \    Batch size per device = 1 | Gradient Accumulation steps = 4
\        /    Total batch size = 4 | Total steps = 10,200
 "-____-"     Number of trainable parameters = 41,943,040

 33%|█████████████████████████▋                                                   | 3401/10200 [00:24<00:48, 141.50it/s]


 33%|██████████████████████████                                                    | 3402/10200 [01:02<02:37, 43.11it/s]

 33%|██████████████████████████                                                    | 3403/10200 [01:22<03:58, 28.46it/s]
{'loss': 0.4027, 'grad_norm': 0.05960981547832489, 'learning_rate': 0.00013327450980392158, 'epoch': 1.0}


 33%|██████████████████████████                                                    | 3405/10200 [02:56<15:12,  7.45it/s]

 33%|██████████████████████████                                                    | 3406/10200 [03:18<19:32,  5.80it/s]

 33%|██████████████████████████                                                    | 3407/10200 [03:42<26:11,  4.32it/s]
{'loss': 0.3981, 'grad_norm': 0.07143402099609375, 'learning_rate': 0.00013319607843137257, 'epoch': 1.0}

 33%|██████████████████████████                                                    | 3408/10200 [04:17<40:15,  2.81it/s]


 33%|█████████████████████████▍                                                  | 3410/10200 [05:23<1:24:07,  1.35it/s]

 33%|█████████████████████████▍                                                  | 3411/10200 [05:57<2:01:39,  1.08s/it]
{'loss': 0.3341, 'grad_norm': 0.05035483464598656, 'learning_rate': 0.00013311764705882352, 'epoch': 1.0}


 33%|█████████████████████████▍                                                  | 3413/10200 [06:59<4:03:09,  2.15s/it]
{'loss': 0.2962, 'grad_norm': 0.0371924564242363, 'learning_rate': 0.00013307843137254903, 'epoch': 1.0}


 33%|█████████████████████████▍                                                  | 3415/10200 [08:11<8:16:29,  4.39s/it]

 33%|█████████████████████████                                                  | 3416/10200 [08:41<10:43:08,  5.69s/it]
{'loss': 0.4002, 'grad_norm': 0.07461776584386826, 'learning_rate': 0.00013301960784313724, 'epoch': 1.0}


 34%|█████████████████████████▏                                                 | 3418/10200 [09:33<16:48:21,  8.92s/it]

 34%|█████████████████████████▏                                                 | 3419/10200 [09:53<19:09:52, 10.17s/it]

 34%|█████████████████████████▏                                                 | 3420/10200 [10:30<26:16:41, 13.95s/it]

 34%|█████████████████████████▏                                                 | 3421/10200 [11:10<34:10:36, 18.15s/it]
 34%|█████████████████████████▏                                                 | 3421/10200 [11:10<34:10:36, 18.15s/it]Traceback (most recent call last):
  File "/home/geo_llm/llama3-finetune/HuMob_wandb_final.py", line 169, in <module>
    trainer.train(resume_from_checkpoint=artifact_dir)
  File "<string>", line 126, in train
  File "<string>", line 358, in _fast_inner_training_loop
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py", line 3324, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/accelerator.py", line 2151, in backward
    loss.backward(**kwargs)
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/geo_llm/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward