[34m[1mwandb[39m[22m: Downloading large artifact model-A_eval:epoch_8.0, 250.87MB. 11 files...
[34m[1mwandb[39m[22m:   11 of 11 files downloaded.
Done. 0:0:0.7
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 11,000 | Num Epochs = 9
O^O/ \_/ \    Batch size per device = 1 | Gradient Accumulation steps = 4
\        /    Total batch size = 4 | Total steps = 24,750
 "-____-"     Number of trainable parameters = 41,943,040
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory:
	eval_steps: 100 (from args) != 500 (from trainer_state.json)

  8%|██████▎                                                                       | 2001/24750 [00:39<07:32, 50.28it/s]


  8%|██████▎                                                                       | 2002/24750 [01:28<20:37, 18.38it/s]

  8%|██████▎                                                                       | 2003/24750 [02:22<41:29,  9.14it/s]
{'loss': 0.3441, 'grad_norm': 0.08815081417560577, 'learning_rate': 0.00019679995693653514, 'epoch': 0.73}


  8%|██████▏                                                                     | 2005/24750 [04:04<1:51:05,  3.41it/s]
{'loss': 0.3103, 'grad_norm': 0.09011247009038925, 'learning_rate': 0.00019679358171256463, 'epoch': 0.73}


  8%|██████▏                                                                     | 2007/24750 [05:26<3:37:53,  1.74it/s]

  8%|██████▏                                                                     | 2008/24750 [06:04<5:02:36,  1.25it/s]

  8%|██████▏                                                                     | 2009/24750 [06:37<6:44:36,  1.07s/it]

  8%|██████▏                                                                     | 2010/24750 [07:06<8:53:41,  1.41s/it]

  8%|██████                                                                     | 2011/24750 [07:44<12:44:38,  2.02s/it]

  8%|██████                                                                     | 2012/24750 [08:18<17:29:55,  2.77s/it]

  8%|██████                                                                     | 2013/24750 [09:10<27:30:59,  4.36s/it]

  8%|██████                                                                     | 2014/24750 [09:53<38:09:56,  6.04s/it]
{'loss': 0.3757, 'grad_norm': 0.08962200582027435, 'learning_rate': 0.00019676481598232139, 'epoch': 0.73}


  8%|██████                                                                     | 2016/24750 [11:11<66:37:43, 10.55s/it]
{'loss': 0.3224, 'grad_norm': 0.07222741097211838, 'learning_rate': 0.00019675840643975056, 'epoch': 0.73}

  8%|██████                                                                     | 2017/24750 [11:46<82:19:01, 13.04s/it]

  8%|██████                                                                    | 2018/24750 [12:26<103:19:58, 16.36s/it]


  8%|██████                                                                    | 2020/24750 [13:47<151:51:51, 24.05s/it]
{'loss': 0.3808, 'grad_norm': 0.06854107230901718, 'learning_rate': 0.00019674556863980933, 'epoch': 0.73}


  8%|██████                                                                    | 2022/24750 [14:59<180:21:36, 28.57s/it]
{'loss': 0.2923, 'grad_norm': 0.07389165461063385, 'learning_rate': 0.00019673914038326662, 'epoch': 0.74}

  8%|██████                                                                    | 2023/24750 [15:54<220:59:41, 35.01s/it]


  8%|██████                                                                    | 2025/24750 [17:06<226:36:18, 35.90s/it]

  8%|██████                                                                    | 2026/24750 [17:53<244:58:15, 38.81s/it]
{'loss': 0.3252, 'grad_norm': 0.0784170925617218, 'learning_rate': 0.0001967262651591093, 'epoch': 0.74}

  8%|██████                                                                    | 2027/24750 [18:26<234:57:18, 37.22s/it]


  8%|██████                                                                    | 2029/24750 [19:48<248:41:34, 39.40s/it]
{'loss': 0.3212, 'grad_norm': 0.07630555331707001, 'learning_rate': 0.00019671659237043788, 'epoch': 0.74}


  8%|██████                                                                    | 2031/24750 [21:01<239:32:36, 37.96s/it]

  8%|██████                                                                    | 2032/24750 [21:53<266:08:41, 42.17s/it]
{'loss': 0.2998, 'grad_norm': 0.07881339639425278, 'learning_rate': 0.00019670690555142245, 'epoch': 0.74}


  8%|██████                                                                    | 2034/24750 [23:15<257:58:48, 40.88s/it]

  8%|██████                                                                    | 2035/24750 [23:42<231:59:50, 36.77s/it]
{'loss': 0.3647, 'grad_norm': 0.07405239343643188, 'learning_rate': 0.0001966972047034682, 'epoch': 0.74}

  8%|██████                                                                    | 2036/24750 [24:23<239:37:08, 37.98s/it]

  8%|██████                                                                    | 2037/24750 [25:00<239:02:56, 37.89s/it]


  8%|██████                                                                    | 2039/24750 [26:48<288:37:50, 45.75s/it]

  8%|██████                                                                    | 2040/24750 [27:28<277:53:45, 44.05s/it]
{'loss': 0.3679, 'grad_norm': 0.0822017639875412, 'learning_rate': 0.0001966810054519481, 'epoch': 0.74}

  8%|██████                                                                    | 2041/24750 [28:03<260:36:17, 41.31s/it]


  8%|██████                                                                    | 2043/24750 [29:20<248:36:23, 39.41s/it]

  8%|██████                                                                    | 2044/24750 [29:58<246:11:40, 39.03s/it]

  8%|██████                                                                    | 2045/24750 [30:38<248:12:01, 39.35s/it]
{'loss': 0.3731, 'grad_norm': 0.09724612534046173, 'learning_rate': 0.0001966647672415912, 'epoch': 0.74}

  8%|██████                                                                    | 2046/24750 [31:17<246:17:57, 39.05s/it]


  8%|██████                                                                    | 2048/24750 [32:40<254:48:56, 40.41s/it]

  8%|██████▏                                                                   | 2049/24750 [33:33<278:29:12, 44.16s/it]

  8%|██████▏                                                                   | 2050/24750 [34:06<257:40:09, 40.86s/it]
{'loss': 0.3859, 'grad_norm': 0.10589607805013657, 'learning_rate': 0.00019664849007894092, 'epoch': 0.75}

  8%|██████▏                                                                   | 2051/24750 [34:43<250:46:28, 39.77s/it]

  8%|██████▏                                                                   | 2052/24750 [35:20<244:53:37, 38.84s/it]

  8%|██████▏                                                                   | 2053/24750 [35:48<224:26:44, 35.60s/it]

  8%|██████▏                                                                   | 2054/24750 [36:41<258:11:18, 40.95s/it]


  8%|██████▏                                                                   | 2056/24750 [38:03<263:06:50, 41.74s/it]
{'loss': 0.3416, 'grad_norm': 0.06464797258377075, 'learning_rate': 0.0001966289060759696, 'epoch': 0.75}


  8%|██████▏                                                                   | 2058/24750 [39:28<267:06:13, 42.37s/it]
