
2024-09-20 03:43:45,147 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-20 03:43:54,506 INFO Model and tokenizer initialized successfully.
2024-09-20 03:43:54,507 INFO Starting inference from index 2500 to 2599 on dataset datasetD_test_3000-5999.json
2024-09-20 03:43:54,507 INFO Loading dataset from datasetD_test_3000-5999.json...
2024-09-20 03:43:55,618 INFO Dataset loaded with 3000 conversations.
2024-09-20 03:43:55,621 INFO Loaded dataset datasetD_test_3000-5999.json with 3000 conversations
2024-09-20 03:43:55,623 INFO Selected subset of dataset from index 2500 to 2600
Map: 100%|███████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2251.48 examples/s]
2024-09-20 03:43:55,674 INFO Applied formatting prompts function to dataset
2024-09-20 03:43:55,675 INFO Total conversations to process: 100
2024-09-20 03:43:55,675 INFO Processing conversation 2500/2599
2024-09-20 03:43:55,700 INFO Input sequence length: torch.Size([1, 15509])
**************************************************Result Report**************************************************
geobleu: 0.0
dtw: 17758.103509762204
*****************************************************************************************************************
2024-09-20 03:53:32,236 INFO User 5500 processed in 576.56s
2024-09-20 03:53:32,240 INFO Processing conversation 2501/2599
2024-09-20 03:53:32,268 INFO Input sequence length: torch.Size([1, 13589])