
2024-09-16 15:07:22,141 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-16 15:07:42,033 INFO Model and tokenizer initialized successfully.
2024-09-16 15:07:42,034 INFO Starting inference from index 1200 to 1399 on dataset datasetD_test_3000-5999.json
2024-09-16 15:07:42,035 INFO Loading dataset from datasetD_test_3000-5999.json...
2024-09-16 15:07:43,610 INFO Dataset loaded with 3000 conversations.
2024-09-16 15:07:43,614 INFO Loaded dataset datasetD_test_3000-5999.json with 3000 conversations
2024-09-16 15:07:43,616 INFO Selected subset of dataset from index 1200 to 1400
Map: 100%|███████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1957.00 examples/s]
2024-09-16 15:07:43,728 INFO Applied formatting prompts function to dataset
2024-09-16 15:07:43,728 INFO Total conversations to process: 200
2024-09-16 15:07:43,729 INFO Processing conversation 1200/1399
2024-09-16 15:07:43,753 INFO Input sequence length: torch.Size([1, 8077])
2024-09-16 15:19:16,482 INFO User 4200 processed in 692.75s
2024-09-16 15:19:16,484 INFO Processing conversation 1201/1399
2024-09-16 15:19:16,517 INFO Input sequence length: torch.Size([1, 15805])
2024-09-16 15:36:54,638 INFO User 4201 processed in 1058.15s
2024-09-16 15:36:54,641 INFO Processing conversation 1202/1399
2024-09-16 15:36:54,657 INFO Input sequence length: torch.Size([1, 8277])
2024-09-16 15:45:14,409 INFO User 4202 processed in 499.77s
2024-09-16 15:45:14,411 INFO Processing conversation 1203/1399
2024-09-16 15:45:14,442 INFO Input sequence length: torch.Size([1, 17269])
2024-09-16 16:16:21,081 INFO User 4203 processed in 1866.67s
2024-09-16 16:16:21,083 INFO Processing conversation 1204/1399
2024-09-16 16:16:21,099 INFO Input sequence length: torch.Size([1, 9309])
2024-09-16 16:24:32,713 INFO User 4204 processed in 491.63s
2024-09-16 16:24:32,715 INFO Processing conversation 1205/1399
