
2024-09-14 03:48:31,378 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-14 03:48:39,940 INFO Model and tokenizer initialized successfully.
2024-09-14 03:48:39,940 INFO Starting inference from index 0 to 1 on dataset datasetC_eval_13600-16999.json
2024-09-14 03:48:39,940 INFO Loading dataset from datasetC_eval_13600-16999.json...
2024-09-14 03:48:40,724 INFO Dataset loaded with 3400 conversations.
2024-09-14 03:48:40,726 INFO Loaded dataset datasetC_eval_13600-16999.json with 3400 conversations
2024-09-14 03:48:40,728 INFO Selected subset of dataset from index 0 to 1
Map: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 42.78 examples/s]
2024-09-14 03:48:40,865 INFO Applied formatting prompts function to dataset
2024-09-14 03:48:40,865 INFO Total conversations to process: 1
2024-09-14 03:48:40,866 INFO Processing conversation 1/1
2024-09-14 03:48:40,875 INFO Input sequence length: torch.Size([1, 3781])
2024-09-14 03:49:34,051 INFO Conversation 1 processed in 53.19s
2024-09-14 03:49:34,053 INFO Failed conversations: []
2024-09-14 03:49:34,667 INFO Saving results to generated_datasetC_eval_13600-16999_range(0, 1).csv.gz
2024-09-14 03:49:34,668 INFO Results saved to generated_datasetC_eval_13600-16999_range(0, 1).csv.gz