
2024-09-14 23:36:25,285 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-14 23:36:34,946 INFO Model and tokenizer initialized successfully.
2024-09-14 23:36:34,947 INFO Starting inference from index 900 to 1199 on dataset datasetB_test_22000-24999.json
2024-09-14 23:36:34,947 INFO Loading dataset from datasetB_test_22000-24999.json...
2024-09-14 23:36:35,599 INFO Dataset loaded with 3000 conversations.
2024-09-14 23:36:35,602 INFO Loaded dataset datasetB_test_22000-24999.json with 3000 conversations
2024-09-14 23:36:35,603 INFO Selected subset of dataset from index 900 to 1200
Map: 100%|███████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4143.00 examples/s]
2024-09-14 23:36:35,680 INFO Applied formatting prompts function to dataset
2024-09-14 23:36:35,680 INFO Total conversations to process: 300
2024-09-14 23:36:35,680 INFO Processing conversation 900/1199
2024-09-14 23:36:35,694 INFO Input sequence length: torch.Size([1, 6949])
2024-09-14 23:38:39,171 INFO User 22900 processed in 123.49s
2024-09-14 23:38:39,173 INFO Processing conversation 901/1199
2024-09-14 23:38:39,184 INFO Input sequence length: torch.Size([1, 5925])
2024-09-14 23:40:29,789 INFO User 22901 processed in 110.62s
2024-09-14 23:40:29,790 INFO Processing conversation 902/1199
