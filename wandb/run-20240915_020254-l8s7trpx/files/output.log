
2024-09-15 02:02:59,776 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-15 02:03:11,729 INFO Model and tokenizer initialized successfully.
2024-09-15 02:03:11,730 INFO Starting inference from index 1800 to 2099 on dataset datasetB_test_22000-24999.json
2024-09-15 02:03:11,730 INFO Loading dataset from datasetB_test_22000-24999.json...
2024-09-15 02:03:12,407 INFO Dataset loaded with 3000 conversations.
2024-09-15 02:03:12,409 INFO Loaded dataset datasetB_test_22000-24999.json with 3000 conversations
2024-09-15 02:03:12,411 INFO Selected subset of dataset from index 1800 to 2100
Map: 100%|███████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4542.81 examples/s]
2024-09-15 02:03:12,482 INFO Applied formatting prompts function to dataset
2024-09-15 02:03:12,482 INFO Total conversations to process: 300
2024-09-15 02:03:12,483 INFO Processing conversation 1800/2099
2024-09-15 02:03:12,493 INFO Input sequence length: torch.Size([1, 3629])
2024-09-15 02:04:44,809 INFO User 23800 processed in 92.33s
2024-09-15 02:04:44,811 INFO Processing conversation 1801/2099
2024-09-15 02:04:44,825 INFO Input sequence length: torch.Size([1, 7421])
2024-09-15 02:09:23,731 INFO User 23801 processed in 278.92s
2024-09-15 02:09:23,732 INFO Processing conversation 1802/2099
2024-09-15 02:09:23,742 INFO Input sequence length: torch.Size([1, 4893])