
2024-09-19 03:00:00,691 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-19 03:00:11,302 INFO Model and tokenizer initialized successfully.
2024-09-19 03:00:11,303 INFO Starting inference from index 1650 to 1699 on dataset datasetD_test_3000-5999.json
2024-09-19 03:00:11,303 INFO Loading dataset from datasetD_test_3000-5999.json...
2024-09-19 03:00:12,409 INFO Dataset loaded with 3000 conversations.
2024-09-19 03:00:12,412 INFO Loaded dataset datasetD_test_3000-5999.json with 3000 conversations
2024-09-19 03:00:12,413 INFO Selected subset of dataset from index 1650 to 1700
Map: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 1508.60 examples/s]
2024-09-19 03:00:12,453 INFO Applied formatting prompts function to dataset
2024-09-19 03:00:12,453 INFO Total conversations to process: 50
2024-09-19 03:00:12,454 INFO Processing conversation 1650/1699
2024-09-19 03:00:12,472 INFO Input sequence length: torch.Size([1, 10349])
**************************************************Result Report**************************************************
geobleu: 0.0
dtw: 13806.746124787556
2024-09-19 03:08:09,841 INFO User 4650 processed in 477.39s
2024-09-19 03:08:09,844 INFO Processing conversation 1651/1699
2024-09-19 03:08:09,879 INFO Input sequence length: torch.Size([1, 14245])
