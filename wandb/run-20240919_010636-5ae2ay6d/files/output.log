
2024-09-19 01:06:40,739 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-19 01:06:50,543 INFO Model and tokenizer initialized successfully.
2024-09-19 01:06:50,544 INFO Starting inference from index 2258 to 2260 on dataset datasetC_test_17000-19999.json
2024-09-19 01:06:50,544 INFO Loading dataset from datasetC_test_17000-19999.json...
2024-09-19 01:06:51,199 INFO Dataset loaded with 3000 conversations.
2024-09-19 01:06:51,201 INFO Loaded dataset datasetC_test_17000-19999.json with 3000 conversations
2024-09-19 01:06:51,203 INFO Selected subset of dataset from index 2258 to 2261
Map: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 125.11 examples/s]
2024-09-19 01:06:51,232 INFO Applied formatting prompts function to dataset
2024-09-19 01:06:51,232 INFO Total conversations to process: 3
2024-09-19 01:06:51,232 INFO Processing conversation 2258/2260
2024-09-19 01:06:51,252 INFO Input sequence length: torch.Size([1, 5181])
2024-09-19 01:08:58,459 INFO User 19261 processed in 127.23s
2024-09-19 01:08:58,464 INFO Processing conversation 2259/2260
2024-09-19 01:08:58,479 INFO Input sequence length: torch.Size([1, 3949])
**************************************************Result Report**************************************************
geobleu: 0.0
dtw: 6978.856109187477
*****************************************************************************************************************
**************************************************Result Report**************************************************
geobleu: 0.0
dtw: 4449.0796115268
*****************************************************************************************************************
2024-09-19 01:10:16,908 INFO User 19256 processed in 78.44s
2024-09-19 01:10:16,913 INFO Processing conversation 2260/2260
2024-09-19 01:10:16,932 INFO Input sequence length: torch.Size([1, 6373])
2024-09-19 01:11:44,642 INFO User 19263 processed in 87.73s
2024-09-19 01:11:44,646 INFO Failed conversations: []
**************************************************Result Report**************************************************
geobleu: 5.536859284184669e-279
dtw: 4678.729608967367
*****************************************************************************************************************
2024-09-19 01:11:45,195 INFO Saving results to generated_datasetC_test_17000-19999_range(2258, 2261).csv.gz
2024-09-19 01:11:45,196 INFO Results saved to generated_datasetC_test_17000-19999_range(2258, 2261).csv.gz