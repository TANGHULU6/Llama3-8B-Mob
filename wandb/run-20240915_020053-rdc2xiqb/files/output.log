
2024-09-15 02:00:57,188 INFO Initializing model and tokenizer...
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: unsloth/llama-3-8b-Instruct-bnb-4bit can only handle sequence lengths of at most 8192.
But with kaiokendev's RoPE scaling of 6.104, it can be magically be extended to 50000!
Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2024-09-15 02:01:10,868 INFO Model and tokenizer initialized successfully.
2024-09-15 02:01:10,869 INFO Starting inference from index 1200 to 1499 on dataset datasetB_test_22000-24999.json
2024-09-15 02:01:10,869 INFO Loading dataset from datasetB_test_22000-24999.json...
2024-09-15 02:01:11,550 INFO Dataset loaded with 3000 conversations.
2024-09-15 02:01:11,553 INFO Loaded dataset datasetB_test_22000-24999.json with 3000 conversations
2024-09-15 02:01:11,554 INFO Selected subset of dataset from index 1200 to 1500
Map: 100%|███████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 4552.82 examples/s]
2024-09-15 02:01:11,624 INFO Applied formatting prompts function to dataset
2024-09-15 02:01:11,624 INFO Total conversations to process: 300
2024-09-15 02:01:11,625 INFO Processing conversation 1200/1499
2024-09-15 02:01:11,636 INFO Input sequence length: torch.Size([1, 3901])
2024-09-15 02:03:44,883 INFO User 23200 processed in 153.26s
2024-09-15 02:03:44,885 INFO Processing conversation 1201/1499
2024-09-15 02:03:44,901 INFO Input sequence length: torch.Size([1, 7061])
2024-09-15 02:09:05,335 INFO User 23201 processed in 320.45s
2024-09-15 02:09:05,336 INFO Processing conversation 1202/1499
2024-09-15 02:09:05,345 INFO Input sequence length: torch.Size([1, 3525])