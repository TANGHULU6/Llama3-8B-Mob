{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件总行数: 2996\n",
      "     user_id  status assistant_json\n",
      "444     4798  failed            NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = 'wandb_export_2024-09-19T19_18_42.011+08_00.csv'  # 替换为实际文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop_duplicates(subset='user_id')\n",
    "\n",
    "# 查看总行数\n",
    "total_rows = len(df)\n",
    "print(f\"文件总行数: {total_rows}\")\n",
    "\n",
    "filtered_df = df[df['status'] == 'failed']\n",
    "\n",
    "# 查看筛选后的行\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉status为failed的行\n",
    "df = df[df['status'] != 'failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺少的 userid:\n",
      "{5224, 5101, 5198, 5241, 4798}\n"
     ]
    }
   ],
   "source": [
    "# 筛选出 userid 在 3000 到 5999 范围内的数据\n",
    "original_userids = set(range(3000, 6000))\n",
    "unique_userids = set(df['user_id'].unique())\n",
    "\n",
    "# 找到去重后消失的 userid\n",
    "removed_userids = original_userids - unique_userids\n",
    "\n",
    "print(\"缺少的 userid:\")\n",
    "print(removed_userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2987\n",
      "未命中：\n",
      "set()\n",
      "仍然失败：\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "fix_df = pd.read_csv('wandb_export_2024-09-20T02_09_57.075+08_00.csv')  # 替换为你的修正文件路径\n",
    "\n",
    "# 获取需要替换的user_id\n",
    "# replace_user_ids = fix_df['user_id'].unique()\n",
    "replace_user_ids = [5224, 5101, 5198, 5241, 4798, 3706, 3751, 3797, 3944, 5426, 5585, 5659, 5976]\n",
    "# 从原始df中删除需要替换的user_id数据\n",
    "df = df[~df['user_id'].isin(replace_user_ids)]\n",
    "print(len(df))\n",
    "print(\"未命中：\")\n",
    "print(set(replace_user_ids) - set(fix_df['user_id'].unique()))\n",
    "fix_df = fix_df[fix_df['status'] == 'success']\n",
    "fix_df = fix_df[fix_df['user_id'].isin(replace_user_ids)]\n",
    "print(\"仍然失败：\")\n",
    "print(sorted(set(replace_user_ids) - set(fix_df['user_id'].unique())))\n",
    "\n",
    "# 将fix_df中的数据合并到df中\n",
    "df = pd.concat([df, fix_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "压缩后的文件总行数: 3000\n",
      "前几行内容:\n",
      "   user_id   status                                     assistant_json\n",
      "0     3000  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "1     3001  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "2     3002  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "3     3003  success  {\"reason\": \"The individual's trajectory shows ...\n",
      "4     3004  success  {\"reason\": \"The individual's trajectory shows ...\n"
     ]
    }
   ],
   "source": [
    "# 重新排序（按user_id排序）\n",
    "df_sorted = df.sort_values(by='user_id').reset_index(drop=True)\n",
    "# 查看总行数\n",
    "total_rows = len(df_sorted)\n",
    "print(f\"压缩后的文件总行数: {total_rows}\")\n",
    "\n",
    "# 查看前几行\n",
    "print(\"前几行内容:\")\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_column(json_str):\n",
    "    return json.loads(json_str)\n",
    "\n",
    "# 解析assistant_json列\n",
    "df_sorted['parsed_json'] = df_sorted['assistant_json'].apply(parse_json_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取后的前几行内容:\n",
      "   user_id   status                                         prediction\n",
      "0     3000  success  [[60, 13, 196, 20], [60, 14, 196, 19], [60, 15...\n",
      "1     3001  success  [[60, 14, 114, 115], [60, 15, 117, 117], [60, ...\n",
      "2     3002  success  [[60, 0, 189, 195], [60, 1, 189, 195], [60, 2,...\n",
      "3     3003  success  [[60, 1, 182, 28], [60, 2, 177, 29], [60, 8, 1...\n",
      "4     3004  success  [[60, 1, 121, 116], [60, 2, 120, 116], [60, 3,...\n",
      "压缩后的文件总行数: 3000\n"
     ]
    }
   ],
   "source": [
    "df_sorted['prediction'] = df_sorted['parsed_json'].apply(lambda x: x['prediction'] if x else None)\n",
    "\n",
    "# 查看解析后的数据\n",
    "print(\"提取后的前几行内容:\")\n",
    "print(df_sorted[['user_id', 'status', 'prediction']].head())\n",
    "\n",
    "# 保存提取后的结果为新文件\n",
    "total_rows = len(df_sorted)\n",
    "print(f\"压缩后的文件总行数: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id   d   t    x   y\n",
      "0     3000  60  13  196  20\n",
      "1     3000  60  14  196  19\n",
      "2     3000  60  15  196  20\n",
      "3     3000  60  16  196  20\n",
      "4     3000  60  17  196  20\n",
      "文件总行数: 824733\n",
      "展开后的数据包含 3000 个不同的用户。\n"
     ]
    }
   ],
   "source": [
    "expanded_data = []\n",
    "\n",
    "# 遍历每一行\n",
    "for index, row in df_sorted.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    predictions = row['prediction']\n",
    "    \n",
    "    # 遍历每个预测值，将其添加到expanded_data列表中\n",
    "    for pred in predictions:  # 使用eval解析字符串形式的列表\n",
    "        expanded_data.append([user_id] + pred)\n",
    "\n",
    "# 将展开的数据转换为DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_data, columns=['user_id', 'd', 't', 'x', 'y'])\n",
    "\n",
    "# 查看转换后的数据\n",
    "print(expanded_df.head())\n",
    "total_rows = len(expanded_df)\n",
    "print(f\"文件总行数: {total_rows}\")\n",
    "# 确保 expanded_df 已经创建\n",
    "unique_users = expanded_df['user_id'].nunique()\n",
    "print(f\"展开后的数据包含 {unique_users} 个不同的用户。\")\n",
    "expanded_df.to_csv('cityD.csv.gz', compression='gzip', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bl4mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
